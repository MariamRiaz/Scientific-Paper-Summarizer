0,1,label2,summary_sentences
A key to the success of probabilistic modeling is the pairing of rich probability models with fast and accurate inference algorithms.,1. Introduction,[0],[0]
Probabilistic graphical models enable this by providing a flexible class of probability distributions together with algorithms that exploit the graph structure for efficient inference.,1. Introduction,[0],[0]
"However, exact inference algorithms are only available when both the distributions involved and the graph structure are simple enough.",1. Introduction,[0],[0]
"How-
",1. Introduction,[0],[0]
"1College of Information and Computer Sciences, University of Massachusetts Amherst 2Department of Computer Science, Mount Holyoke College.",1. Introduction,[0],[0]
"Correspondence to: Kevin Winner <kwinner@cs.umass.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ever, this situation is rare and consequently, much research today is devoted to general-purpose approximate inference techniques (e.g. Ranganath et al., 2014; Kingma & Welling, 2014; Carpenter et al., 2016).
",1. Introduction,[0],[0]
"Despite many advances in probabilistic inference, there remain relatively simple (and useful) models for which exact inference algorithms are not available.",1. Introduction,[0],[0]
This paper considers the case of graphical models with a simple structure but with (unbounded) latent count random variables.,1. Introduction,[0],[0]
"These are a natural modeling choice for many real world problems in ecology (Zonneveld, 1991; Royle, 2004; Dail & Madsen, 2011) and epidemiology (Farrington et al., 2003; Panaretos, 2007; Kvitkovicova & Panaretos, 2011).",1. Introduction,[0],[0]
"However, they pose a unique challenge for inference: even though algorithms like belief propagation (Pearl, 1986) or variable elimination (Zhang & Poole, 1994) are well defined mathematically, they cannot be implemented in an obvious way because factors have a countably infinite number of entries.",1. Introduction,[0],[0]
"As a result, approximations like truncating the support of the random variables or MCMC are applied (Royle, 2004; Gross et al., 2007; Chandler et al., 2011; Dail & Madsen, 2011; Zipkin et al., 2014; Winner et al., 2015).
",1. Introduction,[0],[0]
"Recently, Winner & Sheldon (2016) introduced a new technique for exact inference in models with latent count variables.",1. Introduction,[0],[0]
"Their approach executes the same operations as variable elimination, but with factors, which are infinite sequences of values, represented in a compact way using probability generating functions (PGFs).",1. Introduction,[0],[0]
"They developed an efficient exact inference algorithm for a specific class of Poisson hidden Markov models (HMMs) that represent a population undergoing mortality and immigration, and noisy observations of the population over time.
",1. Introduction,[0],[0]
A key open question is the extent to which PGF-based inference generalizes to a broader class of models.,1. Introduction,[0],[0]
There are two primary considerations.,1. Introduction,[0],[0]
"First, for what types of factors can the required operations (multiplication, marginalization, and conditioning) be “lifted” to PGF-based representations?",1. Introduction,[0],[0]
"Here, there is significant room for generalization: the mathematical PGF operations developed in (Winner & Sheldon, 2016) already apply to a broad class of nonPoisson immigration models, and we will generalize the models further to allow richer models of population survival and growth.",1. Introduction,[0],[0]
"Second, and more significantly, for what
types of PGFs can the requisite mathematical operations be implemented efficiently?",1. Introduction,[0],[0]
Winner & Sheldon (2016) manipulated PGFs symbolically.,1. Introduction,[0],[0]
"Their compact symbolic representation seems to rely crucially on properties of the Poisson distribution; it remains unclear whether symbolic PGF inference can be generalized beyond Poisson models.
",1. Introduction,[0],[0]
"This paper introduces a new algorithmic technique based on higher-order automatic differentiation (Griewank & Walther, 2008) for inference with PGFs.",1. Introduction,[0],[0]
A key insight is that most inference tasks do not require a full symbolic representation of the PGF.,1. Introduction,[0],[0]
"For example, the likelihood is computed by evaluating a PGF F (s) at s = 1.",1. Introduction,[0],[0]
Other probability queries can be posed in terms of derivatives F (k)(s) evaluated at either s = 0 or s = 1.,1. Introduction,[0],[0]
"In all cases, it suffices to evaluate F and its higher-order derivatives at particular values of s, as opposed to computing a compact symbolic representation of F .",1. Introduction,[0],[0]
"It may seem that this problem is then solved by standard techniques, such as higher-order forward-mode automatic differentiation (Griewank & Walther, 2008).",1. Introduction,[0],[0]
"However, the requisite PGF F is complex—it is defined recursively in terms of higher-order derivatives of other PGFs—and offthe-shelf automatic differentiation methods do not apply.",1. Introduction,[0],[0]
"We therefore develop a novel recursive procedure using building blocks of forward-mode automatic differentiation (generalized dual numbers and univariate Taylor polynomials; Griewank & Walther, 2008) to evaluate F and its derivatives.
",1. Introduction,[0],[0]
"Our algorithmic contribution leads to the first efficient exact algorithms for a class of HMMs that includes many well-known models as special cases, and has many applications.",1. Introduction,[0],[0]
"The hidden variables represent a population that undergoes three different processes: mortality (or emigration), immigration, and growth.",1. Introduction,[0],[0]
A variety of different distributional assumptions may be made about each process.,1. Introduction,[0],[0]
The models may also be viewed without this interpretation as a flexible class of models for integer-valued time series.,1. Introduction,[0],[0]
"Special cases include models from population ecology (Royle, 2004; Gross et al., 2007; Dail & Madsen, 2011), branching processes (Watson & Galton, 1875; Heathcote, 1965), queueing theory (Eick et al., 1993), and integer-valued autoregressive models (McKenzie, 2003).",1. Introduction,[0],[0]
Additional details about the relation to these models are given in Section 2.,1. Introduction,[0],[0]
"Our algorithms permit exact calculation of the likelihood for all of these models even when they are partially observed.
",1. Introduction,[0],[0]
"We demonstrate experimentally that our new exact inference algorithms are more scalable than competing approximate approaches, and support learning via exact likelihood calculations in a broad class of models for which this was not previously possible.",1. Introduction,[0],[0]
"We consider a hidden Markov model with integer latent variables N
1 , . . .",2. Model and Problem Statement,[0],[0]
", NK and integer observed variables Y 1
, . . .",2. Model and Problem Statement,[0],[0]
", YK .",2. Model and Problem Statement,[0],[0]
All variables are assumed to be non-negative.,2. Model and Problem Statement,[0],[0]
"The model is most easily understood in the context of its application to population ecology or branching processes (which are similar): in these cases, the variable Nk represents the size of a hidden population at time tk, and Yk represents the number of individuals that are observed at time tk.",2. Model and Problem Statement,[0],[0]
"However, the model is equally valid without this interpretation as a flexible class of autoregressive processes (McKenzie, 2003).
",2. Model and Problem Statement,[0],[0]
We introduce some notation to describe the model.,2. Model and Problem Statement,[0],[0]
"For an integer random variable N , write Y = ⇢ N to mean that Y ⇠ Binomial(N, ⇢).",2. Model and Problem Statement,[0],[0]
This operation is known as “binomial thinning”: the count Y is the number of “survivors” from the original count N .,2. Model and Problem Statement,[0],[0]
We can equivalently write Y = PN i=1,2. Model and Problem Statement,[0],[0]
Xi for iid Xi ⇠ Bernoulli(⇢) to highlight the fact that this is a compound distribution.,2. Model and Problem Statement,[0],[0]
"Indeed, compound distributions will play a key role: for independent integer random variables N and X , let Z = N X denote the compound random variable Z = PN i=1",2. Model and Problem Statement,[0],[0]
"Xi, where {Xi} are independent copies of X .",2. Model and Problem Statement,[0],[0]
"Now, we can describe our model as:
Nk = (Nk 1 Xk) +Mk, (1) Yk = ⇢k Nk. (2)
",2. Model and Problem Statement,[0],[0]
The variable Nk represents the population size at time tk.,2. Model and Problem Statement,[0],[0]
The random variable Nk 1 Xk 1 = PNk 1 i=1,2. Model and Problem Statement,[0],[0]
"Xk 1,i is the number of offspring of individuals from the previous time step, where Xk 1,i is the total number of individuals “caused by” the ith individual alive at time tk 1.",2. Model and Problem Statement,[0],[0]
"This definition of offspring is flexible enough to model immediate offspring, surviving individuals, and descendants of more than one generation.",2. Model and Problem Statement,[0],[0]
"The random variable Mk is the number of immigrants at time tk, and Yk is the number of individuals observed at time tk, with the assumption that each individual is observed independently with probability ⇢",2. Model and Problem Statement,[0],[0]
"k. We have left unspecified the distributions of Mk and Xk, which we term the immigration and offspring distributions, respectively.",2. Model and Problem Statement,[0],[0]
These may be arbitrary distributions over non-negative integers.,2. Model and Problem Statement,[0],[0]
"We will assume the initial condition N
0 = 0, though the model can easily be extended to accommodate arbitrary initial distributions.
",2. Model and Problem Statement,[0],[0]
Problem Statement We use lower case variables to denote specific settings of random variables.,2. Model and Problem Statement,[0],[0]
"Let yi:j = (yi, . . .",2. Model and Problem Statement,[0],[0]
", yj) and ni:j = (ni, . . .",2. Model and Problem Statement,[0],[0]
", nj).",2. Model and Problem Statement,[0],[0]
"The model above defines a joint probability mass function (pmf) p(n
1:K , y1:K ; ✓) where we introduce the vector ✓ containing parameters of all component distributions when necessary.",2. Model and Problem Statement,[0],[0]
"It is clear that the density factors according to a hidden Markov model: p(n
1:K , y1:K) =
QK k=1 p(nk |nk 1)p(yk |nk).",2. Model and Problem Statement,[0],[0]
"We will consider several inference problems that are standard for HMMs, but pose unique challenges when the hidden variables have countably infinite support.",2. Model and Problem Statement,[0],[0]
"Specifically, suppose y
1:K are observed, then we seek to:
• Compute the likelihood L(✓) = p(y 1:K ; ✓) for any ✓,
• Compute moments and values of the pmf of the filtered marginals p(nk | y1:k; ✓), for any k, ✓,
• Estimate parameters ✓ by maximizing the likelihood.
",2. Model and Problem Statement,[0],[0]
"We focus technically on the first two problems, which will enable numerical optimization to maximize the likelihood.",2. Model and Problem Statement,[0],[0]
Another standard problem is to compute smoothed marginals p(nk | y1:K ; ✓) given both past and future observations relative to time step k.,2. Model and Problem Statement,[0],[0]
"Although this is interesting, it is technically more difficult, and we defer it for future work.
",2. Model and Problem Statement,[0],[0]
Connections to Other Models This model specializes to capture many different models in the literature.,2. Model and Problem Statement,[0],[0]
The latent process of Eq.,2. Model and Problem Statement,[0],[0]
"(1) is a Galton-Watson branching process with immigration (Watson & Galton, 1875; Heathcote, 1965).",2. Model and Problem Statement,[0],[0]
"It also captures a number of different AR(1) (first-order autoregressive) processes for integer variables (McKenzie, 2003); these typically assume Xk ⇠ Bernoulli( k), i.e., that the offspring process is binomial thinning of the current individuals.",2. Model and Problem Statement,[0],[0]
"For clarity when describing this as an offspring distribution, we will refer to it as Bernoulli offspring.",2. Model and Problem Statement,[0],[0]
"With Bernoulli offspring and time-homogenous Poisson immigration, the model is an M/M/1 queue (McKenzie, 2003); with time-varying Poisson immigration it is an Mt/M/1 queue (Eick et al., 1993).",2. Model and Problem Statement,[0],[0]
"For each of these models, we contribute the first known algorithms for exact inference and likelihood calculations when the process is partially observed.",2. Model and Problem Statement,[0],[0]
"This allows estimation from data that is noisy and has variability that should not be modeled by the latent process.
",2. Model and Problem Statement,[0],[0]
Special cases of our model with noisy observations occur in statistical estimation problems in population ecology.,2. Model and Problem Statement,[0],[0]
"When immigration is zero after the first time step and Xk = 1, the population size is a fixed random variable, and we recover the N -mixture model of Royle (2004) for estimating the size of an animal population from repeated counts.",2. Model and Problem Statement,[0],[0]
"With Poisson immigration and Bernoulli offspring, we recover the basic model of Dail & Madsen (2011) for open metapopulations; extended versions with overdispersion and population growth also fall within our framework by using negative-binomial immigration and Poisson offspring.",2. Model and Problem Statement,[0],[0]
"Related models for insect populations also fall within our framework (Zonneveld, 1991; Gross et al., 2007; Winner et al., 2015).",2. Model and Problem Statement,[0],[0]
The main goal in most of this literature is parameter estimation.,2. Model and Problem Statement,[0],[0]
"Until very recently, no exact algorithms were known to compute the likelihood, so ap-
proximations such as truncating the support of the latent variables (Royle, 2004; Fiske & Chandler, 2011; Chandler et al., 2011; Dail & Madsen, 2011) or MCMC (Gross et al., 2007; Winner et al., 2015) were used.",2. Model and Problem Statement,[0],[0]
Winner & Sheldon (2016) introduced PGF-based exact algorithms for the restricted version of the model with Bernoulli offspring and Poisson immigration.,2. Model and Problem Statement,[0],[0]
We will build on that work to provide exact inference and likelihood algorithms for all of the aforementioned models.,2. Model and Problem Statement,[0],[0]
"The standard approach for inference in HMMs is the forward-backward algorithm (Rabiner, 1989), which is a special case of more general propagation or messagepassing algorithms (Pearl, 1986; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990; Shenoy & Shafer, 1990).",3. Methods,[0],[0]
"Winner & Sheldon (2016) showed how to implement the forward algorithm using PGFs for models with Bernoulli offspring and Poisson immigration.
",3. Methods,[0],[0]
Forward Algorithm,3. Methods,[0],[0]
"The forward algorithm recursively computes “messages”, which are unnormalized distributions of subsets of the variables.",3. Methods,[0],[0]
"Specifically, define ↵k(nk) :",3. Methods,[0],[0]
"= p(nk, y1:k) and k(nk)",3. Methods,[0],[0]
":= p(nk, y1:k 1).",3. Methods,[0],[0]
"These satisfy the recurrence:
k(nk) =",3. Methods,[0],[0]
"X
nk 1
↵k 1(nk 1)p(nk |nk 1), (3)
↵k(nk) = k(nk)p(yk |nk).",3. Methods,[0],[0]
"(4)
We will refer to Equation (3) as the prediction step (the value of nk is predicted based on the observations y1:k 1), and Equation (4) as the evidence step (the new evidence yk is incorporated).",3. Methods,[0],[0]
"In finite models, the forward algorithm can compute the ↵k messages for k = 1, . . .",3. Methods,[0],[0]
",K directly using Equations (3) and (4).",3. Methods,[0],[0]
"However, if nk is unbounded, this cannot be done directly; for example, ↵k(nk) is an infinite sequence, and Equation (3) contains an infinite sum.",3. Methods,[0],[0]
"Winner & Sheldon (2016) observed that, for some conditional distributions p(nk |nk 1) and p(yk |nk), the operations of the forward algorithm can be carried out using PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Specifically, define the PGFs k(uk) and Ak(sk) of k(nk) and ↵k(nk), respectively, as:
k(uk) := 1X
nk=0
k(nk)u nk k , (5)
Ak(sk) := 1X
nk=0
↵k(nk)s nk k .",3.1. Forward Algorithm with PGFs,[0],[0]
"(6)
The PGFs k and Ak are power series in the variables uk and sk with coefficients equal to the message entries.
",3.1. Forward Algorithm with PGFs,[0],[0]
These functions capture all relevant information about the associated distributions.,3.1. Forward Algorithm with PGFs,[0],[0]
"Technically, k and Ak are unnormalized PGFs because the coefficients do not sum to one.",3.1. Forward Algorithm with PGFs,[0],[0]
"However, the normalization constants are easily recovered by evaluating the PGF on input value 1: for example, Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
"P nk
↵k(nk) = p(y1:k).",3.1. Forward Algorithm with PGFs,[0],[0]
This also shows that we can recover the likelihood as AK(1) = p(y1:K).,3.1. Forward Algorithm with PGFs,[0],[0]
"After normalizing, the PGFs can be interpreted as expectations, for example Ak(sk)/Ak(1) =",3.1. Forward Algorithm with PGFs,[0],[0]
E[sNkk,3.1. Forward Algorithm with PGFs,[0],[0]
"| y1:k].
In general, it is well known that the PGF F (s) of a nonnegative integer-valued random variable X uniquely defines the entries of the probability mass function and the moments of X , which are recovered from (higher-order) derivatives of F evaluated at zero and one, respectively:
Pr(X = r)",3.1. Forward Algorithm with PGFs,[0],[0]
=,3.1. Forward Algorithm with PGFs,[0],[0]
"F (r)(0)/r!, (7)
E[X] = F (1)(1), (8)
Var(X) = F (2)(1)",3.1. Forward Algorithm with PGFs,[0],[0]
h F (1)(1),3.1. Forward Algorithm with PGFs,[0],[0]
i 2 + F (1)(1).,3.1. Forward Algorithm with PGFs,[0],[0]
"(9)
More generally, the first q moments are determined by the derivatives F (r)(1) for r  q.",3.1. Forward Algorithm with PGFs,[0],[0]
"Therefore, if we can evaluate the PGF Ak and its derivatives for sk 2 {0, 1}, we can answer arbitrary queries about the filtering distributions p(nk, y1:k), and, in particular, solve our three stated inference problems.
",3.1. Forward Algorithm with PGFs,[0],[0]
"But how can we compute values of Ak, k, and their derivatives?",3.1. Forward Algorithm with PGFs,[0],[0]
What form do these PGFs have?,3.1. Forward Algorithm with PGFs,[0],[0]
"One key result of Winner & Sheldon (2016), which we generalize here, is the fact that there is also a recurrence relation among the PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
Proposition 1.,3.1. Forward Algorithm with PGFs,[0],[0]
Consider the probability model defined in Equations (1) and (2).,3.1. Forward Algorithm with PGFs,[0],[0]
"Let Fk be the PGF of the offspring random variable Xk, and let Gk be the PGF of the immigration random variable Mk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Then k and Ak satisfy the following recurrence:
k(uk) =",3.1. Forward Algorithm with PGFs,[0],[0]
"Ak 1 Fk(uk) ·Gk(uk) (10)
Ak(sk)",3.1. Forward Algorithm with PGFs,[0],[0]
"= (sk⇢k)yk
yk! · (yk)k sk(1 ⇢k)
",3.1. Forward Algorithm with PGFs,[0],[0]
"(11)
Proof.",3.1. Forward Algorithm with PGFs,[0],[0]
"A slightly less general version of Equation (10) appeared in Winner & Sheldon (2016); the general version appears in the literature on branching processes with immigration (Heathcote, 1965).",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) follows directly from general PGF operations outlined in (Winner & Sheldon, 2016).
",3.1. Forward Algorithm with PGFs,[0],[0]
The PGF recurrence has the same two elements as the pmf recurrence in equations (3) and (4).,3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (10) is the prediction step: it describes the PGF of k(nk) = p(nk, y1:k 1) in terms of previous PGFs.",3.1. Forward Algorithm with PGFs,[0],[0]
"Equation (11) is the evidence step: it describes the PGF for ↵k(nk) =
p(nk, y1:k) in terms of the previous PGF and the new observation yk.",3.1. Forward Algorithm with PGFs,[0],[0]
"Note that the evidence step involves the ykth derivative of the PGF k from the prediction step, where yk is the observed count.",3.1. Forward Algorithm with PGFs,[0],[0]
These high-order derivatives complicate the calculation of the PGFs.,3.1. Forward Algorithm with PGFs,[0],[0]
The recurrence reveals structure about Ak and k but does not immediately imply an algorithm.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Winner & Sheldon (2016) showed how to use the recurrence to compute symbolic representations of all PGFs in the special case of Bernoulli offspring and Poisson immigration: in this case, they proved that all PGFs have the form F (s) = f(s)",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"exp(as + b), where f is a polynomial of bounded degree.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Hence, they can be represented compactly and computed efficiently using the recurrence.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The result is a symbolic representation, so, for example, one obtains a closed form representation of the final PGF AK , from which the likelihood, entries of the pmf, and moments can be calculated.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"However, the compact functional form f(s) exp(as + b) seems to rely crucially on properties of the Poisson distribution.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"When other distributions are used, the size of the symbolic PGF representation grows quickly with K. It is an open question whether the symbolic methods can be extended to other classes of PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
This motivates an alternate approach.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Instead of computing Ak symbolically, we will evaluate Ak and its derivatives at particular values of sk corresponding to the queries we wish to make (cf. Equations (7)–(9)).",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"To develop the approach, it is helpful to consider the feed-forward computation for evaluating Ak at a particular value sk.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"The circuit diagram in Figure 1 is a directed acyclic graph that describes this calculation; the nodes are intermediate quantities in the calculation, and the shaded rectangles illustrate the recursively nested PGFs.
",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Now, we can consider techniques from automatic differentiation (autodiff) to compute Ak and its derivatives.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"How-
ever, these will not apply directly.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Note that Ak is defined in terms of higher-order derivatives of the function k, which depends on higher-order derivatives of k 1, and so forth.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
Standard autodiff techniques cannot handle these recursively nested derivatives.,3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
"Therefore, we will develop a novel algorithm.",3.2. Evaluating Ak via Automatic Differentiation,[0],[0]
We now develop basic notation and building blocks that we will assemble to construct our algorithm.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"It is helpful to abstract from our particular setting and describe a general model for derivatives within a feed-forward computation, following Griewank & Walther (2008).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We consider a procedure that assigns values to a sequence of variables v 0 , v 1
, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn, where v0 is the input variable, vn is the output variable, and each intermediate variable vj is computed via a function 'j(vi)i j of some subset (vi)i j of the variables v
0:j 1.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Here the dependence relation,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i j simply means that 'j depends directly on vi, and (vi)i j is the vector of variables for which that is true.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Note that the dependence relation defines a directed acyclic graph G (e.g., the circuit in Figure 1), and v
0 , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", vn is a topological ordering of G.
We will be concerned with the values of a variable v` and its derivatives with respect to some earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"To represent this cleanly, we first introduce a notation to capture the partial computation between the assignment of vi and v`.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"For i  `, define fi`(v0:i) to be the value that is assigned to v` if the values of the first i variables are given by v 0:i (now treated as fixed input values).",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This can be defined formally in an inductive fashion:
fi`(v0:i) = '`(uij)j `, uij = ( vj if j  i fij(v0:i) if j >",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"i
This can be interpreted as recursion with memoization for v 0:i. When '` “requests” the value of uij of vj : if j  i, this value was given as an input argument of fi`, so we just “look it up”; but if j > i, we recursively compute the correct value via the partial computation from i to j. Now, we define a notation to capture derivatives of a variable v` with respect to an earlier variable vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Definition 1 (Dual numbers).,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"The generalized dual number hv`, dviiq for 0  i  ` and q > 0 is the sequence consisting of v` and its first q derivatives with respect to vi:
hv`,",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"dviiq = ✓ @p
@vpi fi`(v0:i)
◆q
p=0
We say that hv`, dviiq is a dual number of order q with respect to vi.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Let DRq be the set of dual numbers of order q.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"We will commonly write dual numbers as:
hs, duiq = ⇣ s, ds
du , . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
",
dqs
duq
⌘
in which case it is understood that s = v` and u = vi for some 0  i  `, and the function fi`(·) will be clear from context.
",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
Our treatment of dual numbers and partial computations is more explicit than what is standard.,3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In particular, we are explicit both about the variable v` we are differentiating and the variable vi with respect to which we are differentiating.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"This is important for our algorithm, and also helps distinguish our approach from traditional automatic differentiation approaches.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"Forward-mode autodiff computes derivatives of all variables with respect to v
0 , i.e., it computes hvj , dv0iq for j = 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", n. Reverse-mode autodiff computes derivatives of vn with respect to all variables, i.e., it computes hvn, dviiq for i = n 1, . . .",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
", 0.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
"In each case, one of the two variables is fixed, so the notation can be simplified.",3.2.1. COMPUTATION MODEL AND DUAL NUMBERS,[0],[0]
The general idea of our algorithm will resemble forwardmode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Instead of sequentially calculating the values v
1 , . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", vn in our feed-forward computation, we will calculate dual numbers hv
1 , dvi1iq1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hvn, dviniqn , where we leave unspecified (for now) the variables with respect to which we differentiate, and the order of the dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
We will require three high-level operations on dual numbers.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The first one is “lifting” a scalar function.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 2 (Lifted Function).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function of variables x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The qth-order lifted function Lqf : (DRq)m !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"DRq is the function that accepts as input dual numbers hx
1 , duiq, . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", hxm, duiq of order q with respect to the same variable u, and returns the value⌦ f(x
1 , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", xm), du ↵ q .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Lifting is the basic operation of higher-order forward mode autodiff.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For functions f consisting only of “primitive operations”, the lifted function Lqf can be computed at a modest overhead relative to computing f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Proposition 2 (Griewank & Walther, 2008).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Let f : Rm !,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"R be a function that consists only of the following primitive operations, where x and y are arbitrary input variables and all other numbers are constants: x",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"+ cy, x ⇤ y, x/y, xr, ln(x), exp(x), sin(x), cos(x).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Then Lqf can be computed in time O(q2) times the running time of f .
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Based on this proposition, we will write algebraic operations on dual numbers, e.g., hx, duiq⇥hy, duiq , and understand these to be lifted versions of the corresponding scalar operations.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The standard lifting approach is to represent dual numbers as univariate Taylor polynomials (UTPs), in which case many operations (e.g., multiplication, addition) translate directly to the corresponding operations on polynomials.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will use UTPs in the proof of Theorem 1.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The second operation we will require is composition.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Say that variable vj separates vi from v` if all paths from vi to v` in G go through vj .,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Theorem 1 (Composition).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Suppose vj separates vi from v`.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"In this case, the dual number hv`, dviiq depends only on the dual numbers hv`, dvjiq and hvj , dviiq , and we define the composition operation:
hv`, dvjiq hvj , dviiq := hv`, dviiq
If vj does not separate vi from v`, the written composition operation is undefined.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The composition operation can be performed in O(q2 log q) time by composing two UTPs.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Proof.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"If all paths from vi to v` go through vj , then vj is a “bottleneck” in the partial computation fil.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Specifically, there exist functions F and H such that vj = F (vi) and v` = H(vj).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Here, the notation suppresses dependence on variables that either are not reachable from vi, or do not have a path to v`, and hence may be treated as constants because they they do not impact the dual number hv`, viiq .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
A detailed justification of this is given in the supplementary material.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Now, our goal is to compute the higher-order derivatives of v` = H(F (vi)).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let ˆF and ˆH be infinite Taylor expansions about vi and vj , respectively, omitting the constant terms F (vi) and H(vj):
ˆF ("") := 1X
p=1
F (p)(vi)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p, ˆH("") :=
1X
p=1
H(p)(vj)
p!",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"""p.
These are polynomials in "", and the first q coefficients are given in the input dual numbers.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"The coefficient of ""p in ˆU("") := ˆH( ˆF ("")) for p 1 is exactly dpv`/dvpi (see Wheeler, 1987, where the composition of Taylor polynomials is related directly to the higher-order chain rule known as Faà dı́ Bruno’s Formula).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
So it suffices to compute the first q coefficients of ˆH( ˆF (✏)).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This can be done by executing Horner’s method (Horner, 1819) in truncated Taylor polynomial arithmetic (Griewank & Walther, 2008), which keeps only the first q coefficients of all polynomials (i.e., it assumes ✏p = 0 for p > q).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"After truncation, Horner’s method involves q additions and q multiplications of polynomials of degree at most q. Polynomial multiplication takes time O(q log q) using the FFT, so the overall running time is O(q2 log q).
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
The final operation we will require is differentiation.,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This will support local functions '` that differentiate a previous value, e.g., v` = '`(vj) = dpvj/dvpi .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
Definition 3 (Differential Operator).,3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"Let hs, duiq be a dual number.",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"For p  q, the differential operator Dp applied to hs, duiq returns the dual number of order q p given by:
Dphs, duiq := ⇣ dps dup , . . .",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
", dqs duq ⌘
The differential operator can be applied in O(q) time.
",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"This operation was defined in (Kalaba & Tesfatsion, 1986).",3.2.2. OPERATIONS ON DUAL NUMBERS,[0],[0]
"We will now use these operations to lift the function Ak to compute h↵k, skiq = LA hsk, dskiq), i.e., the output of Ak and its derivatives with respect to its input.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 1 gives a sequence of mathematical operations to compute Ak(sk).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Algorithm 2 shows the corresponding operations on dual numbers; we call this algorithm the generalized dual-number forward algorithm or GDUALFORWARD.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Note that a dual number of a variable with respect to itself is simply hx, dxiq = (x, 1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0); such expressions are used without explicit initialization in Algorithm 2.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Also, if the dual number hx, dyiq has been assigned, we will assume the scalar value x is also available, for example, to initialize a new dual variable hx, dxiq (cf.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
the dual number on the RHS of Line 3).,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Note that Algorithm 1 contains a non-primitive operation on Line 5: the derivative dyk k/duykk .,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To evaluate this in Algorithm 2, we must manipulate the dual number of k to be taken with respect to uk, and not the original input value sk, as in forward-mode autodiff.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Our approach can be viewed as following a different recursive principle from either forward or reverse-mode autodiff: in the circuit diagram of Figure 1, we calculate derivatives of each nested circuit with respect to its own input, starting with the innermost circuit and working out.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Theorem 2.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LAK computes h↵k, dskiq in time O K(q",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+
Y )2 log(q + Y ) where Y = PK
k=1 yk is the sum of the observed counts and q is the requested number of derivatives.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the likelihood can be computed in O(KY 2 log Y ) time, and the first q moments or the first q entries of the filtered marginals can be computed in time",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
O K(q,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"+ Y )2 log(q + Y ) .
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Proof.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"To see that GDUAL-FORWARD is correct, note that it corresponds to Algorithm 1, but applies the three operations from the previous section to operate on dual numbers instead of scalars.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
We will verify that the conditions for applying each operation are met.,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
Lines 2–5 each use lifting of algebraic operations or the functions,3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Fk and Gk, which are assumed to consist only of primitive operations.",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Lines 4 and 5 apply the composition operation; here, we can verify from Figure 1 that sk 1 separates uk and ↵k 1 (Line 4) and that uk separates sk and k (Line 5).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The conditions for applying the differential operator on Line 5 are also met.
",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"For the running time, note that the total number of operations on dual numbers in LAK , including recursive calls, is O(K).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"The order of the dual numbers is initially q, but increases by yk in each recursive call (Line 4).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Therefore, the maximum value is q + Y .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Each of the operations on
Algorithm 1 Ak(sk) if k = 0 then
1: return ↵k = 1 end if 2: uk = sk(1 ⇢k) 3: sk 1 = Fk(uk) 4: k = Ak 1(sk 1) ·Gk(uk) 5: ↵k = d yk
du yk k k · (sk⇢k)yk/yk! 6: return ↵k
Algorithm 2 LAk(hsk, dskiq) — GDUAL-FORWARD if k = 0 then
1: return h↵k, dskiq = (1, 0, . . .",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
", 0) end if 2: huk, dskiq = hsk, dskiq · (1 ⇢k) 3: hsk 1, dukiq+yk = LFk huk, dukiq+yk
4: h k, dukiq+yk = ⇥ LAk 1",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"hsk 1, dsk 1iq+yk hsk 1, dukiq+yk ⇤ ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"LGk huk, dukiq+yk 5: h↵k, dskiq = ⇥",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"Dyk h k, dukiq+yk huk, dskiq ⇤ ⇥ ⇢khsk, dskiq
yk/yk! 6: return h↵k, dskiq
dual numbers is O(p2 log p) for dual numbers of order p, so the total is O(K(q + Y )2 log(q + Y )).",3.2.3. THE GDUAL-FORWARD ALGORITHM,[0],[0]
"In this section we describe simulation experiments to evaluate the running time of GDUAL-FORWARD against other algorithms, and to assess the ability to learn a wide variety of models for which exact likelihood calculations were not previously possible, by using GDUAL-FORWARD within a parameter estimation routine.
",4. Experiments,[0],[0]
Running time vs Y .,4. Experiments,[0],[0]
"We compared the running time of GDUAL-FORWARD with the PGF-FORWARD algorithm from (Winner & Sheldon, 2016) as well as TRUNC, the standard truncated forward algorithm (Dail & Madsen, 2011).",4. Experiments,[0],[0]
"PGF-FORWARD is only applicable to the Poisson HMM from (Winner & Sheldon, 2016), which, in our terminology, is a model with a Poisson immigration distribution and a Bernoulli offspring distribution.",4. Experiments,[0],[0]
"TRUNC applies to any choice of distributions, but is approximate.",4. Experiments,[0],[0]
"For these experiments, we restrict to Poisson HMMs for the sake of comparison with the less general PGF-FORWARD algorithm.
",4. Experiments,[0],[0]
A primary factor affecting running time is the magnitude of the counts.,4. Experiments,[0],[0]
We measured the running time for all algorithms to compute the likelihood p(y; ✓) for vectors y,4. Experiments,[0],[0]
:= y 1:K = c ⇥,4. Experiments,[0],[0]
"(1, 1, 1, 1, 1) with increasing c.",4. Experiments,[0],[0]
"In this case, Y = P k yk = 5c.",4. Experiments,[0],[0]
"PGF-FORWARD and GDUAL-FORWARD have running times O(KY 2) and O(KY 2 log Y ), respectively, which depend only on Y and not ✓.",4. Experiments,[0],[0]
"The running time of an FFT-based implementation of TRUNC is O(KN2
max
logN max ), where N max is the value used to truncate the support of each latent variable.",4. Experiments,[0],[0]
"A heuristic is required to choose N
max so that it captures most of the probability mass of p(y; ✓) but is not too big.",4. Experiments,[0],[0]
"The appropriate value depends strongly on ✓, which in practice may be unknown.",4. Experiments,[0],[0]
"In preliminary experiments with realistic immigration and offspring models (see below) and known parameters, we found that an excellent heuristic is N
max = 0.4Y/⇢, which we use here.",4. Experiments,[0],[0]
"With this heuristic, TRUNC’s running time is O(K⇢2Y 2 log Y ).
",4. Experiments,[0],[0]
"Figure 3 shows the results for ⇢ 2 {0.15, 0.85}, averaged over 20 trials with error bars showing 95% confidence intervals of the mean.",4. Experiments,[0],[0]
"GDUAL-FORWARD and TRUNC have the same asymptotic dependence on Y but GDUALFORWARD scales better empirically, and is exact.",4. Experiments,[0],[0]
"It is about 8x faster than TRUNC for the largest Y when ⇢ = 0.15, and 2x faster for ⇢ = 0.85.",4. Experiments,[0],[0]
"PGF-FORWARD is faster by a factor of log Y in theory and scales better in practice, but applies to fewer models.
",4. Experiments,[0],[0]
Running time for different ✓.,4. Experiments,[0],[0]
"We also conducted experiments where we varied parameters and used an oracle method to select N
max for TRUNC.",4. Experiments,[0],[0]
"This was done by running the algorithm for increasing values of N
max and selecting the smallest one such that the likelihood was within 10 6 of the true value (see Winner & Sheldon, 2016).
",4. Experiments,[0],[0]
"We simulated data from Poisson HMMs and measured the time to compute the likelihood p(y; ✓) for the true parameters ✓ = ( , , ⇢), where is a vector whose kth entry is the mean of the Poisson immigration distribution at time k, and and ⇢ are scalars representing the Bernoulli survival probability and detection probability, respectively, which are shared across time steps.",4. Experiments,[0],[0]
"We set and to mimic three different biological models; for each, we varied ⇢ from 0.05 to 0.95.",4. Experiments,[0],[0]
"The biological models were as follows: ‘PHMM’ follows a temporal model for insect populations (Zonneveld, 1991) with = (5.13, 23.26, 42.08, 30.09, 8.56) and = 0.26; ‘PHMM-peaked’ is similar, but sets = (0.04, 10.26, 74.93, 25.13, 4.14) so the immigration is temporally “peaked” at the middle time step; ‘NMix’ sets = (80, 0, 0, 0, 0) and = 0.4, which is similar to the N-mixture model (Royle, 2004), with no immigration following the first time step.
",4. Experiments,[0],[0]
Figure 2 shows the running time of all three methods versus ⇢.,4. Experiments,[0],[0]
"In these models, E[Y ] is proportional to ⇢, and the running times of GDUAL-FORWARD and PGF-FORWARD increase with ⇢ due to the corresponding increase in Y .",4. Experiments,[0],[0]
"PGFFORWARD is faster by a factor of log Y , but is applicable to fewer models.",4. Experiments,[0],[0]
"GDUAL-FORWARD perfoms best relative to PGF-FORWARD for the NMix model, because it is fastest when counts occur in early time steps.
",4. Experiments,[0],[0]
"Recall that the running time of TRUNC is O(N2
max
logN max ).",4. Experiments,[0],[0]
"For these models, the distribution of the hidden population depends only on and , and these are the primary factors determining N
max .",4. Experiments,[0],[0]
"Running time decreases slightly as ⇢ increases, because the observation model p(y |n; ⇢) exerts more influence restricting implausible settings of n when the detection probability is higher.
",4. Experiments,[0],[0]
Parameter Estimation.,4. Experiments,[0],[0]
"To demonstrate the flexibility of the method, we used GDUAL-FORWARD within an optimization routine to compute maximum likelihood estimates (MLEs) for models with different immigration and growth distributions.",4. Experiments,[0],[0]
"In each experiment, we generated 10 independent observation vectors for K = 7 time steps from the same model p(y; ✓), and then used the L-BFGS-B algorithm to numerically find ✓ to maximize the loglikelihood of the 10 replicates.",4. Experiments,[0],[0]
We varied the distributional forms of the immigration and offspring distributions as well as the mean R := E[Xk] of the offspring distribution.,4. Experiments,[0],[0]
"We fixed the mean immigration := E[Mk] = 6 and the de-
tection probability to ⇢ = 0.6 across all time steps.",4. Experiments,[0],[0]
"The quantity R is the “basic reproduction number”, or the average number of offspring produced by a single individual, and is of paramount importance for disease and population models.",4. Experiments,[0],[0]
"We varied R, which was also shared across time steps, between 0.2 and 1.2.",4. Experiments,[0],[0]
"The parameters and R were learned, and ⇢ was fixed to resolve ambiguity between population size and detection probability.",4. Experiments,[0],[0]
"Each experiment was repeated 50 times; a very small number of optimizer runs failed to converge after 10 random restarts and were excluded.
",4. Experiments,[0],[0]
Figure 4 shows the distribution of 50 MLE estimates for R vs. the true values for each model.,4. Experiments,[0],[0]
Results for two additional models appear in the supplementary material.,4. Experiments,[0],[0]
In all cases the distribution of the estimate is centered around the true parameter.,4. Experiments,[0],[0]
It is evident that GDUAL-FORWARD can be used effectively to produce parameter estimates across a variety of models for which exact likelihood computations were not previously possible.,4. Experiments,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1617533.,Acknowledgments,[0],[0]
Graphical models with latent count variables arise in a number of areas.,abstractText,[0],[0]
"However, standard inference algorithms do not apply to these models due to the infinite support of the latent variables.",abstractText,[0],[0]
"Winner & Sheldon (2016) recently developed a new technique using probability generating functions (PGFs) to perform efficient, exact inference for certain Poisson latent variable models.",abstractText,[0],[0]
"However, the method relies on symbolic manipulation of PGFs, and it is unclear whether this can be extended to more general models.",abstractText,[0],[0]
"In this paper we introduce a new approach for inference with PGFs: instead of manipulating PGFs symbolically, we adapt techniques from the autodiff literature to compute the higher-order derivatives necessary for inference.",abstractText,[0],[0]
"This substantially generalizes the class of models for which efficient, exact inference algorithms are available.",abstractText,[0],[0]
"Specifically, our results apply to a class of models that includes branching processes, which are widely used in applied mathematics and population ecology, and autoregressive models for integer data.",abstractText,[0],[0]
Experiments show that our techniques are more scalable than existing approximate methods and enable new applications.,abstractText,[0],[0]
Exact Inference for Integer Latent-Variable Models,title,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.",text,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.
",1. Introduction,[0],[0]
"Consider graphical models with binary random variables and pairwise interactions, also known as Ising models.",1. Introduction,[0],[0]
"For a graph G = (V,E) with node weights θ ∈ RV and edge weights W ∈ RE , the probability of a variable configura-
1Department of Electrical and Computer Engineering, University of Texas at Austin, USA 2Department of Computer Science, University of Texas at Austin, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Erik M. Lindgren <erikml@utexas.edu>, Alexandros G. Dimakis <dimakis@austin.utexas.edu>, Adam Klivans <klivans@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
tion is given by
P(X = x) = 1
Z exp ∑ i∈V θixi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijxixj  , (1) where Z is a normalization constant.
",1. Introduction,[0],[0]
"The MAP problem is to find the configuration x ∈ {0, 1}V that maximizes Equation (1).",1. Introduction,[0],[0]
"We can write this as an integer linear program (ILP) as follows:
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E",1. Introduction,[0],[0]
"Wijqij
s.t. qi ∈ {0, 1} ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈",1. Introduction,[0],[0]
"E. (2)
",1. Introduction,[0],[0]
"The MAP problem on binary, pairwise graphical models contains, as a special case, the Max-cut problem and is therefore NP-hard.",1. Introduction,[0],[0]
"For this reason, a significant amount of attention has focused on analyzing the LP relaxation of the ILP, which can be solved efficiently in practice.
",1. Introduction,[0],[0]
max q∈RV ∪E ∑ i∈V θiqi + ∑ ij∈E,1. Introduction,[0],[0]
"Wijqij
s.t. 0 ≤ qi ≤ 1 ∀i ∈ V qij ≥ max{0, qi + qj",1. Introduction,[0],[0]
− 1} ∀ij ∈,1. Introduction,[0],[0]
E,1. Introduction,[0],[0]
qij ≤,1. Introduction,[0],[0]
"min{qi, qj} ∀ij ∈ E (3)
",1. Introduction,[0],[0]
This relaxation has been an area of intense research in machine learning and statistics.,1. Introduction,[0],[0]
"In (Meshi et al., 2016), the authors state that a major open question is to identify why real world instances of Problem (2) can be solved efficiently despite the theoretical worst case complexity.
",1. Introduction,[0],[0]
"We make progress on this open problem by analyzing the fractional vertices of the LP relaxation, that is, the extreme points of the polytope with fractional coordinates.",1. Introduction,[0],[0]
Vertices of the relaxed polytope with fractional coordinates are called pseudomarginals for graphical models and pseudocodewords in coding theory.,1. Introduction,[0],[0]
"If a fractional vertex has higher objective value (i.e. likelihood) compared to the best integral one, the LP relaxation fails.",1. Introduction,[0],[0]
"We call fractional vertices with an objective value at least as good as the objective
value of the optimal integral vertex confounding vertices.",1. Introduction,[0],[0]
"Our main result is that it is possible to prune all confounding vertices efficiently when their number is polynomial.
",1. Introduction,[0],[0]
"Our contributions:
• Our first contribution is a general result on integer programs.",1. Introduction,[0],[0]
"We show that any 0-1 integer linear program (ILP) can be solved exactly in polynomial time, if the number confounding vertices is bounded by a polynomial.",1. Introduction,[0],[0]
This applies to MAP inference for a graphical model over any alphabet size and any order of connection.,1. Introduction,[0],[0]
"The same result (exact solution if the number of confounding vertices is bounded by a polynomial) was established by (Dimakis et al., 2009) for the special case of LP decoding of LDPC codes (Feldman et al., 2005).",1. Introduction,[0],[0]
"The algorithm from (Dimakis et al., 2009) relies on the special structure of the graphical models that correspond to LDPC codes.",1. Introduction,[0],[0]
In this paper we generalize this result for any ILP in the unit hypercube.,1. Introduction,[0],[0]
"Our results extend to finding all integral vertices among the M -best vertices.
",1. Introduction,[0],[0]
"• Given our condition, one may be tempted to think that we generate the top M -best vertices of a linear program (for M polynomial) and output the best integral one in this list.",1. Introduction,[0],[0]
We actually show that such an approach would be computationally intractable.,1. Introduction,[0],[0]
"Specifically, we show that it is NP-hard to produce a list of the M -best vertices if M = O(nε) for any fixed ε > 0.",1. Introduction,[0],[0]
This result holds even if the list is allowed to be approximate.,1. Introduction,[0],[0]
"This strengthens the previously known hardness result (Angulo et al., 2014) which was M = O(n) for the exact M -best vertices.",1. Introduction,[0],[0]
"In terms of achievability, the best previously known result (from (Angulo et al., 2014)) can only solve the ILP if there is at most a constant number of confounding vertices.
",1. Introduction,[0],[0]
"• We obtain a complete characterization of the fractional vertices of the local polytope for binary, pairwise graphical models.",1. Introduction,[0],[0]
We show that any variable in the fractional support must be connected to a frustrated cycle by other fractional variables in the graphical model.,1. Introduction,[0],[0]
"This is a complete structural characterization that was not previously known, to the best of our knowledge.
",1. Introduction,[0],[0]
• We develop an approach to estimate the number of confounding vertices of a half-integral polytope.,1. Introduction,[0],[0]
We use this method in an empirical evaluation of the number of confounding vertices of previously studied problems and analyze how well common integer programming techniques perform at pruning confounding vertices.,1. Introduction,[0],[0]
"For some classes of graphical models, it is possible to solve the MAP problem exactly.",2. Background and Related Work,[0],[0]
"For example see (Weller et al., 2016) for balanced and almost balanced models, (Jebara, 2009) for perfect graphs, and (Wainwright et al., 2008) for graphs with constant tree-width.
",2. Background and Related Work,[0],[0]
These conditions are often not true in practice and a wide variety of general purpose algorithms are able to solve the MAP problem for large inputs.,2. Background and Related Work,[0],[0]
"One class is belief propagation and its variants (Yedidia et al., 2000; Wainwright et al., 2003; Sontag et al., 2008).",2. Background and Related Work,[0],[0]
"Another class involves general ILP optimization methods (see e.g. (Nemhauser & Wolsey, 1999)).",2. Background and Related Work,[0],[0]
"Techniques specialized to graphical models include cutting-plane methods based on the cycle inequalities (Sontag & Jaakkola, 2007; Komodakis & Paragios, 2008; Sontag et al., 2012).",2. Background and Related Work,[0],[0]
"See also (Kappes et al., 2013) for a comparative survey of techniques.
",2. Background and Related Work,[0],[0]
"In (Weller et al., 2014), the authors investigate how pseudomarginals and relaxations relate to the success of the Bethe approximation of the partition function.
",2. Background and Related Work,[0],[0]
"There has been substantial prior work on improving inference building on these LP relaxations, especially for LDPC codes in the information theory community.",2. Background and Related Work,[0],[0]
"This work ranges from very fast solvers that exploit the special structure of the polytope (Burshtein, 2009), connections to unequal error protection (Dimakis et al., 2007), and graphical model covers (Koetter et al., 2007).",2. Background and Related Work,[0],[0]
"LP decoding currently provides the best known finite-length error-correction bounds for LDPC codes both for random (Daskalakis et al., 2008; Arora et al., 2009), and adversarial bit-flipping errors (Feldman et al., 2007).
",2. Background and Related Work,[0],[0]
"For binary graphical models, there is a body of work which tries to exploit the persistency of the LP relaxation, that is, the property that integer components in the solution of the relaxation must take the same value in the optimal solution, under some regularity assumptions (Boros & Hammer, 2002; Rother et al., 2007; Fix et al., 2012).
",2. Background and Related Work,[0],[0]
"Fast algorithms for solving large graphical models in practice include (Ihler et al., 2012; Dechter & Rish, 2003).
",2. Background and Related Work,[0],[0]
"The work most closely related to this paper involves eliminating fractional vertices (so-called pseudocodewords in coding theory) by changing the polytope or the objective function (Zhang & Siegel, 2012; Chertkov & Stepanov, 2008; Liu et al., 2012).",2. Background and Related Work,[0],[0]
"A binary integer linear program is an optimization problem of the following form.
",3. Provable Integer Programming,[0],[0]
"max x
cTx
subject to Ax ≤ b x ∈ {0, 1}n
which is relaxed to a linear program by replacing the x ∈ {0, 1}n constraint with 0 ≤ x ≤ 1.",3. Provable Integer Programming,[0],[0]
For binary integer programs with the box constraints 0 ≤,3. Provable Integer Programming,[0],[0]
"xi ≤ 1 for all i, every integral vector x is a vertex of the polytope described by the constraints of the LP relaxation.",3. Provable Integer Programming,[0],[0]
"However fraction vertices may also be in this polytope, and fractional solutions can potentially have an objective value larger than every integral vertex.
",3. Provable Integer Programming,[0],[0]
"If the optimal solution to the linear program happens to be integral, then this is the optimal solution to the original integer linear program.",3. Provable Integer Programming,[0],[0]
"If the optimal solution is fractional, then a variety of techniques are available to tighten the LP relaxation and eliminate the fractional solution.
",3. Provable Integer Programming,[0],[0]
"We establish a success condition for integer programming based on the number of confounding vertices, which to the best of our knowledge was unknown.",3. Provable Integer Programming,[0],[0]
"The algorithm used in proving Theorem 1 is a version of branch-and-bound, a classic technique in integer programming (Land & Doig, 1960) (see (Nemhauser & Wolsey, 1999) for a modern reference on integer programming).",3. Provable Integer Programming,[0],[0]
"This algorithm works by starting with a root node, then branching on a fractional coordinate by making two new linear programs with all the constraints of the parent node, with the constraint xi = 0 added to one new leaf and xi = 1 added to the other.",3. Provable Integer Programming,[0],[0]
The decision on which leaf of the tree to branch on next is based on which leaf has the best objective value.,3. Provable Integer Programming,[0],[0]
"When the best leaf is integral, we know that this is the best integral solution.",3. Provable Integer Programming,[0],[0]
"This algorithm is formally written in Algorithm 1.
",3. Provable Integer Programming,[0],[0]
Theorem 1.,3. Provable Integer Programming,[0],[0]
"Let x∗ be the optimal integral solution and let {v1, v2, . . .",3. Provable Integer Programming,[0],[0]
", vM} be the set of confounding vertices in the LP relaxation.",3. Provable Integer Programming,[0],[0]
"Algorithm 1 will find the optimal integral solution x∗ after 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Since MAP inference is a binary integer program regardless of the alphabet size of the variables and order of the clique potentials, we have the following corollary:
Corollary 2.",3. Provable Integer Programming,[0],[0]
"Given a graphical model such that the local polytope has M as cofounding variables, Algorithm 1 can find the optimal MAP configuration with 2M calls to an LP solver.
",3. Provable Integer Programming,[0],[0]
"Cutting-plane methods, which remove a fractional vertex by introducing a new constraint in the polytope may not have this property, since this cut may create new confound-
Algorithm 1 Branch and Bound test Input: an LP {min cTx :",3. Provable Integer Programming,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1}
# branch (v, I0, I1) means v is optimal LP # with xI0 = 0 and xI1 = 1.",3. Provable Integer Programming,[0],[0]
"def LP(I0, I1): v∗ ← argmax cTx subject to:",3. Provable Integer Programming,[0],[0]
Ax ≤ b xI0 = 0,3. Provable Integer Programming,[0],[0]
"xI1 = 1
return v∗ if feasible, else return null
v ← LP(∅, ∅)",3. Provable Integer Programming,[0],[0]
"B ← {(v, ∅, ∅)} while optimal integral vertex not found:
(v, I0, I1)←",3. Provable Integer Programming,[0],[0]
"argmax(v,I0,I1)∈B c T v if v is integral: return v else: find a fractional coordinate i v(0)",3. Provable Integer Programming,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
ing vertices.",3. Provable Integer Programming,[0],[0]
This branch-and-bound algorithm has the desirable property that it never creates a new fractional vertex.,3. Provable Integer Programming,[0],[0]
"We note that other branching algorithms, such as the algorithm presented by the authors in (Marinescu & Dechter, 2009), do not immediately allow us to prove our desired theorem.
",3. Provable Integer Programming,[0],[0]
Note that warm starting a linear program with slightly modified constraints allows subsequent calls to an LP solver to be much more efficient after the root LP has been solved.,3. Provable Integer Programming,[0],[0]
"The proof follows from the following invariants:
• At every iteration we remove at least one fractional vertex.
•",3.1. Proof of Theorem 1,[0],[0]
"Every integral vertex is in exactly one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"Every fractional vertex is in at most one branch.
",3.1. Proof of Theorem 1,[0],[0]
•,3.1. Proof of Theorem 1,[0],[0]
"No fractional vertices are created by the new constraints.
",3.1. Proof of Theorem 1,[0],[0]
"To see the last invariant, note that every vertex of a polytope can be identified by the set of inequality constraints that are satisfied with equality (see (Bertsimas & Tsitsiklis, 1997)).",3.1. Proof of Theorem 1,[0],[0]
"By forcing an inequality constraint to be tight, we cannot possibly introduce new vertices.",3.1. Proof of Theorem 1,[0],[0]
"As mentioned in the introduction, the algorithm used to prove Theorem 1 does not enumerate all the fractional vertices until it finds an integral vertex.",3.2. The M -Best LP Problem,[0],[0]
"Enumerating the M - best vertices of a linear program is theM -best LP problem.
",3.2. The M -Best LP Problem,[0],[0]
Definition.,3.2. The M -Best LP Problem,[0],[0]
"Given a linear program {min cTx : x ∈ P} over a polytope P and a positive integer M , the M -best LP problem is to optimize
max {v1,...,vM}⊆V (P ) M∑",3.2. The M -Best LP Problem,[0],[0]
"k=1 cT vk.
",3.2. The M -Best LP Problem,[0],[0]
"This was established by (Angulo et al., 2014) to be NP-hard when M = O(n).",3.2. The M -Best LP Problem,[0],[0]
"We strengthen this result to hardness of approximation even when M = nε for any ε > 0.
Theorem 3.",3.2. The M -Best LP Problem,[0],[0]
"It is NP-hard to approximate the M -best LP problem by a factor better than O(n ε
M ) for any fixed ε > 0.
",3.2. The M -Best LP Problem,[0],[0]
Proof.,3.2. The M -Best LP Problem,[0],[0]
"Consider the circulation polytope described in (Khachiyan et al., 2008), with the graph and weight vector w described in (Boros et al., 2011).",3.2. The M -Best LP Problem,[0],[0]
"By adding anO(logM) long series of 2×2 bipartite subgraphs, we can make it such that one long path in the original graph implies M long paths in the new graph, and thus it is NP-hard to find any of these long paths in the new graph.",3.2. The M -Best LP Problem,[0],[0]
"By adding the constraint vector wTx ≤ 0, and using the cost function −w, the vertices corresponding to the short paths have value 1/2, the vertices corresponding to the long paths have value O(1/n), and all other vertices have value 0.",3.2. The M -Best LP Problem,[0],[0]
Thus the optimal set has value O(n+ Mn ).,3.2. The M -Best LP Problem,[0],[0]
"However it is NP-hard to find a set of value greater than O(n) in polynomial time, which gives an O( nM ) approximation.",3.2. The M -Best LP Problem,[0],[0]
"Using a padding argument, we can replace n with nε.
",3.2. The M -Best LP Problem,[0],[0]
"The best known algorithm for the M -best LP problem is a generalization of the facet guessing algorithm (Dimakis et al., 2009) developed in (Angulo et al., 2014), which would require O(mM ) calls to an LP solver, where m is the number of constraints of the LP.",3.2. The M -Best LP Problem,[0],[0]
"Since we only care about integral solutions, we can find the single best integral vertex with O(M) calls to an LP solver, and if we want all of the K-best integral solutions among the top M vertices of the polytope, we can find these with O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP-solver, as we will see in the next section.
3.3.",3.2. The M -Best LP Problem,[0],[0]
"K-Best Integral Solutions
Finding the K-best solutions to general optimization problems has been uses in several machine learning applications.",3.2. The M -Best LP Problem,[0],[0]
Producing multiple high-value outputs can be naturally combined with post-processing algorithms that select the most desired solution using additional sideinformation.,3.2. The M -Best LP Problem,[0],[0]
"There is a significant volume of work in the general area, see (Fromer & Globerson, 2009; Batra et al., 2012) for MAP solutions in graphical models and (Eppstein, 2014) for a survey on M -best problems.
",3.2. The M -Best LP Problem,[0],[0]
We further generalize our theorem to find the K-best integral solutions.,3.2. The M -Best LP Problem,[0],[0]
Theorem 4.,3.2. The M -Best LP Problem,[0],[0]
"Under the assumption that there are less than M fractional vertices with objective value at least as good as the K-best integral solutions, we can find all of the Kbest integral solutions, O(nK",3.2. The M -Best LP Problem,[0],[0]
"+M) calls to an LP solver.
",3.2. The M -Best LP Problem,[0],[0]
The algorithm used in this theorem is Algorithm 2.,3.2. The M -Best LP Problem,[0],[0]
"It combines Algorithm 1 with the space partitioning technique used in (Murty, 1968; Lawler, 1972).",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree is fractional, then we use the branching technique in Algorithm 1.",3.2. The M -Best LP Problem,[0],[0]
"If the current optimal solution in the solution tree x∗ is integral, then we branch by creating a new leaf for every i not currently constrained by the parent with the constraint",3.2. The M -Best LP Problem,[0],[0]
xi = ¬x∗i .,3.2. The M -Best LP Problem,[0],[0]
"We now describe the fractional vertices of the local polytope for binary, pairwise graphical models, which is described in Equation 3.",4. Fractional Vertices of the Local Polytope,[0],[0]
"It was shown in (Padberg, 1989) that all the vertices of this polytope are half-integral, that is, all coordinates have a value from {0, 12 , 1} (see (Weller et al., 2016) for a new proof of this).
",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a half-integral point q ∈ {0, 12 , 1} V ∪E in the local polytope, we say that a cycle C = (VC , EC) ⊆ G is frustrated if there is an odd number of edges ij ∈ EC such that qij = 0.",4. Fractional Vertices of the Local Polytope,[0],[0]
"If a point q has a frustrated cycle, then it is a pseudomarginal, as no probability distribution exists that has as its singleton and pairwise marginals the coordinates of q. Half-integral points q with a frustrated cycle do not satisfy the cycle inequalities (Sontag & Jaakkola, 2007; Wainwright et al., 2008), for all cycles C = (VC , EC), F = (VF , EF ) ⊆ C, |EF",4. Fractional Vertices of the Local Polytope,[0],[0]
| odd we must have∑ ij∈EF qi+qj−2qij− ∑ ij∈EC\EF qi+qj−2qij ≤ |FC |−1.,4. Fractional Vertices of the Local Polytope,[0],[0]
"(4)
Frustrated cycles allow a solution to be zero on negative weights in a way that is not possible for any integral solution.
",4. Fractional Vertices of the Local Polytope,[0],[0]
"We have the following theorem describing all the vertices of the local polytope for binary, pairwise graphical models.
",4. Fractional Vertices of the Local Polytope,[0],[0]
Algorithm 2 M -best Integral Input: an LP {max cTx :,4. Fractional Vertices of the Local Polytope,[0],[0]
"Ax ≤ b, 0 ≤ x ≤ 1} Input: number of solutions K
def LP(I0, I1): same as Algorithm 1
def SplitIntegral(v, I0, I1): P ← { } for i ∈",4. Fractional Vertices of the Local Polytope,[0],[0]
[n] if i /∈,4. Fractional Vertices of the Local Polytope,[0],[0]
"I0 ∪ I1: a← ¬vi I ′0, I ′",4. Fractional Vertices of the Local Polytope,[0],[0]
"1 ← copy(I0, I1)
add i to I ′a v′",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I ′0, I ′1) add (v′, I ′0, I ′ a) to P if feasible
return P
v ← LP(∅, ∅)",4. Fractional Vertices of the Local Polytope,[0],[0]
"B ← {(v, ∅, ∅)} results← { } while K integral vertices not found: (v, I0, I1)←",4. Fractional Vertices of the Local Polytope,[0],[0]
"argmax(v,I0,I1)∈B c
T v if v is integral:
add v to results add SplitIntegeral(v, I0, I1) to B remove (v, I0, I1) from B
else: find a fractional coordinate i v(0)",4. Fractional Vertices of the Local Polytope,[0],[0]
"← LP(I0 ∪ {i}, I1) v(1) ← LP(I0, I1 ∪ {i}) remove (v, I0, I1) from B add (v(0), I0 ∪ {i}, I1) to B if feasible add (v(1), I0, I1 ∪ {i}) to B if feasible
return results
Theorem 5.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Given a point q in the local polytope, q is a vertex of this polytope if and only if q ∈ {0, 12 , 1}
V ∪E and the induced subgraph on the fractional nodes of q is such that every connected component of this subgraph contains a frustrated cycle.",4. Fractional Vertices of the Local Polytope,[0],[0]
"Every vertex q of an n-dimensional polytope is such that there are n constraints such that q satisfies them with equality, known as active constraints (see (Bertsimas & Tsitsiklis, 1997)).",4.1. Proof of Theorem 5,[0],[0]
Every integral q is thus a vertex of the local polytope.,4.1. Proof of Theorem 5,[0],[0]
"We now describe the fractional vertices of the local polytope.
Definition.",4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be an induced subgraph of points such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"We say that GF is
full rank if the following system of equations is full rank.
",4.1. Proof of Theorem 5,[0],[0]
qi + qj,4.1. Proof of Theorem 5,[0],[0]
− qij = 1 ∀ij ∈ EF such that qij = 0 qij = 0,4.1. Proof of Theorem 5,[0],[0]
"∀ij ∈ EF such that qij = 0
qi",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
qj",4.1. Proof of Theorem 5,[0],[0]
"− qij = 0 ∀ij ∈ EF such that qij = 1
2
(5)
Theorem 5 follows from the following lemmas.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be the subgraph induced by the nodes i ∈ V such that qi = 12 .",4.1. Proof of Theorem 5,[0],[0]
"The point q is a vertex if and only if every connected component of GF is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let GF = (VF , EF ) be a connected subgraph induced from nodes such that such that qi = 12 for all i ∈ VF .",4.1. Proof of Theorem 5,[0],[0]
"GF is full rank if and only if GF contains cycle that is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 8.,4.1. Proof of Theorem 5,[0],[0]
"Let q ∈ {0, 12 , 1} n+m be a point of the local polytope.",4.1. Proof of Theorem 5,[0],[0]
"Let C = (VC , EC) be a cycle of G such that qi is fractional for all i ∈ VC .",4.1. Proof of Theorem 5,[0],[0]
"C is full rank if and only if C is a frustrated cycle.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 6.,4.1. Proof of Theorem 5,[0],[0]
Suppose every connected component is full rank.,4.1. Proof of Theorem 5,[0],[0]
Then every fractional node and edge between fractional nodes is fully specified by their corresponding equations in Problem 3.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to check that all integral nodes, edges between integral nodes, and edges between integral and fractional nodes is also fixed.",4.1. Proof of Theorem 5,[0],[0]
"Thus q is a vertex.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose that there exists a connected component that is not full rank.,4.1. Proof of Theorem 5,[0],[0]
The only other constraints involving this connected component are those between fractional nodes and integral nodes.,4.1. Proof of Theorem 5,[0],[0]
"However, note that these constraints are always rank 1, and also introduce a new edge variable.",4.1. Proof of Theorem 5,[0],[0]
"Thus all the constraints where q is tight do not make a full rank system of equations.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 7.,4.1. Proof of Theorem 5,[0],[0]
Suppose GF has a full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will build the graph starting with the full rank cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"It is easy to see from Equations 5 that all new variables introduced to the system of equations have a fixed value, and thus the whole connected component is full rank.
",4.1. Proof of Theorem 5,[0],[0]
Now suppose GF has no full rank cycle.,4.1. Proof of Theorem 5,[0],[0]
We will again build the graph starting from the cycle then adding one connected edge at a time.,4.1. Proof of Theorem 5,[0],[0]
"If we add an edge that connects to a new node, then we added two variables and two equations, thus we did not make the graph full rank.",4.1. Proof of Theorem 5,[0],[0]
"If we add an edge between two existing nodes, then we have a cycle involving this edge.",4.1. Proof of Theorem 5,[0],[0]
"We introduce two new equations, however with
one of the equations and the other cycle equations, we can produce the other equation, thus we can increase the rank by one but we also introduced a new edge.",4.1. Proof of Theorem 5,[0],[0]
"Thus the whole graph cannot be full rank.
",4.1. Proof of Theorem 5,[0],[0]
"The proof of Lemma 8 from the following lemma.
",4.1. Proof of Theorem 5,[0],[0]
Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
"Consider a collection of n vectors
v1 = (1, t1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v2 = (0, 1, t2, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
",4.1. Proof of Theorem 5,[0],[0]
"v3 = (0, 0, 1, t3, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0)
...
",4.1. Proof of Theorem 5,[0],[0]
"vn−1 = (0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1, tn−1)
vn = (tn, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0, 1)
",4.1. Proof of Theorem 5,[0],[0]
"for ti ∈ {−1, 1}.",4.1. Proof of Theorem 5,[0],[0]
"We have rank(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn) = n",4.1. Proof of Theorem 5,[0],[0]
"if and only if there is an odd number of vectors such that ti = 1.
",4.1. Proof of Theorem 5,[0],[0]
Proof of Lemma 9.,4.1. Proof of Theorem 5,[0],[0]
Let k be the number of vectors such that ti = 1.,4.1. Proof of Theorem 5,[0],[0]
"Let S1 = v1 and define
Si+1 = { Si − vi+1",4.1. Proof of Theorem 5,[0],[0]
"if Si(i+ 1) = 1 Si + vi+1 if Si(i+ 1) = −1
for i = 2, . .",4.1. Proof of Theorem 5,[0],[0]
.,4.1. Proof of Theorem 5,[0],[0]
", n− 1.
Note that if ti+1 = −1 then Si+1(i+2) = Si(i+1) and if ti+1 = 1 then Si+1(i+2) = −Si(i+1).",4.1. Proof of Theorem 5,[0],[0]
"Thus the number of times the sign changes is exactly the number of ti = 1 for i ∈ {2, . . .",4.1. Proof of Theorem 5,[0],[0]
", n− 1}.
",4.1. Proof of Theorem 5,[0],[0]
"Using the value of Sn−1 we can now we can check for all values of t1 and tn that the following is true.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is odd then (1, 0, . . .",4.1. Proof of Theorem 5,[0],[0]
", 0) ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn), which allows us to create the entire standard basis, showing the vectors are full rank.
",4.1. Proof of Theorem 5,[0],[0]
"• If k is even then vn ∈ span(v1, v2, . . .",4.1. Proof of Theorem 5,[0],[0]
", vn−1) and thus the vectors are not full rank.",4.1. Proof of Theorem 5,[0],[0]
For this section we generalize generalize Theorem 1.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We see after every iteration we potentially remove more than one confounding vertex—we remove all confounding vertices that agree with xI0 = 0 and xI1,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
= 1 and are fractional with any value at coordinate i.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We also observe that we can
handle a mixed integer program (MIP) with the same algorithm.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"max x
cTx+ dT",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"z
subject to Ax+Bz ≤ b x ∈ {0, 1}n
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We will call a vertex (x, z) fractional if its x component is fractional.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For each fractional vertex (x, z), we create a half-integral vector S(x) such that
S(x)i =  0",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"if xi = 0 1 2 if xi is fractional 1 if xi = 1
For a set of vertices V , we define S(V ) to be the set {S(x) : (x, z) ∈ V }, i.e. we remove all duplicate entries.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Theorem 10.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Let (x∗, z∗) be the optimal integral solution and let VC be the set of confounding vertices.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"Algorithm 1 will find the optimal integral solution (x∗, z∗) after 2|S(VC)| calls to an LP solver.
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For MAP inference in graphical models, S(VC) refers to the fractional singleton marginals qV such that there exists a set of pairwise pseudomarginals qE such that (qV , qE) is a cofounding vertex.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
In this case we call qV a confounding singleton marginal.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
We develop Algorithm 3 to estimate the number of confounding singleton marginals for our experiments section.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"It is based on the k-best enumeration method developed in (Murty, 1968; Lawler, 1972).
",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
Algorithm 3 works by a branching argument.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
The root node is the original LP.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"A leaf node is branched on by introducing a new leaf for every node in V and every element of {0, 12 , 1} such that qi 6=",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
a in the parent node and the constraint {qi = a} is not in the constraints for the parent node.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"For i ∈ V , a ∈ {0, 12 , 1}, we create the leaf such that it has all the constraints of its parents plus the constraint qi = a.
Note that Algorithm 3 actually generates a superset of the elements of S(VC), since the introduction of constraints of the type qi = 12 introduce vertices into the new polytope that were not in the original polytope.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"This does not seem to be an issue for the experiments we consider, however this does occur for other graphs.",5. Estimating the number of Confounding Singleton Marginals,[0],[0]
An interesting question is if the vertices of the local polytope can be provably enumerated.,5. Estimating the number of Confounding Singleton Marginals,[0],[0]
"We consider a synthetic experiment on randomly created graphical models, which were also used in (Sontag & Jaakkola, 2007; Weller, 2016; Weller et al., 2014).",6. Experiments,[0],[0]
The graph topology used is the complete graph on 12 nodes.,6. Experiments,[0],[0]
"We first reparametrize the model to use the sufficient statistics
Algorithm 3 Estimate S(VC) for Binary, Pairwise Graphical Models
Input: a binary, pairwise graphical model LP
# branch (v, I0, I 1 2 , I1) means v is optimal LP # with xI0 = 0, xI 1 2 = 12 , and xI1 = 1.",6. Experiments,[0],[0]
"def LP(I0, I 1 2 , I1):
optimize LP with additional constraints: xI0 = 0 xI 1
2 = 12 xI1 = 1
return q∗ if feasible, else return null
q ← LP(∅, ∅, ∅)",6. Experiments,[0],[0]
"B ← {(q, ∅, ∅, ∅)}",6. Experiments,[0],[0]
"solution← { } while optimal integral vertex not found: (q, I0, I 1
2 , I1)←",6. Experiments,[0],[0]
"argmax(q,I0,I 1 2 ,I1)∈B objective val
add q to solution remove (q, I0, I 1
2 , I1) from B
for i ∈ V if i /∈",6. Experiments,[0],[0]
"I0 ∪ I 1 2 ∪ I1:
for a ∈ {0, 12 , 1} if qi 6=",6. Experiments,[0],[0]
"a: I ′0, I
′ 1 2 , I ′1 ← copy(I0, I 12 , I1)",6. Experiments,[0],[0]
I ′a ←,6. Experiments,[0],[0]
I ′a ∪ {i} q′,6. Experiments,[0],[0]
"← LP(I ′0, I ′1
2
, I ′1)
add (q′, I ′0, I ′ 1 2 , I ′1) to B if feasible return solution
1(xi = xj) and 1(xi = 1).",6. Experiments,[0],[0]
"The node weights are drawn θi ∼ Uniform(−1, 1) and the edge weights are drawn Wij ∼ Uniform(−w,w) for varying w.",6. Experiments,[0],[0]
The quantity w determines how strong the connections are between nodes.,6. Experiments,[0],[0]
"We do 100 draws for each choice of edge strength w.
For the complete graph, we observe that Algorithm 3 does not yield any points that do not correspond to vertices, however this does occur for other topologies.
",6. Experiments,[0],[0]
We first compare how the number of fractional singleton marginals |S(VC)| changes with the connection strengthw.,6. Experiments,[0],[0]
"In Figure 1, we plot the sample CDF of the probability that |S(VC)| is some given value.",6. Experiments,[0],[0]
We observe that |S(VC)| increases as the connection strength increases.,6. Experiments,[0],[0]
"Further we see that while most instances have a small number for |S(VC)|, there are rare instances where |S(VC)| is quite large.
",6. Experiments,[0],[0]
Now we compare how the number of cycle constraints from Equation (4) that need to be introduced to find the best integral solution changes with the number of confounding singleton marginals in Figure 2.,6. Experiments,[0],[0]
"We use the algorithm for finding the most frustrated cycle in (Sontag & Jaakkola, 2007) to introduce new constraints.",6. Experiments,[0],[0]
"We observe that each constraint seems to remove many confounding singleton
marginals.
",6. Experiments,[0],[0]
"We also observe the number of introduced confounding singleton marginals that are introduced by the cycle constraints increases with the number of confounding singleton marginals in Figure 3.
",6. Experiments,[0],[0]
Finally we compare the number of branches needed to find the optimal solution increases with the number of confounding singleton marginals in Figure 4.,6. Experiments,[0],[0]
A similar trend arises as with the number of cycle inequalities introduced.,6. Experiments,[0],[0]
"To compare the methods, note that branch-and-bound uses twice as many LP calls as there are branches.",6. Experiments,[0],[0]
"For this family of graphical models, branch-and-bound tends to require less calls to an LP solver than the cut constraints.",6. Experiments,[0],[0]
"Perhaps the most interesting follow-up question to our work is to determine when, in theory and practice, our condition on the number of confounding pseudomarginals in the LP relaxation is small.",7. Conclusion,[0],[0]
Another interesting question is to see if it is possible to prune the number of confounding pseudomarginals at a faster rate.,7. Conclusion,[0],[0]
The algorithm presented for our main theorem removes one pseudomarginal after two calls to an LP solver.,7. Conclusion,[0],[0]
Is it possible to do this at a faster rate?,7. Conclusion,[0],[0]
"From our experiments, this seems to be the case in practice.",7. Conclusion,[0],[0]
"This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344364, 1407278, 1422549, 1618689, 1018829 and ARO YIP W911NF-14-1-0258.",Acknowledgements,[0],[0]
"Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model.",abstractText,[0],[0]
"Although this problem is NP-hard, large instances can be solved in practice and it is a major open question is to explain why this is true.",abstractText,[0],[0]
We give a natural condition under which we can provably perform MAP inference in polynomial time—we require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size.,abstractText,[0],[0]
"This resolves an open question by Dimakis, Gohari, and Wainwright.",abstractText,[0],[0]
"In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution.",abstractText,[0],[0]
We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.,abstractText,[0],[0]
Exact MAP Inference by Avoiding Fractional Vertices,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 694–699 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
694
Many corpora span broad periods of time. Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer). This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years). We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",text,[0],[0]
"Language, and therefore data derived from language, changes over time (Ullmann, 1962).",1 Introduction,[0],[0]
"Word senses can shift over long periods of time (Wilkins, 1993; Wijaya and Yeniterzi, 2011; Hamilton et al., 2016), and written language can change rapidly in online platforms (Eisenstein et al., 2014; Goel et al., 2016).",1 Introduction,[0],[0]
"However, little is known about how shifts in text over time affect the performance of language processing systems.
",1 Introduction,[0],[0]
"This paper focuses on a standard text processing task, document classification, to provide insight into how classification performance varies with time.",1 Introduction,[0],[0]
We consider both long-term variations in text over time and seasonal variations which change throughout a year but repeat across years.,1 Introduction,[0],[0]
"Our empirical study considers corpora contain-
ing formal text spanning decades as well as usergenerated content spanning only a few years.
",1 Introduction,[0],[0]
"After describing the datasets and experiment design, this paper has two main sections, respectively addressing the following research questions:
1.",1 Introduction,[0],[0]
"In what ways does document classification depend on the timestamps of the documents?
2.",1 Introduction,[0],[0]
"Can document classifiers be adapted to perform better in time-varying corpora?
",1 Introduction,[0],[0]
"To address question 1, we train and test on data from different time periods, to understand how performance varies with time.",1 Introduction,[0],[0]
"To address question 2, we apply a domain adaptation approach, treating time intervals as domains.",1 Introduction,[0],[0]
"We show that in most cases this approach can lead to improvements in classification performance, even on future time intervals.",1 Introduction,[0],[0]
"Time is implicitly embedded in the classification process: classifiers are often built to be applied to future data that doesn’t yet exist, and performance on held-out data is measured to estimate performance on future data whose distribution may have changed.",1.1 Related Work,[0],[0]
"Methods exist to adjust for changes in the data distribution (covariate shift) (Shimodaira, 2000; Bickel et al., 2009), but time is not typically incorporated into such methods explicitly.
",1.1 Related Work,[0],[0]
"One line of work that explicitly studies the relationship between time and the distribution of data is work on classifying the time period in which a document was written (document dating) (Kanhabua and Nørvåg, 2008; Chambers, 2012; Kotsakos et al., 2014).",1.1 Related Work,[0],[0]
"However, this task is directed differently from our work: predicting timestamps given documents, rather than predicting information about documents given timestamps.",1.1 Related Work,[0],[0]
"Our study experiments with six corpora:
• Reviews: Three corpora containing reviews labeled with sentiment: music reviews from Amazon (He and McAuley, 2016), and hotel reviews and restaurant reviews from Yelp.1 We discarded reviews that had fewer than 10 tokens or a helpfulness/usefulness score of zero.",2 Datasets and Experimental Setup,[0],[0]
"The reviews with neutral scores were removed.
",2 Datasets and Experimental Setup,[0],[0]
"• Politics: Sentences from the American party platforms of Republicans and Democrats from 1948 to 2016, available every four years.2
• News: Newspaper articles from 1950-2014, labeled with whether the article is relevant to the US economy.3
• Twitter: Tweets labeled with whether they indicate that the user received an influenza vaccination (i.e., a flu shot) (Huang et al., 2017).
",2 Datasets and Experimental Setup,[0],[0]
Our experiments require documents to be grouped into time intervals.,2 Datasets and Experimental Setup,[0],[0]
Table 1 shows the intervals for each corpus.,2 Datasets and Experimental Setup,[0],[0]
Documents that fall outside of these time intervals were removed.,2 Datasets and Experimental Setup,[0],[0]
"We grouped documents into two types of intervals:
• Seasonal: Time intervals within a year (e.g., January through March) that may be repeated across years.
",2 Datasets and Experimental Setup,[0],[0]
"• Non-seasonal: Time intervals that do not repeat (e.g., 1997-1999).
",2 Datasets and Experimental Setup,[0],[0]
"For each dataset, we performed binary classification, implemented in sklearn (Pedregosa et al., 2011).",2 Datasets and Experimental Setup,[0],[0]
"We built logistic regression classifiers with TF-IDF weighted n-gram features (n ∈ {1, 2, 3}), removing features that appeared in less than 2 documents.",2 Datasets and Experimental Setup,[0],[0]
"Except when otherwise specified, we held out a random 10% of documents as
1https://www.yelp.com/dataset 2https://www.comparativeagendas.net/
datasets_codebooks 3https://www.crowdflower.com/ data-for-everyone/
validation data for each dataset.",2 Datasets and Experimental Setup,[0],[0]
"We used Elastic Net (combined `1 and `2) regularization (Zou and Hastie, 2005), and tuned the regularization parameters to maximize performance on the validation data.",2 Datasets and Experimental Setup,[0],[0]
We evaluated the performance using weighted F1 scores.,2 Datasets and Experimental Setup,[0],[0]
We first conduct an analysis of how classifier performance depends on the time intervals in which it is trained and applied.,3 How Does Classification Performance Vary with Time?,[0],[0]
"For each corpus, we train the classifier on each time interval and test on each time interval.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We downsampled the training data within each time interval to match the number of documents in the smallest interval, so that differences in performance are not due to the size of the training data.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"In all experiments, we train a classifier on a partition of 80% of the documents in the time interval, and repeat this five times on different partitions, averaging the five F1 scores to produce the final estimate.",3 How Does Classification Performance Vary with Time?,[0],[0]
"When training and testing on the same interval, we test on the held-out 20% of documents in that interval (standard cross-validation).",3 How Does Classification Performance Vary with Time?,[0],[0]
"When testing on different time intervals, we test on all documents, since they are all held-out from the training interval; however, we still train on five subsets of 80% of documents, so that the training data is identical across all experiments.
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Finally, to understand why performance varies, we also qualitatively examined how the distribution of content changes across time intervals.",3 How Does Classification Performance Vary with Time?,[0],[0]
"To measure the distribution of content, we trained a topic model with 20 topics using gensim (Řehůřek and Sojka, 2010) with default parameters.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We associated each document with one topic (the most probable topic in the document), and then calculated the proportion of each topic within a time period as the proportion of documents in that time period assigned to that topic.",3 How Does Classification Performance Vary with Time?,[0],[0]
"We can then visualize the extent to which the distribution of 20 topics varies by time.
",3 How Does Classification Performance Vary with Time?,[0],[0]
Jan-M ar Apr-Ju n,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.948 0.912 0.913 0.910
0.916 0.949 0.914 0.909
0.916 0.912 0.952 0.910
0.916 0.914 0.918 0.945
Reviews data - music
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.865 0.862 0.862 0.861
0.863 0.862 0.861 0.858
0.862 0.859 0.866 0.861
0.863 0.863 0.863 0.858
Reviews data - hotels
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.898 0.806 0.750 0.769
0.795 0.876 0.745 0.787
0.794 0.795 0.900 0.767
0.791 0.790 0.731 0.891
News data - economy
Jan-M ar Apr-Ju n",3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-Se p Oct-D ec
Train
JanMar",3 How Does Classification Performance Vary with Time?,[0],[0]
Apr -Jun,3 How Does Classification Performance Vary with Time?,[0],[0]
"Jul-S ep
Oct -De
c
Te st
0.896 0.894 0.891 0.856
0.808 0.940 0.853 0.829
0.836 0.904 0.917 0.845
0.849 0.891 0.884 0.902
Twitter data - vaccine
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.823 0.828 0.825 0.859
0.799 0.843 0.830 0.858
0.800 0.819 0.833 0.869
0.790 0.813 0.835 0.880
Reviews data - hotels
2006 -08 2009 -11 2012 -14 2015 -17
Train
200 6-08 200 9-11 201 2-14 201 5-17 Te st
0.829 0.838 0.869 0.883
0.814 0.856 0.870 0.883
0.815 0.842 0.884 0.894
0.814 0.839 0.875 0.902
Reviews data - restaurants
1948 -56 1960 -68 1972 -80 1984 -92",3 How Does Classification Performance Vary with Time?,[0],[0]
"1996 -20042008 -16
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
194 8-56 196 0-68 197 2-80 198 4-92
199 6-20
04 200 8-16
Te st
0.659 0.567 0.518 0.544 0.525 0.532 0.551 0.800 0.529 0.477 0.474 0.495 0.545 0.506 0.678 0.635 0.573 0.523 0.515 0.473 0.565 0.866 0.594 0.569 0.435 0.404 0.490 0.618 0.848 0.684 0.435 0.416 0.480 0.606 0.674 0.819
Politics - US political data
1985 -89 1990 -94 1995 -99 2000 -04 2005 -09 2010 -14
",3 How Does Classification Performance Vary with Time?,[0],[0]
"Train
198 5-89 199 0-94 199 5-99 200 0-04 200 5-09 201 0-14",3 How Does Classification Performance Vary with Time?,[0],[0]
"Te st
0.876 0.758 0.783 0.794 0.777 0.756 0.764 0.883 0.771 0.802 0.789 0.748 0.759 0.760 0.905 0.798 0.806 0.763 0.760 0.756 0.770 0.926 0.805 0.771 0.773 0.767 0.783 0.826 0.900 0.778 0.773 0.750 0.778 0.810 0.786 0.897
News data - economy
Figure 1: Document classification performance when training and testing on different times of year (top) and different years (bottom).",3 How Does Classification Performance Vary with Time?,[0],[0]
Some corpora are omitted for space.,3 How Does Classification Performance Vary with Time?,[0],[0]
The top row of Figure 1 shows the test scores from training and testing on each pair of seasonal time intervals for four of the datasets.,3.1 Seasonal Variability,[0],[0]
"We observe very strong seasonal variations in the economic news corpus, with a drop in F1 score on the order of 10 when there is a mismatch in the season between training and testing.",3.1 Seasonal Variability,[0],[0]
"There is a similar, but weaker, effect on performance in the music reviews from Amazon and the vaccine tweets.",3.1 Seasonal Variability,[0],[0]
"There was virtually no difference in performance in any of the pairs in both review corpora from Yelp (restaurants, not pictured, and hotels).
",3.1 Seasonal Variability,[0],[0]
"To help understand why the performance varies, Figure 2 (left) shows the distribution of topics in each seasonal interval for two corpora: Amazon music reviews and Twitter.",3.1 Seasonal Variability,[0],[0]
"We observe very little variation in the topic distribution across seasons in the Amazon corpus, but some variation in the Twitter corpus, which may explain the large performance differences when testing on held-out seasons in the Twitter data as compared to the Amazon corpus.
",3.1 Seasonal Variability,[0],[0]
"For space, we do not show the descriptions of the topics, but instead only the shape of the distributions to show the degree of variability.",3.1 Seasonal Variability,[0],[0]
"We did qualitatively examine the differences in word features across the time periods, but had difficulty interpreting the observations and were unable to draw clear conclusions.",3.1 Seasonal Variability,[0],[0]
"Thus, characterizing the ways in which content distributions vary over time, and why this affects performance, is still an open question.",3.1 Seasonal Variability,[0],[0]
The bottom row of Figure 1 shows the test scores from training and testing on each pair of nonseasonal time intervals.,3.2 Non-seasonal Variability,[0],[0]
A strong pattern emerges in the political parties corpus: F1 scores can drop by as much as 40 points when testing on different time intervals.,3.2 Non-seasonal Variability,[0],[0]
"This is perhaps unsurprising, as this collection spans decades, and US party positions have substantially changed over time.",3.2 Non-seasonal Variability,[0],[0]
"The performance declines more when testing on time intervals that are further away in time from the training interval, suggesting that changes in party platforms shift gradually over time.",3.2 Non-seasonal Variability,[0],[0]
"In contrast, while there was a performance drop when testing outside the training interval in the economic news corpus, the drop was not gradual.",3.2 Non-seasonal Variability,[0],[0]
"In the Twitter dataset (not pictured), F1 dropped by an average of 4.9 points outside the training interval.
",3.2 Non-seasonal Variability,[0],[0]
"We observe an intriguing non-seasonal pattern that is consistent in both of the review corpora from Yelp, but not in the music review corpus from Amazon (not pictured), which is that the classification performance fairly consistently increases over time.",3.2 Non-seasonal Variability,[0],[0]
"Since we sampled the dataset so that the time intervals have the same number of reviews, this suggests something else changed over time about the way reviews are written that makes the sentiment easier to detect.
",3.2 Non-seasonal Variability,[0],[0]
The right side of Figure 2 shows the topic distribution in the Amazon and Twitter datasets across non-seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
We observe higher levels of variability across time in the non-seasonal intervals as compared to the seasonal intervals.,3.2 Non-seasonal Variability,[0],[0]
"Overall, it is clear that classifiers generally perform best when applied to the same time interval they were trained.",3.3 Discussion,[0],[0]
"Performance diminishes when applied to different time intervals, although different corpora exhibit differ patterns in the way in which the performance diminishes.",3.3 Discussion,[0],[0]
This kind of analysis can be applied to any corpus and could provide insights into characteristics of the corpus that may be helpful when designing a classifier.,3.3 Discussion,[0],[0]
We now consider how to improve classifiers when working with datasets that span different time intervals.,4 Making Classification Robust to Temporality,[0],[0]
We propose to treat this as a domain adaptation problem.,4 Making Classification Robust to Temporality,[0],[0]
"In domain adaptation, any partition of data that is expected to have a different distribution of features can be treated as a domain (Joshi et al., 2013).",4 Making Classification Robust to Temporality,[0],[0]
"Traditionally, domain adaptation is used to adapt models to a common task across rather different sets of data, e.g., a sentiment classifier for different types of products (Blitzer et al., 2007).",4 Making Classification Robust to Temporality,[0],[0]
"Recent work has also applied domain adaptation to adjust for potentially more subtle differences in data, such as adapting for differences in the demographics of authors (Volkova et al., 2013; Lynn et al., 2017).",4 Making Classification Robust to Temporality,[0],[0]
"We follow the same approach, treating time intervals as domains.
",4 Making Classification Robust to Temporality,[0],[0]
"In our experiments, we use the feature augmentation approach of Daumé III (2007) to perform domain adaptation.",4 Making Classification Robust to Temporality,[0],[0]
"Each feature is duplicated to have a specific version of the feature for every domain, as well as a domain-independent version of the feature.",4 Making Classification Robust to Temporality,[0],[0]
"In each instance, the domainindependent feature and the domain-specific feature for that instance’s domain have the same feature value, while the value is zeroed out for the domain-specific features for the other domains.
",4 Making Classification Robust to Temporality,[0],[0]
"This is equivalent to a model where the feature weights are domain specific but share a Gaussian prior across domains (Finkel and Manning, 2009).",4 Making Classification Robust to Temporality,[0],[0]
"This approach is widely used due to its simplicity, and derivatives of this approach have been used in similar work (e.g., (Lynn et al., 2017)).",4 Making Classification Robust to Temporality,[0],[0]
"Following Finkel and Manning (2009), we separately adjust the regularization strength for the domain-independent feature weights and the domain-specific feature weights.",4 Making Classification Robust to Temporality,[0],[0]
"We first examine classification performance on the datasets when grouping the seasonal time intervals (January-March, April-June, July-August, September-December) as domains and applying the feature augmentation approach for domain adaptation.",4.1 Seasonal Adaptation,[0],[0]
"As a baseline comparison, we apply the same classifier, but without domain adaptation.
",4.1 Seasonal Adaptation,[0],[0]
Results are shown in Table 2.,4.1 Seasonal Adaptation,[0],[0]
"We see that applying domain adaptation provides a small boost in three of the datasets, and has no effect on two of the datasets.",4.1 Seasonal Adaptation,[0],[0]
"If this pattern holds in other corpora, then this suggests that it does not hurt performance to apply domain adaptation across different times of year, and in some cases can lead to a small performance boost.",4.1 Seasonal Adaptation,[0],[0]
We now consider the non-seasonal time intervals (spans of years).,4.2 Non-seasonal Adaptation,[0],[0]
"In particular, we consider the scenario when one wants to apply a classifier trained on older data to future data.",4.2 Non-seasonal Adaptation,[0],[0]
"This requires a modification to the domain adaptation approach, because future data includes domains that did not exist in the training data, and thus we cannot learn domain-specific feature weights.",4.2 Non-seasonal Adaptation,[0],[0]
"To solve this, we train in the usual way, but when testing on future data, we only include the domain-independent features.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the domain-independent parameters should be applicable to all domains, and so using only these features should lead to better generalizability to new domains.",4.2 Non-seasonal Adaptation,[0],[0]
"We test this hypothesis by training the classifiers on all but the last time interval, and testing on the final interval.",4.2 Non-seasonal Adaptation,[0],[0]
"For hyperparameter tuning, we used the final time interval of the training data (i.e., the penultimate interval) as the validation set.",4.2 Non-seasonal Adaptation,[0],[0]
"The intuition is that the penultimate interval is the closest to the test data and thus is expected to be most similar to it.
",4.2 Non-seasonal Adaptation,[0],[0]
Results are shown in the first three columns of Table 3.,4.2 Non-seasonal Adaptation,[0],[0]
We see that this approach leads to a small performance boost in all cases except the Twitter dataset.,4.2 Non-seasonal Adaptation,[0],[0]
"This means that this simple feature augmentation approach has the potential to make classifiers more robust to future changes in data.
",4.2 Non-seasonal Adaptation,[0],[0]
How to apply the feature augmentation technique to unseen domains is not well understood.,4.2 Non-seasonal Adaptation,[0],[0]
"By removing the domain-specific features, as we did here, the prediction model has changed, and so its behavior may be hard to predict.",4.2 Non-seasonal Adaptation,[0],[0]
"Nonetheless, this appears to be a successful approach.",4.2 Non-seasonal Adaptation,[0],[0]
We also experimented with including the seasonal features when performing non-seasonal adaptation.,4.2.1 Adding Seasonal Features,[0],[0]
"In this setting, we train the models with two domain-specific features in addition to the domain-independent features: one for the season,
and one for the non-seasonal interval.",4.2.1 Adding Seasonal Features,[0],[0]
"As above, we remove the non-seasonal features at test time; however, we retain the season-specific features in addition to the domain-independent features, as they can be reused in future years.
",4.2.1 Adding Seasonal Features,[0],[0]
The results of this approach are shown in the last column of Table 3.,4.2.1 Adding Seasonal Features,[0],[0]
We find that combining seasonal and non-seasonal features together leads to an additional performance gain in most cases.,4.2.1 Adding Seasonal Features,[0],[0]
"Our experiments suggest that time can substantially affect the performance of document classification, and practitioners should be cognizant of this variable when developing classifiers.",5 Conclusion,[0],[0]
"A simple analysis comparing pairs of time intervals can provide insights into how performance varies with time, which could be a good practice to do when initially working with a corpus.",5 Conclusion,[0],[0]
"Our experiments also suggest that simple domain adaptation techniques can help account for this variation.4
We make two practical recommendations following the insights from this work.",5 Conclusion,[0],[0]
"First, evaluation will be most accurate if the test data is as similar as possible to whatever future data the classifier will be applied to, and one way to achieve this is to select test data from the chronological end of the corpus, rather than randomly sampling data without regard to time.",5 Conclusion,[0],[0]
"Second, we observed that performance on future data tends to increase when hyperparameter tuning is conducted on later data; thus, we also recommend sampling validation data from the chronological end of the corpus.",5 Conclusion,[0],[0]
The authors thank the anonymous reviews for their insightful comments and suggestions.,Acknowledgements,[0],[0]
"This work was supported in part by the National Science Foundation under award number IIS-1657338.
4Our code is available at: https://github.com/ xiaoleihuang/Domain_Adaptation_ACL2018",Acknowledgements,[0],[0]
Many corpora span broad periods of time.,abstractText,[0],[0]
"Language processing models trained during one time period may not work well in future time periods, and the best model may depend on specific times of year (e.g., people might describe hotels differently in reviews during the winter versus the summer).",abstractText,[0],[0]
"This study investigates how document classifiers trained on documents from certain time intervals perform on documents from other time intervals, considering both seasonal intervals (intervals that repeat across years, e.g., winter) and non-seasonal intervals (e.g., specific years).",abstractText,[0],[0]
"We show experimentally that classification performance varies over time, and that performance can be improved by using a standard domain adaptation approach to adjust for changes in time.",abstractText,[0],[0]
Examining Temporality in Document Classification,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 260–269, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"The amount of textual content that is produced and consumed each day all over the world, through news websites, social media, and other information sources, is constantly growing.",1 Introduction,[0],[0]
This makes the process of selecting the right content to read and quickly recognizing basic facts and topics in texts a core task for making content accessible to the users.,1 Introduction,[0],[0]
Automatic summarization strives to provide a means to this end.,1 Introduction,[0],[0]
"This paper describes our automatic summarization system, and its participation in the MultiLing 2015 summarization challenge.
",1 Introduction,[0],[0]
"Our focus has been on producing a largely language-independent solution for the MultiLing 2015 challenge that, in contrast to most attempts in this field, requires a strict minimum of languagespecific components and uses no language-specific materials for the core innovative elements.
",1 Introduction,[0],[0]
"Our motivation comes in part from Hong et al. (2014), who compares a number of single language summarization systems on the same standardized data set and shows that many complex, language-specific, highly optimized and trained
methods do not significantly out-perform simplistic algorithms that date back to the first summarization competitions in 2004.
",1 Introduction,[0],[0]
"Language-independent text summarization is generally based on sentence extractive methods: A subset of sentences in a text are identified and combined to form a summary, rather than performing more complex operations, and the primary task of summarization algorithms is to identify the set of sentences that form the best summary.",1 Introduction,[0],[0]
"In this case, algorithms differ mostly in how sentences are selected.
",1 Introduction,[0],[0]
One textual feature that has proven useful in identifying good summary sentences is the relative prominence of specific words in texts when contrasted to a reference distribution (like frequency in a large general corpus).,1 Introduction,[0],[0]
"For example, the “keyness” metric in El-Haj and Rayson (2013), singular value decomposition on a term-vector matrix (Steinberger, 2013) and neural network-derived transformations of term vectors (Kågebäck et al., 2014) have all produced significant results.",1 Introduction,[0],[0]
There are also a number of rule-based approaches like Anechitei and Ignat (2013).,1 Introduction,[0],[0]
"Hong et al. (2014) provides an overview of various current approaches, ranging from simple baseline algorithms to complex systems with many machine learning and rule-based components of various kinds.
",1 Introduction,[0],[0]
"One promising recent approach is graph theorybased schemes which construct sentence similarity graphs and use various graph techniques to determine the importance of specific sentences as a heuristic to identify good summary sentences (Barth, 2004; Li et al., 2013b; Mihalcea and Tarau, 2004).
",1 Introduction,[0],[0]
"In this paper, we describe ExB’s graphbased summarization approach and its results in two MultiLing 2015 tasks: Multilingual Singledocument Summarization and Multilingual Multidocument Summarization.",1 Introduction,[0],[0]
ExB’s submissions covered all languages in each task.,1 Introduction,[0],[0]
"Furthermore,
260
we summarize and discuss some unexpected negative experimental results, particularly in light of the problems posed by summarization tasks and their evaluation using ROUGE (Lin, 2004).",1 Introduction,[0],[0]
"The procedures used in both tasks start from similar assumptions and use a generalized framework for language-independent sentence selectionbased summarization.
",2 Process Overview,[0],[0]
We start from the same basic model as LDA approaches to text analysis: Every document contains a mixture of topics that are probabilistically indicative of the tokens present in it.,2 Process Overview,[0],[0]
"We select sentences in order to generate summaries whose topic mixtures most closely match that of the document as a whole (Blei et al., 2003).
",2 Process Overview,[0],[0]
"We construct a graph representation of the text in which each node corresponds to a sentence, and edges are weighted by a similarity metric for comparing them.",2 Process Overview,[0],[0]
"We then extract key sentences for use in summaries by applying the PageRank/TextRank algorithm, a well-studied algorithm for measuring graph centrality.",2 Process Overview,[0],[0]
"This technique has proven to be good model for similar extraction tasks in the past (Mihalcea and Tarau, 2004).
",2 Process Overview,[0],[0]
We deliberately chose not to optimize any parameters of our core algorithm for specific languages.,2 Process Overview,[0],[0]
Every parameter and design decision applied to all languages equally and was based on cross-linguistic performance.,2 Process Overview,[0],[0]
"Typically it is possible to increase evaluation performance by 2%-4% through fine tuning, but this tends to produce overfitting and the gains are lost when applied to any broader set of languages or domains.
",2 Process Overview,[0],[0]
"Our approach consists of three stages:
1.",2 Process Overview,[0],[0]
Preprocessing using common NLP tools.,2 Process Overview,[0],[0]
"This includes steps like tokenization and sentence identification, and in the multilingual summarization case, an extractor for time references like dates and specific times of day.",2 Process Overview,[0],[0]
"These tools are not entirely languageindependent.
2.",2 Process Overview,[0],[0]
"Sentence graph construction and sentence ranking as described in Sections 2.2 and 2.3 respectively.
3.",2 Process Overview,[0],[0]
Post-processing using simple and languageindependent rules for selecting the highest ranking sentences up to the desired length of text.,2 Process Overview,[0],[0]
Our processing pipeline starts with tokenization and sentence boundary detection.,2.1 Preprocessing,[0],[0]
For most languages we employ ExB’s proprietary languageindependent rule-based tokenizer.,2.1 Preprocessing,[0],[0]
"For Chinese, Japanese and Thai tokenization we use languagedependent approaches:
• Chinese is tokenized using a proprietary algorithm that relies on a small dictionary, the probability distribution of token lengths in Chinese, and a few handcrafted rules for special cases.
",2.1 Preprocessing,[0],[0]
"• For Thai, we use a dictionary containing data from NECTEC (2003) and Satayamas (2014) to calculate the optimal partition of Thai letter sequences based on a shortest path algorithm in a weighted, directed acyclic character graph using dictionary terms found in the text.
",2.1 Preprocessing,[0],[0]
"• For Japanese, we employ the CRF-based MeCab (Kudo et al., 2004; Kudo, 2013) morphological analyzer and tokenizer.",2.1 Preprocessing,[0],[0]
"MeCab is considered state-of-the-art and is currently being used in the construction of annotated reference corpora for Japanese by Maekawa et al. (2014).
",2.1 Preprocessing,[0],[0]
"Sentence boundary detection is rule-based and uses all sentence separators available in the Unicode range of the document’s main language, along with an abbreviation list and a few rules to correctly identify expressions like “p.ex.” or “...”
Finally, we use a proprietary SVM-based stemmer trained for a wide variety of languages on custom corpora.",2.1 Preprocessing,[0],[0]
"Given a set of tokenized sentences S, we construct a weighted undirected graph G = (V,E), where each vertex Vi ∈ V corresponds to a sentence in S. The weighted edges (Si, Sj , w) of the graph are defined as a subset of S × S where i",2.2 Graph construction,[0],[0]
"6= j and (w ← sim(Si, Sj))",2.2 Graph construction,[0],[0]
≥ t for a given similarity measure sim and a given threshold t.,2.2 Graph construction,[0],[0]
"We always assume a normalized similarity measure with a scale between 0 and 1.
",2.2 Graph construction,[0],[0]
"Sentence similarity is computed with the standard vector space model (Salton, 1989), where each sentence is defined by a vector of its tokens.
",2.2 Graph construction,[0],[0]
"We compared these vectors using a number of techniques:
• An unweighted bag-of-words model with sentence similarity computed using the Jacquard index.
",2.2 Graph construction,[0],[0]
"• Conventional cosine similarity of sentence vectors weighted by term frequency in the sentence.
",2.2 Graph construction,[0],[0]
"• TF-IDF weighted cosine similarity, where term frequencies in sentences are normalized with respect to the document collection.
",2.2 Graph construction,[0],[0]
"• Semantic similarity measured using the ExB Themis semantic approach described in Hänig et al. (2015).
",2.2 Graph construction,[0],[0]
"We also evaluated different settings for the threshold t. We did not optimize t separately for different languages, instead setting a single value for all languages.
",2.2 Graph construction,[0],[0]
"Surprisingly, when averaged over all 38 languages in the MSS training set, the simple bag-ofwords model with a threshold t = 0.3 produced the best result using the ROUGE-2 measure.",2.2 Graph construction,[0],[0]
"We then apply to the sentence similarity graph an iterative extension of the PageRank algorithm (Brin and Page, 1998) that we have called FairTextRank (FRank) to rank the sentences in the graph.",2.3 Sentence ranking,[0],[0]
"PageRank has been used as a ranker for an extractive summarizer before in Mihalcea and Tarau (2004), who named it TextRank when used for this purpose.",2.3 Sentence ranking,[0],[0]
"PageRank constitutes a measure of graph centrality, so intuitively we would expect it to select the most central, topical, and summarizing sentences in the text.
",2.3 Sentence ranking,[0],[0]
"Following our assumption that every document constitutes a mix of topics, we further assume that every topic corresponds to a cluster in the sentence graph.",2.3 Sentence ranking,[0],[0]
"However, PageRank is not a cluster sensitive algorithm and does not, by itself, ensure coverage of the different clusters present in any graph.",2.3 Sentence ranking,[0],[0]
"Therefore, our FRank algorithm invokes PageRank iteratively on the graph, at each step ranking all the sentences, then removing the top ranking sentence from the graph, and then running PageRank again to extract the next highest ranking sentence.",2.3 Sentence ranking,[0],[0]
"Because the most central sentence in the entire graph is also, by definition, the most central sentence in some cluster, removing it weakens
the centrality of the other sentences in that cluster and increases the likelihood that the next sentence selected will be the highest ranking sentence in another cluster.
",2.3 Sentence ranking,[0],[0]
"A similar method of removing selected sentences is used in the UWB Summarizer by Steinberger (2013), which was one of the top performing systems at MultiLing 2013.",2.3 Sentence ranking,[0],[0]
"However, the UWB Summarizer uses an LSA algorithm on a sentence-term matrix to identify representative sentences, where we have employed PageRank.
",2.3 Sentence ranking,[0],[0]
The complete algorithm is detailed in Algorithm 1.,2.3 Sentence ranking,[0],[0]
"The function adj returns the weighted adjacency matrix of the sentence graph G. An inner for-loop transforms the weighted adjacency matrix into a column-stochastic matrix where for each column c, where A[i, c] is the weight of the edge between sentence i and sentence c, the following expression holds: ∑ i∈|A|A[i, c] = 1.",2.3 Sentence ranking,[0],[0]
"Informally, each column is normalized at each iteration so that its values sum to 1.",2.3 Sentence ranking,[0],[0]
"pr is the PageRank-algorithm with the default parameters β = 0.85, a convergence threshold of 0.001 and allowed to run for at most 100 iterations as implemented in the JUNG API (O’Madadhain et al., 2010).
",2.3 Sentence ranking,[0],[0]
Algorithm 1,2.3 Sentence ranking,[0],[0]
FairTextRank 1: function FRANK(G) 2:,2.3 Sentence ranking,[0],[0]
R←,2.3 Sentence ranking,[0],[0]
[] 3: while |G| > 0,2.3 Sentence ranking,[0],[0]
"do 4: A← adj(G) 5: for (r, c)← |A|2 do 6: Anorm[r, c]← A[r,c],∑
i∈|A| A[i,c]
7: rank ← pr(Anorm) 8: v ← rank[0] 9: R← R+ v
10: G← G \ v return R",2.3 Sentence ranking,[0],[0]
The final step in processing is the production of a plain text summary.,2.4 Post-processing,[0],[0]
"Given a fixed maximum summary length, we selected the highest ranked sentences produced by the ranking algorithm until total text length was greater than the maximum allowed length, then truncated the last sentence to fit exactly the maximum allowed length.",2.4 Post-processing,[0],[0]
"Although this reduces the human readability of the summary - the last sentence is interrupted without any consideration of the reader at all - it can only increase
the score of an n-gram based evaluation metric like ROUGE.",2.4 Post-processing,[0],[0]
The Multilingual Single-document Summarization (MSS) task consisted of producing summaries for Wikipedia articles in 38 languages.,3 Single Document Summarizer,[0],[0]
All articles were provided as UTF-8 encoded plain-text files and as XML documents that mark sections and other elements of the text structure.,3 Single Document Summarizer,[0],[0]
"We took advantage of the availability of headers and section boundary information in performing this task.
",3 Single Document Summarizer,[0],[0]
There was no overlap between the training data and the evaluation data for the MSS task.,3 Single Document Summarizer,[0],[0]
The released training data consisted of the evaluation data set from MultiLing 2013 as described in Kubina et al. (2013).,3 Single Document Summarizer,[0],[0]
This training data contains 30 articles in each of 40 languages.,3 Single Document Summarizer,[0],[0]
"The MSS task itself at MultiLing 2015 used 30 articles in each of 38 languages, dropping two languages because there were not enough new articles not included in the training data.
",3 Single Document Summarizer,[0],[0]
"In addition to the preprocessing steps described in Section 2.1, for this task we applied a list of sentence filters developed specifically for Wikipedia texts:
• Skip all headers.",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence with with less than 2 to-
kens (mostly errors in sentence boundary detection).
",3 Single Document Summarizer,[0],[0]
•,3 Single Document Summarizer,[0],[0]
"Skip every sentence that contains double quotes.
",3 Single Document Summarizer,[0],[0]
"We then performed sentence graph construction and ranking as described in Sections 2.2 and 2.3
In the post-processing stage, we sorted the sentences selected to go into the summary in order of their position in the original article, before producing a plain text summary by concatenating them.",3 Single Document Summarizer,[0],[0]
The organizers of the MultiLing 2015 challenge measured the quality of our system’s output using five different versions of the ROUGE score.,3.1 Results,[0],[0]
We provide a summary of the results for all participants in Table 1.,3.1 Results,[0],[0]
"It shows the average ranking of each participating system over all the languages on which it was tested, as well as the number of languages on which each system was tested.",3.1 Results,[0],[0]
The systems labelled Lead and Oracles are special systems.,3.1 Results,[0],[0]
"Lead just uses the beginning of the article
as the summary and represents a very simple baseline.",3.1 Results,[0],[0]
"Oracles, on the other hand, is a cheating system that marks the upper bound for any extractive approach.
",3.1 Results,[0],[0]
Only three submissions - highlighted in bold - participated in more than 3 languages.,3.1 Results,[0],[0]
"We submitted only one run of our system, defined as a fixed set of parameters that are the same over all languages.",3.1 Results,[0],[0]
One of the other two systems that participated in all 38 languages submitted five runs.,3.1 Results,[0],[0]
"According to the frequently used ROUGE-1 and ROUGE-2 scores, our system achieved an average ranking of 3.2 and 3.3, respectively.",3.1 Results,[0],[0]
"This table shows that the CCS system performed better on average than our system, and the LCS-IESI system performed on average worse.
",3.1 Results,[0],[0]
"However, ROUGE-1 only measures matching single words, whereas ROUGE-2 measures matching bigrams.",3.1 Results,[0],[0]
More complex combinations of words are more indicative of topic matches between gold standard data and system output.,3.1 Results,[0],[0]
"We believe that ROUGE-SU4, which measures bigrams of words with some gaps as well as unigrams, would be a better measure of output quality.",3.1 Results,[0],[0]
"When manually inspecting the summaries, we have the strong impression that system runs in which our system scored well by ROUGESU4 measures, but poorly by ROUGE-2, did produce better summaries with greater readability and topic coverage.
",3.1 Results,[0],[0]
"Our system achieves a significantly better overall ranking using ROUGE-SU4 instead of ROUGE-2, even though the system was optimized to produce the highest ROUGE-2 scores.",3.1 Results,[0],[0]
Only two runs of the winning system CCS scored better than our system according to ROUGE-SU4.,3.1 Results,[0],[0]
"This underlines the robustness of our system’s underlying principles, despite the known problems with ROUGE evaluations.",3.1 Results,[0],[0]
The Multilingual Multi-document Summarization (MMS) task involves summarizing ten news articles on a single topic in a single language.,4 Multi Document Summarizer,[0],[0]
"For each language, the dataset consists of ten to fifteen topics, and ten languages were covered in all, including and expanding on the data used in the 2013 MMS task described by Li et al. (2013a).
",4 Multi Document Summarizer,[0],[0]
"The intuition guiding our approach to this task is the idea that if news articles on the same topic contain temporal references that are close together
or overlapping in time, then they are likely to describe the same event.",4 Multi Document Summarizer,[0],[0]
We therefore cluster the documents in each collection by the points in time referenced in the text rather than attempting to summarize the concatenation of the documents directly.,4 Multi Document Summarizer,[0],[0]
"This approach has the natural advantage that we can present summary information in chronological order, thereby often improving readability.",4 Multi Document Summarizer,[0],[0]
"Unfortunately, this improvement is not measurable using ROUGE-style metrics as employed in evaluating this task.
",4 Multi Document Summarizer,[0],[0]
"An official training data set with model summaries was released, but too late to inform our submission, which was not trained with any new 2015 data.",4 Multi Document Summarizer,[0],[0]
"We did, however, use data from the 2011 MultiLing Pilot including gold standard summaries (Giannakopoulos et al., 2011), which forms a part of the 2015 dataset.",4 Multi Document Summarizer,[0],[0]
"We used only the 700 documents and summaries from the 2011 task as training data, and did not use any Chinese, Spanish or Romanian materials in preparing our submission.
",4 Multi Document Summarizer,[0],[0]
"Our submission follows broadly the same procedure as for the single document summarization task, as described in Section 2 and Section 3, except for the final step, which relies on section information not present in the news articles that form the dataset for this task.",4 Multi Document Summarizer,[0],[0]
"Instead, a manual examination of the dataset revealed that the news articles all have a fixed structure: the first line is the headline, the second is the date, and the remaining lines form the main text.",4 Multi Document Summarizer,[0],[0]
"We used this underlying structure in preprocessing to identify the dateline of the news article, and we use this date to disambiguate relative time expressions in the text like “yesterday” or “next week”.",4 Multi Document Summarizer,[0],[0]
"Articles are also ordered in
time with respect to each other on the basis of the article date.
",4 Multi Document Summarizer,[0],[0]
"Furthermore, we remove in preprocessing any sentence that contains only time reference tokens because they are uninformative for summarization.
",4 Multi Document Summarizer,[0],[0]
"We then extract temporal references from the text, using ExB’s proprietary TimeRec framework described in Thomas (2012), which is available for all the languages used in this task.",4 Multi Document Summarizer,[0],[0]
"With the set of disambiguated time references in each document, we can provide a “timeframe” for each document that ranges from the earliest time referenced in the text to the latest.",4 Multi Document Summarizer,[0],[0]
"Note that this may not include the date of the document itself, if, for example, it is a retrospective article about an event that may have happened years in the past.",4 Multi Document Summarizer,[0],[0]
Ng et al. (2014) and,4.1 Time information processing,[0],[0]
Wan (2007) investigate using textural markers of time for multi-document summarization of news articles using very different algorithms.,4.1 Time information processing,[0],[0]
Our approach is more similar to Ng et al. in constructing a timeline for each document and for the collection as a whole based on references extracted from texts.,4.1 Time information processing,[0],[0]
"Once document timeframes are ordered chronologically, we organize them into groups based on their positions on a time line.",4.1 Time information processing,[0],[0]
"We explored two strategies to produce these groups:
• Least Variance Clustering (LVC):",4.1 Time information processing,[0],[0]
Grouping the documents iteratively by adding a new document to the group if the overall variance of the group doesn’t go over a threshold.,4.1 Time information processing,[0],[0]
"We set the standard deviation limit of the group
in 0.1.",4.1 Time information processing,[0],[0]
The algorithm is a divisive clustering algorithm based on the central time of the documents and the standard deviation.,4.1 Time information processing,[0],[0]
"At first the minimal central time of a document collection is subtracted from all other central times, then we compute mean, variance and standard deviation based on days as a unit and normalized by the mean.",4.1 Time information processing,[0],[0]
"Afterwards we recursively split the groups with the goal to minimize the variance of both splits until either a group consists only of one document or the recomputed standard deviation of a group is less than 0.1.
",4.1 Time information processing,[0],[0]
"• Overlapping Time Clustering (OTC): Grouping documents together if their timeframes overlap more than a certain amount, which we empirically set to 0.9 after experimenting with various values.",4.1 Time information processing,[0],[0]
"This means that if two texts A and B are grouped together, then either A’s timeframe includes at least 90% of B’s timeframe, or B’s timeframe includes 90% of A’s.",4.1 Time information processing,[0],[0]
"This approach proceeds iteratively, with each new addition to a group updating the timeframe of the group as a whole, and any text which overlaps more than 90% with this new interval is then grouped with it in the next iteration.
",4.1 Time information processing,[0],[0]
"In addition, we provide two baseline clusterings:
• One document per cluster (1PC): Each document is in a cluster by itself.
",4.1 Time information processing,[0],[0]
"• All in one cluster (AIO): All documents from one topic are clustered together.
",4.1 Time information processing,[0],[0]
"In the LVC and OTC cases, clustering is iterative and starts with the earliest document as determined by a fixed “central” date for each document.",4.1 Time information processing,[0],[0]
"We explored different ways of determining that “central” date: One was using the dateline found in preprocessing on the second line of each document, another was the median of the time references in the document.",4.1 Time information processing,[0],[0]
"Our best result used the dateline from each article and, as can be seen in Table 2, was produced by the OTC strategy.",4.1 Time information processing,[0],[0]
"This is a surprising result, as we expected LVC to perform better since variance is generally a better measure of clustering.",4.1 Time information processing,[0],[0]
"However, we found that LVC generally produced more clusters than OTC and we believe that to account for its poor performance.
",4.1 Time information processing,[0],[0]
"We experimented with a number of other ordering and clustering approaches, although they do not figure into our submission to the MMS task, but in all cases they failed to out-perform the OTC approach according to the ROUGE-2 recall measure.
",4.1 Time information processing,[0],[0]
"For all conditions, identical preprocessing was performed using ExB’s proprietary languagespecific tokenizer and sentence identifier.",4.1 Time information processing,[0],[0]
"ROUGE scores, because they are based on token n-grams, are very sensitive to discrepancies between tokenizers and stemmers.",4.1 Time information processing,[0],[0]
"In English, because most tokenizers perform very similarly, this causes fewer problems in scoring than for Arabic or other languages where tokenizers vary dramatically.",4.1 Time information processing,[0],[0]
"We used the results in Table 2 to decide which conditions to use in the competition, but we cannot be sure to what degree our results have been influenced by these kinds of ROUGE-related problems.
",4.1 Time information processing,[0],[0]
"After clustering, we perform graph-based sentence ranking as described in Sections 2.2 and 2.3 separately for each cluster.",4.1 Time information processing,[0],[0]
"We then select sentences from each cluster, ensuring that they are all represented in the final summary, so that the entire time span of the articles is covered.",4.1 Time information processing,[0],[0]
"We also order the selected sentences in the summary based on the temporal ordering of the clusters, so that summary presentation is in event order.",4.1 Time information processing,[0],[0]
"When experimenting with the challenge data we made several observations:
1.",4.2 Experimental results,[0],[0]
"Since the dataset of MMS is composed of news articles, just selecting the headlines and first sentences will produce a strong baseline with very high ROUGE scores.",4.2 Experimental results,[0],[0]
"It is difficult to beat this baseline using sentence extraction techniques.
",4.2 Experimental results,[0],[0]
2.,4.2 Experimental results,[0],[0]
The quality of the summaries varies a great deal between languages.,4.2 Experimental results,[0],[0]
"Instead of producing fine-tuned configurations for each lan-
guage that optimize ROUGE scores, we focused on increasing the performance in English - a language we can read and in which we can qualitatively evaluate the produced summaries.
",4.2 Experimental results,[0],[0]
3.,4.2 Experimental results,[0],[0]
All the results here of the time information processing are at document-level.,4.2 Experimental results,[0],[0]
"We also tried to apply the time grouping algorithms per sentence, but we noticed a drop of about 3% ROUGE-2 score on average.
",4.2 Experimental results,[0],[0]
"The most important finding is that using temporal expressions and chronological information does improve the performance of the summary system, and that the iterative FairTextRank algorithm shows a solid performance even for multiple documents.
",4.2 Experimental results,[0],[0]
"As can be seen in Table 3, our system gets ranked in middle position in the official scores of the challenge using the NPowER, MeMoG and AutoSummENG measures as described in Giannakopoulos and Karkaletsis (2013) and Giannakopoulos and Karkaletsis (2011).",4.2 Experimental results,[0],[0]
"We also note that our system out-performs all other participants in Chinese, a language for which we had no training data.",4.2 Experimental results,[0],[0]
"We feel that it is important not only to publish positive results, but also negative ones, to counter the strong publication bias identified in many areas in the natural and social sciences (Dickersin et al., 1987; Ioannidis, 2005).",5 Negative results,[0],[0]
"Since we conducted a large number of experiments in creating this system, we inevitably also came across a number of ideas that seemed good, but turned out to not improve our algorithm, at least as measured using ROUGE-2.
",5 Negative results,[0],[0]
In another challenge participation we developed a very powerful “semantic text similarity” (STS) toolkit.,5 Negative results,[0],[0]
"In SemEval 2015 Task 2 (Agirre et al., 2015), it achieved by far the highest scores for Spanish texts and the second best scores for English.",5 Negative results,[0],[0]
"Since our text summarization methodology is based on a sentence similarity graph, our intuitive hypothesis was that when using this module as opposed to simple matching-words strategies, performance should increase significantly.",5 Negative results,[0],[0]
"Matching-words strategies are used as the baseline in SemEval tasks, and it is easily out-performed by more sophisticated approaches.
",5 Negative results,[0],[0]
"Therefore, we tried out our STS module as a replacement for Jacquard and cosine similarity measures when constructing the sentence graph, while keeping all other parameters fixed.",5 Negative results,[0],[0]
"Surprisingly, it did not improve performance, and lowered ROUGE-2 scores by 2%.",5 Negative results,[0],[0]
"We also attempted to use word2vec embeddings precomputed on very large corpora (Mikolov et al., 2013) to represent words and hence compute a much finer-grained sentence similarity, but those results were 4% worse.",5 Negative results,[0],[0]
"It is possible that those systems were, in fact, better, but because ROUGE scoring focuses on word matches, any other improvement cannot be measured directly.",5 Negative results,[0],[0]
"We also attempted to include other factors such as sentence length, position, number of named entities, temporal expressions, and physical measurements into the sentence similarity score, all without seeing any increase in ROUGE scores.
",5 Negative results,[0],[0]
"Since identifying temporal expressions increases ROUGE scores, as this paper shows, we surmised that name recognition might also improve summarization.",5 Negative results,[0],[0]
"We applied our named entity recognition system, which is available in a number of different languages and won the Germeval 2014 (Benikova et al., 2014) NER challenge, and weighted more heavily sentences with detected names before extracting summary sentences.",5 Negative results,[0],[0]
"Interestingly, no matter how the weighting scheme was set up, the performance of the system always dropped by a few percent.",5 Negative results,[0],[0]
"Often, the system would select useless sentences that contain long lists of participating authors, or enumerations of entities participating in some reported event.",5 Negative results,[0],[0]
"Even when these kinds of sentences are explicitly removed, it still selects sentences that simply contain many names with little relevance to the topics of the news article.",5 Negative results,[0],[0]
"We conclude that sen-
tences describing central topics in documents are not strongly correlated with named entity usage.
",5 Negative results,[0],[0]
"Another very intuitive assumption is that filtering stop words, or down-weighting very frequent words, or using a TF-IDF based scheme with a similar effect, would improve the results.",5 Negative results,[0],[0]
"However, we did not observe any improvement by using these techniques.",5 Negative results,[0],[0]
"Nonetheless, there are strong indications that this is due to the limitations of ROUGE-2 scoring and we cannot conclude that these kinds of techniques are useless for summarization.",5 Negative results,[0],[0]
It is easy to achieve very competitive ROUGE-2 scores by just filling the summary with very frequent stop word combinations.,5 Negative results,[0],[0]
"A human would immediately recognize the uselessness of such a “summary”, but ROUGE-2 would count many bigram matches with a gold standard summary.
",5 Negative results,[0],[0]
"Finally, we considered the hypothesis that the summary system could be helped by explicitly removing very similar sentences presenting redundant information.",5 Negative results,[0],[0]
"Surprisingly, explicitly removing such sentences did not improve the performance of the system.",5 Negative results,[0],[0]
"Manually inspecting a number of summaries, we notice that very similar sentences recurring often in texts are rarely selected by the FRank algorithm.",5 Negative results,[0],[0]
We believe this is because our approach is sufficiently robust to discount these sentences on its own.,5 Negative results,[0],[0]
"In this paper we outline ExB’s largely languageindependent system for text summarization based on sentence selection, and show that it supports at least the 38 languages used in this completion without any language-specific fine-tuning.",6 Conclusions,[0],[0]
Sentences are selected using an iterative extension of PageRank calculation on a sentence similarity graph.,6 Conclusions,[0],[0]
"Our results in the MultiLing 2015 challenge have validated this approach by achieving the best scores for several languages and competitive scores for most of them, generally surpassed by only one other participating system.
",6 Conclusions,[0],[0]
"We also show that one basic summarization system can apply to different domains, different languages, and different tasks without special configuration, while retaining state-of-the-art performance.",6 Conclusions,[0],[0]
"Furthermore, for multi-document news summarization, we show that extracting temporal expressions is a useful feature for combining articles on the same topic.
",6 Conclusions,[0],[0]
"Our most relevant conclusion is that both the current evaluation methodology (based on various forms of ROUGE) as well as the current principal approach to language-independent text summarization (context-free, sentence selection based) are highly inadequate to model the vague requirements users associate with a text summarization product.
",6 Conclusions,[0],[0]
Participants in MultiLing 2015 did not receive the scripts and parameters used in producing evaluations.,6 Conclusions,[0],[0]
This made it difficult to optimize parameters and algorithms and has a significant impact on results using ROUGE measures and probably the other measures as well.,6 Conclusions,[0],[0]
"Hong et al. (2014), for example, notes values between 30.8% and 39.1% using ROUGE-1 for one well-known algorithm on one data set by different authors.",6 Conclusions,[0],[0]
It is not clear how the vastly different scores obtained for identical summaries using different ROUGE parameters correlate with the objective quality of a given summary.,6 Conclusions,[0],[0]
"We have no clear indication that ROUGE scores really capture the quality of a given summary at all.
",6 Conclusions,[0],[0]
"While it is possible to formulate summarization solutions based on sentence selection and even iteratively improve them using ROUGE scores, the actual achievable performance measured using ROUGE is very low.",6 Conclusions,[0],[0]
"We have noticed that stemming, stopword filtering and various tokenization strategies can have a very large influence on ROUGE scores, especially in morphologically richer languages than English.",6 Conclusions,[0],[0]
"More modern evaluation measures like MeMog or NPoweR might solve the problems inherent to ROUGE, however they currently lack widespread adoption in the research community.
",6 Conclusions,[0],[0]
"Nonetheless, even if these issues in evaluation can be addressed, we do not believe that summaries based on sentence selection will ever reach a quality where they could be accepted as comparable to a human written summary.",6 Conclusions,[0],[0]
We present our state of the art multilingual text summarizer capable of single as well as multi-document text summarization.,abstractText,[0],[0]
"The algorithm is based on repeated application of TextRank on a sentence similarity graph, a bag of words model for sentence similarity and a number of linguistic preand post-processing steps using standard NLP tools.",abstractText,[0],[0]
We submitted this algorithm for two different tasks of the MultiLing 2015 summarization challenge: Multilingual Singledocument Summarization and Multilingual Multi-document Summarization.,abstractText,[0],[0]
ExB Text Summarizer,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1329–1338 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1329
In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.",text,[0],[0]
"With the availability of large datasets and the recent progress made by neural methods, variants of sequence to sequence learning (seq2seq) (Sutskever et al., 2014) architectures have been successfully applied for building conversational systems (Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"However, despite these methods being the stateof-the art frameworks for conversation generation, they suffer from problems such as lack of diversity in responses and generation of short, repetitive and uninteresting responses (Liu et al., 2016; Serban et al., 2016, 2017b).",1 Introduction,[0],[0]
"A large body of recent
literature has focused on overcoming such challenges (Li et al., 2016a; Lowe et al., 2017).
",1 Introduction,[0],[0]
"In part, such problems arise as all information required to generate responses needs to be captured as part of the model parameters learnt from the training data.",1 Introduction,[0],[0]
These model parameters alone may not be sufficient for generating natural conversations.,1 Introduction,[0],[0]
"Therefore, despite providing enormous amount of data, neural generative systems have been found to be ineffective for use in real world applications (Liu et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we focus our attention on closed domain conversations.",1 Introduction,[0],[0]
"A characteristic feature of such conversations is that over a period of time, some conversation contexts1 are likely to have occurred previously (Lu et al., 2017b).",1 Introduction,[0],[0]
"For instance, Table 1 shows some contexts from the Ubuntu dialog corpus.",1 Introduction,[0],[0]
"Each row presents an input dialog context with its corresponding gold response followed by a similar context and response seen in training data – as can be seen, contexts for “installing dms”, “sharing files”, “blocking ufw ports” have all occurred in training data.",1 Introduction,[0],[0]
"We hypothesize that being able to refer to training responses for previously seen similar contexts could be a helpful signal to use while generating responses.
",1 Introduction,[0],[0]
"In order to exploit this aspect of closed domain conversations we build our neural encoderdecoder architecture called the Exemplar Encoder Decoder (EED), that learns to generate a response for a given context by exploiting similar contexts from training conversations.",1 Introduction,[0],[0]
"Thus, instead of having the seq2seq model learn patterns of language only from aligned parallel corpora, we assist the model by providing it closely related (similar) samples from the training data that it can refer to while generating text.
",1 Introduction,[0],[0]
"Specifically, given a context c, we retrieve a set 1We use the phrase “dialog context”, “conversation con-
text” and “context” interchangeably throughout the paper.
of context-response pairs (c(k), r(k)), 1 ≤ k ≤ K using an inverted index of training data.",1 Introduction,[0],[0]
We create an exemplar vector e(k) by encoding the response r(k) (also referred to as exemplar response) along with an encoded representation of the current context c. We then learn the importance of each exemplar vector e(k) based on the likelihood of it being able to generate the ground truth response.,1 Introduction,[0],[0]
We believe that e(k) may contain information that is helpful in generating the response.,1 Introduction,[0],[0]
"Table 1 highlights the words in exemplar responses that appear in the ground truth response as well.
",1 Introduction,[0],[0]
"Contributions: We present a novel Exemplar Encoder-Decoder (EED) architecture that makes use of similar conversations, fetched from an index of training data.",1 Introduction,[0],[0]
"The retrieved contextresponse pairs are used to create exemplar vectors which are used by the decoder in the EED model, to learn the importance of training context-response pairs, while generating responses.",1 Introduction,[0],[0]
"We present detailed experiments on the publicly benchmarked Ubuntu dialog corpus data set (Lowe et al., 2015) as well a large collection of more than 127,000 technical support conversations.",1 Introduction,[0],[0]
"We compare the performance of the EED model with the existing state of the art generative models such as HRED (Serban et al., 2016) and VHRED (Serban et al., 2017b).",1 Introduction,[0],[0]
"We find that our model out-performs these models on a wide variety of metrics such as the recently proposed Activity Entity metrics (Serban et al., 2017a) as well as Embedding-based metrics (Lowe et al., 2015).",1 Introduction,[0],[0]
"In addition, we present qualitative insights into our results and we find that exemplar based responses
are more informative and diverse.",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 briefly describes the recent works in neural dialogue generation The details of the proposed EED model for dialogue generation are described in detail in Section 3.,1 Introduction,[0],[0]
"In Section 4, we describe the datasets as well as the details of the models used during training.",1 Introduction,[0],[0]
We present quantitative and qualitative results of EED model in Section 5.,1 Introduction,[0],[0]
"In this section, we compare our work against other data-driven end-to-end conversation models.",2 Related Work,[0],[0]
"Endto-end conversation models can be further classified into two broad categories — generation based models and retrieval based models.
",2 Related Work,[0],[0]
Generation based models cast the problem of dialogue generation as a sequence to sequence learning problem.,2 Related Work,[0],[0]
"Initial works treat the entire context as a single long sentence and learn an encoder-decoder framework to generate response word by word (Shang et al., 2015; Vinyals and Le, 2015).",2 Related Work,[0],[0]
"This was followed by work that models context better by breaking it into conversation history and last utterance (Sordoni et al., 2015b).",2 Related Work,[0],[0]
"Context was further modeled effectively by using a hierarchical encoder decoder (HRED) model which first learns a vector representation of each utterance and then combines these representations to learn vector representation of context (Serban et al., 2016).",2 Related Work,[0],[0]
"Later, an alternative hierarchical model called VHRED (Serban et al., 2017b) was proposed, where generated responses were conditioned on latent variables.",2 Related Work,[0],[0]
"This leads to more in-
formative responses and adds diversity to response generation.",2 Related Work,[0],[0]
"Models that explicitly incorporate diversity in response generation have also been studied in literature (Li et al., 2016b; Vijayakumar et al., 2016; Cao and Clark, 2017; Zhao et al., 2017).
",2 Related Work,[0],[0]
"Our work differs from the above as none of these above approaches utilize similar conversation contexts observed in the training data explicitly.
",2 Related Work,[0],[0]
"Retrieval based models on the other hand treat the conversation context as a query and obtain a set of responses using information retrieval (IR) techniques from the conversation logs (Ji et al., 2014).",2 Related Work,[0],[0]
"There has been further work where the responses are further ranked using a deep learning based model (Yan et al., 2016a,b; Qiu et al., 2017).",2 Related Work,[0],[0]
"On the other hand of the spectrum, endto-end deep learning based rankers have also been employed to generate responses (Wu et al., 2017; Henderson et al., 2017).",2 Related Work,[0],[0]
"Recently a framework has also been proposed that uses a discriminative dialog network that ranks the candidate responses received from a response generator network and trains both the networks in an end to end manner (Lu et al., 2017a).
",2 Related Work,[0],[0]
"In contrast to the above models, we use the input contexts as well as the retrieved responses for generating the final responses.",2 Related Work,[0],[0]
"Contemporaneous to our work, a generative model for machine translation that employs retrieved translation pairs has also been proposed (Gu et al., 2017).",2 Related Work,[0],[0]
"We note that while the underlying premise of both the papers remains the same, the difference lies in the mechanism of incorporating the retrieved data.",2 Related Work,[0],[0]
A conversation consists of a sequence of utterances.,3.1 Overview,[0],[0]
"At a given point in the conversation, the utterances expressed prior to it are jointly referred to as the context.",3.1 Overview,[0],[0]
The utterance that immediately follows the context is referred to as the response.,3.1 Overview,[0],[0]
"As discussed in Section 1, given a conversational context, we wish to to generate a response by utilizing similar context-response pairs from the training data.",3.1 Overview,[0],[0]
We retrieve a set of K exemplar contextresponse pairs from an inverted index created using the training data in an off-line manner.,3.1 Overview,[0],[0]
"The input and the retrieved context-response pairs are then fed to the Exemplar Encoder Decoder (EED)
network.",3.1 Overview,[0],[0]
A schematic illustration of the EED network is presented in Figure 1.,3.1 Overview,[0],[0]
The EED encoder combines the input context and the retrieved responses to create a set of exemplar vectors.,3.1 Overview,[0],[0]
The EED decoder then uses the exemplar vectors based on the similarity between the input context and retrieved contexts to generate a response.,3.1 Overview,[0],[0]
We now provide details of each of these modules.,3.1 Overview,[0],[0]
"Given a large collection of conversations as (context, response) pairs, we index each response and its corresponding context in tf",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
− idf vector space.,3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"We further extract the last turn of a conversation and index it as an additional attribute of the context-response document pairs so as to allow directed queries based on it.
",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given an input context c, we construct a query that weighs the last utterance in the context twice as much as the rest of the context and use it to retrieve the top-k similar context-response pairs from the index based on a BM25 (Robertson et al., 2009) retrieval model.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"These retrieved pairs form our exemplar context-response pairs (c(k), r(k)), 1 ≤ k ≤ K.",3.2 Retrieval of Similar Context-Response Pairs,[0],[0]
"Given the exemplar pairs (c(k), r(k)), 1 ≤ k ≤ K and an input context-response pair (c, r), we feed the input context c and the exemplar contexts c(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", c(K)",3.3 Exemplar Encoder Network,[0],[0]
"through an encoder to generate the embeddings as given below:
ce = Encodec(c)
c(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encodec(c (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K
Note that we do not constrain our choice of encoder and that any parametrized differentiable architecture can be used as the encoder to generate the above embeddings.",3.3 Exemplar Encoder Network,[0],[0]
"Similarly, we feed the exemplar responses r(1), . . .",3.3 Exemplar Encoder Network,[0],[0]
", r(K) through a response encoder to generate response embeddings r (1) e , . . .",3.3 Exemplar Encoder Network,[0],[0]
", r (K) e , that is,
r(k)e =",3.3 Exemplar Encoder Network,[0],[0]
"Encoder(r (k)), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (1)
",3.3 Exemplar Encoder Network,[0],[0]
"Next, we concatenate the exemplar response encoding r(k)e with an encoded representation of current context ce as shown in equation 2 to create the exemplar vector e(k).",3.3 Exemplar Encoder Network,[0],[0]
"This allows us to include in-
formation about similar responses along with the encoded input context representation.
e(k) =",3.3 Exemplar Encoder Network,[0],[0]
"[ce; r (k) e ], 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
"K (2)
",3.3 Exemplar Encoder Network,[0],[0]
"The exemplar vectors e(k), 1 ≤ k ≤",3.3 Exemplar Encoder Network,[0],[0]
K are further used by the decoder for generating the ground truth response as described in the next section.,3.3 Exemplar Encoder Network,[0],[0]
Recall that we want the exemplar responses to help generate the responses based on how similar the corresponding contexts are with the input context.,3.4 Exemplar Decoder Network,[0],[0]
"More similar an exemplar context is to the input context, higher should be its effect in generating the response.",3.4 Exemplar Decoder Network,[0],[0]
"To this end, we compute the similarity scores s(k), 1 ≤ k ≤ K using the encodings computed in Section 3.3 as shown below.
s(k) =",3.4 Exemplar Decoder Network,[0],[0]
"exp(cTe c (k) e )∑K
l=1",3.4 Exemplar Decoder Network,[0],[0]
"exp(c T e c (l) e )
(3)
",3.4 Exemplar Decoder Network,[0],[0]
"Next, each exemplar vector e(k) computed in Section 3.3, is fed to a decoder, where the decoder is responsible for predicting the ground truth response from the exemplar vector.",3.4 Exemplar Decoder Network,[0],[0]
Let pdec(r|e(k)),3.4 Exemplar Decoder Network,[0],[0]
be the distribution of generating the ground truth response given the exemplar embedding.,3.4 Exemplar Decoder Network,[0],[0]
"The objective function to be maximized, is expressed as a
function of the scores s(k), the decoding distribution pdec and the exemplar vectors e(k) as shown below:
ll = K∑ k=1 s(k) log pdec(r|e(k))",3.4 Exemplar Decoder Network,[0],[0]
"(4)
Note that we weigh the contribution of each exemplar vector to the final objective based on how similar the corresponding context is to the input context.",3.4 Exemplar Decoder Network,[0],[0]
"Moreover, the similarities are differentiable function of the input and hence, trainable by back propagation.",3.4 Exemplar Decoder Network,[0],[0]
"The model should learn to assign higher similarities to the exemplar contexts, whose responses are helpful for generating the correct response.
",3.4 Exemplar Decoder Network,[0],[0]
The model description uses encoder and decoder networks that can be implemented using any differentiable parametrized architecture.,3.4 Exemplar Decoder Network,[0],[0]
We discuss our choices for the encoders and decoder in the next section.,3.4 Exemplar Decoder Network,[0],[0]
"In this section, we discuss the various encoders and the decoder used by our model.",3.5 The Encoders and Decoder,[0],[0]
The conversation context consists of an ordered sequence of utterances and each utterance can be further viewed as a sequence of words.,3.5 The Encoders and Decoder,[0],[0]
"Thus, context can be viewed as having multiple levels of
hierarchies—at the word level and then at the utterance (sentence) level.",3.5 The Encoders and Decoder,[0],[0]
"We use a hierarchical recurrent encoder—popularly employed as part of the HRED framework for generating responses and query suggestions (Sordoni et al., 2015a; Serban et al., 2016, 2017b).",3.5 The Encoders and Decoder,[0],[0]
The word-level encoder encodes the vector representations of words of an utterance to an utterance vector.,3.5 The Encoders and Decoder,[0],[0]
"Finally, the utterance-level encoder encodes the utterance vectors to a context vector.
",3.5 The Encoders and Decoder,[0],[0]
"Let (u1, . . .",3.5 The Encoders and Decoder,[0],[0]
",uN ) be the utterances present in the context.",3.5 The Encoders and Decoder,[0],[0]
"Furthermore, let (wn1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wnMn) be the words present in the nth utterance for 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
N .,3.5 The Encoders and Decoder,[0],[0]
"For each word in the utterance, we retrieve its corresponding embedding from an embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
The word embedding for wnm will be denoted as wenm.,3.5 The Encoders and Decoder,[0],[0]
"The encoding of the nth utterance can be computed iteratively as follows:
hnm = f1(hnm−1, wenm), 1 ≤ m ≤Mn (5)
We use an LSTM (Hochreiter and Schmidhuber, 1997) to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state hnMn is referred to as the utterance encoding and will be denoted as hn.
",3.5 The Encoders and Decoder,[0],[0]
"The utterance-level encoder takes the utterance encodings h1, . . .",3.5 The Encoders and Decoder,[0],[0]
", hN as input and generates the encoding for the context as follows:
cen = f2(cen−1, hn), 1 ≤ n ≤",3.5 The Encoders and Decoder,[0],[0]
"N (6)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
"The last hidden state ceN is referred to as the context embedding and is denoted as ce.
",3.5 The Encoders and Decoder,[0],[0]
A single level LSTM is used for embedding the response.,3.5 The Encoders and Decoder,[0],[0]
"In particular, let (w1, . . .",3.5 The Encoders and Decoder,[0],[0]
", wM ) be the sequence of words present in the response.",3.5 The Encoders and Decoder,[0],[0]
"For each word w, we retrieve the corresponding word embedding we from a word embedding matrix.",3.5 The Encoders and Decoder,[0],[0]
"The response embedding is computed from the word embeddings iteratively as follows:
rem = g(rem−1, wem), 1 ≤ m ≤M (7)
",3.5 The Encoders and Decoder,[0],[0]
"Again, we use an LSTM to model the above equation.",3.5 The Encoders and Decoder,[0],[0]
The last hidden state rem is referred to as the response embedding and is denoted as re.,3.5 The Encoders and Decoder,[0],[0]
"We conduct experiments on Ubuntu Dialogue Corpus (Lowe et al., 2015)(v2.0)2.",4.1.1 Ubuntu Dataset,[0],[0]
Ubuntu dialogue corpus has about 1M context response pairs along with a label.,4.1.1 Ubuntu Dataset,[0],[0]
The label value 1 indicates that the response associated with a context is the correct response and is incorrect otherwise.,4.1.1 Ubuntu Dataset,[0],[0]
As we are only interested in positive labeled data we work with label = 1.,4.1.1 Ubuntu Dataset,[0],[0]
Table 2 depicts some statistics for the dataset.,4.1.1 Ubuntu Dataset,[0],[0]
We also conduct our experiments on a large technical support dataset with more than 127K conversations.,4.1.2 Tech Support Dataset,[0],[0]
We will refer to this dataset as Tech Support dataset in the rest of the paper.,4.1.2 Tech Support Dataset,[0],[0]
"Tech Support dataset contains conversations pertaining to an employee seeking assistance from an agent (technical support) — to resolve problems such as password reset, software installation/licensing, and wireless access.",4.1.2 Tech Support Dataset,[0],[0]
"In contrast to Ubuntu dataset, this dataset has clearly two distinct users — employee and agent.",4.1.2 Tech Support Dataset,[0],[0]
"In our experiments we model the agent responses only.
",4.1.2 Tech Support Dataset,[0],[0]
"For each conversation in the tech support data, we sample context and response pairs to create a dataset similar to the Ubuntu dataset format.",4.1.2 Tech Support Dataset,[0],[0]
Note that multiple context-response pairs can be generated from a single conversation.,4.1.2 Tech Support Dataset,[0],[0]
"For each conversation, we sample 25% of the possible contextresponse pairs.",4.1.2 Tech Support Dataset,[0],[0]
We create validation pairs by selecting 5000 conversations randomly and sampling context response pairs).,4.1.2 Tech Support Dataset,[0],[0]
"Similarly, we create test pairs from a different subset of 5000 conversations.",4.1.2 Tech Support Dataset,[0],[0]
"The remaining conversations are used to
2https://github.com/rkadlec/ ubuntu-ranking-dataset-creator
create training context-response pairs.",4.1.2 Tech Support Dataset,[0],[0]
Table 3 depicts some statistics for this dataset:,4.1.2 Tech Support Dataset,[0],[0]
"The EED and HRED models were implemented using the PyTorch framework (Paszke et al., 2017).",4.2 Model and Training Details,[0],[0]
We initialize the word embedding matrix as well as the weights of context and response encoders from the standard normal distribution with mean 0 and variance 0.01.,4.2 Model and Training Details,[0],[0]
The biases of the encoders and decoder are initialized with 0.,4.2 Model and Training Details,[0],[0]
The word embedding matrix is shared by the context and response encoders.,4.2 Model and Training Details,[0],[0]
"For Ubuntu dataset, we use a word embedding size of 600, whereas the size of the hidden layers of the LSTMs in context and response encoders and the decoder is fixed at 1200.",4.2 Model and Training Details,[0],[0]
"For Tech support dataset, we use a word embedding size of 128.",4.2 Model and Training Details,[0],[0]
"Furthermore, the size of the hidden layers of the multiple LSTMs in context and response encoders and the decoder is fixed at 256.",4.2 Model and Training Details,[0],[0]
"A smaller embedding size was chosen for the Tech Support dataset since we observed much less diversity in the responses of the Tech Support dataset as compared to Ubuntu dataset.
",4.2 Model and Training Details,[0],[0]
Two different encoders are used for encoding the input context (not shown in Figure 1 for simplicity).,4.2 Model and Training Details,[0],[0]
The output of the first context encoder is concatenated with the exemplar response vectors to generate exemplar vectors as detailed in Section 3.3.,4.2 Model and Training Details,[0],[0]
The output of the second context encoder is used to compute the scoring function as detailed in Section 3.4.,4.2 Model and Training Details,[0],[0]
"For each input context, we retrieve 5 similar context-response pairs for Ubuntu dataset and 3 context-response pairs for Tech support dataset using the tf-idf mechanism discussed in Section 3.2.
",4.2 Model and Training Details,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e",4.2 Model and Training Details,[0],[0]
− 4 for training the model.,4.2 Model and Training Details,[0],[0]
"A batch size of 20 samples was used
during training.",4.2 Model and Training Details,[0],[0]
"In order to prevent overfitting, we use early stopping with log-likelihood on validation set as the stopping criteria.",4.2 Model and Training Details,[0],[0]
"In order to generate the samples using the proposed EED model, we identify the exemplar context that is most similar to the input context based on the learnt scoring function discussed in Section 3.4.",4.2 Model and Training Details,[0],[0]
The corresponding exemplar vector is fed to the decoder to generate the response.,4.2 Model and Training Details,[0],[0]
The samples are generated using a beam search with width 5.,4.2 Model and Training Details,[0],[0]
The average per-word log-likelihood is used to score the beams.,4.2 Model and Training Details,[0],[0]
"A traditional and popular metric used for comparing a generated sentence with a ground truth sentence is BLEU (Papineni et al., 2002) and is frequently used to evaluate machine translation.",5.1.1 Activity and Entity Metrics,[0],[0]
"The metric has also been applied to compute scores for predicted responses in conversations, but it has been found to be less indicative of actual performance (Liu et al., 2016; Sordoni et al., 2015a; Serban et al., 2017a), as it is extremely sensitive to the exact words in the ground truth response, and gives equal importance to stop words/phrases and informative words.
",5.1.1 Activity and Entity Metrics,[0],[0]
Serban et al. (2017a) recently proposed a new set of metrics for evaluating dialogue responses for the Ubuntu corpus.,5.1.1 Activity and Entity Metrics,[0],[0]
"It is important to highlight that these metrics have been specifically designed for the Ubuntu corpus and evaluate a generated response with the ground truth response by comparing the coarse level representation of an utterance (such as entities, activities, Ubuntu OS commands).",5.1.1 Activity and Entity Metrics,[0],[0]
"Here is a brief description of each metric:
• Activity: Activity metric compares the activities present in a predicted response with the ground truth response.",5.1.1 Activity and Entity Metrics,[0],[0]
Activity can be thought of as a verb.,5.1.1 Activity and Entity Metrics,[0],[0]
"Thus, all the verbs in a response are mapped to a set of manually identified list of 192 verbs.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
Entity:,5.1.1 Activity and Entity Metrics,[0],[0]
This compares the technical entities that overlap with the ground truth response.,5.1.1 Activity and Entity Metrics,[0],[0]
"A total of 3115 technical entities is identified using public resources such as Debian package manager APT.
",5.1.1 Activity and Entity Metrics,[0],[0]
•,5.1.1 Activity and Entity Metrics,[0],[0]
"Tense: This measure compares the time tense of ground truth with predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
• Cmd:,5.1.1 Activity and Entity Metrics,[0],[0]
"This metric computes accuracy by comparing commands identified in ground truth utterance with a predicted response.
",5.1.1 Activity and Entity Metrics,[0],[0]
"Table 4 compares our model with other recent generative models (Serban et al., 2017a) — LSTM (Shang et al., 2015), HRED (Serban et al., 2016) & VHRED (Serban et al.,",5.1.1 Activity and Entity Metrics,[0],[0]
"2017b).We do not compare our model with Multi-Resolution RNN (MRNN) (Serban et al., 2017a), as MRNN explicitly utilizes the activities and entities during the generation process.",5.1.1 Activity and Entity Metrics,[0],[0]
"In contrast, the proposed EED model and the other models used for comparison are agnostic to the activity and entity information.",5.1.1 Activity and Entity Metrics,[0],[0]
"We use the standard script3 to compute the metrics.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The EED model scores better than generative models on almost all of the metrics, indicating that we generate more informative responses than other state-of-the-art generative based approaches for Ubuntu corpus.",5.1.1 Activity and Entity Metrics,[0],[0]
"The results show that responses associated with similar contexts may contain the activities and entities present in the ground truth response, and thus help in response generation.",5.1.1 Activity and Entity Metrics,[0],[0]
This is discussed further in Section 5.2.,5.1.1 Activity and Entity Metrics,[0],[0]
"Additionally, we compared our proposed EED with a retrieval only baseline.",5.1.1 Activity and Entity Metrics,[0],[0]
"The retrieval baseline achieves an activity F1 score of 4.23 and entity F1 score of 2.72 compared to 4.87 and 2.99 respectively achieved by our method on the Ubuntu corpus.
",5.1.1 Activity and Entity Metrics,[0],[0]
"The Tech Support dataset is not evaluated using the above metrics, since activity and entity information is not available for this dataset.
",5.1.1 Activity and Entity Metrics,[0],[0]
3https://github.com/julianser/Ubuntu-MultiresolutionTools/blob/master/ActEntRepresentation/eval file.sh,5.1.1 Activity and Entity Metrics,[0],[0]
"Embedding metrics (Lowe et al., 2017) were proposed as an alternative to word by word comparison metrics such as BLEU.",5.1.2 Embedding Metrics,[0],[0]
"We use pre-trained Google news word embeddings4 similar to Serban et al. (2017b), for easy reproducibility as these metrics are sensitive to the word embeddings used.",5.1.2 Embedding Metrics,[0],[0]
"The three metrics of interest utilize the word vectors in ground truth response and a predicted response and are discussed below:
• Average: Average word embedding vectors are computed for the candidate response and ground truth.",5.1.2 Embedding Metrics,[0],[0]
The cosine similarity is computed between these averaged embeddings.,5.1.2 Embedding Metrics,[0],[0]
"High similarity gives as indication that ground truth and predicted response have similar words.
",5.1.2 Embedding Metrics,[0],[0]
"• Greedy: Greedy matching score finds the most similar word in predicted response to ground truth response using cosine similarity.
",5.1.2 Embedding Metrics,[0],[0]
"• Extrema: Vector extrema score computes the maximum or minimum value of each dimension of word vectors in candidate response and ground truth.
",5.1.2 Embedding Metrics,[0],[0]
"Of these, the embedding average metric is the most reflective of performance for our setup.",5.1.2 Embedding Metrics,[0],[0]
"The extrema representation, for instance, is very sensitive to text length and becomes ineffective beyond single length sentences(Forgues et al., 2014).",5.1.2 Embedding Metrics,[0],[0]
We use the publicly available script5 for all our computations.,5.1.2 Embedding Metrics,[0],[0]
"As the test outputs for HRED are not available for Technical Support dataset, we use our
4GoogleNews-vectors-negative300.bin from https:// code.google.com/archive/p/word2vec/
5https://github.com/julianser/ hed-dlg-truncated/blob/master/",5.1.2 Embedding Metrics,[0],[0]
"Evaluation/embedding_metrics.py
own implementation of HRED.",5.1.2 Embedding Metrics,[0],[0]
"Table 5 compares our model with HRED, and depicts that our model scores better on all metrics for Technical Support
dataset, and on majority of the metrics for Ubuntu dataset.
",5.1.2 Embedding Metrics,[0],[0]
"We note that the improvement achieved by the
EED model on activity and entity metrics are much more significant than those on embedding metrics.",5.1.2 Embedding Metrics,[0],[0]
"This suggests that the EED model is better able to capture the specific information (objects and actions) present in the conversations.
",5.1.2 Embedding Metrics,[0],[0]
"Finally, we evaluate the diversity of the generated responses for EED against HRED by counting the number of unique tokens, token-pairs and token-triplets present in the generated responses on Ubuntu and Tech Support dataset.",5.1.2 Embedding Metrics,[0],[0]
The results are shown in Table 6.,5.1.2 Embedding Metrics,[0],[0]
"As can be observed, the responses in EED have a larger number of distinct tokens, token-pairs and token-triplets than HRED, and hence, are arguably more diverse.",5.1.2 Embedding Metrics,[0],[0]
"Table 7 presents the responses generated by HRED, VHRED and the proposed EED for a few selected contexts along with the corresponding similar exemplar responses.",5.2 Qualitative Evaluation,[0],[0]
"As can be observed from the table, the responses generated by EED tend to be more specific to the input context as compared to the responses of HRED and VHRED.",5.2 Qualitative Evaluation,[0],[0]
"For example, in conversations 1 and 2 we find that both HRED and VHRED generate simple generic responses whereas EED generates responses with additional information such as the type of disk partition used or a command not working.",5.2 Qualitative Evaluation,[0],[0]
This is also confirmed by the quantitative results obtained using activity and entity metrics in the previous section.,5.2 Qualitative Evaluation,[0],[0]
We further observe that the exemplar responses contain informative words that are utilized by the EED model for generating the responses as highlighted in Table 7.,5.2 Qualitative Evaluation,[0],[0]
"In this work, we propose a deep learning method, Exemplar Encoder Decoder (EED), that given a conversation context uses similar contexts and corresponding responses from training data for generating a response.",6 Conclusions,[0],[0]
We show that by utilizing this information the system is able to outperform state of the art generative models on publicly available Ubuntu dataset.,6 Conclusions,[0],[0]
"We further show improvements achieved by the proposed method on a large collection of technical support conversations.
",6 Conclusions,[0],[0]
"While in this work, we apply the exemplar encoder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation.",6 Conclusions,[0],[0]
"In our future work we plan to extend
the proposed method to these other applications.",6 Conclusions,[0],[0]
We are grateful to the anonymous reviewers for their comments that helped in improving the paper.,Acknowledgements,[0],[0]
"In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses.",abstractText,[0],[0]
Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model.,abstractText,[0],[0]
The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response.,abstractText,[0],[0]
The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context.,abstractText,[0],[0]
We present detailed experiments on two large data sets and find that our method outperforms state of the art sequence to sequence generative models on several recently proposed evaluation metrics.,abstractText,[0],[0]
We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.,abstractText,[0],[0]
Exemplar Encoder-Decoder for Neural Conversation Generation,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive.",1 Introduction,[0],[0]
"Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts.",1 Introduction,[0],[0]
"Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010).
",1 Introduction,[0],[0]
"However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010).",1 Introduction,[0],[0]
"A large part of NLP problems, however, are structured prediction tasks.",1 Introduction,[0],[0]
"Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations.",1 Introduction,[0],[0]
"Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder.
",1 Introduction,[0],[0]
"Only a few recent studies have investigated crowdsourcing sequential tasks; specifically, named entity recognition (Finin et al., 2010; Rodrigues et al., 2013).",1 Introduction,[0],[0]
Results for this are good.,1 Introduction,[0],[0]
"However, named entities typically use only few labels (LOC, ORG, and PER), and the data contains mostly non-entities, so the complexity is manageable.",1 Introduction,[0],[0]
"The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1
In this paper, we investigate how well lay annotators can produce POS labels for Twitter data.",1 Introduction,[0],[0]
"In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side).",1 Introduction,[0],[0]
"Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary.",1 Introduction,[0],[0]
Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter.,1 Introduction,[0],[0]
"We use a minimum of instructions and require few qualifications.
",1 Introduction,[0],[0]
"Obviously, lay annotation is generally less reliable than professional annotation.",1 Introduction,[0],[0]
It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations.,1 Introduction,[0],[0]
"In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013).",1 Introduction,[0],[0]
"We also show how we can use Wiktionary, a crowdsourced lexicon, to filter crowdsourced annotations.",1 Introduction,[0],[0]
"We evaluate the annotations in several ways: (a) by testing their accuracy with respect to a gold standard, (b) by evaluating the performance of POS models trained on
1One of the reviewers alerted us to an unpublished masters thesis, which uses pre-annotation to reduce tagging to fewer multiple-choice questions.",1 Introduction,[0],[0]
"See Related Work section for details.
",1 Introduction,[0],[0]
"377
the annotations across several existing data sets, as well as (c) by applying our models in downstream tasks.",1 Introduction,[0],[0]
"We show that with minimal context and annotation effort, we can produce structured annotations of near-expert quality.",1 Introduction,[0],[0]
"We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012).",1 Introduction,[0],[0]
"Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks.",1 Introduction,[0],[0]
We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags.,2 Our Approach,[0],[0]
"We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations.",2 Our Approach,[0],[0]
See Figure 1 for an example.,2 Our Approach,[0],[0]
"If the correct label is not among the annotations, we are unable to recover the correct answer.",2 Our Approach,[0],[0]
This was the case for 1497 instances in our data (cf.,2 Our Approach,[0],[0]
the token “:” in the example).,2 Our Approach,[0],[0]
"We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the missing tokens.",2 Our Approach,[0],[0]
"Note that while we report agreement between the crowdsourced annotations and the crowdsourced annotations, our main evaluations are based on models learned from expert vs. crowdsourced annotations and downstream applications thereof (chunking and NER).",2 Our Approach,[0],[0]
We take care in evaluating our models across different data sets to avoid biasing our evaluations to particular annotations.,2 Our Approach,[0],[0]
"All the data sets used in our experiments are publicly available at http://lowlands.ku.dk/results/.
2http://www.ark.cs.cmu.edu/TweetNLP/ 3http://crowdflower.com",2 Our Approach,[0],[0]
"In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels.
",3 Crowdsourcing Sequential Annotation,[0],[0]
Annotators were given a bold-faced word with two words on either side and asked to select the most appropriate tag from a drop down menu.,3 Crowdsourcing Sequential Annotation,[0],[0]
"For each tag, we spell out the name of the syntactic category, and provide a few example words.",3 Crowdsourcing Sequential Annotation,[0],[0]
See Figure 2 for a screenshot of the interface.,3 Crowdsourcing Sequential Annotation,[0],[0]
"Annotators were also told that words can belong to several classes, depending on the context.",3 Crowdsourcing Sequential Annotation,[0],[0]
"No additional guidelines were given.
",3 Crowdsourcing Sequential Annotation,[0],[0]
Only trusted annotators (in Crowdflower: Bronze skills) that had answered correctly on 4 gold tokens (randomly chosen from a set of 20 gold tokens provided by the authors) were allowed to submit annotations.,3 Crowdsourcing Sequential Annotation,[0],[0]
"In total, 177 individual annotators supplied answers.",3 Crowdsourcing Sequential Annotation,[0],[0]
We paid annotators a reward of $0.05 for 10 tokens.,3 Crowdsourcing Sequential Annotation,[0],[0]
"The full data set contains 14,619 tokens.",3 Crowdsourcing Sequential Annotation,[0],[0]
Completion of the task took slightly less than 10 days.,3 Crowdsourcing Sequential Annotation,[0],[0]
Contributors were very satisfied with the task (4.5 on a scale from 1 to 5).,3 Crowdsourcing Sequential Annotation,[0],[0]
"In particular, they felt instructions were clear (4.4/5), and that the pay was reasonable (4.1/5).",3 Crowdsourcing Sequential Annotation,[0],[0]
"After collecting the annotations, we need to aggregate the annotations to derive a single answer for each token.",4 Label Aggregation,[0],[0]
"In the simplest scheme, we choose the majority label, i.e., the label picked by most annotators.",4 Label Aggregation,[0],[0]
"In case of ties, we select the final label at random.",4 Label Aggregation,[0],[0]
"Since this is a stochastic process, we average results over 100 runs.",4 Label Aggregation,[0],[0]
We refer to this as MAJORITY VOTING (MV).,4 Label Aggregation,[0],[0]
Note that in MV we trust all annotators to the same degree.,4 Label Aggregation,[0],[0]
"However, crowdsourcing attracts people with different mo-
tives, and not all of them are equally reliable— even the ones with Bronze level.",4 Label Aggregation,[0],[0]
"Ideally, we would like to factor this into our decision process.
",4 Label Aggregation,[0],[0]
"We use MACE4 (Hovy et al., 2013) as our second scheme to learn both the most likely answer and a competence estimate for each of the annotators.",4 Label Aggregation,[0],[0]
"MACE treats annotator competence and the correct answer as hidden variables and estimates their parameters via EM (Dempster et al., 1977).",4 Label Aggregation,[0],[0]
"We use MACE with default parameter settings to give us the weighted average for each annotated example.
",4 Label Aggregation,[0],[0]
"Finally, we also tried applying the joint learning scheme in Rodrigues et al. (2013), but their scheme requires that entire sequences are annotated by the same annotators, which we don’t have, and it expects BIO sequences, rather than POS tags.
",4 Label Aggregation,[0],[0]
"Dictionaries Decoding tasks profit from the use of dictionaries (Merialdo, 1994; Johnson, 2007; Ravi and Knight, 2009) by restricting the number of tags that need to be considered for each word, also known as type constraints (Täckström et al., 2013).",4 Label Aggregation,[0],[0]
"We follow Li et al. (2012) in including Wiktionary information as type constraints into our decoding: if a word is found in Wiktionary, we disregard all annotations that are not licensed by the dictionary entry.",4 Label Aggregation,[0],[0]
"If the word is not found in Wiktionary, or if none of its annotations is licensed by Wiktionary, we keep the original annotations.",4 Label Aggregation,[0],[0]
"Since we aggregate annotations independently (unlike Viterbi decoding), we basically use Wiktionary as a pre-filtering step, such that MV and MACE only operate on the reduced annotations.",4 Label Aggregation,[0],[0]
Each of the two aggregation schemes above produces a final label sequence ŷ for our training corpus.,5 Experiments,[0],[0]
"We evaluate the resulting annotated data in three ways.
1.",5 Experiments,[0],[0]
We compare ŷ to the available expert annotation on the training data.,5 Experiments,[0],[0]
"This tells us how similar lay annotation is to professional annotation.
2.",5 Experiments,[0],[0]
"Ultimately, we want to use structured annotations for supervised training, where annotation quality influences model performance on held-out test data.",5 Experiments,[0],[0]
"To test this, we train a CRF model (Lafferty et al., 2001) with simple orthographic features and word clusters (Owoputi et al., 2013)
4http://www.isi.edu/publications/ licensed-sw/mace/
on the annotated Twitter data described in Gimpel et al. (2011).",5 Experiments,[0],[0]
"Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al. (2011) used in Derczynski et al. (2013)), the test set from Foster et al. (2011), and the data set described in Hovy et al. (2014).
",5 Experiments,[0],[0]
We will make the preprocessed data sets available to the public to facilitate comparison.,5 Experiments,[0],[0]
"In addition to a supervised model trained on expert annotations, we compare our tagging accuracy with that of a weakly supervised system (Li et al., 2012) re-trained on 400,000 unlabeled tweets to adapt to Twitter, but using a crowdsourced lexicon, namely Wiktionary, to constrain inference.",5 Experiments,[0],[0]
"We use parameter settings from Li et al. (2012), as well as their Wikipedia dump, available from their project website.5
3.",5 Experiments,[0],[0]
"POS tagging is often the first step for further analysis, such as chunking, parsing, etc.",5 Experiments,[0],[0]
We test the downstream performance of the POS models from the previous step on chunking and NER.,5 Experiments,[0],[0]
"We use the models to annotate the training data portion of each task with POS tags, and use them as features in a chunking and NER model.",5 Experiments,[0],[0]
"For both tasks, we train a CRF model on the respective (POS-augmented) training set, and evaluate it on several held-out test sets.",5 Experiments,[0],[0]
"For chunking, we use the test sets from Foster et al. (2011) and Ritter et al. (2011) (with the splits from Derczynski et al. (2013)).",5 Experiments,[0],[0]
"For NER, we use data from Finin et al. (2010) and again Ritter et al. (2011).",5 Experiments,[0],[0]
"For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information.",5 Experiments,[0],[0]
"For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features6 with 2,4,8,16 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013).",5 Experiments,[0],[0]
We report macro-averages over all these data sets.,5 Experiments,[0],[0]
Agreement with expert annotators Table 1 shows the accuracy of each aggregation compared to the gold labels.,6 Results,[0],[0]
"The crowdsourced annotations
5https://code.google.com/p/ wikily-supervised-pos-tagger/
6http://www.ark.cs.cmu.edu/TweetNLP/
aggregated using MV agree with the expert annotations in 79.54% of the cases.",6 Results,[0],[0]
"If we pre-filter the data using Wiktionary, the agreement becomes 80.58%.",6 Results,[0],[0]
MACE leads to higher agreement with expert annotations under both conditions (79.89 and 80.75).,6 Results,[0],[0]
"The small difference indicates that annotators are consistent and largely reliable, thus confirming the Bronze-level qualification we required.",6 Results,[0],[0]
"Both schemes cannot recover the correct answer for the 1497 cases where none of the crowdsourced labels matched the gold label, i.e. y /∈ Zi.",6 Results,[0],[0]
"The best possible result either of them could achieve (the oracle) would be matching all but the missing labels, an agreement of 89.63%.
",6 Results,[0],[0]
Most of the cases where the correct label was not among the annotations belong to a small set of confusions.,6 Results,[0],[0]
"The most frequent was mislabeling “:” and “. . .”, both mapped to X. Annotators mostly decided to label these tokens as punctuation (.).",6 Results,[0],[0]
"They also predominantly labeled your, my and this as PRON (for the former two), and a variety of labels for the latter, when the gold label is DET.
",6 Results,[0],[0]
"Effect on POS Tagging Accuracy Usually, we don’t want to match a gold standard, but we rather want to create new annotated training data.",6 Results,[0],[0]
"Crowdsourcing matches our gold standard to about 80%, but the question remains how useful this data is when training models on it.",6 Results,[0],[0]
"After all, inter-annotator agreement among professional an-
notators on this task is only around 90% (Gimpel et al., 2011; Hovy et al., 2014).",6 Results,[0],[0]
"In order to evaluate how much each aggregation scheme influences tagging performance of the resulting model, we train separate models on each scheme’s annotations and test on the same four data sets.",6 Results,[0],[0]
Table 2 shows the results.,6 Results,[0],[0]
Note that the differences between the four schemes are insignificant.,6 Results,[0],[0]
"More importantly, however, POS tagging accuracy using crowdsourced annotations are on average only 2.6% worse than gold using professional annotations.",6 Results,[0],[0]
"On the other hand, performance is much better than the weakly supervised approach by Li et al. (2012), which only relies on a crowdsourced POS lexicon.
",6 Results,[0],[0]
Downstream Performance Table 3 shows the accuracy when using the POS models trained in the previous evaluation step.,6 Results,[0],[0]
Note that we present the average over the two data sets used for each task.,6 Results,[0],[0]
Note also how the Wiktionary constraints lead to improvements in downstream performance.,6 Results,[0],[0]
"In chunking, we see that using the crowdsourced annotations leads to worse performance than using the professional annotations.",6 Results,[0],[0]
"For NER, however, we find that some of the POS taggers trained on aggregated data produce better NER performance than POS taggers trained on expert-annotated gold data.",6 Results,[0],[0]
"Since the only difference between models are the respective POS features, the results suggest that at least for some tasks, POS taggers learned from crowdsourced annotations may be as good as those learned from expert annotations.",6 Results,[0],[0]
"There is considerable work in the literature on modeling answer correctness and annotator competence as latent variables (Dawid and Skene,
1979; Smyth et al., 1995; Carpenter, 2008; Whitehill et al., 2009; Welinder et al., 2010; Yan et al., 2010; Raykar and Yu, 2012).",7 Related Work,[0],[0]
Rodrigues et al. (2013) recently presented a sequential model for this.,7 Related Work,[0],[0]
They estimate annotator competence as latent variables in a CRF model using EM.,7 Related Work,[0],[0]
"They evaluate their approach on synthetic and NER data annotated on Mechanical Turk, showing improvements over the MV baselines and the multi-label model by Dredze et al. (2009).",7 Related Work,[0],[0]
"The latter do not model annotator reliability but rather model label priors by integrating them into the CRF objective, and re-estimating them during learning.",7 Related Work,[0],[0]
"Both require annotators to supply a full sentence, while we use minimal context, which requires less annotator commitment and makes the task more flexible.",7 Related Work,[0],[0]
"Unfortunately, we could not run those models on our data due to label incompatibility and the fact that we typically do not have complete sequences annotated by the same annotators.
",7 Related Work,[0],[0]
Mainzer (2011) actually presents an earlier paper on crowdsourcing POS tagging.,7 Related Work,[0],[0]
"However, it differs from our approach in several ways.",7 Related Work,[0],[0]
It uses the Penn Treebank tag set to annotate Wikipedia data (which is much more canonical than Twitter) via a Java applet.,7 Related Work,[0],[0]
"The applet automatically labels certain categories, and only presents the users with a series of multiple choice questions for the remainder.",7 Related Work,[0],[0]
"This is highly effective, as it eliminates some sources of possible disagreement.",7 Related Work,[0],[0]
"In contrast, we do not pre-label any tokens, but always present the annotators with all labels.",7 Related Work,[0],[0]
We use crowdsourcing to collect POS annotations with minimal context (five-word windows).,8 Conclusion,[0],[0]
"While the performance of POS models learned from this data is still slightly below that of models trained on expert annotations, models learned from aggregations approach oracle performance for POS tagging.",8 Conclusion,[0],[0]
"In general, we find that the use of a dictionary tends to make aggregations more useful, irrespective of aggregation method.",8 Conclusion,[0],[0]
"For some downstream tasks, models using the aggregated POS tags perform even better than models using expert-annotated tags.",8 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for valuable comments and feedback.,Acknowledgments,[0],[0]
"This research is funded by the ERC Starting Grant LOW-
LANDS",Acknowledgments,[0],[0]
No. 313695.,Acknowledgments,[0],[0]
Crowdsourcing lets us collect multiple annotations for an item from several annotators.,abstractText,[0],[0]
"Typically, these are annotations for non-sequential classification tasks.",abstractText,[0],[0]
"While there has been some work on crowdsourcing named entity annotations, researchers have largely assumed that syntactic tasks such as part-of-speech (POS) tagging cannot be crowdsourced.",abstractText,[0],[0]
This paper shows that workers can actually annotate sequential data almost as well as experts.,abstractText,[0],[0]
"Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks.",abstractText,[0],[0]
Experiments with crowdsourced re-annotation of a POS tagging data set,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3275–3284 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3275",text,[0],[0]
Character-level features are an essential part of many Natural Language Processing (NLP) tasks.,1 Introduction,[0],[0]
"These features are for instance used for language modeling (Kim et al., 2016), part-of-speech tagging (Plank et al., 2016) and machine translation (Luong and Manning, 2016).",1 Introduction,[0],[0]
"They are especially useful in the context of part-of-speech and morphological tagging, where for example the suffix -s can easily differentiate plural words from singular words in English or Spanish.
",1 Introduction,[0],[0]
The use of character-level features is not new.,1 Introduction,[0],[0]
"Rule-based taggers were amongst the earliest systems that used character-level features/rules for grammatical tagging (Klein and Simmons, 1963).",1 Introduction,[0],[0]
"Other approaches rely on fixed lists of affixes (Ratnaparkhi, 1996; Toutanova et al., 2003).",1 Introduction,[0],[0]
"Next, these features are used by a tagging model, such
as a rule-based model or statistical model.",1 Introduction,[0],[0]
"Rulebased taggers are transparent models that allow us to easily trace back why the tagger made a certain decision (e.g., Brill (1994)).",1 Introduction,[0],[0]
"Similarly, statistical models are merely a weighted sum of features.
",1 Introduction,[0],[0]
"For example, Brill (1994)’s transformationbased error-driven tagger uses a set of templates to derive rules by fixing errors.",1 Introduction,[0],[0]
"The following rule template:
""Change the most-likely tag X to Y if the last (1,2,3,4) characters of the word are x"",
resulted in the rule:
""Change the tag common noun to plural common noun if the word has suffix -s"".
",1 Introduction,[0],[0]
"Subsequently, whenever the tagger makes a tagging mistake, it is easy to trace back why this happened.",1 Introduction,[0],[0]
"Following the above rule, the word mistress will mistakingly be tagged as a plural common noun while it actually is a common noun1.
",1 Introduction,[0],[0]
"This is in stark contrast with the most recent generation of part-of-speech and morphological taggers which mainly rely on neural networks.
1In Brill (1994), an additional rule encodes an exception to this rule to correctly tag the word mistress.
",1 Introduction,[0],[0]
"Words are split into individual characters and are in general either aggregated using a Bidirectional Long Short-Term Memory network (BiLSTM) (Plank et al., 2016) or Convolutional Neural Network (CNN) (dos Santos and Zadrozny, 2014).",1 Introduction,[0],[0]
"However, it is currently unknown which characterlevel patterns these neural network models learn and whether these patterns coincide with our linguistic knowledge.",1 Introduction,[0],[0]
"Moreover, different neural network architectures are currently only compared quantitatively and lack a qualitative analysis.
",1 Introduction,[0],[0]
"In this paper, we investigate which character patterns neural networks learn and to what extent those patterns comprise any known linguistic rules.",1 Introduction,[0],[0]
"We do this for three morphologically different languages: Finnish, Spanish and Swedish.",1 Introduction,[0],[0]
A Spanish example is shown in Figure 1.,1 Introduction,[0],[0]
"By visualizing the contributions of each character, we observe that the model indeed uses the suffix -s to correctly predict that the word is plural.
",1 Introduction,[0],[0]
"Our main contributions are as follows:
• We show how word-level tagging decisions can be traced back to specific sets of characters and interactions between them.
",1 Introduction,[0],[0]
"• We extend the contextual decomposition method (Murdoch et al., 2018) to CNNs.
",1 Introduction,[0],[0]
"• We quantitatively compare CNN and BiLSTM models in the context of morphological tagging by performing an evaluation on three manually segmented and morphologically annotated corpora.
",1 Introduction,[0],[0]
"• We found out that the studied neural models are able to implicitly discover character patterns that coincide with the same rules linguists use to indicate the morphological function of subword segments.
",1 Introduction,[0],[0]
Our implementation is available online2.,1 Introduction,[0],[0]
"Neural network-based taggers currently outperform statistical taggers in morphological tagging (Heigold et al., 2017) and part-of-speech tagging (Plank et al., 2016) for a wide variety of languages.",2 Related Work,[0],[0]
Character-level features form a crucial part of many of these systems.,2 Related Work,[0],[0]
"Generally, two neural network architectures are considered for aggregating the individual characters: a BiLSTM (Ling
2https://github.com/FredericGodin/ ContextualDecomposition-NLP
et al., 2015; Plank et al., 2016) or a CNN (dos Santos and Zadrozny, 2014; Bjerva et al., 2016; Heigold et al., 2017).",2 Related Work,[0],[0]
"These architectures outperform similar models that use manually defined features (Ling et al., 2015; dos Santos and Zadrozny, 2014).",2 Related Work,[0],[0]
"However, it is still unclear which useful character-level features they have learned.",2 Related Work,[0],[0]
Architectures are compared quantitatively but lack insight into learned patterns.,2 Related Work,[0],[0]
"Moreover, Vania and Lopez (2017) showed in the context of language modeling that training a BiLSTM on ground truth morphological features still yields better results than eight other character-based neural network architectures.",2 Related Work,[0],[0]
"Hence, this raises the question which patterns neural networks learn and whether these patterns coincide with manually-defined linguistic rules.
",2 Related Work,[0],[0]
"While a number of interpretation techniques have been proposed for images (Springenberg et al., 2014; Selvaraju et al., 2017; Shrikumar et al., 2017), these are generally not applicable in the context of NLP where LSTMs are mainly used.",2 Related Work,[0],[0]
"Moreover, gradient-based techniques are not trustworthy when strongly saturating activation functions such as tanh and sigmoid are used (e.g., Li et al. (2016a)).",2 Related Work,[0],[0]
"Hence, current interpretations in NLP are limited to visualizing the magnitude of the LSTM hidden states of each word (Linzen et al., 2016; Radford et al., 2017; Strobelt et al., 2018), removing words (Li et al., 2016b; Kádár et al., 2017) or changing words (Linzen et al., 2016) and measuring the impact, or training surrogate tasks (Adi et al., 2017; Chrupała et al., 2017; Belinkov et al., 2017).",2 Related Work,[0],[0]
These techniques only provide limited local interpretations and do not model fine-grained interactions of groups of inputs or intermediate representations.,2 Related Work,[0],[0]
"In contrast, Murdoch et al. (2018) recently introduced an LSTM interpretation technique called Contextual Decomposition (CD), providing a solution to the aforementioned issues.",2 Related Work,[0],[0]
"We will build upon this interpretation technique and introduce an extension for CNNs, making it possible to compare different neural network architectures within a single interpretation framework.",2 Related Work,[0],[0]
"For visualizing the contributions of character sets, we use the recently introduced Contextual Decomposition (CD) framework, as originally developed for LSTMs (Murdoch et al., 2018), and extend it to
CNNs.",3 Method,[0],[0]
"First, we introduce the concept of CD, followed by the extension for CNNs.",3 Method,[0],[0]
"For details on CD for LSTMs, we refer the reader to the aforementioned paper.",3 Method,[0],[0]
"Finally, we explain how the CD of the final classification layer is done.",3 Method,[0],[0]
"The idea behind CD is that, in the context of character-level decomposition, we can decompose the output value of the network for a certain class into two distinct groups of contributions: (1) contributions originating from a specific character or set of characters within a word and (2) contributions originating from all the other characters within the same word.
",3.1 Contextual decomposition,[0],[0]
"More generally, we can decompose every output value z of every neural network component into a relevant contribution β and an irrelevant contribution γ:
z = β + γ (1)",3.1 Contextual decomposition,[0],[0]
"A CNN typically consist of three components: the convolution itself, an activation function and an optional max-pooling operation.",3.2 Decomposing CNN layers,[0],[0]
"We will discuss each component in the next paragraphs.
",3.2 Decomposing CNN layers,[0],[0]
"Decomposing the convolution Given a sequence of character embeddings x1, ...,xT ∈ Rd1 of length T , we can calculate the convolution of size n of a single filter over the sequence x1:T by applying the following equation to each n-length subsequence {xt+i, i = 0, .., n",3.2 Decomposing CNN layers,[0],[0]
"− 1}, denoted as xt:t+n−1:
zt = n−1∑",3.2 Decomposing CNN layers,[0],[0]
"i=0 Wi · xt+i + b, (2)
with zt ∈ R and where W ∈ Rd1×n and b ∈ R are the weight matrix and bias of the convolutional filter.",3.2 Decomposing CNN layers,[0],[0]
"Wi denotes the i-th column of the weight matrix W .
",3.2 Decomposing CNN layers,[0],[0]
"When we want to calculate the contribution of a subset of characters, where S is the set of corresponding character position indexes and S ⊆ {1, ..., T}, we should decompose the output of the filter zt into three parts:
zt = βt + γt + b. (3)
",3.2 Decomposing CNN layers,[0],[0]
"That is, the relevant contribution βt originating from the selected subset of characters with indexes S, the irrelevant contribution γt originating
from the remaining characters in the sequence, and a bias which is deemed neutral (Murdoch et al., 2018).
",3.2 Decomposing CNN layers,[0],[0]
"This can be achieved by decomposing the convolution itself as follows:
βt = n−1∑ i=0",3.2 Decomposing CNN layers,[0],[0]
"Wi · xt+i (t+ i) ∈ S, (4)
",3.2 Decomposing CNN layers,[0],[0]
γt = n−1∑ i=0 Wi · xt+i (t+ i) /∈,3.2 Decomposing CNN layers,[0],[0]
"S, (5)
Linearizing the activation function After applying a linear transformation to the input, a nonlinearity is typically applied.",3.2 Decomposing CNN layers,[0],[0]
"In CNNs, the ReLU activation function is often used.
",3.2 Decomposing CNN layers,[0],[0]
"In Murdoch et al. (2018), a linearization method for the non-linear activation function f is proposed, based on the differences of partial sums of all N components yi involved in the preactivation sum zt.",3.2 Decomposing CNN layers,[0],[0]
"In other words, we want to split fReLU (zt) =",3.2 Decomposing CNN layers,[0],[0]
"fReLU ( ∑N i=1 yi) into a sum of individual linearized contributions LfReLU (yi), namely fReLU ( ∑N i=1 yi)",3.2 Decomposing CNN layers,[0],[0]
= ∑N i=1 LfReLU (yi).,3.2 Decomposing CNN layers,[0],[0]
"To that end, we compute LfReLU (yk), the linearized contribution of yk as the average difference of partial sums over all possible permutations π1, ..., πMN of all N components yi involved:
Lf (yk) =
1
MN MN∑ i=1",3.2 Decomposing CNN layers,[0],[0]
[f( π−1i (k)∑ l=1 yπi(l))− f( π−1i (k)−1∑ l=1 yπi(l)),3.2 Decomposing CNN layers,[0],[0]
"]
(6)
Consequently, we can decompose the output ct after the activation function as follows:
ct =",3.2 Decomposing CNN layers,[0],[0]
"fReLU (zt) (7)
=",3.2 Decomposing CNN layers,[0],[0]
"fReLU (βz,t + γz,t + b) (8)
=LReLU (βz,t)
+",3.2 Decomposing CNN layers,[0],[0]
"[LReLU (γz,t) + LReLU (b)] (9)
=βc,t + γc,t (10)
",3.2 Decomposing CNN layers,[0],[0]
"Following Murdoch et al. (2018), βc,t contains the contributions that can be directly attributed to the specific set of input indexes S. Hence, the bias b is part of γc,t. Note that, while the decomposition in Eq.",3.2 Decomposing CNN layers,[0],[0]
"(10) is exact in terms of the total sum, the individual attribution to relevant (βc,t) and irrelevant (γc,t) is an approximation, due to the linearization.
",3.2 Decomposing CNN layers,[0],[0]
"Max-pooling over time When applying a fixedsize convolution over a variable-length sequence, the output is again of variable size.",3.2 Decomposing CNN layers,[0],[0]
"Hence, a maxpooling operation is executed over the time dimension, resulting in a fixed-size representation that is independent of the sequence length:
c = max t (ct).",3.2 Decomposing CNN layers,[0],[0]
"(11)
Instead of applying a max operation over the βc,t and γc,t contributions separately, we first determine the position t of the highest ct value and propagate the corresponding βc,t and γc,t values.",3.2 Decomposing CNN layers,[0],[0]
"The final layer is a classification layer, which is the same for a CNN- or LSTM-based architecture.",3.3 Calculating the final contribution scores,[0],[0]
"The probability pj of predicting class j is defined as follows:
pj = eWj ·x+bj∑C i=1",3.3 Calculating the final contribution scores,[0],[0]
"e Wi·x+bi , (12)
in which W ∈ Rd2×C is a weight matrix and Wi the i-th column, x ∈ Rd2 the input, b ∈ Rd2 the bias vector and bi the i-th element, d2 the input vector size and C the total number of classes.
",3.3 Calculating the final contribution scores,[0],[0]
The input x is either the output c of a CNN or h of a LSTM.,3.3 Calculating the final contribution scores,[0],[0]
"Consequently, we can decompose x into β and γ contributions.",3.3 Calculating the final contribution scores,[0],[0]
"In practice, we only consider the preactivation and decompose it as follows:
Wj · x+ bj =Wj · β +Wj · γ + bj .",3.3 Calculating the final contribution scores,[0],[0]
"(13)
Finally, the contribution of a set of characters with indexes S to the final score of class j is equal to Wj · β.",3.3 Calculating the final contribution scores,[0],[0]
The latter score is used throughout the paper for visualizing contributions of sets of characters.,3.3 Calculating the final contribution scores,[0],[0]
"We execute experiments on morphological tagging in three different languages: Finnish, Spanish and Swedish.",4 Experimental Setup,[0],[0]
"We describe the dataset in Section 4.1, whereas model and training details can be found in Section 4.2.",4 Experimental Setup,[0],[0]
"For our experiments, we use the Universal Dependencies 1.4 (UD) dataset (Nivre et al., 2016), which contains morphological features for a large number of sentences.",4.1 Dataset,[0],[0]
"Additionally, we acquired
manually-annotated character-level morphological segmentations and labels for a subset of the test set for three morphological different languages: Finnish, Spanish and Swedish.",4.1 Dataset,[0],[0]
"3
For each language, Silfverberg and Hulden (2017) selected the first non-unique 300 words from the UD test set and manually segmented each word according to the associated lemma and morphological features in the dataset.",4.1 Dataset,[0],[0]
"Whenever possible, they assigned each feature to a specific subset of characters.",4.1 Dataset,[0],[0]
"For example, the Spanish word ""económicas"" is segmented as follows:
• económic : lemma=económico
• a : gender=feminine
• s : number=plural
For our experiments, we are only interested in word/feature pairs for which a feature can be assigned to a specific subset of characters.",4.1 Dataset,[0],[0]
"Hence, we filter the test set on those specific word/feature pairs.",4.1 Dataset,[0],[0]
"In the above example, we have two word/feature pairs.",4.1 Dataset,[0],[0]
"This resulted in 278, 340 and 137 word/feature pairs for Finnish, Spanish and Swedish, respectively.",4.1 Dataset,[0],[0]
"Using the same procedure, we selected relevant feature classes, resulting in 12, 6 and 9 feature classes for Finnish, Spanish and Swedish, respectively.4 For each class, when a feature was not available, we introduced an additional Not Applicable (NA) label.
",4.1 Dataset,[0],[0]
We always train and validate on the full UD dataset for which we have filtered out all duplicate words.,4.1 Dataset,[0],[0]
"After that, we perform our analysis on either the UD test set or the annotated subset of manually segmented and annotated words.",4.1 Dataset,[0],[0]
"An overview can be found in Table 1.
",4.1 Dataset,[0],[0]
"3Available online: http://github.com/mpsilfve/ud-segmen ter/commit/5959214d494cbc13e53e1b26650813ff950d2ee3
4Full list available as supplementary material",4.1 Dataset,[0],[0]
"We experiment with both a CNN and BiLSTM architecture for character-level modeling of words.
",4.2 Model,[0],[0]
"At the input, we split every word into characters and add a start-of-word (ˆ) and an end-of-word ($) character.",4.2 Model,[0],[0]
"With every character, we associate a character embedding of size 50.
",4.2 Model,[0],[0]
"Our CNN architecture is inspired by Kim et al. (2016) and consists of a set of filters of varying width, followed by a ReLU activation function and a max-over-time pooling operation.",4.2 Model,[0],[0]
"We adopt their small-CNN parameter choices and have 25, 50, 75, 100, 125 and 150 convolutional filters of size 1, 2, 3, 4, 5 and 6, respectively.",4.2 Model,[0],[0]
"We do not add an additional highway layer.
",4.2 Model,[0],[0]
"For the character-level BiLSTM architecture, we follow the variant used in Plank et al. (2016).",4.2 Model,[0],[0]
"That is, we simply run a BiLSTM over all the characters and concatenate the final forward and backward hidden state.",4.2 Model,[0],[0]
"To obtain a similar number of parameters as the CNN model, we set the hidden state size to 100 units for each LSTM.
",4.2 Model,[0],[0]
"Finally, the word-level representation generated by either the CNN or BiLSTM architecture is classified by a multinomial logistic regression layer.",4.2 Model,[0],[0]
Each morphological class type has a different layer.,4.2 Model,[0],[0]
"We do not take into account context to rule out any influence originating from somewhere other than the characters of the word itself.
",4.2 Model,[0],[0]
"Training details For morphological tagging, we train a single model for all classes at once.",4.2 Model,[0],[0]
We minimize the joint loss by summing the cross-entropy losses of each class.,4.2 Model,[0],[0]
"We orthogonally initialize all weight matrices, except for the embeddings, which are uniformly initialized ([- 0.01;0.01]).",4.2 Model,[0],[0]
"All models are trained using Adam (Kingma and Ba, 2015) with minibatches of size 20 and learning rate 0.001.",4.2 Model,[0],[0]
No specific regularization is used.,4.2 Model,[0],[0]
We select our final model based on early stopping on the validation set.,4.2 Model,[0],[0]
"First, we verify that the CD algorithm works correctly by executing a controlled experiment with a synthetic token.",5 Experiments,[0],[0]
"Next, we quantitatively and qualitatively evaluate on the full test set.",5 Experiments,[0],[0]
"To verify that the contextual decomposition of CNNs works correctly, we devise an experiment
in which we add a synthetic token to a word of a certain class, testing whether this token gets a high attribution score with respect to that specific class.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Given a word w and a corresponding binary label t, we add a synthetic character c to the beginning of word w with probability psyn if that word belongs to the class t = 1 and with probability 1 − psyn if that word belongs to the class t = 0.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Consequently, if psyn = 1, the model should predict the label with a 100% accuracy, thus attributing this to the synthetic character c. When psyn = 0.5, the synthetic character does not provide any additional information about the label t, and c should thus have a small contribution.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
Experimental setup We train a CNN model on the Spanish dataset and only use words having the morphological label number.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"This label has two classes plur and sing, and assign those classes to the binary labels zero and one, respectively.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Furthermore, we add a synthetic character to each word with probability psyn, varying psyn from 1 to 0.5 with steps of 0.1.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
We selected 112 unique word/feature pairs from our test set with label sing or plur.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"While plurality is marked by the suffix s, a variety of suffixes are used for the singular form.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Therefore, we focus on the latter class (t = 1).",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"The corresponding suffix is called the Ground Truth (GT) character.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"To measure the impact of psyn, we add a synthetic character to each word of the class t = 1 and
calculate the contribution of each character by using the CD algorithm.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
We run the experiment five times with a different random seed and report the average correct attribution.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"The attribution is correct if the contribution of the synthetic/GT character is the highest contribution of all character contributions.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
Results The results of our evaluation are depicted in Figure 2.,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"When psyn = 1, all words of the class t = 1 contain the synthetic character, and consequently, the accuracy for predicting t = 1 is indeed 100%.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Moreover, the correct prediction is effectively attributed to the synthetic character (‘syn.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
char attr.’,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"in Figure 2 at 100%), with the GT character being deemed irrelevant.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"When the synthetic character probability psyn is lowered, the synthetic character is less trustworthy and the GT character becomes more important (increasing ‘GT char attr.’",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
in Figure 2).,5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Finally, when psyn = 0.5, the synthetic character is equally plausible in both classes.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Hence, the contribution of the synthetic character becomes irrelevant and the model attributes the prediction to other characters.
",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"Consequently, we can conclude that whenever there is a clear character-level pattern, the model learns the pattern and the CD algorithm is able to accurately attribute it to the correct character.",5.1 Validation of contextual decomposition for convolutional neural networks,[0],[0]
"In this section, we measure and analyze (1) which characters contribute most to the final prediction of a certain label and (2) whether those contributions coincide with our linguistic knowledge about a language.",5.2 Evaluation of character-level attribution,[0],[0]
"To that end, we train a model to predict morphological features, given a particular word.",5.2 Evaluation of character-level attribution,[0],[0]
"The model does not have prior word seg-
mentation information and thus needs to discover useful character patterns by itself.",5.2 Evaluation of character-level attribution,[0],[0]
"After training, we calculate the attribution scores of each character pattern within a word with respect to the correct feature class using CD, and evaluate whether this coincides with the ground truth attribution.
",5.2 Evaluation of character-level attribution,[0],[0]
"Model We train CNN and BiLSTM models on Finnish, Spanish and Swedish.",5.2 Evaluation of character-level attribution,[0],[0]
"The average accuracies on the full test set are reported in Table 2.5 As a reference for the trained models’ ability to predict morphological feature classes, we provide a naive baseline, constructed from the majority vote for each feature type.
",5.2 Evaluation of character-level attribution,[0],[0]
"Overall, our neural models yield substantially higher average accuracies than the baseline and perform very similar.",5.2 Evaluation of character-level attribution,[0],[0]
"Consequently, both the CNN and LSTM models learned useful character patterns for predicting the correct morphological feature classes.",5.2 Evaluation of character-level attribution,[0],[0]
"Hence, this raises the question whether these patterns coincide with our linguistic knowledge.
",5.2 Evaluation of character-level attribution,[0],[0]
"Evaluation For each annotated word/feature pair, we measure if the ground truth character se-
5The results of the individual classes are provided as supplementary material.
",5.2 Evaluation of character-level attribution,[0],[0]
ˆ,5.2 Evaluation of character-level attribution,[0],[0]
o,5.2 Evaluation of character-level attribution,[0],[0]
l,5.2 Evaluation of character-level attribution,[0],[0]
"i v a t $ BiLSTM
CNN
-3.2 0",5.2 Evaluation of character-level attribution,[0],[0]
"3.2
(a) Example of Finnish.",5.2 Evaluation of character-level attribution,[0],[0]
"Word (verb): olivat (were), target class: Tense=Past
ˆ g r a t u",5.2 Evaluation of character-level attribution,[0],[0]
"i t a $ BiLSTM
CNN
-2.6 0",5.2 Evaluation of character-level attribution,[0],[0]
"2.6
(b) Example of Spanish.",5.2 Evaluation of character-level attribution,[0],[0]
"Word (adjective): gratuita (free), target: Gender=Fem.
quence corresponds to the set or sequence of characters with the same length within the considered word that has the highest contribution for predicting the correct label for that word.
",5.2 Evaluation of character-level attribution,[0],[0]
"In the first setup, we only compare with character sequences having a consecutive set of characters (denoted cons).",5.2 Evaluation of character-level attribution,[0],[0]
"In the second setup, we compare with any set of characters (denoted all).",5.2 Evaluation of character-level attribution,[0],[0]
"We rank the contributions of each character set and report top one, two, and three scores.",5.2 Evaluation of character-level attribution,[0],[0]
"Because startof-word and end-of-word characters are not annotated in the dataset, we do not consider them part of the candidate character sets.
",5.2 Evaluation of character-level attribution,[0],[0]
"Results The aggregated results for all classes and character sequence lengths are shown in Fig-
ure 3.",5.2 Evaluation of character-level attribution,[0],[0]
"In general, we observe that for almost all models and setups, the contextual decomposition attribution coincides with the manually-defined segmentations for at least half of the word/feature pairs.",5.2 Evaluation of character-level attribution,[0],[0]
"When we only consider the top two consecutive sequences (marked as cons), accuracies range from 76% up to 93% for all three languages.",5.2 Evaluation of character-level attribution,[0],[0]
"For Spanish and Swedish, the top two accuracies for character sets (marked as all) are still above 67%, despite the large space of possible character sets, whereas all ground truth patterns are consecutive sequences.",5.2 Evaluation of character-level attribution,[0],[0]
"While the accuracy for Finnish is lower, the top two accuracy is still above 50%.
",5.2 Evaluation of character-level attribution,[0],[0]
"Examples for Finnish, Spanish and Swedish are shown in Figure 4.",5.2 Evaluation of character-level attribution,[0],[0]
"For Finnish, the character with the highest contribution i coincides with the ground truth character for the CNN model.",5.2 Evaluation of character-level attribution,[0],[0]
"This is not the case for the BiLSTM model which focuses on the character v, even though the correct label is predicted.",5.2 Evaluation of character-level attribution,[0],[0]
"For Spanish, both models strongly focus on the ground truth character a for predicting the feminine gender.",5.2 Evaluation of character-level attribution,[0],[0]
"For Swedish, the ground truth character sequence is the suffix or which denotes plurality.",5.2 Evaluation of character-level attribution,[0],[0]
"Given that or consists of two characters, all contributions of character sets of two characters are visualized.",5.2 Evaluation of character-level attribution,[0],[0]
"As can be seen, the most important set of two characters is {o,r} for the CNN and {k,r} for the BiLSTM model.",5.2 Evaluation of character-level attribution,[0],[0]
"However, {o,r} is the second most important character set for the BiLSTM model.",5.2 Evaluation of character-level attribution,[0],[0]
"Consequently, the BiLSTM model deemed the interaction between a root and suffix character more important than between two suffix characters.",5.2 Evaluation of character-level attribution,[0],[0]
"In the previous section, we showed that there is a strong relationship between the manually-defined morphological segmentation and the patterns a neural network learns.",5.3 Analysis of learned patterns,[0],[0]
"However, there is still an accuracy gap between the results obtained using consecutive sequences only and results obtained using all possible character sets.",5.3 Analysis of learned patterns,[0],[0]
"Hence, this leads to the question which patterns the neural network focuses on, other than the manually defined patterns we evaluated before.",5.3 Analysis of learned patterns,[0],[0]
"To that end, for each of the three languages, we selected a morphological class of interest and evaluated for all words in the full UD test set that were assigned to that class what the most important character set of length one, two and three was.",5.3 Analysis of learned patterns,[0],[0]
"In other words, we evaluated for each word for which the class was cor-
rectly predicted, which character set had the highest positive contribution towards predicting that class.",5.3 Analysis of learned patterns,[0],[0]
"The results can be found in Table 3.
",5.3 Analysis of learned patterns,[0],[0]
"Finnish In Finnish, adding the suffix i to a verb, transforms it in the past tense.",5.3 Analysis of learned patterns,[0],[0]
"Sometimes the character s is added, resulting in the suffix si.",5.3 Analysis of learned patterns,[0],[0]
The latter is a frequently used bigram pattern by the CNN but less by the BiLSTM.,5.3 Analysis of learned patterns,[0],[0]
"The BiLSTM combines the suffix i with another suffix vat which denotes third person plural in the character pattern iv_t.
Spanish While there is no single clear-cut rule for the Spanish gender, in general the suffix a denotes the feminine gender in adjectives.",5.3 Analysis of learned patterns,[0],[0]
"However, there exist many nouns that are feminine but do not have the suffix a. Teschner and Russell (1984) identify d, and ión as typical endings of feminine nouns, which our models identified too as for example ad$ or ió/sió.
",5.3 Analysis of learned patterns,[0],[0]
"Swedish In Swedish, there exist four suffixes for creating a plural form: or, ar, (e)r and n. Both models identified the suffix or.",5.3 Analysis of learned patterns,[0],[0]
"However, similar to Finnish, multiple suffixes are merged.",5.3 Analysis of learned patterns,[0],[0]
"In Swedish, the suffix na only occurs together with one of the first three plural suffixes.",5.3 Analysis of learned patterns,[0],[0]
"Hence, both models correctly identified this pattern as an important pattern for predicting the class number=plural, rather than the linguistically-defined pattern.",5.3 Analysis of learned patterns,[0],[0]
"In the previous section, the pattern a$ showed to be the most important pattern in 34% of the correctly-predicted feminine Spanish words in our dataset.",5.4 Interactions of learned patterns,[0],[0]
"However, there exist many words that end with the character a that are not feminine.",5.4 Interactions of learned patterns,[0],[0]
For example the third person singular form of the verb gustar is gusta.,5.4 Interactions of learned patterns,[0],[0]
"Hence, this raises the question if the model will classify gusta wrongly as feminine or correctly as NA.",5.4 Interactions of learned patterns,[0],[0]
"As an illustration of the applicability of CD for morphological analysis, we will study this case in more detail.
",5.4 Interactions of learned patterns,[0],[0]
"From the full UD test set, we selected all words that end with the character a and that do not belong to the class gender=feminine.",5.4 Interactions of learned patterns,[0],[0]
"Using the Spanish CNN model, we predicted the gender class for each word and divided the words into two groups: predicted as feminine and predicted as not-feminine (_NA_ or masculine).",5.4 Interactions of learned patterns,[0],[0]
The resulted in 44 and 199 words.,5.4 Interactions of learned patterns,[0],[0]
"Next, for each word in both groups we calculated the most positively and negatively contributing character set out of all possible character sets of any length within the considered word, using the CD algorithm.",5.4 Interactions of learned patterns,[0],[0]
"We compared the contribution scores in both groups using a Kruskal-Wallis significance test.6 While no significant (p < 0.05) difference could be found between the positive contributions of both groups (p=1.000), a borderline significant difference could be found between the negative
6The full statistical analysis is provided as supplementary material.
contributions of words predicted as feminine and words predicted as not-feminine (p=0.070).
",5.4 Interactions of learned patterns,[0],[0]
"Consequently, the CNN model’s classification decision is based on finding enough negative evidence to counteract the positive evidence found in the pattern a$, which CD was able to uncover.
",5.4 Interactions of learned patterns,[0],[0]
A visualization of this interaction is shown in Figure 5 for the word gusta.,5.4 Interactions of learned patterns,[0],[0]
"While the positive evidence is the strongest for the class feminine, the model identifies the verb stem gust as negative evidence which ultimately leads to the correct final prediction NA.",5.4 Interactions of learned patterns,[0],[0]
"While neural network-based models are part of many NLP systems, little is understood on how they handle the input data.",6 Conclusion,[0],[0]
"We investigated how specific character sequences at the input of a neural network model contribute to word-level tagging decisions at the output, and if those contributions follow linguistically interpretable rules.
",6 Conclusion,[0],[0]
"First, we presented an analysis and visualization technique to decompose the output of CNN models into separate input contributions, based on the principles outlined by Murdoch et al. (2018) for LSTMs.",6 Conclusion,[0],[0]
This allowed us then to quantitatively and qualitatively compare the character-level patterns the CNNs and BiLSTMs learned for the task of morphological tagging.,6 Conclusion,[0],[0]
"We showed that these patterns generally coincide with the morphological segments as defined by linguists for three morphologically different languages, but that sometimes other linguistically plausible patterns are learned.",6 Conclusion,[0],[0]
"Finally, we showed that our CD algorithm for CNNs is able to explain why the model made a wrong or correct prediction.
",6 Conclusion,[0],[0]
"By visualizing the contributions of each input unit or combinations thereof, we believe that much can be learned on how a neural network handles
the input data, why it makes certain decisions, or even for debugging neural network models.",6 Conclusion,[0],[0]
The authors would like to thank the anonymous reviewers and members of IDLab for their valuable feedback.,Acknowledgments,[0],[0]
"FG would like to thank Kim Bettens for helping out with the statistical analysis.
",Acknowledgments,[0],[0]
"The research activities as described in this paper were funded by Ghent University, imec, Flanders Innovation & Entrepreneurship (VLAIO), the Fund for Scientific Research-Flanders (FWOFlanders), and the European Union.",Acknowledgments,[0],[0]
Character-level features are currently used in different neural network-based natural language processing algorithms.,abstractText,[0],[0]
"However, little is known about the character-level patterns those models learn.",abstractText,[0],[0]
"Moreover, models are often compared only quantitatively while a qualitative analysis is missing.",abstractText,[0],[0]
"In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations.",abstractText,[0],[0]
"To that end, we extend the contextual decomposition (Murdoch et al., 2018) technique to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks.",abstractText,[0],[0]
We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules.,abstractText,[0],[0]
Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?,title,[0],[0]
"It is now well known that modern convolutional neural networks (e.g. Krizhevsky et al. 2012, Simonyan & Zisserman 2015, He et al. 2016, Szegedy et al. 2016) can achieve remarkable performance on large-scale image databases, e.g. ImageNet (Deng et al. 2009) and Places 365 (Zhou et al. 2017), but it is really dissatisfying to see the vast amounts of data, computing time and power consumption that are necessary to train deep networks.",1. Introduction,[0],[0]
"Fortunately, such convolutional networks, once trained on a large database, can be refined to solve related but different visual tasks by means of transfer learning, using fine-tuning (Yosinski et al. 2014, Simonyan & Zisserman 2015).
",1. Introduction,[0],[0]
"Some form of knowledge is believed to be extracted by
1Sorbonne universités, Université de technologie de Compiègne, CNRS, Heudiasyc, UMR 7253, Compiègne, France.",1. Introduction,[0],[0]
"Correspondence to: Xuhong LI <xuhong.li@hds.utc.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
learning from the large-scale database of the source task and this knowledge is then transferred to the target task by initializing the network with the pre-trained parameters.,1. Introduction,[0],[0]
"However, we will show in the experimental section that some parameters may be driven far away from their initial values during fine-tuning.",1. Introduction,[0],[0]
"This leads to important losses of the initial knowledge that is assumed to be relevant for the targeted problem.
",1. Introduction,[0],[0]
"In order to help preserve the knowledge embedded in the initial network, we consider a series of other parameter regularization methods during fine-tuning.",1. Introduction,[0],[0]
"We argue that the standard L2 regularization, which drives the parameters towards the origin, is not adequate in the framework of transfer learning, where the initial values provide a more sensible reference point than the origin.",1. Introduction,[0],[0]
"This simple modification keeps the original control of overfitting, by constraining the effective search space around the initial solution, while encouraging committing to the acquired knowledge.",1. Introduction,[0],[0]
"We show that it has noticeable effects in inductive transfer learning scenarios.
",1. Introduction,[0],[0]
"This paper copes with the inconsistency that still prevails in transfer learning scenarios, where the model is initialized with some parameters, while the abuse of L2 regularization encourages departing from these initial values.",1. Introduction,[0],[0]
"We thus advocate for a coherent parameter regularization approach, where the pre-trained model is both used as the starting point of the optimization process and as the reference in the penalty that encodes an explicit inductive bias.",1. Introduction,[0],[0]
This type of penalty will be designated with SP to recall that they encourage similarity with the starting point of the fine-tuning process.,1. Introduction,[0],[0]
"We evaluate regularizers based on the L2, Lasso and Group-Lasso penalties, which can freeze some individual parameters, or groups of parameters, to the pre-trained parameters.",1. Introduction,[0],[0]
Fisher information is also taken into account when we test L2-SP and Group-Lasso-SP approaches.,1. Introduction,[0],[0]
Our experiments indicate that all tested parameter regularization methods using the pre-trained parameters as a reference get an edge over the standard L2 weight decay approach.,1. Introduction,[0],[0]
We eventually recommend using L2-SP as the standard baseline for solving transfer learning tasks and benchmarking new algorithms.,1. Introduction,[0],[0]
"In this section, we recall the approaches to inductive transfer learning in convolutional networks.",2. Related Work,[0],[0]
We focus on approaches that also encourage similarity (of features or parameters) on different models.,2. Related Work,[0],[0]
Our proposal departs either by the goal pursued or by the type of model used.,2. Related Work,[0],[0]
Regularization has been a means to build shrinkage estimators for decades.,2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Shrinking towards zero is the most common form of shrinkage, but shrinking towards adaptively chosen targets has been around for some time, starting with Stein shrinkage (see e.g. Lehmann & Casella 1998, chapter 5), where it can be related to empirical Bayes arguments.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"In transfer learning, it has been used in maximum entropy models (Chelba & Acero 2006) or SVM (Yang et al. 2007, Aytar & Zisserman 2011, Tommasi et al. 2014).",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"These approaches were shown to outperform standard L2 regularization with limited labeled data in the target task (Aytar & Zisserman 2011, Tommasi et al. 2014).
",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"These relatives differ from the application to deep networks in several respects, the more important one being that they consider a fixed representation, where transfer learning aims at producing similar classification parameters in that space, that is, similar classification rules.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"For deep networks, transfer usually aims at learning similar representations upon which classification parameters will be learned from scratch.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Hence, even though the techniques we discuss here are very similar regarding the analytical form of the regularizers, they operate on parameters having a very different role.",2.1. Shrinking Toward Chosen Parameters,[0],[0]
"Regarding transfer learning, we follow here the nomenclature of Pan & Yang (2010), who categorized several types of transfer learning according to domain and task settings during the transfer.",2.2. Transfer Learning for Deep Networks,[0],[0]
"A domain corresponds to the feature space and its distribution, whereas a task corresponds to the label space and its conditional distribution with respect to features.",2.2. Transfer Learning for Deep Networks,[0],[0]
"The initial learning problem is defined on the source domain and source task, whereas the new learning problem is defined on the target domain and the target task.
",2.2. Transfer Learning for Deep Networks,[0],[0]
"In the typology of Pan & Yang, we consider the inductive transfer learning setting, where the target domain is identical to the source domain, and the target task is different from the source task.",2.2. Transfer Learning for Deep Networks,[0],[0]
"We furthermore focus on the case where a vast amount of data was available for training on the source problem, and some limited amount of labeled data is available for solving the target problem.",2.2. Transfer Learning for Deep Networks,[0],[0]
"Under this setting, we aim at improving the performance on the target problem through parameter regularization methods that explicitly encourage the similarity of the solutions to the target and source prob-
lems.",2.2. Transfer Learning for Deep Networks,[0],[0]
"Note that, though we refer here to problems that were formalized or popularized after (Pan & Yang 2010), such as lifelong learning, Pan & Yang’s typology remains valid.",2.2. Transfer Learning for Deep Networks,[0],[0]
Donahue et al. (2014) repurposed features extracted from different layers of the pre-trained AlexNet of Krizhevsky et al. (2012) and plugged them into an SVM or a logistic regression classifier.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
This approach outperformed the state of the art of that time on the Caltech-101 database (Fei-Fei et al. 2006).,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Later, Yosinski et al. (2014) showed that finetuning the whole AlexNet resulted in better performance than using the network as a static feature extractor.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Finetuning pre-trained VGG (Simonyan & Zisserman 2015) on the image classification task of VOC-2012 (Everingham et al. 2010) and Caltech 256 (Griffin et al. 2007) achieved the best results of that time.
",2.2.1. REPRESENTATION TRANSFER,[0],[0]
Ge & Yu (2017) proposed a scheme for selecting a subset of images from the source problem that have similar local features to those in the target problem and then jointly finetuned a pre-trained convolutional network.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Besides image classification, many procedures for object detection (Girshick et al. 2014, Redmon et al. 2016, Ren et al. 2015) and image segmentation (Long et al. 2015a, Chen et al. 2017, Zhao et al. 2017) have been proposed relying on fine-tuning to improve over training from scratch.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"These approaches showed promising results in a challenging transfer learning setup, as going from classification to object detection or image segmentation requires rather heavy modifications of the architecture of the network.
",2.2.1. REPRESENTATION TRANSFER,[0],[0]
The success of transfer learning with convolutional networks relies on the generality of the learned representations that have been constructed from a large database like ImageNet.,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Yosinski et al. (2014) also quantified the transferability of these pieces of information in different layers, e.g. the first layers learn general features, the middle layers learn highlevel semantic features and the last layers learn the features that are very specific to a particular task.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
That can be also noticed by the visualization of features (Zeiler & Fergus 2014).,2.2.1. REPRESENTATION TRANSFER,[0],[0]
"Overall, the learned representations can be conveyed to related but different domains and the parameters in the network are reusable for different tasks.",2.2.1. REPRESENTATION TRANSFER,[0],[0]
"In lifelong learning (Thrun & Mitchell 1995, Pentina & Lampert 2015), where a series of tasks is learned sequentially by a single model, the knowledge extracted from the previous tasks may be lost as new tasks are learned, resulting in what is known as catastrophic forgetting.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In order to achieve a good performance on all tasks, Li & Hoiem (2017) proposed to use the outputs of the target examples, computed by the original network on the source task, to de-
fine a learning scheme preserving the memory of the source tasks when training on the target task.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"They also tried to preserve the pre-trained parameters instead of the outputs of examples but they did not obtain interesting results.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Kirkpatrick et al. (2017) developed a similar approach with success.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
They get sensible improvements by measuring the sensitivity of the parameters of the network learned on the source data thanks to the Fisher information.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"The Fisher information matrix defines a metric in parameter space that is used in their regularizer to preserve the representation learned on the source data, thereby retaining the knowledge acquired on the previous tasks.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"This scheme, named elastic weight consolidation, was shown to avoid forgetting, but fine-tuning with plain stochastic gradient descent was more effective than elastic weight consolidation for learning new tasks.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Hence, elastic weight consolidation may be thought as being inadequate for transfer learning, where performance is only measured on the target task.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"We will show that this conclusion is not appropriate in typical transfer learning scenarios with few target examples.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In domain adaptation (Long et al. 2015b), where the target domain differs from the source domain whereas the target task is identical to the source task and no (or few) target examples are labeled, most approaches are searching for a common representation space for source and target domains to reduce domain shift.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Rozantsev et al. (2016) proposed a parameter regularization scheme for encouraging the similarity of the representations of the source and the target domains.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Their regularizer encourages similar source and target parameters, up to a linear transformation.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"Still in domain adaptation, besides vision, encouraging similar parameters in deep networks has been proposed in speaker adaptation problems (Liao 2013, Ochiai et al. 2014) and neural machine translation (Barone et al. 2017), where it proved to be helpful.
",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"The L2-SP regularizer was used independently by Grachten & Chacón (2017) for transfer in vision application, but where they used a random reinitialization of parameters.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"For convex optimization problems, this is equivalent to finetuning with L2-SP, but we are obviously not in that situation.",2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
Grachten & Chacón (2017) conclude that their strategy behaves similarly to learning from scratch.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
We will show that using the starting point as an initialization of the fine-tuning process and as the reference in the regularizer improves results consistently upon the standard fine-tuning process.,2.2.2. REGULARIZERS IN RELATED LEARNING SETUPS,[0],[0]
"In this section, we detail the penalties we consider for finetuning.",3. Regularizers for Fine-Tuning,[0],[0]
Parameter regularization is critical when learning from small databases.,3. Regularizers for Fine-Tuning,[0],[0]
"When learning from scratch, regularization is aimed at facilitating optimization and avoiding
overfitting, by implicitly restricting the capacity of the network, that is, the effective size of the search space.",3. Regularizers for Fine-Tuning,[0],[0]
"In transfer learning, the role of regularization is similar, but the starting point of the fine-tuning process conveys information that pertains to the source problem (domain and task).",3. Regularizers for Fine-Tuning,[0],[0]
"Hence, the network capacity has not to be restricted blindly: the pre-trained model sets a reference that can be used to define the functional space effectively explored during finetuning.
",3. Regularizers for Fine-Tuning,[0],[0]
"Since we are using early stopping, fine-tuning a pre-trained model is an implicit form of inductive bias towards the initial solution.",3. Regularizers for Fine-Tuning,[0],[0]
"We explore here how a coherent explicit inductive bias, encoded by a regularization term, affects the training process.",3. Regularizers for Fine-Tuning,[0],[0]
"Section 4 shows that all such schemes get an edge over the standard approaches that either use weight decay or freeze part of the network for preserving the low-level representations that are built in the first layers of the network.
",3. Regularizers for Fine-Tuning,[0],[0]
Let w ∈,3. Regularizers for Fine-Tuning,[0],[0]
Rn be the parameter vector containing all the network parameters that are to be adapted to the target task.,3. Regularizers for Fine-Tuning,[0],[0]
The regularized objective function J̃ that is to be optimized is the sum of the standard objective function J and the regularizer Ω(w).,3. Regularizers for Fine-Tuning,[0],[0]
"In our experiments, J is the negative log-likelihood, so that the criterion J̃ could be interpreted in terms of maximum a posteriori estimation, where the regularizer Ω(w) would act as the log prior of w. More generally, the minimizer of J̃ is a trade-off between the data-fitting term and the regularization term.
L2 penalty The current baseline penalty for transfer learning is the usual L2 penalty, also known as weight decay, since it drives the weights of the network to zero:
Ω(w) = α
2 ‖w‖22 , (1)
where α is the regularization parameter setting the strength of the penalty and ‖·‖p is the p-norm of a vector.
",3. Regularizers for Fine-Tuning,[0],[0]
"L2-SP Let w0 be the parameter vector of the model pretrained on the source problem, acting as the starting point (-SP) in fine-tuning.",3. Regularizers for Fine-Tuning,[0],[0]
"Using this initial vector as the reference in the L2 penalty, we get:
Ω(w) = α
2
∥∥w −w0∥∥2 2 .",3. Regularizers for Fine-Tuning,[0],[0]
"(2)
Typically, the transfer to a target task requires some modifications of the network architecture used for the source task, such as on the last layer used for predicting the outputs.",3. Regularizers for Fine-Tuning,[0],[0]
"Then, there is no one-to-one mapping between w and w0, and we use two penalties: one for the part of the target network that shares the architecture of the source network, denoted wS , the other one for the novel part, denoted wS̄ .
",3. Regularizers for Fine-Tuning,[0],[0]
"The compound penalty then becomes:
Ω(w) = α
2 ∥∥wS −w0S∥∥22 + β2 ‖wS̄‖22 .",3. Regularizers for Fine-Tuning,[0],[0]
"(3) L2-SP-Fisher Elastic weight consolidation (Kirkpatrick et al. 2017) was proposed to avoid catastrophic forgetting in the setup of lifelong learning, where several tasks should be learned sequentially.",3. Regularizers for Fine-Tuning,[0],[0]
"In addition to preserving the initial parameter vector w0, it consists in using the estimated Fisher information to define the distance between wS and w0S .",3. Regularizers for Fine-Tuning,[0],[0]
"More precisely, it relies on the diagonal of the Fisher information matrix, resulting in the following penalty:
Ω(w) = α
2 ∑ j∈S F̂jj ( wj − w0j )2 + β 2 ‖wS̄‖ 2 2 , (4)
where F̂jj is the estimate of the jth diagonal element of the Fisher information matrix.",3. Regularizers for Fine-Tuning,[0],[0]
"It is computed as the average of the squared Fisher’s score on the source problem, using the inputs of the source data:
F̂jj = 1
m m∑ i=1 K∑ k=1 fk(x (i);w0) ( ∂ ∂wj log fk(x (i);w0) )2 ,
where the outer average estimates the expectation with respect to inputs x and the inner weighted sum is the estimate of the conditional expectation of outputs given input x(i), with outputs drawn from a categorical distribution of parameters (f1(x(i);w), . . .",3. Regularizers for Fine-Tuning,[0],[0]
", fk(x(i);w), . . .",3. Regularizers for Fine-Tuning,[0],[0]
", fK(x(i);w)).
",3. Regularizers for Fine-Tuning,[0],[0]
"L1-SP We also experiment the L1 variant of L2-SP:
Ω(w) = α ∥∥wS",3. Regularizers for Fine-Tuning,[0],[0]
−w0S∥∥1 + β2 ‖wS̄‖22 .,3. Regularizers for Fine-Tuning,[0],[0]
"(5)
The usual L1 penalty encourages sparsity; here, by using w0S as a reference in the penalty, L
1-SP encourages some components of the parameter vector to be frozen, equal to the pre-trained initial values.",3. Regularizers for Fine-Tuning,[0],[0]
The penalty can thus be thought as intermediate between L2-SP (3) and the strategies consisting in freezing a part of the initial network.,3. Regularizers for Fine-Tuning,[0],[0]
"We explore below other ways of doing so.
",3. Regularizers for Fine-Tuning,[0],[0]
"Group-Lasso-SP (GL-SP) Instead of freezing some individual parameters, we may encourage freezing some groups of parameters corresponding to channels of convolution kernels.",3. Regularizers for Fine-Tuning,[0],[0]
"Formally, we endow the set of parameters with a group structure, defined by a fixed partition of the index set I = {1, . . .",3. Regularizers for Fine-Tuning,[0],[0]
", p}, that is, I = ⋃G g=0 Gg, with Gg ∩ Gh = ∅ for g 6= h.",3. Regularizers for Fine-Tuning,[0],[0]
"In our setup, G0 = S̄, and for g > 0, Gg is the set of fan-in parameters of channel g.",3. Regularizers for Fine-Tuning,[0],[0]
"Let pg denote the cardinality of group g, and wGg ∈ Rpg be the vector (wj)j∈Gg .",3. Regularizers for Fine-Tuning,[0],[0]
"Then, the GL-SP penalty is:
Ω(w) = α G∑",3. Regularizers for Fine-Tuning,[0],[0]
g=1 sg ∥∥∥wGg −w0Gg∥∥∥,3. Regularizers for Fine-Tuning,[0],[0]
"2 + β 2 ‖wS̄‖ 2 2 , (6)
where w0G0 = w 0 S̄ 4 = 0, and,",3. Regularizers for Fine-Tuning,[0],[0]
"for g > 0, sg is a predefined constant that may be used to balance the different cardinalities of groups.",3. Regularizers for Fine-Tuning,[0],[0]
"In our experiments, we used sg = p 1/2 g .
",3. Regularizers for Fine-Tuning,[0],[0]
"Our implementation of Group-Lasso-SP can freeze feature extractors at any depth of the convolutional network, to preserve the pre-trained feature extractors as a whole instead of isolated pre-trained parameters.",3. Regularizers for Fine-Tuning,[0],[0]
"The group Gg of size pg = hg × wg × dg gathers all the parameters of a convolution kernel of height hg, width wg, and depth dg.",3. Regularizers for Fine-Tuning,[0],[0]
"This grouping is done at each layer of the network, for each output channel, so that the group index g corresponds to two indexes in the network architecture: the layer index l and the output channel index at layer l.",3. Regularizers for Fine-Tuning,[0],[0]
"If we have cl such channels at layer l, we have a total of G = ∑ l cl groups.
",3. Regularizers for Fine-Tuning,[0],[0]
Group-Lasso-SP-Fisher (GL-SP-Fisher),3. Regularizers for Fine-Tuning,[0],[0]
"Following the idea of L2-SP-Fisher, the Fisher version of GL-SP is:
Ω(w) = α G∑ g=1 sg ( ∑ j∈Gg F̂jj ( wj − w0j )2 )1/2 + β 2 ‖wG0‖ 2 2 .",3. Regularizers for Fine-Tuning,[0],[0]
We evaluate the aforementioned parameter regularizers on several pairs of source and target tasks.,4. Experiments,[0],[0]
"We use ResNet (He et al. 2016) as our base network, since it has proven its wide applicability on transfer learning tasks.",4. Experiments,[0],[0]
"Conventionally, if the target task is also a classification task, the training process starts by replacing the last layer with a new one, randomly generated, whose size depends on the number of classes in the target task.",4. Experiments,[0],[0]
"For comparing the effect of similarity between the source problem and the target problem on transfer learning, we chose two source databases: ImageNet (Deng et al. 2009) for generic object recognition and Places 365 (Zhou et al. 2017) for scene classification.",4.1. Source and Target Databases,[0],[0]
"Likewise, we have three different databases related to three target problems: Caltech 256 (Griffin et al. 2007) contains different objects for generic object recognition; MIT Indoors 67 (Quattoni & Torralba 2009) consists of 67 indoor scene categories; Stanford Dogs 120 (Khosla et al. 2011) contains images of 120 breeds of dogs; Each target database is split into training and testing sets following the suggestion of their creators (see Table 1 for details).",4.1. Source and Target Databases,[0],[0]
"In addition, we consider two configurations for Caltech 256: 30 or 60 examples randomly drawn from each category for training, and 20 remaining examples for test.",4.1. Source and Target Databases,[0],[0]
Most images in those databases are color images.,4.2. Training Details,[0],[0]
"If not, we create a three-channel image by duplicating the grayscale data.",4.2. Training Details,[0],[0]
"All images are pre-processed: we resize images to 256×256 and subtract the mean activity computed over the training set from each channel, then we adopt random blur, random mirror and random crop to 224×224 for data augmentation.",4.2. Training Details,[0],[0]
The network parameters are regularized as described in Section 3.,4.2. Training Details,[0],[0]
"Cross validation is used for searching the best regularization hyperparameters α and β: α differs across experiments, and β = 0.01 is consistently picked by cross-validation for regularizing the last layer.",4.2. Training Details,[0],[0]
"Figure 1 illustrates that the test accuracy varies smoothly according to the regularization strength, and that there is a sensible benefit in penalizing the last layer (that is, β ≥ 0) for the best α values.",4.2. Training Details,[0],[0]
"When applicable, the Fisher information matrix is estimated on the source database.",4.2. Training Details,[0],[0]
The two source databases (ImageNet or Places 365) yield different estimates.,4.2. Training Details,[0],[0]
"Regarding testing, we use central crops as inputs to compute the classification accuracy.
",4.2. Training Details,[0],[0]
Stochastic gradient descent with momentum 0.9 is used for optimization.,4.2. Training Details,[0],[0]
We run 9000 iterations and divide the learning rate by 10 after 6000 iterations.,4.2. Training Details,[0],[0]
"The initial learning rates are 0.005, 0.01 or 0.02, depending on the tasks.",4.2. Training Details,[0],[0]
Batch size is 64.,4.2. Training Details,[0],[0]
"Then, under the best configuration, we repeat five times the learning process to obtain an average classification accuracy and standard deviation.",4.2. Training Details,[0],[0]
All the experiments are performed with Tensorflow (Abadi et al. 2015).,4.2. Training Details,[0],[0]
"Table 2 displays the results of fine-tuning with L2-SP and L2-SP-Fisher, which are compared to the current baseline of fine-tuning with L2.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
We report the average accuracies and their standard deviations on 5 different runs.,4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"Since we use the same data and the same starting point, runs differ only due to the randomness of stochastic gradient descent and to the weight initialization of the last layer.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"We can observe that L2-SP and L2-SP-Fisher always improve over L2, and that when less training data are available for the target problem, the improvement of L2-SP and L2-SPFisher compared to L2 are more important.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"Meanwhile, no large difference is observed between L2-SP and L2-SPFisher.
",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
"We can boost the performance and outperform the state of the art (Ge & Yu 2017) in some cases by exploiting more training techniques and post-processing methods, which are described in the supplementary material.",4.3.1. FINE-TUNING FROM A SIMILAR SOURCE,[0],[0]
A comprehensive view of our experimental results is given in Figure 2.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
Each plot corresponds to one of the four target databases listed in Table 1.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The light red points mark the accuracies of transfer learning when using Places 365 as the source database, whereas the dark blue points correspond to the results obtained with ImageNet.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"As expected, the results
Table 2.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Average classification accuracies (in %) of L2, L2-SP and L2-SP-Fisher on 5 different runs.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The source database is Places 365 for MIT Indoors 67 and ImageNet for Stanford Dogs 120 and Caltech 256.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"MIT Indoors 67 Stanford Dogs 120 Caltech 256 – 30 Caltech 256 – 60 L2 79.6±0.5 81.4±0.2 81.5±0.2 85.3±0.2
L2-SP 84.2±0.3 85.1±0.2 83.5±0.1 86.4±0.2 L2-SP-Fisher 84.0±0.4 85.1±0.2 83.3±0.1 86.0±0.1
of transfer learning are much better when source and target are alike: the scene classification target task MIT Indoor 67 (top left) is better transferred from the scene classification source task Places 365, whereas the object recognition target tasks benefit more from the object recognition source task ImageNet.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
It is however interesting to note that the trends are similar for the two source databases: all the fine-tuning strategies based on penalties using the starting point -SP as a reference perform consistently better than standard finetuning (L2).,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"There is thus a benefit in having an explicit bias towards the starting point, even when the target task is not too similar to the source task.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
This benefit is comparable for L2-SP and L2-SP-Fisher penalties; the strategies based on L1 and Group-Lasso penalties behave rather poorly in comparison.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
They are even less accurate than the plain L2 strategy on Caltech 256 – 60 when the source problem is Places 365.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Stochastic gradient
descent does not handle well these penalties whose gradient is discontinuous at the starting point where the optimization starts.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"The stochastic forward-backward splitting algorithm of Duchi & Singer (2009), which is related to proximal methods, leads to substandard results, presumably due to the absence of a momentum term.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"In the end, we used plain stochastic gradient descent on a smoothed version of the penalties eliminating the discontinuities of their gradients, but some instability remains.
","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Finally, the variants using the Fisher information matrix behave like the simpler variants using a Euclidean metric on parameters.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"We believe that this is due to the fact that, contrary to lifelong learning, our objective does not favor solutions that retain accuracy on the source task.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
The metric defined by the Fisher information matrix may thus be less relevant for our actual objective that only relates to the target task.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Table 3 confirms that L2-SP-Fisher is indeed a better approach in the situation of lifelong learning, where accuracies on the source tasks matter.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"It reports the drop in performance when the fine-tuned models are applied on the source task, without any retraining, simply using the original classification layer instead of the classification layer learned for the target task.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
The performance drop is smaller for L2SP-Fisher than for L2-SP.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"In comparison, L2 fine-tuning results in catastrophic forgetting: the performance on the source task is considerably affected by fine-tuning.
4.3.3.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"FINE-TUNING vs. FREEZING THE NETWORK
Freezing the first layers of a network during transfer learning is another way to ensure a very strong inductive bias,
letting less degrees of freedom to transfer learning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Figure 3 shows that this strategy, which is costly to implement if one looks for the optimal number of layers to be frozen, can improve L2 fine-tuning considerably, but that it is a rather inefficient strategy for L2-SP fine-tuning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Among all possible choices, L2 fine-tuning with partial freezing is dominated by the plain L2-SP fine-tuning.","4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
Note that L2-SP-Fisher (not displayed) behaves similarly to L2-SP.,"4.3.2. BEHAVIOR ACROSS PENALTIES, SOURCE AND TARGET DATABASES",[0],[0]
"Among all -SP methods, L2-SP and L2-SP-Fisher always reach a better accuracy on the target task.",4.4. Analysis and Discussion,[0],[0]
"We expected L2SP-Fisher to outperform L2-SP, since Fisher information helps in lifelong learning, but there is no significant difference between the two options.",4.4. Analysis and Discussion,[0],[0]
"Since L2-SP is simpler than L2-SP-Fisher, we recommend the former, and we focus on the analysis of L2-SP, although most of the analysis and the discussion would also apply to L2-SP-Fisher.",4.4. Analysis and Discussion,[0],[0]
"The -SP penalties introduce no extra parameters, and they only increase slightly the computational burden.",4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
L2-SP increases the number of floating point operations required for a learning step of ResNet-101 by less than 1%.,4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
"Hence, at a negligible computational cost, we can obtain significant improvements in classification accuracy, and no additional cost is experienced at test time.",4.4.1. COMPUTATIONAL EFFICIENCY,[0],[0]
Analytical results are very difficult to obtain in the deep learning framework.,4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Under some (highly) simplifying assumptions, we show in supplementary material that the optimum of the regularized objective function with L2-SP is a compromise between the optimum of the unregularized objective function and the pre-trained parameter vector, precisely an affine combination along the directions of eigenvectors of the Hessian matrix of the unregularized objective function.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
This contrasts with L2 that leads to a compromise between the optimum of the unregularized objective function and the origin.,4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Clearly, searching for a solution in the vicinity of the pre-trained parameters is intuitively much more appealing, since it is the actual motivation for using the pre-trained parameters as the starting point of the fine-tuning process.
",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Using L2-SP instead of L2 can also be motivated by an analogy with shrinkage estimation (see e.g. Lehmann & Casella 1998, chapter 5).",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"Although it is known that shrinking toward any reference is better than raw fitting, it is also known that shrinking towards a value that is close to the “true parameters” is more effective.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"The notion of “true parameters” is not readily applicable to deep networks, but the connection with Stein shrinking effect may be inspiring by surveying the literature considering shrinkage towards other references, such as linear subspaces.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"In particular, it is likely that manifolds of parameters defined from the pre-trained network would provide a more relevant reference than the single parameter value provided by the pre-trained network.",4.4.2. THEORETICAL INSIGHTS,[0],[0]
"We complement our experimental results by an analysis relying on the activations of the hidden units of the network, to provide another view on the differences between L2 and L2SP fine-tuning.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"Activation similarities are easier to interpret than parameter similarities, and they provide a view of the network that is closer to the functional perspective we are actually pursuing.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"Matching individual activations makes sense, provided that the networks slightly differ before and after tuning so that few roles are switched between units or feature maps.
",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"The dependency between the pre-trained and the fine-tuned
activations throughout the network is displayed in Figure 4, with boxplots of the R2 coefficients, gathered layer-wise, of the fine-tuned activations with respect to the original activations.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"This figure shows that, indeed, the roles of units or feature maps have not changed much after L2-SP and L2SP-Fisher fine-tuning.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"The R2 coefficients are very close to 1 on the first layers, and smoothly decrease throughout the network, staying quite high, around 0.6, for L2-SP and L2-SP-Fisher at the greatest depth.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"In contrast, for L2 regularization, some important changes are already visible in the first layers, and the R2 coefficients eventually reach quite low values at the greatest depth.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
"This illustrates in details how the roles of the network units is remarkably retained with L2-SP and L2-SP-Fisher fine-tuning, not only for the first layers of the networks, but also for the last high-level representations before classification.",4.4.3. LAYER-WISE ANALYSIS,[0],[0]
We described and tested simple regularization techniques for inductive transfer learning.,5. Conclusion,[0],[0]
"They all encode an explicit bias towards the solution learned on the source task, resulting in a compromise with the pre-trained parameter that is coherent with the original motivation for fine-tuning.",5. Conclusion,[0],[0]
"All the regularizers evaluated here have been already used for other purposes or in other contexts, but we demonstrated their relevance for inductive transfer learning with deep convolutional networks.
",5. Conclusion,[0],[0]
"We show that a simple L2 penalty using the starting point as a reference, L2-SP, is useful, even if early stopping is used.",5. Conclusion,[0],[0]
This penalty is much more effective than the standard L2 penalty that is commonly used in fine-tuning.,5. Conclusion,[0],[0]
It is also more effective and simpler to implement than the strategy consisting in freezing the first layers of a network.,5. Conclusion,[0],[0]
"We provide
theoretical hints and strong experimental evidence showing that L2-SP retains the memory of the features learned on the source database.",5. Conclusion,[0],[0]
"We thus believe that this simple L2-SP scheme should be considered as the standard baseline in inductive transfer learning, and that future improvements of transfer learning should rely on this baseline.
",5. Conclusion,[0],[0]
"Besides, we tested the effect of more elaborate penalties, based on L1 or Group-L1 norms, or based on Fisher information.",5. Conclusion,[0],[0]
"None of the L1 or Group-L1 options seem to be valuable in the context of inductive transfer learning that we considered here, and using the Fisher information with L2SP does not improve accuracy on the target task.",5. Conclusion,[0],[0]
"Different approaches, which implement an implicit bias at the functional level, alike Li & Hoiem (2017), remain to be tested: being based on a different principle, their value should be assessed in the framework of inductive transfer learning.",5. Conclusion,[0],[0]
"This work was carried out with the supports of the China Scholarship Council and of a PEPS grant through the DESSTOPT project jointly managed by the National Institute of Mathematical Sciences and their Interactions (INSMI) and the Institute of Information Science and their Interactions (INS2I) of the CNRS, France.",Acknowledgments,[0],[0]
We acknowledge the support of NVIDIA Corporation with the donation of GPUs used for this research.,Acknowledgments,[0],[0]
"In inductive transfer learning, fine-tuning pretrained convolutional networks substantially outperforms training from scratch.",abstractText,[0],[0]
"When using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task.",abstractText,[0],[0]
"However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task.",abstractText,[0],[0]
"In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model.",abstractText,[0],[0]
"We show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple L penalty with the pre-trained model being a reference as the baseline of penalty for transfer learning tasks.",abstractText,[0],[0]
Explicit Inductive Bias for Transfer Learning with Convolutional Networks,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2826–2831 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).",1 Introduction,[0],[0]
"The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation.",1 Introduction,[0],[0]
"Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).
",1 Introduction,[0],[0]
"The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of
∗Corresponding Author: Zhaopeng Tu
a word.",1 Introduction,[0],[0]
"Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016).",1 Introduction,[0],[0]
"Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).",1 Introduction,[0],[0]
"Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence.",1 Introduction,[0],[0]
"These problems are difficult to alleviate using only limited intra-sentence context.
",1 Introduction,[0],[0]
"The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).",1 Introduction,[0],[0]
"The leverage of global context for NMT, however, has received relatively little attention from the research community.1",1 Introduction,[0],[0]
"In this paper, we propose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document.2
Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015).",1 Introduction,[0],[0]
"After obtaining the global context, we design several strategies to integrate it into NMT to translate the current sentence:
• Initialization, that uses the history represen1To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.
",1 Introduction,[0],[0]
"2In our preliminary experiments, considering target-side history inversely harms translation performance, since it suffers from serious error propagation problems.
2826
tation as the initial state of the encoder, decoder, or both;
• Auxiliary Context, that uses the history representation as static cross-sentence context, which works together with the dynamic intrasentence context produced by an attention model, to good effect.
",1 Introduction,[0],[0]
"• Gating Auxiliary Context, that adds a gate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.
",1 Introduction,[0],[0]
"Experimental results show that the proposed initialization and auxiliary context (w/ or w/o gating) mechanisms significantly improve translation performance individually, and combining them achieves further improvement.",1 Introduction,[0],[0]
"Given a source sentence xm to be translated, we consider its K previous sentences in the same document as cross-sentence context C = {xm−K , ...,xm−1}.",2 Approach,[0],[0]
"In this section, we first model C, which is then integrated into NMT.",2 Approach,[0],[0]
"As shown in Figure 1, we summarize the representation of C in a hierarchical way:
Sentence RNN For a sentence xk in C, the sentence RNN reads the corresponding words {x1,k, ..., xn,k, . . .",2.1 Summarizing Global Context,[0],[0]
", xN,k}",2.1 Summarizing Global Context,[0],[0]
"sequentially and updates its hidden state:
hn,k = f(hn−1,k, xn,k) (1)
where f(·) is an activation function, and hn,k is the hidden state at time n. The last state hN,k stores order-sensitive information about all the words in xk, which is used to represent the summary of the whole sentence, i.e. Sk ≡",2.1 Summarizing Global Context,[0],[0]
"hN,k.",2.1 Summarizing Global Context,[0],[0]
"After processing
each sentence in C, we can obtain all sentencelevel representations, which will be fed into document RNN.
",2.1 Summarizing Global Context,[0],[0]
"Document RNN It takes as input the sequence of the above sentence-level representations {S1, ..., Sk, ..., SK} and computes the hidden state as:
hk = f(hk−1, Sk) (2)
where hk is the recurrent state at time k, which summarizes the previous sentences that have been processed to the position",2.1 Summarizing Global Context,[0],[0]
"k. Similarly, we use the last hidden state to represent the summary of the global context, i.e. D ≡ hK .",2.1 Summarizing Global Context,[0],[0]
"We propose three strategies to integrate the history representation D into NMT:
Initialization We useD to initialize either NMT encoder, NMT decoder or both.",2.2 Integrating Global Context into NMT,[0],[0]
"For encoder, we useD as the initialization state rather than all-zero states as in the standard NMT (Bahdanau et al., 2015).",2.2 Integrating Global Context into NMT,[0],[0]
"For decoder, we rewrite the calculation of the initial hidden state s0 = tanh(WshN ) as s0 = tanh(WshN +WDD) where hN is the last hidden state in encoder and {Ws,WD} are the corresponding weight metrices.
",2.2 Integrating Global Context into NMT,[0],[0]
"Auxiliary Context In standard NMT, as shown in Figure 2 (a), the decoder hidden state for time i is computed by
si = f(si−1, yi−1, ci) (3)
where yi−1 is the most recently generated target word, and ci is the intra-sentence context summarized by NMT encoder for time i.",2.2 Integrating Global Context into NMT,[0],[0]
"As shown in Figure 2 (b), Auxiliary Context method adds the representation of cross-sentence context D to jointly update the decoding state si:
si = f(si−1, yi−1, ci, D) (4)
",2.2 Integrating Global Context into NMT,[0],[0]
"In this strategy, D serves as an auxiliary information source to better capture the meaning of the source sentence.",2.2 Integrating Global Context into NMT,[0],[0]
Now the gated NMT decoder has four inputs rather than the original three ones.,2.2 Integrating Global Context into NMT,[0],[0]
"The concatenation [ci, D], which embeds both intraand cross-sentence contexts, can be fed to the decoder as a single representation.",2.2 Integrating Global Context into NMT,[0],[0]
"We only need to modify the size of the corresponding parameter matrix for least modification effort.
",2.2 Integrating Global Context into NMT,[0],[0]
Gating Auxiliary Context The starting point for this strategy is an observation: the need for information from the global context differs from step to step during generation of the target words.,2.2 Integrating Global Context into NMT,[0],[0]
"For example, global context is more in demand when generating target words for ambiguous source words, while less by others.",2.2 Integrating Global Context into NMT,[0],[0]
"To this end, we extend auxiliary context strategy by introducing a context gate (Tu et al., 2017a) to dynamically control the amount of information flowing from the auxiliary global context at each decoding step, as shown in Figure 2 (c).
",2.2 Integrating Global Context into NMT,[0],[0]
"Intuitively, at each decoding step i, the context gate looks at decoding environment (i.e., si, yi−1, and ci), and outputs a number between 0 and 1 for each element in D, where 1 denotes “completely transferring this” while 0 denotes “completely ignoring this”.",2.2 Integrating Global Context into NMT,[0],[0]
"The global context vector D is then processed with an element-wise multiplication before being fed to the decoder activation layer.
",2.2 Integrating Global Context into NMT,[0],[0]
"Formally, the context gate consists of a sigmoid neural network layer and an element-wise multiplication operation.",2.2 Integrating Global Context into NMT,[0],[0]
"It assigns an element-wise weight to D, computed by
zi = σ(Uzsi−1 +Wzyi−1 + Czci) (5)
",2.2 Integrating Global Context into NMT,[0],[0]
"Here σ(·) is a logistic sigmoid function, and {Wz, Uz, Cz} are the weight matrices, which are trained to learn when to exploit global context to maximize the overall translation performance.",2.2 Integrating Global Context into NMT,[0],[0]
"Note that zi has the same dimensionality asD, and thus each element in the global context vector has its own weight.",2.2 Integrating Global Context into NMT,[0],[0]
"Accordingly, the decoder hidden state is updated by
si = f(si−1, yi−1, ci, zi ⊗D) (6)",2.2 Integrating Global Context into NMT,[0],[0]
We carried out experiments on Chinese–English translation task.,3.1 Setup,[0],[0]
"As the document information is necessary when selecting the previous sentences, we collect all LDC corpora that contain document boundary.",3.1 Setup,[0],[0]
The training corpus consists of 1M sentence pairs extracted from LDC corpora3 with 25.4M Chinese words and 32.4M English words.,3.1 Setup,[0],[0]
"We chose the NIST05 (MT05) as our development set, and NIST06 (MT06) and NIST08",3.1 Setup,[0],[0]
(MT08) as test sets.,3.1 Setup,[0],[0]
"We used case-insensitive BLEU score (Papineni et al., 2002) as our evaluation metric, and sign-test (Collins et al., 2005) for calculating statistical significance.
",3.1 Setup,[0],[0]
"We implemented our approach on top of an open source attention-based NMT model, Nematus4 (Sennrich and Haddow, 2016; Sennrich et al., 2017).",3.1 Setup,[0],[0]
"We limited the source and target vocabularies to the most frequent 35K words in Chinese and English, covering approximately 97.1% and 99.4% of the data in the two languages respectively.",3.1 Setup,[0],[0]
We trained each model on sentences of length up to 80 words in the training data with early stopping.,3.1 Setup,[0],[0]
"The word embedding dimension was 600, the hidden layer size was 1000, and the batch size was 80.",3.1 Setup,[0],[0]
"All our models considered the previous three sentences (i.e., K = 3) as crosssentence context.
",3.1 Setup,[0],[0]
"3The LDC corpora indexes are: 2003E07, 2003E14, 2004T07, 2005E83, 2005T06, 2006E24, 2006E34, 2006E85, 2006E92, 2007E87, 2007E101, 2007T09, 2008E40, 2008E56, 2009E16, 2009E95.
4Available at https://github.com/EdinburghNLP/nematus.",3.1 Setup,[0],[0]
Table 1 shows the translation performance in terms of BLEU score.,3.2 Results,[0],[0]
"Clearly, the proposed approaches significantly outperforms baseline in all cases.
",3.2 Results,[0],[0]
"Baseline (Rows 1-2) NEMATUS significantly outperforms Moses – a commonly used phrasebased SMT system (Koehn et al., 2007), by 2.3 BLEU points on average, indicating that it is a strong NMT baseline system.",3.2 Results,[0],[0]
"It is consistent with the results in (Tu et al., 2017b) (i.e., 26.93 vs. 29.41) on training corpora of similar scale.
",3.2 Results,[0],[0]
"Initialization Strategy (Rows 3-5) Initenc and Initdec improve translation performance by around +1.0 and +1.3 BLEU points individually, proving the effectiveness of warm-start with crosssentence context.",3.2 Results,[0],[0]
"Combining them achieves a further improvement.
",3.2 Results,[0],[0]
Auxiliary Context Strategies (Rows 6-7) The gating auxiliary context strategy achieves a significant improvement of around +1.0 BLEU point over its non-gating counterpart.,3.2 Results,[0],[0]
"This shows that, by acting as a critic, the introduced context gate learns to distinguish the different needs of the global context for generating target words.
",3.2 Results,[0],[0]
Combining (Row 8),3.2 Results,[0],[0]
"Finally, we combine the best variants from the initialization and auxiliary context strategies, and achieve the best performance, improving upon NEMATUS by +2.1 BLEU points.",3.2 Results,[0],[0]
This indicates the two types of strategies are complementary to each other.,3.2 Results,[0],[0]
"We first investigate to what extent the mistranslated errors are fixed by the proposed system.
",3.3 Analysis,[0],[0]
We randomly select 15 documents (about 60 sentences) from the test sets.,3.3 Analysis,[0],[0]
"As shown in Table 2, we count how many related errors: i) are made by NMT (Total), and ii) fixed by our method (Fixed); as well as iii) newly generated (New).",3.3 Analysis,[0],[0]
"About Ambiguity, while we found that 38 words/phrases were translated into incorrect equivalents, 76% of them are corrected by our model.",3.3 Analysis,[0],[0]
"Similarly, we solved 75% of the Inconsistency errors including lexical, tense and definiteness (definite or indefinite articles) cases.",3.3 Analysis,[0],[0]
"However, we also observe that our system brings relative 21% new errors.
",3.3 Analysis,[0],[0]
Case Study Table 3 shows an example.,3.3 Analysis,[0],[0]
The word “腐官” (corrupt officials) is mis-translated as “enemy” by the baseline system.,3.3 Analysis,[0],[0]
"With the help
of the similar word “贪官” in the previous sentence, our approach successfully correct this mistake.",3.3 Analysis,[0],[0]
This demonstrates that cross-sentence context indeed helps resolve certain ambiguities.,3.3 Analysis,[0],[0]
"While our approach is built on top of hierarchical recurrent encoder-decoder (HRED) (Sordoni et al., 2015), there are several key differences which reflect how we have generalized from the original model.",4 Related Work,[0],[0]
"Sordoni et al. (2015) use HRED to summarize a single representation from both the current and previous sentences, which limits itself to (1) it is only applicable to encoder-decoder framework without attention model, (2) the representation can only be used to initialize decoder.",4 Related Work,[0],[0]
"In contrast, we use HRED to summarize the previous sentences alone, which provides additional cross-sentence context for NMT.",4 Related Work,[0],[0]
"Our approach is more flexible at (1) it is applicable to any encoderdecoder frameworks (e.g., with attention), (2) the cross-sentence context can be used to initialize either encoder, decoder or both.
",4 Related Work,[0],[0]
"While both our approach and Serban et al. (2016) use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intrasentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate (Tu et al., 2017a) to control the amount of information from it, while they don’t.
",4 Related Work,[0],[0]
"At the same time, some researchers propose to use an additional set of an encoder and attention to model more information.",4 Related Work,[0],[0]
"For example, Jean et al. (2017) use it to encode and select part of the previous source sentence for generating each target word.",4 Related Work,[0],[0]
Calixto et al. (2017) utilize global image features extracted using a pre-trained convolutional neural network and incorporate them in NMT.,4 Related Work,[0],[0]
"As additional attention leads to more computational cost, they can only incorporate limited information such as single preceding sentence in Jean et al. (2017).",4 Related Work,[0],[0]
"However, our architecture is free to this limitation, thus we use multiple preceding sentences (e.g. K = 3) in our experiments.
",4 Related Work,[0],[0]
"Our work is also related to multi-source (Zoph and Knight, 2016) and multi-target NMT (Dong
et al., 2015), which incorporate additional source or target languages.",4 Related Work,[0],[0]
"They investigate one-tomany or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results.",4 Related Work,[0],[0]
"We proposed two complementary approaches to integrating cross-sentence context: 1) a warmstart of encoder and decoder with global context representation, and 2) cross-sentence context serves as an auxiliary information source for updating decoder states, in which an introduced context gate plays an important role.",5 Conclusion and Future Work,[0],[0]
We quantitatively and qualitatively demonstrated that the presented model significantly outperforms a strong attention-based NMT baseline system.,5 Conclusion and Future Work,[0],[0]
"We release the code for these experiments at https:// www.github.com/tuzhaopeng/LC-NMT.
",5 Conclusion and Future Work,[0],[0]
"Our models benefit from larger contexts, and would be possibly further enhanced by other document level information, such as discourse relations.",5 Conclusion and Future Work,[0],[0]
We propose to study such models for full length documents with more linguistic features in future work.,5 Conclusion and Future Work,[0],[0]
This work is supported by the Science Foundation of Ireland (SFI) ADAPT project (Grant No.:13/RC/2106).,Acknowledgments,[0],[0]
The authors also wish to thank the anonymous reviewers for many helpful comments with special thanks to Henry Elder for his generous help on proofreading of this manuscript.,Acknowledgments,[0],[0]
"In translation, considering the document as a whole can help to resolve ambiguities and inconsistencies.",abstractText,[0],[0]
"In this paper, we propose a cross-sentence context-aware approach and investigate the influence of historical contextual information on the performance of neural machine translation (NMT).",abstractText,[0],[0]
"First, this history is summarized in a hierarchical way.",abstractText,[0],[0]
"We then integrate the historical representation into NMT in two strategies: 1) a warm-start of encoder and decoder states, and 2) an auxiliary context source for updating decoder states.",abstractText,[0],[0]
Experimental results on a large Chinese-English translation task show that our approach significantly improves upon a strong attention-based NMT system by up to +2.1 BLEU points.,abstractText,[0],[0]
Exploiting Cross-Sentence Context for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4253–4262 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4253",text,[0],[0]
"Neural machine translation (NMT) models have advanced the machine translation community in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014).",1 Introduction,[0],[0]
"NMT models generally consist of two components: an encoder network to summarize the input sentence into sequential representations, based on which a decoder network generates target sentence word by word with an attention model (Bahdanau et al., 2015; Luong et al., 2015).
",1 Introduction,[0],[0]
"Nowadays, advanced NMT models generally implement encoder and decoder as multiple layers, regardless of the specific model architectures such as RNN (Zhou et al., 2016; Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017; Chen et al., 2018).
∗ Zhaopeng Tu is the corresponding author of the paper.",1 Introduction,[0],[0]
"This work was conducted when Zi-Yi Dou was interning at Tencent AI Lab.
",1 Introduction,[0],[0]
"Several researchers have revealed that different layers are able to capture different types of syntax and semantic information (Shi et al., 2016; Peters et al., 2018; Anastasopoulos and Chiang, 2018).",1 Introduction,[0],[0]
"For example, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers.
",1 Introduction,[0],[0]
"However, current NMT models only leverage the top layers of encoder and decoder in the subsequent process, which misses the opportunity to exploit useful information embedded in other layers.",1 Introduction,[0],[0]
"Recently, aggregating layers to better fuse semantic and spatial information has proven to be of profound value in computer vision tasks (Huang et al., 2017; Yu et al., 2018).",1 Introduction,[0],[0]
"In natural language processing community, Peters et al. (2018) have proven that simultaneously exposing all layer representations outperforms methods that utilize just the top layer for transfer learning tasks.
",1 Introduction,[0],[0]
"Inspired by these findings, we propose to exploit deep representations for NMT models.",1 Introduction,[0],[0]
"Specifically, we investigate two types of strategies to better fuse information across layers, ranging from layer aggregation to multi-layer attention.",1 Introduction,[0],[0]
"While layer aggregation strategies combine hidden states at the same position across different layers, multi-layer attention allows the model to combine information in different positions.",1 Introduction,[0],[0]
"In addition, we introduce an auxiliary objective to encourage different layers to capture diverse information, which we believe would make the deep representations more meaningful.
",1 Introduction,[0],[0]
We evaluated our approach on two widelyused WMT14 English⇒German and WMT17 Chinese⇒English translation tasks.,1 Introduction,[0],[0]
"We employed TRANSFORMER (Vaswani et al., 2017) as the baseline system since it has proven to outperform other architectures on the two tasks (Vaswani et al., 2017; Hassan et al., 2018).",1 Introduction,[0],[0]
"Experimen-
tal results show that exploiting deep representations consistently improves translation performance over the vanilla TRANSFORMER model across language pairs.",1 Introduction,[0],[0]
It is worth mentioning that TRANSFORMER-BASE with deep representations exploitation outperforms the vanilla TRANSFORMER-BIG model with only less than half of the parameters.,1 Introduction,[0],[0]
"Deep representations have proven to be of profound value in machine translation (Meng et al., 2016; Zhou et al., 2016).",2 Background: Deep NMT,[0],[0]
Multiple-layer encoder and decoder are employed to perform the translation task through a series of nonlinear transformations from the representation of input sequences to final output sequences.,2 Background: Deep NMT,[0],[0]
"The layer can be implemented as RNN (Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention Network (Vaswani et al., 2017).",2 Background: Deep NMT,[0],[0]
"In this work, we take the advanced Transformer as an example, which will be used in experiments later.",2 Background: Deep NMT,[0],[0]
"However, we note that the proposed approach is generally applicable to any other type of NMT architectures.
",2 Background: Deep NMT,[0],[0]
"Specifically, the encoder is composed of a stack of L identical layers, each of which has two sublayers.",2 Background: Deep NMT,[0],[0]
"The first sub-layer is a self-attention network, and the second one is a position-wise fully connected feed-forward network.",2 Background: Deep NMT,[0],[0]
"A residual connection (He et al., 2016) is employed around each of the two sub-layers, followed by layer normalization (Ba et al., 2016).",2 Background: Deep NMT,[0],[0]
"Formally, the output of the first sub-layer Cle and the second sub-layer H l e are calculated as
Cle = LN ( ATT(Qle,K l−1 e ,V l−1 e )",2 Background: Deep NMT,[0],[0]
"+H l−1 e ) ,
Hle = LN ( FFN(Cle) +C l e ) , (1)
where ATT(·), LN(·), and FFN(·) are selfattention mechanism, layer normalization, and feed-forward networks with ReLU activation in between, respectively.",2 Background: Deep NMT,[0],[0]
"{Qle,Kl−1e ,Vl−1e } are query, key and value vectors that are transformed from the (l-1)-th encoder layer Hl−1e .
",2 Background: Deep NMT,[0],[0]
The decoder is also composed of a stack of L identical layers.,2 Background: Deep NMT,[0],[0]
"In addition to two sub-layers in each decoder layer, the decoder inserts a third sublayer Dld to perform attention over the output of
the encoder stack HLe :
Cld = LN ( ATT(Qld,K l−1 d ,V l−1 d )",2 Background: Deep NMT,[0],[0]
"+H l−1 d ) ,
Dld = LN ( ATT(Cld,K L e ,V L e )",2 Background: Deep NMT,[0],[0]
"+C l d ) ,
Hld = LN ( FFN(Dld) +D l d ) , (2)
where {Qld,K l−1 d ,V l−1 d } are transformed from the (l-1)-th decoder layer Hl−1d , and {K L e ,V L e } are transformed from the top layer of the encoder.",2 Background: Deep NMT,[0],[0]
"The top layer of the decoder HLd is used to generate the final output sequence.
",2 Background: Deep NMT,[0],[0]
"Multi-layer network can be considered as a strong feature extractor with extended receptive fields capable of linking salient features from the entire sequence (Chen et al., 2018).",2 Background: Deep NMT,[0],[0]
"However, one potential problem about the vanilla Transformer, as shown in Figure 1a, is that both the encoder and decoder stack layers in sequence and only utilize the information in the top layer.",2 Background: Deep NMT,[0],[0]
"While studies have shown deeper layers extract more semantic and more global features (Zeiler and Fergus, 2014; Peters et al., 2018), these do not prove that the last layer is the ultimate representation for any task.",2 Background: Deep NMT,[0],[0]
"Although residual connections have been incorporated to combine layers, these connections have been “shallow” themselves, and only fuse by simple, one-step operations (Yu et al., 2018).",2 Background: Deep NMT,[0],[0]
"We investigate here how to better fuse information across layers for NMT models.
",2 Background: Deep NMT,[0],[0]
"In the following sections, we simplify the equations to Hl = LAYER(Hl−1) for brevity.",2 Background: Deep NMT,[0],[0]
"In this section, we first introduce how to exploit deep representations by simultaneously exposing all of the signals from all layers (Sec 3.1).",3 Proposed Approaches,[0],[0]
"Then, to explicitly encourage different layers to incorporate various information, we propose one way to measure the diversity between layers and add a regularization term to our objective function to maximize the diversity across layers (Sec 3.2).",3 Proposed Approaches,[0],[0]
"To exploit deep representations, we investigate two types of strategies to fuse information across layers, from layer aggregation to multi-layer attention.",3.1 Deep Representations,[0],[0]
"While layer aggregation strategies combine hidden states at the same position across different layers, multi-layer attention allows the model to combine information in different positions.",3.1 Deep Representations,[0],[0]
"While the aggregation strategies are inspired by previous work, there are several differences since we have simplified and generalized from the original model, as described below.",3.1.1 Layer Aggregation,[0],[0]
Dense Connection.,3.1.1 Layer Aggregation,[0],[0]
"The first strategy is to allow all layers to directly access previous layers:
Hl = f(H1, . . .",3.1.1 Layer Aggregation,[0],[0]
",Hl−1).",3.1.1 Layer Aggregation,[0],[0]
"(3)
In this work, we mainly investigate whether densely connected networks work for NMT, which have proven successful in computer vision tasks (Huang et al., 2017).",3.1.1 Layer Aggregation,[0],[0]
"The basic strategy of densely connected networks is to connect each layer to every previous layer with a residual connection:
Hl = Layer(Hl−1) + l−1∑ i=1",3.1.1 Layer Aggregation,[0],[0]
"Hi. (4)
",3.1.1 Layer Aggregation,[0],[0]
Figure 1b illustrates the idea of this approach.,3.1.1 Layer Aggregation,[0],[0]
"Our implementation differs from (Huang et al., 2017) in that we use an addition instead of a concatenation operation in order to keep the state size constant.",3.1.1 Layer Aggregation,[0],[0]
"Another reason is that concatenation operation is computationally expensive, while residual connections are more efficient.
",3.1.1 Layer Aggregation,[0],[0]
"While dense connection directly feeds previous layers to the subsequent layers, the following mechanisms maintain additional layers to aggregate standard layers, from shallow linear combination, to deep non-linear aggregation.
",3.1.1 Layer Aggregation,[0],[0]
Linear Combination.,3.1.1 Layer Aggregation,[0],[0]
"As shown in Figure 1c, an intuitive strategy is to linearly combine the outputs of all layers:
Ĥ = L∑ l=1",3.1.1 Layer Aggregation,[0],[0]
"WlH l, (5)
",3.1.1 Layer Aggregation,[0],[0]
"where {W1, . . .",3.1.1 Layer Aggregation,[0],[0]
",WL} are trainable matrices.",3.1.1 Layer Aggregation,[0],[0]
"While the strategy is similar in spirit to (Peters et al., 2018), there are two main differences: (1) they use normalized weights while we directly use parameters that could be either positive or negative numbers, which may benefit from more modeling flexibility.",3.1.1 Layer Aggregation,[0],[0]
"(2) they use a scalar that is shared by all elements in the layer states, while we use learnable matrices.",3.1.1 Layer Aggregation,[0],[0]
"The latter offers a more precise control of the combination by allowing the model to be more expressive than scalars (Tu et al., 2017).
",3.1.1 Layer Aggregation,[0],[0]
"We also investigate strategies that iteratively and hierarchically merge layers by incorporating more depth and sharing, which have proven effective for computer vision tasks (Yu et al., 2018).
",3.1.1 Layer Aggregation,[0],[0]
Iterative Aggregation.,3.1.1 Layer Aggregation,[0],[0]
"As illustrated in Figure 1d, iterative aggregation follows the iterated stacking of the backbone architecture.",3.1.1 Layer Aggregation,[0],[0]
"Aggregation begins at the shallowest, smallest scale and then iteratively merges deeper, larger scales.",3.1.1 Layer Aggregation,[0],[0]
"The iterative deep aggregation function I for a series of layers Hl1 = {H1, · · · ,Hl} with increasingly deeper and semantic information is formulated as
Ĥl = I(Hl1) = AGG(H",3.1.1 Layer Aggregation,[0],[0]
"l, Ĥl−1), (6)
where we set Ĥ1 = H1 and AGG(·, ·) is the aggregation function:
AGG(x, y) = LN(FF([x; y])",3.1.1 Layer Aggregation,[0],[0]
+ x+ y).,3.1.1 Layer Aggregation,[0],[0]
"(7)
As seen, in this work, we first concatenate x and y into z =",3.1.1 Layer Aggregation,[0],[0]
"[x; y], which is subsequently fed to a feed-forward network with a sigmoid activation in between.",3.1.1 Layer Aggregation,[0],[0]
Residual connection and layer normalization are also employed.,3.1.1 Layer Aggregation,[0],[0]
"Specifically, both x and y have residual connections to the output.",3.1.1 Layer Aggregation,[0],[0]
"The choice of the aggregation function will be further studied in the experiment section.
",3.1.1 Layer Aggregation,[0],[0]
Hierarchical Aggregation.,3.1.1 Layer Aggregation,[0],[0]
"While iterative aggregation deeply combines states, it may still be insufficient to fuse the layers for its sequential architecture.",3.1.1 Layer Aggregation,[0],[0]
"Hierarchical aggregation, on the other hand, merges layers through a tree structure to preserve and combine feature channels, as shown in Figure 2.",3.1.1 Layer Aggregation,[0],[0]
"The original model proposed by Yu et al. (2018) requires the number of layers to be the power of two, which limits the applicability of these methods to a broader range of NMT architectures (e.g. six layers in (Vaswani et al., 2017)).",3.1.1 Layer Aggregation,[0],[0]
"To solve this problem, we introduce a CNN-like tree with the filter size being two, as shown in Figure 2a.",3.1.1 Layer Aggregation,[0],[0]
"Following (Yu et al., 2018), we first merge aggregation nodes of the same depth for efficiency so that there would be at most one aggregation node for each depth.",3.1.1 Layer Aggregation,[0],[0]
"Then, we further feed the output of an aggregation node back into the backbone as the input to the next sub-tree, instead of only routing intermediate aggregations further up the tree, as shown in Figure 2b.",3.1.1 Layer Aggregation,[0],[0]
"The interaction between aggregation and backbone nodes allows the model to better preserve features.
",3.1.1 Layer Aggregation,[0],[0]
"Formally, each aggregation node Ĥi is calculated as
Ĥi = { AGG(H2i−1,H2i), i = 1 AGG(H2i−1,H2i, Ĥi−1), i = 2, 3
where AGG(H2i−1,H2i) is computed via Eqn. 7, and AGG(H2i−1,H2i, Ĥi−1) is computed as
AGG(x, y, z) = LN(FF([x; y; z])",3.1.1 Layer Aggregation,[0],[0]
"+ x+ y + z).
",3.1.1 Layer Aggregation,[0],[0]
The aggregation node at the top layer ĤL/2 serves as the final output of the network.,3.1.1 Layer Aggregation,[0],[0]
"Partially inspired by Meng et al. (2016), we also propose to introduce a multi-layer attention mechanism into deep NMT models, for more power of
transforming information across layers.",3.1.2 Multi-Layer Attention,[0],[0]
"In other words, for constructing each hidden state in any layer-l, we allow the self-attention model to attend any layers lower than l, instead of just layer l-1:
Cl−1 = ATT(Q l,Kl−1,Vl−1), Cl−2 = ATT(Q l,Kl−2,Vl−2),
. .",3.1.2 Multi-Layer Attention,[0],[0]
".
",3.1.2 Multi-Layer Attention,[0],[0]
Cl−k = ATT(Q,3.1.2 Multi-Layer Attention,[0],[0]
"l,Kl−k,Vl−k),
Cl = AGG(Cl−1, . . .",3.1.2 Multi-Layer Attention,[0],[0]
",C l −k), (8)
where Cl−i is sequential vectors queried from layer",3.1.2 Multi-Layer Attention,[0],[0]
"l-i using a separate attention model, and AGG(·) is similar to the pre-defined aggregation function to transform k vectors {Cl−1, . . .",3.1.2 Multi-Layer Attention,[0],[0]
",Cl−k} to a d-dimension vector, which is subsequently used to construct the encoder and decoder layers via Eqn. 1 and 2 respectively.",3.1.2 Multi-Layer Attention,[0],[0]
"Note that multilayer attention only modifies the self-attention blocks in both encoder and decoder, while does not revises the encoder-decoder attention blocks.",3.1.2 Multi-Layer Attention,[0],[0]
"Intuitively, combining layers would be more meaningful if different layers are able to capture diverse information.",3.2 Layer Diversity,[0],[0]
"Therefore, we explicitly add a regularization term to encourage the diversities between layers:
L = Llikelihood + λLdiversity, (9)
where λ is a hyper-parameter and is set to 1.0 in this paper.",3.2 Layer Diversity,[0],[0]
"Specifically, the regularization term measures the average of the distance between ev-
ery two adjacent layers:
Ldiversity = 1
L− 1 L−1∑ l=1 D(Hl,Hl+1).",3.2 Layer Diversity,[0],[0]
"(10)
HereD(Hl,Hl+1) is the averaged cosine-squared distance between the states in layers",3.2 Layer Diversity,[0],[0]
"Hl = {hl1, . . .",3.2 Layer Diversity,[0],[0]
",hlN} and Hl+1",3.2 Layer Diversity,[0],[0]
"= {h l+1 1 , . . .",3.2 Layer Diversity,[0],[0]
",h l+1 N }:
D(Hl,Hl+1)",3.2 Layer Diversity,[0],[0]
"= 1
N N∑ n=1",3.2 Layer Diversity,[0],[0]
"(1− cos2(hln,hl+1n )).
",3.2 Layer Diversity,[0],[0]
"The cosine-squared distance between two vectors is maximized when two vectors are linearly independent and minimized when two vectors are linearly dependent, which satisfies our goal.1",3.2 Layer Diversity,[0],[0]
Dataset.,4.1 Setup,[0],[0]
"To compare with the results reported by previous work (Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018), we conducted experiments on both Chinese⇒English (Zh⇒En) and English⇒German (En⇒De) translation tasks.",4.1 Setup,[0],[0]
"For the Zh⇒En task, we used all of the available parallel data with maximum length limited to 50, consisting of about 20.62 million sentence pairs.",4.1 Setup,[0],[0]
We used newsdev2017 as the development set and newstest2017 as the test set.,4.1 Setup,[0],[0]
"For the En⇒De task, we trained on the widely-used WMT14 dataset consisting of about 4.56 million sentence pairs.",4.1 Setup,[0],[0]
We used newstest2013 as the development set and newstest2014 as the test set.,4.1 Setup,[0],[0]
"Byte-pair encoding (BPE) was employed to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016) with 32K merge operations for both language pairs.",4.1 Setup,[0],[0]
"We used 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric, and sign-test (Collins et al., 2005) to test for statistical significance.
Models.",4.1 Setup,[0],[0]
"We evaluated the proposed approaches on advanced Transformer model (Vaswani et al., 2017), and implemented on top of an open-source toolkit – THUMT (Zhang et al., 2017).",4.1 Setup,[0],[0]
"We followed Vaswani et al. (2017) to set the configurations and train the models, and have reproduced
1We use cosine-squared distance instead of cosine distance, since the latter is maximized when two vectors are in opposite directions.",4.1 Setup,[0],[0]
"In such case, the two vectors are in fact linearly dependent, while we aim at encouraging the vectors independent from each other.
",4.1 Setup,[0],[0]
their reported results on the En⇒De task.,4.1 Setup,[0],[0]
The parameters of the proposed models were initialized by the pre-trained model.,4.1 Setup,[0],[0]
"We tried k = 2 and k = 3 for the multi-layer attention model, which allows to attend to the lower two or three layers.
",4.1 Setup,[0],[0]
"We have tested both Base and Big models, which differ at hidden size (512 vs. 1024), filter size (2048 vs. 4096) and the number of attention heads (8 vs. 16).2 All the models were trained on eight NVIDIA P40 GPUs where each was allocated with a batch size of 4096 tokens.",4.1 Setup,[0],[0]
"In consideration of computation cost, we studied model variations with Base model on En⇒De task, and evaluated overall performance with Big model on both Zh⇒En and En⇒De tasks.",4.1 Setup,[0],[0]
Table 1 shows the results on WMT14 En⇒De translation task.,4.2 Results,[0],[0]
"As seen, the proposed approaches improve the translation quality in all cases, although there are still considerable differences among different variations.
",4.2 Results,[0],[0]
"Model Complexity Except for dense connection, all other deep representation strategies introduce new parameters, ranging from 14.7M to 33.6M. Accordingly, the training speed decreases due to more efforts to train the new parameters.",4.2 Results,[0],[0]
"Layer aggregation mechanisms only marginally decrease decoding speed, while multi-layer attention decreases decoding speed by 21% due to an additional attention process for each layer.
",4.2 Results,[0],[0]
Layer Aggregation (Rows 2-5):,4.2 Results,[0],[0]
"Although dense connection and linear combination only marginally improve translation performance, iterative and hierarchical aggregation strategies achieve more significant improvements, which are up to +0.99 BLEU points better than the baseline model.",4.2 Results,[0],[0]
"This indicates that deep aggregations outperform their shallow counterparts by incorporating more depth and sharing, which is consistent with the results in computer vision tasks (Yu et al., 2018).
",4.2 Results,[0],[0]
"Multi-Layer Attention (Rows 6-7): Benefiting from the power of attention models, multi-layer attention model can also significantly outperform baseline, although it only attends to one or two additional layers.",4.2 Results,[0],[0]
"However, increasing the number of lower layers to be attended from k = 2 to
2Here “filter size” refers to the hidden size of the feedforward network in the Transformer model.
",4.2 Results,[0],[0]
"k = 3 only gains marginal improvement, at the cost of slower training and decoding speeds.",4.2 Results,[0],[0]
"In the following experiments, we set set k = 2 for the multi-layer attention model.
",4.2 Results,[0],[0]
Layer Diversity (Rows 8-10): The introduced diversity regularization consistently improves performance in all cases by encouraging different layers to capture diverse information.,4.2 Results,[0],[0]
Our best model outperforms the vanilla Transformer by +1.14 BLEU points.,4.2 Results,[0],[0]
"In the following experiments, we used hierarchical aggregation with diversity regularization (Row 8) as the default strategy.
",4.2 Results,[0],[0]
Main Results Table 2 lists the results on both WMT17 Zh⇒En and WMT14 En⇒De translation tasks.,4.2 Results,[0],[0]
"As seen, exploiting deep represen-
tations consistently improves translation performance across model variations and language pairs, demonstrating the effectiveness and universality of the proposed approach.",4.2 Results,[0],[0]
"It is worth mentioning that TRANSFORMER-BASE with deep representations exploitation outperforms the vanilla TRANSFORMER-BIG model, with only less than half of the parameters.",4.2 Results,[0],[0]
We conducted extensive analysis from different perspectives to better understand our model.,4.3 Analysis,[0],[0]
"All results are reported on the En⇒De task with TRANSFORMER-BASE.
",4.3 Analysis,[0],[0]
"Sentence Length
B LE
U
22
23
24
25
26
27
28
29
30
31
Length of Source Sentence
(0, 10
]
(10 , 2
0]
(20 , 3
0]
(30 , 4
0]
(40 , 5
0] (50 , )
",4.3 Analysis,[0],[0]
"Ours Base
B LE
U
25
26
27
28
29
30
Length of Source Sentence
(0, 15
]
(15 , 3
0]
(30 , 4
5] > 4 5
Ours Base
B LE
U
25
26
27
28
29
30
Length of Source Sentence
(0, 15
]
(15 , 3
0]
(30 , 4
5]",4.3 Analysis,[0],[0]
"> 4 5
Hier.+Div. Hier.",4.3 Analysis,[0],[0]
"Base
Figure 4: BLEU scores on the En⇒De test set with respect to various input sentence lengths.",4.3 Analysis,[0],[0]
“Hier.” denotes hierarchical aggregation and “Div.” denotes diversity regularization.,4.3 Analysis,[0],[0]
"Following Bahdanau et al. (2015) and Tu et al. (2016), we grouped sentences of similar lengths together and computed the BLEU score for each group, as shown in Figure 4.",4.3.1 Length Analysis,[0],[0]
"Generally, the performance of TRANSFORMER-BASE goes up with the increase of input sentence lengths, which is superior to the performance of RNN-based NMT models on long sentences reported by (Bentivogli et al., 2016).",4.3.1 Length Analysis,[0],[0]
"We attribute this to the strength of self-attention mechanism to model global dependencies without regard to their distance.
",4.3.1 Length Analysis,[0],[0]
"Clearly, the proposed approaches outperform the baseline model in all length segments, while there are still considerable differences between the two variations.",4.3.1 Length Analysis,[0],[0]
"Hierarchical aggregation consistently outperforms the baseline model, and the improvement goes up on long sentences.",4.3.1 Length Analysis,[0],[0]
One possible reason is that long sentences indeed require deep aggregation mechanisms.,4.3.1 Length Analysis,[0],[0]
"Introducing diversity regularization further improves performance on most sentences (e.g. ≤ 45), while the improvement degrades on long sentences (e.g. > 45).",4.3.1 Length Analysis,[0],[0]
"We conjecture that complex long sentences may need to store duplicate information across layers, which conflicts with the diversity objective.",4.3.1 Length Analysis,[0],[0]
"Both encoder and decoder are composed of a stack of L layers, which may benefit from the proposed approach.",4.3.2 Effect on Encoder and Decoder,[0],[0]
"In this experiment, we investigated how our models affect the two components, as shown
Model Applied to BLEU Encoder Decoder
BASE N/A N/A 26.13
OURS X × 26.32 × X 26.41 X X 26.69
Table 3:",4.3.2 Effect on Encoder and Decoder,[0],[0]
"Experimental results of applying hierarchical aggregation to different components on En⇒De validation set.
in Table 3.",4.3.2 Effect on Encoder and Decoder,[0],[0]
"Exploiting deep representations of encoder or decoder individually consistently outperforms the vanilla baseline model, and exploiting both components further improves the performance.",4.3.2 Effect on Encoder and Decoder,[0],[0]
These results provide support for the claim that exploiting deep representations is useful for both understanding input sequence and generating output sequence.,4.3.2 Effect on Encoder and Decoder,[0],[0]
"As described in Section 3.1.1, the function of hierarchical layer aggregation is defined as
AGG(x, y, z) = LN(FF([x; y; z])",4.3.3 Impact of Aggregation Choices,[0],[0]
"+ x+ y + z),
where FF(·) is a feed-forward network with a sigmoid activation in between.",4.3.3 Impact of Aggregation Choices,[0],[0]
"In addition, all the input layers {x, y, z} have residual connections to the output.",4.3.3 Impact of Aggregation Choices,[0],[0]
"In this experiment, we evaluated the impact of residual connection options, as well as different choices for the aggregation function, as shown in Table 4.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"Concerning residual connections, if none of the input layers are connected to the output layer (“None”), the performance would decrease.",4.3.3 Impact of Aggregation Choices,[0],[0]
"The translation performance is improved when the output is connected to only the top level of the input layers (“Top”), while connecting to all input layers (“All”) achieves the best performance.",4.3.3 Impact of Aggregation Choices,[0],[0]
"This indi-
cates that cross-layer connections are necessary to avoid the gradient vanishing problem.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"Besides the feed-forward network with sigmoid activation, we also tried two other aggregation functions for FF(·): (1) A feed-forward network with a RELU activation in between; and (2) multihead self-attention layer that constitutes the encoder and decoder layers in the TRANSFORMER model.",4.3.3 Impact of Aggregation Choices,[0],[0]
"As seen, all the three functions consistently improve the translation performance, proving the robustness of the proposed approaches.
",4.3.3 Impact of Aggregation Choices,[0],[0]
"4.4 Visualization of Aggregation
To investigate the impact of diversity regularization, we visualized the exploitation of the input representations for hierarchical aggregation in encoder side, as shown in Figure 5.",4.3.3 Impact of Aggregation Choices,[0],[0]
"Let Hi = {H2i, H2i−1, Ĥ i−1} be the input representations, we calculated the exploitation of the j-th input as
sj =
∑ w∈Wj |w|∑
Hj′∈H { ∑ w′∈Wj′ |w′|} , (11)
where Wj is the parameter matrix associated with the input Hj .",4.3.3 Impact of Aggregation Choices,[0],[0]
The score sj is a rough estimation of the contribution of Hj to the aggregation,4.3.3 Impact of Aggregation Choices,[0],[0]
"Ĥ i.
We have two observations.",4.3.3 Impact of Aggregation Choices,[0],[0]
"First, the model tends to utilize the bottom layer more than the top one, indicating the necessity of fusing information across layers.",4.3.3 Impact of Aggregation Choices,[0],[0]
"Second, using the diversity regularization in Figure 5(b) can encourage each layer to contribute more equally to the aggregation.",4.3.3 Impact of Aggregation Choices,[0],[0]
We hypothesize this is because of the diversity regularization term encouraging the different layers to contain diverse and equally important information.,4.3.3 Impact of Aggregation Choices,[0],[0]
Representation learning is at the core of deep learning.,5 Related Work,[0],[0]
"Our work is inspired by technological advances in representation learning, specifically in the field of deep representation learning and representation interpretation.
",5 Related Work,[0],[0]
"Deep Representation Learning Deep neural networks have advanced the state of the art in various communities, such as computer vision and natural language processing.",5 Related Work,[0],[0]
"One key challenge of training deep networks lies in how to transform information across layers, especially when the network consists of hundreds of layers.
",5 Related Work,[0],[0]
"In response to this problem, ResNet (He et al., 2016) uses skip connections to combine layers by simple, one-step operations.",5 Related Work,[0],[0]
"Densely connected network (Huang et al., 2017) is designed to better propagate features and losses through skip connections that concatenate all the layers in stages.",5 Related Work,[0],[0]
"Yu et al. (2018) design structures iteratively and hierarchically merge the feature hierarchy to better fuse information in a deep fusion.
",5 Related Work,[0],[0]
"Concerning machine translation, Meng et al. (2016) and Zhou et al. (2016) have shown that deep networks with advanced connecting strategies outperform their shallow counterparts.",5 Related Work,[0],[0]
"Due to its simplicity and effectiveness, skip connection becomes a standard component of state-of-the-art NMT models (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017).",5 Related Work,[0],[0]
"In this work, we prove that deep representation exploitation can further improve performance over simply using skip connections.
",5 Related Work,[0],[0]
"Representation Interpretation Several researchers have tried to visualize the representation of each layer to help better understand what information each layer captures (Zeiler and Fergus, 2014; Li et al., 2016; Ding et al., 2017).",5 Related Work,[0],[0]
"Concerning natural language processing tasks, Shi et al. (2016) find that both local and global source syntax are learned by the NMT encoder and different types of syntax are captured at different layers.",5 Related Work,[0],[0]
Anastasopoulos and Chiang (2018) show that higher level layers are more representative than lower level layers.,5 Related Work,[0],[0]
Peters et al. (2018) demonstrate that higher-level layers capture context-dependent aspects of word meaning while lower-level layers model aspects of syntax.,5 Related Work,[0],[0]
"Inspired by these observations, we propose to expose all of these representations to better
fuse information across layers.",5 Related Work,[0],[0]
"In addition, we introduce a regularization to encourage different layers to capture diverse information.",5 Related Work,[0],[0]
"In this work, we propose to better exploit deep representations that are learned by multiple layers for neural machine translation.",6 Conclusion,[0],[0]
"Specifically, the hierarchical aggregation with diversity regularization achieves the best performance by incorporating more depth and sharing across layers and by encouraging layers to capture different information.",6 Conclusion,[0],[0]
"Experimental results on WMT14 English⇒German and WMT17 Chinese⇒English show that the proposed approach consistently outperforms the state-of-theart TRANSFORMER baseline by +0.54 and +0.63 BLEU points, respectively.",6 Conclusion,[0],[0]
"By visualizing the aggregation process, we find that our model indeed utilizes lower layers to effectively fuse the information across layers.
",6 Conclusion,[0],[0]
"Future directions include validating our approach on other architectures such as RNN (Bahdanau et al., 2015) or CNN (Gehring et al., 2017) based NMT models, as well as combining with other advanced techniques (Shaw et al., 2018; Shen et al., 2018; Yang et al., 2018; Li et al., 2018) to further improve the performance of TRANSFORMER.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
"Advanced neural machine translation (NMT) models generally implement encoder and decoder as multiple layers, which allows systems to model complex functions and capture complicated linguistic structures.",abstractText,[0],[0]
"However, only the top layers of encoder and decoder are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers.",abstractText,[0],[0]
"In this work, we propose to simultaneously expose all of these signals with layer aggregation and multi-layer attention mechanisms.",abstractText,[0],[0]
"In addition, we introduce an auxiliary regularization term to encourage different layers to capture diverse information.",abstractText,[0],[0]
Experimental results on widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation data demonstrate the effectiveness and universality of the proposed approach.,abstractText,[0],[0]
Exploiting Deep Representations for Neural Machine Translation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2024",text,[0],[0]
"Neural models are powerful in part due to their ability to learn good representations of raw textual inputs, mitigating the need for extensive task-specific feature engineering (Collobert et al., 2011).",1 Introduction,[0],[0]
"However, a downside of learning from scratch is failing to capitalize on prior linguistic or semantic knowledge, often encoded in existing resources such as ontologies.",1 Introduction,[0],[0]
Such prior knowledge can be particularly valuable when estimating highly flexible models.,1 Introduction,[0],[0]
"In this work, we address how to exploit known relationships between words when training neural models for NLP tasks.
",1 Introduction,[0],[0]
"We propose exploiting the feature-hashing trick, originally proposed as a means of neural network compression (Chen et al., 2015).",1 Introduction,[0],[0]
"Here we instead view the partial parameter sharing induced by feature hashing as a flexible mechanism for tying together network node weights that we believe
to be similar a priori.",1 Introduction,[0],[0]
"In effect, this acts as a regularizer that constrains the model to learn weights that agree with the domain knowledge codified in external resources like ontologies.
",1 Introduction,[0],[0]
"More specifically, as external resources we use Brown clusters (Brown et al., 1992), WordNet (Miller, 1995) and the Unified Medical Language System (UMLS) (Bodenreider, 2004).",1 Introduction,[0],[0]
From these we derive groups of words with similar meaning.,1 Introduction,[0],[0]
We then use feature hashing to share a subset of weights between the embeddings of words that belong to the same semantic group(s).,1 Introduction,[0],[0]
"This forces the model to respect prior domain knowledge, insofar as words similar under a given ontology are compelled to have similar embeddings.
",1 Introduction,[0],[0]
"Our contribution is a novel, simple and flexible method for injecting domain knowledge into neural models via stochastic weight sharing.",1 Introduction,[0],[0]
"Results on seven diverse classification tasks (three sentiment and four biomedical) show that our method consistently improves performance over (1) baselines that fail to capitalize on domain knowledge, and (2) an approach that uses retrofitting (Faruqui et al., 2014) as a preprocessing step to encode domain knowledge prior to training.
155",1 Introduction,[0],[0]
"We incorporate similarity relations codified in existing resources (here derived from Brown clusters, SentiWordNet and the UMLS) as prior knowledge in a Convolutional Neural Network (CNN).1 To achieve this we construct a shared embedding matrix such that words known a priori to be similar are constrained to share some fraction of embedding weights.
",2 Grouped Weight Sharing,[0],[0]
"Concretely, suppose we have N groups of words derived from an external resource.",2 Grouped Weight Sharing,[0],[0]
"Note that one could derive such groups in several ways; e.g., using the synsets in SentiWordNet.",2 Grouped Weight Sharing,[0],[0]
"We denote groups by {g1, g2, ..., gN}.",2 Grouped Weight Sharing,[0],[0]
"Each group is associated with an embedding ggi , which we initialize by averaging the pre-trained embeddings of each word in the group.
",2 Grouped Weight Sharing,[0],[0]
"To exploit both grouped and independent word weights, we adopt a two-channel CNN model (Zhang et al., 2016b).",2 Grouped Weight Sharing,[0],[0]
The embedding matrix of the first channel is initialized with pre-trained word vectors.,2 Grouped Weight Sharing,[0],[0]
We denote this input by Ep ∈ RV×d,2 Grouped Weight Sharing,[0],[0]
(V is the vocabulary size and d the dimension of the word embeddings).,2 Grouped Weight Sharing,[0],[0]
The second channel input matrix is initialized with our proposed weightsharing embedding Es ∈ RV×d.,2 Grouped Weight Sharing,[0],[0]
"Es is initialized by drawing from both Ep and the external resource following the process we describe below.
",2 Grouped Weight Sharing,[0],[0]
"Given an input text sequence of length l, we construct sequence embedding representations Wp ∈",2 Grouped Weight Sharing,[0],[0]
Rl×d,2 Grouped Weight Sharing,[0],[0]
and,2 Grouped Weight Sharing,[0],[0]
Ws ∈,2 Grouped Weight Sharing,[0],[0]
Rl×d using the corresponding embedding matrices.,2 Grouped Weight Sharing,[0],[0]
We then apply independent sets of linear convolution filters on these two matrices.,2 Grouped Weight Sharing,[0],[0]
Each filter will generate a feature map vector v ∈,2 Grouped Weight Sharing,[0],[0]
Rl−h+1 (h is the filter height).,2 Grouped Weight Sharing,[0],[0]
"We perform 1-max pooling over each v, extracting one scalar per feature map.",2 Grouped Weight Sharing,[0],[0]
"Finally, we concatenate scalars from all of the feature maps (from both channels) into a feature vector which is fed to a softmax function to predict the label (Figure 2).
",2 Grouped Weight Sharing,[0],[0]
We initialize Es as follows.,2 Grouped Weight Sharing,[0],[0]
Each row ei ∈ Rd of Es is the embedding of word i. Words may belong to one or more groups.,2 Grouped Weight Sharing,[0],[0]
"A mapping function G(i) retrieves the groups that word i belongs to, i.e., G(i) returns a subset of {g1, g2, ..., gN}, which we denote by {g(i)1 , g (i) 2 ...",2 Grouped Weight Sharing,[0],[0]
"g (i) K }, where K is the number of groups that contain word i. To initialize Es, for each dimension j of each word embedding ei, we use a hash function hi to map
1The idea of sharing weights to reflect known similarity is general and could be applied with other neural architectures.
",2 Grouped Weight Sharing,[0],[0]
(hash) the index j to one of the K group IDs:,2 Grouped Weight Sharing,[0],[0]
"hi : N → {g(i)1 , g (i) 2 ...",2 Grouped Weight Sharing,[0],[0]
g (i) K }.,2 Grouped Weight Sharing,[0],[0]
"Following (Weinberger et al., 2009; Shi et al., 2009), we use a second hash function b to remove bias induced by hashing.",2 Grouped Weight Sharing,[0],[0]
"This is a signing function, i.e., it maps (i, j) tuples to {+1,−1} 2.",2 Grouped Weight Sharing,[0],[0]
"We then set ei,j to the product of ghi(j),j and b(i, j).",2 Grouped Weight Sharing,[0],[0]
h and b are both approximately uniform hash functions.,2 Grouped Weight Sharing,[0],[0]
"Algorithm 1 provides the full initialization procedure.
",2 Grouped Weight Sharing,[0],[0]
"Algorithm 1 Initialization of Es
1: for i in {1, . . .",2 Grouped Weight Sharing,[0],[0]
", V } do 2: {g(i)1 , g (i) 2 , . . .",2 Grouped Weight Sharing,[0],[0]
", g (i) K } := G(i).",2 Grouped Weight Sharing,[0],[0]
"3: for j ∈ {1, . . .",2 Grouped Weight Sharing,[0],[0]
", d} do 4: ei,j := ghi(j),j · b(i, j) 5: end for 6: end for
For illustration, consider Figure 1.",2 Grouped Weight Sharing,[0],[0]
"Here g1 contains three words: good, nice and amazing, while g2 has two words: good and interesting.",2 Grouped Weight Sharing,[0],[0]
"The group embeddings gg1 , gg2 are initialized as averages over the pre-trained embeddings of the words they comprise.",2 Grouped Weight Sharing,[0],[0]
"Here, embedding parameters e1,1 and e2,1 are both mapped to gg1,1, and thus share this value.",2 Grouped Weight Sharing,[0],[0]
"Similarly, e1,3 and e2,3 will share value at gg1,3.",2 Grouped Weight Sharing,[0],[0]
"We have elided the second hash function b from this figure for simplicity.
",2 Grouped Weight Sharing,[0],[0]
"2Empirically, we found that using this signing function does not affect performance.
",2 Grouped Weight Sharing,[0],[0]
"During training, we update Ep as usual using back-propagation (Rumelhart et al., 1986).",2 Grouped Weight Sharing,[0],[0]
We update Es and group embeddings g in a manner similar to Chen et al. (2015).,2 Grouped Weight Sharing,[0],[0]
"In the forward propagation before each training step (mini-batch), we derive the value of ei,j from g:
ei,j := ghi(j),j ∗",2 Grouped Weight Sharing,[0],[0]
"b(i, j) (1)
We use this newly updated ei,j to perform forward propagation in the CNN.
",2 Grouped Weight Sharing,[0],[0]
"During backward propagation, we first compute the gradient of Es, and then we use this to derive the gradient w.r.t gs.",2 Grouped Weight Sharing,[0],[0]
"To do this, for each dimension j in ggk , we aggregate the gradients w.r.t E s whose elements are mapped to this dimension:
∇ggk,j := ∑
(i,j)
∇Esi,j · δhi(j)=gk · b(i, j) (2)
where δhi(j)=gk = 1 when h i(j) = gk, and 0 otherwise.",2 Grouped Weight Sharing,[0],[0]
Each training step involves executing Equations 1 and 2.,2 Grouped Weight Sharing,[0],[0]
"Once the shared gradient is calculated, gradient descent proceeds as usual.",2 Grouped Weight Sharing,[0],[0]
"We update all parameters aside from the shared weights in the standard way.
",2 Grouped Weight Sharing,[0],[0]
The number of parameters in our approach scales linearly with the number of channels.,2 Grouped Weight Sharing,[0],[0]
"But the gradients can actually be back-propagated in a distributed way for each channel, since the convolutional and embedding layers are independent across these.",2 Grouped Weight Sharing,[0],[0]
Thus training time scales approximately linearly with the number of parameters in one channel (if the gradient is back-propagated in a distributed way).,2 Grouped Weight Sharing,[0],[0]
"We use three sentiment datasets: a movie review (MR) dataset (Pang and Lee, 2005)3; a customer review (CR) dataset (Hu and Liu, 2004)4; and an opinion dataset (MPQA) (Wiebe et al., 2005)5.
",3.1 Datasets,[0],[0]
"We also use four biomedical datasets, which concern systematic reviews.",3.1 Datasets,[0],[0]
The task here is to classify published articles describing clinical trials as relevant or not to a well-specified clinical question.,3.1 Datasets,[0],[0]
"Articles deemed relevant are included in
3www.cs.cornell.edu/people/pabo/ movie-review-data/
4www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html
5mpqa.cs.pitt.edu/corpora/mpqa_corpus/
the corresponding review, which is a synthesis of all pertinent evidence (Wallace et al., 2010).",3.1 Datasets,[0],[0]
"We use data from reviews that concerned: clopidogrel (CL) for cardiovascular conditions (Dahabreh et al., 2013); biomarkers for assessing iron deficiency in anemia (AN) experienced by patients with kidney disease (Chung et al., 2012); statins (ST) (Cohen et al., 2006); and proton beam (PB) therapy (Terasawa et al., 2009).",3.1 Datasets,[0],[0]
"We use SentiWordNet (Baccianella et al., 2010)6 for the sentiment tasks.",3.2 Implementation Details and Baselines,[0],[0]
"SentiWordNet assigns to each synset of wordnet three sentiment scores: positivity, negativity and objectivity, constrained to sum to 1.",3.2 Implementation Details and Baselines,[0],[0]
"We keep only the synsets with positivity or negativity scores greater than 0, i.e., we remove synsets deemed objective.",3.2 Implementation Details and Baselines,[0],[0]
The synsets in SentiWordNet constitute our groups.,3.2 Implementation Details and Baselines,[0],[0]
We also use the Brown clustering algorithm7 on the three sentiment datasets.,3.2 Implementation Details and Baselines,[0],[0]
"We generate 1000 clusters and treat each as a group.
",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use the Medical Subject Headings (MeSH) terms8 attached to each abstract to classify them.",3.2 Implementation Details and Baselines,[0],[0]
Each MeSH term has a tree number indicating the path from the root in the UMLS.,3.2 Implementation Details and Baselines,[0],[0]
"For example, ‘Alagille Syndrome’ has tree number ‘C06.552.150.125’; periods denote tree splits, numbers are nodes.",3.2 Implementation Details and Baselines,[0],[0]
"We induce groups comprising MeSH terms that share the same first three parent nodes, e.g., all terms with ‘C06.552.150’ as their tree number prefix constitute one group.
",3.2 Implementation Details and Baselines,[0],[0]
We compare our approach to several baselines.,3.2 Implementation Details and Baselines,[0],[0]
"All use pre-trained embeddings to initialize Ep, but we explore several approaches to exploiting Es: (1) randomly initialize Es; (2) initialize Es to reflect the group embedding g, but do not share weights during the training process, i.e., do not constrain their weights to be equal when we perform back-propagation; (3) use linguistic resources to retro-fit (Faruqui et al., 2014) the pretrained embeddings, and use these to initialize Es.",3.2 Implementation Details and Baselines,[0],[0]
"For retro-fitting, we first construct a graph derived from SentiWordNet.",3.2 Implementation Details and Baselines,[0],[0]
Then we run beliefpropagation on the graph to encourage linked words to have similar vectors.,3.2 Implementation Details and Baselines,[0],[0]
"This is a preprocessing step only; we do not impose weight sharing constraints during training.
",3.2 Implementation Details and Baselines,[0],[0]
"6sentiwordnet.isti.cnr.it 7github.com/percyliang/brown-cluster 8www.nlm.nih.gov/bsd/disted/
meshtutorial/
For the sentiment datasets we use three filter heights (3,4,5) for each of the two CNN channels.",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use only one filter height (1), because the inputs are unstructured MeSH terms.9",3.2 Implementation Details and Baselines,[0],[0]
In both cases we use 100 filters of each unique height.,3.2 Implementation Details and Baselines,[0],[0]
"For the sentiment datasets, we use Google word2vec (Mikolov et al., 2013)10 to initialize Ep.",3.2 Implementation Details and Baselines,[0],[0]
"For the biomedical datasets, we use word2vec trained on biomedical texts (Moen and Ananiadou, 2013)11 to initialize Ep.",3.2 Implementation Details and Baselines,[0],[0]
"For parameter estimation, we use Adadelta (Zeiler, 2012).",3.2 Implementation Details and Baselines,[0],[0]
"Because the biomedical datasets are imbalanced, we use downsampling (Zhang et al., 2016a; Zhang and Wallace, 2015) to effectively train on balanced subsets of the data.
",3.2 Implementation Details and Baselines,[0],[0]
"We developed our approach using the MR sentiment dataset, tuning our approach to constructing groups from the available resources – experiments on other sentiment datasets were run after we finalized the model and hyperparameters.",3.2 Implementation Details and Baselines,[0],[0]
"Similarly, we used the anemia (AN) review as a development set for the biomedical tasks, especially w.r.t.",3.2 Implementation Details and Baselines,[0],[0]
constructing groups from MeSH terms using UMLS.,3.2 Implementation Details and Baselines,[0],[0]
"We replicate each experiment five times (each is a 10-fold cross validation), and report the mean (min, max) across these replications.",4 Results,[0],[0]
"Results on the sentiment and biomedical corpora in are presented in Tables 2 and 3, respectively.12 These exploit different external resources to induce the word groupings that in turn inform weight sharing.",4 Results,[0],[0]
"We report AUC for the biomedical datasets because these are highly imbalanced (see Table 1).
",4 Results,[0],[0]
"Our method improves performance compared to all relevant baselines (including an approach that
9For this work we are ignoring title and abstract texts.",4 Results,[0],[0]
"10code.google.com/archive/p/word2vec/ 11bio.nlplab.org/ 12Sentiment task results are not directly comparable to
prior work due to different preprocessing steps.
also exploits external knowledge via retrofitting) in six of seven cases.",4 Results,[0],[0]
"Informing weight initialization using external resources improves performance independently, but additional gains are realized by also enforcing sharing during training.
",4 Results,[0],[0]
"We note that our aim here is not necessarily to achieve state-of-art results on any given dataset, but rather to evaluate the proposed method for incorporating external linguistic resources into neural models via weight sharing.",4 Results,[0],[0]
We have therefore compared to baselines that enable us to assess this.,4 Results,[0],[0]
Neural Models for NLP.,5 Related Work,[0],[0]
"Recently there has been enormous interest in neural models for NLP generally (Collobert et al., 2011; Goldberg, 2016).",5 Related Work,[0],[0]
"Most relevant to this work, simple CNN based models (which we have built on here) have proven extremely effective for text categorization (Kim, 2014; Zhang and Wallace, 2015).",5 Related Work,[0],[0]
Exploiting Linguistic Resources.,5 Related Work,[0],[0]
A potential drawback to learning from scratch in end-to-end neural models is a failure to capitalize on existing knowledge sources.,5 Related Work,[0],[0]
"There have been efforts to exploit such resources specifically to induce better word vectors (Yu and Dredze, 2014; Faruqui et al., 2014; Yu et al., 2016; Xu et al., 2014).",5 Related Work,[0],[0]
"But these models do not attempt to exploit external resources jointly during training for a particular downstream task (which uses word embeddings as inputs), as we do here.
",5 Related Work,[0],[0]
Past work on sparse linear models has shown the potential of exploiting linguistic knowledge in statistical NLP models.,5 Related Work,[0],[0]
"For example, Yogatama and Smith (2014) used external resources to inform structured, grouped regularization of loglinear text classification models, yielding improvements over standard regularization approaches.",5 Related Work,[0],[0]
"Elsewhere, Doshi-Velez et al. (2015) proposed a variant of LDA that exploits a priori known tree-
structured relations between tokens (e.g., derived from the UMLS) in topic modeling.",5 Related Work,[0],[0]
Weight-sharing in NNs.,5 Related Work,[0],[0]
Recent work has considered stochastically sharing weights in neural models.,5 Related Work,[0],[0]
"Notably, Chen et al. (2015).",5 Related Work,[0],[0]
proposed randomly sharing weights in neural networks.,5 Related Work,[0],[0]
"Elsewhere, Han et al. (2015) proposed quantized weight sharing as an intermediate step in their deep compression model.",5 Related Work,[0],[0]
"In these works, the primary motivation was model compression, whereas here we view the hashing trick as a mechanism to encode domain knowledge.",5 Related Work,[0],[0]
We have proposed a novel method for incorporating prior semantic knowledge into neural models via stochastic weight sharing.,6 Conclusion,[0],[0]
We have showed it generally improves text classification performance vs. model variants which do not exploit external resources and vs. an approach based on retrofitting prior to training.,6 Conclusion,[0],[0]
"In future work, we will investigate generalizing our approach beyond classification, and to inform weight sharing using other varieties and sources of linguistic knowledge.
Acknowledgements.",6 Conclusion,[0],[0]
"This work was made possible by NPRP grant NPRP 7-1313-1-245 from the Qatar National Research
Fund (a member of Qatar Foundation).",6 Conclusion,[0],[0]
"The statements made
herein are solely the responsibility of the authors.",6 Conclusion,[0],[0]
A fundamental advantage of neural models for NLP is their ability to learn representations from scratch.,abstractText,[0],[0]
"However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS).",abstractText,[0],[0]
"We propose a general, novel method for exploiting such resources via weight sharing.",abstractText,[0],[0]
Prior work on weight sharing in neural networks has considered it largely as a means of model compression.,abstractText,[0],[0]
"In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models.",abstractText,[0],[0]
We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.,abstractText,[0],[0]
Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 918–924 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
918",text,[0],[0]
"The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries.",1 Introduction,[0],[0]
"Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017).",1 Introduction,[0],[0]
All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form.,1 Introduction,[0],[0]
"These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM).",1 Introduction,[0],[0]
"This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse.
",1 Introduction,[0],[0]
"However, the syntactic features capture important structural information of the natural lan-
∗ Work done when the author was at IBM Research.
guage input, which complements the simple word sequence.",1 Introduction,[0],[0]
"For example, a dependency graph presents grammatical relations that hold among the words; and a constituent tree provides a phrase structure representation.",1 Introduction,[0],[0]
"Intuitively, by incorporating such additional information, the encoder could produce a more meaningful and robust sentence representation.",1 Introduction,[0],[0]
"The combination of these features (i.e., sequence + trees) forms a general graph structure (see Figure 1).",1 Introduction,[0],[0]
This inspires us to apply a graph encoder to produce a representation of a graph-structured input.,1 Introduction,[0],[0]
"The graph encoder also has the advantages that it could simultaneously encode all types of syntactic contexts, and incorporate multiple types of syntactic structures in a unified way.
",1 Introduction,[0],[0]
"In this paper, we first introduce a structure, namely syntactic graph, to represent three types of syntactic information, i.e., word order, dependency and constituency features (see §2).",1 Introduction,[0],[0]
"We then employ a novel graph-to-sequence (Graph2Seq) model (Xu et al., 2018), which consists of a graph encoder and a sequence decoder, to learn the representation of the syntactic graph (see §3).",1 Introduction,[0],[0]
"Specifically, the graph encoder learns the representation of each node by aggregating information from its K-hop neighbors.",1 Introduction,[0],[0]
"Given the learned node embeddings, the graph encoder uses a pooling-based method to generate the graph embedding.",1 Introduction,[0],[0]
"On the decoder side, a Recurrent Neural Network (RNN) decoder takes the graph embedding as its initial hidden state to generate the logical form while employing an attention mechanism over the node embeddings.",1 Introduction,[0],[0]
"Experimental results show that our model achieves the competitive performance on Jobs640, ATIS, and Geo880 datasets.
",1 Introduction,[0],[0]
"Different from existing works, we also investigate the robustness of our model by evaluating the model on two types of adversarial examples (Belinkov and Bisk, 2017; Cheng et al., 2018).
",1 Introduction,[0],[0]
"Experimental results show that the model coupling all syntactic features has the best robustness, achieving the best performance.",1 Introduction,[0],[0]
Our code and data is available at https://github.com/IBM/ Text-to-LogicForm.,1 Introduction,[0],[0]
"We represent three types of syntactic features, i.e., word order, dependency parse and constituency parse, as the syntactic graph (see Figure 1).",2 Syntactic Graph,[0],[0]
•Word Order Features.,2 Syntactic Graph,[0],[0]
Previous neural semantic parsers mainly use these features by building a SeqLSTM that works on the word sequence.,2 Syntactic Graph,[0],[0]
Our syntactic graph also incorporates this information by generating a node for each word and connecting them in the chain form.,2 Syntactic Graph,[0],[0]
"In order to capture the forward and backward contextual information, we link these nodes in two directions, that is, from left to right and from right to left.",2 Syntactic Graph,[0],[0]
• Dependency Features.,2 Syntactic Graph,[0],[0]
A dependency parse describes the grammatical relations that hold among words.,2 Syntactic Graph,[0],[0]
"Reddy et al. (2016, 2017) have demonstrated that the dependency parse tree could be directly transformed to a logical form, which indicates that the dependency information (i.e., tree structure and dependency labels) is critical to the semantic parsing task.",2 Syntactic Graph,[0],[0]
We incorporate this information into the syntactic graph by adding directed edges between the word nodes and assign them with dependency labels.,2 Syntactic Graph,[0],[0]
• Constituency Features.,2 Syntactic Graph,[0],[0]
"Similar to the dependency parse, the constituency parse represents the phrase structure, which is also important to the semantic parsing task.",2 Syntactic Graph,[0],[0]
"Take Figure 1 as an example: given the constituent tree that explicitly annotates “not related with AI” (node #1) is a proposition phrase, the model could learn a meaningful embedding for this phrase by encoding this structure into the model.",2 Syntactic Graph,[0],[0]
"Motivated by this observation, we
add the non-terminal nodes of the constituent tree and the edges describing their parent-child relationships into the syntactic graph.",2 Syntactic Graph,[0],[0]
"After building the syntactic graph for the input text, we employ a novel graph-to-sequence model (Xu et al., 2018), which includes a graph encoder and a sequence decoder with attention mechanism, to map the syntactic graph to the logical form.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Conceptually, the graph encoder generates node embeddings for each node by accumulating information from its K-hop neighbors, and then produces a graph embedding for the entire graph by abstracting all these node embeddings.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Next, the sequence decoder takes the graph embedding as the initial hidden state, and calculates the attention over all node embeddings on the encoder side to generate logical forms.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Note that this graph encoder does not explicitly encode the edge label information, therefore, for each labeled edge, we add a node whose text attribute is the edge’s label.
",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Node Embedding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Given the syntactic graph G = (V, E), we take the embedding generation process for node v ∈ V as an example to explain the node embedding generation algorithm1: (1) We first transform node v’s text attribute to a feature vector, av, by looking up the embedding matrix We; (2) The neighbors of v are categorized into forward neighbors N`(v) and backward neighbors Na(v) according to the edge direction.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In particular, N`(v) returns the nodes that v directs to and Na(v) returns the nodes that direct to v; (3) We aggregate the forward representations of v’s forward neighbors {hk−1u` , ∀u ∈ N`(v)} into
1Interested readers may refer to (Xu et al., 2018) for more implementation details.
",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"a single vector, hkN`(v), where k∈{1, ...,K} is the iteration index.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Specifically, this aggregator feeds each neighbor’s vector to a fully-connected neural network and applies an element-wise max-pooling operation to capture different aspects of the neighbor set.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"Notice that, at iteration k, this aggregator only uses the representations generated at k − 1.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The initial forward representation of each node is its feature vector calculated in step (1); (4) We concatenate v’s current forward representation hk−1v` with the newly generated neighborhood vector hkN`(v).,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"The resulted vector is fed into a fully connected layer with nonlinear activation function σ, which updates the forward representation of v, hkv`, to be used at the next iteration; (5) We update the backward representation of v, hkva using the similar procedure as introduced in step (3) and (4) except that operating on the backward representations; (6) We repeat steps (3)∼(5) K times, and the concatenation of the final forward and backward representations is used as the final representation of v. Since the neighbor information from different hops may have different impacts on the node embedding, we learn a distinct aggregator at each iteration.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Graph Embedding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"We feed the obtained node embeddings into a fully-connected neural network, and apply the element-wise max-pooling operation on all node embeddings.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
We did not find substantial performance improvement using mean-pooling.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
Sequence Decoding.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"The decoder is an RNN which predicts the next token yi given all the previous words y<i = y1, ..., yi−1, the RNN hidden state si for time-step i and the context vector ci that captures the attention of the encoder side.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In particular, the context vector ci depends on a set of node representations (h1,...,hV ) to which the encoder maps the input graph.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The context vector ci is dynamically computed using an attention mechanism over the node representations.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
The whole model is jointly trained to maximize the conditional log-probability of the correct description given a source graph.,3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"In the inference phase, we use the beam search algorithm to generate a description with beam size = 3.",3 Graph-to-sequence Model for Semantic Parsing,[0],[0]
"We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings;
Geo880, a set of 880 queries to a database of U.S. geography; and ATIS, a set of 5,410 queries to a flight booking system.",4 Experiments,[0],[0]
"We use the standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric.
",4 Experiments,[0],[0]
"The model is trained using the Adam optimizer (Kingma and Ba, 2014), with mini-batch size 30.",4 Experiments,[0],[0]
"Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS.",4 Experiments,[0],[0]
The learning rate is set to 0.001.,4 Experiments,[0],[0]
"The decoder has 1 layer, and its hidden state size is 300.",4 Experiments,[0],[0]
"The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting.",4 Experiments,[0],[0]
We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300.,4 Experiments,[0],[0]
"For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized.",4 Experiments,[0],[0]
"We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees.
Results and Discussion.",4 Experiments,[0],[0]
Table 1 summarizes the results of our model and existing semantic parsers on three datasets.,4 Experiments,[0],[0]
"Our model achieves competitive performance on Jobs640, ATIS and Geo880.",4 Experiments,[0],[0]
"Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information.",4 Experiments,[0],[0]
Comparison with Baseline.,4 Experiments,[0],[0]
"To better demonstrate that our work is an effective way to utilize both multiple trees and the word sequence for semantic parsing, we compare with an addi-
tional straightforward baseline method (referred as BASELINE in Table 1).",4 Experiments,[0],[0]
"To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq; (",4 Experiments,[0],[0]
"2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings.",4 Experiments,[0],[0]
"The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder.
",4 Experiments,[0],[0]
We can see that our model significantly outperforms the BASELINE.,4 Experiments,[0],[0]
One possible reason is that our graph encoder jointly extracts these features in a unified model by propagating the dependency and constituency information to all nodes in the syntactic graph.,4 Experiments,[0],[0]
"However, BASELINE separately models these features using two distinct TreeLSTMs.",4 Experiments,[0],[0]
"As a result, the non-terminal tree nodes only retain only one type of syntactic information propagated from their descendants in the tree.
",4 Experiments,[0],[0]
Ablation Study.,4 Experiments,[0],[0]
"In Table 1, we also report the results of three ablation variants of our model, i.e., without word order features/dependency features/constituency features.",4 Experiments,[0],[0]
"We find that Graph2Seq is superior to Seq2Seq (Dong and Lapata, 2016) which is expected since Graph2Seq exploits more syntactic information.",4 Experiments,[0],[0]
"Among these features, the word order feature have more impact on the performance than other two syntactic features.",4 Experiments,[0],[0]
"By incorporating either the dependency or the constituency features, the model could gain further performance improvement, which underlines the importance of utilizing more aspects of syntactic information.",4 Experiments,[0],[0]
"Finally, removing both syntactic features (w/ ONLY word order) performs slightly worse compared to the Seq2Seq baseline.",4 Experiments,[0],[0]
"This shows that using K=10 hops is good enough for memorizing the sentences in our benchmarks, although still weaker compared to a bidirectional LSTM encoder.
",4 Experiments,[0],[0]
A natural question here is on which type of queries our model could benefit from incorporating these parse features.,4 Experiments,[0],[0]
"By analyzing the queries and our predicted logical forms, we find that the parse features mainly improve the prediction accuracy for the queries with complex logical forms.",4 Experiments,[0],[0]
Table 2 gives some running examples of complicated queries in three datasets.,4 Experiments,[0],[0]
"We find that the model that exploits three syntactic information
could correctly predict these logical forms while the model that only uses word order features may fail.
",4 Experiments,[0],[0]
Robustness Study.,4 Experiments,[0],[0]
"Different from previous works, we evaluate the robustness of our model by creating adversarial examples with the hope to investigate the impact of introducing more syntactic information on robustness.",4 Experiments,[0],[0]
"Specifically, we create two types of adversarial examples and conduct experiments on the ATIS dataset.",4 Experiments,[0],[0]
"Following Belinkov and Bisk (2017), we first experiment with the synthetic noise, SWAP, which swaps two letters (e.g. noise→nosie).",4 Experiments,[0],[0]
It is common to see such noisy information when typing quickly.,4 Experiments,[0],[0]
"Given a text, we randomly perform swap on m ∈ {1, 2, 3, 4, 5} words that not correspond to the operators or arguments in logical forms, ensuring the meaning of the text is not changed.",4 Experiments,[0],[0]
"We train Graph2Seq on the training data and first evaluate it on the original development data, Devori.",4 Experiments,[0],[0]
"Then we use the same model but evaluate it on a variant of Devori, whose queries contain m swapped words.
",4 Experiments,[0],[0]
"Figure 2 summarizes the results of our model on the first type of adversarial examples, i.e., the ATIS development set with the SWAP noise.",4 Experiments,[0],[0]
"From Figure 2, we can see that (1) the performance of our model on all combinations of features degrade significantly when increasing the number of swapped words; (2) the model that uses three syntactic features (our default model) always achieves the best performance, and the performance gap
compared to others increases when rising the number of swapped words; (3) word order features are the most sensitive to the word sequence while the dependency and constituency features seem more robust to such noisy information; (4) thanks to the robustness of the dependency and constituency features, the default model performs significantly better than the one that only uses word order features on the noisy sentences.",4 Experiments,[0],[0]
"These findings demonstrate that incorporating more aspects of syntactic information could enhance the robustness of the model.
",4 Experiments,[0],[0]
We also experiment with the paraphrase of the input text as the second type of adversarial examples.,4 Experiments,[0],[0]
"More specifically, we collect the paraphrase of a text by first translating it to the other language such as Chinese and then translating it back to English, using the Google Translate service.",4 Experiments,[0],[0]
We use this method to collect a new variant of Devori whose queries are the paraphrases of the original ones.,4 Experiments,[0],[0]
"By manually reading these queries, we find 94% queries convey the same meaning as original ones.",4 Experiments,[0],[0]
"Similar to the first experiment, we still train the model on Devori and evaluate it on the newly created dataset.
",4 Experiments,[0],[0]
"Table 3 shows the results of our model on the second type of adversarial examples, i.e., the paraphrased ATIS development set.",4 Experiments,[0],[0]
"We also report the
result of our model on the original ATIS development set.",4 Experiments,[0],[0]
"We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset; (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features.",4 Experiments,[0],[0]
(3) simultaneously utilizing three syntactic features could greatly enhance the robustness of our model.,4 Experiments,[0],[0]
These results again demonstrate that our model could benefit from incorporating more aspects of syntactic information.,4 Experiments,[0],[0]
Existing works of generating text representation has evolved into two main streams.,5 Related Work,[0],[0]
"The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018).",5 Related Work,[0],[0]
"These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008).",5 Related Work,[0],[0]
"To address this, recent works attempt to incorporate these syntactic information into the text representation.",5 Related Work,[0],[0]
"For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation.",5 Related Work,[0],[0]
Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently.,5 Related Work,[0],[0]
"Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs.",5 Related Work,[0],[0]
"In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018).",5 Related Work,[0],[0]
Existing neural semantic parsers mainly leverage word order features while neglecting other valuable syntactic information.,6 Conclusions,[0],[0]
"To address this, we propose to build a syntactic graph which represents three types of syntactic information, and further apply a novel graph-to-sequence model to map the syntactic graph to a logical form.",6 Conclusions,[0],[0]
"Experimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems.",6 Conclusions,[0],[0]
"Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees.",abstractText,[0],[0]
"In this paper, we first propose to use the syntactic graph to represent three types of syntactic information, i.e., word order, dependency and constituency features; then employ a graph-tosequence model to encode the syntactic graph and decode a logical form.",abstractText,[0],[0]
"Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880.",abstractText,[0],[0]
Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.,abstractText,[0],[0]
Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2193–2203, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
The problem of discovering semantic relationships between two sentences has given birth to several NLP tasks over the years.,1 Introduction,[0],[0]
"Textual entailment (Dagan et al., 2013, inter alia) asks about the truth of a hypothesis sentence given another sentence (or more generally a paragraph).",1 Introduction,[0],[0]
"Paraphrase identification (Dolan et al., 2004, inter alia) asks whether two sentences have the same meaning.",1 Introduction,[0],[0]
"Foregoing the binary entailment and paraphrase decisions, the semantic textual similarity (STS) task (Agirre et al., 2012) asks for a numeric measure of semantic equivalence between two sentences.",1 Introduction,[0],[0]
"All three tasks have attracted much interest in the form of shared tasks.
",1 Introduction,[0],[0]
"While various approaches have been proposed to predict these sentence relationships, a commonly employed strategy (Das and Smith, 2009; Chang et
al., 2010a) is to postulate an alignment between constituents of the sentences and use this alignment to make the final prediction (a binary decision or a numeric similarity score).",1 Introduction,[0],[0]
"The implicit assumption in such approaches is that better constituent alignments can lead to better identification of semantic relationships between sentences.
",1 Introduction,[0],[0]
Constituent alignments serve two purposes.,1 Introduction,[0],[0]
"First, they act as an intermediate representation for predicting the final output.",1 Introduction,[0],[0]
"Second, the alignments help interpret (and debug) decisions made by the overall system.",1 Introduction,[0],[0]
"For example, the alignment between the sentences in Figure 1 can not only be useful to determine the equivalence of the two sentences, but also help reason about the predictions.
",1 Introduction,[0],[0]
"The importance of this intermediate representation led to the creation of the interpretable semantic textual similarity task (Agirre et al., 2015a) that focuses on predicting chunk-level alignments and similarities.",1 Introduction,[0],[0]
"However, while extensive resources exist for sentence-level relationships, human annotated chunk-aligned data is comparatively smaller.
",1 Introduction,[0],[0]
"In this paper, we address the following question: can we use sentence-level resources to better pre-
2193
dict constituent alignments and similarities?",1 Introduction,[0],[0]
"To answer this question, we focus on the semantic textual similarity (STS) task and its interpretable variant.",1 Introduction,[0],[0]
We propose a joint model that aligns constituents and integrates the information across the aligned edges to predict both constituent and sentence level similarity.,1 Introduction,[0],[0]
"The key advantage of modeling these two problems jointly is that, during training, the sentence-level information can provide feedback to the constituent-level predictions.
",1 Introduction,[0],[0]
We evaluate our model on the SemEval-2016 task of interpretable STS.,1 Introduction,[0],[0]
"We show that even without the sentence information, our joint model that uses constituent alignments and similarities forms a strong baseline.",1 Introduction,[0],[0]
"Further, our easily extensible joint model can incorporate sentence-level similarity judgments to produce alignments and chunk similarities that are comparable to the best results in the shared task.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are:
1.",1 Introduction,[0],[0]
We present the first joint model for predicting constituent alignments and similarities.,1 Introduction,[0],[0]
"Our model can naturally take advantage of the much larger sentence-level annotations.
2.",1 Introduction,[0],[0]
We evaluate our model on the SemEval-2016 task of interpretable semantic similarity and show state-of-the-art results.,1 Introduction,[0],[0]
"In this section, we will introduce the notation used in the paper using the sentences in Figure 1 as a running example.",2 Problem Definition,[0],[0]
"The input to the problem is a pair of sentences, denoted by",2 Problem Definition,[0],[0]
"x. We will assume that the sentences are chunked (Tjong Kim Sang and Buchholz, 2000) into constituents.",2 Problem Definition,[0],[0]
We denote the chunks using subscripts.,2 Problem Definition,[0],[0]
"Thus, the input x consists of two sequences of chunks s = (s1, s2, · · · ) and t = (t1, t2, · · · ) respectively.",2 Problem Definition,[0],[0]
"In our running example, we have s = (Gunmen, abduct, seven foreign workers) and t =",2 Problem Definition,[0],[0]
"(Seven foreign workers, kidnapped).
",2 Problem Definition,[0],[0]
"The output consists of three components:
1.",2 Problem Definition,[0],[0]
Alignment:,2 Problem Definition,[0],[0]
"The alignment between a pair of chunks is a labeled, undirected edge that explains the relation that exists between them.",2 Problem Definition,[0],[0]
"The labels can be one of EQUI (semantically
equivalent), OPPO (opposite meaning in context), SPE1, SPE2 (the chunk from s is more specific than the one from t and vice versa), SIMI (similar meaning, but none of the previous ones) or REL (related, but none of the above)1.",2 Problem Definition,[0],[0]
"In Figure 1, we see two EQUI edges.",2 Problem Definition,[0],[0]
"A chunk from either sentence can be unaligned, as in the case of the chunk Gunmen.
",2 Problem Definition,[0],[0]
We will use y to denote the alignment for an input x.,2 Problem Definition,[0],[0]
"The alignment y consists of a sequence of triples of the form (si, tj , l).",2 Problem Definition,[0],[0]
"Here, si and tj denote a pair of chunks that are aligned with a label l. For brevity, we will include unaligned chunks into this format using a special null chunk and label to indicate that a chunk is unaligned.",2 Problem Definition,[0],[0]
"Thus, the alignment for our running example contain the triple (Gunmen, ∅, ∅).
2.",2 Problem Definition,[0],[0]
"Chunk similarity: Every aligned chunk is associated with a relatedness score between zero and five, denoting the range from unrelated to equivalent.",2 Problem Definition,[0],[0]
Note that even chunks labeled OPPO can be assigned a high score because the polarity is captured by the label rather than the score.,2 Problem Definition,[0],[0]
"We will denote the chunk similarities using z, comprising of numeric zi,j,l for elements of the corresponding alignment y. For an unaligned chunk, the corresponding similarity z is fixed to zero.
3.",2 Problem Definition,[0],[0]
"Sentence similarity: The pair of sentences is associated with a scalar score from zero to five, to be interpreted as above.",2 Problem Definition,[0],[0]
"We will use r to denote the sentence similarity for an input x.
Thus, the prediction problem is the following: Given a pair of chunked sentences x = (s, t), predict the alignment y, the alignment similarities z and the sentence similarity r. Note that this problem definition integrates the canonical semantic textual similarity task (only predicting r) and its interpretable variant (predicting both y and z) into a single task.
",2 Problem Definition,[0],[0]
"1We refer the reader to the guidelines of the task (Agirre et al., 2015a) for further details on these labels.",2 Problem Definition,[0],[0]
"Also, for simplicity, in this paper, we ignore the factuality and polarity tags from the interpretable task.",2 Problem Definition,[0],[0]
"This section describes our model for predicting alignments, alignment scores, and the sentence similarity scores for a given pair of sentences.",3 Predicting Alignments and Similarities,[0],[0]
"We will assume that learning is complete and we have all the scoring functions we need and defer discussing the parameterization and learning to Section 4.
",3 Predicting Alignments and Similarities,[0],[0]
We frame the problem of inference as an instance of an integer linear program (ILP).,3 Predicting Alignments and Similarities,[0],[0]
We will first see the scoring functions and the ILP formulation in Section 3.1.,3 Predicting Alignments and Similarities,[0],[0]
"Then, in Section 3.2, we will see how we can directly read off the similarity scores at both chunk and sentence level from the alignment.",3 Predicting Alignments and Similarities,[0],[0]
"We have two kinds of 0-1 inference variables to represent labeled aligned chunks and unaligned chunks.
",3.1 Alignment via Integer Linear Programs,[0],[0]
"We will use the inference variables 1i,j,l to denote the decision that chunks si and tj are aligned with a label l. To allow chunks to be unaligned, the variables 1i,0 and 10,j denote the decisions that si and tj are unaligned respectively.
",3.1 Alignment via Integer Linear Programs,[0],[0]
Every inference decision is scored by the trained model.,3.1 Alignment via Integer Linear Programs,[0],[0]
"Thus, we have score(i, j, l), score(i, 0) and score(0, j) for the three kinds of inference variables respectively.",3.1 Alignment via Integer Linear Programs,[0],[0]
"All scores are of the form A ( wTΦ (·, s, t) ) , where w is a weight vector that is learned, Φ (·, s, t) is a feature function whose arguments include the constituents and labels in question, and A is a sigmoidal activation function that flattens the scores to the range",3.1 Alignment via Integer Linear Programs,[0],[0]
"[0, 5].",3.1 Alignment via Integer Linear Programs,[0],[0]
"In all our experiments, we used the function A(x) = 5
1+e−x .",3.1 Alignment via Integer Linear Programs,[0],[0]
The goal of inference is to find the assignment to the inference variables that maximizes total score.,3.1 Alignment via Integer Linear Programs,[0],[0]
"That is, we seek to solve
arg max 1∈C
∑
i,j,l
score(i, j, l)1i,j,l
+ ∑
i
score(i, 0)1i,0
+ ∑
j
score(0, j)10,j (1)
",3.1 Alignment via Integer Linear Programs,[0],[0]
"Here 1 represents all the inference variables together and C denotes the set of all valid assignments to the variables, defined by the following set of constraints:
1.",3.1 Alignment via Integer Linear Programs,[0],[0]
"A pair of chunks can have at most one label.
2.",3.1 Alignment via Integer Linear Programs,[0],[0]
"Either a chunk can be unaligned or it should participate in a labeled alignment with exactly one chunk of the other sentence.
",3.1 Alignment via Integer Linear Programs,[0],[0]
"We can convert these constraints into linear inequalities over the inference variables using standard techniques for ILP inference (Roth and Yih, 2004)2.",3.1 Alignment via Integer Linear Programs,[0],[0]
"Note that, by construction, there is a oneto-one mapping from an assignment to the inference variables 1 and the alignment y.",3.1 Alignment via Integer Linear Programs,[0],[0]
"In the rest of the paper, we use these two symbols interchangeably, using 1 referring details of inference and y referring to the alignment as a sequence of labeled edges.",3.1 Alignment via Integer Linear Programs,[0],[0]
"To complete the prediction, we need to compute the numeric chunk and sentence similarities given the alignment y.",3.2 From Alignments to Similarities,[0],[0]
"In each case, we make modeling assumptions about how the alignments and similarities are related, as described below.
",3.2 From Alignments to Similarities,[0],[0]
"Chunk similarities To predict the chunk similarities, we assume that the label-specific chunk similarities of aligned chunks are the best edge-weights for the corresponding inference variables.",3.2 From Alignments to Similarities,[0],[0]
"That is, for a pair of chunks (si, tj) that are aligned with a label l, the chunk pair similarity zi,j,l is the coefficient associated with the corresponding inference variable.",3.2 From Alignments to Similarities,[0],[0]
"If the alignment edge indicates an unaligned chunk, then the corresponding score is zero.",3.2 From Alignments to Similarities,[0],[0]
"That is,
zi,j,l =
{ A ( wTΦ (si, tj , l, s, t) )",3.2 From Alignments to Similarities,[0],[0]
"if l 6= ∅
0 if l = ∅. (2)
But can chunk similarities directly be used to find good alignments?",3.2 From Alignments to Similarities,[0],[0]
"To validate this assumption, we performed a pilot experiment on the chunk aligned part of our training dataset.",3.2 From Alignments to Similarities,[0],[0]
"We used the gold standard chunk similarities as scores of the inference variables in the integer program in Eq. 1, with the variables associated with unaligned chunks being scored zero.",3.2 From Alignments to Similarities,[0],[0]
"We found that this experiment gives a near-perfect typed alignment F-score of 0.9875.
2While it may be possible to find the score maximizing alignment in the presence of these constraints using dynamic programming (say, a variant of the Kuhn-Munkres algorithm), we model inference as an ILP to allow us the flexibility to explore more sophisticated output interactions in the future.
",3.2 From Alignments to Similarities,[0],[0]
"The slight disparity is because the inference only allows 1-to-1 matches between chunks (constraint 2), which does not hold in a small number of examples.
",3.2 From Alignments to Similarities,[0],[0]
"Sentence similarities Given the aligned chunks y, the similarity between the sentences s and t (i.e., in our notation, r) is the weighted average of the chunk similarities (i.e., zi,j,l).",3.2 From Alignments to Similarities,[0],[0]
"Formally,
r = 1 |y| ∑
(si,tj ,l)∈y αlzi,j,l. (3)
Note that the weights αl depend only on the labels associated with the alignment edge and are designed to capture the polarity and strength of the label.",3.2 From Alignments to Similarities,[0],[0]
Eq. 3 bridges sentence similarities and chunk similarities.,3.2 From Alignments to Similarities,[0],[0]
"During learning, this provides the feedback from sentence similarities to chunk similarities.",3.2 From Alignments to Similarities,[0],[0]
The values of theα’s can be learned or fixed before learning commences.,3.2 From Alignments to Similarities,[0],[0]
"To simplify our model, we choose the latter approach .",3.2 From Alignments to Similarities,[0],[0]
"Section 5 gives more details.
",3.2 From Alignments to Similarities,[0],[0]
"Features To complete the description of the model, we now describe the features that define the scoring functions.",3.2 From Alignments to Similarities,[0],[0]
"We use standard features from the STS literature (Karumuri et al., 2015; Agirre et al., 2015b; Banjade et al., 2015).
",3.2 From Alignments to Similarities,[0],[0]
"For a pair of chunks, we extract the following similarity features: (1) Absolute cosine similarities of GloVe embeddings (Pennington et al., 2014) of head words, (2) WordNet based Resnik (Resnik, 1995), Leacock (Leacock and Chodorow, 1998) and Lin (Lin, 1998) similarities of head words, (3) Jaccard similarity of content words and lemmas.",3.2 From Alignments to Similarities,[0],[0]
"In addition, we also add indicators for: (1) the part of speech tags of the pair of head words, (2) the pair of head words being present in the lexical large section of the Paraphrase Database (Ganitkevitch et al., 2013), (3) a chunk being longer than the other while both are not named entity chunks, (4) a chunk having more content words than the other, (5) contents of one chunk being a part of the other, (6) having the same named entity type or numeric words, (7) sharing synonyms or antonyms, (8) sharing conjunctions or prepositions, (9) the existence of unigram/bigram/trigram overlap, (10) if only one chunk has a negation, and (11) a chunk having extra content words that are also present in the other sentence.
",3.2 From Alignments to Similarities,[0],[0]
"For a chunk being unaligned, we conjoin an indicator that the chunk is unaligned with the part of speech tag of its head word.",3.2 From Alignments to Similarities,[0],[0]
"In the model proposed above, by predicting the alignment, we will be able to deterministically calculate both chunk and sentence level similarities.",3.3 Discussion,[0],[0]
"This is in contrast to other approaches for the STS task, which first align constituents and then extract features from alignments to predict similarities in a pipelined fashion.",3.3 Discussion,[0],[0]
"The joint prediction of alignment and similarities allows us to address the primary motivation of the paper, namely using the abundant sentence level data to train the aligner and scorer.
",3.3 Discussion,[0],[0]
The crucial assumption that drives the joint model is that the same set of parameters that can discover a good alignment can also predict similarities.,3.3 Discussion,[0],[0]
"This assumption – similar to the one made by Chang et al. (2010b) – and the associated model described above, imply that the goal of learning is to find parameters that drive the inference towards good alignments and similarities.",3.3 Discussion,[0],[0]
"Under the proposed model, the alignment directly predicts the chunk and sentence similarities as well.",4 Learning the Alignment Model,[0],[0]
"We utilize two datasets to learn the model:
1.",4 Learning the Alignment Model,[0],[0]
"The alignment dataset DA consists of fully annotated aligned chunks and respective chunk similarity scores.
",4 Learning the Alignment Model,[0],[0]
2.,4 Learning the Alignment Model,[0],[0]
"The sentence dataset DS that consists of pairs of sentences where each pair is labeled with a numeric similarity score between zero and five.
",4 Learning the Alignment Model,[0],[0]
The goal of learning is to use these two datasets to train the model parameters.,4 Learning the Alignment Model,[0],[0]
"Note that unlike standard multi-task learning problems, the two tasks in our case are tightly coupled both in terms of their definition and via the model described in Section 3.
",4 Learning the Alignment Model,[0],[0]
"We define three types of loss functions corresponding to the three components of the final output (i.e., alignment, chunk similarity and sentence similarity).",4 Learning the Alignment Model,[0],[0]
"Naturally, for each kind of loss, we assume that we have the corresponding ground truth.",4 Learning the Alignment Model,[0],[0]
We will denote ground truth similarity scores and alignments using asterisks.,4 Learning the Alignment Model,[0],[0]
"Also, the loss functions
defined below depend on the weight vector w, but this is not shown to simplify notation.
1.",4 Learning the Alignment Model,[0],[0]
The alignment loss La is a structured loss function that penalizes alignments that are far away from the ground truth.,4 Learning the Alignment Model,[0],[0]
"We used the structured hinge loss (Taskar et al., 2004; Tsochantaridis et al., 2005) for this purpose.
La(s, t,y ∗) = max y wTΦ",4 Learning the Alignment Model,[0],[0]
"(s, t,y)
+∆",4 Learning the Alignment Model,[0],[0]
"(y,y∗)−wTΦ (s, t,y∗) .
",4 Learning the Alignment Model,[0],[0]
"Here, ∆ refers to the Hamming distance between the alignments.
2.",4 Learning the Alignment Model,[0],[0]
The chunk score loss Lc is designed to penalize errors in predicted chunk level similarities.,4 Learning the Alignment Model,[0],[0]
"To account for cases where chunk boundaries may be incorrect, we define this loss as the sum of squared errors of token similarities.",4 Learning the Alignment Model,[0],[0]
"However, neither our output nor the gold standard similarities are at the granularity of tokens.",4 Learning the Alignment Model,[0],[0]
"Thus, to compute the loss, we project the chunk scores zi,j,l for an aligned chunk pair (si, tj , l) to the tokens that constitute the chunks by equally partitioning the scores among all possible internal alignments.",4 Learning the Alignment Model,[0],[0]
"In other words, for a token wi in the chunk si and token wj in chunk sj , we define token similarity scores as
z(wi, wj , l) = zi,j,l
N(si,tj)
Here, the normalizing function N is the product of the number of tokens in the chunks3.",4 Learning the Alignment Model,[0],[0]
Note that this definition of the token similarity scores applies to both predicted and gold standard similarities.,4 Learning the Alignment Model,[0],[0]
"Unaligned tokens are associated with a zero score.
",4 Learning the Alignment Model,[0],[0]
"We can now define the loss for a token pair (wi, wj) ∈ (s, t) and a label l as the squared error of their token similarity scores:
l(wi, wj , l) =",4 Learning the Alignment Model,[0],[0]
"(z(wi, wj , l)− z∗(wi, wj ,",4 Learning the Alignment Model,[0],[0]
"l))2
3Following the official evaluation of the interpretable STS task, we also experimented with the max(|si|, |tj |) for the normalizer, but we found via cross validation that the product performs better.
",4 Learning the Alignment Model,[0],[0]
"The chunk loss score Lc for a sentence pair is the sum of all the losses over all pairs of tokens and labels.
",4 Learning the Alignment Model,[0],[0]
"Lc(s, t,y,y ∗, z, z∗) =
∑
wi,wj ,l
l(wi, wj , l)
3.",4 Learning the Alignment Model,[0],[0]
The sentence similarity loss Ls provides feedback to the aligner by penalizing alignments that are far away from the ground truth in their similarity assessments.,4 Learning the Alignment Model,[0],[0]
"For a pair of sentences (s, t), given the ground truth sentence similarity r∗ and the predicted sentence similarity r (using Equation (3)), the sentence similarity loss is the squared error:
Ls(s, t, r ∗)",4 Learning the Alignment Model,[0],[0]
"= (r − r∗)2 .
",4 Learning the Alignment Model,[0],[0]
Our learning objective is the weighted combination of the above three components and a `2 regularizer on the weight vector.,4 Learning the Alignment Model,[0],[0]
"The importance of each type of loss is controlled by a corresponding hyperparameter: λa, λc and λs respectively.
",4 Learning the Alignment Model,[0],[0]
Learning algorithm,4 Learning the Alignment Model,[0],[0]
"We have two scenarios to consider: with only alignment dataset DA, and with both DA and sentence dataset DS .",4 Learning the Alignment Model,[0],[0]
"Note that even if we train only on the alignment dataset DA, our learning objective is not convex because the activation function is sigmoidal (in Section 3.1).
",4 Learning the Alignment Model,[0],[0]
"In both cases, we use stochastic gradient descent with minibatch updates as the optimizer.",4 Learning the Alignment Model,[0],[0]
"In the first scenario, we simply perform the optimization using the alignment and the chunk score losses.",4 Learning the Alignment Model,[0],[0]
"We found by preliminary experiments on training data that initializing the weights to one performed best.
",4 Learning the Alignment Model,[0],[0]
"Algorithm 1 Learning alignments and similarities, given alignment dataset DA and sentence similarity dataset DS .",4 Learning the Alignment Model,[0],[0]
"See the text for more details.
1: Initialize all weights to one.",4 Learning the Alignment Model,[0],[0]
2: w0 ← SGD(DA): Train an initial model 3: Use w0 to predict alignments on examples in DS .,4 Learning the Alignment Model,[0],[0]
Call this D̂S .,4 Learning the Alignment Model,[0],[0]
4: w ← SGD(DA ∪ D̂S): Train on both sets of examples.,4 Learning the Alignment Model,[0],[0]
"5: return w
When we have both DA and DS (Algorithm 1), we first initialize the model on the alignment data
only.",4 Learning the Alignment Model,[0],[0]
"Using this initial model, we hypothesize alignments on all examples in DS to get fully labeled examples.",4 Learning the Alignment Model,[0],[0]
"Then, we optimize the full objective (all three loss terms) on the combined dataset.",4 Learning the Alignment Model,[0],[0]
"Because our goal is to study the impact on the chunk level predictions, in the full model, the sentence loss does not play a part on examples from DA.",4 Learning the Alignment Model,[0],[0]
"The primary research question we seek to answer via experiments is: Can we better predict chunk alignments and similarities by taking advantage of sentence level similarity data?
",5 Experiments and Results,[0],[0]
"Datasets We used the training and test data from the 2016 SemEval shared tasks of predicting semantic textual similarity (Agirre et al., 2016a) and interpretable STS (Agirre et al., 2016b), that is, tasks 1 and 2 respectively.",5 Experiments and Results,[0],[0]
"For our experiments, we used the headlines and images sections of the data.",5 Experiments and Results,[0],[0]
"The data for the interpretable STS task, consisting of manually aligned and scored chunks, provides the alignment datasets for training (DA).",5 Experiments and Results,[0],[0]
"The headlines section of the training data consists for 756 sentence pairs, while the images section consists for 750 sentence pairs.",5 Experiments and Results,[0],[0]
The data for the STS task acts as our sentence level training dataset (DS).,5 Experiments and Results,[0],[0]
"For the headlines section, we used the 2013 headlines test set consisting of 750 sentence pairs with gold sentence similarity scores.",5 Experiments and Results,[0],[0]
"For the images section, we used the 2014 images test set consisting of 750 examples.",5 Experiments and Results,[0],[0]
"We evaluated our models on the official Task 2 test set, consisting of 375 sentence pairs for both the headlines and images sections.",5 Experiments and Results,[0],[0]
"In all experiments, we used gold standard chunk boundaries if they are available (i.e., for DA).
",5 Experiments and Results,[0],[0]
"Pre-processing We pre-processed the sentences with parts of speech using the Stanford CoreNLP toolkit (Manning et al., 2014).",5 Experiments and Results,[0],[0]
"Since our setting assumes that we have the chunks as input, we used the Illinois shallow parser (Clarke et al., 2012) to extract chunks from DS .",5 Experiments and Results,[0],[0]
We post-processed the predicted chunks to correct for errors using the following steps: 1.,5 Experiments and Results,[0],[0]
Split on punctuation; 2.,5 Experiments and Results,[0],[0]
Split on verbs in NP; 3.,5 Experiments and Results,[0],[0]
Split on nouns in VP; 4.,5 Experiments and Results,[0],[0]
Merge PP+NP into PP; 5.,5 Experiments and Results,[0],[0]
"Merge VP+PRT into VP if the PRT chunk is not a preposition or a subordinating
conjunction; 6.",5 Experiments and Results,[0],[0]
Merge SBAR+NP into SBAR; and 7.,5 Experiments and Results,[0],[0]
Create new contiguous chunks using tokens that are marked as being outside a chunk by the shallow parser.,5 Experiments and Results,[0],[0]
"We found that using the above postprocessing rules, improved the F1 of chunk accuracy from 0.7865 to 0.8130.",5 Experiments and Results,[0],[0]
We also found via crossvalidation that this post-processing improved overall alignment accuracy.,5 Experiments and Results,[0],[0]
"The reader may refer to other STS resources (Karumuri et al., 2015) for further improvements along this direction.
",5 Experiments and Results,[0],[0]
"Experimental setup We performed stochastic gradient descent for 200 epochs in our experiments, with a mini-batch size of 20.",5 Experiments and Results,[0],[0]
"We determined the three λ’s using cross-validation, with different hyperparameters for examples fromDA andDS .",5 Experiments and Results,[0],[0]
Table 1 lists the best hyperparameter values.,5 Experiments and Results,[0],[0]
"For performing inference, we used the Gurobi optimizer4.
",5 Experiments and Results,[0],[0]
"As noted in Section 3.1, the parameter αl combines chunk scores into sentence scores.",5 Experiments and Results,[0],[0]
"To find these hyper-parameters, we used a set of 426 sentences from the from the headlines training data that had both sentence and chunk annotation.",5 Experiments and Results,[0],[0]
We simplified the search by assuming that αEqui is always 1.0 and all labels other than OPPO have the same α.,5 Experiments and Results,[0],[0]
"Using grid search over [−1, 1] in increments of 0.1, we selected α’s that gave us the highest Pearson correlation for sentence level similarities.",5 Experiments and Results,[0],[0]
"The best α’s (with a Pearson correlation of 0.7635) were:
αl =    1, l = EQUI, −1, l = OPPO, 0.7, otherwise
Results Following the official evaluation for the SemEval task, we evaluate both alignments and their
4http://www.gurobi.com/
corresponding similarity scores.",5 Experiments and Results,[0],[0]
"The typed alignment evaluation (denoted by typed ali in the results table) measures F1 over the alignment edges where the types need to match, but scores are ignored.",5 Experiments and Results,[0],[0]
"The typed similarity evaluation (denoted by typed score) is the more stringent evaluation that measures F1 of the alignment edge labels, but penalizes them if the similarity scores do not match.",5 Experiments and Results,[0],[0]
The untyped versions of alignment and scored alignment evaluations ignore alignment labels.,5 Experiments and Results,[0],[0]
"These metrics, based on Melamed (1997), are tailored for the interpretable STS task5.",5 Experiments and Results,[0],[0]
We refer the reader to the guidelines of the task for further details.,5 Experiments and Results,[0],[0]
We report both scores in Table 2.,5 Experiments and Results,[0],[0]
"We also list the performance of the baseline system (Sultan et al., 2014a) and the top ranked systems from the 2016 shared task for each dataset6.
",5 Experiments and Results,[0],[0]
"By comparing the rows labeledDA andDA +DS in Table 2 (a) and Table 2 (b), we see that in both the headlines and the images datasets, adding sentence level information improves the untyped score, lifting the stricter typed score F1.",5 Experiments and Results,[0],[0]
"On the headlines dataset, incorporating sentence-level information degrades both the untyped and typed alignment quality because we cross-validated on the typed score metric.
",5 Experiments and Results,[0],[0]
"The typed score metric is the combination of untyped alignment, untyped score and typed alignment.",5 Experiments and Results,[0],[0]
"From the row DA +DS in Table 2(a), we observe that the typed score F1 is slightly behind that of rank 1 system while all other three metrics are significantly better, indicating that we need to improve our modeling of the intersection of the three aspects.",5 Experiments and Results,[0],[0]
"However, this does not apply to images
5In the SemEval 2016 shared task, the typed score is the metric used for system ranking.
",5 Experiments and Results,[0],[0]
"6http://alt.qcri.org/semeval2016/task2/
dataset where the improvement on the typed score F1 comes from the typed alignment.
",5 Experiments and Results,[0],[0]
"Further, we see that even our base model that only depends on the alignment data offers strong alignment F1 scores.",5 Experiments and Results,[0],[0]
This validates the utility of jointly modeling alignments and chunk similarities.,5 Experiments and Results,[0],[0]
Adding sentence data to this already strong system leads to performance that is comparable to or better than the state-of-the-art systems.,5 Experiments and Results,[0],[0]
"Indeed, our final results would have been ranked first on the images task and a close second on the headlines task in the official standings.
",5 Experiments and Results,[0],[0]
The most significant feedback coming from sentence-level information is with respect to the chunk similarity scores.,5 Experiments and Results,[0],[0]
"While we observed slight change in the unscored alignment performance, for both the headlines and the images datasets, we saw improvements in both scored precision and recall when sentence level data was used.",5 Experiments and Results,[0],[0]
"In this section, first, we report the results of manual error analysis.",6 Analysis and Discussion,[0],[0]
"Then, we study the ability of our model to handle data from different domains.",6 Analysis and Discussion,[0],[0]
"To perform a manual error analysis, we selected 40 examples from the development set of the headlines section.",6.1 Error Analysis,[0],[0]
We classified the errors made by the full model trained on the alignment and sentence datasets.,6.1 Error Analysis,[0],[0]
"Below, we report the four most significant types of errors:
1.",6.1 Error Analysis,[0],[0]
"Contextual implication: Chunks that are meant to be aligned are not synonyms by them-
selves but are implied by the context.",6.1 Error Analysis,[0],[0]
"For instance, Israeli forces and security forces might be equivalent in certain contexts.",6.1 Error Analysis,[0],[0]
"Out of the 16 instances of EQUI being misclassified as SPE, eight were caused by the features’ inability to ascertain contextual implications.",6.1 Error Analysis,[0],[0]
"This also accounted for four out of the 15 failures to identify alignments.
2.",6.1 Error Analysis,[0],[0]
"Semantic phrase understanding: These are the cases where our lexical resources failed, e. g., ablaze and left burning.",6.1 Error Analysis,[0],[0]
This accounted for ten of the 15 chunk alignment failures and nine of the 21 labeling errors.,6.1 Error Analysis,[0],[0]
"Among these, some errors (four alignment failures and four labeling errors) were much simpler than others that could be handled with relatively simple features (e.g. family reunions↔ family unions).
",6.1 Error Analysis,[0],[0]
3.,6.1 Error Analysis,[0],[0]
Preposition semantics:,6.1 Error Analysis,[0],[0]
The inability to account for preposition semantics accounts for three of the 16 cases where EQUI is mistaken as a SPE.,6.1 Error Analysis,[0],[0]
"Some examples include at 91 ↔ aged 91 and catch fire↔ after fire.
4.",6.1 Error Analysis,[0],[0]
"Underestimated EQUI score: Ten out of 14 cases of score underestimation happened on EQUI label.
",6.1 Error Analysis,[0],[0]
Our analysis suggests that we need better contextual features and phrasal features to make further gains in aligning constituents.,6.1 Error Analysis,[0],[0]
"In all the experiments in Section 5, we used sentence datasets belonging to the same domain as the alignment dataset (either headlines or images).",6.2 Does the text domain matter?,[0],[0]
"Given that our model can take advantage of two separate datasets, a natural question to ask is how the domain of the sentence dataset influences overall alignment performance.",6.2 Does the text domain matter?,[0],[0]
"Additionally, we can also ask how well the trained classifiers perform on out-ofdomain data.",6.2 Does the text domain matter?,[0],[0]
We performed a series of experiments to explore these two questions.,6.2 Does the text domain matter?,[0],[0]
"Table 3 summarizes the results of these experiments.
",6.2 Does the text domain matter?,[0],[0]
The columns labeled Train and Test of the table show the training and test sets used.,6.2 Does the text domain matter?,[0],[0]
"Each dataset can be either the headlines section (denoted by hdln), or the images section (img) or not used
(∅).",6.2 Does the text domain matter?,[0],[0]
The last two columns report performance on the test set.,6.2 Does the text domain matter?,[0],[0]
"The rows 1 and 5 in the table correspond to the in-domain settings and match the results of typed alignment and score in Table 2.
",6.2 Does the text domain matter?,[0],[0]
"When the headlines data is tested on the images section, we see that there is the usual domain adaptation problem (row 3 vs row 1) and using target images sentence data does not help (row 4 vs row 3).",6.2 Does the text domain matter?,[0],[0]
"In contrast, even though there is a domain adaptation problem when we compare the rows 5 and 7, we see that once again, using headlines sentence data improves the predicted scores (row 7 vs row 8).",6.2 Does the text domain matter?,[0],[0]
"This observation can be explained by the fact that the images sentences are relatively simpler and headlines dataset can provide richer features in comparison, thus allowing for stronger feedback from sentences to constituents.
",6.2 Does the text domain matter?,[0],[0]
The next question concerns how the domain of the sentence dataset DS influences alignment and similarity performance.,6.2 Does the text domain matter?,[0],[0]
"To answer this, we can compare the results in every pair of rows (i.e., 1 vs 2, 3 vs 4, etc.)",6.2 Does the text domain matter?,[0],[0]
"We see that when the sentence data from the image data is used in conjunction to the headlines chunk data, it invariably makes the classifiers worse.",6.2 Does the text domain matter?,[0],[0]
"In contrast, the opposite trend is observed when the headlines sentence data augments the images chunk data.",6.2 Does the text domain matter?,[0],[0]
"This can once again be explained by relatively simpler sentence constructions in the images set, suggesting that we can leverage linguistically complex corpora to improve alignment on simpler ones.",6.2 Does the text domain matter?,[0],[0]
"Indeed, surprisingly, we obtain marginally better performance on the images set when we use images chunk level data in conjunction
with the headlines sentence data (row 6 vs the row labeled DA +DS in the Table 2(b)).",6.2 Does the text domain matter?,[0],[0]
Aligning words and phrases between pairs of sentences is widely studied in NLP.,7 Related Work,[0],[0]
"Machine translation has a rich research history of using alignments (for e.g., (Koehn et al., 2003; Och and Ney, 2003)), going back to the IBM models (Brown et al., 1993).",7 Related Work,[0],[0]
"From the learning perspective, the alignments are often treated as latent variables during learning, as in this work where we treated alignments in the sentence level training examples as latent variables.",7 Related Work,[0],[0]
"Our work is also conceptually related to (Ganchev et al., 2008), which asked whether improved alignment error implied better translation.
",7 Related Work,[0],[0]
"Outside of machine translation, alignments are employed either explicitly or implicitly for recognizing textual entailment (Brockett, 2007; Chang et al., 2010a) and paraphrase recognition (Das and Smith, 2009; Chang et al., 2010a).",7 Related Work,[0],[0]
"Additionally, alignments are explored in multiple ways (tokens, phrases, parse trees and dependency graphs) as a foundation for natural logic inference (Chambers et al., 2007; MacCartney and Manning, 2007; MacCartney et al., 2008).",7 Related Work,[0],[0]
"Our proposed aligner can be used to aid such applications.
",7 Related Work,[0],[0]
"For predicting sentence similarities, in both variants of the task, word or chunk alignments have extensively been used (Sultan et al., 2015; Sultan et al., 2014a; Sultan et al., 2014b; Hänig et al., 2015; Karumuri et al., 2015; Agirre et al., 2015b; Banjade et al., 2015, and others).",7 Related Work,[0],[0]
"In contrast to these systems, we proposed a model that is trained jointly to predict alignments, chunk similarities and sentence similarities.",7 Related Work,[0],[0]
"To our knowledge, this is the first approach that combines sentence-level similarity data with fine grained alignments to train a chunk aligner.",7 Related Work,[0],[0]
"In this paper, we presented the first joint framework for aligning sentence constituents and predicting constituent and sentence similarities.",8 Conclusion,[0],[0]
We showed that our predictive model can be trained using both aligned constituent data and sentence similarity data.,8 Conclusion,[0],[0]
"Our jointly trained model achieves stateof-the-art performance on the task of predicting in-
terpretable sentence similarities.",8 Conclusion,[0],[0]
The authors wish to thank the anonymous reviewers and the members of the Utah NLP group for their valuable comments and pointers to references.,Acknowledgments,[0],[0]
We study the problem of jointly aligning sentence constituents and predicting their similarities.,abstractText,[0],[0]
"While extensive sentence similarity data exists, manually generating reference alignments and labeling the similarities of the aligned chunks is comparatively onerous.",abstractText,[0],[0]
This prompts the natural question of whether we can exploit easy-to-create sentence level data to train better aligners.,abstractText,[0],[0]
"In this paper, we present a model that learns to jointly align constituents of two sentences and also predict their similarities.",abstractText,[0],[0]
"By taking advantage of both sentence and constituent level data, we show that our model achieves state-of-the-art performance at predicting alignments and constituent similarities.",abstractText,[0],[0]
Exploiting Sentence Similarities for Better Alignments,title,[0],[0]
We consider the problem of regularized empirical risk minimization (ERM) of linear predictors.,1. Introduction,[0],[0]
"Let a1, . . .",1. Introduction,[0],[0]
", an ∈ Rd be the feature vectors of n data samples, φi : R → R be a convex loss function associated with the linear prediction aTi x, for i = 1, . . .",1. Introduction,[0],[0]
",",1. Introduction,[0],[0]
"n,",1. Introduction,[0],[0]
and g : Rd → R be a convex regularization function for the predictor x ∈ Rd.,1. Introduction,[0],[0]
"ERM amounts to solving the following convex optimization problem:
min x∈Rd
{ P (x)
",1. Introduction,[0],[0]
def = 1n,1. Introduction,[0],[0]
∑n i=1,1. Introduction,[0],[0]
φi(a,1. Introduction,[0],[0]
T i x) + g(x) } .,1. Introduction,[0],[0]
"(1)
This formulation covers many well-known classification and regression problems.",1. Introduction,[0],[0]
"For example, logistic regression is obtained by setting φi(z) = log(1 + exp(−biz))",1. Introduction,[0],[0]
where bi ∈ {±1}.,1. Introduction,[0],[0]
"For linear regression problems, the loss
1Department of Computer Science, The University of Chicago, Chicago, Illinois 60637, USA.",1. Introduction,[0],[0]
"2Microsoft Research, Redmond, Washington 98052, USA.",1. Introduction,[0],[0]
"Correspondence to: Jialei Wang <jialei@uchicago.edu>, Lin Xiao <lin.xiao@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
function is φi(z) =",1. Introduction,[0],[0]
"(1/2)(z − bi)2, and we get ridge regression with g(x) =",1. Introduction,[0],[0]
(λ/2)‖x‖22 and the elastic net with g(x),1. Introduction,[0],[0]
= λ1‖x‖1,1. Introduction,[0],[0]
+ (λ2/2)‖x‖22.,1. Introduction,[0],[0]
LetA =,1. Introduction,[0],[0]
"[a1, . . .",1. Introduction,[0],[0]
", an]T be the n by d data matrix.",1. Introduction,[0],[0]
"Throughout this paper, we make the following assumptions: Assumption 1.",1. Introduction,[0],[0]
"The functions φi, g and matrix A satisfy:
•",1. Introduction,[0],[0]
"Each φi is δ-strongly convex and 1/γ-smooth where γ > 0 and δ ≥ 0, and γδ ≤ 1;
• g is λ-strongly convex where λ ≥ 0;",1. Introduction,[0],[0]
"• λ+ δµ2 > 0, where µ = √ λmin(ATA).
",1. Introduction,[0],[0]
"The strong convexity and smoothness mentioned above are with respect to the standard Euclidean norm, denoted as ‖x‖ = √ xTx.",1. Introduction,[0],[0]
"(See, e.g., Nesterov (2004, Sections 2.1.1 and 2.1.3) for the exact definitions.)",1. Introduction,[0],[0]
"We allow δ = 0, which simply means φi is convex.",1. Introduction,[0],[0]
"Let R = maxi{‖ai‖} and assuming λ > 0, then R2/(γλ) is a popular definition of condition number for analyzing complexities of different algorithms.",1. Introduction,[0],[0]
"The last condition above means that the primal objective function P (x) is strongly convex, even if λ = 0.
",1. Introduction,[0],[0]
There have been extensive research activities in recent years on developing efficiently algorithms for solving problem (1).,1. Introduction,[0],[0]
A broad class of randomized algorithms that exploit the finite sum structure in the ERM problem have emerged as very competitive both in terms of theoretical complexity and practical performance.,1. Introduction,[0],[0]
"They can be put into three categories: primal, dual, and primal-dual.
",1. Introduction,[0],[0]
Primal randomized algorithms work with the ERM problem (1) directly.,1. Introduction,[0],[0]
"They are modern versions of randomized incremental gradient methods (e.g., Bertsekas, 2012; Nedic & Bertsekas, 2001) equipped with variance reduction techniques.",1. Introduction,[0],[0]
Each iteration of such algorithms only process one data point ai with complexity O(d).,1. Introduction,[0],[0]
"They includes SAG (Roux et al., 2012), SAGA (Defazio et al., 2014), and SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), which all achieve the iteration complexity O ( (n+R2/(γλ))",1. Introduction,[0],[0]
log(1/ǫ) ) to find an ǫoptimal solution.,1. Introduction,[0],[0]
"In fact, they are capable of exploiting the strong convexity from data, meaning that the condition number R2/(γλ) in the complexity can be replaced by the more favorable oneR2/(γ(λ+δµ2/n)).",1. Introduction,[0],[0]
"This improvement can be achieved without explicit knowledge of µ from data.
",1. Introduction,[0],[0]
"Dual algorithms solve Fenchel dual of (1) by maximizing
D(y) def = 1n",1. Introduction,[0],[0]
∑n i=1,1. Introduction,[0],[0]
−φ∗i (yi)− g∗ ( − 1n ∑n i=1 yiai ),1. Introduction,[0],[0]
(2) using randomized coordinate ascent algorithms.,1. Introduction,[0],[0]
(Here φ∗i and g∗ denotes the conjugate functions of φi and g.),1. Introduction,[0],[0]
"They include SDCA (Shalev-Shwartz & Zhang, 2013), Nesterov (2012) and Richtárik & Takáč (2014).",1. Introduction,[0],[0]
They have the same complexity O ( (n+R2/(γλ)),1. Introduction,[0],[0]
"log(1/ǫ) ) , but cannot exploit strong convexity, if any (when δµ2 > 0), from data.
",1. Introduction,[0],[0]
"Primal-dual algorithms solve the convex-concave saddle point problem minxmaxy L(x, y) where
L(x, y) def=",1. Introduction,[0],[0]
1n,1. Introduction,[0],[0]
∑n i=1,1. Introduction,[0],[0]
"( yi〈ai, x〉 − φ∗i (yi) ) + g(x).",1. Introduction,[0],[0]
"(3)
In particular, SPDC (Zhang & Xiao, 2015) achieves an accelerated linear convergence rate with iteration complexity O ( (n+ √ nR/ √ γλ) log(1/ǫ) ) , which is better than the aforementioned non-accelerated complexity when R2/(γλ)",1. Introduction,[0],[0]
"> n. Lan & Zhou (2015) developed dual-free variants of accelerated primal-dual algorithms, but without considering the linear predictor structure in ERM.",1. Introduction,[0],[0]
"Balamurugan & Bach (2016) extended SVRG and SAGA to solving saddle point problems.
",1. Introduction,[0],[0]
Accelerated primal and dual randomized algorithms have also been developed.,1. Introduction,[0],[0]
"Nesterov (2012), Fercoq & Richtárik (2015) and Lin et al. (2015b) developed accelerated coordinate gradient algorithms, which can be applied to solve the dual problem (2).",1. Introduction,[0],[0]
Allen-Zhu (2016) developed an accelerated variant of SVRG.,1. Introduction,[0],[0]
"Acceleration can also be obtained using the Catalyst framework (Lin et al., 2015a).",1. Introduction,[0],[0]
They all achieve the same O ( (n+ √ nR/ √ γλ) log(1/ǫ) ) complexity.,1. Introduction,[0],[0]
A common feature of accelerated algorithms is that they require good estimate of the strong convexity parameter.,1. Introduction,[0],[0]
"This makes hard for them to exploit strong convexity from data because the minimum singular value µ of the data matrix A is very hard to estimate in general.
",1. Introduction,[0],[0]
"In this paper, we show that primal-dual algorithms are capable of exploiting strong convexity from data if the algorithm parameters (such as step sizes) are set appropriately.",1. Introduction,[0],[0]
"While these optimal setting depends on the knowledge of the convexity parameter µ from the data, we develop adaptive variants of primal-dual algorithms that can tune the parameter automatically.",1. Introduction,[0],[0]
"Such adaptive schemes rely critically on the capability of evaluating the primal-dual optimality gaps by primal-dual algorithms.
",1. Introduction,[0],[0]
A major disadvantage of primal-dual algorithms is that the required dual proximal mapping may not admit closedform or efficient solution.,1. Introduction,[0],[0]
"We follow the approach of Lan & Zhou (2015) to derive dual-free variants of the primal-dual algorithms customized for ERM problems with the linear predictor structure, and show that they can also exploit strong convexity from data with correct choices of parameters or using an adaptation scheme.
",1. Introduction,[0],[0]
"Algorithm 1 Batch Primal-Dual (BPD) Algorithm input: parameters τ , σ, θ, initial point (x̃(0) = x(0), y(0))
for t = 0, 1, 2, . . .",1. Introduction,[0],[0]
"do y(t+1) = proxσf∗ ( y(t) + σAx̃(t) )
x(t+1) = proxτg ( x(t)",1. Introduction,[0],[0]
"− τAT y(t+1) )
x̃(t+1)",1. Introduction,[0],[0]
= x(t+1),1. Introduction,[0],[0]
+ θ(x(t+1),1. Introduction,[0],[0]
− x(t)) end for,1. Introduction,[0],[0]
"We first study batch primal-dual algorithms, by considering a “batch” version of the ERM problem (1),
minx∈Rd { P (x) def = f(Ax) + g(x) } .",2. Batch primal-dual algorithms,[0],[0]
"(4)
where A ∈ Rn×d.",2. Batch primal-dual algorithms,[0],[0]
We make the following assumptions: Assumption 2.,2. Batch primal-dual algorithms,[0],[0]
"The functions f , g and matrix A satisfy:
• f is δ-strongly convex and 1/γ-smooth where γ > 0 and δ ≥ 0, and γδ ≤ 1;
• g is λ-strongly convex where λ ≥ 0;",2. Batch primal-dual algorithms,[0],[0]
"• λ+ δµ2 > 0, where µ = √ λmin(ATA).
",2. Batch primal-dual algorithms,[0],[0]
"Using conjugate functions, we can derive the dual of (4) as
maxy∈Rn { D(y) def = −f∗(y)− g∗(−AT y) } , (5)
and the convex-concave saddle point formulation is
min x∈Rd max y∈Rn
{ L(x, y) def= g(x)",2. Batch primal-dual algorithms,[0],[0]
+,2. Batch primal-dual algorithms,[0],[0]
yTAx− f∗(y) } .,2. Batch primal-dual algorithms,[0],[0]
"(6)
We consider the primal-dual first-order algorithm proposed by Chambolle & Pock (2011; 2016) for solving the saddle point problem (6), given in Algorithm 1, where proxψ(·), for any convex function ψ",2. Batch primal-dual algorithms,[0],[0]
": Rn ∪ {∞}, is defined as
proxψ(β) = arg min α∈Rn
( ψ(α)",2. Batch primal-dual algorithms,[0],[0]
+,2. Batch primal-dual algorithms,[0],[0]
"(1/2)‖α− β‖2 ) .
",2. Batch primal-dual algorithms,[0],[0]
"Assuming that f is smooth and g is strongly convex, Chambolle & Pock (2011; 2016) showed that Algorithm 1 achieves accelerated linear convergence rate if λ > 0.",2. Batch primal-dual algorithms,[0],[0]
"However, they did not consider the case where additional or the sole source of strong convexity comes from f(Ax).",2. Batch primal-dual algorithms,[0],[0]
"In the following theorem, we show how to set the parameters τ , σ and θ to exploit both sources of strong convexity to achieve fast linear convergence.
",2. Batch primal-dual algorithms,[0],[0]
Theorem 1.,2. Batch primal-dual algorithms,[0],[0]
"Suppose Assumption 2 holds and (x⋆, y⋆) is the unique saddle point of L defined in (6).",2. Batch primal-dual algorithms,[0],[0]
Let L = ‖A‖ =√ λmax(ATA).,2. Batch primal-dual algorithms,[0],[0]
"If we set the parameters in Algorithm 1 as
σ = 1L
√ λ+δµ2
γ , τ = 1 L
√ γ
λ+δµ2 , (7)
and θ = max{θx, θy} where
θx = ( 1− δ(δ+2σ) µ2 L2 ) 1 1+τλ , θy = 1 1+σγ/2 , (8)
then we have (
1 2τ + λ 2 ) ‖x(t)",2. Batch primal-dual algorithms,[0],[0]
− x⋆‖2 + γ4 ‖y(t),2. Batch primal-dual algorithms,[0],[0]
"− y⋆‖2 ≤ θtC,
L(x(t), y⋆)− L(x⋆, y(t))",2. Batch primal-dual algorithms,[0],[0]
"≤ θtC,
whereC = (
1 2τ + λ 2
)",2. Batch primal-dual algorithms,[0],[0]
"‖x(0)−x⋆‖2+ ( 1 2σ+ γ 4 ) ‖y(0)−y⋆‖2.
",2. Batch primal-dual algorithms,[0],[0]
The proof of Theorem 1 is given in Appendices B and C. Here we give a detailed analysis of the convergence rate.,2. Batch primal-dual algorithms,[0],[0]
Substituting σ,2. Batch primal-dual algorithms,[0],[0]
"and τ in (7) into the expressions for θy and θx in (8), and assuming γ(λ+ δµ2) ≪ L2, we have
θx ≈ 1− γδµ 2
L2
( 2 √ γ(λ+δµ2)",2. Batch primal-dual algorithms,[0],[0]
L + γδ )−1,2. Batch primal-dual algorithms,[0],[0]
"− λL √ γ λ+δµ2 ,
θy = 1 1+ √ γ(λ+δµ2)/(2L)
",2. Batch primal-dual algorithms,[0],[0]
"≈ 1− √ γ(λ+δµ2)
2L .
",2. Batch primal-dual algorithms,[0],[0]
"Since the overall condition number of the problem is L2 γ(λ+δµ2) , it is clear that θy is an accelerated convergence rate.",2. Batch primal-dual algorithms,[0],[0]
"Next we examine θx in two special cases.
",2. Batch primal-dual algorithms,[0],[0]
The case of δµ2 = 0,2. Batch primal-dual algorithms,[0],[0]
but λ > 0.,2. Batch primal-dual algorithms,[0],[0]
"In this case, we have τ = 1L √ γ λ",2. Batch primal-dual algorithms,[0],[0]
"and σ = 1 L √ λ γ , and thus
θx= 1 1+ √ γλ/L",2. Batch primal-dual algorithms,[0],[0]
≈,2. Batch primal-dual algorithms,[0],[0]
1−,2. Batch primal-dual algorithms,[0],[0]
"√ γλ L , θy= 1 1+ √ γλ/(2L)",2. Batch primal-dual algorithms,[0],[0]
≈ 1−,2. Batch primal-dual algorithms,[0],[0]
√ γλ 2L .,2. Batch primal-dual algorithms,[0],[0]
"Therefore we have θ = max{θx, θy} ≈ 1 − √ λγ 2L .",2. Batch primal-dual algorithms,[0],[0]
"This indeed is an accelerated convergence rate, recovering the result of Chambolle & Pock (2011; 2016).
",2. Batch primal-dual algorithms,[0],[0]
The case of λ = 0 but δµ2 > 0.,2. Batch primal-dual algorithms,[0],[0]
"In this case, we have τ = 1Lµ √ γ δ",2. Batch primal-dual algorithms,[0],[0]
"and σ = µ L √ δ γ , and
θx = 1− γδµ 2 L2 · 12√γδµ/L+γδ , θy ≈ 1− √ γδµ 2L .
",2. Batch primal-dual algorithms,[0],[0]
"Notice that 1γδ L2
µ2 is the condition number of f(Ax).",2. Batch primal-dual algorithms,[0],[0]
"Next we assume µ≪ L and examine how θx varies with γδ.
•",2. Batch primal-dual algorithms,[0],[0]
"If γδ ≈ µ2L2 , meaning f is badly conditioned, then
θx ≈ 1− γδµ 2 L2 · 13√γδµ/L = 1− √ γδµ 3L .
",2. Batch primal-dual algorithms,[0],[0]
"Because the overall condition number is 1γδ L2
µ2 , this is an accelerated linear rate, and so is θ = max{θx, θy}.
",2. Batch primal-dual algorithms,[0],[0]
•,2. Batch primal-dual algorithms,[0],[0]
"If γδ ≈ µL , meaning f is mildly conditioned, then
θx ≈ 1− µ 3 L3 1 2(µ/L)3/2+µ/L ≈ 1− µ2L2 .
",2. Batch primal-dual algorithms,[0],[0]
"This represents a half-accelerated rate, because the overall condition number is 1γδ L2 µ2 ≈ L 3 µ3 .
",2. Batch primal-dual algorithms,[0],[0]
•,2. Batch primal-dual algorithms,[0],[0]
"If γδ = 1, i.e., f is a simple quadratic function, then
θx ≈ 1− µ 2 L2 1 2µ/L+1 ≈ 1− µ2 L2 .
",2. Batch primal-dual algorithms,[0],[0]
"This rate does not have acceleration, because the overall condition number is 1γδ L2 µ2 ≈ L 2 µ2 .
",2. Batch primal-dual algorithms,[0],[0]
"Algorithm 2 Adaptive Batch Primal-Dual (Ada-BPD) input: problem constants λ, γ, δ, L and µ̂ > 0, initial
point (x(0), y(0)), and adaptation period T .",2. Batch primal-dual algorithms,[0],[0]
"Compute σ, τ , and θ as in (7) and (8) using µ = µ̂ for t = 0, 1, 2, . . .",2. Batch primal-dual algorithms,[0],[0]
"do y(t+1) = proxσf∗ ( y(t) + σAx̃(t) )
x(t+1) = proxτg ( x(t)",2. Batch primal-dual algorithms,[0],[0]
− τAT y(t+1) ) x̃(t+1),2. Batch primal-dual algorithms,[0],[0]
= x(t+1),2. Batch primal-dual algorithms,[0],[0]
+ θ(x(t+1),2. Batch primal-dual algorithms,[0],[0]
"− x(t)) if mod(t+ 1, T )",2. Batch primal-dual algorithms,[0],[0]
"== 0 then
(σ, τ, θ) = BPD-Adapt ( {P (s), D(s)}t+1s=t−T )
end if end for
Algorithm 3 BPD-Adapt (simple heuristic) input: previous estimate µ̂, adaption period T , primal and
dual objective values {P (s), D(s)}ts=t−T if P (t) −D(t)",2. Batch primal-dual algorithms,[0],[0]
< θT (P (t−T ) −D(t−T )),2. Batch primal-dual algorithms,[0],[0]
"then µ̂ := √ 2µ̂ else µ̂ := µ̂/ √ 2 end if Compute σ, τ , and θ as in (7) and (8) using µ = µ̂
output: new parameters (σ, τ, θ)
",2. Batch primal-dual algorithms,[0],[0]
"In summary, the extent of acceleration in the dominating factor θx (which determines θ) depends on the relative size of γδ and µ2/L2, i.e., the relative conditioning between the function f and the matrix A.",2. Batch primal-dual algorithms,[0],[0]
"In general, we have full acceleration if γδ ≤",2. Batch primal-dual algorithms,[0],[0]
µ2/L2.,2. Batch primal-dual algorithms,[0],[0]
The theory predicts that the acceleration degrades as the function f gets better conditioned.,2. Batch primal-dual algorithms,[0],[0]
"However, in our numerical experiments, we often observe acceleration even if γδ gets closer to 1.
",2. Batch primal-dual algorithms,[0],[0]
"As explained in Chambolle & Pock (2011), Algorithm 1 is equivalent to a preconditioned ADMM.",2. Batch primal-dual algorithms,[0],[0]
"Deng & Yin (2016) characterized various conditions for ADMM to obtain linear convergence, but did not derive the convergence rate for the case we consider in this paper.",2. Batch primal-dual algorithms,[0],[0]
"In practice, it is often very hard to obtain a good estimate of the problem-dependent constants, especially µ =√ λmin(ATA), in order to apply the algorithmic parameters specified in Theorem 1.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"Here we explore heuristics that can enable adaptive tuning of such parameters, which often lead to much improved performance in practice.
",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"A key observation is that the convergence rate of the BPD algorithm changes monotonically with the overall convexity parameter λ + δµ2, regardless of the extent of acceleration.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"In other words, the larger λ + δµ2 is, the faster the convergence.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"Therefore, if we can monitor the progress of
Algorithm 4 BPD-Adapt (robust heuristic) input: previous rate estimate ρ > 0, ∆ = δµ̂2, period T ,
constants c < 1 and c > 1, and {P (s), D(s)}ts=t−T Compute new rate estimate ρ̂ = P
(t)−D(t)",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"P (t−T )−D(t−T )
",2.1. Adaptive batch primal-dual algorithms,[0],[0]
if ρ̂ ≤,2.1. Adaptive batch primal-dual algorithms,[0],[0]
"c ρ then ∆ := 2∆, ρ := ρ̂ else if ρ̂ ≥ c ρ then ∆ := ∆/2, ρ := ρ̂ else ∆ := ∆ end if σ = 1L √ λ+∆",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"γ , τ = 1 L √ γ λ+∆
Compute θ using (8) or set θ = 1 output: new parameters (σ, τ, θ)
the convergence and compare it with the predicted convergence rate in Theorem 1, then we can adjust the estimated parameters to exploit strong convexity from data.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"More specifically, if the observed convergence rate is slower than the predicted rate, then we should reduce the estimate of µ; otherwise we should increase µ for faster convergence.
",2.1. Adaptive batch primal-dual algorithms,[0],[0]
We formalize the above reasoning in Algorithm 2 (called Ada-BPD).,2.1. Adaptive batch primal-dual algorithms,[0],[0]
"This algorithm maintains an estimate µ̂ of the true constant µ, and adjust it every T iterations.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"We use P (t) and D(t) to represent the primal and dual objective values at P (x(t)) and D(y(t)), respectively.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"We give two implementations of the tuning procedure BPD-Adapt: Algorithm 3 is a simple heuristic for tuning the estimate µ̂, where the increasing and decreasing factor √ 2 can be changed to other values larger than 1.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
Algorithm 4 is a more robust heuristic.,2.1. Adaptive batch primal-dual algorithms,[0],[0]
It does not rely on the specific convergence rate θ established in Theorem 1.,2.1. Adaptive batch primal-dual algorithms,[0],[0]
"Instead, it simply compares the current estimate of objective reduction rate ρ̂ with the previous estimate ρ.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"It also specifies a non-tuning range of changes in ρ, specified by the interval [c, c].
",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"The capability of accessing both the primal and dual objective values allows primal-dual algorithms to have good estimate of the convergence rate, which enables effective tuning heuristics.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"Automatic tuning of primal-dual algorithms have also been studied by, e.g., Malitsky & Pock (2016) and Goldstein et al. (2013), but with different goals.",2.1. Adaptive batch primal-dual algorithms,[0],[0]
"In this section, we come back to the ERM problem and consider its saddle-point formulation in (3).",3. Randomized primal-dual algorithm,[0],[0]
"Due to its finite sum structure in the dual variables yi, we can develope randomized algorithms to exploit strong convexity from data.",3. Randomized primal-dual algorithm,[0],[0]
"In particular, we extend the stochastic primal-dual coordinate (SPDC) algorithm by Zhang & Xiao (2015).",3. Randomized primal-dual algorithm,[0],[0]
"SPDC is
Algorithm 5 Adaptive SPDC (Ada-SPDC) input: parameters σ, τ , θ > 0, initial point (x(0), y(0)),
and adaptation period T .",3. Randomized primal-dual algorithm,[0],[0]
"Set x̃(0) = x(0) for t = 0, 1, 2, . . .",3. Randomized primal-dual algorithm,[0],[0]
"do pick k ∈ {1, . . .",3. Randomized primal-dual algorithm,[0],[0]
", n}",3. Randomized primal-dual algorithm,[0],[0]
"uniformly at random for i ∈ {1, . . .",3. Randomized primal-dual algorithm,[0],[0]
", n} do
if i == k then y (t+1) k = proxσφ∗k ( y (t) k + σa T k x̃ (t) ) else y (t+1)",3. Randomized primal-dual algorithm,[0],[0]
"i = y (t) i
end if end for
x(t+1)",3. Randomized primal-dual algorithm,[0],[0]
"= proxτg
( x(t)− τ",3. Randomized primal-dual algorithm,[0],[0]
"( u(t)+ (y
(t+1) k −y (t) k )",3. Randomized primal-dual algorithm,[0],[0]
"ak
))
u(t+1) = u(t) + 1n",3. Randomized primal-dual algorithm,[0],[0]
(y (t+1) k,3. Randomized primal-dual algorithm,[0],[0]
− y (t) k )ak x̃(t+1) = x(t+1),3. Randomized primal-dual algorithm,[0],[0]
+ θ(x(t+1),3. Randomized primal-dual algorithm,[0],[0]
"− x(t)) if mod(t+ 1, T · n) = 0",3. Randomized primal-dual algorithm,[0],[0]
"then
(τ, σ, θ) = SPDC-Adapt ( {P (t−sn), D(t−sn)}Ts=0 )
end if end for
a special case of the Ada-SPDC algorithm in Algorithm 5, by setting the adaption period T = ∞ (no adaption).",3. Randomized primal-dual algorithm,[0],[0]
The following theorem is proved in Appendix E. Theorem 2.,3. Randomized primal-dual algorithm,[0],[0]
Suppose Assumption 1 holds.,3. Randomized primal-dual algorithm,[0],[0]
"Let (x⋆, y⋆) be the saddle point of the function L defined in (3), and R = max{‖a1‖, . . .",3. Randomized primal-dual algorithm,[0],[0]
", ‖an‖}.",3. Randomized primal-dual algorithm,[0],[0]
"If we set T = ∞ in Algorithm 5 (no adaption) and let
τ = 14R
√ γ
nλ+δµ2 , σ",3. Randomized primal-dual algorithm,[0],[0]
"= 1 4R
√ nλ+δµ2
γ , (9)
and θ = max{θx, θy} where
θx = ( 1− τσδµ22n(σ+4δ) )",3. Randomized primal-dual algorithm,[0],[0]
"1 1+τλ , θy = 1+((n−1)/n)σγ/2 1+σγ/2 , (10)
then we have (
1 2τ + λ 2
)",3. Randomized primal-dual algorithm,[0],[0]
E [ ‖x(t),3. Randomized primal-dual algorithm,[0],[0]
− x⋆‖2 ] + γ4E [ ‖y(t),3. Randomized primal-dual algorithm,[0],[0]
"− y⋆‖2 ] ≤ θtC,
E [ L(x(t), y⋆)− L(x⋆, y(t))",3. Randomized primal-dual algorithm,[0],[0]
"] ≤ θtC,
whereC = (
1 2τ + λ 2
)",3. Randomized primal-dual algorithm,[0],[0]
"‖x(0)−x⋆‖2+ ( 1 2σ+ γ 4 ) ‖y(0)−y⋆‖2.
",3. Randomized primal-dual algorithm,[0],[0]
"The expectation E[·] is taken with respect to the history of random indices drawn at each iteration.
",3. Randomized primal-dual algorithm,[0],[0]
"Below we give a detailed discussion on the expected convergence rate established in Theorem 2.
",3. Randomized primal-dual algorithm,[0],[0]
The cases of σµ2 = 0 but λ > 0.,3. Randomized primal-dual algorithm,[0],[0]
In this case we have τ = 14R √ γ nλ,3. Randomized primal-dual algorithm,[0],[0]
"and σ = 1 4R √ nλ γ , and
θx = 1 1+τλ = 1− 11+4R√n/(λγ) ,
θy = 1+((n−1)/n)σγ/2 1+σγ/2 = 1− 1n+8R√n/(λγ) .
",3. Randomized primal-dual algorithm,[0],[0]
Hence θ = θy .,3. Randomized primal-dual algorithm,[0],[0]
"These recover the parameters and convergence rate of the standard SPDC (Zhang & Xiao, 2015).
",3. Randomized primal-dual algorithm,[0],[0]
The cases of σµ2 > 0,3. Randomized primal-dual algorithm,[0],[0]
but λ = 0.,3. Randomized primal-dual algorithm,[0],[0]
In this case we have τ = 14Rµ,3. Randomized primal-dual algorithm,[0],[0]
√ γ δ,3. Randomized primal-dual algorithm,[0],[0]
"and σ = µ 4R √ δ γ , and
θx = 1− τσδµ 2 2n(σ+4δ)",3. Randomized primal-dual algorithm,[0],[0]
= 1− γδµ2 32nR2 · 1√γδµ/(4R)+4γδ .,3. Randomized primal-dual algorithm,[0],[0]
θy,3. Randomized primal-dual algorithm,[0],[0]
= 1− 1n+8nR/(µ√γδ),3. Randomized primal-dual algorithm,[0],[0]
"≈ 1− √ γδµ 8nR ( 1 + √ γδµ 8R )−1 .
",3. Randomized primal-dual algorithm,[0],[0]
"Since the objective is R2/γ-smooth and δµ2/n-strongly convex, θy is an accelerated rate if √ γδµ 8R ≪ 1 (otherwise θy ≈ 1− 1n ).",3. Randomized primal-dual algorithm,[0],[0]
"For θx, we consider different situations:
•",3. Randomized primal-dual algorithm,[0],[0]
"If µ ≥ R, then we have θx ≈ 1− √ γδµ nR , which is an
accelerated rate.",3. Randomized primal-dual algorithm,[0],[0]
"So is θ = max{θx, θy}.",3. Randomized primal-dual algorithm,[0],[0]
•,3. Randomized primal-dual algorithm,[0],[0]
If µ < R and γδ,3. Randomized primal-dual algorithm,[0],[0]
"≈ µ2R2 , then θx",3. Randomized primal-dual algorithm,[0],[0]
"≈ 1− √ γδµ nR , which
represents an accelerated rate.",3. Randomized primal-dual algorithm,[0],[0]
"The iteration complexity of SPDC is Õ( nR
µ √ γδ ), which is better than that of
SVRG in this case, which is Õ( nR 2
γδµ2 ).
",3. Randomized primal-dual algorithm,[0],[0]
•,3. Randomized primal-dual algorithm,[0],[0]
If µ < R and γδ,3. Randomized primal-dual algorithm,[0],[0]
≈ µR,3. Randomized primal-dual algorithm,[0],[0]
", then we get θx",3. Randomized primal-dual algorithm,[0],[0]
"≈ 1− µ2
nR2 .",3. Randomized primal-dual algorithm,[0],[0]
"This is a half-accelerated rate, because in this case SVRG requires Õ(nR 3
µ3 ) iterations, versus Õ",3. Randomized primal-dual algorithm,[0],[0]
"( nR2 µ2 ) for SPDC.
",3. Randomized primal-dual algorithm,[0],[0]
•,3. Randomized primal-dual algorithm,[0],[0]
If µ < R and γδ,3. Randomized primal-dual algorithm,[0],[0]
"≈ 1, meaning the φi’s are well conditioned, then we get θx",3. Randomized primal-dual algorithm,[0],[0]
≈ 1,3. Randomized primal-dual algorithm,[0],[0]
− γδµ 2 nR2,3. Randomized primal-dual algorithm,[0],[0]
"≈ 1 − µ2
nR2 , which is a non-accelerated rate.",3. Randomized primal-dual algorithm,[0],[0]
The corresponding iteration complexity is the same as SVRG.,3. Randomized primal-dual algorithm,[0],[0]
"The SPDC-Adapt procedure called in Algorithm 5 follows the same logics as the batch adaption schemes in Algorithms 3 and 4, and we omit the details here.",3.1. Parameter adaptation for SPDC,[0],[0]
"One thing we emphasize here is that the adaptation period T is in terms of epochs, or number of passes over the data.",3.1. Parameter adaptation for SPDC,[0],[0]
"In addition, we only compute the primal and dual objective values after each pass or every few passes, because computing them exactly usually need to take a full pass of the data.
",3.1. Parameter adaptation for SPDC,[0],[0]
"Unlike the batch case where the duality gap decreases monotonically, the duality gap for randomized algorithms can fluctuate wildly.",3.1. Parameter adaptation for SPDC,[0],[0]
So instead of using only the two end values P (t−Tn),3.1. Parameter adaptation for SPDC,[0],[0]
"− D(t−Tn) and P (t) − D(t), we can use more points to estimate the convergence rate through a linear regression.",3.1. Parameter adaptation for SPDC,[0],[0]
"Suppose the primaldual objective values for the last T + 1 passes are (P (0), D(0)), (P (1), D(1)), . . .",3.1. Parameter adaptation for SPDC,[0],[0]
", (P (T ), D(T )), and we need to estimate ρ (rate per pass) such that
P (t)−D(t)",3.1. Parameter adaptation for SPDC,[0],[0]
"≈ ρt ( P (0)−D(0) ) , t = 1, . . .",3.1. Parameter adaptation for SPDC,[0],[0]
", T.
We can turn it into a linear regression problem after taking logarithm and obtain the estimate ρ̂",3.1. Parameter adaptation for SPDC,[0],[0]
"through
log(ρ̂) = 112+22+···+T 2",3.1. Parameter adaptation for SPDC,[0],[0]
∑T t=1,3.1. Parameter adaptation for SPDC,[0],[0]
"t log P (t)−D(t) P (0)−D(0) .
",3.1. Parameter adaptation for SPDC,[0],[0]
"Algorithm 6 Dual-Free BPD Algorithm input: parameters σ, τ , θ > 0, initial point (x(0), y(0))
",3.1. Parameter adaptation for SPDC,[0],[0]
Set x̃(0) = x(0) and v(0) =,3.1. Parameter adaptation for SPDC,[0],[0]
"(f∗)′(y(0)) for t = 0, 1, 2, . . .",3.1. Parameter adaptation for SPDC,[0],[0]
do v(t+1) = v,3.1. Parameter adaptation for SPDC,[0],[0]
"(t)+σAx̃(t)
1+σ , y (t+1) = f ′(v(t+1))
",3.1. Parameter adaptation for SPDC,[0],[0]
x(t+1),3.1. Parameter adaptation for SPDC,[0],[0]
= proxτg ( x(t),3.1. Parameter adaptation for SPDC,[0],[0]
"− τAT y(t+1) )
x̃(t+1)",3.1. Parameter adaptation for SPDC,[0],[0]
= x(t+1),3.1. Parameter adaptation for SPDC,[0],[0]
+ θ(x(t+1),3.1. Parameter adaptation for SPDC,[0],[0]
− x(t)) end for,3.1. Parameter adaptation for SPDC,[0],[0]
"Compared with primal algorithms, one major disadvantage of primal-dual algorithms is the requirement of computing the proximal mapping of the dual function f∗ or φ∗i , which may not admit closed-formed solution or efficient computation.",4. Dual-free Primal-dual algorithms,[0],[0]
"This is especially the case for logistic regression, one of the most popular loss functions used in classification.
",4. Dual-free Primal-dual algorithms,[0],[0]
Lan & Zhou (2015) developed “dual-free” variants of primal-dual algorithms that avoid computing the dual proximal mapping.,4. Dual-free Primal-dual algorithms,[0],[0]
Their main technique is to replace the Euclidean distance in the dual proximal mapping with a Bregman divergence defined over the dual loss function itself.,4. Dual-free Primal-dual algorithms,[0],[0]
We show how to apply this approach to solve the structured ERM problems considered in this paper.,4. Dual-free Primal-dual algorithms,[0],[0]
They can also exploit strong convexity from data if the algorithmic parameters are set appropriately or adapted automatically.,4. Dual-free Primal-dual algorithms,[0],[0]
"First, we consider the batch setting.",4.1. Dual-free BPD algorithm,[0],[0]
"We replace the dual proximal mapping (computing y(t+1)) in Algorithm 1 with
y(t+1)=argmin y
{ f∗(y)−yTAx̃(t)+ 1σD(y, y(t)) } , (11)
where D is the Bregman divergence of a strictly convex kernel function h, defined as
Dh(y, y(t))",4.1. Dual-free BPD algorithm,[0],[0]
"= h(y)− h(y(t))− 〈∇h(y(t)), y − y(t)〉.",4.1. Dual-free BPD algorithm,[0],[0]
"Algorithm 1 is obtained in the Euclidean setting with h(y) = 12‖y‖2 and D(y, y(t))",4.1. Dual-free BPD algorithm,[0],[0]
= 12‖y−y(t)‖2.,4.1. Dual-free BPD algorithm,[0],[0]
"Here we use f∗ as the kernel function, and show that it allows us to compute y(t+1) in (11) very efficiently.",4.1. Dual-free BPD algorithm,[0],[0]
"The following lemma explains the details (Cf. Lan & Zhou, 2015, Lemma 1).",4.1. Dual-free BPD algorithm,[0],[0]
Lemma 1.,4.1. Dual-free BPD algorithm,[0],[0]
"Let the kernel h ≡ f∗ in the Bregman divergence D. If we construct a sequence of vectors {v(t)} such that v(0) = (f∗)′(y(0)) and for all t ≥ 0,
v(t+1) = v (t)+σAx̃(t)
1+σ , (12)
then the solution to problem (11) is y(t+1) = f ′(v(t+1)).
",4.1. Dual-free BPD algorithm,[0],[0]
Proof.,4.1. Dual-free BPD algorithm,[0],[0]
Suppose v(t) = (f∗)′(y(t)),4.1. Dual-free BPD algorithm,[0],[0]
"(true for t = 0), then
D(y, y(t))",4.1. Dual-free BPD algorithm,[0],[0]
"= f∗(y)− f∗(y(t))− v(t)T (y − y(t)).
",4.1. Dual-free BPD algorithm,[0],[0]
"The solution to (11) can be written as
y(t+1)= argmin y
{",4.1. Dual-free BPD algorithm,[0],[0]
"f∗(y)−yTAx̃(t)+ 1σ ( f∗(y)−v(t)T y )}
= argmin y
{( 1 + 1σ ) f∗(y)− ( Ax̃(t)",4.1. Dual-free BPD algorithm,[0],[0]
+ 1σv (t) ),4.1. Dual-free BPD algorithm,[0],[0]
"T y }
= argmax y
{( v(t)+σAx̃(t)
1+σ
)T y",4.1. Dual-free BPD algorithm,[0],[0]
"− f∗(y) }
= argmax y
{ v(t+1) T y",4.1. Dual-free BPD algorithm,[0],[0]
"− f∗(y) } = f ′(v(t+1)),
where in the last equality we used the property of conjugate function when f is strongly convex and smooth.",4.1. Dual-free BPD algorithm,[0],[0]
"Moreover,
v(t+1) =",4.1. Dual-free BPD algorithm,[0],[0]
(f ′)−1(y(t+1)),4.1. Dual-free BPD algorithm,[0],[0]
"= (f∗)′(y(t+1)),
which completes the proof.
",4.1. Dual-free BPD algorithm,[0],[0]
"According to Lemma 1, we only need to provide initial points such that v(0) =",4.1. Dual-free BPD algorithm,[0],[0]
(f∗)′(y(0)) is easy to compute.,4.1. Dual-free BPD algorithm,[0],[0]
We do not need to compute (f∗)′(y(t)),4.1. Dual-free BPD algorithm,[0],[0]
"directly for any t > 0, because it is can be updated as v(t) in (12).",4.1. Dual-free BPD algorithm,[0],[0]
"Consequently, we can update y(t) in the BPD algorithm using the gradient f ′(v(t)), without the need of dual proximal mapping.",4.1. Dual-free BPD algorithm,[0],[0]
The resulting dual-free algorithm is given in Algorithm 6.,4.1. Dual-free BPD algorithm,[0],[0]
Theorem 3.,4.1. Dual-free BPD algorithm,[0],[0]
"Suppose Assumption 2 holds and let (x⋆, y⋆) be the unique saddle point of L defined in (6).",4.1. Dual-free BPD algorithm,[0],[0]
"If we set the parameters in Algorithm 6 as
τ = 1L
√ γ
λ+δµ2 , σ = 1 L
√ γ(λ+ δµ2), (13)
and θ = max{θx, θy} where
θx = ( 1− τσδµ2(4+2σ) )",4.1. Dual-free BPD algorithm,[0],[0]
"1 1+τλ , θy = 1 1+σ/2 , (14)
then we have (
1 2τ + λ 2 ) ‖x(t)",4.1. Dual-free BPD algorithm,[0],[0]
"− x⋆‖2 + 12D(y⋆, y(t))",4.1. Dual-free BPD algorithm,[0],[0]
"≤ θtC,
L(x(t), y⋆)− L(x⋆, y(t))",4.1. Dual-free BPD algorithm,[0],[0]
"≤ θtC,
where C = (
1 2τ + λ 2
) ‖x(0) − x⋆‖2 + ( 1 σ+ 1 2 ) D(y⋆, y(0)).
",4.1. Dual-free BPD algorithm,[0],[0]
"Theorem 3 is proved in Appendices B and D. Assuming γ(λ+ δµ2) ≪ L2, we have
θx",4.1. Dual-free BPD algorithm,[0],[0]
≈ 1−,4.1. Dual-free BPD algorithm,[0],[0]
"γδµ 2 16L2 − λ2L √ γ λ+δµ2 , θy ≈ 1− √ γ(λ+δµ2)",4.1. Dual-free BPD algorithm,[0],[0]
"4L .
",4.1. Dual-free BPD algorithm,[0],[0]
"Again, we gain insights by consider the special cases:
•",4.1. Dual-free BPD algorithm,[0],[0]
If δµ2 = 0,4.1. Dual-free BPD algorithm,[0],[0]
"and λ > 0, then θy ≈ 1",4.1. Dual-free BPD algorithm,[0],[0]
"− √ γλ 4L and θx ≈
1− √ γλ 2L .",4.1. Dual-free BPD algorithm,[0],[0]
"So θ = max{θx, θy} is an accelerated rate.
",4.1. Dual-free BPD algorithm,[0],[0]
•,4.1. Dual-free BPD algorithm,[0],[0]
If δµ2 > 0,4.1. Dual-free BPD algorithm,[0],[0]
"and λ = 0, then θy ≈ 1 − √ γδµ2
4L and
θx",4.1. Dual-free BPD algorithm,[0],[0]
≈ 1− γδµ 2 16L2 .,4.1. Dual-free BPD algorithm,[0],[0]
"Thus θ = max{θx, θy} ≈ 1− γδµ2
16L2 is not accelerated.",4.1. Dual-free BPD algorithm,[0],[0]
"This conclusion does not depends on the relative sizes of γδ and µ2/L2, and it is the major difference from the Euclidean case in Section 2.
",4.1. Dual-free BPD algorithm,[0],[0]
"Algorithm 7 Adaptive Dual-Free SPDC (ADF-SPDC) input: parameters σ, τ , θ > 0, initial point (x(0), y(0)),
and adaptation period T .
",4.1. Dual-free BPD algorithm,[0],[0]
Set x̃(0) = x(0) and v(0)i =,4.1. Dual-free BPD algorithm,[0],[0]
(φ ∗ i ),4.1. Dual-free BPD algorithm,[0],[0]
"′(y(0)i ) for i = 1, . . .",4.1. Dual-free BPD algorithm,[0],[0]
", n for t = 0, 1, 2, . . .",4.1. Dual-free BPD algorithm,[0],[0]
"do
pick k ∈ {1, . . .",4.1. Dual-free BPD algorithm,[0],[0]
", n}",4.1. Dual-free BPD algorithm,[0],[0]
"uniformly at random for i ∈ {1, . . .",4.1. Dual-free BPD algorithm,[0],[0]
", n} do
if i == k then
v (t+1) k =
v (t) k",4.1. Dual-free BPD algorithm,[0],[0]
"+σa T k x̃ (t)
1+σ , y (t+1) k",4.1. Dual-free BPD algorithm,[0],[0]
= φ ′,4.1. Dual-free BPD algorithm,[0],[0]
"k(v (t+1) k )
else v (t+1) i = v (t) i , y (t+1)",4.1. Dual-free BPD algorithm,[0],[0]
"i = y (t) i
end if end for
x(t+1)",4.1. Dual-free BPD algorithm,[0],[0]
"= proxτg
( x(t)− τ",4.1. Dual-free BPD algorithm,[0],[0]
"( u(t)+ (y
(t+1) k −y (t) k )",4.1. Dual-free BPD algorithm,[0],[0]
"ak
))
u(t+1) = u(t) + 1n",4.1. Dual-free BPD algorithm,[0],[0]
(y (t+1) k,4.1. Dual-free BPD algorithm,[0],[0]
− y (t) k )ak x̃(t+1) = x(t+1),4.1. Dual-free BPD algorithm,[0],[0]
+ θ(x(t+1),4.1. Dual-free BPD algorithm,[0],[0]
"− x(t)) if mod(t+ 1, T · n) = 0",4.1. Dual-free BPD algorithm,[0],[0]
"then
(τ, σ, θ) = SPDC-Adapt ( {P (t−sn), D(t−sn)}Ts=0 )
end if end for
If both δµ2 > 0",4.1. Dual-free BPD algorithm,[0],[0]
"and λ > 0, then the extent of acceleration depends on their relative size.",4.1. Dual-free BPD algorithm,[0],[0]
"If λ is on the same order as δµ2 or larger, then accelerated rate is obtained.",4.1. Dual-free BPD algorithm,[0],[0]
"If λ is much smaller than δµ2, then the theory predicts no acceleration.",4.1. Dual-free BPD algorithm,[0],[0]
"Following the same approach, we can derive an Adaptive Dual-Free SPDC algorithm, given in Algorithm 7.",4.2. Dual-free SPDC algorithm,[0],[0]
"On related work, Shalev-Shwartz & Zhang (2016) and (Shalev-Shwartz, 2016) introduced dual-free SDCA.
",4.2. Dual-free SPDC algorithm,[0],[0]
The following theorem characterizes the choice of algorithmic parameters that can exploit strong convexity from data to achieve linear convergence (proof given in Appendix F).,4.2. Dual-free SPDC algorithm,[0],[0]
Theorem 4.,4.2. Dual-free SPDC algorithm,[0],[0]
Suppose Assumption 1 holds.,4.2. Dual-free SPDC algorithm,[0],[0]
"Let (x⋆, y⋆) be the saddle point of L in (3) andR=max{‖a1‖, . . .",4.2. Dual-free SPDC algorithm,[0],[0]
", ‖an‖}.",4.2. Dual-free SPDC algorithm,[0],[0]
"If we set T = ∞ in Algorithm 7 (non adaption) and let
σ = 14R √ γ(nλ+ δµ2), τ = 14R",4.2. Dual-free SPDC algorithm,[0],[0]
"√ γ nλ+δµ2 , (15)
and θ = max{θx, θy} where
θx = ( 1− τσδµ2n(4+2σ) )",4.2. Dual-free SPDC algorithm,[0],[0]
"1 1+τλ , θy = 1+((n−1)/n)σ/2 1+σ/2 ,
(16) then we have (
1 2τ + λ 2
)",4.2. Dual-free SPDC algorithm,[0],[0]
E [ ‖x(t),4.2. Dual-free SPDC algorithm,[0],[0]
"− x⋆‖2 ] + γ4E [ D(y⋆, y(t))",4.2. Dual-free SPDC algorithm,[0],[0]
"] ≤ θtC,
E [ L(x(t), y⋆)− L(x⋆, y(t))",4.2. Dual-free SPDC algorithm,[0],[0]
"] ≤ θtC,
where C = (
1 2τ + λ 2
) ‖x(0) − x⋆‖2 + ( 1 σ+ 1 2 ) D(y⋆, y(0)).
",4.2. Dual-free SPDC algorithm,[0],[0]
"Now we discuss the results of Theorem 4 in further details.
",4.2. Dual-free SPDC algorithm,[0],[0]
The cases of σµ2 = 0 but λ > 0.,4.2. Dual-free SPDC algorithm,[0],[0]
In this case we have τ = 14R √ γ nλ,4.2. Dual-free SPDC algorithm,[0],[0]
"and σ = 1 4R √ nγλ, and
θx = 1− 1 1+4R √ n/(λγ) , θy = 1− 1 n+8R √ n/(λγ) .
",4.2. Dual-free SPDC algorithm,[0],[0]
"The rate is the same as for SPDC in Zhang & Xiao (2015).
",4.2. Dual-free SPDC algorithm,[0],[0]
The cases of σµ2 > 0,4.2. Dual-free SPDC algorithm,[0],[0]
but λ = 0.,4.2. Dual-free SPDC algorithm,[0],[0]
In this case we have τ = 14Rµ,4.2. Dual-free SPDC algorithm,[0],[0]
√ γ δ,4.2. Dual-free SPDC algorithm,[0],[0]
"and σ = µ 4R √ δγ, thus
θx = 1− τσδµ 2 2n(σ+4)",4.2. Dual-free SPDC algorithm,[0],[0]
"= 1− γδµ2 32nR2 · 1√γδµ/(4R)+4 , θy = 1+((n−1)/n)σ/2
1+σ/2 = 1− 1n+8nR/(µ√γδ) .
",4.2. Dual-free SPDC algorithm,[0],[0]
We note that the primal function now is R2/γ-smooth and δµ2/n-strongly convex.,4.2. Dual-free SPDC algorithm,[0],[0]
"We discuss the following cases:
•",4.2. Dual-free SPDC algorithm,[0],[0]
"If √γδµ > R, then we have θx ≈ 1 − √ γδµ
8nR and θy ≈ 1− 1n .",4.2. Dual-free SPDC algorithm,[0],[0]
"Therefore θ = max{θx, θy} ≈ 1− 1n .
",4.2. Dual-free SPDC algorithm,[0],[0]
"• Otherwise, we have θx ≈ 1 − γδµ 2
64nR2 and θy is of the same order.",4.2. Dual-free SPDC algorithm,[0],[0]
"This is not an accelerated rate, and we have the same iteration complexity as SVRG.
",4.2. Dual-free SPDC algorithm,[0],[0]
"Finally, we give concrete examples of how to compute the initial points y(0) and v(0) such that v(0)i =",4.2. Dual-free SPDC algorithm,[0],[0]
(φ ∗ i ),4.2. Dual-free SPDC algorithm,[0],[0]
"′(y(0)i ).
",4.2. Dual-free SPDC algorithm,[0],[0]
•,4.2. Dual-free SPDC algorithm,[0],[0]
"For squared loss, φi(α) = 12 (α − bi)2 and φ∗i (β) = 1 2β 2 + biβ.",4.2. Dual-free SPDC algorithm,[0],[0]
So v (0) i = (φ ∗ i ),4.2. Dual-free SPDC algorithm,[0],[0]
′(y(0)i ) = y (0),4.2. Dual-free SPDC algorithm,[0],[0]
i + bi.,4.2. Dual-free SPDC algorithm,[0],[0]
•,4.2. Dual-free SPDC algorithm,[0],[0]
"For logistic regression, we have bi ∈ {1,−1} and φi(α) = log(1 + e
−biα).",4.2. Dual-free SPDC algorithm,[0],[0]
The conjugate function is φ∗i (β) = (−biβ) log(−biβ) + (1 + biβ) log(1 + biβ) if biβ ∈,4.2. Dual-free SPDC algorithm,[0],[0]
"[−1, 0] and +∞ otherwise.",4.2. Dual-free SPDC algorithm,[0],[0]
We can choose y (0) i =− 12bi and v (0) i =0 such that v (0),4.2. Dual-free SPDC algorithm,[0],[0]
i =(φ ∗ i ),4.2. Dual-free SPDC algorithm,[0],[0]
"′(y(0)i ).
",4.2. Dual-free SPDC algorithm,[0],[0]
"For logistic regression, we have δ = 0 over the full domain of φi.",4.2. Dual-free SPDC algorithm,[0],[0]
"However, each φi is locally strongly convex in bounded domain (Bach, 2014): if z ∈",4.2. Dual-free SPDC algorithm,[0],[0]
"[−B,B], then we know δ = minz φi′′(z)",4.2. Dual-free SPDC algorithm,[0],[0]
≥ exp(−B)/4.,4.2. Dual-free SPDC algorithm,[0],[0]
Therefore it is well suitable for an adaptation scheme similar to Algorithm 4 that do not require knowledge of either δ or µ.,4.2. Dual-free SPDC algorithm,[0],[0]
We present preliminary experiments to demonstrate the effectiveness of our proposed algorithms.,5. Preliminary experiments,[0],[0]
"First, we consider batch primal-dual algorithms for ridge regression over a synthetic dataset.",5. Preliminary experiments,[0],[0]
"The data matrix A has sizes n = 5000 and d = 3000, and its entries are sampled from multivariate normal distribution with mean zero and covariance matrix Σij = 2|i−j|/2.",5. Preliminary experiments,[0],[0]
"We normalize all datasets
such that ai = ai/ (maxj ‖aj‖), to ensure the maximum norm of the data points is 1.",5. Preliminary experiments,[0],[0]
We use ℓ2-regularization g(x) =,5. Preliminary experiments,[0],[0]
"(λ/2)‖x‖2 with three choices of parameter λ: 1/n, 10−2/n and 10−4/n, which represent the strong, medium, and weak levels of regularization, respectively.
",5. Preliminary experiments,[0],[0]
"Figure 1 shows the performance of four different algorithms: the primal accelerated gradient (Primal AG) algorithm (Nesterov, 2004) using λ as strong convexity parameter, the BPD algorithm (Algorithm 1) that uses the same λ and µ2δ = 0, the optimal BPD algorithm (Opt-BPD) that uses µ2δ = λmin(A
TA) n",5. Preliminary experiments,[0],[0]
"≈ 0.022n computed from data,
and the Ada-BPD algorithm (Algorithm 2) with the robust adaptation heuristic (Algorithm 4) with T = 10, c = 0.95 and c = 1.5.",5. Preliminary experiments,[0],[0]
"As expected, the performance of Primal-AG is very similar to that of BPD, and Opt-BPD has the fastest convergence.",5. Preliminary experiments,[0],[0]
"The Ada-BPD algorithm can partially exploit strong convexity from data without knowledge of µ.
",5. Preliminary experiments,[0],[0]
"Next we compare DF-SPDC (Algorithm 5 without adaption) and ADF-SPDC (Algorithm 7 with adaption) against several state-of-the-art randomized algorithms for ERM: SVRG (Johnson & Zhang, 2013), SAGA (Defazio et al., 2014)",5. Preliminary experiments,[0],[0]
"Katyusha (Allen-Zhu, 2016) and the standard SPDC method (Zhang & Xiao, 2015).",5. Preliminary experiments,[0],[0]
"For SVRG and Katyusha (an accelerated variant of SVRG), we choose the variance reduction period as m = 2n.",5. Preliminary experiments,[0],[0]
The step sizes of all algorithms are set as their original paper suggested.,5. Preliminary experiments,[0],[0]
"For
Ada-SPDC and ADF-SPDC, we use the robust adaptation scheme with T = 10, c = 0.95 and c = 1.5.
",5. Preliminary experiments,[0],[0]
We first compare these randomized algorithms for ridge regression over the cpuact data from the LibSVM website (https://www.csie.ntu.edu.tw/˜cjlin/libsvm/).,5. Preliminary experiments,[0],[0]
The results are shown in Figure 2.,5. Preliminary experiments,[0],[0]
"With relatively strong regularization λ = 1/n, all methods perform similarly as predicted by theory.",5. Preliminary experiments,[0],[0]
"When λ becomes smaller, the nonaccelerated algorithms (SVRG and SAGA) automatically exploit strong convexity from data, so they become faster than the non-adaptive accelerated methods (Katyusha, SPDC and DF-SPDC).",5. Preliminary experiments,[0],[0]
"The adaptive accelerated method, ADF-SPDC, has the fastest convergence.",5. Preliminary experiments,[0],[0]
"This indicates that our theoretical results, which predict no acceleration in this case, may be further improved.
",5. Preliminary experiments,[0],[0]
"Finally we compare these algorithms for logistic regression on the rcv1 dataset (from LibSVM website) and another synthetic dataset with n = 5000 and d = 500, generated similarly as before but with covariance matrix Σij = 2
|i−j|/100.",5. Preliminary experiments,[0],[0]
"For the standard SPDC, we compute the coordinate-wise dual proximal mapping using a few steps of scalar Newton’s method to high precision.",5. Preliminary experiments,[0],[0]
The dualfree SPDC algorithms only use gradients of the logistic function.,5. Preliminary experiments,[0],[0]
The results are presented in Figure 3.,5. Preliminary experiments,[0],[0]
"For both datasets, the strong convexity from data is very weak, and the accelerated algorithms performs better.",5. Preliminary experiments,[0],[0]
We consider empirical risk minimization of linear predictors with convex loss functions.,abstractText,[0],[0]
Such problems can be reformulated as convex-concave saddle point problems and solved by primal-dual first-order algorithms.,abstractText,[0],[0]
"However, primal-dual algorithms often require explicit strongly convex regularization in order to obtain fast linear convergence, and the required dual proximal mapping may not admit closed-form or efficient solution.",abstractText,[0],[0]
"In this paper, we develop both batch and randomized primal-dual algorithms that can exploit strong convexity from data adaptively and are capable of achieving linear convergence even without regularization.",abstractText,[0],[0]
"We also present dual-free variants of adaptive primal-dual algorithms that do not need the dual proximal mapping, which are especially suitable for logistic regression.",abstractText,[0],[0]
Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 765–771, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
Negation is a linguistic phenomenon where a negation cue (e.g. not) can alter the meaning of a particular text segment or of a fact.,1 Introduction,[0],[0]
This text segment (or fact) is said to be inside the scope of that negation (cue).,1 Introduction,[0],[0]
"In the context of RE, there is not much work that aims to exploit the scope of negations.1 The only work on RE that we are aware of is SanchezGraillet and Poesio (2007) where they used various heuristics to extract negative protein interaction.
",1 Introduction,[0],[0]
Despite the recent interest on automatically detecting the scope of negation2 till now there seems to be no empirical evidence supporting its exploitation for the purpose of RE.,1 Introduction,[0],[0]
"Even if we could manage to obtain highly accurate automatically detected
1In the context of event extraction (a closely related task of RE), there have been efforts in BioNLP shared tasks of 2009 and 2011 for (non-mandatory sub-task of) event negation detection (3 participants in 2009; 2 in 2011)",1 Introduction,[0],[0]
"(Kim et al., 2009; Kim et al., 2011).",1 Introduction,[0],[0]
"The participants approached the sub-task using either pre-defined patterns or some heuristics.
",1 Introduction,[0],[0]
"2This task is popularized by various recently held shared tasks (Farkas et al., 2010; Morante and Blanco, 2012).
negation scopes, it is not clear how to feed this information inside the RE approach.",1 Introduction,[0],[0]
"Simply considering whether a pair of candidate mentions falls under the scope of a negation cue might not be helpful.
",1 Introduction,[0],[0]
"In this paper, we propose that the scope of negations can be exploited at two different levels.",1 Introduction,[0],[0]
"Firstly, the system would check whether all the target entity3 mentions inside a sentence along with possible relation clues (or trigger words), if any, fall (directly or indirectly) under the scope of a negation cue.",1 Introduction,[0],[0]
"If such a sentence is found, then it should be discarded (i.e. candidate mention pairs4 inside that sentence would not be considered).",1 Introduction,[0],[0]
"Secondly, for each of the remaining pairs of candidate mentions, the system should exploit features related to the scope of negation (rather than simply adding a feature for negation cue, approach adopted in various RE systems) that can provide indication (if any such evidence exists) that the corresponding relation of interest actually does not hold in that particular context.
",1 Introduction,[0],[0]
"In the subsequent sections, we describe our approach.",1 Introduction,[0],[0]
The RE task considered is drug-drug interaction (DDI) extraction.,1 Introduction,[0],[0]
"The task has significant importance for public health safety.5 We used
3The target entities, for example, for DDI extraction and for EMP-ORG relation extraction would be {DRUG} and {PER, GPE, ORG} respectively.",1 Introduction,[0],[0]
Any entity other than the target entities (w.r.t.,1 Introduction,[0],[0]
"the particular RE task) belongs to non-target entities.
",1 Introduction,[0],[0]
"4Candidate mention pairs for RE are taken from target entity mentions.
",1 Introduction,[0],[0]
"5After the death of pop star Michael Jackson, allegedly due to DDI, it was reported that about 2.2 million people in USA, age 57 to 85, were taking potentially dangerous combinations of drugs (Landau, 2009).",1 Introduction,[0],[0]
"An earlier report mentioned that deaths from accidental drug interactions rose 68 percent between 1999 and 2004 (Payne, 2007).
",1 Introduction,[0],[0]
"765
the DDIExtraction-2011 challenge corpus (SeguraBedmar et al., 2011).",1 Introduction,[0],[0]
"The official training and test data of the corpus contain 4,267 and 1,539 sentences, and 2,402 and 755 DDI annotations respectively.",1 Introduction,[0],[0]
We propose a two stage RE approach.,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"In the first stage, our goal is to exploit the scope of negations to reduce the number of candidate mention pairs by discarding sentences.",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"For this purpose, we propose the following features to train a binary classifier:
• has2TM: If the sentence has exactly 2 target entity mentions (i.e. drug mentions for DDI extraction).
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
• has3OrMoreTM:,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"Whether the sentence has more than 2 target entity mentions.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"• allTMonRight: Whether all target entity mentions inside the sentence appear after the negation cue.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
• neitherAllTMonLeftOrRight:,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"Whether some but not all target entity mentions appear after the negation cue.
• negCue:",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
The negation cue itself.,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
• immediateGovernor:,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"The word on which the cue is
directly syntactically dependent.
• nearestVerbGovernor: The nearest verb in the dependency graph on which the cue is syntactically dependent.
• isVerbGovernorRoot: Whether the nearestVerbGovernor is root of the dependency graph of the sentence.
• allTMdependentOnNVG: Whether all target entity mentions are syntactically dependent (directly/indirectly) on the nearestVerbGovernor.
• allButOneTMdependentOnNVG: Whether all but one target entity mentions are syntactically dependent on the nearestVerbGovernor.
• although*PrecedeCue: Whether the syntactic clause containing the negation cue begins with “although / though / despite / in spite”.
• commaBeforeNextTM: Whether there is a comma in the text between the negation cue and the next target entity mention after the cue.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
• commaAfterPrevTM:,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"Whether there is a comma in the text between the previous target entity mention before the negation cue and the cue itself.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"• sentHasBut: Whether the sentence contains the word “but”.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"The objective of the classifier is to decide whether all of the target entity mentions (i.e. drugs) as well as any possible evidence of the relation of interest (for which we assume the immediate and the nearest verb governors of the negation cue would be good candidates) inside the corresponding sentence fall under the scope of a negation cue in such a way that the sentence is unlikely to contain a DDI.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"At present, we limit our focus only on the first occurrence of the following negation cues: “no”, “n’t” or “not”.6 In the Stage 1, any sentence that contains at least one DDI is considered by the classifier as a positive (training/test) instance.",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
Other sentences are considered as negative instances.,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"We rule out any sentence (i.e. we do not consider as training/test instance for the classifier that filters less informative sentences) during both training and testing if any of the following conditions holds:
•",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"The sentence contains less than two target entity mentions (such sentence would not contain the relation of interest anyway).
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"• It has any of the following phrases – “not recommended”, “should not be” or “must not be”.7
•",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"There is no “no”, “n’t” or “not” in the sentence.",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
•,2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"No target entity mention appears in the sentence af-
ter “no”, “n’t” or “not”.
",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"To assess the effectiveness of the proposed Stage 1 classifier, we defined a baseline classifier that filters any sentence that contains “no”, “n’t” or “not”.",2.1 Stage 1: Exploiting scope of negation to filter out sentences,[0],[0]
"Once the sentences which are likely to have no DDI are identified and removed, the next step is to apply a state-of-the-art RE approach on the remaining sentences.",2.2 Stage 2,[0],[0]
"In this section, we propose a new hybrid kernel, KHybrid, for this purpose.",2.2 Stage 2,[0],[0]
"It is defined as follows:
KHybrid (R1, R2) = KHF (R1, R2) + KSL (R1, R2) +",2.2 Stage 2,[0],[0]
"w * KPET (R1, R2)
6These cues usually occur more frequently and generally have larger negation scope than other negation cues.
",2.2 Stage 2,[0],[0]
"7These expressions often provide clues that one of the bioentity mentions negatively influences the level of activity of the other.
",2.2 Stage 2,[0],[0]
"Here, KHF stands for a new feature based kernel (proposed in this paper) that uses a heterogeneous set of features.",2.2 Stage 2,[0],[0]
KSL stands for the Shallow Linguistic (SL) kernel proposed by Giuliano et al. (2006).,2.2 Stage 2,[0],[0]
"KPET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004).",2.2 Stage 2,[0],[0]
w is a multiplicative constant used for the PET kernel.,2.2 Stage 2,[0],[0]
"It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus.
",2.2 Stage 2,[0],[0]
The proposed kernel composition is valid according to the closure properties of kernels.,2.2 Stage 2,[0],[0]
"We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation.",2.2 Stage 2,[0],[0]
"In Stage 2, each candidate drug mention pair represents an instance.
2.2.1 Proposed KHF kernel As mentioned earlier, this proposed kernel uses heterogeneous features.",2.2 Stage 2,[0],[0]
"The first version of the heterogeneous feature set (henceforth, HF v1) combines features proposed by two previous RE works.",2.2 Stage 2,[0],[0]
"The former is Zhou et al. (2005), which uses 51 different features.",2.2 Stage 2,[0],[0]
"We select the following 27 of their features for our feature set:
WBNULL, WBFL, WBF, WBL, WBO, BM1F, BM1L, AM2F, AM2L, #MB, #WB, CPHBNULL, CPHBFL, CPHBF, CPHBL, CPHBO, CPHBM1F, CPHBM1L, CPHAM2F, CPHAM2F, CPP, CPPH, ET12SameNP, ET12SamePP, ET12SameVP, PTP, PTPH
",2.2 Stage 2,[0],[0]
"The latter is the TPWF kernel (Chowdhury and Lavelli, 2012a) from which we use following features:
HasTriggerWord, Trigger-X, DepPattern-i, ewalk, v-walk
The TPWF kernel extracts the HasTriggerWord, Trigger-X and DepPattern-i features from a subgraph called reduced graph.",2.2 Stage 2,[0],[0]
We also follow this approach with one minor difference.,2.2 Stage 2,[0],[0]
"Unlike Chowdhury and Lavelli (2012a), we look for trigger words in the whole reduced graph instead of using only the root of the sub-graph.
",2.2 Stage 2,[0],[0]
"Due to space limitation we refer the readers to the corresponding papers for the description of the above mentioned features and the definition of reduced graph.
",2.2 Stage 2,[0],[0]
"In addition, HF v1 also includes surrounding tokens within the window of {-2,+2} for each candidate mention.",2.2 Stage 2,[0],[0]
We are unaware of any available list of trigger words for drug-drug interaction.,2.2 Stage 2,[0],[0]
"So, we created such a list.8
We extend the heterogeneous feature set by adding features related to the scope of negation (henceforth, HF v2).",2.2 Stage 2,[0],[0]
We use a list of 13 negation cues9 to search inside the reduced graph of a candidate pair.,2.2 Stage 2,[0],[0]
"If the reduced graph contains any of the negation cues or their morphological variants then we add the following features:
• negCue:",2.2 Stage 2,[0],[0]
The corresponding negation cue.,2.2 Stage 2,[0],[0]
•,2.2 Stage 2,[0],[0]
immediateNegatedWord,2.2 Stage 2,[0],[0]
:,2.2 Stage 2,[0],[0]
"If the word following the
negation cue is neither a preposition nor a “be verb”, then that word, otherwise the word after the next word.10
Furthermore, if the corresponding matched negation cue is either “no”, “n’t” or “not”, then we add additional features related to negation scope:
• bothEntDependOnImmediateGovernor:",2.2 Stage 2,[0],[0]
"Whether the immediate governor (if any) of the negation cue is also governor of a dependency sub-tree (of the dependency graph of the corresponding sentence) that includes both of the candidate mentions.
",2.2 Stage 2,[0],[0]
• immediateGovernorIsVerbGovernor:,2.2 Stage 2,[0],[0]
"Whether the immediate governor of the negation cue is a verb.
",2.2 Stage 2,[0],[0]
• nearestVerbGovernor:,2.2 Stage 2,[0],[0]
"The closest verb governor (i.e. parent or grandparent inside the dependency graph), if any, of the negation cue.
",2.2 Stage 2,[0],[0]
"We further extend the heterogeneous feature set by adding features related to relevant non-target entities (with respect to the relation of interest; henceforth, HF v3).",2.2 Stage 2,[0],[0]
"For the purpose of DDI extraction, we deem the presence of DISEASE mentions (which might result as a consequence of a DDI) can provide some clues.",2.2 Stage 2,[0],[0]
"So, we use a publicly available state-of-the-art disease NER system called BioEnEx (Chowdhury and Lavelli, 2010) to annotate the DDIExtraction-2011 challenge corpus.",2.2 Stage 2,[0],[0]
"For
8The RE system developed for this work and the created list of trigger words for DDI can be downloaded from https://github.com/fmchowdhury/HyREX .
9No, not, neither, without, lack, fail, unable, abrogate, absence, prevent, unlikely, unchanged, rarely.
",2.2 Stage 2,[0],[0]
"10For example, “interested” from “... not interested ...”, and “confused” from “... not to be confused ...”.
",2.2 Stage 2,[0],[0]
"each candidate (drug) mention pair, we add the following features in HF v3:
• NTEMinsideSentence: Whether the corresponding sentence contains important non-target entity mention(s) (e.g. disease for DDI).
",2.2 Stage 2,[0],[0]
"• immediateGovernorIsVerbGovernorOfNTEM: The immediate governor (if any) of the non-target entity mention, only if such governor is also governing a dependency sub-tree that includes both of the target candidate entity mentions.
",2.2 Stage 2,[0],[0]
• nearestVerbGovernorOfNTEM:,2.2 Stage 2,[0],[0]
"The closest verb governor (if any) of the non-target entity mention, only if it also governs the candidate entity mentions.
• immediateGovernorIsVerbGovernorOfNTEM: Whether the immediate governor is a verb.",2.2 Stage 2,[0],[0]
We train a linear SVM classifier in Stage 1 and tune the hyper-parameters (by doing 5-fold crossvalidation) for obtaining maximum possible recall.,3 Results and Discussion,[0],[0]
"In this way we minimize the number of false negatives (i.e. sentences that contain DDIs but are wrongly identified as not having any).
",3 Results and Discussion,[0],[0]
"During the cross-validation experiments on the training data, 334 sentences (7.83% of the total sentences) containing at least 2 drug mentions were identified by our proposed classifier (in Section 2.1) as unlikely to have any DDI and hence are candidates for discarding.",3 Results and Discussion,[0],[0]
Only 19 of these sentences were incorrectly identified.,3 Results and Discussion,[0],[0]
"When we trained on the training data and tested on the official test data of DDIExtraction-2011 challenge corpus, 121 sentences (7.86% of the total test sentences) were identified by the classifier as candidates for discarding.",3 Results and Discussion,[0],[0]
"Only 5 of them were incorrectly identified.
",3 Results and Discussion,[0],[0]
"Unlike Stage 1, in Stage 2 where we train the hybrid kernel based RE classifier and use it for RE (i.e. DDI extraction) from the test data, sentences are not the RE training/test instances.",3 Results and Discussion,[0],[0]
"Instead, a RE instance corresponds to a candidate mention pair.
",3 Results and Discussion,[0],[0]
"All the DDIs (i.e. positive RE instances) of the incorrectly identified sentences in Stage 1 (i.e. the sentences which are incorrectly labelled as not having any DDI and filtered) are automatically considered as false negatives during the calculation of DDI extraction results in Stage 2.
",3 Results and Discussion,[0],[0]
"To verify whether our proposed hybrid kernel achieves state-of-the-art results without taking benefits of the output of Stage 1, we did some experiments without discarding any sentence.",3 Results and Discussion,[0],[0]
"These experiments are done using Zhou et al. (2005), TPWF kernel, SL kernel, different versions of proposed KHF kernel and KHybrid kernel.",3 Results and Discussion,[0],[0]
Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score).,3 Results and Discussion,[0],[0]
"As the results show, there is a gain +0.9 points in F-score (mainly due to the boost in recall) after the addition of features related to negation scope.",3 Results and Discussion,[0],[0]
"There is also some minor improvement due to the proposed non-target entity specific features.
",3 Results and Discussion,[0],[0]
"We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al. (2005), TPWF kernel, SL kernel, PET kernel, KHF kernel and KHybrid kernel separately (only the results of KHybrid are reported in Table 1 due to space limitation).",3 Results and Discussion,[0],[0]
"In each case, there were improvements in precision, recall and Fscore.",3 Results and Discussion,[0],[0]
"The gain in F-score ranged from 1.0 to 1.4 points.
",3 Results and Discussion,[0],[0]
Table 2 reports the results of the previously published studies that used the same corpus.,3 Results and Discussion,[0],[0]
"Our proposed KHybrid kernel obtains an F-score that is higher than that of the previous state of the art.
",3 Results and Discussion,[0],[0]
"When the Stage 1 classifier (based on negation scope features) is exploited before using the KHybrid kernel, the F-score reaches up to 67.4.",3 Results and Discussion,[0],[0]
"This is +1.0 points higher than without exploiting the Stage 1 classifier and +1.7 higher than previous state of
the art.",3 Results and Discussion,[0],[0]
We did separate experiments (also reported in Table 2) to assess the performance improvement when the output of Stage 1 is used to filter sentences from either training or test data only.,3 Results and Discussion,[0],[0]
The results remain the same when only training sentences are filtered; while there are some improvements when only test sentences are filtered.,3 Results and Discussion,[0],[0]
"Filtering both training and test sentences provides the larger gain which is statistically significant.
",3 Results and Discussion,[0],[0]
"Usually, the number of negative instances in a corpus is much higher than that of the positive instances.",3 Results and Discussion,[0],[0]
"In a recent work, Chowdhury and Lavelli (2012b) showed that by removing less informative (negative) instances (henceforth, LIIs), not only the skewness in instance distribution could be reduced but it also leads to a better result.",3 Results and Discussion,[0],[0]
"The proposed Stage 1 classifier, presented in this work, also reduces skewness in instance distribution.",3 Results and Discussion,[0],[0]
This is because we are only removing those sentences that are unlikely to contain any positive instance.,3 Results and Discussion,[0],[0]
"So, in principle, the Stage 1 classifier is focused on removing only negative instances (although the classifier mistakenly discards few positive instances, too).
",3 Results and Discussion,[0],[0]
We wanted to study how the Stage 1 classifier would contribute if we use it on top of the techniques that were proposed in Chowdhury and Lavelli (2012b) to remove LIIs.,3 Results and Discussion,[0],[0]
"As Table 2 shows, by using the Stage 1 classifier along with LLI filtering, we could further improve the results (+3.2 points difference in F-score with the previous state of the art).",3 Results and Discussion,[0],[0]
A major flexibility in the proposed approach is that it does not require a separate dataset (which needs to match the genre of the text to be used for RE) annotated with negation scopes.,4 Conclusion,[0],[0]
"Instead, the proposed Stage 1 classifier uses the RE training data (which do not have negation scope annotations) to self-supervise itself.",4 Conclusion,[0],[0]
Various new features have been exploited (both in stages 1 and 2) that can provide strong indications of the scope of negation cues with respect to the relation to be extracted.,4 Conclusion,[0],[0]
"The only thing needed is the list of possible negation cues (Morante (2010) includes such a comprehensive list).
",4 Conclusion,[0],[0]
"Our proposed kernel, which has a component that exploits a heterogeneous set of features including negation scope and presence of non-target entities, already obtains better results than previous studies.
",4 Conclusion,[0],[0]
"The results considerably improve when possible irrelevant sentences from both training and test data are filtered by exploiting features related to the scope of negations.
",4 Conclusion,[0],[0]
"In future, we would like to exploit the scope of more negation cues, apart from the three cues that are used in this study.",4 Conclusion,[0],[0]
We believe our approach would help to improve RE in other genres of text (such as newspaper) as well.,4 Conclusion,[0],[0]
This work was carried out in the context of the project “eOnco - Pervasive knowledge and data management in cancer care”.,Acknowledgement,[0],[0]
This paper presents an approach that exploits the scope of negation cues for relation extraction (RE) without the need of using any specifically annotated dataset for building a separate negation scope detection classifier.,abstractText,[0],[0]
New features are proposed which are used in two different stages.,abstractText,[0],[0]
These also include non-target entity specific features.,abstractText,[0],[0]
The proposed RE approach outperforms the previous state of the art for drug-drug interaction (DDI) extraction.,abstractText,[0],[0]
Exploiting the Scope of Negations and Heterogeneous Features for Relation Extraction: A Case Study for Drug-Drug Interaction Extraction,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 85–91 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2014",text,[0],[0]
"Neural sequence to sequence models have been successfully used in many applications (Graves, 2012), from speech and signal processing to text processing or dialogue systems (Serban et al., 2015).",1 Introduction,[0],[0]
"Neural machine translation (Cho et al., 2014; Bahdanau et al., 2014) is a particular type of sequence to sequence model that recently attracted a lot of attention from industry (Wu et al., 2016) and academia, especially due to the capability to obtain state-of-the-art results for various translation tasks (Bojar et al., 2016).",1 Introduction,[0],[0]
"Unlike classical statistical machine translation (SMT) systems (Koehn, 2010), neural networks are being trained end-to-end, without the need to have external decoders, language models or phrase tables.",1 Introduction,[0],[0]
"The architectures are relatively simpler and more flexible, making possible the use of character models (Luong and Manning, 2016) or even training multilingual systems in one go (Firat et al., 2016).
",1 Introduction,[0],[0]
"Automated text simplification (ATS) systems are meant to transform original texts into differ-
∗Both authors have contributed equally to this work
ent (simpler) variants which would be understood by wider audiences and more successfully processed by various NLP tools.",1 Introduction,[0],[0]
"In the last several years, great attention has been given to addressing ATS as a monolingual machine translation problem translating from ‘original’ to ‘simple’ sentences.",1 Introduction,[0],[0]
"So far, attempts were made at standard phrase-based SMT (PBSMT) models (Specia, 2010; Štajner et al., 2015), PBSMT models with added phrasal deletion rules (Coster and Kauchak, 2011) or reranking of the n-best outputs according to their dissimilarity to the output (Wubben et al., 2012), tree-based translation models (Zhu et al., 2010; Paetzold and Specia, 2013), and syntax-based MT with specially designed tuning function (Xu et al., 2016).",1 Introduction,[0],[0]
"Recently, lexical simplification (LS) was addressed by unsupervised approaches leveraging word-embeddings, with reported good success (Glavaš and Štajner, 2015; Paetzold and Specia, 2016).
",1 Introduction,[0],[0]
"To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS.",1 Introduction,[0],[0]
We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task.,1 Introduction,[0],[0]
We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems.,1 Introduction,[0],[0]
"We use the OpenNMT framework (Klein et al., 2017) to train and build our architecture with two LSTM layers (Hochreiter and Schmidhuber, 1997), hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability (Srivastava et al., 2014).",2 Neural Text Simplification (NTS),[0],[0]
"The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the
85
learning rate.",2 Neural Text Simplification (NTS),[0],[0]
At the end of each epoch we save the current state of the model and predict the perplexity values of the models on the development set.,2 Neural Text Simplification (NTS),[0],[0]
We employ early-stopping and select the model resulted from the epoch with the best perplexity to avoid over-fitting.,2 Neural Text Simplification (NTS),[0],[0]
"The parameters are initialized over uniform distribution with support [-0.1, 0.1].",2 Neural Text Simplification (NTS),[0],[0]
"Additionally, for the decoder we employ global attention in combination with input feeding as described by Luong et al. (2015).",2 Neural Text Simplification (NTS),[0],[0]
"The architecture1 is depicted in Figure 1, with the input feeding approach represented only for the last hidden state of the decoder.
",2 Neural Text Simplification (NTS),[0],[0]
"For the attention layer, we compute a context vector ct by using the information provided from the hidden states of the source sentence and by computing a weighted average with the alignment weights at.",2 Neural Text Simplification (NTS),[0],[0]
"The new hidden state is obtained using a concatenation of the previous hidden state and the context vector:
h̃t = tanhW",2 Neural Text Simplification (NTS),[0],[0]
"[ct;ht]
",2 Neural Text Simplification (NTS),[0],[0]
"The global alignment weights at are being computed with a softmax function over the general scoring method for attention:
at(s) = exphTt Wash̄s∑",2 Neural Text Simplification (NTS),[0],[0]
"s′ exph T t Was′ h̄s′
Input feeding is a process that sends the previous hidden state obtained using the alignment
1The architecture configurations, data, and the pretrained models are released in https://github.com/ senisioi/NeuralTextSimplification
method, to the input at the next step, presumably making the model keep track of anterior alignment decisions.",2 Neural Text Simplification (NTS),[0],[0]
"Luong et al. (2015) showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments.",2 Neural Text Simplification (NTS),[0],[0]
"Our approach does not involve the use of character-based models (Sennrich et al., 2015; Luong and Manning, 2016) to handle out of vocabulary words and entities.",2 Neural Text Simplification (NTS),[0],[0]
"Instead, we make use of alignment probabilities between the predictions and the original sentences to retrieve the original words.",2 Neural Text Simplification (NTS),[0],[0]
"Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models.",2.1 Word2vec Embeddings,[0],[0]
Kauchak (2013) indicates that combining normal data with simplified data can increase the performance of ATS systems.,2.1 Word2vec Embeddings,[0],[0]
"Therefore, we construct a secondary model (NTSw2v) using a combination of pre-trained word2vec from Google News corpus (Mikolov et al., 2013a) of size 300 and locally trained embeddings of size 200.",2.1 Word2vec Embeddings,[0],[0]
"To ensure good representations of lowfrequency words, we use word2vec (Řehůřek and Sojka, 2010; Mikolov et al., 2013b) to train skipgram with hierarchical softmax and we set a window of 10 words.
",2.1 Word2vec Embeddings,[0],[0]
"Following Garten et al. (2015) who showed that simple concatenation can improve the word representations, we construct two different sets of embeddings for the encoder and for the decoder.",2.1 Word2vec Embeddings,[0],[0]
The former are constructed using the word2vec trained on the original English texts combined with Google News and the later (decoder) embeddings are built from word2vec trained on the simplified version of the training data combined with Google News.,2.1 Word2vec Embeddings,[0],[0]
"To merge the local and global embeddings, we concatenate the representations for each word in the vocabulary, thus obtaining a new representation of size 500.",2.1 Word2vec Embeddings,[0],[0]
"If a word is missing in the global embeddings, we replace it with a sample from a Gaussian distribution with mean 0 and standard deviation of 0.9.",2.1 Word2vec Embeddings,[0],[0]
The remaining parameters are left unchanged from the previous model description.,2.1 Word2vec Embeddings,[0],[0]
"To ensure the best predictions and the best simplified sentences at each step, we use beam search to sample multiple outputs from the two systems
described previously (NTS and NTS-w2v).",2.2 Prediction Ranking,[0],[0]
Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence.,2.2 Prediction Ranking,[0],[0]
"By default, we use a beam size of 5 and take the first hypothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results.",2.2 Prediction Ranking,[0],[0]
"Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12.",2.2 Prediction Ranking,[0],[0]
"We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU (Papineni et al., 2002; Bird et al., 2009) with NIST smoothing (Bird et al., 2009), and SARI (Xu et al., 2016), a recent text-simplification metric.",2.2 Prediction Ranking,[0],[0]
"To train our models, we use the publicly available dataset provided by Hwang et al. (2015) based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia (EW–SEW).",2.3 Dataset,[0],[0]
"We discard the uncategorized matches, and use only good matches and partial matches which were above the 0.45 threshold (Hwang et al., 2015), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches).",2.3 Dataset,[0],[0]
"It is one of the largest freely available resources for text simplification, and unlike the previously used EW–SEW corpus2",2.3 Dataset,[0],[0]
"(Kauchak, 2013), which only contains full matches (167K pairs), the newer dataset also contains partial matches.",2.3 Dataset,[0],[0]
"Therefore, it is not only larger, but it also allows for learning sentence shortening (dropping irrelevant parts) transformations (see Table 3, Appendix A).
",2.3 Dataset,[0],[0]
"We use the Stanford NER system (Finkel et al., 2005) to get an approximate number of locations, persons, organizations and miscellaneous entities
2http://www.cs.pomona.edu/˜dkauchak/ simplification/
in the corpus.",2.3 Dataset,[0],[0]
"A brief analysis of the vocabulary is rendered in Table 1.
",2.3 Dataset,[0],[0]
"The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model’s 50,000 words vocabulary due to their small frequency.",2.3 Dataset,[0],[0]
These words are replaced with ’UNK’ symbols during training.,2.3 Dataset,[0],[0]
"At prediction time, we replace the unknown words with the highest probability score from the attention layer.",2.3 Dataset,[0],[0]
"We believe it is important to ensure that the models learn good word representations, either during the model training or through word2vec, in order to accurately create alignments between source and target sentences.
",2.3 Dataset,[0],[0]
"Given that in TS there is not only one best simplification, and that the quality of simplifications in Simple English Wikipedia has been disputed before (Amancio and Specia, 2014; Xu et al., 2015), for tuning and testing we use the dataset previously released by Xu et al. (2016), which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers.3",2.3 Dataset,[0],[0]
The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking.,2.3 Dataset,[0],[0]
"For the first 70 original sentences of the Xu et al.’s (2016) test set4 we perform three types of human evaluation to assess the output of our best systems and three ATS systems of different architectures: (1) the PBSMT system with reranking of n-best outputs (Wubben et al., 2012), which represent the best PBSMT approach to ATS, trained and tuned over the same datasets as our systems; (2) the state-of-the-art SBMT system (Xu et al., 2016) with modified tuning function (using SARI) and using PPDB paraphrase database (Ganitkevitch et al., 2013);5 and (3) one of the state-of-theart unsupervised lexical simplification (LS) systems that leverages word-embeddings (Glavaš and
3None of the 359 test sentences was present in the datasets we used for training and tuning.
",3 Evaluation,[0],[0]
"4https://github.com/cocoxu/ simplification/
5For the first two systems, we use publicly available output at: https://github.com/ cocoxu/simplification/tree/master/data/ systemoutputs
Štajner, 2015).6
We evaluate the output of all systems using three types of human evaluation.
",3 Evaluation,[0],[0]
Correctness and Number of Changes.,3 Evaluation,[0],[0]
"First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. “become defunct” → “was dissolved”) as one change.",3 Evaluation,[0],[0]
"Those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers) are marked as Correct.",3 Evaluation,[0],[0]
"In the case of content reduction, we instructed the annotators to count the deletion of each array of consecutive words as one change and consider the meaning unchanged if the main information of the sentence was retained and unchanged.",3 Evaluation,[0],[0]
"The sentences for which the two annotators did not agree were given to a third annotator to obtain the majority vote.
",3 Evaluation,[0],[0]
Grammaticality and Meaning Preservation.,3 Evaluation,[0],[0]
"Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each (whole) sentence with at least one change on a 1–5 Likert scale (1 – very bad; 5 – very good).",3 Evaluation,[0],[0]
"The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.78 for G and 0.63 for M.
Simplicity of sentences.",3 Evaluation,[0],[0]
"Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences, one pair at the time, and asked whether the target sentence is: +2 – much simpler; +1 – somewhat simpler; 0 – equally difficult; -1 – somewhat more difficult; -2 – much more difficult, than the reference sentence.",3 Evaluation,[0],[0]
"The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.66.
",3 Evaluation,[0],[0]
"While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.",3 Evaluation,[0],[0]
"The results of the human evaluation (Table 2) revealed that all NTS models achieve higher percentage of correct changes and more simplified output than any of the state-of-the-art ATS systems
6For the LightLS system (Glavaš and Štajner, 2015) we use the output of the original system provided by the authors.
with different architectures (PBSMT-R, SBMT, and LightLS).",4 Results and Discussion,[0],[0]
"We also notice that the best models according to BLEU are obtained with hypothesis 1 and the maximum beam size for both models, while the SARI re-ranker prefers hypothesis 2 and beam size 5 for the first NTS and the maximum beam size for the custom word embeddings model.
",4 Results and Discussion,[0],[0]
"The NTS with custom word2vec embeddings ranked with the text simplification specific metric (SARI) obtained the highest total number of changes among the neural systems, one of the highest percentage of correct changes, the second highest simplicity score, and solid grammaticality and meaning preservation scores.",4 Results and Discussion,[0],[0]
"An example of the output of different systems is presented in Table 4 (Appendix A).
",4 Results and Discussion,[0],[0]
"The use of different metrics for ranking the NTS predictions optimizes the output towards different evaluation objectives: SARI leads to the highest number of total changes, BLEU to the highest percentage of correct changes, and the default beam scores to the best grammaticality (G) and meaning preservation (M).",4 Results and Discussion,[0],[0]
"In addition, custom composed global and local word embeddings in combination with SARI metric improve the default translation system, given the joint scores for each evaluation criterion.
",4 Results and Discussion,[0],[0]
"Here is important to note that for ATS systems, the precision of the system (correctness of changes, grammaticality, meaning preservation, and simplicity of the output) is more important than the recall (the total number of changes made).",4 Results and Discussion,[0],[0]
"The low recall would just leave the sentences similar to their originals thus not improving much the understanding or reading speed of the target users, or not improving much the NLP systems in which they are used as a pre-processing step.",4 Results and Discussion,[0],[0]
"A low precision, on the other hand, would make texts even more difficult to read and understand, and would worsen the performances of the NLP systems in which ATS is used as a pre-processing step.",4 Results and Discussion,[0],[0]
We presented a first attempt at modelling sentence simplification with a neural sequence to sequence model.,5 Conclusions,[0],[0]
"Our extensive human evaluation showed that our NTS systems, if the output is ranked with the right metric, can significantly7 outperform the best phrase-based and syntax-based MT approaches, and unsupervised lexical ATS approach,
7Wilcoxon’s signed rank test, p < 0.001.
by grammaticality, meaning preservation and simplicity of the output sentences, the percentage of correct transformations, while at the same time achieving more than 1.5 changes per sentence, on average.",5 Conclusions,[0],[0]
"Furthermore, we discovered that NTS systems are capable of correctly performing significant content reduction, thus being the only TS models proposed so far which can jointly perform lexical simplification and content reduction.",5 Conclusions,[0],[0]
"This work has been partially supported by a grant of the Romanian National Authority for Scientific Research and Innovation, CNCS/CCCDI UEFISCDI, project number PN-III-P2-2.1-53BG/2016, within PNCDI III, and by the SFB 884 on the Political Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG).",Acknowledgments,[0],[0]
We present the first attempt at using sequence to sequence neural networks to model text simplification (TS).,abstractText,[0],[0]
"Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction.",abstractText,[0],[0]
An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems.,abstractText,[0],[0]
Exploring Neural Text Simplification Models,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 652–658 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
652",text,[0],[0]
"“You know, ever since we were little, I would get this feeling like... Like I’m floating outside of my body, looking down at myself...",1 Introduction,[0],[0]
And I hate what I see...,1 Introduction,[0],[0]
"How I’m acting, the way I sound.",1 Introduction,[0],[0]
And I don’t know how to change it.,1 Introduction,[0],[0]
And I’m so scared...,1 Introduction,[0],[0]
"That the feeling is never gonna go away.”
The Edge of Seventeen1
Much has been written about optimism and pessimism in psychological studies for decades (Scheier and Carver, 1992).",1 Introduction,[0],[0]
These feelings are affected by one’s personality from an early age (as pinpointed above) and can strongly impact one’s psychological and physical health.,1 Introduction,[0],[0]
"For example, pessimism and negative attitudes impact negatively one’s mental health, can induce suicidal thoughts, and affect negatively not only the person in question, but also their family and friends (Peterson and Bossio, 2001; Achat et al., 2000;
1https://www.imdb.com/title/tt1878870/
Scheier et al., 2001).",1 Introduction,[0],[0]
"On the other hand, optimism reduces stress and promotes better physical health and overall well-being (Carver et al., 2010).
",1 Introduction,[0],[0]
"Despite that optimism and pessimism are under the scrutiny of many researchers (Rasmussen et al., 2009; Kumar et al., 2017), large scale analyses that explore optimism and pessimism in social media have just started to emerge (Ruan et al., 2016).",1 Introduction,[0],[0]
"However, Ruan et al. (2016) focused on identifying optimism and pessimism in Twitter using a simple “bag of words” representation with no emphasis on incorporating semantic information hidden in text.",1 Introduction,[0],[0]
"Often, a deeper understanding of the text that accounts for textual semantic similarities and the writer’s intention are required in order to correctly detect the characteristics of optimistic and pessimistic feelings in tweets.",1 Introduction,[0],[0]
"Towards this end, our contributions in this paper are as follows.",1 Introduction,[0],[0]
"First, we focus on the question: “Would a deep learning approach help to discover these characteristics better than traditional machine learning classifiers used in prior work?”",1 Introduction,[0],[0]
"To our knowledge, we take the first step towards exploring the performance of deep learning models for optimism/pessimism prediction in Twitter and identify the most promising deep learning models for this task.",1 Introduction,[0],[0]
"Identifying optimism and pessimism in Twitter has many applications including identifying suicidal/depressive people and providing better social support (e.g., emotional/empathetic support) that can improve people’s moods and attitudes (Yan and Tan, 2014; Biyani et al., 2014; Khanpour et al., 2018, 2017; Qiu et al., 2011).
",1 Introduction,[0],[0]
"Second, since it may seem intuitive that a positive sentiment is associated with optimism and a negative sentiment with pessimism, we address the question: “Would a sentiment classifier be sufficient to correctly identify optimism and pessimism in social media?”",1 Introduction,[0],[0]
"Figure 1 shows evidence that a sentiment tool would not suffice on accurately
predicting tweets with pessimistic and optimistic connotations (left and right side of the figure, respectively).",1 Introduction,[0],[0]
"We answer the above question by investigating a spectrum of sentiment analysis tools and datasets for optimism/pessimism prediction.
",1 Introduction,[0],[0]
"Third, we perform a linguistic analysis, first of its kind, and study the usage of verb tenses (past, present, future) in optimistic and pessimistic tweets, as well as the presence of polarity words associated with both types of tweets.",1 Introduction,[0],[0]
"In this section, we first describe the optimism/ pessimism Twitter dataset and then present two datasets used for sentiment analysis.
",2 Datasets,[0],[0]
The Optimism/Pessimism Twitter dataset (OPT) was made available to us by Ruan et al. (2016).,2 Datasets,[0],[0]
"The total number of tweets in the dataset is 7,475.",2 Datasets,[0],[0]
"These tweets were sampled from data corresponding to 500 optimist and 500 pessimist users, and were manually annotated using Amazon Mechanical Turk.",2 Datasets,[0],[0]
"Precisely, each tweet was manually annotated by five independent annotators using a score between −3 (very pessimistic) and 3 (very optimistic).",2 Datasets,[0],[0]
"For our evaluation, we consider two different thresholds (0 and 1/-1) on the above score and create two settings as follows.",2 Datasets,[0],[0]
"In the first evaluation setting, a tweet is labeled as pessimistic if its score is smaller than or equal to 0, and optimistic, otherwise.",2 Datasets,[0],[0]
"In the second evaluation setting, a tweet is labeled as pessimistic if its score is smaller than or equal to −1, and optimistic if its score is greater than or equal to 1.",2 Datasets,[0],[0]
"A summary of this dataset is given in Table 1.
",2 Datasets,[0],[0]
"The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a corpus for sentiment analysis that capture complex linguistic patterns.",2 Datasets,[0],[0]
"This dataset2 is based on a dataset originally introduced by Pang and Lee (2005) and consists of 10,662 sentences from movie reviews downloaded from rottentomatoes.com.",2 Datasets,[0],[0]
"From these sentences, 215,154 phrases were extracted using the Stanford Parser (Klein and Manning, 2003) and labeled using Amazon Mechanical Turk such that each phrase was annotated by 3 human judges.
",2 Datasets,[0],[0]
"The Twitter Sentiment Analysis (TSA) dataset,3 available online for download, contains 1,578,627 tweets that are classified as 1 for positive sentiment and 0 for negative sentiment.",2 Datasets,[0],[0]
"In experiments, we explore several deep learning models for optimism/pessimism prediction.",3 Experiments and Results,[0],[0]
The general training strategy is as follows: sentence embeddings are fed into a sentence encoder to obtain the sentence representation.,3 Experiments and Results,[0],[0]
The classifier consists of three fully connected layers topped by a softmax layer.,3 Experiments and Results,[0],[0]
Dropout was applied to the first layer only.,3 Experiments and Results,[0],[0]
"We used several encoders as follows, based on: (1) Bidirectional Long Short Term Memory networks (BiLSTMs), which are a special type of Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997); (2) Convolutional Neural Networks (CNNs), which consist of convolution and max pooling (Kim, 2014); and (3) Stacked Gated RNNs (Chung et al., 2015).
",3 Experiments and Results,[0],[0]
"We used SGD optimizer (Goodfellow et al., 2016) with a learning rate of 0.1 and no weight decay.",3 Experiments and Results,[0],[0]
"At every tenth epoch we decreased the learn-
2https://nlp.stanford.edu/sentiment/ 3http://thinknook.com/twitter-sentiment-analysis-
training-corpus-dataset-2012-09-22/
ing rate by half.",3 Experiments and Results,[0],[0]
We used mini-batches of 40 samples.,3 Experiments and Results,[0],[0]
"Dropout rate was set to 0.5 and the classifier’s last three layers have 300, 200, and 100 neurons.",3 Experiments and Results,[0],[0]
"We used GloVe vectors (Pennington et al., 2014) trained on Common Crawl 840B4 with 300 dimensions as fixed word embeddings.
",3 Experiments and Results,[0],[0]
"For sentence embedding, after a cleanup process, sentences were transformed into a list of words, then words were replaced with word embeddings (GloVe) and padding was used to align batch sentences to the same size.",3 Experiments and Results,[0],[0]
"In our first experiment, we explore the above deep learning models on the OPT dataset and compare their performance with that of two traditional machine learning classifiers, Naı̈ve Bayes (NB) and Support Vector Machines (SVM), which were used in the previous work for this task by Ruan et al. (2016).",3.1 Optimism/Pessimism Prediction,[0],[0]
"In this experiment, the OPT dataset is split in train-dev-test as 80-10-10(%), respectively.",3.1 Optimism/Pessimism Prediction,[0],[0]
We repeated each experiment 5 times and averaged the results.,3.1 Optimism/Pessimism Prediction,[0],[0]
"Our deep learning implementation is built on top of TensorFlow (Abadi et al., 2015).",3.1 Optimism/Pessimism Prediction,[0],[0]
"For NB and SVM, we used their implementation available in scikit-learn (Pedregosa et al., 2011).",3.1 Optimism/Pessimism Prediction,[0],[0]
"Table 2 shows the accuracy of all these models at tweet and user level for the two thresholds 0 and 1/-1 (as discussed in Section 2).
",3.1 Optimism/Pessimism Prediction,[0],[0]
"We can see that overall, the deep learning models achieve a much higher performance compared with the work by Ruan et al. (2016), i.e., the NB and SVM classifiers on “bag of words,” for both tweet and user level with both thresholds, yielding an improvement in performance between 5%- 10%.",3.1 Optimism/Pessimism Prediction,[0],[0]
"For example, at tweet level and 1/-1 threshold, CNN yields an accuracy of 90.32% as compared with NB, which achieves an accuracy of 84.10%.",3.1 Optimism/Pessimism Prediction,[0],[0]
"At user level and 1/-1 threshold, GRUStack yields an accuracy of 92.24%, as compared
4https://nlp.stanford.edu/projects/glove/
with 81.80% achieved by SVM.",3.1 Optimism/Pessimism Prediction,[0],[0]
"Not surprising, for both tweet and user level, when we use a threshold of 0, the performance of all models is smaller compared with that of models obtained when we use a 1/-1 threshold.",3.1 Optimism/Pessimism Prediction,[0],[0]
"Intuitively, this is true since most of the tweets with a humanannotated score between -1 and 1 are in the “gray” area that is harder to classify.",3.1 Optimism/Pessimism Prediction,[0],[0]
Note that Ruan et al. (2016) considered the tweets with a score between -1 and 1 as being neutral.,3.1 Optimism/Pessimism Prediction,[0],[0]
"In our second experiment, we investigate the correlation between sentiment and optimism / pessimism, and argue that sentiment analyzers, that are trained to predict sentiment (Liu, 2012; Pang and Lee, 2008), fail to detect optimism and pessimism.",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"Specifically, we train several sentiment classifiers on the large SST and TSA sentiment datasets (described in Section 2) and evaluate the performance of these classifiers on the optimism/pessimism categories from the OPT dataset.
",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
Table 3 shows the performance of several deep learning models trained on either SST or TSA datasets and evaluated on the OPT dataset.,3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
Note that the Dev set was used for model selection.,3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"As can be seen from the table, the models trained on the sentiment datasets perform poorly on the optimism/pessimism dataset.",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"For example, there is a drop in performance from 80.19% to 67.60% when training on TSA (with an even larger decrease when we train on SST).
",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"The SST/TSA sentiment classifiers are trained to predict the sentiment as negative, neutral, or positive.",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"To calculate the accuracy in Table 3, an optimistic tweet predicted as positive by the sentiment classifier counts as a correct prediction, whereas an optimistic tweet predicted as either neutral or negative by the sentiment classifier counts as an incorrect prediction (similarly for pessimistic tweets).",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"This analysis is done at tweet level for the threshold of 0.
",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"Figure 2 shows the normalized number of examples from optimism and pessimism categories classified as positive, negative and neutral, using the CNN model trained on TSA.",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"Precisely, we show how many tweets from the set of optimistic (or pessimistic) tweets in the OPT dataset are predicted as negative, neutral or positive by the TSA sentiment classifier.",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
The numbers on each row sum up to 1.,3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"As we can see from the figure, although pessimism is more correlated with a negative sentiment, 13% of the pessimistic tweets are classified as positive (with similar results on the optimism category).",3.2 Sentiment vs. Optimism/Pessimism,[0],[0]
"In this section, we perform a linguistic analysis and study the usage of verb tenses in optimistic and pessimistic tweets, as well as the presence of polarity words associated with both types of tweets.",3.3 Linguistic Analysis,[0],[0]
This analysis is done at tweet level with 1/- 1 threshold.,3.3 Linguistic Analysis,[0],[0]
The reason for using the 1/-1 threshold is that we wanted to study the usage of verb tenses and polarity words in tweets that are clear optimistic or clear pessimistic (far from the decision boundary).,3.3 Linguistic Analysis,[0],[0]
"For this analysis, we used the part of speech tagger spaCy5 and assigned the verbs to their corresponding tenses according to the Penn Treebank Project; that is, the tags VBD and VBN correspond to past tense, VBG, VBZ , VBP correspond to present tense, whereas an MD tag followed by VB (possibly with a negation between them) corresponds to the future tense.
",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"5http://textanalysisonline.com/spacy-pos-tagging
As mentioned, a tweet was considered optimist if its manually annotated score was above 1 and pessimist if the score was below −1.",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"The numbers of tweets with past, present, and future tenses in the optimistic category are: 1,474, 7,444, and 561, respectively, whereas these numbers in the pessimistic category are: 1,276, 5,311, and 325, respectively.
",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"Figure 3 shows the normalized verb occurrences at past, present and future tenses in optimistic and pessimistic tweets.",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"As can be seen from the figure, the present tense is the most prevalent for both categories, although there are more present tense verbs in the optimistic category compared with the pessimistic one.",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"We can also observe that more past tense verbs occur in the pessimistic category and less future tense verbs in the pessimistic one.
",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"While there are some common verbs such as “be,” “have,” and “do,” that appear most frequently in both optimistic and pessimistic categories at all three tenses, there are some verbs that are more specific to one category than the other.",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
Examples of such verbs and their frequencies from both categories at the present tense are shown in Table 4.,3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"As we can see, optimism is characterized more by verbs with a positive connotation.",3.3.1 Verb Tenses in Optimism/Pessimism,[0],[0]
"Next, we analyze the association of polarity words from the positive and negative lexicons constructed by Hu and Liu (2004), in both tweet categories: optimism and pessimism.",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"Instead of using the presence or absence of the words from the two lexicons in tweets, we calculated the cosine similarity between the word embeddings of the words in the two lexicons with the words in the tweets.",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"If the similarity is above 0.8, then we consider the word from the corresponding lexicon to be present in the tweet (or synonym with a word in tweet).",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"Using the cosine similarity between the word embeddings of words in lexicons with words in tweets captures not only the exact match between the words (a cosine similarity of 1 for exact match), but also incorporates the semantic information that exists in the text.
",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"Although this word similarity computation relaxes the exact match/presence of a word in a tweet and aims at incorporating semantic similarity, a high similarity between antonyms may occur since word embeddings are known to not differentiate well between synonyms and antonyms, which tend to appear in similar contexts.
",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
Figure 4 shows the number of polarity words in optimistic and pessimistic tweets.,3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"As shown in the figure, more positive words appear in optimistic tweets compared with negative words (1,242 vs. 71), while there is not a substantial difference between the numbers of positive and negative words in pessimistic tweets (118 vs. 210).
",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
Table 5 shows the top most frequent polarity words associated with optimism and pessimism.,3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"As we can see, words with a negative polarity (e.g., bad) although not very frequent, still appear in optimistic tweets.",3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
This supports our intuition that a sentiment model is not enough to accurately predict pessimism and optimism in Twitter.,3.3.2 Polarity Words in Optimism/Pessimism,[0],[0]
"In this paper, we explored deep learning models for optimism and pessimism prediction in Twitter and showed that these models substantially outperform traditional classifiers such as Naı̈ve Bayes and Support Vector Machines.",4 Concluding Remarks,[0],[0]
"To our knowledge, this work is the first computational study that explores optimism and pessimism using deep learning.",4 Concluding Remarks,[0],[0]
We also showed that a sentiment classifier would not be sufficient for accurately predicting optimism and pessimism.,4 Concluding Remarks,[0],[0]
"This topic is less explored despite its importance in many applications such as identifying suicidal/depressive people.
",4 Concluding Remarks,[0],[0]
"Interesting future directions are: understanding how one’s age is correlated with optimism / pessimism; if one user is characterized by a mixture of topics, is that user optimist (pessimist) across all these topics?",4 Concluding Remarks,[0],[0]
"Thus, decomposing a user’s textual data into topic and correlating this with optimism and pessimism may be interesting to explore; last, studying how optimism and pessimism are affected by sarcasm.
",4 Concluding Remarks,[0],[0]
"As we started our study with a pessimistic quote from the movie “The Edge of Seventeen,” we end our study with a quote from the same movie, with a positive sentiment and full of optimism:
“Life’s about taking risks.",4 Concluding Remarks,[0],[0]
Don’t be afraid to put yourself out there.”,4 Concluding Remarks,[0],[0]
All authors contributed equally.,Acknowledgments,[0],[0]
"LP Dinu was supported by UEFISCDI, project #53BG/2016.",Acknowledgments,[0],[0]
We thank our reviewers for their constructive comments and feedback.,Acknowledgments,[0],[0]
"Identifying optimistic and pessimistic viewpoints and users from Twitter is useful for providing better social support to those who need such support, and for minimizing the negative influence among users and maximizing the spread of positive attitudes and ideas.",abstractText,[0],[0]
"In this paper, we explore a range of deep learning models to predict optimism and pessimism in Twitter at both tweet and user level and show that these models substantially outperform traditional machine learning classifiers used in prior work.",abstractText,[0],[0]
"In addition, we show evidence that a sentiment classifier would not be sufficient for accurately predicting optimism and pessimism in Twitter.",abstractText,[0],[0]
"Last, we study the verb tense usage as well as the presence of polarity words in optimistic and pessimistic tweets.",abstractText,[0],[0]
Exploring Optimism and Pessimism in Twitter Using Deep Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4785–4790 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4785",text,[0],[0]
"Recently, end-to-end Neural Machine Translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015) have achieved notable success.",1 Introduction,[0],[0]
"A remarkable characteristic of NMT is that the decoder, which is typically implemented using Recurrent Neural Network (RNN), can capture the features of the entire decoding history.",1 Introduction,[0],[0]
"This model
∗Zhisong Zhang was a graduate student at SJTU and a research intern at NICT when conducting this work.",1 Introduction,[0],[0]
"This work is partially supported by the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of MIC, Japan.",1 Introduction,[0],[0]
"Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15-ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04).",1 Introduction,[0],[0]
"Thanks a lot for the helpful discussions with Kehai Chen.
",1 Introduction,[0],[0]
"†Co-corresponding authors
with “cities” correspond to the nodes box ed in Figure 1 (only three hypotheses are listed for brevity).",1 Introduction,[0],[0]
"The negative log probabilities calculated by the model for the words predicted after “cities” are given in parentheses.
",1 Introduction,[0],[0]
"nodes inside the box represent the hidden features of partial hypotheses ending with “cities”.
does not depend on any independence assumptions and treats sequences with different prefixes as totally different hypotheses.",1 Introduction,[0],[0]
"However, many of the NMT output sequences are quite similar and they typically contain only local differences that do not influence future decoding significantly.
",1 Introduction,[0],[0]
Table 1 and Figure 1 present an example of such pattern of local differences in NMT decoding.,1 Introduction,[0],[0]
"As shown in Table 1, the three partial hypotheses that
Algorithm 1 Merging for Beam Search.",1 Introduction,[0],[0]
"Require: list of sorted candidates C; beam size k;
equivalence function Eq.",1 Introduction,[0],[0]
"Ensure: list of candidates surviving in the beam: C′.
1:",1 Introduction,[0],[0]
C′ =,1 Introduction,[0],[0]
[ ] 2: # Scan according to the sorted order.,1 Introduction,[0],[0]
3: for c in C: 4: merge flag = False 5: #,1 Introduction,[0],[0]
Check previous surviving states for merging.,1 Introduction,[0],[0]
6: for s in C′: 7: # Check with candidate merger states.,1 Introduction,[0],[0]
"8: for s′ in sequence(s): 9: if Eq(c, s′) and score(c)<score(s′):
10: merge flag = True 11:",1 Introduction,[0],[0]
# Pruning by the merger.,1 Introduction,[0],[0]
12: if not merge flag: 13: C′.append(c) 14:,1 Introduction,[0],[0]
# Pruning by the beam size.,1 Introduction,[0],[0]
15: if len(C′),1 Introduction,[0],[0]
">= k: 16: break 17: return C′
end with “cities” share similar patterns.",1 Introduction,[0],[0]
"Firstly, as shown in Figure 1, their hidden layer features are close in the latent space.",1 Introduction,[0],[0]
"Moreover, for future predictions, the model predicts identical sequences and gives similar scores for them.",1 Introduction,[0],[0]
"Although going through different paths, these partial hypotheses appear to be similar or likely equivalent.
",1 Introduction,[0],[0]
"Intuitively, for efficiency, we do not need to expand all of these partial hypotheses (states) since they have similar future predictions.",1 Introduction,[0],[0]
"In fact, this corresponds to the idea of hypothesis recombination (also known as state merging, which will be used interchangeably) from traditional PhraseBased Statistical Machine Translation (PBSMT) (Koehn et al., 2003).",1 Introduction,[0],[0]
"Given a method to find mergeable states, we can employ recombination in NMT decoding as well.
",1 Introduction,[0],[0]
"In this paper, we adopt the mechanism of recombination in NMT decoding based on the definition of “equivalence” of partial hypotheses.",1 Introduction,[0],[0]
"Heuristically, we try a simple n-gram suffix based equivalence function and apply it to beam search without adding any neural computation cost.",1 Introduction,[0],[0]
"Through experiments on two large-scale translation tasks, we show that it can help to make the decoding more efficient.
",1 Introduction,[0],[0]
"Most recent NMT studies have focused on model improvement (Luong et al., 2015; Tu et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017), and only a few have studied the search problem directly.",1 Introduction,[0],[0]
"For example, Khayrallah et al. (2017) and Stahlberg et al. (2016) explored searching on lattices generated by traditional Statistical Machine Translation (SMT).",1 Introduction,[0],[0]
"In addition, Freitag and Al-
Onaizan (2017) investigated different beam search pruning strategies; however, they primarily focused on pruning candidates locally.",1 Introduction,[0],[0]
"(Niehues et al., 2017) analyzed the effects of modeling and searching, but focused on re-ranking analysis.",1 Introduction,[0],[0]
"Rather than considering candidates from other model’s k-best lists, we focus on the own exploration space of a single NMT model and provide a method for more efficient searching.",1 Introduction,[0],[0]
"For state merging, “equivalence” should be defined from the aspect of future predictions: states with the same predictions in the future decoding process can be regarded as equivalent.",2 Method,[0],[0]
"We use an equivalence function Eq(s1, s2) to denote that the two states s1 and s2 can be regarded as equivalent.
",2 Method,[0],[0]
"With the concept of equivalence, we can build the method of recombination over it.",2 Method,[0],[0]
There are mainly two problems to solve: 1.,2 Method,[0],[0]
How to merge states given function Eq?,2 Method,[0],[0]
(§2.1) 2.,2 Method,[0],[0]
How to obtain this equivalence function?,2 Method,[0],[0]
(§2.2),2 Method,[0],[0]
"To adopt an equivalence function Eq(s1, s2) to merge states in a search process, we need to specify the logic of the merging mechanism.",2.1 Search with Merging,[0],[0]
"Here, without loss of generality, we specifically focus on the typical beam search.
",2.1 Search with Merging,[0],[0]
We adopt merging in NMT beam search with a simple method: retaining the word-level search process and adding a state merger when pruning the beam at each time step.,2.1 Search with Merging,[0],[0]
"Algorithm 1 shows the proposed merging-enhanced pruning method.
",2.1 Search with Merging,[0],[0]
"Ordinary beam search only prunes candidates based on beam size (Lines 15-16), while the proposed method adds a merger to prune extra equivalent states (Lines 6-10).",2.1 Search with Merging,[0],[0]
"To manage the merging process, candidate list C are ordered1 by model score and considered in turn.",2.1 Search with Merging,[0],[0]
"When checking equivalence for one candidate state c, we consider all current-step surviving states and their previous-step antecedences.",2.1 Search with Merging,[0],[0]
"We include previousstep states, because equivalent states may have different sequence lengths and thus not be in the same beam-search step.",2.1 Search with Merging,[0],[0]
"In Line 8, we define “sequence” as a function of obtaining the possible states that
1In plain beam search, the candidates may not need to be sorted.",2.1 Search with Merging,[0],[0]
"We use a local selector to make the sorting efficient: a local k-best selector is first applied on each previous-step candidate states, making the size of the candidate list at most k ∗ k rather than k ∗ |V",2.1 Search with Merging,[0],[0]
"|, where |V | is the vocabulary size.
can merge the current candidate c.",2.1 Search with Merging,[0],[0]
"If a candidate state c is not merged with any higher-ranked state, it is added to the surviving list C ′",2.1 Search with Merging,[0],[0]
"(Line 13) and can possibly merge the lower-ranked ones later.
",2.1 Search with Merging,[0],[0]
"When deciding whether to merge, we also consider a criterion on model scores: we only merge state c when its score is lower than s′. Since we also consider previous-step states with different sequence lengths, a length reward λ is added for this comparison of partial hypotheses: score(s) =∑
y∈s λ+ log p(y).",2.1 Search with Merging,[0],[0]
"We also attempted length normalization, but found it performed slightly worse.
",2.1 Search with Merging,[0],[0]
"The merged partial hypotheses can be stored, and by assuming that their future predictions will be the same as their mergers, a lattice-like translation graph can be obtained.",2.1 Search with Merging,[0],[0]
We can further extract k-best list from this structure using another beam-search on the lattice (also with length reward when comparing partial hypotheses).,2.1 Search with Merging,[0],[0]
"Note that this beam search process can be fast, since we reuse the model scores from previous search and no extra neural computations will be included.",2.1 Search with Merging,[0],[0]
"Finding an exact equivalence function for NMT is difficult, because future predictions relies on the features from the entire previous sequence and any different sequences are not the same according to the NMT model.",2.2 Equivalence Function,[0],[0]
"Here, we consider a n-gram suffix based heuristic approximation for this problem.
",2.2 Equivalence Function,[0],[0]
"We adopt an approximate equivalence function:
Eq′(s1, s2) ≡ s1.suffix(n) = s2.suffix(n) ∧ |s1.length− s2.length|",2.2 Equivalence Function,[0],[0]
"< r
Here, suffix(n) represents the n-gram suffix of the sequence of a state, and r is the threshold for the length different of the two states.
",2.2 Equivalence Function,[0],[0]
"This definition of equivalence only considers a subset of state features, which are inspired by PBSMT.",2.2 Equivalence Function,[0],[0]
"In PBSMT, different sequences could lead to states with identical features based on n-gram suffix, and these states are exactly equivalent.",2.2 Equivalence Function,[0],[0]
"Although this is not the case for NMT, the subset may encodes important and relevant features.
",2.2 Equivalence Function,[0],[0]
"Although this function is simple and brings extra approximation, it has the merit of efficiency.",2.2 Equivalence Function,[0],[0]
"In Algorithm 1, we can store the n-gram features of the surviving states in a hash-map and replace the for-loop checking (Line 6-10) with hashing, making the extra time-complexity O(1) for each state.",2.2 Equivalence Function,[0],[0]
"During experiments, we found the extra cost
brought by feature matching is far less than the cost of original neural computation.",2.2 Equivalence Function,[0],[0]
The proposed method was evaluated on two translation tasks: NIST Chinese-English (Zh-En) and WMT English-German (En-De).,3 Experiments and Analysis,[0],[0]
"For Zh-En, the training set comprised 1.4M sentences pairs from LDC corpora.",3 Experiments and Analysis,[0],[0]
NIST 02 was selected as the development set and NIST 03 to 06 were used for testing.,3 Experiments and Analysis,[0],[0]
"For En-De, 4.5M WMT training data were utilized, the concatenation of newstest 2012 and 2013 was adopted as the development set, and newstest 2014 to 2016 were adopted as the test set.
",3 Experiments and Analysis,[0],[0]
"We implemented2 an attentional RNN-based NMT model and its decoder in Python with the DyNet toolkit (Neubig et al., 2017).",3 Experiments and Analysis,[0],[0]
All the experiments were carried out on one P100 GPU.,3 Experiments and Analysis,[0],[0]
"For Zh-En, we set the vocabulary size of both sides to 30K, and for En-De, we adopted 50K BPE operations (Sennrich et al., 2016).",3 Experiments and Analysis,[0],[0]
"The evaluation metric was tokenized BLEU (Papineni et al., 2002) calculated by multi-bleu.perl.",3 Experiments and Analysis,[0],[0]
"Detailed settings can be found in the supplementary material.
",3 Experiments and Analysis,[0],[0]
We added a local threshold pruner to exclude unlikely words whose probabilities were less than 10% of the highest and adopted length normalization for final hypotheses ranking.,3 Experiments and Analysis,[0],[0]
"For comparing partial hypotheses, the length reward λ was set to 1.0 and 0.4 for Zh-En and En-De, respectively.",3 Experiments and Analysis,[0],[0]
"For the equivalence function, we utilized a suffix of 4- gram and a length difference threshold r of 2.
",3 Experiments and Analysis,[0],[0]
These hyper-parameters were set by preliminary experiments.,3 Experiments and Analysis,[0],[0]
"For the length difference threshold r, we found that relatively small r like 1 or 2 was better than larger ones, which is reasonable since if the merged hypotheses differs too much in length, there are higher chances that they covered different information.",3 Experiments and Analysis,[0],[0]
"For n-gram suffix, we found smaller n-grams made more bad merges and 4-gram is a reasonably good choice, slightly larger ones gave slightly worse results and also less chances of recombination.",3 Experiments and Analysis,[0],[0]
Figure 2 show the results of various beam sizes on the concatenation of all test sets.,3.1 Results,[0],[0]
"Separate results are given in the supplementary material.
",3.1 Results,[0],[0]
"As shown by the speed curves, merging adds little extra cost (less than 10%) to decoding at
2https://github.com/zzsfornlp/znmt-merge
the same beam size.",3.1 Results,[0],[0]
"Moreover, since bringing no extra neural computations, the proposed merging mechanism is transparent to neural architectures and easy to adopt.",3.1 Results,[0],[0]
"In our experiments, we used batched decoding on GPU and merging did not influence the efficiency of this implementation.
",3.1 Results,[0],[0]
"For translation quality, the results indicate that the proposed methods can yield improvements at various beam sizes for Zh-En and small beam sizes for En-De.",3.1 Results,[0],[0]
"Moreover, in some way, merging can make the search more efficient.",3.1 Results,[0],[0]
"For example, in both datasets, merge-enhanced searchers with beam-size 6 can obtain comparable or better results compared to those of ordinary searchers with beam-size 12 (on BLEU, 37.17 vs. 37.11 for ZhEn, 24.64 vs. 24.67 for En-De).",3.1 Results,[0],[0]
"As for decoding speed, the one of beam-size 6 can be more than twice of the one of beam-size 12 (over 200 tokens/second vs. around 100 tokens/second).",3.1 Results,[0],[0]
"That is to say, with merging, we can achieve similar translation quality with a smaller beam size, which leads to higher decoding speed.
",3.1 Results,[0],[0]
"The results show that for large beam sizes, expanding explored search space by increasing beam size or adopting merging helps more in Zh-En than En-De.",3.1 Results,[0],[0]
"A possible explanation for this is that in NIST Zh-En dataset, each source sentences has four references for evaluation, which encourages the diversity brought by expanding reached search space.",3.1 Results,[0],[0]
"In Table 2, we compare the BLEU scores with multiple and single references on several beam sizes, and the single-reference results does not always increase along the beam size like the multiple ones.",3.1 Results,[0],[0]
"The En-De dataset also has only one reference and is similar to this case.
",3.1 Results,[0],[0]
The results also show that expanding explored search space does not always bring improvements.,3.1 Results,[0],[0]
"This concerns more on modeling than searching and corresponds with previous findings on the relations between NMT searching and modeling (Tu et al., 2016a; Niehues et al., 2017; Li et al., 2018).
",3.1 Results,[0],[0]
The potential of the proposed method might be better realized with improved NMT models.,3.1 Results,[0],[0]
We further analyzed the merge-enhanced search process.,3.2 Analysis,[0],[0]
"For these analyses, we mainly checked decoding with a beam size of 10 on Zh-En dataset.
",3.2 Analysis,[0],[0]
"Frequency of Merging First, we investigated how often recombination occurs and how much it expands the explored output space.",3.2 Analysis,[0],[0]
"For a beam size of 10, with influences from the local pruner and the proposed merger, the average expanding size is 7.60 for each step, and the average number of merger-pruned partial hypotheses is 0.61 per step (22.5 per sentence).",3.2 Analysis,[0],[0]
This indicates that a partial hypothesis is recombined in every two steps.,3.2 Analysis,[0],[0]
"The output translation graph can hold much more output space than the original k-best list, and we found that on average the possible output sequences were averagely 200 times the beam size.",3.2 Analysis,[0],[0]
"Figure 3 shows an example of the output translation graph.
",3.2 Analysis,[0],[0]
"Merging and Similarity of Hidden States It is nearly impossible to explore such a large space with an exact NMT model; thus, we depend on the assumption that merged hypotheses have nearly the same features.",3.2 Analysis,[0],[0]
"To evaluate this assumption, we calculated the similarity between the hidden layers of the merged partial hypotheses.",3.2 Analysis,[0],[0]
"Among the 122772 merge points in 5453 Zh-En sentences, the average cosine similarity (in range [−1, 1]) was 0.986, which indicates that the recombinations are reasonable.",3.2 Analysis,[0],[0]
"In addition, we tried adding simple cosine similarity constraints (using another
threshold) in the equivalence function, however, we found that this does not bring obvious additional benefits.
",3.2 Analysis,[0],[0]
Effects of Merging We further conducted comparisons between the predictions of ordinary and merge-enhanced beam search.,3.2 Analysis,[0],[0]
"First, we investigated the model scores of their predictions.",3.2 Analysis,[0],[0]
"As shown in Table 3, we selected “Beam=10, no merge” as the basic setting, and compared the predictions of other settings with it.",3.2 Analysis,[0],[0]
"Overall, the merge-enhanced searcher can obtain higher model score predictions, which suggests its stronger search ability, because the goal of searching is to return hypotheses with higher model scores.
",3.2 Analysis,[0],[0]
"Moreover, we tried a re-ranking experiment on 100-best lists with 4-checkpoint-model-ensemble, and only found similar slight improvements for plain and merge-enhanced search.",3.2 Analysis,[0],[0]
"Nevertheless, since merge-enhanced search can obtain a output translation graph, we expect that the graph can contain more diverse hypotheses.
",3.2 Analysis,[0],[0]
"To verify this, we compared the oracle BLEU scores within the reached space.",3.2 Analysis,[0],[0]
"To extract or-
acle hypotheses from the translation graphs, we simply adopted approximate Partial BLEU Oracle (Dreyer et al., 2007; Sokolov et al., 2012).",3.2 Analysis,[0],[0]
"Merge-based searcher could obtain an oracle score of 47.83, while ordinary beam searcher could only get 42.57.",3.2 Analysis,[0],[0]
Only by increasing the beam size up to 100 could the ordinary beam searcher achieve a better result of 48.74.,3.2 Analysis,[0],[0]
This indicates that recombination helps to touch more output space.,3.2 Analysis,[0],[0]
"In this work, 1) we show that decoding with heuristic recombination can obtain similar translation qualities with smaller beam sizes, thus increasing efficiency, and, 2) we empirically explore the decoding process and analyze the influences of recombination from various aspects.
",4 Conclusion and Discussion,[0],[0]
"Although the improvements brought by recombination depend on careful refinements of the model, this concerns more on modeling, since the goal of decoding is to find hypotheses with higher model scores.",4 Conclusion and Discussion,[0],[0]
The potential of recombination may be further realized by improving how the output sequences are modeled.,4 Conclusion and Discussion,[0],[0]
"Another interesting topic will be the combination with SMT or extra larger language models (Wang et al., 2013, 2014).
",4 Conclusion and Discussion,[0],[0]
"For the equivalence function, there can also be extensions.",4 Conclusion and Discussion,[0],[0]
"For example, a model-based equivalence function can be trained by using the neural features (hidden layers in RNN).",4 Conclusion and Discussion,[0],[0]
"However, modelbased equivalence functions may bring extra neural computation cost and be harder to efficiently implemented.",4 Conclusion and Discussion,[0],[0]
"In this work, we focus on the merging mechanism and leave the study of equivalence function for future work.",4 Conclusion and Discussion,[0],[0]
"In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations.",abstractText,[0],[0]
This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are.,abstractText,[0],[0]
"However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions.",abstractText,[0],[0]
"In this work, we introduce recombination in NMT decoding based on the concept of the “equivalence” of partial hypotheses.",abstractText,[0],[0]
"Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding.",abstractText,[0],[0]
"Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.",abstractText,[0],[0]
Exploring Recombination for Efficient Decoding of Neural Machine Translation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 632–637 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
632",text,[0],[0]
Neural vector representations have become ubiquitous in all subfields of natural language processing.,1 Introduction,[0],[0]
"For the case of word vectors, important properties of the representations have been studied, including their linear substructures (Mikolov et al., 2013; Levy and Goldberg, 2014), the linear superposition of word senses (Arora et al., 2016b), and the nexus to pointwise mutual information scores between co-occurring words (Arora et al., 2016a).
",1 Introduction,[0],[0]
"However, thus far, only little is known about the properties of sentence embeddings.",1 Introduction,[0],[0]
Sentence embedding methods attempt to encode a variablelength input sentence into a fixed length vector.,1 Introduction,[0],[0]
"A number of such sentence embedding methods have been proposed in recent years (Le and Mikolov, 2014; Kiros et al., 2015; Wieting et al., 2015; Conneau et al., 2017; Arora et al., 2017).
",1 Introduction,[0],[0]
"Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments of semantic relatedness, typically with respect to the SemEval Semantic Textual Similarity competitions.",1 Introduction,[0],[0]
"The SICK
dataset (Marelli et al., 2014) was created to better benchmark the effectiveness of different models across a broad range of challenging lexical, syntactic, and semantic phenomena, in terms of both similarities and the ability to be predictive of entailment.",1 Introduction,[0],[0]
"However, even on SICK, oftentimes very shallow methods prove effective at obtaining fairly competitive results (Wieting et al., 2015).",1 Introduction,[0],[0]
Adi et al. investigated to what extent different embedding methods are predictive of i),1 Introduction,[0],[0]
"the occurrence of words in the original sentence, ii) the order of words in the original sentence, and iii) the length of the original sentence (Adi et al., 2016, 2017).",1 Introduction,[0],[0]
"Belinkov et al. (2017) inspected neural machine translation systems with regard to their ability to acquire morphology, while Shi et al. (2016) investigated to what extent they learn source side syntax.",1 Introduction,[0],[0]
Wang et al. (2016) argue that the latent representations of advanced neural reading comprehension architectures encode information about predication.,1 Introduction,[0],[0]
"Finally, sentence embeddings have also often been investigated in classification tasks such as sentiment polarity or question type classification (Kiros et al., 2015).",1 Introduction,[0],[0]
"Concurrently with our research, Conneau et al. (2018) investigated to what extent one can learn to classify specific syntactic and semantic properties of sentences using large amounts of training data (100,000 instances) for each property.
",1 Introduction,[0],[0]
"Overall, still, remarkably little is known about what specific semantic properties are directly reflected by such embeddings.",1 Introduction,[0],[0]
"In this paper, we specifically focus on a few select aspects of sentence semantics and inspect to what extent prominent sentence embedding methods are able to capture them.",1 Introduction,[0],[0]
Our framework generates triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.,1 Introduction,[0],[0]
"To conduct our analysis, we proceed by generating new phenomena-specific evaluation datasets.
",2 Analysis,[0],[0]
Our starting point is that even minor alterations of a sentence may lead to notable shifts in meaning.,2 Analysis,[0],[0]
"For instance, a sentence S such as A rabbit is jumping over the fence and a sentence S∗ such as A rabbit is not jumping over the fence diverge with respect to many of the inferences that they warrant.",2 Analysis,[0],[0]
"Even if sentence S∗ is somewhat less idiomatic than alternative wordings such as There are no rabbits jumping over the fence, we nevertheless expect sentence embedding methods to interpret both correctly, just as humans do.
",2 Analysis,[0],[0]
"Despite the semantic differences between the two sentences due to the negation, we still expect the cosine similarity between their respective embeddings to be fairly high, in light of their semantic relatedness in touching on similar themes.
",2 Analysis,[0],[0]
"Hence, only comparing the similarity between sentence pairs of this sort does not easily lend itself to insightful automated analyses.",2 Analysis,[0],[0]
"Instead, we draw on another key idea.",2 Analysis,[0],[0]
It is common for two sentences to be semantically close despite differences in their specific linguistic realizations.,2 Analysis,[0],[0]
"Building on the previous example, we can construct a further contrasting sentence S+ such as A rabbit is hopping over the fence.",2 Analysis,[0],[0]
"This sentence is very close in meaning to sentence S, despite minor differences in the choice of words.",2 Analysis,[0],[0]
"In this case, we would want for the semantic relatedness between sentences S and S+ to be assessed as higher than between sentence S and sentence",2 Analysis,[0],[0]
"S∗.
We refer to this sort of scheme as sentence triplets.",2 Analysis,[0],[0]
We rely on simple transformations to generate several different sets of sentence triplets.,2 Analysis,[0],[0]
"In the following, we first describe the kinds of transformations we apply to generate altered sentences.",2.1 Sentence Modification Schemes,[0],[0]
"Subsequently, in Section 2.2, we shall consider how to assemble such sentences into sentence triplets of various kinds so as to assess different semantic properties of sentence embeddings.
",2.1 Sentence Modification Schemes,[0],[0]
Not-Negation.,2.1 Sentence Modification Schemes,[0],[0]
"We negate the original sentence by inserting the negation marker not before the first verb of the original sentence A to generate a new sentence B, including contractions as appropriate, or removing negations when they are already present, as in:
A: The young boy is climbing the wall made of rock.
",2.1 Sentence Modification Schemes,[0],[0]
B:,2.1 Sentence Modification Schemes,[0],[0]
"The young boy isn’t climbing the wall made of rock.
",2.1 Sentence Modification Schemes,[0],[0]
Quantifier-Negation.,2.1 Sentence Modification Schemes,[0],[0]
"We prepend the quantifier expression there is no to original sentences beginning with A to generate new sentences.
",2.1 Sentence Modification Schemes,[0],[0]
A: A girl is cutting butter into two pieces.,2.1 Sentence Modification Schemes,[0],[0]
B:,2.1 Sentence Modification Schemes,[0],[0]
"There is no girl cutting butter into two pieces.
",2.1 Sentence Modification Schemes,[0],[0]
Synonym Substitution.,2.1 Sentence Modification Schemes,[0],[0]
"We substitute the verb in the original sentence with an appropriate synonym to generate a new sentence B.
A: The man is talking on the telephone.",2.1 Sentence Modification Schemes,[0],[0]
B:,2.1 Sentence Modification Schemes,[0],[0]
"The man is chatting on the telephone.
",2.1 Sentence Modification Schemes,[0],[0]
Embedded Clause Extraction.,2.1 Sentence Modification Schemes,[0],[0]
"For those sentences containing verbs such as say, think with embedded clauses, we extract the clauses as the new sentence.
",2.1 Sentence Modification Schemes,[0],[0]
A: Octel said the purchase was expected.,2.1 Sentence Modification Schemes,[0],[0]
B:,2.1 Sentence Modification Schemes,[0],[0]
"The purchase was expected.
",2.1 Sentence Modification Schemes,[0],[0]
Passivization.,2.1 Sentence Modification Schemes,[0],[0]
"Sentences that are expressed in active voice are changed to passive voice.
",2.1 Sentence Modification Schemes,[0],[0]
A: Harley asked Abigail to bake some muffins.,2.1 Sentence Modification Schemes,[0],[0]
B:,2.1 Sentence Modification Schemes,[0],[0]
"Abigail is asked to bake some muffins.
",2.1 Sentence Modification Schemes,[0],[0]
Argument Reordering.,2.1 Sentence Modification Schemes,[0],[0]
"For sentences matching the structure “〈somebody〉 〈verb〉 〈somebody〉 to 〈do something〉”, we swap the subject and object of the original sentence A to generate a new sentence B.
A: Matilda encouraged Sophia to compete in a match.
",2.1 Sentence Modification Schemes,[0],[0]
"B: Sophia encouraged Matilda to compete in a match.
",2.1 Sentence Modification Schemes,[0],[0]
Fixed Point Inversion.,2.1 Sentence Modification Schemes,[0],[0]
We select a word in the sentence as the pivot and invert the order of words before and after the pivot.,2.1 Sentence Modification Schemes,[0],[0]
"The intuition here is that this simple corruption is likely to result in a new sentence that does not properly convey the original meaning, despite sharing the original words in common with it.",2.1 Sentence Modification Schemes,[0],[0]
"Hence, these sorts of corruptions can serve as a useful diagnostic.
",2.1 Sentence Modification Schemes,[0],[0]
"A: A dog is running on concrete and is holding a blue ball
B: concrete and is holding a blue ball a dog is running on.",2.1 Sentence Modification Schemes,[0],[0]
"Given the above forms of modified sentences, we induce five evaluation datasets, consisting of triplets of sentences as follows.
1.",2.2 Sentence Triplet Generation,[0],[0]
"Negation Detection: Original sentence, Synonym Substitution, Not-Negation
With this dataset, we seek to explore how well sentence embeddings can distinguish sentences with similar structure and opposite meaning, while using Synonym Substitution as the contrast set.",2.2 Sentence Triplet Generation,[0],[0]
"We would want the similarity between the original sentence and the negated sentence to be lower than that between the original sentence and its synonym version.
2.",2.2 Sentence Triplet Generation,[0],[0]
"Negation Variants: Quantifier-Negation, Not-Negation, Original sentence
In the second dataset, we aim to investigate how well the sentence embeddings reflect negation quantifiers.",2.2 Sentence Triplet Generation,[0],[0]
"We posit that the similarity between the Quantifier-Negation and Not-Negation versions should be a bit higher than between either the Not-Negation or the Quantifier-Negation and original sentences.
3.",2.2 Sentence Triplet Generation,[0],[0]
"Clause Relatedness: Original sentence, Embedded Clause Extraction, Not-Negation
In this third set, we want to explore whether the similarity between a sentence and its embedded clause is higher than between a sentence and its negation.
4.",2.2 Sentence Triplet Generation,[0],[0]
"Argument Sensitivity: Original sentence, Passivization, Argument Reordering
With this last test, we wish to ascertain whether the sentence embeddings succeed in distinguishing semantic information from structural information.",2.2 Sentence Triplet Generation,[0],[0]
"Consider, for instance, the following triplet.
",2.2 Sentence Triplet Generation,[0],[0]
S: Lilly loves Imogen.,2.2 Sentence Triplet Generation,[0],[0]
S+,2.2 Sentence Triplet Generation,[0],[0]
:,2.2 Sentence Triplet Generation,[0],[0]
Imogen is loved by Lilly. S∗:,2.2 Sentence Triplet Generation,[0],[0]
"Imogen loves Lilly.
",2.2 Sentence Triplet Generation,[0],[0]
"Here, S and S+ mostly share the same meaning, whereas S+ and S∗ have a similar word order, but do not possess the same specific meaning.",2.2 Sentence Triplet Generation,[0],[0]
"If the sentence embeddings focus more on semantic cues, then the similarity
between S and S+ ought to be larger than that between S+ and S∗. If the sentence embedding however is easily misled by matching sentence structures, the opposite will be the case.
",2.2 Sentence Triplet Generation,[0],[0]
5.,2.2 Sentence Triplet Generation,[0],[0]
"Fixed Point Reorder: Original sentence, Semantically equivalent sentence, Fixed Point Inversion
With this dataset",2.2 Sentence Triplet Generation,[0],[0]
", our objective is to explore how well the sentence embeddings account for shifts in meaning due to the word order in a sentence.",2.2 Sentence Triplet Generation,[0],[0]
We select sentence pairs from the SICK dataset according to their semantic relatedness score and entailment labeling.,2.2 Sentence Triplet Generation,[0],[0]
Sentence pairs with a high relatedness score and the Entailment tag are considered semantically similar sentences.,2.2 Sentence Triplet Generation,[0],[0]
"We rely on the Levenshtein Distance as a filter to ensure a structural similarity between the two sentences, i.e., sentence pairs whose Levenshtein Distance is sufficiently high are regarded as eligible.
",2.2 Sentence Triplet Generation,[0],[0]
"Additionally, we use the Fixed Point Inversion technique to generate a contrastive sentence.",2.2 Sentence Triplet Generation,[0],[0]
The resulting sentence likely no longer adequately reflects the original meaning.,2.2 Sentence Triplet Generation,[0],[0]
"Hence, we expect that, on average, the similarity between the original sentence and the semantically similar sentence should be higher than that between the original sentence and the contrastive version.",2.2 Sentence Triplet Generation,[0],[0]
We now proceed to describe our experimental evaluation based on this paradigm.,3 Experiments,[0],[0]
"Using the aforementioned triplet generation methods, we create the evaluation datasets listed in Table 1, drawing on source sentences from SICK, Penn Treebank WSJ and MSR Paraphase corpus.",3.1 Datasets,[0],[0]
"Although the process to modify the sentences is automatic, we rely on human annotators to double-check the results for grammaticality and semantics.",3.1 Datasets,[0],[0]
"This is particularly important for synonym substitution, for which we relied on WordNet (Fellbaum, 1998).",3.1 Datasets,[0],[0]
"Unfortunately, not all synonyms are suitable as replacements in a given context.",3.1 Datasets,[0],[0]
"In our experiments, we compare three particularly prominent sentence embedding methods:
1.",3.2 Embedding Methods,[0],[0]
GloVe Averaging (GloVe Avg.):,3.2 Embedding Methods,[0],[0]
The simple approach of taking the average of the word vectors for all words in a sentence.,3.2 Embedding Methods,[0],[0]
"Although this method neglects the order of words entirely, it can fare reasonably well on some of the most commonly invoked forms of evaluation (Wieting et al., 2015; Arora et al., 2017).",3.2 Embedding Methods,[0],[0]
"Note that we here rely on regular unweighted GloVe vectors (Pennington et al., 2014) instead of fine-tuned or weighted word vectors.
2.",3.2 Embedding Methods,[0],[0]
"Concatenated P-Mean Embeddings (PMeans): Rücklé et al. (2018) proposed concatenating different p-means of multiple kinds of word vectors.
3.",3.2 Embedding Methods,[0],[0]
"Sent2Vec: Pagliardini et al. (2018) proposed a method to learn word and n-gram embeddings such that the average of all words and n-grams in a sentence can serve as a highquality sentence vector.
4.",3.2 Embedding Methods,[0],[0]
"The Skip-Thought Vector approach (SkipThought) by Kiros et al. (2015) applies the neighbour prediction intuitions of the word2vec Skip-Gram model at the level of entire sentences, as encoded and decoded via recurrent neural networks.",3.2 Embedding Methods,[0],[0]
"The method trains an encoder to process an input sentence such that the resulting latent representation is optimized for predicting neighbouring sentences via the decoder.
5.",3.2 Embedding Methods,[0],[0]
"InferSent (Conneau et al., 2017) is based on supervision from an auxiliary task, namely the Stanford NLI dataset.",3.2 Embedding Methods,[0],[0]
Negation Detection.,3.3 Results and Discussion,[0],[0]
"Table 2 lists the results for the Negation Detection dataset, where S, S+, S∗
refer to the original, Synonym Substitution, and Not-Negation versions of the sentences, respectively.",3.3 Results and Discussion,[0],[0]
"For each of the considered embedding methods, we first report the average cosine similarity scores between all relevant sorts of pairings of two sentences, i.e. between the original and the Synonym-Substitution sentences (S and S+), between original and Not-Negated (S and S∗), and between Not-Negated and Synonym-Substitution (S+ and S∗).",3.3 Results and Discussion,[0],[0]
"Finally, in the last column, we report the Accuracy, computed as the percentage of sentence triplets for which the proximity relationships were as desired, i.e., the cosine similarity between the original and synonym-substituted versions was higher than the similarity between that same original and its Not-Negation version.
",3.3 Results and Discussion,[0],[0]
"On this dataset, we observe that GloVe Avg. is more often than not misled by the introduction of synonyms, although the corresponding word vector typically has a high cosine similarity with the original word’s embedding.",3.3 Results and Discussion,[0],[0]
"In contrast, both InferSent and SkipThought succeed in distinguishing unnegated sentences from negated ones.
",3.3 Results and Discussion,[0],[0]
Negation Variants.,3.3 Results and Discussion,[0],[0]
"In Table 3, S, S+, S∗ refer to the original, Not-Negation, and QuantifierNegation versions of a sentence, respectively.",3.3 Results and Discussion,[0],[0]
Accuracy in this problem is defined as percentage of sentence triples whose similarity between S+ and S∗ is the higher than similarity between S and S+ and S+ and S∗,3.3 Results and Discussion,[0],[0]
The results of both averaging of word embeddings.,3.3 Results and Discussion,[0],[0]
and SkipThought are dismal in terms of the accuracy.,3.3 Results and Discussion,[0],[0]
"InferSent, in contrast, appears to have acquired a better understanding of negation quantifiers, as these are commonplace in many NLI datasets.
",3.3 Results and Discussion,[0],[0]
Clause Relatedness.,3.3 Results and Discussion,[0],[0]
"In Table 4, S, S+, S∗ refer to original, Embedded Clause Extraction, and Not-Negation, respectively.",3.3 Results and Discussion,[0],[0]
"Although not particularly more accurate than random guessing, among the considered approaches, Sent2vec fares best in distinguishing the embedded clause of a sentence
from a negation of said sentence.",3.3 Results and Discussion,[0],[0]
"For a detailed analysis, we can divide the sentence triplets in this dataset into two categories as exemplified by the following examples:
a)",3.3 Results and Discussion,[0],[0]
Copperweld said it doesn’t expect a protracted strike. —,3.3 Results and Discussion,[0],[0]
Copperweld said it expected a protracted strike.,3.3 Results and Discussion,[0],[0]
"— It doesn’t expect a protracted strike.
",3.3 Results and Discussion,[0],[0]
"b) ”We made our own decision,” he said.",3.3 Results and Discussion,[0],[0]
"— ”We didn’t make our own decision,” he said.",3.3 Results and Discussion,[0],[0]
"— We made our own decision.
",3.3 Results and Discussion,[0],[0]
"For cases resembling a), the average SkipThought similarity between the sentence and its Not-Negation version is 79.90%, while for cases resembling b), it is 26.71%.",3.3 Results and Discussion,[0],[0]
"The accuracy of SkipThought on cases resembling a is 36.90%, and the accuracy of SkipThought on cases like b is only 0.75%",3.3 Results and Discussion,[0],[0]
It seems plausible that SkipThought is more sensitive to the word order due to the recurrent architecture.,3.3 Results and Discussion,[0],[0]
"Infersent also achieved better performance on sentences resembling a) compared with sentences resembling b), its accuracy on these two structures is 28.37% and 15.73% respectively.
",3.3 Results and Discussion,[0],[0]
Argument Sensitivity.,3.3 Results and Discussion,[0],[0]
"In Table 5, S, S+, S∗ to refer to the original sentence, it Passivization form, and the Argument Reordering version, respectively.",3.3 Results and Discussion,[0],[0]
"Although recurrent architectures are able to consider the order of words, unfortunately, none of the analysed approaches prove adept at distinguishing the semantic information from structural information in this case.
",3.3 Results and Discussion,[0],[0]
Fixed Point Reorder.,3.3 Results and Discussion,[0],[0]
"In Table 6, S, S+, S∗ to refer to the original sentence, its semantically
equivalent one and Fixed Point Inversion Version.",3.3 Results and Discussion,[0],[0]
"As Table 6 indicates, sentence embeddings based on means (GloVe averages), weighted means (Sent2Vec), or concatenation of p-mean embeddings (P-Means) are unable to distinguish the fixed point inverted sentence from the semantically equivalent one, as they do not encode sufficient word order information into the sentence embeddings.",3.3 Results and Discussion,[0],[0]
Sent2Vec does consider ngrams but these do not affect the results sufficiently.,3.3 Results and Discussion,[0],[0]
SkipThought and InferSent did well when the original sentence and its semantically equivalence share similar structure.,3.3 Results and Discussion,[0],[0]
"This paper proposes a simple method to inspect sentence embeddings with respect to their semantic properties, analysing three popular embedding methods.",4 Conclusion,[0],[0]
We find that both SkipThought and InferSent distinguish negation of a sentence from synonymy.,4 Conclusion,[0],[0]
InferSent fares better at identifying semantic equivalence regardless of the order of words and copes better with quantifiers.,4 Conclusion,[0],[0]
"SkipThoughts is more suitable for tasks in which the semantics of the sentence corresponds to its structure, but it often fails to identify sentences with different word order yet similar meaning.",4 Conclusion,[0],[0]
"In almost all cases, dedicated sentence embeddings from hidden states a neural network outperform a simple averaging of word embeddings.",4 Conclusion,[0],[0]
This research is funded in part by ARO grant no.,Acknowledgments,[0],[0]
W911NF-17-C-0098 as part of the DARPA SocialSim program.,Acknowledgments,[0],[0]
Neural vector representations are ubiquitous throughout all subfields of NLP.,abstractText,[0],[0]
"While word vectors have been studied in much detail, thus far only little light has been shed on the properties of sentence embeddings.",abstractText,[0],[0]
"In this paper, we assess to what extent prominent sentence embedding methods exhibit select semantic properties.",abstractText,[0],[0]
We propose a framework that generate triplets of sentences to explore how changes in the syntactic structure or semantics of a given sentence affect the similarities obtained between their sentence embeddings.,abstractText,[0],[0]
Exploring Semantic Properties of Sentence Embeddings,title,[0],[0]
Public debate forums provide to participants a common platform for expressing their point of view on a topic; they also present to participants the different sides of an argument.,1 Introduction,[0],[0]
"The latter can be particularly important: awareness of divergent points of view allows one, in theory, to make a fair and informed decision about an issue; and exposure to new points of view can furthermore possibly persuade a reader to change his overall stance on a topic.
Research in natural language processing (NLP) has begun to study persuasive writing and the role of language in persuasion.",1 Introduction,[0],[0]
"Tan et al. (2016) and Zhang et al. (2016), for example, have shown that the language of opinion holders or debaters and their patterns of interaction play a key role in changing the mind of a reader.",1 Introduction,[0],[0]
"At the same time,
research in psychology has shown that prior beliefs can affect our interpretation of an argument even when the argument consists of numbers and empirical studies that would seemingly belie misinterpretation (Lord et al., 1979; Vallone et al., 1985; Chambliss and Garner, 1996).
",1 Introduction,[0],[0]
"We hypothesize that studying the actual effect of language on persuasion will require a more controlled experimental setting — one that takes into account any potentially confounding userlevel (i.e., reader-level) factors1 that could cause a person to change, or keep a person from changing, his opinion.",1 Introduction,[0],[0]
In this paper we study one such type of factor: the prior beliefs of the reader as impacted by their political or religious ideology.,1 Introduction,[0],[0]
"We adopt this focus since it has been shown that ideologies play an important role for an individual when they form beliefs about controversial topics, and potentially affect how open the individual is to being persuaded (Stout and Buddenbaum, 1996; Goren, 2005; Croucher and Harris, 2012).
",1 Introduction,[0],[0]
We first present a dataset of online debates that enables us to construct the setting described above in which we can study the effect of language on persuasion while taking into account selected userlevel factors.,1 Introduction,[0],[0]
"In addition to the text of the debates, the dataset contains a multitude of background information on the users of the debate platform.",1 Introduction,[0],[0]
"To the best of our knowledge, it is the first publicly available dataset of debates that simultaneously provides such comprehensive information about the debates, the debaters and those voting on the debates.
",1 Introduction,[0],[0]
"With the dataset in hand, we then propose the novel task of studying persuasion (1) at the level of individual users, and (2) in a setting that can control for selected user-level factors, in our case, the prior beliefs associated with the political or
1Variables that affect both the dependent and independent variables causing misleading associations.
",1 Introduction,[0],[0]
"ar X
iv :1
90 6.
11 30
1v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
6 Ju
n 20
19
religious ideology of the debaters and voters.",1 Introduction,[0],[0]
"In particular, previous studies focus on predicting the winner of a debate based on the cumulative change in pre-debate vs. post-debate votes for the opposing sides (Zhang et al., 2016; Potash and Rumshisky, 2017).",1 Introduction,[0],[0]
"In contrast, we aim to predict which debater an individual user (i.e., reader of the debate) perceives as more successful, given their stated political and religious ideology.
",1 Introduction,[0],[0]
"Finally, we identify which features appear to be most important for persuasion, considering the selected user-level factors as well as the more traditional linguistic features associated with the language of the debate itself.",1 Introduction,[0],[0]
"We hypothesize that the effect of political and religious ideology will be stronger when the debate topic is Politics and Religion, respectively.",1 Introduction,[0],[0]
"To test this hypothesis, we experiment with debates on only Politics or only Religion vs. debates from all topics including Music, Health, Arts, etc.
",1 Introduction,[0],[0]
Our main finding is that prior beliefs associated with the selected user-level factors play a larger role than linguistic features when predicting the successful debater in a debate.,1 Introduction,[0],[0]
"In addition, the effect of these factors varies according to the topic of the debate topic.",1 Introduction,[0],[0]
"The best performance, however, is achieved when we rely on features extracted from user-level factors in conjunction with linguistic features derived from the debate text.",1 Introduction,[0],[0]
"Finally, we find that the set of linguistic features that emerges as the most predictive changes when we control for user-level factors (political and religious ideology) vs. when we do not, showing the importance of accounting for these factors when studying the effect of language on persuasion.
",1 Introduction,[0],[0]
"In the remainder of the paper, we describe the debate dataset (Section 2) and the prediction task (Section 3) followed by the experimental results and analysis (Section 4), related work (Section 5) and conclusions (Section 6).",1 Introduction,[0],[0]
"For this study, we collected 67, 315 debates from debate.org2 from 23 different topic categories including Politics, Religion, Health, Science and Music3.",2 Dataset,[0],[0]
"In addition to text of the debates, we collected 198, 759 votes from the readers of these debates.",2 Dataset,[0],[0]
"Votes evaluate different dimensions of the
2www.debate.org 3The dataset will be made publicly available at
http://www.cs.cornell.edu/ esindurmus/.
debate.",2 Dataset,[0],[0]
"To study the effect of user characteristics, we collected user information for 36, 294 different users.",2 Dataset,[0],[0]
Aspects of the dataset most relevant to our task are explained in the following section in more detail.,2 Dataset,[0],[0]
Debate rounds.,2.1 Debates,[0],[0]
"Each debate consists of a sequence of ROUNDS in which two debaters from opposing sides (one is supportive of the claim (i.e., PRO) and the other is against the claim (i.e., CON)) provide their arguments.",2.1 Debates,[0],[0]
Each debater has a single chance in a ROUND to make his points.,2.1 Debates,[0],[0]
Figure 1 shows an example ROUND 1 for the debate claim “PRESCHOOL IS A WASTE OF TIME”.,2.1 Debates,[0],[0]
"The number of ROUNDS in debates ranges from 1 to 5 and the majority of debates (61, 474 out of 67, 315) contain 3 or more ROUNDS.
",2.1 Debates,[0],[0]
Votes.,2.1 Debates,[0],[0]
All users in the debate.org community can vote on debates.,2.1 Debates,[0],[0]
"As shown in Figure 2, voters share their stances on the debate topic before and after the debate and evaluate the debaters’ conduct, their spelling and grammar, the convincingness of their arguments and the reliability of the sources they refer to.",2.1 Debates,[0],[0]
"For each such dimension, voters have the option to choose one of the debaters as better or indicate a tie.",2.1 Debates,[0],[0]
This fine-grained voting system gives a glimpse into the reasoning behind the voters’ decisions.,2.1 Debates,[0],[0]
There are two alternate criteria for determining the successful debater in a debate.,2.1.1 Determining the successful debater,[0],[0]
"Our experiments consider both.
",2.1.1 Determining the successful debater,[0],[0]
Criterion 1: Argument quality.,2.1.1 Determining the successful debater,[0],[0]
"As shown in Figure 2, debaters get points for each dimension of the debate.",2.1.1 Determining the successful debater,[0],[0]
"The most important dimension — in
that it contributes most to the point total — is making convincing arguments.",2.1.1 Determining the successful debater,[0],[0]
"debate.org uses Criterion 1 to determine the winner of a debate.
",2.1.1 Determining the successful debater,[0],[0]
Criterion 2: Convinced voters.,2.1.1 Determining the successful debater,[0],[0]
"Since voters share their stances before and after the debate, the debater who convinces more voters to change their stance is declared as the winner.",2.1.1 Determining the successful debater,[0],[0]
"On debate.org, each user has the option to share demographic and private state information such as their age, gender, ethnicity, political ideology, religious ideology, income level, education level, the president and the political party they support.",2.2 User information,[0],[0]
"Beyond that, we have access to information about their activities on the website such as their overall success rate of winning debates, the debates they participated in as a debater or voter, and their votes.",2.2 User information,[0],[0]
"An example of a user profile is shown in Figure 3.
",2.2 User information,[0],[0]
Opinions on the big issues.,2.2 User information,[0],[0]
debate.org maintains a list of the most controversial debate topics as determined by the editors of the website.,2.2 User information,[0],[0]
"These are referred to as big issues.4 Each user shares his stance on each big issue on his profile (see Figure 3): either PRO (in favor), CON (against), N/",2.2 User information,[0],[0]
"O (no opinion), N/S (not saying) or UND (undecided).",2.2 User information,[0],[0]
"In this section, we first analyze which dimensions of argument quality are the most important for determining the successful debater.",3 Prediction task: which debater will be declared as more successful by an individual voter?,[0],[0]
"Then, we analyze whether there is any connection between selected user-level factors and users’ opinions on the
4http://www.debate.org/big-issues/
big issues to see if we can infer their opinions from these factors.",3 Prediction task: which debater will be declared as more successful by an individual voter?,[0],[0]
"Finally, using our findings from these analyses, we perform the task of predicting which debater will be perceived as more successful by an individual voter.",3 Prediction task: which debater will be declared as more successful by an individual voter?,[0],[0]
Figure 4 shows the correlation between pairs of voting dimensions (in the first 8 rows and columns) and the correlation of each dimension with (1) getting more points (row or column 9) and (2) convincing more people as a debater (final row or column).,3.1 Relationships between argument quality dimensions,[0],[0]
"Abbreviations stand for (on the CON side): has better conduct (CBC), makes more convincing arguments (CCA), uses more reliable sources (CRS), has better spelling and grammar (CBSG), gets more total points (CMTP) and convinces more voters (CCMV).",3.1 Relationships between argument quality dimensions,[0],[0]
"For the PRO side we
use PBC, PCA, and so on.",3.1 Relationships between argument quality dimensions,[0],[0]
"From Figure 4, we can see that making more convincing arguments (CCA) correlates the most with total points (CMTP) and convincing more voters (CCMV).",3.1 Relationships between argument quality dimensions,[0],[0]
This analysis motivates us to identify the linguistic features that are indicators of more convincing arguments.,3.1 Relationships between argument quality dimensions,[0],[0]
"opinions on the big issues and their prior beliefs
We disentangle different aspects of a person’s prior beliefs to understand how well each correlates with their opinions on the big issues.",3.2 The relationship between a user’s,[0],[0]
"As noted earlier, we focus here only on prior beliefs in the form of self-identified political and religious ideology.
",3.2 The relationship between a user’s,[0],[0]
Representing the big issues.,3.2 The relationship between a user’s,[0],[0]
"To represent the opinions of a user on a big issue, we use a fourdimensional one-hot encoding where the indices of the vector correspond to PRO, CON, N/O (no opinion), and UND (undecided), consecutively (1 if the user chooses that value for the issue, 0 otherwise).",3.2 The relationship between a user’s,[0],[0]
Note that we do not have a representation for N/S since we eliminate users having N/S for at least one big issue for this study.,3.2 The relationship between a user’s,[0],[0]
We then concatenate the vector for each big issue to get a representation for a user’s stance on all the big issues as shown in Figure 5.,3.2 The relationship between a user’s,[0],[0]
"We denote this vector by BIGISSUES.
We test the correlation between the individual’s opinions on big issues and the selected userlevel factors in this study using two different approaches: clustering and classification.
",3.2 The relationship between a user’s,[0],[0]
Clustering the users’ decisions on big issues.,3.2 The relationship between a user’s,[0],[0]
We apply PCA on the BIGISSUES vectors of users who identified themselves as CONSERVATIVE vs. LIBERAL (740 users).,3.2 The relationship between a user’s,[0],[0]
We do the same for the users who identified themselves as ATHEIST vs. CHRISTIAN (1501 users).,3.2 The relationship between a user’s,[0],[0]
"In Figure 6, we see that there are distinctive clusters of CONSERVATIVE vs. LIBERAL users in the two-dimensional representation
while for ATHEIST vs. CHRISTIAN, the separation is not as distinct.",3.2 The relationship between a user’s,[0],[0]
"This suggests that people’s opinions on the big issues identified by debate.org correlate more with their political ideology than their religious ideology.
",3.2 The relationship between a user’s,[0],[0]
Classification approach.,3.2 The relationship between a user’s,[0],[0]
We also treat this as a classification task5 using the BIGISSUES vectors for each user as features and the user’s religious and political ideology as the labels to be predicted.,3.2 The relationship between a user’s,[0],[0]
"So the classification task is: Given the user’s BIGISSUES vector, predict his political and religious ideology.",3.2 The relationship between a user’s,[0],[0]
Table 1 shows the accuracy for each case.,3.2 The relationship between a user’s,[0],[0]
"We see that using the BIGISSUES vectors as features performs significantly better6 than majority baseline7.
",3.2 The relationship between a user’s,[0],[0]
This analysis shows that there is a clear relationship between people’s opinions on the big issues and the selected user-level factors.,3.2 The relationship between a user’s,[0],[0]
It raises the question of whether it is even possible to persuade someone with prior beliefs relevant to a debate claim to change their stance on the issue.,3.2 The relationship between a user’s,[0],[0]
"It may be the case that people prefer to agree with the individuals having the same (or similar) beliefs regardless of the quality of the arguments and the
5For all the classification tasks described in this paper, we experiment with logistic regression, optimizing the regularizer (`1 or `2) and the regularization parameter C (between 10−5 and 105).
",3.2 The relationship between a user’s,[0],[0]
6We performed the McNemar significance test.,3.2 The relationship between a user’s,[0],[0]
"7The majority class baseline predicts CONSERVATIVE for political and CHRISTIAN for religious ideology for each example, respectively.
particular language used.",3.2 The relationship between a user’s,[0],[0]
"Therefore, it is important to understand the relative effect of prior beliefs vs. argument strength on persuasion.",3.2 The relationship between a user’s,[0],[0]
"Some of the previous work in NLP on persuasion focuses on predicting the winner of a debate as determined by the change in the number of people supporting each stance before and after the debate (Zhang et al., 2016; Potash and Rumshisky, 2017).",3.3 Task descriptions,[0],[0]
"However, we believe that studies of the effect of language on persuasion should take into account other, extra-linguistic, factors that can affect opinion change: in particular, we propose an experimental framework for studying the effect of language on persuasion that aims to control for the prior beliefs of the reader as denoted through their self-identified political and religious ideologies.",3.3 Task descriptions,[0],[0]
"As a result, we study a more fine-grained prediction task: for an individual voter, predict which side/debater/argument the voter will declare as the winner.
",3.3 Task descriptions,[0],[0]
Task 1 : Controlling for religious ideology.,3.3 Task descriptions,[0],[0]
"In the first task, we control for religious ideology by selecting debates for which each of the two debaters is from a different religious ideology (e.g., debater 1 is ATHEIST, debater 2 is CHRISTIAN).",3.3 Task descriptions,[0],[0]
"In addition, we consider only voters that (a) self-identify with one of these religious ideologies (e.g., the voter is either ATHEIST or CHRISTIAN) and (b) changed their stance on the debate claim post-debate vs. pre-debate.",3.3 Task descriptions,[0],[0]
"For each such voter, we want to predict which of the PRO-side debater or the CON-side debater did the convincing.",3.3 Task descriptions,[0],[0]
"Thus, in this task, we use Criterion 2 to determine the winner of the debate from the point of view of the voter.",3.3 Task descriptions,[0],[0]
"Our hypothesis is that the voter will be convinced by the debater that espouses the religious ideology of the voter.
",3.3 Task descriptions,[0],[0]
"In this setting, we can study the factors that are important for a particular voter to be convinced by a debater.",3.3 Task descriptions,[0],[0]
"This setting also provides an opportunity to understand how the voters who change their minds perceive arguments from a debater who is expressing the same vs. the opposing prior belief.
",3.3 Task descriptions,[0],[0]
"To study the effect of the debate topic, we perform this study for two cases — debates belonging to the Religion category and then all the categories.",3.3 Task descriptions,[0],[0]
"The Religion category contains debates like “IS THE BIBLE AGAINST WOMEN’S RIGHTS?” and “RELIGIOUS THEORIES SHOULD
NOT BE TAUGHT IN SCHOOL”.",3.3 Task descriptions,[0],[0]
We want to see how strongly a user’s religious ideology affects the persuasive effect of language in such a topic as compared to the all topics.,3.3 Task descriptions,[0],[0]
"We expect to see stronger effects of prior beliefs for debates on Religion.
",3.3 Task descriptions,[0],[0]
Task 2: Controlling for political ideology.,3.3 Task descriptions,[0],[0]
"Similar to the setting described above, Task 2 controls for political ideology.",3.3 Task descriptions,[0],[0]
"In particular, we only use debates where the two debaters are from different political ideologies (CONSERVATIVE vs. LIBERAL).",3.3 Task descriptions,[0],[0]
"In contrast to Task 1, we consider all voters that self-identify with one of the two debater ideologies (regardless of whether the voter’s stance changed post-debate vs. pre-debate).",3.3 Task descriptions,[0],[0]
"This time, we predict whether the voter gives more total points to the PRO side or the CON side argument.",3.3 Task descriptions,[0],[0]
"Thus, Task 2 uses Criterion 1 to determine the winner of the debate from the point of view of the voter.",3.3 Task descriptions,[0],[0]
"Our hypothesis is that the voter will assign more points to the debater that has the same political ideology as the voter.
",3.3 Task descriptions,[0],[0]
"For this task too, we perform the study for two cases — debates from the Politics category only and debates from all categories.",3.3 Task descriptions,[0],[0]
And we expect to see stronger effects of prior beliefs for debates on Politics.,3.3 Task descriptions,[0],[0]
The features we use in our model are shown in Table 2.,3.4 Features,[0],[0]
"They can be divided into two groups — features that describe the prior beliefs of the users and linguistic features of the arguments themselves.
",3.4 Features,[0],[0]
User features We use the cosine similarities between the voter and each of the debaters’ big issue vectors.,3.4 Features,[0],[0]
These features give a good approximation of the overall similarity of two user’s opinions.,3.4 Features,[0],[0]
"Second, we use indicator features to encode whether the religious and political beliefs of the voter match those of each of the debaters.
",3.4 Features,[0],[0]
Linguistic features We extract linguistic features separately for both the PRO and CON side of the debate (combining all the utterances of PRO across different turns and doing the same for CON).,3.4 Features,[0],[0]
Table 2 contains a list of these features.,3.4 Features,[0],[0]
"It includes features that carry information about the style of the language (e.g., usage of modal verbs, length, punctuation), represent different semantic aspects of the argu-
ment (e.g., showing evidence, connotation (Feng and Hirst, 2011), subjectivity (Wilson et al., 2005), sentiment, swear word features) as well as features that convey different argumentation styles (argument lexicon features (Somasundaran and Wiebe, 2010).",3.4 Features,[0],[0]
"Argument lexicon features include the counts for the phrases that match with the regular expressions of argumentation styles such as assessment, authority, conditioning, contrasting, emphasizing, generalizing, empathy, inconsistency, necessity, possibility, priority, rhetorical questions, desire, and difficulty.",3.4 Features,[0],[0]
We then concatenate these features to get a single feature representation for the entire debate.,3.4 Features,[0],[0]
"For each of the tasks, prediction accuracy is evaluated using 5-fold cross validation.",4 Results and Analysis,[0],[0]
We pick the model parameters for each split with 3-fold cross validation on the training set.,4 Results and Analysis,[0],[0]
We do ablation for each of user-based and linguistic features.,4 Results and Analysis,[0],[0]
"We report the results for the feature sets that perform better than the baseline.
",4 Results and Analysis,[0],[0]
"We perform analysis by training logistic regression models using only user-based features, only linguistic features and finally combining userbased and linguistic features for both the tasks.
",4 Results and Analysis,[0],[0]
Task 1 for debates in category Religion.,4 Results and Analysis,[0],[0]
"As shown in Table 3, the majority baseline (predicting the winner side of the majority of training examples out of PRO or CON) gets 56.10% accuracy.",4 Results and Analysis,[0],[0]
User features alone perform significantly better than the majority baseline.,4 Results and Analysis,[0],[0]
The most important user-based feature is matching religious ideology.,4 Results and Analysis,[0],[0]
This means it is very likely that people change their views in favor of a debater with the same religious ideology.,4 Results and Analysis,[0],[0]
"In a linguistic-only features analysis, combination of the personal pronouns and connotation features emerge as most important and also perform significantly better than the majority baseline at 65.37% accuracy.",4 Results and Analysis,[0],[0]
"When we use both user-based and linguistic features to predict, the accuracy improves to 66.42% with connotation features.",4 Results and Analysis,[0],[0]
An interesting observation is that including the user-based features along with the linguistic features changes the set of important linguistic features for persuasion removing the personal pronouns from the important linguistic features set.,4 Results and Analysis,[0],[0]
"This shows the importance of studying potentially confounding user-level factors.
",4 Results and Analysis,[0],[0]
Task 1 for debates in all categories.,4 Results and Analysis,[0],[0]
"As shown in Table 4, for the experiments with user-based features only, matching religious ideology and opinion similarity features are the most important.",4 Results and Analysis,[0],[0]
"For this task, length is the most predictive linguistic feature and can achieve significant improve-
ment over the baseline (61.01%).",4 Results and Analysis,[0],[0]
"When we combine the language features with user-based features, we see that with exclamation mark the accuracy improves to (65.74%).
",4 Results and Analysis,[0],[0]
Task 2 for debates in category Politics.,4 Results and Analysis,[0],[0]
"As shown in Table 5, using user-based features only, the matching political ideology feature performs the best (80.40%).",4 Results and Analysis,[0],[0]
"Linguistic features (refer to Table 5 for the full list) alone, however, can still obtain significantly better accuracy than the baseline (59.60%).",4 Results and Analysis,[0],[0]
"The most important linguistic features include approval, politeness, modal verbs, punctuation and argument lexicon features such as rhetorical questions and emphasizing.",4 Results and Analysis,[0],[0]
"When combining this linguistic feature set with the matching political ideology feature, we see that with the accuracy improves to (81.81%).",4 Results and Analysis,[0],[0]
"Length feature does not give any improvement when it is combined with the user features.
",4 Results and Analysis,[0],[0]
Task 2 for debates in all categories.,4 Results and Analysis,[0],[0]
"As shown in Table 6, when we include all categories, we see that the best performing user-based feature is the opinion similarity feature (73.96%).",4 Results and Analysis,[0],[0]
"When using language features only, length feature (56.88%) is the most important.",4 Results and Analysis,[0],[0]
"For this setting, the best accuracy is achieved when we combine user features with length and Tf-idf features.",4 Results and Analysis,[0],[0]
We see that the set of language features that improve the performance of user-based features do not include some of that perform significantly better than the baseline when used alone (modal verbs and politeness features).,4 Results and Analysis,[0],[0]
"Below we provide an overview of related work from the multiple disciplines that study persuasion.
",5 Related Work,[0],[0]
Argumentation mining.,5 Related Work,[0],[0]
"Although most recent work on argumentation has focused on identifying the structure of arguments and extracting argument components (Persing and Ng, 2015; Palau and Moens, 2009; Biran and Rambow, 2011; Mochales and Moens, 2011; Feng and Hirst, 2011; Stab and Gurevych, 2014; Lippi and Torroni, 2015; Park and Cardie, 2014; Nguyen and Litman, 2015; Peldszus and Stede, 2015; Niculae et al., 2017; Rosenthal and McKeown, 2015), more relevant is research on identifying the characteristics of persuasive text, e.g., what distinguishes persuasive from non-persuasive text (Tan et al., 2016; Zhang et al., 2016; Wachsmuth et al., 2016; Habernal and Gurevych, 2016a,b; Fang et al., 2016; Hidey et al., 2017).",5 Related Work,[0],[0]
"Similar to these, our work aims to understand the characteristics of persuasive text but also considers the effect of people’s prior beliefs.
",5 Related Work,[0],[0]
Persuasion.,5 Related Work,[0],[0]
"There has been a tremendous amount of research effort in the social sciences (including computational social science) to understand the characteristics of persuasive text (Kelman, 1961; Burgoon et al., 1975; Chaiken, 1987; Tykocinskl et al., 1994; Chambliss and Garner, 1996; Dillard and Pfau, 2002; Cialdini, 2007; Durik et al., 2008; Tan et al., 2014; Marquart
and Naderer, 2016).",5 Related Work,[0],[0]
"Most relevant among these is the research of Tan et al. (2016), Habernal and Gurevych (2016a) and Hidey et al. (2017).",5 Related Work,[0],[0]
Tan et al. (2016) focused on the effect of user interaction dynamics and language features looking at the ChangeMyView9 (an internet forum) community on Reddit and found that user interaction patterns as well as linguistic features are connected to the success of persuasion.,5 Related Work,[0],[0]
"In contrast, Habernal and Gurevych (2016a) created a crowd-sourced corpus consisting of argument pairs and, given a pair of arguments, asked annotators which is more convincing.",5 Related Work,[0],[0]
This allowed them to experiment with different features and machine learning techniques for persuasion prediction.,5 Related Work,[0],[0]
"Taking motivation from Aristotle’s definition for modes of persuasion, Hidey et al. (2017) annotated claims and premises extracted from the ChangeMyView community with their semantic types to study if certain semantic types or different combinations of semantic types appear in persuasive but not in non-persuasive essays.",5 Related Work,[0],[0]
"In contrast to the above, our work focuses on persuasion in debates than monologues and forum datasets and accounts for the user-based features.
",5 Related Work,[0],[0]
Persuasion in debates.,5 Related Work,[0],[0]
Debates are another resource for studying the different aspects of persuasive arguments.,5 Related Work,[0],[0]
"Different from monologues where the audience is exposed to only one side of the opinions about an issue, debates allow the audi-
9https://www.reddit.com/r/changemyview/
ence to see both sides of a particular issue via a controlled discussion.",5 Related Work,[0],[0]
There has been some work on argumentation and persuasion on online debates.,5 Related Work,[0],[0]
"Sridhar et al. (2015), Somasundaran and Wiebe (2010) and Hasan and Ng (2014), for example, studied detecting and modeling stance on online debates.",5 Related Work,[0],[0]
Zhang et al. (2016) found that the side that can adapt to their opponents’ discussion points over the course of the debate is more likely to be the winner.,5 Related Work,[0],[0]
"None of these studies investigated the role of prior beliefs in stance detection or persuasion.
",5 Related Work,[0],[0]
User effects in persuasion.,5 Related Work,[0],[0]
Persuasion is not independent from the characteristics of the people to be persuaded.,5 Related Work,[0],[0]
"Research in psychology has shown that people have biases in the ways they interpret the arguments they are exposed to because of their prior beliefs (Lord et al., 1979; Vallone et al., 1985; Chambliss and Garner, 1996).",5 Related Work,[0],[0]
"Understanding the effect of persuasion strategies on people, the biases people have and the effect of prior beliefs of people on their opinion change has been an active area of research interest (Correll et al., 2004; Hullett, 2005; Petty et al., 1981).",5 Related Work,[0],[0]
"Eagly and Chaiken (1975), for instance, found that the attractiveness of the communicator plays an important role in persuasion.",5 Related Work,[0],[0]
Work in this area could be relevant for the future work on modeling shared characteristics between the user and the debaters.,5 Related Work,[0],[0]
"To the best of our knowledge, Lukin et al. (2017) is the most relevant work to ours since they consider features of the audience on persuasion.",5 Related Work,[0],[0]
"In particular, they studied the effect of an individual’s personality features (open, agreeable, extrovert, neurotic, etc.) on the type of argument (factual vs. emotional) they find more persuasive.",5 Related Work,[0],[0]
Our work differs from this work since we study debates and in our setting the voters can see the debaters’ profiles as well as all the interactions between the two sides of the debate rather than only being exposed to a monologue.,5 Related Work,[0],[0]
"Finally, we look at different types of user profile information such as a user’s religious and ideological beliefs and their opinions on various topics.",5 Related Work,[0],[0]
In this work we provide a new dataset of debates and a more controlled setting to study the effects of prior belief on persuasion.,6 Conclusion,[0],[0]
The dataset we provide and the framework we propose open several avenues for future research.,6 Conclusion,[0],[0]
"One could explore
the effect different aspects of people’s background (e.g., gender, education level, ethnicity) on persuasion.",6 Conclusion,[0],[0]
"Furthermore, it would be interesting to study how people’s prior beliefs affect their other activities on the website and the language they use while interacting with people with the same and different prior beliefs.",6 Conclusion,[0],[0]
"Finally, one could also try to understand in what aspects and how the language people with different prior beliefs/backgrounds use is different.",6 Conclusion,[0],[0]
These different directions would help people better understand characteristics of persuasive arguments and the effects of prior beliefs in language.,6 Conclusion,[0],[0]
This work was supported in part by NSF grant SES-1741441 and DARPA DEFT Grant FA875013-2-0015.,7 Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government.",7 Acknowledgements,[0],[0]
"We thank Yoav Artzi, Faisal Ladhak, Amr Sharaf, Tianze Shi, Ashudeep Singh and the anonymous reviewers for their helpful feedback.",7 Acknowledgements,[0],[0]
We also thank the Cornell NLP group for their insightful comments.,7 Acknowledgements,[0],[0]
Public debate forums provide a common platform for exchanging opinions on a topic of interest.,abstractText,[0],[0]
"While recent studies in natural language processing (NLP) have provided empirical evidence that the language of the debaters and their patterns of interaction play a key role in changing the mind of a reader, research in psychology has shown that prior beliefs can affect our interpretation of an argument and could therefore constitute a competing alternative explanation for resistance to changing one’s stance.",abstractText,[0],[0]
"To study the actual effect of language use vs. prior beliefs on persuasion, we provide a new dataset and propose a controlled setting that takes into consideration two reader-level factors: political and religious ideology.",abstractText,[0],[0]
We find that prior beliefs affected by these reader-level factors play a more important role than language use effects and argue that it is important to account for them in NLP studies of persuasion.,abstractText,[0],[0]
Exploring the Role of Prior Beliefs for Argument Persuasion,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1190–1199 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1190",text,[0],[0]
Statistical parsers are often criticized for their performance outside of the domain they were trained on.,1 Introduction,[0],[0]
"The most straightforward remedy would be more training data in the target domain, but building treebanks (Marcus et al., 1993) is expensive.
",1 Introduction,[0],[0]
"In this paper, we revisit this issue in light of recent developments in neural natural language processing.",1 Introduction,[0],[0]
"Our paper rests on two observations:
1.",1 Introduction,[0],[0]
It is trivial to train on partial annotations using a span-focused model.,1 Introduction,[0],[0]
Stern et al. (2017a) demonstrated that a parser with minimal dependence between the decisions that produce a parse can achieve state-of-the-art performance.,1 Introduction,[0],[0]
"We modify their parser, hence-",1 Introduction,[0],[0]
"forth MSP, so that it trains directly on individual labeled spans instead of parse trees.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"This results in a parser that can be trained, with no adjustments to the training regime, from partial sentence bracketings.
2.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"The use of contextualized word representations (Peters et al., 2017; McCann et al., 2017) greatly reduces the amount of data needed to train linguistic models.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"Contextualized word representations, which encode tokens conditioned on their context in a sentence, have been shown to give significant boosts across a variety of NLP tasks, and also to reduce the amount of data needed by an order of magnitude in some tasks.
","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"Taken together, this suggests a way to rapidly extend a newswire-trained parser to new domains.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"Specifically, we will show it is possible to achieve large out-of-domain performance improvements using only dozens of partially annotated sentences, like those shown in Figure 1.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"The resulting parser also does not suffer any degradation on the newswire domain.
","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"Along the way, we provide several other notable contributions:
• We raise the state-of-the-art single-model F1score for constituency parsing from 92.6% to 94.3% on the Wall Street Journal (WSJ) test set.","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"A trained model is publicly available.1
•","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"We show that, even without domain-specific training data, our parser has much less out-ofdomain degradation than previous parsers on “newswire-adjacent” domains like the Brown corpus.
","In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
• We provide a version of MSP which predicts its own POS tags (rather than requiring a third-party tagger).,"In [ the figure above ] , [ [ AD = 4 ] , [ AB =",[0],[0]
"When we allow annotators to selectively annotate important phenomena, we make the process faster and simpler (Mielens et al., 2015).",2 The Reconciled Span Parser (RSP),[0],[0]
"Unfortunately, this produces a disconnect between the model (which typically asserts the probability of a full parse tree) and the annotation task (which asserts the correctness of some subcomponent, like a constituent span or a dependency arc).",2 The Reconciled Span Parser (RSP),[0],[0]
"There is a body of research (Hwa, 1999; Li et al., 2016) that discusses how to bridge this gap by modifying the training data, training algorithm, or the training objective.
",2 The Reconciled Span Parser (RSP),[0],[0]
"Alternatively, we could just better align the model with the annotation task.",2 The Reconciled Span Parser (RSP),[0],[0]
"Specifically, we could train a parser whose base model predicts exactly what we ask the annotator to annotate, e.g. whether a particular span is a constituent.",2 The Reconciled Span Parser (RSP),[0],[0]
"This makes it trivial to train with partial or full annotations, because the training data reduces to a collection of span labels in either case.
",2 The Reconciled Span Parser (RSP),[0],[0]
"Luckily, recent state-of-the-art results that model NLP tasks as independently classified spans (Stern et al., 2017a) suggest this strategy is currently viable.",2 The Reconciled Span Parser (RSP),[0],[0]
"In this section, we present the Reconciled Span Parser (RSP), a modified version of the Minimal Span Parser (MSP) of Stern et al. (2017a).",2 The Reconciled Span Parser (RSP),[0],[0]
"RSP differs from MSP in the following ways:
• It is trained on a span classification task.",2 The Reconciled Span Parser (RSP),[0],[0]
"MSP trains on a maximum margin objective; that is, the loss function penalizes the
1http://allennlp.org/models
violation of a margin between the scores of the gold parse and the next highest scoring parse decoded.",2 The Reconciled Span Parser (RSP),[0],[0]
"This couples its training procedure with its decoding procedure, resulting in two versions, a top-down parser and a chart parser.",2 The Reconciled Span Parser (RSP),[0],[0]
"To allow our model to be trained on partial annotations, we change the training task to be the span classification task described below.
",2 The Reconciled Span Parser (RSP),[0],[0]
• It uses contextualized word representations instead of predicted part-of-speech tags.,2 The Reconciled Span Parser (RSP),[0],[0]
Our model uses contextualized word representations as described in Peters et al. (2018).,2 The Reconciled Span Parser (RSP),[0],[0]
"It does not take part-of-speech-tags as input, eliminating the dependence of the parser on a newswire-trained POS-tagger.",2 The Reconciled Span Parser (RSP),[0],[0]
"We will view a parse tree as a labeling of all the spans of a sentence such that:
• Every constituent span is labeled with the sequence of non-terminals assigned to it in the parse tree.",2.1 Overview,[0],[0]
"For instance, span (2, 4) in Figure 2b is labeled with the sequence 〈S,VP〉, as shown in Figure 2a.
•",2.1 Overview,[0],[0]
"Every non-constituent is labeled with the empty sequence.
",2.1 Overview,[0],[0]
"Given a sentence represented by a sequence of tokens x of length n, define spans(x) =",2.1 Overview,[0],[0]
"{(i, j) | 0 ≤",2.1 Overview,[0],[0]
i < j ≤ n}.,2.1 Overview,[0],[0]
"Define a parse for sentence x as a function π : spans(x) 7→ L where L is the set of all sequences of non-terminal tags, including the empty sequence.
",2.1 Overview,[0],[0]
"We model the probability of a parse as the independent product of its span labels:
Pr(π|x) = ∏
s∈spans(x)
Pr(π(s) | x, s)
⇒ logPr(π|x) = ∑
s∈spans(x)
logPr(π(s)",2.1 Overview,[0],[0]
"| x, s)
Hence, we will train a base model σ(l | x, s) to estimate the log probability of label l for span s (given sentence x), and we will score the overall parse with:
score(π|x) = ∑
s∈spans(x)
σ(π(s) | x, s)
",2.1 Overview,[0],[0]
"Note that this probability model accords mass to mis-structured trees (e.g. overlapping spans like (2, 5) and (3, 7) cannot both be constituents of a well-formed tree).",2.1 Overview,[0],[0]
"We solve the following Integer Linear Program (ILP)2 to find the highest scoring parse that admits a well-formed tree:
max δ ∑ (i,j)∈spans(x) v+(i,j)δ(i,j) + v",2.1 Overview,[0],[0]
"− (i,j)(1− δ(i,j))
subject to:
i <",2.1 Overview,[0],[0]
k,2.1 Overview,[0],[0]
"< j < m =⇒ δ(i,j) + δ(k,m) ≤ 1",2.1 Overview,[0],[0]
"(i, j) ∈ spans(x) =⇒ δ(i,j) ∈ {0, 1}
where:
v+(i,j) =",2.1 Overview,[0],[0]
maxl s.t,2.1 Overview,[0],[0]
.,2.1 Overview,[0],[0]
"l 6=∅ σ(l | x, (i, j))
v−(i,j) =",2.1 Overview,[0],[0]
"σ(∅ | x, (i, j)) 2There are a number of ways to reconcile the span conflicts, including an adaptation of the standard dynamic programming chart parsing algorithm to work with spans of an unbinarized tree.",2.1 Overview,[0],[0]
"However it turns out that the classification model rarely produces span conflicts, so all methods we tried performed equivalently well.",2.1 Overview,[0],[0]
"For our span classification model σ(l | x, s), we use the model from (Stern et al., 2017a), which leverages a method for encoding spans from (Wang and Chang, 2016; Cross and Huang, 2016).",2.2 Classification Model,[0],[0]
"First, it creates a sentence encoding by running a two-layer bidirectional LSTM over the sentence to obtain forward and backward encodings for each position i, denoted by fi and bi respectively.",2.2 Classification Model,[0],[0]
"Then, spans are encoded by the difference in LSTM states immediately before and after the span; that is, span (i, j) is encoded as the concatenation of the vector differences fj − fi−1 and bi−bj+1.",2.2 Classification Model,[0],[0]
"A one-layer feedforward network maps each span representation to a distribution over labels.
",2.2 Classification Model,[0],[0]
"Classification Model Parameters and Initializations
We preserve the settings used in Stern et al. (2017a) where possible.",2.2 Classification Model,[0],[0]
"As a result, the size of the hidden dimensions of the LSTM and the feedforward network is 250.",2.2 Classification Model,[0],[0]
The dropout ratio for the LSTM is set to 0.4 .,2.2 Classification Model,[0],[0]
"Unlike the model it is based on, our model uses word embeddings of length 1124.",2.2 Classification Model,[0],[0]
"These result from concatenating a 100 dimension learned word embedding, with a 1024 di-
mension learned linear combination of the internal states of a bidirectional language model run on the input sentence as described in Peters et al. (2018).",2.2 Classification Model,[0],[0]
We refer to them below as ELMo (Embeddings from Language Models).,2.2 Classification Model,[0],[0]
"For the learned embeddings, words with n occurrences in the training data are replaced by 〈UNK〉 with probability 1+ n
10 1+n .",2.2 Classification Model,[0],[0]
This does not affect the ELMo component of the word embeddings.,2.2 Classification Model,[0],[0]
"As a result, even common words are replaced with probability at least 1 10 , making the model rely on the ELMo embeddings instead of the learned embeddings.",2.2 Classification Model,[0],[0]
"To make the model self-contained, it does not take part-ofspeech tags as input.",2.2 Classification Model,[0],[0]
"Using a linear layer over the last hidden layer of the classification model, partof-speech tags are predicted for spans containing single words.",2.2 Classification Model,[0],[0]
"On WSJTEST3, RSP outperforms (see Table 1) all previous single models trained on WSJTRAIN by a significant margin, raising the state-of-the-art result from 92.6% to 94.3%.",3.1 Performance on Newswire,[0],[0]
"Additionally, our predicted part-of-speech tags achieve 97.72%4 accuracy on WSJTEST.
3For all our experiments on the WSJ component of the Penn Treebank (Marcus et al., 1993), we use the standard split which is sections 2-21 for training, henceforth WSJTRAIN, section 22 for development, henceforth WSJDEV, and 23 for testing, henceforth WSJTEST.
4The split we used is not standard for part-of-speech tagging.",3.1 Performance on Newswire,[0],[0]
"As a result, we do not compare to part-of-speech taggers.",3.1 Performance on Newswire,[0],[0]
"The Brown Corpus The Brown corpus (Marcus et al., 1993) is a standard benchmark used to assess WSJ-trained parsers outside of the newswire domain.",3.2 Beyond Newswire,[0],[0]
"When (Kummerfeld et al., 2012) parsed the various Brown verticals with the (then state-of-the-art) Charniak parser (Charniak, 2000; Charniak and Johnson, 2005; McClosky et al., 2006a), it achieved F1 scores between 83% and 86%, even though its F1 score on WSJTEST was 92.1%.
",3.2 Beyond Newswire,[0],[0]
"In Table 3, we discover that RSP does not suffer nearly as much degradation, with an average F1-score of 90.3%.",3.2 Beyond Newswire,[0],[0]
"To determine whether this increased portability is because of the parser architecture or the use of ELMo vectors, we also run MSP on the Brown verticals.",3.2 Beyond Newswire,[0],[0]
"We used the Stanford tagger5 (Toutanova et al., 2003) to tag WSJTRAIN and the Brown verticals so that MSP could be given these at train and test time.",3.2 Beyond Newswire,[0],[0]
We learned that most of the improvement can be attributed to the ELMo word representations.,3.2 Beyond Newswire,[0],[0]
"In fact, even if we use MSP with gold POS tags, the average performance is 3.4% below RSP.
Question Bank and Genia Despite being a standard benchmark for parsing domain adaptation, the Brown corpus has considerable commonality with newswire text.",3.2 Beyond Newswire,[0],[0]
It is primarily composed of well-formed sentences with similar syntactic phenomena.,3.2 Beyond Newswire,[0],[0]
"Perhaps the main challenge with the Brown corpus is a difference in vocabulary, rather than a difference in syntax, which may explain the success of RSP, which leverages contextualized embeddings learned from a large corpus.
",3.2 Beyond Newswire,[0],[0]
"If we try to run RSP on a more syntactically divergent corpus like QuestionBank6 (Judge et al., 2006), we find much more performance degradation.",3.2 Beyond Newswire,[0],[0]
"This is unsurprising, since WSJTRAIN does not contain many examples of question syntax.",3.2 Beyond Newswire,[0],[0]
"But how many examples do we need, to get good performance?
",3.2 Beyond Newswire,[0],[0]
"5We used the english-left3words-distsim.tagger model from the 2017-06-09 release of the Stanford POS tagger since it achieved the best accuracy on the Brown corpus.
6For all our experiments on QuestionBank, we use the following split: sentences 1-1000 and 2001-3000 for training, henceforth QBANKTRAIN, 1001-1500 and 3001-3500 for development, henceforth QBANKDEV, and 1501-2000 and 2501-4000 for testing, henceforth QBANKTEST.",3.2 Beyond Newswire,[0],[0]
"This split is described at https://nlp.stanford.edu/data/QuestionBankStanford.shtml.
",3.2 Beyond Newswire,[0],[0]
"For the experiments summarized in table 4 and table 5 involving 40k sentences from WSJTRAIN, we started with RSP trained on WSJTRAIN, and fine-tuned it on minibatches containing an equal number of target domain and WSJTRAIN sentences.
",3.2 Beyond Newswire,[0],[0]
"Surprisingly, with only 50 annotated questions (see Table 4), performance on QBANKDEV jumps 5 points, from 89.9% to 94.9%.",3.2 Beyond Newswire,[0],[0]
"This is only
1.5% below training with all of WSJTRAIN and QBANKTRAIN.",3.2 Beyond Newswire,[0],[0]
"The resulting system improves slightly on WSJTEST getting 94.38%.
",3.2 Beyond Newswire,[0],[0]
"On the more difficult GENIA corpus of biomedical abstracts (Tateisi et al., 2005), we see a similar, if somewhat less dramatic, trend.",3.2 Beyond Newswire,[0],[0]
See Table 5.,3.2 Beyond Newswire,[0],[0]
"With 50 annotated sentences, performance on GENIADEV jumps from 79.5% to 86.2%, outperforming all but one parser from David McClosky’s thesis (McClosky, 2010) – the one that trains on all 14k sentences from GENIATRAIN and self-trains using 270k sentences from PubMed.",3.2 Beyond Newswire,[0],[0]
"That parser achieves 87.6%, which we outperform with just 500 sentences from GENIATRAIN.
",3.2 Beyond Newswire,[0],[0]
These results suggest that it is currently feasible to extend a parser to a syntactically distant domain (for which no gold parses exist) with a couple hours of effort.,3.2 Beyond Newswire,[0],[0]
We explore this possibility in the next section.,3.2 Beyond Newswire,[0],[0]
"To create a parser for their geometry question answering system, (Seo et al., 2015) did the following:
• Designed regular expressions to identify mathematical expressions.
",4 Rapid Parser Extension,[0],[0]
"• Replaced the identified expressions with dummy words.
",4 Rapid Parser Extension,[0],[0]
•,4 Rapid Parser Extension,[0],[0]
"Parsed the resulting sentences.
",4 Rapid Parser Extension,[0],[0]
"FRAG
.
.
",4 Rapid Parser Extension,[0],[0]
"NP
24 and QS = 10
SYM
=
NP
PR
,
,
NP
PQRS
PP
In the rhombus
(a) Training only on WSJTRAIN.
FRAG
.
.
",4 Rapid Parser Extension,[0],[0]
"NP
PR = 24 and QS = 10
,
,
PP
In the rhombus PQRS
(b) Retraining on partial annotations.
",4 Rapid Parser Extension,[0],[0]
It is clear why this was necessary.,4 Rapid Parser Extension,[0],[0]
Figure 3 (top) shows how RSP (trained only on WSJTRAIN),4 Rapid Parser Extension,[0],[0]
"parses the sentence “In the rhombus PQRS, PR = 24 and QS = 10.”",4 Rapid Parser Extension,[0],[0]
"The result is completely wrong, and useless to a downstream application.
",4 Rapid Parser Extension,[0],[0]
"Still, beyond just the inconvenience of building additional infrastructure, there are downsides to the “regex-and-replace” strategy:
1.",4 Rapid Parser Extension,[0],[0]
It assumes that each expression always maps to the same constituent label.,4 Rapid Parser Extension,[0],[0]
Consider “2x = 3y”.,4 Rapid Parser Extension,[0],[0]
"This is a verb phrase in the sentence “In the above figure, x is prime and 2x = 3y.”",4 Rapid Parser Extension,[0],[0]
"However, it is a noun phrase in the sentence “The equation 2x = 3y has 2 solutions.”",4 Rapid Parser Extension,[0],[0]
"If we replace both instances with the same dummy word, the parser will almost certainly become confused in one of the two instances.
2.",4 Rapid Parser Extension,[0],[0]
It assumes that each expression is always a constituent.,4 Rapid Parser Extension,[0],[0]
Suppose that we replace the expression “AB < 30” with a dummy word.,4 Rapid Parser Extension,[0],[0]
"This means we cannot properly parse a sentence like “When angle AB < 30, the lines are parallel,” because the constituent “angle AB” no longer exists in the resulting sentence.
3.",4 Rapid Parser Extension,[0],[0]
It does not handle other syntactic variation.,4 Rapid Parser Extension,[0],[0]
"As we will see in the next section, the
geometry domain has a propensity for using right-attaching participial adjective phrases, like “labeled x” in the phrase “the segment labeled x.” Encouraging a parser to recognize this syntactic construct is out-of-scope for the “regex-and-replace” strategy.
",4 Rapid Parser Extension,[0],[0]
"Instead, we propose directly extending the parser by providing a few domain-specific examples like those in Figure 1.",4 Rapid Parser Extension,[0],[0]
"Because RSP’s model directly predicts span constituency, we can simply mark up a sentence with the “tricky” domain-specific constituents that the model will not already have learned from WSJTRAIN.",4 Rapid Parser Extension,[0],[0]
"For instance, we mark up NOUN-LABEL constructs like “chord BD”, and equations like “AD = 4”.
",4 Rapid Parser Extension,[0],[0]
"From these marked-up sentences, we can extract training instances declaring the constituency of certain spans (like “to chord BD” in the third example) and the implied non-constituency of certain spans (like “perpendicular to chord” in the third example).",4 Rapid Parser Extension,[0],[0]
"We also allow annotators to explicitly declare the non-constituency of a span via an alternative markup (not shown).
",4 Rapid Parser Extension,[0],[0]
We do not require annotators to provide span labels (although they can if desired).,4 Rapid Parser Extension,[0],[0]
"If a training instance merely declares a span to be a constituent (but does not provide a particular label), then the loss function only records loss when that span is classified as a non-constituent (i.e. any label is ok).",4 Rapid Parser Extension,[0],[0]
"We took the publicly available training data from (Seo et al., 2015), split the data into sentences, and then annotated each sentence as in Figure 1.",5.1 Geometry Questions,[0],[0]
"Next, we randomly split these sentences into GEOTRAIN and GEODEV7.",5.1 Geometry Questions,[0],[0]
"After removing duplicate sentences spanning both sets, we ended up with 63 annotated sentences in GEOTRAIN and 62 in GEODEV.",5.1 Geometry Questions,[0],[0]
"In GEOTRAIN, we made an average of 2.8 constituent declarations and 0.3 (explicit) nonconstituent declarations per sentence.
",5.1 Geometry Questions,[0],[0]
"After preparing the data, we started with RSP trained on WSJTRAIN, and fine-tuned it on minibatches containing 50 randomly selected WSJTRAIN sentences, plus all of GEOTRAIN.",5.1 Geometry Questions,[0],[0]
The results are in table 6.,5.1 Geometry Questions,[0],[0]
"After fine-tuning, the model
7GEOTRAIN and GEODEV are available at https://github.com/vidurj/parser-adaptation/tree/master/data.
gets 87% of the 185 annotations on GEODEV correct, compared with 71.9% before fine-tuning8.",5.1 Geometry Questions,[0],[0]
"Moreover, the fraction of sentences with no errors increases from 45.2% to 72.6%.",5.1 Geometry Questions,[0],[0]
"With only a few dozen partially-annotated training examples, not only do we see a large increase in domain performance, but there is also no degradation in the parser’s performance on newswire.",5.1 Geometry Questions,[0],[0]
"Some GEODEV parses have enormous qualitative differences, like the example shown in Figure 3.
",5.1 Geometry Questions,[0],[0]
"For the GEODEV sentences on which we get errors after retraining, the errors fall predominantly into three categories.",5.1 Geometry Questions,[0],[0]
"First, approximately 44% have some mishandled math syntax, like failing to recognize “dimensions 16 by 8” as a constituent, or providing a flat structuring of the equation “BAC = 1/4 * ACB” (instead of recognizing “1/4 * ACB” as a subconstituent).",5.1 Geometry Questions,[0],[0]
"Second, approximately 19% have PP-attachment errors.",5.1 Geometry Questions,[0],[0]
"Third, another 19% fail to correctly analyze right-attaching participial adjectives like “labeled x” in the noun phrase “the segment labeled x” or
8This improvement has a p-value of 10−4 under the onesided, two-sample difference between proportions test.
“indicated” in the noun phrase “the center indicated.”",5.1 Geometry Questions,[0],[0]
This phenomenon is unusually frequent in geometry but was insufficiently marked-up in our training examples.,5.1 Geometry Questions,[0],[0]
"For instance, while we have a training instance “Find [ the measure of [ the angle designated by x ]",5.1 Geometry Questions,[0],[0]
"],” it does not explicitly highlight the constituency of “designated by x”.",5.1 Geometry Questions,[0],[0]
"This suggests that in practice, this domain adaptation method could benefit from an iterative cycle in which a user assesses the parser’s errors on their target domain, creates some partial annotations that address these issues, retrains the parser, and then repeats the process until satisfied.",5.1 Geometry Questions,[0],[0]
"As a proof-of-concept, we invented 3 additional sentences with right-attaching participial adjectives (shown in Figure 4), added them to GEOTRAIN, and then retrained.",5.1 Geometry Questions,[0],[0]
"Indeed, the handling of participial adjectives in GEODEV improved, increasing the overall percentage of correctly identified constituents to 88.6% and the percentage of errorfree sentences to 75.8%.",5.1 Geometry Questions,[0],[0]
"We ran a similar experiment using biomedical and chemistry text, taken from the unannotated data provided by (Nivre et al., 2007).",5.2 Biomedicine and Chemistry,[0],[0]
We partially annotated 134 sentences and randomly split them into BIOCHEMTRAIN (72 sentences) and BIOCHEMDEV (62 sentences)9.,5.2 Biomedicine and Chemistry,[0],[0]
"In BIOCHEMTRAIN, we made an average of 4.2 constituent declarations per sentence.",5.2 Biomedicine and Chemistry,[0],[0]
"We made no nonconstituent declarations.
",5.2 Biomedicine and Chemistry,[0],[0]
"Again, we started with RSP trained on WSJTRAIN, and fine-tuned it on minibatches containing annotations from 50 randomly selected WSJ-
9BIOCHEMTRAIN and BIOCHEMDEV are available at https://github.com/vidurj/parser-adaptation/tree/master/data.
TRAIN sentences, plus all of BIOCHEMTRAIN.",5.2 Biomedicine and Chemistry,[0],[0]
Table 7 shows the improvement in the percentage of correctly-identified annotated constituents and the percentage of test sentences for which the parse agrees with every annotation.,5.2 Biomedicine and Chemistry,[0],[0]
"As with the geometry domain, we get significant improvements using only dozens of partially annotated training sentences.",5.2 Biomedicine and Chemistry,[0],[0]
"The two major themes of this paper, domain adaptation and learning from partial annotation, each have a long tradition in natural language processing.",6 Related Work,[0],[0]
"Domain adaptation has been recognized as a major NLP problem for over a decade (Ben-David et al., 2006; Blitzer et al., 2006; Daumé, 2007; Finkel and Manning, 2009).",6.1 Domain Adaptation,[0],[0]
"In particular, domain adaptation for parsers (Plank, 2011; Ma and Xia, 2013) has received considerable attention.",6.1 Domain Adaptation,[0],[0]
"Much of this work (McClosky et al., 2006b; Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; Kawahara and Uchimoto, 2008; McClosky et al., 2010; Sagae, 2010; Baucom et al., 2013; Yu et al., 2015) has focused on how to best use co-training (Blum and Mitchell, 1998) or self-training to augment a small domain corpus, or how to best combine models to perform well on a particular domain.
",6.1 Domain Adaptation,[0],[0]
"In this work, we focus on the direct impact that just a few dozen partially annotated out-of-domain examples can have, when using a particular neural model with contextualized word representations.",6.1 Domain Adaptation,[0],[0]
"Co-training, self-training, and model combination are orthogonal to our approach.",6.1 Domain Adaptation,[0],[0]
"Our work is a spiritual successor to (Garrette and Baldridge, 2013), which shows how to train a part-of-speech tagger with a minimal amount of annotation effort.",6.1 Domain Adaptation,[0],[0]
"Most literature on training parsers from partial annotations (Sassano and Kurohashi, 2010; Spreyer et al., 2010; Flannery et al., 2011; Flannery and Mori, 2015; Mielens et al., 2015) focuses on dependency parsing.",6.2 Learning from Partial Annotation,[0],[0]
"(Li et al., 2016) provides a good overview.",6.2 Learning from Partial Annotation,[0],[0]
"Here we highlight three important highlevel strategies.
",6.2 Learning from Partial Annotation,[0],[0]
"The first is “complete-then-train” (Mirroshandel and Nasr, 2011; Majidi and Crane, 2013), which “completes” every partially annotated de-
pendency parse by finding the most likely parse (according to an already trained parser model) that respects the constraints of the partial annotations.",6.2 Learning from Partial Annotation,[0],[0]
"These “completed” parses are then used to train a new parser.
",6.2 Learning from Partial Annotation,[0],[0]
"The second strategy (Nivre et al., 2014; Li et al., 2016) is similar to “complete-then-train,” but integrates parse completion into the training process.",6.2 Learning from Partial Annotation,[0],[0]
"At each iteration, new “complete” parses are created using the parser model from the most recent training iteration.
",6.2 Learning from Partial Annotation,[0],[0]
"The third strategy (Li et al., 2014, 2016) transforms each partial annotation into a forest of parses that encodes all fully-specified parses permitted by the partial annotation.",6.2 Learning from Partial Annotation,[0],[0]
"Then, the training objective is modified to support optimization over these forests.
",6.2 Learning from Partial Annotation,[0],[0]
Our work differs from these in two respects.,6.2 Learning from Partial Annotation,[0],[0]
"First, since we are training a constituency parser, our partial annotations are constituent bracketings rather than dependency arcs.",6.2 Learning from Partial Annotation,[0],[0]
"Second, and more importantly, we can use the partial annotations for training without modifying either the training algorithm or the training data.
",6.2 Learning from Partial Annotation,[0],[0]
"While the bulk of the literature on training from partial annotations focuses on dependency parsing, the earliest papers (Pereira and Schabes, 1992; Hwa, 1999) focus on constituency parsing.",6.2 Learning from Partial Annotation,[0],[0]
These leverage an adapted version of the inside-outside algorithm for estimating the parameters of a probabilistic context-free grammar (PCFG).,6.2 Learning from Partial Annotation,[0],[0]
"Our work is not tied to PCFG parsing, nor does it require a specialized training algorithm when going from full annotations to partial annotations.",6.2 Learning from Partial Annotation,[0],[0]
Recent developments in neural natural language processing have made it very easy to build custom parsers.,7 Conclusion,[0],[0]
"Not only do contextualized word representations help parsers learn the syntax of new domains with very few examples, but they also work extremely well with parsing models that correspond directly with a granular and intuitive annotation task (like identifying whether a span is a constituent).",7 Conclusion,[0],[0]
"This allows you to train with either full or partial annotations without any change to the training process.
",7 Conclusion,[0],[0]
"This work provides a convenient path forward for the researcher who requires a parser for their domain, but laments that “parsers don’t work outside of newswire.”",7 Conclusion,[0],[0]
"With a couple hours of effort
(and a layman’s understanding of syntactic building blocks), they can get significant performance improvements.",7 Conclusion,[0],[0]
"We envision an iterative use case in which a user assesses a parser’s errors on their target domain, creates some partial annotations to teach the parser how to fix these errors, then retrains the parser, repeating the process until they are satisfied.",7 Conclusion,[0],[0]
We revisit domain adaptation for parsers in the neural era.,abstractText,[0],[0]
First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain.,abstractText,[0],[0]
"As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus.",abstractText,[0],[0]
"For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations.",abstractText,[0],[0]
"For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples.",abstractText,[0],[0]
"In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%.",abstractText,[0],[0]
This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.,abstractText,[0],[0]
Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 644–649 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
644",text,[0],[0]
"Automatically extracting common sense from text is a long-standing challenge in natural language processing (Schubert, 2002; Van Durme and Schubert, 2008; Vanderwende, 2005).",1 Introduction,[0],[0]
"As argued by Forbes and Yejin (2017), typical language use may reflect common sense, but the commonsense knowledge itself is not often explicitly stated, due to reporting bias (Gordon and Van Durme, 2013).",1 Introduction,[0],[0]
"Thus, additional human knowledge or annotated training data are often used to help systems learn common sense.
",1 Introduction,[0],[0]
"In this paper, we study methods for reducing the amount of human input needed to learn common sense.",1 Introduction,[0],[0]
"Specifically, we focus on learning relative comparisons of (one-dimensional) object properties, such as the fact that a cantaloupe is more round than a hammer.",1 Introduction,[0],[0]
"Methods for learning this kind of common sense have been developed previously (e.g. Forbes and Choi, 2017), but the best-performing methods in that previous work requires dozens of manually-annotated frames for each comparison property, to connect the property to how it is indirectly reflected in text—e.g., if
text asserts that “x carries y,” this implies that x is probably larger than y.
Our architecture for relative comparisons follows the zero-shot learning paradigm (Palatucci et al., 2009).",1 Introduction,[0],[0]
"It takes the form of a neural network that compares a projection of embeddings for each of two objects (e.g. “elephant” and “tiger”) to the embeddings for the two poles of the target dimension of comparison (e.g., “big” and ”small” for the size property).",1 Introduction,[0],[0]
"The projected object embeddings are trained to be closer to the appropriate pole, using a small training set of hand-labeled comparisons.",1 Introduction,[0],[0]
"Our experiments reveal that our architecture outperforms previous work, despite using less annotated data.",1 Introduction,[0],[0]
"Further, because our architecture takes the property (pole) labels as arguments, it can extend to the zero-shot setting in which we evaluate on properties not seen in training.",1 Introduction,[0],[0]
"We find that in zero-shot, our approach outperforms baselines and comes close to supervised results, but providing labels for both poles of the relation rather than just one is important.",1 Introduction,[0],[0]
"Finally, because the number of properties we wish to learn is large, we experiment with active learning (AL) over a larger property space.",1 Introduction,[0],[0]
"We show that synthesizing AL queries can be effective using an approach that explicitly models which comparison questions are nonsensical (e.g., is Batman taller than Democracy?).",1 Introduction,[0],[0]
We release our code base and a new commonsense data set to the research community.1,1 Introduction,[0],[0]
"We define the task of comparing object properties in two different ways: a three-way classification task, and a four-way classification task.",2 Problem Definition and Methods,[0],[0]
"In the three-way classification task, we want to estimate the following conditional probability:
P (L|O1,O2,Property),L ∈ { < , > , ≈ }.",2 Problem Definition and Methods,[0],[0]
"1https://github.com/yangyiben/PCE
For example, Prob(An elephant is larger than a dog) can be expressed as P (L = > |O1 = ”elephant”,O2 = ”dog”,Property = ”size”).",2 Problem Definition and Methods,[0],[0]
"The three-way classification task has been explored in previous work (Forbes and Choi, 2017) and is only performed on triples where both objects have the property, so that the comparison is meaningful.",2 Problem Definition and Methods,[0],[0]
"In applications, however, we may not know in advance which comparisons are meaningful.",2 Problem Definition and Methods,[0],[0]
"Thus, we also define a four-way classification task to include ”not applicable” as the fourth label, so that inference can be performed on any objectproperty triples.",2 Problem Definition and Methods,[0],[0]
"In the four-way task, the system is tasked with identifying the nonsensical comparisons.",2 Problem Definition and Methods,[0],[0]
"Formally, we want to estimate the following conditional probability:",2 Problem Definition and Methods,[0],[0]
"For each comparison property, we pick an adjective and its antonym to represent the { < , > } labels.",2.1 Three-way Model,[0],[0]
"For example, for the property size, we pick ”big” and ”small”.",2.1 Three-way Model,[0],[0]
The adjective ”similar” serves as the label for ≈ for all properties.,2.1 Three-way Model,[0],[0]
"Under this framework, a relative comparison question, for instance, ”Is a dog bigger than an elephant?”, can be formulated as a quintuple query to the model, namely {dog, elephant, small, similar, big}.",2.1 Three-way Model,[0],[0]
"Denoting the word embeddings for tokens in a quintuple query as X , Y , R<, R≈, R>, our three-way model is defined as follows:
P (L = s|Q) = softmax(Rs · σ((X ⊕ Y )W )),
for s ∈ {<, >, ≈}, where Q is an quintuple query, σ(·) is an activation function and W is a learnable weight matrix.",2.1 Three-way Model,[0],[0]
The symbol ⊕ represents concatenation.,2.1 Three-way Model,[0],[0]
We refer to this method as PCE (Property Comparison from Embeddings) for the 3-way task.,2.1 Three-way Model,[0],[0]
"We also experiment with generating label representations from just a single adjective (property) embedding R<, namely R≈ = σ(R<W2), R> = σ(R<W3).",2.1 Three-way Model,[0],[0]
"We refer to this simpler method as PCE(one-pole).
",2.1 Three-way Model,[0],[0]
"We note that in both the three- and four-way settings, the question ”A>B?” is equivalent to ”B<A?”.",2.1 Three-way Model,[0],[0]
"We leverage this fact at test time by feeding our network a reversed object pair, and taking the average of the aligned network outputs before the softmax layer to reduce prediction variance.",2.1 Three-way Model,[0],[0]
"We refer to our model without this technique as PCE(no reverse).
",2.1 Three-way Model,[0],[0]
The key distinction of our method is that it learns a projection from the object word embedding space to the label embedding space.,2.1 Three-way Model,[0],[0]
This allows the model to leverage the property label embeddings to perform zero-shot prediction on properties not observed in training.,2.1 Three-way Model,[0],[0]
"For example, from a training example ”dogs are smaller than elephants”, the model will learn a projection that puts ”dogs” relatively closer to ”small,” and far from ”big” and ”similar.”",2.1 Three-way Model,[0],[0]
"Doing so may also result in projecting ”dog” to be closer to ”light” than to ”heavy,” such that the model is able to predict ”dogs are lighter than elephants” despite never being trained on any weight comparison examples.",2.1 Three-way Model,[0],[0]
"Our four-way model is the same as our three-way model, with an additional module to learn whether the comparison is applicable.",2.2 Four-way Model,[0],[0]
"Keeping the other output nodes unchanged, we add an additional component into the softmax layer to output the probability of ”N/A”:
hx = σ(XWa), hy = σ(YWa), Ai = hi ·R> + hi ·R<, P (L = N/A |Q) ∝",2.2 Four-way Model,[0],[0]
exp(Ax +Ay).,2.2 Four-way Model,[0],[0]
"We propose a method to synthesize informative queries to pose to annotators, a form of active learning (Settles, 2009).",2.3 Synthesis for Active Learning,[0],[0]
We use the common heuristic that an informative training example will have a high uncertainty in the model’s predictive distribution.,2.3 Synthesis for Active Learning,[0],[0]
"We adopt the confidence measure (Culotta and McCallum, 2005) to access the uncertainty of a given example:
Uncertainty(x) = 1−max y P (y|x,Dtrain).
",2.3 Synthesis for Active Learning,[0],[0]
"Good candidates for acquisition should have high uncertainty measure, but we also want to avoid querying outliers.",2.3 Synthesis for Active Learning,[0],[0]
"As the vocabulary is finite, it is possible to evaluate the uncertainty measures for all possible inputs to synthesize the most uncertain query.",2.3 Synthesis for Active Learning,[0],[0]
"However, such a greedy policy is expensive and prone to selecting outliers.",2.3 Synthesis for Active Learning,[0],[0]
"Hence, we adopt a sampling based synthesis strategy: at each round, we generate one random object pair per property, and query the one that achieves the highest uncertainty measure.
",2.3 Synthesis for Active Learning,[0],[0]
"A classical difficulty faced by synthesis approaches to active learning is that they may pro-
duce unnatural queries that are difficult for a human to label (Baum and Lang, 1992).",2.3 Synthesis for Active Learning,[0],[0]
"However, our task formulation includes ”similar” and ”N/A” classes that encompass many of the more difficult or confusing comparisons, which we believe aids the effectiveness of the synthesis approach.",2.3 Synthesis for Active Learning,[0],[0]
We now present our experimental results on both the three-way and four-way tasks.,3 Experiments,[0],[0]
"We test our three-way model on the VERB PHYSICS data set from (Forbes and Choi, 2017).",3.1 Data Sets,[0],[0]
"As there are only 5 properties in VERB PHYSICS, we also develop a new data set we call PROPERTY COMMON SENSE.",3.1 Data Sets,[0],[0]
"We select 32 commonsense properties to form our property set (e.g., value, roundness, deliciousness, intelligence, etc.).",3.1 Data Sets,[0],[0]
"We extract object nouns from the McRae Feature Norms dataset (McRae et al., 2005) and add selected named entities to form a object vocabulary of 689 distinct objects.",3.1 Data Sets,[0],[0]
"We randomly generate 3148 object-property triples, label them and reserve 45% of the data for the test set.",3.1 Data Sets,[0],[0]
"We further add 5 manually-selected applicable comparison examples per property to our test set, in order to make sure each property has some applicable testing examples.",3.1 Data Sets,[0],[0]
"To verify the labeling, we have a second annotator redundantly label 200 examples and find a Cohen’s Kappa of 0.64, which indicates good annotator agreement (we analyze the source of the disagreements in Section 4.1).",3.1 Data Sets,[0],[0]
"The training set is used for the passive learning and pool-based active learning, and a human oracle provides labels in the synthesis active learning setting.",3.1 Data Sets,[0],[0]
"We experiment with three types of embeddings: GloVe, normalized 300-dimensional embeddings trained on a corpus of 6B tokens (Pennington et al., 2014) (the F&C method (Forbes and Choi, 2017) uses the 100-dimensional version, as it achieves the highest validation accuracy for their methods); Word2vec, normalized 300- dimensional embeddings trained on 100B tokens (Mikolov et al., 2013); and LSTM, the normalized 1024-dimensional weight matrix from the softmax layer of the Google 1B LSTM language model (Jozefowicz et al., 2016).
",3.2 Experimental Setup,[0],[0]
"For training PCE, we use an identity activation function and apply 50% dropout.",3.2 Experimental Setup,[0],[0]
"We use the Adam optimizer with default settings to train the models for 800 epochs, minimizing cross entropy loss.",3.2 Experimental Setup,[0],[0]
"For zero-shot learning, we adopt a hold-oneproperty-out scheme to test our models’ zero-shot performance.
",3.2 Experimental Setup,[0],[0]
"Finally, for active learning, we use Word2vec embeddings.",3.2 Experimental Setup,[0],[0]
All the models are trained on 200 random training examples to warm up.,3.2 Experimental Setup,[0],[0]
We train for 20 epochs after each label acquisition.,3.2 Experimental Setup,[0],[0]
"To smooth noise, we report the average of 20 different runs of random (passive learning) and least confident (LC) pool-based active learning (Culotta and McCallum, 2005) baselines.",3.2 Experimental Setup,[0],[0]
"We report the average of only 6 runs for an expected model change (EMC) pool-based active learning (Cai et al., 2013) baseline due to its high computational cost, and of only 2 runs for our synthesis active learning approach due to its high labeling cost.",3.2 Experimental Setup,[0],[0]
The pool size is 1540 examples.,3.2 Experimental Setup,[0],[0]
"In Table 1, we compare the performance of the three-way PCE model against the existing state of the art on the VERB PHYSICS data set.",3.3 Results,[0],[0]
The use of LSTM embeddings in PCE yields the best accuracy for all properties.,3.3 Results,[0],[0]
"Across all embedding choices, PCE performs as well or better than F&C, despite the fact that PCE does not use the annotated frames that F&C requires (approximately 188 labels per property).",3.3 Results,[0],[0]
"Thus, our approach matches or exceeds the performance of previous work using significantly less annotated knowledge.",3.3 Results,[0],[0]
"The lower performance of ”no reverse” shows that the simple method of averaging over the reversed object pair is effective.
",3.3 Results,[0],[0]
Table 2 evaluates our models on properties not seen in training (zero-shot learning).,3.3 Results,[0],[0]
"We compare against a random baseline, and an Emb-Similarity baseline that classifies based on the cosine similarity of the object embeddings to the pole label embeddings (i.e., without the projection layer in PCE).",3.3 Results,[0],[0]
PCE outperforms the baselines.,3.3 Results,[0],[0]
"Although the one-pole method was shown to perform similarly to the two-pole method for properties seen in training (Table 1), we see that for zero-shot learning, using two poles is important.
",3.3 Results,[0],[0]
"In Table 3, we show that our four-way models with different embeddings beat both the majority and random baselines on the PROPERTY
COMMON SENSE data.",3.3 Results,[0],[0]
"Here, the LSTM embeddings perform similarly to the Word2vec embeddings, perhaps because the PROPERTY COMMON SENSE vocabulary consists of less frequent nouns than in VERB PHYSICS.",3.3 Results,[0],[0]
"Thus, the Word2vec embeddings are able to catch up due to their larger vocabulary and much larger training corpus.
",3.3 Results,[0],[0]
"Finally, in Figure 1, we evaluate in the active learning setting.",3.3 Results,[0],[0]
"The synthesis approach performs best, especially later in training when the training pool for the pool-based methods has only uninformative examples remaining.",3.3 Results,[0],[0]
Figure 2 helps explain the relative advantage of the synthesis approach: it is able to continue synthesizing informative (uncertain) queries throughout the entire training run.,3.3 Results,[0],[0]
"As noted above, we found a “good” level of agreement (Cohen’s Kappa of 0.64) for our PROPERTY COMMON SENSE data, which is lower than one might expect for task aimed at common sense.",4.1 Sources of annotator disagreement,[0],[0]
"We
analyzed the disagreements and found that they stem from two sources of subjectivity in the task.",4.1 Sources of annotator disagreement,[0],[0]
"The first is that different labelers may have different thresholds for what counts as similar—a spider and an ant might be marked similar in size for one labeler, but not for another labeler.",4.1 Sources of annotator disagreement,[0],[0]
"In our data, 58% of the disagreements are cases in which one annotator marks similar while the other says not similar.",4.1 Sources of annotator disagreement,[0],[0]
The second is that different labelers have different standards for whether a comparison is N/A.,4.1 Sources of annotator disagreement,[0],[0]
"For example, in our data set, one labeler labels that a toaster is physically stronger than alcohol, and the other labeler says the comparison is N/A. 37% of our disagreements are due to this type of subjectivity.",4.1 Sources of annotator disagreement,[0],[0]
"The above two types of subjectivity account for almost all disagreements
(95%), and the remaining 5% are due to annotation errors (one of the annotators makes mistake).",4.1 Sources of annotator disagreement,[0],[0]
"Since we adopt an identity activation function and a single layer design, it is possible to simplify the mathematical expression of our model to make it more interpretable.",4.2 Model Interpretation,[0],[0]
"After accounting for model averaging, we have the following equality:
P (L =< |Q) ∝",4.2 Model Interpretation,[0],[0]
exp(R< · ((X ⊕ Y )W ),4.2 Model Interpretation,[0],[0]
+,4.2 Model Interpretation,[0],[0]
"R> · ((Y ⊕X)W ))
",4.2 Model Interpretation,[0],[0]
= exp(RT<(XW1 + YW2) +R T,4.2 Model Interpretation,[0],[0]
>,4.2 Model Interpretation,[0],[0]
(YW1 +XW2)),4.2 Model Interpretation,[0],[0]
∝ exp((R,4.2 Model Interpretation,[0],[0]
< −R>)T,4.2 Model Interpretation,[0],[0]
"(XW1 +XW2)),
where W = W1 ⊕W2.",4.2 Model Interpretation,[0],[0]
"So we can define a score of ”R<” for a object with embedding X as the following:
score(X,R<) =",4.2 Model Interpretation,[0],[0]
"(R< −R>)T (XW1 +XW2).
",4.2 Model Interpretation,[0],[0]
An object with a higher score for R< is more associated with the R< pole than the R> one.,4.2 Model Interpretation,[0],[0]
"For example, score(”elephant”,”small”) represents how small an elephant is—a larger score indicates a smaller object.",4.2 Model Interpretation,[0],[0]
Table 4 shows smallness scores for 5 randomly picked objects from the VERB PHYSICS data set.,4.2 Model Interpretation,[0],[0]
PCE tends to assign higher scores to the smaller objects in the set.,4.2 Model Interpretation,[0],[0]
PCE requires labels for the poles of the target object property.,4.3 Sensitivity to pole labels,[0],[0]
"Table 5 presents a limited sensitivity
analysis to pole labels, evaluating the test accuracy of PCE as the pole label varies among different combinations of synonyms for the size and speed relations.",4.3 Sensitivity to pole labels,[0],[0]
We evaluate in both the trained setting (comparable to the results in Table 1) and the zero-shot setting (comparable to Table 2).,4.3 Sensitivity to pole labels,[0],[0]
We see that the trained accuracy remains essentially unchanged for different pole labels.,4.3 Sensitivity to pole labels,[0],[0]
"In the zeroshot setting, all combinations achieve accuracy that beats the baselines in Table 2, but the accuracy value is somewhat sensitive to the choice of pole label.",4.3 Sensitivity to pole labels,[0],[0]
Exploring how to select pole labels and experimenting with richer pole representations such as textual definitions are items of future work.,4.3 Sensitivity to pole labels,[0],[0]
"In this paper, we presented a method for extracting commonsense knowledge from embeddings.",5 Conclusion,[0],[0]
Our experiments demonstrate that the approach is effective at performing relative comparisons of object properties using less hand-annotated knowledge than in previous work.,5 Conclusion,[0],[0]
"A synthesis active learner was found to boost accuracy, and further experiments with this approach are an item of future work.",5 Conclusion,[0],[0]
This work was supported in part by NSF Grant IIS-1351029.,Acknowledgments,[0],[0]
We thank the anonymous reviewers for helpful comments.,Acknowledgments,[0],[0]
"Intelligent systems require common sense, but automatically extracting this knowledge from text can be difficult.",abstractText,[0],[0]
"We propose and assess methods for extracting one type of commonsense knowledge, object-property comparisons, from pretrained embeddings.",abstractText,[0],[0]
"In experiments, we show that our approach exceeds the accuracy of previous work but requires substantially less hand-annotated knowledge.",abstractText,[0],[0]
"Further, we show that an active learning approach that synthesizes common-sense queries can boost accuracy.",abstractText,[0],[0]
Extracting Commonsense Properties from Embeddings with Limited Human Guidance,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 622–631, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Reflecting the rapid growth in the use of opinionated texts on the Web, such as comments on news articles and customer reviews, opinion mining has been explored to facilitate utilizing opinions mainly for improving products and decisionmaking purposes.",1 Introduction,[0],[0]
"While in a broad sense opinion mining refers to a process to discover useful knowledge latent in a corpus of opinionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect.",1 Introduction,[0],[0]
"Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit.",1 Introduction,[0],[0]
"We take the following review sentence as an example opinionated description.
",1 Introduction,[0],[0]
"(1) I think hotel A offers a reasonable price if you take a family trip with small kids.
",1 Introduction,[0],[0]
"From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit.
",1 Introduction,[0],[0]
"Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A
Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating.
",1 Introduction,[0],[0]
"Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements.",1 Introduction,[0],[0]
"For example, those who intend to improve the quality of hotel A may investigate representative values for “Aspect” in the units satisfying “Target=hotel A & Polarity=negative”, while those who look for accommodation may collect the opinion units for one or more candidate hotels and investigate the distribution of values for “Polarity” on an aspect-by-aspect basis.
",1 Introduction,[0],[0]
"However, in the above example (1), the evaluation for hotel A (“a reasonable price”) is valid for “if you take a family trip with small kids”, and thus it is not clear whether this evaluation is valid irrespective of the condition.",1 Introduction,[0],[0]
"For example, the price may not be reasonable for a single customer intending for business purposes.",1 Introduction,[0],[0]
"In this paper, we shall call such a condition “condition for opinion (CFO)”.",1 Introduction,[0],[0]
"We define CFO as a condition for which an opinion unit has a polarity.
",1 Introduction,[0],[0]
"The existing methods for opinion mining, which do not consider whether a target opinion is conditional, potentially overestimate or underestimate
622
the utility of hotel A and consequently decrease the quality of opinion mining.",1 Introduction,[0],[0]
"We manually analyzed the first 7 000 sentences in the Rakuten Travel data, which consists of 348 564 Japanese reviews for hotels in Japan (see Section 4 for details of this data) and found that 2 272 sentences are opinions, of which 630 opinions are conditional and thus the result for an existing method includes up to 28% (630/2272) errors.
",1 Introduction,[0],[0]
"Motivated by the above discussion, in this paper we propose a method to extract pairs of a CFO and its corresponding opinion unit from online reviews.",1 Introduction,[0],[0]
This method provides two solutions to the above problem.,1 Introduction,[0],[0]
"First, a passive solution is detecting whether an opinion includes a CFO and, if any, isolating that opinion from the target of opinion mining.",1 Introduction,[0],[0]
"As a result, we can avoid potential errors as much as possible but the coverage is decreased.
",1 Introduction,[0],[0]
"Second, an active solution is identifying the span of each CFO in conditional opinions and classify them according to semantic categories, such as purpose, situation, and user attribute so that finer-grained opinion mining can be realized.",1 Introduction,[0],[0]
"For example, the distribution of positive and negative opinions can be available on a category-bycategory basis.",1 Introduction,[0],[0]
"However, in this paper we focus only on the identification for CFOs and leave the semantic classification future work.
",1 Introduction,[0],[0]
"To produce a practical model for CFOs, it is important to investigate them from a grammar point of view.",1 Introduction,[0],[0]
It can easily be predicted that a typical grammatical unit for CFOs is a conditional clause as in example (1).,1 Introduction,[0],[0]
"Additionally, restrictive modifiers in general can potentially be CFOs because they restrict the validity of an opinion unit from a specific perspective.",1 Introduction,[0],[0]
"A restrictive modifier comprises a word, phrase, or clause.",1 Introduction,[0],[0]
"The CFO in example (1), which is a dependent clause functioning as a condition, is also a restrictive modifier.
",1 Introduction,[0],[0]
"Example (2), which has the same meaning as example (1), includes a CFO as a prepositional phrase.
",1 Introduction,[0],[0]
"(2) Hotel A offers a reasonable price for taking a family trip with small kids.
",1 Introduction,[0],[0]
"We denote CFOs and opinion words in bold and italic faces, respectively.",1 Introduction,[0],[0]
Examples (3) and (4) also include a CFO as a prepositional phrase.,1 Introduction,[0],[0]
"Unlike example (2), the validity of “reasonable” is restricted from time and comparison points of view, respectively.
(3) Hotel A offers a reasonable price during this holiday season.
",1 Introduction,[0],[0]
"(4) Hotel A offers a reasonable price for a four star hotel.
",1 Introduction,[0],[0]
"In example (5), which has a similar meaning to example (1), the CFO is a dependent clause functioning as a reason.
",1 Introduction,[0],[0]
"(5) Hotel A offers a reasonable price because we take a family trip with small kids.
",1 Introduction,[0],[0]
"Finally, as in example (6), an opinion holder can also be a CFO because the evaluation is restricted from a perspective of that specific person.
",1 Introduction,[0],[0]
"(6) My mother regarded hotel A as a reasonable choice.
",1 Introduction,[0],[0]
"If the restriction by a CFO is associated with a user-related perspective, we call such CFOs “userrestrictive CFOs (U-CFOs)”.",1 Introduction,[0],[0]
"In other words, target users to whom an opinion unit is relevant are restricted by its corresponding U-CFO, although those users may agree or disagree with the opinion.",1 Introduction,[0],[0]
"The CFOs in examples (1), (2), and (5) are U-CFOs because the target users are mainly those who intend to travel with their children.
",1 Introduction,[0],[0]
The CFO in example (3) is also U-CFO because the target users are those who intend to travel during a specific holiday season.,1 Introduction,[0],[0]
The CFO in example (6) is also U-CFO because the opinion holder (“my mother”) implies the opinion is relevant mainly to adult females.,1 Introduction,[0],[0]
"However, opinion holders who do not represent user-related perspectives, such as “I” without any profile, are not U-CFOs.
",1 Introduction,[0],[0]
The CFO in example (4) is not a U-CFO because the relevance of the opinion is not restricted to specific customers.,1 Introduction,[0],[0]
It may be argued that in example (4) the target users are restricted to those who are interested in the price.,1 Introduction,[0],[0]
"However, in example (4) the price restricts the aspect of the opinion unit, and should not be confused with U-CFOs and even CFOs, which restrict the validity of the opinion unit.
",1 Introduction,[0],[0]
"If we fully utilize U-CFOs, as discussed for the active solution above, we need to classify U-CFOs into semantic categories so that users can selectively read relevant opinions.",1 Introduction,[0],[0]
"In other words, the identification for U-CFOs facilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012).",1 Introduction,[0],[0]
"Candidate categories
include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place).",1 Introduction,[0],[0]
"However, we leave the classification for U-CFOs future work.",1 Introduction,[0],[0]
"As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time)",2 Related work,[0],[0]
"(He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013).",2 Related work,[0],[0]
"However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition.
",2 Related work,[0],[0]
Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences.,2 Related work,[0],[0]
"Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research.",2 Related work,[0],[0]
"Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral.",2 Related work,[0],[0]
"Examples (7) and (8) are such conditional sentences associated with neutral and positive categories, respectively.
(7) Hotel A would not have survived if the price was not reasonable.
",2 Related work,[0],[0]
"(8) If you are looking for a hotel with a reasonable price, stay at hotel A.
",2 Related work,[0],[0]
"In example (7), although the subordinate clause includes the opinion word “reasonable”, none of the subordinate clause, main clause, or entire sentence is an opinion.",2 Related work,[0],[0]
"In example (8), the entire sentence is an unconditional opinion about the price for hotel A, but the main and subordinate clauses are not opinions independently.",2 Related work,[0],[0]
"In contrast, the purpose of our research is to identify conditional opinions, in which the main and subordinate clauses are an opinion and its condition, respectively.
",2 Related work,[0],[0]
"Kim and Hovy (2006) proposed a method to identify a reason for the evaluation in an opinion,
such as “the service was terrible because the staff was rude”.",2 Related work,[0],[0]
"Although as discussed in Section 1 reasons can be CFOs, their purpose is to identify grounds that justify the evaluation and thus is different from our purpose.
",2 Related work,[0],[0]
"As discussed in Section 1, our research is related to predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012).",2 Related work,[0],[0]
"The method proposed by O’Mahony and Smyth (2010) determines the helpfulness of a product review independent of the user profile and thus cannot recommend reviews based on userrelated attributes.
",2 Related work,[0],[0]
Moghaddam et al. (2012) used collaborative filtering to predict the review helpfulness.,2 Related work,[0],[0]
"The evaluation by a target user for past reviews is used to model the user and predict the helpfulness for unread reviews, which results in different predictions depending on the user.",2 Related work,[0],[0]
"An advantage of collaborative filtering is its applicability to items whose content is usually difficult to analyze, such as videos.",2 Related work,[0],[0]
"However, this advantage is diluted in recommending review text, from which effective features for user modeling, such as U-CFOs, can be obtained by opinion mining.",2 Related work,[0],[0]
The task in this paper is to extract conditionopinion relations from reviews in Japanese.,3 Proposed method,[0],[0]
"Currently, we assume that an opinion unit and its corresponding CFO are in the same sentence, and thus perform the extraction on a sentence-by-sentence basis.",3 Proposed method,[0],[0]
"Given a sentence in reviews, we first search for an opinion unit, and if found, we also search for its corresponding CFO.",3 Proposed method,[0],[0]
"Because in the first process we rely on an existing method for the opinion extraction, in this paper we focus only on the extraction for CFOs.
",3 Proposed method,[0],[0]
"As discussed in Section 1, because CFOs can be different grammatical units, their length and structure are not standardized.",3 Proposed method,[0],[0]
"We model the extraction for CFOs as the BIO chunking, which labels each token in a sentence as being the beginning (B), inside (I), or outside (O) of a span of interest.",3 Proposed method,[0],[0]
We use “Other” to refer to “O” to avoid confusion between “O” and “0” (zero).,3 Proposed method,[0],[0]
"To subdivide “B” and “I” into U-CFOs and other CFOs, we use suffixes “U” and “C”, respectively, such as “BU” denoting the beginning of a U-CFO.",3 Proposed method,[0],[0]
"We use “Cond” to refer to any of BU, IU, BC, or IC.
Because we use the same method for both U-
CFOs and other CFOs, the above distinction only increases the number of categories to which each token is classified.",3 Proposed method,[0],[0]
"If the distinction of U-CFOs is not important, the above suffixes can be omitted.
",3 Proposed method,[0],[0]
"We regard Japanese bunsetsu phrases, which consist of a content word and one or more postpositional particles, as tokens, and extract a sequence of a BU-phrase and one or more IU-phrases, or an independent BU-phrase as a condition.",3 Proposed method,[0],[0]
The same method is used for BC/IC-phrases.,3 Proposed method,[0],[0]
"However, words and phrases in an opinion unit are classified into its corresponding element.",3 Proposed method,[0],[0]
"For example, an aspect phrase is classified into the aspect category.
",3 Proposed method,[0],[0]
"Given an input sequence of bunsetsu phrases, x = x1 . . .",3 Proposed method,[0],[0]
"xn, our task is to predict a sequence of labels, y = y1 . . .",3 Proposed method,[0],[0]
"yn, where yi ∈ {BU, IU,BC, IC, Other, Target, Aspect, OpinionWord}.",3 Proposed method,[0],[0]
"However, because an opinion unit in an input sentence has been identified in advance, the task is a quinary classification with respect to yi ∈ {BU, IU,BC, IC,Other}.",3 Proposed method,[0],[0]
"We use Conditional Random Fields (CRF) (Lafferty et al., 2001) to train a classifier for categorizing each bunsetsu phrase into any of the aforementioned five categories.",3 Proposed method,[0],[0]
"We use a combination of unigram and bigram models and calculate the conditional probability, p(y|x), for linear-chain CRF by Equation (1).
p(y|x)",3 Proposed method,[0],[0]
= 1,3 Proposed method,[0],[0]
"Zx
exp (∑
i,k λk ·fk(yi, x)+ ∑ i,k µk ·gk(yi−1, yi, x) )",3 Proposed method,[0],[0]
"(1)
Here, Zx denotes a normalization factor, and fk and gk denote feature functions for unigram and bigram models, respectively.",3 Proposed method,[0],[0]
"Let xi,v denote a feature value for xi.",3 Proposed method,[0],[0]
"While in the unigram model yi depends on either xi−1,v or xi,v, in the bigram model yi depends on either a combination of xi,v and yi−1 or that of xi−1,v and yi−1.",3 Proposed method,[0],[0]
"Feature functions are produced for any possible combinations of the values for the variables used (xi,v, yi−1, and yi in fk), and take 1 if the corresponding combination appears and 0 otherwise.",3 Proposed method,[0],[0]
"We use the four combinations “unigram xi,v”, “unigram xi−1,v”, “bigram yi−1 xi,v”, and “bigram yi−1 xi−1,v” for feature functions.
",3 Proposed method,[0],[0]
The question here is how CFOs and U-CFOs can be modeled and what kind of features are needed.,3 Proposed method,[0],[0]
"We assume characteristics of CFOs, and U-CFOs and partially exemplify their validity us-
ing Figure 1, which depicts an example input sentence and information related to its constituent bunsetsu phrases.",3 Proposed method,[0],[0]
"In the upper part of Figure 1, a rectangle and an arrow denote a bunsetstu phrase and a syntactic dependency between two phrases, respectively, and in each phrase we show Japanese words based on the Hepburn system and their English translations in parentheses.
CFOs are associated with the following characteristics.
",3 Proposed method,[0],[0]
"(a) By definition, CFOs determine the validity of the evaluation in an opinion unit, and thus syntactically modify an opinion word.",3 Proposed method,[0],[0]
"Consequently, CFOs usually do not modify other elements in an opinion unit, such as an aspect.
",3 Proposed method,[0],[0]
"(b) Like a conjunction in a conditional clause in English, such as “if”, a CFO in Japanese also includes a clue expression, which is usually a functional expression (Matsuyoshi et al., 2006) in the tail phrase, such as “ni wa (“for” in English)”.
",3 Proposed method,[0],[0]
(c),3 Proposed method,[0],[0]
"The distribution for parts of speech as the head of CFOs is skewed and heads of CFOs are usually a noun or verb.
",3 Proposed method,[0],[0]
"Additionally, U-CFOs are associated with the following characteristics.
",3 Proposed method,[0],[0]
(d),3 Proposed method,[0],[0]
"If a CFO is an opinion holder as in example (6) in Section 1, it is usually a U-CFO, which is the subject appearing at the beginning of a target sentence.
",3 Proposed method,[0],[0]
"(e) By definition, U-CFOs include expressions related to user attributes, such as “nervosity” in Figure 1.
",3 Proposed method,[0],[0]
"In this paper, we propose thirteen features to model CFOs and U-CFOs.",3 Proposed method,[0],[0]
"In the bottom part of Figure 1, for each phrase we show the values of the thirteen features F1–F13 described below.",3 Proposed method,[0],[0]
These features were developed for the above five characteristics.,3 Proposed method,[0],[0]
"F1–F5, F7–F10 and F13 are associated with (a), (b) and (c), respectively, while F6 and F11–F12 are associated with (d) and (e).
",3 Proposed method,[0],[0]
"F1: Dependency distance to opinion word CFOs, which affect the evaluation in that opinion, usually syntactically modifies the opinion word.",3 Proposed method,[0],[0]
"Thus, there should be a pass of dependencies between a Cond-phrase and the opinion word, and a
phrase that leads to the opinion word via a smaller number of dependency arrows is more likely to be a Cond-phrase.",3 Proposed method,[0],[0]
"We use the dependency distance (i.e., the number of dependencies) between a phrase in question and the opinion word as the value for feature F1.",3 Proposed method,[0],[0]
The value for a phrase is −1 if there is no pass between that phrase and the opinion word.,3 Proposed method,[0],[0]
"We use “CaboCha” (Kudo and Matsumoto, 2002) for dependency analysis purposes.
F2: Phrase distance to opinion word F1 is not robust against errors of the dependency analysis.",3 Proposed method,[0],[0]
"To alleviate this problem, we approximate the dependency distance by a phrase distance.",3 Proposed method,[0],[0]
"In practice, we subtract the ID for a phrase in question from that for the opinion word as the value for feature F2.",3 Proposed method,[0],[0]
"If the opinion word consists of more than one phrase, we take the minimum difference.",3 Proposed method,[0],[0]
"Because in Japanese a modifier is usually followed by its modifying object, a phrase with a negative value for feature F2 is usually an Other-phrase.",3 Proposed method,[0],[0]
"For example, in the last phrase in Figure 1, which cannot be a modifier for the opinion word, is an Other-phrase.
",3 Proposed method,[0],[0]
"F3: Dependency pass to aspect Because a CFO rarely modifies an aspect, for the value of feature F3 we take 0 if there is a pass of dependencies between a phrase in question and an aspect and 1 otherwise.
",3 Proposed method,[0],[0]
F4: Phrase distance to aspect Similar to F1,3 Proposed method,[0],[0]
", F3 is not robust against errors of the dependency analysis.",3 Proposed method,[0],[0]
"As in F2, we approximate the value of F4 by a phrase distance between a phrase including an aspect and a phrase in question.
",3 Proposed method,[0],[0]
"F5: Difference between values for F2 and F1 A CFO usually consists of a sequence of Cond-phrases where each phrase modifies the next phrase, as in Figure 1.",3 Proposed method,[0],[0]
"There is a tendency that as the difference of values of F1 and F2 for a phrase becomes smaller, that phrase is more likely to be a Cond-phrase.",3 Proposed method,[0],[0]
"In Figure 1, the values for Condphrases #3–#6 are smaller than those for Otherphrases #0–#1.
",3 Proposed method,[0],[0]
F6:,3 Proposed method,[0],[0]
Beginning of sentence The subject of an opinion sentence is often its U-CFO because the evaluation is valid only from the perspective of that specific subject.,3 Proposed method,[0],[0]
"For example, in “my daughter was pleased with toys in the room” the positive evaluation is restricted by the daughter’s perspective.",3 Proposed method,[0],[0]
"Thus, the value of feature F6 takes 1 for the first phrase in a sentence excluding a conjunction, and 0 otherwise.
",3 Proposed method,[0],[0]
"F7: Clue expression Because a CFO often ends with one or more specific particles or auxiliary verbs, we use the existence of those clue expressions in a phrase as the value for feature F7.",3 Proposed method,[0],[0]
"We use words in a dictionary of Japanese functional expressions “Tsutsuji” (Matsuyoshi et al., 2006)
as the clue expressions.",3 Proposed method,[0],[0]
Table 1 shows examples of entries for Tsutsuji.,3 Proposed method,[0],[0]
Each entry is represented in a hierarchy structure with nine abstraction levels.,3 Proposed method,[0],[0]
"We firstly collected “Head words” in the nineteen categories (e.g., resultative condition and purpose in L2) associated with our purpose, consulting “Meaning categories”.",3 Proposed method,[0],[0]
Then we collected “Surface forms” corresponding to the collected head words and identified their corresponding surface forms to standardize different forms.,3 Proposed method,[0],[0]
"For example, for ID 1 and ID 3 in Table 1, “to sure ba” and “nde” are regarded as identical to “to suru to” and “node”, respectively.",3 Proposed method,[0],[0]
"As a result, we collected 388 words, such as “ba (if)” and “ni (for)” and used their existence in a phrase in question as value for F7.",3 Proposed method,[0],[0]
"Because the data sparseness is a crucial problem for F7, we use the existence of semantic categories in Tsutsuji as the values of F8 for smoothing purposes.",F8: Semantic categories for clue expression,[0],[0]
"For example, in Table 1, “to suru to” and “ba” have the same feature values “resultative condition”.",F8: Semantic categories for clue expression,[0],[0]
"If a clue expression belongs to more than one semantic category as in “ni” of Table 1, the feature value is a set of these categories.
",F8: Semantic categories for clue expression,[0],[0]
F9: Dependency pass to phrase including clue expression (Surface form),F8: Semantic categories for clue expression,[0],[0]
"As described in F7 above, the last phrase in a CFO often includes one or more clue expressions.",F8: Semantic categories for clue expression,[0],[0]
"In addition, a CFO often consists of more than one phrase.",F8: Semantic categories for clue expression,[0],[0]
"Given those conditions, a phrase that modifies a phrase containing a clue expression is also likely to be a Condphrase.",F8: Semantic categories for clue expression,[0],[0]
"We use the existence of a dependency pass between a phrase in question and a phrase containing a clue expression as the values of feature F9.
F10:",F8: Semantic categories for clue expression,[0],[0]
"Dependency pass to phrase including clue expression (Category) As with F8, we use the existence of semantic categories of Tsutsuji as the values of feature F10.
",F8: Semantic categories for clue expression,[0],[0]
F11: Restrictive words We use the existence of words that are strongly associated with U-CFO as the value for F11.,F8: Semantic categories for clue expression,[0],[0]
We call such words restrictive words.,F8: Semantic categories for clue expression,[0],[0]
"We automatically produced a dictionary of restrictive words from advertising slogans for hotels, which often include descriptions for target users, such as “Fjoshikai ya kappuru ni osusume!!F (Recommended to girls get-together and couples)”.",F8: Semantic categories for clue expression,[0],[0]
"First, we extracted words in the advertising slogan based on the following steps.
",F8: Semantic categories for clue expression,[0],[0]
"Step 1: Extracting sentences that match to a regular expression “( | hito | mono | kata) ni ( | wa | mo) osusume” (i.e., “recommended to” or “recommend to those who”).
",F8: Semantic categories for clue expression,[0],[0]
"Step 2: Collecting a sequence of content words for each bunsetsu-phrase in the extracted sentences.
",F8: Semantic categories for clue expression,[0],[0]
"For the above advertising slogan, we can collect two restrictive words “joshikai (girls gettogether)” and “kappuru (couple)” by performing those 2 steps.
",F8: Semantic categories for clue expression,[0],[0]
"Second, we collected a sequence of independent words for bunsetsu phrases which comprises UCFO in an annotated corpus.",F8: Semantic categories for clue expression,[0],[0]
"We combined the extracted words from the advertising slogans an annotated corpora, discarded redundancy, and standardized similar words, such as “kanko suru (do sightseeing) and “kanko (sightseeing)”.",F8: Semantic categories for clue expression,[0],[0]
"As a result, we collected 934 words.
",F8: Semantic categories for clue expression,[0],[0]
"Finally, we calculated a mutual information like score, Score(r, u), between a restrictive word r and labels u, Cond-phrases for U-CFOs (i.e., phrases labeled with either of BU or IU), by Equation 2.
",F8: Semantic categories for clue expression,[0],[0]
"Score(r, u) = P (r, u) log P (r, u)
P (r)P (u) (2)
P (r, u) denotes the probability that a phrase including r is labeled with BU or IU in the annotated corpus.",F8: Semantic categories for clue expression,[0],[0]
P (r) denotes the probability that a phrase including r appears in the annotated corpus while P (u) denotes the probability that a phrase labeled with BU or IU in the annotated corpus.,F8: Semantic categories for clue expression,[0],[0]
"If a phrase includes a restrictive word r and Score(r, u) is greater than threshold θ, the feature value is r, and “nothing” otherwise.
",F8: Semantic categories for clue expression,[0],[0]
"F12: Existence of restrictive word Because the data sparseness is a crucial problem for F11, we integrate all the restrictive words for F11 into a single category for smoothing purposes.",F8: Semantic categories for clue expression,[0],[0]
"The value for F12 is the existence (1/0) of restrictive words.
",F8: Semantic categories for clue expression,[0],[0]
F13: Part of speech for head The likelihood that a phrase in question is a Cond-phrases partially depends on the part of speech for the head in that phrase.,F8: Semantic categories for clue expression,[0],[0]
"For example, in Figure 1, a phrase whose head is a noun or verb tends to be a Condphrase",F8: Semantic categories for clue expression,[0],[0]
"To evaluate the effectiveness of our method, we used the Rakuten Travel data1, which consists of 348 564 Japanese reviews for hotels in Japan.",4 Experiments,[0],[0]
"From this dataset, we selected 580 reviews and manually identified elements for opinion units.",4 Experiments,[0],[0]
We removed sentences consisting only of opinion unit such as “The location is good” from the evaluation.,4 Experiments,[0],[0]
"As a result, 3 155 sentences remained, which comprise our corpus.",4 Experiments,[0],[0]
"To evaluate the effectiveness for identifying CFOs, we used the manually annotated opinion elements as output of a pseudo automatic method.
",4 Experiments,[0],[0]
"Given the above corpus, two annotators independently identified U-CFOs or CFOs, if any, for each opinion unit.",4 Experiments,[0],[0]
"For both annotations of CFOs and U-CFOs, the Kappa value for the interannotator agreement was 0.87, indicating strong agreement.",4 Experiments,[0],[0]
We show the details of our corpus in Table 2.,4 Experiments,[0],[0]
"Using this corpus, we performed 10-fold cross-validation and compared different methods from different perspectives.",4 Experiments,[0],[0]
"Also, we determined the threshold for Score (see Eq 2) by a development set for each fold.
",4 Experiments,[0],[0]
"To evaluate the effectiveness of extracting UCFOs and CFOs independently, we first classified bunsetsu phrases into any of BU, IU, BC, IC, or Other.",4 Experiments,[0],[0]
"Then, for the U-CFO extraction we regarded phrases for BU and IU as the Cond-phrases while for the CFO extraction we regarded phrases for BU, IU, BC, and IC as the Cond-phrases.
",4 Experiments,[0],[0]
"We used “Partial match” and “Exact match”, which denote different criteria for the correctness of methods under evaluation.",4 Experiments,[0],[0]
"While in the partial match each method was requested to only detect whether or not a test sentence includes CFO, in the exact match each method was also requested to identify the span of each CFO.",4 Experiments,[0],[0]
"Also, we used different evaluation measures, namely precision (P), recall (R), F-measure (F), and accuracy (A).
",4 Experiments,[0],[0]
Rule-based method and SVM-based method are used for comparison purposes.,4 Experiments,[0],[0]
"Rule-based
1https://alaginrc.nict.go.jp/resources/rakutendataset/rakuten-outline.html
method first identifies a bunsetsu phrase whose dependency distance to the opinion word is 1 and including a clue expression (see Section 3), and also identifies a sequence of the phrases from which there is a dependency path to the above phrase as a CFO.",4 Experiments,[0],[0]
"For example, in Figure 1 because phrase #6 includes a clue expression, a sequence of phrases #3–#6 is extracted as a CFO.",4 Experiments,[0],[0]
"These rules are based on features F1, F7 and F9.",4 Experiments,[0],[0]
"For the U-CFO extraction task, we regarded a sequence of Cond-phrases extracted by the above method as U-CFO if that sequence includes a restrictive word.",4 Experiments,[0],[0]
"For SVM, the thirteen features F1–F13 proposed in Section 3 was used.",4 Experiments,[0],[0]
"We used LIBSVM (Chang and Lin, 2011) to train a classifier.",4 Experiments,[0],[0]
Our method used CRF to train a classifier with the thirteen features and four patterns for feature functions.,4 Experiments,[0],[0]
"We used CRF++2 to train a classifier for each phrase and regularized the parameters using L2-norm.
",4 Experiments,[0],[0]
Figure 2 shows the relationship between values of regularization parameter and F-measure for exact match.,4 Experiments,[0],[0]
"In Figure 2, “Rule”, “SVM”, and “CRF” denote a rule-based method, SVM-based method, and our method, respectively.",4 Experiments,[0],[0]
"The Fmeasure for Rule, independent of the regularization parameter, is a constant.",4 Experiments,[0],[0]
"While the F-measure for SVM substantially varied depending on the parameter value, that for CRF did not vary that much.",4 Experiments,[0],[0]
"Additionally, the F-measure for CRF was larger than that for SVM irrespective of the parameter value and matching criterion.
",4 Experiments,[0],[0]
Table 3 shows results obtained with the optimal value for the regularization parameter.,4 Experiments,[0],[0]
"Looking at Table 3, one can see that CRF outperformed the other methods in terms of F-measure and accuracy for both partial and exact matches.",4 Experiments,[0],[0]
"We used the two-tailed paired t-test for statistical testing and found that the differences of CRF and each of the other methods in F-measure and accuracy were statistically significant at the 1% level irrespective of the configuration.
",4 Experiments,[0],[0]
Figure 3 shows the effectiveness of the proposed features for exact match.,4 Experiments,[0],[0]
The horizontal axis “w/o X” denotes a method without feature X.,4 Experiments,[0],[0]
The vertical axis denotes a ratio of each method to our method.,4 Experiments,[0],[0]
"If a method without feature X takes less than 1 for value of vertical axis, the feature X is effective for extracting CFOs.",4 Experiments,[0],[0]
"Looking at Figure 3, one can see that our complete method outperformed any variation of our method in terms of
2http://crfpp.googlecode.com/svn/trunk/doc/index.html
F-measure.",4 Experiments,[0],[0]
"Thus, we conclude that each of our thirteen features was independently effective for extracting CFO and U-CFO in review sentences and that when used together the improvement was even greater.
",4 Experiments,[0],[0]
"For the U-CFO extraction, we analyzed the errors by our method.",4 Experiments,[0],[0]
The total number of errors was 363 by condition unit.,4 Experiments,[0],[0]
"We describe causes of the errors with example sentences, translated into English by the authors.",4 Experiments,[0],[0]
"In those examples, double and single underlines denote false positive and false negative, respectively.",4 Experiments,[0],[0]
"For each cause, we show the number of errors in parentheses.
",4 Experiments,[0],[0]
E1 (124) Errors were due to F11 and F12 with insufficient dictionary for restrictive words.,4 Experiments,[0],[0]
"Typically, low frequency words (e.g., pilgrimage) and words related to miscellaneous activities during a travel (e.g., charging a battery of a mobile phone) were not included in our dictionary.",4 Experiments,[0],[0]
"While it is important to increase the vocabulary size of our dictionary, identifying synonymous expressions with partial matching (e.g., go to sleep / go to bed) is also important.
",4 Experiments,[0],[0]
"E2 (53) Errors were due to dependency analysis, which often mistakenly recognizes sentence boundaries in an informal writing style and dependency relations in a sentence comprising a phrase, such as “the best location for fully enjoying Asakusa”.",4 Experiments,[0],[0]
"In this example, CaboCha mistakenly associated the adnominal modifier “for fully enjoying Asakusa” with “location (aspect)” instead of “best (opinion word)”.",4 Experiments,[0],[0]
"As a result, F1 and F3 did not regard this modifier as a U-CFO.
E3 (40) Restrictive modifiers that modify a nonopinion segment were mistakenly extracted as U-CFOs.",4 Experiments,[0],[0]
"For example, in “I used this hotel for business and the meal was good”, “for business” includes the clue expression “for” but does not modifies the opinion unit.
",4 Experiments,[0],[0]
E4 (39) Similar to E3 but errors were due to restrictive words instead of clue expressions.,4 Experiments,[0],[0]
"In the example for E3, the restrict word “business” caused the error.
E5 (26)",4 Experiments,[0],[0]
"U-CFOs that consist of a large number of phrases were often not extracted due to F5, such as “This hotel is acceptable for one night to take the train at the Chuo station next morning”.
",4 Experiments,[0],[0]
"E6 (25) Errors were due to irrelevant entries in our restrictive word dictionary.
",4 Experiments,[0],[0]
"E7 (11) Due to the sparseness problem for restrictive words in the training data, U-CFOs and CFOs were not correctly distinguished.
E8 (9) Errors were due to part-of-speech tagging.
",4 Experiments,[0],[0]
"E9 (6) Errors were due to extracting modifiers consisting of a personal pronoun without additional user-related attributes, such as “enough for me” , as U-CFOs.",4 Experiments,[0],[0]
"We need to identify whether an expression for a person is associated with userrelated attributes, such as “the bed is small for a person who is tall”, which indicates a physical attribute of a user.
",4 Experiments,[0],[0]
"Additionally, there are 65 errors for which we have not found a reason.",4 Experiments,[0],[0]
"Although a number of methods have been proposed to search an opinionated corpus for opinion units, few attempts have so far been made at addressing cases where the validity of an evaluation is restricted on a condition in the source text.",5 Conclusion,[0],[0]
We proposed a method to identify such conditions from sentences including opinion units.,5 Conclusion,[0],[0]
Our method performs sequence labeling to determine whether each phrase is a constituent of an condition for opinion.,5 Conclusion,[0],[0]
"We proposed thirteen features associated with lexical and syntactic information of Japanese, and showed their effectiveness using reviews for hotels.",5 Conclusion,[0],[0]
"The contributions of this paper are introducing the notion of conditions for opinions, which is language-independent, proposing a method to extract condition-opinion relations from opinionated corpora, and giving an insight into its potential applications in opinion mining.",5 Conclusion,[0],[0]
We would like to thank Professor Takenobu Tokunaga (Tokyo Institute of Technology) for his valuable comments.,Acknowledgments,[0],[0]
This research was supported in part by Grant-in-Aid for Scientific Research (Grant No. 15H02747).,Acknowledgments,[0],[0]
"A fundamental issue in opinion mining is to search a corpus for opinion units, each of which typically comprises the evaluation by an author for a target object from an aspect, such as “This hotel is in a good location”.",abstractText,[0],[0]
"However, few attempts have been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as “for traveling with small kids”.",abstractText,[0],[0]
"In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation.",abstractText,[0],[0]
Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions.,abstractText,[0],[0]
"We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally.",abstractText,[0],[0]
Extracting Condition-Opinion Relations Toward Fine-grained Opinion Mining,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 142–151 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
142",text,[0],[0]
Automatic summarization aims to shorten a text document while maintaining the salient information of the original text.,1 Introduction,[0],[0]
"The practical need for such systems is growing with the rapid and continuous increase in textual information sources in multiple domains.
",1 Introduction,[0],[0]
Summarization tools can be broadly classified into two categories: extractive and abstractive.,1 Introduction,[0],[0]
Extractive summarization selects parts of the input document to create its summary while abstractive summarization generates summaries that may have words or phrases not present in the input document.,1 Introduction,[0],[0]
Abstractive summarization is clearly harder as methods have to address factual and grammatical errors that may be introduced and problems in utilizing external knowledge sources to obtain paraphrasing or generalization.,1 Introduction,[0],[0]
"Extractive summarizers obviate the need to
solve these problems by selecting the most salient textual units (usually sentences) from the input documents.",1 Introduction,[0],[0]
"As a result, they generate summaries that are grammatically and semantically more accurate than those from abstractive methods.",1 Introduction,[0],[0]
"While they may have problems like incorrect or unclear referring expressions or lack of coherence, they are computationally simpler and more efficient to generate.",1 Introduction,[0],[0]
"Indeed, state-of-the-art extractive summarizers are comparable or often better in performance to competitive abstractive summarizers (see (Nallapati et al., 2017) for a recent empirical comparison).
",1 Introduction,[0],[0]
Classical approaches to extractive summarization have relied on human-engineered features from the text that are used to score sentences in the input document and select the highestscoring sentences.,1 Introduction,[0],[0]
These include graph or constraint-optimization based approaches as well as classifier-based methods.,1 Introduction,[0],[0]
A review of these approaches can be found in Nenkova et al. (2011).,1 Introduction,[0],[0]
Some of these methods generate summaries from multiple documents.,1 Introduction,[0],[0]
"In this paper, we focus on single document summarization.
",1 Introduction,[0],[0]
Modern approaches that show the best performance are based on end-to-end deep learning models that do not require human-crafted features.,1 Introduction,[0],[0]
"Neural models have tremendously improved performance in several difficult problems in NLP such as machine translation (Chen et al., 2017) and question-answering (Hao et al., 2017).",1 Introduction,[0],[0]
"Deep models with thousands of parameters require large, labeled datasets and for summarization this hurdle of labeled data was surmounted by Cheng and Lapata (2016), through the creation of a labeled dataset of news stories from CNN and Daily Mail consisting of around 280,000 documents and human-generated summaries.
",1 Introduction,[0],[0]
"Recurrent neural networks with encoderdecoder architecture (Sutskever et al., 2014) have
been successful in a variety of NLP tasks where an encoder obtains representations of input sequences and a decoder generates target sequences.",1 Introduction,[0],[0]
"Attention mechanisms (Bahdanau et al., 2015) are used to model the effects of different loci in the input sequence during decoding.",1 Introduction,[0],[0]
"Pointer networks (Vinyals et al., 2015) use this mechanism to obtain target sequences wherein each decoding step is used to point to elements of the input sequence.",1 Introduction,[0],[0]
"This pointing ability has been effectively utilized by state-of-the-art extractive and abstractive summarizers (Cheng and Lapata, 2016; Nallapati et al., 2016; See et al., 2017).
",1 Introduction,[0],[0]
"In this work, we design SWAP-NET a new deep learning model for extractive summarization.",1 Introduction,[0],[0]
"Similar to previous models, we use an encoderdecoder architecture with attention mechanism to select important sentences.",1 Introduction,[0],[0]
Our key contribution is to design an architecture that utilizes key words in the selection process.,1 Introduction,[0],[0]
"Salient sentences of a document, that are useful in summaries, often contain key words and, to our knowledge, none of the previous models have explicitly modeled this interaction.",1 Introduction,[0],[0]
"We model this interaction through a two-level encoder and decoder, one for words and the other for sentences.",1 Introduction,[0],[0]
"An attention-based mechanism, similar to that of Pointer Networks, is used to learn important words and sentences from labeled data.",1 Introduction,[0],[0]
A switch mechanism is used to select between words and sentences during decoding and the final summary is generated using a combination of selected sentences and words.,1 Introduction,[0],[0]
We demonstrate the efficacy of our model on the CNN/Daily Mail corpus where it outperforms state-of-the-art extractive summarizers.,1 Introduction,[0],[0]
Our experiments also suggest that the semantic redundancy in SWAPNET generated summaries is comparable to that of human-generated summaries.,1 Introduction,[0],[0]
"Let D denote an input document, comprising of a sequence of N sentences: s1, . . .",2 Problem Formulation,[0],[0]
", sN .",2 Problem Formulation,[0],[0]
"Ignoring sentence boundaries, let w1, . . .",2 Problem Formulation,[0],[0]
", wn be the sequence of n words in document D. An extractive summary aims to obtain a subset of the input sentences that forms a salient summary.
",2 Problem Formulation,[0],[0]
We use the interaction between words and sentences in a document to predict important words and sentences.,2 Problem Formulation,[0],[0]
"Let the target sequence of indices of important words and sentences be V = v1, . . .",2 Problem Formulation,[0],[0]
", vm, where each index vj can point to ei-
ther a sentence or a word in an input document.",2 Problem Formulation,[0],[0]
"We design a supervised sequence-to-sequence recurrent neural network model, SWAP-NET, that uses these target sequences (of sentences and words) to learn salient sentences and key words.",2 Problem Formulation,[0],[0]
"Our objective is to find SWAP-NET model parameters M that maximize the probability p(V |M,D) =",2 Problem Formulation,[0],[0]
"∏ j p(vj |v1, . .",2 Problem Formulation,[0],[0]
.,2 Problem Formulation,[0],[0]
", vj−1,M,D) =∏
j p(vj |v<j ,M,D).",2 Problem Formulation,[0],[0]
We omit M in the following to simplify notation.,2 Problem Formulation,[0],[0]
"SWAP-NET predicts both key words and salient sentences, that are subsequently used for extractive summary generation.",2 Problem Formulation,[0],[0]
"We briefly describe Pointer Networks (Vinyals et al., 2015).",3 Background,[0],[0]
"Our approach, detailed in the following sections, uses a similar attention mechanism.
",3 Background,[0],[0]
"Given a sequence of n vectors X = x1, ....xn and a sequence of indices R = r1, ....rm, each between 1 and n, the Pointer Network is an encoder-decoder architecture trained to maximize p(R|X; θ) = ∏m j=1 pθ(rj |r1, ....rj−1,X; θ), where θ denotes the model parameters.",3 Background,[0],[0]
"Let the encoder and decoder hidden states be (e1, ...., en) and (d1, ...., dm) respectively.",3 Background,[0],[0]
"The attention vector at each output step j is computed as follows:
uji = v T tanh(Weei +Wddj), i ∈",3 Background,[0],[0]
"(1, . . .",3 Background,[0],[0]
",",3 Background,[0],[0]
"n)
",3 Background,[0],[0]
αji =,3 Background,[0],[0]
"softmax(u j i ), i ∈ (1, . . .",3 Background,[0],[0]
",",3 Background,[0],[0]
"n)
",3 Background,[0],[0]
The softmax normalizes vector uj to be an attention mask over inputs.,3 Background,[0],[0]
"In a pointer network, the same attention mechanism is used to select one of the n input vectors with the highest probability, at each decoding step, thus effectively pointing to an input:
p(rj |r1, ....rj−1,X) = softmax(uj)
Here, v,Wd, and We are learnable parameters of the model.",3 Background,[0],[0]
We use an encoder-decoder architecture with an attention mechanism similar to that of Pointer Networks.,4 SWAP-NET,[0],[0]
"To model the interaction between words and sentences in a document we use two encoders and decoders, one at the word level and the other at the sentence level.",4 SWAP-NET,[0],[0]
"The sentence-level decoder learns to point to important sentences while the
word-level decoder learns to point to important words.",4 SWAP-NET,[0],[0]
A switch mechanism is trained to select either a word or a sentence at each decoding step.,4 SWAP-NET,[0],[0]
The final summary is created using the output words and sentences.,4 SWAP-NET,[0],[0]
We now describe the details of the architecture.,4 SWAP-NET,[0],[0]
We use two encoders: a bi-directional LSTM at the word level and a LSTM at the sentence level.,4.1 Encoder,[0],[0]
"Each word wi is represented by a K-dimensional embedding (e.g., via word2vec), denoted by xi.",4.1 Encoder,[0],[0]
"The word embedding xi is encoded as ei using bi-directional LSTM for i = 1, . . .",4.1 Encoder,[0],[0]
",",4.1 Encoder,[0],[0]
"n. The vector output of BiLSTM at the end of a sentence is used to represent that entire sentence, which is further encoded by the sentence-level LSTM as Ek = LSTM(ekl , Ek−1), where k
l is the index of the last word in the kth sentence in D and Ek is the hidden state at the kth step of LSTM, for k = 1, . . .",4.1 Encoder,[0],[0]
", N .",4.1 Encoder,[0],[0]
See figure 1.,4.1 Encoder,[0],[0]
"We use two decoders – a sentence-level and a word-level decoder, that are both LSTMs, with each decoder pointing to sentences and words re-
spectively (similar to a pointer network).",4.2 Decoder,[0],[0]
"Thus, we can consider the output of each decoder step to be an index in the input sequence to the encoder.",4.2 Decoder,[0],[0]
Let m be the number of steps in each decoder.,4.2 Decoder,[0],[0]
"Let T1, . . .",4.2 Decoder,[0],[0]
", Tm be the sequence of indices generated by the sentence-level decoder, where each index Tj ∈ {1, . . .",4.2 Decoder,[0],[0]
", N}; and let t1, . . .",4.2 Decoder,[0],[0]
", tm be the sequence of indices generated by the word-level decoder, where each index tj ∈ {1, . . .",4.2 Decoder,[0],[0]
", n}.",4.2 Decoder,[0],[0]
"At the jth decoding step, we have to select a sentence or a word which is done through a binary switch Qj that has two states Qj = 0 and Qj = 1 to denote word and sentence selection respectively.",4.3 Network Details,[0],[0]
"So, we first determine the switch probability p(Qj |v<j , D).",4.3 Network Details,[0],[0]
"Let αskj denote the probability of selecting the kth input sentence at the jth decoding step of sentence decoder:
αskj = p(Tj = k|v<j , Qj = 1, D),
and let αwij denote the probability of selecting the ith input word at the jth decoding step of word decoder:
αwij = p(tj = i|v<j , Qj = 0, D),
both conditional on the corresponding switch selection.",4.3 Network Details,[0],[0]
"We set vj based on the probability values:
vj =
{ k = argmaxk p s kj if maxk p s kj > maxi p w ij
i = argmaxi p w ij if maxi p w ij",4.3 Network Details,[0],[0]
>,4.3 Network Details,[0],[0]
"maxk p s kj
pskj = α s kjp(Qj = 1|v<j , D),
pwij = α w ijp(Qj = 0|v<j , D).
",4.3 Network Details,[0],[0]
"These probabilities are obtained through the attention weight vectors at the word and sentence levels and the switch probabilities:
αwij = softmax(v T t φ(whhj + wtei)),
αskj = softmax(V T T φ(WHHj +WTEk)).
",4.3 Network Details,[0],[0]
"Parameters vt, wh, wt, VT ,WH and WT are trainable parameters.",4.3 Network Details,[0],[0]
"Parameters hj and Hj are the hidden vectors at the jth step of the wordlevel and sentence-level decoder respectively defined as:
hj = LSTM(hj−1, aj−1, φ(Aj−1))",4.3 Network Details,[0],[0]
"(1)
Hj = LSTM(Hj−1, Aj−1, φ(aj−1))",4.3 Network Details,[0],[0]
"(2)
where aj = ∑n i=0 α w ijei, Aj = ∑N k=0 α s kjEk.",4.3 Network Details,[0],[0]
"The non-linear transformation, φ (we choose tanh), is used to connect the word-level encodings to the sentence decoder and the sentence-level encodings to the word decoder.",4.3 Network Details,[0],[0]
"Specifically, the word-level decoder updates its state by considering a sum of sentence encodings, weighted by the attentions from the previous state and mutatis mutandis for the sentence-level decoder.
",4.3 Network Details,[0],[0]
"The switch probability p(Qj |v<j , D) at the jth decoding step is given by:
p(Qj = 1|v<j , D) = σ(wTQ(Hj−1, Aj−1, φ(hj−1, aj−1)))
p(Qj = 0|v<j , D) = 1− p(Qj = 1|v<j , D)
where wQ is a trainable parameter and σ denotes the sigmoid function and φ is the chosen nonlinear transformation (tanh).
",4.3 Network Details,[0],[0]
During training the loss function lj at jth step is set to lj = − log(pskjqsj + pwijqwj ),4.3 Network Details,[0],[0]
"− log p(Qj |v<j , D).",4.3 Network Details,[0],[0]
"Note that at each decoding step, switch is either qwj = 1, q s j = 0",4.3 Network Details,[0],[0]
"if the j th output is a word or qwj = 0, q s j = 1 if the j
th output is a sentence.",4.3 Network Details,[0],[0]
The switch probability is also considered in the loss function.,4.3 Network Details,[0],[0]
"Given a document whose summary is to be generated, its sentences and words are given as input to the trained encoder.",4.4 Summary Generation,[0],[0]
"At the jth decoding step, either a sentence or a word is chosen based on the probability values αskj and α w ij and the switch probability p(Qj |v<j , D).",4.4 Summary Generation,[0],[0]
We assign importance scores to the selected sentences based on their probability values during decoding as well as the probabilities of the selected words that are present in the selected sentences.,4.4 Summary Generation,[0],[0]
Thus sentences with words selected by the decoder are given higher importance.,4.4 Summary Generation,[0],[0]
Let the kth input sentence sk be selected at the jth decoding step and ith input word wi be selected at the lth decoding step.,4.4 Summary Generation,[0],[0]
"Then the importance of sk is defined as
I(sk) = α s kj",4.4 Summary Generation,[0],[0]
"+ λ ∑ wi∈sk αwil (3)
",4.4 Summary Generation,[0],[0]
In our experiments we choose λ = 1.,4.4 Summary Generation,[0],[0]
The final summary consists of three sentences with the highest importance scores.,4.4 Summary Generation,[0],[0]
"Traditional approaches to extractive summarization rely on human-engineered features based on, for example, part of speech (Erkan and Radev, 2004) and term frequency (Nenkova et al., 2006).",5 Related Work,[0],[0]
"Sentences in the input document are scored using these features, ranked and then selected for the final summary.",5 Related Work,[0],[0]
"Methods used for extractive summarization include graph-based approaches (Mihalcea, 2005) and Integer Linear Programming (Gillick and Favre, 2009).",5 Related Work,[0],[0]
"There are many classifier-based approaches that select sentences for the extractive summary using methods such as Conditional Random Fields (Shen et al., 2007) and Hidden Markov models (Conroy and O’leary, 2001).",5 Related Work,[0],[0]
"A review of these classical approaches can be found in Nenkova et al. (2011).
",5 Related Work,[0],[0]
"End-to-end deep learning based neural models that can effectively learn from text data, without human-crafted features, have witnessed rapid development, resulting in improved performance in multiple areas such as machine translation (Chen et al., 2017) and question-answering (Hao et al., 2017), to name a few.",5 Related Work,[0],[0]
"Large labelled corpora based on news stories from CNN and Daily Mail, with human generated summaries have become available (Cheng and Lapata, 2016), that have
spurred the use of deep learning models in summarization.",5 Related Work,[0],[0]
"Recurrent neural network based architectures have been designed for both extractive (Cheng and Lapata, 2016; Nallapati et al., 2017) and abstractive (See et al., 2017; Tan et al., 2017) summarization problems.",5 Related Work,[0],[0]
"Among these, the work of Cheng and Lapata (2016) and Nallapati et al. (2017) are closest to our work on extractive singledocument summarization.
",5 Related Work,[0],[0]
An encoder-decoder architecture with an attention mechanism similar to that of a pointer network is used by Cheng and Lapata (2016).,5 Related Work,[0],[0]
Their hierarchical encoder uses a CNN at the word level leading to sentence representations that are used in an RNN to obtain document representations.,5 Related Work,[0],[0]
"They use a hierarchical attention model where the first level decoder predicts salient sentences used for an extractive summary and based on this output, the second step predicts keywords which are used for abstractive summarization.",5 Related Work,[0],[0]
Thus they do not use key words for extractive summarization and for abstractive summarization they generate key words based on sentences predicted independently of key words.,5 Related Work,[0],[0]
"SWAP-NET, in contrast, is simpler using only two-level RNNs for word and sentence level representations in both the encoder and decoder.",5 Related Work,[0],[0]
In our model we predict both words and sentences in such a way that their attentions interact with each other and generate extractive summaries considering both the attentions.,5 Related Work,[0],[0]
"By modeling the interaction between these key words and important sentences in our decoder architecture, we are able to extract sentences that are closer to the gold summaries.
",5 Related Work,[0],[0]
"SummaRuNNer, the method developed by Nallapati et al. (2017) is not similar to our method in its architecture but only in the aim of extractive summary generation.",5 Related Work,[0],[0]
It does not use an encoderdecoder architecture; instead it is an RNN based binary classifier that decides whether or not to include a sentence in the summary.,5 Related Work,[0],[0]
"The RNN is multi-layered representing inputs, words, sentences and the final sentence labels.",5 Related Work,[0],[0]
"The decision of selecting a sentence at each step of the RNN is based on the content of the sentence, salience in the document, novelty with respect to previously selected sentences and other positional features.",5 Related Work,[0],[0]
"Their approach is considerably simpler than that of Cheng and Lapata (2016) but obtains summaries closer to the gold summaries, and additionally, facilitates interpretable visualization and
training from abstractive summaries.",5 Related Work,[0],[0]
"Their experiments show improved performance over both abstractive and extractive summarizers from several previous models (Nallapati et al., 2017).
",5 Related Work,[0],[0]
We note that several elements of our architecture have been introduced and used in earlier work.,5 Related Work,[0],[0]
"Pointer networks (Vinyals et al., 2015) used the attention mechanism of (Bahdanau et al., 2015) to solve combinatorial optimization problems.",5 Related Work,[0],[0]
"They have also been used to point to sentences in extractive (Cheng and Lapata, 2016) and abstractive (Nallapati et al., 2016; See et al., 2017) summarizers.",5 Related Work,[0],[0]
"The switch mechanism was introduced to incorporate rare or out-of-vocabulary words (Gulcehre et al., 2016) and are used in several summarizers (e.g. (Nallapati et al., 2016)).",5 Related Work,[0],[0]
"However, we use it to select between word and sentence level decoders in our model.
",5 Related Work,[0],[0]
"The importance of all the three interactions: (i) sentence-sentence, (ii) word-word and (iii) sentence-word, for summarization, have been studied by Wan et al. (2007) using graph-based approaches.",5 Related Work,[0],[0]
"In particular, they show that methods that account for saliency using both the following considerations perform better than methods that consider either one of them alone, and SWAP-NET is based on the same principles.
",5 Related Work,[0],[0]
"• A sentence should be salient if it is heavily linked with other salient sentences, and a word should be salient if it is heavily linked with other salient words.
",5 Related Work,[0],[0]
"• A sentence should be salient if it contains many salient words, and a word should be salient if it appears in many salient sentences.",5 Related Work,[0],[0]
"In our experiments the maximum number of words per document is limited to 800, and the maximum number of sentences per document to 50 (padding is used to maintain the length of word sequences).",6.1 Experimental Settings,[0],[0]
We also use the symbols <GO> and <EOS> to indicate start and end of prediction by decoders.,6.1 Experimental Settings,[0],[0]
"The total vocabulary size is 150,000 words.
",6.1 Experimental Settings,[0],[0]
"We use word embeddings of dimension 100 pretrained using word2vec (Mikolov et al., 2013) on the training dataset.",6.1 Experimental Settings,[0],[0]
We fix the LSTM hidden state size at 200.,6.1 Experimental Settings,[0],[0]
"We use a batch size of 16 and the ADAM optimizer (Kingma and Ba, 2015) with parameters: learning rate = 0.001, β1 = 0.9, β2 =
0.999 to train SWAP-NET.",6.1 Experimental Settings,[0],[0]
"We employ gradient clipping to regularize our model and an early stopping criterion based on the validation loss.
",6.1 Experimental Settings,[0],[0]
During training we find that SWAP-NET learns to predict important sentences faster than to predict words.,6.1 Experimental Settings,[0],[0]
"To speed up learning of word probabilities, we add the term− logαwij to our loss function lj in the final iterations of training.",6.1 Experimental Settings,[0],[0]
It is possible to get the same sentence or word in multiple (usually consecutive) decoding steps.,6.1 Experimental Settings,[0],[0]
"In that case, in Eq. 3 we consider the maximum value of alpha obtained across these steps and calculate maximum scores of distinct sentences and words.
",6.1 Experimental Settings,[0],[0]
"We select 3 top scoring sentences for the summary, as there are 3.11 sentences on average in the gold summary of the training set (similar to settings used by others, e.g., (Narayan et al., 2017)).",6.1 Experimental Settings,[0],[0]
"Two state-of-the-art methods for extractive summarization are SummaRuNNer (Nallapati et al., 2017) and NN, the neural summarizer of Cheng and Lapata (2016).",6.2 Baselines,[0],[0]
"SummaRuNNer can also provide extractive summaries while being trained abstractively (Nallapati et al., 2017); we denote this method by SummaRuNNer-abs.",6.2 Baselines,[0],[0]
"In addition, we compare our method with the Lead-3 summary which consists of the first three sentences from each document.",6.2 Baselines,[0],[0]
"We also compare our method with an abstractive summarizer that uses a similar attention-based encoder-decoder architecture (Nallapati et al., 2016), denoted by ABS.",6.2 Baselines,[0],[0]
"For our experiments, we use the CNN/DailyMail corpus (Hermann et al., 2015).",6.3 Benchmark Datasets,[0],[0]
"We use the anonymized version of this dataset, from Cheng and Lapata (2016), which has labels for important sentences, that are used for training.",6.3 Benchmark Datasets,[0],[0]
"To obtain labels for words, we extract keywords from each gold summary using RAKE, an unsupervised keyword extraction method (Rose et al., 2010).",6.3 Benchmark Datasets,[0],[0]
These keywords are used to label words in the corresponding input document during training.,6.3 Benchmark Datasets,[0],[0]
"We replace numerical values in the documents by zeros to limit the vocabulary size.
",6.3 Benchmark Datasets,[0],[0]
"We have 193,986 training documents, 12,147 validation documents and 10,346 test documents from the DailyMail corpus and 83,568 training documents, 1,220 validation documents and 1,093 test documents from CNN subset with labels for sentences and words.",6.3 Benchmark Datasets,[0],[0]
"We use the ROUGE toolkit (Lin and Hovy, 2003) for evaluation of the generated summaries in comparison to the gold summaries.",6.4 Evaluation Metrics,[0],[0]
"We use three variants of this metric: ROUGE-1 (R1), ROUGE-2 (R2) and ROUGE-L (RL) that are computed by matching unigrams, bigrams and longest common subsequences respectively between the two summaries.",6.4 Evaluation Metrics,[0],[0]
"To compare with (Cheng and Lapata, 2016) and (Nallapati et al., 2017) we use limited length ROUGE recall at 75 and 275 bytes for the Daily-Mail test set, and full length ROUGE-F1 score, as reported by them.",6.4 Evaluation Metrics,[0],[0]
"Performance on Daily Mail Data
Table 1 shows the performance of SWAP-NET, state-of-the-art baselines NN and SummaRuNNer and other baselines, using ROUGE recall with summary length of 75 bytes, on the entire Daily Mail test set.",6.5 Results on Benchmark Datasets,[0],[0]
The performance of SWAP-NET is comparable to that of SummaRuNNer and better than NN and other baselines.,6.5 Results on Benchmark Datasets,[0],[0]
Table 2 compares the same algorithms using ROUGE recall with summary length of 275 bytes.,6.5 Results on Benchmark Datasets,[0],[0]
"SWAP-NET outperforms both state-of-the-art summarizers SummaRuNNer as well as NN.
Performance on CNN/DailyMail Data SWAP-NET has the best performance on the combined CNN and Daily Mail corpus, outperforming
the previous best reported F-score by SummaRuNNer, as seen in table 3, with a consistent improvement of over 2 ROUGE points in all three metrics.",6.5 Results on Benchmark Datasets,[0],[0]
"SWAP-NET outperforms state-of-the-art extractive summarizers SummaRuNNer (Nallapati et al., 2017) and NN (Cheng and Lapata, 2016) on benchmark datasets.",6.6 Discussion,[0],[0]
"Our model is similar, although simpler, than that of NN and the main difference between SWAP-NET and these baselines is its explicit modeling of the interaction between key words and salient sentences.
",6.6 Discussion,[0],[0]
"Automatic keyword extraction has been studied extensively (Hasan and Ng, 2014).",6.6 Discussion,[0],[0]
"We use a popular and well tested method, RAKE (Rose et al., 2010) to obtain key words in the training documents.",6.6 Discussion,[0],[0]
"A disadvantage with such methods is that they do not guarantee representation, via extracted keywords, of all the topics in the text (Hasan and Ng, 2014).",6.6 Discussion,[0],[0]
"So, if RAKE key words are directly applied to the input test document (without using word decoder trained on RAKE words, obtained from gold summary as done in SWAP-NET), then there is a possibility of missing sentences from the missed topics.",6.6 Discussion,[0],[0]
"So, we train SWAP-NET to predict key words and also model their interactions with sentences.
",6.6 Discussion,[0],[0]
We investigate the importance of modeling this interaction and the role of key words in the final summary.,6.6 Discussion,[0],[0]
Table 4 shows statistics that reflect the importance of key words in extractive summaries.,6.6 Discussion,[0],[0]
"Key word coverage measures the proportion of key
words from those in the gold summary present in the generated summary.",6.6 Discussion,[0],[0]
SWAP-NET obtains nearly 74% of the key words.,6.6 Discussion,[0],[0]
"In comparison Lead3 has only about 62% of the key words from the gold summary.
",6.6 Discussion,[0],[0]
Sentences with key words measures the proportion of sentences containing at least one key word.,6.6 Discussion,[0],[0]
"It is not surprising that in SWAP-NET summaries 98% of the sentences, on average, contain at least one key word: this is by design of SWAP-NET.",6.6 Discussion,[0],[0]
"However, note that Lead-3 which has poorer performance in all the benchmark datasets has much fewer sentences with key words.",6.6 Discussion,[0],[0]
"This highlights the importance of key words in finding salient sentences for extractive summaries.
",6.6 Discussion,[0],[0]
We also find the SWAP-NET obtains summaries that have less semantic redundancy.,6.6 Discussion,[0],[0]
"Table 6 shows the average distance between pairs of sentences from the gold summary, and summaries generated from SWAP-NET and Lead-3.",6.6 Discussion,[0],[0]
"Distances are measured using cosine distance of paragraph vectors of each sentence (Le and Mikolov, 2014) from randomly selected 500 documents of the Daily Mail test set.",6.6 Discussion,[0],[0]
"Paragraph vectors have been found to be effective semantic representations of sentences (Le and Mikolov, 2014) and experiments in (Dai et al., 2015) also show that paragraph vectors can be effectively used to measure semantic similarity using cosine distance.",6.6 Discussion,[0],[0]
"For training we use GENSIM (Řehůřek and Sojka, 2010) with embedding size 200 and initial learning rate 0.025.",6.6 Discussion,[0],[0]
"The model is trained on 500 documents from DailyMail dataset for 10 epochs and learning rate is decreased by 0.002 at each epoch.
",6.6 Discussion,[0],[0]
"The average pair-wise distance of SWAP-NET is very close to that of the gold summary, both
nearly 0.8.",6.6 Discussion,[0],[0]
"In contrast, the average pairwise distance in Lead-3 summaries is 0.553 indicating higher redundancy.",6.6 Discussion,[0],[0]
"This highly desirable feature of SWAP-NET is likely due to use of of key words, that is affecting the choice of sentences in the final summary.
",6.6 Discussion,[0],[0]
"Table 5 shows a sample gold summary from the Daily Mail dataset and the generated summary from SWAP-NET and, for comparison, from Lead-3.",6.6 Discussion,[0],[0]
We observe the presence of key words in all the overlapping segments of text with the gold summary indicating the importance of key words in finding salient sentences.,6.6 Discussion,[0],[0]
"Modeling this interaction, we believe, is the reason for the superior performance of SWAP-NET in our experiments.
",6.6 Discussion,[0],[0]
An implementation of SWAP-NET and all the generated summaries from the test sets are available online in a github repository1.,6.6 Discussion,[0],[0]
"We present SWAP-NET, a neural sequence-tosequence model for extractive summarization that outperforms state-of-the-art extractive summarizers SummaRuNNer (Nallapati et al., 2017) and NN (Cheng and Lapata, 2016) on large scale benchmark datasets.",7 Conclusion,[0],[0]
"The architecture of SWAPNET is simpler than that of NN but due to its effective modeling of interaction between salient sentences and key words in a document, SWAPNET achieves superior performance.",7 Conclusion,[0],[0]
SWAP-NET models this interaction using a new two-level pointer network based architecture with a switching mechanism.,7 Conclusion,[0],[0]
Our experiments also suggest that modeling sentence-keyword interaction has the desirable property of less semantic redundancy in summaries generated by SWAP-NET.,7 Conclusion,[0],[0]
The authors thank the ACL reviewers for their valuable comments.,8 Acknowledgment,[0],[0]
Vaibhav Rajan acknowledges the support from Singapore Ministry of Education Academic Research Fund Tier 1 towards funding this research.,8 Acknowledgment,[0],[0]
We present a new neural sequence-tosequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks).,abstractText,[0],[0]
"Extractive summaries comprising a salient subset of input sentences, often also contain important key words.",abstractText,[0],[0]
"Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new twolevel pointer network based architecture.",abstractText,[0],[0]
"SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary.",abstractText,[0],[0]
Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.,abstractText,[0],[0]
Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks,title,[0],[0]
"Learning a ranking function based on pairwise comparisons has been studied extensively in recent years, with many successful applications in building search engines and other information retrieval tasks.",1. Introduction,[0],[0]
"Given a set of training instances with features x1, ...,xn and pairwise comparisons, the goal is to find the optimal decision function f(·) such that f(xi) > f(xj) if i is preferred over j. This is usually referred to as a learning-to-rank problem, and several algorithms have been proposed, including RankSVM (Herbrich et al., 1999),",1. Introduction,[0],[0]
"gradient boosting decision tree (Li et al., 2007), and many others (Cao et al., 2007; Yue et al., 2007;
1Department of Computer Science, University of California Davis, USA. 2Department of Statistics, University of California Davis, USA .",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Minhao Cheng <mhcheng@ucdavis.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Negahban et al., 2012; Wauthier et al., 2013).
",1. Introduction,[0],[0]
"However, in many modern applications, a single global ranking is not sufficient to represent the variety of individual users preferences.",1. Introduction,[0],[0]
"For example, in movie ranking systems, it is preferable to learn an individual ranking function for each user since users’ tastes can vary largely.",1. Introduction,[0],[0]
"The issue also arises in many other applications such as product recommendation, and personalized web search ranking.
",1. Introduction,[0],[0]
"Motivated by these real scenarios, we consider the problem of learning hundreds of thousands of ranking functions jointly, one for each user.",1. Introduction,[0],[0]
"Our target problem is different from collaborative ranking and BPR (Rendle et al., 2009; Wu et al., 2017a) since they only aim to recover ranking over existing items without using item features, while our goal is to obtain the ranking functions (taking item features as input) that can generalize to unseen items.",1. Introduction,[0],[0]
"This is also different from existing work on learning multiple ranking functions (i.e. (Qian et al., 2014))",1. Introduction,[0],[0]
because in that setting they are learning only several ranking functions.,1. Introduction,[0],[0]
"Here we focus on problems where the number of ranking functions T is very large (e.g.,100K) but the amount of data to learn each ranking function is limited.",1. Introduction,[0],[0]
"The naive extensions of learning to rank algorithms fail since the training time grows dramatically, and also due to the over-fitting problem because of insufficient number of pairs for training.
",1. Introduction,[0],[0]
"To resolve this dilemma, we propose the Factorization RankSVM model for learning multiple ranking functions jointly.",1. Introduction,[0],[0]
The main idea is to assume the T ranking functions can be represented by a dictionary of k ranking functions with k T .,1. Introduction,[0],[0]
"In the linear RankSVM case, this assumption implies a low-rank structure when we stack all the T linear hyper-planes together into a matrix.",1. Introduction,[0],[0]
"By exploiting this low rank structure, our algorithm can be efficient for both time and sample complexity.
",1. Introduction,[0],[0]
"Our contributions can be summarized as follows:
• We propose the Factorization RankSVM model for learning a large number of different ranking functions on different sets of data simultaneously.",1. Introduction,[0],[0]
"By exploiting the low-rank structure, we show that the gradient can be calculated very efficiently, and the resulting algorithm can scale to problems with large number of tasks.",1. Introduction,[0],[0]
"• We derive the generalization error bound of our model,
showing that by training all the T tasks jointly, the sample complexity is much better than training individual rankSVMs under the low rank assumption.",1. Introduction,[0],[0]
"• We conduct experiments on real world datasets, showing that the algorithm achieves higher accuracy and faster training time compared with state-of-the-art methods.",1. Introduction,[0],[0]
This is a critical result as it shows the low rank ranking conjecture that underlies our research does occur.,1. Introduction,[0],[0]
"• We further visualize the basic ranking functions learned by our algorithm, which has some interesting and meaningful patterns.",1. Introduction,[0],[0]
Learning to rank.,2. Related Work,[0],[0]
"Given a set of pairwise comparisons between instances and the feature vectors associated with each instance, the goal of learning to rank is to discover the ranking function.",2. Related Work,[0],[0]
"There are three main categories of learning to rank algorithms: pointwise (Li et al., 2007), listwise (Cao et al., 2007; Yue et al., 2007), and pairwise methods (Herbrich et al., 2000; Cao et al., 2006).",2. Related Work,[0],[0]
"In this paper, we mainly focus on pairwise methods, which process a pair of documents or entries at a time.",2. Related Work,[0],[0]
"Among all the pairwise methods, rankSVM is a very popular one, so we choose it as our basic model.
",2. Related Work,[0],[0]
We focus on the problem of solving T learning-to-rank problems jointly when the problems share the same feature space and there is some hidden correlation between tasks.,2. Related Work,[0],[0]
"Obviously, we could apply existing learning-to-rank algorithms to solve each task independently, but this approach has several major drawbacks, as will be discussed in next section.
",2. Related Work,[0],[0]
"Collaborative filtering and matrix factorization Lowrank approximation has been widely used in matrix completion and collaborative filtering (Koren et al., 2009), and there are several extensions for matrix completion (Weimer et al., 2007).",2. Related Work,[0],[0]
"However, these methods cannot be applied in our setting, since our predictions are based on item features, and the corresponding items may not even appear in the training data.",2. Related Work,[0],[0]
"To conduct prediction based on item features, the inductive matrix factorization model has been recently proposed in (Jain & Dhillon, 2013; Xu et al., 2013), and factorization machine (Rendle, 2010) also uses a similar model.",2. Related Work,[0],[0]
"However, this model only allows input to be user-item ratings, not the pairwise comparisons used in our problem.",2. Related Work,[0],[0]
"In the experiments, we observe that even if the rating data is available, our model still outperforms inductive matrix completion significantly.
",2. Related Work,[0],[0]
"Bayesian Personalized Ranking (Rendle et al., 2009) proposes Bayesian Personalized Ranking(BPR) method to solve personalized ranking task.",2. Related Work,[0],[0]
"However, there are several major differences with our work.",2. Related Work,[0],[0]
"First, our target problem
is different from BPR.",2. Related Work,[0],[0]
"We consider problems given both pairwise comparisons and “explicit” item features, and the goal is to learn the personalized ranking “functions” that can generalize to unseen items as long as we know their features.",2. Related Work,[0],[0]
"In comparison, the BPR does not take item features into account, and the goal of BPR is to recover the ranking among existing items.",2. Related Work,[0],[0]
"Also, the ranking cannot generalize to unseen items.",2. Related Work,[0],[0]
"Moreover, BPR considers implicit (0/1) feedback instead of explicit feedback.
",2. Related Work,[0],[0]
Collaborative Ranking is another line of research that incorporates ranking loss in collaborative filtering.,2. Related Work,[0],[0]
"(Park et al., 2015; Weimer et al., 2007; Wu et al., 2017a) combines the ranking loss with matrix completion model, and (Yun et al., 2014) also uses a low-rank model with ranking loss given a binary observed matrix.",2. Related Work,[0],[0]
"However, similar to matrix completion and BPR, these collaborative ranking approaches do not use the item features.",2. Related Work,[0],[0]
So they are not able to predict the preferences for unseen items.,2. Related Work,[0],[0]
"Also in this category, (Barjasteh et al., 2015) uses a trace norm to constraint ranking function, which is similar with our idea.",2. Related Work,[0],[0]
"However, they use implicit feedback which will lose certain information.
",2. Related Work,[0],[0]
"Multi-task Learning: has been extensively studied, especially in computer vision application.",2. Related Work,[0],[0]
"To model the shared information across tasks, a low-rank structure is widely assumed (Chen et al., 2012; 2009).",2. Related Work,[0],[0]
"(Hwang et al., 2011; Su et al., 2015) takes the attributes correlation as low-rank embeddings to learn SVM.",2. Related Work,[0],[0]
"However, our approach of learning basic ranking functions has not been discussed in the literature.
",2. Related Work,[0],[0]
A summary of the differences between our algorithm with others are showed in Table 1.,2. Related Work,[0],[0]
"Our goal is to learn multiple ranking functions together, one for each user.",3. Problem Setting,[0],[0]
"Assume there are in total T ranking functions to be learned (each one can be viewed as a task), and we are given pairwise comparisons for these ranking functions among n items with features x1,x2, . . .",3. Problem Setting,[0],[0]
",xn ∈ Rd.",3. Problem Setting,[0],[0]
"For each task i, the pairwise comparisons are denoted as Ωi = {(j, k)}, where (j, k) ∈",3. Problem Setting,[0],[0]
"Ωi means task i compares item j with k, and yijk ∈ {+1,−1} is the observed outcome.",3. Problem Setting,[0],[0]
"For convenience, we use Ω to denote the union of all Ωi.",3. Problem Setting,[0],[0]
"Given these pairwise comparisons, we aim to learn a set of linear ranking functions w1,w2, . . .",3. Problem Setting,[0],[0]
",wT ∈ Rd such that
sign(wTi (xj−xk))",3. Problem Setting,[0],[0]
"≈ yijk, ∀(j, k) ∈ Ωi, ∀i = 1, . . .",3. Problem Setting,[0],[0]
", T
The only assumption we make for these T ranking tasks is that the items involved in each task share the same feature space with d features.",3. Problem Setting,[0],[0]
"Note that our algorithm allows each task has non-overlapping items—in that case we can still gather all the items together, and define Ωi to be the comparisons within each task’s own item block.
",3. Problem Setting,[0],[0]
"This model can be easily deployed into recommendation systems where each user i has a corresponding ranking function and the items could be movies, music, goods etc.",3. Problem Setting,[0],[0]
Then the objective of the task is to learn a ranking function for each user i.,3. Problem Setting,[0],[0]
Note,3. Problem Setting,[0],[0]
that after obtaining wi for each,3. Problem Setting,[0],[0]
"i, we can predict the preference for any pairs of items xj ,xk even when they are “unseen items” that are not in the training set.",3. Problem Setting,[0],[0]
And most collaborative filtering approaches such as matrix completion cannot solve this problem.,3. Problem Setting,[0],[0]
"We are able to predict preferences on unseen items because we try to learn ranking functions based on features instead of just completing the rating matrix over “seen” items.
",3. Problem Setting,[0],[0]
"Naive approaches: For a single ranking function, (Herbrich et al., 1999) proposes the following RankSVM algorithm:
min w∈Rd
1 2",3. Problem Setting,[0],[0]
"‖w‖2 + C ∑ (i,j,k)∈Ω ξ2ijk
s.t. yijkwT",3. Problem Setting,[0],[0]
"(xj − xk) ≥ 1− ξijk, ξijk ≥ 0.",3. Problem Setting,[0],[0]
"∀i, j, k.
",3. Problem Setting,[0],[0]
"Here we use L2 hinge loss in our model, however it could be extended to L1 loss as well.",3. Problem Setting,[0],[0]
We can take RankSVM into multiple-user case by simply assuming that all ranking functions share a common w. We denote this method as RANKSVM JOINTLY.,3. Problem Setting,[0],[0]
"(Evgeniou & Pontil, 2004) provides a variation by assuming each ranking function to be wi = w + vi, where w is the centralized model and vi is the task-dependent variance.",3. Problem Setting,[0],[0]
"However, this algorithm follows the strong assumption that T ranking functions {wi}d1i=1 are all close to a single base function w. We call this algorithm RANKSVM VAR.",3. Problem Setting,[0],[0]
"This assumption is not always true in practice so that it will cause the model to under-fit training data (see our experimental results).
",3. Problem Setting,[0],[0]
"On the other hand, we can treat every user separately, which means we train every ranking function wi independently by solving the following problem for every i = 1, . . .",3. Problem Setting,[0],[0]
", T :
min wi
1 2 ‖wi‖2 + C ∑ (j,k)∈Ωi ξ2ijk
s.t. yijkwTi (xj − xk) ≥ 1− ξijk, ξijk ≥ 0,∀(j, k) ∈",3. Problem Setting,[0],[0]
"Ωi
We call this method as RANKSVM SEPARATELY.",3. Problem Setting,[0],[0]
It is obvious that this model has more freedom to fit the training data.,3. Problem Setting,[0],[0]
"However, due to the limited number of observed pairs Ωi per user, each wi has poor prediction quality due to over-fitting.",3. Problem Setting,[0],[0]
"We will analyze the sample complexity of RANKSVM SEPARATELY in Section 4, and experimental results in Section 5 also support our analysis.",3. Problem Setting,[0],[0]
"Our low rank personalized ranking conjecture assumes that all the T ranking functions can be well-approximated by a linear combination of k basic ranking functions, where k T .",4. Proposed Algorithm,[0],[0]
"This makes sense in many real applications; for example, in personalized recommender systems, there are group of users who have similar preferences.",4. Proposed Algorithm,[0],[0]
"Let {uj}kj=1 to be the basic (linear) ranking functions, we can linearly combine weight then using vi to obtain a ranking function for user i as follows: wi = ∑k j=1 vijuj for all i.",4. Proposed Algorithm,[0],[0]
"This can be written as W = UV T , where columns of W,U are wi and uj respectively, and V is the coefficients.",4. Proposed Algorithm,[0],[0]
"Therefore, W will be a rank-k matrix, which leads to the following nuclear norm regularized problem to enforce the low-rankness of W :
min W∈Rd×T
‖W‖∗ + C ∑
(i,j,k)∈Ω
ξ2ijk
s.t. yijkw T",4. Proposed Algorithm,[0],[0]
"i (xj − xk) ≥ 1− ξijk,
ξijk ≥ 0, ∀(i, j, k) ∈",4. Proposed Algorithm,[0],[0]
"Ω.
where || · ||∗ is the nuclear norm of matrix, defined by summation of singular values.",4. Proposed Algorithm,[0],[0]
"We could use some recent developed nuclear norm solvers to solve (4) (see (Cai et al., 2010; Hsieh & Olsen, 2014)).
",4. Proposed Algorithm,[0],[0]
"While the nuclear norm regularized formulation is statistically near optimal for recovering the underlying low-rank model, it cannot be efficiently solved since there are dT parameters in the problem.",4. Proposed Algorithm,[0],[0]
"Therefore, we solve the following
equivalent non-convex formulation:
min U,V
C ∑
(i,j,k)∈Ω
ξ2ijk + 1
2 (‖U‖2F + ‖V ‖2F )
s.t. yijkv̄Ti U T (xj − xk) ≥ 1− ξijk,
ξijk ≥ 0,∀(i, j, k) ∈",4. Proposed Algorithm,[0],[0]
"Ω. (1)
where we replace the nuclear norm regularization in Equation (4) using the property ‖W‖∗",4. Proposed Algorithm,[0],[0]
"= minW=UV T 12 (‖U‖ 2 F+ ‖V ‖2F ), U ∈ Rd×k, V ∈ RT×k, and v̄Ti is the i-th row of V .",4. Proposed Algorithm,[0],[0]
"With this non-convex relaxation, there are only (d+T )k parameters involved.",4. Proposed Algorithm,[0],[0]
"So it is preferred over the convex form.
",4. Proposed Algorithm,[0],[0]
"However, developing a fast solver for (1) is still nontrivial.",4. Proposed Algorithm,[0],[0]
"Although RankSVM is often solved in the dual space using stochastic dual coordinate ascent (SDCA) (ShalevShwartz & Zhang, 2013), in our case, it is not suitable because there are |Ω|k dual variables, where each corresponds to one constraint.",4. Proposed Algorithm,[0],[0]
So applying a dual coordinate ascent will take O(|Ωk|) time complexity (to go through all dual variables) and the same order of memory complexity to store all of them.,4. Proposed Algorithm,[0],[0]
"Therefore, we solve the problem in the primal space using alternating minimization.",4. Proposed Algorithm,[0],[0]
"Instead of solving the constrained form, we solve the following equivalent unconstrained problem:
min U,V
{ C ∑ (i,j,k)∈Ω max(0, 1− yijkv̄Ti UT (xj − xk))2
+ 1
2 (‖U‖2F + ‖V ‖2F )
} := f(U, V ).
",4. Proposed Algorithm,[0],[0]
"(2)
Following the alternating minimization scheme, our algorithm iteratively updates one ofU, V while keeping the other one fixed.",4. Proposed Algorithm,[0],[0]
"When updating U with V fixed, the subproblem becomes:
U = argmin U∈Rd×k
C ∑
(i,j,k)∈Ω
max(0, 1− yijkv̄Ti UT (xj − xk))2
+ 1
2 ‖U‖2F .
",4. Proposed Algorithm,[0],[0]
"(3) To solve the problem in the primal space, the main bottleneck is the gradient computation when we apply gradient descent.",4. Proposed Algorithm,[0],[0]
"The gradient can be written as
∇Uf(U, V ) = U+ T∑ i=1",4. Proposed Algorithm,[0],[0]
"∑ (j,k)∈Ωi −2Cyijk max ( 0, 1− yijkv̄Ti UT (xj − xk) ) (xj − xk)v̄Ti
(4) Computing (4) naively takes O(|Ω|kd) time, since we need to go through the summation, and each term requires O(kd) computing time for computing (xj − xk)v̄Ti .",4. Proposed Algorithm,[0],[0]
"However, by re-organizing the computation using a book-keeping technique, we are able to do this in O(T n̄k + dkn + |Ω|)
",4. Proposed Algorithm,[0],[0]
Algorithm 1 Factorization RankSVM:,4. Proposed Algorithm,[0],[0]
"Computing ∇Uf(U, V )
",4. Proposed Algorithm,[0],[0]
"Input: X,V,Ω, Y Compute pj = U
Txj and set zj = 0 for all j = 1, . . .",4. Proposed Algorithm,[0],[0]
", n",4. Proposed Algorithm,[0],[0]
"for i = 1, 2, . . .",4. Proposed Algorithm,[0],[0]
", T do
Compute qj =",4. Proposed Algorithm,[0],[0]
"v̄ T i pj for all j ∈ Ωi Set sj = 0 for all j ∈ Ωi for (j, k) ∈",4. Proposed Algorithm,[0],[0]
Ωi do sj ← sj,4. Proposed Algorithm,[0],[0]
"− 2Cyijk max(0, 1− yijk(qj − qk))",4. Proposed Algorithm,[0],[0]
"sk ← sk + 2Cyijk max(0, 1− yijk(qj − qk)) end for zj ← zj + sjvi for all j ∈ Ωi
end for for j = 1, 2, . . .",4. Proposed Algorithm,[0],[0]
", n do ∇Uf(U, V )← ∇Uf(U, V ) + xjzTj end for Output ∇Uf(U, V ) +",4. Proposed Algorithm,[0],[0]
"U
time, where n̄ is the average number of ratings per user.",4. Proposed Algorithm,[0],[0]
"The details are presented in Algorithm 1.
",4. Proposed Algorithm,[0],[0]
"For updating V , the objective function (1) can be decomposed into T subproblems:
v̄i = argmin v̄i∈Rk
C ∑
(j,k)∈Ωi
max(0, 1− yijkv̄Ti UT (xj − xk))2
+ 1
2 ‖vi‖22,
(5) where each of them is just an RankSVM problem that can be easily solved by gradient descent or Newton method (Chapelle & Keerthi, 2010).",4. Proposed Algorithm,[0],[0]
"The details are omitted here, but the time complexity for this part isO(T n̄k+dkn+ |Ω|), which is exactly the same with the U part.
",4. Proposed Algorithm,[0],[0]
"To sum up, our algorithm has an overall time complexity O(T n̄k + dkn+ |Ω|) per iteration, which is quite small because the dominated term |Ω| (number of pairs) is separated from rest of the terms.",4. Proposed Algorithm,[0],[0]
"Also, k (rank) and n̄ (averaged items involves in a ranking task) are usually small.",4. Proposed Algorithm,[0],[0]
"Furthermore, we could adapt Newton method proposed by (Wu et al., 2017b) to further speed up the optimization.",4. Proposed Algorithm,[0],[0]
"As a result, we are able to scaleto very large datasets.",4. Proposed Algorithm,[0],[0]
Now we analyze the sample complexity of the proposed model.,5. Sample Complexity Analysis,[0],[0]
"If we keep growing T (number of ranking functions), under the low-rank assumptionW = O(T 1/2), the samples needed for Factorization RankSVM to achieve the same -error is approximately O(T 1/2), which is much better than training T individual RankSVMs which requires O(T ) samples.",5. Sample Complexity Analysis,[0],[0]
"Detailed proofs can be found in the appendix.
",5. Sample Complexity Analysis,[0],[0]
"Sample complexity of our model
Assume we observe a set of (i, j, k) pairs and comparison results yijk ∈ {+1,−1} from a fixed but unknown distribution.",5. Sample Complexity Analysis,[0],[0]
"To recover the underlying model, we proposed to solve (4), and it is equivalent to the constraint form:
Ŵ = arg min W∈Rd×T ∑",5. Sample Complexity Analysis,[0],[0]
(,5. Sample Complexity Analysis,[0],[0]
"i,j,k)∈Ω `((ITi W T (xj − xk)), yijk),
s.t. ‖W‖∗ ≤",5. Sample Complexity Analysis,[0],[0]
"W, (6)
where I ∈ RT×T is the indicator matrix, each column Ii is [0, 0, ..., 1, 0, 0] where the i-th element equals to one.",5. Sample Complexity Analysis,[0],[0]
"Without loss of generality, we assume ‖xj‖ ≤ 1 for all j. The prediction function we want to learn is
fW",5. Sample Complexity Analysis,[0],[0]
"(i, j, k) =",5. Sample Complexity Analysis,[0],[0]
I T,5. Sample Complexity Analysis,[0],[0]
"i W T (xj − xk) = 〈W, (xj − xk)ITi 〉,
and in our formulation (6), we search within the function class FW := {fW : ‖W‖∗ ≤ W}.
",5. Sample Complexity Analysis,[0],[0]
"The quality of any ranking function fW can be measured by the following expected ranking error (where 1(·) is the indicator function):
R(f) := Ei,j,k[1 ( sign(f(i, j, k))",5. Sample Complexity Analysis,[0],[0]
6= sign(yijk) ),5. Sample Complexity Analysis,[0],[0]
].,5. Sample Complexity Analysis,[0],[0]
"(7)
We denote R∗ = minf R(f) to be the optimal risk we can get.",5. Sample Complexity Analysis,[0],[0]
"Since optimizing 0/1 loss is hard, our algorithm uses a convex surrogate loss `, and the following concepts of `-risk will be used in our analysis:
• Expected `-risk: R`(f) =",5. Sample Complexity Analysis,[0],[0]
"Ei,j,k[`(f(i, j, k), yijk)]",5. Sample Complexity Analysis,[0],[0]
"• Empirical `-risk: R̂`(f) = 1 m ∑ (i,j,k)∈Ω `(f(i, j, k), yijk)
",5. Sample Complexity Analysis,[0],[0]
"We begin with the following lemma to bound the expected `-risk:
Lemma 1 (Bound on Expected `-risk (Bartlett & Mendelson, 2002)).",5. Sample Complexity Analysis,[0],[0]
"Assume `(·, ·) is a loss function upper bounded by B and with Lipschitz constant L` with respect to its first argument.",5. Sample Complexity Analysis,[0],[0]
"Let R(FW ) be the model complexity of the function class FW (w.r.t Ω and associated with `) defined as:
R(FW ) =",5. Sample Complexity Analysis,[0],[0]
"Eσ[ sup f∈FW
1
m ∑ (i,j,k)∈Ω σα`(f(i, j, k), yijk)],
(8) where each σα takes values {±1} with equal probability.",5. Sample Complexity Analysis,[0],[0]
"Then with probability at least 1 − δ, for all f ∈ FW , we have
R`(f) ≤ R̂`(f) + 2EΩ[R(FW )]",5. Sample Complexity Analysis,[0],[0]
"+ B √ log 1δ 2m .
",5. Sample Complexity Analysis,[0],[0]
"To achieve an upper bound for R`(f), we derive a bound of the Radamacker complexity EΩ[R(FW )]:
Lemma 2.",5. Sample Complexity Analysis,[0],[0]
"The model complexity of (6) can be upper bounded by:
EΩ[R(FW )]",5. Sample Complexity Analysis,[0],[0]
"≤ min { 2L`W √ log 2d
m ,
√ 9L`BCW( √ T + n)
m
} ,
(9) where L` is the Lipchitz constant of loss function and C is a universal constant.
",5. Sample Complexity Analysis,[0],[0]
"With the above lemma, we now derive the following theorem to bound the expected ranking error:
Theorem 1.",5. Sample Complexity Analysis,[0],[0]
"With probability 1− δ, the expected error of the optimal solution of our model (6) is:
R(fŴ )−R ∗ ≤O(R̂`(fŴ )−R ∗ ` )",5. Sample Complexity Analysis,[0],[0]
"+O(B
√ log(1/δ)
",5. Sample Complexity Analysis,[0],[0]
"m )
",5. Sample Complexity Analysis,[0],[0]
"+O( min(
√ WB(
√ T + n),W log d) √ m ),
(10) where R∗ = inff R(f) and R∗` := inff R`(f).
",5. Sample Complexity Analysis,[0],[0]
Note that all the hidden constants can be found in the appendix.,5. Sample Complexity Analysis,[0],[0]
"In general, the first term on the right hand side will be small since fŴ minimizes the empirical error.",5. Sample Complexity Analysis,[0],[0]
"This is a standard generalization error bound (as shown in (Kakade et al., 2009))",5. Sample Complexity Analysis,[0],[0]
"that works for any distribution of yijk.
",5. Sample Complexity Analysis,[0],[0]
"If we further assume the yijk is generated from an unseen groudtruth W ∗ with ‖W∗‖∗ ≤ W , then the following theorem shows that the error is small when m goes to infinity:
Lemma 3.",5. Sample Complexity Analysis,[0],[0]
"If the observed yijk = xTj w∗i − xTkw∗i for all i, j, k, and loss function satisfies `(a, b) = 0 if sign(a) = sign(b), then we have R(fŴ ) ≤
O( min( √ WB( √ T+n),W log d)√ m )",5. Sample Complexity Analysis,[0],[0]
+,5. Sample Complexity Analysis,[0],[0]
"O(B √ log(1/δ) m ).
",5. Sample Complexity Analysis,[0],[0]
"Note that the loss `(a, y) = max(−ay, 0)2 satisfies the assumption of Lemma 3, but in practice adding a margin will improve the performance (using `(a, y) = max(1−ay, 0)2).",5. Sample Complexity Analysis,[0],[0]
"From Theorem 1 and Lemma 3, we can conclude that the error of our model decreases roughly with 1/ √ m (m is number of samples), and increases with √ W (nuclear norm of the underlying model).
",5. Sample Complexity Analysis,[0],[0]
"Comparison with RANKSVM SEPARATELY.
",5. Sample Complexity Analysis,[0],[0]
"Training T independent RankSVMs separately can also achieve arbitrary small error under similar condition, so the main question is whether our model can reduce the number of samplesm needed.",5. Sample Complexity Analysis,[0],[0]
"In RANKSVM SEPARATELY, it is equivalent to solving problem (6) with the constraint replaced by ‖wi‖ ≤",5. Sample Complexity Analysis,[0],[0]
w,5. Sample Complexity Analysis,[0],[0]
"for all i. Assume there are m/T pairs per ranking function, then we can prove the following sample complexity based on standard analysis from (Kakade et al., 2009):
Lemma 4.",5. Sample Complexity Analysis,[0],[0]
"Under the same condition of Lemma 3, the RANKSVM SEPARATELY solution f̃ satisfies
R(f̃) =",5. Sample Complexity Analysis,[0],[0]
O( w√ m/T ),5. Sample Complexity Analysis,[0],[0]
"+O(B
√ log(1/δ)
m/T ).
",5. Sample Complexity Analysis,[0],[0]
"Note that we assumeW := ‖W ∗‖∗ and w := maxi ‖W ∗:,i‖ where W ∗ is the underlying matrix.",5. Sample Complexity Analysis,[0],[0]
"Clearly, if the nuclear norm W is small constant, our sample complexity (Lemma 3) is much better than the bound for RANKSVM SEPARATELY (Lemma 4), since our dependency to T is O(T 1/4) while it is O( √ T ) for rankSVM.",5. Sample Complexity Analysis,[0],[0]
"Moreover, in another setting (see, for example, (Shamir & Shalev-Shwartz, 2014)), if each element of W ∗ is bounded and rank of W ∗ is a constant,W = O( √ Td) and w =",5. Sample Complexity Analysis,[0],[0]
"O( √ d), our bound in Lemma 3 is still better than Lemma 4.",5. Sample Complexity Analysis,[0],[0]
"Although a better sample complexity upper bound doesnt directly imply our method is always better, however, by obtaining a smaller Rademacher complexity, it is clear that our formulation has benefits to achieve a tighter upper bounds, which leads to better performance in practice.",5. Sample Complexity Analysis,[0],[0]
"In this section, we show our method outperforms other algorithms on both synthetic and real datasets.",6. Experimental Results,[0],[0]
"All the experiments are conducted on a server with an Intel E7-4820 CPU and 256G memory.
",6. Experimental Results,[0],[0]
Experimental Setting.,6. Experimental Results,[0],[0]
"For each ranking task, we randomly split the items into training items and testing items.",6. Experimental Results,[0],[0]
"In the training phase, we use all the pairs between training items to train the model, and in the testing phase we evaluate the prediction accuracy for all the testing-testing item pairs and testing-training item pairs, which is similar with BPR (Rendle et al., 2009).",6. Experimental Results,[0],[0]
"The accuracy is defined to be the correctly predicted pairs divided by total number of predicted pairs.
",6. Experimental Results,[0],[0]
"We mainly compare our algorithm with RANKSVM JOINTLY (training a single rankSVM model), RANKSVM SEPARATELY (training an individual rankSVM model for each task), and RANKSVM VAR (the multi-task rankSVM model proposed in (Evgeniou & Pontil, 2004)).",6. Experimental Results,[0],[0]
All the algorithm above are using square hinge loss in the experiments.,6. Experimental Results,[0],[0]
"We choose the best regularization parameter for each method by a validation set.
",6. Experimental Results,[0],[0]
Synthetic Data.,6. Experimental Results,[0],[0]
"For synthetic dataset, we assume there are 1, 000 tasks, 10, 000 items and each item has 64 features.",6. Experimental Results,[0],[0]
"The underlying ranking models are generated by W ∗ = U∗(V ∗)T , where U∗ ∈ R64×20, V ∗ ∈ R1000,20, and U, V ∼ N (0, 1).",6. Experimental Results,[0],[0]
"The feature matrix is generated by X ∈ R64×10,000, X ∼ N (0, 1).",6. Experimental Results,[0],[0]
"We sample 800 pairs for each user as training data, with labels based on underlying
rating R = (W ∗)TX .
",6. Experimental Results,[0],[0]
Table 2 shows that our algorithms outperform other rankSVM algorithms on synthetic datasets.,6. Experimental Results,[0],[0]
"Also, as showed in Figure 1, We observe that RANKSVM JOINTLY suffers from under-fitting (low training and test accuracy).",6. Experimental Results,[0],[0]
"On the other hand, RANKSVM SEPARATELY has the over-fitting problem (high training accuracy but low test accuracy) since it does not have enough samples for learning each individual task.",6. Experimental Results,[0],[0]
"Since the underlying U, V have rank 20, our model with rank 20 performs the best.",6. Experimental Results,[0],[0]
"However, even if we choose rank to be 10 or 30, our model still significantly outperforms the other models.
",6. Experimental Results,[0],[0]
Real World Datasets.,6. Experimental Results,[0],[0]
We use recommender system as an application to compare our algorithm with other ranking algorithms.,6. Experimental Results,[0],[0]
"Each user is treated as a “ranking task”, and the observed pairs are generated from training ratings.",6. Experimental Results,[0],[0]
"Note that we are also given item features x1, . . .",6. Experimental Results,[0],[0]
",xn, and the goal is to learn a personalized ranking model wi for each user.",6. Experimental Results,[0],[0]
"The testing items are unseen in the training phase, which is different from classical matrix completion problem—the goal of classical matrix completion is to complete the matrix, while our goal is to learn the function that can generalize to unseen items.",6. Experimental Results,[0],[0]
"The only matrix completion work that can utilize the feature information to predict unseen items is inductive matrix completion (Jain & Dhillon, 2013) (IMC), which is a special case of factorization machine (Rendle, 2010).",6. Experimental Results,[0],[0]
"Although they do not allow pairwise comparisons as input, for the completeness of comparison, we still include them into comparison and give them the original rating data as input.
",6. Experimental Results,[0],[0]
We choose three datasets in our real-world application experiments: (1) Yahoo!,6. Experimental Results,[0],[0]
"Movies User Ratings and Descriptive Content Information V1 01 (2) HetRec2011-MovieLens-2K (Cantador et al., 2011).",6. Experimental Results,[0],[0]
"(3) MovieLens 20M Dataset (Harper
1http://research.yahoo.com/Academic Relations
& Konstan, 2016).",6. Experimental Results,[0],[0]
"For the first dataset, we use the title and abstract of each movie and combine them as the feature matrix X .",6. Experimental Results,[0],[0]
"For the second and third datasets, we take the genres information of each movie as features.",6. Experimental Results,[0],[0]
"See Table3 for more information.
",6. Experimental Results,[0],[0]
The results for datasets (1) and (2) are presented in Table4.,6. Experimental Results,[0],[0]
"Clearly, our method outperforms other algorithms both in accuracy and in speed.",6. Experimental Results,[0],[0]
"Note that dataset (1) has dense features and dataset (2) has sparse features, and our algorithm performs well in both cases.",6. Experimental Results,[0],[0]
"For dataset (3), there are more than 100,000 ranking tasks and other algorithms take more than 1000 seconds per epoch.",6. Experimental Results,[0],[0]
"However, our algorithm only takes about 100 seconds per epoch, and converges to a solution with 63.4% testing accuracy.
",6. Experimental Results,[0],[0]
We also plot the time vs accuracy curves in Figure Our algorithms consistently get better accuracy compared to all other methods.,6. Experimental Results,[0],[0]
"Note that sometimes RANKSVM JOINTLY is fast in the beginning, but eventually it cannot converge to a good solution.",6. Experimental Results,[0],[0]
Visualize basic ranking functions.,6.1. Feature Embedding,[0],[0]
"Finally, we visualize the basic ranking functions learned by our model.",6.1. Feature Embedding,[0],[0]
"We take the Yahoo! movie dataset, where each feature corresponds
to a word in movie title and abstract.",6.1. Feature Embedding,[0],[0]
"We select a basic ranking function (a column of U ) from our model and show the top 25 features with most positive weights and bottom 25 features with most negative weights in Figure 2, 3.",6.1. Feature Embedding,[0],[0]
The visualization of ranking function clearly demonstrates interesting common patterns of users’ tastes.,6.1. Feature Embedding,[0],[0]
We propose a new algorithm for learning multiple ranking functions based on the combination of RankSVM and matrix factorization.,7. Conclusions,[0],[0]
"We show that the model can be solved efficiently, has good statistical guarantee, and outperforms other methods on real datasets in both training time and prediction accuracy.",7. Conclusions,[0],[0]
Our algorithm can be used in many online personalized ranking systems.,7. Conclusions,[0],[0]
"An interesting direction is to introduce non-linearity (e.g., neural networks) in the feature side of our model and learn U, V with neural network weights jointly.",7. Conclusions,[0],[0]
Cho-Jui Hsieh and Minhao Cheng acknowledge the support of NSF via IIS-1719097 and the computing resources provided by Google cloud and Nvidia.,Acknowledgments,[0],[0]
We consider the setting where we wish to perform ranking for hundreds of thousands of users which is common in recommender systems and web search ranking.,abstractText,[0],[0]
Learning a single ranking function is unlikely to capture the variability across all users while learning a ranking function for each person is time-consuming and requires large amounts of data from each user.,abstractText,[0],[0]
"To address this situation, we propose a Factorization RankSVM algorithm which learns a series of k basic ranking functions and then constructs for each user a local ranking function that is a combination of them.",abstractText,[0],[0]
"We develop a fast algorithm to reduce the time complexity of gradient descent solver by exploiting the low-rank structure, and the resulting algorithm is much faster than existing methods.",abstractText,[0],[0]
"Furthermore, we prove that the generalization error of the proposed method can be significantly better than training individual RankSVMs.",abstractText,[0],[0]
"Finally, we present some interesting patterns in the principal ranking functions learned by our algorithms.",abstractText,[0],[0]
Extreme Learning to Rank via Low Rank Assumption,title,[0],[0]
"The success stories of deep learning form an ever lengthening list of practical breakthroughs and state-ofthe-art performances, ranging the fields of computer vision [23, 14, 25, 33], audio and natural language processing and generation [5, 15, 11, 34], as well as robotics [24, 26], to name just a few.",1 Introduction,[0],[0]
"The list of success stories can be matched and surpassed by a list of practical “tips and tricks”, from different optimization algorithms, parameter tuning methods [30, 22], initialization schemes [10], architecture designs [31], loss functions, data augmentation [23] and so on.
",1 Introduction,[0],[0]
The current theoretical understanding of deep learning is far from being sufficient for a rigorous analysis of the difficulties faced by practitioners.,1 Introduction,[0],[0]
"Progress must be made from both parties: from a practitioner’s perspective, emphasizing the difficulties provides practical insights to the theoretician, which in turn, supplies theoretical insights and guarantees, further strengthening and sharpening practical intuitions and wisdom.",1 Introduction,[0],[0]
"In particular, understanding failures of existing algorithms is as important as understanding where they succeed.
",1 Introduction,[0],[0]
Our goal in this paper is to present and discuss families of simple problems for which commonly used methods do not show as exceptional a performance as one might expect.,1 Introduction,[0],[0]
"We use empirical results and insights as a ground on which to build a theoretical analysis, characterising the sources of failure.",1 Introduction,[0],[0]
"Those understandings are aligned, and sometimes lead to, different approaches, either for an architecture, loss function, or an optimization scheme, and explain their superiority when applied to members of those families.",1 Introduction,[0],[0]
"Interestingly, the sources for failure in our experiment do not seem to relate to stationary point issues such as spurious local minima or a plethora of saddle points, a topic of much recent interest (e.g. [6, 3]),
1This paper was done with the support of the Intel Collaborative Research institute for Computational Intelligence (ICRI-CI) and is part of the “Why & When Deep Learning works – looking inside Deep Learning” ICRI-CI paper bundle
ar X
iv :1
70 3.
07 95
0v 2
[ cs
.L",1 Introduction,[0],[0]
"G
] 2
6 A
pr 2
but rather to more subtle issues, having to do with informativeness of the gradients, signal-to-noise ratios, conditioning etc.",1 Introduction,[0],[0]
"The code for running all our experiments is available online2.
",1 Introduction,[0],[0]
"We start off in Section 2 by discussing a class of simple learning problems for which the gradient information, central to deep learning algorithms, provably carries negligible information on the target function which we attempt to learn.",1 Introduction,[0],[0]
"This result is a property of the learning problems themselves, and holds for any specific network architecture one may choose for tackling the learning problem, implying that no gradientbased method is likely to succeed.",1 Introduction,[0],[0]
"Our analysis relies on tools and insights from the Statistical Queries literature, and underscores one of the main deficiencies of Deep Learning: its reliance on local properties of the loss function, with the objective being of a global nature.
",1 Introduction,[0],[0]
"Next, in Section 3, we tackle the ongoing dispute between two common approaches to learning.",1 Introduction,[0],[0]
"Most, if not all, learning and optimization problems can be viewed as some structured set of sub-problems.",1 Introduction,[0],[0]
"The first approach, which we refer to as the “end-to-end” approach, will tend to solve all of the sub-problems together in one shot, by optimizing a single primary objective.",1 Introduction,[0],[0]
"The second approach, which we refer to as the “decomposition” one, will tend to handle these sub-problems separately, solving each one by defining and optimizing additional objectives, and not rely solely on the primary objective.",1 Introduction,[0],[0]
"The benefits of the end-to-end approach, both in terms of requiring a smaller amount of labeling and prior knowledge, and perhaps enabling more expressive architectures, cannot be ignored.",1 Introduction,[0],[0]
"On the other hand, intuitively and empirically, the extra supervision injected through decomposition is helpful in the optimization process.",1 Introduction,[0],[0]
"We experiment with a simple problem in which application of the two approaches is possible, and the distinction between them is clear and intuitive.",1 Introduction,[0],[0]
"We observe that an end-to-end approach can be much slower than a decomposition method, to the extent that, as the scale of the problem grows, no progress is observed.",1 Introduction,[0],[0]
"We analyze this gap by showing, theoretically and empirically, that the gradients are much more noisy and less informative with the end-to-end approach, as opposed to the decomposition approach, explaining the disparity in practical performance.
",1 Introduction,[0],[0]
"In Section 4, we demonstrate the importance of both the network’s architecture and the optimization algorithm on the training time.",1 Introduction,[0],[0]
"While the choice of architecture is usually studied in the context of its expressive power, we show that even when two architectures have the same expressive power for a given task, there may be a tremendous difference in the ability to optimize them.",1 Introduction,[0],[0]
We analyze the required runtime of gradient descent for the two architectures through the lens of the condition number of the problem.,1 Introduction,[0],[0]
We further show that conditioning techniques can yield additional orders of magnitude speedups.,1 Introduction,[0],[0]
The experimental setup in this section is around a seemingly simple problem — encoding a piece-wise linear one-dimensional curve.,1 Introduction,[0],[0]
"Despite the simplicity of this problem, we show that following the common rule of “perhaps I should use a deeper/wider network”3 does not significantly help here.
",1 Introduction,[0],[0]
"Finally, in Section 5, we consider deep learning’s reliance on “vanilla” gradient information for the optimization process.",1 Introduction,[0],[0]
We previously discussed the deficiency of using a local property of the objective in directing global optimization.,1 Introduction,[0],[0]
"Here, we focus on a simple case in which it is possible to solve the optimization problem based on local information, but not in the form of a gradient.",1 Introduction,[0],[0]
"We experiment with architectures that contain activation functions with flat regions, which leads to the well known vanishing gradient problem.",1 Introduction,[0],[0]
"Practitioners take great care when working with such activation functions, and many heuristic tricks are applied in order to initialize the network’s weights in non-flat areas of its activations.",1 Introduction,[0],[0]
"Here, we show that by using a different update rule, we manage to solve the learning problem efficiently.",1 Introduction,[0],[0]
"Moreover, one can show convergence guarantees for a family of such functions.",1 Introduction,[0],[0]
"This provides a clean example where non-gradient-based optimization schemes can overcome the limitations of gradient-based
2 https://github.com/shakedshammah/failures_of_DL.",1 Introduction,[0],[0]
"See command lines in Appendix D. 3See http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/ for the inspiration behind this quote.
learning.",1 Introduction,[0],[0]
"Most existing deep learning algorithms are gradient-based methods; namely, algorithms which optimize an objective through access to its gradient w.r.t.",2 Parities and Linear-Periodic Functions,[0],[0]
"some weight vector w, or estimates of the gradient.",2 Parities and Linear-Periodic Functions,[0],[0]
"We consider a setting where the goal of this optimization process is to learn some underlying hypothesis class H, of which one member, h ∈ H, is responsible for labelling the data.",2 Parities and Linear-Periodic Functions,[0],[0]
"This yields an optimization problem of the form
min w Fh(w).
",2 Parities and Linear-Periodic Functions,[0],[0]
The underlying assumption is that the gradient of the objective w.r.t.,2 Parities and Linear-Periodic Functions,[0],[0]
"w, ∇Fh(w), contains useful information regarding the target function h, and will help us make progress.
",2 Parities and Linear-Periodic Functions,[0],[0]
"Below, we discuss a family of problems for which with high probability, at any fixed point, the gradient, ∇Fh(w), will be essentially the same regardless of the underlying target function h.",2 Parities and Linear-Periodic Functions,[0],[0]
"Furthermore, we prove that this holds independently of the choice of architecture or parametrization, and using a deeper/wider network will not help.",2 Parities and Linear-Periodic Functions,[0],[0]
"The family we study is that of compositions of linear and periodic functions, and we experiment with the classical problem of learning parities.",2 Parities and Linear-Periodic Functions,[0],[0]
"Our empirical and theoretical study shows that indeed, if there’s little information in the gradient, using it for learning cannot succeed.",2 Parities and Linear-Periodic Functions,[0],[0]
"We begin with the simple problem of learning random parities: After choosing some v∗ ∈ {0, 1}d uniformly at random, our goal is to train a predictor mapping x ∈ {0, 1}d to y = (−1)〈x,v∗〉, where x is uniformly distributed.",2.1 Experiment,[0],[0]
"In words, y indicates whether the number of 1’s in a certain subset of coordinates of x (indicated by v∗) is odd or even.
",2.1 Experiment,[0],[0]
"For our experiments, we use the hinge loss, and a simple network architecture of one fully connected layer of width 10d > 3d2 with ReLU activations, and a fully connected output layer with linear activation and a single unit.",2.1 Experiment,[0],[0]
"Note that this class realizes the parity function corresponding to any v∗ (see Lemma 5 in the appendix).
",2.1 Experiment,[0],[0]
"Empirically, as the dimension d increases, so does the difficulty of learning, which can be measured in the accuracy we arrive at after a fixed number of training iterations, to the point where around d = 30, no advance beyond random performance is observed after reasonable time.",2.1 Experiment,[0],[0]
Figure 1 illustrates the results.,2.1 Experiment,[0],[0]
"To formally explain the failure from a geometric perspective, consider the stochastic optimization problem associated with learning a target function h,
min w Fh(w) :",2.2 Analysis,[0],[0]
= E x,2.2 Analysis,[0],[0]
"[`(pw(x), h(x))] , (1)
where ` is a loss function, x are the stochastic inputs (assumed to be vectors in Euclidean space), and pw is some predictor parametrized by a parameter vector",2.2 Analysis,[0],[0]
w,2.2 Analysis,[0],[0]
(e.g. a neural network of a certain architecture).,2.2 Analysis,[0],[0]
We will assume that F is differentiable.,2.2 Analysis,[0],[0]
"A key quantity we will be interested in studying is the variance of the
gradient of F with respect to h, when h is drawn uniformly at random from a collection of candidate target functionsH:
Var(H, F,w) =",2.2 Analysis,[0],[0]
E h ∥∥∥∥∇Fh(w)− Eh′∇Fh′(w),2.2 Analysis,[0],[0]
"∥∥∥∥2 (2)
Intuitively, this measures the expected amount of “signal” about the underlying target function contained in the gradient.",2.2 Analysis,[0],[0]
"As we will see later, this variance correlates with the difficulty of solving (1) using gradientbased methods4.
",2.2 Analysis,[0],[0]
"The following theorem bounds this variance term.
",2.2 Analysis,[0],[0]
"Theorem 1 Suppose that
• H consists of real-valued functions h satisfying Ex[h2(x)]",2.2 Analysis,[0],[0]
"≤ 1, such that for any two distinct h, h′ ∈ H, Ex[h(x)h′(x)]",2.2 Analysis,[0],[0]
"= 0.
• pw(x) is differentiable w.r.t.",2.2 Analysis,[0],[0]
"w, and satisfies Ex [ ‖ ∂∂wpw(x)‖ 2 ] ≤",2.2 Analysis,[0],[0]
"G(w)2 for some scalar G(w).
",2.2 Analysis,[0],[0]
•,2.2 Analysis,[0],[0]
"The loss function ` in (1) is either the square loss `(ŷ, y) = 12(ŷ − y) 2 or a classification loss of the
form `(ŷ, y) =",2.2 Analysis,[0],[0]
"r(ŷ ·y) for some 1-Lipschitz function r, and the target function h takes values in {±1}.
",2.2 Analysis,[0],[0]
"Then
Var(H, F,w) ≤ G(w) 2
|H| .
",2.2 Analysis,[0],[0]
The proof is given in Appendix B.1.,2.2 Analysis,[0],[0]
"The theorem implies that if we try to learn an unknown target function, possibly coming from a large collection of uncorrelated functions, then the sensitivity of the gradient to the target function at any point decreases linearly with |H|.
",2.2 Analysis,[0],[0]
"Before we make a more general statement, let us return to the case of parities, and study it through the lens of this framework.",2.2 Analysis,[0],[0]
"Suppose that our target function is some parity function chosen uniformly at random, i.e. a random element from the set of 2d functions H = {x 7→",2.2 Analysis,[0],[0]
"(−1)〈x,v∗〉 : v∗ ∈ {0, 1}d}.",2.2 Analysis,[0],[0]
"These
4This should not be confused with the variance of gradient estimates used by SGD, which we discuss in Section 3.
",2.2 Analysis,[0],[0]
"are binary functions, which are easily seen to be mutually orthogonal:",2.2 Analysis,[0],[0]
"Indeed, for any v,v′,
E x
[ (−1)〈x,v〉(−1)〈x,v′〉 ] = E
x
[ (−1)〈x,v+v′〉 ] =
d∏ i=1",2.2 Analysis,[0],[0]
E [ (−1)xi(vi+v′i) ] = d∏ i=1,2.2 Analysis,[0],[0]
"(−1)vi+v′i + (−1)−(vi+v′i) 2
which is non-zero if and only if v = v′. Therefore, by Theorem 1, we get that Var(H, F,w) ≤ G(w)2/2d – that is, exponentially small in the dimension d. By Chebyshev’s inequality, this implies that the gradient at any point w will be extremely concentrated around a fixed point independent of h.
This phenomenon of exponentially-small variance can also be observed for other distributions, and learning problems other than parities.",2.2 Analysis,[0],[0]
"Indeed, in [29], it was shown that this also holds in a more general setup, when the output y corresponds to a linear function composed with a periodic one, and the input x is sampled from a smooth distribution:
Theorem 2 (Shamir 2016)",2.2 Analysis,[0],[0]
"Let ψ be a fixed periodic function, and let H = {x 7→ ψ(v∗>x) : ‖v∗‖ = r} for some r > 0.",2.2 Analysis,[0],[0]
"Suppose x ∈ Rd is sampled from an arbitrary mixture of distributions with the following property: The square root of the density function ϕ has a Fourier transform ϕ̂ satisfying ∫ x:‖x‖>r ϕ̂
2(x)dx∫ x ϕ̂ 2(x)dx ≤
exp(−Ω(r)).",2.2 Analysis,[0],[0]
"Then if F denotes the objective function with respect to the squared loss,
Var(H, F,w) ≤ O (exp(−Ω(d))",2.2 Analysis,[0],[0]
"+ exp(−Ω(r))) .
",2.2 Analysis,[0],[0]
"The condition on the Fourier transform of the density is generally satisfied for smooth distributions (e.g. arbitrary Gaussians whose covariance matrices are positive definite, with all eigenvalues at least Ω(1/r)).",2.2 Analysis,[0],[0]
"Thus, the bound is extremely small as long as the norm r and the dimension d are moderately large, and indicates that the gradients contains little signal on the underlying target function.
",2.2 Analysis,[0],[0]
"Based on these bounds, one can also formally prove that a gradient-based method, under a reasonable model, will fail in returning a reasonable predictor, unless the number of iterations is exponentially large in r and d 5 .",2.2 Analysis,[0],[0]
This provides strong evidence that gradient-based methods indeed cannot learn random parities and linear-periodic functions.,2.2 Analysis,[0],[0]
We emphasize that these results hold regardless of which class of predictors we use (e.g. they can be arbitrarily complex neural networks) – the problem lies in using a gradient-based method to train them.,2.2 Analysis,[0],[0]
"Also, we note that the difficulty lies in the random choice of v∗, and the problem is not difficult if v∗ is known and fixed in advance (for example, for a full parity v∗ = (1, . . .",2.2 Analysis,[0],[0]
", 1), this problem is known to be solvable with an appropriate LSTM network [17]).
",2.2 Analysis,[0],[0]
"Finally, we remark that the connection between parities, difficulty of learning and orthogonal functions is not new, and has already been made in the context of statistical query learning [21, 1].",2.2 Analysis,[0],[0]
"This refers to algorithms which are constrained to interact with data by receiving estimates of the expected value of some query over the underlying distribution (e.g. the expected value of the first coordinate), and it is well-known that parities cannot be learned with such algorithms.",2.2 Analysis,[0],[0]
"Recently, [8] have formally shown that gradient-based methods with an approximate gradient oracle can be implemented as a statistical query algorithm, which implies that gradient-based methods are indeed unlikely to solve learning problems which are known to be hard in the statistical queries framework, in particular parities.",2.2 Analysis,[0],[0]
"In the discussion on random parities above, we have simply made the connection between gradient-based methods and parities more explicit, by direct examination of gradients’ variance w.r.t.",2.2 Analysis,[0],[0]
"the target function.
",2.2 Analysis,[0],[0]
"5Formally, this requires an oracle-based model, where given a point w, the algorithm receives the gradient at w up to some arbitrary error much smaller than machine precision.",2.2 Analysis,[0],[0]
"See [29, Theorem 4] for details.",2.2 Analysis,[0],[0]
"Many practical learning problems, and more generally, algorithmic problems, can be viewed as a structured composition of sub-problems.",3 Decomposition vs. End-to-end,[0],[0]
"Applicable approaches for a solution can either be tackling the problem in an end-to-end manner, or by decomposition.",3 Decomposition vs. End-to-end,[0],[0]
"Whereas for a traditional algorithmic solution, the “divideand-conquer” strategy is an obvious choice, the ability of deep learning to utilize big data and expressive architectures has made “end-to-end training” an attractive alternative.",3 Decomposition vs. End-to-end,[0],[0]
"Prior results of end-to-end [24, 11] and decomposition and added feedback [13, 16, 32, 2] approaches show success in both directions.",3 Decomposition vs. End-to-end,[0],[0]
"Here, we try to address the following questions: What is the price of the rather appealing end-to-end approach?",3 Decomposition vs. End-to-end,[0],[0]
Is letting a network “learn by itself” such a bad idea?,3 Decomposition vs. End-to-end,[0],[0]
"When is it necessary, or worth the effort, to “help” it?
",3 Decomposition vs. End-to-end,[0],[0]
There are various aspects which can be considered in this context.,3 Decomposition vs. End-to-end,[0],[0]
"For example, [28] analyzed the difference between the approaches from the sample complexity point of view.",3 Decomposition vs. End-to-end,[0],[0]
"Here, we focus on the optimization aspect, showing that an end-to-end approach might suffer from non-informative or noisy gradients, which may significantly affect the training time.",3 Decomposition vs. End-to-end,[0],[0]
Helping the SGD process by decomposing the problem leads to much faster training.,3 Decomposition vs. End-to-end,[0],[0]
"We present a simple experiment, motivated by questions every practitioner must answer when facing a new, non trivial problem: What exactly is the required training data, what network architecture should be used, and what is the right distribution of development efforts.",3 Decomposition vs. End-to-end,[0],[0]
These are all correlated questions with no clear answer.,3 Decomposition vs. End-to-end,[0],[0]
Our experiments and analysis show that making the wrong choice can be expensive.,3 Decomposition vs. End-to-end,[0],[0]
"Our experiment compares the two approaches in a computer vision setting, where convolutional neural networks (CNN) have become the most widely used and successful algorithmic architectures.",3.1 Experiment,[0],[0]
"We define a family of problems, parameterized by k ∈ N, and show a gap (rapidly growing with k) between the performances of the end-to-end and decomposition approaches.
",3.1 Experiment,[0],[0]
"LetX denote the space of 28×28 binary images, with a distributionD defined by the following sampling procedure:
• Sample θ ∼ U([0, π]), l ∼ U([5, 28− 5]), (x, y) ∼ U([0, 27])2.
",3.1 Experiment,[0],[0]
"• The image xθ,l,(x,y) associated with the above sample is set to 0 everywhere, except for a straight line of length l, centered at (x, y), and rotated at an angle θ.",3.1 Experiment,[0],[0]
"Note that as the images space is discrete, we round the values corresponding to the points on the lines to the closest integer coordinate.
",3.1 Experiment,[0],[0]
"Let us define an “intermediate” labeling function y : X → {±1}, denoting whether the line in a given image slopes upwards or downwards, formally:
y(xθ,l,(x,y))",3.1 Experiment,[0],[0]
= { 1 if θ < π/2 −1,3.1 Experiment,[0],[0]
"otherwise .
",3.1 Experiment,[0],[0]
Figure 2 shows a few examples.,3.1 Experiment,[0],[0]
We can now define the problem for each k. Each input instance is a tuple xk1,3.1 Experiment,[0],[0]
":= (x1, . . .",3.1 Experiment,[0],[0]
",xk) of k images sampled i.i.d.",3.1 Experiment,[0],[0]
as above,3.1 Experiment,[0],[0]
.,3.1 Experiment,[0],[0]
"The target output is the parity over the image labels y(x1), . . .",3.1 Experiment,[0],[0]
", y(xk), namely ỹ(xk1) = ∏ j=1...k y(xj).
",3.1 Experiment,[0],[0]
Many architectures of DNN can be used for predicting ỹ(xk1) given x k 1 .,3.1 Experiment,[0],[0]
"A natural “high-level” choice
can be:
• Feed each of the images, separately, to a single CNN (of some standard specific architecture, for example, LeNet-like), denoted N (1)w1 and parameterized by its weights vector w1, outputting a single scalar, which can be regarded as a “score”.
",3.1 Experiment,[0],[0]
"• Concatenate the “scores” of a tuple’s entries, transform them to the range [0, 1] using a sigmoid function, and feed the resulting vector into another network, N (2)w2 , of a similar architecture to the one defined in Section 2, outputting a single “tuple-score”, which can then be thresholded for obtaining the binary prediction.
",3.1 Experiment,[0],[0]
Let the whole architecture be denoted Nw.,3.1 Experiment,[0],[0]
"Assuming that N (1) is expressive enough to provide, at least, a weak learner for y (a reasonable assumption), and that N (2) can express the relevant parity function (see Lemma 5 in the appendix), we obtain that this architecture has the potential for good performance.
",3.1 Experiment,[0],[0]
The final piece of the experimental setting is the choice of a loss function.,3.1 Experiment,[0],[0]
"Clearly, the primary loss which we’d like to minimize is the expected zero-one loss over the prediction, Nw(xk1), and the label, ỹ(xk1), namely:
L̃0−1(w) := E xk1
[ Nw(x k 1) 6= ỹ(xk1) ]",3.1 Experiment,[0],[0]
"A “secondary” loss which can be used in the decomposition approach is the zero-one loss over the
prediction of N (1)w1 (x k 1) and the respective y(x k 1) value:
L0−1(w1)",3.1 Experiment,[0],[0]
":= E xk1
[ N
(1) w1 (x k 1) 6= y(xk1) ]",3.1 Experiment,[0],[0]
"Let L̃, L be some differentiable surrogates for L̃0−1, L0−1.",3.1 Experiment,[0],[0]
"A classical end-to-end approach will be to minimize L̃, and only it; this is our “primary” objective.",3.1 Experiment,[0],[0]
"We have no explicit desire for N (1) to output any
specific value, and hence L is, a priori, irrelevant.",3.1 Experiment,[0],[0]
"A decomposition approach would be to minimize both losses, under the assumption that L can “direct” w1 towards an “area” in which we know that the resulting outputs of N (1) can be separated by N (2).",3.1 Experiment,[0],[0]
"Note that using L is only possible when the y values are known to us.
",3.1 Experiment,[0],[0]
"Empirically, when comparing performances based on the “primary” objective, we see that the end-to-end approach is significantly inferior to the decomposition approach (see Figure 3).",3.1 Experiment,[0],[0]
"Using decomposition, we quickly arrive at a good solution, regardless of the tuple’s length, k (as long as k is in the range where perfect input toN (2) is solvable by SGD, as described in Section 2).",3.1 Experiment,[0],[0]
"However, using the end-to-end approach works only for k = 1, 2, and completely fails already when k = 3 (or larger).",3.1 Experiment,[0],[0]
"This may be somewhat surprising, as the end-to-end approach optimizes exactly the primary objective, composed of two sub-problems each of which is easily solved on its own, and with no additional irrelevant objectives.",3.1 Experiment,[0],[0]
"We study the experiment from two directions: Theoretically, by analyzing the gradient variance (as in Section 2), for a somewhat idealized version of the experiment, and empirically, by estimating a signalto-noise ratio (SNR) measure of the stochastic gradients used by the algorithm.",3.2 Analysis,[0],[0]
"Both approaches point to a similar issue: With the end-to-end approach, the gradients do not seem to be sufficiently informative for the optimization process to succeed.
",3.2 Analysis,[0],[0]
"Before continuing, we note that a conceptually similar experiment to ours has been reported in [13] (also involving a composition of an image recognition task and a simple Boolean formula, and with qualitatively similar results).",3.2 Analysis,[0],[0]
"However, that experiment came without a formal analysis, and the failure was attributed to local minima.",3.2 Analysis,[0],[0]
"In contrast, our analysis indicates that the problem is not due to local-minima (or saddle points), but from the gradients being non-informative and noisy.
",3.2 Analysis,[0],[0]
"We begin with a theoretical result, which considers our experimental setup under two simplifying assumptions: First, the input is assumed to be standard Gaussian, and second, we assume the labels are generated by a target function of the form hu(xk1)",3.2 Analysis,[0],[0]
=,3.2 Analysis,[0],[0]
"∏k l=1 sign(u
>xl).",3.2 Analysis,[0],[0]
"The first assumption is merely to simplify the analysis (similar results can be shown more generally, but the argument becomes more involved).",3.2 Analysis,[0],[0]
"The second assumption is equivalent to assuming that the labels y(x) of individual images can be realized by a linear predictor, which is roughly the case for simple image labelling task such as ours.
",3.2 Analysis,[0],[0]
"Theorem 3 Let xk1 denote a k-tuple (x1, . . .",3.2 Analysis,[0],[0]
",xk) of input instances, and assume that each xl is i.i.d.",3.2 Analysis,[0],[0]
"stan-
dard Gaussian in Rd.",3.2 Analysis,[0],[0]
"Define
hu(x k 1) = k∏ l=1 sign(u>xl),
and the objective (w.r.t.",3.2 Analysis,[0],[0]
"some predictor pw parameterized by w)
",3.2 Analysis,[0],[0]
F (w) =,3.2 Analysis,[0],[0]
"E xk1
[ `(pw(x k 1), hu(x k 1) ] .
",3.2 Analysis,[0],[0]
"Where the loss function ` is either the square loss `(ŷ, y) =",3.2 Analysis,[0],[0]
"12(ŷ − y) 2 or a classification loss of the form `(ŷ, y) =",3.2 Analysis,[0],[0]
r(ŷ · y) for some 1-Lipschitz function r.,3.2 Analysis,[0],[0]
"Fix some w, and suppose that pw(x) is differentiable w.r.t.",3.2 Analysis,[0],[0]
w and satisfies Exk1,3.2 Analysis,[0],[0]
[ ‖ ∂∂wpw(x k 1‖2 ] ≤ G(w)2.,3.2 Analysis,[0],[0]
Then ifH = {hu :,3.2 Analysis,[0],[0]
"u ∈ Rd, ‖u‖ = 1}, then
Var(H, F,w) ≤",3.2 Analysis,[0],[0]
"G(w)2 ·O
(√ k log(d)
d
)k .
",3.2 Analysis,[0],[0]
The proof is given in Section B.2.,3.2 Analysis,[0],[0]
"The theorem shows that the “signal” regarding hu (or, if applying to our experiment, the signal for learning N (1), had y been drawn uniformly at random from some set of functions overX) decreases exponentially with k.",3.2 Analysis,[0],[0]
"This is similar to the parity result in Section 2, but with an important difference:",3.2 Analysis,[0],[0]
"Whereas the base of the exponent there was 1/2, here it is the much smaller quantity k log(d)/ √ d",3.2 Analysis,[0],[0]
"(e.g. in our experiment, we have k ≤ 4 and d = 282).",3.2 Analysis,[0],[0]
"This indicates that already for very small values of k, the information contained in the gradients about u can become extremely small, and prevent gradient-based methods from succeeding, fully according with our experiment.
",3.2 Analysis,[0],[0]
"To complement this analysis (which applies to an idealized version of our experiment), we consider a related “signal-to-noise” (SNR) quantity, which can be empirically estimated in our actual experiment.",3.2 Analysis,[0],[0]
"To motivate it, note that a key quantity used in the proof of Theorem 3, for estimating the amount of signal carried by the gradient, is the squared norm of the correlation between the gradient of the predictor pw, g(xk1) := ∂ ∂wpw(x k 1) and the target function hu, which we denote by Sigu:
Sigu := ∥∥∥∥∥Exk1",3.2 Analysis,[0],[0]
"[ hu(x k 1)g(x k 1) ]∥∥∥∥∥ 2 .
",3.2 Analysis,[0],[0]
"We will consider the ratio between this quantity and a “noise” term Noiu, i.e. the variance of this correlation over the samples:
",3.2 Analysis,[0],[0]
"Noiu := E xk1 ∥∥∥∥∥hu(xk1)g(xk1)− Exk1 [ hu(x k 1)g(x k 1) ]∥∥∥∥∥ 2 .
",3.2 Analysis,[0],[0]
"Since here the randomness is with respect to the data rather than the target function (as in Theorem 3), we can estimate this SNR ratio in our experiment.",3.2 Analysis,[0],[0]
It is well-known (e.g. [9]) that the amount of noise in the stochastic gradient estimates used by stochastic gradient descent crucially affects its convergence rate.,3.2 Analysis,[0],[0]
"Hence, smaller SNR should be correlated with worse performance.
",3.2 Analysis,[0],[0]
"We empirically estimated this SNR measure, Sigy/Noiy, for the gradients w.r.t.",3.2 Analysis,[0],[0]
the weights of the last layer of N (1) (which potentially learns our intermediate labeling function y) at the initialization point in parameter space.,3.2 Analysis,[0],[0]
The SNR estimate for various values of k are plotted in Figure 4.,3.2 Analysis,[0],[0]
"We indeed see that when k ≥ 3, the SNR appears to approach extremely small values, where the estimator’s noise, and the additional
noise introduced by a finite floating point representation, can completely mask the signal, which can explain the failure in this case.
",3.2 Analysis,[0],[0]
"In Section A in the Appendix, we also present a second, more synthetic, experiment, which demonstrates a case where the decomposition approach directly decreases the stochastic noise in the SGD optimization process, hence benefiting the convergence rate.",3.2 Analysis,[0],[0]
Network architecture choice is a crucial element in the success of deep learning.,4 Architecture and Conditioning,[0],[0]
"New variants and development of novel architectures are one of the main tools for achieving practical breakthroughs [14, 32].",4 Architecture and Conditioning,[0],[0]
"When choosing an architecture, one consideration is how to inject prior knowledge on the problem at hand, improving the network’s expressiveness for that problem, while not dramatically increasing sample complexity.",4 Architecture and Conditioning,[0],[0]
Another aspect involves improving the computational complexity of training.,4 Architecture and Conditioning,[0],[0]
"In this section we formally show how the choice of architecture affects the training time through the lens of the condition number of the problem.
",4 Architecture and Conditioning,[0],[0]
"The study and practice of conditioning techniques, for convex and non-convex problems, gained much attention recently (e.g., [18, 22, 7, 27]).",4 Architecture and Conditioning,[0],[0]
"Here we show how architectural choice may have a dramatic effect on the applicability of better conditioning techniques.
",4 Architecture and Conditioning,[0],[0]
"The learning problem we consider in this section is that of encoding one-dimensional, piecewise linear curves.",4 Architecture and Conditioning,[0],[0]
"We show how different architectures, all of them of sufficient expressive power for solving the problem, have orders-of-magnitude difference in their condition numbers.",4 Architecture and Conditioning,[0],[0]
"In particular, this becomes apparent when considering convolutional vs. fully connected layers.",4 Architecture and Conditioning,[0],[0]
"This sheds a new light over the success of convolutional neural networks, which is generally attributed to their sample complexity benefits.",4 Architecture and Conditioning,[0],[0]
"Moreover, we show how conditioning, applied in conjunction with a better architecture choice, can further decrease the condition number by orders of magnitude.",4 Architecture and Conditioning,[0],[0]
"The direct effect on the convergence rate is analyzed, and is aligned with the significant performance gaps observed empirically.",4 Architecture and Conditioning,[0],[0]
"We also demonstrate how performance may not significantly improve by employing deeper and more powerful architectures, as well as the price that comes with choosing a sub-optimal architecture.",4 Architecture and Conditioning,[0],[0]
"We experiment with various deep learning solutions for encoding the structure of one-dimensional, continuous, piecewise linear (PWL) curves.",4.1 Experiments and Analysis,[0],[0]
"Any PWL curve with k pieces can be written as: f(x) = b +∑k
i=1",4.1 Experiments and Analysis,[0],[0]
ai[x,4.1 Experiments and Analysis,[0],[0]
"− θi]+, where ai is the difference between the slope at the i’th segment and the (i − 1)’th segment.",4.1 Experiments and Analysis,[0],[0]
"For example, the curve below can be parametrized by b = 1, a = (1,−2, 3), θ = (0, 2, 6).
",4.1 Experiments and Analysis,[0],[0]
"The problem we consider is that of receiving a vector of the values of f at x ∈ {0, 1, . . .",4.1 Experiments and Analysis,[0],[0]
", n−1}, namely f := (f(0), f(1), . . .",4.1 Experiments and Analysis,[0],[0]
", f(n−1)), and outputting the values of b, {ai, θi}ki=1.",4.1 Experiments and Analysis,[0],[0]
"We can think of this problem as an encoding problem, since we would like to be able to rebuild f from the values of b, {ai, θi}ki=1.",4.1 Experiments and Analysis,[0],[0]
"Observe that b = f(0), so from now on, let us assume without loss of generality that b = 0.
",4.1 Experiments and Analysis,[0],[0]
"Throughout our experiments, we use n = 100, k = 3.",4.1 Experiments and Analysis,[0],[0]
"We sample {θi}i∈[k] uniformly without replacement from {0, 1, . . .",4.1 Experiments and Analysis,[0],[0]
", n− 1}, and sample each ai i.i.d.",4.1 Experiments and Analysis,[0],[0]
"uniformly from [−1, 1].",4.1 Experiments and Analysis,[0],[0]
"As we assume that each θi is an integer in {0, 1, . . .","4.1.1 Convex Problem, Large Condition Number",[0],[0]
", n","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"− 1}, we can represent {ai, θi}ki=1 as a vector p ∈","4.1.1 Convex Problem, Large Condition Number",[0],[0]
Rn such that pj = 0,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"unless there is some i such that θi = j, and in this case we set pj = ai.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"That is, pj = ∑k i=1 ai 1[θi=j−1].
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
This allows us to formalize the problem as a convex optimization problem.,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Define a matrix W ∈ Rn,n such that Wi,j =","4.1.1 Convex Problem, Large Condition Number",[0],[0]
[i − j + 1]+.,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
It is not difficult to show that f = Wp.,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Moreover, W can be shown to be invertible, so we can extract p from f by p = W−1f .
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"We hence start by attempting to learn this linear transformation directly, using a connected architecture of one layer, with n output channels.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
Let the weights of this layer be denoted Û .,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"We therefore minimize the objective:
min Û E f
[ 1
2 (W−1f − Û f)2
] (3)
where f is sampled according to some distribution.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"As a convex, realizable (by Û = W−1) problem, convergence is guaranteed, and we can explicitly analyze its rate.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"However, perhaps unexpectedly, we observe a very slow rate of convergence to a satisfactory solution, where significant inaccuracies are present at the non-smoothness points.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Figure 5a illustrates the results.
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"To analyze the convergence rate of this approach, and to benchmark the performance of the next set of experiments, we start off by giving an explicit expression for W−1:
Lemma 1 The inverse of W is the matrix U s.t.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Ui,i = Ui+2,i = 1, Ui+1,i = −2, and the rest of the coordinates of U are zero.
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
The proof is given in Appendix B.3.,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Next, we analyze the iteration complexity of SGD for learning the matrix U .","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"To that end, we give an explicit expression for the expected value of the learned weight matrix at each iteration t, denoted as Û t:
Lemma 2 Assume Û0 = 0, and that Ef [Uff>U>] = λI for some λ.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Then, running SGD with learning rate η over objective 3 for t iterations yields:
E Ût = ηλW> t−1∑ i=0 (I − ηλWW>)i
The proof is given in Appendix B.4.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Note that the assumption that Ef [Uff>U>] = λI holds under the distributional assumption over the curves, as changes of direction in the curve are independent, and are sampled each time from the same distribution.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"The following theorem establishes a lower bound on ‖E Ût+1 − U‖, which by Jensen’s inequality, implies a lower bound on E ‖Ût+1 − U‖, the expected distance of Ût+1 from U .","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Note that the lower bound holds even if we use all the data for updating (that is, gradient descent and not stochastic gradient descent).
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
Theorem 4 Let W = QSV > be the singular value decomposition of W .,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"If η λS21,1 ≥ 1 then E Ût+1 diverges.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Otherwise, we have
t+ 1 ≤ S21,1
2S2n,n ⇒ ‖E Ût+1 − U‖ ≥ 0.5 ,
where the norm is the spectral norm.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Furthermore, the condition number S21,1 S2n,n (where S1,1, Sn,n are the top and bottom singular values of W ) is Ω(n3.5).
","4.1.1 Convex Problem, Large Condition Number",[0],[0]
The proof is given in Appendix B.5.,"4.1.1 Convex Problem, Large Condition Number",[0],[0]
"The theorem implies that the condition number of W, and hence, the number of GD iterations required for convergence, scales quite poorly with n. In the next subsection, we will try to decrease the condition number of the problem.","4.1.1 Convex Problem, Large Condition Number",[0],[0]
"Examining the explicit expression for U given in Lemma 1, we see that U f can be written as a onedimensional convolution of f with the kernel [1,−2, 1].",4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
"Therefore, the mapping from f to p is realizable using a convolutional layer.
",4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
"Empirically, convergence to an accurate solution is faster using this architecture.",4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
Figure 5b illustrates a few examples.,4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
"To theoretically understand the benefit of using a convolution, from the perspective of the required number of iterations for training, we will consider the new problem’s condition number, providing understanding of the gap in training time.",4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
In the previous section we saw that GD requires Ω(n3.5) iterations to learn the full matrixU .,4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
"In the appendix (sections B.6 and B.7) we show that under some mild assumptions, the condition number is only Θ(n3), and GD requires only that order of iterations to learn the optimal filter [1,−2, 1].",4.1.2 Improved Condition Number through Convolutional Architecture,[0],[0]
"In Section 4.1.2, despite observing an improvement from the fully connected architecture, we saw that GD still requires Ω(n3) iterations even for the simple problem of learning the filter [1,−2, 1].",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"This motivates an application of additional conditioning techniques, in the hope for extra performance gains.
",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"First, let us explicitly represent the convolutional architecture as a linear regression problem.",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"We perform Vec2Row operation on f as follows: given a sample f , construct a matrix, F , of size n× 3, such that the t’th row of F is [ft−1, ft, ft+1].",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"Then, we obtain a vanilla linear regression problem in R3, with the filter [1,−2, 1]
as its solution.",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"Given a sample f , we can now approximate the correlation matrix of F , denotedC ∈ R3,3, by setting Ci,j = Ef ,t[ft−2+ift−2+j ].",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"We then calculate the matrix C−1/2 and replace every instance (namely, a row of F )",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"[ft−1, ft, ft+1] by the instance [ft−1, ft, ft+1]C−1/2.",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"By construction, the correlation matrix of the resulting instances is approximately the identity matrix, hence the condition number is approximately 1.",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"It follows (see again Appendix B.6) that SGD converges using order of log(1/ ) iterations, independently of n. Empirically, we quickly converge to extremely accurate results, illustrated in Figure 5c.
",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
"We note that the use of a convolution architecture is crucial for the efficiency of the conditioning; had the dimension of the problem not been reduced so dramatically, the difficulty of estimating a large n×n correlation matrix scales strongly with n, and furthermore, its inversion becomes a costly operation.",4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
The combined use of a better architecture and of conditioning is what allows us to gain this dramatic improvement.,4.1.3 Additional Improvement through Explicit Conditioning,[0],[0]
The solution arrived at in Section 4.1.3 indicates that a suitable architecture choice and conditioning scheme can provide training time speedups of multiple orders of magnitude.,4.1.4 Perhaps I should use a deeper network?,[0],[0]
"Moreover, the benefit of reducing the number of parameters, in the transition from a fully connected architecture to a convolutional one, is shown to be helpful in terms of convergence time.",4.1.4 Perhaps I should use a deeper network?,[0],[0]
"However, we should not rule out the possibility that a deeper, wider network will not suffer from the deficiencies analyzed above for the convex case.
",4.1.4 Perhaps I should use a deeper network?,[0],[0]
"Motivated by the success of deep auto-encoders, we experiment with a deeper architecture for encoding f .",4.1.4 Perhaps I should use a deeper network?,[0],[0]
"Namely, we minimize minv1,v2",4.1.4 Perhaps I should use a deeper network?,[0],[0]
Ef,4.1.4 Perhaps I should use a deeper network?,[0],[0]
"[(f −Mv2(Nv1(f)))2], Where Nv1 ,Mv2 are deep networks parametrized by their weight vectors v1,v2, with the output of N being of dimension 2k, enough for realization of the encoding problem.",4.1.4 Perhaps I should use a deeper network?,[0],[0]
"Each of the two networks has three layers with ReLU activations, except for the output layer of M having a linear activation.",4.1.4 Perhaps I should use a deeper network?,[0],[0]
"The dimensions of the layers are, 500, 100, 2k for N , and 100, 100, n for M .
Aligned with the intuition gained through the previous experiments, we observe that additional expressive power, when unnecessary, does not solve inherent optimization problems, as this stronger Auto-Encoder fails to capture the fine details of f at its non-smooth points.",4.1.4 Perhaps I should use a deeper network?,[0],[0]
See Figure 5d for examples.,4.1.4 Perhaps I should use a deeper network?,[0],[0]
"We now examine a different aspect of gradient-based learning which poses difficulties for optimization: namely, flatness of the loss surface due to saturation of the activation functions, leading to vanishing gradients and a slow-down of the training process.",5 Flat Activations,[0],[0]
"This problem is amplified in deeper architectures, since it is likely that the backpropagated message to lower layers in the architecture would vanish due to a saturated activation somewhere along the way.",5 Flat Activations,[0],[0]
"This is a major problem when using sigmoids as a gating mechanisms in Recurrent Neural Networks such as LSTMS and GRUs [12, 4].
",5 Flat Activations,[0],[0]
"While non-local search-based optimization for large scale problems seems to be beyond reach, variants on the gradient update, whether by adding momentum, higher order methods, or normalized gradients, are quite successful, leading to consideration of update schemes deviating from “vanilla” gradient updates.
",5 Flat Activations,[0],[0]
"In this section, we consider a family of activation functions which amplify the “vanishing gradient due to saturated activation” problem; they are piecewise flat.",5 Flat Activations,[0],[0]
"Using such activations in a neural network architecture will result in a gradient equal to 0, which will be completely useless.",5 Flat Activations,[0],[0]
"We consider different ways to implement, approximate or learn such activations, such that the error will effectively propagate through them.",5 Flat Activations,[0],[0]
"Using a different variant of a local search-based update, based on [20, 19] , we arrive at
an efficient solution.",5 Flat Activations,[0],[0]
Convergence guarantees exist for a one-layer architecture.,5 Flat Activations,[0],[0]
We leave further study of deeper networks to future work.,5 Flat Activations,[0],[0]
Consider the following optimization setup.,5.1 Experimental Setup,[0],[0]
The sample space X ⊂ Rd is symmetrically distributed.,5.1 Experimental Setup,[0],[0]
"The target function y : Rd → R is of the form y(x) = u(v∗>x + b∗), where v∗ ∈ Rd, b∗ ∈ R, and u : R → R is a monotonically non-decreasing function.",5.1 Experimental Setup,[0],[0]
"The objective of the optimization problem is given by:
min w E x",5.1 Experimental Setup,[0],[0]
"[` (u(Nw(x)), y(x))]
where Nw is some neural network parametrized by w, and ` is some loss function (for example, the squared or absolute difference).
",5.1 Experimental Setup,[0],[0]
"For the experiments, we use u of the form: u(r) = z0 + ∑ i∈[55] 1[r>zi] · (zi − zi−1) ,
where z0 < z1 < . . .",5.1 Experimental Setup,[0],[0]
< z55 are known.,5.1 Experimental Setup,[0],[0]
"In words, given r, the function rounds down to the nearest zi.",5.1 Experimental Setup,[0],[0]
We also experiment with normally distributed X .,5.1 Experimental Setup,[0],[0]
"Our theoretical analysis is not restricted to u of this specific form, nor to normal X .",5.1 Experimental Setup,[0],[0]
"All figures are found in Figure 6.
",5.1 Experimental Setup,[0],[0]
"Of course, applying gradient-based methods to solve this problem directly, is doomed to fail as the derivative of u is identically 0.",5.1 Experimental Setup,[0],[0]
Is there anything which can be done instead?,5.1 Experimental Setup,[0],[0]
"We start off by trying to approximate u using a non flat function ũ defined by ũ(r) = z0 + ∑ i∈[55] (zi − zi−1) · σ(c · (r − zi)),
where c is some constant, and σ is the sigmoid function σ(z) =",5.2 Non-Flat Approximation Experiment,[0],[0]
(1 + exp(−z))−1.,5.2 Non-Flat Approximation Experiment,[0],[0]
"Intuitively, we approximate the “steps” in u using a sum of sigmoids, each of amplitude corresponding to the step’s height, and centered at the step’s position.",5.2 Non-Flat Approximation Experiment,[0],[0]
This is similar to the motivation for using sigmoids as activation functions and as gates in LSTM cells — a non-flat approximation of the step function.,5.2 Non-Flat Approximation Experiment,[0],[0]
"Below is an example for u, and its approximation ũ.
u ũ
The objective is the expected squared loss, propagated through ũ, namely
min v,b E x
[( ũ(v>x + b)− y(x) )2] .
",5.2 Non-Flat Approximation Experiment,[0],[0]
"Although the objective is not completely flat, and is continuous, it suffers from the flatness and non continuity deficiencies of the original u, and training using this objective is much slower, and sometimes completely failing.",5.2 Non-Flat Approximation Experiment,[0],[0]
"In particular, sensitivity to the initialization of bias term is observed, where the wrong initialization can cause the starting point to be in a very wide flat region of u, and hence a very flat region of ũ.",5.2 Non-Flat Approximation Experiment,[0],[0]
"Next, we attempt to solve the problem using improper learning, with the objective now being:
min w E",5.3 End-to-End Experiment,[0],[0]
"x
[ (Nw(x)− y(x))2 ] where Nw is a network parametrized by its weight vector w.",5.3 End-to-End Experiment,[0],[0]
"We use a simple architecture of four fully connected layers, the first three with ReLU activations and 100 output channels, and the last, with only one output channel and no activation function.
",5.3 End-to-End Experiment,[0],[0]
"As covered in Section 4, difficulty arises when regressing to non smooth functions.",5.3 End-to-End Experiment,[0],[0]
"In this case, with u not even being continuous, the inaccuracies in capturing the non continuity points are brought to the forefront.",5.3 End-to-End Experiment,[0],[0]
"Moreover, this solution has its extra price in terms of sample complexity, training time, and test time, due to the use of a much larger than necessary network.",5.3 End-to-End Experiment,[0],[0]
An advantage is of course the minimal prior knowledge about u which is required.,5.3 End-to-End Experiment,[0],[0]
"While this approach manages to find a reasonable solution, it is far from being perfect.",5.3 End-to-End Experiment,[0],[0]
"In this experiment, we approach the problem as a general multi-class classification problem, with each value of the image of u is treated as a separate class.",5.4 Multi-Class Experiment,[0],[0]
"We use a similar architecture to that of the end-to-end experiment, with one less hidden layer, and with the final layer outputting 55 outputs, each corresponding to one of the steps defined by the zis.",5.4 Multi-Class Experiment,[0],[0]
"A problem here is the inaccuracies at the boundaries between classes, due to the lack of structure imposed over the predictor.",5.4 Multi-Class Experiment,[0],[0]
The fact that the linear connection between x and the input to u is not imposed through the architecture results in “blurry” boundaries.,5.4 Multi-Class Experiment,[0],[0]
"In addition, the fact that we rely on an “improper” approach, in the sense that we ignore the ordering imposed by u, results in higher sample complexity.",5.4 Multi-Class Experiment,[0],[0]
"Let us go back to a direct formulation of the problem, in the form of the objective function
min w F (w) = E",5.5 The “Forward-Only” Update Rule,[0],[0]
"x
[ (u(w>x)− y(x))2 ] where y(x) = u(v∗>x).",5.5 The “Forward-Only” Update Rule,[0],[0]
The gradient update rule in this case is w(t+1) = w(t),5.5 The “Forward-Only” Update Rule,[0],[0]
"− η∇F (w(t)), where for our objective we have
∇F (w) = E x
[ (u(w>x)− y(x))",5.5 The “Forward-Only” Update Rule,[0],[0]
"· u′(w>x) · x ] Since u′ is zero a.e., the gradient update is meaningless.",5.5 The “Forward-Only” Update Rule,[0],[0]
"[20, 19] proposed to replace the gradient with the following:
∇̃F (w) =",5.5 The “Forward-Only” Update Rule,[0],[0]
"E x
[ (u(w>x)− y(x))",5.5 The “Forward-Only” Update Rule,[0],[0]
"· x ] (4)
In terms of the backpropagation algorithm, this kind of update can be interpreted as replacing the backpropagation message for the activation function u with an identity message.",5.5 The “Forward-Only” Update Rule,[0],[0]
"For notation simplicity, we omitted the bias terms b, b∗, but the same Forward-only concept is applied to them too.
",5.5 The “Forward-Only” Update Rule,[0],[0]
"This method empirically achieves the best results, both in terms of final accuracy, training time, and test time cost.",5.5 The “Forward-Only” Update Rule,[0],[0]
"As mentioned before, the method is due to [20, 19], where it is proven to converge to an -optimal solution in O(L2/ 2), under the additional assumptions that the function u is L-Lipschitz, and that w is constrained to have bounded norm.",5.5 The “Forward-Only” Update Rule,[0],[0]
"For completeness, we provide a short proof in Appendix B.8.",5.5 The “Forward-Only” Update Rule,[0],[0]
"In this paper, we considered different families of problems, where standard gradient-based deep learning approaches appear to suffer from significant difficulties.",6 Summary,[0],[0]
"Our analysis indicates that these difficulties are not necessarily related to stationary point issues such as spurious local minima or a plethora of saddle points, but rather more subtle issues: Insufficient information in the gradients about the underlying target function; low SNR; bad conditioning; or flatness in the activations (see Figure 7 for a graphical illustration).",6 Summary,[0],[0]
"We consider it as a first step towards a better understanding of where standard deep learning methods might fail, as well as what approaches might overcome these failures.
",6 Summary,[0],[0]
Acknowledgements:,6 Summary,[0],[0]
"This research is supported in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and by the European Research Council (TheoryDL project).",6 Summary,[0],[0]
OS was also supported in part by an FP7 Marie Curie CIG grant and an Israel Science Foundation grant 425/13.,6 Summary,[0],[0]
"A.1 Experiment
For this experiment, consider the problem of training a predictor, which given a “positive media reference” x to a certain stock option, will distribute our assets between the k = 500 stocks in the S&P500 index in some manner.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"One can, again, come up with two rather different strategies for solving the problem.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
• An end-to-end approach: train a deep network Nw that given x outputs a distribution over the k stocks.,A Reduced Noise through Decomposition - Experiment,[0],[0]
"The objective for training is maximizing the gain obtained by allocating our money according to this distribution.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"• A decomposition approach: train a deep network Nw that given x outputs a single stock, y ∈",A Reduced Noise through Decomposition - Experiment,[0],[0]
"[k], whose future gains are the most positively correlated to x.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"Of course, we may need to gather extra labeling for training Nw based on this criterion.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
We make the (non-realistic) assumption that every instance of media reference is strongly and positively correlated to a single stock y ∈,A Reduced Noise through Decomposition - Experiment,[0],[0]
"[k], and it has no correlation with future performance of other stocks.",A Reduced Noise through Decomposition - Experiment,[0],[0]
This obviously makes our problem rather toyish; the stock exchange and media worlds have highly complicated correlations.,A Reduced Noise through Decomposition - Experiment,[0],[0]
"However, it indeed arises from, and is motivated by, practical problems.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"To examine the problem in a simple and theoretically clean manner, we design a synthetic experiment defined by the following optimization problem: Let X × Z ⊂",A Reduced Noise through Decomposition - Experiment,[0],[0]
Rd,A Reduced Noise through Decomposition - Experiment,[0],[0]
"× {±1}k be the sample space, and let y : X",A Reduced Noise through Decomposition - Experiment,[0],[0]
→ [k] be some labelling function.,A Reduced Noise through Decomposition - Experiment,[0],[0]
"We would like to learn a mapping Nw : X → Sk−1, with the objective being:
min w L(w) := E x,z∼X×Z
[ −z>Nw(x) ] .
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"To connect this to our story, Nw(x) is our asset distribution, z indicates the future performance of the stocks, and thus, we are seeking minimization of our expected future negative gains, or in other words, maximization of expected profit.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"We further assume that given x, the coordinate zy(x) equals 1, and the rest of the coordinates are sampled i.i.d from the uniform distribution over {±1}.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"Whereas in Section 3.1’s experiment, the difference between the end-to-end and decomposition approaches could be summarized by a different loss function choice, in this experiment, the difference boils down to the different gradient estimators we would use, where we are again taking as a given fact that exact gradient computations are expensive for large-scale problems, implying the method of choice to be SGD.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"For the purpose of the experimental discussion, let us write the two estimators explicitly as two unconnected update rules.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"We will later analyze their (equal) expectation.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"For an end-to-end approach, we sample a pair (x, z), and use ∇w(−z>Nw(x)) as a gradient estimate.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"It is clear that this is an unbiased estimator of the gradient.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"For a decomposition approach, we sample a pair (x, z), completely ignore z, and instead, pay the extra costs and gather the required labelling to get y(x).",A Reduced Noise through Decomposition - Experiment,[0],[0]
We will then use ∇w(−e>y(x)Nw(x)) as a gradient estimate.,A Reduced Noise through Decomposition - Experiment,[0],[0]
"It will be shown later that this too is an unbiased estimator of the gradient.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"Figure 8 clearly shows that optimizing using the end-to-end estimator is inferior to working with the decomposition one, in terms of training time and final accuracy, to the extent that for large k, the end-to-end estimator cannot close the gap in performance in reasonable time.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"A.2 Analysis
We examine the experiment from a SNR perspective.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"First, let us show that indeed, both estimators are unbiased estimators of the true gradient.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"As stated above, it is clear, by definition of L, that the end-toend estimator is an unbiased estimator of ∇wL(w).",A Reduced Noise through Decomposition - Experiment,[0],[0]
"To observe this is also the case for the decomposition estimator, we write:
∇wL(w) = ∇w E x,z
[−z>Nw(x)]
=E x",A Reduced Noise through Decomposition - Experiment,[0],[0]
"[ E z|x
[∇w(−z>Nw(x))",A Reduced Noise through Decomposition - Experiment,[0],[0]
"]]
(1) =",A Reduced Noise through Decomposition - Experiment,[0],[0]
"E
x",A Reduced Noise through Decomposition - Experiment,[0],[0]
"[ E z|x
[−z>∇w(Nw(x))",A Reduced Noise through Decomposition - Experiment,[0],[0]
]] (2) =,A Reduced Noise through Decomposition - Experiment,[0],[0]
E x,A Reduced Noise through Decomposition - Experiment,[0],[0]
[−e>y(x)∇w(Nw(x)),A Reduced Noise through Decomposition - Experiment,[0],[0]
"]
where (1) follows from the chain rule, and (2) from the assumption on the distribution of z given x.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"It is now easy to see that the decomposition estimator is indeed a (different) unbiased estimator of the gradient, hence the “signal” is the same.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"Intuition says that when a choice between two unbiased estimators is presented, we should choose the one with the lower variance.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"In our context, [9] showed that when running SGD (even on non-convex objectives), arriving at a point where ‖∇wL(w)‖2 ≤ requires order of ν̄2/ 2 iterations, where
ν̄2 = max t E x,q ‖∇tw(x, q)‖2 − ‖∇wL(w(t))‖2,
wt is the weight vector at time t, q is sampled along with x (where it can be replaced by z or y(x), in our
experiment), and ∇tw is the unbiased estimator for the gradient.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"This serves as a motivation for analyzing the problem through this lens.
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"Motivated by [9]’s result, and by our results regarding Section 3.1, we examine the quantity Ex,q ‖∇tw(x, q)‖2, or “noise”, explicitly.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"For the end-to-end estimator, this quantity equals
E x,z ‖",A Reduced Noise through Decomposition - Experiment,[0],[0]
"− z>∇wNw(x)‖2 = E x,z ‖ − k∑ i=1",A Reduced Noise through Decomposition - Experiment,[0],[0]
"zi∇wNw(x)i‖2
Denoting by Gi := ∇wNw(x)i, we get:
= E x E z|x ‖",A Reduced Noise through Decomposition - Experiment,[0],[0]
− k∑ i=1,A Reduced Noise through Decomposition - Experiment,[0],[0]
ziGi‖2 = E x k∑ i=1,A Reduced Noise through Decomposition - Experiment,[0],[0]
"‖Gi‖2 (5)
",A Reduced Noise through Decomposition - Experiment,[0],[0]
"where the last equality follows from expanding the squared sum, and taking expectation over z, while noting that mixed terms cancel out (from independence of z’s coordinates), and that z2i = 1 for all i.
As for the decomposition estimator, it is easy to see that
E x ‖",A Reduced Noise through Decomposition - Experiment,[0],[0]
− e>y(x)∇wNw(x)‖ 2 = E x ‖Gy(x)‖2.,A Reduced Noise through Decomposition - Experiment,[0],[0]
"(6)
Observe that in 5 we are summing up, per x, k summands, compared to the single element in 6.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"When randomly initializing a network it is likely that the values of ‖Gi‖2 are similar, hence we obtain that at the beginning of training, the variance of the end-to-end estimator is roughly k times larger than that of the decomposition estimator.",A Reduced Noise through Decomposition - Experiment,[0],[0]
"B.1 Proof of Theorem 1
",B Proofs,[0],[0]
"Proof Given two square-integrable functions f, g on an Euclidean space Rn, let 〈f, g〉L2 = Ex[f(x)g(x)] and ‖f‖L2 = √ Ex[f2(x)] denote inner product and norm in the L2 space of square-integrable functions (with respect to the relevant distribution).",B Proofs,[0],[0]
"Also, define the vector-valued function
g(x) = ∂
∂w pw(x),
and let g(x) =",B Proofs,[0],[0]
"(g1(x), g2(x), . . .",B Proofs,[0],[0]
", gn(x))",B Proofs,[0],[0]
"for real-valued functions g1, . . .",B Proofs,[0],[0]
", gn.",B Proofs,[0],[0]
"Finally, let Eh denote an expectation with respect to h chosen uniformly at random fromH. Let |H| = d.
We begin by proving the result for the squared loss.",B Proofs,[0],[0]
"To prove the bound, it is enough to show that Eh ‖∇Fh(w)−a‖2 ≤",B Proofs,[0],[0]
"G 2 |H| for any vector a independent of h. In particular, let us choose a = Ex [pw(x)g(x)].
",B Proofs,[0],[0]
"We thus bound the following:
E h ‖∇Fh(w)− E x",B Proofs,[0],[0]
[pw(x)g(x)],B Proofs,[0],[0]
‖2 = E h ‖E x,B Proofs,[0],[0]
[(pw(x)− h(x)),B Proofs,[0],[0]
g(x)]− E x,B Proofs,[0],[0]
"[pw(x)g(x)] ‖2
= E h",B Proofs,[0],[0]
‖E,B Proofs,[0],[0]
x,B Proofs,[0],[0]
[h(x)g(x)] ‖2 = E h n∑ j=1,B Proofs,[0],[0]
"( E x [h(x)gj(x)] )2
= E h n∑ j=1 〈h, gj〉2L2 = n∑ j=1
( 1
|H| d∑ i=1",B Proofs,[0],[0]
"〈hi, gj〉2L2 ) (∗) ≤
n∑ j=1",B Proofs,[0],[0]
( 1 |H| ‖gj‖2L2 ),B Proofs,[0],[0]
= 1 |H| n∑ j=1 E x,B Proofs,[0],[0]
"[g2j (x)]
= 1
|H| E x
[ ‖g(x)‖2 ] ≤ G(w) 2
|H| ,
where (∗) follows from the functions in H being mutually orthogonal, and satisfying ‖h‖L2 ≤ 1 for all h ∈ H.
To handle a classification loss, note that by its definition and the fact that h(x) ∈ {−1,+1},
∇Fh(w) =",B Proofs,[0],[0]
"E x
[ r′(h(x)pw(x)) · ∂
∂w pw(x) ]",B Proofs,[0],[0]
"= E
x
[( r′(pw(x))",B Proofs,[0],[0]
"+ r
′(−pw(x))",B Proofs,[0],[0]
2 + h(x) ·,B Proofs,[0],[0]
r ′(pw(x))− r′(−pw(x)) 2 ) · ∂ ∂w pw(x) ],B Proofs,[0],[0]
"= E
x
[ r′(pw(x))",B Proofs,[0],[0]
"+ r
′(−pw(x))",B Proofs,[0],[0]
"2 · ∂ ∂w pw(x)
]",B Proofs,[0],[0]
"+ E
x
[ h(x) · ( r′(pw(x))− r′(−pw(x))
2
) · ∂ ∂w pw(x) ] .
",B Proofs,[0],[0]
Letting g(x) =,B Proofs,[0],[0]
"( r′(pw(x))−r′(−pw(x))
2
) ·",B Proofs,[0],[0]
∂∂wpw(x) (which satisfies Ex[‖g(x)‖,B Proofs,[0],[0]
"2] ≤ G2 since r is 1-Lipschitz)
and a = Ex [ r′(pw(x))+r′(−pw(x)) 2 · ∂ ∂wpw(x) ]",B Proofs,[0],[0]
"(which does not depend on h), we get that
E h ‖∇Fh(w)− a‖2 = E h ‖E",B Proofs,[0],[0]
x,B Proofs,[0],[0]
"[h(x)g(x)]‖2.
",B Proofs,[0],[0]
"Proceeding now exactly in the same manner as the squared loss case, the result follows.
",B Proofs,[0],[0]
"B.2 Proof of Theorem 3
",B Proofs,[0],[0]
"Proof We first state and prove two auxiliary lemmas.
",B Proofs,[0],[0]
"Lemma 3 Let h1, . . .",B Proofs,[0],[0]
", hn be real-valued functions on some Euclidean space, which belong to some weighted L2 space.",B Proofs,[0],[0]
"Suppose that ‖hi‖L2 = 1 and maxi 6=j |〈hi, hj〉L2 | ≤ c.",B Proofs,[0],[0]
"Then for any function g on the same domain,
1
n n∑ i=1",B Proofs,[0],[0]
"〈hi, g〉2L2 ≤",B Proofs,[0],[0]
"‖g‖ 2 L2 ( 1 n + c ) .
",B Proofs,[0],[0]
"Proof For simplicity, suppose first that the functions are defined over some finite domain equipped with a uniform distribution, so that h1, . . .",B Proofs,[0],[0]
", hn and g can be thought of as finite-dimensional vectors, and the L2 inner product and norm reduce to the standard inner product and norm in Euclidean space.",B Proofs,[0],[0]
"Let H = (h1, . . .",B Proofs,[0],[0]
", hn) denote the matrix whose i-th column is hi.",B Proofs,[0],[0]
"Then
n∑ i=1
〈hi, g〉2 =",B Proofs,[0],[0]
"g> (
n∑ i=1",B Proofs,[0],[0]
hih >,B Proofs,[0],[0]
"i
)",B Proofs,[0],[0]
"g = g>HH>g ≤ ‖g‖2‖HH>‖ = ‖g‖2‖H>H‖,
where ‖·‖ for a matrix denotes the spectral norm.",B Proofs,[0],[0]
"SinceH>H is simply the n×nmatrix with entry 〈hi, hj〉 in location i, j, we can write it as I + M , where I is the n × n identity matrix, and M is a matrix with 0 along the main diagonal, and entries of absolute value at most c otherwise.",B Proofs,[0],[0]
"Therefore, letting ‖ · ‖F denote the Frobenius norm, we have that the above is at most
‖g‖2",B Proofs,[0],[0]
(‖I‖+ ‖M‖) ≤ ‖g‖2,B Proofs,[0],[0]
(1 + ‖M‖F ) = ‖g‖2,B Proofs,[0],[0]
"(1 + cn) ,
from which the result follows.",B Proofs,[0],[0]
"Finally, it is easily verified that the same proof holds even when h1, . . .",B Proofs,[0],[0]
", hn, g are functions over some Euclidean space, belonging to some weighted L2 space.",B Proofs,[0],[0]
"In that case, H is a bounded linear operator, and it holds that ‖H∗H‖ = ‖H‖2 = ‖H∗‖2 = ‖HH∗‖ where H∗ is the Hermitian adjoint of H and the norm is the operator norm.",B Proofs,[0],[0]
"The rest of the proof is essentially identical.
",B Proofs,[0],[0]
Lemma 4,B Proofs,[0],[0]
"If w,v are two unit vectors in Rd, and x is a standard Gaussian random vector, then∣∣∣E x [ sign(w>x)sign(v>x)",B Proofs,[0],[0]
"]∣∣∣ ≤ |〈w,v〉| Proof Note that w>x,v>x are jointly zero-mean Gaussian, each with variance 1 and with covariance E[w>xx>v] =",B Proofs,[0],[0]
"w>v. Therefore,
E x
[ sign(w>x)sign(v>x) ]",B Proofs,[0],[0]
"= Pr(w>x ≥ 0,v>x ≥ 0) + Pr(w>x ≤ 0,v>x ≤ 0)
",B Proofs,[0],[0]
"− Pr(w>x ≥ 0,v>x ≤ 0)− Pr(w>x ≤ 0,v>x ≥ 0) = 2",B Proofs,[0],[0]
"Pr(w>x ≥ 0,v>x ≥ 0)− 2",B Proofs,[0],[0]
"Pr(w>x ≥ 0,v>x ≤ 0),
which by a standard fact on the quadrant probability of bivariate normal distributions, equals
2
( 1
4 +
sin−1(w>v)
2π
)",B Proofs,[0],[0]
"− 2 ( cos−1(w>v)
2π
) = 1
2 +
1
π
( sin−1(w>v)− cos−1(w>v) )",B Proofs,[0],[0]
"= 1
2 +
1
π
( 2 sin−1(w>v)−",B Proofs,[0],[0]
"π
2
) = 2 sin−1(w>v)
π .
",B Proofs,[0],[0]
"The absolute value of the above can be easily verified to be upper bounded by |w>v|, from which the result follows.
",B Proofs,[0],[0]
"With these lemmas at hand, we turn to prove our theorem.",B Proofs,[0],[0]
"By a standard measure concentration argument, we can find dk unit vectors u1,u2, . . .",B Proofs,[0],[0]
",udk in Rd such that their inner product is at most
O( √ k log(d)/d) (where the O(·) notation is w.r.t. d).",B Proofs,[0],[0]
"This induces dk functions hu1 , hu2 , . . .",B Proofs,[0],[0]
", hudk where
hu(x1, . . .",B Proofs,[0],[0]
",xk) =",B Proofs,[0],[0]
"∏k l=1 sign(u
>xl).",B Proofs,[0],[0]
Their L2 norm (w.r.t.,B Proofs,[0],[0]
the distribution over xk1 =,B Proofs,[0],[0]
"(x1, . . .",B Proofs,[0],[0]
",xk)) is 1, as they take values in {−1,+1}.",B Proofs,[0],[0]
"Moreover, since x1, . . .",B Proofs,[0],[0]
",xk are i.i.d.",B Proofs,[0],[0]
"standard Gaussian, we have by Lemma 4 that for any i 6= j,
〈hui , huj 〉L2 = ∣∣∣∣∣E",B Proofs,[0],[0]
[ k∏ l=1 sign(u>i xl) k∏,B Proofs,[0],[0]
l=1 sign(u>j xl),B Proofs,[0],[0]
],B Proofs,[0],[0]
"∣∣∣∣∣ =
∣∣∣∣∣ k∏ l=1 E [ sign(u>i xl)sign(u > j xl) ]∣∣∣∣∣ = ∣∣∣E",B Proofs,[0],[0]
"[sign(u>i xl)sign(u>j xl)]∣∣∣k
≤ |u>i uj |k ≤",B Proofs,[0],[0]
"O
(√ k log(d)
d
)k .
",B Proofs,[0],[0]
"Using this and Lemma 3, we have that for any function g,
1
dk dk∑ i=1",B Proofs,[0],[0]
"〈hui , g〉2L2 ≤",B Proofs,[0],[0]
‖g‖ 2 L2 ·  1 dk +O (√ k log(d) d ),B Proofs,[0],[0]
"k ≤ ‖g‖2L2 ·O (√ k log(d) d )k .
",B Proofs,[0],[0]
"Moreover, since this bound is derived based only on an inner product condition between u1, . . .",B Proofs,[0],[0]
",udk , the same result would hold for Uu1, . . .",B Proofs,[0],[0]
", Uudk where U is an arbitrary orthogonal matrix, and in particular if we pick it uniformly at random:
E U  1 dk dk∑ i=1",B Proofs,[0],[0]
"〈hUui , g〉2L2  ≤ ‖g‖2L2 · ( 1 dk +O",B Proofs,[0],[0]
"(√ k log(d) d )) .
",B Proofs,[0],[0]
"Now, note that for any fixed i, Uui is uniformly distributed on the unit sphere, so the left hand side simply equals Eu [ 〈hu, g〉2L2 ] , and we get
E u",B Proofs,[0],[0]
"[ 〈hu, g〉2L2 ] ≤ ‖g‖2",B Proofs,[0],[0]
"·O
(√ k log(d)
d
)k .",B Proofs,[0],[0]
"(7)
With this key inequality at hand, the proof is now very similar to the one of Theorem 1.",B Proofs,[0],[0]
"Given the predictor pw(xk1), where w ∈ Rn, define the vector-valued function g(xk1) = ∂∂wpw(x k 1), and let g(x k 1) =",B Proofs,[0],[0]
"(g1(x k 1), g2(x k 1), . . .",B Proofs,[0],[0]
", gn(x k 1)) for real-valued functions g1, . . .",B Proofs,[0],[0]
", gn.",B Proofs,[0],[0]
"To prove the bound, it is enough to upper bound Eu ‖∇Fu(w)",B Proofs,[0],[0]
"− a‖2 for any vector a independent of u. In particular, let us choose a =
Exk1",B Proofs,[0],[0]
[ pw(x k 1)g(x k 1) ] .,B Proofs,[0],[0]
"We thus bound the following:
E u ‖∇Fu(w)− E
xk1
[ pw(x k 1)g(x k 1) ]",B Proofs,[0],[0]
"‖2 = E
u ‖ E xk1
",B Proofs,[0],[0]
[( pw(x k 1)− hu(xk1) ) g(xk1) ],B Proofs,[0],[0]
"− E
xk1
[ pw(x k 1)g(x k 1) ]",B Proofs,[0],[0]
"‖2
= E u ‖ E xk1
[ hu(x k 1)g(x k 1) ]",B Proofs,[0],[0]
"‖2 = E
u n∑ j=1 ( E xk1",B Proofs,[0],[0]
"[ hu(x k 1)gj(x k 1) ])2
= E u n∑ j=1 〈hu, gj〉2L2 = n∑ j=1 E u 〈hu, gj〉2L2
(∗) ≤ n∑ j=1 ‖gj‖2 ·O
(√ k log(d)
d
)",B Proofs,[0],[0]
"k =
n∑ j=1 E xk1",B Proofs,[0],[0]
"[g2j (x k 1)] ·O
(√ k log(d)
d
)k
= E xk1 ‖g(xk1)‖2 ·O
(√ k log(d)
d
)k ≤ G(w)2 ·O (√ k log(d)
d
)k ,
where (∗) follows from (7).",B Proofs,[0],[0]
"By definition of Var(H, F,w), the result follows.",B Proofs,[0],[0]
"Generalization for the classification loss is obtained in the exact same way to the one used in the proof of Theorem 1.
",B Proofs,[0],[0]
"B.3 Proof of lemma 1
Proof (UW )i,j = ∑ t Ui,tWt,j = Wi,j − 2Wi−1,j +Wi−2,j
",B Proofs,[0],[0]
"If i ≥ j + 1 then Wi,j+Wi−2,j2 = Wi−1,j and therefore the above is clearly zero.",B Proofs,[0],[0]
If i < j then all the values of W are zeros.,B Proofs,[0],[0]
"Finally, if i = j we obtain 1.",B Proofs,[0],[0]
"This concludes our proof.
",B Proofs,[0],[0]
B.4 Proof of lemma,B Proofs,[0],[0]
"2
Proof Given a sample f , and that our current weight matrix is Û , let p = W−1f .",B Proofs,[0],[0]
"The loss function on f is given by
1 2 ‖Û f − p‖2
The gradient w.r.t.",B Proofs,[0],[0]
Û is ∇ =,B Proofs,[0],[0]
"(Û f − p)f> = Ûff> − pf>
We obtain that the update rule is Ût+1 = Ût",B Proofs,[0],[0]
− η ( Ûtff > − pf> ) = Ût(I − ηff>),B Proofs,[0],[0]
"+ ηpf>
",B Proofs,[0],[0]
"Taking expectation with respect to the random choice of the pair, using again f = Wp, and assuming Epp> = λI , we obtain that the stochastic gradient update rule satisfies
E Ût+1 = Ût(I − ηλWW>) + ηλW>
Continuing recursively, we obtain
E Ût+1 = E Ût(I − ηλWW>) + ηλW> =",B Proofs,[0],[0]
[ E Ût−1(I − ηλWW>),B Proofs,[0],[0]
+ ηλW,B Proofs,[0],[0]
> ],B Proofs,[0],[0]
"(I − ηλWW>) + ηλW>
= E Ût−1(I",B Proofs,[0],[0]
− ηλWW>)2 + ηλW>(I,B Proofs,[0],[0]
− ηλWW>),B Proofs,[0],[0]
"+ ηλW>
= Û0(I",B Proofs,[0],[0]
− ηλWW>)t + ηλW>,B Proofs,[0],[0]
"t∑ i=0 (I − ηλWW>)i
We assume that Û0 = 0, and thus
E Ût+1 = ηλW> t∑ i=0 (I − ηλWW>)i
B.5 Proof of Theorem 4
",B Proofs,[0],[0]
"Proof Fix some i, we have that
(I − η λWW>)i = (QIQ> − η λQSV >V SQ>)i = Q(I − η λSS)iQ> = QΛiQ>
where Λi is diagonal with Λij,j = (1 − ηλS2j )",B Proofs,[0],[0]
"i. Therefore, by the properties of geometric series, E Ût+1 converges if and only if η λS21,1 < 1.",B Proofs,[0],[0]
"When this condition holds we have that
Û∞ = η λW > ∞∑ i=0 (I − η λWW>)i
= η λW>(η λWW>)−1 = W>(WW>)−1
= V SQ>(QSV >V SQ>)−1 = V SQ>QS−2Q> = V S−1Q> = U .
",B Proofs,[0],[0]
"Therefore,
E Ût+1",B Proofs,[0],[0]
"− U = η λW> ∞∑
i=t+1
(I − η λWW>)i
= η λV SQ> ∞∑
i=t+1
QΛiQ>
= V [ ∞∑ i=t+1 (η λS)Λi ] Q> .
",B Proofs,[0],[0]
"The matrix in the parentheses is diagonal, where the j’th diagonal element is
ηλSj,j · (1− ηλS2j,j)t+1
ηλS2j,j = S−1j,j (1− ηλS 2 j,j) t+1
It follows that ‖E Ût+1",B Proofs,[0],[0]
"− U‖ = max
j S−1j,j (1− ηλS 2 j,j) t+1
Using the inequality (1− a)t+1 ≥ 1− (t+ 1)a, that holds for every a ∈ (−1, 1), we obtain that
‖E Ût+1",B Proofs,[0],[0]
"− U‖ ≥ S−1n,n(1− (t+ 1)ηλS2n,n) ≥ S−1n,n(1− (t+ 1)",B Proofs,[0],[0]
"S2n,n S21,1 ) .
",B Proofs,[0],[0]
"It follows that whenever,
t+ 1 ≤ S21,1
2S2n,n ,
we have that ‖E Ût+1",B Proofs,[0],[0]
"− U‖ ≥ 0.5S−1n,n. Finally, observe that
S2n,n =",B Proofs,[0],[0]
min x:‖x‖=1,B Proofs,[0],[0]
"x>WW>x ≤ e>1 WW>e1 = 1 ,
hence S−1n,n ≥ 1.",B Proofs,[0],[0]
"We now prove the second part of the theorem, regarding the condition number of W>W , namely,
S21,1 2S2n,n ≥ Ω(n3.5).",B Proofs,[0],[0]
"We note that the condition number of W>W can be calculated through its inverse matrix’s, namely, U>U ’s condition number, as those are equal.
",B Proofs,[0],[0]
It is easy to verify that Uen = en.,B Proofs,[0],[0]
"Therefore, the maximal eigenvalue of U>U is at least 1.",B Proofs,[0],[0]
"To construct an upper bound on the minimal eigenvalue of U>U , consider v ∈",B Proofs,[0],[0]
Rn s.t. for i ≤,B Proofs,[0],[0]
√ n,B Proofs,[0],[0]
we have vi = −12(i/n) 2 and for i >,B Proofs,[0],[0]
√ n,B Proofs,[0],[0]
we have vi = 12n,B Proofs,[0],[0]
− i/n√ n .,B Proofs,[0],[0]
We have that |vi| = O(1/n) for i < √ n+ 2 and v is linear for i ≥,B Proofs,[0],[0]
√ n.,B Proofs,[0],[0]
This implies that (Uv)i = 0 for i ≥ √ n + 2.,B Proofs,[0],[0]
"We also have (Uv)1 = v1 = −0.5/n2, (Uv)2 = −2v1 +",B Proofs,[0],[0]
"v2 ≈ −1/n2, and for i ∈ {3, . . .",B Proofs,[0],[0]
", √ n}",B Proofs,[0],[0]
"we have
(Uv)i = vi−2 − 2vi−1 + vi = −3/n2
Finally, for i = √ n+ 1 we have
(Uv)i = vi−2 − 2vi−1 + vi = vi",B Proofs,[0],[0]
"− vi−1 − (vi−1 − vi−2)
",B Proofs,[0],[0]
= 1 n √ n,B Proofs,[0],[0]
− 1 2n2 ( (i− 1)2,B Proofs,[0],[0]
− (i− 2)2 ),B Proofs,[0],[0]
= 1 n √ n,B Proofs,[0],[0]
"− 1 2n2 (2i− 3) = 1 2n2 .
",B Proofs,[0],[0]
"This yields ‖Uv‖2 ≈ Θ( √ n/n4) = Θ(n−3.5)
",B Proofs,[0],[0]
"In addition,
‖v‖2 ≥ n∑
i=n/2
v2i ≥ n
2 v2n/2 =
n
2
( 1
2n",B Proofs,[0],[0]
"− 1 2 √ n
)2 = Ω(1) .
",B Proofs,[0],[0]
"Therefore, ‖Uv‖2
‖v‖2 = O(n−3.5) ,
which implies that the minimal eigenvalue of U>U is at most O(n−3.5).",B Proofs,[0],[0]
"All in all, we have shown that the condition number of U>U is Ω(n−3.5), implying the same over W>W .
",B Proofs,[0],[0]
"B.6 Gradient Descent for Linear Regression
The loss function is E x,y 1 2 (x>w − y)2
The gradient at w is ∇ = E
x,y x(x>w − y) =",B Proofs,[0],[0]
( E x xx> ),B Proofs,[0],[0]
w,B Proofs,[0],[0]
"− E x,y xy :=",B Proofs,[0],[0]
"Cw − z
For the optimal solution we have Cw∗",B Proofs,[0],[0]
"− z = 0, hence z = Cw∗.",B Proofs,[0],[0]
"The update is therefore
wt+1 =",B Proofs,[0],[0]
wt − η(Cwt − z) =,B Proofs,[0],[0]
(I − ηC)wt + ηz = . . .,B Proofs,[0],[0]
"= t∑ i=0 (I − ηC)iηz = t∑ i=0 (I − ηC)iηCw∗
Let C = V DV > be the eigenvalue decomposition of C. Observe that
wt+1 = V t∑ i=0 η(I − ηD)iDV >w∗
Hence
‖w∗",B Proofs,[0],[0]
− wt+1‖ = ‖(V V > − V t∑ i=0 η(I − ηD)iDV >),B Proofs,[0],[0]
"w∗‖
= ‖(I − t∑ i=0 η(I − ηD)iD)V >w∗‖ = ‖(I",B Proofs,[0],[0]
"− ηD)t+1 V >w∗‖ ,
where the last equality is because for every j we have
(I − t∑ i=0 η(I − ηD)iD)j,j = 1− t∑ i=0 η(1− ηDj,j)iDj,j = (1− ηDj,j)t+1 .
",B Proofs,[0],[0]
Denote v∗ = V >w∗.,B Proofs,[0],[0]
"We therefore obtain that
‖w∗",B Proofs,[0],[0]
"− wt+1‖2 = n∑ j=1 ( (1− ηDj,j)t+1v∗j )2",B Proofs,[0],[0]
"To obtain an upper bound, choose η = 1/D1,1 and t+ 1 ≥ D1,1Dn,n log(‖w
∗‖/ ), and then, using 1− a ≤ e−a, we get that
‖w∗",B Proofs,[0],[0]
"− wt+1‖2 ≤ n∑ j=1 ( exp(−ηDj,j(t+ 1))v∗j )2 ≤ 2 ‖w∗‖2 n∑ j=1 ( v∗j )2 = 2 .
",B Proofs,[0],[0]
"To obtain a lower bound, observe that if v∗1 is non-negligible then η must be at most 1/D1,1 (otherwise the process will diverge).",B Proofs,[0],[0]
"If in addition v∗n is a constant (for simplicity, say v ∗ n = 1), then
‖w∗",B Proofs,[0],[0]
"− wt+1‖ ≥ (1− ηDn,n)t+1 ≥ (1−Dn,n/D1,1)t+1 ≥ 1− (t+ 1)Dn,n/D1,1 ,
where we used (1− a)t+1 ≥ 1− (t+ 1)a for a ∈",B Proofs,[0],[0]
"[−1, 1].",B Proofs,[0],[0]
"It follows that if t+ 1 < 0.5Dn,n/",B Proofs,[0],[0]
"D1,1 then we have that ‖w∗",B Proofs,[0],[0]
"− wt+1‖ ≥ 0.5.
",B Proofs,[0],[0]
"B.7 The Covariance Matrix of Section 4.1.2
Denote by C ∈ R3,3 the covariance matrix, and let λi(C) denote the i’th eigenvalue of C (in a decreasing order).",B Proofs,[0],[0]
The condition number of C is λ1(C)/λ3(C).,B Proofs,[0],[0]
"Below we derive lower and upper bounds on the condition number under some assumptions.
",B Proofs,[0],[0]
"Lower bound We assume that Ef,t f2t = Ω(n2) (this would be the case in many typical cases, for example when the allowed slopes are in {±1}), that k (the number of pieces of the curve) is constant, and that the changes of slope are in [−1, 1].
",B Proofs,[0],[0]
"Now, take v =",B Proofs,[0],[0]
"[1, 1, 1]>, then
v>Cv = E f,t (ft−1 + ft + ft+1) 2 = Ω(n2)
",B Proofs,[0],[0]
This yields λ1(C) ≥ Ω(n2).,B Proofs,[0],[0]
"Next, take v =",B Proofs,[0],[0]
"[1,−2, 1]> we obtain
v>Cv = E f,t (ft−1 − ft + ft+1)2 = O(k/n)
",B Proofs,[0],[0]
This yields λ3(C) ≤ O(k/n).,B Proofs,[0],[0]
"All in all, we obtain that the condition number of C is Ω(n3).
",B Proofs,[0],[0]
"Upper bound We consider distribution over f s.t. for every f , at exactly k indices f changes slope from 1 to −1 or from −1 to 1 (with equal probability over the indices), and at the rest of the indices we have that f is linear with a slope of 1 or −1.",B Proofs,[0],[0]
"Denote p = k/n. Take any unit vector v, and denote v̄ = v1 + v2 + v3.",B Proofs,[0],[0]
"Then
v>Cv = E f,t (v1ft−1 + v2ft + v3ft+1) 2
= E f
[ 0.5(1− p) ( (v̄ft + v3 − v1)2 + (v̄ft + v1 − v3)2 ) + 0.5p ( (v̄ft + v3 + v1) 2 + (v̄ft − v3 − v1)2 )]
= v̄2 E f f2t + (1− p)(v1 − v3)2 + p(v1 + v3)2 .
",B Proofs,[0],[0]
"Since Ef,t f2t = Θ(n2), it is clear that v>Cv = O(n2).",B Proofs,[0],[0]
We next establish a lower bound of Ω(1/n).,B Proofs,[0],[0]
"Observe that if v̄2 ≥ Ω(1/n3), we are done.",B Proofs,[0],[0]
"If this is not the case, then −v2 ≈ v1 + v3.",B Proofs,[0],[0]
"If |v1 + v3| > 0.1, we are done.",B Proofs,[0],[0]
"Otherwise, 0.9 ≤ 1",B Proofs,[0],[0]
"− v22 = v21 + v23 , so we must have that v1 and v3 has opposite signs, and one of them is large, hence (v1 − v3)2 is larger than a constant.",B Proofs,[0],[0]
"This concludes our proof.
",B Proofs,[0],[0]
"B.8 Proof of Update Rule 4 Convergence in the Lipschitz Case
Proof Let B be an upper bound over ‖w(t)‖ for all time step t, and over ‖v∗‖.",B Proofs,[0],[0]
"Moreover, assume |u| ≤ c for some constant c. We denote the update rule by w(t) = w(t−1) + η∇̃, and bound from below:
‖w(t)",B Proofs,[0],[0]
− v∗‖2 − ‖w(t+1),B Proofs,[0],[0]
− v∗‖2 = 2〈w(t),B Proofs,[0],[0]
"− v∗, η∇̃〉 − η2‖∇̃‖2 = 2η E",B Proofs,[0],[0]
[ (u((w(t))>x)− u((v∗)>x))(w(t),B Proofs,[0],[0]
"− v∗)x ] − η2‖∇̃‖2
(1) ≥ 2η L
E [ (u((w(t))>x)− u((v∗)>x))2 ]",B Proofs,[0],[0]
"− η2‖∇̃‖2
(2) ≥ 2η L
E [ (u((w(t))>x)− u((v∗)>x))2 ]",B Proofs,[0],[0]
"− η2B2c2
where (1) follows from L-Lipschitzness and monotonicity of u, (2) follows from bounding ‖w‖, ‖x‖ and u. Let the expected error of the regressor parametrized by wt be denoted et.",B Proofs,[0],[0]
"We separate into cases:
• If ‖wt − v∗‖2 − ‖wt+1",B Proofs,[0],[0]
"− v∗‖2 ≥ η2B2c2, we can rewrite ‖wt − v∗‖2 − η2B2c2 ≥ ‖wt+1",B Proofs,[0],[0]
"− v∗‖2, and note that since ‖wt+1",B Proofs,[0],[0]
"− v∗‖2 ≥ 0, and ‖w0 − v∗‖2 ≤ B2, there can be at most B2
η2B2c2",B Proofs,[0],[0]
"= 1 η2c2
iterations where this condition will hold.
",B Proofs,[0],[0]
"• Otherwise, we get that et ≤ ηB2c2L.
Therefore, for a given , by taking T = c 4B4L2
2 , and setting η = √ 1 Tc2 , we obtain that after T iterations, the
first case is not holding anymore, and the second case implies eT ≤ .",B Proofs,[0],[0]
"Lemma 5 Any parity function over d variables is realizable by a network with one fully connected layer of width d̃ > 3d2 with ReLU activations, and a fully connected output layer with linear activation and a single unit.
",C Technical Lemmas,[0],[0]
"Proof Let the weights entering each of the first 3d2 hidden units be set to v ∗, and the rest to 0.",C Technical Lemmas,[0],[0]
Further assume that for i ∈,C Technical Lemmas,[0],[0]
"[d/2], the biases of the first 3i + {1, 2, 3} units are set to −(2i − 12), −2i, −(2i + 1 2) respectively, and that their weights in the output layer are 1,−2, and 1.",C Technical Lemmas,[0],[0]
"It is not hard to see that the weighted sum of those triads of neurons is 12 if 〈x,v
∗〉 = 2i, and 0 otherwise.",C Technical Lemmas,[0],[0]
Observe that there’s such a triad defined for each even number in the range [d].,C Technical Lemmas,[0],[0]
"Therefore, the output of this net is 0 if y = −1, and 12 otherwise.",C Technical Lemmas,[0],[0]
"It is easy to see that scaling of the output layer’s weights by 4, and introduction of a −1 bias value to it, results in a perfect predictor.",C Technical Lemmas,[0],[0]
Our experiments are implemented in a simple manner in python.,D Command Lines for Experiments,[0],[0]
We use the tensorflow package for optimization.,D Command Lines for Experiments,[0],[0]
"The following command lines can be used for viewing all optional arguments:
To run experiment 2.1, use:
python ./parity.py --help
To run experiment 3.1, use:
python ./tuple_rect.py --help
",D Command Lines for Experiments,[0],[0]
"For SNR estimations, use:
python ./tuple_rect_SNR.py --help
",D Command Lines for Experiments,[0],[0]
"To run experiment A, use:
python ./dec_vs_e2e_stocks.py --help
",D Command Lines for Experiments,[0],[0]
"For Section 4’s experiments, given below are the command lines used to generate the plots.",D Command Lines for Experiments,[0],[0]
"Additional arguments can be viewed by running:
python PWL_fail1.py --help
To run experiment 4.1.1, use:
python PWL_fail1.py --FtoK
To run experiment 4.1.2, use:
python PWL_fail1.py --FtoKConv
To run experiment 4.1.3, use:
python PWL_fail1.py --FtoKConvCond",D Command Lines for Experiments,[0],[0]
"--batch_size 10 --number_of_iterations 500 --learning_rate 0.99
To run experiment 4.1.4, use:
python PWL_fail1.py --FAutoEncoder
To run Section 5’s experiments, run:
python step_learn.py --help",D Command Lines for Experiments,[0],[0]
"In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art.",abstractText,[0],[0]
"However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms.",abstractText,[0],[0]
"We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties.",abstractText,[0],[0]
"We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied1.",abstractText,[0],[0]
Failures of Gradient-Based Deep Learning,title,[0],[0]
"A problem facing many services – from search engines and news feeds to machine learning – is data summarization: how can one select a small but representative, i.e., diverse, subset from a large dataset.",1. Introduction,[0],[0]
"For instance, Google Images outputs a small subset of images from its enormous dataset given a user query.",1. Introduction,[0],[0]
"Similarly, in training a learning algorithm one may be required to choose a subset of data points to train on as training on the entire dataset may be costly.",1. Introduction,[0],[0]
"However, data summarization algorithms prevalent in the online world have been recently shown to be biased with respect to sensitive attributes such as gender, race or ethnicity.",1. Introduction,[0],[0]
"For instance, a recent study found evidence of systematic
1École Polytechnique Fédérale de Lausanne (EPFL), Switzerland 2Microsoft Research, India 3UC Berkeley.",1. Introduction,[0],[0]
"Correspondence to: L. Elisa Celis <elisa.celis@epfl.ch>, Nisheeth K. Vishnoi <nisheeth.vishnoi@epfl.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
under-representation of women in search results (Kay et al., 2015).",1. Introduction,[0],[0]
"Concretely, the above work studied the output of Google Images for various search terms involving occupations and found, e.g., that for the search term “CEO”, the percentage of women in top 100 results was 11%, significantly lower than the ground truth of 27%.",1. Introduction,[0],[0]
"Through studies on human subjects, they also found that such misrepresentations have the power to influence people’s perception about reality.",1. Introduction,[0],[0]
"Beyond humans, since data summaries are used to train algorithms, there is a danger that these biases in the data might be passed on to the algorithms that use them; a phenomena that is being revealed more and more in automated data-driven processes in education, recruitment, banking, and judiciary systems, see (O’Neil, 2016).
",1. Introduction,[0],[0]
"A robust and widely deployed method for data summarization is to associate a diversity score to each subset and select a subset with probability proportional to this score; see (Hesabi et al., 2015).",1. Introduction,[0],[0]
"This paper focuses on a concrete geometric measure of diversity of a subset S of a dataset {vx}x∈X of vectors – the determinantal measure denoted by G(S) (Kulesza & Taskar, 2012); and the resulting probability distribution is called a determinantal point process (DPP).",1. Introduction,[0],[0]
"G(S) generalizes the correlation measure for two vectors to multiple vectors and, intuitively, the larger G(S), the more diverse is S in the feature space.",1. Introduction,[0],[0]
"Among benefits of G(·) are its overall simplicity, wide applicability – not depending on combinatorial properties of the data, and efficient computability.",1. Introduction,[0],[0]
"A potential downside might be the additional effort required in modeling, i.e., to represent the data in a suitable vector form so that the geometry of the dataset indeed corresponds to diversity.
",1. Introduction,[0],[0]
"Despite the well-acknowledged ability of DPPs to produce diverse subsets, unfortunately, there seems to be no obvious way to ensure that this also guarantees fairness in the DPP samples in the form of appropriate representation of sensitive attributes in the subset selected.",1. Introduction,[0],[0]
"Partially, this is due to the fact that fairness could mean different things in different contexts.",1. Introduction,[0],[0]
"For instance, consider a dataset in which each data point has a gender.",1. Introduction,[0],[0]
"One notion of fairness, useful in ensuring that the ground truth does not get distorted, is proportional representation: i.e., the distribution of sensitive characteristics in the output set should be identical to that of the input dataset (Kay et al., 2015).",1. Introduction,[0],[0]
"Another notion of fairness, argued to be necesseary to reverse the effect of
historical biases (Koriyama et al., 2013), could be equal representation – the representation of sensitive characteristics should be equal independent of the ratio in the input dataset.",1. Introduction,[0],[0]
"While these measures of fairness have natural generalizations to the case when the number of sensitive types is more than two, and can be refined in several ways, one thing remains common: they all operate in the combinatorial space of sensitive attributes of the data points.",1. Introduction,[0],[0]
"Simple examples (see, e.g., Figure 1 in the Supplementary File) show that, in certain settings, geometric diversity does not imply fairness and vice-versa; however, there seems to be no intrinsic barrier in attaining both.
",1. Introduction,[0],[0]
We initiate a rigorous study of the problem of incorporating fairness with respect to sensitive attributes of data in DPPbased sampling for data summarization.,1. Introduction,[0],[0]
"Our contributions are: A framework that can incorporate a wide class of notions of fairness with respect to disjoint sensitive attributes and, conditioned on being fair in the specified sense, outputs subsets where the probability of a set is still proportional to G()̇.",1. Introduction,[0],[0]
"In particular, we model the problem as sampling from a partition DPP – the parts correspond to different sensitive attributes and the goal is to select a specified number of points from each.",1. Introduction,[0],[0]
"Unfortunately, the problem of sampling from partition DPPs has been recently shown to be intractable in a strong sense (Celis et al., 2017) and the question of designing fast algorithms for it, at the expense of being approximate, has been open.",1. Introduction,[0],[0]
Our main technical result is a linear time algorithm (see Section 3.1) to sample from partition DPPs that is guaranteed to output samples from close to the DPP distribution under a natural condition on the data (see Definition 4).,1. Introduction,[0],[0]
We prove that random data matrices satisfy this condition in Section 3.3.,1. Introduction,[0],[0]
"We run our algorithm on the Adult dataset (Blake & Merz, 1998) and a curated image dataset with various parameter settings and observe a marked improvement in fairness without compromising geometric diversity by much.",1. Introduction,[0],[0]
"A theoretical justification of this low price of fairness is provided in Section 4; while there have been few works on controlling fairness, ours is the first to give a rigorous, quantitative price of fairness guarantee in any setting.",1. Introduction,[0],[0]
"Overall, our work gives a general and rigorous algorithmic solution to the problem of controlling bias in DPP-based sampling algorithms for data summarization while maximizing diversity.
",1. Introduction,[0],[0]
Related Work.,1. Introduction,[0],[0]
There are several data pre-processing approaches to reduce bias in training data.,1. Introduction,[0],[0]
"For example, in (Kamiran & Calders, 2012) or (He & Garcia, 2009), bias is removed from training data by over- or under-sampling from the dataset with appropriately defined cardinality constraints on the parts of a partition.",1. Introduction,[0],[0]
The sampling approach used is often either uniform or preferential (according to a problem-dependent ranking).,1. Introduction,[0],[0]
"We show that sampling using partition-DPPs has better results in ensuring diversity of the sampled subset than any such sampling method.
DPP-based sampling has been deployed for many data summarization tasks including text and images (Kulesza & Taskar, 2011), videos (Gong et al., 2014), documents (Lin & Bilmes, 2012), recommendation systems (Zhou et al., 2010), and sensors (Krause et al., 2008); and the study of DPPs with additional budget or resource constraints is of importance.",1. Introduction,[0],[0]
"While for unconstrained DPPs there are efficient algorithms to sample (Hough et al., 2006), the problem of sampling from constrained DPPs is intractable; see (Celis et al., 2017), where pseudopolynomial time algorithms for partition DPPs are presented.",1. Introduction,[0],[0]
"There is also work on approximate MCMC algorithms for sampling from various discrete point processes (see (Rebeschini & Karbasi, 2015; Anari et al., 2016) and the references therein), and algorithms that are efficient for constrained DPPs under certain restrictions on the data matrix and constraints (see (Li et al., 2016) and the references therein).",1. Introduction,[0],[0]
"To the best of our knowledge, ours is the first algorithm for constrained DPPs that is near-linear time.",1. Introduction,[0],[0]
"Our algorithm is a greedy, approximate algorithm, and can be considered an extension of a similar algorithm for unconstrained DPPs given by (Deshpande & Vempala, 2006).",1. Introduction,[0],[0]
"Finally, our work contributes towards an ongoing effort to measure, understand and incorporate fairness (e.g., see (Barocas & Selbst, 2015; Caliskan et al., 2017; Dwork et al., 2012; Zafar et al., 2017)) in fundamental algorithmic problems, including ranking (Celis et al., 2018b), voting (Celis et al., 2018a), and personalization (Celis & Vishnoi, 2017).",1. Introduction,[0],[0]
"In this section we present the formal notions, model and other theoretical constructs studied in this paper.",2. Our Model,[0],[0]
X will denote the dataset and we let m denote its size.,2. Our Model,[0],[0]
"We assume that for each x ∈ X , we are given a (feature) vector vx ∈ Rn, where n ≤ m is the dimension of the data.",2. Our Model,[0],[0]
Let V denote the m × n matrix whose rows correspond to the vectors vx for x ∈ X .,2. Our Model,[0],[0]
"For a set S ⊆ X , we use VS to denote the submatrix of V that is obtained by picking the rows of V corresponding to the elements of S. We can now describe geometric diversity formally.
Definition 1.",2. Our Model,[0],[0]
(Geometric Diversity),2. Our Model,[0],[0]
"Given a dataset X and the corresponding feature vectors V ∈ Rm×n, the geometric diversity of a subset S ⊆ X is defined as G(S)",2. Our Model,[0],[0]
":= det ( VSV > S ) , which is the squared volume of the parallelepiped spanned by the rows of VS .
",2. Our Model,[0],[0]
"This volume generalizes the correlation measure for two vectors to multiple vectors and, intuitively, the larger the volume, the more diverse is S in the feature space; see
Figure 2 in the Supplementary File for an illustration.",2. Our Model,[0],[0]
"Geometric diversity gives rise to the following distribution on subsets known as a determinantal point process (DPP).
",2. Our Model,[0],[0]
Definition 2.,2. Our Model,[0],[0]
(DPPs and k-DPPs),2. Our Model,[0],[0]
"Given a dataset X and the corresponding feature vectors V ∈ Rm×n, the DPP is a distribution over subsets S ⊆ X such that the probability
P[S] ∝",2. Our Model,[0],[0]
det ( VSV > S ) .,2. Our Model,[0],[0]
"The induced probability distribution over k-sized subsets is called k-DPP.
",2. Our Model,[0],[0]
A characteristic of a DPP measure is that the inclusion of one item makes including other similar items less likely.,2. Our Model,[0],[0]
"Consequently, DPPs assign greater probability to subsets of points that are diverse; for example, a DPP prefers search results that cover multiple aspects of a user’s query, rather than the most popular one.
",2. Our Model,[0],[0]
Our Algorithmic Framework: We are given a dataset X along with corresponding feature vectors V ∈ Rm×n and a positive number k ≤ m that denotes the size of the subset or summary that needs to be generated.,2. Our Model,[0],[0]
"The dataset X is partitioned into p disjoint classes X1 ∪X2 ∪ · · · ∪Xp, each corresponding to a sensitive class.",2. Our Model,[0],[0]
"A key feature of our model is that we do not fix one notion of fairness; rather, we allow for the specification of fairness constraints with respect to these sensitive classes.",2. Our Model,[0],[0]
"Formally, we do this by taking as input p natural numbers (k1, k2, . . .",2. Our Model,[0],[0]
", kp) such that ∑p j=1 kj = k is the sample size.",2. Our Model,[0],[0]
These numbers give rise to a fair family of allowed subsets defined to be B := {S ⊆ X : |S ∩,2. Our Model,[0],[0]
"Xj | = kj for all j = 1, 2, . . .",2. Our Model,[0],[0]
", p}.",2. Our Model,[0],[0]
"By setting (k1, . . .",2. Our Model,[0],[0]
", kp) appropriately, the user can ensure their desired notion of fairness.",2. Our Model,[0],[0]
"For example, if the dataset hasmi items with the i-th sensitive attribute, then we can set ki := kmi/m to obtain proportional representation.",2. Our Model,[0],[0]
"Similarly, equal representation can be implemented by setting ki = k/p for all i.
The fair data summarization problem is to sample from a distribution that is supported on B. However, there could be many such distributions; we pick one that is “closest” to the to the k-DPP described by V .",2. Our Model,[0],[0]
We use the Kullback-Leibler (KL) divergence between distributions q and q̃ defined as DKL(q||q̃) := ∑ S qS log qS q̃S .1,2. Our Model,[0],[0]
"The following lemma characterizes the distribution supported on B that has the least KL-divergence to a given distribution (see Appendix B.1 in the Supplementary File for the proof).
",2. Our Model,[0],[0]
Lemma 1.,2. Our Model,[0],[0]
"Given a distribution q̃ with support set C, let B ⊆ C and q be any distribution on B. Then the optimal value of minqDKL(q||q̃) is achieved by the distribution q?, such that q?S ∝ q̃S , for S ∈ B and 0 otherwise.",2. Our Model,[0],[0]
"Thus, the distribution above can be thought of as the most diverse while being fair; we call it partition DPP, or P -DPP.
Definition 3.",2. Our Model,[0],[0]
(P -DPP),2. Our Model,[0],[0]
"Given a dataset X , the corresponding feature vectors V ∈ Rm×n, a partition X = X1 ∪ X2 ∪ · · · ∪ Xp into p parts, and natural numbers k1, . . .",2. Our Model,[0],[0]
", kp, P -DPP defines a distribution q?",2. Our Model,[0],[0]
"over subsets S ⊆ X of size k = ∑p i=1 ki such that for all S ∈ B we
1Note that when there are only two parts, one can recover the percentages of elements from each part from the KL-distance.",2. Our Model,[0],[0]
"For multiple parts, the KL-distance is a natural (and general) singledimensional function of the percentage vector with which to measure the deviation from the target distribution.
",2. Our Model,[0],[0]
"have q?S := det(VSV
> S )∑
T∈B",2. Our Model,[0],[0]
"det(VTV > T ) , and q?S = 0",2. Our Model,[0],[0]
"otherwise.
",2. Our Model,[0],[0]
"Given the results of (Celis et al., 2017), we know that sampling from P -DPPs is #P-hard and exact sampling algorithm for P -DPPs are unlikely.",2. Our Model,[0],[0]
"Correspondingly, the flexibility that our framework provides in specifying the fairness constraints comes at a computational cost.",2. Our Model,[0],[0]
"In this paper, we give a fast, approximate sampling algorithm for P -DPPs.",2. Our Model,[0],[0]
Notions of Volume and Projection.,3. Our Algorithm,[0],[0]
Let us recall the interpretation of determinants in terms of volumes.,3. Our Algorithm,[0],[0]
"For S ⊆ X , VS is the set of vectors {vx}x∈S .",3. Our Algorithm,[0],[0]
"If the vectors in S are pairwise orthogonal, then the matrix VSV >S is diagonal with entries {‖vx‖2}x∈S on the diagonal and, hence, det(VSV > S ) = ∏",3. Our Algorithm,[0],[0]
"x∈S ‖vx‖
2.",3. Our Algorithm,[0],[0]
"In the general case, the determinant is not simply the (squared) product of the norms of vectors, however a similar formula still holds.",3. Our Algorithm,[0],[0]
"Let H ⊆ Rn be any linear subspace and H⊥ be its orthogonal complement, i.e., H⊥ := {y ∈",3. Our Algorithm,[0],[0]
"Rn | 〈x, y〉 = 0 for all x ∈ H}.",3. Our Algorithm,[0],[0]
"Let ΠH : Rn → Rn be the orthogonal projection operator on the subspace H⊥, i.e., whenever w ∈",3. Our Algorithm,[0],[0]
"Rn decomposes as w1+w2 forw1 ∈ H andw2 ∈ H⊥, then ΠH(w) = w2.",3. Our Algorithm,[0],[0]
"By a slight abuse of notation, we also denote by Πv the operator that projects a vector to another that is orthogonal to a given vector v ∈",3. Our Algorithm,[0],[0]
"Rn, i.e., Πv(w)",3. Our Algorithm,[0],[0]
":= w − 〈w, v〉 / ‖v‖2 .
",3. Our Algorithm,[0],[0]
The following lemma is a simple generalization of the formula derived above for orthogonal families of vectors and inspires our algorithm for P -DPPs.,3. Our Algorithm,[0],[0]
The proof of this lemma is presented in Section B.3 in the Supplementary File.,3. Our Algorithm,[0],[0]
Lemma 2 (Determinant Volume Lemma).,3. Our Algorithm,[0],[0]
"Let w1, . . .",3. Our Algorithm,[0],[0]
", wk ∈",3. Our Algorithm,[0],[0]
"Rn be the rows of a matrix W ∈ Rk×n, then det(WW>)",3. Our Algorithm,[0],[0]
=,3. Our Algorithm,[0],[0]
∏k i=1,3. Our Algorithm,[0],[0]
"‖ΠHiwi‖ 2 , whereHi is the subspace spanned by {w1, . . .",3. Our Algorithm,[0],[0]
", wi−1} for all i = 1, 2, . . .",3. Our Algorithm,[0],[0]
", k.",3. Our Algorithm,[0],[0]
"Before we describe our algorithms for sampling from P - DPPs, it is instructive to consider the special case of k-DPPs itself and the simple “orthogonal” scenario – where all the vectors vx, for x ∈ X , are pairwise orthogonal.",3.1. Our Sample and Project Algorithm,[0],[0]
"In such a case, there is a simple iterative algorithm: sample x ∈ X with probability ∝ ‖vx‖2, then add x to S and remove x from X; repeat until |S|",3.1. Our Sample and Project Algorithm,[0],[0]
= k.,3.1. Our Sample and Project Algorithm,[0],[0]
"It is intuitively clear, and not hard to prove, that the final probability of obtaining a given set S as a sample is proportional to ∏ x∈S ‖vx‖ 2 = det(VSV > S ) and, hence, recovers the k-DPP exactly.
",3.1. Our Sample and Project Algorithm,[0],[0]
"In case of P -DPPs where all the vectors are pairwise orthogonal, and we need to sample ki vectors from partition Xi, we can sample the required number of elements from each partition independently using the procedure in the previous paragraph.",3.1. Our Sample and Project Algorithm,[0],[0]
"The orthogonality of the vectors and the disjointness of the parts implies that this sampling procedure gives the right probability distribution.
",3.1. Our Sample and Project Algorithm,[0],[0]
"Algorithm 1 Sample-And-Project 1: Input: V, (X1, .., Xp), (k1, .., kp) 2: S ← ∅ 3: k ← k1 + k2 + · · ·+ kp 4: Let wx := vx for all x ∈ X 5: while |S| < k do 6: Pick any2 i ∈ {1, . . .",3.1. Our Sample and Project Algorithm,[0],[0]
", p} such that |S ∩Xi| < ki 7: Define q ∈ RXi by qx := ‖wx‖2 for x ∈ Xi 8: Sample x̃ ∈",3.1. Our Sample and Project Algorithm,[0],[0]
"Xi from distribution { qx∑
y∈Xi qy } x∈Xi
9: S ← S ∪ {x̃} 10: Let v := wx̃ 11: For all x ∈ X , set wx := Πv(wx) 12: end while However, when the vectors vx are no longer pairwise orthogonal, the above heuristic can fail miserably.",3.1. Our Sample and Project Algorithm,[0],[0]
This is where we invoke Lemma 2.,3.1. Our Sample and Project Algorithm,[0],[0]
"It suggests the following strategy: once we select a vector, then we should orthogonalize all the remaining vectors with respect to it before repeating the sampling procedure.",3.1. Our Sample and Project Algorithm,[0],[0]
"For the case of k-DPPs, it can be shown that this heuristic outputs a set S with probability no more than k! times its desired probability (Deshpande & Vempala, 2006).",3.1. Our Sample and Project Algorithm,[0],[0]
The k! term is primarily because the k vectors can be chosen in any of the k! orders.,3.1. Our Sample and Project Algorithm,[0],[0]
"Taking this simple heuristic as a starting point and incorporating an additional idea to deal with partition constraints, we arrive at our Sample and Project algorithm – see Algorithm 1.
",3.1. Our Sample and Project Algorithm,[0],[0]
Given that we have made several simplifications and informal “jumps” when deriving the algorithm one cannot expect that the distribution over sets S produced by Algorithm 1 to be exactly the same as P -DPP.,3.1. Our Sample and Project Algorithm,[0],[0]
"Later in this section we give evidence that in fact the distribution output by the “Sample and Project” heuristic can be formally related to the P -DPP distribution, and hence the constructed algorithm is provably an approximation to a P -DPP.",3.1. Our Sample and Project Algorithm,[0],[0]
"However, we first note an attractive feature of this algorithm – it is fast and practical.",3.1. Our Sample and Project Algorithm,[0],[0]
"For a V ∈ Rm×n matrix and k = ∑p i=1 ki, Algorithm 1 can be implemented in O(mnk) time.
",3.1. Our Sample and Project Algorithm,[0],[0]
"Note that the size of the data for this problem is already Θ(mn), hence, the algorithm does only linear work per sampled point.",3.1. Our Sample and Project Algorithm,[0],[0]
"For P -DPPs there is only one known exact algorithm which samples in time mO(p), which is polynomial only when p = O(1) (Celis et al., 2017).
",3.1. Our Sample and Project Algorithm,[0],[0]
Another possible approach for sampling from DPPs is the Markov Chain Monte Carlo method.,3.1. Our Sample and Project Algorithm,[0],[0]
"It was proved in (Anari et al., 2016) that Markov Chains can be used to sample from k-DPPs in time roughly Õ(mk4 + mn2) given a “warm start”, i.e., a set S0 of significant probability.",3.1. Our Sample and Project Algorithm,[0],[0]
"This approach does not extend to P -DPPs – indeed in (Anari et al., 2016)",3.1. Our Sample and Project Algorithm,[0],[0]
"the underlying probability distribution is required to be Strongly Rayleigh, a property which holds for k-DPPs, but fails for P -DPPs whenever the number of parts is at least
two.",3.1. Our Sample and Project Algorithm,[0],[0]
One can still formulate an analogous MCMC algorithm for the case of P -DPPs – it fails on specially crafted “bad instances” but seems to perform well on real world data.,3.1. Our Sample and Project Algorithm,[0],[0]
"However, even ignoring the lack of provable guarantees for this algorithm, it does not seem possible to reduce its running time below O(mk4",3.1. Our Sample and Project Algorithm,[0],[0]
+mn2),3.1. Our Sample and Project Algorithm,[0],[0]
", which significantly limits its practical applicability.",3.1. Our Sample and Project Algorithm,[0],[0]
We now present a theorem which connects the output distribution of Algorithm 1 to the corresponding P -DPP.,3.2. Provable Guarantees for Our Algorithm,[0],[0]
"To establish such a guarantee we require the following assumption on the singular values of the matrices VXi .
",3.2. Provable Guarantees for Our Algorithm,[0],[0]
Definition 4 (β-balance).,3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Let X be a set of m elements partitioned into p parts X1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", Xp and let V ∈",3.2. Provable Guarantees for Our Algorithm,[0],[0]
Rm×n be a matrix.,3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Denote by σ1 ≥ · · · ≥ σn the singular values of V and for each i ∈ {1, 2, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", p}, let σi,1 ≥ · · · ≥ σi,n denote the singular values of VXi .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"For β ≥ 1, the partition X1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", Xp is called β-balanced with respect to V if for all i ∈ {1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", p} and for all j ∈ {1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", n}, σi,j ≥ 1βσj .
",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"The β-balance property informally requires that the diversity within each of the partitions VXi , relative to V , is significant.",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"A more concrete geometric way to think about this condition is as follows: if one thinks of the positive semidefinite matrix V >V ∈ Rn×n as representing an ellipsoid in Rn whose axes are the singular values, then the β-balance condition essentially says that the ellipsoids corresponding to each of the partitions are a β-approximation to that of V (see Figure 4 in the Supplementary File).
",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Importantly, Algorithm 1 never outputs a set S /∈",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"B, hence the only way its output distribution could significantly differ from the P -DPP would be if certain sets S ∈ B appeared in the output with larger probabilities than specified by the P -DPP.",3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Our main theoretical result for Sample and Project is that for β-balanced instances we can control the scale at which such a violation can happen.
",3.2. Provable Guarantees for Our Algorithm,[0],[0]
Theorem 1 (Approximation Guarantee).,3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Let X be a set of m elements partitioned into p parts X1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", Xp, a matrix V ∈ Rm×n and integers k1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", kp, such that X1, . . .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
", Xp is a β-balanced partition with respect to V and ∑p j=1 kj .",3.2. Provable Guarantees for Our Algorithm,[0],[0]
Let B ⊆ 2X denote the following family of sets,3.2. Provable Guarantees for Our Algorithm,[0],[0]
"Then Algorithm 1, with V , (X1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", Xp) and (k1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", kp) as input, returns a subset S ∈ B with probability q̃(S) ≤ ηk · β2k · q?S where q?S = det(VSV > S )∑
T∈B","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"det(VTV > T )
, k = ∑p j=1 kj
and ηk = k1! · k2! · · · kp!.
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"The proof of the approximation guarantee uses techniques inspired by (Deshpande & Vempala, 2006) who prove a similar bound for k-DPP sampling.
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
We use the following lemmas in the proof of the theorem.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"The proof of these lemmas appear in Appendix B.4 and Appendix B.5 in the Supplementary File.
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
Lemma 3.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
For any matrix V ∈ Rm×n with m ≥ n ≥,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"k,∑ i1<i2<···<ik σ2i1σ 2 i2 · · ·σ 2 ik = ∑ S:|S|=k det(VSV > S )
where σ1, σ2, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", σn are the singular values of V and VS is the sub-matrix of V with rows corresponding to S. Lemma 4.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Given a β-balanced partition, Algorithm 1 returns a set S such that det(VSV >S ) is non-zero with probability one.
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
Proof of Theorem 1.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
Let π be the random variable representing the ordered output of the algorithm.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Suppose that the algorithm outputs the set S = {x1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", xk}.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Since the partition X1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", Xp is β-balanced with respect to V , by Lemma 4 the algorithm will always output a set which has non-zero determinant value, i.e, det(VSV >S ) 6= 0.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Consider any ordering of the set S, say, τ := (x1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", xk).","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Let Hj ⊆ Rn denote the linear subspace spanned by the vectors corresponding to the first j − 1 elements, i.e., {vx1 , . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", vxj−1}.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
We also define a mapping f :,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"X → {1, . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", p} such that f(x) =","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
i if x ∈ Xi.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
In the first iteration say we choose partition X1.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
Then the algorithm will sample an element from X1 with probability proportional to the squared norm of the vector.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"After (j − 1) iterations wx will be the orthogonal projection of vx onto the subspace orthogonal to span{vx1 , vx2 , . . .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
", vxj−1}.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"This is a consequence of the fact that (Πvx1 Πvx1 · · ·Πvxj−1 ) = ΠHj .
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Hence in the (j − 1)-th iteration, wx = ΠHj (vx) for all x ∈ X .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Therefore, the probability that the sequence τ is the output of the algorithm is
P(π = τ) = k∏ j=1 ∥∥ΠHj (vxj )∥∥2∑ x∈Xf(xj)
∥∥ΠHj (vx)∥∥2 .","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
(1) The numerator of (1) is det(VSV >S ) by Lemma 2.,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Let Dx1,...,xk denote the denominator.","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"For each term in the denominator
∑ x∈Xl ∥∥ΠHj (vx)∥∥2 = ∥∥VXl","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
− V ′Xl∥∥2F,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
where ‖·‖F denotes the Frobenius norm and V ′Xl is the rank j,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
− 1 matrix with rows {v′x}x∈Xl such that v′x is the projection of vector vx on Hj .,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"By a result on low rank approximations (see Theorem 1), we can bound the above quantity as
∑ x∈Xl ∥∥ΠHj (vx)∥∥2 ≥ n∑ t=j σ2l,t ≥ 1 β2 n∑ t=j σ2t
where σl,t is the t-th singular value of VXl and second inequality is due to the β-balanced property of the partition.
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Using above, the denominator of (1) becomes
Dx1,...,xk ≥ k∏ j=1 1 β2 n∑ t=j σ2t ≥ 1 β2k ∑ t1<···<tk σ2t1 · · ·σ 2 tk .
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"By applying Lemma 3, it then follows
Dx1,...,xk","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"≥ 1
β2k ∑ |S|=k","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
det(VSV > S ),"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"≥ 1 β2k ∑ S∈B det(VSV > S ).
","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Thus, P(π = τ) ≤ β2k","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"det(VSV > S )∑
T∈B","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
det(VTV > T ) .,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Since the order
in which the partitions are considered by the algorithm is fixed, the vectors of each Xi in τ can be permuted amongst themselves and the output set will still be S. Correspondingly there are ηk = k1! ·","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
k2! · · · kp! valid permutations of τ .,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"Let TS be the set of all valid permutations of elements of S, then q̃S =
∑ τ∈TS","B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
P(π = τ) ≤ ηk · β2k · q?S .,"B := {S ⊆ X : |S ∩Xj | = kj for all j = 1, 2, . . . , p}",[0],[0]
"For a given matrix V ∈ Rm×n, suppose we choose the partitions randomly.",3.3. β-balanced property for random data,[0],[0]
"For each element x ∈ X , we put x in Xi with probability 1/p.",3.3. β-balanced property for random data,[0],[0]
"Using the Matrix Chernoff bounds (Tropp, 2012), we prove the following theorem.",3.3. β-balanced property for random data,[0],[0]
Theorem 2.,3.3. β-balanced property for random data,[0],[0]
"Assume that all the rows vj (for j ∈ X = {1, 2, . . .",3.3. β-balanced property for random data,[0],[0]
",m}) of V ∈ Rm×n satisfy v>j",3.3. β-balanced property for random data,[0],[0]
"(V >V )−1vj ≤
δ2
8p log(np) , where δ ∈ (0, 1) is a constant.",3.3. β-balanced property for random data,[0],[0]
If X is randomly partitioned into X = X1 ∪X2 ∪ . . .,3.3. β-balanced property for random data,[0],[0]
"∪Xp then with probability at least 1e , the partition X1, . . .",3.3. β-balanced property for random data,[0],[0]
", Xp is β-balanced with respect to V , for β = √ (1 + δ)p.
",3.3. β-balanced property for random data,[0],[0]
The proof of this theorem is given in Appendix B.6 in the Supplementary File.,3.3. β-balanced property for random data,[0],[0]
"The quantity v>j (V
>V )−1vj is also called the statistical leverage score of vj with respect to V >V .",3.3. β-balanced property for random data,[0],[0]
"For two partitions, the theorem states that if the leverage score of all rows is O( 1logn ), then the partitions are β-balanced for β",3.3. β-balanced property for random data,[0],[0]
≈ √ 2.,3.3. β-balanced property for random data,[0],[0]
In this section we present conditions under which the k-DPP and P -DPP distributions are close to each other.,4. Price of Fairness,[0],[0]
Note that the support of a P -DPP is a subset of the support of the corresponding k-DPP.,4. Price of Fairness,[0],[0]
"Thus, a natural definition of the price of fairness is the KL-divergence between them.",4. Price of Fairness,[0],[0]
Definition 5 (Price of Fairness).,4. Price of Fairness,[0],[0]
"Given a matrix V ∈ Rm×n, partitions X1, . . .",4. Price of Fairness,[0],[0]
", Xp and integers k1, . . .",4. Price of Fairness,[0],[0]
", kp, let k = k1 + · · ·",4. Price of Fairness,[0],[0]
+ kp.,4. Price of Fairness,[0],[0]
Suppose q is the distribution defined by k-DPP over subsets of size k and q? is the distribution defined by P -DPP over subsets with ki elements from each Xi.,4. Price of Fairness,[0],[0]
"Then, the price of fairness is DKL(q?||q).
",4. Price of Fairness,[0],[0]
"We define the following property for the input data and analyze its price of fairness.
",4. Price of Fairness,[0],[0]
Definition 6 (δ-drop).,4. Price of Fairness,[0],[0]
"For 0 ≤ δ ≤ 1, the partition X1, . . .",4. Price of Fairness,[0],[0]
", Xp is called a δ-drop partition with respect to V and k1, . . .",4. Price of Fairness,[0],[0]
", kp if for all i ∈",4. Price of Fairness,[0],[0]
"{1, . . .",4. Price of Fairness,[0],[0]
", p}, σi,ki+1 ≤ δσi,ki .",4. Price of Fairness,[0],[0]
"Here σi,j is the j-th largest singular value of VXi .
",4. Price of Fairness,[0],[0]
"Roughly, this says that, if δ is small, then each of the matrices VXi is effectively a rank-ki matrix.",4. Price of Fairness,[0],[0]
"Such a notion of low effective rank appears frequently in the machine learning literature (Roy & Vetterli, 2007; Drineas et al., 1999).",4. Price of Fairness,[0],[0]
"We prove the following theorem that asserts that if the δdrop condition is satisfied, then we can be sure that most of the probability mass is concentrated on subsets which satisfy partition constraints.",4. Price of Fairness,[0],[0]
"In such a case, sampling a k sized subset using any k-DPP algorithm will output a subset which satisfies partition constraints with high probability.",4. Price of Fairness,[0],[0]
"The proof of the theorem is provided in the Appendix B.7 in the Supplementary File.
Theorem 3.",4. Price of Fairness,[0],[0]
"Let ε ∈ (0, 1) and suppose that the partition X1, . . .",4. Price of Fairness,[0],[0]
", Xp is δ-drop w.r.t.",4. Price of Fairness,[0],[0]
"V and k1, . . .",4. Price of Fairness,[0],[0]
", kp, with δ ≤ ε nN0 and N0 := ( k+p−1 p−1 ) .",4. Price of Fairness,[0],[0]
"If n ≥ √ 2k · ( γ σn )2 (with γ := max{σi,1}i, where σi,1 is the largest singular value of VXi and σn is the smallest non-zero singular value of V ) then the price of ensuring fairness is DKL(q?||q) ≤ log 1(1−ε) .",4. Price of Fairness,[0],[0]
"In each simulation, we compare several different probability distributions from which to select k samples from a dataset: As benchmarks we consider the (unconstrained) distributions, k-DPP (see Def 2), and UNIF, which selects a uniformly random subset of size k from the dataset X .",5.1. Algorithms and Baselines,[0],[0]
"We compare this against different methods which select from a fair family of allowed subsets, P -DPP (see Def 3), and ki-DPP (see Def 7 below).
",5.1. Algorithms and Baselines,[0],[0]
Definition 7.,5.1. Algorithms and Baselines,[0],[0]
(ki-DPP),5.1. Algorithms and Baselines,[0],[0]
"Given a dataset X , the corresponding feature vectors V ∈ Rm×n, a partition X = X1 ∪ · · · ∪ Xp into p parts, and numbers k1, . . .",5.1. Algorithms and Baselines,[0],[0]
", kp, kiDPP defines a distribution over k1 + · · ·+ kp-sized subsets S ⊆ X that is a product distribution: for each i, we obtain
a sample Si ⊆ Xi of size ki independently with probability proportional to P[Si]",5.1. Algorithms and Baselines,[0],[0]
"∝ det ( VSiV > Si ) , and combine these samples to output S = S1 ∪ · · · ∪ Sp.",5.1. Algorithms and Baselines,[0],[0]
Algorithms for ki-DPPs are simply obtained by independently using a k-DPP sampler with k = ki on each part Xi.,5.1. Algorithms and Baselines,[0],[0]
"For sampling from all the above listed distribution we use the Sample and Project algorithm as described in Section 3.1.
Metrics.",5.1. Algorithms and Baselines,[0],[0]
"In each simulation, we report the geometric diversity G(·) (see Def 1) and the fairness as measured by the KL-divergence from the desired frequency over parts.",5.1. Algorithms and Baselines,[0],[0]
"Formally, given a probability distribution q over the p parts of the dataset, we define the relative unfairness measure of a set S ⊆ X as Dq(S) := DKL(q||s), where s = (s1, . . .",5.1. Algorithms and Baselines,[0],[0]
", sp) denotes the vector of frequencies, i.e., si = |Xi∩S| |S| for i = 1, 2, . . .",5.1. Algorithms and Baselines,[0],[0]
", p.",5.1. Algorithms and Baselines,[0],[0]
"In particular, typically we want to have Dq(·) as small as possible – ideally equal to 0.",5.1. Algorithms and Baselines,[0],[0]
"When qi = 1/p for all i, we refer to Dq as Dun.",5.1. Algorithms and Baselines,[0],[0]
"When qi = |Xi|/m, we refer to Dq as Dprop.",5.1. Algorithms and Baselines,[0],[0]
Curated Dataset.,5.2. Empirical Results on the Image Dataset,[0],[0]
"We gathered a collection of images curated using Google image search as follows: Four search terms were used: (a) “Scientist Male”, (b) “Scientist Female”, (c) “Painter Male”, and (d) “Painter Female” (Imagedataset).
",5.2. Empirical Results on the Image Dataset,[0],[0]
"Following (Kulesza & Taskar, 2011), each image was processed with the vlfeat toolbox to obtain sets of 128- dimensional SIFT descriptors (Lowe, 1999; Vedaldi & Fulkerson, 2008).",5.2. Empirical Results on the Image Dataset,[0],[0]
All such descriptors are collected in a single set and subsampled to roughly 10% of its total size.,5.2. Empirical Results on the Image Dataset,[0],[0]
The resulting set of ≈ 104 descriptors was clustered using the k-means algorithm where k = 128 is the number of means.,5.2. Empirical Results on the Image Dataset,[0],[0]
"The feature vector for an image is the normalized histogram of the nearest clusters to the descriptors in the image.
",5.2. Empirical Results on the Image Dataset,[0],[0]
Empirical Results on the Biased Datasets.,5.2. Empirical Results on the Image Dataset,[0],[0]
"Our goal is to understand how the bias in the underlying dataset can affect the performance of the different sampling distributions with
respect to fairness and geometric diversity.",5.2. Empirical Results on the Image Dataset,[0],[0]
"We include all female (b and d) images, but vary how many of the male images (a and c) appear in the dataset in order to create biased sets that have between 10% to 50% male images.",5.2. Empirical Results on the Image Dataset,[0],[0]
The male images are selected uniformly at random from the set of all male scientists and male artists for each repetition of the simulation.,5.2. Empirical Results on the Image Dataset,[0],[0]
We sample 40 images from each biased dataset; roughly the number that fits on the first page of an image search result.,5.2. Empirical Results on the Image Dataset,[0],[0]
We conduct 200 repetitions.,5.2. Empirical Results on the Image Dataset,[0],[0]
"We place fairness constraints so that P -DPP and ki-DPP select exactly 50% of their samples from the male (a and c) images and female (b and d) images, regardless of the bias in the underlying dataset.",5.2. Empirical Results on the Image Dataset,[0],[0]
"Note that we do not enforce constraints across scientist (a and b) images and artist (c and d) images, but measure the unfariness Dun(·) with respect to all four attributes.
",5.2. Empirical Results on the Image Dataset,[0],[0]
Results.,5.2. Empirical Results on the Image Dataset,[0],[0]
"With respect to Dun(·), P -DPP significantly outperforms k-DPP, and UNIF (paired one-sided t-tests, p < 0.05), see Figure 1.",5.2. Empirical Results on the Image Dataset,[0],[0]
"As expected, the bias in the underlying dataset can dramatically affect the fairness of UNIF and k-DPP as neither approach is designed to correct for such biases.",5.2. Empirical Results on the Image Dataset,[0],[0]
"However, P -DPP and ki-DPP both enforce fairness constraints; note that this is despite the fact that the sampling was only equal with respect to gender and not profession.",5.2. Empirical Results on the Image Dataset,[0],[0]
"The latter does not appear to affect the outcome here.
",5.2. Empirical Results on the Image Dataset,[0],[0]
"With respect to the diversity G(·), P -DPP has significantly higher G(·) than UNIF and ki-DPP (paired one-sided ttests, p < 0.05).",5.2. Empirical Results on the Image Dataset,[0],[0]
"Moreover, P -DPP performs comparatively to k-DPP; the mean diversity of k-DPP is higher, but not significantly so.",5.2. Empirical Results on the Image Dataset,[0],[0]
"Thus, we observe that, when the underlying data is biased, there is a tradeoff between Dun(·) (for which P -DPP performs best) and G(·) (for which k-DPP performs best); however the differences in geometric diversity are negligible while differences in unfairness can be very large.",5.2. Empirical Results on the Image Dataset,[0],[0]
The Adult Dataset.,5.3. Empirical Results on Real-World Dataset,[0],[0]
"The Adult income dataset (Blake & Merz, 1998) consists of roughly 45000 records of subjects each with 14 features such as age, ethnicity, education and a binary label indicating whether a subject’s incomes is above or below 50K USD.3",5.3. Empirical Results on Real-World Dataset,[0],[0]
"This dataset has been widely studied in the context of fairness (see, (Yang & Stoyanovich, 2017; Zafar et al., 2017; Zemel et al., 2013; Zadrozny, 2004)).
",5.3. Empirical Results on Real-World Dataset,[0],[0]
"In preprocessing the data we filter out incomplete entries, and from the remaining ones we pick a random subset of 5000 records for our simulations.",5.3. Empirical Results on Real-World Dataset,[0],[0]
We vectorize the data as follows: Categorical fields (with a small number of possible values) we turn into sets of binary fields.,5.3. Empirical Results on Real-World Dataset,[0],[0]
As the dimension n of such feature vectors is quite small – 50 – the DPP framework allows sampling sets of cardinality at most k ≤ 50.,5.3. Empirical Results on Real-World Dataset,[0],[0]
"For this reason we enrich the feature vectors in a standard way – by adding pairwise products of all existing features as separate ones – this, after removing redundant columns, yields feature vectors of dimension 992.
",5.3. Empirical Results on Real-World Dataset,[0],[0]
Empirical Results on Equal and Proportional Representation.,5.3. Empirical Results on Real-World Dataset,[0],[0]
We conduct our simulations across either gender or ethnicity as the sensitive attribute.,5.3. Empirical Results on Real-World Dataset,[0],[0]
"For the former, we use the gender categories provided in the dataset; all entries were labeled either male (68.3%) or female (31.7%).",5.3. Empirical Results on Real-World Dataset,[0],[0]
"For the latter, we use the ethnicity categories provided in the dataset; we consider the partition Caucasian (85.7%) and non-Caucasian (14.3%).
",5.3. Empirical Results on Real-World Dataset,[0],[0]
"In addition to the algorithms mentioned above, we report the performance of an additional benchmark ki-UNIF, which selects a uniformly random subset of size ki fromXi.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"In our subsampling, we consider both equal representation, where
3Data downloaded from https://archive.ics.uci.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"edu/ml/datasets/adult.
",5.3. Empirical Results on Real-World Dataset,[0],[0]
"each attribute makes up of 50% of the selected points, and proportional representation, where each attribute is represented with the same ratio as in the original population.
",5.3. Empirical Results on Real-World Dataset,[0],[0]
Results.,5.3. Empirical Results on Real-World Dataset,[0],[0]
We observe that P -DPP has the highest diversity out of all constrained sampling methods regardless of the proportion of representation or sensitive attribute; see Table 1.,5.3. Empirical Results on Real-World Dataset,[0],[0]
"Surprisingly, the diversity of P -DPP matches that of the unconstrained k-DPP for Gender under proportional representation and for Ethnicity under equal representation.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"In the other two settings – Gender under equal representation and Ethnicity under proportional representation – the P - DPP score is lower than that of k-DPP, but minimally so, and outperforms ki-DPP by several standard deviations.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"We note that ki-UNIF, although it has very poor geometric diversity as a whole, performs better under equal representation than it does under proportional representation.",5.3. Empirical Results on Real-World Dataset,[0],[0]
This fact suggests that there could be value in selecting sensitive attributes equally beyond the consideration of fairness.,5.3. Empirical Results on Real-World Dataset,[0],[0]
"The fact that P -DPP performs so well, especially when significantly changing the distribution of sensitive attributes (e.g., for ethnicity, from 14.3% non-Caucasian to 50% nonCaucasian), is quite surprising.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"Overall, it appears that one can support very dramatic changes to the underlying distributions of attributes with minimal or even zero loss to geometric diversity by using our P -DPP algorithm.",5.3. Empirical Results on Real-World Dataset,[0],[0]
"We look at the effect of the scaling of singular values, suggested by Theorem 3, on the sampled subsets of our Algorithm.",5.4. Empirical Results on the Price of Fairness,[0],[0]
"In this simulation we take an instance of random vectors and use different sampling methods to sample a subset from the dataset, and report the Dun(·) and logG(·) value of the sampled subset.",5.4. Empirical Results on the Price of Fairness,[0],[0]
"Following this, we scale the tail singular values of the partition matrices by δ = O(1/n) and again report the Dun(·) and logG(·) values.
",5.4. Empirical Results on the Price of Fairness,[0],[0]
"We also present a heuristic approach, Scale-And-Sample, for constrained sampling which will use any k-DPP algorithm as a sub-routine.",5.4. Empirical Results on the Price of Fairness,[0],[0]
The algorithm is simple.,5.4. Empirical Results on the Price of Fairness,[0],[0]
"For each VXi , scale the smallest (n − ki) singular values by 1/n. Then sample a ∑p i=1 ki sized subset using any k-DPP algorithm.
Results.",5.4. Empirical Results on the Price of Fairness,[0],[0]
The results are presented in Table 2.,5.4. Empirical Results on the Price of Fairness,[0],[0]
"It can be seen that after scaling the tail singular values of the partition matrices, the mean Dun(·) value for k-DPP is very low, and resembles closely the constrained sampling case.",5.4. Empirical Results on the Price of Fairness,[0],[0]
We also note that the Scale-And-Sample approach to constrained sampling suggested earlier performs very well.,5.4. Empirical Results on the Price of Fairness,[0],[0]
The mean relative unfairness measure Dun(·) is almost zero.,5.4. Empirical Results on the Price of Fairness,[0],[0]
"Furthermore, the value of the geometric diversity parameter logG(·) is also similar to unscaled P -DPP.",5.4. Empirical Results on the Price of Fairness,[0],[0]
In this paper we initiated the study of fair and diverse DPPbased sampling for data summarization.,6. Conclusion and Future Work,[0],[0]
We provide a novel and fast algorithm that can sample from a DPP that satisfy fairness constraints based on the desired proportion of samples with a given attribute.,6. Conclusion and Future Work,[0],[0]
Our algorithm gives provably good guarantees when the data matrix satisfies a natural β-balance property.,6. Conclusion and Future Work,[0],[0]
We prove that a large class of datasets satisfy the β-balance condition.,6. Conclusion and Future Work,[0],[0]
"We define a notion of price of fairness, the KL-divergence between the fairness constrained distribution and the unconstrained distribution and theoretically show that, when the data satisfies reasonable properties, this price would be low.",6. Conclusion and Future Work,[0],[0]
"We further show in silico that adding fairness constraints results in minimal loss to diversity, even when the underlying dataset is very biased, or when the proportion of attributes is changed significantly.
",6. Conclusion and Future Work,[0],[0]
"Several challenging problems remain from a technical standpoint; naturally, a first question would be whether the theorems can be improved either by attaining better approximation guarantees, or by weakening the necessary conditions.",6. Conclusion and Future Work,[0],[0]
"Extending these results to arbitrary group structures (as opposed to partitions) would be very relevant, but appears to be significantly more challenging.
",6. Conclusion and Future Work,[0],[0]
"From a practical point of view, it remains to be seen what effect de-biasing a sampler",6. Conclusion and Future Work,[0],[0]
"has on the end result of an ML algorithm (e.g., classification), both on its accuracy and on the output bias.",6. Conclusion and Future Work,[0],[0]
"Indeed, this P -DPP model can be used to pre-process the training data by taking a fair subsample; evaluating the performance of ML algorithms in this regard would be an interesting direction for future research.",6. Conclusion and Future Work,[0],[0]
Sampling methods that choose a subset of the data proportional to its diversity in the feature space are popular for data summarization.,abstractText,[0],[0]
"However, recent studies have noted the occurrence of bias – e.g., under or over representation of a particular gender or ethnicity – in such data summarization methods.",abstractText,[0],[0]
In this paper we initiate a study of the problem of outputting a diverse and fair summary of a given dataset.,abstractText,[0],[0]
We work with a well-studied determinantal measure of diversity and corresponding distributions (DPPs) and present a framework that allows us to incorporate a general class of fairness constraints into such distributions.,abstractText,[0],[0]
"Designing efficient algorithms to sample from these constrained determinantal distributions, however, suffers from a complexity barrier; we present a fast sampler that is provably good when the input vectors satisfy a natural property.",abstractText,[0],[0]
Our empirical results on both real-world and synthetic datasets show that the diversity of the samples produced by adding fairness constraints is not too far from the unconstrained case.,abstractText,[0],[0]
Fair and Diverse DPP-Based Data Summarization,title,[0],[0]
Consider a speech recognizer that is deployed to millions of users.,1. Introduction,[0],[0]
"State-of-the art speech recognizers achieve high overall accuracy, yet it is well known that such systems have systematically high errors on minority accents (Amodei et al., 2016).",1. Introduction,[0],[0]
"We refer to this phenomenon of high overall accuracy but low minority accuracy as a representation disparity, which is the result of optimizing for average loss.",1. Introduction,[0],[0]
"This representation disparity forms our definition of unfairness, and has been observed in face recognition (Grother et al., 2011), language identification (Blodgett et al., 2016;
1Department of Computer Science, Stanford, USA 2Department of Statistics, Stanford, USA 3Management Science & Engineering, Stanford, USA.",1. Introduction,[0],[0]
"Correspondence to: Tatsunori Hashimoto <thashim@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Jurgens et al., 2017), dependency parsing (Blodgett et al., 2016), part-of-speech tagging (Hovy & Sgaard, 2015), academic recommender systems (Sapiezynski et al., 2017), and automatic video captioning (Tatman, 2017).
",1. Introduction,[0],[0]
"Moreover, a minority user suffering from a higher error rate will become discouraged and more likely to stop using the system, thus no longer providing data to the system.",1. Introduction,[0],[0]
"As a result, the minority group will shrink and might suffer even higher error rates from a retrained model in a future time step.",1. Introduction,[0],[0]
"Machine learning driven feedback loops have been observed in predictive policing (Fuster et al., 2017) and credit markets (Fuster et al., 2017), and this problem of disparity amplification is a possibility in any deployed machine learning system that is retrained on user data.
",1. Introduction,[0],[0]
"In this paper, we aim to mitigate the representation disparity problem and its amplification through time.",1. Introduction,[0],[0]
"We focus on the following setting: at each time step, each user interacts with the current model and incurs some loss, based on which she decides to keep or quit using the service.",1. Introduction,[0],[0]
A model is trained on the resulting user data which is used at the next time step.,1. Introduction,[0],[0]
"We assume that each user comes from one of K groups, and our goal is to minimize the worst case risk of any group across time.",1. Introduction,[0],[0]
"However, the group membership and number of groups K are both unknown, as full demographic information is likely missing in real online services.
",1. Introduction,[0],[0]
We first show that empirical risk minimization (ERM) does not control the worst-case risk over the disparate K groups and show examples where ERM turns initially fair models unfair (Section 3).,1. Introduction,[0],[0]
"To remedy this issue, we propose the use of distributionally robust optimization (DRO) (Section 4).",1. Introduction,[0],[0]
"Given a lower bound on the smallest group proportion, we show that optimizing the worst-case risk over an appropriate chi-square divergence ball bounds the worst-case risk over groups.",1. Introduction,[0],[0]
"Our approach is computationally efficient, and can be applied as a small modification to a wide class machine learning models trained by stochastic gradient descent methods.",1. Introduction,[0],[0]
"We show that DRO succeeds on the examples where ERM becomes unfair, and demonstrate higher average minority user satisfaction and lower disparity amplification on a Amazon Mechanical Turk based autocomplete task.",1. Introduction,[0],[0]
"Recently, there has been a surge of interest in fairness in machine learning (Barocas & Selbst, 2016).",1.1. Fairness in Machine Learning,[0],[0]
"Our work can be seen as a direct instantiation of John Rawls’ theory on distributive justice and stability, where we view predictive accuracy as a resource to be allocated.",1.1. Fairness in Machine Learning,[0],[0]
"Rawls argues that the difference principle, defined as maximizing the welfare of the worst-off group, is fair and stable over time since it ensures that minorities consent to and attempt to maintain the status quo (Rawls, 2001, p155).
",1.1. Fairness in Machine Learning,[0],[0]
"In this work, we assume the task is general loss minimization, and demographic data is unavailable.",1.1. Fairness in Machine Learning,[0],[0]
"This differs from the substantial body of existing research into fairness for classification problems involving protected labels such as the use of race in recidivism protection (Chouldechova, 2017).",1.1. Fairness in Machine Learning,[0],[0]
"There has been extensive work (Barocas & Selbst, 2016) on guaranteeing fairness for classification over a protected label through constraints such as equalized odds (Woodworth et al., 2017; Hardt et al., 2016), disparate impact (Feldman et al., 2015) and calibration (Kleinberg et al., 2017).",1.1. Fairness in Machine Learning,[0],[0]
"However, these approaches require the use of demographic labels, and are designed for classification tasks.",1.1. Fairness in Machine Learning,[0],[0]
"This makes it difficult to apply such approaches to mitigate representation disparity in tasks such as speech recognition or natural language generation where full demographic information is often unavailable.
",1.1. Fairness in Machine Learning,[0],[0]
"A number of authors have also studied individual notions of fairness, either through a fixed similarity function (Dwork et al., 2012) or subgroups of a set of protected labels (Kearns et al., 2018; Hébert-Johnson et al., 2017).",1.1. Fairness in Machine Learning,[0],[0]
"Dwork et al. (2012) provides fairness guarantees without explicit groups, but requires a fixed distance function which is difficult to define for real-world tasks.",1.1. Fairness in Machine Learning,[0],[0]
"Kearns et al. (2018); HébertJohnson et al. (2017) consider subgroups of a set of protected features, but defining non-trivial protected features which cover the latent demographics in our setting is difficult.",1.1. Fairness in Machine Learning,[0],[0]
"Although these works generalize the demographic group structure, similarity and subgroup structure are both ill-defined for many real-world tasks.
",1.1. Fairness in Machine Learning,[0],[0]
"In the online setting, works on fairness in bandit learning (Joseph et al., 2016; Jabbari et al., 2017) propose algorithms compatible with Rawls’ principle on equality of opportunity—an action is preferred over another only if the true quality of the action is better.",1.1. Fairness in Machine Learning,[0],[0]
"Our work differs in considering Rawlsian fairness for distributive justice (Rawls, 2009).",1.1. Fairness in Machine Learning,[0],[0]
"Simultaneous with our work, Liu et al. (2018) analyzed fairness over time in the context of constraint based fairness criteria, and show that enforcing static fairness constraints do not ensure fairness over time.",1.1. Fairness in Machine Learning,[0],[0]
"In this paper, we consider latent demographic groups and study a loss-based approach to fairness and stability.",1.1. Fairness in Machine Learning,[0],[0]
"We begin by outlining the two parts of our motivation: representation disparity and disparity amplification.
",2. Problem setup,[0],[0]
"Representation disparity: Consider the standard lossminimization setting where a user makes a query Z ∼ P , a model θ ∈ Θ makes a prediction, and the user incurs loss `(θ;Z).",2. Problem setup,[0],[0]
We denote the expected loss as the risk R(θ) = EZ∼P,2. Problem setup,[0],[0]
[`(θ;Z)].,2. Problem setup,[0],[0]
The observations Z are assumed to arise from one of K latent groups such that Z ∼ P := ∑ k∈[K] αkPk.,2. Problem setup,[0],[0]
We assume that neither the population proportions {αk} nor the group distributions {Pk} are known.,2. Problem setup,[0],[0]
"The goal is to control the worst-case risk over all K groups:
Rmax(θ) = max k∈[K] Rk(θ), Rk(θ) := EPk",2. Problem setup,[0],[0]
[`(θ;Z)].,2. Problem setup,[0],[0]
"(1)
Representation disparity refers to the phenomenon of low R(θ) and highRmax(θ) due to a group with small αk.
",2. Problem setup,[0],[0]
"Disparity amplification: To understand the amplification of representation disparity over time, we will make several assumptions on the behavior of users in response to observed losses.",2. Problem setup,[0],[0]
These assumptions are primarily for clarity of exposition—we will indicate whenever the assumptions can be relaxed leave generalizations to the supplement.,2. Problem setup,[0],[0]
"Roughly speaking, minimizing the worst-case risk Rmax(θ) should mitigate disparity amplification as long as lower losses lead to higher user retention.",2. Problem setup,[0],[0]
"We now give assumptions that make this intuition precise.
",2. Problem setup,[0],[0]
"In the sequential setting, loss minimization proceeds over t = 1, 2, . . .",2. Problem setup,[0],[0]
"T rounds, where the group proportion α(t)k depends on t and varies according to past losses.",2. Problem setup,[0],[0]
"At each round λ(t+1)k is the expected number of users from group k, which is determined by ν(Rk(θ)), the fraction of users retained, and bk, the number of new users (see Definition 1).",2. Problem setup,[0],[0]
"Here, ν is a differentiable, strictly decreasing retention function which maps a risk levelR to the fraction of users who continue to use the system.",2. Problem setup,[0],[0]
"Modeling user retention as a decreasing function of the risk implies that each user makes an independent decision of whether to interact with the system at time t+ 1 based on their expected loss at time t. For example, selecting ν(x) = 1− x andRk equal to the expected zero-one loss implies that users leave proportional to the misclassification rates of their queries.
",2. Problem setup,[0],[0]
At each round we learn parameters θ(t+1) based on n(t+1),2. Problem setup,[0],[0]
∼ Pois( ∑ k λ (t+1) k ) users (data points).,2. Problem setup,[0],[0]
"While we define the sample size as a Poisson process for concreteness, our main results hold for any distribution fulfilling the strong law of large numbers, as we perform all stability analyses in the population limit.
",2. Problem setup,[0],[0]
Definition 1 (Dynamics).,2. Problem setup,[0],[0]
"Given a sequence θ(t), for each t = 1 . . .",2. Problem setup,[0],[0]
"T , the expected number of users λ and samples
Z (t) i starting at λ (0)",2. Problem setup,[0],[0]
k,2. Problem setup,[0],[0]
"= bk is governed by:
λ (t+1) k",2. Problem setup,[0],[0]
:= λ (t) k ν(Rk(θ,2. Problem setup,[0],[0]
(t))),2. Problem setup,[0],[0]
"+ bk
α (t+1) k",2. Problem setup,[0],[0]
":= λ (t+1) k∑
k′∈[K] λ (t+1) k′
n(t+1)",2. Problem setup,[0],[0]
":= Pois( ∑ k λ (t+1) k )
Z (t+1) 1 . . .",2. Problem setup,[0],[0]
"Z (t+1) n(t+1) i.i.d.∼ P (t+1) := ∑ k∈[K] α (t+1) k Pk.
",2. Problem setup,[0],[0]
"If we use ERM at each time step the parameter sequence is defined as θ(t) = arg minθ∈Θ ∑ i `(θ;Z (t) i ).
",2. Problem setup,[0],[0]
"Our goal is to control over all groups k = 1, . . .",2. Problem setup,[0],[0]
",K and time periods t = 1, . . .",2. Problem setup,[0],[0]
", T the group-wise riskRk(θ(t)),
RTmax(θ(0), · · · , θ(T ))",2. Problem setup,[0],[0]
"= max k,t
{ Rk(θ(t)) } .",2. Problem setup,[0],[0]
"(2)
Without knowledge of group membership labels, population proportions α(t)k , new user rate bk, and retention rate ν, minimizingRTmax gives rise to two major challenges.",2. Problem setup,[0],[0]
"First, without group membership labels there is no way to directly measure the worst-case risk RTmax, let alone minimize it.",2. Problem setup,[0],[0]
"Second, we must ensure that the group proportions α(t)k are stable, since if α(t)k → 0 as t→∞ for some group k ∈",2. Problem setup,[0],[0]
"[K], then no algorithm can controlRTmax when a group has near zero probability of appearing in our samples.
",2. Problem setup,[0],[0]
We begin by illustrating how models that are initially fair with low representation disparity may become unfair over time if we use ERM (Section 3).,2. Problem setup,[0],[0]
"We then propose a solution based on distributionally robust optimization (Section 4), and study examples where this approach mitigates representation disparity in our experimental section (Section 5).",2. Problem setup,[0],[0]
The standard approach to fitting a sequence of models θ(t) is to minimize an empirical approximation to the population risk at each time period.,3. Disparity amplification,[0],[0]
"In this section, we show that even minimizing the population risk fails to control minority risk over time, since expected loss (average case) leads to disparity amplification.",3. Disparity amplification,[0],[0]
"The decrease in user retention for the minority group is exacerbated over time since once a group shrinks sufficiently, it receives higher losses relative to others, leading to even fewer samples from the group.",3. Disparity amplification,[0],[0]
"Consider the two-class classification problem in Figure 1 where the two groups are drawn from Gaussians and the
optimal classification boundary is given along x2 = 0.",3.1. Motivating example,[0],[0]
"Assume that the sampling distribution evolves according to definition 1 with ν(x) = 1.0−x, ` equal to the zero one loss, and b0 = b1 = n (0) 0",3.1. Motivating example,[0],[0]
= n (0) 1 = 1000.,3.1. Motivating example,[0],[0]
"Initially, ERM has similar and high accuracy on both groups with the boundary x2 > 0, but over time random fluctuations in accuracy result in slightly fewer samples from the cluster on the right.",3.1. Motivating example,[0],[0]
This leads to disparity amplification since ERM will further improve the loss on the left cluster at the expense of the right cluster.,3.1. Motivating example,[0],[0]
"After 500 rounds, there are nearly no samples from the right cluster, and as a result, the right cluster ends up suffering high loss.",3.1. Motivating example,[0],[0]
The example above demonstrated that disparity amplification can occur easily even in a situation where the two groups have identical population size and initial risk.,3.2. Conditions for disparity amplification,[0],[0]
"In general if we view the expected user counts λ(t) as a dynamical system, the long-term fairness properties for any fairness criteria are controlled by two factors - whether λ has a fair fixed point (defined as a population fraction where risk minimization maintains the same population fraction over time) and whether this fixed point is stable.
",3.2. Conditions for disparity amplification,[0],[0]
"Fixed points of risk minimization are determined by a combination of user retention function ν and the models θ(t), and without knowledge of ν it is hard to ensure that a model has a fair fixed point.",3.2. Conditions for disparity amplification,[0],[0]
"Even if a fixed point is fair, such as when the population fraction and risk received by each group is equal, and we start at this fair fixed point, minimizing the empirical loss may deviate from this fair fixed point over time due to finite sample fluctuations or noise in the model estimation procedure.
",3.2. Conditions for disparity amplification,[0],[0]
"To show this result, we study the dynamical system Φ, which is defined by dynamics in Definition 1 with θ derived from minimizing the population, rather than empirical risk.
",3.2. Conditions for disparity amplification,[0],[0]
Definition 2.,3.2. Conditions for disparity amplification,[0],[0]
"Let Φ be the update for the expected population size
λ (t+1)",3.2. Conditions for disparity amplification,[0],[0]
k,3.2. Conditions for disparity amplification,[0],[0]
:= Φ(λ (t) k ),3.2. Conditions for disparity amplification,[0],[0]
= λ (t) k ν(Rk(θ(λ (t) k ))),3.2. Conditions for disparity amplification,[0],[0]
"+ bk,
θ(λ (t) k ) = arg",3.2. Conditions for disparity amplification,[0],[0]
min,3.2. Conditions for disparity amplification,[0],[0]
θ E∑,3.2. Conditions for disparity amplification,[0],[0]
k α (t) k,3.2. Conditions for disparity amplification,[0],[0]
Pk,3.2. Conditions for disparity amplification,[0],[0]
"[`(θ;Z)].
",3.2. Conditions for disparity amplification,[0],[0]
"The arrival intensity λ∗ is called a fixed point if λ∗ = Φ(λ∗).
",3.2. Conditions for disparity amplification,[0],[0]
"This fixed point is stable whenever the maximum modulus of the eigenvalues of the Jacobian of Φ is less than one and unstable whenever it is greater than one (Luo, 2012, Theorem 2.1).
",3.2. Conditions for disparity amplification,[0],[0]
Proposition 1 gives a precise statement of this phenomenon.,3.2. Conditions for disparity amplification,[0],[0]
"We prove the result in Section A.1, and further show a generalization to general dynamics Φ(λk) = h(λk,Rk) where h is differentiable and monotone in the second argument.",3.2. Conditions for disparity amplification,[0],[0]
"We denote by ρmax(A) the maximum modulus of the eigenvalues of A.
Proposition 1.",3.2. Conditions for disparity amplification,[0],[0]
"Let λ∗ = Φ(λ∗) be a fixed point, and θ∗ = arg minθ E∑
k α ∗ kPk
[`(θ;Z)] be the minimizer at λ∗.
Define HR(α∗) as the positive definite Hessian of the expected risk at θ∗, λ∗ and define∇L as the per-group parameter gradients at θ∗,
∇L = ∇θEP1",3.2. Conditions for disparity amplification,[0],[0]
"[`(θ ∗;Z)]
... ∇θEPk",3.2. Conditions for disparity amplification,[0],[0]
[`(θ∗;Z)]  .,3.2. Conditions for disparity amplification,[0],[0]
"The arrival intensity λ∗ is unstable whenever
ρmax
( diag(ν(R(θ(λ∗))))− diag(λ∗ν′(R(θ(λ∗))
",3.2. Conditions for disparity amplification,[0],[0]
"∇LHR(α∗)−1∇L> (
I∑ k λ ∗ k",3.2. Conditions for disparity amplification,[0],[0]
− 1λ ∗>,3.2. Conditions for disparity amplification,[0],[0]
"( ∑ k λ ∗ k) 2
))",3.2. Conditions for disparity amplification,[0],[0]
"> 1.
",3.2. Conditions for disparity amplification,[0],[0]
"We see that the major quantities which control risk are the retention rate ν and its derivative, as well as a K × K square matrix ∇LHR(α∗)−1∇L> which roughly encodes the changes in one group’s risk as a function of another.
",3.2. Conditions for disparity amplification,[0],[0]
We can specialize the stability condition to obtain an intuitive and negative result for the stability of risk minimization (average case).,3.2. Conditions for disparity amplification,[0],[0]
"Even if we start at a fair fixed point with λ∗1 = · · · = λ∗k and R1 = · · · = Rk, if decreasing the risk for one group increases the risk for others sufficiently, the fixed point is unstable and the model will eventually converge to a different, possibly unfair, fixed point.
",3.2. Conditions for disparity amplification,[0],[0]
Corollary 1 (Counterexample under symmetry).,3.2. Conditions for disparity amplification,[0],[0]
"Let λ∗1 = · · · = λ∗k be a fixed point with R1 = · · · = Rk, then for any strongly convex loss,
ρmax ( ∇LHR(α∗)−1∇L> )",3.2. Conditions for disparity amplification,[0],[0]
">
1− ν(R1) −ν′(R1)/k .",3.2. Conditions for disparity amplification,[0],[0]
"(3)
is a sufficient condition for instability.
",3.2. Conditions for disparity amplification,[0],[0]
"See Section A.2 for proof and generalizations.
",3.2. Conditions for disparity amplification,[0],[0]
The bound (3) has a straightforward interpretation.,3.2. Conditions for disparity amplification,[0],[0]
"The left hand side is the stability of the model, where maximal eigenvalue of the matrix∇LHR(α∗)−1∇L> represents the maximum excess risk that can be incurred due to a small
perturbation in the mixture weights α.",3.2. Conditions for disparity amplification,[0],[0]
"The right hand side represents the underlying stability of the dynamics and measures the sensitivity of λ with respect to risk.
",3.2. Conditions for disparity amplification,[0],[0]
"Mean and median estimation: Consider a simple mean estimation example where each user belongs to one of two groups, −1 or 1 and incurs loss (θ − Z)2. θ",3.2. Conditions for disparity amplification,[0],[0]
"= 0 is clearly a fair fixed point, since it equalizes losses to both groups, with Hrisk(α∗) = 1/2 and ∇L =",3.2. Conditions for disparity amplification,[0],[0]
"[2,−2] mak-
ing ρmax ( ∇LHR(α∗)−1∇L> )",3.2. Conditions for disparity amplification,[0],[0]
= 4.,3.2. Conditions for disparity amplification,[0],[0]
"If we select ν(x) =
exp(−x), the right hand side becomes 2(1− e−1)e ≈ 3.4, and thus any perturbation will eventually result in λ1 6= λ2.",3.2. Conditions for disparity amplification,[0],[0]
"In this case the only other fixed points are the unfair solutions of returning the mean of either one of the groups.
",3.2. Conditions for disparity amplification,[0],[0]
"The situation is even worse for models which are not strongly convex, such as median estimation.",3.2. Conditions for disparity amplification,[0],[0]
Replacing the squared loss above with the absolute value results in a loss which has a non-unique minimizer at 0 when λ1 = λ2 but immediately becomes −1 whenever λ1 > λ2.,3.2. Conditions for disparity amplification,[0],[0]
"In this case, no conditions on the retention function ν can induce stability.",3.2. Conditions for disparity amplification,[0],[0]
This fundamental degeneracy motivates us to search for loss minimization schemes with better stability properties than ERM (average case).,3.2. Conditions for disparity amplification,[0],[0]
Recall that our goal is to control the worst-case risk (2) over all groups and over all time steps t. We will proceed in two steps.,4. Distributionally robust optimization (DRO),[0],[0]
"First, we show that performing distributionally robust optimization controls the worst-case riskRmax(θ(t)) for a single time step.",4. Distributionally robust optimization (DRO),[0],[0]
"Then, we show that this results in a lower bound on group proportions {α(t)k }Kk=1, and thus ensures control over the worst-case risk for all time steps.",4. Distributionally robust optimization (DRO),[0],[0]
"As a result of the two steps, we show in Section 4.4 that our procedure mitigates disparity amplification over all time steps.",4. Distributionally robust optimization (DRO),[0],[0]
"For notational clarity, we omit the superscript t in Sections 4.1-4.3.",4. Distributionally robust optimization (DRO),[0],[0]
The fundamental difficulty in controlling the worst-case group risk over a single time-step Rmax(θ(t)) comes from not observing the group memberships from which the data was sampled.,4.1. Bounding the risk over unknown groups,[0],[0]
"For many machine learning systems such as speech recognition or machine translation, such situations are common since we either do not ask for sensitive demographic information, or it is unclear a priori which demographics should be protected.",4.1. Bounding the risk over unknown groups,[0],[0]
"To achieve reasonable performance across different groups, we postulate a formulation that protects against all directions around the data generating distribution.",4.1. Bounding the risk over unknown groups,[0],[0]
"We build on the distributionally robust formulation of Duchi et al. (2016) which will allow us to control the worst-case group riskRmax(θ(t)).
",4.1. Bounding the risk over unknown groups,[0],[0]
"To formally describe our approach, let Dχ2 (P ||Q) be the χ2-divergence between probability distributions P and
Q given by Dχ2 (P ||Q)",4.1. Bounding the risk over unknown groups,[0],[0]
":= ∫ ( dP dQ − 1 )2 dQ. If P is not absolutely continuous with respect to Q, we define Dχ2 (P ||Q)",4.1. Bounding the risk over unknown groups,[0],[0]
":=∞.
Let B(P, r) be the chi-squared ball around a probability distribution P of radius r so that B(P, r) := {Q P : Dχ2 (Q||P ) ≤",4.1. Bounding the risk over unknown groups,[0],[0]
r}.,4.1. Bounding the risk over unknown groups,[0],[0]
"We consider the worst-case loss over all r-perturbations around P ,
Rdro(θ; r) := sup Q∈B(P,r) EQ[`(θ;Z)].",4.1. Bounding the risk over unknown groups,[0],[0]
"(4)
Intuitively, the distributionally robust risk Rdro(θ; r) upweights examples Z with high loss `(θ;Z).",4.1. Bounding the risk over unknown groups,[0],[0]
"If there is a group suffering high loss, the corresponding mixture component will be over-represented (relative to the original mixture weights) in the distributionally robust risk Rdro(θ; r).",4.1. Bounding the risk over unknown groups,[0],[0]
"We show in the following proposition that Rdro(θ; r) bounds the risk of each group Rk(θ), and hence the group-wise worst-case risk (1), for an appropriate choice of the robustness radius r.
Proposition 2.",4.1. Bounding the risk over unknown groups,[0],[0]
"For P := ∑ k∈[K] αkPk, we haveRk(θ) ≤",4.1. Bounding the risk over unknown groups,[0],[0]
"Rdro(θ; rk) for all θ ∈ Θ where rk := (1/αk − 1)2 is the robustness radius.
",4.1. Bounding the risk over unknown groups,[0],[0]
We prove the result in Section A.4.,4.1. Bounding the risk over unknown groups,[0],[0]
"Roughly speaking, the above bound becomes tighter if the variation in the loss `(θ;Z) is substantially higher between groups than within each group.",4.1. Bounding the risk over unknown groups,[0],[0]
"In particular, this would be the case if the loss distribution for each group have distinct support with relatively well-concentrated components within each group.
",4.1. Bounding the risk over unknown groups,[0],[0]
"As a consequence of Proposition 2, if we have a lower bound on the group proportions αmin ≤ mink∈[K] αk, then we can control the worst-case group risk Rmax(θ) by minimizing the upper bound θ 7→ Rdro(θ; rmax) where rmax := (1/αmin − 1)2.
",4.1. Bounding the risk over unknown groups,[0],[0]
"Similar formulations for robustness around the empirical distribution with radius shrinking as r/n had been considered in (Ben-Tal et al., 2013; Lam & Zhou, 2015; Duchi & Namkoong, 2016).",4.1. Bounding the risk over unknown groups,[0],[0]
"While there are many possible robustness balls B which could provide upper bounds on group risk, we opt to use the chi-squared ball since it is straightforward to optimize (Ben-Tal et al., 2013; Namkoong & Duchi, 2016; 2017) and we found it empirically outperformed other f -divergence balls.",4.1. Bounding the risk over unknown groups,[0],[0]
"The dual of the maximization problem (4) provides additional intuition on the behavior of the robust risk.
",4.2. Interpreting the dual,[0],[0]
"Proposition 3 ((Duchi & Namkoong, 2018)).",4.2. Interpreting the dual,[0],[0]
"If `(θ; ·) is upper semi-continuous for any θ, then for rmax ≥ 0 and
any θ,Rdro(θ; rmax) is equal to the following expression
inf η∈R
{ F (θ; η) := C ( EP [ [`(θ, Z)− η]2+ ])",4.2. Interpreting the dual,[0],[0]
"1 2 + η } (5)
where C = ( 2(1/αmin − 1)2 + 1 )1/2 .
Denoting by η?",4.2. Interpreting the dual,[0],[0]
"the optimal dual variable (5), we see from the proposition that all examples suffering less than η?levels of loss are completely ignored, and large losses above η?",4.2. Interpreting the dual,[0],[0]
"are upweighted due to the squared term.
",4.2. Interpreting the dual,[0],[0]
"However, unlike standard parameter regularization techniques, which encourage θ to be close to some point, our objective biases the model to have fewer high loss examples which matches our goal of mitigating representation disparity.
",4.2. Interpreting the dual,[0],[0]
Median Estimation: Recall the median estimation problem over two groups mentioned in Section 3.2 where the loss is `(θ;Z) =,4.2. Interpreting the dual,[0],[0]
‖θ − Z‖1.,4.2. Interpreting the dual,[0],[0]
Figure 2 shows the behavior of both ERM and DRO on this median estimation task with unbalanced (αmin = 0.1) groups.,4.2. Interpreting the dual,[0],[0]
The parameter estimate which minimizes Rmax for this problem is θfair = 0,4.2. Interpreting the dual,[0],[0]
since this is equidistant from both groups.,4.2. Interpreting the dual,[0],[0]
ERM on the other hand focuses entirely on the majority and returns θERM,4.2. Interpreting the dual,[0],[0]
"≈ −1.0.
DRO returns θ∗DRO which is close to θfair.",4.2. Interpreting the dual,[0],[0]
"Analyzing the risk, we find that the single-step worst-case group riskRmax(θ) in (1) is an upper bound on ERM, and DRO forms a tight upper bound this quantity (Figure 2b).",4.2. Interpreting the dual,[0],[0]
We can also understand the behavior of DRO through the worst-case distribution Q in Equation 4.,4.2. Interpreting the dual,[0],[0]
Figure 2a shows the worst-case distribution Q at the minimizer θ∗DRO which completely removes points within distance η∗.,4.2. Interpreting the dual,[0],[0]
"Additionally, points far from θ∗DRO are upweighted, resulting in a large contribution to the loss from the minority group.
",4.2. Interpreting the dual,[0],[0]
We expect the bound to be tight when all individuals within a group receive the same loss.,4.2. Interpreting the dual,[0],[0]
"In this case, thresholding
by η∗ corresponds to selecting the single highest risk group which is equivalent to directly minimizingRmax(θ) (1).
",4.2. Interpreting the dual,[0],[0]
"On the other hand, the worst case for our approach is if αmin is small, and a group with low expected loss has a high loss tail with population size αmin.",4.2. Interpreting the dual,[0],[0]
"In this case DRO is a loose upper bound and optimizes the losses of the group with already low expected loss.
",4.2. Interpreting the dual,[0],[0]
"This is closely related to recent observations that the DRO bound can be loose for classification losses such as the zeroone loss due to the worst-case distribution consisting purely of misclassified examples (Hu et al., 2018).",4.2. Interpreting the dual,[0],[0]
"Even in this case, the estimated loss is still a valid upper bound on the worst case group risk, and as Figure 2 shows, there are examples where the DRO estimate is nearly tight.",4.2. Interpreting the dual,[0],[0]
We now show how to minimize θ 7→ Rdro(θ; rmax) efficiently for a large class of problems.,4.3. Optimization,[0],[0]
"For models such as deep neural networks that rely on stochastic gradient descent, the dual objective F (θ; η) in (5) can be used directly since it only involves an expectation over the data generating distribution P .
",4.3. Optimization,[0],[0]
"Formally, the following procedure optimizes (4): for a given value of η, compute the approximate minimizer θ̂η
minimize θ∈Θ
EP [`(θ;Z)− η]2+ .",4.3. Optimization,[0],[0]
"(6)
From Propositions 2 and 3, we have
Rmax(θ̂η) ≤ Rdro(θ̂η; rmax) ≤",4.3. Optimization,[0],[0]
"F (θ̂η, η)
which implies that we can treat η as a hyperparameter.",4.3. Optimization,[0],[0]
"For convex losses θ 7→ `(θ;Z), the function η 7→ F (θ̂η, η) is convex, and thus we can perform a binary search over η to find the global optimum efficiently.
",4.3. Optimization,[0],[0]
"Alternatively, for models where we can compute θ∗(Q) ∈ argminθ∈Θ EQ[`(θ;Z)] efficiently, we can use existing primal solvers that compute the worst-case probability distribution Q∗(θ) ∈ argmaxQ∈B(P,r) EQ[`(θ;Z)] for a given θ based on projected gradient ascent on Q (Namkoong & Duchi, 2016).",4.3. Optimization,[0],[0]
"By alternating between optimization on θ and Q, we can efficiently find the saddle point (θ∗, Q∗) that satisfies θ∗ = θ∗(Q∗) and Q∗ = Q∗(θ∗).",4.3. Optimization,[0],[0]
"We have thus far demonstrated that for a single time step, the worst-case risk over all groups Rmax(θ) = maxkRk(θ) can be controlled by the distributionally robust riskRdro(θ; rmax) where rmax := (1/αmin",4.4. Stability of minority loss minimization,[0],[0]
− 1)2 and αmin is the minority group proportion.,4.4. Stability of minority loss minimization,[0],[0]
"Now, we study how the individual group risk Rk(θ) affects user retention and
hence future risk.",4.4. Stability of minority loss minimization,[0],[0]
"By virtue of providing an upper bound toRmax(θ), optimizingRdro(θ; rmax) at each time step can thus control the future group riskRmax(θ).
",4.4. Stability of minority loss minimization,[0],[0]
"We show that if the initial group proportions satisfy α(0)k ≥ αmin and the worst-case riskRmax(θ(t)) is sufficiently small at each time t, then we can ensure α(t+1)k > αmin.",4.4. Stability of minority loss minimization,[0],[0]
"Thus, to controlRTmax, the worst-case group risk over all time steps, it suffices to control Rdro(θ(t); rmax) using the procedure in Section 4.3.
",4.4. Stability of minority loss minimization,[0],[0]
Proposition 4.,4.4. Stability of minority loss minimization,[0],[0]
Assume the retention model in Definition 1.,4.4. Stability of minority loss minimization,[0],[0]
Let α(t)k >,4.4. Stability of minority loss minimization,[0],[0]
"αmin, bk∑ k bk > αmin, λ(t) := ∑ k λ (t) k ≤",4.4. Stability of minority loss minimization,[0],[0]
∑,4.4. Stability of minority loss minimization,[0],[0]
"k bk 1−νmax , and ν(Rk(θ(t)))",4.4. Stability of minority loss minimization,[0],[0]
< νmax.,4.4. Stability of minority loss minimization,[0],[0]
"Then, whenever we have
Rk(θ(t)) ≤",4.4. Stability of minority loss minimization,[0],[0]
"ν−1 (
1− (1− νmax)bk αmin ∑ k bk
) ,
α (t+1) k = λ(t)α",4.4. Stability of minority loss minimization,[0],[0]
(t) k ν(Rk(θ(t))),4.4. Stability of minority loss minimization,[0],[0]
+,4.4. Stability of minority loss minimization,[0],[0]
"bk∑
l λ (t)α (t) l ν(Rl(θ(t)))",4.4. Stability of minority loss minimization,[0],[0]
"+ bl
> αmin.
",4.4. Stability of minority loss minimization,[0],[0]
"We conclude that as long as we can guarantee
Rdro(θ(t); rmax) ≤ ν−1 (
1− (1− νmax)bk αmin ∑ k bk
) , (7)
we can control RTmax(θ(0), . . .",4.4. Stability of minority loss minimization,[0],[0]
", θ(T )), the unknown worstcase group risk over all time steps by optimizing Rdro(θ(t); rmax) at each step t.",4.4. Stability of minority loss minimization,[0],[0]
"While the condition (7) is hard to verify in practice, we observe empirically in Section 5 that optimizing the distributionally robust risk Rdro(θ(t); rmax) at time step t indeed significantly reduces disparity amplification in comparison to using ERM.
",4.4. Stability of minority loss minimization,[0],[0]
Proposition 4 gives stronger fairness guarantees than the stability conditions for ERM in Proposition 1.,4.4. Stability of minority loss minimization,[0],[0]
In ERM the best one can do is to add strong convexity to the model to stabilize to a possibly unfair fixed point.,4.4. Stability of minority loss minimization,[0],[0]
"In contrast, Proposition 4 gives conditions for controlling Rmax over time without assuming that there exists a fair fixed point.
",4.4. Stability of minority loss minimization,[0],[0]
"Stability of median estimation: Returning to our running example of geometric median estimation, we can show that under the same dynamics, ERM is highly unstable while DRO is stable.",4.4. Stability of minority loss minimization,[0],[0]
"Consider a three Gaussian mixture on the corners of the simplex, with L2 loss, retention function ν(r) = exp(−r), and b1 = b2 = 50, n(t) = 1000.",4.4. Stability of minority loss minimization,[0],[0]
"By construction, (1/3, 1/3, 1/3) is the fair parameter estimate.
",4.4. Stability of minority loss minimization,[0],[0]
"Figure 3 shows that ERM is highly unstable, with the only stable fixed points being the corners, where a single group dominates all others.",4.4. Stability of minority loss minimization,[0],[0]
"The fair parameter estimate is an unstable fixed point for ERM, and any perturbation eventually results in a completely unfair parameter estimate.",4.4. Stability of minority loss minimization,[0],[0]
"On the other hand, DRO has the reverse behavior, with the fair parameter estimate being the unique stable fixed point.",4.4. Stability of minority loss minimization,[0],[0]
We demonstrate the effectiveness of DRO on our motivating example (Figure 1) and human evaluation of a text autocomplete system on Amazon Mechanical Turk.,5. Experiments,[0],[0]
"In both cases, DRO controls the worst-case riskRTmax",5. Experiments,[0],[0]
over time steps,5. Experiments,[0],[0]
and improves minority retention.,5. Experiments,[0],[0]
"Recall the motivating example in Figure 1 which shows that logistic regression applied to a two-class classification problem is unstable and becomes pathologically unfair.
",5.1. Simulated task,[0],[0]
"The data is constructed by drawing from a mixture of two Gaussians (groups) centered at (−1.5, 0) and (0, 1.5).",5.1. Simulated task,[0],[0]
"The two groups are labeled according to the linear decision boundaries (−3/2, √ 32 − 1/3) and (3/2, √ 32 − 1/3) respectively such that classifying with x2 > 0 is accurate, but the optimal linear classifier on one group achieves 50% accuracy on the other.
",5.1. Simulated task,[0],[0]
"At each round we fit a logistic regression classifier using ERM or DRO and gradient descent, constraining the norm of the weight vector to 1.",5.1. Simulated task,[0],[0]
"Our dynamics follow Definition 1 with ν(x) = 1− x,R as the zero-one loss, and bk = 1000.",5.1. Simulated task,[0],[0]
"The DRO model is trained using the dual objective with logistic loss, and η = 0.95, which was the optimal dual solution to αmin = 0.2.",5.1. Simulated task,[0],[0]
"The results do not qualitatively change for choices of αmin < 0.5, and we show that we obtain control even for group sizes substantially smaller than 0.2 (Figure 6).
",5.1. Simulated task,[0],[0]
Figure 5 shows that ERM is unstable and the minority group rapidly loses accuracy beyond 300 rounds on most runs.,5.1. Simulated task,[0],[0]
"In contrast, DRO is stable, and maintains an accuracy of 0.8.
",5.1. Simulated task,[0],[0]
"This stability is due to the fact that the regularized loss for DRO prevents small losses in the minority fraction from amplifying, as we discuss in Proposition 4.",5.1. Simulated task,[0],[0]
"Even when the minority fraction falls as low as 1%, the DRO loss ensures that the accuracy of this minority fraction remains at 75% accuracy (Figure 6).",5.1. Simulated task,[0],[0]
"We now present a real-world, human evaluation of user retention and satisfaction on a text autocomplete task.",5.2. Autocomplete task,[0],[0]
"The task consists of the prediction of next words in a corpus of tweets built from two estimated demographic groups, African Americans and White Americans (Blodgett et al., 2016).",5.2. Autocomplete task,[0],[0]
"There are several distinguishing linguistic patterns between tweets from these groups, whose language dialects we henceforth refer to as African-American English (AAE) and Standard-American English (SAE), respectively, following the nomenclature in Blodgett et al. (2016).",5.2. Autocomplete task,[0],[0]
"Our overall experimental design is to measure the retention rate ν and riskR for various choices of demographic proportions (αAAE, αSAE) and simulate the implied dynamics, since running a fully online experiment would be prohibitively expensive.
",5.2. Autocomplete task,[0],[0]
"For both ERM and DRO, we train a set of five maximum likelihood bigram language models on a corpus with 366,361 tweets total and a f ∈ {0.1, 0.4, 0.5, 0.6, 0.9} fraction of the tweets labeled as AAE.",5.2. Autocomplete task,[0],[0]
"This results in 10 possible autocomplete systems a given Mechanical Turk user can be assigned to during a task.
",5.2. Autocomplete task,[0],[0]
"To evaluate the retention and loss for AAE and SAE separately, a turk user is assigned 10 tweets from either the held out AAE tweets or SAE tweets, which they must replicate using a web-based keyboard augmented by the autocomplete system.",5.2. Autocomplete task,[0],[0]
This assignment of a turk user to a demographic group simulates the situation where a user from a particular demographic group attempts to use the autocomplete system to write a tweet.,5.2. Autocomplete task,[0],[0]
"Details of the autocomplete task are included in the supplement.
",5.2. Autocomplete task,[0],[0]
"After completing the task, users were asked to fill out a survey which included a rank from 1 to 5 on their satisfaction with the task, and a yes/no question asking whether they would continue to use such a system.",5.2. Autocomplete task,[0],[0]
"We assign 50 users to each of the two held out set types and each of the 10 autocomplete models, resulting in 1,000 users’ feedback across autocomplete models and assigned demographics.
",5.2. Autocomplete task,[0],[0]
The response to whether a user would continue to use the autocomplete system provides samples ν(RK(α)) with n = 366361 and each of possible demographic proportions α.,5.2. Autocomplete task,[0],[0]
The user satisfaction survey provides a surrogate forRK(α) at these same points.,5.2. Autocomplete task,[0],[0]
We interpolate ν andRK to α ∈,5.2. Autocomplete task,[0],[0]
"[0, 1] via isotone regression which then allows us to simulate the user dynamics and satisfaction over time using Definition 1.",5.2. Autocomplete task,[0],[0]
"We estimate variability in these estimates via bootstrap replicates on the survey responses.
",5.2. Autocomplete task,[0],[0]
"Our results in Figure 4 show an improvement in both minority satisfaction and retention rate due to DRO: we improve the median user satisfaction from 3.7 to 4.0 and retention from 0.7 to 0.85, while only slightly decreasing the SAE
satisfaction and retention.",5.2. Autocomplete task,[0],[0]
"Implied user counts follow the same trend with larger differences between groups due to compounding.
",5.2. Autocomplete task,[0],[0]
"Counterintuitively, the minority group has higher satisfaction and retention under DRO.",5.2. Autocomplete task,[0],[0]
Analysis of long-form comments from Turkers suggest this is likely due to users valuing the model’s ability to complete slang more highly than completion of common words and indicates a slight mismatch between our training loss and human satisfaction with an autocomplete system.,5.2. Autocomplete task,[0],[0]
In this work we argued for a view of loss minimization as a distributive justice problem and showed that ERM often results in disparity amplification and unfairness.,6. Discussion,[0],[0]
"We demonstrate that DRO provides a upper bound on the risk incurred by minority groups and performs well in practice.
",6. Discussion,[0],[0]
"Our proposed algorithm is straightforward to implement, and induces distributional robustness, which can be viewed as a benefit in and of itself.
",6. Discussion,[0],[0]
"Our arguments against ERM and in favor of minority risk minimization mirror Rawls’ arguments against utilitarianism, and thus inherit the critiques of Rawlsian distributive justice.",6. Discussion,[0],[0]
"Examples of such critiques are the focus on an abstract worst-off group rather than demographic groups or individuals (Altham, 1973), extreme risk-aversion (Mueller et al., 1974), and utilitarianism with diminishing returns as an alternative (Harsanyi, 1975).",6. Discussion,[0],[0]
"In this work, we do not address the debate on the correctness of Rawlsian justice (Rawls, 2001), and leave finding a suitable philosophical framework for loss minimization to future work.
",6. Discussion,[0],[0]
There are two large open questions from our work.,6. Discussion,[0],[0]
"First, as fairness is fundamentally a causal question, observational approaches such as DRO can only hope to control limited aspects of fairness.",6. Discussion,[0],[0]
"The generality with which our algorithm can be applied also limits its ability to enforce fairness as a constraint, and thus our approach here is unsuitable for high-stakes fairness applications such as classifiers for loans, criminality, or admissions.",6. Discussion,[0],[0]
In such problems the implied minorities from DRO may differ from well-specified demographic groups who are known to suffer from historical and societal biases.,6. Discussion,[0],[0]
"This gap arises due to looseness in the DRO bound (Hu et al., 2018), and could be mitigated using smoothness assumptions (Dwork et al., 2012).
",6. Discussion,[0],[0]
"Second, distributional robustness proposed here runs counter to classical robust estimation for rejecting outlier samples, as high loss groups created by an adversary can easily resemble a minority group.",6. Discussion,[0],[0]
"Adversarial or high-noise settings loosen the DRO upper bound substantially, and it is an open question whether it is possible to design algorithms which are both fair to unknown latent groups and robust.
",6. Discussion,[0],[0]
"Reproducibility: Code to generate results available on the CodaLab platform at https://bit.ly/2sFkDpE.
Acknowledgements: This work was funded by an Open Philanthropy Project Award.",6. Discussion,[0],[0]
"Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity— minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss.",abstractText,[0],[0]
"Worse, as model accuracy affects user retention, a minority group can shrink over time.",abstractText,[0],[0]
"In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair.",abstractText,[0],[0]
"To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution.",abstractText,[0],[0]
"We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups.",abstractText,[0],[0]
"We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.",abstractText,[0],[0]
Fairness Without Demographics in Repeated Loss Minimization,title,[0],[0]
The recent proliferation of malicious fake news in social media has been a source of widespread concern.,1. Introduction,[0],[0]
"Given that more than 62% of U.S. adults turn to social media for news, with 18% doing so often, fake news can have potential real-world consequences on a large scale (Gottfried & Shearer, 2016).",1. Introduction,[0],[0]
"For example, within the final three months of the 2016 U.S. presidential election, news stories that favored either of the two nominees–later proved to be fake– were shared over 37 million times on Facebook alone, and over half of those who recalled seeing fake news stories believed them (Allcott & Gentzkow, 2017).",1. Introduction,[0],[0]
"An analysis by Buzzfeed News shows that the top 20 false election stories from hoax websites generated nearly 1.5 million more user engagement activities on Facebook than the top 20 sto-
1School of Computational Science and Engineering, Georgia Tech.",1. Introduction,[0],[0]
"2Department of Mathematics and Statistics, Georgia State University.",1. Introduction,[0],[0]
"3School of Industrial and Systems Engineering, Georgia Tech..",1. Introduction,[0],[0]
"Correspondence to: Mehrdad Farajtabar <mehrdad@gatech.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"ries from reputable major news outlets (Silverman, 2016).",1. Introduction,[0],[0]
"Therefore, there is an urgent call to develop effective rectifying strategies to mitigate the impact of fake news.
",1. Introduction,[0],[0]
Policies to counter fake news can be categorized by the level of manual oversight and the aggressiveness of action required.,1. Introduction,[0],[0]
Aggressively acting on fake news has various drawbacks.,1. Introduction,[0],[0]
"For example, Facebook’s strategy allows users to report stories as potential fake news, sends these stories to fact-checking organizations, and flags them as disputed in users’ newsfeed (Mosseri, 2016).",1. Introduction,[0],[0]
"Such direct action on the offending news requires a high degree of human oversight, which can be costly and slow and also may violate civil rights.",1. Introduction,[0],[0]
The report-and-flag mechanism is also open to abuse by adversaries who maliciously report real news.,1. Introduction,[0],[0]
"Given these disadvantages, we consider an alternative strategy: optimizing the performance of real news propagation over the network.",1. Introduction,[0],[0]
"Intuitively, we want people’s exposure to real news to match their exposure to fake news.
",1. Introduction,[0],[0]
We face several key modeling and computational issues.,1. Introduction,[0],[0]
"For example, how to quantify the uncertainty of user activities and news propagation within the network?",1. Introduction,[0],[0]
How to measure the effect of mitigation incentives and activities?,1. Introduction,[0],[0]
Is it possible to steer the spontaneous user mitigation activities by an intervention strategy?,1. Introduction,[0],[0]
"To address these questions, we model the temporal randomness of fake news and mitigation events (“valid news”) as multivariate point processes with self and mutual excitations, in which the control incentivizes more spontaneous mitigation events by contributing to the exogenous activity of campaigner nodes.",1. Introduction,[0],[0]
"The influence of fake news and mitigation activities is quantified using event exposure counts (i.e. the number of times that a user is exposed to fake or real news posts from other users whom she follows).
",1. Introduction,[0],[0]
Our key contributions are as follows.,1. Introduction,[0],[0]
We present the first formulation of fake news mitigation as the problem of optimal point process intervention in a network.,1. Introduction,[0],[0]
The goal is to optimize the activity policy of a set of campaigner nodes to mitigate a fake news process stemming from another set of nodes.,1. Introduction,[0],[0]
"It creates opportunities for designing a variety of objectives, e.g. minimizing the number of users who see fake news but were not reached by real news.",1. Introduction,[0],[0]
"We give the first derivation of second-order statistics of random exposure counts in the non-stationary case, which is essential in policy evaluation and improvement.",1. Introduction,[0],[0]
"By defining a
state space for the network, formulating actions as exogenous intensity, and defining reward functions, we map the fake news mitigation problem to an optimal policy problem in a Markov decision process (MDP), which is solved by model-based least-squares temporal difference learning (LSTD) specific to the context of point processes.",1. Introduction,[0],[0]
"Furthermore, to the best of our knowledge, we are the first to conduct a real-time point process intervention experiment.",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"The emergence of social media as a prominent news source in the past few years raises concomitant concerns about the quality, truthfulness, and credibility of information presented (Mitra et al., 2017).",1. Introduction,[0],[0]
"To reduce the amount of labor-intensive manual fact-checking, there have been research efforts devoted to building classifiers to detect factuality of information, predicting credibility level of posts, and detecting controversial information from inquiry phrases (Mitra et al., 2017; Zeng et al., 2016; Zhao et al., 2015).",1. Introduction,[0],[0]
These works mainly focused on extracting linguistic features from texts to determine the credibility of news and posts.,1. Introduction,[0],[0]
"Our focus in this paper, however, is to design an incentive strategy so that users can spontaneously take action to mitigate a real-world fake news epidemic.
",1. Introduction,[0],[0]
"Steering user activities by adding external incentives to the exogenous intensity of Hawkes processes was first considered in (Farajtabar et al., 2014).",1. Introduction,[0],[0]
"In (Farajtabar et al., 2016), a multistage campaigning method to optimally distribute incentive resources based on dynamic programming was developed.",1. Introduction,[0],[0]
"In these previous works, objective functions were designed using expected values of exposure counts rather than the stochastic exposure process, which may reduce the accuracy of solutions.",1. Introduction,[0],[0]
"Furthermore, it faced the demanding problem of computing the cost-to-go using the Hawkes model, while we address this using linear function approximation.",1. Introduction,[0],[0]
"For stationary Hawkes processes, second order statistics was derived in (Bacry & Muzy, 2014a;b);
however, it is essential to compute both first and second order statistics for Hawkes processes in the non-stationary stages due to time sensitivity of the fake news mitigation task, and we derive it for the first time in this paper.",1. Introduction,[0],[0]
"Recent work has also applied methods in stochastic differential equations to the context of point processes, to find the best intensity for information guiding (Wang et al., 2016) and achieving highest visibility (Zarezade et al., 2017).",1. Introduction,[0],[0]
"While these works consider networks with only a single process, our work focuses on optimizing a mitigation process with respect to a second competing process.",1. Introduction,[0],[0]
"Finally, it’s notable that our approach is related to but much more general than the influence maximization problems (Kempe et al., 2003; Bharathi et al., 2007) as it allows recurrent activity in social networks (in contrast to binary infection states), a variety of objectives (not only maximization), and budget constraints.
",1. Introduction,[0],[0]
"Reinforcement learning tackles the problem of finding good policies for actions to take in MDP where exact solutions are intractable, either due to size or lack of complete knowledge.",1. Introduction,[0],[0]
"Large-scale policy evaluation and iteration problems can be tackled by function approximation, which reduces the solution dimension using feature vector basis (Sutton & Barto, 1998).",1. Introduction,[0],[0]
"By adding control terms to a multivariate Hawkes process model of random network activities, fake news mitigation can be formulated as a policy optimization problem in an MDP.",1. Introduction,[0],[0]
"To address the randomness of Hawkes processes, batch reinforcement learning using samples collected from the trajectory of a fixed behavior policy can be applied (Antos et al., 2007).",1. Introduction,[0],[0]
"In particular, linear Least Squares Temporal Difference (LSTD) uses a batch of samples to learn a linear approximation of the value function under a policy with provable convergence (Bradtke & Barto, 1996).",1. Introduction,[0],[0]
This policy evaluation step alternates with a model-based policy improvement step in a policy iteration to arrive at successively improved policies.,1. Introduction,[0],[0]
Multivariate Hawkes processes.,2. Preliminaries and Problem Statement,[0],[0]
"Hawkes process is a doubly stochastic point process with self-excitations, meaning that past events increase the chance of arrivals of new events (Hawkes, 1971), and has been extensively used to model activities in social networks (Farajtabar et al., 2015; Linderman & Adams, 2014; He et al., 2015; Rizoiu et al.; Lee et al., 2016).",2. Preliminaries and Problem Statement,[0],[0]
"Let t` be the time of the `-th event, then the Hawkes process can be represented by the counting process N(t)",2. Preliminaries and Problem Statement,[0],[0]
"=
P t`t h(t t`) that tracks the number
of events up to time t, where h(t) is the standard Heaviside function such that h(t) = 1 if t 0 and = 0",2. Preliminaries and Problem Statement,[0],[0]
if t < 0.,2. Preliminaries and Problem Statement,[0],[0]
The conditional intensity function of a point process is defined as the probability of observing an event in an infinitesimal window given the history.,2. Preliminaries and Problem Statement,[0],[0]
"For Hawkes process it is given by (t) = µ +
P t`<t
(t t`).",2. Preliminaries and Problem Statement,[0],[0]
"Here, µ 0 is the exogenous (base) intensity and (t) is the Hawkes kernel that describes how fast the excitement of a past event decays.",2. Preliminaries and Problem Statement,[0],[0]
"In this paper, we employ the standard (stationary) exponential Hawkes kernel, i.e., (t) = ↵e !th(t) with !",2. Preliminaries and Problem Statement,[0],[0]
>,2. Preliminaries and Problem Statement,[0],[0]
↵ > 0.,2. Preliminaries and Problem Statement,[0],[0]
"In an n-dimensional multivariate Hawkes process (MHP), there are n such processes N1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", Nn(t) that can also mutually excite one another, and the conditional intensity (t) :=
1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", n(t) >",2. Preliminaries and Problem Statement,[0],[0]
2 Rn+ is given by (t) = µ + R t 0,2. Preliminaries and Problem Statement,[0],[0]
(t s) dN(s).,2. Preliminaries and Problem Statement,[0],[0]
"Here, N(t) :",2. Preliminaries and Problem Statement,[0],[0]
"=
N1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", Nn(t) > 2 Nn0 , µ := (µ1, . . .",2. Preliminaries and Problem Statement,[0],[0]
", µn)> 2 Rn+, and [ (t)]ij = ij(t) := ↵ije !th(t).",2. Preliminaries and Problem Statement,[0],[0]
"We let H(t) denote the filtration of N(t), generated by the -algebra of history (t`, i`)|t`  t of this point process, where i` 2 {1, . . .",2. Preliminaries and Problem Statement,[0],[0]
", n} is the identity (node) of the `-th event.",2. Preliminaries and Problem Statement,[0],[0]
Network activities.,2. Preliminaries and Problem Statement,[0],[0]
We model the activities of both fake news and mitigation events as MHP in the network.,2. Preliminaries and Problem Statement,[0],[0]
"Basically, MHP is a networked point process model with dependent dimensions (nodes), and can capture the underlying dynamics of networks and activities (Blundell et al., 2012; Xu et al., 2016; Guo et al., 2015).",2. Preliminaries and Problem Statement,[0],[0]
"Define F (t) =
F1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", Fn(t) > 2 Nn0 , where Fi(t) counts the number of times user i shares a piece of news from the fake campaign up to time t. Similarly, define M(t) = M1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
",Mn(t)
> 2 Nn0 for the mitigation process.",2. Preliminaries and Problem Statement,[0],[0]
"Correspondingly, we have 2 intensity functions: M (t) =",2. Preliminaries and Problem Statement,[0],[0]
"(
M 1 (t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", M n (t))",2. Preliminaries and Problem Statement,[0],[0]
>,2. Preliminaries and Problem Statement,[0],[0]
and F (t) =,2. Preliminaries and Problem Statement,[0],[0]
"( F1 (t), . . .",2. Preliminaries and Problem Statement,[0],[0]
",",2. Preliminaries and Problem Statement,[0],[0]
Fn (t)),2. Preliminaries and Problem Statement,[0],[0]
>,2. Preliminaries and Problem Statement,[0],[0]
and two sets of exogenous intensities µM and µF .,2. Preliminaries and Problem Statement,[0],[0]
Goal.,2. Preliminaries and Problem Statement,[0],[0]
"Given that both F (t) and M(t) are modeled by the Hawkes processes, our goal is to find the optimal mitigation campaign by imposing interventions to users such that the mitigation effect (rigorously defined in sec. 3.1) can be maximized or equivalently the fake news be rectified under budget constraints.",2. Preliminaries and Problem Statement,[0],[0]
"To this end, we measure the influence of fake news and mitigation activities using event exposures, describe the mechanism of mitigation interventions, and quantify the effect of interventions mathematically.
",2. Preliminaries and Problem Statement,[0],[0]
Event exposure.,2. Preliminaries and Problem Statement,[0],[0]
"Event exposure is a quantitative measure of campaign influence, and is represented as a counting process, E(t) = E1(t), . . .",2. Preliminaries and Problem Statement,[0],[0]
", En(t)
",2. Preliminaries and Problem Statement,[0],[0]
>.,2. Preliminaries and Problem Statement,[0],[0]
"Here, Ei(t) records the number of times user i is exposed (she or one of her neighbors performs an activity) to a campaign N(t) by time t. Let B be the adjacency matrix of the user network, i.e., bij = 1 if user i follows user j. Assume bii",2. Preliminaries and Problem Statement,[0],[0]
= 1 for all i. Then the exposure process is given by E(t) = BN(t).,2. Preliminaries and Problem Statement,[0],[0]
"We define F(t) = BF (t) and M(t) = BM(t) as the fake news and mitigation processes, respectively.
Intervention.",2. Preliminaries and Problem Statement,[0],[0]
"Suppose we can perform intervention by incentivizing a subset of users in the k-th stage during time [⌧k, ⌧k+1) for k = 0, 1, . . . .",2. Preliminaries and Problem Statement,[0],[0]
"For simplicity we consider uniform time duration ⌧k+1 ⌧k = T for all k, since generalization to nonuniform time durations is trivial.",2. Preliminaries and Problem Statement,[0],[0]
"In order to steer the mitigation activities to counter the fake news (criteria given below) at these stages, we impose an additional constant intervention uki 0 to the exogenous intensity µi during time [⌧k, ⌧k+1) for each stage k = 0, 1, . . . .",2. Preliminaries and Problem Statement,[0],[0]
The mitigation activity intensity at the kth stage is M (t) = µ + uk + R t 0,2. Preliminaries and Problem Statement,[0],[0]
"(t s) dM(s) for t 2 [⌧k, ⌧k+1).",2. Preliminaries and Problem Statement,[0],[0]
"Note that the intervention itself exhibits a stochastic nature: adding uki to µi is equivalent to incentivizing user i to increase her activity rate but it is still uncertain when she will perform an activity, which appropriately mimics the randomness in the real world.
",2. Preliminaries and Problem Statement,[0],[0]
Reward function.,2. Preliminaries and Problem Statement,[0],[0]
"For each stage k, xk (defined later) is the state of the whole MDP that encodes all the information from previous stages and uk is the current control imposed at this stage.",2. Preliminaries and Problem Statement,[0],[0]
"Let Mki (t;xk, uk) := P j bij R t ⌧k dMj(s) be the number of times user i is exposed to the mitigation campaign by time t 2 [⌧k, ⌧k+1) within stage k, then the goal is to steer the expected total number of exposure Mki (t;xk, uk) using uk, s.t.",2. Preliminaries and Problem Statement,[0],[0]
"the sum of reward functions R(x k , u k ) (rigorously defined in sec. 3.1) is maximized.
",2. Preliminaries and Problem Statement,[0],[0]
Problem statement.,2. Preliminaries and Problem Statement,[0],[0]
"By observing the counting process in previous stages (summarized in a sequence of xk) and taking the future uncertainty into account, the control problem is to design a policy ⇡ such that the controls uk = ⇡(xk) can maximize the total discounted objective E[ P1 k=0",2. Preliminaries and Problem Statement,[0],[0]
"k R k ], where 2 (0, 1] is the discount rate and Rk is the observed reward at",2. Preliminaries and Problem Statement,[0],[0]
stage k.,2. Preliminaries and Problem Statement,[0],[0]
"In addition, we may have constraints on the amount of control, such as a budget constraint on the sum of all interventions to users at each stage, or a cap over the amount of intensity a user can handle.",2. Preliminaries and Problem Statement,[0],[0]
A feasible set or an action space over which we find the best intervention is represented as Uk := u 2 Rn|u>ck  ,2. Preliminaries and Problem Statement,[0],[0]
"Ck, 0  u  ↵k .",2. Preliminaries and Problem Statement,[0],[0]
"Here, cki is the price per unit increase of exogenous intensity of user i and Ck 2 R+ is the total budget at stage k. Also, ↵ki is the cap on the amount of activities of the user i.",2. Preliminaries and Problem Statement,[0],[0]
"In this section, we present the formulation of reward functions in terms of event exposures of fake news and mitigation activities.",3. Proposed Method,[0],[0]
"Then we derive the key statistics of the MHP required for reward function evaluation, followed by the policy iteration scheme to find the optimal intervention.",3. Proposed Method,[0],[0]
"As we discussed above, the total reward of policy ⇡ is defined by the value function
V ⇡ (x 0 )",3.1. Fake news mitigation,[0],[0]
=,3.1. Fake news mitigation,[0],[0]
"E
 1X
k=0
k",3.1. Fake news mitigation,[0],[0]
"R k x 0
(1)
for the initial state x0 of fake and mitigation processes, where the observed reward R quantifies the effect of mitigation activities M(t) in each stage and 2 (0, 1] is the discount rate.",3.1. Fake news mitigation,[0],[0]
"In this paper, we consider two types of reward functions",3.1. Fake news mitigation,[0],[0]
"R(x, u):
1) Correlation Maximization: One possible way is to require correlation between mitigation exposures and fake news exposures: people exposed more to fake news should also be exposed more to the true news, to counter the fake news campaign.",3.1. Fake news mitigation,[0],[0]
"Therefore, we can form the reward function R in stage k as follows:
R(x k , u k )",3.1. Fake news mitigation,[0],[0]
"= 1
n Mk(⌧k+1;xk, uk)>Fk(⌧k+1;xk, uk).",3.1. Fake news mitigation,[0],[0]
"2) Difference Minimization: Suppose the goal is to minimize the number of unmitigated fake news events, then we can form a reward function R in stage k as the least squares of unmitigated numbers:
R(x k , u k )",3.1. Fake news mitigation,[0],[0]
"= 1 n
Mk(⌧k+1;xk, uk) Fk(⌧k+1;xk, uk) 2
These are two sample realizations of the MHP-MDP based intervention one can formulate, among many others.",3.1. Fake news mitigation,[0],[0]
"To solve the policy optimization problem argmax⇡ V ⇡(x0) for V ⇡ defined in (1), we need to evaluate the value function V ⇡ for any given policy ⇡, which requires the first and second order statistics (moments) of any multivariate Hawkes processes N(t), as we derive next.",3.1. Fake news mitigation,[0],[0]
"For an n-dim MHP N(t) with standard exponential kernel (t), the following proposition provides closed-form solution of the mean intensity ⌘(t) := E[ (t)] for both constant and time-varying exogenous intensity µ(t): Proposition 1 (Theorem 3 (Farajtabar et al., 2014; 2016)).",3.2. Second order statistics of non-stationary MHP,[0],[0]
"Let N(t) be an n-dimensional MHP defined in sec. 2 with exogenous intensity µ(t) and Hawkes kernel (t) = Ae !t h(t), then the mean intensity ⌘(t) is given by
⌘(t) = ⇥",3.2. Second order statistics of non-stationary MHP,[0],[0]
e (A !I)t,3.2. Second order statistics of non-stationary MHP,[0],[0]
+ !,3.2. Second order statistics of non-stationary MHP,[0],[0]
(A !I) 1 e(A !I)t,3.2. Second order statistics of non-stationary MHP,[0],[0]
"I ⇤µ(t).
",3.2. Second order statistics of non-stationary MHP,[0],[0]
"(2)
Let ⇤(t) =",3.2. Second order statistics of non-stationary MHP,[0],[0]
R t 0,3.2. Second order statistics of non-stationary MHP,[0],[0]
"(s) ds be the compensator of N(t), then by Doob-Meyer’s decomposition theorem N(t)",3.2. Second order statistics of non-stationary MHP,[0],[0]
⇤(t) is a zero mean martingale.,3.2. Second order statistics of non-stationary MHP,[0],[0]
This implies that the first order statistics E[N(t)] can be obtained by E[N(t)],3.2. Second order statistics of non-stationary MHP,[0],[0]
= E[⇤(t)],3.2. Second order statistics of non-stationary MHP,[0],[0]
= E[ R t 0 (s) ds] = R t 0,3.2. Second order statistics of non-stationary MHP,[0],[0]
E[ (s)],3.2. Second order statistics of non-stationary MHP,[0],[0]
ds = R t 0,3.2. Second order statistics of non-stationary MHP,[0],[0]
⌘(s) ds using eq.,3.2. Second order statistics of non-stationary MHP,[0],[0]
"(2).
",3.2. Second order statistics of non-stationary MHP,[0],[0]
"To evaluate the reward function R defined previously, we need to derive second order statistics of multivariate Hawkes process N(t) in its non-stationary stage.",3.2. Second order statistics of non-stationary MHP,[0],[0]
The following theorem states the key ingredients for the second order statistics.,3.2. Second order statistics of non-stationary MHP,[0],[0]
The proof is provided in the appendix.,3.2. Second order statistics of non-stationary MHP,[0],[0]
Theorem 2.,3.2. Second order statistics of non-stationary MHP,[0],[0]
Let N(t) be an n-dim MHP with exogenous intensity µ and Hawkes kernel defined in sec.,3.2. Second order statistics of non-stationary MHP,[0],[0]
"2, then the second order statistics of N(t) for t, t0 0 is given by E h dN(t) dN(t 0 )",3.2. Second order statistics of non-stationary MHP,[0],[0]
>,3.2. Second order statistics of non-stationary MHP,[0],[0]
"i = G(t 0 , t) >",3.2. Second order statistics of non-stationary MHP,[0],[0]
⌃(t 0 ),3.2. Second order statistics of non-stationary MHP,[0],[0]
dt dt 0,3.2. Second order statistics of non-stationary MHP,[0],[0]
"+
(t t0)⌃(t0) dt dt0 + ⌘(t)⌘(t0)> dt dt0 (3)
where ⌘(t) =",3.2. Second order statistics of non-stationary MHP,[0],[0]
"E[ (t)] is given in (2), ⌃(t) = diag([⌘i(t)]) is diagonal, and G is the unique solution of
G(t 0 , t) = G(t 0 , t)",3.2. Second order statistics of non-stationary MHP,[0],[0]
⇤ (t) +,3.2. Second order statistics of non-stationary MHP,[0],[0]
"(t t0) (t t0)I. (4)
Moreover G(t0, t)>⌃(t0) = ⌃(t)G(t, t0) for all t, t0 0.",3.2. Second order statistics of non-stationary MHP,[0],[0]
"Based on Theorem 2, we can compute second order statistics such as E[Ni(t)Nj(t0)] for all i, j and t, t0 0.",3.2. Second order statistics of non-stationary MHP,[0],[0]
Hawkes process is non-Markovian and one needs complete knowledge of the history to characterize the entire process.,3.3. State Representation,[0],[0]
"However, when the standard exponential kernel (t, s) = Ae !(t s) h(t s) is employed, the effect of history up to time ⌧k on the future t >",3.3. State Representation,[0],[0]
"⌧k can be cleverly summarized by one scalar per dimension (Simma & Jordan, 2012; Farajtabar et al., 2016).",3.3. State Representation,[0],[0]
"For 1  i  n, define y
k",3.3. State Representation,[0],[0]
i := k 1,3.3. State Representation,[0],[0]
"i (⌧k) uk 1i µi, (and yi0 = 0 by convention), then the intensity due to events of all previous k stages can be written as R ⌧k 0 Ae !(t s) dN(s) = y k e
!",3.3. State Representation,[0],[0]
(t ⌧k).,3.3. State Representation,[0],[0]
"In other words, yk is sufficient to encode the information of activities in the past k stages that are relevant to future.",3.3. State Representation,[0],[0]
"Note that we have two separate ykM and y k F to track the dynamics of both mitigation and fake processes.
",3.3. State Representation,[0],[0]
"Also, in order to tackle objectives over multiple stages, we add aggregated number of events at L previous f -time intervals over all dimensions.",3.3. State Representation,[0],[0]
Define a vector zk 2 RnL where zk(l 1)n+i = R ⌧k (l 1) f,3.3. State Representation,[0],[0]
⌧k l f dNi(s),3.3. State Representation,[0],[0]
for 1  i  n and 1  l  L.,3.3. State Representation,[0],[0]
"In other words, zk(l 1)n+i records the number of events of i-th dimension in the l-th interval of length f prior to time ⌧k.",3.3. State Representation,[0],[0]
"For example, choosing f = T and setting L = 2 means that events from the two most recent stages are counted.",3.3. State Representation,[0],[0]
"Similarly, we have two",3.3. State Representation,[0],[0]
"separate zkM and z
k F corresponding to the two processes.",3.3. State Representation,[0],[0]
"Now, the state vector xk 2 R2nL+2n is the concatenation of the above four vectors xk =",3.3. State Representation,[0],[0]
[ykM ; y k F ; z k M ;,3.3. State Representation,[0],[0]
"z k F ].
",3.3. State Representation,[0],[0]
"Algorithm 1 LSTD policy iteration in point processes Input: set of samples S , feature (·), discount repeat
Initialize A⇡ = 0 and b⇡ = 0.",3.3. State Representation,[0],[0]
"for each state x 2 S do A
⇡ A⇡ + (x)( (x) (x0))",3.3. State Representation,[0],[0]
>,3.3. State Representation,[0],[0]
"b
⇡ b⇡ + (x)r⇡ end for w
⇡ (A⇡) 1b⇡ for each state x 2 S do ⇡(x) argmax
u {E[R(x, u)]+ E[V ⇡(x0)|u,w⇡]}
end for until k w⇡k < 0.1",3.3. State Representation,[0],[0]
return w⇡,3.3. State Representation,[0],[0]
"The optimal value function satisfies the Bellman equation:
V ⇡ (x) = E[R(x,⇡(x))]",3.4. Least Squares Temporal Difference,[0],[0]
"+ E[V ⇡(x0)], (5)
where x0 is the next state after taking action based on policy ⇡ at state x. Least squares temporal difference learning (LSTD) is a sample-efficient procedure for policy evaluation, which subsequently facilitates policy improvement.",3.4. Least Squares Temporal Difference,[0],[0]
"The value function is approximated by ˆV ⇡(x) =PD
d=1 w ⇡ d d(x), where d is the d-th feature of state x and w⇡d is its coefficient for policy ⇡.",3.4. Least Squares Temporal Difference,[0],[0]
This can be compactly represented as ˆV ⇡(x) =,3.4. Least Squares Temporal Difference,[0],[0]
"(x)>w⇡ , where (x) = ( 1(x), . . .",3.4. Least Squares Temporal Difference,[0],[0]
", D(x))
",3.4. Least Squares Temporal Difference,[0],[0]
>.,3.4. Least Squares Temporal Difference,[0],[0]
"The following presents our choice of features, and explains the policy evaluation and improvement steps inspired by LSTD(0) (Sutton & Barto, 1998).
",3.4. Least Squares Temporal Difference,[0],[0]
Features.,3.4. Least Squares Temporal Difference,[0],[0]
"The number of events in a few recent consecutive intervals of point processes have been used as a reliable feature to parameterize point processes (Parikh et al., 2012; Qin & Shelton, 2015; Lian et al., 2015).",3.4. Least Squares Temporal Difference,[0],[0]
Following their work we take L prior intervals of length f for each dimension of the fake news process and record the number of events in that period as one feature.,3.4. Least Squares Temporal Difference,[0],[0]
k(l,3.4. Least Squares Temporal Difference,[0],[0]
1)n+i = z k,3.4. Least Squares Temporal Difference,[0],[0]
(l 1)n+i for 1  i  n,3.4. Least Squares Temporal Difference,[0],[0]
and 1  l  L.,3.4. Least Squares Temporal Difference,[0],[0]
This will count for nL features.,3.4. Least Squares Temporal Difference,[0],[0]
Similarly we take nL features from the mitigation process.,3.4. Least Squares Temporal Difference,[0],[0]
"Finally, we add a last feature k2nL+1 = 1 as the bias term.",3.4. Least Squares Temporal Difference,[0],[0]
"Therefore, k = [zkM ; z k F ; 1] and the feature space has dimension D = 2nL+ 1.
Policy Evaluation.",3.4. Least Squares Temporal Difference,[0],[0]
"Substituting the approximation into the Bellman equation, we have:
(x) >",3.4. Least Squares Temporal Difference,[0],[0]
"w ⇡ = E[R(x,⇡(x))]",3.4. Least Squares Temporal Difference,[0],[0]
+,3.4. Least Squares Temporal Difference,[0],[0]
E[ (x0)>]w⇡.,3.4. Least Squares Temporal Difference,[0],[0]
"(6)
To find the best fit of w⇡ we have to consider all possible x; however, since the state space is infinite-dimensional, enumerating all states is impossible and we utilize a set S of samples S = {x1, . . .",3.4. Least Squares Temporal Difference,[0],[0]
", xS}.
",3.4. Least Squares Temporal Difference,[0],[0]
"Algorithm 2 Real-time fake news mitigation Input: network A, learned w⇡ , feature (·), discount repeat
Observe state x of the network activities u = argmaxa{E[R(x, a)]",3.4. Least Squares Temporal Difference,[0],[0]
"+ E[V ⇡(x0)|a,w⇡]} Add u to base exogenous intensity µ and generate mitigation event times {ti} using point process model Create posts at times {ti} using campaigner accounts
until end of campaign
Let (xs) = s 2 RD, E[ (x0s)] = 0s 2 RD, and r
⇡ s = E[R(xs,⇡(xs))]",3.4. Least Squares Temporal Difference,[0],[0]
2 R. Then define matrices of current features =,3.4. Least Squares Temporal Difference,[0],[0]
[ >1 ; . . .,3.4. Least Squares Temporal Difference,[0],[0]
; >S ] > 2 RS⇥D and next features 0 =,3.4. Least Squares Temporal Difference,[0],[0]
[ 0>1 ; . . .,3.4. Least Squares Temporal Difference,[0],[0]
"; 0 > S ]
> 2 RS⇥D, the rewards r ⇡ =",3.4. Least Squares Temporal Difference,[0],[0]
"[r
⇡ 1 , . . .",3.4. Least Squares Temporal Difference,[0],[0]
", r ⇡ S ] > 2 RS , and the sample value functions as v⇡ =",3.4. Least Squares Temporal Difference,[0],[0]
"[V ⇡(x1), . . .",3.4. Least Squares Temporal Difference,[0],[0]
", V ⇡(xS)]",3.4. Least Squares Temporal Difference,[0],[0]
> 2 RS .,3.4. Least Squares Temporal Difference,[0],[0]
"Appendix C presents how we leverage the first and second order statistics of Hawkes process to find E[R(x, u)] and E[V ⇡(x0)].",3.4. Least Squares Temporal Difference,[0],[0]
"Given the above definition, the Bellman optimality of eq. (6) can be written in matrix format:
v ⇡ = w ⇡ = r ⇡ +
0 w ⇡ , T⇡v⇡, (7) where T⇡ is the Bellman optimality operator.",3.4. Least Squares Temporal Difference,[0],[0]
"A way to find a good estimate is to force the approximate value function to be a fixed point of the optimality equation under the Bellman operator, i.e., T⇡ v̂⇡ ⇡ v̂⇡.",3.4. Least Squares Temporal Difference,[0],[0]
"(Lagoudakis & Parr, 2003).",3.4. Least Squares Temporal Difference,[0],[0]
"For that, the fixed point has to lie in the space of approximate value functions, spanned by the basis functions .",3.4. Least Squares Temporal Difference,[0],[0]
"v̂⇡ lies in that space by definition, but T ⇡ v̂
⇡ may have an orthogonal component and must be projected.",3.4. Least Squares Temporal Difference,[0],[0]
This is achieved by the orthogonal projection operator ( ( > ) 1 >).,3.4. Least Squares Temporal Difference,[0],[0]
"Therefore the approximate value function v̂⇡ must be invariant under one application of the Bellman operator T⇡ followed by orthogonal projection:
v̂ ⇡ = ( > )",3.4. Least Squares Temporal Difference,[0],[0]
1 > (T ⇡ v̂ ⇡ ).,3.4. Least Squares Temporal Difference,[0],[0]
"(8)
By substituting the linear approximation w⇡ = v⇡ into the above equation and some manipulations, we get a D ⇥ D linear systems of equations A⇡!⇡ = b⇡ , where A ⇡ = > ( 0) and b⇡ = >r⇡ , and whose solution is the fitted coefficients w⇡ .",3.4. Least Squares Temporal Difference,[0],[0]
"It has been shown that the estimated w⇡ converges to the best w⇤ as the available number of samples tends to infinity (Bradtke & Barto, 1996).",3.4. Least Squares Temporal Difference,[0],[0]
"Appendix B presents a detailed derivation.
",3.4. Least Squares Temporal Difference,[0],[0]
Policy Improvement.,3.4. Least Squares Temporal Difference,[0],[0]
"The second part of the algorithm implements policy improvement, i.e., getting an improved policy ⇡0 via one-step look-ahead as follows:
⇡ 0",3.4. Least Squares Temporal Difference,[0],[0]
(x) =,3.4. Least Squares Temporal Difference,[0],[0]
"argmax u E[R(x, u) +",3.4. Least Squares Temporal Difference,[0],[0]
V ⇡(x0)].,3.4. Least Squares Temporal Difference,[0],[0]
"(9)
LSTD(0)",3.4. Least Squares Temporal Difference,[0],[0]
"alternates between the policy improvement and policy evaluation iteratively until w⇡ converges (Bradtke & Barto, 1996).",3.4. Least Squares Temporal Difference,[0],[0]
"Alg. 1 summarizes this procedure.
",3.4. Least Squares Temporal Difference,[0],[0]
LSTD in Hawkes context.,3.4. Least Squares Temporal Difference,[0],[0]
LSTD is particularly suitable to the problem we are interested in.,3.4. Least Squares Temporal Difference,[0],[0]
"It learns the value function V ⇡(x), and as such, policy improvement can be challenging without knowing the model.",3.4. Least Squares Temporal Difference,[0],[0]
"Because of this, methods that aim to learn the Q-function Q⇡(x, u), such as LSPI (Lagoudakis & Parr, 2003), are widely applied.",3.4. Least Squares Temporal Difference,[0],[0]
The downside of Q-function based methods is that they typically require more samples than learning the value function.,3.4. Least Squares Temporal Difference,[0],[0]
"Yet, in our setup, learning the value function is sufficient, by writing the action-value function as Q⇡(x, u) =",3.4. Least Squares Temporal Difference,[0],[0]
"E[R(x, u)+V ⇡(x0)], and observing that the learned model of the multivariate Hawkes process enables analytical computation of the expectation (see Appendix C for details):
E[V ⇡(x0)]
=
nX
i=1
L 1X
l=1
w ⇡ ln+iz",3.4. Least Squares Temporal Difference,[0],[0]
"k 1 M,(l",3.4. Least Squares Temporal Difference,[0],[0]
"1)n+i + w ⇡ nL+ln+iz k 1 F,(l 1)n+i
+
nX
i=1
wiE[zkM,i] + wnL+iE[zkF,i] + w⇡2nL+1,
E[R(x, u)]",3.4. Least Squares Temporal Difference,[0],[0]
= 1 n,3.4. Least Squares Temporal Difference,[0],[0]
E[zkM,3.4. Least Squares Temporal Difference,[0],[0]
"]> B>B E[zkF ], % correlation
E[R(x, u)]",3.4. Least Squares Temporal Difference,[0],[0]
"= 1 n
E[zkM > B > B z k M ] 1
n
E[zkF > B > B",3.4. Least Squares Temporal Difference,[0],[0]
"z k F ]
+
2
n
E[zkM",3.4. Least Squares Temporal Difference,[0],[0]
]> B>B E[zkF ].,3.4. Least Squares Temporal Difference,[0],[0]
"% difference
We require much fewer samples to learn V ⇡(x) compared to learning an approximate Q⇡(x, u), and in particular compared to LSPI we avoid explicitly discretizing the continuous action space from which the action u is chosen.
",3.4. Least Squares Temporal Difference,[0],[0]
We further remark that the policy improvement step finds the optimal action u at any state x by computing,3.4. Least Squares Temporal Difference,[0],[0]
"argmaxu E[R(x, u) +",3.4. Least Squares Temporal Difference,[0],[0]
"V ⇡(x0)], where the action u to be optimized appears in the calculation of both the expected current reward and the expected value at the next state.",3.4. Least Squares Temporal Difference,[0],[0]
"This optimization problem is convex under our choice of reward functions and the form of the Hawkes conditional intensity.
",3.4. Least Squares Temporal Difference,[0],[0]
After learning the optimal policy (implicit by w⇡ of the linearly-approximated value function) we start at the realtime intervention part.,3.4. Least Squares Temporal Difference,[0],[0]
By observing the state we find the optimal intervention intensity by simply solving eq.,3.4. Least Squares Temporal Difference,[0],[0]
(9).,3.4. Least Squares Temporal Difference,[0],[0]
Alg. 2 summarizes the real-time mitigation procedure.,3.4. Least Squares Temporal Difference,[0],[0]
"We evaluate our fake news mitigation framework by both simulated and real-time real-world experiments and show our approach, Least-squares Temporal Difference (LTD), significantly outperforms several state-of-the-art methods and alternatives: CEC (an approximate dynamic programming), OPL (an open loop optimization), CLS (a centrality based measure), EXP (an exposure based centrality measure), and RND (the random policy).",4. Experiments,[0],[0]
"Their details are given in appendix D. Before explaining the intervention results we verify the theoretical second order statistics in Fig. 2 whose details can be found in appendix E. Furthermore, we examine convergence properties and representative power of linear features in Fig. 5 with the details postponed to appendix F due to space limitations.",4. Experiments,[0],[0]
Setup.,4.1. Synthetic Experiments,[0],[0]
"For all except the experiment over network size, the networks were generated synthetically with n = 300 nodes.",4.1. Synthetic Experiments,[0],[0]
"Endogenous intensity coefficients were set as aij ⇠ U [0, 0.5].",4.1. Synthetic Experiments,[0],[0]
"To mimic real world networks, sparsity was set to 0.02, i.e., each edge was kept with probability 0.02.",4.1. Synthetic Experiments,[0],[0]
The influence matrix was scaled appropriately such that the spectral radius is a random number smaller than one to ensure the stability of the process.,4.1. Synthetic Experiments,[0],[0]
The Hawkes kernel parameter was set to !,4.1. Synthetic Experiments,[0],[0]
"= 1, which means loosing roughly 63 % of influence after 1 unit of time (minutes, hours, etc).",4.1. Synthetic Experiments,[0],[0]
Both fake news and mitigation processes obey these network settings.,4.1. Synthetic Experiments,[0],[0]
"Among n nodes, we assume 20 nodes create fake news and another 20 nodes can be incentivized (via the exogenous intensity) to spread true news.",4.1. Synthetic Experiments,[0],[0]
Each stage has length of T = 1.,4.1. Synthetic Experiments,[0],[0]
The discount factor was set to = 0.7.,4.1. Synthetic Experiments,[0],[0]
"For determining features, we set L = 2",4.1. Synthetic Experiments,[0],[0]
and we choose f = T for simplicity.,4.1. Synthetic Experiments,[0],[0]
"The upper bound for the intervention intensity was chosen by ↵i ⇠ U [0, 0.5].",4.1. Synthetic Experiments,[0],[0]
"The price of each person was cki = 1, and the total budget at stage k was randomly generated as Ck ⇠ (n ⇥",4.1. Synthetic Experiments,[0],[0]
"U [0, 0.5]).",4.1. Synthetic Experiments,[0],[0]
1000 randomly sampled states were used for the LSTD algorithm.,4.1. Synthetic Experiments,[0],[0]
To evaluate a policy (learnt by an algorithm) we simulated the network under that policy 50 times and take the discounted total reward averaged over these 50 runs as an empirical valuation of the policy.,4.1. Synthetic Experiments,[0],[0]
"Furthermore, each single run was simulated for 10 consecutive stages; from the eleventh stage onward, the objectives contribute 0.02 of the total reward and can be safely discarded.",4.1. Synthetic Experiments,[0],[0]
"For all experiments, the above settings are assumed unless it is explicitly mentioned otherwise.
",4.1. Synthetic Experiments,[0],[0]
Intervention results.,4.1. Synthetic Experiments,[0],[0]
Fig. 3 demonstrates the performance of different methods.,4.1. Synthetic Experiments,[0],[0]
"Performance of a policy is quantified as the ratio of the total reward achieved by running the policy, over the total reward achieved by the random policy (RND).",4.1. Synthetic Experiments,[0],[0]
This allows us to compare the effectiveness of the algorithms over a variety of settings.,4.1. Synthetic Experiments,[0],[0]
All the results reported are averages over 10 runs with random networks generated according to the above setup.,4.1. Synthetic Experiments,[0],[0]
"Overall, it is clear that LTD is almost consistently the best.",4.1. Synthetic Experiments,[0],[0]
It improves over the random policy by roughly 20 percent.,4.1. Synthetic Experiments,[0],[0]
CEC is the second best and shows the effectiveness of multi-stage and closed loop intervention.,4.1. Synthetic Experiments,[0],[0]
"This validates our intuition that although CEC computes the reward from both fake news and mitigation processes, the lack of explicit features corresponding to previous events in its value function prevents it from learning the reason for the reward.",4.1. Synthetic Experiments,[0],[0]
"Roughly, OPL is the third best algorithm, due to its negligence of the state and the actual events that occurred.",4.1. Synthetic Experiments,[0],[0]
"Next, comes the EXP algorithm followed by the CLS.",4.1. Synthetic Experiments,[0],[0]
The poor performance of these (compared to others) shows that structural properties are not sufficient to tackle the fake news mitigation problem.,4.1. Synthetic Experiments,[0],[0]
"EXP is roughly better than CEC because it heuristically takes into account the fake news exposure.
",4.1. Synthetic Experiments,[0],[0]
"Fig. 3-a shows the performance with respect to increasing
network size.",4.1. Synthetic Experiments,[0],[0]
The difference between alternative methods and the gap between LTD and others increase with the network size.,4.1. Synthetic Experiments,[0],[0]
"Furthermore, the performance of all methods show an increase over random policy when the problem size gets larger.",4.1. Synthetic Experiments,[0],[0]
"This illustrates the fact that efficient distribution of budget matters more when confronted with problems of increasing complexity and size.
",4.1. Synthetic Experiments,[0],[0]
Fig. 3-b shows the performance with respect to increasing the mitigation campaign size.,4.1. Synthetic Experiments,[0],[0]
"Larger campaigns imply greater flexibility of intervention, which can be exploited by clever algorithms to achieve higher performance.
",4.1. Synthetic Experiments,[0],[0]
Fig. 3-c shows the performance with respect to increasing sparsity of the network.,4.1. Synthetic Experiments,[0],[0]
"Interestingly, the performance of all the algorithms move towards to the random policy as the network becomes denser.",4.1. Synthetic Experiments,[0],[0]
"This can be understood by considering a complete graph, so that no matter how and to whom we distribute the mitigation budget, all the nodes are exposed to the mitigation campaign almost equally.",4.1. Synthetic Experiments,[0],[0]
"However, since real social networks are usually sparse, the effectiveness of the proposed method stands out.
",4.1. Synthetic Experiments,[0],[0]
"Finally, Fig. 3-d shows the performance with respect to the length of an stage.",4.1. Synthetic Experiments,[0],[0]
"Longer stage lengths increase the potential for a good policy to attain higher reward than a random policy, and this is reflected by the sharp increase and larger performance gap between LTD and others for longer lengths.",4.1. Synthetic Experiments,[0],[0]
We observe the same patterns for the distance minimization in Fig. 4 problem and avoid repeating them.,4.1. Synthetic Experiments,[0],[0]
In this section we explain our real-time intervention results.,4.2. Real experiments,[0],[0]
"To the best of our knowledge, we are the first to employ a real-time experiment to evaluate a point process based social network intervention strategy.
",4.2. Real experiments,[0],[0]
Setup.,4.2. Real experiments,[0],[0]
"Using five Twitter accounts, each of which made five posts on machine learning topics at random times per day for a span of two months (Nov.-Dec. 2016), we accumulated a network of 1894 real users with 23407 directed edges in total.",4.2. Real experiments,[0],[0]
"We used this historical data to learn the network parameters {↵ij , µi} using maximum likelihood (similar to related work (Zhou et al., 2013; Farajtabar et al., 2014)) with one hour as the time resolution and the kernel decay parameter !",4.2. Real experiments,[0],[0]
set to 0.1.,4.2. Real experiments,[0],[0]
As illustrated in Fig.1 the optimal policy was learned using LSTD and policy improvement.,4.2. Real experiments,[0],[0]
"Then the real-time experiment starts: Two of the accounts, interpreted as the source of fake news, continued to behave using the same randomized policy as they did in the data collection stage, while the posting times of the other three accounts were generated from (u1, u2, u3)T , produced by our LTD strategy or a competitor strategy.",4.2. Real experiments,[0],[0]
Each policy was run for 10 stages of length 12 hours.,4.2. Real experiments,[0],[0]
"Therefore, T = f = 12.",4.2. Real experiments,[0],[0]
"Since both fake news and mitigation accounts were tweeting random posts on machine learning, we assume negligible bias in the content that can confound the performance.",4.2. Real experiments,[0],[0]
"At the end of each stage, all retweets–by users within the network– of the posts made during the two most recent stages were used to construct the feature vector and compute the value function, which was used to find the optimal intervention for the next stage.",4.2. Real experiments,[0],[0]
"The methods CEC and OPL belong to the same category, and it has been shown that CEC outperforms OPL in (Farajtabar et al., 2016).",4.2. Real experiments,[0],[0]
"Furthermore, EXP and CLS also belong to similar families and our synthetic experiments confirm the superiority of the former.",4.2. Real experiments,[0],[0]
"So, to save time in real interventions, we only test CEC from the first and EXP from the second pair, and compare them with the random policy (RND) and with our algorithm (LTD).
",4.2. Real experiments,[0],[0]
Real-time intervention results.,4.2. Real experiments,[0],[0]
Fig. 6 shows the performance of our results compared to competitors.,4.2. Real experiments,[0],[0]
The results show that our approach outperforms the other three baselines by a reasonable margin.,4.2. Real experiments,[0],[0]
As expected CEC is the second best algorithm with a margin of 5 for the correlation maximization objective.,4.2. Real experiments,[0],[0]
"It translates to increase in amount of correlation equal to 5, which is a noticeable amount.",4.2. Real experiments,[0],[0]
"Furthermore, in the difference minimization task, our approach reached around 7 in difference.",4.2. Real experiments,[0],[0]
"This means that we decreased the difference in exposure to the two processes to less 2.6 per user, which is considerable improvement.",4.2. Real experiments,[0],[0]
"For both tasks, LTD made more mitigation posts over all day-
time phases than it did over all nighttime phases, whereas the competitor strategies did the opposite.",4.2. Real experiments,[0],[0]
This could be a reason for its better performance.,4.2. Real experiments,[0],[0]
"One surprising fact is that the number of retweets by users outside the network, which was not used for our features, can exceed the number of retweets by users within the network.",4.2. Real experiments,[0],[0]
"This is because the “hashtag” feature on Twitter allows posts to be seen by a much larger set of users, who do not necessarily follow the source accounts.",4.2. Real experiments,[0],[0]
"In addition to retweets, users can also “like” a post, indicating that they were exposed to fake or real news; while we measured this, we did not include it in the reward.",4.2. Real experiments,[0],[0]
Future experiments can use these two observations to widen the experimental scope and more accurately measure the effectiveness of a mitigation strategy.,4.2. Real experiments,[0],[0]
"Despite having these limitations, our experiment serves as a proofof-concept for the applicability of point process based intervention in networks, and–to the best of our knowledge– is the first to verify the superiority of a method in a realtime, real-world intervention setting.",4.2. Real experiments,[0],[0]
Prediction evaluation results.,4.2. Real experiments,[0],[0]
The previous part describes the more interesting evaluation scheme of real-time intervention in a social media platform.,4.2. Real experiments,[0],[0]
"In this part, we used historical real data to mimic this procedure.",4.2. Real experiments,[0],[0]
We extracted 12 full 10-stage trajectory of events from the 2-month random policy historical data.,4.2. Real experiments,[0],[0]
"For any of these 10 pairs, the methods were evaluated according to how well they predict the relative ordering among these 12 trajectories (with respect to the objective function).",4.2. Real experiments,[0],[0]
"To evaluate each method, we created a sorted list of these 12 trajectories according to increasing objective, and created a second list sorted by increasing closeness to the intervention method.",4.2. Real experiments,[0],[0]
"This closeness is the mean squared error between the prescribed intervention and actual intensity, which we inferred using maximum likelihood.",4.2. Real experiments,[0],[0]
"Then, by computing the rank correlation of the two sorted lists, and repeating for each of the five methods, we can find out how well they perform on the prediction task.",4.2. Real experiments,[0],[0]
A better predictor is expected to be a better mitigation strategy.,4.2. Real experiments,[0],[0]
"Fig. 7 shows the performance.
",4.2. Real experiments,[0],[0]
"A short discussion is presented in appendix G.
Acknowledgement.",4.2. Real experiments,[0],[0]
"This project is supported in part by NSF IIS-1639792, CNS-1409635, NSF DMS-1620342, NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, ONR N00014-15-1-2340, NVIDIA, Intel, and Amazon AWS.",4.2. Real experiments,[0],[0]
Abstract We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model.,abstractText,[0],[0]
The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms.,abstractText,[0],[0]
"By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework.",abstractText,[0],[0]
"We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal total reward under budget constraints.",abstractText,[0],[0]
"Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.",abstractText,[0],[0]
Fake News Mitigation via Point Process Based Intervention,title,[0],[0]
"Matrix completion method has been used in a wide range of applications such as collaborative filtering for recommendation (Koren et al., 2009), multi-label learning (Cabral et al., 2011) and clustering (Hsieh et al., 2012).",1. Introduction,[0],[0]
"In these applications, every entry is modeled as the inner product between factors corresponding to the row and column variables.",1. Introduction,[0],[0]
"For example, in movie recommendation, each row factor represents the latent representation of a user and each column factor represents the latent representation of a movie.
",1. Introduction,[0],[0]
"In many applications of significant interest, besides the partially observed matrix, side information, in the form of features, is also available.",1. Introduction,[0],[0]
"These might correspond to de-
*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA.",1. Introduction,[0],[0]
"2Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.",1. Introduction,[0],[0]
"3Department of Computer Science, University of California, Los Angeles, CA 90095, USA.",1. Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qgu@cs.ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"mographic information (genders, occupation) for users or product information (genre, director) in a movie recommender system for example.",1. Introduction,[0],[0]
"With such features at hand, one can model an observation as a specific linear interaction between features to reduce the model complexity.",1. Introduction,[0],[0]
"Formally, let L∗ ∈ Rd1×d2 be the unknown low-rank matrix with rank r, and let XL ∈ Rd1×n1 and XR ∈ Rd2×n2 be the known feature matrices with d1 ≥ n1 ≥ r and d2 ≥ n2",1. Introduction,[0],[0]
≥ r.,1. Introduction,[0],[0]
"We assume the unknown rank-r matrix L∗ can be represented by XLM∗X>R for some unknown matrix M
∗ ∈ Rn1×n2 .",1. Introduction,[0],[0]
"Thus instead of learning a large d1 × d2 matrix L∗, we only need to recover a smaller low-rank matrix M∗.",1. Introduction,[0],[0]
"This inductive approach has been applied successfully in many applications including collaborative filtering (Abernethy et al., 2009; Menon et al., 2011; Chen et al., 2012), multi-label learning (Xu et al., 2013; Si et al., 2016), semi-supervised clustering (Yi et al., 2013; Si et al., 2016), gene-disease prediction (Natarajan & Dhillon, 2014) and blog recommendation (Shin et al., 2015).
",1. Introduction,[0],[0]
"From the theoretical point of view, side information allows us to reduce the overall sample and computational complexities.",1. Introduction,[0],[0]
Xu et al. (2013) and Jain & Dhillon (2013) pioneered the theoretical investigation in this direction.,1. Introduction,[0],[0]
"Specifically, Xu et al. (2010) adapted the convex relaxation approach (Candès & Recht, 2009; Candès & Tao, 2010) and requires only O(rn log n log d)1 samples to recover the underlying matrix, which we believe is tight up to logarithmic factors.",1. Introduction,[0],[0]
"However, the computational cost is usually high because they need to solve a nuclear norm minimization problem, which is inherently slow due to its high per-iteration complexity and non-strongly convex objective function (",1. Introduction,[0],[0]
"c.f. Equation (2) in Xu et al. (2013)), which does not have linear convergence rate.",1. Introduction,[0],[0]
"On the other hand, Jain & Dhillon (2013) (also see Zhong et al. (2015))",1. Introduction,[0],[0]
"proposed an algorithm which first does a spectral initialization to obtain a coarse estimate, then uses alternating minimization to refine the estimate.",1. Introduction,[0],[0]
"Their algorithm has a locally linear rate of convergence but requires O(r3n2 log n log(1/ )) samples, which has an unsatisfactory quadratic dependency on n and cannot achieve exact recovery because sample complexity also depends on the target accuracy .",1. Introduction,[0],[0]
"A natural and open question is:
1For the ease of presentation, we assume d1 = d2 = d and n1 = n2 = n when discussing complexities.
",1. Introduction,[0],[0]
"Can we recover the ground truth matrix at a linear rate with sample complexity linear in n?
",1. Introduction,[0],[0]
"In this paper, we answer this question affirmatively.",1. Introduction,[0],[0]
"Specifically, we propose a multi-phase gradient-based algorithm that converges to the underlying true matrix at a linear rate with sample complexity linearly depending on n and logarithmically depending on d. Our algorithm is a novel and highly nontrivial extension of Procrustes Flow (Tu et al., 2015) in which we add an additional phase to reduce the variance of gradient estimate, and therefore we call it MultiPhase Procrustes Flow.",1. Introduction,[0],[0]
The main challenges and technical insights are summarized in the following section.,1. Introduction,[0],[0]
"In recent years, a surge of non-convex optimization algorithms for estimating low-rank matrices have been established.",1.1. Main Challenges and Technical Insights,[0],[0]
"A typical procedure is first to do a spectral initialization to obtain a coarse estimate, and then to use BurerMonteiro factorization (Burer & Monteiro, 2003) with projected gradient descent (a.k.a., Procrustes flow) on the partially observed entries to recover the underlying matrix, where the projection is introduced to control the variance of gradient descent (Tu et al., 2015; Zheng & Lafferty, 2016; Yi et al., 2016).",1.1. Main Challenges and Technical Insights,[0],[0]
Our proposed algorithm also follows this framework.,1.1. Main Challenges and Technical Insights,[0],[0]
"However, direct adaptation does not achieve the desired statistical and computational rates.",1.1. Main Challenges and Technical Insights,[0],[0]
"Statistically, in the classical matrix completion setting, after the initialization phase, the variance of the gradient is at a smaller order than the magnitude of expected gradient for all iterations.",1.1. Main Challenges and Technical Insights,[0],[0]
"However, in our setting, because of limited samples, such uniform bound does not hold.",1.1. Main Challenges and Technical Insights,[0],[0]
"Computationally, the projection step in the inductive setting is more costly than that in the classical setting because we need to solve a convex quadratically-constrained-quadratic-programming (QCQP) problem (",1.1. Main Challenges and Technical Insights,[0],[0]
"c.f. Section 4).
",1.1. Main Challenges and Technical Insights,[0],[0]
Our first key observation is that the variance of the gradient converges to 0 at a faster rate than the magnitude of expectation of the gradient.,1.1. Main Challenges and Technical Insights,[0],[0]
"Therefore, if the iterate is close enough to the optimum, say in a ball with radius O(1/n) around the optimum, the desired uniform bound still holds.",1.1. Main Challenges and Technical Insights,[0],[0]
"Further, this observation also indicates when we are close to the optimum, projection step is not needed, i.e., vanilla gradient descent suffices.",1.1. Main Challenges and Technical Insights,[0],[0]
"Nevertheless, with limited samples, the spectral initialization cannot directly achieve this goal.
",1.1. Main Challenges and Technical Insights,[0],[0]
"Our second key observation is that after a rough spectral initialization, if we use fresh samples to calculate the gradient at each iteration, the variance is still small compared with the expectation of the gradient.",1.1. Main Challenges and Technical Insights,[0],[0]
"In light of this, we add a new phase to the original algorithm where we use fresh samples to estimate the gradient at each iteration and use projected gradient descent to refine our estimation.",1.1. Main Challenges and Technical Insights,[0],[0]
"Though the projection is costly, we only need O(r log n) iterations
to converge to a ball with radius O(1/n) around the optimum, since gradient descent in our problem enjoys a linear rate of convergence.",1.1. Main Challenges and Technical Insights,[0],[0]
"Putting all these phases together, we propose the first gradient-based algorithm that requires only O ( r2n log n log d ) samples and converges to the ground truth matrix at a linear rate.
Notation.",1.1. Main Challenges and Technical Insights,[0],[0]
"Capital boldface letters such as A are used for matrices, and [`] is used to denote the index set {1, 2, . . .",1.1. Main Challenges and Technical Insights,[0],[0]
",",1.1. Main Challenges and Technical Insights,[0],[0]
`}.,1.1. Main Challenges and Technical Insights,[0],[0]
"Denote the d × d identity matrix by Id. Let Ai,∗, A∗,j and Aij be the i-th row, j-th column and (i, j)-th entry of matrix A, respectively.",1.1. Main Challenges and Technical Insights,[0],[0]
"Denote the `-th largest singular value of A by σ`(A) and its projection onto the index set Ω by PΩ(A), i.e., the (i, j)-th entry of PΩ(A) is equal to Aij if (i, j) ∈ Ω and zero otherwise.",1.1. Main Challenges and Technical Insights,[0],[0]
Let ‖x‖2 be the `2 norm of a d-dimensional vector x ∈ Rd.,1.1. Main Challenges and Technical Insights,[0],[0]
"Let ‖A‖F , ‖A‖2 be the Frobenius norm and the spectral norm of matrix A respectively.",1.1. Main Challenges and Technical Insights,[0],[0]
"The largest `2 norm of its rows is defined as ‖A‖2,∞ = maxi ‖Ai,∗‖2.",1.1. Main Challenges and Technical Insights,[0],[0]
"For any two sequences {an} and {bn}, we say an = O(bn) if there exists a positive constant C such that an ≤ C bn.",1.1. Main Challenges and Technical Insights,[0],[0]
"Classical approach for matrix completion relies on convex relaxation (Candès & Recht, 2009; Candès & Tao, 2010; Recht, 2011; Chen, 2015; Allen-Zhu et al., 2017), which can be solved by nuclear norm minimization.",2.1. Low-Rank Matrix Completion,[0],[0]
"Such methods usually have tight sample complexity (Balcan et al., 2017), but due to the use of nuclear norm and non-strongly convex objective function, they cannot achieve linear convergence rate and often scale cubically with the dimension.",2.1. Low-Rank Matrix Completion,[0],[0]
"Some faster algorithms have been proposed (Jain & Netrapalli, 2015) but they often incur additional sample complexity.
",2.1. Low-Rank Matrix Completion,[0],[0]
"To reduce the runtime complexity, various non-convex algorithms have been proposed.",2.1. Low-Rank Matrix Completion,[0],[0]
"Jain et al. (2013); Hardt (2014); Hardt & Wootters (2014); Gu et al. (2016); Gamarnik et al. (2017) showed that with proper initialization, alternating minimization enjoys a linear convergence rate.",2.1. Low-Rank Matrix Completion,[0],[0]
"Proofs of these works often build on a general analytical framework, noisy-power-method (Hardt & Price, 2014; Balcan et al., 2016).",2.1. Low-Rank Matrix Completion,[0],[0]
"Nevertheless, the sample complexity often depends on the inverse of target accuracy.",2.1. Low-Rank Matrix Completion,[0],[0]
"Thus these methods often cannot recover the ground truth matrix exactly.
",2.1. Low-Rank Matrix Completion,[0],[0]
"Another line of research studies the landscape of optimization problem and showed that with proper modification of objective function, all local minima are global and all saddle points are strict (Bhojanapalli et al., 2016b; Ge et al., 2016; 2017).",2.1. Low-Rank Matrix Completion,[0],[0]
"Therefore, perturbed gradient descent algorithms can solve this non-convex problem efficiently (Ge et al., 2015; Jin et al., 2017; Du et al., 2017a).",2.1. Low-Rank Matrix Completion,[0],[0]
"However, to guarantee the landscape having nice properties, they all require the
sample complexity scales with the fourth power of the rank, which is suboptimal.
",2.1. Low-Rank Matrix Completion,[0],[0]
"Lastly, Tu et al. (2015); Zhao et al. (2015); Zheng & Lafferty (2015); Sun & Luo (2015); Bhojanapalli et al. (2016a); Zheng & Lafferty (2016); Yi et al. (2016); Wang et al. (2016; 2017); Ma et al. (2017); Xu et al. (2017); Zhang et al. (2018) proposed first-order algorithms to solve low-rank matrix estimation problems.",2.1. Low-Rank Matrix Completion,[0],[0]
"Similar to Jain et al. (2013); Hardt & Wootters (2014); Hardt (2014), these algorithms first use spectral initialization to find a good starting point, but instead of performing alternating minimization, they use (projected) gradient descent to refine the initial solution, and are guaranteed to converge to the global optimum at a linear rate.",2.1. Low-Rank Matrix Completion,[0],[0]
"Notably, the sample complexity of these algorithms does not depend on the target accuracy and is only slightly larger than that of convex programming approaches.",2.1. Low-Rank Matrix Completion,[0],[0]
Our proposed algorithm also belongs to this line of research but with significant innovations in both algorithm and theory (c.f. Section 1.1).,2.1. Low-Rank Matrix Completion,[0],[0]
Matrix completion with side information has drawn much attention for improving the performance of traditional matrix completion methods in various applications.,2.2. Matrix Completion with Side Information,[0],[0]
"This method dates back to Jain & Dhillon (2013); Xu et al. (2013), where they proposed the so-called Inductive Matrix Completion methods independently.",2.2. Matrix Completion with Side Information,[0],[0]
"The method is “inductive”, in that it can be generalized to previously unobserved data points, which resolves a major drawback in traditional recommender systems.",2.2. Matrix Completion with Side Information,[0],[0]
"Extensions to noisy features (Chiang et al., 2015) and non-linear models (Si et al., 2016) have been studied and similar formulation has also been extended to the problem of robust PCA (Chiang et al., 2016; Niranjan et al., 2017; Xue et al., 2017).
",2.2. Matrix Completion with Side Information,[0],[0]
"Theoretically, side information allows us to recover the target matrix with sample complexity depending on the intrinsic feature dimension rather than the ambient dimension.",2.2. Matrix Completion with Side Information,[0],[0]
"Information theoretically speaking, with known features, O(rn) samples are sufficient for exact recovery and this is achieved up to some logarithmic factors by the convex relaxation based algorithm proposed in Xu et al. (2013).",2.2. Matrix Completion with Side Information,[0],[0]
"However, such formulation requires solving a nuclear norm minimization problem and in general cannot have the linear convergence.",2.2. Matrix Completion with Side Information,[0],[0]
Jain & Dhillon (2013) adopted ideas from Jain et al. (2013); Hardt (2014); Hardt & Wootters (2014) to obtain a linear convergent algorithm but it requires O ( r3n2 log n log(1/ ) ),2.2. Matrix Completion with Side Information,[0],[0]
samples.,2.2. Matrix Completion with Side Information,[0],[0]
See Table 1 for a detailed comparison between our method and two existing inductive matrix completion algorithms:,2.2. Matrix Completion with Side Information,[0],[0]
"Maxide (Xu et al., 2013) and AltMin2 (Jain & Dhillon, 2013).",2.2. Matrix Completion with Side Information,[0],[0]
"It is worth noting that
2Jain & Dhillon (2013) requires a weaker incoherence condition in that they only assume the features are incoherent.",2.2. Matrix Completion with Side Information,[0],[0]
"However,
our approach achieves both linear rate of convergence and sample complexity linear in the feature dimension n.",2.2. Matrix Completion with Side Information,[0],[0]
Recall that our goal is to recover the unknown rank-r matrix L∗ ∈ Rd1×d2 by learning a lower-dimensional matrix M∗ ∈ Rn1×n2 given the side information in terms of XL and XR.,3. Problem Setup and Preliminaries,[0],[0]
Denote the rank-r singular value decomposition (SVD) of M∗ by M∗,3. Problem Setup and Preliminaries,[0],[0]
= U ∗,3. Problem Setup and Preliminaries,[0],[0]
Σ∗V ∗> .,3. Problem Setup and Preliminaries,[0],[0]
Let σ∗1 ≥ σ∗2 ≥ . . .,3. Problem Setup and Preliminaries,[0],[0]
≥ σ∗r > 0,3. Problem Setup and Preliminaries,[0],[0]
be the sorted singular values of M∗ and κ = σ∗1/σ ∗,3. Problem Setup and Preliminaries,[0],[0]
r be the condition number.,3. Problem Setup and Preliminaries,[0],[0]
"Assume each entry of L∗ is observed independently with probability p ∈ (0, 1).",3. Problem Setup and Preliminaries,[0],[0]
"In particular, for any (i, j) ∈",3. Problem Setup and Preliminaries,[0],[0]
"[d1] × [d2], we consider the following Bernoulli observation model
Lij = { L∗ij , with probability p; ∗, otherwise.",3. Problem Setup and Preliminaries,[0],[0]
"(3.1)
",3. Problem Setup and Preliminaries,[0],[0]
"Let Ω be the index set of observed entries in L∗, i.e., Ω ={ (i, j) ∈",3. Problem Setup and Preliminaries,[0],[0]
[d1]× [d2] ∣∣Lij 6= ∗}.,3. Problem Setup and Preliminaries,[0],[0]
"Note that restricting on the observed index set Ω, we have PΩ(L) = PΩ(L∗).
",3. Problem Setup and Preliminaries,[0],[0]
"In order to fully exploit the side information, following Xu et al. (2013); Yi et al. (2013); Chiang et al. (2016), we assume the following standard feasibility condition: col(XL) ⊇ col(L∗), col(XR) ⊇ col(L∗>), where col(A) represents the column space of matrix A. Intuitively, this condition suggests that the feature matrices are correlated to the underlying true low-rank space, so that we could make use of the feature information to improve our recovery.",3. Problem Setup and Preliminaries,[0],[0]
"In other words, we assume L∗ can be decomposed as L∗ = XLM∗X>R.",3. Problem Setup and Preliminaries,[0],[0]
"In addition, without loss of generality, we assume both feature matrices XL and XR have orthonormal columns3, i.e., X>LXL = In1 , X > RXR = In2 .
",3. Problem Setup and Preliminaries,[0],[0]
"It is well-known in matrix completion (Gross, 2011) that if L∗ is equal to zero in nearly all of the rows or columns, recovering L∗ exactly is impossible unless all of its entries
when additional incoherence condition is imposed, it is unclear whether their algorithm can reduce the sample complexity or not.
3In practice, one could conduct QR factorization or SVD to acquire the corresponding orthonormal feature matrices.
are sampled.",3. Problem Setup and Preliminaries,[0],[0]
"Therefore, we impose the standard incoherence condition on the unknown low-rank matrix L∗ (Candès & Recht, 2009; Recht, 2011; Yi et al., 2016).",3. Problem Setup and Preliminaries,[0],[0]
"Note that given feature matrices XL,XR, the singular value decomposition of L∗ can be formulated as (XLU ∗ )Σ∗(XRV)
>.",3. Problem Setup and Preliminaries,[0],[0]
Assumption 3.1 (Incoherence for L∗).,3. Problem Setup and Preliminaries,[0],[0]
"The unknown lowrank matrix L∗ is µ0-incoherent, i.e., ‖XLU
∗‖2,∞ ≤√ µ0r/d1, ‖XRV ∗‖2,∞ ≤ √ µ0r/d2.
",3. Problem Setup and Preliminaries,[0],[0]
"Furthermore, following Jain & Dhillon (2013); Xu et al. (2013); Chiang et al. (2016), we impose the following incoherence condition on the feature matrices.",3. Problem Setup and Preliminaries,[0],[0]
Assumption 3.2 (Incoherence for feature matrices).,3. Problem Setup and Preliminaries,[0],[0]
"The feature matrices XL and XR are both self-incoherent with parameter µ1, i.e, ‖XL‖2,∞ ≤ √ µ1n1/d1, ‖XR‖2,∞ ≤√
µ1n2/d2.
",3. Problem Setup and Preliminaries,[0],[0]
"With the aid of additional feature information, inductive matrix completion can be formulated as follows
min M∈Rn1×n2
1
2p ∥∥PΩ(XLMX>R − L)∥∥2F , subject to rank(M) ≤",3. Problem Setup and Preliminaries,[0],[0]
"r,
(3.2)
where Ω is the index set of observed entries and p = |Ω|/(d1d2) denotes the sampling probability in the observation model.",3. Problem Setup and Preliminaries,[0],[0]
"In order to estimate the low-rank matrix M∗ more efficiently, following Tu et al. (2015), Zheng & Lafferty (2015) and Yi et al. (2016), we propose to solve the following factorized non-convex optimization problem
min U∈Rn1×r V∈Rn2×r
1
2p ∥∥PΩ(XLUV>X>R − L)∥∥2F .",3. Problem Setup and Preliminaries,[0],[0]
"(3.3) Due to the reparameterization M = UV>, the rank constraint in (3.2) is automatically guaranteed in (3.3).",3. Problem Setup and Preliminaries,[0],[0]
Let U∗ = U ∗ Σ∗1/2 and V∗ = V ∗ Σ∗1/2 be the true factorized matrices.,4. The Proposed Algorithm,[0],[0]
"It is obvious that (U∗,V∗) is the optimal solution to optimization problem (3.3).",4. The Proposed Algorithm,[0],[0]
"However, for any invertible matrix P ∈ Rr×r, (U∗P,V∗(P−1)>) is also an optimal solution.",4. The Proposed Algorithm,[0],[0]
"In order to deal with this identifiability issue, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016), we impose an additional regularizer to the objective function in (3.3) to penalize the scale difference between U and V. Specifically, we consider the following regularized optimization problem
min U∈Rn1×r V∈Rn2×r
fΩ(U,V) := 1
2p ∥∥PΩ(XLUV>X>R − L)∥∥2F + 1
8
∥∥U>U−V>V∥∥2 F ,
(4.1)
where U ∈ Rn1×r, V ∈ Rn2×r, and fΩ denotes the regularized sample loss function.",4. The Proposed Algorithm,[0],[0]
"Intuitively speaking, the regularization term encourages the two factorized matrices U and V to have a similar scale.
",4. The Proposed Algorithm,[0],[0]
"We propose a multi-phase gradient-based algorithm to solve the proposed estimator (4.1), as shown in Algorithm 1.",4. The Proposed Algorithm,[0],[0]
"More specifically, we first randomly split the observed index set Ω into S + 1 independent subsets {Ωs}Ss=0, where Ω0 has cardinality |Ω|/2 and each of the rest has cardinality |Ω|/(2S).",4. The Proposed Algorithm,[0],[0]
"In Phase 1, we project the observed matrix L onto the first subset Ω0 and perform rank-r SVD on p−10 PΩ0(L∗) to get an initial estimator (Uinit,Vinit), where p0 = |Ω0|/(d1d2).",4. The Proposed Algorithm,[0],[0]
"We use SVDr(·) to denote the rank-r SVD.
",4. The Proposed Algorithm,[0],[0]
"In Phase 2, we perform projected gradient descent with resampling (Jain et al., 2013; Jain & Dhillon, 2013) (a.k.a., sample splitting), where we use one fresh subset for each gradient descent update.",4. The Proposed Algorithm,[0],[0]
"The projection step guarantees that each intermediate iterate satisfies the similar incoherence condition as that of (U∗,V∗), while the resampling scheme ensures the independence of the samples used in the current iteration and the previous iterates.",4. The Proposed Algorithm,[0],[0]
"As will be clear in the next section and in the proofs, the second phase is crucial in reducing the variance of gradient estimate and ensures the uniform convergence in the third phase.",4. The Proposed Algorithm,[0],[0]
"The constraint sets C1 and C2 associated with the projection are defined as
C1 = { U ∈ Rn1×r ∣∣∣ ‖XLU‖2,∞ ≤√µ0r
d1 ‖Zinit‖2
} ,
C2 = { V ∈ Rn2×r ∣∣∣ ‖XRV‖2,∞ ≤√µ0r
d2 ‖Zinit‖2
} ,
(4.2) where Zinit is specified in Phase 1.",4. The Proposed Algorithm,[0],[0]
"Let PC1(Û) be the projection of Û ∈ Rn1×r onto C1, which can be alternatively regarded as the exact solution to the following convex quadratically-constrained-quadratic-programming (QCQP)
argmin U∈Rn1×r
1 2 ‖U− Û‖2F ,
subject to ∥∥[XLU]i,∗∥∥22 ≤ µ0rd1",4. The Proposed Algorithm,[0],[0]
"‖Zinit‖22,∀ i ∈",4. The Proposed Algorithm,[0],[0]
[d1].,4. The Proposed Algorithm,[0],[0]
"(4.3)
It is worth noting that convex QCQP problem can be solved approximately and efficiently using interior point methods (Nemirovskii, 2004).",4. The Proposed Algorithm,[0],[0]
"Let PC1(Û, δ) be the δ-approximate solution to optimization problem (4.3), i.e., ‖PC1(Û, δ)− PC1(Û)‖F ≤ δ.",4. The Proposed Algorithm,[0],[0]
"Similarly, the QCQP problem with respect to V is formulated in a similar way, except that XL (resp.",4. The Proposed Algorithm,[0],[0]
d1) is replaced with XR (resp. d2).,4. The Proposed Algorithm,[0],[0]
"Accordingly, we use PC2(V̂) to denote the exact projection, and PC2(V̂, δ) to be the δ-approximate projection.
",4. The Proposed Algorithm,[0],[0]
"In addition, the loss function used in the s-th iteration of Phase 2 is based on the subset Ωs, and it is identical to the loss function in (4.1), except that Ω (resp.",4. The Proposed Algorithm,[0],[0]
p) is replaced with Ωs (resp.,4. The Proposed Algorithm,[0],[0]
"ps = |Ωs|/(d1d2)).
",4. The Proposed Algorithm,[0],[0]
"Finally, in Phase 3, vanilla gradient descent is performed based on the entire observed matrixPΩ(L∗).",4. The Proposed Algorithm,[0],[0]
"Provided these three phases, as will be seen in later analysis, Algorithm 1 is guaranteed to converge to the true factorized matrices (U∗,V∗) with a linear rate of convergence.
",4. The Proposed Algorithm,[0],[0]
"Algorithm 1 GD for IMC Input: Observed matrix PΩ(L∗); feature matrices XL,
XR; parameter p0 = |Ω|/(2d1d2); step size τ, η; number of iterations S, T , approximation error δ.
",4. The Proposed Algorithm,[0],[0]
"Randomly split Ω into subsets Ω0,Ω1, . . .",4. The Proposed Algorithm,[0],[0]
",ΩS with |Ω0| = |Ω|/2 and |Ωs| = |Ω|/(2S), for any s ∈",4. The Proposed Algorithm,[0],[0]
"[S]
//",4. The Proposed Algorithm,[0],[0]
Phase 1:,4. The Proposed Algorithm,[0],[0]
"Initialization [Ũ0,Σ0, Ṽ0] = SVDr ( p−10 PΩ0(L∗) )",4. The Proposed Algorithm,[0],[0]
Uinit = X >,4. The Proposed Algorithm,[0],[0]
"LŨ0Σ 1/2 0 ; Vinit = X > RṼ0Σ 1/2 0
Zinit =",4. The Proposed Algorithm,[0],[0]
[Uinit; Vinit] //,4. The Proposed Algorithm,[0],[0]
"Phase 2: PGD with subsamples
U0 = PC1(Uinit, δ),V0 = PC2(Vinit, δ) for: s = 1, 2, . . .",4. The Proposed Algorithm,[0],[0]
", S do
Us = PC1 ( Us−1 − η∇UfΩs(Us−1,Vs−1), δ )",4. The Proposed Algorithm,[0],[0]
Vs = PC2,4. The Proposed Algorithm,[0],[0]
"( Vs−1 − η∇VfΩs(Us−1,Vs−1), δ
) end for
//",4. The Proposed Algorithm,[0],[0]
"Phase 3: Vanilla GD U0 = US , V0 = VS for: t = 0, 1, . . .",4. The Proposed Algorithm,[0],[0]
", T − 1 do
Ut+1 =",4. The Proposed Algorithm,[0],[0]
"Ut − τ∇UfΩ(Ut,Vt) Vt+1",4. The Proposed Algorithm,[0],[0]
"= Vt − τ∇VfΩ(Ut,Vt)
end for Output: (UT ,VT )",4. The Proposed Algorithm,[0],[0]
"Before presenting the main theoretical results, we note that the optimal solution to optimization problem (4.1) is not unique.",5. Main Theory,[0],[0]
"Therefore, following Tu et al. (2015), we introduce the so-called Procrustes distance.",5. Main Theory,[0],[0]
"For simplicity, we let Z∗ =",5. Main Theory,[0],[0]
[U∗; V∗] be the stacked true parameter matrix.,5. Main Theory,[0],[0]
Definition 5.1.,5. Main Theory,[0],[0]
"For any Z ∈ R(n1+n2)×r, let D(Z,Z∗) be the minimal distance between Z and Z∗ in terms of the optimal rotation, or more precisely, D(Z,Z∗) = minR∈Qr ‖Z− Z∗R‖F , where Qr denotes the set of r-byr othorgonal matrices.
",5. Main Theory,[0],[0]
"In the following discussions, we use d and n to denote max{d1, d2} and max{n1, n2}, respectively.",5. Main Theory,[0],[0]
Our main theoretical result on Algorithm 1 is presented as follows.,5. Main Theory,[0],[0]
Theorem 5.2.,5. Main Theory,[0],[0]
"Assume the observed index set Ω follows Bernoulli model (3.1) and incoherence Assumptions 3.1, 3.2 hold.",5. Main Theory,[0],[0]
"There exist constants c1, c2, c3, c4, c5 such that under condition |Ω|",5. Main Theory,[0],[0]
"≥ c1 max{µ1n, µ0rκ}µ0r2κ2 log n log d, if step size η = c2/(rσ∗1), τ = c3/σ ∗ 1 and approximation
error δ =",5. Main Theory,[0],[0]
"O ( 1/(rκn2) ) , after S = O(rκ log n) iterations in Phase 2 and T = O (κ log(1/ )) iterations in Phase 3,
with probability at least 1 − c4rκ log n/d, the output of Algorithm 1 satisfies
‖MT −M∗‖F ≤ c5",5. Main Theory,[0],[0]
"√ σ∗1 ,
where MT = UTVT> and M∗ = U∗V∗>.
",5. Main Theory,[0],[0]
Theorem 5.2 shows that the overall sample complexity of Algorithm 1 is O ( r2κ2n log n log d ) .,5. Main Theory,[0],[0]
"Here, we explicitly write down the dependency on condition number κ in the O(·) notation for completeness.",5. Main Theory,[0],[0]
"It is worth noting that our gradient-based Algorithm 1 achieves both linear rate of convergence and sample complexity linearly depending on n, compared with convex relaxation based approach (Xu et al., 2013) whose convergence rate is sublinear (i.e., O(1/ √ )) and alternation minimization (Jain & Dhillon, 2013), which requires at least O ( r3n2 log n log(1/ ) ) samples.
",5. Main Theory,[0],[0]
Theorem 5.2 can be achieved by analyzing the three phases of Algorithm 1.,5. Main Theory,[0],[0]
"In the sequel, we are going to provide the theoretical guarantees of each phase.
",5. Main Theory,[0],[0]
Theorem 5.3 (Initialization).,5. Main Theory,[0],[0]
Assume the observed index set Ω follows Bernoulli model (3.1).,5. Main Theory,[0],[0]
"Suppose Assumptions 3.1, 3.2 hold for the unknown low-rank matrix L∗ and the feature matrices XL,XR, respectively.",5. Main Theory,[0],[0]
"For any γ ∈ (0, 1), there exist constants c1, c2 such that under the condition |Ω0| ≥ c1µ0µ1r2κ2n log d/γ2, with probability at least 1− c2/d, the output of Phase 1 in Algorithm 1 satisfies
D(Zinit,Z ∗) ≤",5. Main Theory,[0],[0]
"4γ √ σ∗r .
",5. Main Theory,[0],[0]
Theorem 5.3 suggests that the output of Initialization Phase 1 is already in a small neighbourhood of the optimum with radius O( √ σ∗r ).,5. Main Theory,[0],[0]
"Notably, the sample complexity is linear in n, in sharp contrast to that of the classical matrix completion setting which is at least linear in d.
Theorem 5.4 (PGD with subsamples).",5. Main Theory,[0],[0]
"Under the same conditions as in Theorem 5.3, suppose the output of Phase 1, Zinit, satisfies D(Zinit,Z∗) ≤ α √ σ∗r/2 with constant α ≤ 1/40.",5. Main Theory,[0],[0]
"There exist constants c1, c2, c3, c4, c5 such that, if the total sample size |Ω| ≥ c1S ·max{µ0µ1rκn, µ20r2κ2} log d, with step size η = c2/(rσ∗1) and approximation error δ ≤",5. Main Theory,[0],[0]
"c3 √ σ∗r/(rκ), the final iterate (US ,VS) in Phase 2 of Algorithm 1 satisfies
D2(ZS ,Z ∗) ≤",5. Main Theory,[0],[0]
"( 1− c2
16rκ
)S α2σ∗r + c4δrκ",5. Main Theory,[0],[0]
"√ σ∗r (5.1)
with probability at least 1− c5S/d,",5. Main Theory,[0],[0]
where ZS =,5. Main Theory,[0],[0]
"[US ; VS ].
",5. Main Theory,[0],[0]
The last term on the right hand side of (5.1) originates from the approximation error δ when solving the convex QCQP (4.3) with respect to U (or V).,5. Main Theory,[0],[0]
"Theorem 5.4 suggests that under proper initialization, the gradient iteration in Phase 2 converges at a linear rate with contraction parameter 1−
O(1/(rκ)).",5. Main Theory,[0],[0]
Note that the step size is chosen asO(1/(rσ∗1)).,5. Main Theory,[0],[0]
"In practice, since σ∗1 is unknown, we can approximate σ ∗ 1 by C · ‖UinitV>init‖2 and tune the coefficient C.
Theorem 5.5 (Vanilla GD).",5. Main Theory,[0],[0]
"Under the same conditions as in Theorem 5.3, suppose the final iterate (US ,VS) of Phase 2 in Algorithm 1 satisfiesD(ZS ,Z∗) ≤ c0 √ σ∗r/(µ1n) with ZS =",5. Main Theory,[0],[0]
[US ; VS ] and constant c0 small enough.,5. Main Theory,[0],[0]
"Then there exist constants c1, c2, c3 such that if |Ω| ≥ c1µ0µ1rn log d, with step size τ = c2/σ∗1 , the output of Phase 3 satisfies
D2(ZT ,Z∗) ≤",5. Main Theory,[0],[0]
( 1− τσ ∗,5. Main Theory,[0],[0]
"r
16
)T D2(ZS ,Z ∗)
",5. Main Theory,[0],[0]
"with probability at least 1− c3/d, where ZT = [UT ; VT ].
Theorem 5.5 implies that if the final iterate of Phase 2 falls into a even smaller neighbourhood around the optimum with radius O(1/n), vanilla gradient descent suffices to guarantee the linear rate of convergence.
",5. Main Theory,[0],[0]
Remark 5.6 (Computational Complexity).,5. Main Theory,[0],[0]
The rank-r SVD in Phase 1 requires O(r|Ω0|) computation.,5. Main Theory,[0],[0]
"The runtime of the gradient computation for the s-th iteration in the second phase is O(rn|Ωs| + r2n), while solving the convex QCQP subproblem requiresO(r2n2d3/2 log d) computation if using the path-following interior point method (Nemirovskii, 2004).",5. Main Theory,[0],[0]
"Thus to perform S = O(rκ log n) iterations, the overall computational complexity of Phase 2 is O(r3n2d3/2 log n log d).",5. Main Theory,[0],[0]
"The runtime of gradient computation in each iteration of Phase 3 is O(rn|Ω|) and the total number of iterations required in Phase 3 is T = O(κ log(1/ )), which implies the overall computational cost in Phase 3 is O(r3n2 log n log d log(1/ )).",5. Main Theory,[0],[0]
"Putting all these pieces together, we conclude the total computational complexity of Algorithm 1 is O(r3n2 log n log d log(1/ )",5. Main Theory,[0],[0]
+ r3n2d3/2 log n log d).,5. Main Theory,[0],[0]
"In this section, we compare the proposed gradient-based algorithm with existing inductive matrix completion methods, including the convex relaxation based approach, Maxide (Xu et al., 2013) and alternating minimization based algorithm, AltMin (Jain & Dhillon, 2013) on both synthetic and real datasets.",6. Experiments,[0],[0]
"In addition, the standard matrix completion approach based on non-convex projected gradient descent (Zheng & Lafferty, 2016) (MC) is compared as a baseline for simulations and the second real data experiment on genedisease prediction, while the Binary Relevance approach (Boutell et al., 2004) using linear kernel SVM (Chang & Lin, 2011) (BR-linear) is included as a baseline for the first real data experiment on multi-label learning.",6. Experiments,[0],[0]
All algorithms are implemented in Matlab on a machine with Intel 8-core Core i7 3.40 GHz with 8GB RAM.,6. Experiments,[0],[0]
"For simplicity, we choose d1 = d2 = d and n1 = n2 = n. Additional experiments regarding the rectangular setting are postponed to the supplemental materials.",6.1. Simulations,[0],[0]
The unknown low-rank matrix M∗ ∈ Rn×n is generated such that M∗,6.1. Simulations,[0],[0]
"= U∗V∗>, and the entries of U∗,V∗ ∈ Rn×r are drawn independently from centered Gaussian distribution with variance 1/n. Let the singular value decomposition of a random matrix F ∈ Rd×d be F = YLΣY>R , where each entry of F is drawn independently from standard normal distribution.",6.1. Simulations,[0],[0]
"The feature matrices XL,XR ∈ Rd×n are then generated as the first n columns of the singular matrices YL and YR respectively.",6.1. Simulations,[0],[0]
"The observed data matrix L follows from the Bernoulli model (3.1) with the full data matrix defined by L∗ = XLM∗X>R.
To begin with, we investigate the sample complexity of the proposed gradient-based method.",6.1. Simulations,[0],[0]
"In particular, we consider the following settings: (i) d = 500, n = 50, r = 10; (ii) d = 500, n = 100, r = 5; (iii) d = 1000, n = 50, r = 5; (iv) d = 1000, n = 100, r = 10.",6.1. Simulations,[0],[0]
"We compute the empirical probability of successful recovery after 50 repeated trials, where we regard the trial as successful if the relative error ‖XLMTX>R − L∗‖F /‖L∗‖F is less than 10−6.",6.1. Simulations,[0],[0]
The experimental results are shown in Figure 1(a).,6.1. Simulations,[0],[0]
"Here, m represents the total number of observed entries.",6.1. Simulations,[0],[0]
"Under all of the aforementioned settings, the phase transition happens to be around m/(nr) = 6, which implies that the optimal sample complexity for gradient-based inductive matrix completion approach may be linear in both n and r.
Moreover, we compare our algorithm with the aforementioned algorithms, including MC, Maxide and AltMin.",6.1. Simulations,[0],[0]
"All the parameters, such as step size and regularization parameters, are tuned by 5-fold cross validation.",6.1. Simulations,[0],[0]
"We measure the performance by the relative reconstruction error ‖L̂ − L∗‖F /‖L∗‖F under the setting that d = 1000, n = 100, r = 10 with sampling rate p varied in the range {2%, 5%, 10%}.",6.1. Simulations,[0],[0]
"For the sake of fairness, we use the same initialization procedure as in Algorithm 1 for all the compared algorithms.",6.1. Simulations,[0],[0]
"The results are demonstrated in Figures 1(b), 1(c) and 1(d).",6.1. Simulations,[0],[0]
"Here, each effective data pass evaluates |Ω| observed entries.",6.1. Simulations,[0],[0]
"It can be seen that inductive methods can recover the unknown low-rank matrix L∗ successfully using less observed entries compared with the standard matrix completion approach, which proves the effectiveness of feature information.",6.1. Simulations,[0],[0]
"In addition, our approach achieves the lowest recovery error with respect to the same number of effective data passes, and outperforms existing inductive matrix completion algorithms by a large margin.",6.1. Simulations,[0],[0]
"In addition, we also plot the relative error with respect to CPU time, and similar trend in results can be observed.",6.1. Simulations,[0],[0]
"Due to space limit, these plots are deferred to the supplementary materials.",6.1. Simulations,[0],[0]
"All these comparison results clearly demonstrate the superiority
of our proposed algorithm in terms of computation and is well aligned with our theory.",6.1. Simulations,[0],[0]
"We also apply our proposed algorithm to multi-label learning on the image classification dataset NUS-WIDEOBJECT obtained from Chua et al. (2009), which is one of the prominent applications of inductive matrix completion.",6.2. Multi-Label Learning,[0],[0]
"Additional experiments on Yahoo datasets (Ueda & Saito, 2003) are deferred to the supplementary materials.",6.2. Multi-Label Learning,[0],[0]
"The NUS-WIDE-OBJECT dataset consists of d1 = 30000 images classified by d2 = 31 object categories, along with 5 types of low-level features extracted from these images.",6.2. Multi-Label Learning,[0],[0]
"We construct the feature matrix by further extracting the top-50 principle components from each type of side information, which leads to n1 = 250 features in total.",6.2. Multi-Label Learning,[0],[0]
Detailed information regarding the dataset can be found in Chua et al. (2009).,6.2. Multi-Label Learning,[0],[0]
"Our goal is to predict the labels associated with the unseen instances, based on both the side information as well as the label assignments of the observed instances.",6.2. Multi-Label Learning,[0],[0]
"By leveraging the low-rankness property of the unknown instance-label matrix (Ji et al., 2008; Goldberg et al., 2010), multi-label learning can be reformulated as an inductive matrix completion problem (3.2), where L∗ is the instancelabel matrix, XL represents the feature matrix and XR is set as an identity matrix in this context.
",6.2. Multi-Label Learning,[0],[0]
"We randomly sample p× 100% instances as the observed (training) data for each dataset, and treat the remaining (1− p)× 100% instances as the unobserved (testing) data,
with p chosen from {10%, 25%, 50%}.",6.2. Multi-Label Learning,[0],[0]
"We estimate the unknown matrix of parameters based on the training data, and report the average precision (AP) (Zhang & Zhou, 2014) computed from the testing data.",6.2. Multi-Label Learning,[0],[0]
"Specifically, the average precision measures the averaged fraction of relevant labels ranked higher than a specific label.",6.2. Multi-Label Learning,[0],[0]
"We compare our algorithm with the baseline approach, BR-linear, and existing inductive matrix completion algorithms, Maxide and AltMin.",6.2. Multi-Label Learning,[0],[0]
"All the parameters, including the rank r (we tune it over the grid {5, 10, . . .",6.2. Multi-Label Learning,[0],[0]
", 30}), are tuned via 5-fold cross validation based on the training data.",6.2. Multi-Label Learning,[0],[0]
Table 2 depicts the detailed experimental results.,6.2. Multi-Label Learning,[0],[0]
"In detail, for each setting of observed training data, we report the averaged AP over 10 trials and the corresponding standard deviation as well as the total run time.",6.2. Multi-Label Learning,[0],[0]
We can observe from Table 2 that the proposed gradient-based algorithm outperforms the BR-linear by a large margin.,6.2. Multi-Label Learning,[0],[0]
"Compared with existing inductive matrix completion algorithms, our algorithm also achieves significantly better results under all of the experimental settings in terms of both prediction accuracy and running time.",6.2. Multi-Label Learning,[0],[0]
This again illustrates the advantage of our algorithm.,6.2. Multi-Label Learning,[0],[0]
"We further apply our proposed method for predicting genedisease associations on the OMIM4 data used in Singh-Blom et al. (2013), which is another successful application of inductive matrix completion.",6.3. Gene-Disease Prediction,[0],[0]
"In the context of gene-disease
4OMIM is short for Online Mendelian Inheritance in Man, which is a public database for human gene-disease studies.
association prediction, we let L∗ ∈ Rd1×d2 be the genedisease association matrix, such that L∗ij = 1 if gene i is associated with disease j; L∗ij = 0",6.3. Gene-Disease Prediction,[0],[0]
if the association is unobserved.,6.3. Gene-Disease Prediction,[0],[0]
"On this dataset, the association matrix L∗ is highly sparse, consisting of d1 = 12331 different genes and d2 = 3209 different diseases with only 3954 discovered gene-disease associations.",6.3. Gene-Disease Prediction,[0],[0]
"In addition, we obtain the gene feature matrix XL ∈ Rd1×n1 and disease feature matrix XR ∈ Rd2×n2 from Natarajan & Dhillon (2014), where n1 = 300 gene features and n2 = 200 disease features are extracted respectively.",6.3. Gene-Disease Prediction,[0],[0]
"Our objective is to predict potential genes for certain diseases of interest based on both the observed associations and feature information, which can thus be formulated as an inductive matrix completion problem.",6.3. Gene-Disease Prediction,[0],[0]
"Following Natarajan & Dhillon (2014), we include an additional regularization term in (4.1) to take into account the sparsity of the underlying association matrix
min U∈Rn1×r V∈Rn2×r
fΩ(U,V) + λ ‖PΩc(XLUV>X>R)‖2F , (6.1)
where r is the supposed rank of L∗, Ω stands for the (training) index set of gene-disease associations, and Ωc represents its complement.",6.3. Gene-Disease Prediction,[0],[0]
Note that all the algorithms we studied here including ours can be directly applied to solve (6.1) with slight modification.,6.3. Gene-Disease Prediction,[0],[0]
"In our experiment, we tune the regularization parameter λ via cross validation and choose the best value λ = 0.5.
To evaluate the performance of our method, we equally split the known gene-disease associations into three groups and perform 3-fold cross validation.",6.3. Gene-Disease Prediction,[0],[0]
"Specifically, we treat each group as testing data once and apply our gradient-based method on the remaining two groups to obtain the estimation matrix of L∗. For every gene-disease pair (g, d) in the testing group, we order all the genes by the corresponding estimated values associated with disease d, and then record the ranking of gene g in the list.",6.3. Gene-Disease Prediction,[0],[0]
"We use the cumulative distribution of the rankings (Singh-Blom et al., 2013; Natarajan
& Dhillon, 2014) as the performance measure for evaluation, i.e., the probability that the ranking is less than a specific threshold k ∈ {1, 2, . . .",6.3. Gene-Disease Prediction,[0],[0]
", 100}.",6.3. Gene-Disease Prediction,[0],[0]
"The experimental results with rank r varied in the range {10, 30, 50, 100, 200} based on our method are displayed in Figure 2(a), which indicates that the rank plays an important role in gene-disease prediction: higher rank leads to better performance.",6.3. Gene-Disease Prediction,[0],[0]
"In later experiments, we choose r = 200 because the performance of inductive matrix completion on this dataset tends to be saturated when r = 200.
",6.3. Gene-Disease Prediction,[0],[0]
"Moreover, we compare our algorithm with the following algorithms: MC, Maxide and AltMin, which are discussed at the beginning of Section 6.",6.3. Gene-Disease Prediction,[0],[0]
The comparison results in terms of the cumulative distribution of the rankings are illustrated in Figure 2(b).,6.3. Gene-Disease Prediction,[0],[0]
It can be seen that our proposed algorithm uniformly outperforms other methods over all threshold values k.,6.3. Gene-Disease Prediction,[0],[0]
"In addition, we present the precision-recall curves for all the methods we compared in Figure 2(c).",6.3. Gene-Disease Prediction,[0],[0]
Here the precision is defined as the ratio of true recovered gene-disease associations to the total number of associations we assessed; and the recall is the fraction of the true gene-disease associations that are recovered.,6.3. Gene-Disease Prediction,[0],[0]
"Again, the proposed method dominates other relevant approaches, which suggests that our method can better serve for biologists to discover new gene-disease associations.",6.3. Gene-Disease Prediction,[0],[0]
In this paper we proposed the first gradient-based nonconvex optimization algorithm for inductive matrix completion with sample complexity linear in the number of features and converges to the unknown low-rank matrix at a linear rate.,7. Conclusions and Future Work,[0],[0]
"One possible future direction is to extend our algorithm to the case with noisy side information (Chiang et al., 2015) or the agnostic setting, i.e., the underlying matrix has high rank (Du et al., 2017b).",7. Conclusions and Future Work,[0],[0]
"Another direction is to generalize our approach to non-linear models (Si et al., 2016).",7. Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers and Yining Wang for their helpful comments.,Acknowledgments,[0],[0]
"XZ and QG are partially supported by the National Science Foundation IIS-1618948, IIS-1652539, IIS-1717206 and BIGDATA IIS-1741342, SD is partially supported by NSF grant IIS-1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.",Acknowledgments,[0],[0]
We revisit the inductive matrix completion problem that aims to recover a rank-r matrix with ambient dimension d given n features as the side prior information.,abstractText,[0],[0]
The goal is to make use of the known n features to reduce sample and computational complexities.,abstractText,[0],[0]
"We present and analyze a new gradient-based non-convex optimization algorithm that converges to the true underlying matrix at a linear rate with sample complexity only linearly depending on n and logarithmically depending on d. To the best of our knowledge, all previous algorithms either have a quadratic dependency on the number of features in sample complexity or a sub-linear computational convergence rate.",abstractText,[0],[0]
"In addition, we provide experiments on both synthetic and real world data to demonstrate the effectiveness of our proposed algorithm.",abstractText,[0],[0]
Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow,title,[0],[0]
"Deep learning methods have had enormous recent success in fields where prediction accuracy is important, e.g., computer vision and speech recognition.",1. Introduction,[0],[0]
"However, for these methods to be useful in fields such as robotics and medical diagnostics, we need to know the uncertainty of our predictions.",1. Introduction,[0],[0]
"For example, physicians might need such uncertainty estimates to choose a safe but effective treatment for their patients.",1. Introduction,[0],[0]
"Lack of such estimates might result in unreliable decisions which can sometime have disastrous consequences.
",1. Introduction,[0],[0]
"One of the goals of Bayesian inference is to provide uncertainty estimates by using the posterior distribution obtained
*Equal contribution 1RIKEN Center for Advanced Intelligence project, Tokyo, Japan 2University of British Columbia, Vancouver, Canada 3University of Oxford, Oxford, UK 4University of Edinburgh, Edinburgh, UK.",1. Introduction,[0],[0]
"Correspondence to: Mohammad Emtiyaz Khan <emtiyaz.khan@riken.jp>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
using Bayes’ rule.",1. Introduction,[0],[0]
"Unfortunately, this is infeasible in large models such as Bayesian neural networks.",1. Introduction,[0],[0]
"Traditional methods such as Markov Chain Monte Carlo (MCMC) methods converge slowly and might require a large memory (Balan et al., 2015).",1. Introduction,[0],[0]
"In contrast, variational inference (VI) methods can scale to large models by using stochastic-gradient (SG) methods, as recent work has shown (Graves, 2011; Blundell et al., 2015; Ranganath et al., 2014; Salimans et al., 2013).",1. Introduction,[0],[0]
"These works employ adaptive learning-rate methods, such as RMSprop (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2015) and AdaGrad (Duchi et al., 2011), for which easy-to-use implementations are available in existing codebases.
",1. Introduction,[0],[0]
"Despite their simplicity, these VI methods require more computation, memory, and implementation effort compared to maximum-likelihood estimation (MLE).",1. Introduction,[0],[0]
"One reason for this is that the number of parameters in VI is usually much larger than in MLE, which increases the memory and computation costs.",1. Introduction,[0],[0]
"Another reason is that existing codebases are designed and optimized for tasks such as MLE, and their application to VI involves significant amount of modifications in the code.",1. Introduction,[0],[0]
"We ask the following question: is it possible to avoid these issues and make VI as easy as MLE?
",1. Introduction,[0],[0]
"In this paper, we propose to use natural-gradient methods to address these issues for Gaussian mean-field VI.",1. Introduction,[0],[0]
"By proposing a natural-momentum method along with a series of approximations, we obtain algorithms that can be implemented with minimal changes to the existing codebases of adaptive learning-rate methods.",1. Introduction,[0],[0]
The main change involves perturbing the network weights during the gradient computation (see Fig. 1).,1. Introduction,[0],[0]
An uncertainty estimate can be cheaply obtained by using the vector that adapts the learning rate.,1. Introduction,[0],[0]
"This requires lower memory, computation, and implementation efforts than existing methods for VI while obtaining uncertainty estimates of comparable quality.",1. Introduction,[0],[0]
"Our experimental results confirm this, and suggest that the estimated uncertainty could improve exploration in problems such as reinforcement learning and stochastic optimization.",1. Introduction,[0],[0]
"Bayesian inference in models such as neural networks has a long history in machine learning (MacKay, 2003; Bishop, 2006).",1.1. Related Work,[0],[0]
"Earlier work proposed a variety of algo-
rithms such as MCMC methods (Neal, 1995), Laplace’s method (Denker & Lecun, 1991), and variational inference (Hinton & Van Camp, 1993; Barber & Bishop, 1998).",1.1. Related Work,[0],[0]
"The mean-field approximation has also been a popular tool from very early on (Saul et al., 1996; Anderson & Peterson, 1987).",1.1. Related Work,[0],[0]
"These previous works lay the foundation of methods now used for Bayesian deep learning (Gal, 2016).
",1.1. Related Work,[0],[0]
"Recent approaches (Graves, 2011; Blundell et al., 2015) enable the application of Gaussian mean-field VI methods to large deep-learning problems.",1.1. Related Work,[0],[0]
They do so by using gradientbased methods.,1.1. Related Work,[0],[0]
"In contrast, we propose to use naturalgradient methods which, as we show, lead to algorithms that are simpler to implement and require lower memory and computations than gradient-based methods.",1.1. Related Work,[0],[0]
"Natural gradients are also better suited for VI because they can improve convergence rates by exploiting the information geometry of posterior approximations (Khan et al., 2016).",1.1. Related Work,[0],[0]
"Some of our algorithms inherit these properties too.
",1.1. Related Work,[0],[0]
"A recent independent work on noisy-Adam by Zhang et al. (2018) is algorithmically very similar to our Vadam method, however their derivation lacks a strong motivation for the use of momentum.",1.1. Related Work,[0],[0]
"In our derivation, we incorporate a naturalmomentum term based on Polyak’s heavy-ball method, which provides a theoretical justification for the use of momentum.",1.1. Related Work,[0],[0]
"In addition, we analyze the approximation error introduced in Vadam and discuss ways to reduce it.
",1.1. Related Work,[0],[0]
"Zhang et al. (2018) also propose an interesting extension by using K-FAC, which could find better approximations than the mean-field method.",1.1. Related Work,[0],[0]
"The goal of this approach is similar to other approaches that employ structured approximations (Ritter et al., 2018; Louizos & Welling, 2016; Sun et al., 2017).",1.1. Related Work,[0],[0]
"Many other works have explored variety of approximation methods, e.g., Gal & Ghahramani (2016) use dropout for VI, Hernandez-Lobato & Adams (2015); Hasenclever et al. (2017) use expectation propagation, Li et al.
(2016); Balan et al. (2015) use stochastic-gradient Langevin dynamics.",1.1. Related Work,[0],[0]
"Such approaches are viable alternatives to the mean-field VI approach we use.
",1.1. Related Work,[0],[0]
"Another related work by Mandt et al. (2017) views SG descent as VI but requires additional effort to obtain posterior approximations, while in our approach the approximation is automatically obtained within an adaptive method.
",1.1. Related Work,[0],[0]
"Our weight-perturbed algorithms are also related to globaloptimization methods, e.g., Gaussian-homotopy continuation methods (Mobahi & Fisher III, 2015), smoothedoptimization method (Leordeanu & Hebert, 2008), graduated optimization method (Hazan et al., 2016), and stochastic search methods (Zhou & Hu, 2014).",1.1. Related Work,[0],[0]
"In particular, our algorithm is related to recent approaches in deep learning for exploration to avoid local minima, e.g., natural evolution strategy (Wierstra et al., 2014), entropy-SGD (Chaudhari et al., 2016), and noisy networks for reinforcement learning (Fortunato et al., 2018; Plappert et al., 2018).",1.1. Related Work,[0],[0]
"An earlier version of our work (Khan et al., 2017) focuses exclusively on this problem, and in this paper we modify it to be implemented within an adaptive algorithm like Adam.",1.1. Related Work,[0],[0]
"We consider modeling of a dataset D = {D1,D2, . . .",2. Gaussian Mean-Field Variational Inference,[0],[0]
",DN} by using a deep neural network (DNN).",2. Gaussian Mean-Field Variational Inference,[0],[0]
"We assume a probabilistic framework where each data example Di is sampled independently from a probability distribution p(Di|θ) parameterized by a DNN with weights θ ∈ RD, e.g., the distribution could be an exponential-family distribution whose mean parameter is the output of a DNN (Bishop, 2006).
",2. Gaussian Mean-Field Variational Inference,[0],[0]
"One of the most popular approaches to estimate θ given D is maximum-likelihood estimation (MLE), where we maximize the log-likelihood: log p(D|θ).",2. Gaussian Mean-Field Variational Inference,[0],[0]
"This optimization problem can be efficiently solved by applying SG methods
such as RMSProp, AdaGrad and Adam.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"For large problems, these methods are extremely popular, partly due to the simplicity and efficiency of their implementations (see Fig. 1 for Adam’s pseudocode).
",2. Gaussian Mean-Field Variational Inference,[0],[0]
One of the goals of Bayesian deep learning is to go beyond MLE and estimate the posterior distribution of θ to obtain an uncertainty estimate of the weights.,2. Gaussian Mean-Field Variational Inference,[0],[0]
"Unfortunately, the computation of the posterior is challenging in deep models.",2. Gaussian Mean-Field Variational Inference,[0],[0]
The posterior is obtained by specifying a prior distribution p(θ) and then using Bayes’ rule: p(θ|D) := p(D|θ)p(θ)/p(D).,2. Gaussian Mean-Field Variational Inference,[0],[0]
This requires computation of the normalization constant p(D) = ∫,2. Gaussian Mean-Field Variational Inference,[0],[0]
p(D|θ)p(θ)dθ which is a very difficult task for DNNs.,2. Gaussian Mean-Field Variational Inference,[0],[0]
One source of the difficulty is the size of θ and D which are usually very large in deep learning.,2. Gaussian Mean-Field Variational Inference,[0],[0]
"Another source is the nonconjugacy of the likelihood p(Di|θ) and the prior p(θ), i.e., the two distributions do not take the same form with respect to θ (Bishop, 2006).",2. Gaussian Mean-Field Variational Inference,[0],[0]
"As a result, the product p(D|θ)p(θ) does not take a form with which p(D) can be easily computed.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"Due to these issues, Bayesian inference in deep learning is computationally challenging.
",2. Gaussian Mean-Field Variational Inference,[0],[0]
Variational inference (VI) simplifies the problem by approximating p(θ|D) with a distribution q(θ) whose normalizing constant is relatively easier to compute.,2. Gaussian Mean-Field Variational Inference,[0],[0]
"Following previous work (Ranganath et al., 2014; Blundell et al., 2015; Graves, 2011), we choose both p(θ) and q(θ) to be Gaussian distributions with diagonal covariances:
p(θ) := N (θ|0, I/λ), q(θ) :",2. Gaussian Mean-Field Variational Inference,[0],[0]
"= N (θ|µ, diag(σ2)), (1)
where λ ∈ R is a known precision parameter with λ > 0, and µ,σ ∈ RD are mean and standard deviation of q.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"The distribution q(θ) is known as the Gaussian mean-field variational distribution and its parameters µ and σ2 can be obtained by maximizing the following variational objective:
L(µ,σ2)",2. Gaussian Mean-Field Variational Inference,[0],[0]
:= N∑ i=1,2. Gaussian Mean-Field Variational Inference,[0],[0]
Eq,2. Gaussian Mean-Field Variational Inference,[0],[0]
[log p(Di|θ)],2. Gaussian Mean-Field Variational Inference,[0],[0]
+,2. Gaussian Mean-Field Variational Inference,[0],[0]
Eq [ log p(θ) q(θ) ] .,2. Gaussian Mean-Field Variational Inference,[0],[0]
"(2)
A straightforward approach used in the previous work (Ranganath et al., 2014; Blundell et al., 2015; Graves, 2011) is to maximize L by using an SG method, e.g., we can use the following update:
µt+1 = µt + ρt∇̂µLt, σt+1 = σt + δt∇̂σLt, (3)
where t is the iteration number, ∇̂xLt denotes an unbiased SG estimate of L at µt,σ2t with respect to x, and ρt, δt > 0 are learning rates which can be adapted using methods such as RMSprop or AdaGrad.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"These approaches make use of existing codebases for adaptive learning-rate methods to perform VI, which can handle many network architectures and can scale well to large datasets.
",2. Gaussian Mean-Field Variational Inference,[0],[0]
"Despite this, a direct application of adaptive learning-rate methods for VI may result in algorithms that use more computation and memory than necessary, and also require more implementation effort.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"Compared to MLE, the memory and computation costs increase because the number of parameters to be optimized is doubled and we now have two vectors µ and σ to estimate.",2. Gaussian Mean-Field Variational Inference,[0],[0]
Using adaptive methods increases this cost further as these methods require storing the scaling vectors that adapt the learning rate for both µ and σ.,2. Gaussian Mean-Field Variational Inference,[0],[0]
"In addition, using existing codebases require several modifications as they are designed and optimized for MLE.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"For example, we need to make changes in the computation graph where the objective function is changed to the variational objective and network weights are replaced by random variables.",2. Gaussian Mean-Field Variational Inference,[0],[0]
"Together, these small issues make VI more difficult to implement and execute than MLE.
",2. Gaussian Mean-Field Variational Inference,[0],[0]
The algorithms developed in this paper solve some of these issues and can be implemented within Adam with minimal changes to the code.,2. Gaussian Mean-Field Variational Inference,[0],[0]
We derive our algorithm by approximating a natural-gradient method and then using a naturalmomentum method.,2. Gaussian Mean-Field Variational Inference,[0],[0]
We now describe our method in detail.,2. Gaussian Mean-Field Variational Inference,[0],[0]
"In this section, we introduce a natural-gradient method to perform VI and then propose several approximations that enable implementation within Adam.
",3. Approximate Natural-Gradient VI,[0],[0]
Natural-gradient VI methods exploit the Riemannian geometry of q(θ) by scaling the gradient with the inverse of its Fisher information matrix (FIM).,3. Approximate Natural-Gradient VI,[0],[0]
"We build upon the naturalgradient method of Khan & Lin (2017), which simplifies the update by avoiding a direct computation of the FIM.",3. Approximate Natural-Gradient VI,[0],[0]
The main idea is to use the expectation parameters of the exponential-family distribution to compute natural gradients in the natural-parameter space.,3. Approximate Natural-Gradient VI,[0],[0]
"We provide a brief description of their method in Appendix B.
For Gaussian mean-field VI, the method of Khan & Lin (2017) gives the following update:
NGVI : µt+1 = µt + βt σ 2 t+1 ◦",3. Approximate Natural-Gradient VI,[0],[0]
"[ ∇̂µLt ] , (4)
σ−2t+1 = σ −2 t − 2βt",3. Approximate Natural-Gradient VI,[0],[0]
"[ ∇̂σ2Lt ] , (5)
where βt > 0 is a scalar learning rate and a ◦ b denotes the element-wise product between vectors a",3. Approximate Natural-Gradient VI,[0],[0]
and b.,3. Approximate Natural-Gradient VI,[0],[0]
We refer to this update as natural-gradient variational inference (NGVI).,3. Approximate Natural-Gradient VI,[0],[0]
"A detailed derivation is given in Appendix C.
The NGVI update differs from (3) in one major aspect: the learning rate βt in (4) is adapted by the variance σ2t+1.",3. Approximate Natural-Gradient VI,[0],[0]
"This plays a crucial role in reducing the NGVI update to an Adam-like update, as we show the next section.",3. Approximate Natural-Gradient VI,[0],[0]
The update requires a constraint σ2 > 0,3. Approximate Natural-Gradient VI,[0],[0]
"but, as we show in Section 3.2, we can eliminate this constraint using an approximation.",3. Approximate Natural-Gradient VI,[0],[0]
"We start by expressing the NGVI update in terms of the MLE objective, so that we can directly compute gradients on the MLE objective using backpropagation.",3.1. Variational Online-Newton (VON),[0],[0]
"We start by defining the MLE objective (denoted by f ) and minibatch stochastic-gradient estimates (denoted by ĝ):
f(θ) := 1
N N∑ i=1 fi(θ), ĝ(θ) := 1 M ∑ i∈M ∇θfi(θ), (6)
where fi(θ) := − log p(Di|θ) is the negative log-likelihood of i’th data example, and the minibatch M contains M examples chosen uniformly at random.",3.1. Variational Online-Newton (VON),[0],[0]
"Similarly, we can obtain a minibatch stochastic-approximation of the Hessian which we denote by ∇̂2θθf(θ).
",3.1. Variational Online-Newton (VON),[0],[0]
"As we show in Appendix D, the NGVI update can be written in terms of the stochastic gradients and Hessian of f :
VON : µt+1 = µt − βt (ĝ(θt) +",3.1. Variational Online-Newton (VON),[0],[0]
"λ̃µt)/(st+1 + λ̃), (7)
st+1 =",3.1. Variational Online-Newton (VON),[0],[0]
"(1− βt)st + βt diag[∇̂2θθf(θt)], (8)
where a/b is an element-wise division operation between vectors a and b, and we have approximated the expectation with respect to q using one Monte-Carlo (MC) sample θt ∼ N (θ|µt,σ2t ) with σ2t := 1/[N(st + λ̃)] and λ̃ := λ/N .",3.1. Variational Online-Newton (VON),[0],[0]
The update can be easily modified when multiple samples are used.,3.1. Variational Online-Newton (VON),[0],[0]
This update can leverage backpropagation to perform the gradient and Hessian computation.,3.1. Variational Online-Newton (VON),[0],[0]
"Since the scaling vector st contains an online estimate of the diagonal of the Hessian, we call this the “variational online-Newton” (VON) method.",3.1. Variational Online-Newton (VON),[0],[0]
"VON is expected to perform as well as NGVI, but does not require the gradients of the variational objective.
",3.1. Variational Online-Newton (VON),[0],[0]
The Hessian can be computed by using methods such as automatic-differentiation or the reparameterization trick.,3.1. Variational Online-Newton (VON),[0],[0]
"However, since f is a non-convex function, the Hessian can be negative which might make σ2 negative, in which case the method will break down.",3.1. Variational Online-Newton (VON),[0],[0]
"One could use a constrained optimization method to solve this issue, but this might be difficult to implement and execute (we discuss this briefly in Appendix D.1).",3.1. Variational Online-Newton (VON),[0],[0]
"In the next section, we propose a simple fix to this problem by using an approximation.",3.1. Variational Online-Newton (VON),[0],[0]
"To avoid negative variances in the VON update, we propose to use the Generalized Gauss-Newton (GGN) approximation (Schraudolph, 2002; Martens, 2014; Graves, 2011):
∇2θjθjf(θ)",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"≈ 1
M ∑ i∈M",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"[ ∇θjfi(θ) ]2 := ĥj(θ), (9)
where θj is the j’th element of θ.",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"This approximation will always be nonnegative, therefore if the initial σ2 at t = 1 is
positive, it will remain positive in the subsequent iterations.",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"Using this approximation to update st in (8) and denoting the vector of ĥj(θ) by ĥ(θ), we get,
VOGN : st+1 = (1− βt)st + βt ĥ(θt).",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"(10)
Using this update in VON, we get the “variational online Gauss-Newton” (VOGN) algorithm.
",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
The GGN approximation is proposed by Graves (2011) for mean-field Gaussian VI to derive a fast gradient-based method (see Eq. (17) in his paper1).,3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"This approximation is very useful for our natural-gradient method since it eliminates the constraint on σ2, giving VOGN an algorithmic advantage over VON.
",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
How good is this approximation?,3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"For an MLE problem, the approximation error of the GGN in (9) decreases as the model-fit improves during training (Martens, 2014).",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"For VI, we expect the same however, since θ are sampled from q, the expectation of the error is unlikely to be zero.",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"Therefore, the solutions found by VOGN will typically differ from those found by VON, but their performances are expected to be similar.
",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
An issue with VOGN is that its implementation is not easy within existing deep-learning codebases.,3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"This is because these codebases are optimized to directly compute the sum of the gradients over minibatches, and do not support computation of individual gradients as required in (9).",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"A solution for such computations is discussed by Goodfellow (2015), but this requires additional implementation effort.",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"Instead, we address this issue by using another approximation in the next section.",3.2. Variational Online Gauss-Newton (VOGN),[0],[0]
"To simplify the implementation of VOGN, we propose to approximate the Hessian by the gradient magnitude (GM) (Bottou et al., 2016):
∇2θjθjf(θ)",3.3. Variational RMSprop (Vprop),[0],[0]
"≈
[ 1
M ∑ i∈M ∇θjfi(θ)
]2 = [ĝj(θ)] 2 . (11)
",3.3. Variational RMSprop (Vprop),[0],[0]
"Compared to the GGN which computes the sum of squaredgradients, this approximation instead computes the square of the sum.",3.3. Variational RMSprop (Vprop),[0],[0]
"This approximation is also used in RMSprop which uses the following update given weights θt:
RMSprop : θt+1 = θt − αt ĝ(θt)/( √
s̄t+1 + δ), (12) s̄t+1",3.3. Variational RMSprop (Vprop),[0],[0]
= (1− βt)s̄t + βt,3.3. Variational RMSprop (Vprop),[0],[0]
"[ĝ(θt) ◦ ĝ(θt)] , (13)
where s̄t is the vector that adapts the learning rate and δ is a small positive scalar added to avoid dividing by zero.
1There is a discrepancy between Eq. (17) and (12) in Graves (2011), however the text below Eq.",3.3. Variational RMSprop (Vprop),[0],[0]
"(12) mentions relationship to FIM, from which it is clear that the GGN approximation is used.
",3.3. Variational RMSprop (Vprop),[0],[0]
"The update of s̄t uses the GM approximation to the Hessian (Bottou et al., 2016).",3.3. Variational RMSprop (Vprop),[0],[0]
"Adam and AdaGrad also use this approximation.
",3.3. Variational RMSprop (Vprop),[0],[0]
"Using the GM approximation and an additional modification in the VON update, we can make the VON update very similar to RMSprop.",3.3. Variational RMSprop (Vprop),[0],[0]
Our modification involves taking the square-root over st+1 in (7) and then using the GM approximation for the Hessian.,3.3. Variational RMSprop (Vprop),[0],[0]
"We also use different learning rates αt and βt to update µ and s, respectively.",3.3. Variational RMSprop (Vprop),[0],[0]
"The resulting update is very similar to the RMSprop update:
Vprop: µt+1 = µt",3.3. Variational RMSprop (Vprop),[0],[0]
"− αt (ĝ(θt)+λ̃µt)/( √ st+1 + λ̃),
st+1 = (1− βt)st + βt",3.3. Variational RMSprop (Vprop),[0],[0]
"[ĝ(θt) ◦ ĝ(θt)] , (14)
where θt ∼ N (θ|µt,σ2t ) with σ2t := 1/[N(st + λ̃)].",3.3. Variational RMSprop (Vprop),[0],[0]
"We call this update “Variational RMSprop” or simply “Vprop”.
",3.3. Variational RMSprop (Vprop),[0],[0]
The Vprop update resembles RMSprop but with three differences (highlighted in red).,3.3. Variational RMSprop (Vprop),[0],[0]
"First, the gradient in Vprop is evaluated at the weights θt sampled from N (θ|µt,σ2t ).",3.3. Variational RMSprop (Vprop),[0],[0]
This is a weight-perturbation where the variance σ2t of the perturbation is obtained from the vector st that adapts the learning rate.,3.3. Variational RMSprop (Vprop),[0],[0]
The variance is also the uncertainty estimates.,3.3. Variational RMSprop (Vprop),[0],[0]
"Therefore, VI can be performed simply by using an RMSprop update with a few simple changes.",3.3. Variational RMSprop (Vprop),[0],[0]
The second difference between Vprop and RMSprop is that Vprop has an extra term λ̃µt in the update of µt which is due to the Gaussian prior.,3.3. Variational RMSprop (Vprop),[0],[0]
"Finally, the third difference is that the constant δ in RMSprop is replaced by λ̃.",3.3. Variational RMSprop (Vprop),[0],[0]
It is clear that the GM approximation might not be the best approximation of the Hessian.,3.4. Analysis of the GM approximation,[0],[0]
"Taking square of a sum leads to a sum with M2 terms which, depending on the correlations between the individual gradients, would either shrink or expand the estimate.",3.4. Analysis of the GM approximation,[0],[0]
The following theorem formalizes this intuition.,3.4. Analysis of the GM approximation,[0],[0]
"It states that, given a minibatch of size M , the expectation of the GM approximation is somewhere between the GGN and square of the full-batch gradient.
",3.4. Analysis of the GM approximation,[0],[0]
Theorem 1.,3.4. Analysis of the GM approximation,[0],[0]
Denote the full-batch gradient with respect to θj by gj(θ) and the corresponding full-batch GGN approximation by hj(θ).,3.4. Analysis of the GM approximation,[0],[0]
"Suppose minibatchesM are sampled from the uniform distribution p(M) over all ( N M ) minibatches, and denote a minibatch gradient by ĝj(θ;M), then the expected value of the GM approximation is the following,
Ep(M) [ ĝj(θ;M)2 ] = whj(θ) + (1− w)[gj(θ)]2, (15)
where w = 1M (N −M)/(N",3.4. Analysis of the GM approximation,[0],[0]
"− 1).
",3.4. Analysis of the GM approximation,[0],[0]
A proof is given in Appendix G. This result clearly shows the bias introduced in the GM approximation and also that the bias increases with the minibatch size.,3.4. Analysis of the GM approximation,[0],[0]
"For a minibatch
of size M = 1, we have w = 1 and the GM is an unbiased estimator of the GGN, but when M = N it is purely the magnitude of the gradient and does not contain any secondorder information.
",3.4. Analysis of the GM approximation,[0],[0]
"Therefore, if our focus is to obtain uncertainty estimates with good accuracy, VOGN with M = 1 might be a good choice since it is as easy as Vprop to implement.",3.4. Analysis of the GM approximation,[0],[0]
"However, this might require a small learning-rate and converge slowly.",3.4. Analysis of the GM approximation,[0],[0]
"Vprop with M > 1 will converge fast and is much easier to implement than VOGN with M > 1, but might result in slightly worse estimates.",3.4. Analysis of the GM approximation,[0],[0]
Using Vprop with M = 1 may not be as good because of the square-root2 over st.,3.4. Analysis of the GM approximation,[0],[0]
"We now propose a natural-momentum method which will enable an Adam-like update.
",4. Variational Adam (Vadam),[0],[0]
"Momentum methods generally take the following form that uses Polyak’s heavy-ball method:
θt+1 = θt + ᾱt∇θf1(θt) +",4. Variational Adam (Vadam),[0],[0]
γ̄t(θt,4. Variational Adam (Vadam),[0],[0]
"− θt−1), (16)
where f1 is the function we want to maximize and the last term is the momentum term.",4. Variational Adam (Vadam),[0],[0]
We propose a naturalmomentum version of this algorithm which employs naturalgradients instead of the gradients.,4. Variational Adam (Vadam),[0],[0]
We assume q to be an exponential-family distribution with natural-parameter η.,4. Variational Adam (Vadam),[0],[0]
"We propose the following natural-momentum method in the natural-parameter space:
ηt+1 = ηt + ᾱt∇̃ηLt + γ̄t(ηt",4. Variational Adam (Vadam),[0],[0]
"− ηt−1), (17)
where ∇̃ denotes the natural-gradients in the naturalparameter space, i.e., the gradient scaled by the Fisher information matrix of q(θ).
",4. Variational Adam (Vadam),[0],[0]
"We show in Appendix E that, for Gaussian q(θ), we can express the above update as a VON update with momentum3:
µt+1 = µt",4. Variational Adam (Vadam),[0],[0]
"− ᾱt [
1
st+1 + λ̃
] ◦ ( ∇θf(θt) + λ̃µt )",4. Variational Adam (Vadam),[0],[0]
"+ γ̄t [ st + λ̃
st+1 + λ̃
] ◦",4. Variational Adam (Vadam),[0],[0]
"(µt − µt−1), (18)
st+1 = (1− ᾱt) st + ᾱt ∇2θθf(θt), (19)
where θt ∼ N (θ|µt,σ2t ) with σ2t = 1/[N(st + λ̃)].",4. Variational Adam (Vadam),[0],[0]
"This update is similar to (17), but here the learning rates are adapted.",4. Variational Adam (Vadam),[0],[0]
An attractive feature of this update is that it is very similar to Adam.,4. Variational Adam (Vadam),[0],[0]
"Specifically the Adam update shown in
2Note that the square-root does not affect a fixed point (see Appendix H) but it might still affect the steps taken by the algorithm.
",4. Variational Adam (Vadam),[0],[0]
3This is not an exact update for (17).,4. Variational Adam (Vadam),[0],[0]
"We make one approximation in Appendix E to derive it.
",4. Variational Adam (Vadam),[0],[0]
"Fig. 1 can be expressed as the following adaptive version of (16) as shown in Wilson et al. (2017)4,
θt+1 = θt − α̃t
[ 1√
ŝt+1 + δ
] ◦ ∇θf(θt)
+ γ̃t
[ √ ŝt + δ√
ŝt+1 + δ
] ◦ (θt − θt−1), (20)
",4. Variational Adam (Vadam),[0],[0]
ŝt+1,4. Variational Adam (Vadam),[0],[0]
= γ2ŝt + (1− γ2),4. Variational Adam (Vadam),[0],[0]
"[ĝ(θt)]2, (21)
where α̃t, γ̃t are appropriately defined in terms of the Adam’s learning rate α and γ1: α̃t := α (1− γ1) / (1− γt1) and γ̃t := γ1 ( 1− γt−11 ) (1− γt1).
",4. Variational Adam (Vadam),[0],[0]
"Using a similar procedure as the derivation of Vprop, we can express the update as an Adam-like update, which we call “variational Adam” or simply “Vadam”.",4. Variational Adam (Vadam),[0],[0]
"A pseudocode is given in Fig. 1, where we use learning rates of the Adam update insteof of choosing them accoring to ᾱt and γ̄t.",4. Variational Adam (Vadam),[0],[0]
A derivation is given in Appendix E.4.,4. Variational Adam (Vadam),[0],[0]
"Vprop and Vadam perform variational inference, but they can be modified to perform optimization instead of inference.",5. Variational AdaGrad (VadaGrad),[0],[0]
"We now derive such an algorithm which turns out to be a variational version of AdaGrad.
",5. Variational AdaGrad (VadaGrad),[0],[0]
We follow Staines & Barber (2013) who consider minimization of black-box functions F (θ) via the variational optimization5 (VO) framework.,5. Variational AdaGrad (VadaGrad),[0],[0]
"In this framework, instead of directly minimizing F (θ), we minimize its expectation Eq [F (θ)] under a distribution q(θ) := N (θ|µ,σ2) with respect to µ and σ2.",5. Variational AdaGrad (VadaGrad),[0],[0]
The main idea behind VO is that the expectation can be used as a surrogate to the original optimization problem since minθ F (θ) ≤,5. Variational AdaGrad (VadaGrad),[0],[0]
Eq [F (θ)].,5. Variational AdaGrad (VadaGrad),[0],[0]
"The equality is attained when σ2 → 0, i.e., all mass of N (θ|µ,σ2) is at the mode.",5. Variational AdaGrad (VadaGrad),[0],[0]
The main advantage of VO is that Eq [F (θ)] is differentiable even when F itself is non-differentiable.,5. Variational AdaGrad (VadaGrad),[0],[0]
"This way we can use SG optimizers to solve such problems.
",5. Variational AdaGrad (VadaGrad),[0],[0]
"Similarly to Vprop, we can derive an algorithm for VO by noting that VO can be seen as a special case of the VI problem (2) where the KL term is absent and F (θ) is the negative log-likelihood.",5. Variational AdaGrad (VadaGrad),[0],[0]
"With this in mind, we define the following variational objective with an additional parameter τ ∈",5. Variational AdaGrad (VadaGrad),[0],[0]
"[0, 1]:
LF (µ,σ2) := −Eq [F (θ)] + τEq [ log p(θ)
q(θ)
] .",5. Variational AdaGrad (VadaGrad),[0],[0]
"(22)
",5. Variational AdaGrad (VadaGrad),[0],[0]
The parameter τ allows us to interpolate between inference and optimization.,5. Variational AdaGrad (VadaGrad),[0],[0]
"When τ = 1, the objective corresponds to
4Wilson et al. (2017) do not use the constant δ, but in Adam a small constant is added for numerical stability.
",5. Variational AdaGrad (VadaGrad),[0],[0]
"5The exact conditions on F under which VO can be applied are also discussed by Staines & Barber (2013).
",5. Variational AdaGrad (VadaGrad),[0],[0]
"VI with a negative log-likelihood F (θ), and when τ = 0, it corresponds to VO.",5. Variational AdaGrad (VadaGrad),[0],[0]
"Similar objectives have been proposed in existing works (Blundell et al., 2015; Higgins et al., 2016) where τ is used to improve convergence.
",5. Variational AdaGrad (VadaGrad),[0],[0]
"For twice-differentiable F , we can follow a similar derivation as Section 3, and obtain the following algorithm,
µt+1 = µt",5. Variational AdaGrad (VadaGrad),[0],[0]
"− αt (∇̂θF (θ) + τλµt)/(st+1 + τλ), (23)
st+1",5. Variational AdaGrad (VadaGrad),[0],[0]
= (1− τβt)st + βt,5. Variational AdaGrad (VadaGrad),[0],[0]
"∇̂2θθF (θ), (24)
where θt ∼ N (θ|µt,σ2t ) with σ2t := 1/(st + τλ).",5. Variational AdaGrad (VadaGrad),[0],[0]
"This algorithm is identical to the VON algorithm when τ = 1, but when τ = 0, we perform VO with an algorithm which is a diagonal version of the Variational Adaptive-Newton (VAN) algorithm proposed in Khan et al. (2017).",5. Variational AdaGrad (VadaGrad),[0],[0]
"By setting the value of τ between 0 and 1, we can interpolate between VO and VI.",5. Variational AdaGrad (VadaGrad),[0],[0]
"When the function is not differentiable, we can still compute the derivative of Eq[F (θ)] by using methods such as REINFORCE (Williams, 1992).
",5. Variational AdaGrad (VadaGrad),[0],[0]
"When Hessian is difficult to compute, we can employ a GM approximation and take the square-root as we did in Vprop.",5. Variational AdaGrad (VadaGrad),[0],[0]
"For τ = 0, the updates turn out to be similar to AdaGrad, which we call “variational AdaGrad” or simply “VadaGrad”.",5. Variational AdaGrad (VadaGrad),[0],[0]
"The exact updates are given in Appendix F. Unlike Vprop and Vadam, the scaling vector st in VadaGrad is a weighted sum of the past gradient-magnitudes.",5. Variational AdaGrad (VadaGrad),[0],[0]
"Therefore, the entries in st never decrease, and the variance estimate of VadaGrad never expands.",5. Variational AdaGrad (VadaGrad),[0],[0]
This implies that it is highly likely that q(θ) will converge to a Dirac delta and therefore arrive at a minimum of F .,5. Variational AdaGrad (VadaGrad),[0],[0]
"In this section, our goal is to show that the quality of the uncertainty approximations obtained using our algorithms are comparable to existing methods, and computation of uncertainty is scalable.",6. Results,[0],[0]
"We present results on Bayesian logistic regression for classification, Bayesian neural networks for regression, and deep reinforcement learning.",6. Results,[0],[0]
An additional result illustrating avoidance of local-minima using Vadam is in Appendix L. Another result showing benefits of weight-perturbation in Vadam is in Appendix M. The code to reproduce our results is available at https://github.com/emtiyaz/vadam.,6. Results,[0],[0]
"In this experiment, we compare the posterior approximations found with our algorithms to the optimal variational approximation that minimizes the variational objective.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"For Bayesian logistic regression we can compute the optimal mean-field Gaussian approximations using the method described in Marlin et al. (2011) (refer to as ‘MF-Exact’), and compare it to the following methods: VOGN with
minibatch size M = 1 and a momentum term (referred to as ‘VOGN-1’), and Vadam with M ≥ 1 (referred to as ‘Vadam’).",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"Since our goal is to compare the accuracy of posterior approximations and not the speed of convergence, we run both the methods for many iterations with a small learning rate to make sure that they converge.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"We use three datasets: a toy dataset (N = 60, D = 2), USPS-3vs5 (N = 1781, D = 256) and Breast-Cancer (N = 683, D = 10).",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"Details are in Appendix I.
Fig. 2(a) visualizes the approximations on a twodimensional toy example from Murphy (2012).",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
The true posterior distribution is shown with the contour in the background.,6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"Both, Vadam and VOGN-1 find approximations that are different from MF-Exact, which is clearly due to differences in the type of Hessian approximations they use.
",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"For real datasets, we compare performances using three metrics.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"First, the negative of the variational objective on the training data (the evidence lower-bound or ELBO), log-loss on the test data, and the symmetric KL distance between MF-Exact and the approximation found by a method.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
Fig. 2(b) shows the results averaged over 20 random splits of the USPS-3vs5 dataset.,6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"ELBO and log-loss are comparable for all methods, although Vadam does slightly worse on ELBO and VOGN-1 has slightly higher variance for log-loss.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"However, performance on the KL distance clearly shows the difference in the quality of posterior approximations.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
VOGN-1 performs quite well since it uses an unbiased approximation of the GNN.,6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"Vadam does worse due to the bias introduced in the GM approximation with minibatch M > 1, as indicated by Theorem 1.
",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"Fig. 2(c) further shows the effect of M where, for each M , we plot results for 20 random initializations on one split of the Breast-Cancer dataset.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"As we decrease M , Vadam’s performance gets better, as expected.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"For M = 1, it closely
matches VOGN-1.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
"The results are still different because Vadam does not reduce to VOGN-1, even when M = 1 due to the use of the square-root over st.",6.1. Uncertainty Estimation in Logistic Regression,[0],[0]
We show results on the standard UCI benchmark.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
We repeat the experimental setup used in Gal & Ghahramani (2016).,6.2. Uncertainty Estimation in Neural Network,[0],[0]
"Following their work, we use a neural network with one hidden layer, 50 hidden units, and ReLU activation functions.",6.2. Uncertainty Estimation in Neural Network,[0],[0]
We use the 20 splits of the data provided by Gal & Ghahramani (2016) for training and testing.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
We use Bayesian optimization to select the prior precision λ and noise precision of the Gaussian likelihood.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
"Further details of the experiments are given in Appendix J.
We compare Vadam to MC-Dropout (Gal & Ghahramani, 2016) using the results reported in Gal & Ghahramani (2016).",6.2. Uncertainty Estimation in Neural Network,[0],[0]
We also compare to an SG method using the reparameterization trick and the Adam optimizer (referred to as ‘BBVI’).,6.2. Uncertainty Estimation in Neural Network,[0],[0]
"For a fair comparison, the Adam optimizer is run with the same learning rates as Vadam, although these can be tuned further to get better performance.
",6.2. Uncertainty Estimation in Neural Network,[0],[0]
Table 1 shows the performance in terms of the test RMSE and the test log-likelihood.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
The better method out of BBVI and Vadam is shown in boldface found using a paired t-test with p-value > 0.01.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
"Both methods perform comparably, which supports our conclusion, however, MC-Dropout outperforms both the methods.",6.2. Uncertainty Estimation in Neural Network,[0],[0]
We also find that VOGN shows similar results to Vadam and BBVI (we omit the results due to lack of space).,6.2. Uncertainty Estimation in Neural Network,[0],[0]
"The convergence plots for the final runs is given in Appendix J.
For many tasks, we find that VOGN and Vadam converge much faster than BBVI.",6.2. Uncertainty Estimation in Neural Network,[0],[0]
"An example is shown in Figure 3 (see the first 3 figures in the left; details are in Appendix J).
",6.2. Uncertainty Estimation in Neural Network,[0],[0]
We have observed similar trends on other datasets.,6.2. Uncertainty Estimation in Neural Network,[0],[0]
A good exploration strategy is crucial in reinforcement learning (RL) since the data is sequentially collected.,6.3. Exploration in Deep Reinforcement Learning,[0],[0]
We show that weight-perturbation in Vadam improves exploration in RL.,6.3. Exploration in Deep Reinforcement Learning,[0],[0]
"Due to space constraints, we only provide a brief summary of our results, and give details in Appendix K.
We consider the deep deterministic policy gradient (DDPG) method for the Half-Cheetah task using a two-layer neural networks with 400 and 300 ReLU hidden units (Lillicrap et al., 2015).",6.3. Exploration in Deep Reinforcement Learning,[0],[0]
"We compare Vadam and VadaGrad to two SGD methods, one of which does exploration (referred to as ‘SGD-Explore’), and the other does not (referred to as ‘SGD-plain’).",6.3. Exploration in Deep Reinforcement Learning,[0],[0]
The rightmost plot in Figure 3 shows the cumulative rewards (higher is better) of each method against training iterations.,6.3. Exploration in Deep Reinforcement Learning,[0],[0]
VadaGrad and Vadam clearly learn faster than both SGD-Plain and SGD-Explore.,6.3. Exploration in Deep Reinforcement Learning,[0],[0]
We also compare the performances against Adam variants of SGD-Plain and SGD-Explore.,6.3. Exploration in Deep Reinforcement Learning,[0],[0]
"Their results, given in the Appendix K, show that Vadam and VadaGrad still learn faster, but only in the beginning and Adam based methods can catch up quickly.
",6.3. Exploration in Deep Reinforcement Learning,[0],[0]
"This suggests that the exploration strategy has a high impact on the early learning performance in the Half-Cheetah task, and the effect of good exploration decreases over time as the agent collect more informative training samples.",6.3. Exploration in Deep Reinforcement Learning,[0],[0]
"In this paper, we present new VI algorithms which are as simple to implement and execute as algorithms for MLE.",7. Discussion,[0],[0]
We obtain them by using a series of approximations and a natural momentum method for a natural-gradient VI method.,7. Discussion,[0],[0]
The resulting algorithms can be implemented within Adam with minimal changes.,7. Discussion,[0],[0]
"Our empirical findings confirm that our proposed algorithms obtain comparable uncertainty estimates to existing VI methods, but require less computational and implementation effort6.
",7. Discussion,[0],[0]
"An interesting direction we hope to pursue in the future is to generalize our natural-gradient approach to other types of approximation, e.g., exponetial-family distributions and their mixtures.",7. Discussion,[0],[0]
"We would also like to further explore the application to areas such as RL and stochastic optimization.
",7. Discussion,[0],[0]
6We made many new changes in this camera-ready version of the paper.,7. Discussion,[0],[0]
A list of the changes is given in Appendix A.,7. Discussion,[0],[0]
We thank the anonymous reviewers for their feedback.,Acknowledgements,[0],[0]
"We greatly appreciate many insightful discussions with Aaron Mishkin (UBC) and Frederik Kunstner (EPFL), and also thank them for their help on carrying out experiments and reviewing the manuscript.",Acknowledgements,[0],[0]
We would also like to thank Roger Grosse and David Duvenaud from the University of Toronto for useful discussions.,Acknowledgements,[0],[0]
"We would like to thank Zuozhu Liu (SUTD, Singapore) for his help with the experiment on deep RL and logistic regression.",Acknowledgements,[0],[0]
"Finally, we are thankful for the RAIDEN computing system at the RIKEN Center for AI Project, which we extensively used for our experiments.",Acknowledgements,[0],[0]
Uncertainty computation in deep learning is essential to design robust and reliable systems.,abstractText,[0],[0]
"Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximumlikelihood methods.",abstractText,[0],[0]
"In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI.",abstractText,[0],[0]
"Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate.",abstractText,[0],[0]
"This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality.",abstractText,[0],[0]
Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.,abstractText,[0],[0]
Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 145–152 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Voice powered artificial agents have become widespread among consumer devices, with agents like Amazon Alexa, Google Now and Apple Siri being popular and widely used.",1 Introduction,[0],[0]
"Their success relies not only on accurately recognizing user requests, but also on continuously expanding the range of requests that they can understand.",1 Introduction,[0],[0]
"An ever growing set of functionalities is critical for creating an agent that is engaging, useful and human-like.
",1 Introduction,[0],[0]
This presents significant scalability challenges regarding rapidly developing the models at the heart of the natural language understanding (NLU) engines of such agents.,1 Introduction,[0],[0]
"Building accurate models for new functionality typically requires collection and manual annotation of new data resources, an expensive and lengthy process, often requiring highly skilled teams.",1 Introduction,[0],[0]
"In addition, data collected from real user interactions is very valuable for developing accurate models but without an accurate model already in place, the agent will not
enjoy widespread use thereby hindering collection of high quality data.
",1 Introduction,[0],[0]
"Presented with this challenge, our goal is to speed up the natural language expansion process for Amazon Alexa, a popular commercial artificial agent, through methods that maximize re-usability of resources across areas of functionality.",1 Introduction,[0],[0]
"Each area of Alexa’s functionality, e.g., Music, Calendar, is called a domain.",1 Introduction,[0],[0]
"Our focus is to a) increase accuracy of low resource domains b) rapidly build new domains such that the functionality can be made available to Alexa’s users as soon as possible, and thus start benefiting from user interaction data.",1 Introduction,[0],[0]
"To achieve this, we adapt recent ideas at the intersection of deep learning and transfer learning that enable us to leverage available user interaction data from other areas of functionality.
",1 Introduction,[0],[0]
"To summarize our contributions, we describe data efficient deep learning architectures for NLU that facilitate knowledge transfer from similar tasks.",1 Introduction,[0],[0]
"We evaluate our methods at a much larger scale than related transfer learning work in NLU, for fast and scalable expansion of hundreds of new natural language domains of Amazon Alexa, a commercial artificial agent.",1 Introduction,[0],[0]
We show that our methods achieve significant performance gains in low resource settings and enable building accurate functionality faster during early stages of model development by reducing reliance on large annotated datasets.,1 Introduction,[0],[0]
"Deep learning models, including Long-Short term memory networks (LSTM) (Gers et al., 1999), are state of the art for many natural language processing tasks (NLP), such as sequence labeling (Chung et al., 2014), named entity recognition (NER) (Chiu and Nichols, 2015) and part of speech (POS) tagging (Huang et al., 2015).
",2 Related Work,[0],[0]
"145
Multitask learning is also widely applied in NLP, where a network is jointly trained for multiple related tasks.",2 Related Work,[0],[0]
"Multitask architectures have been succesfully applied for joint learning of NER, POS, chunking and supertagging tasks, as in (Collobert et al., 2011; Collobert and Weston, 2008; Søgaard and Goldberg, 2016).
",2 Related Work,[0],[0]
"Similarly, transfer learning addresses the transfer of knowledge from data-rich source tasks to under-resourced target tasks.",2 Related Work,[0],[0]
"Neural transfer learning has been successfully applied in computer vision tasks where lower layers of a network learn generic features that are transferred well to different tasks (Zeiler and Fergus, 2014; Krizhevsky et al., 2012).",2 Related Work,[0],[0]
"Such methods led to impressive results for image classification and object detection (Donahue et al., 2014; Sharif Razavian et al., 2014; Girshick et al., 2014)",2 Related Work,[0],[0]
"In NLP, transferring neural features across tasks with disparate label spaces is relatively less common.",2 Related Work,[0],[0]
"In (Mou et al., 2016), authors conclude that network transferability depends on the semantic relatedness of the source and target tasks.",2 Related Work,[0],[0]
"In cross-language transfer learning, (Buys and Botha, 2016) use weak supervision to project morphology tags to a common label set, while (Kim et al., 2017a) transfer lower layer representations across languages for POS tagging.",2 Related Work,[0],[0]
"Other related work addresses transfer learning where source and target share the same label space, while feature and label distributions differ, including deep learning methods (Glorot et al., 2011; Kim et al., 2017b), and earlier domain adaptation methods such as EasyAdapt (Daumé III, 2007), instance weighting (Jiang and Zhai, 2007) and structural correspondence learning (Blitzer et al., 2006).
",2 Related Work,[0],[0]
Fast functionality expansion is critical in industry settings.,2 Related Work,[0],[0]
"Related work has focused on scalability and ability to learn from few resources when developing a new domain, and includes zero-shot learning (Chen et al., 2016; Ferreira et al., 2015), domain attention (Kim et al., 2017c), and scalable, modular classifiers (Li et al., 2014).",2 Related Work,[0],[0]
"There is a multitude of commercial tools for developers to build their own custom natural language applications, including Amazon Alexa ASK (Kumar et al., 2017), DialogFlow by Google (DialogFlow) and LUIS by Microsoft (LUIS).",2 Related Work,[0],[0]
"Along these lines, we propose scalable methods that can be applied for rapid development of hundreds of low resource domains across disparate label spaces.",2 Related Work,[0],[0]
"We focus on Amazon Alexa, an intelligent conversational agent that interacts with the user through voice commands and is able to process requests on a range of natural language domains, e.g., playing music, asking for weather information and editing a calendar.",3 NLU Functionality Expansion,[0],[0]
"In addition to this built-in functionality that is designed and built by internal developers, the Alexa Skills Kit (ASK) (Kumar et al., 2017) enables external developers to build their own custom functionality which they can share with other users, effectively allowing for unlimited new capabilities.",3 NLU Functionality Expansion,[0],[0]
"Below, we describe the development process and challenges associated with natural language domain expansion.
",3 NLU Functionality Expansion,[0],[0]
"For each new domain, the internal or external developers define a set of intents and slots for the target functionality.",3 NLU Functionality Expansion,[0],[0]
"Intents correspond to user intention, e.g., ‘FindRecipeIntent’, and slots correspond to domain-specific entities of interest e.g.,‘FoodItem’.",3 NLU Functionality Expansion,[0],[0]
"Developers also define a set of commonly used utterances that cover the core use cases of the functionality, e.g., ‘find a recipe for chicken’.",3 NLU Functionality Expansion,[0],[0]
We call those core utterances.,3 NLU Functionality Expansion,[0],[0]
"In addition, developers need to create gazetteers for their domain, which are lists of slot values.",3 NLU Functionality Expansion,[0],[0]
"For example, a gazetteer for ‘FoodItem’ will contain different food names like ‘chicken’.",3 NLU Functionality Expansion,[0],[0]
"We have developed infrastructure to allow internal and external teams to define their domain, and create or expand linguistic resources such as core utterances and gazetteers.",3 NLU Functionality Expansion,[0],[0]
"We have also built tools that enable extracting carrier phrases from the example utterances by abstracting the utterance slot values, such as ‘find a recipe for {FoodItem}’.",3 NLU Functionality Expansion,[0],[0]
The collection of carrier phrases and gazetteers for a domain is called a grammar.,3 NLU Functionality Expansion,[0],[0]
Grammars can be sampled to generate synthetic data for model training.,3 NLU Functionality Expansion,[0],[0]
"For example, we can generate the utterance ‘find a recipe for pasta’ if the latter dish is contained in the ‘FoodItem’ gazetteer.
",3 NLU Functionality Expansion,[0],[0]
"Next, developers enrich the linguistic resources available for a new domain, to cover more linguistic variations for intents and slots.",3 NLU Functionality Expansion,[0],[0]
"This includes creating bootstrap data for model development, including collecting utterances that cover the new functionality, manually writing variations of example utterances, and expanding the gazetteer values.",3 NLU Functionality Expansion,[0],[0]
"In general, this is a time and data intensive process.",3 NLU Functionality Expansion,[0],[0]
"External developers can also continuously enrich the data they provide for their cus-
tom domain.",3 NLU Functionality Expansion,[0],[0]
"However, external developers typically lack the time, resources or expertise to provide rich datasets, therefore in practice custom domains are significantly under-resourced compared to built-in domains.
",3 NLU Functionality Expansion,[0],[0]
"Once the new domain model is bootstrapped using the collected datasets, it becomes part of Alexa’s natural language functionality and is available for user interactions.",3 NLU Functionality Expansion,[0],[0]
The data from such user interactions can be sampled and annotated in order to provide additional targeted training data for improving the accuracy of the domain.,3 NLU Functionality Expansion,[0],[0]
"A good bootstrap model accuracy will lead to higher user engagement with the new functionality and hence to a larger opportunity to learn from user interaction data.
",3 NLU Functionality Expansion,[0],[0]
"Considering these challenges, our goal is to reduce our reliance on large annotated datasets for a new domain by re-using resources from existing domains.",3 NLU Functionality Expansion,[0],[0]
"Specifically, we aim to achieve higher model accuracy in low resource settings and accelerate new domain development by building good quality bootstrap models faster.",3 NLU Functionality Expansion,[0],[0]
"In this section, we describe transfer learning methods for efficient data re-use.",4 Methodology,[0],[0]
Transfer learning refers to transferring the knowledge gained while performing a task in a source domain Ds to benefit a related task in a target domain Dt.,4 Methodology,[0],[0]
"Typically, we have a large dataset for Ds, while Dt is an under-resourced new task.",4 Methodology,[0],[0]
"Here, the target domain is the new built-in or custom domain, while the source domain contains functionality that we have released, for which we have large amounts of data.",4 Methodology,[0],[0]
"The tasks of interest in both Ds and Dt are the same, namely slot tagging and intent classification.",4 Methodology,[0],[0]
"However Ds and Dt have different label spaces Ys and Yt, because a new domain will contain new intent and slot labels compared to previously released domains.",4 Methodology,[0],[0]
We first present our NLU system where we perform slot tagging (ST) and intent classification (IC) for a given input user utterance.,4.1 DNN-based natural language engine,[0],[0]
"We are inspired by the neural architecture of (Søgaard and Goldberg, 2016), where a multi-task learning architecture is used with deep bi-directional Recurrent Neural Networks (RNNs).",4.1 DNN-based natural language engine,[0],[0]
Supervision for the different tasks happens at different layers.,4.1 DNN-based natural language engine,[0],[0]
"Our neural network contains three layers
of bi-directional Long Short Term Memory networks (LSTMs) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997).",4.1 DNN-based natural language engine,[0],[0]
"The two top layers are optimized separately for the ST and IC tasks, while the common bottom layer is optimized for both tasks, as shown in Figure 1.
",4.1 DNN-based natural language engine,[0],[0]
"Specifically let rct denote the common representation computed by the bottommost bi-LSTM for each word input at time t. The ST forward LSTM layer learns a representation rST,ft = φ(r c t , r ST t−1), where φ denotes the LSTM operation.",4.1 DNN-based natural language engine,[0],[0]
"The IC forward LSTM layer learns rIC,ft = φ(r c t , r IC t−1).",4.1 DNN-based natural language engine,[0],[0]
"Similarly, the backward LSTM layers learn rST,bt and rIC,bt .",4.1 DNN-based natural language engine,[0],[0]
"To obtain the slot tagging decision, we feed the ST bi-LSTM layer’s output per step into a softmax, and produce a slot label at each time step (e.g., at each input word).",4.1 DNN-based natural language engine,[0],[0]
"For the intent decision, we concatenate the last time step from the forward LSTM with the first step of the backward LSTM, and feed it into a softmax for classification:
rslott = r ST,f t ⊕ rST,bt , rintent = rIC,fT ⊕ r IC,b 1
Ŝt = softmax(Wsr slot t + bs)
Î = softmax(WIr intent + bI)
where ⊕ denotes concatenation.",4.1 DNN-based natural language engine,[0],[0]
"Ws,WI , bs, bI are the weights and biases for the slot and intent softmax layers respectively.",4.1 DNN-based natural language engine,[0],[0]
"Ŝt is the predicted slot tag per time step (per input word), and Î is the predicted intent label for the sentence.
",4.1 DNN-based natural language engine,[0],[0]
"The overall objective function for the multitask network combines the IC and ST objectives.
",4.1 DNN-based natural language engine,[0],[0]
"Therefore we jointly learn a shared representation rct that leverages the correlations between the related IC and ST tasks, and shares beneficial knowledge across tasks.",4.1 DNN-based natural language engine,[0],[0]
"Empirically, we have observed that this multitask architecture achieves better results than separately training intent and slot models, with the added advantage of having a single model, and a smaller total parameter size.
",4.1 DNN-based natural language engine,[0],[0]
"In our setup, each input word is embedded into a 300-dimensional embedding, where the embeddings are estimated from our data.",4.1 DNN-based natural language engine,[0],[0]
"We also use pre-trained word embeddings as a separate input, that allows incorporating unsupervised word information from much larger corpora (FastText (Bojanowski et al., 2016)).",4.1 DNN-based natural language engine,[0],[0]
"We encode slot spans using the IOB tagging scheme (Ramshaw and Marcus, 1995).",4.1 DNN-based natural language engine,[0],[0]
"When we have available gazetteers relevant to the ST task, we use gazetteer features as an additional input.",4.1 DNN-based natural language engine,[0],[0]
"Such features are binary indicators of the presence of an n-gram in a gazetteer, and are common for ST tasks (Radford et al., 2015; Nadeau and Sekine, 2007).",4.1 DNN-based natural language engine,[0],[0]
"Typically, a new domain Dt contains little available data for training the multitask DNN architecture of Sec 4.1.",4.2 Transfer learning for the DNN engine,[0],[0]
"We propose to leverage existing data from mature released domains (source Ds) to build generic models, which are then adapted to the new tasks (target Dt).
",4.2 Transfer learning for the DNN engine,[0],[0]
We train our DNN engine using labeled data from Ds in a supervised way.,4.2 Transfer learning for the DNN engine,[0],[0]
The source slot tags space Y slots and intent label space Y intent s contain labels from previously released slots and intents respectively.,4.2 Transfer learning for the DNN engine,[0],[0]
"We refer to this stage as pretraining, where the stacked layers in the network learn to generate features which are useful for the ST and IC tasks of Ds.",4.2 Transfer learning for the DNN engine,[0],[0]
Our hypothesis is that such features will also be useful for Dt.,4.2 Transfer learning for the DNN engine,[0],[0]
"After pre-training is complete, we replace the top-most affine transform and softmax layers for IC and ST with layer dimensions that correspond to the target label space for intents and slots respectively, i.e., Y intentt and Y slot t .",4.2 Transfer learning for the DNN engine,[0],[0]
The network is then trained again using the available target labeled data for IC and ST.,4.2 Transfer learning for the DNN engine,[0],[0]
"We refer to this stage as fine-tuning of the DNN parameters for adapting to Dt.
",4.2 Transfer learning for the DNN engine,[0],[0]
A network can be pre-trained on large datasets from Ds and later fine tuned separately for many low resource new domains Dt.,4.2 Transfer learning for the DNN engine,[0],[0]
"In some cases, when developing a new domain Dt, new domain-
specific information becomes available, such as domain gazetteers (which were not available at pre-training).",4.2 Transfer learning for the DNN engine,[0],[0]
"To incorporate this information during fine-tuning, we add gazetteer features as an extra input to the two top-most ST and IC layers, as shown in Figure 1.",4.2 Transfer learning for the DNN engine,[0],[0]
We found that adding new features during fine-tuning significantly changes the upper layer distributions.,4.2 Transfer learning for the DNN engine,[0],[0]
"Therefore, in such cases, it is better to train the ST and IC layers from scratch and only transfer and fine-tune weights from the common representation rc of the bottom layer.",4.2 Transfer learning for the DNN engine,[0],[0]
"However, when no gazetteers are available, it is beneficial to pre-train all stacked Bi-LSTM layers (common, IC and ST), except from the taskspecific affine transform leading to the softmax.",4.2 Transfer learning for the DNN engine,[0],[0]
"While DNNs are strong models for both ST and IC, they typically need large amounts of training data.",4.3 Baseline natural language engine,[0],[0]
"As we focus on under-resourced functionality, we examine an alternative baseline that relies on simpler models; namely a Maximum Entropy (MaxEnt) (Berger et al., 1996) model for intent classification and a Conditional Random Field (CRF) (Lafferty et al., 2001) model for slot tagging.",4.3 Baseline natural language engine,[0],[0]
"MaxEnt models are regularized log-linear models that have been shown to be effective for text classification tasks (Berger et al., 1996).",4.3 Baseline natural language engine,[0],[0]
"Similarly, CRFs have been popular tagging models in the NLP literature (Nadeau and Sekine, 2007)",4.3 Baseline natural language engine,[0],[0]
prior to the recent growth in deep learning.,4.3 Baseline natural language engine,[0],[0]
"In our experience, these models require less data to train well and represent strong baselines for low resource classification and tagging tasks.",4.3 Baseline natural language engine,[0],[0]
"We evaluate the transfer learning methods of Section 4.2 for both custom and built-in domains, and compare with baselines that do not benefit from knowledge transfer (Sections 4.1, 4.3).",5 Experiments and Results,[0],[0]
"We experiment with around 200 developer defined custom domains, whose statistics are presented in Table 1.",5 Experiments and Results,[0],[0]
"Looking at the median numbers, which are less influenced by a few large custom domains compared to mean values, we note that typically developers provide just a few tens of example phrases and few tens of values per gazetteer (slot gazetteer size).",5 Experiments and Results,[0],[0]
"Therefore, most custom domains are significantly under-resourced.",5 Experiments and Results,[0],[0]
"We also select three new built-in domains, and evaluate them at various early stages of domain development.",5 Experiments and Results,[0],[0]
"Here, we assume that variable amounts of training data grad-
ually become available, including bootstrap and user interaction data.
",5 Experiments and Results,[0],[0]
We pre-train DNN models using millions of annotated utterances from existing mature built-in domains.,5 Experiments and Results,[0],[0]
"Each annotated utterance has an associated domain label, which we use to make sure that the pre-training data does not contain utterances labeled as any of the custom or built-in target domains.",5 Experiments and Results,[0],[0]
"After excluding the target domains, the pre-training data is randomly selected from a variety of mature Alexa domains covering hundreds of intents and slots across a wide range of natural language functionality.",5 Experiments and Results,[0],[0]
"For all experiments, we use L1 and L2 to regularize our DNN, CRF and MaxEnt models, while DNNs are additionally regularized with dropout.
",5 Experiments and Results,[0],[0]
"The test sets contain user data, annotated for each custom or built-in domain.",5 Experiments and Results,[0],[0]
"For custom domains, test set size is a few hundred utterances per domain, while for built-in domains it is a few thousand utterances per domain.",5 Experiments and Results,[0],[0]
"Our metrics include standard F1 scores for the SC and IC tasks, and a sentence error rate (SER) defined as the ratio of utterances with at least one IC or ST error over all utterances.",5 Experiments and Results,[0],[0]
The latter metric combines IC and ST errors per utterance and reflects how many utterances we could not understand correctly.,5 Experiments and Results,[0],[0]
"For the custom domain experiments, we focus on a low resource experimental setup, where we assume that our only target training data is the data provided by the external developer.",5.1 Results for custom developer domains,[0],[0]
"We report results for around 200 custom domains, which is a subset of all domains we support.",5.1 Results for custom developer domains,[0],[0]
"We compare the proposed transfer learning method, denoted as DNN Pretrained, with the two baseline methods described in sections 4.1 and 4.3, denoted as DNN Baseline and CRF/MaxEnt Baseline, respectively.",5.1 Results for custom developer domains,[0],[0]
"For training the baselines, we use the available data provided by the developer for each domain, e.g., example phrases and gazetteers.",5.1 Results for custom developer domains,[0],[0]
"From these resources, we create grammars and we sample them to generate 50K training utterances per
domain, using the process described in Section 3.",5.1 Results for custom developer domains,[0],[0]
This training data size was selected empirically based on baseline model accuracy.,5.1 Results for custom developer domains,[0],[0]
The generated utterances may contain repetitions for domains where the external developer provided a small amount of example phrases and few slot values per gazetteer.,5.1 Results for custom developer domains,[0],[0]
"For the proposed method, we pre-train a DNN model on 4 million utterances and fine tune it per domain using the 50K grammar utterances of that domain and any available gazetteer information (for extracting gazetteer features).",5.1 Results for custom developer domains,[0],[0]
"In Table 2, we show the mean and median across custom domains for F1slot, F1intent and SER.
",5.1 Results for custom developer domains,[0],[0]
"Table 2 shows that the CRF and MaxEnt models present a strong baseline and generally outperform the DNN model without pretraining, which has a larger number of parameters.",5.1 Results for custom developer domains,[0],[0]
This suggests that the baseline DNN models (without pretraining) cannot be trained robustly without large available training data.,5.1 Results for custom developer domains,[0],[0]
"The proposed pre-trained DNN significantly outperforms both baselines across all metrics (paired t-test, p < .01).",5.1 Results for custom developer domains,[0],[0]
Median SER reduces by around 14% relative when we use transfer learning compared to both baselines.,5.1 Results for custom developer domains,[0],[0]
We are able to harness the knowledge obtained from data of multiple mature source domains,5.1 Results for custom developer domains,[0],[0]
Ds and transfer it to our under-resourced target domains,5.1 Results for custom developer domains,[0],[0]
"Dt, across disparate label spaces.
",5.1 Results for custom developer domains,[0],[0]
To investigate the effect of semantic similarity across source and target domains we selected a subset of 30 custom domains with high semantic similarity with the source tasks.,5.1 Results for custom developer domains,[0],[0]
"Semantic similarity was computed by comparing the sentence representations computed by the common bi-LSTM layer across source and target sentences, and selecting target custom domains with sentences close to at least one of the source tasks.",5.1 Results for custom developer domains,[0],[0]
"For these 30 domains, we observed higher gains of around 19% relative median SER reduction.",5.1 Results for custom developer domains,[0],[0]
"This corroborates observations of (Mou et al., 2016), that neural feature transferability for NLP depends on the semantic similarity between source and target.",5.1 Results for custom developer domains,[0],[0]
"In our low resource tasks, we see a benefit from transfer learning and this benefit increases as we select more semantically similar data.
",5.1 Results for custom developer domains,[0],[0]
"Our approach is scalable and is does not rely on manual domain-specific annotations, besides developer provided data.",5.1 Results for custom developer domains,[0],[0]
"Also, pretrained DNN models are about five times faster to train during the fine-tuning stage, compared to training the model from scratch for each custom domain,
which speeds up model turn-around time.",5.1 Results for custom developer domains,[0],[0]
"We evaluate our methods on three new built-in domains referred here as domain A (5 intents, 36 slot types), domain B (2 intents, 17 slot types) and domain C (22 intents, 43 slot types).",5.2 Results for built-in domains,[0],[0]
"Table 3 shows results for domains A, B and C across experimental early stages of domain development, where different data types and amounts of data per data type gradually become available.",5.2 Results for built-in domains,[0],[0]
"Core data refers to core example utterances, bootstrap data refers to domain data collection and generation of synthetic (grammar) utterances, and user data refers to user interactions with our agent.",5.2 Results for built-in domains,[0],[0]
"As described in Section 3, the collection and annotation of these data sources is a lengthy process.",5.2 Results for built-in domains,[0],[0]
"Here we evaluate whether we can accelerate the development process by achieving accuracy gains in early, low resource stages, and bootstrap a model faster.
",5.2 Results for built-in domains,[0],[0]
"For each data setting and size, we compare our proposed pretrained DNN models with the baseline CRF/MaxEnt baseline, which is the better performing baseline of Section 5.1.",5.2 Results for built-in domains,[0],[0]
"Results for the non pre-trained DNN baseline are similar, and omitted for lack of space.",5.2 Results for built-in domains,[0],[0]
Our proposed DNN models are pre-trained on 4 million data from mature domains and then fine tuned on the available target data.,5.2 Results for built-in domains,[0],[0]
The baseline CRF/MaxEnt models are trained on the available target data.,5.2 Results for built-in domains,[0],[0]
Note that the datasets of Table 3 represent early stages of model development and do not reflect final training size or model performance.,5.2 Results for built-in domains,[0],[0]
The types of target data slightly differ across domains according to domain development characteristics.,5.2 Results for built-in domains,[0],[0]
"For example, for domain B there was very small amount of core data available and it was combined with the bootstrap data for experiments.
",5.2 Results for built-in domains,[0],[0]
"Overall, we notice that our proposed DNN pretraining method improves performance over the CRF/MaxEnt baseline, for almost all data settings.",5.2 Results for built-in domains,[0],[0]
"As we would expect, we see the largest gains for the most low resource data settings.",5.2 Results for built-in domains,[0],[0]
"For example, for domain A, we observe a 7% and 5% relative
SER improvement on core and bootstrap data settings respectively.",5.2 Results for built-in domains,[0],[0]
The performance gain we obtain on those early stages of development brings us closer to our goal of rapidly bootstrapping models with less data.,5.2 Results for built-in domains,[0],[0]
"From domains A and C, we also notice that we achieve the highest performance in settings that leverage user data, which highlights the importance of such data.",5.2 Results for built-in domains,[0],[0]
"Note that the drop in Fintent for domain C between core and bootstrap data is because the available bootstrap data did not contain data for all of the 22 intents of domain C. Finally, we notice that the gain from transfer learning diminishes in some larger data settings, and we
may see degradation (domain C, 126K data setting).",5.2 Results for built-in domains,[0],[0]
We hypothesize that as larger training data becomes available it may be better to not pre-train or pre-train with source data that are semantically similar to the target.,5.2 Results for built-in domains,[0],[0]
We will investigate this as part of future work.,5.2 Results for built-in domains,[0],[0]
"We have described the process and challenges associated with large scale natural language functionality expansion for built-in and custom domains for Amazon Alexa, a popular commercial intelligent agent.",6 Conclusions and Future Work,[0],[0]
"To address scalability and data collection bottlenecks, we have proposed data efficient deep learning architectures that benefit from transfer learning from resource-rich functionality domains.",6 Conclusions and Future Work,[0],[0]
"Our models are pre-trained on existing resources and then adapted to hundreds of new, low resource tasks, allowing for rapid and accurate expansion of NLU functionality.",6 Conclusions and Future Work,[0],[0]
"In the future, we plan to explore unsupervised methods for transfer learning and the effect of semantic similarity between source and target tasks.",6 Conclusions and Future Work,[0],[0]
Fast expansion of natural language functionality of intelligent virtual agents is critical for achieving engaging and informative interactions.,abstractText,[0],[0]
"However, developing accurate models for new natural language domains is a time and data intensive process.",abstractText,[0],[0]
We propose efficient deep neural network architectures that maximally re-use available resources through transfer learning.,abstractText,[0],[0]
"Our methods are applied for expanding the understanding capabilities of a popular commercial agent and are evaluated on hundreds of new domains, designed by internal or external developers.",abstractText,[0],[0]
We demonstrate that our proposed methods significantly increase accuracy in low resource settings and enable rapid development of accurate models with less data.,abstractText,[0],[0]
Fast and Scalable Expansion of Natural Language Understanding Functionality for Intelligent Agents,title,[0],[0]
"Spectral clustering (SC) is one of the most utilized methods for clustering multivariate data (Von Luxburg, 2007; Fortunato, 2010).",1. Introduction,[0],[0]
"However, because of its inherent dependence on the spectrum of some large graph, SC is computationally expensive.",1. Introduction,[0],[0]
"Let n and k be the number of nodes and clusters, respectively.",1. Introduction,[0],[0]
Clustering a graph takes O(n3) operations if a full eigendecomposition is performed and O(kn2) if the Lanczos method is used.,1. Introduction,[0],[0]
"This has motivated a surge of research focusing in reducing its complexity, for example using matrix sketching (Fowlkes et al., 2004; Li et al., 2011; Gittens et al., 2013), coarsening (Loukas and Vandergheynst, 2018), and compressive sampling (Ramasamy and Madhow, 2015; Tremblay et al., 2016), attaining a complexity reduction by roughly a factor of n.
Yet, computation is still an issue for dynamic networks, where the edge set is a function of time.",1. Introduction,[0],[0]
"Temporal dynamics constitute an important aspect of many network datasets
1École Polytechnique Fédérale de Lausanne, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Lionel Martin<lionel.martin@epfl.ch>, Andreas Loukas <andreas.loukas@epfl.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
and should be taken into account in the algorithmic design and analysis.,1. Introduction,[0],[0]
"Unfortunately, SC is poorly suited to this setting as eigendecomposition –its main computational bottleneck– has to be recomputed from scratch whenever the graph is updated, or at least periodically (Ning et al., 2007).",1. Introduction,[0],[0]
"This is a missed opportunity since the clustering assignments of many real networks change slowly with time, suggesting that successive algorithmic runs wastefully repeat similar computations.
",1. Introduction,[0],[0]
This paper proposes an algorithm that reuses information of past cluster assignments to expedite computation.,1. Introduction,[0],[0]
"Different from previous work on dynamic clustering, our objective is not to improve the clustering quality, e.g., by enforcing a temporal-smoothness hypothesis (Chakrabarti et al., 2006; Chi et al., 2007) or by using tensor decompositions (Gauvin et al., 2014; Tu et al., 2016).",1. Introduction,[0],[0]
"Similar to recent work by (Dhanjal et al., 2014), we focus entirely on reducing the complexity while producing assignments that are provably close to those of SC.
",1. Introduction,[0],[0]
"Our work starts from the recent idea of sidestepping eigendecomposition by utilizing as features random vectors filtered by Chebyshev polynomials of the graph Laplacian (Tremblay et al., 2016).",1. Introduction,[0],[0]
"We notice that, in the dynamic setting, there are ample opportunities to reuse information from previous cluster assignments, both in terms of approximating the k-th eigenvalue (a necessary step of Chebyshev filter design), as well as in terms of computing the features themselves.",1. Introduction,[0],[0]
"When the consecutive graphs are appropriately similar, these ideas lead to complexity reductions.
",1. Introduction,[0],[0]
"Concretely, we provide the following contributions:
1.",1. Introduction,[0],[0]
"In Section 3 we refine the analysis of compressive spectral clustering (CSC) presented in (Tremblay et al., 2016).",1. Introduction,[0],[0]
Our goal is to move from assertions about feature approximation to guarantees about the quality of the solution of CSC itself.,1. Introduction,[0],[0]
We prove that w.h.p.,1. Introduction,[0],[0]
"the quality of the clustering assignments of CSC and SC differ by O(2k/ √ d), and thus d ∝ k2 filtered vectors are sufficient to obtain a good approximation.",1. Introduction,[0],[0]
"Importantly, our analysis does not make restricting assumptions about the graph structure, such as assuming a stochastic block model (Pydi and Dukkipati, 2017).
2.",1. Introduction,[0],[0]
"In Section 4, we focus on dynamic graphs and propose dynamic CSC (dCSC), an algorithm that reuses informa-
tion of past cluster assignments to expedite computation.",1. Introduction,[0],[0]
"We discover that the algorithm’s ability to reuse features is determined by a measure of spectral similarity ρ between consecutive graphs: we prove that, when pd features are reused (i.e., each new instance of the dynamic graph is clustered using pd features of the previous graph and (1 − p)d new features, where 0 < p ≤ 0.5), w.h.p.",1. Introduction,[0],[0]
"the clustering quality of dCSC approximates that of CSC up to an additive term in the order of pρ.
",1. Introduction,[0],[0]
The paper concludes with a proof of concept comparison against SotA approximation algorithms for Spectral Clustering (Section 5).,1. Introduction,[0],[0]
Our experiments confirm that dCSC yields computational benefits when the graph dynamics are bounded.,1. Introduction,[0],[0]
A case in point: we can cluster 30’000 node graphs 3.9× faster than SC and 1.5× faster than CSC in average.,1. Introduction,[0],[0]
"Due to space constraints, certain proofs and implementation details are presented as an appendix in a supplementary document.",1. Introduction,[0],[0]
We start by summarizing the standard method for spectral clustering as well as the idea behind the more recent accelerated methods.,2. Background,[0],[0]
"Due to space constraints, our exposition is brief; the reader is encouraged to refer to the original works for a more comprehensive discussion.",2. Background,[0],[0]
"To determine the best node-to-cluster assignment, spectral clustering solves a k-means problem with the eigenvectors of the graph Laplacian as features (Shi and Malik, 2000; Ng et al., 2002).
",2.1. Spectral clustering (SC),[0],[0]
"Let G = (V, E ,W) be a weighted undirected graph with n nodes V = {v1, v2, . . .",2.1. Spectral clustering (SC),[0],[0]
", vn}, and m edges E ⊂ V ×V .",2.1. Spectral clustering (SC),[0],[0]
"The graph Laplacian is defined as L = I − D−1/2WD−1/2, where D is a diagonal matrix whose entries are the degree of the nodes in the graph (i.e. the sum of the weighed edges adjacent to each node).",2.1. Spectral clustering (SC),[0],[0]
"We denote the eigendecomposition of the Laplacian by L = UΛU>, with the eigenvalues contained in Λ sorted in non-decreasing order, such that 0 = λ1 ≤ λ2 ≤ . . .",2.1. Spectral clustering (SC),[0],[0]
≤,2.1. Spectral clustering (SC),[0],[0]
"λn.
",2.1. Spectral clustering (SC),[0],[0]
Spectral clustering consists of computing the first k eigenvectors of L arranged in matrix Uk and subsequently computing a k-means assignment of the n vectors of size k found in the rows of Uk.,2.1. Spectral clustering (SC),[0],[0]
"Formally, if Φ ∈ Rn×d is the feature matrix (here Φ = Uk and d = k), and k is a positive integer denoting the number of clusters, the k-means clustering problem finds the indicator matrix X ∈ Rn×k which satisfies
XΦ = arg min X∈X
‖Φ−XX>Φ‖F , (1)
with associated cost CΦ = ‖Φ −XΦXTΦΦ‖F .",2.1. Spectral clustering (SC),[0],[0]
Symbol X denotes the set of all n×k indicator matrices X.,2.1. Spectral clustering (SC),[0],[0]
"These matrices indicates the cluster membership of each data point by setting
Xi,j =
{ 1√ sj if data point i belongs to cluster j
0 otherwise, (2)
where sj is the size of cluster j, also equals to the number of non-zero elements in column j. Note that the cost described in eq.",2.1. Spectral clustering (SC),[0],[0]
"(1) is the square root of the more traditional definition expressed with the distances to the cluster centers (Cohen et al., 2015, Sec 2.3).",2.1. Spectral clustering (SC),[0],[0]
"We refer the reader to the work by (Boutsidis et al., 2015) and references therein for more details.",2.1. Spectral clustering (SC),[0],[0]
"To reduce the computational cost of spectral clustering, (Tremblay et al., 2016) proposed to approximate Uk using a filtering of random vectors (a similar idea was also examined by (Boutsidis et al., 2015)).",2.2. Compressive spectral clustering (CSC),[0],[0]
"The former work also introduced the benefits of compressed sampling techniques reducing the total cost down toO(k2 log2(k)+cn(log(n)+ k)), where c is the order of the polynomial approximation.",2.2. Compressive spectral clustering (CSC),[0],[0]
"Their algorithm consists of two steps:
Step 1.",2.2. Compressive spectral clustering (CSC),[0],[0]
Approximate features.,2.2. Compressive spectral clustering (CSC),[0],[0]
Feature matrix Uk is approximated by the projection of a random matrix over the same subspace.,2.2. Compressive spectral clustering (CSC),[0],[0]
"In particular, let R ∈ RN×d be a random (gaussian) matrix with centered i.i.d.",2.2. Compressive spectral clustering (CSC),[0],[0]
"entries, each having variance 1d .",2.2. Compressive spectral clustering (CSC),[0],[0]
"We can project R onto span{Uk} by multiplying each one of its columns by a projector H defined as
H = U ( Ik 0 0 0 )",2.2. Compressive spectral clustering (CSC),[0],[0]
U>.,2.2. Compressive spectral clustering (CSC),[0],[0]
"(3)
It is then a simple consequence of the JohnshonLindenstrauss lemma that the rowsψ>i of matrix Ψ = HR can act as a replacement of the features used in spectral clustering, i.e., the rows φ>i of Φ = Uk.
Theorem 2.1 (adapted from (Tremblay et al., 2016)).",2.2. Compressive spectral clustering (CSC),[0],[0]
"For every two nodes vi and vj the restricted isometry relation
(1−ε)‖φi−φj‖2 ≤ ‖ψi−ψj‖2 ≤",2.2. Compressive spectral clustering (CSC),[0],[0]
"(1+ε)‖φi−φj‖2 (4)
holds with probability larger than 1 − n−β , as long as the dimension is d > 4+2βε2/2−ε3/3 log(n).
",2.2. Compressive spectral clustering (CSC),[0],[0]
"We note that, even though HR is also expensive to compute exactly, it can be easily approximated by applying the graph filter h(L) on each column of R, which entailsO(dc) sparse matrix-vector multiplications (each costing O(m)) using graph Chebychev polynomials (Shuman et al., 2011a; Hammond et al., 2011) or rational graph filters (Isufi et al., 2017; Loukas et al., 2015) (c relates to the quality of the
approximation and is usually below 100).",2.2. Compressive spectral clustering (CSC),[0],[0]
"A more elaborate discussion on the approximation of HR can be found in the appendix.
",2.2. Compressive spectral clustering (CSC),[0],[0]
Step 2.,2.2. Compressive spectral clustering (CSC),[0],[0]
Compressive k-means.,2.2. Compressive spectral clustering (CSC),[0],[0]
The complexity is reduced further by computing the k-means step for only a subset of the nodes.,2.2. Compressive spectral clustering (CSC),[0],[0]
"The remaining cluster assignments are then inferred by solving a graph Tikhonov regularized interpolation problem involving k additional graph filtering operations, each with a cost linear in cm.",2.2. Compressive spectral clustering (CSC),[0],[0]
"To guarantee a good approximation, it is sufficient to sample O(k log(k))",2.2. Compressive spectral clustering (CSC),[0],[0]
"nodes using variable density sampling (Puy et al., 2016).",2.2. Compressive spectral clustering (CSC),[0],[0]
"For simplicity, in the following, we present our theoretical results w.r.t.",2.2. Compressive spectral clustering (CSC),[0],[0]
the non-compressed version of their algorithm.,2.2. Compressive spectral clustering (CSC),[0],[0]
"The proofs can be generalized using similar arguments as in (Tremblay et al., 2016).",2.2. Compressive spectral clustering (CSC),[0],[0]
"Before delving to the dynamic setting, we refine the analysis of compressive spectral clustering.",3. The Approximation Quality of Static CSC,[0],[0]
Our objective is to move from assertions about distance preservation currently known (see Thm. 2.1) to guarantees about the quality of the solution of CSC itself.,3. The Approximation Quality of Static CSC,[0],[0]
"Formally, let
XΨ = arg min X∈X
‖Ψ−XX>Ψ‖F (5)
be the clustering assignment obtained from using k-means with Ψ as features (CSC assignment), and define the CSC cost CΨ as
CΨ = ‖Φ−XΨX>ΨΦ‖F .",3. The Approximation Quality of Static CSC,[0],[0]
"(6)
The question we ask is: how close is CΨ to the cost CΦ of the same problem, where the assignment has been computed using Φ as features, i.e., the SC cost corresponding to (1)?",3. The Approximation Quality of Static CSC,[0],[0]
"Note that, as in previous work (Boutsidis et al., 2015), we express the approximation quality in terms of the difference of clustering assignment costs and not of the distance between the assignments themselves.",3. The Approximation Quality of Static CSC,[0],[0]
"We are not aware of any analysis that would allow us to characterize (the perhaps more intuitive goal of) how well XΨ approximates XΦ, which is a combinatorial objective.",3. The Approximation Quality of Static CSC,[0],[0]
"Yet, our approach exhibits the benefit of not penalizing approximation algorithms that choose alternative assignments of the same or similar quality1.
",3. The Approximation Quality of Static CSC,[0],[0]
"Our central theorem asserts that with high probability the assignments of SC and CSC have similar costs.
",3. The Approximation Quality of Static CSC,[0],[0]
Theorem 3.1.,3. The Approximation Quality of Static CSC,[0],[0]
"The SC cost CΦ and the CSC cost CΨ are related by
1The k-means objective is a non convex objective and has multiple minima.",3. The Approximation Quality of Static CSC,[0],[0]
"For instance, any of the k! re-labelings of the optimal assignment are valid solutions with the same cost.
",3. The Approximation Quality of Static CSC,[0],[0]
CΦ ≤,3. The Approximation Quality of Static CSC,[0],[0]
"CΨ ≤ CΦ + 2 √ k
d ( √ k + ε), (7)
with probability at least 1− exp(−ε2/2).
",3. The Approximation Quality of Static CSC,[0],[0]
This result emphasizes the importance of the number of random vectors d and directly links it to the distance with the optimal assignment for the spectral features.,3. The Approximation Quality of Static CSC,[0],[0]
"Indeed, one can see that the difference between the two costs vanishes when d is sufficiently large.",3. The Approximation Quality of Static CSC,[0],[0]
"Importantly, d ∝ k2 is sufficient to guarantee a small error.",3. The Approximation Quality of Static CSC,[0],[0]
The first step in proving Thm. 3.1 is to establish the relation between CΦ and CΨ.,3.1. The approximation quality of CSC,[0],[0]
The following lemma relates the two costs by an additive error term that depends on the feature’s differences ‖Ψ−ΦIk×dQ‖F when d ≥ k.Since Φ and Ψ have different sizes we introduced the multiplication by a unitary matrix Q. We will first show that any unitary Q can be picked in Lem.,3.1. The approximation quality of CSC,[0],[0]
"3.1 and then derive the optimal Q, the one minimizing the additive term, in Thm. 3.2.
",3.1. The approximation quality of CSC,[0],[0]
Lemma 3.1.,3.1. The approximation quality of CSC,[0],[0]
"For any unitary matrix Q ∈ Rd×d, the SC cost CΦ and the CSC cost CΨ are related by
CΦ ≤ CΨ ≤ CΦ + 2‖Ψ−ΦIk×dQ‖F , (8)
where, the matrix I`×m of size ` ×m above contains only ones on its diagonal and serves to resize matrices.
",3.1. The approximation quality of CSC,[0],[0]
"Being able to show that the additive term is small encompasses the result of Thm. 2.1, ensuring distance preservation.",3.1. The approximation quality of CSC,[0],[0]
"However, this statement is stronger than the previous one as our lemma is not necessarily true under distance preservation only.
",3.1. The approximation quality of CSC,[0],[0]
The remaining of this section is devoted to bounding the Frobenius error ‖Ψ−ΦIk×dQ‖F between the features of SC and CSC.,3.1. The approximation quality of CSC,[0],[0]
"In order to prove this result, we will first express our Frobenius norm exclusively in terms of the singular values of the random matrix R and then in a second step we will study the distribution of these singular values.
",3.1. The approximation quality of CSC,[0],[0]
"Our next result, which remarkably is an equality, reveals that the achieved error is exactly determined by how close a Gaussian matrix is to a unitary matrix.
",3.1. The approximation quality of CSC,[0],[0]
Theorem 3.2.,3.1. The approximation quality of CSC,[0],[0]
"There exists a d× d unitary matrix Q, such that
‖Ψ−ΦIk×dQ‖F = ‖Σ−",3.1. The approximation quality of CSC,[0],[0]
"Ik×d‖F , (9)
where Σ is the diagonal matrix holding the singular values of R′ = Ik×nU>R.
Before presenting the proof, let us observe that R′ is an i.i.d.",3.1. The approximation quality of CSC,[0],[0]
Gaussian random matrix of size k × d and its entries have zero mean,3.1. The approximation quality of CSC,[0],[0]
"and the same variance as that of R. We use
this fact in the following to control the error by appropriately selecting the number of random vectors",3.1. The approximation quality of CSC,[0],[0]
"d.
To bound the feature error further, we will use the following result by Vershynin, whose proof is not reproduced.
",3.1. The approximation quality of CSC,[0],[0]
Corollary 3.1 (adapted from Cor.,3.1. The approximation quality of CSC,[0],[0]
"5.35 (Vershynin, 2010)).",3.1. The approximation quality of CSC,[0],[0]
Let N be an d × k matrix whose entries are independent standard normal random variables.,3.1. The approximation quality of CSC,[0],[0]
"Then for every ε, i ≥ 0, with probability at least 1− exp(−ε2/2) one has
σi(N)− √ d ≤",3.1. The approximation quality of CSC,[0],[0]
"√ k + ε, (10)
where σi(N) is the ith singular value of N.
Exploiting this result, the following corollary of Thm.",3.1. The approximation quality of CSC,[0],[0]
"3.2 reveals the relation of the feature error and the number of random vectors d.
Corollary 3.2.",3.1. The approximation quality of CSC,[0],[0]
"There exists a d×d unitary matrix Q, such that, for every ε ≥ 0, one has
‖Ψ−ΦIk×dQ‖F ≤ √ k
d ( √ k + ε), (11)
with probability at least 1− exp(−ε2/2).
",3.1. The approximation quality of CSC,[0],[0]
"Finally, Cor. 3.2 combined with Lem. 3.1 provide the direct proof of Thm. 3.1 that we introduced earlier.
",3.1. The approximation quality of CSC,[0],[0]
"Before proceeding, we would like to make some remarks about the tightness of the bound.",3.1. The approximation quality of CSC,[0],[0]
"First, guaranteeing that the feature error is small is a stronger condition than distance preservation (though necessary for a complete analysis of CSC).",3.1. The approximation quality of CSC,[0],[0]
"For this reason, the bound derived can be larger than that of Thm. 2.1.",3.1. The approximation quality of CSC,[0],[0]
"Nevertheless, we should stress it is tight: the main inequality in our analysis stems from bounding the k largest singular values of the random matrix by Vershynin’s tight bound of the maximal singular value.",3.1. The approximation quality of CSC,[0],[0]
The study presented above assumes that H is defined as in eq.,3.2. Practical aspects,[0],[0]
"(3), namely that it is a projector on the subspace spanned by the first k eigenvectors of L. However, as discussed in the appendix, to be computationally efficient we choose to compute H by an application of a polynomial function h on L (Shuman et al., 2011a).",3.2. Practical aspects,[0],[0]
"More specifically, we select a polynomial that approximates the ideal low-pass response (Allen-Zhu and Li, 2016).",3.2. Practical aspects,[0],[0]
"As long as λk is known, the approximated projector h(L) can be designed to be very close to H: using the arguments of (Shuman et al., 2011b, Proposition 3) and (Laurent and Massart, 2000, Lemma 1) it is easy to prove that w.h.p.",3.2. Practical aspects,[0],[0]
"using h(L) instead of H does not add more than O(c−c √ n) error to CΨ, where c is the polynomial order (the proof is omitted due to space limitations).
",3.2. Practical aspects,[0],[0]
"Moreover, we need to estimate λk accurately (the design of h involves finding a polynomial which takes the value 1 when the input is smaller than λk and 0 otherwise).",3.2. Practical aspects,[0],[0]
"Towards this goal, we refer the readers to (Di Napoli et al., 2016; Paratte and Martin, 2016) and their respective eigencount techniques that allow to approximate the filter in O(cm log((λk+1 − λk)−1)) operations.",3.2. Practical aspects,[0],[0]
"In this section, we consider the problem of spectral clustering a sequence of graphs.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"We focus on graphs Gt where t ∈ {1, . . .",4. Compressive Clustering of Dynamic Graphs,[0],[0]
", τ}, composed of a static node set V and evolving edge sets Et.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"Identifying each assignment from scratch (using SC or CSC) is a computationally demanding task, as the complexity increases linearly with the number of time-steps.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"However, when consecutive graphs are “appropriately similar”, we should be able to cut on this cost by reusing information.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"We will utilize two alternative similarity measures:
Definition 4.1 (Measures of graph similarity).",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"Two graphs Gt−1 and Gt are:
• (ρ, k)-spectrally similar if the spaces spanned by their first k eigenvectors are almost aligned: ‖Ht",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"− Ht−1‖F ≤ ρ.
• ρ-edge similar if the edge-wise difference of their Laplacians is bounded: ‖Lt − Lt−1‖F ≤ ρ.
",4. Compressive Clustering of Dynamic Graphs,[0],[0]
Both measures are relevant in the context of dynamic clustering.,4. Compressive Clustering of Dynamic Graphs,[0],[0]
"Two spectrally similar graphs might have different connectivity, but possess similar clustering assignments.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
"On the other hand, assuming that two graphs are edge similar is a stronger condition that postulates fine-grained similarities between them.",4. Compressive Clustering of Dynamic Graphs,[0],[0]
It is however more intuitive and computationally economical to ascertain (see Section 4.3).,4. Compressive Clustering of Dynamic Graphs,[0],[0]
We now present an accelerated method for spectral clustering an evolving network.,4.1. Algorithm,[0],[0]
"Without loss of generality, suppose that we need to compute the assignment for Gt while knowing already that of Gt−1 and possessing the features Ψt−1 used to compute it.
",4.1. Algorithm,[0],[0]
Component 1.,4.1. Algorithm,[0],[0]
We reuse a portion of features Ψt−1 to cluster Gt.,4.1. Algorithm,[0],[0]
"Let p be a number between 0 and 0.5, and set q = 1 − p.2",4.1. Algorithm,[0],[0]
"Instead of recomputing Ψt from scratch running a new CSC routine, we construct a feature matrix Θt which consists of dq new features (corresponding to Gt)
2Although in practice p can go up to 1, the analysis only considers the case when reused features are only from Gt−1 (and not from earlier graphs).
",4.1. Algorithm,[0],[0]
"Algorithm 1 Dynamic CSC Input: (G1,G2, . . .",4.1. Algorithm,[0],[0]
",Gτ ), p, d Output: (X1,X2, . . .",4.1. Algorithm,[0],[0]
",Xτ )
1: Determine λk and filter h1 for G1.",4.1. Algorithm,[0],[0]
2: Find X1 for G1 using CSC with Ψ1 = h1(L1)R. 3: for t from 2 to τ do 4: Select dp features from Ψt−1 and call them Ψ (1) t .,4.1. Algorithm,[0],[0]
5: Generate d(1−p) features Ψ(2)t by filtering as many random vectors by ht−1(Lt).,4.1. Algorithm,[0],[0]
6: Test whether λk ∈,4.1. Algorithm,[0],[0]
"[λk(Lt), λk+1(Lt)] with eigencount and using Ψ(2)t .",4.1. Algorithm,[0],[0]
7: if the test fails then 8: Determine λk for Gt using eigencount.,4.1. Algorithm,[0],[0]
9: Update ht and recompute Ψ (2) t based on ht(Lt).,4.1. Algorithm,[0],[0]
"10: end if 11: Find assignment Xt by applying the compressive k-
means to features Θt = [Ψ (1) t ,Ψ (2) t ].
12: Set Ψt = Ψ (2) t .",4.1. Algorithm,[0],[0]
"13: end for
and dp randomly selected features of Gt−1:
Θt =",4.1. Algorithm,[0],[0]
"[Ht−1Rdp, HtRdq] = Ψt−1S d dp + ΨtS d dp (12)
",4.1. Algorithm,[0],[0]
"Above, we use the sub-identity matrix Sddp = Id×dpIdp×d and its complement Sddp = Id×d − Sddp.",4.1. Algorithm,[0],[0]
Component 2.,4.1. Algorithm,[0],[0]
An important part of the complexity of CSC stems from using the eigencount algorithm to estimate λk and construct the Chebyshev polynomials (step 1 of their algorithm).,4.1. Algorithm,[0],[0]
"To avoid recomputing λk, we start by assuming that the estimated value for λk at t − 1 is a also good candidate for t and proceed to use the same polynomial in order to filter the qd new random vectors Rdq in Gt.",4.1. Algorithm,[0],[0]
Notice that the eigencount method requires exactly these new features to determine if λk was correctly estimated and thus to validate our assumption.,4.1. Algorithm,[0],[0]
"If our assumption is invalid, i.e., the λk has changed from t − 1 to t, then we rerun the eigencount method from scratch as in (Di Napoli et al., 2016) but providing λk as an initial estimate.",4.1. Algorithm,[0],[0]
The final set of features generated in the eigencount now serves as Ψt.,4.1. Algorithm,[0],[0]
"When the assumption is valid, we proceed as is.
",4.1. Algorithm,[0],[0]
Complexity analysis.,4.1. Algorithm,[0],[0]
There are two steps where the complexity is reduced with respect to CSC.,4.1. Algorithm,[0],[0]
"First, the optimization proposed for the determination of λk avoids computing steps of dichotomy for every graph.",4.1. Algorithm,[0],[0]
Spectrally similar graphs generally possess close spectrum and close values for λk.,4.1. Algorithm,[0],[0]
"One could then expect to recompute λk only intermittently, in which cases he/she would also benefit from a reduced number of iterations due to a good initialization3.",4.1. Algorithm,[0],[0]
"If S are the total number of eigencount steps gained,
3Though this trend has been confirmed by our numerical experiments, a formal proof remains elusive.
",4.1. Algorithm,[0],[0]
the total gain is O(Scm).,4.1. Algorithm,[0],[0]
"Second, since we reuse random features from one graph to the next, the total number of computed random vectors will necessarily be reduced compared to the use of τ independent CSC calls.",4.1. Algorithm,[0],[0]
"The gain here is O(cmdp) per time-step.
",4.1. Algorithm,[0],[0]
All reductions applied through compression can also benefit to our dynamic method.,4.1. Algorithm,[0],[0]
"Indeed, we theoretically showed that reusing features from the past can replace the creation of new random vectors.",4.1. Algorithm,[0],[0]
"Thus, sampling the combination of old and new vectors can be applied exactly as defined in CSC.",4.1. Algorithm,[0],[0]
"Then, the result of the sub-assignment can be interpolated also as defined in (Tremblay et al., 2016).",4.1. Algorithm,[0],[0]
"Similarly to the static case, our objective is to provide probabilistic guarantees about the approximation quality of the proposed method.",4.2. Analysis of dynamic CSC,[0],[0]
"Let
XΘt = arg min X∈X
‖Θt −XX>Θt‖F .",4.2. Analysis of dynamic CSC,[0],[0]
"(13)
be the clustering assignment obtained from k-means with Θt as features, and define the dynamic CSC cost CΘt as
CΘt",4.2. Analysis of dynamic CSC,[0],[0]
= ‖Φ−XΘtX>ΘtΦ‖F .,4.2. Analysis of dynamic CSC,[0],[0]
"(14)
As the following theorem claims, the graph evolution introduces an additional error term that is a function of the graph similarity (spectral- or edge- wise).
",4.2. Analysis of dynamic CSC,[0],[0]
Theorem 4.1.,4.2. Analysis of dynamic CSC,[0],[0]
"At time t, the dynamic CSC cost CΘt and the SC cost CΦt are related by
CΦt ≤",4.2. Analysis of dynamic CSC,[0],[0]
CΘt ≤ CΦt,4.2. Analysis of dynamic CSC,[0],[0]
+ 2 √ k d ( √ k + ε) + (1 + δ)p,4.2. Analysis of dynamic CSC,[0],[0]
"γ, (15)
with probability at least 1− exp ( −ε 2
2
)",4.2. Analysis of dynamic CSC,[0],[0]
"− exp ( 2 log(n)− dp ( δ2
4 − δ
3
6
)) ,
where 0 < δ ≤ 1.",4.2. Analysis of dynamic CSC,[0],[0]
"Above, γ depends only on the similarity of the graphs in question.",4.2. Analysis of dynamic CSC,[0],[0]
"If graphs Gt−1 and Gt are
• (ρ, k)-spectrally similar, then γ = ρ, • ρ-edge similar, then γ = ( √
2 ρ)/α, where α = min{λtk, λ (t−1) k+1 − λtk} is the Laplacian eigen-gap.
",4.2. Analysis of dynamic CSC,[0],[0]
Proof.,4.2. Analysis of dynamic CSC,[0],[0]
"Let XΦt and XΘt be respectively the optimal SC and dCSC clustering assignments at time t, and denote E = Θt −ΦtIk×dQ.",4.2. Analysis of dynamic CSC,[0],[0]
"We have that,
CΘt ≤ CΦt",4.2. Analysis of dynamic CSC,[0],[0]
"+ 2‖Θt −ΦtIk×dQ‖F , (16)
",4.2. Analysis of dynamic CSC,[0],[0]
following the exact same steps as in the proof of Lemma 3.1.,4.2. Analysis of dynamic CSC,[0],[0]
"By completing the matrices containing the filtering of both graphs, we can see that the error term can be
rewritten as
‖E‖F = ‖Ψt−1Sddp + ΨtSddp −ΦtIk×dQ‖F (17)
= ‖(Ψt−1 −Ψt)Sddp +",4.2. Analysis of dynamic CSC,[0],[0]
Ψt −ΦtIk×dQ‖F ≤,4.2. Analysis of dynamic CSC,[0],[0]
"‖(Ψt −Ψt−1)Sddp‖F + ‖Ψt −ΦtIk×dQ‖F .
",4.2. Analysis of dynamic CSC,[0],[0]
The rightmost term of eq.,4.2. Analysis of dynamic CSC,[0],[0]
(17) corresponds to the effects of random filtering and has been studied in depth in Thm. 3.2 and Cor. 3.2.,4.2. Analysis of dynamic CSC,[0],[0]
"The rest of the proof is devoted to studying the leftmost term.
",4.2. Analysis of dynamic CSC,[0],[0]
"We apply the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984) on the term of interest.",4.2. Analysis of dynamic CSC,[0],[0]
"Setting R′ = 1√pRId×dp, we have that
‖(Ψt −Ψt−1)Sddp‖2F = ‖(Ht −Ht−1)RId×dp‖2F
= p n∑ i=1",4.2. Analysis of dynamic CSC,[0],[0]
‖R′>,4.2. Analysis of dynamic CSC,[0],[0]
"(Ht −Ht−1)>δi‖22.
",4.2. Analysis of dynamic CSC,[0],[0]
Matrix R′ = p−1/2RId×dp has n × dp Gaussian i.i.d. entries with zero-mean and variance 1/dp.,4.2. Analysis of dynamic CSC,[0],[0]
It follows from the Johnson-Lindenstrauss lemma that ‖(Ψt −Ψt−1)Sddp‖2F ≤ p (1 + δ) n∑ i=1,4.2. Analysis of dynamic CSC,[0],[0]
"‖(Ht −Ht−1)>δi‖22
",4.2. Analysis of dynamic CSC,[0],[0]
"≤ p (1 + δ)‖Ht −Ht−1‖2F ,
with probability at least 1 − n−β and for dp ≥ 4+2β δ2( 12− δ 3 )",4.2. Analysis of dynamic CSC,[0],[0]
log(n).,4.2. Analysis of dynamic CSC,[0],[0]
"Coupling the two together we obtain a probability at least equal to 1−exp(2 log(n)−dpδ 2
2 ( 1 2− δ 3 )),
where δ can be set between 0 and 1.",4.2. Analysis of dynamic CSC,[0],[0]
A loose bound gives 2p‖H(2)−H(1)‖2F with probability 1−exp(2,4.2. Analysis of dynamic CSC,[0],[0]
"log(n)− dp 12 ).
",4.2. Analysis of dynamic CSC,[0],[0]
This concludes the part of the proof concerning spectrally similar graphs.,4.2. Analysis of dynamic CSC,[0],[0]
The result for edge-wise similarity follows from Cor.,4.2. Analysis of dynamic CSC,[0],[0]
5.1 found in the appendix.,4.2. Analysis of dynamic CSC,[0],[0]
Theorem 4.1 can be used to adaptively determine p at each time-step with the objective of attaining a bounded error for the term (1+δ)pρ.,4.3. Controlling the approximation error,[0],[0]
"For instance, if the graph did not evolve much between the last two time-steps, more signals should be reused than otherwise.",4.3. Controlling the approximation error,[0],[0]
"For that, one needs to be able to approximate the graph similarity, without computing the spectral basis.
",4.3. Controlling the approximation error,[0],[0]
"This can be quite easily achieved for the edge similarity measure, by simply computing the Frobenius norm of the edge weight difference between the two graph Laplacian matrices.",4.3. Controlling the approximation error,[0],[0]
"As we show next, the spectral similarity measure ρ can also be estimated as follows:
ρ̂ := ‖HtR−Ht−1R‖F (18)
",4.3. Controlling the approximation error,[0],[0]
"Algorithm 2 Dynamic CSC with adaptive p Input: (G1,G2, . . .",4.3. Controlling the approximation error,[0],[0]
",Gτ ), d, k, δ, Output: (X1,X2, . . .",4.3. Controlling the approximation error,[0],[0]
",Xτ )
1: Determine λk and filter h1 for G1.",4.3. Controlling the approximation error,[0],[0]
2: Find X1 for G1 using CSC with Ψ1 = h1(L1)R. 3: for t from 2 to τ do 4: Split features Ψt−1 =,4.3. Controlling the approximation error,[0],[0]
"[Ψ (1) t−1,Ψ (2) t−1] s.t. Ψ (1) t−1 con-
tains d2 vectors.",4.3. Controlling the approximation error,[0],[0]
"Let R (1) be random vectors used to
generate Ψ(1)t−1.",4.3. Controlling the approximation error,[0],[0]
"5: Set ht = ht−1 and compute Ψ (1) t = ht(Lt)R
(1).",4.3. Controlling the approximation error,[0],[0]
6: Test whether λk ∈,4.3. Controlling the approximation error,[0],[0]
"[λk(Lt), λk+1(Lt)] with eigencount and using Ψ(1)t .",4.3. Controlling the approximation error,[0],[0]
7: if the test fails then 8: Determine λk for Gt using eigencount.,4.3. Controlling the approximation error,[0],[0]
"9: Update ht and recompute Ψ (1) t = ht(Lt)R
(1).",4.3. Controlling the approximation error,[0],[0]
"10: end if 11: Set p = min ( 1 2 , ε2 1+δ‖Ψ (1) t −Ψ (1) t−1‖ −1 F ) .",4.3. Controlling the approximation error,[0],[0]
12: Select dp features from Ψ(2)t−1 and call them Ψ (2) t .,4.3. Controlling the approximation error,[0],[0]
"13: Generate d( 12 − p) features Ψ (3) t = ht(Lt)R
(3), where R(3) are new random vectors.
14: Find assignment Xt by applying the compressive kmeans to features Θt = [Ψ (1) t ,Ψ (2) t ,Ψ (3) t ].",4.3. Controlling the approximation error,[0],[0]
"15: Set Ψt = [Ψ (1) t ,Ψ (3) t ].",4.3. Controlling the approximation error,[0],[0]
"16: end for
To motivate this, observe that ρ̃2 is an unbiased estimator:
",4.3. Controlling the approximation error,[0],[0]
E,4.3. Controlling the approximation error,[0],[0]
[ ρ̂2 ] = E [ tr ( R> (Ht −Ht−1)>,4.3. Controlling the approximation error,[0],[0]
(Ht −Ht−1) R )],4.3. Controlling the approximation error,[0],[0]
"= tr ( (Ht −Ht−1)E [ RR> ] (Ht −Ht−1)>
) = tr ( (Ht −Ht−1) (Ht −Ht−1)> )",4.3. Controlling the approximation error,[0],[0]
"= ρ2, (19)
which implies that ρ̃ approaches ρ as d grows.
",4.3. Controlling the approximation error,[0],[0]
"With this in place, we proceed to modify Alg.",4.3. Controlling the approximation error,[0],[0]
4.1 so as to include the estimation of ρ and the adaptive estimation of p.,4.3. Controlling the approximation error,[0],[0]
"The detailed procedure is summarized in Alg. 2, focusing on the case of spectral similarity (i.e., γ = ρ).",4.3. Controlling the approximation error,[0],[0]
"Since Thm. 4.1 proved an additive error of at most (1 + δ)pρ, we fix an upper bound on the error that we tolerate ε2 and δ that controls the probability of success, and set p = ε2(1+δ)ρ̂ .
",4.3. Controlling the approximation error,[0],[0]
"Though the adaptive algorithm features the same complexity, it is slightly more involved than Alg. 4.1.",4.3. Controlling the approximation error,[0],[0]
"The main difference is that ρ is estimated based on features Ψ(1)t and Ψ(1)t−1 that correspond to the same random vectors R(1) ∈ Rn×d filtered on two consecutive graphs (i.e., Gt and Gt−1).",4.3. Controlling the approximation error,[0],[0]
Features Ψ(1)t are combined with the pd reused features Ψ(2)t and the d(1/2−p) new features Ψ (3) t to identify assignment Xt.,4.3. Controlling the approximation error,[0],[0]
This section complements the theoretical results described in Section 4.,5. Experiments,[0],[0]
"All our experiments are designed using the GSPBox (Perraudin et al., 2014).",5. Experiments,[0],[0]
"As is common practice, we use the Stochastic Block Model (SBM) to evaluate the efficiency of our spectral clustering method (e.g., Görke et al., 2013; Tremblay et al., 2016).",5.1. Experimental setup,[0],[0]
"In SBM, data are clustered in k classes and the n nodes are connected at random with edge-wise probability that depends if the two extremities belong to the same cluster (q1) or not (q2 with q2 q1).",5.1. Experimental setup,[0],[0]
"In the following, we qualify the SBM parameters in terms of the nodes’ average degree δ̄ and the ratio q2/q1 that captures the graph clusterability (Decelle et al., 2011).",5.1. Experimental setup,[0],[0]
"Following the recommendations of the former work, we set q2q1 = δ̄− √ δ̄ 2(δ̄+ √ δ̄(k−1))
to construct non-trivially clusterable graphs.
",5.1. Experimental setup,[0],[0]
"We compare the quality and complexity of our dynamic method (dCSC) against the algorithm of Tremblay et al. (CSC) and an optimized spectral clustering (Ng et al., 2002) that uses the Lanczos algorithm to compute the first k-eigenvectors (this is significantly faster than doing the entire eigendecomposition while introducing negligible error).",5.1. Experimental setup,[0],[0]
"We use relative error measures to compare the achieved clustering accuracy of CSC and dCSC with that of SC (i.e., |CA",5.1. Experimental setup,[0],[0]
"− CSC |/CSC , where CA is the cost of algorithm A and CSC the cost of SC).",5.1. Experimental setup,[0],[0]
We considered two cost measures: the k-means cost (eq. (6)) and the normalized cut (ncut) cost.,5.1. Experimental setup,[0],[0]
"Since the obtained results were almost identical, we only report the results for ncut in the rest of this section (except for Table 1).",5.1. Experimental setup,[0],[0]
"After all, the k-means cost of the spectral features is a relaxation of the ncut cost.
",5.1. Experimental setup,[0],[0]
Our analysis highlights the importance of the spectral similarity between consecutive graphs.,5.1. Experimental setup,[0],[0]
It is thus important to define how the graph changes between consecutive steps.,5.1. Experimental setup,[0],[0]
"Starting from a SBM, we perform two types of perturbations: edge redrawing and node reassignment.",5.1. Experimental setup,[0],[0]
Edge redrawing consists of removing some edges at random from the original graph and then adding the same number following the probabilities defined by the graph model (using q1 and q2).,5.1. Experimental setup,[0],[0]
"In node reassignment, one selects nodes, removes all edges that share at least one end with the nodes previously picked, reassigns those nodes to any other class at random and reconnects these nodes with new edges using again the same probabilities q1 and q2.",5.1. Experimental setup,[0],[0]
Both perturbations are combined in the synthetic graph that we are studying.,5.1. Experimental setup,[0],[0]
"We replicate the construction of 100 different SBM with the same parameters, then we alter each with 1% of node reassignment and 1% of edges modifications.",5.1. Experimental setup,[0],[0]
The modified graph is used for the evaluation of all methods.,5.1. Experimental setup,[0],[0]
"We first study the error-complexity trade-off achieved by the compressive clustering methods as a function of d. We set n = 15000, k = 25, δ̄ = 60.",5.2. When does reusing features pay off?,[0],[0]
Each point in Figure 1 corresponds to a single graph being clustered.,5.2. When does reusing features pay off?,[0],[0]
"For each of the two methods, there are 1600 points resulting from 100 repetitions when the number of features is d ∈",5.2. When does reusing features pay off?,[0],[0]
"[6, 200] with logarithmic increments.",5.2. When does reusing features pay off?,[0],[0]
"To comprehend the results, it is helpful to consider each of the six sextants in the figure separately.",5.2. When does reusing features pay off?,[0],[0]
"The top-middle sextant shows that when d is large enough (left side), the relative error of CSC and dCSC is close to zero.",5.2. When does reusing features pay off?,[0],[0]
"Increasing d reduces the error but increases the time required for the computation, following the elbow from right to left.",5.2. When does reusing features pay off?,[0],[0]
The top-right and top-left sextants occur because Lloyd’s algorithm (despite being rerun 100 times) sometimes fails to retrieve the optimal solution to the k-means problem: the top-left (resp.,5.2. When does reusing features pay off?,[0],[0]
right) sextant corresponds to cases when Lloyd’s algorithm produces a suboptimal assignment for SC (resp.,5.2. When does reusing features pay off?,[0],[0]
CSC/dCSC).,5.2. When does reusing features pay off?,[0],[0]
"The bot-
tom three sextants correspond to cases when dCSC did not have to recompute λk (step 7 of Alg. 4.1).",5.2. When does reusing features pay off?,[0],[0]
"In these cases, dCSC is up to 2× faster than CSC.",5.2. When does reusing features pay off?,[0],[0]
"Though the frequency of this phenomenon depends on many factors, such as the size of the eigengap and the spectral similarity of consecutive graphs, we report that in our experiment dCSC could avoid recomputing λk, roughly 50% of the times.
",5.2. When does reusing features pay off?,[0],[0]
"In summary, reusing features produces a clear computational benefit with a reasonable loss of accuracy.",5.2. When does reusing features pay off?,[0],[0]
"Most benefit comes from λk estimation (component 2) that can be often avoided when consecutive graphs are spectrally similar, especially for well-clusterable graphs (where the gap λk+1",5.2. When does reusing features pay off?,[0],[0]
− λk is large).,5.2. When does reusing features pay off?,[0],[0]
"To quantify the benefit of reusing a portion of features (component 1), we compare here the execution time of CSC and dCSC, excluding the time for λk estimation.",5.2. When does reusing features pay off?,[0],[0]
Increasing p by 0.25 saved 0.97 and 9.98 seconds respectively when n = 10′000 and 50′000—the later corresponds to a speedup of 1.29x w.r.t.,5.2. When does reusing features pay off?,[0],[0]
feature estimation.,5.2. When does reusing features pay off?,[0],[0]
"To evaluate the efficiency of dCSC, we varied the number of nodes n (while fixing k = 25, d = 50, δ̄ = 60).",5.3. Comparison with state-of-the-art,[0],[0]
Figure 2 shows the results.,5.3. Comparison with state-of-the-art,[0],[0]
"As expected, the difference of complexity between spectral clustering using the partial eigen-decomposition and dCSC is clearly visible.",5.3. Comparison with state-of-the-art,[0],[0]
"Increasing p from 0.25 to 0.5 incurs a non-negligible computational benefit for larger n (14.5 seconds when n =30’000, corresponding to a 12% improvement).",5.3. Comparison with state-of-the-art,[0],[0]
We also report that the achieved relative error for both methods remained consistently below 0.1% and did not grow as n increased.,5.3. Comparison with state-of-the-art,[0],[0]
"We do not present values of n above 30’000 as, for such cases, SC took too long to complete.",5.3. Comparison with state-of-the-art,[0],[0]
"For example, with 64Gb of RAM, SC took one hour to process the graph and return an assignment when n = 50′000.",5.3. Comparison with state-of-the-art,[0],[0]
"For the same graph, dCSC run in 6.5 minutes and resulted in a similar k-means cost.
",5.3. Comparison with state-of-the-art,[0],[0]
"Table 1 further compares our proposed method to SC, CSC and IASC, the state-of-the-art method for spectral clustering suitable for dynamic graphs (Dhanjal et al., 2014).",5.3. Comparison with state-of-the-art,[0],[0]
"Note that there is a long list of heuristic-based clustering algo-
rithms optimized for speed (Dhillon et al., 2007; Karypis and Kumar, 1998), but we only consider here algorithms that provably approximate spectral clustering.",5.3. Comparison with state-of-the-art,[0],[0]
We can see that dCSC achieves a significant improvement in timing when n is large enough.,5.3. Comparison with state-of-the-art,[0],[0]
Note that IASC results were obtained by running the optimized and parallel implementation kindly provided by the original authors4.,5.3. Comparison with state-of-the-art,[0],[0]
"Our hypothesis is that the poor complexity of IASC is attributed to the fact that, in our tests (and as is frequently the case) the eigengap was not particularly large.",5.3. Comparison with state-of-the-art,[0],[0]
The major contribution of this paper has been the presentation of a clustering algorithm for dynamic graphs that achieves solution quality provably approximating that of Spectral Clustering.,6. Conclusion and Future Work,[0],[0]
"Numerical experiments suggest that our method is faster than previous approximation methods.
",6. Conclusion and Future Work,[0],[0]
"Recent advances in spectral clustering, including this work, can provide a huge complexity gain.",6. Conclusion and Future Work,[0],[0]
"Nevertheless, practitioners must pay attention because these works require a proper setup.",6. Conclusion and Future Work,[0],[0]
"In particular, n must be large for the approximated method to make sense.",6. Conclusion and Future Work,[0],[0]
"Moreover, when working with dynamic networks, the spectral similarity should remain bounded for our algorithm to perform best.
",6. Conclusion and Future Work,[0],[0]
We highlight in this paper two open directions of research.,6. Conclusion and Future Work,[0],[0]
It appears clearly in the experiments that the majority of the remaining complexity lies in the estimation of H and more precisely the capability to reuse the previous determination of λk.,6. Conclusion and Future Work,[0],[0]
"Finally, expressing the approximation error in function of the assignment instead of ncut could produce a more insightful explaination of the impact of the various factors.",6. Conclusion and Future Work,[0],[0]
We would like to acknowledge the reviewers for their valuable comments.,Acknowledgements,[0],[0]
"Their suggestions helped us clarify and improve our work.
",Acknowledgements,[0],[0]
4Available at https://github.com/charanpald/sandbox,Acknowledgements,[0],[0]
"Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size.",abstractText,[0],[0]
We claim that it is possible to reuse information of past cluster assignments to expedite computation.,abstractText,[0],[0]
"Our approach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., computing the graph eigenvectors, by a polynomialbased randomized sketching technique.",abstractText,[0],[0]
We show that the proposed algorithm achieves clustering assignments with quality approximating that of spectral clustering and that it can yield significant complexity benefits when the graph dynamics are appropriately bounded.,abstractText,[0],[0]
"In our experiments, our method clusters 30k node graphs 3.9× faster in average and deviates from the correct assignment by less than 0.1%.",abstractText,[0],[0]
Fast Approximate Spectral Clustering for Dynamic Networks,title,[0],[0]
The Poisson process is an important model for point data in which samples of the process are locally finite subsets of some domain such as time or space.,1. Introduction,[0],[0]
"The process is parametrised by an intensity function, the integral of which gives the expected number of points in the domain of integration — for a gentle introduction we recommend (Baddeley, 2007).",1. Introduction,[0],[0]
"In the typical case of unknown intensity function we may place a non-parametric prior over it via e.g. the Gaussian Process (GP) and perform Bayesian inference.
",1. Introduction,[0],[0]
"Inference under such models is challenging due to both the GP prior and the non factorial nature of the Poisson process likelihood (1), which includes an integral of the intensity function.",1. Introduction,[0],[0]
"One may resort to discretising the domain (Rathbun & Cressie, 1994; Møller et al., 1998; Rue et al., 2009) or performing Monte Carlo approximations (Adams et al., 2009; Diggle et al., 2013).",1. Introduction,[0],[0]
"Fast Laplace approximates were studied in (Cunningham et al., 2008; Illian et al., 2012; Flaxman et al., 2015) and variational methods were applied
1Data61, CSIRO, Australia 2The Australian National University 3University of Technology Sydney.",1. Introduction,[0],[0]
"Correspondence to: Christian <christian.walder@anu.edu.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
in (Lloyd et al., 2015; Kom Samo & Roberts, 2015).
",1. Introduction,[0],[0]
To satisfy non-negativity of the intensity function one transforms the GP prior.,1. Introduction,[0],[0]
"The log-Gaussian Cox Process, with GP distributed log intensity, has been the subject of much study; see e.g. (Rathbun & Cressie, 1994; Møller et al., 1998; Illian et al., 2012; Diggle et al., 2013), Alternative formulations for introducing a GP prior exist, e.g. (Adams et al., 2009).",1. Introduction,[0],[0]
"More recent research has highlighted the analytical and computational advantages (Lloyd et al., 2015; 2016; Flaxman et al., 2017; Møller et al., 1998) of the permanental process, which has GP distributed square root intensity (Shirai & Takahashi, 2003; McCullagh & Møller, 2006) — we discuss the relationship between these methods and the present work in more detail in subsection 2.2.
",1. Introduction,[0],[0]
"In section 2 we introduce the Poisson and permanental processes, and place our work in the context of existing literature.",1. Introduction,[0],[0]
"Section 3 reviews Flaxman et al. (2017), slightly recasting it as regularised maximum likelihood for the permanental process.",1. Introduction,[0],[0]
Our Bayesian scheme is then derived in section 4.,1. Introduction,[0],[0]
"In section 5 we discuss the choice of covariance function for the GP prior, before presenting some numerical experiments in section 6 and concluding in section 7.",1. Introduction,[0],[0]
"We view the inhomogeneous Poisson process on Ω as a distribution over locally finite subsets of Ω. The number N(X ) of elements in some X ⊆ Ω is assumed to be distributed as Poisson(Λ(X , µ)), where Λ(S, µ) :=∫ x∈S λ(x)dµ(x) gives the mean of the Poisson distribution.",2.1. The Poisson Process,[0],[0]
"It turns out that this implies the likelihood function
p ({xi}mi=1 |λ,Ω) =",2.1. The Poisson Process,[0],[0]
m∏ i=1,2.1. The Poisson Process,[0],[0]
λ(xi),2.1. The Poisson Process,[0],[0]
exp (−Λ(Ω)),2.1. The Poisson Process,[0],[0]
.,2.1. The Poisson Process,[0],[0]
(1),2.1. The Poisson Process,[0],[0]
"To model unknown λ(x), we employ a non-parametric prior over functions, namely the Gaussian process (GP).",2.2. Latent Gaussian Process Intensities,[0],[0]
"To ensure that λ is non-negative valued we include a deterministic “link” function g : R → R+ so that we have the prior over λ defined by λ = g ◦ f and f ∼ GP(k), where k is the covariance function for f .",2.2. Latent Gaussian Process Intensities,[0],[0]
"The most com-
mon choice for g is the exponential function exp(·), leading to the log-Gaussian Cox process (LGCP) (Møller et al., 1998).",2.2. Latent Gaussian Process Intensities,[0],[0]
Recently Adams et al. (2009) employed the transformation g(z),2.2. Latent Gaussian Process Intensities,[0],[0]
= λ∗(1,2.2. Latent Gaussian Process Intensities,[0],[0]
"+ exp(−z))−1 , which permits efficient sampling via thinning (Lewis & Shedler, 1979) due to the bound 0 ≤ λ(x) ≤",2.2. Latent Gaussian Process Intensities,[0],[0]
λ∗.,2.2. Latent Gaussian Process Intensities,[0],[0]
"In this paper we focus on the choice g(z) = 12z 2, known as the permanental process (Shirai & Takahashi, 2003; McCullagh & Møller, 2006).",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"Two recent papers have demonstrated the analytical and computational advantages of this link function.
1.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"Flaxman et al. (2017) derived a non-probabilistic regularisation based algorithm which we review in section 3, and which exploited properties of reproducing kernel Hilbert spaces.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"The present work generalises their result, providing probabilistic predictions and Bayesian model selection.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"Our derivation is by necessity entirely different to Flaxman et al. (2017), as their representer theorem (Schölkopf et al., 2001) argument is insufficient for our probabilistic setting (see e.g. subsubsection 4.1.6).
2.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"(Lloyd et al., 2015) derived a variational approximation to a Bayesian model with the squared link function, based on an inducing variable scheme similar to (Titsias, 2009), and exploiting the tractability of certain required integrals.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"The present work has the advantage of 1) not requiring the inducing point approximation, 2) being free of non-closed form expressions such as their G̃ and 3) being simpler to implement and orders of magnitude faster in practice while, as we demonstrate, exhibiting comparable predictive accuracy.",2.2.1. PERMANENTAL PROCESSES: SQUARED LINK FUNCTION,[0],[0]
"Flaxman et al. (2017) combined (1) with the regularisation term ‖f‖2H(k), leading to the regularised maximum likelihood estimator for f , namely f̂ :=
argmax f m∑ i=1 log 1 2 f2(xi)− 1 2 ( ‖f‖2L2(Ω,µ) + ‖f‖ 2 H(k) )",3. Regularised Maximum Likelihood,[0],[0]
"︸ ︷︷ ︸
:=‖f‖2H(k,Ω,µ)
,
(2)
where we have implicitly defined the new RKHS H(k,Ω, µ) := H(k̃).",3. Regularised Maximum Likelihood,[0],[0]
"Now, provided we can compute the associated new reproducing kernel k̃, then we may appeal to the representer theorem (Kimeldorf & Wahba, 1971) in order to compute the f̂ , which takes the form
∑m i=1 αik̃(xi, ·) for some αi.",3. Regularised Maximum Likelihood,[0],[0]
"The function k̃ may be expressed in terms of the Mercer expansion (Mercer, 1909)
k(x,y) = N∑ i=1 λiφi(x)φi(y), (3)
where φi are orthonormal in L2(Ω, µ).",3. Regularised Maximum Likelihood,[0],[0]
"To satisfy for arbitrary f = ∑ i wiφi the reproducing property (Aronszajn, 1950)〈 k(x, ·),
∑ i wiφ(·)i 〉 H(k)",3. Regularised Maximum Likelihood,[0],[0]
":= f(x) = ∑ i wi, φi(x) (4)
we let φi be orthogonal in H(k), obtaining 〈φi, φj〉 = δijλ −1",3. Regularised Maximum Likelihood,[0],[0]
i .,3. Regularised Maximum Likelihood,[0],[0]
"Hence, ‖ ∑ i wiφi‖ 2 H(k)",3. Regularised Maximum Likelihood,[0],[0]
= ∑,3. Regularised Maximum Likelihood,[0],[0]
"i w 2 i /λi, and from
(2) we have ‖ ∑ i wiφi‖ 2 H(k,Ω,µ) =",3. Regularised Maximum Likelihood,[0],[0]
"∑ i w 2 i (1 + λ −1 i ), so
k̃(x,y) = N∑ i=1
1
1 + λ−1i φi(x)φi(y).",3. Regularised Maximum Likelihood,[0],[0]
"(5)
For approximate Bayesian inference however, we cannot simply appeal to the representer theorem.",3. Regularised Maximum Likelihood,[0],[0]
"In subsection A.3 of the supplementary material, we review the standard Laplace approximation to the GP with non-Gaussian likelihood.",4. Approximate Bayesian Inference,[0],[0]
"This a useful set-up for what follows, but is not directly generalisable to our case due to the integral in (1).",4. Approximate Bayesian Inference,[0],[0]
"Instead, in subsection 4.1 we now take a different approach based on the Mercer expansion.",4. Approximate Bayesian Inference,[0],[0]
"It is tempting to naı̈vely substitute k̃ into subsection A.3 of the supplementary material, and to neglect the integral part of the likelihood.",4.1. Laplace Approximation,[0],[0]
"Indeed, this gives the correct approximate predictive distribution.",4.1. Laplace Approximation,[0],[0]
The marginal likelihood does not work in this way however (due to the log determinant in (18)).,4.1. Laplace Approximation,[0],[0]
We now perform a more direct analysis.,4.1. Laplace Approximation,[0],[0]
"Mercer’s theorem allows us to write (3), where for nondegenerate kernels, N = ∞. Assume a linear model in Φ(x) =",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"(φi(x))i so that1
f(x) = w>Φ(x), (6)
and let w ∼ N (0,Λ) where Λ = (λi)ii is a diagonal covariance matrix.",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"This is equivalent to f ∼ GP(k) because
cov(f(x), f(z))",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"= Φ(x)>Λ Φ(z) = k(x, z).
",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"1We use a sloppy notation where (x)i is the i-th element of x while (xi)i is a vector with i-th element xi, etc.
",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"Recall that the Poisson process on Ω with intensity λ(x) = 1 2f 2(x) has likelihood for X := {xi}mi=1
log p(X|w,Ω, k) = m∑ i=1 log 1
2 f2(xi)︸ ︷︷ ︸
:=log h(X|w)
−1",4.1.1. MERCER EXPANSION SETUP,[0],[0]
2 ∫,4.1.1. MERCER EXPANSION SETUP,[0],[0]
"x∈Ω
f2(x)dµ(x)︸ ︷︷ ︸ w>w
The joint in w, X is
log p(w, X|Ω, k)
= log h(X|w)−1 2 w>(I+Λ−1)w−1 2 log |Λ|−N 2 log 2π.",4.1.1. MERCER EXPANSION SETUP,[0],[0]
"We make a Laplace approximation to the posterior, which is the normal distribution
log p(w|X,Ω, k)",4.1.2. LAPLACE APPROXIMATION,[0],[0]
"≈ logN (w|ŵ, Q) (7)
",4.1.2. LAPLACE APPROXIMATION,[0],[0]
= −1 2 (w −,4.1.2. LAPLACE APPROXIMATION,[0],[0]
ŵ)>Q−1(w,4.1.2. LAPLACE APPROXIMATION,[0],[0]
"− ŵ)− 1 2 log |Q| − N 2 log 2π
:= log q(w|X,Ω, k),
where ŵ is chosen as the mode of the true posterior, and Q is the inverse Hessian of the true posterior, evaluated at ŵ.",4.1.2. LAPLACE APPROXIMATION,[0],[0]
"The mode ŵ is
ŵ = argmax w
log p(w|X,Ω, k)
= argmax w log h(X|w)− 1 2 w>(I + Λ−1)w.",4.1.3. PREDICTIVE MEAN,[0],[0]
"(8)
Crucially, ŵ must satisfy the stationarity condition
ŵ =",4.1.3. PREDICTIVE MEAN,[0],[0]
"(I + Λ−1)−1 ∇w log h(X|w)|w=ŵ , (9)
",4.1.3. PREDICTIVE MEAN,[0],[0]
"where
∇w log h(X|w)|w=ŵ = 2 m∑ i=1 Φ(xi) Φ(xi)>ŵ .
",4.1.3. PREDICTIVE MEAN,[0],[0]
"The approximate predictive mean is therefore
f̂(x∗) := E [f(x∗)|X,Ω, k] = Φ(x∗)>ŵ
= m∑ i=1
2
Φ(xi)>ŵ · Φ(xi)>(I + Λ−1)−1Φ(x∗)
",4.1.3. PREDICTIVE MEAN,[0],[0]
":= m∑ i=1 αik̃(xi,x ∗).",4.1.3. PREDICTIVE MEAN,[0],[0]
"(10)
",4.1.3. PREDICTIVE MEAN,[0],[0]
This reveals the same k̃ as (5).,4.1.3. PREDICTIVE MEAN,[0],[0]
"From (10) we have
α̂i = 2/f̂(xi).",4.1.3. PREDICTIVE MEAN,[0],[0]
"(11)
Putting (9), (10) and (11) into (8), we obtain
α̂ = argmin α m∑ i=1 logα2i",4.1.3. PREDICTIVE MEAN,[0],[0]
"+ 1 2 α>K̃α,
where K̃ = (k̃(xi),xj)ij .",4.1.3. PREDICTIVE MEAN,[0],[0]
"This is equivalent to Flaxman et al. (2017), though slightly simplified by (11).",4.1.3. PREDICTIVE MEAN,[0],[0]
"Interestingly, unlike Flaxman et al. (2017) (or the analogous section 3), we did not appeal to the representer theorem.",4.1.3. PREDICTIVE MEAN,[0],[0]
We now compute theQ in (7).,4.1.4. PREDICTIVE VARIANCE,[0],[0]
"The Hessian term giving the inverse covariance becomes
Q−1 =",4.1.4. PREDICTIVE VARIANCE,[0],[0]
"− ∂ 2
∂w∂w> log p(w, X,Ω, k) ∣∣∣∣ w=ŵ
= I + Λ−1 +W
W = − ∂ 2
∂w∂w> log h(X|w) ∣∣∣∣ w=ŵ
= 2 m∑ i=1 Φ(xi)Φ(xi) >",4.1.4. PREDICTIVE VARIANCE,[0],[0]
"(Φ(xi)>ŵ)2 := V DV >,
where V:i = αi × Φ(xi) and D = 12I ∈ R m×m.",4.1.4. PREDICTIVE VARIANCE,[0],[0]
The approximate predictive variance can now be rewritten as an m-dimensional matrix expression using the identity (Z + V DV >),4.1.4. PREDICTIVE VARIANCE,[0],[0]
=,4.1.4. PREDICTIVE VARIANCE,[0],[0]
"Z−1−Z−1V (V >Z−1V +D−1)V >Z−1 with and Z = I + Λ−1 along with a little algebra, to derive2
σ2(x∗)",4.1.4. PREDICTIVE VARIANCE,[0],[0]
":= Var [f(x∗)|X,Ω, k] = Φ(x∗)>Q Φ(x∗)
= k̃(x∗,x∗)− ( k̃(x∗, X) α )",4.1.4. PREDICTIVE VARIANCE,[0],[0]
"S−1 ( α> k̃(X,x∗) ) ,
where is the Hadamard product, or element-wise multiplication, and S := ( k̃(X,X) (αα>) + 2I ) .",4.1.4. PREDICTIVE VARIANCE,[0],[0]
"Given the approximate predictive distribution f(x∗)|X,Ω, k ∼ N (f̂(x∗), σ2(x∗))",4.1.5. PREDICTIVE DISTRIBUTION,[0],[0]
":= N (µ, σ2) and the relation λ(·) = 12f
2(·) it is straightforward to derive the corresponding3 λ(x∗)|X,Ω, k ∼ Gamma(α, β) where the shape α = (µ 2+σ2)2
2σ2(2µ2+σ2) and the scale β = 2µ2σ2+σ4 µ2+σ2 .",4.1.5. PREDICTIVE DISTRIBUTION,[0],[0]
"Letting q(ŵ, X|Ω, k) be the Taylor expansion of log p(w, X|Ω, k) about the mode w = ŵ and evaluating at ŵ gives, as linear and quadratic terms vanish,
log q(ŵ, X|Ω, k) = log p(ŵ, X|Ω, k)
= log h(X|ŵ)−1 2 ŵ>(I+Λ−1)ŵ−1 2 log |Λ|−N 2 log 2π.
",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"2Where e.g. k̃(X,x∗) is an m× 1 matrix of evaluations of k̃. 3Gamma(x|α, β) has p.d.f. 1
Γ(k)βk xα−1 exp(−x/β).
",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"Similarly to (19) we get approximate marginal likelihood
log p(X|Ω, k)",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"≈ log q(ŵ, X|Ω, k)− log q(ŵ|X,Ω, k)
= log h(X|ŵ)︸ ︷︷ ︸",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
− ∑m i=1,4.1.6. MARGINAL LIKELIHOOD,[0],[0]
log 2α 2 i −1 2 ( ŵ>(I,4.1.6. MARGINAL LIKELIHOOD,[0],[0]
+ Λ−1)ŵ︸ ︷︷ ︸,4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"α>k̃(X,X)α",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"− log |Λ|+log |Q| ) (12)
We now use the determinant identity |Z + V DV >| = |Z||D||V >Z−1V + D−1| with the same Z, V and D as subsubsection 4.1.4 to derive
− log |Λ|+ log |Q| = − log |Λ| − log |Z + V DV >| = − log ∣∣Λ(I + Λ−1)∣∣− log ∣∣D−1 + V >Z−1V ∣∣+ c =
N∑ i=1",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"log 1
1 +",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"λi︸ ︷︷ ︸ :=V(k,Ω,µ)
",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"− log ∣∣∣k̃(X,X) (αα>) + 2I∣∣∣+ c,
(13)
where c = m log(2).",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"V(k,Ω, µ) is the crucial ingredient, not accounted for by naı̈vely putting k̃ into subsection A.3.",4.1.6. MARGINAL LIKELIHOOD,[0],[0]
"To apply our inference scheme we need to compute:
1.",5. Covariance Functions,[0],[0]
"The function k̃ from equation (10), studied recently by Flaxman et al. (2017) and earlier by Sollich & Williams (2005) as the equivalent kernel.
2.",5. Covariance Functions,[0],[0]
"The associated term V(k,Ω, µ) from equation (13), required for the marginal likelihood.
",5. Covariance Functions,[0],[0]
This is often challenging for compact domains such as the unit hyper-cube.,5. Covariance Functions,[0],[0]
"Such domains are crucial however, if we are to avoid the well-known edge-effects which arise from neglecting the fact that our data are sampled from, say, a two dimensional rectangle.",5. Covariance Functions,[0],[0]
In subsection 5.1 we provide a simple constructive approach to the case Ω =,5. Covariance Functions,[0],[0]
"[0, 1]d.",5. Covariance Functions,[0],[0]
"The following subsection 5.2 presents the general approximation scheme due to Flaxman et al. (2017), for the case when we have k but not its Mercer expansion.",5. Covariance Functions,[0],[0]
Consider the input domain Ω =,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"[0, π]d with Lebesgue measure µ.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"A classical function regularisation term is the so called m-th order thin-plate spline semi-norm,
〈f, g〉T P(m) := ∑ |α|=m m!∏ j αj ! ∫",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"x∈Ω ∂mf ∂xα ∂mg ∂xα dµ(x)
= 〈f,∆mg〉L2(Ω) + B. (14)
",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Here α is a multi-index running over all indices of total order |α| := ∑ j αj = m, and the boundary conditions B come from formal integration (see e.g. Wahba (1990, section 2.4).",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"We neglect B (for reasons explained shortly) and include the zero-th derivative to define
〈f, g〉H(k)",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
":= 〈f, (a∆m + b)g〉L2(Ω).
",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"We may select the free parameters a > 0, b > 0 and m ∈ Z+ using the maximum marginal likelihood criterion.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"In general, it is challenging to obtain the expressions we require in closed form for arbitrary d, Ω and m. The analytical limit in the literature appears to be the case m = 2 with dimension d = 1 along with so-called Neumann boundary conditions (which impose a vanishing gradient on the boundary (Sommerfeld & Straus, 1949)).",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"That k̃ has been derived in closed form as the reproducing kernel of an associated Sobolev space by Thomas-Agnan (1996).
",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
We now present a simple but powerful scheme which sidesteps these challenges via a well chosen series expansion.,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Consider the basis function
φβ(x) := (2/π) d/2 d∏ j=1 √ 1/2",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"[βj=0] cos(βjxj),
where β is a multi-index with non-negative (integral) values, and [·] denotes the indicator function (which is one if the condition is satisfied and zero otherwise).",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
The φβ form a convenient basis for our purposes.,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"They are orthonormal:
〈φβ , φγ〉L2(Ω) =",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"[β = γ],
and also eigenfunctions of our regularisation operator with
(a∆m + b)φβ",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
= ( a ( d∑ j=1 β2j )m + b ) φβ .,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"(15)
",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
Now if we restrict the function space to H(k) := { f = ∑ β≥0 cβφβ : ‖f‖2H(k) = ∑,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"β≥0 c2β/λβ <∞ } ,
then it is easily verified that the boundary conditions B in (14) vanish.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
This is a common approach to solving partial differential equations with Neumann boundary conditions (see e.g. Sommerfeld & Straus (1949)).,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"By restricting in this way, we merely impose zero partial derivatives at the boundary, while otherwise enjoying the usual Fourier series approximation properties.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Hence we can combine the reproducing property (4) with (14) and (15) to derive
k(x,y) = ∑ β≥0 λβφβ(x)φβ(y), (16)
where λβ := 1/(a (∑d j=1 β 2 j )m + b).
",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
The above covariance function is not required for our inference algorithm.,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Rather, the point is that since the basis is also orthonormal, we may substitute λβ and φβ into (10) and (13) to obtain k̃ and V(k), as required.
Series truncation.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
We have discovered closed form expressions for k̃ only for m ≤ 2 and d = 1.,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
In practice we may truncate the series at any order and still obtain a valid model due to the equivalence with the linear model (6).,5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Hence, a large approximation error (in terms of k̃) due to truncation may be irrelevant from a machine learning perspective, merely implying a different GP prior over functions.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Indeed, the maximum marginal likelihood criterion based on subsubsection 4.1.6 may guide the selection of an appropriate truncation order, although some care needs to be taken in this case.",5.1. Thin-Plate Semi-norms on the Hyper-Cube,[0],[0]
"Flaxman et al. (2017) suggested the following approximation for k̃, for the case when k is known but the associated Mercer expansion is not.",5.2. Arbitrary Covariances and Domains,[0],[0]
"The approximation is remarkably general and elegant, and may even be applied to nonvectorial data by employing, say, a kernel function defined on strings (Lodhi et al., 2002).",5.2. Arbitrary Covariances and Domains,[0],[0]
"The idea is to note that the φi, λi pairs are eigenfunctions of the integral operator (see Rasmussen & Williams (2006) section 4.3)
",5.2. Arbitrary Covariances and Domains,[0],[0]
Tk : H(k)→ H(k),5.2. Arbitrary Covariances and Domains,[0],[0]
f 7→,5.2. Arbitrary Covariances and Domains,[0],[0]
"Tkf := ∫ x∈Ω k(x, ·)",5.2. Arbitrary Covariances and Domains,[0],[0]
"f(x)p(x) dx,
where p is related to µ of the previous subsection by µ(x) = p(x) dx.",5.2. Arbitrary Covariances and Domains,[0],[0]
"The Nyström approximation (Nyström, 1928) to Tk draws m samples X from p and defines T
(X) k",5.2. Arbitrary Covariances and Domains,[0],[0]
"g := 1 m ∑ x∈X k(x, ·)g(x).",5.2. Arbitrary Covariances and Domains,[0],[0]
"Then the eigenfunctions and eigenvectors of Tk may be approximated via the eigenvectors e(mat)i and eigenvalues λ (mat) i of 2 k(X,X), as
φ (X) i :=
√ m/λ
(mat) i k(·, X)e (mat)",5.2. Arbitrary Covariances and Domains,[0],[0]
"i
λ (X) i := λ (mat) i /m.
",5.2. Arbitrary Covariances and Domains,[0],[0]
"These approximations may be used for k̃, as in (Flaxman et al., 2017), as well as our V(k,Ω, µ).",5.2. Arbitrary Covariances and Domains,[0],[0]
Evaluation We use two metrics.,6.1. Setup,[0],[0]
The `2 Error is the squared difference to the ground truth w.r.t.,6.1. Setup,[0],[0]
the Lebesgue measure: ∫ x∈Ω(λ(x),6.1. Setup,[0],[0]
"− λtrue(x))
2 dx.",6.1. Setup,[0],[0]
"The test log likelihood is the logarithm of (1) at an independent test sample (one sample being a set of points, i.e. a sample from the process), which we summarise by averaging over a finite number of test sets (for real data where the ground truth intensity is unknown) and otherwise (if we have the ground truth) by the analytical expression EX∼PP(λ) [ log pX∼PP(λ̂)(X)
] =∫
x∈Ω
( λ(x) log λ̂(x)− λ̂(x) )",6.1. Setup,[0],[0]
"dx,
where PP (λ) is the process with intensity λ (see the supplementary subsection A.1).",6.1. Setup,[0],[0]
"This evaluation metric is novel in this context, yet more accurate and computationally cheaper than the sampling of e.g. (Adams et al., 2009).
",6.1. Setup,[0],[0]
Decision Theory The above metrics are functions of a single estimated intensity.,6.1. Setup,[0],[0]
In all cases we use the predictive mean intensity for evaluation.,6.1. Setup,[0],[0]
"We demonstrate in subsection A.2 of the supplementary material that this is optimal
for the expected test log likelihood evaluation (the `2 error cases is similar as is trivial to show).
",6.1. Setup,[0],[0]
"Algorithms We compare our new Laplace Bayesian Point Process (LBPP) with two covariances: the cosine kernel of subsection 5.1 with fixed m = 2 and hyperparameters a and b (LBPP-Cos), and the Gaussian kernel k(x, z) = γ2 exp(|x− y|2 /(2β2))",6.1. Setup,[0],[0]
with the method of subsection 5.2 (LBPP-G).,6.1. Setup,[0],[0]
"We compared with the Variational Bayesian Point Process (VBPP) (Lloyd et al., 2015) using the same Gaussian kernel.",6.1. Setup,[0],[0]
"LBPP-G and VBPP use a regular grid for X (of subsection 5.2) and the inducing points, respectively.",6.1. Setup,[0],[0]
"To compare timing we vary the number of basis functions, i.e. the number of grid points for LBPP-G and VBPP, and cosine terms for LBPP-Cos.",6.1. Setup,[0],[0]
"We include the baseline kernel smoothing with edge correction (KS+EC) method (Diggle, 1985; Lloyd et al., 2015).",6.1. Setup,[0],[0]
"All
inference is performed with maximum marginal likelihood, except for KS+EC where we maximise the leave one out metric described in (Lloyd et al., 2015).",6.1. Setup,[0],[0]
"We drew five toy intensities, λ0, λ2, . . .",6.2. 1D Toy Examples,[0],[0]
", λ4 as 12f 2 where f was sampled from the GP of Gaussian covariance (defined above) with γ = 5 and β = 0.5.",6.2. 1D Toy Examples,[0],[0]
Figure 1 depicts λ0 — see the caption for a description.,6.2. 1D Toy Examples,[0],[0]
The remaining test functions are shown in figure 6 of the supplementary material.,6.2. 1D Toy Examples,[0],[0]
"As the marginal likelihood log p(X|Ω, k) is a key advantage of our method over the non-probabilistic approach of Flaxman et al. (2017), we investigated its efficacy for model selection.",6.2.1. MODEL SELECTION,[0],[0]
"Figure 3 plots log p(X|Ω, k) against our two error metrics, both rescaled to [0, 1] for effective visualisation, based on a single training sample per test function.",6.2.1. MODEL SELECTION,[0],[0]
"We observe a strong relationship, with larger values of log p(X|Ω, k) generally corresponding to lower error.",6.2.1. MODEL SELECTION,[0],[0]
"This demonstrates the practical utility of both the marginal likelihood itself, and our Laplace approximation to it.",6.2.1. MODEL SELECTION,[0],[0]
We sampled 100 training sets from each of our five toy functions.,6.2.2. EVALUATION,[0],[0]
Figure 4 shows our evaluation metrics along with the fitting time as a function of the number of basis functions.,6.2.2. EVALUATION,[0],[0]
"For visualisation all metrics (including fit time) are scaled to [0, 1] by dividing by the maximum for the given test function, over data replicates and algorithms.",6.2.2. EVALUATION,[0],[0]
"LBBP-G and and VBPP achieve the best performance, but our LBPP-G is two orders of magnitude faster.",6.2.2. EVALUATION,[0],[0]
Our KS+EC implementation follows the methodology of Lloyd et al. (2015): we fit the kernel density bandwidth using average leave one out log likelihood.,6.2.2. EVALUATION,[0],[0]
This involves a quadratic number of log p.d.f.,6.2.2. EVALUATION,[0],[0]
"of the truncated normal calculations,
and log-sum-exp calculations, both of which involve large time constants, but are asymptotically superior to the other methods we considered.",6.2.2. EVALUATION,[0],[0]
"LBBP-Cos is slightly inferior in terms of expected test log likelihood, which is expected due to the toy functions having been sampled according to the same Gaussian kernel of LBPP-G and VBPP (as well as the density estimator of KS+EC).",6.2.2. EVALUATION,[0],[0]
"We compared the methods on three real world datasets,
• coal: 190 points in one temporal dimension, indicating the time of fatal coal mining accidents in the United Kingdom, from 1851 to 1962 (Collins, 2013);
• redwood: 195 California redwood tree locations from a square sampling region (Ripley, 1977);
• cav: 138 caveolae locations from a square sampling region of muscle fiber (Davison & Hinkley, 2013).",6.3. Real Data,[0],[0]
Similarly to subsubsection 6.2.2 we evaluate the fitting speed and statistical performance vs. number of basis functions — see figure 5.,6.4. Computational Speed,[0],[0]
We omit the `2 error as the ground truth is unknown.,6.4. Computational Speed,[0],[0]
Instead we generate 100 test problems by each time randomly assigning each original datum to either the training or the testing set with equal probability.,6.4. Computational Speed,[0],[0]
"Again we observe similar predictive performance of LBPP and VBPP, but with much faster fit times for our LBPP.",6.4. Computational Speed,[0],[0]
Interestingly LBPP-Cos slightly outperform LBPP-G.,6.4. Computational Speed,[0],[0]
We conclude by further investigating the redwood dataset.,6.5. 2D California Redwood Dataset,[0],[0]
"Once again we employed the ML-II procedure to determine a and b, fixing m = 2, for the covariance function of subsection 5.1, using the lowest 32 cosine frequencies in each
dimension for a total of N = 322 basis functions in the expansion (16).",6.5. 2D California Redwood Dataset,[0],[0]
"For ease of visualisation we also fixed a = b.
Figure 2 plots the results, including a decomposition of the log marginal likelihood log p(X|Ω, k) and a visualisation of the predictive mean.",6.5. 2D California Redwood Dataset,[0],[0]
"The mean function strongly resembles the result presented by Adams et al. (2009), where computationally expensive MCMC was employed.
",6.5. 2D California Redwood Dataset,[0],[0]
"The decomposition of the marginal likelihood on the left of figure 2 provides insight into the role of the individual terms in (12) and (13) which make up log p(X|Ω, k).",6.5. 2D California Redwood Dataset,[0],[0]
"In particular, the term V(k,Ω, µ) from (13) acts as a regulariser, guarding against over-fitting, and balancing against the data term h of (12).",6.5. 2D California Redwood Dataset,[0],[0]
"We have discussed the permanental process, which places a Gaussian Process prior over the square root of the inten-
sity function of the Poisson process, and derived the equations required for empirical Bayes under a Laplace posterior approximation.",7. Conclusion,[0],[0]
"Our analysis provides 1) an alternative derivation and probabilistic generalization of (Flaxman et al., 2017), and 2) an efficient and easier to implement alternative which does not rely on inducing inputs (but rather reproducing kernel Hilbert space theory), to the related Bayesian approach of Lloyd et al. (2015).",7. Conclusion,[0],[0]
"This further demonstrates, in a new way, the mathematical convenience and practical utility of the permanental process formulation (in comparison with say log Gaussian Cox processes).",7. Conclusion,[0],[0]
"Thanks to Young Lee, Kar Wai Lim and Cheng Soon Ong for useful discussions.",Acknowledgements,[0],[0]
Adrian is supported by the Australian Research Council (ARC) via a Discovery Early Career Researcher Award (DE-120102873).,Acknowledgements,[0],[0]
The Cox process is a stochastic process which generalises the Poisson process by letting the underlying intensity function itself be a stochastic process.,abstractText,[0],[0]
"In this paper we present a fast Bayesian inference scheme for the permanental process, a Cox process under which the square root of the intensity is a Gaussian process.",abstractText,[0],[0]
"In particular we exploit connections with reproducing kernel Hilbert spaces, to derive efficient approximate Bayesian inference algorithms based on the Laplace approximation to the predictive distribution and marginal likelihood.",abstractText,[0],[0]
"We obtain a simple algorithm which we apply to toy and real-world problems, obtaining orders of magnitude speed improvements over previous work.",abstractText,[0],[0]
Fast Bayesian Intensity Estimation for the Permanental Process,title,[0],[0]
"Optimisation problems arise in numerous fields ranging from science and engineering to economics and management (Brochu et al., 2010).",1. Introduction,[0],[0]
"In classical optimisation tasks, the objective function is usually known and cheap to evaluate (Hennig and Schuler, 2012).",1. Introduction,[0],[0]
"However, in many situations, we face another type of tasks for which the above assumptions do not apply.",1. Introduction,[0],[0]
"For example, in the cases of clinical trials, financial investments or constructing a sensor network, it is very costly to draw a sample from the latent function underlying the real-world processes (Brochu et al., 2010).",1. Introduction,[0],[0]
"The objective functions in such type
1Department of Engineering Science, University of Oxford, Oxford, UK 2Mind Foundry Ltd., Oxford.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Binxin Ru <robin@robots.ox.ac.uk>, Mark McLeod <mark.mcleod@magd.ox.ac.uk>, Diego Granziol <diego@robots.ox.ac.uk>, Michael A. Osborne <mosb@robots.ox.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"of problems are generally non-convex and their closed-form expressions and derivatives are unknown (Shahriari et al., 2016).",1. Introduction,[0],[0]
"Bayesian optimisation is a powerful tool to tackle such optimisation challenges (Brochu et al., 2010).
",1. Introduction,[0],[0]
"A core step in Bayesian optimisation is to define an acquisition function which uses the available observations effectively to recommend the next query location (Shahriari et al., 2016).",1. Introduction,[0],[0]
"There are many types of acquisition functions such as Probability of Improvement (PI) (Kushner, 1964), Expected Improvement (EI) (Močkus et al., 1978; Jones et al., 1998) and Gaussian Process Upper Confidence Bound (GP-UCB) (Srinivas et al., 2009).",1. Introduction,[0],[0]
"The most recent type is based on information theory and offers a new perspective to efficiently select the sequence of sampling locations based on entropy of the distribution over the unknown minimiser x∗ (Shahriari et al., 2016).",1. Introduction,[0],[0]
"The information-theoretic approaches guide our evaluations to locations where we can maximise our learning about the unknown minimum rather than to locations where we expect to obtain lower function values (Hennig and Schuler, 2012).",1. Introduction,[0],[0]
"Such methods have demonstrated impressive empirical performance and tend to outperform traditional methods in tasks with highly multimodal and noisy latent functions.
",1. Introduction,[0],[0]
"One popular information-based acquisition function is Predictive Entropy Search (PES) (Villemonteix et al., 2009; Hennig and Schuler, 2012; Hernández-Lobato et al., 2014) .",1. Introduction,[0],[0]
"However, it is very slow to evaluate in comparison with traditional methods like EI, PI and GP-UCB and faces serious constraints in its application.",1. Introduction,[0],[0]
"For example, the implementation of PES requires the first and second partial derivatives as well as the spectral density of the Gaussian process kernel function (Hernández-Lobato et al., 2014; Requeima, 2016).",1. Introduction,[0],[0]
This limits our kernel choices.,1. Introduction,[0],[0]
"Moreover, PES deals with the input space, thus less efficient in higher dimensional problems (Wang and Jegelka, 2017).",1. Introduction,[0],[0]
"The more recent methods such as Output-space Predictive Entropy Search (OPES) (Hoffman and Ghahramani, 2015) and Max-value Entropy Search (MES) (Wang and Jegelka, 2017) improve on PES by focusing on the information content in output space instead of input space.",1. Introduction,[0],[0]
"However, current entropy search methods, whether dealing with the minimiser or the minimum value, all involve two separate sampling processes : 1) sampling
hyperparameters for marginalisation and 2) sampling the global minimum/minimiser for entropy computation.",1. Introduction,[0],[0]
"The second sampling process not only contributes significantly to the computational burden of these information-based acquisition functions but also requires the construction of a good approximation for the objective function based on Bochner’s theorem (Hernández-Lobato et al., 2014), which limits the kernel choices to the stationary ones (Bochner, 1959).
",1. Introduction,[0],[0]
"In view of the limitations of the existing methods, we propose a fast information-theoretic Bayesian optimisation technique (FITBO).",1. Introduction,[0],[0]
"Inspired by the Bayesian integration work in (Gunter et al., 2014), the creative contribution of our technique is to approximate any black-box function in a parabolic form: f(x) = η + 1/2g(x)2.",1. Introduction,[0],[0]
"The global minimum is explicitly represented by a hyperparameter η, which can be sampled together with other hyperparameters.",1. Introduction,[0],[0]
"As a result, our approach has the following three major advantages:
1.",1. Introduction,[0],[0]
"Our approach reduces the expensive process of sampling the global minimum/minimiser to the much more efficient process of sampling one additional hyperparameter, thus overcoming the speed bottleneck of information-theoretic approaches.
2.",1. Introduction,[0],[0]
"Our approach faces fewer constraints on the choice of appropriate kernel functions for the Gaussian process prior.
3.",1. Introduction,[0],[0]
"Similar to MES (Wang and Jegelka, 2017), our approach works on information in the output space and thus is more efficient in high dimensional problems.",1. Introduction,[0],[0]
"Information-theoretic techniques aim to reduce the uncertainty about the unknown global minimiser x∗ by selecting a query point that leads to the largest reduction in entropy of the distribution p(x∗|Dn) (Hennig and Schuler, 2012).",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"The acquisition function for such techniques has the form (Hennig and Schuler, 2012; Hernández-Lobato et al., 2014):
αES(x|Dn) = H[p(x∗|Dn)]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"− Ep(y|Dn,x) [ H [ p ( x∗|Dn ∪ (x, y) )",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
],2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
] .,2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"(1)
PES makes use of the symmetry of mutual information and arrives at the following equivalent acquisition function:
αPES(x|Dn) = H[p(y|Dn,x)]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"− Ep(x∗|Dn) [ H [ p(y|Dn,x,x∗)",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"]] , (2)
where p(y|Dn,x,x∗) is the predictive posterior distribution for y conditioned on the observed data Dn, the test location x and the global minimiser x∗ of the objective function.
",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
FITBO harnesses the same information-theoretic thinking but measures the entropy about the latent global minimum f∗ = f(x∗) instead of that of the global minimiser x∗.,2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"Thus, the acquisition function of FITBO method is the mutual information between the function minimum f∗ and the next query point (Wang and Jegelka, 2017).",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"In other words, FITBO aims to select the next query point which minimises the entropy of the global minimum:
αFITBO(x|Dn) = H[p(y|Dn,x)]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"− Ep(f∗|Dn) [ H [ p(y|Dn,x, f∗) ]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
] .,2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"(3)
This idea of changing entropy computation from the input space to the output space is also shared by Hoffman and Ghahramani (2015) and Wang and Jegelka (2017).",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"Hence, the acquisition function of the FITBO method is very similar to those of OPES (Hoffman and Ghahramani, 2015) and MES (Wang and Jegelka, 2017).
",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"However, our novel contribution is to express the unknown objective function in a parabolic form f(x) = η+ 1/2g(x)2, thus representing the global minimum f∗ by a hyperparameter η and circumventing the laborious process of sampling the global minimum.",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"FITBO acquisition function can then be reformulated as:
αFITBO(x|Dn) = H[p(y|Dn,x)]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"− Ep(η|Dn) [ H [ p(y|Dn,x, η) ]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"] = H [ ∫ p(y|Dn,x, η)p(η|Dn)dη
] − ∫ p(η|Dn)H [ p(y|Dn,x, η) ] dη. (4)
The intractable integral terms can be approximated by drawing M samples of η from the posterior distribution p(η|Dn) and using a Monte Carlo method (HernándezLobato et al., 2014).",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"The predictive posterior distribution p(y|Dn,x, η) can be turned into a neat Gaussian form by applying a local linearisation technique on our parabolic transformation as described in Section 2.1.",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"Thus, the first term in the above FITBO acquisition function is an entropy of a Gaussian mixture, which is intractable and demands approximation as described in Section 2.3.",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"The second term is the expected entropy of a one-dimensional Gaussian distribution and can be computed analytically because the entropy of a Gaussian has the closed form: H[p(y|Dn,x, η)]",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
"= 0.5 log [ 2πe ( vf (x|Dn, η) + σ2n
)] where the variance vf (x|Dn, η) = Kf (x,x′) and σ2n is the variance of observation noise.",2. Fast Information-theoretic Bayesian Optimisation,[0],[0]
Gunter et al. (2014) use a square-root transformation on the integrand in their warped sequential active Bayesian integration method to ensure non-negativity.,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"Inspired by this work, we creatively express any unknown objective function f(x) in the parabolic form:
f(x) = η + 1/2g(x)2, (5)
where η is the global minimum of the objective function.",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"Given the noise-free observation data Df = {(xi, fi)|i = 1, . . .",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"n} = {Xn, fn}, the observation data on g is Dg = {(xi, gi)|i = 1, . . .",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
n},2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"= {Xn,gn} where gi = √ 2(fi − η) .
",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"We impose a zero-mean Gaussian process prior on g(x), g ∼ GP ( 0, k(x,x′) ) , so that the posterior distribution for g conditioned on the observation data Dg and the test point x also follows a Gaussian process:
p(g|Dg,x, η) =",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"GP ( g;mg(·),Kg(·, ·) )",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"(6)
where
mg(x) = K(x,Xn)K(Xn,Xn) −1gn,
Kg(x,x ′) = K(x,x′)−K(x,Xn)K(Xn,Xn)−1K(Xn,x′).
",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"The parabolic transformation causes the distribution for any f to become a non-central χ2 process, making the analysis intractable.",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"In order to tackle this problem and obtain a posterior distribution p(f |Df ,x, η) that is also Gaussian, we employ a linearisation technique (Gunter et al., 2014).
",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
We perform a local linearisation of the parabolic transformation h(g),2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
= η + 1/2g2 around g0 and obtain f ≈ h(g0) +,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
h′(g0)(g − g0),2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
where the gradient h′(g),2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
= g.,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"By setting g0 to the mode of the posterior distribution p(g|Dg,x, η) (i.e. g0 = mg), we obtain an expression for f which is linear in g:
f(x)",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
≈,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
[η + 1/2mg(x)2] +mg(x)[g(x)−mg(x)],2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
= η,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
− 1/2mg(x)2 +mg(x)g(x).,2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"(7)
Since the affine transformation of a Gaussian process remains Gaussian, the predictive posterior distribution for f now has a closed form:
p(f |Df ,x, η) =",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"GP ( f ;mf (·),Kf (·, ·) ) (8)
where mf (x) = η + 1/2mg(x) 2
Kf (x,x ′) = mg(x)Kg(x,x ′)mg(x ′).
",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"However, in real world situations, we do not have access to the true function values but only noisy observations of the function, y(x) = f(x) + , where is assumed to be an independently and identically distributed Gaussian noise with variance σ2n (Rasmussen and Williams, 2006).",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"Given noisy observation data Dn = {(xi, yi)|i = 1, . . .",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
n},2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"= {Xn,yn}, the predictive posterior distribution (8) becomes:
p(y|Dn,x, η) = GP ( y;mf (·),Kf (·, ·) + σ2nδ(·, ·) ) .",2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
(9),2.1. Parabolic Transformation and Predictive Posterior Distribution,[0],[0]
"Hyperparameters are the free parameters, such as output scale and characteristic length scales in the kernel function for the Gaussian processes as well as noise variance.",2.2. Hyperparameter Treatment,[0],[0]
We use θ to represent a vector of hyperparameters that includes all the kernel parameters and the noise variance.,2.2. Hyperparameter Treatment,[0],[0]
Recall that we introduce a new hyperparameter η in our model to represent the global minimum.,2.2. Hyperparameter Treatment,[0],[0]
"To ensure that η is not greater than the minimum observation ymin, we assume that log(ymin − η) follows a broad normal distribution.",2.2. Hyperparameter Treatment,[0],[0]
"Thus the prior for η has the form:
p(η) = 1 (ymin − η) N",2.2. Hyperparameter Treatment,[0],[0]
"( log(ymin − η);µ, σ2 ) .",2.2. Hyperparameter Treatment,[0],[0]
"(10)
The most popular approach to hyperparameter treatment is to learn hyperparameter values via maximum likelihood estimation (MLE) or maximum a posterior estimation (MAP).",2.2. Hyperparameter Treatment,[0],[0]
"However, both MLE and MAP are not desirable as they give point estimates and ignore our uncertainty about the hyperparameters (Hernández-Lobato et al., 2014).",2.2. Hyperparameter Treatment,[0],[0]
"In a fully Bayesian treatment of the hyperparameters, we should consider all possible hyperparameter values.",2.2. Hyperparameter Treatment,[0],[0]
"This can be done by marginalising the terms in the acquisition function with respect to the posterior p(ψ|Dn) where ψ = {θ, η}:
αFITBO(x|Dn) = H [ ∫ p(y|Dn,x,ψ)p(ψ|Dn)dψ ]
− ∫ p(ψ|Dn)H [ p(y|Dn,x,ψ) ]",2.2. Hyperparameter Treatment,[0],[0]
"dψ.
Since complete marginalisation over hyperparameters is analytically intractable, the integral can be approximated using the Monte Carlo method (Hoffman and Ghahramani, 2015; Snoek et al., 2012), leading to the final expression:
αFITBO(x|Dn)
",2.2. Hyperparameter Treatment,[0],[0]
"= H [ 1 M M∑ j p(y|Dn,x,θ(j), η(j))",2.2. Hyperparameter Treatment,[0],[0]
"]
− 1 2M M∑ j log [ 2πe ( vf (x|D,θ(j), η(j))",2.2. Hyperparameter Treatment,[0],[0]
"+ σ2n )] .
",2.2. Hyperparameter Treatment,[0],[0]
(11),2.2. Hyperparameter Treatment,[0],[0]
"The entropy of a Gaussian mixture is intractable and can be estimated via a number of methods: the Taylor expansion proposed in (Huber et al., 2008), numerical integration and Monte Carlo integration.",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"Of these three, our experimentation revealed that numerical integration (in particular, an adaptive Simpson’s method) was clearly the most performant for our application (see the supplementary material).",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"Note that our Gaussian mixture is univariate.
",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
A faster alternative is to approximate the first entropy term by matching the first two moments of a Gaussian mixture.,2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"The mean and variance of a univariate Gaussian mixture model p(z) = ∑M j 1 MN (z|mj ,Kj) have the analytical form:
E[z] = M∑ j 1 M mj (12)
Var(z) = M∑",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
j 1 M (Kj +m 2 j ),2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
− E[z]2.,2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"(13)
By fitting a Gaussian to the Gaussian mixture, we can obtain a closed-form upper bound for the first entropy term: H[p(z)]",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"≈ 0.5 log [ 2πe ( Var(z)+σ2n )] , thus further enhancing the computational speed of FITBO approach.",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
"However, the moment-matching approach results in a looser approximation than numerical integration (shown in the supplementary material) and we will compare both approaches in our experiments in Section 3.",2.3. Approximation for the Gaussian Mixture Entropy,[0],[0]
The procedures of computing the acquisition function of FITBO are summarised by Algorithm 1.,2.4. The Algorithm,[0],[0]
Figure 1 illustrates the sampling behaviour of FITBO method for a simple 1D Bayesian optimisation problem.,2.4. The Algorithm,[0],[0]
The optimisation process is started with 3 initial observation data.,2.4. The Algorithm,[0],[0]
"As more samples are taken, the mean of the posterior distribution for the objective function gradually resembles the objective function and the distribution of η converges to the global minimum.",2.4. The Algorithm,[0],[0]
We conduct a series of experiments to test the empirical performance of FITBO and compare it with other popular acquisition functions.,3. Experiments,[0],[0]
"In this section, FITBO denotes the version using numerical integration to estimate the entropy of the Gaussian mixture while FITBO-MM denotes the version using moment matching.",3. Experiments,[0],[0]
"In all experiments, we adopt a zero-mean Gaussian process prior with the squared exponential kernel function and use the elliptical slice sampler (Murray et al., 2010) for sampling hyperparameters θ and η.",3. Experiments,[0],[0]
"For the implementation of EI, PI, GP-UCB, MES
Algorithm 1 FITBO acquisition function 1: Input: a test input x; noisy observation data Dn = {(xi, yi)|i = 1, . . .",3. Experiments,[0],[0]
", n}
2: Sample hyperparameters and η from p(ψ|Dn): Ψ = {θ(j), η(j)|j = 1, . . .",3. Experiments,[0],[0]
",M} 3: for j = 1, . . .",3. Experiments,[0],[0]
",M do 4: Use f(x) = η + 1/2g(x)2 to approximate
p(f |Dn,x,θ(j), η(j))",3. Experiments,[0],[0]
"= GP ( mf (·),Kf (·, ·) ) 5: Compute p(y|Dn,x,θ(j), η(j)) 6: Compute H[p(y|Dn,x,θ(j), η(j))] 7: end for 8: Estimate the entropy of the Gaussian mixture :
E1(x|Dn) = H [ 1 M ∑M",3. Experiments,[0],[0]
"j p(y|Dn,x,θ (j), η(j))",3. Experiments,[0],[0]
"]
9: Compute the entropy expectation: E2(x|Dn)",3. Experiments,[0],[0]
= 1,3. Experiments,[0],[0]
"M ∑M j H[p(y|Dn,x,θ
(j), η(j))]",3. Experiments,[0],[0]
"= 1
2M ∑M j log [ 2πe ( vf (x|Dn,θ(j), η(j)) + σ2n )] 10: return αn(x|Dn) = E1(x|Dn)− E2(x|Dn)
and PES, we use the open source Matlab code by Wang and Jegelka (2017) and Hernández-Lobato et al. (2014).",3. Experiments,[0],[0]
Our Matlab code for FITBO will be available at https: //github.com/rubinxin/FITBO.,3. Experiments,[0],[0]
"We use the type of MES method that samples the global minimum f(x∗) from an approximated posterior function f̃(x) = φ(x)Tã where φ(x) is an m-dimensional feature vector and ã is a Gaussian weight vector (Wang and Jegelka, 2017).",3. Experiments,[0],[0]
"This is also the minimiser sampling strategy adopted by PES (Hernández-Lobato et al., 2014).",3. Experiments,[0],[0]
The computational complexity of sampling ã from its posterior distribution p(ã|Dn) is O(n2m) when n,3. Experiments,[0],[0]
"< m (Hernández-Lobato et al., 2014).",3. Experiments,[0],[0]
"Minimising f̃(x) to within ζ accuracy using any grid search or branch and bound optimiser requires O(ζ−d) calls to f̃(x) for d-dimensional input data (Kandasamy et al., 2015).",3. Experiments,[0],[0]
"For both PES and MES, we apply their fastest versions which draw only 1 minimum or minimiser sample to estimate the acquisition function.",3. Experiments,[0],[0]
"The first set of experiments measure and compare the runtime of evaluating the acquisition functions αn(x|Dn) for methods including GP-UCB, PI, EI, PES, MES, FITBO and FITBO-MM.",3.1. Runtime Tests,[0],[0]
All the timing tests were performed exclusively on a 2.3 GHz Intel Core i5.,3.1. Runtime Tests,[0],[0]
The runtime measured excludes the time taken for sampling hyperparameters as well as optimising the acquisition functions.,3.1. Runtime Tests,[0],[0]
"The methodology of the tests can be summarised as follows:
1.",3.1. Runtime Tests,[0],[0]
"Generate 10 initial observation data from a ddimensional test function and sample a set of M hyperparameters Ψ = {θ(j), η(j)|j = 1, . . .",3.1. Runtime Tests,[0],[0]
",M} from the log posterior distribution log p̃(ψ|Dn) using the
elliptical slice sampler.
2.",3.1. Runtime Tests,[0],[0]
"Use this set of hyperparameters to evaluate all acquisition functions at 100 test points.
3.",3.1. Runtime Tests,[0],[0]
"Repeat the procedures 1 and 2 for 100 different initialisations and compute the mean and standard deviation of the runtime taken for evaluating various acquisition functions.
",3.1. Runtime Tests,[0],[0]
We did not include the time for sampling η alone into the runtime of evaluating FITBO and FITBO-MM because η is sampled jointly with other hyperparameters and does not add to the overall sampling burden significantly.,3.1. Runtime Tests,[0],[0]
"In fact, we have tested that sampling η by the elliptical slice sampler adds 0.09 seconds on average when drawing 2 000 samples and 0.93 seconds when drawing 20 000 samples.",3.1. Runtime Tests,[0],[0]
"Note further that we will limit all methods to a fixed number of hyperparameter samples in both runtime tests and performance experiments: this will impart a slight performance penalty to our method, which must sample from a hyperparameter space of one additional dimension.
",3.1. Runtime Tests,[0],[0]
"The above tests are repeated for different hyperparameter sample sizes M = 100, 300, 500, 700, 900 and input data of different dimensions d = 2, 4, 6, 8, 10.",3.1. Runtime Tests,[0],[0]
"The results are presented graphically in Figure 2 with the
evaluation runtime being expressed in the logarithm to the base 10 and the exact numerical results for methods that are very close in runtime are presented in Tables 1 and 2.
",3.1. Runtime Tests,[0],[0]
Figure 2 shows that FITBO is significantly faster to evaluate than PES and MES for various hyperparameter sample sizes used and for problems of different input dimensions.,3.1. Runtime Tests,[0],[0]
"Moreover, FITBO even gains a clear speed advantage over EI.",3.1. Runtime Tests,[0],[0]
"The moment matching technique manages to further enhance the speed of FITBO, making FITBO-MM comparable with, if not slightly faster than, simple algorithms like PI and GP-UCB.",3.1. Runtime Tests,[0],[0]
"In addition, we notice that the runtime of evaluating FITBO-MM, EI, PI and GP-UCB tend to remain constant regardless of the input dimensions while the runtime for PES and MES tends to increase with input dimensions at a rate of 10d.",3.1. Runtime Tests,[0],[0]
"Thus, our
approach is more efficient and applicable in dealing with high-dimensional problems.",3.1. Runtime Tests,[0],[0]
"We perform optimisation tasks on three challenging benchmark functions: Branin (defined in [0, 1]2), Eggholder (defined in [0, 1]2) and Hartmann (defined in [0, 1]6).",3.2. Tests with Benchmark Functions,[0],[0]
"In all tests, we set the observation noise to σ2n = 10
−3 and resample all the hyperparameters after each function evaluation.",3.2. Tests with Benchmark Functions,[0],[0]
"In evaluating the optimisation performance of various Bayesian optimisation methods, we use the two common metrics adopted by Hennig and Schuler (2012).",3.2. Tests with Benchmark Functions,[0],[0]
"The first metric is Immediate regret (IR) which is defined as:
IR = |f(x∗)− f(x̂n)| (14)
where x∗ is the location of true global minimum and x̂n is the best guess recommended by a Bayesian optimiser after n iterations, which corresponds to the minimiser of the posterior mean.",3.2. Tests with Benchmark Functions,[0],[0]
"The second metric is the Euclidean distance of an optimiser’s recommendation x̂n from the true global minimiser x∗, which is defined as:
‖L‖2 = ‖x∗",3.2. Tests with Benchmark Functions,[0],[0]
"− x̂n‖. (15)
",3.2. Tests with Benchmark Functions,[0],[0]
We compute the median IR and the median ‖L‖2 over 40 random initialisations.,3.2. Tests with Benchmark Functions,[0],[0]
"At each initialisation, all Bayesian optimisation algorithms start from 3 random observation data for Branin-2D and Eggholder-2D problems and from 9 random observation data for Hartmann-6D problem.
",3.2. Tests with Benchmark Functions,[0],[0]
The results are presented in Figure 3.,3.2. Tests with Benchmark Functions,[0],[0]
The plots on the left show the median IR achieved by each approach as more evaluation steps are taken.,3.2. Tests with Benchmark Functions,[0],[0]
The plots on the right show the median ‖L‖2 between each optimiser’s recommended global minimiser and the true global minimiser.,3.2. Tests with Benchmark Functions,[0],[0]
"The error bars indicate one standard deviation.
",3.2. Tests with Benchmark Functions,[0],[0]
"In the case of Branin-2D, FITBO and FITBO-MM lose out to other methods initially but surpass other methods after 50 evaluations.",3.2. Tests with Benchmark Functions,[0],[0]
One interesting point we would like to illustrate through the Branin problem is the fundamentally different mechanisms behind information-based approaches like FITBO and improvement-based approaches like EI.,3.2. Tests with Benchmark Functions,[0],[0]
"As shown in Figure 4, FITBO is much more explorative compared to EI in taking new evaluations because FITBO selects the query points that maximise the information gain about the minimiser instead of those that lead to an improvement over the best function value observed.",3.2. Tests with Benchmark Functions,[0],[0]
"FITBO successfully finds all three global minimisers but EI quickly concentrates its searches into regions of low function values, missing out one of the global minimisers.
",3.2. Tests with Benchmark Functions,[0],[0]
"In the case of Eggholder-2D which is more complicated and multimodal, FITBO and FITBO-MM perform
not as well as other methods in finding lower function values but outperform all competitors in locating the global minimiser by a large margin.",3.2. Tests with Benchmark Functions,[0],[0]
One reason is that the function value near the global minimiser of Eggholder-2D rises sharply.,3.2. Tests with Benchmark Functions,[0],[0]
"Thus, although FITBO and FITBO-MM are able to better identify the location of the true global minimum, they return higher function values than other methods that are trapped in locations of good local minima.
",3.2. Tests with Benchmark Functions,[0],[0]
"As for a higher dimensional problem, Hartmann-6D, FITBO and FITBO-MM outperform all other methods in finding both the lower function value and the location of the global minimum.",3.2. Tests with Benchmark Functions,[0],[0]
"In all three tasks, FITBO-MM, despite using a looser upper bound of the Gaussian mixture entropy, still manages to demonstrate similar, sometimes better, results compared with FITBO.",3.2. Tests with Benchmark Functions,[0],[0]
This shows that the performance of our information-theoretic approach is robust to slightly worse approximation of the Gaussian mixture entropy.,3.2. Tests with Benchmark Functions,[0],[0]
"Finally, we experiment with a series of real-world optimisation problems.",3.3. Test with Real-world Problems,[0],[0]
"The first problem (Boston) returns the L2 validation loss of a 1-hidden layer neural network (Wang and Jegelka, 2017) on the Boston housing dataset (Bache and Lichman, 2013).",3.3. Test with Real-world Problems,[0],[0]
The dataset is randomly partitioned into train/validation/test sets and the neural network is trained with Levenberg-Marquardt optimisation.,3.3. Test with Real-world Problems,[0],[0]
"The 2 variables tuned with Bayesian optimisation are the number
of neurons and the damping factor µ.
The second problem (MNIST-SVM) outputs the classification error of a support vector machine (SVM) classifier on the validation set of the MNIST dataset (LeCun et al., 1998).",3.3. Test with Real-world Problems,[0],[0]
"The SVM classifier adopts a radial basis kernel and the 2 variables to optimise are the kernel scale parameter and the box constraint.
",3.3. Test with Real-world Problems,[0],[0]
"The third problem (Cancer) returns the cross-entropy loss of a 1-hidden layer neural network (Wang and Jegelka, 2017) on the validation set of the breast cancer dataset (Bache and Lichman, 2013).",3.3. Test with Real-world Problems,[0],[0]
"This neural network is trained with the scaled conjugate gradient method and we use Bayesian optimisation methods to tune the number of neurons, the damping factor µ, the µ−increase factor and the µ−decrease factor.
",3.3. Test with Real-world Problems,[0],[0]
"We initialise all Bayesian optimisation algorithms with 3 random observation data and set the observation noise to σ2n = 10
−3.",3.3. Test with Real-world Problems,[0],[0]
All experiments are repeated 40 times.,3.3. Test with Real-world Problems,[0],[0]
"In each case, the ground truth is unknown but our aim is to minimise the validation loss.",3.3. Test with Real-world Problems,[0],[0]
"Thus, the corresponding loss functions are used to compare the performance of various Bayesian optimisation algorithms.
",3.3. Test with Real-world Problems,[0],[0]
Figure 5 shows the median of the best validation losses achieved by all Bayesian optimisation algorithms after n iterations for the Boston and MNIST-SVM problems.,3.3. Test with Real-world Problems,[0],[0]
"Our FITBO and FITBO-MM perform competitively well compared to their information-theoretic counterparts and
all information-theoretic methods outperform EI in these real-world applications.
",3.3. Test with Real-world Problems,[0],[0]
"As for the Cancer problem (Figure 6), FITBO and FITBO-MM converge to the stable median value of the validation loss at a much faster speed than MES and EI and are almost on par with PES.",3.3. Test with Real-world Problems,[0],[0]
"By examining the mean validation loss shown in the right plot of Figure 6, it is evident that both FITBO and FITBO-MM demonstrate better performance than all other methods on average with FITBO gaining a slight advantage over FITBO-MM.",3.3. Test with Real-world Problems,[0],[0]
"Moreover, the comparable performance of FITBO and FITBO-MM in all three real-world tasks re-affirmed the robustness of our approach to entropy approximation as our moment matching technique, while improving the speed of the algorithm, does not really compromise the performance.",3.3. Test with Real-world Problems,[0],[0]
"We have proposed a novel information-theoretic approach for Bayesian optimisation, FITBO.",4. Conclusion,[0],[0]
"With the creative use of the parabolic transformation and the hyperparameter η, FITBO enjoys the merits of less sampling effort, more flexible kernel choices and much simpler implementation in comparison with other information-based methods like PES and MES.",4. Conclusion,[0],[0]
"As a result, its computational speed outperforms current information-based methods by a large margin and even exceeds EI to be on par with PI and GP-UCB.",4. Conclusion,[0],[0]
"While requiring much lower runtime, it still manages to achieve satisfactory optimisation performance which is as good as or even better than PES and MES in a variety of tasks.",4. Conclusion,[0],[0]
"Therefore, FITBO approach offers a very efficient and competitive alternative to existing Bayesian optimisation approaches.",4. Conclusion,[0],[0]
"We wish to thank Roman Garnett and Tom Gunter for the insightful discussions and Zi Wang for sharing the Matlab implementation of EI, PI, GP-UCB, MES and PES.",Acknowledgements,[0],[0]
"We would also like to thank Favour Mandanji Nyikosa, Logan Graham, Arno Blaas and Olga Isupova for their helpful comments about improving the paper.",Acknowledgements,[0],[0]
Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems.,abstractText,[0],[0]
"However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective.",abstractText,[0],[0]
"We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead.",abstractText,[0],[0]
"Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space.",abstractText,[0],[0]
"We demonstrate empirically that FITBO inherits the performance associated with informationtheoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.",abstractText,[0],[0]
Fast Information-theoretic Bayesian Optimisation,title,[0],[0]
"The method of k-nearest neighbours is a fundamental building block of many machine learning algorithms and also has broad applications beyond artificial intelligence, including in statistics, bioinformatics and database systems, e.g. (Biau et al., 2011; Behnam et al., 2013; Eldawy & Mokbel, 2015).",1. Introduction,[0],[0]
"Consequently, since the problem of nearest neighbour search was first posed by Minsky & Papert (1969), it has for decades intrigued the artificial intelligence and theoretical computer science communities alike.",1. Introduction,[0],[0]
"Unfortunately, the myriad efforts at devising efficient algorithms have encountered a recurring obstacle: the
1University of California, Berkeley, CA 94720, United States.",1. Introduction,[0],[0]
"Correspondence to: Ke Li <ke.li@eecs.berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"curse of dimensionality, which describes the phenomenon of query time complexity depending exponentially on dimensionality.",1. Introduction,[0],[0]
"As a result, even on datasets with moderately high dimensionality, practitioners often have resort to naı̈ve exhaustive search.
",1. Introduction,[0],[0]
Two notions of dimensionality are commonly considered.,1. Introduction,[0],[0]
"The more familiar notion, ambient dimensionality, refers to the dimensionality of the space data points are embedded in.",1. Introduction,[0],[0]
"On the other hand, intrinsic dimensionality1 characterizes the intrinsic properties of the data and measures the rate at which the number of points inside a ball grows as a function of its radius.",1. Introduction,[0],[0]
"More precisely, for a dataset with intrinsic dimension d0, any ball of radius r contains at most O(rd 0 ) points.",1. Introduction,[0],[0]
"Intuitively, if the data points are uniformly distributed on a manifold, then the intrinsic dimensionality is roughly the dimensionality of the manifold.
",1. Introduction,[0],[0]
Most existing methods suffer from some form of curse of dimensionality.,1. Introduction,[0],[0]
"Early methods like k-d trees (Bentley, 1975) and R-trees (Guttman, 1984) have query times that grow exponentially in ambient dimensionality.",1. Introduction,[0],[0]
"Later methods (Krauthgamer & Lee, 2004; Beygelzimer et al., 2006; Dasgupta & Freund, 2008) overcame the exponential dependence on ambient dimensionality, but have not been able to escape from an exponential dependence on intrinsic dimensionality.",1. Introduction,[0],[0]
"Indeed, since a linear increase in the intrinsic dimensionality results in an exponential increase in the number of points near a query, the problem seems fundamentally hard when intrinsic dimensionality is high.
",1. Introduction,[0],[0]
"Recently, Li & Malik (2016) proposed an approach known as Dynamic Continuous Indexing (DCI) that successfully reduces the dependence on intrinsic dimensionality from exponential to sublinear, thereby making high-dimensional nearest neighbour search more practical.",1. Introduction,[0],[0]
"The key observation is that the difficulties encountered by many existing methods, including k-d trees and Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998), may arise from their reliance on space partitioning, which is a popular divideand-conquer strategy.",1. Introduction,[0],[0]
"It works by partitioning the vector space into discrete cells and maintaining a data structure
1The measure of intrinsic dimensionality used throughout this paper is the expansion dimension, also known as the KRdimension, which is defined as log
2 c, where c is the expansion rate introduced in (Karger & Ruhl, 2002).
",1. Introduction,[0],[0]
that keeps track of the points lying in each cell.,1. Introduction,[0],[0]
"At query time, these methods simply look up of the contents of the cell containing the query and possibly adjacent cells and perform brute-force search over points lying in these cells.",1. Introduction,[0],[0]
"While this works well in low-dimensional settings, would it work in high dimensions?
",1. Introduction,[0],[0]
"Several limitations of this approach in high-dimensional space are identified in (Li & Malik, 2016).",1. Introduction,[0],[0]
"First, because the volume of space grows exponentially in dimensionality, either the number or the volumes of cells must grow exponentially.",1. Introduction,[0],[0]
"Second, the discretization of the space essentially limits the “field of view” of the algorithm, as it is unaware of points that lie in adjacent cells.",1. Introduction,[0],[0]
"This is especially problematic when the query lies near a cell boundary, as there could be points in adjacent cells that are much closer to the query.",1. Introduction,[0],[0]
"Third, as dimensionality increases, surface area grows faster than volume; as a result, points are increasingly likely to lie near cell boundaries.",1. Introduction,[0],[0]
"Fourth, when the dataset exhibits varying density across space, choosing a good partitioning is non-trivial.",1. Introduction,[0],[0]
"Furthermore, once chosen, the partitioning is fixed and cannot adapt to changes in density arising from updates to the dataset.
",1. Introduction,[0],[0]
"In light of these observations, DCI is built on the idea of avoiding partitioning the vector space.",1. Introduction,[0],[0]
"Instead, it constructs a number of indices, each of which imposes an ordering of all data points.",1. Introduction,[0],[0]
Each index is constructed so that two points with similar ranks in the associated ordering are nearby along a certain random direction.,1. Introduction,[0],[0]
"These indices are then combined to allow for retrieval of points that are close to the query along multiple random directions.
",1. Introduction,[0],[0]
"In this paper, we propose a variant of DCI, which assigns a priority to each index that is used to determine which index to process in the upcoming iteration.",1. Introduction,[0],[0]
"For this reason, we will refer to this algorithm as Prioritized DCI.",1. Introduction,[0],[0]
This simple change results in a significant improvement in the dependence of query time on intrinsic dimensionality.,1. Introduction,[0],[0]
"Specifically, we show a remarkable result: a linear increase in intrinsic dimensionality, which could mean an exponential increase in the number of points near a query, can be mostly counteracted with a corresponding linear increase in the number of indices.",1. Introduction,[0],[0]
"In other words, Prioritized DCI can make a dataset with high intrinsic dimensionality seem almost as easy as a dataset with low intrinsic dimensionality, with just a linear increase in space.",1. Introduction,[0],[0]
"To our knowledge, there had been no exact method that can cope with high intrinsic dimensionality; Prioritized DCI represents the first method that can do so.
",1. Introduction,[0],[0]
We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods.,1. Introduction,[0],[0]
"In particular, compared to LSH, it achieves a 14- to 116-fold reduction in the number of distance evaluations and a 21-fold reduction in the memory usage.",1. Introduction,[0],[0]
There is a vast literature on algorithms for nearest neighbour search.,2. Related Work,[0],[0]
They can be divided into two categories: exact algorithms and approximate algorithms.,2. Related Work,[0],[0]
Early exact algorithms are deterministic and store points in treebased data structures.,2. Related Work,[0],[0]
"Examples include k-d trees (Bentley, 1975), R-trees (Guttman, 1984) and X-trees (Berchtold et al., 1996; 1998), which divide the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons and keep track of the points that lie in each cell.",2. Related Work,[0],[0]
"While their query times are logarithmic in the size of the dataset, they exhibit exponential dependence on the ambient dimensionality.",2. Related Work,[0],[0]
"A different method (Meiser, 1993) partitions the space by intersecting multiple hyperplanes.",2. Related Work,[0],[0]
"It effectively trades off space for time and achieves polynomial query time in ambient dimensionality at the cost of exponential space complexity in ambient dimensionality.
",2. Related Work,[0],[0]
"To avoid poor performance on worst-case configurations of the data, exact randomized algorithms have been proposed.",2. Related Work,[0],[0]
"Spill trees (Liu et al., 2004), RP trees (Dasgupta & Freund, 2008) and virtual spill trees (Dasgupta & Sinha, 2015) extend the ideas behind k-d trees by randomizing the orientations of hyperplanes that partition the space into halfspaces at each node of the tree.",2. Related Work,[0],[0]
"While randomization enables them to avoid exponential dependence on the ambient dimensionality, their query times still scale exponentially in the intrinsic dimensionality.",2. Related Work,[0],[0]
"Whereas these methods rely on space partitioning, other algorithms (Orchard, 1991; Clarkson, 1999; Karger & Ruhl, 2002) have been proposed that utilize local search strategies.",2. Related Work,[0],[0]
These methods start with a random point and look in the neighbourhood of the current point to find a new point that is closer to the query than the original in each iteration.,2. Related Work,[0],[0]
"Like space partitioningbased approaches, the query time of (Karger & Ruhl, 2002) scales exponentially in the intrinsic dimensionality.",2. Related Work,[0],[0]
"While
the query times of (Orchard, 1991; Clarkson, 1999) do not exhibit such undesirable dependence, their space complexities are quadratic in the size of the dataset, making them impractical for large datasets.",2. Related Work,[0],[0]
A different class of algorithms performs search in a coarse-to-fine manner.,2. Related Work,[0],[0]
"Examples include navigating nets (Krauthgamer & Lee, 2004), cover trees (Beygelzimer et al., 2006) and rank cover trees (Houle & Nett, 2015), which maintain sets of subsampled data points at different levels of granularity and descend through the hierarchy of neighbourhoods of decreasing radii around the query.",2. Related Work,[0],[0]
"Unfortunately, the query times of these methods again scale exponentially in the intrinsic dimensionality.
",2. Related Work,[0],[0]
"Due to the difficulties of devising efficient algorithms for the exact version of the problem, there has been extensive work on approximate algorithms.",2. Related Work,[0],[0]
"Under the approximate setting, returning any point whose distance to the query is within a factor of 1 + ✏ of the distance between the query and the true nearest neighbour is acceptable.",2. Related Work,[0],[0]
Many of the same strategies are employed by approximate algorithms.,2. Related Work,[0],[0]
"Methods based on tree-based space partitioning (Arya et al., 1998) and local search (Arya & Mount, 1993) have been developed; like many exact algorithms, their query times also scale exponentially in the ambient dimensionality.",2. Related Work,[0],[0]
"Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998; Datar et al., 2004; Andoni & Indyk, 2006) partitions the space into regular cells, whose shapes are implicitly defined by the choice of the hash function.",2. Related Work,[0],[0]
"It achieves a query time of O(dn⇢) using O(dn1+⇢) space, where d is the ambient dimensionality, n is the dataset size and ⇢ ⇡ 1/(1 + ✏)2 for large n in Euclidean space,
though the dependence on intrinsic dimensionality is not made explicit.",2. Related Work,[0],[0]
"In practice, the performance of LSH degrades on datasets with large variations in density, due to the uneven distribution of points across cells.",2. Related Work,[0],[0]
"Consequently, various data-dependent hashing schemes have been proposed (Paulevé et al., 2010; Weiss et al., 2009; Andoni & Razenshteyn, 2015); unlike data-independent hashing schemes, however, they do not allow dynamic updates to the dataset.",2. Related Work,[0],[0]
"A related approach (Jégou et al., 2011) decomposes the space into mutually orthogonal axis-aligned subspaces and independently partitions each subspace.",2. Related Work,[0],[0]
It has a query time linear in the dataset size and no known guarantee on the probability of correctness under the exact or approximate setting.,2. Related Work,[0],[0]
"A different approach (Anagnostopoulos et al., 2015) projects the data to a lower dimensional space that approximately preserves approximate nearest neighbour relationships and applies other approximate algorithms like BBD trees (Arya et al., 1998) to the projected data.",2. Related Work,[0],[0]
Its query time is also linear in ambient dimensionality and sublinear in the dataset size.,2. Related Work,[0],[0]
"Unlike LSH, it uses space linear in the dataset size, at the cost of longer query time than LSH.",2. Related Work,[0],[0]
"Unfortunately, its query time is exponential in intrinsic dimensionality.
",2. Related Work,[0],[0]
"Our work is most closely related to Dynamic Continuous Indexing (DCI) (Li & Malik, 2016), which is an exact randomized algorithm for Euclidean space whose query time is linear in ambient dimensionality, sublinear in dataset size and sublinear in intrinsic dimensionality and uses space linear in the dataset size.",2. Related Work,[0],[0]
"Rather than partitioning the vector space, it uses multiple global one-dimensional indices, each of which orders data points along a certain random direction and combines these indices to find points that are near the query along multiple random directions.",2. Related Work,[0],[0]
"The proposed algorithm builds on the ideas introduced by DCI and achieves a significant improvement in the dependence on intrinsic dimensionality.
",2. Related Work,[0],[0]
A summary of the query times of various prior algorithms and the proposed algorithm is presented in Table 1 and their growth as a function of intrinsic dimensionality is illustrated in Figure 1.,2. Related Work,[0],[0]
"DCI constructs a data structure consisting of multiple composite indices of data points, each of which in turn consists of a number of simple indices.",3. Prioritized DCI,[0],[0]
Each simple index orders data points according to their projections along a particular random direction.,3. Prioritized DCI,[0],[0]
"Given a query, for every composite index, the algorithm finds points that are near the query in every constituent simple index, which are known as candidate points, and adds them to a set known as the candidate set.",3. Prioritized DCI,[0],[0]
"The true distances from the query to every candidate point are evaluated and the ones that are among the k clos-
est to the query are returned.
",3. Prioritized DCI,[0],[0]
"More concretely, each simple index is associated with a random direction and stores the projections of every data point along the direction.",3. Prioritized DCI,[0],[0]
"They are implemented using standard data structures that maintain one-dimensional ordered sequences of elements, like self-balancing binary search trees (Bayer, 1972; Guibas & Sedgewick, 1978) or skip lists (Pugh, 1990).",3. Prioritized DCI,[0],[0]
"At query time, the algorithm projects the query along the projection directions associated with each simple index and finds the position where the query would have been inserted in each simple index, which takes logarithmic time.",3. Prioritized DCI,[0],[0]
"It then iterates over, or visits, data points in each simple index in the order of their distances to the query under projection, which takes constant time for each iteration.",3. Prioritized DCI,[0],[0]
"As it iterates, it keeps track of how many times each data point has been visited across all simple indices of each composite index.",3. Prioritized DCI,[0],[0]
"If a data point has been visited in every constituent simple index, it is added to the candidate set and is said to have been retrieved from the composite index.
",3. Prioritized DCI,[0],[0]
"Algorithm 1 Data structure construction procedure
Require:",3. Prioritized DCI,[0],[0]
"A dataset D of n points p1, . . .",3. Prioritized DCI,[0],[0]
", pn, the number of simple indices m that constitute a composite index and the number of composite indices L function CONSTRUCT(D,m,L)
{ujl}j2[m],l2[L] mL random unit vectors in Rd {Tjl}j2[m],l2[L] mL empty binary search trees or skip lists for j = 1 to m do
for l = 1 to L do for i = 1 to n do
pijl hpi, ujli Insert (pijl, i) into Tjl with p",3. Prioritized DCI,[0],[0]
"i jl being the key and
i being the value end for
end for
end for
return {(Tjl, ujl)}j2[m],l2[L] end function
DCI has a number of appealing properties compared to methods based on space partitioning.",3. Prioritized DCI,[0],[0]
"Because points are visited by rank rather than location in space, DCI performs well on datasets with large variations in data density.",3. Prioritized DCI,[0],[0]
It naturally skips over sparse regions of the space and concentrates more on dense regions of the space.,3. Prioritized DCI,[0],[0]
"Since construction of the data structure does not depend on the dataset, the algorithm supports dynamic updates to the dataset, while being able to automatically adapt to changes in data density.",3. Prioritized DCI,[0],[0]
"Furthermore, because data points are represented in the indices as continuous values without being discretized, the granularity of discretization does not need to be chosen at construction time.",3. Prioritized DCI,[0],[0]
"Consequently, the same data structure can support queries at varying desired levels of accuracy, which allows a different speed-vs-accuracy trade-off to be
made for each individual query.
",3. Prioritized DCI,[0],[0]
Prioritized DCI differs from standard DCI in the order in which points from different simple indices are visited.,3. Prioritized DCI,[0],[0]
"In standard DCI, the algorithm cycles through all constituent simple indices of a composite index at regular intervals and visits exactly one point from each simple index in each pass.",3. Prioritized DCI,[0],[0]
"In Prioritized DCI, the algorithm assigns a priority to each constituent simple index; in each iteration, it visits the upcoming point from the simple index with the highest priority and updates the priority at the end of the iteration.",3. Prioritized DCI,[0],[0]
"The priority of a simple index is set to the negative absolute difference between the query projection and the next data point projection in the index.
",3. Prioritized DCI,[0],[0]
"Algorithm 2 k-nearest neighbour querying procedure
Require: Query point q in Rd, binary search trees/skip lists and their associated projection vectors {(Tjl, ujl)}j2[m],l2[L], the number of points to retrieve k
0 and the number of points to visit k 1
in each composite index function QUERY(q, {(Tjl, ujl)}j,l, k0, k1)
",3. Prioritized DCI,[0],[0]
Cl array of size n with entries initialized to 0 8l 2,3. Prioritized DCI,[0],[0]
"[L] qjl hq, ujli 8j 2",3. Prioritized DCI,[0],[0]
"[m], l 2",3. Prioritized DCI,[0],[0]
[L] Sl ; 8l 2,3. Prioritized DCI,[0],[0]
[L] Pl empty priority queue 8l 2,3. Prioritized DCI,[0],[0]
"[L] for l = 1 to L do
for j = 1 to m do (p(1)jl , h (1)
jl )",3. Prioritized DCI,[0],[0]
"the node in Tjl whose key is the closest to qjl
Insert (p(1)jl , h (1) jl ) with priority |p (1) jl qjl| into Pl
end for
end for for i0 = 1 to k 1
1 do for l = 1 to L do
if |Sl| < k0 then (p(i)jl , h (i) jl )",3. Prioritized DCI,[0],[0]
"the node with the highest priority
in Pl Remove (p(i)jl , h (i) jl ) from Pl and insert the node
in Tjl whose key is the next closest to qjl, which is denoted as (p(i+1)jl , h (i+1) jl ), with
priority |p(i+1)jl qjl| into Pl Cl[h",3. Prioritized DCI,[0],[0]
(i) jl ],3. Prioritized DCI,[0],[0]
Cl[h (i) jl ],3. Prioritized DCI,[0],[0]
+ 1 if Cl[h (i) jl,3. Prioritized DCI,[0],[0]
"] = m then
Sl Sl [ {h(i)jl } end if
end if
end for
end for return k points in S
l2[L]",3. Prioritized DCI,[0],[0]
"Sl that are the closest in Euclidean distance in Rd to q
end function
Intuitively, this ensures data points are visited in the order of their distances to the query under projection.",3. Prioritized DCI,[0],[0]
"Because data points are only retrieved from a composite index when they have been visited in all constituent simple indices, data
points are retrieved in the order of the maximum of their distances to the query along multiple projection directions.",3. Prioritized DCI,[0],[0]
"Since distance under projection forms a lower bound on the true distance, the maximum projected distance approaches the true distance as the number of projection directions increases.",3. Prioritized DCI,[0],[0]
"Hence, in the limit as the number of simple indices approaches infinity, data points are retrieved in the ideal order, that is, the order of their true distances to the query.
",3. Prioritized DCI,[0],[0]
The construction and querying procedures of Prioritized DCI are presented formally in Algorithms 1 and 2.,3. Prioritized DCI,[0],[0]
"To ensure the algorithm retrieves the exact knearest neighbours with high probability, the analysis in the next section shows that one should choose k 0 2 ⌦(kmax(log(n/k), (n/k)1 m/d0)) and k 1
2 ⌦(mkmax(log(n/k), (n/k)1 1/d 0 )), where d0 denotes the intrinsic dimensionality.",3. Prioritized DCI,[0],[0]
"Though because this assumes worst-case configuration of data points, it may be overly conservative in practice; so, these parameters may be chosen by cross-validation.
",3. Prioritized DCI,[0],[0]
We summarize the time and space complexities of Prioritized DCI in Table 2.,3. Prioritized DCI,[0],[0]
"Notably, the first term of the query complexity, which dominates when the ambient dimensionality d is large, has a more favourable dependence on the intrinsic dimensionality d0 than the query complexity of standard DCI.",3. Prioritized DCI,[0],[0]
"In particular, a linear increase in the intrinsic dimensionality, which corresponds to an exponential increase in the expansion rate, can be mitigated by just a linear increase in the number of simple indices m.",3. Prioritized DCI,[0],[0]
"This suggests that Prioritized DCI can better handle datasets with high intrinsic dimensionality than standard DCI, which is confirmed by empirical evidence later in this paper.",3. Prioritized DCI,[0],[0]
We analyze the time and space complexities of Prioritized DCI below and derive the stopping condition of the algorithm.,4. Analysis,[0],[0]
"Because the algorithm uses standard data structures, analysis of the construction time, insertion time, deletion time and space complexity is straightforward.",4. Analysis,[0],[0]
"Hence, this section focuses mostly on analyzing the query time.
",4. Analysis,[0],[0]
"In high-dimensional space, query time is dominated by the
time spent on evaluating true distances between candidate points and the query.",4. Analysis,[0],[0]
"Therefore, we need to find the number of candidate points that must be retrieved to ensure the algorithm succeeds with high probability.",4. Analysis,[0],[0]
"To this end, we derive an upper bound on the failure probability for any given number of candidate points.",4. Analysis,[0],[0]
The algorithm fails if sufficiently many distant points are retrieved from each composite index before some of the true k-nearest neighbours.,4. Analysis,[0],[0]
"We decompose this event into multiple (dependent) events, each of which is the event that a particular distant point is retrieved before some true k-nearest neighbours.",4. Analysis,[0],[0]
"Since points are retrieved in the order of their maximum projected distance, this event happens when the maximum projected distance of the distant point is less than that of a true k-nearest neighbour.",4. Analysis,[0],[0]
We start by finding an upper bound on the probability of this event.,4. Analysis,[0],[0]
"To simplify notation, we initially consider displacement vectors from the query to each data point, and so relationships between projected distances of triplets of points translate relationships between projected lengths of pairs of displacement vectors.
",4. Analysis,[0],[0]
We start by examining the event that a vector under random one-dimensional projection satisfies some geometric constraint.,4. Analysis,[0],[0]
"We then find an upper bound on the probability that some combinations of these events occur, which is related to the failure probability of the algorithm.",4. Analysis,[0],[0]
Lemma 1.,4. Analysis,[0],[0]
"Let vl, vs 2 Rd be such that vl
2
> vs
2
,
u0j M j=1 be i.i.d. unit vectors in Rd drawn uniformly
at random.",4. Analysis,[0],[0]
"Then Pr
maxj hvl, u0ji  vs
2
=
1 2⇡ cos 1
vs
2
/ vl
2
M .
",4. Analysis,[0],[0]
Proof.,4. Analysis,[0],[0]
"The event
maxj hvl, u0ji  kvsk 2 is equivalent to the event that
hvl, u0ji  kvsk 2 8j , which is the intersection of the events
hvl, u0ji  kvsk 2 .",4. Analysis,[0],[0]
"Because u0j’s are drawn independently, these events are independent.
",4. Analysis,[0],[0]
"Let ✓j be the angle between vl and u0j , so that hvl, u0ji =
vl
2 cos ✓j .",4. Analysis,[0],[0]
"Since u0j is drawn uniformly, ✓j is uniformly distributed on [0, 2⇡].",4. Analysis,[0],[0]
"Hence,
Pr
✓
max
j
n
hvl, u0ji
o
 kvsk 2
◆
=
M Y
j=1
Pr
⇣
hvl, u0ji  kvsk 2
⌘
=
M Y
j=1
Pr
✓ |cos ✓j |  kvsk 2
kvlk 2
◆
=
M Y
j=1
✓
2Pr
✓ ✓j 2  cos 1 ✓ kvsk 2
kvlk 2
◆ ,⇡ cos 1 ✓ kvsk 2
kvlk 2
◆ ◆◆
=
✓
1 2 ⇡ cos
1 ✓ kvsk 2
kvlk 2
◆◆M
Lemma 2.",4. Analysis,[0],[0]
"For any set of events {Ei}Ni=1, the probability that at least k0 of them happen is at most 1k0 PN i=1",4. Analysis,[0],[0]
"Pr (Ei).
",4. Analysis,[0],[0]
Proof.,4. Analysis,[0],[0]
"For any set T ✓ [N ], define ˜ET to be the intersection of events indexed by T and complements of events not indexed by T , i.e. ˜ET = T i2T Ei \ T i/2T Ei .",4. Analysis,[0],[0]
Observe that n,4. Analysis,[0],[0]
˜ET,4. Analysis,[0],[0]
"o
T✓[N ] are disjoint and that for any I ✓",4. Analysis,[0],[0]
"[N ],
T i2I Ei = S T◆I ˜ET .",4. Analysis,[0],[0]
"The event that at least k0 of Ei’s happen is S
I✓[N ]:|I|=k0 T i2I Ei, which is equivalent to S
I✓[N ]:|I|=k0 S T◆I ˜ET = S T✓[N ]:|T | k0 ˜ET .",4. Analysis,[0],[0]
We will henceforth use T to denote {T ✓ [N ] : |T | k0}.,4. Analysis,[0],[0]
"Since T is a finite set, we can impose an ordering on its elements and denote the lth element as Tl.",4. Analysis,[0],[0]
"The event can therefore be rewritten as
S|T | l=1 ˜ETl .
",4. Analysis,[0],[0]
"Define E0i,j to be Ei \ ⇣ S|T |",4. Analysis,[0],[0]
l=j+1 ˜ETl ⌘ .,4. Analysis,[0],[0]
"We claim
that PN
i=1",4. Analysis,[0],[0]
"Pr E0i,j
k0 Pj
l=1 Pr
⇣
˜ETl
⌘
for all j 2 {0, . . .",4. Analysis,[0],[0]
", |T |}.",4. Analysis,[0],[0]
"We will show this by induction on j.
For j = 0, the claim is vacuously true because probabilities are non-negative.",4. Analysis,[0],[0]
"For j > 0, we observe that E0i,j = ⇣
E0i,j \",4. Analysis,[0],[0]
"˜ETj ⌘ [ ⇣ E0i,j \",4. Analysis,[0],[0]
"˜ETj ⌘ = E0i,j 1 [ ⇣ E0i,j \",4. Analysis,[0],[0]
"˜ETj ⌘ for all i. Since E0i,j \ ˜ETj and E0i,j \ ˜ETj are disjoint, Pr
E0i,j = Pr E0i,j 1 + Pr
⇣ E0i,j \",4. Analysis,[0],[0]
"˜ETj ⌘ .
Consider the quantity P
i2Tj Pr
E0i,j , which is P
i2Tj
⇣
Pr E0i,j 1 + Pr
⇣ E0i,j \",4. Analysis,[0],[0]
"˜ETj ⌘⌘ by the above
observation.",4. Analysis,[0],[0]
"For each i 2 Tj , ˜ETj ✓ Ei, and so ˜ETj \ ⇣ S|T | l=j+1 ˜ETl ⌘ ✓ Ei \ ⇣ S|T | l=j+1 ˜ETl ⌘ = E0i,j . Be-
cause n
˜ETl
o|T |
l=j are disjoint, ˜ETj \
⇣
S|T |",4. Analysis,[0],[0]
"l=j+1 ˜ETl
⌘
= ˜ETj .
",4. Analysis,[0],[0]
"Hence, ˜ETj ✓ E0i,j and so E0i,j \ ˜ETj = ˜ETj .",4. Analysis,[0],[0]
"Thus, P
i2Tj Pr
E0i,j = |Tj |Pr ⇣ ˜ETj ⌘ + P
i2Tj Pr
E0i,j 1 .
",4. Analysis,[0],[0]
"It follows that PN
i=1",4. Analysis,[0],[0]
"Pr E0i,j
= |Tj |Pr ⇣ ˜ETj ⌘ +
P
i2Tj Pr
E0i,j 1 + P
i/2Tj Pr
E0i,j .",4. Analysis,[0],[0]
"Because
Pr E0i,j
= Pr E0i,j 1 + Pr
⇣ E0i,j \ ˜ETj ⌘
Pr E0i,j 1 and |Tj | k0, PN i=1",4. Analysis,[0],[0]
"Pr E0i,j k0Pr ⇣
˜ETj
⌘
+ PN i=1",4. Analysis,[0],[0]
"Pr E0i,j 1 .",4. Analysis,[0],[0]
"By the inductive
hypothesis, PN
i=1",4. Analysis,[0],[0]
"Pr E0i,j 1
k0 Pj 1
l=1 Pr
⇣
˜ETl
⌘
.
",4. Analysis,[0],[0]
"Therefore, PN
i=1",4. Analysis,[0],[0]
"Pr E0i,j
k0 Pj
l=1 Pr
⇣
˜ETl
⌘
, which concludes the induction argument.
",4. Analysis,[0],[0]
"The lemma is a special case of this claim when j = |T |, since E0i,|T | = Ei and P|T | l=1",4. Analysis,[0],[0]
Pr ⇣ ˜ETl ⌘ = Pr ⇣ S|T |,4. Analysis,[0],[0]
l=1,4. Analysis,[0],[0]
"˜ETl ⌘ .
",4. Analysis,[0],[0]
"Combining the above yields the following theorem, the proof of which is found in the supplementary material.
",4. Analysis,[0],[0]
Theorem 1.,4. Analysis,[0],[0]
Let vli N i=1,4. Analysis,[0],[0]
"and vsi0 N 0 i0=1 be sets of vectors such that
vli
2
>
vsi0
2 8i 2",4. Analysis,[0],[0]
"[N ], i0 2",4. Analysis,[0],[0]
[N 0].,4. Analysis,[0],[0]
"Furthermore, let
u0ij i2[N ],j2[M ] be random uniformly distributed unit vectors such that u0i1, . . .",4. Analysis,[0],[0]
", u0iM are independent for any given i. Consider the events
9vsi0 s.t. maxj hvli, u0iji  vsi0
2
N i=1 .",4. Analysis,[0],[0]
"The prob-
ability that at least k0 of these events occur is at most 1k0 PN i=1 1 2⇡ cos 1 vs max
2
/ vli
2
M , where
vs max
2
= maxi0 vsi0
2
.",4. Analysis,[0],[0]
"Furthermore, if k0 = N , it is
at most mini2[N ] n
1 2⇡ cos 1 vs max
2
/ vli
2
M o
.
",4. Analysis,[0],[0]
We now apply the results above to analyze specific properties of the algorithm.,4. Analysis,[0],[0]
"For convenience, instead of working directly with intrinsic dimensionality, we will analyze the query time in terms of a related quantity, global relative sparsity, as defined in (Li & Malik, 2016).",4. Analysis,[0],[0]
We reproduce its definition below for completeness.,4. Analysis,[0],[0]
Definition 1.,4. Analysis,[0],[0]
"Given a dataset D ✓ Rd, let Bp(r) be the set of points in D that are within a ball of radius r around a point p.",4. Analysis,[0],[0]
"A dataset D has global relative sparsity of (⌧, ) if for all r and p 2 Rd such that |Bp(r)| ⌧ , |Bp( r)|  2 |Bp(r)|, where 1.
",4. Analysis,[0],[0]
"Global relative sparsity is related to the expansion rate (Karger & Ruhl, 2002) and intrinsic dimensionality in the following way: a dataset with global relative sparsity of (⌧, ) has (⌧, 2(1/ log2 ))-expansion and intrinsic dimensionality of 1/ log
2
.
",4. Analysis,[0],[0]
"Below we derive two upper bounds on the probability that some of the true k-nearest neighbours are missing from the set of candidate points retrieved from a given composite index, which are in expressed in terms of k
0 and k 1 respectively.",4. Analysis,[0],[0]
"These results inform us how k
0 and k 1 should be chosen to ensure the querying procedure returns the correct results with high probability.",4. Analysis,[0],[0]
"In the results that follow, we use {p(i)}ni=1 to denote a re-ordering of the points {pi}ni=1 so that p(i) is the ith closest point to the query q.",4. Analysis,[0],[0]
Proofs are found in the supplementary material.,4. Analysis,[0],[0]
Lemma 3.,4. Analysis,[0],[0]
Consider points in the order they are retrieved from a composite index that consists of m simple indices.,4. Analysis,[0],[0]
"The probability that there are at least n
0 points that are not the true k-nearest neighbours but are retrieved before some of them is at most
1 n0 k Pn i=2k+1 1 2⇡ cos 1 p(k) q 2 / p(i) q 2 m .",4. Analysis,[0],[0]
Lemma 4.,4. Analysis,[0],[0]
Consider point projections in a composite index that consists of m simple indices in the order they are visited.,4. Analysis,[0],[0]
"The probability that n
0 point projections that are not of the true k-nearest neighbours are visited before all true k-nearest neighbours have been retrieved is at most
m n0",4. Analysis,[0],[0]
"mk Pn i=2k+1 1 2⇡ cos 1 p(k) q 2 / p(i) q 2 .
",4. Analysis,[0],[0]
Lemma 5.,4. Analysis,[0],[0]
"On a dataset with global relative sparsity (k, ), the quantity Pn
i=2k+1 1 2⇡ cos 1
p(k) q
2
/ p(i) q
2
m is
at most O kmax(log(n/k), (n/k)1 m log2 ) .
",4. Analysis,[0],[0]
Lemma 6.,4. Analysis,[0],[0]
"For a dataset with global relative sparsity (k, ) and a given composite index consisting of m simple indices, there is some k
0 2 ⌦(kmax(log(n/k), (n/k)1 m log2 )) such that the probability that the candidate points retrieved from the composite index do not include some of the true k-nearest neighbours is at most some constant ↵
0
< 1.
",4. Analysis,[0],[0]
Lemma 7.,4. Analysis,[0],[0]
"For a dataset with global relative sparsity (k, ) and a given composite index consisting of m simple indices, there is some k
1 2 ⌦(mkmax(log(n/k), (n/k)1 log2 )) such that the probability that the candidate points retrieved from the composite index do not include some of the true k-nearest neighbours is at most some constant ↵
1
< 1.
Theorem 2.",4. Analysis,[0],[0]
"For a dataset with global relative sparsity (k, ), for any ✏ > 0, there is some L, k 0 2 ⌦(kmax(log(n/k), (n/k)1 m log2 )) and k 1
2 ⌦(mkmax(log(n/k), (n/k)1 log2 )) such that the algorithm returns the correct set of k-nearest neighbours with probability of at least 1 ✏.
",4. Analysis,[0],[0]
"Now that we have found a choice of k 0 and k 1 that suffices to ensure correctness with high probability, we can derive a bound on the query time that guarantees correctness.",4. Analysis,[0],[0]
"We then analyze the time complexity for construction, insertion and deletion and the space complexity.",4. Analysis,[0],[0]
"Proofs of the following are found in the supplementary material.
",4. Analysis,[0],[0]
Theorem 3.,4. Analysis,[0],[0]
"For a given number of simple indices m, the algorithm takes O ⇣ dkmax(log(n/k), (n/k)1 m/d 0 )",4. Analysis,[0],[0]
"+
mk logm ⇣ max(log(n/k), (n/k)1 1/d 0 )",4. Analysis,[0],[0]
"⌘⌘
time to retrieve the k-nearest neighbours at query time, where d0 denotes the intrinsic dimensionality.
",4. Analysis,[0],[0]
Theorem 4.,4. Analysis,[0],[0]
"For a given number of simple indices m, the algorithm takes O(m(dn+n log n)) time to preprocess the data points in D at construction time.
",4. Analysis,[0],[0]
Theorem 5.,4. Analysis,[0],[0]
"The algorithm requires O(m(d+log n)) time to insert a new data point and O(m log n) time to delete a data point.
",4. Analysis,[0],[0]
Theorem 6.,4. Analysis,[0],[0]
The algorithm requires O(mn) space in addition to the space used to store the data.,4. Analysis,[0],[0]
"We compare the performance of Prioritized DCI to that of standard DCI (Li & Malik, 2016), product quantization (Jégou et al., 2011) and LSH (Datar et al., 2004), which is perhaps the algorithm that is most widely used
in high-dimensional settings.",5. Experiments,[0],[0]
"Because LSH operates under the approximate setting, in which the performance metric of interest is how close the returned points are to the query rather than whether they are the true k-nearest neighbours.",5. Experiments,[0],[0]
"All algorithms are evaluated in terms of the time they would need to achieve varying levels of approximation quality.
",5. Experiments,[0],[0]
"Evaluation is performed on two datasets, CIFAR100 (Krizhevsky & Hinton, 2009) and MNIST (LeCun et al., 1998).",5. Experiments,[0],[0]
"CIFAR-100 consists of 60, 000 colour images of 100 types of objects in natural scenes and MNIST consists of 70, 000 grayscale images of handwritten digits.",5. Experiments,[0],[0]
"The images in CIFAR-100 have a size of 32⇥ 32 and three colour channels, and the images in MNIST have a size of 28⇥ 28 and a single colour channel.",5. Experiments,[0],[0]
We reshape each image into a vector whose entries represent pixel intensities at different locations and colour channels in the image.,5. Experiments,[0],[0]
"So, each vector has a dimensionality of 32⇥32⇥3 = 3072 for CIFAR-100 and 28 ⇥ 28 = 784 for MNIST.",5. Experiments,[0],[0]
"Note that the dimensionalities under consideration are much higher than those typically used to evaluate prior methods.
",5. Experiments,[0],[0]
"For the purposes of nearest neighbour search, MNIST is a more challenging dataset than CIFAR-100.",5. Experiments,[0],[0]
"This is because images in MNIST are concentrated around a few modes; consequently, data points form dense clusters, leading to higher intrinsic dimensionality.",5. Experiments,[0],[0]
"On the other hand, images in CIFAR-100 are more diverse, and so data points are more dispersed in space.",5. Experiments,[0],[0]
"Intuitively, it is much harder to find the closest digit to a query among 6999 other digits of the same category that are all plausible near neighbours than to find the most similar natural image among a few other natural images with similar appearance.",5. Experiments,[0],[0]
"Later results show that all algorithms need fewer distance evaluations to achieve the same level of approximation quality on CIFAR100 than on MNIST.
",5. Experiments,[0],[0]
"We evaluate performance of all algorithms using crossvalidation, where we randomly choose ten different splits of query vs. data points.",5. Experiments,[0],[0]
"Each split consists of 100 points from the dataset that serve as queries, with the remainder designated as data points.",5. Experiments,[0],[0]
"We use each algorithm to retrieve the 25 nearest neighbours at varying levels of approximation quality and report mean performance and standard deviation over all splits.
",5. Experiments,[0],[0]
"Approximation quality is measured using the approximation ratio, which is defined to be the ratio of the radius of the ball containing the set of true k-nearest neighbours to the radius of the ball containing the set of approximate knearest neighbours returned by the algorithm.",5. Experiments,[0],[0]
"The closer the approximation ratio is to 1, the higher the approximation quality.",5. Experiments,[0],[0]
"In high dimensions, the time taken to compute true distances between the query and the candidate points dominates query time, so the number of distance evalua-
tions can be used as an implementation-independent proxy for the query time.
",5. Experiments,[0],[0]
"For LSH, we used 24 hashes per table and 100 tables, which we found to achieve the best approximation quality given the memory constraints.",5. Experiments,[0],[0]
"For product quantization, we used a data-independent codebook with 256 entries so that the algorithm supports dynamic updates.",5. Experiments,[0],[0]
"For standard DCI, we used the same hyparameter settings used in (Li & Malik, 2016) (m = 25 and L = 2 on CIFAR-100 and m = 15 and L = 3 on MNIST).",5. Experiments,[0],[0]
"For Prioritized DCI, we used two different settings: one that matches the hyperparameter settings of standard DCI, and another that uses less space (m = 10 and L = 2 on both CIFAR-100 and MNIST).
",5. Experiments,[0],[0]
We plot the number of distance evaluations that each algorithm requires to achieve each desired level of approximation ratio in Figure 2.,5. Experiments,[0],[0]
"As shown, on CIFAR-100, under the same hyperparameter setting used by standard DCI, Prioritized DCI requires 87.2% to 92.5% fewer distance evaluations than standard DCI, 91.7% to 92.8% fewer distance evaluations than product quantization, and 90.9% to 93.8% fewer distance evaluations than LSH to achieve same levels approximation quality, which represents a 14-fold reduction in the number of distance evaluations relative to LSH on average.",5. Experiments,[0],[0]
"Under the more space-efficient hyperparameter setting, Prioritized DCI achieves a 6-fold reduction compared to LSH.",5. Experiments,[0],[0]
"On MNIST, under the same hyperparameter setting used by standard DCI, Prioritized DCI requires 96.4% to 97.0% fewer distance evaluations than standard DCI, 87.1% to 89.8% fewer distance evaluations than product quantization, and 98.8% to 99.3% fewer distance evaluations than LSH, which represents a 116-fold reduction relative to LSH on average.",5. Experiments,[0],[0]
"Under the more space-efficient hyperparameter setting, Prioritized DCI achieves a 32-fold reduction compared to LSH.
",5. Experiments,[0],[0]
"We compare the space efficiency of Prioritized DCI to that
of standard DCI and LSH.",5. Experiments,[0],[0]
"As shown in Figure 3 in the supplementary material, compared to LSH, Prioritized DCI uses 95.5% less space on CIFAR-100 and 95.3% less space on MNIST under the same hyperparameter settings used by standard DCI.",5. Experiments,[0],[0]
This represents a 22-fold reduction in memory consumption on CIFAR-100 and a 21-fold reduction on MNIST.,5. Experiments,[0],[0]
"Under the more space-efficient hyperparameter setting, Prioritized DCI uses 98.2% less space on CIFAR-100 and 97.9% less space on MNIST relative to LSH, which represents a 55-fold reduction on CIFAR-100 and a 48-fold reduction on MNIST.
",5. Experiments,[0],[0]
"In terms of wall-clock time, our implementation of Prioritized DCI takes 1.18 seconds to construct the data structure and execute 100 queries on MNIST, compared to 104.71 seconds taken by LSH.",5. Experiments,[0],[0]
"In this paper, we presented a new exact randomized algorithm for k-nearest neighbour search, which we refer to as Prioritized DCI.",6. Conclusion,[0],[0]
We showed that Prioritized DCI achieves a significant improvement in terms of the dependence of query time complexity on intrinsic dimensionality compared to standard DCI.,6. Conclusion,[0],[0]
"Specifically, Prioritized DCI can to a large extent counteract a linear increase in the intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, using just a linear increase in the number of simple indices.",6. Conclusion,[0],[0]
"Empirical results validated the effectiveness of Prioritized DCI in practice, demonstrating the advantages of Prioritized DCI over prior methods in terms of speed and memory usage.
Acknowledgements.",6. Conclusion,[0],[0]
This work was supported by DARPA W911NF-16-1-0552.,6. Conclusion,[0],[0]
Ke Li thanks the Natural Sciences and Engineering Research Council of Canada (NSERC) for fellowship support.,6. Conclusion,[0],[0]
"Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exponential dependence on either the ambient or the intrinsic dimensionality.",abstractText,[0],[0]
"Dynamic Continuous Indexing (DCI) (Li & Malik, 2016) offers a promising way of circumventing the curse and successfully reduces the dependence of query time on intrinsic dimensionality from exponential to sublinear.",abstractText,[0],[0]
"In this paper, we propose a variant of DCI, which we call Prioritized DCI, and show a remarkable improvement in the dependence of query time on intrinsic dimensionality.",abstractText,[0],[0]
"In particular, a linear increase in intrinsic dimensionality, or equivalently, an exponential increase in the number of points near a query, can be mostly counteracted with just a linear increase in space.",abstractText,[0],[0]
We also demonstrate empirically that Prioritized DCI significantly outperforms prior methods.,abstractText,[0],[0]
"In particular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumption by a factor of 21.",abstractText,[0],[0]
Fast k-Nearest Neighbour Search via Prioritized DCI,title,[0],[0]
"As a natural extension of finite sets S (equivalently, {0, 1}S), optimization of discrete functions on the integer lattice NS has received attention recently (Alon et al., 2012; Demaine et al., 2014; Soma & Yoshida, 2015).",1. Introduction,[0],[0]
"As an example, consider the placement of sensors in a water network (Krause et al., 2008a); in the set version, each sensor takes a value in {0, 1}, which corresponds to whether the sensor was placed.",1. Introduction,[0],[0]
"In the lattice version (Soma & Yoshida, 2015), each sensor has a power level in {0, . . .",1. Introduction,[0],[0]
", b} ⊆ N, to which the sensitivity of the sensor is correlated.",1. Introduction,[0],[0]
"As a second example, consider the influence maximization problem (Kempe et al., 2003); instead of the binary seeding of a user, the lattice version enables partial incentives or discounts to be used (Demaine et al., 2014).
",1. Introduction,[0],[0]
"Although many results from the optimization of submodular set functions have been generalized to the integer lattice (Soma & Yoshida, 2015; 2016; Ene & Nguyen, 2016), many objective functions arising from applications are not submodular (Bian et al., 2017b; Lin et al., 2017; Das &
1University of Florida, Gainesville, Florida.",1. Introduction,[0],[0]
"Correspondence to: Alan Kuhnle <kuhnle@ufl.edu>, My T. Thai <mythai@ufl.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Kempe, 2011; Horel & Singer, 2016).",1. Introduction,[0],[0]
"In this work, we consider maximization subject to a cardinality constraint (MCC), where the function f to be maximized may be nonsubmodular.",1. Introduction,[0],[0]
Let k ∈ N,1. Introduction,[0],[0]
"(the budget), b ∈ (N∪{∞})S (the box), and let",1. Introduction,[0],[0]
f : {x ∈ NS : x ≤ b} → R+ (the objective) be a non-negative and monotonic1 function with f(0) = 0.,1. Introduction,[0],[0]
"Then determine
max ‖w‖1≤k f(w), (MCC)
where w = (ws)s∈S ∈ NS , ‖w‖1 = ∑ s∈S |ws|.
",1. Introduction,[0],[0]
"Since the integer lattice may be represented as a multiset of size k|S|, one may use results for Problem MCC with non-submodular set functions.",1. Introduction,[0],[0]
"In particular, the tight ratio 1α (1− e−αγs) of the standard greedy algorithm by Bian et al. (2017b), where α, γs are discussed below, applies with the lattice adaptation of the standard greedy algorithm (StandardGreedy) given in Alg.",1. Introduction,[0],[0]
1.,1. Introduction,[0],[0]
"However, this approach requires Ω(|S|k) queries of f , which is not polynomial in the input2 size O(|S| log k).",1. Introduction,[0],[0]
"Even for applications with set functions, Ω(|S|k) queries may be prohibitive, and researchers (Leskovec et al., 2007; Mirzasoleiman et al., 2015; Badanidiyuru & Vondrák, 2014) have sought ways to speed up the StandardGreedy algorithm.",1. Introduction,[0],[0]
"Unfortunately, these approaches rely upon the submodularity of f , and there has been no analogous effort for non-submodular functions.
",1. Introduction,[0],[0]
"To quantify the non-submodularity of a lattice function f , we generalize the following quantities defined for set functions to the lattice: (1) the diminishing-return (DR) ratio γd of f (Lehmann et al., 2006), (2) the weak DR ratio γs of f (Das & Kempe, 2011), and (3) the generalized curvature α of f (Bian et al., 2017b).",1. Introduction,[0],[0]
"Our main contributions are:
• To speed up StandardGreedy (Alg. 1), we adapt the threshold greedy framework of Badanidiyuru & Vondrák (2014) to non-submodular functions; this yields an algorithm (ThresholdGreedy, Alg. 2) with approximation ratio (1 − e−γdγs − η), for
1for all v ≤ w (coordinate-wise),",1. Introduction,[0],[0]
f(v) ≤ f(w) 2The input is considered to be the vector b of length n = |S| and the number k represented in log k bits (w.l.o.g.,1. Introduction,[0],[0]
"each component of b is at most k); the function is regarded as an oracle and hence does not contribute to input size.
",1. Introduction,[0],[0]
Algorithm 1 StandardGreedy 1: Input: f ∈,1. Introduction,[0],[0]
"Fb, k ∈ N, b ∈ NS 2:",1. Introduction,[0],[0]
Output: g ∈ NS 3: g← 0 4: for i = 1 to k do 5: g←,1. Introduction,[0],[0]
"g + arg maxs∈S:g+s≤b δs(g) 6: return g
any η",1. Introduction,[0],[0]
"> 0, the first approximation algorithm with polynomial query complexity for Problem MCC on the lattice.",1. Introduction,[0],[0]
"The query complexity of the StandardGreedy algorithm is improved from Ω(nk) to O ( n log k logκ ( ε2/k )) , where κ, ε ∈ (0, 1) are parameters of ThresholdGreedy.
",1. Introduction,[0],[0]
"• We introduce the novel approximation algorithm FastGreedy, which combines elements of StandardGreedy and ThresholdGreedy to improve the performance ratio to (1− e−β∗γs − η), where β∗ is at least γd and in many cases3 is determined by the algorithm.",1. Introduction,[0],[0]
"Furthermore, FastGreedy exploits the non-submodularity of the function to decrease its runtime in practice without sacrificing its performance guarantee, while maintaining the same worst-case query complexity as ThresholdGreedy up to a constant factor.
",1. Introduction,[0],[0]
"• To demonstrate our algorithms, we introduce a general budget allocation problem for viral marketing, which unifies submodular influence maximization (IM) under the independent cascade model (Kempe et al., 2003) with the non-submodular boosting problem (Lin et al., 2017) and in addition allows partial incentives.",1. Introduction,[0],[0]
"We prove a lower bound on the DR and weak DR ratios for this unified framework, and we experimentally validate our proposed algorithms in this setting.",1. Introduction,[0],[0]
The study of optimization of submodular set functions is too extensive to give a comprehensive overview.,2. Related Work,[0],[0]
"On the integer lattice, there have been many efforts to maximize submodular functions, e.g Soma & Yoshida (2017); Bian et al. (2017a); Gottschalk & Peis (2016).",2. Related Work,[0],[0]
"To the best of our knowledge, we are the first to study the optimization of non-submodular functions on the integer lattice.",2. Related Work,[0],[0]
"In the following discussion, we primarily restrict our attention to the maximization of monotonic, submodular lattice functions subject to a cardinality constraint and the maximization of non-submodular set functions.
",2. Related Work,[0],[0]
3When the solution g returned by FastGreedy satisfies ‖g‖1 =,2. Related Work,[0],[0]
k.,2. Related Work,[0],[0]
"Otherwise, an upper bound on β∗ is returned.
",2. Related Work,[0],[0]
Reduction of Ene & Nguyen (2016).,2. Related Work,[0],[0]
"Ene & Nguyen (2016) have given a polynomial-time reduction from the lattice to a set that enables unified translation of submodular optimization strategies to DR-submodular (i.e. DR ratio γd = 1, see Section 3) functions on the integer lattice.",2. Related Work,[0],[0]
"Since this translation is designed for DR-submodular functions, it does not give a polynomial-time algorithm for Problem MCC when f is non-submodular.",2. Related Work,[0],[0]
"Specifically, for the case of maximization subject to a cardinality constraint, Ene & Nguyen (2016) rely upon the threshold greedy algorithm for submodular set functions (Badanidiyuru & Vondrák, 2014), which does not work for nonsubmodular functions without modifications such as the ones in our paper.
",2. Related Work,[0],[0]
Threshold Greedy and Lattice Optimization.,2. Related Work,[0],[0]
"To speed up the StandardGreedy for submodular set functions, Badanidiyuru & Vondrák (2014) introduced the threshold greedy framework, which speeds up the StandardGreedy algorithm for maximizing submodular set functions under cardinality constraint from O(nk) function evaluations to O ( n ε log n ε ) , and it maintains the approximation ratio (1− 1/e− ε), for ε > 0.",2. Related Work,[0],[0]
Soma & Yoshida (2016) adapted the threshold approach for efficiently maximizing DRsubmodular functions on the integer lattice and provided (1 − 1/e − ε)-approximation algorithms.,2. Related Work,[0],[0]
"Other adaptations of the threshold approach of Badanidiyuru & Vondrák (2014) to the integer lattice include (Ene & Nguyen, 2016; Soma & Yoshida, 2015).",2. Related Work,[0],[0]
Elenberg et al. (2017) recently extended a similar algorithm (Sieve Streaming of Badanidiyuru et al. (2014)) to non-submodular functions in a somewhat similar fashion.,2. Related Work,[0],[0]
"The key difference is Sieve Streaming makes one pass over the data using many geometrically spaced thresolds in parallel, whereas the approach of Badanidiyuru & Vondrák (2014) makes many sequential passes at geometrically spaced thresholds.
",2. Related Work,[0],[0]
Our ThresholdGreedy algorithm is an adaptation of the algorithm of Soma & Yoshida (2016) for DR-submodular maximization to non-submodular functions.,2. Related Work,[0],[0]
"The nonsubmodularity requires new analysis, in the following specific ways: (1) during the binary search phase, we cannot guarantee that we find the maximum number of copies whose average gain exceeds the threshold τ ; hence, we must settle for any number of copies whose average gain exceeds τ , while ensuring that the gain of adding one additional copy falls belows τ .",2. Related Work,[0],[0]
"(2) To prove the performance ratio, we require a combination of the DR ratio γd and the weak DR ratio γs.
",2. Related Work,[0],[0]
"The very recent work of Qian et al. (2018) considers the same problem as in our paper, and they define the same DR ratio as ours; their lattice submodularity ratio is different from the weak DR ratio, however.",2. Related Work,[0],[0]
"The algorithm (POMS) in Qian et al. (2018) is based upon a Pareto optimization
technique, which is substantially different from our approach; our emphasis is on developing an approximation algorithm with runtime logarithmic in k, while POMS has running time Ω(k2n).
",2. Related Work,[0],[0]
Optimization of Non-Submodular Set Functions.,2. Related Work,[0],[0]
"For non-submodular set functions, the weak DR ratio γs was introduced by Das & Kempe (2011) under the name submodularity ratio; we generalize γs to lattice functions in Section 3, and we show the DR ratio γd ≤ γs.",2. Related Work,[0],[0]
"Bian et al. (2017b) introduced generalized curvature α of a set function, an analogous concept to the DR ratio as we discuss in Section 3.",2. Related Work,[0],[0]
"Bian et al. (2017b) extended the analysis of Conforti & Cornuéjols (1984) to non-submodular set functions; together with the weak DR ratio γs, they proved StandardGreedy has tight approximation ratio 1α (1− e−γsα) under cardinality constraint.",2. Related Work,[0],[0]
"The DR ratio γd is introduced as inverse curvature in Bogunovic et al. (2018), wherein robust maximization of set functions is considered; the DR ratio has also been introduced by Lehmann et al. (2006); Qian et al. (2018).
",2. Related Work,[0],[0]
"Many other notions of non-submodular set functions have been introduced (Krause et al., 2008b; Horel & Singer, 2016; Borodin et al., 2014; Feige & Izsak, 2013).",2. Related Work,[0],[0]
"For a comprehensive discussion of the relation of these and additional notions to the weak DR ratio γs, we refer the reader to Bian et al. (2017b).",2. Related Work,[0],[0]
"In this section, we define the lattice versions of DR ratio γd, weak DR ratio γs, and generalized curvature α, which are used in the approximation ratios proved in Section 4.
Notations.",3. Non-Submodularity on the Lattice,[0],[0]
"For each s ∈ S, let s be the unit vector with 1 in the coordinate corresponding to s, and 0 elsewhere.",3. Non-Submodularity on the Lattice,[0],[0]
We write δw(v) = f(v + w),3. Non-Submodularity on the Lattice,[0],[0]
"− f(v) for v,w ∈ NS .",3. Non-Submodularity on the Lattice,[0],[0]
"Given a box in the integer lattice b ∈ NS , let the set of all nonnegative, monotonic lattice functions with f(0) = 0, and domain {x ∈ NS : x ≤ b} be denoted Fb.",3. Non-Submodularity on the Lattice,[0],[0]
"It is often useful to think of a vector v ∈ NS as a multi-set containing vs copies of s ∈ S, where vs is the value of v’s coordinate corresponding to s. We use the notation {v} to represent the multiset corresponding to the vector v. Finally, we define v ∨w and v ∧w for v,w ∈ NS to be the vector with the coordinate-wise maximum and minimum respectively.",3. Non-Submodularity on the Lattice,[0],[0]
"Rather than an algorithm taking an explicit description of the function f as input, we consider the function f as an oracle and measure the complexity of an algorithm in terms of the number of oracle calls or queries.
",3. Non-Submodularity on the Lattice,[0],[0]
"We begin with the related concepts of DR ratio and generalized curvature.
",3. Non-Submodularity on the Lattice,[0],[0]
Definition 1.,3. Non-Submodularity on the Lattice,[0],[0]
Let,3. Non-Submodularity on the Lattice,[0],[0]
f ∈ Fb.,3. Non-Submodularity on the Lattice,[0],[0]
"The diminishing-return (DR)
ratio of f , γd(f), is the maximum value in [0, 1] such that for any s ∈ S, and for all v ≤ w such that w + s ≤ b, γd(f)δs(w) ≤ δs(v).",3. Non-Submodularity on the Lattice,[0],[0]
Definition 2.,3. Non-Submodularity on the Lattice,[0],[0]
Let,3. Non-Submodularity on the Lattice,[0],[0]
f ∈ Fb.,3. Non-Submodularity on the Lattice,[0],[0]
"The generalized curvature of f , α(f), is the minimum value in [0, 1] such that for any s ∈ S, and for all v ≤ w such that w + s ≤ b, δs(w) ≥ (1− α(f))δs(v).
",3. Non-Submodularity on the Lattice,[0],[0]
"The DR ratio extends the notion of DR-submodularity of Soma & Yoshida (2015), which is obtained as the special case γd = 1.",3. Non-Submodularity on the Lattice,[0],[0]
Generalized curvature for set functions was introduced in Bian et al. (2017b).,3. Non-Submodularity on the Lattice,[0],[0]
"Notice that α results in lower bounds on the marginal gain of s to a vector w, while γd results in upper bounds on the same quantity: (1 − α)δs(v) ≤ δs(w) ≤ 1γd δs(v), whenever v ≤ w",3. Non-Submodularity on the Lattice,[0],[0]
and the above expressions are defined.,3. Non-Submodularity on the Lattice,[0],[0]
"Next, we generalize the weak DR ratio of Das & Kempe (2011) to the integer lattice.
",3. Non-Submodularity on the Lattice,[0],[0]
Definition 3.,3. Non-Submodularity on the Lattice,[0],[0]
Let,3. Non-Submodularity on the Lattice,[0],[0]
f ∈ Fb.,3. Non-Submodularity on the Lattice,[0],[0]
"The weak DR ratio of f , γs(f), is the maximum value in [0, 1] such that for all v,w, such that v ≤ w, γs(f)(f(w)− f(v))",3. Non-Submodularity on the Lattice,[0],[0]
"≤ ∑ s∈{w−v} δs(v).
",3. Non-Submodularity on the Lattice,[0],[0]
"The next proposition, proved in Appendix C, shows the relationship between DR ratio and weak DR ratio.
",3. Non-Submodularity on the Lattice,[0],[0]
Proposition 1.,3. Non-Submodularity on the Lattice,[0],[0]
"For all f ∈ Fb, γd(f) ≤ γs(f).",3. Non-Submodularity on the Lattice,[0],[0]
"The function f is DR submodular iff γd(f) = γs(f) = 1.
",3. Non-Submodularity on the Lattice,[0],[0]
"In the rest of this work, we will parameterize functions by the non-submodularity ratios defined above and partition functions into the sets Fγd,γs,αb = {f ∈",3. Non-Submodularity on the Lattice,[0],[0]
"Fb : γd(f) = γd, γs(f) = γs, α(f) = α}.
",3. Non-Submodularity on the Lattice,[0],[0]
Greedy versions.,3. Non-Submodularity on the Lattice,[0],[0]
"In the proofs of this paper, the full power of the parameters defined above is not required.",3. Non-Submodularity on the Lattice,[0],[0]
"It suffices to consider restricted versions, where the maximization is taken over only those vectors which appear in the ratio proofs.",3. Non-Submodularity on the Lattice,[0],[0]
We define these greedy versions in Appendix B and include more discussion in Remark 1 of Section 4.1.,3. Non-Submodularity on the Lattice,[0],[0]
"In this section, we present the algorithm ThresholdGreedy (Alg. 2) to approximate Problem MCC with ratio 1 − e−γgγs",4.1. The ThresholdGreedy Algorithm,[0],[0]
− η with polynomial query complexity.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"Appendix D contains the proofs of all lemmas, claims, and omitted details from this section.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Description.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"ThresholdGreedy operates by considering decreasing thresholds for the marginal gain in its outer for loop; for each threshold τ , the algorithm adds on line 2 elements whose marginal gain exceeds τ as described be-
low.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"The parameter κ ∈ (0, 1) determines the stepsize between successive thresholds; the algorithm continues until the budget k is met (line 2) or the threshold is below a minimum value dependent on the parameter ε ∈",4.1. The ThresholdGreedy Algorithm,[0],[0]
"(0, 1).",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Intuitively, the goal of the threshold approach (Badanidiyuru & Vondrák, 2014) for submodular set functions is as follows.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"At each threshold (i.e., iteration of the outer for loop), add all elements whose marginal gain exceeds τ to the solution g. On the lattice, adding all copies of s ∈ S whose average gain exceeds τ on line 2 would require the addition of the maximum multiple ls such that the average marginal gain exceeds τ :
δls(g) ≥ lτ, (P1)
as in the threshold algorithm of Soma & Yoshida (2016) for DR-submodular functions, in which the maximum l is identified by binary search.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"However, since f is not DRsubmodular, it is not always the case that δs(g",4.1. The ThresholdGreedy Algorithm,[0],[0]
+ ls) ≥ δs(g,4.1. The ThresholdGreedy Algorithm,[0],[0]
+,4.1. The ThresholdGreedy Algorithm,[0],[0]
"(l + 1)s), for each l.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"For this reason, we cannot find the maximum such l by binary search.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Furthermore, even if we found the maximum l for each s ∈ S, we could not guarantee that all elements of marginal gain at least τ were added due to the non-submodularity of f : an element whose gain is less than τ when considered in the inner for loop might have gain greater than τ after additional elements are added to the solution.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"ThresholdGreedy more conservatively ensures that the number l chosen for each s ∈ S satisfies both (P1) and
δs(g + ls) <",4.1. The ThresholdGreedy Algorithm,[0],[0]
"τ, (P2)
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"but it is not necessarily the maximum such l.
Pivot.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Any l satisfying both (P1) and (P2) is termed a pivot4 with respect to g, s, τ .",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Perhaps surprisingly, a valid pivot can be found with binary search in O(log bmax) = O(log k) function queries, where bmax = maxs∈S bs; discussion of BinarySearchPivot and proof of this results is provided in Appendix D, Lemma 2.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"By finding a pivot for each s ∈ S, ThresholdGreedy does not attempt to add all elements exceeding the marginal gain of threshold τ ; instead, ThresholdGreedy maintains the following property at each threshold.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Property 1.,4.1. The ThresholdGreedy Algorithm,[0],[0]
Let gτ be the solution of ThresholdGreedy immediately after the iteration of the outer for loop corresponding to threshold τ .,4.1. The ThresholdGreedy Algorithm,[0],[0]
"Then for each s ∈ S, there exists h ≤",4.1. The ThresholdGreedy Algorithm,[0],[0]
"gτ such that δs(h) < τ .
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"4For convenience, we also define the maximum value of l, lmax = min{bs − gs, k − ‖g‖1} to be a pivot if lmax satisfies (P1) only, and set δs(g + lmaxs) = 0, so that all pivots satisfy both properties.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Algorithm 2 ThresholdGreedy 1: Input: f ∈,4.1. The ThresholdGreedy Algorithm,[0],[0]
"Fb, k ∈ N, κ, ε ∈ (0, 1).",4.1. The ThresholdGreedy Algorithm,[0],[0]
"2: Output: g ∈ NS 3: g← 0, M ← maxs∈S f(s).",4.1. The ThresholdGreedy Algorithm,[0],[0]
"4: for ( τ = M ; τ ≥ κε2Mk ; τ ← κτ ) do
5: for s ∈ S do 6: l←BinarySearchPivot(f,g,b, s, k, τ) 7:",4.1. The ThresholdGreedy Algorithm,[0],[0]
"g← g + ls 8: if ‖g‖1 = k then 9: return g
10: return g
Performance ratios.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Next, we present the main result of this section, the performance guarantee involving the DR and weak DR ratios.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Observe that the query complexity of ThresholdGreedy is polynomial in the input size O(n log k).
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Theorem 1.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"Let an instance of Problem MCC be given, with f ∈ Fγd,γs,αb .",4.1. The ThresholdGreedy Algorithm,[0],[0]
"If g is the solution returned by ThresholdGreedy and Ω is an optimal solution to this instance, then
f(g) ≥ ( 1− e−κγdγs − ε ) f(Ω).
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"The query complexity of ThresholdGreedy is O ( n log k logκ ( ε2/k )) .
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"If η > 0 is given, the assignment κ = (1 − η/2), ε = η/2 yields performance ratio at least 1− e−γdγs − η.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Proof.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"If γd < ε, the ratio holds trivially; so assume γd ≥ ε.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"The proof of the following claim requires an application of the DR ratio.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Claim 1.,4.1. The ThresholdGreedy Algorithm,[0],[0]
Let g be produced by a modified version of ThresholdGreedy that continues until ‖g‖1 = k.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"If we show f(g) ≥ (1− e−κγdγs)f(Ω), the results follows.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Thus, for the rest of the proof let g be as described in Claim 1.",4.1. The ThresholdGreedy Algorithm,[0],[0]
Let gt be the value of g after the tth execution of line 2 of ThresholdGreedy.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"Let lt be the tth pivot, such that gt = gt−1+ltst.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"The next claim lower bounds the marginal gain in terms of the DR ratio and the previous threshold.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Claim 2.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"For each s ∈ {Ω− gt−1 ∧Ω},
ltγdκδs(g t−1) ≤ f(gt)− f(gt−1).
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Proof.,4.1. The ThresholdGreedy Algorithm,[0],[0]
Let τ be the threshold at which ltst is added to gt−1; let s ∈ {Ω − gt−1 ∧ Ω}.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"If τ is the first threshold, γdδs(g
t−1) ≤ δs(0) ≤",4.1. The ThresholdGreedy Algorithm,[0],[0]
τ < τκ .,4.1. The ThresholdGreedy Algorithm,[0],[0]
"If τ is not the first threshold, τ ′",4.1. The ThresholdGreedy Algorithm,[0],[0]
= τ/κ is the previous threshold value of the previous iteration of the outer for loop.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"By Property 1, there
exists h ≤",4.1. The ThresholdGreedy Algorithm,[0],[0]
gτ ′,4.1. The ThresholdGreedy Algorithm,[0],[0]
"≤ gt−1, such that δs(h) < τ",4.1. The ThresholdGreedy Algorithm,[0],[0]
"′. By the definition of DR ratio, γdδs(gt−1) ≤",4.1. The ThresholdGreedy Algorithm,[0],[0]
δs(h) < τ ′,4.1. The ThresholdGreedy Algorithm,[0],[0]
= τ/κ.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"In either case, by the fact that property (P1) of a pivot holds for lt, we have
f(gt)− f(gt−1)",4.1. The ThresholdGreedy Algorithm,[0],[0]
"≥ ltτ ≥ ltγdκδs(gt−1).
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Since |Ω| ≤ k, we have by Claim 2
f(gt)− f(gt−1)",4.1. The ThresholdGreedy Algorithm,[0],[0]
"≥ l tγdκ
k ∑ s∈{Ω−(gt−1∧Ω)} δs(g t−1)
= ltγdκ
k ∑ s∈{Ω∨gt−1−gt−1} δs(g t−1)
",4.1. The ThresholdGreedy Algorithm,[0],[0]
≥,4.1. The ThresholdGreedy Algorithm,[0],[0]
"l tγdγsκ
k
( f(Ω ∨ gt−1)− f(gt−1) )",4.1. The ThresholdGreedy Algorithm,[0],[0]
"≥ l tγdγsκ
k
( f(Ω)− f(gt−1) ) ,
where the equality follows from the lattice identity v ∨ w",4.1. The ThresholdGreedy Algorithm,[0],[0]
"− v = w − v ∧ w for all v,w ∈ NS , the second inequality is by definition of the weak DR ratio, and the third inequality is from monotonicity.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"From here, we obtain f(g) ≥ ( 1−∏Tt=1 (1− ltγdγsκk ))",4.1. The ThresholdGreedy Algorithm,[0],[0]
"f(Ω), from which the hypothesis of Claim 1 follows.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Query complexity.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"The for loop on line 2 (Alg. 2) iterates at most logκ ε
2/k times; each iteration requires O(n log k) queries, by Lemma 2.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
"For additional speedup, the inner for loop of FastGreedy may be parallelized, which divides the factor of n in the query complexity by the number of threads but worsens the performance ratio; in addition to γd, γs, the generalized curvature α is required in the proof.
",4.1. The ThresholdGreedy Algorithm,[0],[0]
Corollary 1.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"If the inner for loop of ThresholdGreedy is parallelized, the performance ratio becomes 1 − e−(1−α)γdγs − η, for η > 0.",4.1. The ThresholdGreedy Algorithm,[0],[0]
Remark 1.,4.1. The ThresholdGreedy Algorithm,[0],[0]
"A careful analysis of the usage of γd,γs in the proof of Theorem 1 shows that the full power of the definitions of these quantities is not required.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"Rather, it is sufficient to consider ThresholdGreedy versions of these parameters, as defined in Appendix B. In the same way, we also have FastGreedy version of γs based upon the proof of Theorem 2.",4.1. The ThresholdGreedy Algorithm,[0],[0]
"The FastGreedy version of the DR ratio is an integral part of how the algorithm works and is calculated directly by the algorithm, as we discuss in the next section.",4.1. The ThresholdGreedy Algorithm,[0],[0]
The proof of the performance ratio of ThresholdGreedy requires both the submodularty ratio γs and the DR ratio γd.,4.2. The FastGreedy Algorithm,[0],[0]
"In this section, we provide an algorithm (FastGreedy, Alg.
3) that achieves ratio 1− e−β∗γs",4.2. The FastGreedy Algorithm,[0],[0]
"− η, with factor β∗ ≥ γd that it can determine during its execution.",4.2. The FastGreedy Algorithm,[0],[0]
"Appendix E provides proofs for all lemmas, claims, and omitted details.
",4.2. The FastGreedy Algorithm,[0],[0]
Description.,4.2. The FastGreedy Algorithm,[0],[0]
FastGreedy employs a threshold framework analogous to ThresholdGreedy.,4.2. The FastGreedy Algorithm,[0],[0]
"Each iteration of the outer while loop of FastGreedy is analogous to an iteration of the outer for loop in ThresholdGreedy, in which elements are added whose marginal gain exceeds a threshold.",4.2. The FastGreedy Algorithm,[0],[0]
FastGreedy employs BinarySearchPivot to find pivots for each s ∈ S for each threshold value τ .,4.2. The FastGreedy Algorithm,[0],[0]
"Finally, the parameter ε determines a minimum threshold value.
",4.2. The FastGreedy Algorithm,[0],[0]
"As its threshold, FastGreedy uses τ = βκm, where m is the maximum marginal gain found on line 3, parameter κ is the intended stepsize between thresholds as in ThresholdGreedy, and β is an upper bound on the DR ratio γd, as described below.",4.2. The FastGreedy Algorithm,[0],[0]
"This choice of τ has the following advantages over the approach of ThresholdGreedy: (1) since the threshold is related to the maximum marginal gain m, the theoretical performance ratio is improved; (2) the use of β to lower the threshold ensures the same5 worst-case query complexity as ThresholdGreedy and leads to substantial reduction of the number of queries in practice, as we demonstrate in Section 6.
",4.2. The FastGreedy Algorithm,[0],[0]
FastGreedy DR ratio β∗.,4.2. The FastGreedy Algorithm,[0],[0]
If FastGreedy is modified6 to continue until ‖g‖1,4.2. The FastGreedy Algorithm,[0],[0]
"= k, let the final, smallest value β∗ of β be termed the FastGreedy DR ratio on the instance.",4.2. The FastGreedy Algorithm,[0],[0]
"The FastGreedy DR ratio β∗ is at least the DR ratio γd of the function, up to the parameter δ:
Lemma 1.",4.2. The FastGreedy Algorithm,[0],[0]
"Let parameters κ, δ, ε ∈",4.2. The FastGreedy Algorithm,[0],[0]
"(0, 1) be given.",4.2. The FastGreedy Algorithm,[0],[0]
"Throughout the execution of FastGreedy on an instance of Problem MCC with f ∈ Fγd,γsb , β ≥ γdδ.",4.2. The FastGreedy Algorithm,[0],[0]
"Since ε can be arbitrarily small, β∗ ≥ γdδ.
",4.2. The FastGreedy Algorithm,[0],[0]
Proof.,4.2. The FastGreedy Algorithm,[0],[0]
"Initally, β = 1; it decreases by a factor of δ ∈ (0, 1) at most once per iteration of the while loop.",4.2. The FastGreedy Algorithm,[0],[0]
"Suppose β ≤ γd for some iteration i of the while loop, and let g have the value assigned immediately after iteration i, m have the value assigned after line 3 of iteration i. Since a valid pivot was found for each s ∈ S during iteration i, by property (P2) there exists gs ≤ g, δs(gs) < βκm ≤ γdκm.",4.2. The FastGreedy Algorithm,[0],[0]
"Hence δs(g) ≤ κm, by the definition of DR ratio.",4.2. The FastGreedy Algorithm,[0],[0]
"In iteration i + 1, m′ has the value of m from iteration i, so the value of m computed during iteration i+ 1 is at most κm′, and β does not decrease during iteration i+ 1.
",4.2. The FastGreedy Algorithm,[0],[0]
Performance ratio.,4.2. The FastGreedy Algorithm,[0],[0]
"Next, we present the main result of this section.",4.2. The FastGreedy Algorithm,[0],[0]
"In contrast to ThresholdGreedy, the factor of
5Up to a constant factor, which depends on γd.",4.2. The FastGreedy Algorithm,[0],[0]
"6This modification can be accomplished by setting ε to ensure
the condition on line 3 is always true on this instance.
",4.2. The FastGreedy Algorithm,[0],[0]
Algorithm 3 FastGreedy 1: Input: f ∈,4.2. The FastGreedy Algorithm,[0],[0]
"Fb, k ∈ N, κ, δ, ε ∈ (0, 1).",4.2. The FastGreedy Algorithm,[0],[0]
"2: Output: g ∈ NS 3: g ← 0, M ← maxs∈S f(s), m ← M,m′",4.2. The FastGreedy Algorithm,[0],[0]
"← M/κ, β ← 1
4: while m ≥Mε2/k",4.2. The FastGreedy Algorithm,[0],[0]
do 5: m← maxs∈S δs(g) 6: if m > κm′ then 7: β ← βδ 8: m′,4.2. The FastGreedy Algorithm,[0],[0]
"← m 9: τ ← βκm
10: for s ∈ S do 11: l←BinarySearchPivot(f,g,b, s, k, τ) 12:",4.2. The FastGreedy Algorithm,[0],[0]
"g← g + ls 13: if ‖g‖1 = k then 14: return g 15: return g
γd in the performance ratio has been replaced with β∗; at the termination of the algorithm, the value of β∗ is an output of FastGreedy if the solution g satisfies ‖g‖1",4.2. The FastGreedy Algorithm,[0],[0]
= k.,4.2. The FastGreedy Algorithm,[0],[0]
"In any case, by Lemma 1, the performance ratio is at worst the same as that of ThresholdGreedy.
",4.2. The FastGreedy Algorithm,[0],[0]
Theorem 2.,4.2. The FastGreedy Algorithm,[0],[0]
"Let an instance of Problem MCC be given, with f ∈ Fγd,γsb .",4.2. The FastGreedy Algorithm,[0],[0]
"Let g be the solution returned by FastGreedy with parameters κ, δ, ε ∈",4.2. The FastGreedy Algorithm,[0],[0]
"(0, 1), and let Ω be an optimal solution to this instance; also, suppose γd ≥ ε.",4.2. The FastGreedy Algorithm,[0],[0]
Let β∗ be the FastGreedy DR ratio on this instance.,4.2. The FastGreedy Algorithm,[0],[0]
"Then,
f(g) ≥ ( 1− e−κβ∗γs − ε ) f(Ω)
The worst-case query complexity of FastGreedy is O (( logδ(γd) logκ(γd) + logκ ε 2/k ) n",4.2. The FastGreedy Algorithm,[0],[0]
"log k ) .
",4.2. The FastGreedy Algorithm,[0],[0]
"If η > 0 is given, the assignment κ = (1 − η/2), ε = η/2 yields performance ratio at least 1− e−β∗γs − η.
",4.2. The FastGreedy Algorithm,[0],[0]
Proof of query complexity.,4.2. The FastGreedy Algorithm,[0],[0]
"The performance ratio is proved in Appendix E. Let m′1, . . .",4.2. The FastGreedy Algorithm,[0],[0]
",m ′",4.2. The FastGreedy Algorithm,[0],[0]
K be the sequence of m′ values in the order considered by the algorithm.,4.2. The FastGreedy Algorithm,[0],[0]
"By Lemma 1, m′j > κm ′",4.2. The FastGreedy Algorithm,[0],[0]
"j−1 at most Γ = logδ γd times; label each such index j an uptick, and let j1, . .",4.2. The FastGreedy Algorithm,[0],[0]
.,4.2. The FastGreedy Algorithm,[0],[0]
", jl be the indices of each uptick in order of their appearance.",4.2. The FastGreedy Algorithm,[0],[0]
"Also, let ki be the first index after ji such that m′ki ≤ κm′ji−1, for each i ∈ {1, . . .",4.2. The FastGreedy Algorithm,[0],[0]
", l}.",4.2. The FastGreedy Algorithm,[0],[0]
"Next, we will iteratively delete from the sequence of m′ values.",4.2. The FastGreedy Algorithm,[0],[0]
"Initially, let ` = l be the last uptick in the sequence; delete all termsm′j` , . . .",4.2. The FastGreedy Algorithm,[0],[0]
",m ′ k`−1 from them
′ sequence.",4.2. The FastGreedy Algorithm,[0],[0]
"Set ` = `− 1 and repeat this process until ` = 0.
Claim 3.",4.2. The FastGreedy Algorithm,[0],[0]
"For each ` selected in the iterative deletion above, there are at most logκ γd values deleted from the sequence.
",4.2. The FastGreedy Algorithm,[0],[0]
"By Claim 3 and the bound on the number of upticks, we have deleted at most logκ γd logδ γd thresholds m
′ from the sequence; every term in the remaining sequence satisfies m′j ≤ κm′j−1; hence, the remaining sequence contains at most logκ ε
2/k terms, by its initial and terminal values.",4.2. The FastGreedy Algorithm,[0],[0]
"The query complexity follows from the number of queries per value of m′, which is O(n log k) by Lemma 2.",4.2. The FastGreedy Algorithm,[0],[0]
"In this section, we provide a non-submodular framework for viral marketing on a social network that unifies the classical influence maximization (Kempe et al., 2003) with the boosting problem (Lin et al., 2017).
Overview.",5. Influence Maximization: A General Framework,[0],[0]
"The goal of influence maximization is to select seed users (i.e. initially activated users) to maximize the expected adoption in the social network, where the total number of seeds is restricted by a budget, such that the expected adoption in the social network is maximized.",5. Influence Maximization: A General Framework,[0],[0]
"The boosting problem is, given a fixed seed set S, to incentivize (i.e. increase the susceptibility of a user to the influence of his friends) users within a budget such that the expected adoption with seed set S increases the most.
",5. Influence Maximization: A General Framework,[0],[0]
"Our framework combines the above two scenarios with a partial incentive: an incentive (say, x% off the purchase price) increases the probability a user will purchase the product independently and increases the susceptibility of the user to the influence of his friends.",5. Influence Maximization: A General Framework,[0],[0]
"Hence, our problem asks how to best allocate the budget between (partially) seeding users and boosting the influence of likely extant seeds.",5. Influence Maximization: A General Framework,[0],[0]
"Both the classical influence maximization and the non-submodular boosting problem can be obtained as special cases, as shown in Appendix F.
Our model is related to the formulation of Demaine et al. (2014); however, they employ a submodular threshold-based model, while our model is inherently nonsubmodular due to the boosting mechanism (Lin et al., 2017).",5. Influence Maximization: A General Framework,[0],[0]
"Also, GIM is related to the submodular budgeted allocation problem of Alon et al. (2012), in which the influence of an advertiser increases with the amount of budget allocated; the main difference with GIM is that we modify incoming edge weights with incentives instead of outgoing, which creates the boosting mechanism responsible for the non-submodularity.
",5. Influence Maximization: A General Framework,[0],[0]
Model.,5. Influence Maximization: A General Framework,[0],[0]
"Given a social network G = (V,E), and a product p, we define the following model of adoption.",5. Influence Maximization: A General Framework,[0],[0]
The allocation of budget to u is thought of as a discount towards purchasing the product; this discount increases the probability that this user will adopt or purchase the product.,5. Influence Maximization: A General Framework,[0],[0]
"Furthermore, this discount increases the susceptibility of the
user to influence from its (incoming) social connections.
",5. Influence Maximization: A General Framework,[0],[0]
"Formally, an incentive level xu is chosen for each user u. With independent probability p(u,xu), user u initially activates or adopts the product; altogether, this creates a probabilistic initial set S of activated users.",5. Influence Maximization: A General Framework,[0],[0]
"Next, through the classical Independent Cascade (IC) model7 of adoption, users influence their neighbors in the social network; wherein the weight p(v, u,xu) for edge (v, u) is determined by the incentive level xu of user u as well as the strength of the social connection from v to u.
We write px(H,T ) to denote the probability of full graph realization H and seed set T when x gives the incentive levels for each user.",5. Influence Maximization: A General Framework,[0],[0]
"We write R(H,T ) to denote the size of the reachable set from T in realization H .",5. Influence Maximization: A General Framework,[0],[0]
"The expected activation in the network given a choice x of incentive levels is given by I(x) = ∑ T⊆V ∑ H⊆G p
x(H,",5. Influence Maximization: A General Framework,[0],[0]
"T )R(H,T ), where an explicit formula for px(H,T ) is given in Appendix F. Finally, let A(x) = I(x)− I(0).",5. Influence Maximization: A General Framework,[0],[0]
Definition 4 (Generalized Influence Maximization (GIM)).,5. Influence Maximization: A General Framework,[0],[0]
"Let social network G = (V,E) be given, together with the mappings",5. Influence Maximization: A General Framework,[0],[0]
i 7→,5. Influence Maximization: A General Framework,[0],[0]
"p(u, i), i 7→ p(u, v, i), for all u ∈ V, (u, v) ∈",5. Influence Maximization: A General Framework,[0],[0]
"E, for each i ∈ {0, . . .",5. Influence Maximization: A General Framework,[0],[0]
", L}, where L is the number of incentive levels.",5. Influence Maximization: A General Framework,[0],[0]
"Given budget k, determine incentive levels x, with ‖x‖1 ≤ k, such that A(x) is maximized.
",5. Influence Maximization: A General Framework,[0],[0]
Bound on non-submodularity.,5. Influence Maximization: A General Framework,[0],[0]
"Next, we provide a lower bound on the greedy DR ratios (see Appendix B).",5. Influence Maximization: A General Framework,[0],[0]
We emphasize that the assumption that the probability mappings as a function of incentive level be submodular does not imply the objective A(x) is DR-submodular.,5. Influence Maximization: A General Framework,[0],[0]
"Theorem 3 is proved in Appendix F.
Theorem 3.",5. Influence Maximization: A General Framework,[0],[0]
"Let I be an instance of GIM, with budget k.",5. Influence Maximization: A General Framework,[0],[0]
"Let ce = max(u,v)∈E,i∈L p(u,v,i+1) p(u,v,i) , cn = maxx∈V,i∈L",5. Influence Maximization: A General Framework,[0],[0]
"p(x,i+1) p(x,i) .",5. Influence Maximization: A General Framework,[0],[0]
"Suppose for all (u, v) ∈",5. Influence Maximization: A General Framework,[0],[0]
"E,w ∈ V , the mappings i 7→ p(u, v, i), i 7→",5. Influence Maximization: A General Framework,[0],[0]
"p(w, i) are submodular set functions.",5. Influence Maximization: A General Framework,[0],[0]
"Then, the greedy DR ratios defined in Appendix B and the FastGreedy DR ratio are lower bounded by c−k∆e c −k n , where ∆ is the maximum in-degree in G.",5. Influence Maximization: A General Framework,[0],[0]
"In this section, we evaluate our proposed algorithms for the GIM problem defined in Section 5.",6. Experimental Evaluation,[0],[0]
Source code for the implementation is available at https://gitlab.,6. Experimental Evaluation,[0],[0]
com/emallson/lace.,6. Experimental Evaluation,[0],[0]
"We evaluate our algorithms as compared with StandardGreedy; by the naive reduction of the lattice to sets in exponential time, this algorithm is equivalent to performing this reduction and running the standard greedy for sets, the performance of which for
7The IC model is defined in Appendix F.
non-submodular set functions was analyzed by Bian et al. (2017b).
",6. Experimental Evaluation,[0],[0]
"In Section 6.1, we describe our methodology; in Section 6.2, we compare the algorithms and non-submodularity parameters.",6. Experimental Evaluation,[0],[0]
"In Appendix G.1, we explore the behavior of FastGreedy as the parameters δ, κ, and ε are varied.",6. Experimental Evaluation,[0],[0]
"Our implementation uses Monte Carlo sampling to estimate the objective value A(x), with 10 000 samples used.",6.1. Methodology,[0],[0]
"As a result, each function query is relatively expensive.
",6.1. Methodology,[0],[0]
"We evaluate on two networks taken from the SNAP dataset (Leskovec & Krevl, 2014): ca-GrQc (“GrQc”; 15k nodes, 14.5K edges) and facebook (“Facebook”; 4k nodes, 176K edges).",6.1. Methodology,[0],[0]
"Unless otherwise specified, we use 10 repetitions per datapoint and display the mean.",6.1. Methodology,[0],[0]
The width of shaded intervals is one standard deviation.,6.1. Methodology,[0],[0]
Standard greedy is omitted from some figures where running time is prohibitive.,6.1. Methodology,[0],[0]
"Unless noted otherwise, we use default settings of ε = 0.05, δ = 0.9, κ = 0.95.",6.1. Methodology,[0],[0]
"We use a uniform box constraint and assign each user the same number of incentive levels; the maximum incentive level for a user corresponds to giving the product to the user for free and hence deterministically seeds the user; we adopt linear models for the mappings i 7→ p(u, i), i 7→ p(u, v, i).",6.1. Methodology,[0],[0]
"We often plot versus K, which is defined as the maximum number of deterministic seeds; for example, if k = 200 with 10 incentive levels, then K = 20.",6.1. Methodology,[0],[0]
"In this section, we demonstrate the following: (1) our algorithms exhibit virtually identical quality of solution with StandardGreedy, (2) our algorithms query the function much fewer times, which leads to dramatic runtime improvement over StandardGreedy, (3) FastGreedy further reduces the number of queries of ThresholdGreedy while sacrificing little in solution quality, and (4) the nonsubmodularity parameters on a small instance are com-
puted, which provides evidence that our theoretical performance ratios are useful.
",6.2. Results,[0],[0]
"Quality of Solution In Fig. 1(a), we plot A(g) for the solution returned by each algorithm on the GrQc network with 10 incentive levels; the difference in quality of solution returned by the three algorithms is negligible.",6.2. Results,[0],[0]
"In Fig. 1(b), we plot the same for the Facebook network with 100 incentive levels; on Facebook, we drop StandardGreedy due to its prohibitive runtime.",6.2. Results,[0],[0]
"FastGreedy is observed to lose a small (up to 3%) factor, which we consider acceptable in light of its large runtime improvement, which we discuss next.
",6.2. Results,[0],[0]
"Number of Queries Next, we present in Fig. 2 the number of function queries8 each algorithm requires on the GrQc and Facebook networks.",6.2. Results,[0],[0]
"StandardGreedy required up to 20M queries on Facebook, hence it is not shown in Fig. 2(b).",6.2. Results,[0],[0]
"Both of our algorithms provide a large improvement over StandardGreedy; in particular, notice that StandardGreedy increases linearly with k, while both of the others exhibit logarithmic increase in agreement with the theoretical query complexity of each.",6.2. Results,[0],[0]
"Furthermore, FastGreedy uses at least 14.5% fewer function queries than ThresholdGreedy and up to 43% fewer as k grows.
8Our implementation is in terms of the marginal gain.",6.2. Results,[0],[0]
"The number of function queries shown is the number of times the marginal gain function was called.
",6.2. Results,[0],[0]
"Non-Submodularity Parameters The value of the FastGreedy DR ratio β∗ on GrQc is shown in Fig. 4(a); notice that it is relatively stable as the budget increases from K = 20 to 100, although there is substantial drop from 10 incentive levels to 100; this may be explained as an increase in the non-submodularity resulting from inaccurate sampling of A, since it is more difficult to detect differences between the finer levels.",6.2. Results,[0],[0]
"Still, on all instances tested, β∗ > 0.6, which suggests the worst-case performance ratio of FastGreedy is not far from that of StandardGreedy.
",6.2. Results,[0],[0]
"Finally, we examine the various non-submodularity parameters on a very small instance which admits their computation: a random Barabasi-Albert network with 10 nodes and 10 incentive levels.",6.2. Results,[0],[0]
We compute the FastGreedy version of the submodularity ratio γs defined in Appendix B by direct enumeration and consider the FastGreedy DR ratio β∗.,6.2. Results,[0],[0]
Results are shown in Fig. 4(b).,6.2. Results,[0],[0]
"The value of β∗ is close to 1 and remains constant with increasing budget k, while the FastGreedy submodularity ratio decreases slowly with k. With β∗ and the FastGreedy γs, we can compute the worst-case performance ratio of FastGreedy across these instances: 0.449692.",6.2. Results,[0],[0]
"In this work, we provide two approximation algorithms for maximizing non-submodular functions with respect to a cardinality constraint on the integer lattice with polynomial query complexity.",7. Conclusions,[0],[0]
"Since set functions are a special case, our work provides faster algorithms for the same problem with set functions than the standard greedy algorithm, although the performance ratio degrades from at least 1−e−γs to 1−e−β∗γs , where β∗ is the FastGreedy DR Ratio.",7. Conclusions,[0],[0]
"We propose a natural application of non-submodular influence maximization, for which we lower bound the relevant non-submodularity parameters and validate our algorithms.",7. Conclusions,[0],[0]
"This work was supported in part by US NSF EFRI 1441231, CCF 1422116, and DTRA HDTRA1-14-1-0055.",Acknowledgement,[0],[0]
"The optimization of submodular functions on the integer lattice has received much attention recently, but the objective functions of many applications are non-submodular.",abstractText,[0],[0]
We provide two approximation algorithms for maximizing a nonsubmodular function on the integer lattice subject to a cardinality constraint; these are the first algorithms for this purpose that have polynomial query complexity.,abstractText,[0],[0]
"We propose a general framework for influence maximization on the integer lattice that generalizes prior works on this topic, and we demonstrate the efficiency of our algorithms in this context.",abstractText,[0],[0]
"Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice",title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 12–23 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding.,1 Introduction,[0],[0]
"But transition-based dependency parsers (TBDPs) have recently achieved state-of-the-art performance, despite the fact that for efficiency reasons, they are usually trained to make local, rather than global, decisions and the decoding process is done approximately, rather than exactly (Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016).",1 Introduction,[0],[0]
The key efficiency issue for decoding is as follows.,1 Introduction,[0],[0]
"In order to make accurate (local) attachment decisions, historically, TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration.",1 Introduction,[0],[0]
"But consulting many positions means that although polynomial-time exact-decoding algo-
rithms do exist, having been introduced by Huang and Sagae (2010) and Kuhlmann et al. (2011), unfortunately, they are prohibitively costly in practice, since the number of positions considered can factor into the exponent of the running time.",1 Introduction,[0],[0]
"For instance, Huang and Sagae employ a fairly reduced set of nine positions, but the worst-case running time for the exact-decoding version of their algorithm is Opn6q (originally reported as Opn7q) for a length-n sentence.",1 Introduction,[0],[0]
"As an extreme case, Dyer et al. (2015) use an LSTM to summarize arbitrary information on the stack, which completely rules out dynamic programming.
",1 Introduction,[0],[0]
"Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) applied bidirectional long short-term memory networks (Graves and Schmidhuber, 2005, bi-LSTMs) to derive feature representations for parsing, because these networks capture wide-window contextual information well.",1 Introduction,[0],[0]
"Collectively, these two sets of authors demonstrated that with bi-LSTMs, four positional features suffice for the arc-hybrid parsing system (K&G), and three suffice for arcstandard (C&H).1
Inspired by their work, we arrive at a minimal feature set for arc-hybrid and arc-eager: it contains only two positional bi-LSTM vectors, suffers almost no loss in performance in comparison to larger sets, and out-performs a single position.",1 Introduction,[0],[0]
"(Details regarding the situation with arc-standard can be found in §2.)
",1 Introduction,[0],[0]
"Our minimal feature set plugs into Huang and Sagae’s and Kuhlmann et al.’s dynamic program-
1We note that K&G were not focused on minimizing positions, although they explicitly noted the implications of doing so: “While not explored in this work, [fewer positions] results in very compact state signatures, [which is] very appealing for use in transition-based parsers that employ dynamicprogramming search” (pg. 319).",1 Introduction,[0],[0]
"C&H also noted in their follow-up (Cross and Huang, 2016b) the possibility of future work using dynamic programming thanks to simple features.
12
ming framework to produce the first implementation of Opn3q exact decoders for arc-hybrid and arc-eager parsers.",1 Introduction,[0],[0]
We also enable and implement Opn3q global training methods.,1 Introduction,[0],[0]
"Empirically, ensembles containing our minimal-feature, globallytrained and exactly-decoded models produce the best unlabeled attachment score (UAS) reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank.2
Additionally, we provide a slight update to the theoretical connections previously drawn by Gómez-Rodrı́guez, Carroll, and Weir (2008, 2011) between TBDPs and the graph-based dependency parsing algorithms of Eisner (1996) and Eisner and Satta (1999), including results regarding the arc-eager parsing system.",1 Introduction,[0],[0]
TBDPs incrementally process a sentence by making transitions through search states representing parser configurations.,2 A Minimal Feature Set,[0],[0]
"Three of the main transition systems in use today (formal introduction in §3.1) all maintain the following two data structures in their configurations: (1) a stack of partially parsed subtrees and (2) a buffer (mostly) of unprocessed sentence tokens.
",2 A Minimal Feature Set,[0],[0]
"To featurize configurations for use in a scoring function, it is common to have features that extract information about the first several elements on the stack and the buffer, such as their word forms and part-of-speech (POS) tags.",2 A Minimal Feature Set,[0],[0]
"We refer to these as positional features, as each feature relates to a particular position in the stack or buffer.",2 A Minimal Feature Set,[0],[0]
"Typically, millions of sparse indicator features (often developed via manual engineering) are used.
",2 A Minimal Feature Set,[0],[0]
"In contrast, Chen and Manning (2014) introduce a feature set consisting of dense word-, POS-, and dependency-label embeddings.",2 A Minimal Feature Set,[0],[0]
"While dense, these features are for the same 18 positions that have been typically used in prior work.",2 A Minimal Feature Set,[0],[0]
"Recently, Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) adopt bi-directional LSTMs, which have nice expressiveness and context-sensitivity properties, to reduce the number of positions considered down to four and three,
2Our ideas were subsequently adapted to the labeled setting by Shi, Wu, Chen, and Cheng (2017) in their submission to the CoNLL 2017 shared task on Universal Dependencies parsing.",2 A Minimal Feature Set,[0],[0]
"Their team achieved the second-highest labeled attachment score in general and had the top average performance on the surprise languages.
for different transition systems, respectively.",2 A Minimal Feature Set,[0],[0]
"This naturally begs the question, what is the lower limit on the number of positional features necessary for a parser to perform well?",2 A Minimal Feature Set,[0],[0]
"Kiperwasser and Goldberg (2016a) reason that for the arc-hybrid system, the first and second items on the stack and the first buffer item — denoted by s0, s1, and b0, respectively — are required; they additionally include the third stack item, s2, because it may not be adjacent to the others in the original sentence.",2 A Minimal Feature Set,[0],[0]
"For arc-standard, Cross and Huang (2016a) argue for the necessity of s0, s1, and b0.
",2 A Minimal Feature Set,[0],[0]
"We address the lower-limit question empirically, and find that, surprisingly, two positions suffice for the greedy arc-eager and arc-hybrid parsers.",2 A Minimal Feature Set,[0],[0]
We also provide empirical support for Cross and Huang’s argument for the necessity of three features for arc-standard.,2 A Minimal Feature Set,[0],[0]
"In the rest of this section, we explain our experiments, run only on an English development set, that support this conclusion; the results are depicted in Table 1.",2 A Minimal Feature Set,[0],[0]
"We later explore the implementation implications in §3-4 and then test-set parsing-accuracy in §6.
",2 A Minimal Feature Set,[0],[0]
We employ the same model architecture as Kiperwasser and Goldberg (2016a).,2 A Minimal Feature Set,[0],[0]
"Specifically, we first use a bi-LSTM to encode an n-token sentence, treated as a sequence of per-token concatenations of word- and POS-tag embeddings, into a sequence of vectors r ÑÐ w1, . . .",2 A Minimal Feature Set,[0],[0]
", ÑÐ wns, where each ÑÐ wi
is the output of the bi-LSTM at time step i. (The double-arrow notation for these vectors emphasizes the bi-directionality of their origin).",2 A Minimal Feature Set,[0],[0]
"Then, for a given parser configuration, stack positions are represented by ÑÐ s j , defined as ÑÐ wipsjq where ipsjq gives the position in the sentence of the token that is the head of the tree in sj .",2 A Minimal Feature Set,[0],[0]
"Similarly, buffer positions are represented by ÑÐ b j , defined as ÑÐ wipbjq for the token at buffer position j. Finally, as in Chen and Manning (2014), we use a multilayer perceptron to score possible transitions from the given configuration, where the input is the concatenation of some selection of the ÑÐ s js and ÑÐ b ks.",2 A Minimal Feature Set,[0],[0]
"We use greedy decoders, and train the models with dynamic oracles (Goldberg and Nivre, 2013).
",2 A Minimal Feature Set,[0],[0]
"Table 1 reports the parsing accuracy that results for feature sets of size four, three, two, and one for three commonly-used transition systems.",2 A Minimal Feature Set,[0],[0]
"The data is the development section of the English Penn Treebank (PTB), and experimental settings are as described in our other experimental section, §6.",2 A Minimal Feature Set,[0],[0]
"We see that we can go down to three or, in the arc-hybrid and arc-eager transition systems, even two positions with very little loss in performance, but not further.",2 A Minimal Feature Set,[0],[0]
"We therefore call t ÑÐ s0, ÑÐ b 0u our minimal feature set with respect to arc-hybrid and arc-eager, and empirically confirm that Cross and Huang’s t ÑÐ s0, ÑÐ s1, ÑÐ b 0u is minimal for arc-standard; see Table 1 for a summary.3",2 A Minimal Feature Set,[0],[0]
"As stated in the introduction, our minimal feature set from §2 plugs into Huang and Sagae and Kuhlmann et al.’s dynamic programming (DP) framework.",3 Dynamic Programming for TBDPs,[0],[0]
"To help explain the connection, this section provides an overview of the DP framework.",3 Dynamic Programming for TBDPs,[0],[0]
We draw heavily from the presentation of Kuhlmann et al. (2011).,3 Dynamic Programming for TBDPs,[0],[0]
"Transition-based parsing (Nivre, 2008; Kübler et al., 2009) is an incremental parsing framework based on transitions between parser configura-
3We tentatively conjecture that the following might explain the observed phenomena, but stress that we don’t currently see a concrete way to test the following hypothesis.",3.1 Three Transition Systems,[0],[0]
"With t ÑÐ s 0, ÑÐ b 0u, in the arc-standard case, situations can arise where there are multiple possible transitions with missing information.",3.1 Three Transition Systems,[0],[0]
"In contrast, in the arc-hybrid case, there is only one possible transition with missing information (namely, reñ, introduced in §3.1); perhaps ÑÐs 1 is therefore not so crucial for arc-hybrid in practice?
tions.",3.1 Three Transition Systems,[0],[0]
"For a sentence to be parsed, the system starts from a corresponding initial configuration, and attempts to sequentially apply transitions until a configuration corresponding to a full parse is produced.",3.1 Three Transition Systems,[0],[0]
"Formally, a transition system is defined as S “ pC, T, cs, Cτ q, where C is a nonempty set of configurations, each t P T : C á C is a transition function between configurations, cs is an initialization function that maps an input sentence to an initial configuration, and Cτ Ď C is a set of terminal configurations.
",3.1 Three Transition Systems,[0],[0]
"All systems we consider share a common tripartite representation for configurations: when we write c “ pσ, β,Aq for some c P C, we are referring to a stack σ of partially parsed subtrees; a buffer β of unprocessed tokens and, optionally, at its beginning, a subtree with only left descendants; and a set A of elements ph,mq, each of which is an attachment (dependency arc) with head h and modifier m.4",3.1 Three Transition Systems,[0],[0]
"We write mðh to indicate that m left-modifies h, and hñm to indicate that m rightmodifies h. For a sentence w “ w1, ..., wn, the initial configuration is pσ0, β0, A0q, where σ0 and A0 are empty and β0 “ rROOT|w1, ..., wns; ROOT is a special node denoting the root of the parse tree5 (vertical bars are a notational convenience for indicating different parts of the buffer or stack; our convention is to depict the buffer first element leftmost, and to depict the stack first element rightmost).",3.1 Three Transition Systems,[0],[0]
"All terminal configurations have an empty buffer and a stack containing only ROOT.
",3.1 Three Transition Systems,[0],[0]
"Arc-Standard The arc-standard system (Nivre, 2004) is motivated by bottom-up parsing: each dependent has to be complete before being attached.",3.1 Three Transition Systems,[0],[0]
"The three transitions, shift (sh, move a token from the buffer to the stack), right-reduce (reñ, reduce and attach a right modifier), and left-reduce (reð, reduce and attach a left modifier), are defined as:
shrpσ, b0|β,Aqs “ pσ|b0, β, Aq reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq reðrpσ|s1|s0, β, Aqs “ pσ|s0, β, A Y tps0, s1quq
Arc-Hybrid The arc-hybrid system",3.1 Three Transition Systems,[0],[0]
(,3.1 Three Transition Systems,[0],[0]
"Yamada and Matsumoto, 2003; Gómez-Rodrı́guez et al., 2008; Kuhlmann et al., 2011) has the same definitions of sh and reñ as arc-standard, but forces
4For simplicity, we only present unlabeled parsing here.",3.1 Three Transition Systems,[0],[0]
"See Shi et al. (2017) for labeled-parsing results.
",3.1 Three Transition Systems,[0],[0]
"5Other presentations place ROOT at the end of the buffer or omit it entirely (Ballesteros and Nivre, 2013).
",3.1 Three Transition Systems,[0],[0]
the collection of left modifiers before right modifiers via its b0-modifier reð transition.,3.1 Three Transition Systems,[0],[0]
"This contrasts with arc-standard, where the attachment of left and right modifiers can be interleaved on the stack.
",3.1 Three Transition Systems,[0],[0]
"shrpσ, b0|β, Aqs “ pσ|b0, β, Aq reñrpσ|s1|s0, β, Aqs “ pσ|s1, β, A Y tps1, s0quq reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq
Arc-Eager",3.1 Three Transition Systems,[0],[0]
"In contrast to the former two systems, the arc-eager system (Nivre, 2003) makes attachments as early as possible — even if a modifier has not yet received all of its own modifiers.",3.1 Three Transition Systems,[0],[0]
"This behavior is accomplished by decomposing the right-reduce transition into two independent transitions, one making the attachment (ra) and one reducing the right-attached child (re).
",3.1 Three Transition Systems,[0],[0]
"shrpσ, b0|β, Aqs “ pσ|b0, β, Aq reðrpσ|s0, b0|β, Aqs “ pσ, b0|β, A Y tpb0, s0quq
(precondition: s0 not attached to any word)
rarpσ|s0, b0|β, Aqs “ pσ|s0|b0, β, A Y tps0, b0quq",3.1 Three Transition Systems,[0],[0]
"rerpσ|s0, β, Aqs “ pσ, β, Aq
(precondition: s0 has been attached to its head)",3.1 Three Transition Systems,[0],[0]
"Kuhlmann et al. (2011) reformulate the three transition systems just discussed as deduction systems (Pereira and Warren, 1983; Shieber et al., 1995), wherein transitions serve as inference rules; these are given as the lefthand sides of the first three subfigures in Figure 1.",3.2 Deduction and Dynamic Programming,[0],[0]
"For a given w “ w1, ..., wn, assertions take the form ri, j, ks (or, when applicable, a two-index shorthand to be discussed soon), meaning that there exists a sequence of transitions that, starting from a configuration wherein headps0q",3.2 Deduction and Dynamic Programming,[0],[0]
"“ wi, results in an ending configuration wherein headps0q",3.2 Deduction and Dynamic Programming,[0],[0]
“ wj and headpb0q “ wk.,3.2 Deduction and Dynamic Programming,[0],[0]
"If we define w0 as ROOT and wn`1 as an endof-sentence marker, then the goal theorem can be stated as r0, 0, n ` 1s.
",3.2 Deduction and Dynamic Programming,[0],[0]
"For arc-standard, we depict an assertion ri, h, ks as a subtree whose root (head) is the token at h. Assertions of the form ri, i, ks play an important role for arc-hybrid and arc-eager, and we employ the special shorthand ri, ks for them in Figure 1.",3.2 Deduction and Dynamic Programming,[0],[0]
"In that figure, we also graphically depict such situations as two consecutive half-trees with roots wi and wk, where all tokens between i and k are already attached.",3.2 Deduction and Dynamic Programming,[0],[0]
"The superscript b in an arc-eager
assertion rib, js is an indicator variable for whether wi has been attached to its head (b “ 1) or not (b “ 0) after the transition sequence is applied.
",3.2 Deduction and Dynamic Programming,[0],[0]
"Kuhlmann et al. (2011) show that all three deduction systems can be directly “tabularized” and dynamic programming (DP) can be applied, such that, ignoring for the moment the issue of incorporating complex features (we return to this later), time and space needs are low-order polynomial.",3.2 Deduction and Dynamic Programming,[0],[0]
"Specifically, as the two-index shorthand ri, js suggests, arc-eager and arc-hybrid systems can be implemented to take Opn2q space and Opn3q time; the arc-standard system requires Opn3q space and Opn4q time (if one applies the so-called hook trick (Eisner and Satta, 1999)).
",3.2 Deduction and Dynamic Programming,[0],[0]
"Since an Opn4q running time is not sufficiently practical even in the simple-feature case, in the remainder of this paper we consider only the archybrid and arc-eager systems, not arc-standard.",3.2 Deduction and Dynamic Programming,[0],[0]
"By Our Minimal Feature Set
Until now, no one had suggested a set of positional features that was both information-rich enough for accurate parsing and small enough to obtain the Opn3q running-time promised above.",4 Practical Optimal Algorithms Enabled,[0],[0]
"Fortunately, our bi-LSTM-based t ÑÐ s0, ÑÐ b 0u feature set qualifies, and enables the fast optimal procedures described in this section.",4 Practical Optimal Algorithms Enabled,[0],[0]
"Given an input sentence, a TBDP must choose among a potentially exponential number of corresponding transition sequences.",4.1 Exact Decoding,[0],[0]
"We assume access to functions ft that score individual configurations, where these functions are indexed by the transition functions t P T .",4.1 Exact Decoding,[0],[0]
"For a fixed transition sequence t “ t1, t2, . .",4.1 Exact Decoding,[0],[0]
"., we use ci to denote the configuration that results after applying ti.
Typically, for efficiency reasons, greedy left-toright decoding is employed: the next transition t˚i out of ci´1 is arg maxt ftpci´1q, so that past and future decisions are not taken into account.",4.1 Exact Decoding,[0],[0]
"The score F ptq for the transition sequence is induced by summing the relevant ftipci´1q values.
",4.1 Exact Decoding,[0],[0]
"However, our use of minimal feature sets enables direct computation of an argmax over the entire space of transition sequences, arg maxt F ptq, via dynamic programming, because our positions don’t rely on any information “outside” the deduction rule indices, thus eliminating the need for ad-
ditional state-keeping.",4.1 Exact Decoding,[0],[0]
We show how to integrate the scoring functions for the arc-eager system; the arc-hybrid system is handled similarly.,4.1 Exact Decoding,[0],[0]
"The score-annotated rules are as follows:
rib, js :",4.1 Exact Decoding,[0],[0]
"v rj0, j ` 1s : 0 pshq rkb, is : v1 ri0, js : v2 rkb, js : v1 ` v2 ` ∆ preðq
where ∆ “ fshp ÑÐ wk, ÑÐ wiq ` freðp ÑÐ wi, ÑÐ wjq — abusing notation by referring to configurations by their features.",4.1 Exact Decoding,[0],[0]
"The left-reduce rule says that we can first take the sequence of transitions asserted by rkb, is, which has a score of v1, and then a shift transition moving wi from b0 to s0.",4.1 Exact Decoding,[0],[0]
"This means that the initial condition for ri0, js is met, so we can take the sequence of transitions asserted by ri0, js — say it has score v2 — and finally a left-reduce transition to finish composing the larger transition sequence.",4.1 Exact Decoding,[0],[0]
"Notice that the scores for sh and ra are 0, as the scoring of these transitions is accounted for by reduce rules elsewhere in the sequence.",4.1 Exact Decoding,[0],[0]
We employ large-margin training that considers each transition sequence globally.,4.2 Global Training,[0],[0]
"Formally, for a training sentence w “ w1, . . .",4.2 Global Training,[0],[0]
", wn with gold transition sequence tgold, our loss function is
max t
´",4.2 Global Training,[0],[0]
"F ptq ` costptgold, tq ´ F ptgoldq ¯
where costptgold, tq is a custom margin for taking t instead of tgold — specifically, the number of mis-attached nodes.",4.2 Global Training,[0],[0]
"Computing this max can again be done efficiently with a slight modification to the scoring of reduce transitions:
rkb, is : v1 ri0, js : v2 rkb, js : v1 ` v2 ` ∆1 preðq
where ∆1 “ ∆ ` 1 pheadpwiq ‰ wjq.",4.2 Global Training,[0],[0]
"This lossaugmented inference or cost-augmented decoding (Taskar et al., 2005; Smith, 2011) technique has previously been applied to graph-based parsing by Kiperwasser and Goldberg (2016a).
",4.2 Global Training,[0],[0]
Efficiency,4.2 Global Training,[0],[0]
Note,4.2 Global Training,[0],[0]
"The computation decomposes into two parts: scoring all feature combinations, and using DP to find a proof for the goal theorem in the deduction system.",4.2 Global Training,[0],[0]
"Time-complexity analysis is usually given in terms of the latter, but the former might have a large constant factor, such as 104 or worse for neural-network-based scoring
functions.",4.2 Global Training,[0],[0]
"As a result, in practice, with a small n, scoring with the feature set t ÑÐ s0, ÑÐ b 0u (Opn2q) can be as time-consuming as the decoding steps (Opn3q) for the arc-hybrid and arc-eager systems.",4.2 Global Training,[0],[0]
"Our minimal feature set brings implementation of practical optimal algorithms to TBDPs, whereas previously only graph-based dependency parsers (GBDPs) — a radically different, non-incremental paradigm — enjoyed the ability to deploy them.",5 Theoretical Connections,[0],[0]
"Interestingly, for both the transition- and graphbased paradigms, the optimal algorithms build dependency trees bottom-up from local structures.",5 Theoretical Connections,[0],[0]
"It is thus natural to wonder if there are deeper, more formal connections between the two.
",5 Theoretical Connections,[0],[0]
"In previous work, Kuhlmann et al. (2011) related the arc-standard system to the classic CKY algorithm (Cocke, 1969; Kasami, 1965; Younger, 1967) in a manner clearly suggested by Figure 1a; CKY can be viewed as a very simple graph-based approach.",5 Theoretical Connections,[0],[0]
"Gómez-Rodrı́guez et al. (2008, 2011) formally prove that sequences of steps in the edgefactored GBDP (Eisner, 1996) can be used to emulate any individual step in the arc-hybrid system (Yamada and Matsumoto, 2003) and the Eisner and Satta (1999, Figure 1d) version.",5 Theoretical Connections,[0],[0]
"However, they did not draw an explicitly direct connection between Eisner and Satta (1999) and TBDPs.
",5 Theoretical Connections,[0],[0]
"Here, we provide an update to these previous findings, stated in terms of the expressiveness of scoring functions, considered as parameterization.
",5 Theoretical Connections,[0],[0]
"For the edge-factored GBDP, we write the score for an edge as fGp ÑÐ h, ÑÐ mq, where h is the head and m the modifier.",5 Theoretical Connections,[0],[0]
A tree’s score is the sum of its edge scores.,5 Theoretical Connections,[0],[0]
"We say that a parameterized dependency parsing model A contains model B if for every instance of parameterization in model B, there exists an instance of model A such that the two models assign the same score to every parse tree.",5 Theoretical Connections,[0],[0]
"We claim:
Lemma 1.",5 Theoretical Connections,[0],[0]
"The arc-eager model presented in §4.1 contains the edge-factored model.
",5 Theoretical Connections,[0],[0]
Proof Sketch.,5 Theoretical Connections,[0],[0]
"Consider a given edge-factored GBDP parameterized by fG. For any parse tree, every edge iðj involves two deduction rules, and their contribution to the score of the final proof is fsh( ÑÐ wk, ÑÐ wi) ` freðp ÑÐ wi, ÑÐ wjq.",5 Theoretical Connections,[0],[0]
"We set fsh( ÑÐ wk, ÑÐ wi) “ 0 and freðp ÑÐ wi, ÑÐ wjq “ fGp ÑÐ wj , ÑÐ wiq.",5 Theoretical Connections,[0],[0]
"Similarly, for edges kñi in the other direction, we set
fra( ÑÐ wk, ÑÐ wi) “ fGp ÑÐ wk, ÑÐ wiq and frep ÑÐ wi, ÑÐ wjq “ 0.",5 Theoretical Connections,[0],[0]
"The parameterization we arrive at emulates exactly the scoring model of fG.
We further claim that the arc-eager model is more expressive than not only the edge-factored GBDP, but also the arc-hybrid model in our paper.
",5 Theoretical Connections,[0],[0]
Lemma 2.,5 Theoretical Connections,[0],[0]
"The arc-eager model contains the archybrid model.
",5 Theoretical Connections,[0],[0]
Proof Sketch.,5 Theoretical Connections,[0],[0]
"We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions, sh and ra.",5 Theoretical Connections,[0],[0]
"When we constrain the parameters fsh “ fra in the arc-eager model, the model hypothesis space becomes exactly the same as arc-hybrid’s.
",5 Theoretical Connections,[0],[0]
The extra expressiveness of the arc-eager model comes from the scoring functions fsh and fre that capture structural contexts other than headmodifier relations.,5 Theoretical Connections,[0],[0]
"Unlike traditional higher-order graph-based parsing that directly models relations such as siblinghood (McDonald and Pereira, 2006) or grandparenthood (Carreras, 2007), however, the arguments in those two functions do not have any fixed type of structural interactions.",5 Theoretical Connections,[0],[0]
Data and Evaluation We experimented with English and Chinese.,6 Experiments,[0],[0]
"For English, we used the Stanford Dependencies (de Marneffe and Manning, 2008) conversion (via the Stanford parser 3.3.0) of the Penn Treebank (Marcus et al., 1993, PTB).",6 Experiments,[0],[0]
"As is standard, we used §2-21 of the Wall Street Journal for training, §22 for development,
and §23 for testing; POS tags were predicted using 10-way jackknifing with the Stanford max entropy tagger (Toutanova et al., 2003).",6 Experiments,[0],[0]
"For Chinese, we used the Penn Chinese Treebank 5.1 (Xue et al., 2002, CTB), with the same splits and head-finding rules for conversion to dependencies as Zhang and Clark (2008).",6 Experiments,[0],[0]
We adopted the CTB’s goldstandard tokenization and POS tags.,6 Experiments,[0],[0]
We report unlabeled attachment score (UAS) and sentencelevel unlabeled exact match (UEM).,6 Experiments,[0],[0]
"Following prior work, all punctuation is excluded from evaluation.",6 Experiments,[0],[0]
"For each model, we initialized the network parameters with 5 different random seeds and report performance average and standard deviation.
",6 Experiments,[0],[0]
Implementation Details,6 Experiments,[0],[0]
Our model structures reproduce those of Kiperwasser and Goldberg (2016a).,6 Experiments,[0],[0]
We use 2-layer bi-directional LSTMs with 256 hidden cell units.,6 Experiments,[0],[0]
"Inputs are concatenations of 28-dimensional randomly-initialized partof-speech embeddings and 100-dimensional word vectors initialized from GloVe vectors (Pennington et al., 2014) (English) and pre-trained skipgram-model vectors (Mikolov et al., 2013) (Chinese).",6 Experiments,[0],[0]
The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron (MLP) with 1 hidden layer which has 256 hidden units and activation function tanh.,6 Experiments,[0],[0]
"We set the dropout rate for the bi-LSTM (Gal and Ghahramani, 2016) and MLP (Srivastava et al., 2014) for each model according to development-set performance.6 All parameters except the word embed-
6For bi-LSTM input and recurrent connections, we consider dropout rates in t0., 0.2u, and for MLP, t0., 0.4u.
dings are initialized uniformly (Glorot and Bengio, 2010).",6 Experiments,[0],[0]
"Approximately 1,000 tokens form a mini-batch for sub-gradient computation.",6 Experiments,[0],[0]
We train each model for 20 epochs and perform model selection based on development UAS.,6 Experiments,[0],[0]
"The proposed structured loss function is optimized via Adam (Kingma and Ba, 2015).",6 Experiments,[0],[0]
"The neural network computation is based on the python interface to DyNet (Neubig et al., 2017), and the exact decoding algorithms are implemented in Cython.7
Main Results We implement exact decoders for the arc-hybrid and arc-eager systems, and present the test performance of different model configurations in Table 2, comparing global models with local models.",6 Experiments,[0],[0]
All models use the same decoder for testing as during the training process.,6 Experiments,[0],[0]
"Though no global decoder for the arc-standard system has been explored in this paper, its local models are listed for comparison.",6 Experiments,[0],[0]
"We also include an edgefactored graph-based model, which is conventionally trained globally.",6 Experiments,[0],[0]
The edge-factored model scores bi-LSTM features for each head-modifier pair; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores.,6 Experiments,[0],[0]
"For this model, we use Dozat and Man-
7See https://github.com/tzshi/dp-parser-emnlp17 .
ning’s (2017) biaffine scoring model, although in our case the model size is smaller.8
Analogously to the dev-set results given in §2, on the test data, the minimal feature sets perform as well as larger ones in locally-trained models.",6 Experiments,[0],[0]
And there exists a clear trend of global models outperforming local models for the two different transition systems on both datasets.,6 Experiments,[0],[0]
This illustrates the effectiveness of exact decoding and global training.,6 Experiments,[0],[0]
"Of the three types of global models, the arceager arguably has the edge, an empirical finding resonating with our theoretical comparison of their model expressiveness.
",6 Experiments,[0],[0]
"Comparison with State-of-the-Art Models Figure 2 compares our algorithms’ results with those of the state-of-the-art.9 Our models are competitive and an ensemble of 15 globallytrained models (5 models each for arc-eager DP, arc-hybrid DP and edge-factored) achieves 95.33 and 90.22 on PTB and CTB, respectively, reach-
8The same architecture and model size as other transitionbased global models is used for fair comparison.
9We exclude Choe and Charniak (2016), Kuncoro et al. (2017) and Liu and Zhang (2017), which convert constituentbased parses to dependency parses.",6 Experiments,[0],[0]
"They produce higher PTB UAS, but access more training information and do not directly apply to datasets without constituency annotation.
",6 Experiments,[0],[0]
"ing the highest reported UAS on the CTB dataset, and the second highest reported on the PTB dataset among dependency-based approaches.",6 Experiments,[0],[0]
"Approximate Optimal Decoding/Training Besides dynamic programming (Huang and Sagae, 2010; Kuhlmann et al., 2011), various other approaches have been proposed for approaching global training and exact decoding.",7 Related Work Not Yet Mentioned,[0],[0]
"Best-first and A* search (Klein and Manning, 2003; Sagae and Lavie, 2006; Sagae and Tsujii, 2007; Zhao et al., 2013; Thang et al., 2015; Lee et al., 2016) give optimality certificates when solutions are found, but have the same worst-case time complexity as the original search framework.",7 Related Work Not Yet Mentioned,[0],[0]
"Other common approaches to search a larger space at training or test time include beam search (Zhang and Clark, 2011), dynamic oracles (Goldberg and Nivre, 2012, 2013; Cross and Huang, 2016b) and error states (Vaswani and Sagae, 2016).",7 Related Work Not Yet Mentioned,[0],[0]
"Beam search records the k best-scoring transition prefixes to delay local hard decisions, while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment.
",7 Related Work Not Yet Mentioned,[0],[0]
"Neural Parsing Neural-network-based models are widely used in state-of-the-art dependency parsers (Henderson, 2003, 2004; Chen and Manning, 2014; Weiss et al., 2015; Andor et al., 2016; Dozat and Manning, 2017) because of their expressive representation power.",7 Related Work Not Yet Mentioned,[0],[0]
"Recently, Stern et al. (2017) have proposed minimal span-based features for constituency parsing.
",7 Related Work Not Yet Mentioned,[0],[0]
"Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree (Le and Zuidema, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016b), but these models cannot be readily combined with DP approaches, because their state spaces cannot be merged into smaller sets and thus remain exponentially large.",7 Related Work Not Yet Mentioned,[0],[0]
"In this paper, we have shown the following.
",8 Concluding Remarks,[0],[0]
"• The bi-LSTM-powered feature set tÑÐs0, ÑÐ b 0u
is minimal yet highly effective for arc-hybrid and arc-eager transition-based parsing.
",8 Concluding Remarks,[0],[0]
"• Since DP algorithms for exact decoding (Huang and Sagae, 2010; Kuhlmann et al.,
2011) have a run-time dependence on the number of positional features, using our mere two effective positional features results in a running time of Opn3q, feasible for practice.
",8 Concluding Remarks,[0],[0]
•,8 Concluding Remarks,[0],[0]
"Combining exact decoding with global training — which is also enabled by our minimal feature set — with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank: these are, to our knowledge, the best and secondbest results to date on these data sets among “purely” dependency-based approaches.
",8 Concluding Remarks,[0],[0]
There are many directions for further exploration.,8 Concluding Remarks,[0],[0]
"Two possibilities are to create even better training methods, and to find some way to extend our run-time improvements to other transition systems.",8 Concluding Remarks,[0],[0]
It would also be interesting to further investigate relationships between graph-based and dependency-based parsing.,8 Concluding Remarks,[0],[0]
"In §5 we have mentioned important earlier work in this regard, and provided an update to those formal findings.
",8 Concluding Remarks,[0],[0]
"In our work, we have brought exact decoding, which was formerly the province solely of graphbased parsing, to the transition-based paradigm.",8 Concluding Remarks,[0],[0]
"We hope that the future will bring more inspiration from an integration of the two perspectives.
",8 Concluding Remarks,[0],[0]
Acknowledgments: an author-reviewer success story We sincerely thank all the reviewers for their extraordinarily careful and helpful comments.,8 Concluding Remarks,[0],[0]
"Indeed, this paper originated as a short paper submission by TS&LL to ACL 2017, where an anonymous reviewer explained in the review comments how, among other things, the DP runtime could be improved from Opn4q to Opn3q.",8 Concluding Remarks,[0],[0]
"In their author response, TS&LL invited the reviewer to co-author, suggesting that they ask the conference organizers to make the connection between anonymous reviewer and anonymous authors.",8 Concluding Remarks,[0],[0]
"All three of us are truly grateful to PC co-chair Regina Barzilay for implementing this idea, bringing us together!
",8 Concluding Remarks,[0],[0]
"We also thank Kai Sun for help with Chinese word vectors, and Xilun Chen, Yao Cheng, Dezhong Deng, Juneki Hong, Jon Kleinberg, Ryan McDonald, Ashudeep Singh, and Kai Zhao for discussions and suggestions.",8 Concluding Remarks,[0],[0]
TS and LL were supported in part by a Google focused research grant to Cornell University.,8 Concluding Remarks,[0],[0]
"LH was supported in part by NSF IIS-1656051, DARPA N66001-17-24030, and a Google Faculty Research Award.",8 Concluding Remarks,[0],[0]
"We first present a minimal feature set for transition-based dependency parsing, continuing a recent trend started by Kiperwasser and Goldberg (2016a) and Cross and Huang (2016a) of using bi-directional LSTM features.",abstractText,[0],[0]
We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae (2010) and Kuhlmann et al. (2011) to produce the first implementation of worst-case Opn3q exact decoders for arc-hybrid and arceager transition systems.,abstractText,[0],[0]
"With our minimal features, we also present Opn3q global training methods.",abstractText,[0],[0]
"Finally, using ensembles including our new parsers, we achieve the best unlabeled attachment score reported (to our knowledge) on the Chinese Treebank and the “second-best-in-class” result on the English Penn Treebank.",abstractText,[0],[0]
Fast(er) Exact Decoding and Global Training for Transition-Based Dependency Parsing via a Minimal Feature Set,title,[0],[0]
"Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition.",1 Introduction,[0],[0]
"Statistical LMs are probabilistic models that assign a probability to a sequence of words wN1 , indicating how likely the sequence is in the language.",1 Introduction,[0],[0]
"m-gram LMs are popular, and prove to be accurate when estimated using large corpora.",1 Introduction,[0],[0]
"In these LMs, the probabilities ofm-grams are often precomputed and stored explicitly.
",1 Introduction,[0],[0]
"Although widely successful, current m-gram LM approaches are impractical for learning high-order LMs on large corpora, due to their poor scaling properties in both training and query phases.",1 Introduction,[0],[0]
"Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute allm-gram probabilities, and consequently
need to store and access as many as a hundred of billions of m-grams for a typical moderate-order LM.
",1 Introduction,[0],[0]
"Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007).",1 Introduction,[0],[0]
"Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying.",1 Introduction,[0],[0]
"Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly.
",1 Introduction,[0],[0]
"In our previous work (Shareghi et al., 2015), we extended this line of research using a Compressed Suffix Tree (CST) (Ohlebusch et al., 2010), which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree.",1 Introduction,[0],[0]
This approach showed favourable scaling properties with m and had only a modest memory requirement.,1 Introduction,[0],[0]
"However, the method only supported Kneser-Ney smoothing, not its modified variant (Chen and Goodman, 1999) which overall performs better and has become the de-facto standard.",1 Introduction,[0],[0]
"Additionally, querying was significantly slower than for leading LM toolkits, making the method impractical for widespread use.
",1 Introduction,[0],[0]
"In this paper we extend Shareghi et al. (2015) to support modified Kneser-Ney smoothing, and
477
Transactions of the Association for Computational Linguistics, vol. 4, pp.",1 Introduction,[0],[0]
"477–490, 2016.",1 Introduction,[0],[0]
Action Editor: Brian Roark.,1 Introduction,[0],[0]
"Submission batch: 1/2016; Revision batch: 6/2016; Published 9/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"present new optimisation methods for fast construction and querying.1 Critical to our approach are:
• Precomputation of several modified counts, which would be very expensive to compute at query time.",1 Introduction,[0],[0]
"To orchestrate this, a subset of the CST nodes is selected based on the cost of computing their modified counts (which relates with the branching factor of a node).",1 Introduction,[0],[0]
"The precomputed counts are then stored in a compressed data structure supporting efficient memory usage and lookup.
",1 Introduction,[0],[0]
"• Re-use of CST nodes within m-gram probability computation as a sentence gets scored leftto-right, thus saving many expensive lookups.
",1 Introduction,[0],[0]
"Empirical comparison against our earlier work (Shareghi et al., 2015) shows the significance of each of these optimisations.",1 Introduction,[0],[0]
"The strengths of our method are apparent when applied to very large training datasets (≥ 16 GiB) and for high order models, m ≥ 5.",1 Introduction,[0],[0]
"In this setting, while our approach is more memory efficient than the leading KenLM model, both in the construction (training) and querying (testing) phases, we are highly competitive in terms of runtimes of both phases.",1 Introduction,[0],[0]
"When memory is a limiting factor at query time, our approach is orders of magnitude faster than the state of the art.",1 Introduction,[0],[0]
"Moreover, our method allows for efficient querying with an unlimited Markov order, m → ∞, without resorting to approximations or heuristics.",1 Introduction,[0],[0]
"In an m-gram language model, the probability of a sentence is decomposed into ∏N i=1",2 Modified Kneser-Ney Language Model,[0],[0]
"P (wi|wi−1i−m+1), where P (wi|wi−1i−m+1) is the conditional probability of the next word given its finite history.",2 Modified Kneser-Ney Language Model,[0],[0]
Smoothing techniques are employed to deal with sparsity when estimating the parameters of P (wi|wi−1i−m+1).,2 Modified Kneser-Ney Language Model,[0],[0]
"A comprehensive comparison of different smoothing techniques is provided in (Chen and Goodman, 1999).",2 Modified Kneser-Ney Language Model,[0],[0]
"We focus on interpolated Modified KneserNey (MKN) smoothing, which is widely regarded as a state-of-the-art technique and is supported by popular language modelling toolkits, e.g. SRILM (Stolcke, 2002) and KenLM (Heafield, 2011).
",2 Modified Kneser-Ney Language Model,[0],[0]
"1https://github.com/eehsan/cstlm
MKN is a recursive smoothing technique which uses lower order k-gram language models to smooth higher order models.",2 Modified Kneser-Ney Language Model,[0],[0]
Figure 1 describes the recursive smoothing formula employed in MKN.,2 Modified Kneser-Ney Language Model,[0],[0]
It is distinguished from Kneser-Ney (KN) smoothing in its use of adaptive discount parameters (denoted as Dk(j) in Figure 1) based on the k-gram counts.,2 Modified Kneser-Ney Language Model,[0],[0]
"Importantly, MKN is based on not just m-gram frequencies, c(x), but also several modified counts based on numbers of unique contexts, namely
Ni+(α·) =",2 Modified Kneser-Ney Language Model,[0],[0]
|{w s.t. c(αw) ≥ i}| Ni+(·α) = |{w,2 Modified Kneser-Ney Language Model,[0],[0]
s.t. c(wα) ≥ i}| Ni+(·α·) = |{w1w2 s.t. c(w1αw2) ≥ i}| N ′i+(α·) = ∣∣{w s.t. N1+(·αw),2 Modified Kneser-Ney Language Model,[0],[0]
"≥ i}∣∣ .
Ni+(·α) and Ni+(α·) are the number of words with frequency at least i that come before and after a pattern α, respectively.",2 Modified Kneser-Ney Language Model,[0],[0]
Ni+(·α·) is the number of word-pairs with frequency at least i which surround α.,2 Modified Kneser-Ney Language Model,[0],[0]
N ′i+(α·) is the number of words coming after α to form a pattern αw for which the number of unique left contexts is at least i; it is specific to MKN and not needed in KN.,2 Modified Kneser-Ney Language Model,[0],[0]
"Table 1 illustrates the
different types of quantities required for computing an example 4-gram MKN probability.
",2 Modified Kneser-Ney Language Model,[0],[0]
"Efficient computation of these quantities is challenging with limited memory and time resources, particularly when the order of the language model m is high and/or the training corpus is large.",2 Modified Kneser-Ney Language Model,[0],[0]
"In this paper, we make use of advanced data structures to efficiently obtain the required quantities to answer probabilistic queries as they arrive.",2 Modified Kneser-Ney Language Model,[0],[0]
"Our solution involves precomputing and caching expensive quantities, N1+(·α·), N1+(·α), N{1,2,3+}(·α) and N ′{1,2,3+}(α·), which we will explain in §4.",2 Modified Kneser-Ney Language Model,[0],[0]
We start in §3 by providing a review of the approach in Shareghi et al. (2015) upon which we base our work.,2 Modified Kneser-Ney Language Model,[0],[0]
"Shareghi et al. (2015) proposed a method for Kneser-Ney (KN) language modelling based on onthe-fly probability computation from a compressed suffix tree (CST) (Ohlebusch et al., 2010).",3.1 Compressed Data Structures,[0],[0]
"The CST emulates the functionality of the Suffix Tree (ST) (Weiner, 1973) using substantially less space.",3.1 Compressed Data Structures,[0],[0]
The suffix tree is a classical search index consisting of a rooted labelled search tree constructed from a text T of length n drawn from an alphabet of size σ.,3.1 Compressed Data Structures,[0],[0]
Each root to leaf path in the suffix tree corresponds to a suffix of T .,3.1 Compressed Data Structures,[0],[0]
"The leaves, considered in left-toright order define the suffix array (SA) (Manber and Myers, 1993) such that the suffix T",3.1 Compressed Data Structures,[0],[0]
"[SA[i], n − 1] is lexicographically smaller than T [SA[i+1], n−1] for i ∈",3.1 Compressed Data Structures,[0],[0]
"[0, n− 2].",3.1 Compressed Data Structures,[0],[0]
Searching for a pattern α of length m in T can be achieved by finding the “highest” node v in the ST such that the path from the root to v is prefixed by α.,3.1 Compressed Data Structures,[0],[0]
All leaf nodes in the subtree starting at v correspond to the locations of α in T .,3.1 Compressed Data Structures,[0],[0]
"This is translated to finding the specific range SA[lb, rb] such that
T [SA[j], SA[j +m− 1]]",3.1 Compressed Data Structures,[0],[0]
= α for j ∈,3.1 Compressed Data Structures,[0],[0]
"[lb, rb]
as illustrated in the ST and SA of Figure 2 (left).",3.1 Compressed Data Structures,[0],[0]
"While searching using the ST or the SA is efficient in theory, it requires large amounts of main memory.",3.1 Compressed Data Structures,[0],[0]
"A CST reduces the space requirements of ST by utilizing the compressibility of the BurrowsWheeler transform (BWT) (Burrows and Wheeler, 1994).",3.1 Compressed Data Structures,[0],[0]
The BWT corresponds to a reversible permutation of the text used in data compression tools such as BZIP2 to increase the compressibility of the input.,3.1 Compressed Data Structures,[0],[0]
"The transform is defined as
BWT[i] = T[SA[i]− 1 mod n] (1)
and is the core component of the FM-Index (Ferragina and Manzini, 2000) which is a subcomponent of a CST to provide efficient search for locating arbitrary length patterns (m-grams), determining occurrence frequencies etc.",3.1 Compressed Data Structures,[0],[0]
"The key functionality provided by the FM-Index is the ability to efficiently determine the range SA[lb, rb] matching a given pattern α described above without the need to store the ST or SA explicitly.",3.1 Compressed Data Structures,[0],[0]
"This is achieved by iteratively processing α in reverse order using the BWT, which is usually referred to as backward-search.
",3.1 Compressed Data Structures,[0],[0]
"The backward-search procedure utilizes the duality between the BWT and SA to iteratively determine SA[lb, rb] for suffixes of α.",3.1 Compressed Data Structures,[0],[0]
"Suppose SA[spj , epj ] corresponds to all suffixes in T matchingα[j,m−1].",3.1 Compressed Data Structures,[0],[0]
"Range SA[spj−1, epj−1] matching α[j − 1,m − 1] with c def= α[j − 1] can be expressed as
spj−1 = C[c] + RANK(BWT, spj , c)",3.1 Compressed Data Structures,[0],[0]
"epj−1 = C[c+ 1] + RANK(BWT, epj + 1, c)− 1
where C[c] refers to the starting position of all suffixes prefixed by c in SA and RANK(BWT, spj , c) determines the number of occurrences of symbol c in BWT[0, spj ].
",3.1 Compressed Data Structures,[0],[0]
"Operation RANK(BWT, i, c) (and its inverse operation SELECT(BWT,i,c)2) can be performed efficiently using a wavelet tree (Grossi et al., 2003) representation of the BWT.",3.1 Compressed Data Structures,[0],[0]
"A wavelet tree is a versatile, space-efficient representation of a sequence which can efficiently support a variety of operations (Navarro, 2014).",3.1 Compressed Data Structures,[0],[0]
The structure of the wavelet tree is derived by recursively decomposing the alphabet into subsets.,3.1 Compressed Data Structures,[0],[0]
"At each level the alphabet is
2SELECT(BWT,i,c) returns the position of the ith occurrence of symbol c in BWT.
split into two subsets based on which symbols in the sequence are assigned to the left and right child nodes respectively.",3.1 Compressed Data Structures,[0],[0]
"Using compressed bitvector representations and Huffman codes to define the alphabet partitioning, the space usage of the wavelet tree and associated RANK structures of the BWT is bound by Hk(T)n + o(n log σ) bits (Grossi et al., 2003).",3.1 Compressed Data Structures,[0],[0]
"Thus the space usage is proportional to the order k entropy (Hk(T)) of the text.
",3.1 Compressed Data Structures,[0],[0]
Figure 2 (right) shows a sample wavelet tree representation.,3.1 Compressed Data Structures,[0],[0]
"Using the wavelet tree structure, RANK over a sequence drawn from an alphabet of size σ can be reduced to log σ binary RANK operations which can be answered efficiently in constant time (Jacobson, 1989).",3.1 Compressed Data Structures,[0],[0]
"The range SA[lb, rb] corresponding to a pattern α, can be determined in O(m log σ) time using a wavelet tree of the BWT.
",3.1 Compressed Data Structures,[0],[0]
"In addition to the FM-index, a CST efficiently stores the tree topology of the ST to emulate tree operations such efficiently (Ohlebusch et al., 2010).",3.1 Compressed Data Structures,[0],[0]
"Shareghi et al. (2015) showed how the requisite counts for a KN-LM, namely c(α), N1+(·α), N1+(·α·) and N1+(α·), can be computed directly from CST.",3.2 Computing KN modified counts,[0],[0]
"Consider the example in Figure 2, the number of occurrences of b corresponds to counting the number of leaves, size(v), in the subtree rooted at v.",3.2 Computing KN modified counts,[0],[0]
"This can be computed in O(1) time by computing the size of the range [lb, rb]
implicitly associated with each node.",3.2 Computing KN modified counts,[0],[0]
The number of unique right contexts of b can be determined using degree(v) (again O(1) but requires bit operations on the succinct tree representation of the ST).,3.2 Computing KN modified counts,[0],[0]
"That is, N1+(b·) = degree(v) = 3.
",3.2 Computing KN modified counts,[0],[0]
Determining the number of left-contexts and surrounding contexts is more involved.,3.2 Computing KN modified counts,[0],[0]
Computing N1+(·α) relies on the BWT.,3.2 Computing KN modified counts,[0],[0]
Recall that BWT[i] corresponds to the symbol preceding the suffix starting at SA[i].,3.2 Computing KN modified counts,[0],[0]
For example computing N1+(·b),3.2 Computing KN modified counts,[0],[0]
"first requires finding the interval of suffixes starting with b in SA, namely lb = 6 and rb = 10, and then counting the number of unique symbols in BWT[6, 10] = {d, b, a, a, a}, i.e., 3.",3.2 Computing KN modified counts,[0],[0]
"Determining all unique symbols in BWT[i, j] can be performed efficiently (independently of the size of the range) using the wavelet tree encoding of the BWT.",3.2 Computing KN modified counts,[0],[0]
"The set of symbols preceding pattern α, denoted by P (α) can be computed in O(|P",3.2 Computing KN modified counts,[0],[0]
(α)| log σ),3.2 Computing KN modified counts,[0],[0]
"by visiting all unique leafs in the wavelet tree which correspond to symbols in BWT[i, j].",3.2 Computing KN modified counts,[0],[0]
"This is usually referred to as the interval-symbols (Schnattinger et al., 2010) procedure and uses RANK operations to find the set of symbols s ∈ P (α) and corresponding ranges for sα in SA.",3.2 Computing KN modified counts,[0],[0]
"In the above example, identifying the SA range of ab requires finding the lb, rb in the SA for suffixes starting with a (SA [3,5]) and then adjusting the bounds to cover only the suffixes starting with ab.",3.2 Computing KN modified counts,[0],[0]
"This last step is done via computing the rank of three a symbols
in BWT[8,10] using the wavelet tree, see Figure 2 (right) for RANK(BWT, a, 8).",3.2 Computing KN modified counts,[0],[0]
"As illustrated, answering RANK(BWT, 8, a) corresponds to processing the first digit of the code word at the root level, which translates into RANK(WTroot, 8, 0)",3.2 Computing KN modified counts,[0],[0]
"= 4, followed by a RANK(WT1, 4, 1) = 1 on the left branch.",3.2 Computing KN modified counts,[0],[0]
"Once the ranks are computed lb, rb are refined accordingly to SA",3.2 Computing KN modified counts,[0],[0]
"[3+ (1 - 1), 3+ (3 - 1)].",3.2 Computing KN modified counts,[0],[0]
"Finally, for N1+(·α·) all patterns which can follow α are enumerated, and for each of these extended patterns, the number of preceding symbols is computed using interval-symbols.",3.2 Computing KN modified counts,[0],[0]
"This proved to be the most expensive operation in their approach.
",3.2 Computing KN modified counts,[0],[0]
"Given these quantities, Shareghi et al. (2015) show how m-gram probabilities can be computed on demand using an iterative algorithm to search for matching nodes in the suffix tree for the required kgram (k ≤ m) patterns in the numerator and denominator of the KN recursive equations, which are then used to compute the probabilities.",3.2 Computing KN modified counts,[0],[0]
We refer the reader to Shareghi et al. (2015) for further details.,3.2 Computing KN modified counts,[0],[0]
"Overall their approach showed promise, in that it allowed for unlimited order KN-LMs to be evaluated with a modest memory footprint, however it was many orders of magnitude slower for smaller m than leading LM toolkits.
",3.2 Computing KN modified counts,[0],[0]
"To illustrate the cost of querying, see Figure 3 (top) which shows per-sentence query time for KN,
Algorithm 1 N{1,2,3+}(α·) or N ′{1,2,3+}(α·) 1: function N123PFRONT(t, v, α, is-prime)
.",3.2 Computing KN modified counts,[0],[0]
"t is a CST, v is the node matching pattern α 2: N1, N2, N3+← 0 3: for u← children(v) do 4: if is-prime then 5: f ← interval-symbols(t, [lb(u), rb(u)]) 6: else 7: f ← size(u) 8: if 1 ≤ f ≤ 2 then 9:",3.2 Computing KN modified counts,[0],[0]
"Nf ← Nf + 1
10: N3+← degree(v)−N1 −N2 11: return N1, N2, N3+
based on the approach of Shareghi et al. (2015) (also shown is MKN, through an extension of their method as described in §4).",3.2 Computing KN modified counts,[0],[0]
"It is clear that the runtimes for KN are much too long for practical use – about 5 seconds per sentence, with the majority of this time spent computingN1+(·α·).",3.2 Computing KN modified counts,[0],[0]
"Clearly optimisation is warranted, and the gains from it are spectacular (see Figure 3 bottom, which uses the precomputation method as described in §4.2).",3.2 Computing KN modified counts,[0],[0]
"A central requirement for extending Shareghi et al. (2015) to support MKN are algorithms for computing N{1,2,3+}(α·) and N ′{1,2,3+}(α·), which we now expound upon.",4.1 Computing MKN modified counts,[0],[0]
"Algorithm 1 computes both of these quantities, taking as input a CST t, a node v matching the pattern α, the pattern and a flag is-prime, denoting which of the N and N ′ variants is required.",4.1 Computing MKN modified counts,[0],[0]
"This method enumerates the children of the node (line 3) and calculates either the frequency of each child (line 7) or the modified countN1+(·α x), for each child u where x is the first symbol on the edge vu (line 5).",4.1 Computing MKN modified counts,[0],[0]
"Lines 8 and 9 accumulate the number of these values equal to one or two, and finally in line 10, N3+ is computed by the difference between N1+(α·) = degree(v) and the already counted events N1 +N2.
",4.1 Computing MKN modified counts,[0],[0]
"For example, computing N{1,2,3+}(b·) in Figure 2 corresponds to enumerating over its three children.",4.1 Computing MKN modified counts,[0],[0]
"Two of v’s children are leaf nodes {10, 8}, and one child has three leaf descendants {11, 2, 5}, hence N1 and N2 are 2 and 0 respectively, and N3+ is 1.",4.1 Computing MKN modified counts,[0],[0]
"Further, consider computing N ′{1,2,3+}(b·) in
Figure 2, which again enumerates over child nodes (whose path labels start with symbols b, c and d) and computes the number of preceding symbols for the extended patterns.3 Accordingly N ′1(b·)",4.1 Computing MKN modified counts,[0],[0]
"= 2, N ′2(b·) = 1 and N ′3+(b·) = 0.
",4.1 Computing MKN modified counts,[0],[0]
"While roughly similar in approach, computing N ′{1,2,3+}(α·) is in practice slower than N{1,2,3+}(α·) since it requires calling intervalsymbols (line 7) instead of calling the constant time size operation (line 5).",4.1 Computing MKN modified counts,[0],[0]
"This gives rise to a time complexity of O(d|P (α)| log σ) for N ′{1,2,3+}(α·) where d is the number of children of v.
As illustrated in Figure 3 (top), the modified counts (§2) combined are responsible for 99% of the query time.",4.1 Computing MKN modified counts,[0],[0]
Moreover the already expensive runtime of KN is considerably worsened in MKN due to the additional counts required.,4.1 Computing MKN modified counts,[0],[0]
"These facts motivate optimisation, which we achieve by precomputing values, resulting in a 2500× speed up in query time as shown in Figure 3 (bottom).",4.1 Computing MKN modified counts,[0],[0]
"Language modelling toolkits such as KenLM and SRILM precompute real valued probabilities and backoff-weights at training time, such that querying becomes largely a problem of retrieval.",4.2 Efficient Precomputation,[0],[0]
"We might consider taking a similar route in optimising our language model, however we would face the problem that floating point numbers cannot be compressed very effectively.",4.2 Efficient Precomputation,[0],[0]
"Even with quantisation, which can have a detrimental effect on modelling perplexity, we would not expect good compression and thus this technique would limit the scaling potential of our approach.
",4.2 Efficient Precomputation,[0],[0]
"For these reasons, instead we store the most expensive count data, targeting those counts which have the greatest effect on runtime (see Figure 3 top).",4.2 Efficient Precomputation,[0],[0]
"We expect these integer values to compress well: as highlighted by Figure 4 most counts will have low values, and accordingly a variable byte compression scheme will be able to realise high compression rates.",4.2 Efficient Precomputation,[0],[0]
"Removing the need for computing these values at query time leaves only pattern search and a few floating point operations in order to compute language model probabilities (see §4.3) which can be done cheaply.
",4.2 Efficient Precomputation,[0],[0]
"3That is N1+(· bb) = 1, N1+(· bc) = 2, N1+(· bd) = 1.
",4.2 Efficient Precomputation,[0],[0]
Our first consideration is how to structure the cache.,4.2 Efficient Precomputation,[0],[0]
"Given that each precomputed value is computed using a CST node, v, (with the pattern as its path-label), we structure the cache as a mapping between unique node identifiers id(v) and the precomputed values.4",4.2 Efficient Precomputation,[0],[0]
"Next we consider which values to cache: while it is possible to precompute values for every node in the CST, many nodes are unlikely to be accessed at query time.",4.2 Efficient Precomputation,[0],[0]
"Moreover, these rare patterns are likely to be cheap to process using the onthe-fly methods, given they occur in few contexts.",4.2 Efficient Precomputation,[0],[0]
"Consequently precomputing these values will bring minimal speed benefits, while still incurring a memory cost.",4.2 Efficient Precomputation,[0],[0]
"For this reason we precompute the values only for nodes corresponding to k-grams up to length m̂ (for our word-level experiments m̂ = 10), which are most likely to be accessed at runtime.5
The precomputation method is outlined in Algorithm 2, showing how a compressed cache is created for the quantities x ∈ {N1+(·α), N1+(·α·), N12(α·), N ′12(α·)}.",4.2 Efficient Precomputation,[0],[0]
"The algorithm visits the suffix tree nodes in depthfirst-search (DFS) order, and selects a subset of nodes for precomputation (line 7), such that the remaining nodes are either rare or trivial to handle
4Each node can uniquely be identified by the order which it is visited in a DFS traversal of the suffix tree.",4.2 Efficient Precomputation,[0],[0]
"This corresponds to the RANK of the opening parenthesis of the node in the balanced parenthesis representation of the tree topology of the CST which can be determined inO(1) time (Ohlebusch et al., 2010).
",4.2 Efficient Precomputation,[0],[0]
5We did not test other selection criteria.,4.2 Efficient Precomputation,[0],[0]
"Other methods may be more effective, such as selecting nodes for precomputation based on the frequency of their corresponding patterns in the training set.
",4.2 Efficient Precomputation,[0],[0]
Algorithm 2,4.2 Efficient Precomputation,[0],[0]
"Precomputing expensive counts N{1,2}(α·), N1+(·α·), N1+(·α), N ′{1,2}(α·).",4.2 Efficient Precomputation,[0],[0]
"1: function PRECOMPUTE(t, m̂) 2: bvl← 0 ∀l ∈",4.2 Efficient Precomputation,[0],[0]
"[0, nodes(t)− 1] 3: i(x)l ← 0 ∀l",4.2 Efficient Precomputation,[0],[0]
∈,4.2 Efficient Precomputation,[0],[0]
"[0, nodes(t)− 1], x ∈ count types 4: j← 0 5: for v ← descendants(root(t)) do .",4.2 Efficient Precomputation,[0],[0]
DFS 6: d←,4.2 Efficient Precomputation,[0],[0]
string-depth(v) 7: if not is-leaf(v) ∧ d ≤ m̂ then 8: l← id(v) .,4.2 Efficient Precomputation,[0],[0]
"unique id 9: bvl← 1 10: Call N1PFRONTBACK1(t, v, ·) 11: Call N123PFRONT(t, v, ·, 0) 12: Call N123PFRONT(t, v, ·, 1) 13: i(x)j ← counts from above, for each output x 14: j← j + 1 15: bvrrr ← compress-rrr(bv) 16: i← compress-dac({i(x) ∀x}) 17: write-to-disk(bvrrr ,i)
on-the-fly (i.e., leaf nodes).",4.2 Efficient Precomputation,[0],[0]
"A node included in the cache is marked by storing a 1 in the bit vector bv (lines 8-9) at index l, where l is the node identifier.",4.2 Efficient Precomputation,[0],[0]
"For each selected node we precompute the expensive counts in lines 10-12,
N1+(·α·), N1+(·α)",4.2 Efficient Precomputation,[0],[0]
"via6 N1PFRONTBACK1(t, v, ·), N{1,2}(α·) via N123PFRONT(t, v, ·, 0), N ′{1,2}(α·) via N123PFRONT(t, v, ·, 1),
which are stored into integer vectors i(x) for each count type x (line 13).",4.2 Efficient Precomputation,[0],[0]
"The integer vectors are streamed to disk and then compressed (lines 15-17) in order to limit memory usage.
",4.2 Efficient Precomputation,[0],[0]
The final steps in lines 15 and 16 compress the integer and bit-vectors.,4.2 Efficient Precomputation,[0],[0]
"The integer vectors i(x) are compressed using a variable length encoding, namely Directly Addressable Variable-Length Codes (DAC; Brisaboa et al. (2009)) which allows for efficient storage of integers while providing efficient random access.",4.2 Efficient Precomputation,[0],[0]
"As the overwhelming majority of our precomputed values are small (see Figure 4 left), this gives rise to a dramatic compression rate of only ≈ 5.2 bits per integer.",4.2 Efficient Precomputation,[0],[0]
"The bit vector bv of sizeO(n) where n is the number of nodes in the suffix tree, is compressed using the scheme of Raman et al. (2002) which supports constant time rank operation over very large bit vectors.
",4.2 Efficient Precomputation,[0],[0]
"6The function N1PFRONTBACK1 is defined as Algorithm 5 in Shareghi et al. (2015).
",4.2 Efficient Precomputation,[0],[0]
This encoding allows for efficient retrieval of the precomputed counts at query time.,4.2 Efficient Precomputation,[0],[0]
"The compressed vectors are loaded into memory and when an expensive count is required for node v, the precomputed quantities can be fetched in constant time via LOOKUP(v, bv, i(x))",4.2 Efficient Precomputation,[0],[0]
"= i(x)RANK(bv,id(v),1).",4.2 Efficient Precomputation,[0],[0]
We use RANK to determine the number of 1s preceding v’s position in the bit vector bv.,4.2 Efficient Precomputation,[0],[0]
"This corresponds to v’s index in the compressed integer vectors i(x), from which its precomputed count can be fetched.",4.2 Efficient Precomputation,[0],[0]
"This strategy only applies for precomputed nodes; for other nodes, the values are computed on-the-fly.
",4.2 Efficient Precomputation,[0],[0]
"Figure 3 compares the query time breakdown for on-the-fly count computation (top) versus precomputation (bottom), for both KN and MKN and with different Markov orders, m. Note that query speed improves dramatically, by a factor of about 2500×, for precomputed cases.",4.2 Efficient Precomputation,[0],[0]
This improvement comes at a modest cost in construction space.,4.2 Efficient Precomputation,[0],[0]
Precomputing for CST nodes with m ≤ 10 resulted in 20% of the nodes being selected for precomputation.,4.2 Efficient Precomputation,[0],[0]
The space used by the precomputed values accounts for 20% of the total space usage (see Figure 4 right).,4.2 Efficient Precomputation,[0],[0]
Index construction time increased by 70%.,4.2 Efficient Precomputation,[0],[0]
"Having established a means of computing the requisite counts for MKN and an efficient precomputation strategy, we now turn to the algorithm for computing the language model probability.",4.3 Computing MKN Probability,[0],[0]
"This is presented in Algorithm 3, which is based on Shareghi et al. (2015)’s single CST approach for computing the KN probability (reported in their paper as Algorithm 4.)",4.3 Computing MKN Probability,[0],[0]
"Similar to their method, our approach implements the recursive m-gram probability formulation as an iterative loop (here using MKN).",4.3 Computing MKN Probability,[0],[0]
"The core of the algorithm are the two nodes vfull and v which correspond to nodes matching the full k-gram and its (k − 1)-gram context, respectively.
",4.3 Computing MKN Probability,[0],[0]
"Although similar to Shareghi et al. (2015)’s method, which also features a similar right-to-left pattern lookup, in addition we optimise the computation of a full sentence probability by sliding a window of widthm over the sequence from left-to-right, adding one new word at a time.7",4.3 Computing MKN Probability,[0],[0]
"This allows for the re-use of nodes in one window matching the full k-
7Pauls and Klein (2011) propose a similar algorithm for triebased LMs.
",4.3 Computing MKN Probability,[0],[0]
"Algorithm 3 MKN probability P ( wi|wi−1i−(m−1) )
1: function PROBMKN(t, wii−m+1,m, [vk] m−1 k=0 )",4.3 Computing MKN Probability,[0],[0]
2: Assumption: vk is the matching node for wi−1i−k 3: vfull0 ← root(t) .,4.3 Computing MKN Probability,[0],[0]
"tracks match for wii−k 4: p← 1/|σ| 5: for k ← 1 to m do 6: if vk−1 does not match then 7: break out of loop 8: vfullk ← back-search([lb(vfullk−1), rb(vfullk−1)], wi−k+1) 9:",4.3 Computing MKN Probability,[0],[0]
"Dk(1),Dk(2),Dk(3+)← discounts for k-grams 10: if k = m then 11: c← size(vfullk ) 12: d← size(vk−1) 13: N1,2,3+← N123PFRONT(t, vk−1, wi−1i−k+1, 0) 14: else 15: c← N1PBACK1(t, vfullk , wi−1i−k+1) 16: d← N1PFRONTBACK1(t, vk−1, wi−1i−k+1) 17: N1,2,3+← N123PFRONT(t, vk−1, wi−1i−k+1, 1) 18: if 1 ≤ c ≤ 2 then 19: c← c− Dk(c) 20: else 21: c← c− Dk(3+) 22: γ← Dk(1)N1 + Dk(2)N2 + Dk(3+)N3+ 23: p← 1
d (c+ γp)
24: return ( p, [ vfullk ]m−1 k=0 )
grams, vfull, as the nodes matching the context in the subsequent window, denoted v.
For example, in the sentence “The Force is strong with this one.”, computing the 4-gram probability of “The Force is strong” requires matches into the CST for “strong”, “is strong”, etc.",4.3 Computing MKN Probability,[0],[0]
"As illustrated in Table 1, for the next 4-gram resulting from sliding the window to include “with”, the denominator terms require exactly these nodes, see Figure 5.",4.3 Computing MKN Probability,[0],[0]
"Practically, this is achieved by storing the matching vfull nodes computed in line 8, and passing this vector as the input argument [vk] m−1 k=0 to the next call to PROBMKN (line 1).",4.3 Computing MKN Probability,[0],[0]
"This saves half the calls to backward-search, which, as shown in Figure 3, represent a significant fraction of the querying cost, resulting in a 30% improvement in query runtime.
",4.3 Computing MKN Probability,[0],[0]
"The algorithm starts by considering the unigram probability, and grows the context to its left by one word at a time until the m-gram is fully covered (line 5).",4.3 Computing MKN Probability,[0],[0]
"This best suits the use of backward-search in a CST, which proceeds from right-to-left over the search pattern.",4.3 Computing MKN Probability,[0],[0]
"At each stage the search for vfullk uses the span from the previous match, v full k−1,
along with the BWT to efficiently locate the matching node.",4.3 Computing MKN Probability,[0],[0]
"Once the nodes matching the full sequence and its context are retrieved, the procedure is fairly straightforward: the discounts are loaded on line 9 and applied in lines 18-21, while the numerator, denominator and smoothing quantities as required for computing P and P̄ are calculated in lines 10-13 and 15-17, respectively.8 Note that the calls for functions N123PFRONT, N1PBACK1, and N1PFRONTBACK1 are avoided if the corresponding node is amongst the selected nodes in the precomputation step; instead the LOOKUP function is called.",4.3 Computing MKN Probability,[0],[0]
"Finally, the smoothing weight γ is computed in line 22 and the conditional probability computed on line 23.",4.3 Computing MKN Probability,[0],[0]
The loop terminates when we reach the length limit k = m,4.3 Computing MKN Probability,[0],[0]
"or we cannot match the context, i.e., wi−1i−k is not in the training corpus, in which case the probability value p for the longest match is returned.
",4.3 Computing MKN Probability,[0],[0]
"We now turn to the discount parameters, Dk(j) , k ≤ m, j ∈ 1, 2, 3+, which are functions of the corpus statistics as outlined in Figure 1.",4.3 Computing MKN Probability,[0],[0]
"While these could be computed based on raw m-gram statistics, this approach is very inefficient for large m ≥ 5; instead these values can be computed efficiently from the compressed data structures.",4.3 Computing MKN Probability,[0],[0]
Algorithm 4 outlines how the Dk(i) values can be computed directly from the CST.,4.3 Computing MKN Probability,[0],[0]
"This method iterates
8N1PBACK1 and N1PFRONTBACK1 are defined in Shareghi et al. (2015); see also §3 for an overview.
",4.3 Computing MKN Probability,[0],[0]
"Algorithm 4 Compute discounts 1: function COMPUTEDISCOUNTS(t, m̄, bv′, SA ) 2: ni(k)← 0, n̄i(k)← 0",4.3 Computing MKN Probability,[0],[0]
∀i ∈,4.3 Computing MKN Probability,[0],[0]
"[1, 4], k ∈",4.3 Computing MKN Probability,[0],[0]
"[1, m̄] 3: N1+(··)← 0 4: for v ← descendants(root(t)) do .",4.3 Computing MKN Probability,[0],[0]
"DFS 5: dP ← string-depth(parent(v)) 6: d← string-depth(v) 7: dS ← depth-next-sentinel(SA, bv′, lb(v)) 8: i← size(v) .",4.3 Computing MKN Probability,[0],[0]
"frequency 9: c← interval-symbols(t, [lb(v), rb(v)]) .",4.3 Computing MKN Probability,[0],[0]
"left occ. 10: for k ← dP + 1 to min (d, m̄, dS − 1) do 11: if k = 2 then 12: N1+(··)← N1+(··) + 1 13: if 1 ≤",4.3 Computing MKN Probability,[0],[0]
"i ≤ 4 then 14: ni(k)← ni(k) + 1 15: if 1 ≤ c ≤ 4 then 16: n̄c(k)← n̄c(k) + 1 17: Dk(i)← computed using formula in Figure 1 18: return Dk(i), k ∈",4.3 Computing MKN Probability,[0],[0]
"[1, m̄], i ∈ {1, 2, 3+}
over the nodes in the suffix tree, and for each node considers the k-grams encoded in the edge label, where each k-gram is taken to start at the root node (to avoid duplicate counting, we consider k-grams only contained on the given edge but not in the parent edges, i.e., by bounding k based on the string depth of the parent and current nodes, dP ≤",4.3 Computing MKN Probability,[0],[0]
k ≤ d).,4.3 Computing MKN Probability,[0],[0]
"For each k-gram we record its count, i (line 8), and the number of unique symbols to the left, c (line 9), which are accumulated in an array for each kgram size for values between 1 and 4 (lines 13-14 and 15-16, respectively).",4.3 Computing MKN Probability,[0],[0]
"We also record the number of unique bigrams by incrementing a counter during the traversal (lines 11-12).
",4.3 Computing MKN Probability,[0],[0]
"Special care is required to exclude edge labels that span sentence boundaries, by detecting special sentinel symbols (line 8) that separate each sentence or conclude the corpus.",4.3 Computing MKN Probability,[0],[0]
"This check could be done by repeatedly calling edge(v, k) to find the kth symbol on the given edge to check for sentinels, however this is a slow operation as it requires multiple backward search calls.",4.3 Computing MKN Probability,[0],[0]
"Instead we precalculate a bit vector, bv′, of size equal to the number of tokens in the corpus, n, in which sentinel locations in the text are marked by 1 bits.",4.3 Computing MKN Probability,[0],[0]
"Coupled with this, we use the suffix array SA, such that
depth-next-sentinel(SA, bv′, `) =
SELECT(bv′,RANK(bv′, SA`, 1) + 1, 1)− SA` ,
where SA` returns the offset into the text for index `, and the SA is stored uncompressed to avoid the expensive cost of recovering these values.9",4.3 Computing MKN Probability,[0],[0]
"This function can be understood as finding the first occurrence of the pattern in the text (using SA`) then finding the location of the next 1 in the bit vector, using constant time RANK and SELECT operations.",4.3 Computing MKN Probability,[0],[0]
"This locates the next sentinel in the text, after which it computes the distance to the start of the pattern.",4.3 Computing MKN Probability,[0],[0]
"Using this method in place of explicit edge calls improved the training runtime substantially up to 41×.
",4.3 Computing MKN Probability,[0],[0]
We precompute the discount values for k ≤ m̄grams.,4.3 Computing MKN Probability,[0],[0]
For querying with m > m̄,4.3 Computing MKN Probability,[0],[0]
(including∞) we reuse the discounts for the largest m̄-grams.10,4.3 Computing MKN Probability,[0],[0]
"To evaluate our approach we measure memory and time usage, along with the predictive perplexity score of word-level LMs on a number of different corpora varying in size and domain.",5 Experiments,[0],[0]
"For all of our word-level LMs, we use m̄, m̂ ≤ 10.",5 Experiments,[0],[0]
"We also demonstrate the positive impact of increasing the set limit on m̄, m̂ from 10 to 50 on improving characterlevel LM perplexity.",5 Experiments,[0],[0]
"The SDSL library (Gog et al., 2014) is used to implement our data structures.",5 Experiments,[0],[0]
"The benchmarking experiments were run on a single core of a Intel Xeon E5-2687 v3 3.10GHz server with 500GiB of RAM.
",5 Experiments,[0],[0]
"In our word-level experiments, we use the German subset of the Europarl (Koehn, 2005) as a small corpus, which is 382 MiB in size measuring the raw uncompressed text.",5 Experiments,[0],[0]
"We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus (Buck et al., 2014).",5 Experiments,[0],[0]
"As test sets, we used newstest-2014 for all languages except Spanish, for which we used newstest-2013.11",5 Experiments,[0],[0]
"In our
9Although the SA can be very large, we need not store it in memory.",5 Experiments,[0],[0]
The DFS traversal in Algorithm 4 (lines 4–16) means that the calls to SA` occur in increasing order of `.,5 Experiments,[0],[0]
"Hence, we use on-disk storage for the SA with a small memory mapped buffer, thereby incurring a negligible memory overhead.
10It is possible to compute the discounts for all patterns of the text using our algorithm with complexity linear in the length of the text.",5 Experiments,[0],[0]
"However, the discounts appear to converge by pattern length m̄ = 10.",5 Experiments,[0],[0]
"This limit also helps to avoid problems of wild fluctuations in discounts for very long patterns arising from noise for low count events.
",5 Experiments,[0],[0]
"11http://www.statmt.org/wmt{13,14}/test.tgz
benchmarking experiments we used the bottom 1M sentences (not used in training) of the German Comman Crawl corpus.",5 Experiments,[0],[0]
"We used the preprocessing script of Buck et al. (2014), then removed sentences with ≤ 2 words, and replaced rare words12 c ≤ 9 in the training data with a special token.",5 Experiments,[0],[0]
"In our characterlevel experiments, we used the training and test data of the benchmark 1-billion-words corpus (Chelba et al., 2013).
",5 Experiments,[0],[0]
Small data:,5 Experiments,[0],[0]
"German Europarl First, we compare the time and memory consumption of both the SRILM and KenLM toolkits, and the CST on the small German corpus.",5 Experiments,[0],[0]
"Figure 6 shows the memory usage for construction and querying for CST-based methods w/o precomputation is independent of m, but becomes substantially with m for the SRILM and KenLM benchmarks.",5 Experiments,[0],[0]
"To make our results comparable to those reported in (Shareghi et al., 2015) for query time measurements we reported the loading and query time combined.",5 Experiments,[0],[0]
"The construction cost is modest, requiring less memory than the benchmark systems for m ≥ 3, and running in a similar time13 (despite our method supporting queries
12Running with the full vocabulary increased the memory requirement by 40% for construction and 5% for querying with our model, and 10% and 30%, resp.",5 Experiments,[0],[0]
for KenLM.,5 Experiments,[0],[0]
"Construction times for both approaches were 15% slower, but query runtime was 20% slower for our model versus 80% for KenLM.
13",5 Experiments,[0],[0]
"For all timings reported in the paper we manually flushed the system cache between each operation (both for construction
of unlimited size).",5 Experiments,[0],[0]
"Precomputation adds to the construction time, which rose from 173 to 299 seconds, but yielded speed improvements of several orders of magnitude for querying (218k to 98 seconds for 10- gram).",5 Experiments,[0],[0]
"In querying, the CST-precompute method is 2-4× slower than both SRILM and KenLM for large m ≥ 5, with the exception of m = 10 where it outperforms SRILM.",5 Experiments,[0],[0]
"A substantial fraction of the query time is loading the structures from disk; when this cost is excluded, our approach is between 8-13× slower than the benchmark toolkits.",5 Experiments,[0],[0]
"Note that perplexity computed by the CST closely matched KenLM (differences ≤ 0.1).
",5 Experiments,[0],[0]
"Big Data: Common Crawl Table 2 reports the perplexity results for training on 32GiB subsets of the English, Spanish, French, and German Common Crawl corpus.",5 Experiments,[0],[0]
"Note that with such large datasets, perplexity improves with increasing m, with substantial gains available moving above the widely used m = 5.",5 Experiments,[0],[0]
"This highlights the importance of our approach being independent from m, in that we can evaluate for any m, including∞, at low cost.
",5 Experiments,[0],[0]
"Heterogeneous Data To illustrate the effects of domain shift, corpus size and language model capacity on modelling accuracy, we now evaluate the system using a variety of different training corpora.",5 Experiments,[0],[0]
Table 3 reports the perplexity for German when training over datasets ranging from the small Europarl up to 32GiB of the Common Crawl corpus.,5 Experiments,[0],[0]
"Note that the test set is from the same domain as the News
and querying) to remove the effect of caching on runtime.",5 Experiments,[0],[0]
"To query KenLM, we used the speed optimised populate method.",5 Experiments,[0],[0]
(We also compare the memory optimised lazy method in Figure 7.),5 Experiments,[0],[0]
"To train and query SRILM we used the default method which is optimised for speed, but had slightly worse memory usage than the compact method.
",5 Experiments,[0],[0]
"Crawl, which explains the vast difference in perplexities.",5 Experiments,[0],[0]
"The domain effect is strong enough to eliminate the impact of using much larger corpora, compare 10-gram perplexities for training on the smaller News Crawl 2007 corpus versus Europarl.",5 Experiments,[0],[0]
However ‘big data’ is still useful: in all cases the perplexity improves as we provide more data from the same source.,5 Experiments,[0],[0]
"Moreover, the magnitude of the gain in perplexity when increasing m is influenced by the data size: with more training data higher order m-grams provide richer models; therefore, the scalability of our method to large datasets is crucially important.
",5 Experiments,[0],[0]
"Benchmarking against KenLM Next we compare our model against the state-of-the-art method, KenLM trie.",5 Experiments,[0],[0]
"The perplexity difference between CST and KenLM was less than 0.003 in all experiments.
",5 Experiments,[0],[0]
Construction Cost.,5 Experiments,[0],[0]
Figure 7a compares the peak memory usage of our CST models and KenLM.,5 Experiments,[0],[0]
"KenLM is given a target memory usage of the peak usage of our CST models.14 The construction phase for the CST required more time for lower order models (see Figure 7c) but was comparable for larger
14Using the memory budget option, -S. Note that KenLM often used more memory than specified.",5 Experiments,[0],[0]
"Allowing KenLM use of 80% of the available RAM reduced training time by a factor of between 2 and 4.
m, roughly matching KenLM for m = 10.15 For the 32GiB dataset, the CST model took 14 hours to build, compared to KenLM’s 13.5 and 4 hours for the 10-gram and 5-gram models, respectively.
",5 Experiments,[0],[0]
Query Cost.,5 Experiments,[0],[0]
"As shown in Figure 7b, the memory requirements for querying with the CST method were consistently lower than KenLM for m ≥ 4: for m = 10 the memory consumption of KenLM was 277GiB compared to our 27GiB, a 10× improvement.",5 Experiments,[0],[0]
This closely matches the file sizes of the stored models on disk.,5 Experiments,[0],[0]
"Figure 7d reports the query runtimes, showing that KenLM becomes substantially slower with increasing dataset size and increasing language model order.",5 Experiments,[0],[0]
"In contrast, the runtime of our CST approach is much less affected by data size or model order.",5 Experiments,[0],[0]
"Our approach is faster than KenLM with the memory optimised lazy option for m ≥ 3, often by several orders of magnitude.",5 Experiments,[0],[0]
"For the faster KenLM populate, our model is still highly competitive, growing to 4× faster for the largest data size.16 The loading time is still a significant part of the runtime; without this cost, our model is 5× slower than KenLM populate for m = 10 on the largest dataset.",5 Experiments,[0],[0]
"Running our model with m =∞ on the largest data size did not change the memory usage and only had a minor effect on runtime, taking 645s.
",5 Experiments,[0],[0]
"Character-level modelling To demonstrate the full potential of our approach, we now consider character based language modelling, evaluated on the large benchmark 1-billion-words language modelling corpus, a 3.9GiB (training) dataset with 768M words and 4 billion characters.17 Table 4 shows the test perplexity results for our models, using the full training vocabulary.",5 Experiments,[0],[0]
"Note that perplexity improves with m for the character based model, but plateaus at m = 10 for the word based model; one reason for this is the limited discount computation, m̄ ≤ 10,
15The CST method uses a single thread for construction, while KenLM uses several threads.",5 Experiments,[0],[0]
"Most stages of construction for our method could be easily parallelised.
",5 Experiments,[0],[0]
"16KenLM benefits significantly from caching which can occur between runs or as more queries are issued (from m-gram repetition in our large 1 million sentence test set), whereas the CST approach does not benefit noticeably (as it does not incorporate any caching functionality).
",5 Experiments,[0],[0]
"17http://www.statmt.org/lm-benchmark/
for the word model, which may not be a good parameterisation for m > m̄.
Despite the character based model (implicitly) having a massive parameter space, estimating this model was tractable with our approach: the construction time was a modest 5 hours (and 2.3 hours for the word based model.)",5 Experiments,[0],[0]
"For the same dataset, Chelba et al. (2013) report that training a MKN 5- gram model took 3 hours using a cluster of 100 CPUs; our algorithm is faster than this, despite only using a single CPU core.18 Queries were also fast: 0.72-0.87ms and 15ms per sentence for word and character based models, respectively.",5 Experiments,[0],[0]
"We proposed a language model based on compressed suffix trees, a representation that is highly
18Chelba et al. (2013) report a better perplexity of 67.6, but they pruned the training vocabulary, whereas we did not.",6 Conclusions,[0],[0]
"Also we use a stringent treatment of OOV, following Heafield (2013).
compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on the fly.",6 Conclusions,[0],[0]
"We presented several optimisations to accelerate this process, with only a modest increase in construction time and memory usage, yet improving query runtimes up to 2500×.",6 Conclusions,[0],[0]
"In benchmarking against the state-of-the-art KenLM package on large corpora, our method has superior memory usage and highly competitive runtimes for both querying and training.",6 Conclusions,[0],[0]
"Our approach allows easy experimentation with high order language models, and our results provide evidence that such high orders are most useful when using large training sets.
",6 Conclusions,[0],[0]
"We posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011).",6 Conclusions,[0],[0]
"Our ongoing work will explore this avenue, as well as integrating our language model into the Moses machine translation system, and improving the querying time by caching the lower order probabilities (e.g., m < 4) which we believe can improve query time substantially while maintaining a modest memory footprint.",6 Conclusions,[0],[0]
"This research was supported by the Australian Research Council (FT130101105), National ICT Australia (NICTA) and a Google Faculty Research Award.",Acknowledgements,[0],[0]
Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora.,abstractText,[0],[0]
"We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly.",abstractText,[0],[0]
"We present several optimisations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage.",abstractText,[0],[0]
"For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package.",abstractText,[0],[0]
"It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying).",abstractText,[0],[0]
"Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees",title,[0],[0]
"Determinantal point processes (DPPs) are elegant probabilistic models, first introduced by (Macchi, 1975), who called them ‘fermion processes’.",1. Introduction,[0],[0]
"Since then, DPPs have been extensively studied in the fields of quantum physics and random matrices (Johansson, 2006), giving rise to a beautiful theory (Daley & Vere-Jones, 2007).",1. Introduction,[0],[0]
"The characteristic of DPPs is repulsive behavior, which makes them useful for modeling diversity.
",1. Introduction,[0],[0]
"Recently, they have been applied in many machine learning tasks such as summarization (Gong et al., 2014), human
1School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea.",1. Introduction,[0],[0]
"2Bloomberg LP, 731 Lexington Avenue, New York, NY, 10069.",1. Introduction,[0],[0]
"Correspondence to: Jinwoo Shin <jinsoos@kaist.ac.kr>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"pose detection (Kulesza et al., 2012), clustering (Kang, 2013) and tweet time-line generation (Yao et al., 2016).",1. Introduction,[0],[0]
"In particular, their computational advantage compared to other probabilistic models is that many important inference tasks are computationally tractable.",1. Introduction,[0],[0]
"For example, conditioning, sampling (Kang, 2013) and marginalization of DPPs admit polynomial-time/efficient algorithms, while those on popular graphical models (Jordan, 1998) do not, i.e., they are NP-hard.",1. Introduction,[0],[0]
"One exception is the MAP inference (finding the most likely configuration), which is our main interest; the MAP computation is known to be NPhard even for DPPs (Kulesza et al., 2012).
",1. Introduction,[0],[0]
"The distribution of diverse sets under DPPs is characterized by determinants of submatrices formed by their features, and the corresponding MAP inference reduces to finding a submatrix that maximizes its determinant.",1. Introduction,[0],[0]
"It is well known that the matrix log-determinant is a submodular function; that is, the MAP inference of DPPs is a special instance of submodular maximization (Kulesza et al., 2012).",1. Introduction,[0],[0]
"Greedy algorithms have been shown to have the best worst-case approximation guarantees for many instances of submodular maximization; for example, (1 − 1/e)-approximation for monotone functions.",1. Introduction,[0],[0]
"Furthermore, it has been often empirically observed that greedy algorithms provide near optimal solutions (Krause et al., 2008).",1. Introduction,[0],[0]
"Hence, greedy algorithms have been also applied for the DPP task (Kulesza et al., 2012; Yao et al., 2016; Zhang & Ou, 2016).",1. Introduction,[0],[0]
"Known implementations of greedy selection on DPP require computation of log-determinants, matrix inversions (Kulesza et al., 2012) or solving linear systems (Li et al., 2016b).",1. Introduction,[0],[0]
"Consequently, they run in O(d4) time where d is the total number of items (see Section 2.3).",1. Introduction,[0],[0]
"In this paper, we propose faster greedy implementations that run in O(d3) time.
Contribution.",1. Introduction,[0],[0]
Our high-level idea is to amortize greedy operations by utilizing log-determinant approximation schemes.,1. Introduction,[0],[0]
A greedy selection requires computation of marginal gains of log-determinants; we consider their firstorder (linear) approximations.,1. Introduction,[0],[0]
"We observe that the computation of multiple marginal gains can be amortized into a single run of a linear solver, in addition to multiple vector inner products.",1. Introduction,[0],[0]
"We choose the popular conjugate gradient descent (CG) (Saad, 2003) as a linear solver.",1. Introduction,[0],[0]
"In addition, for improving the quality of first-order approximations, we partition remaining items into p ≥ 1 sets (via some cluster-
ing algorithm), and apply the first-order approximations in each partition.",1. Introduction,[0],[0]
"The resulting approximate computation of multiple marginal gains at each greedy selection requires 2p runs of CG under the Schur complement, and the overall running time of the proposed greedy algorithm becomes O(d3) under the choice of p = O(1) (see Section 3).
",1. Introduction,[0],[0]
"Next, for larger-scale DPPs, we develop an even faster greedy algorithm using a batch strategy.",1. Introduction,[0],[0]
"In addition to using the first-order approximations of log-determinants under a partitioning scheme, we add k > 1 elements instead of a single element to the current set, where we sample some candidates among all possible k elements to relax the expensive cost of computing all marginal gains.",1. Introduction,[0],[0]
"Intuitively, the random batch selection makes the algorithm k times faster, while potentially hurting the approximation quality.",1. Introduction,[0],[0]
"Now, we suggest running the recent fast log-determinant approximation scheme (LDAS) (Han et al., 2015)",1. Introduction,[0],[0]
"p times, instead of running CG pk times under the Schur complement, where LDAS utilizes high-order, i.e., polynomial, approximations to the scalar log function with stochastic trace estimators.",1. Introduction,[0],[0]
"Since the complexities of running LDAS and CG are comparable, running the former p times is faster than running the latter pk times if k > 1.
",1. Introduction,[0],[0]
"Finally, we discovered a novel scheme for boosting the approximation quality by sharing random vectors among many runs of LDAS, and also establish theoretical justification why this helps.",1. Introduction,[0],[0]
"Our experiments on both synthetic and real-world dataset show that the proposed algorithms are significantly faster than competitors for large-scale instances, while losing marginal approximation ratio.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"To the best of our knowledge, this is the first work that aims for developing faster greedy algorithms specialized for the MAP inference of DPP, while there has been several efforts on those for general submodular maximization.",1. Introduction,[0],[0]
"An accelerated greedy algorithm, called lazy evaluation, was first proposed by (Minoux, 1978) which maintains the upper bounds on the marginal gains instead of recomputing exact values.",1. Introduction,[0],[0]
"In each iteration, only elements with the maximal bound compute the exact gain, which still bounds on the exact value due to submodularity.",1. Introduction,[0],[0]
"For the DPP case, we also observe that the lazy algorithm is significantly faster than the standard greedy one, while the outputs of both are equal.",1. Introduction,[0],[0]
"Hence, we compare our algorithms with the lazy one (see Section 5).
",1. Introduction,[0],[0]
Another natural approach is on stochastic greedy selections computing marginal gains of randomly selected elements.,1. Introduction,[0],[0]
"Its worst-case approximation guarantee was also studied (Mirzasoleiman et al., 2015), under the standard, non-batch, greedy algorithm.",1. Introduction,[0],[0]
"The idea of stochastic selections can be also applied to our algorithms, where we indeed apply it for designing our faster batch greedy algorithm as mentioned earlier.",1. Introduction,[0],[0]
"Recently, (Buchbinder et al.,
2015) proposed a ‘one-pass’ greedy algorithm where each greedy selection requires computing only a single marginal gain, i.e., the number of marginal gains necessary to compute can be significantly reduced.",1. Introduction,[0],[0]
"However, this algorithm is attractive only for the case when evaluating a marginal gain does not increase with respect to the size of the current set, which does not hold for the DPP case.",1. Introduction,[0],[0]
"As reported in Section 5, it performs significantly worse than ours in both their approximation qualities and running times.
",1. Introduction,[0],[0]
"There have been also several efforts to design parallel/distributed implementations of greedy algorithms: (Pan et al., 2014) use parallel strategies for the above one-pass greedy algorithm and (Kumar et al., 2015) adapt a MapReduce paradigm for implementing greedy algorithms in distributed settings.",1. Introduction,[0],[0]
"One can also parallelize our algorithms easily since they require independent runs of matrix-vector (or vector inner) products, but we do not explore this aspect in this paper.",1. Introduction,[0],[0]
"Finally, we remark that a non-greedy algorithm was studied in (Gillenwater et al., 2012) for better MAP qualities of DPP, but it is much slower than ours as reported in Section 5.",1. Introduction,[0],[0]
We start by defining a necessary notation.,2. Preliminaries,[0],[0]
Our algorithms for determinantal point processes (DPPs) select elements from the ground set of d items Y =,2. Preliminaries,[0],[0]
"[d] := {1, 2, . . .",2. Preliminaries,[0],[0]
", d} and denote the set of all subsets of Y by 2Y .",2. Preliminaries,[0],[0]
"For any positive semidefinite matrix L ∈ Rd×d, we denote λmin and λmax to be the smallest and the largest eigenvalues of L. Given subset X,Y ⊆ Y , we use LX,Y to denote the submatrix of L obtained by entries in rows and columns indexed by X and Y , respectively.",2. Preliminaries,[0],[0]
"For notational simplicity, we let LX,X = LX and LX,{i} = LX,i for i ∈ Y .",2. Preliminaries,[0],[0]
"In addition, LX is defined as the average of LX∪{i} for i ∈",2. Preliminaries,[0],[0]
Y,2. Preliminaries,[0],[0]
\ X .,2. Preliminaries,[0],[0]
"Finally, 〈·, ·〉 means the matrix/vector inner product or element-wise product sum.
",2. Preliminaries,[0],[0]
"In Section 2.1, we introduce the maximum a posteriori (MAP) inference of DPP, then the standard greedy optimization scheme and its naı̈ve implementations are described in Section 2.2 and Section 2.3, respectively.",2. Preliminaries,[0],[0]
DPPs are probabilistic models for subset selection of a finite ground set Y =,2.1. Determinantal Point Processes,[0],[0]
[d] that captures both quality and diversity.,2.1. Determinantal Point Processes,[0],[0]
"Formally, it defines the following distribution on 2Y : for random variable X ⊆ Y drawn from given DPP, we have
Pr [X = X]",2.1. Determinantal Point Processes,[0],[0]
"∝ det (LX) ,
where L ∈ Rd×d is a positive definite matrix called an L-ensemble kernel.",2.1. Determinantal Point Processes,[0],[0]
"Under the distribution, several probabilistic inference tasks are required for real-world applica-
tions, including MAP (Gong et al., 2014; Gillenwater et al., 2012; Yao et al., 2016), sampling (Kathuria & Deshpande, 2016; Kang, 2013; Li et al., 2016a), marginalization and conditioning (Gong et al., 2014).",2.1. Determinantal Point Processes,[0],[0]
"In particular, we are interested in the MAP inference, i.e., finding the most diverse subset Y of Y that achieves the highest probability, i.e., arg maxY⊆Y det(LY ), possibly under some constraints on Y .",2.1. Determinantal Point Processes,[0],[0]
"Unlike other inference tasks on DPP, it is known that MAP is a NP-hard problem (Kulesza et al., 2012).",2.1. Determinantal Point Processes,[0],[0]
"A set function f : 2Y → R is submodular if its marginal gains are decreasing, i.e.,
f(X ∪ {i})− f(X) ≥ f(Y ∪ {i})− f(Y ),
for every X ⊆ Y ⊂ Y and every",2.2. Greedy Submodular Maximization,[0],[0]
i ∈,2.2. Greedy Submodular Maximization,[0],[0]
Y,2.2. Greedy Submodular Maximization,[0],[0]
\ Y .,2.2. Greedy Submodular Maximization,[0],[0]
We say f is monotone if f(X) ≤ f(Y ) for every X ⊆ Y .,2.2. Greedy Submodular Maximization,[0],[0]
"It is well known that DPP has the submodular structure, i.e., f = log det is submodular.
",2.2. Greedy Submodular Maximization,[0],[0]
"The submodular maximization task is to find a subset maximizing a submodular function f , which corresponds to the MAP inference task in the DPP case.",2.2. Greedy Submodular Maximization,[0],[0]
"Hence, it is NP-hard and a popular approximate scheme is the following greedy procedure (Nemhauser et al., 1978): initially, X ← ∅ and iteratively update X ← X ∪ {imax} for
imax = argmax i∈Y\X
f(X ∪ {i})− f(X), (1)
as long as f(X ∪ {imax}) > f(X).",2.2. Greedy Submodular Maximization,[0],[0]
"For the monotone case, it guarantees (1 − 1/e)-approximation (Nemhauser et al., 1978).",2.2. Greedy Submodular Maximization,[0],[0]
"Under some modifications of the standard greedy procedure, 2/5-approximation can be guaranteed even for non-monotone functions (Feige et al., 2011).",2.2. Greedy Submodular Maximization,[0],[0]
"Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice (Krause et al., 2008; Sharma et al., 2015; Yao et al., 2016; Zhang & Ou, 2016).",2.2. Greedy Submodular Maximization,[0],[0]
"Log-determinant or related computations, which are at the heart of greedy algorithms for MAP inference of DPPs, are critical to compute the marginal gain log detLX∪{i} − log detLX .",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Since the exact computations of logdeterminants might be slow, i.e., requires O(d3) time for d-dimensional matrices, we introduce recent efficient logdeterminant approximation schemes (LDAS).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"The logdeterminant of a symmetric positive definite matrix A can be approximated by combining (a) Chebyshev polynomial expansion of scalar log function and (b) matrix trace estimators via Monte Carlo methods:
log detA = tr (logA) (a) ≈ tr (pn(A))",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
(b),2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"≈ 1 m m∑ t=1 v(t)>pn(A)v (t).
",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Here, pn(x) is a polynomial expansion of degree n approximating log x and v(1), . . .",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
",v(m) are random vectors used for estimating the trace of pn(A).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Several polynomial expansions, including Taylor (Boutsidis et al., 2015), Chebyshev (Han et al., 2015) and Legendre (Peng & Wang, 2015) have been studied.",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"For trace estimation, several random vectors have been also studied (Avron & Toledo, 2011), e.g., the Hutchinson method (Hutchinson, 1990) chooses elements of v as i.i.d. random numbers in {−1,+1} so that E",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
[ v>Av ] = tr (A).,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"In this paper, we use LDAS using the Chebyshev polynomial and Hutchinson method (Han et al., 2015), but one can use other alternatives as well.
",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
Log-determinant Approximation Scheme (LDAS),2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"(Han et al., 2015)
",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Input: symmetric matrix A ∈ Rd×d with eigenvalues in [δ, 1− δ], sampling number m and polynomial degree n",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
Initialize:,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Γ← 0 cj ← j-th coefficient of Chebyshev expansion of log x on [δ, 1− δ] for 0 ≤ j ≤ n. for i = 1 to m do
Draw a random vector v(i) ∈ {−1,+1}d",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
whose entries are uniformly distributed.,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"w
(i) 0",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
← v(i) and w (i) 1 ← 21−2δAv,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
(i),2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"− 11−2δv (i)
u← c0w(i)0 + c1w (i) 1 for j = 2 to n",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"do w
(i) 2",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
← 41−2δAw (i) 1 − 21−2δw (i) 1,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"−w (i) 0
u← u + cj w(i)2",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"w
(i) 0",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
← w (i) 1 and w (i) 1 ← w,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"(i) 2
end for Γ←",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Γ + v(i)>u/m
end for Output: Γ
Observe that LDAS only requires matrix-vector multiplications and its running time is Θ ( d2 )
for constants m,n = O(1).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
One can directly use LDAS for computing (1) and the resulting greedy algorithm runs in Θ(d · T 3GR) time where the number of greedy updates on the current set X is TGR.,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Since TGR = O(d), the complexity is simply O(d4).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"An alternative way to achieve the same complexity is to use the Schur complement (Ouellette, 1981):
log detLX∪{i} − log detLX = log ( Li,i − Li,XL−1X LX,i ) .
",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"(2)
This requires a linear solver to compute L−1X LX,i; conjugate gradient descent (CG) (Greenbaum, 1997) is a popular choice in practice.",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Hence, if one applies CG to compute the max-marginal gain (1), the resulting greedy algorithm runs in Θ(d · T 3GR · TCG) time, where TCG denotes the number of iterations of each CG run.",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"In the worst case, CG converges to the exact solution when TCG grows with the matrix dimension, but for practical purposes, it typically provides a
very accurate solution in few iterations, i.e., TCG = O(1).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Recently, Gauss quadrature via Lanczos iteration is used for efficient computing of Li,XL−1X LX,i (Li et al., 2016b).",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"Although it guarantees rigorous upper/lower bounds, CG is faster and accurate enough for most practical purposes.
",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"In summary, the greedy MAP inference of DPP can be implemented efficiently via LDAS or CG.",2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
The faster implementations proposed in this paper smartly employ both of them as key components utilizing their complementary benefits.,2.3. Naı̈ve Implementations of Greedy Algorithm,[0],[0]
"In this section, we provide a faster greedy submodular maximization scheme for the MAP inference of DPP.",3. Faster Greedy DPP Inference,[0],[0]
"We explain our key ideas in Section 3.1 and then, provide the formal algorithm description in Section 3.2.",3. Faster Greedy DPP Inference,[0],[0]
First-order approximation of log-determinant.,3.1. Key Ideas,[0],[0]
The main computational bottleneck of a greedy algorithm is to evaluate the marginal gain (1) for every element not in the current set.,3.1. Key Ideas,[0],[0]
"To reduce the time complexity, we consider the following first-order, i.e., linear, approximation of logdeterminant as:1
argmax i∈Y\X
log detLX∪{i} − log detLX
= argmax i∈Y\X
log detLX∪{i} − log detLX
",3.1. Key Ideas,[0],[0]
"≈ argmax i∈Y\X
〈 L −1 X , LX∪{i}",3.1. Key Ideas,[0],[0]
"− LX 〉 , (3)
where we recall that LX is the average of LX∪{i}.",3.1. Key Ideas,[0],[0]
"Observe that computing (3) requires the vector inner product of a single column (or row) of L −1 X and LX∪{i} − LX because LX∪{i} and LX share almost all entries except a single row and a column.
",3.1. Key Ideas,[0],[0]
"To obtain a single column of L −1 X , one can solve a linear system using the CG algorithm.",3.1. Key Ideas,[0],[0]
"More importantly, it suffices to run CG once for computing (3), while the naı̈ve greedy implementation in Section 2.3 has to run CG |Y \X| times.",3.1. Key Ideas,[0],[0]
"As we mentioned earlier, after obtaining the single column of L −1 X using CG, one has to perform |Y \X| vector inner products in (3), but it is much cheaper than |Y \ X|",3.1. Key Ideas,[0],[0]
"CG runs requiring matrix-vector multiplications.
",3.1. Key Ideas,[0],[0]
Partitioning.,3.1. Key Ideas,[0],[0]
"In order to further improve the quality of first-order approximation (3), we partition Y \ X into p distinct subsets so that
‖LX∪{i} − LX‖F ‖LX∪{i} − L (j) X ‖F ,
where an element i is in the partition j ∈",3.1. Key Ideas,[0],[0]
"[p], L(j)X is the 1 ∇X log detX = ( X−1 )>
average of LX∪{i} for i in the partition j, and ‖·‖F is the Frobenius norm.",3.1. Key Ideas,[0],[0]
"Since LX∪{i} becomes closer to the average L (j)
X , one can expect that the first-order approximation quality in (3) is improved.",3.1. Key Ideas,[0],[0]
"But, we now need a more expensive procedure to approximate the marginal gain:
log detLX∪{i} − log detLX = ( log detLX∪{i} − log detL (j) X ) +",3.1. Key Ideas,[0],[0]
( log detL (j) X,3.1. Key Ideas,[0],[0]
"− log detLX )
",3.1. Key Ideas,[0],[0]
"≈ 〈( L (j)
X )−1 , LX∪{i} − L (j) X 〉 ︸ ︷︷ ︸
(a)
+",3.1. Key Ideas,[0],[0]
( log detL (j) X,3.1. Key Ideas,[0],[0]
"− log detLX )
︸ ︷︷ ︸ (b) .
",3.1. Key Ideas,[0],[0]
"The first term (a) can be computed efficiently as we explained earlier, but we have to run CG p times for computing single columns of L (1)
X , . . .",3.1. Key Ideas,[0],[0]
", L (p)
X .",3.1. Key Ideas,[0],[0]
The second term (b) can be also computed using CG similarly to (2) under the Schur complement.,3.1. Key Ideas,[0],[0]
"Hence, one has to run CG 2p times in total.",3.1. Key Ideas,[0],[0]
"If p is large, the overall complexity becomes larger, but the approximation quality improves as well.",3.1. Key Ideas,[0],[0]
"We also note that one can try various clustering algorithms, e.g., k-means or Gaussian mixture.",3.1. Key Ideas,[0],[0]
"Instead, we use a simple random partitioning scheme because it is not only the fastest method but it also works well in our experiments.",3.1. Key Ideas,[0],[0]
"The formal description of the proposed algorithm is described in Algorithm 1.
",3.2. Algorithm Description and Guarantee,[0],[0]
"Algorithm 1 Faster Greedy DPP Inference
1: Input: kernel matrix L ∈ Rd×d and number of partitions p 2: Initialize: X ← ∅ 3: while Y \X 6= ∅",3.2. Algorithm Description and Guarantee,[0],[0]
do 4: Partition Y \X randomly into p subsets.,3.2. Algorithm Description and Guarantee,[0],[0]
"5: for j = 1 to p do 6: L (j)
X ← average of LX∪{i} for i in the partition j 7: z(j) ← (|X|+ 1)-th column of ( L (j)
X )−1 8: Γj ← log detL (j)
",3.2. Algorithm Description and Guarantee,[0],[0]
X,3.2. Algorithm Description and Guarantee,[0],[0]
"− log detLX 9: end for
10: for i ∈ Y \X do 11:",3.2. Algorithm Description and Guarantee,[0],[0]
"∆i ← 〈 LX∪{i} − L (j) X , Mat ( z(j) )〉 2 + Γj where element i is included in partition j. 12: end for 13: imax ← argmaxi∈Y\X ∆i 14: if log detLX∪{imax} − log detLX < 0",3.2. Algorithm Description and Guarantee,[0],[0]
"then 15: return X 16: end if 17: X ← X ∪ {imax} 18: end while
As we explained in Section 3.1, the lines 7, 8 require to run CG.",3.2. Algorithm Description and Guarantee,[0],[0]
"Hence, the overall complexity becomes Θ(T 3GR ·TCG ·p+ d·T 2GR) =",3.2. Algorithm Description and Guarantee,[0],[0]
"Θ(T 3GR+d·T 2GR), where we choose p, TCG = O(1).",3.2. Algorithm Description and Guarantee,[0],[0]
"Since TGR = O(d), it is simply O(d3) and better than the complexity O(d4) of the naı̈ve implementations described in Section 2.3.",3.2. Algorithm Description and Guarantee,[0],[0]
"In particular, if kernel matrix L is sparse, i.e., number of non-zeros of each column/row isO(1), ours has the complexity Θ(T 2GR + d · TGR) while the naı̈ve approaches are still worse having the complexity Θ(d · T 2GR).
",3.2. Algorithm Description and Guarantee,[0],[0]
"We also provide the following approximation guarantee of Algorithm 1 for the monotone case, where its proof is given in the supplementary material.
",3.2. Algorithm Description and Guarantee,[0],[0]
Theorem 1.,3.2. Algorithm Description and Guarantee,[0],[0]
Suppose the smallest eigenvalue of L is greater than 1.,3.2. Algorithm Description and Guarantee,[0],[0]
"Then, it holds that
log detLX ≥ (1− 1/e) max Z⊆Y,|Z|=|X| log detLZ − 2|X|ε.
where
ε = max X⊆Y,i∈Y\X
j∈[p]
∣∣∣∣∣log detLX∪{i}detL(j)X",3.2. Algorithm Description and Guarantee,[0],[0]
"− 〈( L (j) X )−1 , LX∪{i} − L (j) X 〉∣∣∣∣∣ and X is the output of Algorithm 1.
",3.2. Algorithm Description and Guarantee,[0],[0]
The above theorem captures the relation between the firstorder approximation error ε > 0,3.2. Algorithm Description and Guarantee,[0],[0]
in (3) and the worst-case approximation ratio of the algorithm.,3.2. Algorithm Description and Guarantee,[0],[0]
"In this section, we present an even faster greedy algorithm for the MAP inference task of DPP, in particular for largescale tasks.",4. Faster Batch-Greedy DPP Inference,[0],[0]
"On top of ideas described in Section 3.1, we use a batch strategy, i.e., add k elements instead of a single element to the current set, where LDAS in Section 2.3 is now used as a key component.",4. Faster Batch-Greedy DPP Inference,[0],[0]
The batch strategy accelerates our algorithm.,4. Faster Batch-Greedy DPP Inference,[0],[0]
We first provide the formal description of the batch greedy algorithm in Section 4.1.,4. Faster Batch-Greedy DPP Inference,[0],[0]
"In Section 4.2, we describe additional ideas on applying LDAS as a subroutine of the proposed batch algorithm.",4. Faster Batch-Greedy DPP Inference,[0],[0]
The formal description of the proposed algorithm is described in Algorithm 2.,4.1. Algorithm Description,[0],[0]
"Similar to the line 7 in Algorithm 1, the line 8 of Algorithm 2 can be solved by the CG algorithms.",4.1. Algorithm Description,[0],[0]
"However, the line 9 of Algorithm 2 uses the LDAS and we remind that it runs in Θ(d2) time.",4.1. Algorithm Description,[0],[0]
"In addition, the line 12 requires the vector inner products ks times.",4.1. Algorithm Description,[0],[0]
"Thus, the total complexity becomes Θ ( T 3GR · ( TCG + mn k ) ·",4.1. Algorithm Description,[0],[0]
p+ s · T 2GR + s · TCG ) =,4.1. Algorithm Description,[0],[0]
"Θ(T 3GR)
2For Z ∈ Rd×k, Mat(Z) ∈ Rd×d is defined whose the last k columns and rows are equal to Z and Z>, respectively, and other entries set to 0.
",4.1. Algorithm Description,[0],[0]
"Algorithm 2 Faster Batch-Greedy DPP Inference
1: Input: kernel matrix L ∈ Rd×d, number of partitions p, batch size k and the number of batch samples s 2: Initialize: X ← ∅ 3: while Y \X is not empty do 4: Ii ← Randomly draw a batch of size k for i ∈",4.1. Algorithm Description,[0],[0]
[s].,4.1. Algorithm Description,[0],[0]
5: Partition [s] randomly into p subsets.,4.1. Algorithm Description,[0],[0]
"6: for j = 1 to p do 7: L (j)
X",4.1. Algorithm Description,[0],[0]
← average of LX∪Ii for i in the partition,4.1. Algorithm Description,[0],[0]
"j 8: Z(j) ← (|X| + 1) to (|X| + k)-th columns of(
L (j)
X )−1 9: Γj ← log detL (j)
X using LDAS.",4.1. Algorithm Description,[0],[0]
10: end for 11: for i = 1 to s do 12: ∆Batchi,4.1. Algorithm Description,[0],[0]
"← 〈 LX∪Ii − L (j) X , Mat ( Z(j) )〉 2 + Γj
where a batch index i is included in j-th partition.",4.1. Algorithm Description,[0],[0]
13: end for 14: imax ← argmaxi∈[s] ∆Batchi 15: if log detLX∪Iimax,4.1. Algorithm Description,[0],[0]
− log detLX < 0,4.1. Algorithm Description,[0],[0]
"then 16: return X 17: end if 18: X ← X ∪ Iimax 19: end while
where TGR is the number of greedy updates on the current set X and we choose all parameters p, TCG, k, s,m, n = O(1).",4.1. Algorithm Description,[0],[0]
We note that Algorithm 2 is expected to perform faster than Algorithm 1 when both TGR and d are large.,4.1. Algorithm Description,[0],[0]
This is primarily because the size of the current set X increases by k > 1 for each greedy iteration.,4.1. Algorithm Description,[0],[0]
"A larger choice of k speeds up the algorithm up to k times, but it might hurt its output quality.",4.1. Algorithm Description,[0],[0]
"We explain more details of key components of the batch algorithm below.
",4.1. Algorithm Description,[0],[0]
Batch selection.,4.1. Algorithm Description,[0],[0]
"The essence of Algorithm 2 is adding k > 1 elements, called batch, simultaneously to the current set with an improved marginal gain.",4.1. Algorithm Description,[0],[0]
"Formally, it starts from the empty set and recursively updates X ← X ∪ Imax for
Imax = argmax I⊆Y\X,|I|=k log detLX∪I .",4.1. Algorithm Description,[0],[0]
"(4)
until no gain is attained.",4.1. Algorithm Description,[0],[0]
The non-batch greedy procedure (1) corresponds to k = 1.,4.1. Algorithm Description,[0],[0]
"Such batch greedy algorithms have been also studied for submodular maximization (Nemhauser et al., 1978; Hausmann et al., 1980) and recently, (Liu et al., 2016) studied their theoretical guarantees showing that they can be better than their non-batch counterparts under some conditions.",4.1. Algorithm Description,[0],[0]
The main drawback of the standard batch greedy algorithms is that finding the optimal batch of size k requires computing too many marginal gains of (|Y\X| k ) subsets.,4.1. Algorithm Description,[0],[0]
"To address the issue, we sample
s (|Y\X|
k
) bunches of batch subsets randomly and com-
pute approximate batch marginal gains using them.",4.1. Algorithm Description,[0],[0]
"(Mirzasoleiman et al., 2015) first propose an uniformly random sampling to the standard non-batch greedy algorithm.",4.1. Algorithm Description,[0],[0]
The authors show that it guarantees (1 − 1/e − O(e−s)) approximation ratio in expectation and report that it performs well in many applications.,4.1. Algorithm Description,[0],[0]
"In our experiments, we choose s = 50 batch samples.
",4.1. Algorithm Description,[0],[0]
High-order approximation of log-determinant.,4.1. Algorithm Description,[0],[0]
"Recall that for Algorithm 1, we suggest using the CG algorithm under the Schur complement for computing
log detL (j)
",4.1. Algorithm Description,[0],[0]
X,4.1. Algorithm Description,[0],[0]
− log detLX .,4.1. Algorithm Description,[0],[0]
"(5)
One can apply the same strategy for Algorithm 2, which requires running the CG algorithm k times for (5).",4.1. Algorithm Description,[0],[0]
"Instead, we suggest running LDAS (using polynomial/high-order approximations of the scalar log function) only once, i.e., the line 9, which is much faster if k is large.",4.1. Algorithm Description,[0],[0]
We remind that the asymptotic complexities of CG and LDAS are comparable.,4.1. Algorithm Description,[0],[0]
"To improve the approximation quality of Algorithm 2, we further suggest running LDAS using the same random vectors v(1), . . .",4.2. Sharing Randomness in Trace Estimators,[0],[0]
",v(m) across j ∈",4.2. Sharing Randomness in Trace Estimators,[0],[0]
[p].,4.2. Sharing Randomness in Trace Estimators,[0],[0]
"This is because we are interested in relative values log detL (j)
X for j ∈",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"[p] instead of their absolute ones.
",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Our intuition is that different random vectors have different bias, which hurt the comparison task.",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Figure 1 demonstrates an experiment on the estimation of log detL (j)
X
when random vectors are shared and independent, respectively.",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"This implies that sharing random vectors might be worse for estimating the absolute values of logdeterminants, but better for comparing them.
",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"We also formally justify the idea of sharing random vectors as stated in the follows theorem whose proof is given in the supplementary material.
",4.2. Sharing Randomness in Trace Estimators,[0],[0]
Theorem 2.,4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Suppose A,B are positive definite matrices whose eigenvalues are in [δ, 1− δ] for δ > 0.",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Let ΓA,ΓB
be the estimations of log detA, log detB by LDAS using the same random vectors v(1), . . .",4.2. Sharing Randomness in Trace Estimators,[0],[0]
",v(m)",4.2. Sharing Randomness in Trace Estimators,[0],[0]
for both.,4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Then, it holds that
Var [ΓA − ΓB ] ≤ 32M2ρ2 (ρ+ 1)
2
m (ρ− 1)6 (1− 2δ)2 ‖A−B‖2F
where M = 5 log (2/δ) and ρ = 1 + 2√ 2/δ−1−1 .
",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"Without sharing random vectors, the variance should grow linearly with respect to ‖A‖2F + ‖B‖ 2 F .",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"In our case, matrices A and B correspond to some of L (j)
X , and ‖A−B‖2F is significantly smaller than ‖A‖2F + ‖B‖ 2 F .",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"We believe that our idea of sharing randomness might be of broader interest in many applications of LDAS or its variants, requiring multiple log-determinant computations.",4.2. Sharing Randomness in Trace Estimators,[0],[0]
"In this section, we evaluate our proposed algorithms for the MAP inference on synthetic and real-world DPP instances.",5. Experimental Results,[0],[0]
"3
Setups.",5. Experimental Results,[0],[0]
"The experiments are performed using a machine with a hexa-core Intel CPU (Core i7-5930K, 3.5 GHz) and 32 GB RAM.",5. Experimental Results,[0],[0]
"We compare our algorithms with following competitors: the lazy greedy algorithm (LAZY) (Minoux, 1978), double greedy algorithm (DOUBLE) (Buchbinder et al., 2015) and softmax extension (SOFTMAX) (Gillenwater et al., 2012).",5. Experimental Results,[0],[0]
"In all our experiments, LAZY is significantly faster than the naı̈ve greedy algorithms described in Section 2.3, while they produce the same outputs.",5. Experimental Results,[0],[0]
"Hence, we use LAZY as the baseline of evaluation.
",5. Experimental Results,[0],[0]
"Unless stated otherwise, we choose parameters of p = 5, k = 10, s = 50, m = 20 and n = 15, regardless matrix dimension, for our algorithms.",5. Experimental Results,[0],[0]
"We also run CG until it achieves convergence error less than 10−10 and typically TCG ≤ 30.
",5. Experimental Results,[0],[0]
Additional tricks for boosting accuracy.,5. Experimental Results,[0],[0]
"For boosting approximation qualities of our algorithms, we use the simple trick in our experiments: recompute top ` marginal gains exactly (using CG) where they are selected based on estimated marginal gains, i.e., ∆i for Algorithm 1 and ∆Batchi for Algorithm 2.",5. Experimental Results,[0],[0]
"Then, our algorithms choose the best element among ` candidates, based on their exact marginal gains.",5. Experimental Results,[0],[0]
"Since we choose small ` = 20 in our experiments, this additional process increases the running times of our algorithms marginally, but makes them more accurate.",5. Experimental Results,[0],[0]
"In fact, the trick is inspired from (Minoux, 1978) where the authors also recompute the exact marginal gains of few elements.",5. Experimental Results,[0],[0]
"In addition, for boosting further approximation
3The codes are available in https://github.com/ insuhan/fastdppmap.
qualities of Algorithm 2, we also run Algorithm 1 in parallel and choose the largest one among {∆i,∆Batchi } given the current set.",5. Experimental Results,[0],[0]
"Hence, at most iterations, the batch with the maximal ∆Batchi is chosen and increases the current set size by k (i.e., making speed-up) as like Algorithm 2, and the non-batch with the maximal ∆i is chosen at very last iterations, which fine-tunes the solution quality.",5. Experimental Results,[0],[0]
"We still call the synthesized algorithm by Algorithm 2 in this section.
",5. Experimental Results,[0],[0]
Performance metrics.,5. Experimental Results,[0],[0]
"For the performance measure on approximation qualities of algorithms, we use the following ratio of log-probabilities:
log detLX/log detLXLAZY .
where X and XLAZY are the outputs of an algorithm and LAZY, respectively.",5. Experimental Results,[0],[0]
"Namely, we compare outputs of algorithms with that of LAZY since the exact optimum is hard to compute.",5. Experimental Results,[0],[0]
"Similarly, we report the running time speedup of each algorithm over LAZY.",5. Experimental Results,[0],[0]
"In this section, we use synthetic DPP datasets generated as follows.",5.1. Synthetic Dataset,[0],[0]
"As (Kulesza & Taskar, 2011; Kulesza et al., 2012) proposed, a kernel matrix L for DPP can be reparameterized as
Li,j =",5.1. Synthetic Dataset,[0],[0]
qiφ >,5.1. Synthetic Dataset,[0],[0]
"i φjqj ,
where qi ∈ R+ is considered as the quality of item i and φi ∈ Rd is the normalized feature vector of item",5.1. Synthetic Dataset,[0],[0]
"i so that φ>i φj measures the similarity between i and j. We use qi = exp (β1xi + β2) for the quality measurement xi ∈ R and choose β1 = 0.01, β2 = 0.2.",5.1. Synthetic Dataset,[0],[0]
"We choose each entry of φi and xi drawn from the normal distribution N (0, 1) for all i ∈",5.1. Synthetic Dataset,[0],[0]
"[d], and then normalize φi so that ‖φi‖2",5.1. Synthetic Dataset,[0],[0]
"= 1.
We first show how much the number of clusters p and the batch size k are sensitive for Algorithm 1 and Algorithm 2, respectively.",5.1. Synthetic Dataset,[0],[0]
Figure 3(a) shows the accuracy of Algorithm 1 with different numbers of clusters.,5.1. Synthetic Dataset,[0],[0]
"It indeed confirms that a larger cluster improves its accuracy since it makes first-
order approximations tighter.",5.1. Synthetic Dataset,[0],[0]
"Figure 3(b) shows the performance trend of Algorithm 2 as the batch size k increases, which shows that a larger batch might hurt its accuracy.",5.1. Synthetic Dataset,[0],[0]
"Based on these experiments, we choose p = 5, k = 10 in order to target 0.01 approximation ratio loss compared to LAZY.
",5.1. Synthetic Dataset,[0],[0]
"We generate synthetic kernel matrices with varying dimension d up to 40, 000, and the performances of tested algorithms are reported in Figure 2(a).",5.1. Synthetic Dataset,[0],[0]
"One can observe that LAZY seems to be near-optimal, where only SOFTMAX often provides marginally larger log-probabilities than LAZY under small dimensions.",5.1. Synthetic Dataset,[0],[0]
"Interestingly, we found that DOUBLE has the strong theoretical guarantee for general submodular maximization (Buchbinder et al., 2015), but its practical performance for DPP is worst among evaluating algorithms.",5.1. Synthetic Dataset,[0],[0]
"Moverover, it is slightly slower than LAZY.",5.1. Synthetic Dataset,[0],[0]
"In summary, one can conclude that our algorithms can be at orders of magnitude faster than LAZY, DOUBLE and SOFTMAX, while loosing 0.01-approximation ratio.",5.1. Synthetic Dataset,[0],[0]
"For example, Algorithm 2 is 19 times faster than LAZY for d = 40, 000, and the gap should increase for larger dimension d.",5.1. Synthetic Dataset,[0],[0]
"We use real-world datasets of the following two tasks of matched and video summarizations.
",5.2. Real Dataset,[0],[0]
Matched summarization.,5.2. Real Dataset,[0],[0]
"We evaluate our proposed algorithms for matched summarization that is first proposed by (Gillenwater et al., 2012).",5.2. Real Dataset,[0],[0]
This task gives useful information for comparing the texts addressed at different times by the same speaker.,5.2. Real Dataset,[0],[0]
Suppose we have two different documents and each one consists of several statements.,5.2. Real Dataset,[0],[0]
"The goal is to apply DPP for finding statement pairs that are similar to each other, while they summarize (i.e., diverse) well the two documents.",5.2. Real Dataset,[0],[0]
"We use transcripts of debates in 2016 US Republican party presidential primaries speeched by following 8 participates: Bush, Carson, Christie, Kasich, Paul, Trump, Cruz and Rubio.4
We follow similar pre-processing steps of (Gillenwater et al., 2012).",5.2. Real Dataset,[0],[0]
"First, every sentence is parsed and only nouns except the stopwords are extracted via NLTK (Bird, 2006).",5.2. Real Dataset,[0],[0]
"Then, we remove the ‘rare’ words occurring less than 10% of the whole debates, and then ignore each statement which contains more ‘rare’ words than ’frequent’ ones in it.",5.2. Real Dataset,[0],[0]
"This gives us a dataset containing 3, 406 distinct ‘frequent’ words and 1, 157 statements.",5.2. Real Dataset,[0],[0]
"For each statement pair (i, j), feature vector φ(i,j) = wi + wj ∈ R3406 where wi is generated as a frequency of words in the statement i. Then, we normalize φ(i,j).",5.2. Real Dataset,[0],[0]
"The match quality x(i,j) is measured as the cosine similarity between two statements i and j, i.e., x(i,j) =",5.2. Real Dataset,[0],[0]
w >,5.2. Real Dataset,[0],[0]
"i wj , and we remove statement pairs (i, j) such that its match quailty x(i,j) is smaller than 15% of the maximum one.",5.2. Real Dataset,[0],[0]
"Finally, by choosing q(i,j) = exp ( 0.01 · x(i,j) ) ,
we obtain ( 8 2 ) = 28 kernel matrices of dimension d from 516 to 4, 000.
",5.2. Real Dataset,[0],[0]
Figure 4 reports log-probability ratios and speedups of Algorithm 2 under the 28 kernels.,5.2. Real Dataset,[0],[0]
"We observe that Algorithm 2 looses 0.03-approximation ratio on average, compared to LAZY, under the real-world kernels.",5.2. Real Dataset,[0],[0]
"Interestingly, SOFTMAX runs much slower than even LAZY, while our algorithm runs faster than LAZY for large dimension, e.g., 8 times faster for d = 4, 000 corresponding to transcripts of Bush and Rubio.
",5.2. Real Dataset,[0],[0]
"4Details of the primaries are provided in http://www. presidency.ucsb.edu/debates.php.
Video summarization.",5.2. Real Dataset,[0],[0]
We evaluate our proposed algorithms video summarization.,5.2. Real Dataset,[0],[0]
"We use 39 videos from a Youtube dataset (De Avila et al., 2011), and the trained DPP kernels from (Gong et al., 2014).",5.2. Real Dataset,[0],[0]
"Under the kernels, we found that the numbers of selected elements from algorithms are typically small (less than 10), and hence we use Algorithm 1 instead of its batch version Algorithm 2.",5.2. Real Dataset,[0],[0]
"For performance evaluation, we use an F-score based on five sets of user summaries where it measures the quality across two summaries.
",5.2. Real Dataset,[0],[0]
Figure 5(a) illustrates F-score for LAZY and Algorithm 1 and Figure 5(b) reports its speedup.,5.2. Real Dataset,[0],[0]
"Our algorithm achieves over 13 times speedup in this case, while it produces Fscores that are very similar to those of LAZY.",5.2. Real Dataset,[0],[0]
"For some video, it achieves even better F-score, as illustrated in 5(c).",5.2. Real Dataset,[0],[0]
We have presented fast algorithms for the MAP inference task of large-scale DPPs.,6. Conclusion,[0],[0]
Our main idea is to amortize common determinant computations via linear algebraic techniques and recent log-determinant approximation methods.,6. Conclusion,[0],[0]
"Although we primarily focus on a special matrix optimization, we expect that several ideas developed in this paper would be useful for other related matrix computational problems, in particular, involving multiple determinant computations.",6. Conclusion,[0],[0]
"This work was supported in part by the ICT R&D Program of MSIP/IITP, Korea, under [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion].",Acknowledgements,[0],[0]
"Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants.",abstractText,[0],[0]
"In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general.",abstractText,[0],[0]
"Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success.",abstractText,[0],[0]
"Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration.",abstractText,[0],[0]
We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) highorder expansions to the scalar log function with stochastic trace estimators.,abstractText,[0],[0]
"In our experiments, our algorithms are significantly faster than their competitors for large-scale instances, while sacrificing marginal accuracy.",abstractText,[0],[0]
Faster Greedy MAP Inference for Determinantal Point Processes,title,[0],[0]
"In machine learning and statistics, it is often desirable to represent a large-scale dataset in a more tractable, lowerdimensional form, without losing too much information.",1 Introduction,[0],[0]
"One of the most robust ways to achieve this goal is through principal component projection (PCP):
PCP: project vectors onto the span of the top principal components of the a matrix.
",1 Introduction,[0],[0]
It is well-known that PCP decreases noise and increases efficiency in downstream tasks.,1 Introduction,[0],[0]
"One of the main applications is principal component regression (PCR):
PCR: linear regression but restricted to the subspace of top principal components.
",1 Introduction,[0],[0]
"Classical algorithms for PCP or PCR rely on a principal component analysis (PCA) solver to recover the top principal components first; with these components available, the
*Equal contribution .",1 Introduction,[0],[0]
Future version of this paper shall be found at https://arxiv.org/abs/1608.04773.,1 Introduction,[0],[0]
1Microsoft Reseaerch 2Princeton University.,1 Introduction,[0],[0]
"Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
tasks of PCP and PCR become trivial because the projection matrix can be constructed explicitly.
",1 Introduction,[0],[0]
"Unfortunately, PCA solvers demand a running time that at least linearly scales with the number of top principal components chosen for the projection.",1 Introduction,[0],[0]
"For instance, to project a vector onto the top 1000 principal components of a high-dimensional dataset, even the most efficient Krylovbased (Musco & Musco, 2015) or Lanczos-based (AllenZhu & Li, 2016a) methods require a running time that is proportional to 1000×40 = 4×104 times the input matrix sparsity, if the Krylov or Lanczos method is executed for 40 iterations.",1 Introduction,[0],[0]
This is usually computationally intractable.,1 Introduction,[0],[0]
"In this paper, we propose the following notion of PCP approximation.",1.1 Approximating PCP Without PCA,[0],[0]
"Given a data matrix A ∈ Rd′×d (with singular values no greater than 1) and a threshold λ > 0, we say that an algorithm solves (γ, ε)-approximate PCP if — informally speaking and up to a multiplicative 1±ε error— it projects (see Def. 3.1 for a formal definition)
1.",1.1 Approximating PCP Without PCA,[0],[0]
"eigenvector ν of A>A with value in [ λ(1 +γ), 1 ] to ν,
2.",1.1 Approximating PCP Without PCA,[0],[0]
"eigenvector ν of A>A with value in [ 0, λ(1− γ) ] to ~0,
3.",1.1 Approximating PCP Without PCA,[0],[0]
"eigenvector ν of A>A with value in [ λ(1 − γ), λ(1 +
γ) ]",1.1 Approximating PCP Without PCA,[0],[0]
"to “anywhere between ~0 and ν.”
Such a definition also extends to (γ, ε)-approximate PCR (see Def. 3.2).
",1.1 Approximating PCP Without PCA,[0],[0]
"It was first noticed by Frostig et al. (Frostig et al., 2016) that approximate PCP and PCR be solved with a running time independent of the number of principal components above threshold λ.",1.1 Approximating PCP Without PCA,[0],[0]
"More specifically, they reduced (γ, ε)approximate PCP and PCR to
O ( γ−2 log(1/ε) )",1.1 Approximating PCP Without PCA,[0],[0]
"black-box calls of
any ridge regression subroutine
where each call computes (A>A + λI)−1u for some vector u.1",1.1 Approximating PCP Without PCA,[0],[0]
"Our main focus of this paper is to quadratically improve this performance and reduce PCP and PCR to
1Ridge regression is often considered as an easy-to-solve machine learning problem: using for instance SVRG (Johnson & Zhang, 2013), one can usually solve ridge regression to an 10−8 accuracy with at most 40 passes of the data.
",1.1 Approximating PCP Without PCA,[0],[0]
O ( γ−1 log(1/γε) ),1.1 Approximating PCP Without PCA,[0],[0]
"black-box calls of
any ridge regression subroutine
where each call again computes (A>A + λI)−1u.",1.1 Approximating PCP Without PCA,[0],[0]
Remark 1.1.,1.1 Approximating PCP Without PCA,[0],[0]
"Frostig et al. only showed their algorithm satisfies the properties 1 and 2 of (γ, ε)-approximation (but not the property 3), and thus their proof was only for matrix A with no singular value in the range [ √ λ(1− γ), √ λ(1 + γ)].",1.1 Approximating PCP Without PCA,[0],[0]
"This is known as the eigengap assumption, which is rarely satisfied in practice (Musco & Musco, 2015).",1.1 Approximating PCP Without PCA,[0],[0]
"In this paper, we prove our result both with and without such eigengap assumption.",1.1 Approximating PCP Without PCA,[0],[0]
"Since our techniques also imply the algorithm of Frostig et al. satisfies property 3, throughout the paper, we say Frostig et al. solve (γ, ε)-approximate PCP and PCR.",1.1 Approximating PCP Without PCA,[0],[0]
The main technique of Frostig et al. is to construct a polynomial to approximate the sign function sgn(x) :,1.2 From PCP to Polynomial Approximation,[0],[0]
"[−1, 1]→ {±1}:
sgn(x) := { +1, x ≥ 0; −1, x < 0.
",1.2 From PCP to Polynomial Approximation,[0],[0]
"In particular, given any polynomial g(x) satisfying∣∣g(x)− sgn(x)∣∣ ≤ ε ∀x ∈",1.2 From PCP to Polynomial Approximation,[0],[0]
"[−1,−γ] ∪ [γ, 1] , (1.1)∣∣g(x)∣∣ ≤ 1 ∀x ∈",1.2 From PCP to Polynomial Approximation,[0],[0]
"[−γ, γ] , (1.2) the problem of (γ, ε)-approximate PCP can be reduced to computing the matrix polynomial g(S) for S := (A>A + λI)−1(A>A− λI) (cf. Fact 7.1).",1.2 From PCP to Polynomial Approximation,[0],[0]
"In other words, • to project any vector χ ∈ Rd to top principal compo-
nents, we can compute g(S)χ instead; and
• to compute g(S)χ, we can reduce it to ridge regression for each evaluation of Su for some vector u. Remark 1.2.",1.2 From PCP to Polynomial Approximation,[0],[0]
"Since the transformation from A>A to S is not linear, the final approximation to the PCP is a rational function (as opposed to a polynomial) over A>A.",1.2 From PCP to Polynomial Approximation,[0],[0]
"We restrict to polynomial choices of g(·) because in this way, the final rational function has all the denominators being A>A + λI, thus reduces to ridge regressions.",1.2 From PCP to Polynomial Approximation,[0],[0]
Remark 1.3.,1.2 From PCP to Polynomial Approximation,[0],[0]
The transformation from A>A to S ensures that all the eigenvalues of A>A in the range (1 ± γ)λ roughly map to the eigenvalues of S in the range,1.2 From PCP to Polynomial Approximation,[0],[0]
"[−γ, γ].
",1.2 From PCP to Polynomial Approximation,[0],[0]
Main Challenges.,1.2 From PCP to Polynomial Approximation,[0],[0]
"There are two main challenges regarding the design of polynomial g(x).
",1.2 From PCP to Polynomial Approximation,[0],[0]
• EFFICIENCY.,1.2 From PCP to Polynomial Approximation,[0],[0]
"We wish to minimize the degree n = deg(g(x)) because the computation of g(S)χ usually requires n calls of ridge regression.
",1.2 From PCP to Polynomial Approximation,[0],[0]
• STABILITY.,1.2 From PCP to Polynomial Approximation,[0],[0]
"We wish g(x) to be stable; that is, g(S)χ must be given by a recursive formula where if we make ε′ error in each recursion (due to error incurred from ridge regression), the final error of g(S)χ must be at most ε′ × poly(d).
",1.2 From PCP to Polynomial Approximation,[0],[0]
Remark 1.4.,1.2 From PCP to Polynomial Approximation,[0],[0]
"Efficient routines such as SVRG (Johnson & Zhang, 2013) solve ridge regression and thus compute Su for any u ∈ Rd, with running times only logarithmically in 1/ε′. Therefore, by setting ε′ = ε/poly(d), one can blow up the running time by a small factor O(log(d)) in order to obtain an ε-accurate solution for g(S)χ.
",1.2 From PCP to Polynomial Approximation,[0],[0]
The polynomial g(x) constructed by Frostig et al. comes from truncated Taylor expansion.,1.2 From PCP to Polynomial Approximation,[0],[0]
It has degree O ( γ−2 log(1/ε) ) and is stable.,1.2 From PCP to Polynomial Approximation,[0],[0]
"This γ−2 dependency limits the practical performance of their proposed PCP and PCR algorithms, especially in a high accuracy regime.",1.2 From PCP to Polynomial Approximation,[0],[0]
"At the same time,
• the optimal degree for a polynomial to satisfy even only (1.1) is Θ ( γ−1 log(1/ε) )",1.2 From PCP to Polynomial Approximation,[0],[0]
"(Eremenko & Yuditskii,
2007; 2011).
",1.2 From PCP to Polynomial Approximation,[0],[0]
Frostig et al. were unable to find a stable polynomial matching this optimal degree and left it as open question.2,1.2 From PCP to Polynomial Approximation,[0],[0]
We provide an efficient and stable polynomial approximation to the matrix sign function that has a near-optimal degree O(γ−1 log(1/γε)).,1.3 Our Results and Main Ideas,[0],[0]
"At a high level, we construct a polynomial q(x) that approximately equals ( 1+κ−x
2 )−1/2 for some κ = Θ(γ2); then we set g(x) := x·q(1+κ−2x2) which approximates sgn(x).
",1.3 Our Results and Main Ideas,[0],[0]
"To construct q(x), we first note that (
1+κ−x 2
)−1/2 has
no singular point on [−1, 1] so we can apply Chebyshev approximation theory to obtain some q(x) of degree O(γ−1 log(1/γε))",1.3 Our Results and Main Ideas,[0],[0]
"satisfying∣∣∣q(x)− (1 + κ− x
2 )−1/2∣∣∣ ≤ ε for every x ∈",1.3 Our Results and Main Ideas,[0],[0]
"[−1, 1] .",1.3 Our Results and Main Ideas,[0],[0]
"This can be shown to imply
∣∣g(x)− sgn(x)∣∣ ≤ ε for every x ∈",1.3 Our Results and Main Ideas,[0],[0]
"[−1,−γ] ∪ [γ, 1], so (1.1) is satisfied.",1.3 Our Results and Main Ideas,[0],[0]
"In order to prove (1.2) , we prove a separate lemma:3
q(x) ≤ (1 + κ− x
2
)−1/2 for every x ∈",1.3 Our Results and Main Ideas,[0],[0]
"[1, 1 + κ] .
",1.3 Our Results and Main Ideas,[0],[0]
Note that this does not follow from standard Chebyshev theory because Chebyshev approximation guarantees are only with respect to x ∈,1.3 Our Results and Main Ideas,[0],[0]
"[−1, 1].",1.3 Our Results and Main Ideas,[0],[0]
This proves the “EFFICIENCY” part of the main challenges discussed earlier.,1.3 Our Results and Main Ideas,[0],[0]
"As for the “STABILITY” part, we prove a general theorem regarding any weighted sum of Chebyshev polynomials applied to matrices.",1.3 Our Results and Main Ideas,[0],[0]
"We provide a backward recurrence algorithm and show that it is stable under noisy
2Using degree reduction, Frostig et al. found an explicit polynomial g(x) of degree O ( γ−1 log(1/γε) )",1.3 Our Results and Main Ideas,[0],[0]
satisfying (1.1).,1.3 Our Results and Main Ideas,[0],[0]
"However, that polynomial is unstable because it is constructed monomial by monomial and has exponentially large coefficients in front of each monomial.",1.3 Our Results and Main Ideas,[0],[0]
"Furthermore, it is not clear if their polynomial satisfies the (1.2).
",1.3 Our Results and Main Ideas,[0],[0]
"3We proved a general lemma which holds for any function whose all orders of derivatives are non-negative at x = 0.
computations.",1.3 Our Results and Main Ideas,[0],[0]
"This may be of independent interest.
",1.3 Our Results and Main Ideas,[0],[0]
"For interested readers, we compare our polynomial q(x) with that of Frostig et al. in Figure 1.",1.3 Our Results and Main Ideas,[0],[0]
"There are a few attempts to reduce the cost of PCA when solving PCR, by for instance approximating the matrix APλ where Pλ is the PCP projection matrix (Chan & Hansen, 1990; Boutsidis & Magdon-Ismail, 2014).",1.4 Related Work,[0],[0]
"However, they cost a running time that linearly scales with the number of principal components above λ.
",1.4 Related Work,[0],[0]
"A significant number of papers have focused on the lowrank case of PCA (Musco & Musco, 2015; Allen-Zhu & Li, 2016a; 2017) and its online variant (Allen-Zhu & Li, 2016b).",1.4 Related Work,[0],[0]
"Unfortunately, all of these methods require a running time that scales at least linearly with respect to the number of top principal components.
",1.4 Related Work,[0],[0]
"More related to this paper is work on matrix sign function, which plays an important role in control theory and quantum chromodynamics.",1.4 Related Work,[0],[0]
"Several results have addressed Krylov methods for applying the sign function in the socalled Krylov subspace, without explicitly constructing any approximate polynomial (van den",1.4 Related Work,[0],[0]
"Eshof et al., 2002; Schilders et al., 2008).",1.4 Related Work,[0],[0]
"However, Krylov methods are not (γ, ε)-approximate PCP solvers, and there is no supporting stability theory behind them.4 Other iterative methods have also been proposed, see Section 5 of textbook (Higham, 2008).",1.4 Related Work,[0],[0]
"For instance, Schur’s method is a slow one and also requires the matrix to be explicitly given.",1.4 Related Work,[0],[0]
"The Newton’s iteration and its numerous variants (e.g. (Nakatsukasa & Freund, 2016)) provide rational approximations to the matrix sign function as opposed to polynomial approximations.",1.4 Related Work,[0],[0]
"Our result and Frostig et al. (Frostig et al., 2016) differ from these cited works, because we have only accessed an approximate ridge regression oracle, so ensuring a polynomial approximation to the sign function and ensuring its stability are crucial.
",1.4 Related Work,[0],[0]
Using matrix Chebyshev polynomials to approximate matrix functions is not new.,1.4 Related Work,[0],[0]
"Perhaps the most celebrated example is to approximate S−1 using polynomials on S, used in the analysis of conjugate gradient (Shewchuk, 1994).",1.4 Related Work,[0],[0]
"Independent from this paper,5 Han et al. (Han et al., 2016) used Chebyshev polynomials to approximate the trace of the matrix sign function, i.e., Tr(sgn(S)), which is similar but a different problem.6 Also, they did not study the
4We anyways have included Krylov method in our empirical evaluation section and shall discuss its performance there, see the full version of this paper.
",1.4 Related Work,[0],[0]
"5Their paper appeared online two months before us, and we became aware of their work in March 2017.
",1.4 Related Work,[0],[0]
"6In particular, their degree of the Chebyshev polynomial is O ( γ−1(log2(1/γ)+ log(1/γ) log(1/ε)) )",1.4 Related Work,[0],[0]
"in the language of this
paper; in contrast, we have degree O ( γ−1 log(1/γε) ) .
case when the matrix-vector multiplication oracle is only approximate (like we do in this paper), or the case when S has eigenvalues in the range [−γ, γ].
",1.4 Related Work,[0],[0]
Roadmap.,1.4 Related Work,[0],[0]
"In Section 2, we provide notions for this paper and basics for Chebyshev polynomials.",1.4 Related Work,[0],[0]
"In Section 3, we formally define approximate PCP and PCR, and reduce PCR to PCP.",1.4 Related Work,[0],[0]
"In Section 4, we show a general lemma for Chebyshev approximations.",1.4 Related Work,[0],[0]
"In Section 5, we design our polynomial approximation to sgn(x).",1.4 Related Work,[0],[0]
"In Section 6, we show how to stably compute Chebyshev polynomials.",1.4 Related Work,[0],[0]
"In Section 7, we state our main theorems regarding PCP and PCR.",1.4 Related Work,[0],[0]
"In Section 8, we provide empirical evaluations.",1.4 Related Work,[0],[0]
"We denote by 1[e] ∈ {0, 1} the indicator function for event e, by ‖v‖ or ‖v‖2 the Euclidean norm of a vector v, by M† the Moore-Penrose pseudo-inverse of a symmetric matrix M, and by ‖M‖2 its spectral norm.",2 Preliminaries,[0],[0]
"We sometimes use ~v to emphasize that v is a vector.
",2 Preliminaries,[0],[0]
"Given a symmetric d × d matrix M and any f : R → R, f(M) is the matrix function applied to M, which is equal to Udiag{f(D1), . . .",2 Preliminaries,[0],[0]
", f(Dd)}U> if M = Udiag{D1, . . .",2 Preliminaries,[0],[0]
", Dd}U> is its eigendecomposition.",2 Preliminaries,[0],[0]
"Throughout the paper, matrix A is of dimension d′ ×",2 Preliminaries,[0],[0]
d. We denote by σmax(A) the largest singular value of A.,2 Preliminaries,[0],[0]
"Following the tradition of (Frostig et al., 2016) and keeping the notations light, we assume without loss of generality that σmax(A) ≤ 1.",2 Preliminaries,[0],[0]
"We are interested in PCP and PCR problems with an eigenvalue threshold λ ∈ (0, 1).",2 Preliminaries,[0],[0]
"Throughout the paper, we denote by λ1 ≥ · · ·",2 Preliminaries,[0],[0]
≥ λd ≥ 0,2 Preliminaries,[0],[0]
"the eigenvalues of A>A, and by ν1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", νd ∈ Rd the eigenvectors of A>A corresponding to λ1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", λd.",2 Preliminaries,[0],[0]
"We denote by Pλ the projection matrix Pλ := (ν1, . . .",2 Preliminaries,[0],[0]
", νj)(ν1, . . .",2 Preliminaries,[0],[0]
", νj)
>",2 Preliminaries,[0],[0]
where j is the largest index satisfying λj ≥ λ.,2 Preliminaries,[0],[0]
"In other words, Pλ is a projection matrix to the eigenvectors of A>A with eigenvalues ≥ λ.",2 Preliminaries,[0],[0]
Definition 2.1.,2 Preliminaries,[0],[0]
The principal component projection (PCP) of χ ∈ Rd at threshold λ is ξ∗ = Pλχ.,2 Preliminaries,[0],[0]
Definition 2.2.,2 Preliminaries,[0],[0]
The principal component regression (PCR) of regressand b ∈ Rd′ at threshold λ is x∗ = arg miny∈Rd ‖APλy,2 Preliminaries,[0],[0]
− b‖2 or equivalently x∗ = (A>A)†Pλ(A >b) .,2 Preliminaries,[0],[0]
Definition 2.3.,2.1 Ridge Regression,[0],[0]
"A black-box algorithm ApxRidge(A, λ, u) is an ε-approximate ridge regression solver, if for every u ∈ Rd, it satisfies ‖ApxRidge(A, λ, u)−(A>A+λI)−1u‖ ≤ ε‖u‖.
Ridge regression is equivalent to solving well-conditioned linear systems, or minimizing strongly convex and smooth objectives f(y) :",2.1 Ridge Regression,[0],[0]
= 12y >,2.1 Ridge Regression,[0],[0]
(A>A + λI)y,2.1 Ridge Regression,[0],[0]
"− u>y.
Remark 2.4.",2.1 Ridge Regression,[0],[0]
There is huge literature on efficient algorithms solving ridge regression.,2.1 Ridge Regression,[0],[0]
"Most notably,
(1) Conjugate gradient (Shewchuk, 1994) or accelerated gradient descent (Nesterov, 2004) gives fastest fullgradient methods;
(2) SVRG (Johnson & Zhang, 2013) and its acceleration Katyusha (Allen-Zhu, 2017) give the fastest stochasticgradient method; and
(3) NUACDM (Allen-Zhu et al., 2016) gives the fastest coordinate-descent method.
",2.1 Ridge Regression,[0],[0]
The running time of (1) is O(nnz(A)λ−1/2 log(1/ε)),2.1 Ridge Regression,[0],[0]
where nnz(A) is time to multiply A to any vector.,2.1 Ridge Regression,[0],[0]
"The running times of (2) and (3) depend on structural properties of A and are always faster than (1).
",2.1 Ridge Regression,[0],[0]
"Because the best complexity of ridge regression depends on the structural properties of A, following Frostig et al., we only compute our running time in terms of the “number of black-box calls” to a ridge regression solver.",2.1 Ridge Regression,[0],[0]
Definition 2.5.,2.2 Chebyshev Polynomials,[0],[0]
Chebyshev polynomials of 1st and 2nd kind are {Tn(x)}n≥0 and {Un(x)}n≥0 where T0(x) :,2.2 Chebyshev Polynomials,[0],[0]
"= 1, T1(x)",2.2 Chebyshev Polynomials,[0],[0]
":= x, Tn+1(x) := 2x · Tn(x)− Tn−1(x) U0(x)",2.2 Chebyshev Polynomials,[0],[0]
":= 1, U1(x)",2.2 Chebyshev Polynomials,[0],[0]
":= 2x, Un+1(x) := 2x · Un(x)− Un−1(x)
Fact 2.6 ((Trefethen, 2013)).",2.2 Chebyshev Polynomials,[0],[0]
"It satisfies ddxTn(x) = nUn−1(x) for n ≥ 1 and
∀n ≥ 0: Tn(x) =  cos(n arccos(x)), if |x| ≤ 1;cosh(n arccosh(x)), if x ≥ 1;(−1)n cosh(n arccosh(−x)), if x ≤ −1.",2.2 Chebyshev Polynomials,[0],[0]
"In particular, when x ≥ 1,
Tn(x) = 1
2
[( x− √ x2",2.2 Chebyshev Polynomials,[0],[0]
− 1 )n,2.2 Chebyshev Polynomials,[0],[0]
+,2.2 Chebyshev Polynomials,[0],[0]
"( x+ √ x2 − 1 )n] Un(x) = 1
2 √ x2",2.2 Chebyshev Polynomials,[0],[0]
"− 1
[( x+ √ x2",2.2 Chebyshev Polynomials,[0],[0]
− 1 )n+1,2.2 Chebyshev Polynomials,[0],[0]
− (x−√x2 − 1)n+1] Definition 2.7.,2.2 Chebyshev Polynomials,[0],[0]
"For function f(x) whose domain contains [−1, 1], its degree-n Chebyshev truncated series and degree-n",2.2 Chebyshev Polynomials,[0],[0]
"Chebyshev interpolation are respectively
pn(x) := n∑ k=0 akTk(x) and qn(x) := n∑ k=0 ckTk(x) ,
where ak := 2− 1[k = 0]
π
∫ 1 −1",2.2 Chebyshev Polynomials,[0],[0]
"f(x)Tk(x)√ 1− x2 dx
ck := 2− 1[k = 0]
n+ 1
n∑ j=0",2.2 Chebyshev Polynomials,[0],[0]
f ( xj ),2.2 Chebyshev Polynomials,[0],[0]
"Tk ( xj ) .
",2.2 Chebyshev Polynomials,[0],[0]
"Above, xj := cos ( (j+0.5)π
n+1
) ∈",2.2 Chebyshev Polynomials,[0],[0]
"[−1, 1] is the j-th Cheby-
shev point of order",2.2 Chebyshev Polynomials,[0],[0]
"n.
The following lemma is known as the aliasing formula for Chebyshev coefficients:
Lemma 2.8 (cf.",2.2 Chebyshev Polynomials,[0],[0]
"Theorem 4.2 of (Trefethen, 2013)).",2.2 Chebyshev Polynomials,[0],[0]
"Let f be Lipschitz continuous on [−1, 1] and {ak}, {ck} be defined in Def. 2.7, then
c0 = a0+a2n+a4n+... , cn = an+a3n+a5n+... , and
for every k ∈ {1, 2, . . .",2.2 Chebyshev Polynomials,[0],[0]
", n− 1}, ck = ak+(ak+2n+ak+4n+...)+(a−k+2n+a−k+4n+...)
",2.2 Chebyshev Polynomials,[0],[0]
Definition 2.9.,2.2 Chebyshev Polynomials,[0],[0]
"For every ρ > 0, let Eρ be the ellipse E of foci ±1 with major radius 1 + ρ.",2.2 Chebyshev Polynomials,[0],[0]
"(This is also known as Bernstein ellipse with parameter 1 + ρ+ √ 2ρ+ ρ2.)
",2.2 Chebyshev Polynomials,[0],[0]
The following lemma is the main theory regarding Chebyshev approximation: Lemma 2.10 (cf.,2.2 Chebyshev Polynomials,[0],[0]
"Theorem 8.1 and 8.2 of (Trefethen, 2013)).",2.2 Chebyshev Polynomials,[0],[0]
Suppose f(z) is analytic on Eρ and |f(z)| ≤ M on Eρ.,2.2 Chebyshev Polynomials,[0],[0]
"Let pn(x) and qn(x) be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x) on [−1, 1].",2.2 Chebyshev Polynomials,[0],[0]
"Then, • max x∈[−1,1] |f(x)−pn(x)|",2.2 Chebyshev Polynomials,[0],[0]
"≤ 2M ρ+ √ 2ρ+ρ2 ( 1+ρ+ √ 2ρ+ ρ2
)−n; • max x∈[−1,1] |f(x)−qn(x)| ≤ 4M ρ+ √ 2ρ+ρ2 ( 1+ρ+ √ 2ρ+ ρ2
)−n.",2.2 Chebyshev Polynomials,[0],[0]
"• |a0| ≤M and |ak| ≤ 2M ( 1+ρ+ √ 2ρ+ ρ2
)−k for k ≥ 1.",2.2 Chebyshev Polynomials,[0],[0]
"We formalize our notions of approximation for PCP and PCR, and provide a reduction from PCR to PCP.",3 Approximate PCP and PCR,[0],[0]
"Recall that Frostig et al. (Frostig et al., 2016) work only with matrices A that satisfy the eigengap assumption, that is, A has no singular value in the range
[ √ λ(1− γ), √ λ(1 + γ)].",3.1 Our Notions of Approximation,[0],[0]
"Their approximation guarantees are very straightforward:
• an output ξ is ε-approximate for PCP on vector χ if ‖ξ − ξ∗‖ ≤ ε‖χ‖;
• an output x is ε-approximate for PCR with regressand b",3.1 Our Notions of Approximation,[0],[0]
if ‖x− x∗‖ ≤,3.1 Our Notions of Approximation,[0],[0]
"ε‖b‖.
Unfortunately, these notions are too strong and impossible to satisfy for matrices that do not have a large eigengap around the projection threshold λ.
",3.1 Our Notions of Approximation,[0],[0]
"In this paper we propose the following more general (but yet very meaningful) approximation notions.
",3.1 Our Notions of Approximation,[0],[0]
Definition 3.1.,3.1 Our Notions of Approximation,[0],[0]
"An algorithm B(χ) is (γ, ε)-approximate PCP for threshold λ, if for every χ ∈ Rd
1. ∥∥P(1+γ)λ(B(χ)− χ)∥∥ ≤ ε‖χ‖.
2.",3.1 Our Notions of Approximation,[0],[0]
"∥∥(I−P(1−γ)λ)B(χ)∥∥ ≤ ε‖χ‖.
3.",3.1 Our Notions of Approximation,[0],[0]
∀i such that λi ∈,3.1 Our Notions of Approximation,[0],[0]
"[ (1 − γ)λ, (1 + γ)λ ] , it satisfies
|〈νi,B(χ)− χ〉| ≤ |〈νi, χ〉|+ ε‖χ‖.
Intuitively, the first property above states that, if projected to the eigenspace with eigenvalues above (1 + γ)λ, then B(χ) and χ are almost identical; the second property states that, if projected to the eigenspace with eigenvalues below (1 − γ)λ, then B(χ) is almost zero; and the third property states that, for each eigenvector νi with eigenvalue in the range [(1− γ)λ, (1 + γ)λ], the projection 〈νi,B(χ)〉 must be between 0 and 〈νi, χ〉 (but up to an error ε‖χ‖).",3.1 Our Notions of Approximation,[0],[0]
"Naturally, Pλ(χ) itself is a (0, 0)-approximate PCP.
",3.1 Our Notions of Approximation,[0],[0]
"We propose the following notion for approximate PCR:
Definition 3.2.",3.1 Our Notions of Approximation,[0],[0]
"An algorithm C(b) is (γ, ε)-approximate PCR for threshold λ, if for every b ∈ Rd′
1.",3.1 Our Notions of Approximation,[0],[0]
∥∥(I−P(1−γ)λ)C(b)∥∥ ≤,3.1 Our Notions of Approximation,[0],[0]
"ε‖b‖.
2.",3.1 Our Notions of Approximation,[0],[0]
‖AC(b)− b‖ ≤ ‖Ax∗,3.1 Our Notions of Approximation,[0],[0]
"− b‖+ ε‖b‖. where x∗ = (A>A)†P(1+γ)λA>b is the exact PCR solution for threshold (1 + γ)λ.
",3.1 Our Notions of Approximation,[0],[0]
"The first notion states that the output x = C(b) has nearly no correlation with eigenvectors below threshold (1− γ)λ; and the second states that the regression error should be nearly optimal with respect to the exact PCR solution but at a different threshold (1 + γ)λ.
",3.1 Our Notions of Approximation,[0],[0]
"Relationship to Frostig et al. Under eigengap assumption, our notions are equivalent to Frostig et al.:",3.1 Our Notions of Approximation,[0],[0]
Fact 3.3.,3.1 Our Notions of Approximation,[0],[0]
"If A has no singular value in [ √ λ(1− γ), √ λ(1 + γ)], then
• Def. 3.1 is equivalent to ‖B(χ)−Pλ(χ)‖ ≤",3.1 Our Notions of Approximation,[0],[0]
"O(ε)‖χ‖.
• Def.",3.1 Our Notions of Approximation,[0],[0]
3.2 implies ‖C(χ),3.1 Our Notions of Approximation,[0],[0]
"− x∗‖ ≤ O(ε/λ)‖b‖ and ‖C(χ)− x∗‖ ≤ O(ε)‖b‖ implies Def. 3.2.
",3.1 Our Notions of Approximation,[0],[0]
"Above, x∗ = (A>A)†PλA>b is the exact PCR solution.",3.1 Our Notions of Approximation,[0],[0]
"If the PCP solution ξ = Pλ(A>b) is computed exactly, then by definition one can compute (A>A)†ξ which gives a solution to PCR by solving a linear system.",3.2 Reductions from PCR to PCP,[0],[0]
"However, as pointed by Frostig et al. (Frostig et al., 2016), this computation is problematic if ξ is only approximate.",3.2 Reductions from PCR to PCP,[0],[0]
"The following approach has been proposed to improve its accuracy by Frostig et al.
• “compute p((A>",3.2 Reductions from PCR to PCP,[0],[0]
A + λI)−1)ξ where p(x) is a polynomial that approximates function,3.2 Reductions from PCR to PCP,[0],[0]
x1−λx .”,3.2 Reductions from PCR to PCP,[0],[0]
"This is a good approximation to (A>A)†ξ because the composition of functions x1−λx and 1 1+λx is exactly x
−1.",3.2 Reductions from PCR to PCP,[0],[0]
"Frostig et al. picked p(x) = pm(x) = ∑m t=1 λ
t−1xt which is a truncated Taylor series, and used the following procedure to compute sm",3.2 Reductions from PCR to PCP,[0],[0]
≈,3.2 Reductions from PCR to PCP,[0],[0]
pm((A>,3.2 Reductions from PCR to PCP,[0],[0]
"A + λI)−1)ξ:
s0 = B(A>b), s1 = ApxRidge(A, λ, s0), ∀k",3.2 Reductions from PCR to PCP,[0],[0]
"≥ 1: sk+1 = s1 + λ · ApxRidge(A, λ, sk) .",3.2 Reductions from PCR to PCP,[0],[0]
"(3.1)
Above, B is an approximate PCP solver and ApxRidge is an approximate ridge regression solver.",3.2 Reductions from PCR to PCP,[0],[0]
"Under eigengap assumption, Frostig et al. (Frostig et al., 2016) showed
Lemma 3.4 (PCR-to-PCP).",3.2 Reductions from PCR to PCP,[0],[0]
"For fixed λ, γ, ε ∈ (0, 1), let A be a matrix whose singular values lie in[ 0, √ (1− γ)λ ] ∪ [√ (1− γ)λ, 1 ] .",3.2 Reductions from PCR to PCP,[0],[0]
"Let ApxRidge be any O( εm2 )-approximate ridge regression solver, and let B be any (γ,O( ελm2 ))",3.2 Reductions from PCR to PCP,[0],[0]
"-approximate PCP solver
7.",3.2 Reductions from PCR to PCP,[0],[0]
"Then, procedure (3.1) satisfies
‖sm−(A>A)†PλA>b‖ ≤",3.2 Reductions from PCR to PCP,[0],[0]
"ε‖b‖ if m = Θ(log(1/εγ)) .
",3.2 Reductions from PCR to PCP,[0],[0]
"Unfortunately, the above lemma does not hold without eigengap assumption.",3.2 Reductions from PCR to PCP,[0],[0]
"In this paper, we fix this issue by proving the following analogous lemma:
Lemma 3.5 (gap free PCR-to-PCP).",3.2 Reductions from PCR to PCP,[0],[0]
"For fixed λ, ε ∈ (0, 1) and γ ∈ (0, 2/3], let A be a matrix whose singular values are no more than 1.",3.2 Reductions from PCR to PCP,[0],[0]
"Let ApxRidge be any O( εm2 )-approximate ridge regression solver, and B be any (γ,O( ελm2 ))",3.2 Reductions from PCR to PCP,[0],[0]
-approximate PCP solver.,3.2 Reductions from PCR to PCP,[0],[0]
"Then, procedure (3.1) satisfies,{
‖(I−P(1−γ)λ)sm‖ ≤",3.2 Reductions from PCR to PCP,[0],[0]
"ε‖b‖ , and
‖Asm − b‖ ≤ ‖A(A>A)†P(1+γ)λA>b− b‖+ ε‖b‖ } if m = Θ(log(1/εγ)
",3.2 Reductions from PCR to PCP,[0],[0]
Note that the conclusion of this lemma exactly corresponds to the two properties in our Def. 3.2.,3.2 Reductions from PCR to PCP,[0],[0]
"The proof of Lemma 3.5 is not hard, but requires a very careful case analysis by decomposing vectors b and each sk into three components, each corresponding to eigenvalues of A>A in the range",3.2 Reductions from PCR to PCP,[0],[0]
"[0, (1−γ)λ], [(1−γ)λ, (1+γ)λ] and [(1+γ)λ, 1].
7Recall from Fact 3.3 that this requirement is equivalent to saying that ‖B(χ)−Pλχ‖ ≤",3.2 Reductions from PCR to PCP,[0],[0]
"O( ε √ λ
m2 )‖χ‖.
We defer the details to the full version.",3.2 Reductions from PCR to PCP,[0],[0]
"Classical Chebyshev approximation theory (such as Lemma 2.10) only talks about the behaviors of pn(x) or gn(x) on interval [−1, 1].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"However, for the purpose of this paper, we must also bound its value for x > 1.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"We prove the following general lemma in the full version, and believe it could be of independent interest: (we denote by f (k)(x) the k-th derivative of f at x)
Lemma 4.1.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Suppose f(z) is analytic on Eρ and for every k ≥ 0, f (k)(0) ≥ 0.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Then, for every n ∈ N, letting pn(x) and qn(x) be be the degree-n Chebyshev truncated series and Chebyshev interpolation of f(x), we have
∀y ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[0, ρ] : 0 ≤ pn(1 + y), qn(1 + y) ≤","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"f(1 + y) .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
5 Our Polynomial Approximation of sgn(x),"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"For fixed κ ∈ (0, 1], we consider the degree-n Chebyshev interpolation qn(x) = ∑n k=0 ckTk(x) of the function
f(x) =","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"(
1+κ−x 2
)−1/2 on [−1, 1].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Def. 2.7 tells us that
ck := 2− 1[k = 0]
n+ 1
n∑ j=0","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"(√ 2 cos (k(j + 0.5)π n+ 1 )) × ( 1 + κ− cos ( (j + 0.5)π
n+ 1
))−1/2 .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Our final polynomial to approximate sgn(x) is therefore
gn(x) = x·qn(1+κ−2x2) and deg(gn(x))","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"= 2n+1 .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"We prove the following theorem in this section:
Theorem 5.1.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"For every α ∈ (0, 1], ε ∈ (0, 1/2), choosing κ = 2α2, our function gn(x) :","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"= x · qn(1 + κ− 2x2) satisfies that as long as n ≥ 1√
2α log 3εα2 , then (see also
Figure 1)
• |gn(x)− sgn(x)| ≤ ε for every x ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−1, α] ∪ [α, 1].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
• gn(x) ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[0, 1] for every x ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[0, α] and gn(x) ∈
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−1, 0] for every x ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−α, 0].
Note that our degree n =","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"O ( α−1 log(1/αε) ) is nearoptimal, because the minimum degree for a polynomial to satisfy even only the first item is Θ ( α−1 log(1/ε) )","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"(Eremenko & Yuditskii, 2007; 2011).","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"However, the results of (Eremenko & Yuditskii, 2007; 2011) are not constructive, and thus may not lead to stable matrix polynomials.
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
We prove Theorem 5.1 by first establishing two simple lemmas.,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"The following lemma is a consequence of Lemma 2.10:
Lemma 5.2.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"For every ε ∈ (0, 1/2) and κ ∈ (0, 1], if n ≥ 1√
κ ( log 1κ + log 4 ε ) then
∀x ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−1, 1], |f(x)− qn(x)| ≤ ε .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
Proof of Lemma 5.2.,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Denoting by f(z) = (
1+κ−z 2
)−0.5 ,
we know that f(z) is analytic on ellipse Eρ with ρ = κ/2, and it satisfies |f(z)| ≤ √ 2/κ in Eρ.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Applying Lemma 2.10, we know that when n ≥ 1√ κ ( log 1κ + log 4 ε
)","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"it satisfies |f(x)− qn(x)| ≤ ε.
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"The next lemma an immediate consequence of our Lemma 4.1 with f(z) = ( 1+κ−z
2
)−0.5","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
":
Lemma 5.3.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"For every ε ∈ (0, 1/2), κ ∈ (0, 1], n ∈ N, and x ∈","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[0, κ], we have
0 ≤ qn(1 + x) ≤ (κ− x
2
)−1/2 .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
Proof of Theorem 5.1.,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"We are now ready to prove Theorem 5.1.
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
•,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
When x ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−1, α] ∪ [α, 1], it satisfies 1 + κ","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
− 2x2 ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−1, 1].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Therefore, applying Lemma 5.2 we have whenever n ≥ 1√
κ log 6εκ = 1√ 2α log 3εα2 it satisfies |f(1 +","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
κ−2x2)−qn(1+κ−2x2)|∞ ≤ ε.,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"This further implies
|gn(x)−sgn(x)| = |xqn(1+κ−2x2)−xf(1+κ−2x2)| ≤ |x||f(1","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
+,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"κ− 2x2)− qn(1 + κ− 2x2)| ≤ ε .
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
•,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
When |x| ≤,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"α, it satisfies 1 + κ","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
− 2x2 ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[1, 1 + κ].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
Applying Lemma 5.3 we have for all x ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[0, α], 0 ≤ gn(x) = x · qn(1 + κ− 2x2)","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
≤,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
x · (x2)−1/2 = 1 and similarly for x ∈,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"[−α, 0]","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
it satisfies 0,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"≥ gn(x) ≥ −1.
","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
A Bound on Chebyshev Coefficients.,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
We also give an upper bound to the coefficients of polynomial qn(x).,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Its proof can be found in the full version, and this upper bound shall be used in our final stability analysis.","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
Lemma 5.4 (coefficients of qn).,"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Let qn(x) =∑n k=0 ckTk(x) be the degree-n Chebyshev interpolation
of f(x) = (
1+κ−x 2
)−1/2 on [−1, 1].","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"Then, for all i ∈
{0, 1, . . .","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
",","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
n},"4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
", |ci| ≤ e √ 32(i+ 1)
κ
( 1 + κ+ √ 2κ+ κ2 )−i","4 Chebyshev Approximation Outside [−1, 1]",[0],[0]
"In this section we show that any polynomial that is a weighted summation of Chebyshev polynomials with bounded coefficients, can be stably computed when applied to matrices with approximate computations.",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"We achieve so by first generalizing Clenshaw’s backward method to matrix case in Section 6.1 in order to compute a matrix variant of Chebyshev sum, and then analyze its stability in Section 6.2 with the help from Elloit’s forward-backward transformation (Elliott, 1968).
",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
Remark 6.1.,6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"We wish to point out that although Chebyshev polynomials are known to be stable under error when computed on scalars (Gil et al., 2007), it is not immediately clear why it holds also for matrices.",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
Recall that Chebyshev polynomials satisfy Tn+1(x) = 2xTn(x),6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
− Tn−1(x).,6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"In the matrix case, we have Tn+1(M)χ = 2MTn(M)χ − Tn−1(M)χ where χ ∈ Rd is a vector.",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"If we analyzed this formula coordinate by coordinate, error could blow up by a factor d per iteration.
",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"In addition, we need to ensure that the stability theorem holds for matrices M with eigenvalues that can exceed 1.",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
This is not standard because Chebyshev polynomials are typically analyzed only on domain,6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"[−1, 1].",6 Stable Computation of Matrix Chebyshev Polynomials,[0],[0]
"Consider any computation of the form
~sN := N∑ k=0",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"Tk(M)~ck ∈ Rd (6.1)
where M ∈ Rd×d is symmetric and each ~ck is in Rd.",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"(Note that for PCP and PCR purposes, we it suffices to consider ~ck = c ′ kχ where c ′",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
k ∈ R is a scalar and χ ∈ Rd is a fixed vector for all k.,6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"However, we need to work on this more general form for our stability analysis.)
",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"Vector sN can be computed using the following procedure:
Lemma 6.2 (backward recurrence). ~sN",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"= ~b0−M~b1 where ~bN+1 := ~0, ~bN := ~cN , and
∀r ∈ {N − 1, . . .",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
", 0} : ~br := 2M~br+1 −~br+2 + ~cr ∈ Rd .",6.1 Clenshaw’s Method in Matrix Form,[0],[0]
"We show that, if implemented using the backward recurrence formula, the Chebyshev sum of (6.1) can be stably computed.",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"We define the following model to capture the error with respect to matrix-vector multiplications.
",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
Definition 6.3 (inexact backward recurrence).,6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
Let M be an approximate algorithm that satisfies ‖M(u)−Mu‖2 ≤ ε‖u‖2 for every u ∈ Rd.,6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"Then, define inexact backward recurrence to be
b̂N+1 := 0, b̂N := ~cN , and ∀r ∈ {N − 1, . . .",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
", 0} : b̂r := 2M ( b̂r+1 ) −",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"b̂r+2 + ~cr ∈ Rd ,
and define the output as ŝN := b̂0 −M(̂b1).
",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
The following theorem gives an error analysis to our inexact backward recurrence.,6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"We prove it in full version, and the main idea of our proof is to convert each error vector of a recursion of the backward procedure into an error vector corresponding to some original ~ck.
",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
Theorem 6.4 (stable Chebyshev sum).,6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"For every N ∈ N∗, suppose the eigenvalues of M are in [a, b] and suppose there are parameters CU ≥ 1, CT ≥ 1, ρ ≥ 1, Cc ≥
0 satisfying ∀k ∈ {0, 1, . . .",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
", N} :{ ρk‖~ck‖ ≤",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
Cc ∧ ∀x ∈,6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"[a, b] : |Tk(x)|≤CT ρ",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"k
|Uk(x)|≤CUρk
} .
",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"Then, if the inexact backward recurrence in Def.",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
"6.3 is applied with ε ≤ 14NCU , we have
‖ŝN − ~sN‖ ≤ ε · 2(1 + 2NCT )NCUCc .",6.2 Inexact Clenshaw’s Method in Matrix Form,[0],[0]
We are now ready to state our main theorems for PCP and PCR.,7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"We first note a simple fact:
Fact 7.1.",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
(Pλ)χ = I+sgn(S)2 where S := 2(A >,7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
A + λI)−1A>A−,7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"I = (A>A + λI)−1(A>A− λI).
",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"In other words, for every vector χ ∈ Rd, the exact PCP solution Pλ(χ) is the same as computing (Pλ)χ = I+sgn(S)
2 χ.",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"Thus, we can use our polynomial gn(x) introduced in Section 5 and compute gn(S)χ ≈ sgn(S)χ.",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"Finally, in order to compute gn(S), we need to multiply S to deg(gn) vectors; whenever we do so, we call perform ridge regression once.
",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"Since the high-level structure of our PCP algorithm is very clear, due to space limitation, we present the pseudocodes of our PCP and PCR algorithms in the full version.",7 Algorithms and Main Theorems for PCP and PCR,[0],[0]
"We first state our main theorem under the eigengap assumption, in order to provide a direct comparison to that of Frostig et al. (Frostig et al., 2016).
",7.1 Our Main Theorems,[0],[0]
Theorem 7.2 (eigengap assumption).,7.1 Our Main Theorems,[0],[0]
"Given A ∈ Rd′×d and λ, γ ∈ (0, 1), assume that the singular values of A are in the range [0, √ (1− γ)λ]∪ [ √ (1 + γ)λ, 1].",7.1 Our Main Theorems,[0],[0]
"Given χ ∈ Rd and b ∈ Rd′ , denote by ξ∗ = Pλχ and x∗ =",7.1 Our Main Theorems,[0],[0]
"(A>A)−1PλA>b
the exact PCP and PCR solutions, and by ApxRidge any ε′-approximate ridge regression solver.",7.1 Our Main Theorems,[0],[0]
"Then,
• QuickPCP outputs ξ satisfying ‖ξ∗ − ξ‖ ≤ ε‖χ‖ with O ( γ−1 log 1γε )",7.1 Our Main Theorems,[0],[0]
"oracle calls to ApxRidge as long as
log(1/ε′) =",7.1 Our Main Theorems,[0],[0]
"Θ ( log 1γε ) .
",7.1 Our Main Theorems,[0],[0]
• QuickPCR outputs x satisfying ‖x−x∗‖ ≤ ε‖b‖ with O ( γ−1 log 1γλε ),7.1 Our Main Theorems,[0],[0]
"oracle calls to ApxRidge, as long as
log(1/ε′) =",7.1 Our Main Theorems,[0],[0]
"Θ ( log 1γλε ) .
",7.1 Our Main Theorems,[0],[0]
"In contrast, the number of ridge-regression oracle calls was Θ(γ−2 log 1γε ) for PCP and Θ(γ
−2 log 1γλε ) for PCR in (Frostig et al., 2016).",7.1 Our Main Theorems,[0],[0]
"We include the proof of Theorem 7.2 in the full version.
",7.1 Our Main Theorems,[0],[0]
"We state our theorem without the eigengap assumption.
",7.1 Our Main Theorems,[0],[0]
Theorem 7.3 (gap-free).,7.1 Our Main Theorems,[0],[0]
"Given A ∈ Rd′×d, λ ∈ (0, 1), and γ ∈ (0, 2/3], assume that ‖A‖2 ≤ 1.",7.1 Our Main Theorems,[0],[0]
"Given χ ∈ Rd and b ∈ Rd′ , and suppose ApxRidge is an ε′approximate ridge regression solver, then
• QuickPCP outputs ξ that is (γ, ε)-approximate PCP withO ( γ−1 log 1γε )",7.1 Our Main Theorems,[0],[0]
"oracle calls to ApxRidge as long
as log(1/ε′) =",7.1 Our Main Theorems,[0],[0]
"Θ ( log 1γε ) .
",7.1 Our Main Theorems,[0],[0]
"• QuickPCR outputs x that is (γ, ε)-approximate PCR with O",7.1 Our Main Theorems,[0],[0]
( γ−1 log 1γλε ),7.1 Our Main Theorems,[0],[0]
"oracle calls to ApxRidge as
long as elog(1/ε′) =",7.1 Our Main Theorems,[0],[0]
"Θ ( log 1γλε ) .
",7.1 Our Main Theorems,[0],[0]
We make a final remark here regarding the practical usage of QuickPCP and QuickPCR.,7.1 Our Main Theorems,[0],[0]
Remark 7.4.,7.1 Our Main Theorems,[0],[0]
"Since our theory is for (γ, ε)-approximations that have two parameters, the user in principle has to feed in both γ and n where n is the degree of the polynomial approximation to the sign function.",7.1 Our Main Theorems,[0],[0]
"In practice, however, it is usually sufficient to obtain (ε, ε)-approximate PCP and PCR.",7.1 Our Main Theorems,[0],[0]
"Therefore, our pseudocodes allow users to set γ = 0 and thus ignore this parameter γ; in such a case, we shall use γ = log(n)/n which is equivalent to setting γ = Θ(ε) because n = Θ(γ−1 log(1/γε)).",7.1 Our Main Theorems,[0],[0]
We provide empirical evaluations in the full version of this paper.,8 Experiments,[0],[0]
"We summarize our contributions.
",9 Conclusion,[0],[0]
• We put forward approximate notions for PCP and PCR that do not rely on any eigengap assumption.,9 Conclusion,[0],[0]
Our notions reduce to standard ones under the eigengap assumption.,9 Conclusion,[0],[0]
• We design near-optimal polynomial approximation g(x) to sgn(x) satisfying (1.1) and (1.2).,9 Conclusion,[0],[0]
"• We develop general stable recurrence formula for matrix Chebyshev polynomials; as a corollary, our g(x) can be applied to matrices in a stable manner.",9 Conclusion,[0],[0]
"• We obtain faster, provable PCA-free algorithms for PCP and PCR than known results.",9 Conclusion,[0],[0]
"We solve principal component regression (PCR), up to a multiplicative accuracy 1+γ, by reducing the problem to Õ(γ−1) black-box calls of ridge regression.",abstractText,[0],[0]
"Therefore, our algorithm does not require any explicit construction of the top principal components, and is suitable for large-scale PCR instances.",abstractText,[0],[0]
"In contrast, previous result requires Õ(γ−2) such black-box calls.",abstractText,[0],[0]
"We obtain this result by developing a general stable recurrence formula for matrix Chebyshev polynomials, and a degree-optimal polynomial approximation to the matrix sign function.",abstractText,[0],[0]
"Our techniques may be of independent interests, especially when designing iterative methods.",abstractText,[0],[0]
Faster Principal Component Regression  and Stable Matrix Chebyshev Approximation,title,[0],[0]
Pure exploration multi-armed bandit (MAB) problems provide a framework for determining via a sequential experiment which of a set of distributions meet some criteria.,1. Introduction,[0],[0]
"In this setting, there are K distributions ν1, . . .",1. Introduction,[0],[0]
", νK and the agent sequentially chooses from which distribution to sample an observation.",1. Introduction,[0],[0]
"At the end of the sampling stage, the agent outputs the distributions which he believes meet the desired criteria and the performance of the agent is measured based on the quality of this decision.",1. Introduction,[0],[0]
"In the MAB literature, distributions are also referred to as arms, and sampling a realization from a distribution νi is referred to as pulling arm i.",1. Introduction,[0],[0]
The most well-studied of these problems is top-k arm identification.,1. Introduction,[0],[0]
"In this problem, the goal is to find the k best arms, that is, k arms with the largest means.",1. Introduction,[0],[0]
"This problem and other pure exploration problems have applications in a wide range of areas, including crowdsourcing, A/B testing, and online advertising.
",1. Introduction,[0],[0]
"In many application domains, the arms and the criteria for
1Department of Computer Science and Electrical Engineering, University of Michigan.",1. Introduction,[0],[0]
"Correspondence to: Julian Katz-Samuels <jkatzsam@umich.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
a good arm are multi-dimensional in nature.,1. Introduction,[0],[0]
"For example, in crowdsourcing it is important to distinguish good workers from bad workers.",1. Introduction,[0],[0]
"For a multilabel classification task (where examples are associated with multiple labels), a worker can be modeled as a multi-dimensional arm where each dimension corresponds to her accuracy at identifying a particular label, and a natural definition for a “good worker” is that her accuracy is above some threshold for each label (e.g., 90%).",1. Introduction,[0],[0]
A common approach for finding such workers is to use a collection of examples labeled by domain experts as a set of tests.,1. Introduction,[0],[0]
"Since workers are paid for each example that they label, often an organization is only willing to spend a limited number of queries to find good workers and an effective method under this budget constraint is needed.
",1. Introduction,[0],[0]
"As another example, consider A/B testing for designing products such as websites, ads, and video games.",1. Introduction,[0],[0]
"In this setting, there are several options for a product and a company diverts traffic to each of the options to determine which one to choose.",1. Introduction,[0],[0]
"Multi-dimensional criteria arise naturally in this domain, as well.",1. Introduction,[0],[0]
"For example, a company that wants to grow its user base for its website might desire the rate of new subscriptions to be above some level, while still maintaining a certain level of user retention among its current users.",1. Introduction,[0],[0]
"If the product is a video game, the company might also be interested in maintaining some metric of user engagement above a certain threshold.
",1. Introduction,[0],[0]
The pure exploration MAB literature lacks (i) a simple framework for describing problems where the arms and criteria are multi-dimensional and (ii) practical algorithms for addressing these problems.,1. Introduction,[0],[0]
"In this paper, we aim to address this gap.",1. Introduction,[0],[0]
We introduce the feasible arm identification problem in which arms are associated with multi-dimensional distributions and the goal is to find arms whose means belong to a given polyhedron1 P “ tx :,1. Introduction,[0],[0]
Ax ď bu.,1. Introduction,[0],[0]
"Polyhedra encompass a large class of regions that can model common user-defined constraints, including thresholds or ranges on individual dimensions and linear constraints involving multiple dimensions.",1. Introduction,[0],[0]
We propose several algorithms for the fixed budget setting and provide upper and lower bounds.,1. Introduction,[0],[0]
"Finally, we demonstrate through experiments on synthetic and real-world datasets that by leveraging the geometry of the
1There are several conflicting definitions of polyhedra.",1. Introduction,[0],[0]
"We define a polyhedron as the intersection of a finite number of closed halfspaces (Boyd and Vandenberghe, 2004).
problem, our methods significantly outperform a uniform allocation strategy.",1. Introduction,[0],[0]
"Indeed, in several of our experiments, our methods find the feasible arms with a probability that is a factor of 10 better than that of a uniform allocation strategy.",1. Introduction,[0],[0]
All proofs are contained in the supplementary material.,1. Introduction,[0],[0]
MABs have received a significant amount of attention.,2. Related Work,[0],[0]
Most work considers minimizing the cumulative regret instead of a pure exploration objective.,2. Related Work,[0],[0]
"There have been relatively few works on multi-dimensional arms and criteria in this regime (Drugan and Now, 2013; Busa-Fekete et al., 2017; Tekin and Turgay, 2017).",2. Related Work,[0],[0]
Drugan and Now (2013) modify a UCB algorithm to find all arms on the Pareto front.,2. Related Work,[0],[0]
Busa-Fekete et al. (2017) use the Generalized Gini Index to optimize all objectives in a fair way.,2. Related Work,[0],[0]
"Tekin and Turgay (2017) consider a contextual MAB setting where the goal is to maximize the total reward in a non-dominant objective, subject to the constraint that the total reward in a dominant objective is maximized.",2. Related Work,[0],[0]
"These works differ from our work in that (i) they consider the cumulative regret setting, which is fundamentally different from the pure exploration setting (Bubeck et al., 2009), and (ii) they aim to either balance multiple objective functions or find arms on the Pareto front, whereas we aim to find feasible arms, where feasibility is defined by membership in a given polyhedron.
",2. Related Work,[0],[0]
"In recent years, there have been many advances in pure exploration MABs in the fixed confidence and fixed budget settings (Mannor and Tistisklis, 2004; Gabillon et al., 2012; Bubeck et al., 2013; Chen et al., 2014; Jamieson et al., 2014).",2. Related Work,[0],[0]
A limited number of works have considered multi-dimensional feedback.,2. Related Work,[0],[0]
"Auer et al. (2016) considered a variant of the top arm identification problem where arms are multi-dimensional with each dimension corresponding to a distinct objective that an agent wishes to optimize, and the goal is to identify the Pareto front of the arms.",2. Related Work,[0],[0]
"In contrast to our work, they consider the fixed confidence setting.",2. Related Work,[0],[0]
"More importantly, Pareto front identification and feasible arm identification are mathematically very different problems and apply to distinct situations.",2. Related Work,[0],[0]
"Whereas Pareto front identification is relevant to multi-objective optimization, the feasible arm identification problem is useful for situations where there are user-defined criteria for what qualifies as a good arm.
",2. Related Work,[0],[0]
"Chen et al. (2017) recently proposed the general sampling problem, which can model a setting where arms are multidimensional and the goal is to find arms with means belonging to a given polyhedron.",2. Related Work,[0],[0]
There are several major differences with our work.,2. Related Work,[0],[0]
"First, Chen et al. (2017) do not consider multi-dimensional feedback as the agent can sample from one dimension of one arm at a time.",2. Related Work,[0],[0]
"Second, whereas they study the fixed confidence setting, we
study the fixed budget setting.",2. Related Work,[0],[0]
"Third, they assume isotropic Gaussian arms, whereas we assume each arm is associated with a multi-dimensional sub-Gaussian distribution.",2. Related Work,[0],[0]
"Finally, their proposed algorithm (see their Algorithm 7) is sampleinefficient and impractical since in its first stage, it employs a uniform allocation strategy until the confidence bounds (defined with δ “ 0.01) of all of the means either intersect with the given polyhedron or do not intersect with the given polyhedron.
",2. Related Work,[0],[0]
"Locatelli et al. (2016) introduced the thresholding bandit problem (TBP), which is essentially the scalar version of the feasible arm identification problem, and the algorithm APT.",2. Related Work,[0],[0]
"In TBP, there are K scalar-valued distributions, a threshold τ , and a budget T .",2. Related Work,[0],[0]
The goal is to identify all of the distributions with means above τ .,2. Related Work,[0],[0]
Our work significantly generalizes TBP by considering multi-dimensional arms and the problem of identifying those arms with means belonging to a given polyhedron.,2. Related Work,[0],[0]
"Unlike Locatelli et al. (2016) who only analyze APT, we provide an unified analysis of three algorithms for the feasible arm identification problem.",2. Related Work,[0],[0]
"One of our algorithm, MD-APT, reduces to APT in the onedimensional thresholding case and our upper bound also reduces to the upper bound of APT (up to constant factors).",2. Related Work,[0],[0]
"To deal with this general setting, we introduce a novel complexity measure that characterizes the hardness of determining whether an arm is in P .",2. Related Work,[0],[0]
This measure is essentially the distance of the mean of an arm to the boundary of the polyhedron.,2. Related Work,[0],[0]
"In addition, our general setting introduces technical challenges for establishing upper and lower bounds.",2. Related Work,[0],[0]
"We overcome these by using tools from convex analysis, properties of multi-dimensional sub-Gaussian distributions, and change-of-measure arguments involving multi-dimensional distributions.
",2. Related Work,[0],[0]
"Recently, Zheng et al. (2017) considered a problem with a polyhedral constraint, but their setup is very different from our own.",2. Related Work,[0],[0]
"In their setting, the goal is to solve a linear program where either the constraints are not fully known or the cost function is not fully known but can be estimated by adaptive sampling.",2. Related Work,[0],[0]
"In our work, the constraints are known and we wish to learn which out of a collection of distributions have feasible means.",2. Related Work,[0],[0]
"In this section, we formalize the feasible arm identification problem.",3. Setup,[0],[0]
"To begin, we define some notation.",3. Setup,[0],[0]
"For all n P N, let rns “ t1, . . .",3. Setup,[0],[0]
", nu.",3. Setup,[0],[0]
"For any x P RD and A Ă RD, let distpx, Aq “ infyPA }x´ y}2.",3. Setup,[0],[0]
"Let 1 “ p1, . . .",3. Setup,[0],[0]
", 1qt P RD and 1t¨u denote the indicator function.",3. Setup,[0],[0]
"Define SD´1 “ tx P RD : }x}2 “ 1u.
",3. Setup,[0],[0]
Suppose we are given K stochastic arms.,3. Setup,[0],[0]
"When the ith arm is pulled, a reward is drawn i.i.d.",3. Setup,[0],[0]
"from a D-dimensional
distribution νi.",3. Setup,[0],[0]
Denote µi “ EX„νiX .,3. Setup,[0],[0]
We assume that the agent is given a polyhedron P “ tx :,3. Setup,[0],[0]
Ax ď bu where A P RMˆD and b P RM .,3. Setup,[0],[0]
"Let atj denote the jth row of A. By dividing each constraint by }aj}2, we can assume without loss of generality that }aj}2 “ 1 for all j P rM s. Let BP denote the boundary of P , i.e., BP “ sP zP ˝ where sP denotes the closure of P and P ˝ denotes the interior of P .",3. Setup,[0],[0]
"For simplicity, we assume that P has positive volume.
",3. Setup,[0],[0]
We consider the fixed budget setting.,3. Setup,[0],[0]
"The game is as follows: there are T rounds and at each round t, the agent chooses an arm It P rKs and observes a realization Xt „ νIt .",3. Setup,[0],[0]
The goal is to identify all of the arms whose means belong to the polyhedron.,3. Setup,[0],[0]
"To define a performance measure, let ą 0 denote the tolerance, and define SintP, – ti P rKs : µi P P and distpµi, BP q ě u and SoutP, – ti P rKs : distpµi, P q ą u. SintP, is the set of arms that lie in the interior of P by at least and SoutP, is the set of arms that lie outside of P by at least .",3. Setup,[0],[0]
Let pS Ă rKs denote the set of arms outputted by an algorithm.,3. Setup,[0],[0]
"We define the following error measure:
LT,P, ppSq – 1tpS X SoutP, ‰ H_ pSc X SintP, ‰ Hu
",3. Setup,[0],[0]
"In words, the goal is to identify all of the arms with means belonging to the polyhedron up to tolerance in the sense that an algorithm is successful if its output includes every arm i such that µi P P and distpµi, BP q ě and excludes every arm l such that distpµl, P q ą .
",3. Setup,[0],[0]
"We define the margin of arm i as
∆ pP, q i – distpµi, BP q `
“ "" minjPrMs distpµi, tx : atjx “ bjuq ` : µi P P distpµi, P q",3. Setup,[0],[0]
"` : µi R P
(1)
“ "" minjPrMs bj ´ atjµi ` : µi P P distpµi, P q",3. Setup,[0],[0]
"` : µi R P
(2)
where line (1) follows by Lemma H.1 and line (2) follows by the closed form solution of the distance from a point to a hyperplane and }aj}2 “ 1 (Boyd and Vandenberghe, 2004).
",3. Setup,[0],[0]
"The complexity of an instance of the feasible arm identification problem is defined to be:
HP, – ÿ iPrKs r∆pP, qi s ´2.
",3. Setup,[0],[0]
"In words, an instance has low complexity if all of the arms are far from the boundary of the polyhedron and high complexity if some of the arms are very close to the boundary.",3. Setup,[0],[0]
The intuition behind this complexity measure is that for an algorithm to output the correct answer about arm,3. Setup,[0],[0]
"i, it is sufficient to guarantee that an estimate pµi is within a ball centered at µi with radius ∆ pP, q",3. Setup,[0],[0]
"i
2 (see Lemma 3).",3. Setup,[0],[0]
"For
the sake of brevity, we usually write LT, ppSq, ∆p qi , and H instead of LT,P, ppSq, ∆pP, qi , and HP, , respectively.
",3. Setup,[0],[0]
"Our analysis assumes that each νi is a multi-dimensional sub-Gaussian distribution, which we now define (see Vershynin et al. (2017) for more details).",3. Setup,[0],[0]
Let X be a scalar random variable.,3. Setup,[0],[0]
"We say that X is R-sub-Gaussian if E exppX 2
R2 q ď 2.",3. Setup,[0],[0]
"We define the sub-Gaussian norm of X as the smallest R that satisfies the above requirement:
}X}ψ2 “ inftR ą 0 :",3. Setup,[0],[0]
"E expp X2
R2 q ď",3. Setup,[0],[0]
"2u.
",3. Setup,[0],[0]
A random vector X P RD is sub-Gaussian if Xta is subGaussian for all a P RD.,3. Setup,[0],[0]
"The sub-Gaussian norm ofX is defined as
}X}ψ2 “ sup aPSD´1
› ›Xta › ›
ψ2 .
",3. Setup,[0],[0]
"We say that a random vector X is R-sub-Gaussian if }X}ψ2 ď R. Henceforth, we assume that ν1, . . .",3. Setup,[0],[0]
", νK are R-sub-Gaussian.",3. Setup,[0],[0]
See Vershynin (2012) for a discussion of sub-Gaussian distributions.,3. Setup,[0],[0]
"In this section, we establish a lower bound for the feasible arm identification problem.",4. Lower Bound,[0],[0]
"Our construction takes any polyhedron P and means µ1, . . .",4. Lower Bound,[0],[0]
",µK P P ˝ and produces a collection of problems such that any algorithm makes a mistake on one of the problems with probability at least on the order of expp´c TH q",4. Lower Bound,[0],[0]
(where c is a constant).,4. Lower Bound,[0],[0]
"In fact, this lower bound holds even when the algorithm is given the distance of each arm to the boundary of the polyhedron.",4. Lower Bound,[0],[0]
"If A Ă RD is closed and x P RD, let ProjApxq denote the projection of x onto A.
Theorem 1.",4. Lower Bound,[0],[0]
Let P,4. Lower Bound,[0],[0]
“ tx P RD :,4. Lower Bound,[0],[0]
"Ax ď bu have positive volume and ě 0 such that P ˝ – tx P P : distpx, BP q ą qu is nonempty.",4. Lower Bound,[0],[0]
"Letµ1, . . .",4. Lower Bound,[0],[0]
",µK P P ˝ , τi P ProjBP pµiq for all i P rKs, and µ1i “ µi ` 2pτi ´ µiq for all i P rKs.",4. Lower Bound,[0],[0]
"Let νi denote the distribution Npµi, Iq and ν1i the distribution Npµ1i, Iq.",4. Lower Bound,[0],[0]
Let B0 denote the product distribution ν1b . .,4. Lower Bound,[0],[0]
.b νK,4. Lower Bound,[0],[0]
"and Bi denote the product distribution
ν1 b . .",4. Lower Bound,[0],[0]
.b,4. Lower Bound,[0],[0]
νi´1 b ν1i b νi`1 b . .,4. Lower Bound,[0],[0]
.b,4. Lower Bound,[0],[0]
"νK .
",4. Lower Bound,[0],[0]
"Then, B0, . . .",4. Lower Bound,[0],[0]
",BK have the same problem complexity
H “ K ÿ
i“1 r distpµi, BP q ` s´2
and for any algorithm,
max iPt0,...,Ku
EBipLT, ppSqq
ě expp´13 T H ´ 25D logp48plogpT q ` 1qKDqqq.
",4. Lower Bound,[0],[0]
This lower bound is equal to the lower bound of Locatelli et al. (2016) (see their Theorem 1) up to the factor of D and constants.,4. Lower Bound,[0],[0]
"Since D logpplogpT q ` 1qKDqq grows very slowly as a function of T in comparison with TH , the dependence on D is quite mild.",4. Lower Bound,[0],[0]
"We also note that the lower bound does not depend on the number of constraints M in the polyhedron P , which suggests that the number of constraints of P does not directly affect the statistical difficulty of the feasible arm identification problem.",4. Lower Bound,[0],[0]
"Since polyhedra approximate convex sets arbitrarily well, the independence of our lower bound from M enables us to derive a nearly identical lower bound for the setting where P is convex (see the supplementary material for details).
",4. Lower Bound,[0],[0]
The proof of Theorem 1 is based on a novel lower bound construction with multidimensional distributions for MABs.,4. Lower Bound,[0],[0]
"Often, lower bounds in the bandit literature modify scalar distributions and the main idea is to perturb the mean of a scalar distribution by making it either larger or smaller.",4. Lower Bound,[0],[0]
"In the feasible arm identification problem, picking a direction to perturb the mean of a distribution is not so simple.",4. Lower Bound,[0],[0]
"Indeed, the direction depends on the polyhedron since for some polyhedra, changing the first coordinate does not produce points lying outside of the polyhedron.",4. Lower Bound,[0],[0]
"In our construction, we interchange a distribution νi with mean µi P P ˝ with a distribution ν1i with mean µ 1 i that is shifted away from µi in the direction of its projection onto the boundary of P .
",4. Lower Bound,[0],[0]
"Theorem 1 also implies the following non-asymptotic minimax bound.
",4. Lower Bound,[0],[0]
Corollary 1.,4. Lower Bound,[0],[0]
Let P,4. Lower Bound,[0],[0]
“ tx P RD :,4. Lower Bound,[0],[0]
"Ax ď bu have positive volume, ě 0 such that P ˝ is nonempty, and R ą 0.",4. Lower Bound,[0],[0]
Let H̃ ą 0,4. Lower Bound,[0],[0]
"such that there exists µ1, . .",4. Lower Bound,[0],[0]
.,4. Lower Bound,[0],[0]
",µK P P ˝ with
H̃ “ K ÿ
i“1 r distpµi, BP q",4. Lower Bound,[0],[0]
"` s´2.
Let BP, ,H̃,R denote the set of feasible arm identification problems on polyhedron P , with tolerance , and with K arms such that the distributions are R-sub-Gaussian and the problem complexity is less than H̃ .",4. Lower Bound,[0],[0]
"Then, T ě 25H̃R2D logp48plogpT q ` 1qKDqq implies that, for any algorithm,
sup BPBP, ,H̃,R
EBpLT, ppSqq ě expp´14 T
H̃R2 q.
",4. Lower Bound,[0],[0]
"In words, this result says essentially that for any polyhedron P and tolerance ě 0, the induced class of feasible arm identification problems with P and has a minimax lower bound on the order of expp´c THR2 q where c is a constant.",4. Lower Bound,[0],[0]
"Henceforth, we say that an algorithm is nearly optimal if for large enough T its expected loss decays as Opexpp´c THR2 qq where c is a constant.",4. Lower Bound,[0],[0]
"In this section, we extend three algorithms to the feasible arm identification problem, namely, an upper confidence bound based algorithm (UCBE) (Audibert and Bubeck, 2010), a successive accepts and rejects algorithm (SAR) (Bubeck et al., 2013; Chen et al., 2014), and the Anytime Parameter-free Thresholding algorithm (APT) (Locatelli et al., 2016).",5. Algorithms,[0],[0]
The main novelty of our approach is that our algorithms estimate the distance of the mean of each arm to the boundary of the polyhedron to decide which arm to pull.,5. Algorithms,[0],[0]
"To begin, we introduce some notation.",5. Algorithms,[0],[0]
"Let It denote the index of the arm chosen at time t. Let Xi,j,t denote the tth realization of the jth coordinate of νi, Tiptq “ řt´1 s“1 1tIs “ iu denote the number of pulls of arm i at round t, and pµi,t denote the estimate of µi after t samples, i.e., pµi,t “ ppµi,1,t, . . .",5. Algorithms,[0],[0]
", pµi,D,tqt where pµi,j,t “ 1t řt",5. Algorithms,[0],[0]
"s“1Xi,j,s.
",5. Algorithms,[0],[0]
"The key quantity in each of these algorithms is the following empirical estimator of the margin of each arm:
p∆ p q",5. Algorithms,[0],[0]
"i,t “
""
minjPrMs bj ´ atj pµi,t ` : pµi,t P P distppµi,t, P q",5. Algorithms,[0],[0]
"` : pµi,t R P
",5. Algorithms,[0],[0]
"Given pµi,t, distppµi,t, P q can be computed by solving a quadratic program and, thus, the interior point method can compute p∆p qi,t in runtime polynomial in M and D. Each of our algorithms updates one p∆p qi,t in each round, thus solving at most T quadratic programs.",5. Algorithms,[0],[0]
"Therefore, each algorithm can be implemented efficiently.
",5. Algorithms,[0],[0]
"Algorithm 1 MD-UCBE: Multi-dimensional Upper Confidence Bound Exploration algorithm
1: Input: K arms, polyhedron P , tolerance , budget T , hyperparameter a 2: for t “ 1, . . .",5. Algorithms,[0],[0]
", T do 3: if t ď K then 4: SampleXt „ νt. 5: else 6: Choose It “ arg mini p∆ p q",5. Algorithms,[0],[0]
"i,Tiptq´ b a Tiptq and sampleXt „ νIt .",5. Algorithms,[0],[0]
"7: end if 8: end for 9: Return: pS “ ti P rKs : pµi,TipT`1q P P u
Next, we describe each of the algorithms and our results.",5. Algorithms,[0],[0]
"Each algorithm outputs pS “ ti P rKs : pµi,TipT`1q P P u.",5. Algorithms,[0],[0]
The algorithms differ in how they decide which arm to pull.,5. Algorithms,[0],[0]
MD-UCBE (Algorithm 1) is a modification of the algorithm UCBE from Audibert and Bubeck (2010).,5. Algorithms,[0],[0]
"At each time step t, it pulls an arm i that minimizes p∆p qi,Tiptq´ b a Tiptq breaking ties arbitrarily where a is a hyperparameter.",5. Algorithms,[0],[0]
"Theorem 2 gives an upper bound on its expected loss.
Theorem 2.",5. Algorithms,[0],[0]
"Let K ě 0, T ě K and ě 0.",5. Algorithms,[0],[0]
Suppose 0 ď a ď 2536 T´K H .,5. Algorithms,[0],[0]
"Then, the expected loss of MD-UCBE satisfies:
ErLT, ppSqs ď 2plogpT q ` 1qK5D expp´ a
1600R2 q.
Paralleling our upper bounds for MD-SAR and MD-APT, this result says that the degree of difficulty of a problem for MD-UCBE depends on H , i.e., the distance of the arms to the boundary of the polyhedron and the tolerance parameter .",5. Algorithms,[0],[0]
"Theorem 2 suggests setting a “ 2536 T´K H , in which case MD-UCBE is nearly optimal.",5. Algorithms,[0],[0]
"One important shortcoming of this algorithm is that H is not known in practice, so it is unclear how to set the hyperparameter",5. Algorithms,[0],[0]
"a. Indeed, in our experiments, we show that the performance of MD-UCBE is highly sensitive to the selection of a.
Algorithm 2 MD-SAR: Multi-dimensional Successive Accepts and Rejects algorithm
1: Input: K arms, polyhedron P , tolerance , budget T 2: Ďlogpxq “ 12` řx i“2 1 i , n0 “ 0, nk “ Q T´K",5. Algorithms,[0],[0]
"ĚlogpKqpK`1´kq U
pk ą 1q 3: Q “ rKs 4: for k “ 1, . . .",5. Algorithms,[0],[0]
",K ´ 1 do 5: Query nk ´ nk´1 samples from all arms i P Q 6: QÐÝ Qz arg maxiPQ p∆ p q",5. Algorithms,[0],[0]
"i,nk 7: end for 8: Return: pS “ ti P rKs : pµi,TipT`1q P P u
MD-SAR (Algorithm 2) extends the SAR algorithm from Bubeck et al. (2013).",5. Algorithms,[0],[0]
It divides the budget T into K ´ 1 rounds.,5. Algorithms,[0],[0]
"In each round, it samples all of the arms belonging to Q Ă rKs the same number of times.",5. Algorithms,[0],[0]
"At the end of each round, it removes from Q an arm i that maximizes p∆ p q",5. Algorithms,[0],[0]
"i,Tiptq.",5. Algorithms,[0],[0]
"Intuitively, MD-SAR stops sampling from an arm i for which there is the least amount of uncertainty about whether µi P P .",5. Algorithms,[0],[0]
Theorem 3 provides an upper bound on the expected loss of MD-SAR.,5. Algorithms,[0],[0]
It depends on a different complexity term that is nevertheless related to H .,5. Algorithms,[0],[0]
Let piq denote the index of the arm with the ith smallest margin so that ∆p qp1q ď ∆ p q p2q ď . . .,5. Algorithms,[0],[0]
ď ∆,5. Algorithms,[0],[0]
p q pKq,5. Algorithms,[0],[0]
"and define the complexity parameter
H2 “ max iPrKs
ir∆p qpiqs ´2.
",5. Algorithms,[0],[0]
"The analysis of Audibert and Bubeck (2010) of the analogous quantities immediately implies that H2 ď H ď logp2KqH2.
",5. Algorithms,[0],[0]
Theorem 3.,5. Algorithms,[0],[0]
"Let K ě 0, T ě K and ě 0.",5. Algorithms,[0],[0]
"Then, the
expected loss of MD-SAR satisfies: ErLT, ppSqs ď
2plogpT q ` 1qK5D expp´ T ´K 1296 logp2KqH2",5. Algorithms,[0],[0]
1,5. Algorithms,[0],[0]
"R2 q
`4K35D expp´ T ´K 512R2H2 q.
Similar to previous results on SAR-type algorithms in the fixed budget setting (Audibert and Bubeck, 2010; Chen et al., 2014), our upper bound on MD-SAR is loose by a factor of logpKq in the exponential.",5. Algorithms,[0],[0]
"While the guarantee is not tight, it has the significant practical advantage over MD-UCBE that it does not involve a difficult-to-tune hyperparameter.",5. Algorithms,[0],[0]
"On the other hand, MD-SAR has the limitation that it needs to know T in advance.
",5. Algorithms,[0],[0]
"Algorithm 3 MD-APT: Multi-dimensional Anytime Parameter-Free Thresholding algorithm
1: Input: K arms, polyhedron P , tolerance , budget T 2: for t “ 1, . . .",5. Algorithms,[0],[0]
", T do 3: if t ď K then 4: SampleXt „ νt. 5: else 6: Choose It “ arg mini p∆ p q",5. Algorithms,[0],[0]
"i,Tiptq a
Tiptq and sampleXt „ νIt .
7: end if 8: end for 9: Return: pS “ ti P rKs : pµi,Tipt`1q P P u
MD-APT (Algorithm 3) is a modification of the APT algorithm in Locatelli et al. (2016).",5. Algorithms,[0],[0]
"After an initialization phase in which it pulls each arm once, at each round t, it pulls an arm i that minimizes p∆p qi,Tiptq a
Tiptq.",5. Algorithms,[0],[0]
"The intuition behind the algorithm is that if the margins ∆p qi were known in advance, then a nearly optimal strategy would allocate samples to the arms proportionally to the r∆p qi s´2s.",5. Algorithms,[0],[0]
"For simplicity, let “ 0; the case ą 0 is not as clear since arms whose distance to the boundary is less than do not need to be sampled at all.",5. Algorithms,[0],[0]
Proposition 1.,5. Algorithms,[0],[0]
Let “ 0.,5. Algorithms,[0],[0]
"A static allocation strategy with a total of T
r∆p qi s2H pulls of the ith arm @i P rKs achieves
ErLT, ppSqs ď 2K5D expp´ 1
8
T
HR2 q.
",5. Algorithms,[0],[0]
"Thus, such a static allocation is nearly optimal.",5. Algorithms,[0],[0]
Since the ∆ p q,5. Algorithms,[0],[0]
"i s are unknown, MD-APT samples the arms proportionally to the estimates rp∆p qi,Tiptqs ´2.",5. Algorithms,[0],[0]
Theorem 4 gives an upper bound on the expected loss of MD-APT.,5. Algorithms,[0],[0]
Theorem 4.,5. Algorithms,[0],[0]
"Let K ě 0, T ě 2K, and ě 0.",5. Algorithms,[0],[0]
"Then, the expected loss of MD-APT satisfies:
ErLT, ppSqs ď 2plogpT q ` 1qK5D expp´ T
1296R2H q.
",5. Algorithms,[0],[0]
This Theorem implies that MD-APT is nearly optimal.,5. Algorithms,[0],[0]
"Further, unlike MD-UCBE, it is parameter-free and, unlike MD-SAR, it is an anytime algorithm in the sense that MDAPT does not require knowledge of the budget T .",5. Algorithms,[0],[0]
"These properties make MD-ADT practical for many applications (Jun and Nowak, 2016).
",5. Algorithms,[0],[0]
"We note that although the runtime of our algorithms depends on M , our upper bounds on their statistical performance are independent of M .",5. Algorithms,[0],[0]
We leverage this result and the fact that one can approximate convex sets arbitrarily well with polyhedra to obtain a computationally inefficient algorithm with nearly the same guarantee as Theorem 4 for the setting where P is convex (see the supplementary material for details).,5. Algorithms,[0],[0]
Our analyses of the three algorithms are unified through a series of lemmas.,6. Analysis,[0],[0]
"The first key idea is a sufficient condition for p∆p qi,t to concentrate around ∆ p q",6. Analysis,[0],[0]
i .,6. Analysis,[0],[0]
Lemma 1 shows that concentration of pµiptq around its mean in the norm sense is sufficient.,6. Analysis,[0],[0]
Lemma 1.,6. Analysis,[0],[0]
"Let γ ą 0, i P rKs, and t P rT s.",6. Analysis,[0],[0]
"If }pµi,t ´ µi}2 ď γ, then
|p∆p qi,t ´∆ p q",6. Analysis,[0],[0]
"i | ď 2γ.
",6. Analysis,[0],[0]
"In the scalar case, concentration of the empirical margin around the true margin often follows by the triangle inequality.",6. Analysis,[0],[0]
"In our setting, because of the more complicated relationship between pµi,t and p∆ p q",6. Analysis,[0],[0]
"i,t such an argument is not sufficient.
",6. Analysis,[0],[0]
"The second key idea is that with an appropriately high probability, pµi,t concentrates around its mean in the norm sense.",6. Analysis,[0],[0]
"The main tools are Hoeffding’s maximal inequality (see Lemma H.2) and an -net, which we now define (Vershynin et al., 2017).",6. Analysis,[0],[0]
Definition 1.,6. Analysis,[0],[0]
Let A Ă RD and ą 0.,6. Analysis,[0],[0]
"N Ă A is an -net of A if @x P A, there exists y P N such that }x´ y}2 ď .",6. Analysis,[0],[0]
Let N Ă A be an -net of A.,6. Analysis,[0],[0]
"We say that N is minimal if, for any other -net",6. Analysis,[0],[0]
"M of A, it holds that |M| ě |N |.",6. Analysis,[0],[0]
Lemma 2.,6. Analysis,[0],[0]
"Let N be a minimal 12 -net on S
D´1.",6. Analysis,[0],[0]
Let ω ą 0.,6. Analysis,[0],[0]
"Define the event
Ξ “ t@i,@y P N ,@r P rT s : |ytppµi.r ´ µiq| ď c ω2
4r u.
Then, on Ξ, for all i P rKs and for all r P rT s,
}pµi.r ´ µi}2 ď c ω2
r
and
PrpΞq ě 1´ 2plogpT q ` 1qK5D expp´ ω 2
16R2 q.
",6. Analysis,[0],[0]
"In effect, Lemma 1 and Lemma 2 together imply that with high probability, (i) pµi,t concentrates around µi in the norm sense and (ii) p∆p qi,t concentrates around ∆ p q",6. Analysis,[0],[0]
"i .
",6. Analysis,[0],[0]
"Finally, the third idea is the simple observation that if for all i P rKs, pµi,t lies in a ball centered at µi with radius ∆ p q",6. Analysis,[0],[0]
"i
2 , then an algorithm does not make a mistake.",6. Analysis,[0],[0]
Lemma 3.,6. Analysis,[0],[0]
"Fix t P rT s and i P rKs and suppose that }pµi,t ´ µi}2 ă 1 2∆ p q",6. Analysis,[0],[0]
i .,6. Analysis,[0],[0]
"Then, Aµi ď b ´ 1 implies that Apµi,t ă b and distpµi, P q ě implies that pµi,t R P .
",6. Analysis,[0],[0]
The analysis of each algorithm then proceeds as follows.,6. Analysis,[0],[0]
"First, suppose some appropriately defined variant of the event Ξ in Lemma 2.",6. Analysis,[0],[0]
"Second, by Lemmas 1 and 2, (i) pµi,t concentrates around µi in the norm sense and (ii) p∆",6. Analysis,[0],[0]
p q,6. Analysis,[0],[0]
"i,t concentrates around ∆p qi .",6. Analysis,[0],[0]
"Given these concentration results, it is shown that each algorithm pulls each arm a sufficient number of times so that Lemma 3 can be applied.",6. Analysis,[0],[0]
"In this section, we conduct experiments on synthetic and real-world datasets.",7. Experiments,[0],[0]
"In addition to the algorithms MDUCBE, MD-SAR, and MD-APT, we consider a uniform allocation algorithm (UA), which samples the arms in a cyclic fashion.",7. Experiments,[0],[0]
We consider the performance of MD-UCBE under four hyperparameter settings ai “ i 2536,7. Experiments,[0],[0]
T´K,7. Experiments,[0],[0]
"H for i P t.1, 1, 10, 100u.",7. Experiments,[0],[0]
Let MD-UCBE[i] denote MD-UCBE with hyperparameter ai.,7. Experiments,[0],[0]
"Note that the larger i is, the more MD-UCBE[i] explores and that our theoretical guarantee in Theorem 2 only covers i ď 1.",7. Experiments,[0],[0]
"To calculate p∆p qi,t , we use the quadratic programming solver in the CVXOPT package for python.",7. Experiments,[0],[0]
We average all experiments over 2000 trials.,7. Experiments,[0],[0]
Each experiment has 20 5-dimensional arms and is run for 2000 time steps.,7.1. Synthetic Experiments,[0],[0]
We use Gaussian distributions with variance 14 .,7.1. Synthetic Experiments,[0],[0]
"For experiments 1, 2, and 3 we use a cube P “ tx P R5 : 0 ď xi ď",7.1. Synthetic Experiments,[0],[0]
1u.,7.1. Synthetic Experiments,[0],[0]
"In experiments 4 and 5, we use more complicated feasibility regions.",7.1. Synthetic Experiments,[0],[0]
"In the following, we say an arm i is irrelevant if the error measure LT, p¨q does not depend on how i is categorized.
",7.1. Synthetic Experiments,[0],[0]
"Experiment 1 (Four Groups with Irrelevant Arms): We set “ 0.075 and use µ0:1 “ p.8qb5, µ2:3 “ p.9qb5, µ4:5 “ p1.1qb5, µ6:7 “ p1.2qb5, µ8 “ p.975qb5, µ9 “ p1.025qb5, µ10:19 “ p.3qb5.",7.1. Synthetic Experiments,[0],[0]
"Note that this problem has two irrelevant arms, µ8 and µ9.
",7.1. Synthetic Experiments,[0],[0]
"Experiment 2 (Four Groups with no Irrelevant Arms): We set “ 0 and useµ0:1 “ p.8qb5,µ2:3 “ p.9qb5,µ4:5 “ p1.1qb5, µ6:7 “ p1.2qb5, µ8 “ p.95qb5, µ9 “ p1.05qb5, µ10:19 “ p.3qb5.",7.1. Synthetic Experiments,[0],[0]
"In comparison to experiment 1, we make it slightly easier to determine whether the arms µ8 and µ9
belong to the polyhedron because otherwise the difficulty of the problem prevents any algorithm from achieving substantial progress after 2000 time steps.
",7.1. Synthetic Experiments,[0],[0]
"Experiment 3 (Linear Progression with Irrelevant Arms): We set “ 0.075 and use µ0:3 “ p.75qb5 ` p0 : 3q ˆ .05, µ4 “ p.975qb5, µ5 “ p1.025qb5, µ6:9 “ p1.25qb5´p0 : 3qˆ .05, µ10:19 “ p1.15qb5.",7.1. Synthetic Experiments,[0],[0]
"Note that this problem has two irrelevant arms, µ4 and µ5.
",7.1. Synthetic Experiments,[0],[0]
Experiment 4 (Four Groups on the Simplex):,7.1. Synthetic Experiments,[0],[0]
"For this experiment, we use P “ tx P R5 : xi ě 0, ř
i xi ď 2u.",7.1. Synthetic Experiments,[0],[0]
We set “ .1.,7.1. Synthetic Experiments,[0],[0]
Let c “ p.2qb5.,7.1. Synthetic Experiments,[0],[0]
"We use µ0:4 “ c, µ5:9 “ 1.85 ¨ c, µ10:14 “ 2.25 ¨ c, and µ15:19 “ 1.95 ¨ c. µ0:9 are good arms, µ10:14 are bad arms, and µ15:19 are irrelevant.
",7.1. Synthetic Experiments,[0],[0]
Experiment 5 (Ordered Polyhedron):,7.1. Synthetic Experiments,[0],[0]
"For this experiment, we use P “ tx P R5 : xi ď xi`1@i P r4su and “ .1.",7.1. Synthetic Experiments,[0],[0]
"We use µ0:3 “ p0, .2, .4, .6, .8qt, µ4:7 “ p.0, .15, .3, .45, .6qt, µ8:11 “ p0, .2, .15, .6, .8sqt, µ12:15 “ p0, .2, .05, .6, .8qt, and µ16:19 “ p0, .2, .4, .2, 0qt.",7.1. Synthetic Experiments,[0],[0]
"The arms µ8:11 are irrelevant.
",7.1. Synthetic Experiments,[0],[0]
The performance of MD-UCBE is very sensitive to the selection of its hyperparameter.,7.1. Synthetic Experiments,[0],[0]
"MD-UCBE[1] and MDUCBE[10] tend to do well, but MD-UCBE[100] explores too much so that it tends to perform only slightly better than UA and MD-UCBE[.1] does not explore enough.",7.1. Synthetic Experiments,[0],[0]
"Although MD-UCBE[.1] has a theoretical guarantee, the constants are too large so that it never makes progress in solving the problems.",7.1. Synthetic Experiments,[0],[0]
"MD-APT performs better than MD-SAR in experiments 1, 4, and 5 and worse than MD-SAR in experiments 2 and 3.",7.1. Synthetic Experiments,[0],[0]
"In experiment 2, MD-APT pulls arm 8, which minimizes ∆p qi , too frequently.",7.1. Synthetic Experiments,[0],[0]
"It pulls arm 8 on average 904.8125 times, whereas MD-SAR more evenly spreads out its pulls, pulling arm 8 on average 317.751 times.",7.1. Synthetic Experiments,[0],[0]
We observe a similar phenomenon in a variant of experiment 3 where we set “ 0 and which we defer to the supplementary material due to lack of space.,7.1. Synthetic Experiments,[0],[0]
This suggests that in certain problems MD-APT focuses too much on specific arms with means near the boundary and does not allocate enough samples to other arms.,7.1. Synthetic Experiments,[0],[0]
"On the other hand, MDSAR utilizes knowledge of the time horizon T to effectively spread out samples.",7.1. Synthetic Experiments,[0],[0]
MD-APT’s agnosticism about T may put it a disadvantage in the regime where some of the ∆p qi are very small and T is small relative to H .,7.1. Synthetic Experiments,[0],[0]
"As suggested by experiment 1, the parameter can be used to counteract the sensitivity of MD-APT to arms with means near the boundary.",7.1. Synthetic Experiments,[0],[0]
"In clinical trials, an important challenge is determining the appropriate dosage of a drug.",7.2. Application 1: Dose-Finding,[0],[0]
"The main difficulty is the trade-off that as the dosage increases, the effectiveness of the drug tends to increase, but the likelihood of adverse
effects also increases.",7.2. Application 1: Dose-Finding,[0],[0]
"Thus, one must find a dosage that is sufficiently effective, but does not have too many side effects.",7.2. Application 1: Dose-Finding,[0],[0]
"We assume a situation where the side effects are mild enough not to be a concern for clinical trials, but could nevertheless be unacceptable for a final commercial product.
",7.2. Application 1: Dose-Finding,[0],[0]
We investigate this problem by considering the data in Genovese et al. (2013) (see ARCR20 in week 16 in Table 2 and Table 3).,7.2. Application 1: Dose-Finding,[0],[0]
"In this study, the authors examine the drug secukinumab for treating rheumatoid arthritis.",7.2. Application 1: Dose-Finding,[0],[0]
"They consider four dosage levels (25mg, 75mg, 150mg, 300mg) and a placebo.",7.2. Application 1: Dose-Finding,[0],[0]
"We design a simulation based on their data where each arm corresponds to a drug and has two attributes, the likelihood of being effective and the likelihood of causing an adverse effect.",7.2. Application 1: Dose-Finding,[0],[0]
"Let µi,1 denote the probability of being effective and µi,2 the probability of causing an adverse effect.",7.2. Application 1: Dose-Finding,[0],[0]
"Then, dosage levels 25mg, 75mg, 150mg, and 300mg have means µ1 “ p.34, .519qt,µ2 “ p.469, .612qt,µ3 “ p.465, .465qt,µ4",7.2. Application 1: Dose-Finding,[0],[0]
"“ p.537, .61qt, respectively, and the placebo has mean µ5 “ p.36, .58qt.",7.2. Application 1: Dose-Finding,[0],[0]
We suppose that a drug is considered good if the probability of success is above .4 and the probability of adverse effects is below .5,7.2. Application 1: Dose-Finding,[0],[0]
and we set “ 0.,7.2. Application 1: Dose-Finding,[0],[0]
"Thus, only arm 3 is good and all other arms are bad.",7.2. Application 1: Dose-Finding,[0],[0]
We chose these thresholds so that one drug is good; we did not try other threshold settings.,7.2. Application 1: Dose-Finding,[0],[0]
"We run the experiment for 1000 time steps.
",7.2. Application 1: Dose-Finding,[0],[0]
Figure 6 gives the results of the experiment.,7.2. Application 1: Dose-Finding,[0],[0]
MD-APT and MD-UCBE[10] perform better than the rest of the algorithms.,7.2. Application 1: Dose-Finding,[0],[0]
"MD-UCBE[1] performs slightly worse than UA, which may be because there are only 5 arms so that UA is not that bad of a strategy and MD-UCBE[1] does not explore sufficiently.",7.2. Application 1: Dose-Finding,[0],[0]
MD-SAR only performs slightly better than UA.,7.2. Application 1: Dose-Finding,[0],[0]
This may be because the time horizon is only 1000 time steps and there are only 5 arms.,7.2. Application 1: Dose-Finding,[0],[0]
"We use a real-world dataset for the natural language processing task of affective text analysis (Snow et al., 2008).",7.3. Application 2: Crowdsourcing,[0],[0]
"In this task, workers are asked to rate a short headline on valence and six emotions: disgust, fear, joy, anger, sadness and surprise.",7.3. Application 2: Crowdsourcing,[0],[0]
"A group of experts also provide such ratings for the headlines.
",7.3. Application 2: Crowdsourcing,[0],[0]
We consider the problem of finding workers that tend to agree with the expert views on each of the tasks.,7.3. Application 2: Crowdsourcing,[0],[0]
We examine the deviation of a worker’s ratings with the experts ratings.,7.3. Application 2: Crowdsourcing,[0],[0]
"We normalize this deviation onto a scale of r0, 1s.",7.3. Application 2: Crowdsourcing,[0],[0]
"Let µi,j denote the mean of worker i on task j and let µ̄j denote the mean of all of the workers on task j. We deem a worker i good if µi,j ď µ̄j for all j P r7s.",7.3. Application 2: Crowdsourcing,[0],[0]
"In words, a worker is good if for every task, he performs better than the average worker.",7.3. Application 2: Crowdsourcing,[0],[0]
"To make this realistic, we assume that we are in a setting where the average worker performance on each task is known based on another pool of workers.",7.3. Application 2: Crowdsourcing,[0],[0]
"We
0 250 500 750 1000 1250 1500 1750 2000 horizon
−5
−4
−3
−2
−1
0
lo g(
es t.
fa ilu
re p
ro ba
bi lit
y)
Figure 1.",7.3. Application 2: Crowdsourcing,[0],[0]
"Four Groups on Cube with Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon
−1.50
−1.25
−1.00
−0.75
−0.50
−0.25
0.00
lo g(
es t.
fa ilu
re p
ro ba
bi lit
y)
Figure 2.",7.3. Application 2: Crowdsourcing,[0],[0]
"Four Groups on Cube, no Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon
−6
−5
−4
−3
−2
−1
0
lo g(
es t.
fa ilu
re p
ro ba
bi lit
y)
Figure 3.",7.3. Application 2: Crowdsourcing,[0],[0]
"Linear Progression on Cube with Irrelevant Arms 0 250 500 750 1000 1250 1500 1750 2000 horizon
−2.0
−1.5
−1.0
−0.5
0.0
lo g(
es t.
fa ilu
re p
ro ba
bi lit
y)
Figure 5.",7.3. Application 2: Crowdsourcing,[0],[0]
"Ordered polyhedron 0 200 400 600 800 1000 horizon
−2.5
−2.0
−1.5
−1.0
−0.5
0.0
−3.0
−2.5
−2.0
−1.5
−1.0
−0.5
0.0
lo g(
es t.
fa ilu
re",7.3. Application 2: Crowdsourcing,[0],[0]
"p
ro ba
bi lit
y)
Figure 7.",7.3. Application 2: Crowdsourcing,[0],[0]
"Crowdsourcing Experiment
use a tolerance of “ 0.02.",7.3. Application 2: Crowdsourcing,[0],[0]
"There is a total of 38 workers, where 30 workers are bad arms, 3 workers are good arms, and 5 workers are irrelevant.",7.3. Application 2: Crowdsourcing,[0],[0]
"Because each worker only provides a small number (at least 20) of ratings, whenever an arm is pulled, the algorithm observes an observation chosen uniformly at random with replacement from the data associated with the arm.",7.3. Application 2: Crowdsourcing,[0],[0]
"We run each algorithm for 4000 time steps and in each trial, we randomly permute the samples of each worker.",7.3. Application 2: Crowdsourcing,[0],[0]
"In the supplementary material, we repeat this experiment, but we simulate each arm as a Gaussian distribution (see Section J); the results are very similar.
",7.3. Application 2: Crowdsourcing,[0],[0]
Figure 7 gives the results of the experiment.,7.3. Application 2: Crowdsourcing,[0],[0]
"Until roughly time step 3000, MD-APT and MD-UCBE[10] perform the best.",7.3. Application 2: Crowdsourcing,[0],[0]
"Afterwards, MD-SAR does substantially better than MD-APT and MD-UCBE[10].",7.3. Application 2: Crowdsourcing,[0],[0]
MD-UCBE[1] and MDUCBE[100] perform only marginally better than UA.,7.3. Application 2: Crowdsourcing,[0],[0]
"The experiments suggest that although MD-UCBE is a competitive algorithm, it is highly sensitive to hyperparameter selection, which limits its applicability in practice.",7.4. Summary of Results,[0],[0]
MDSAR and MD-APT tend to perform dramatically better than UA.,7.4. Summary of Results,[0],[0]
"For example, in the crowdsourcing experiment, UA has a final error rate of roughly 52%, whereas MD-SAR has a final error rate of roughly 5%.",7.4. Summary of Results,[0],[0]
"Further, our algorithms can handle complicated polyhedra such as the polyhdron that requires that coordinates are sorted in ascending order (see experiment 5).",7.4. Summary of Results,[0],[0]
"These results suggest that MD-APT tends to perform better than MD-SAR, but in some settings
(e.g., some arms with small ∆p qi and H large relative to T )",7.4. Summary of Results,[0],[0]
MD-APT focuses too much on some of the arms with means near the boundary.,7.4. Summary of Results,[0],[0]
"Because MD-SAR more evenly spreads out its pulls among the arms, it performs better in this regime.",7.4. Summary of Results,[0],[0]
"In this paper, we introduced the feasible arm identification problem.",8. Conclusion,[0],[0]
This problem provides a flexible framework for settings where arms are multi-dimensional and it is of interest to determine whether each arm satisfies user-defined multi-dimensional criteria.,8. Conclusion,[0],[0]
We provided a characterization of the difficulty of these problems that yielded a lower bound and we provided a unified analysis of three algorithms,8. Conclusion,[0],[0]
"MDUCBE, MD-SAR, and MD-APT.",8. Conclusion,[0],[0]
"Our experiments suggest that by leveraging the geometry of the feasible arm identification problem, MD-SAR and MD-APT are able to dramatically outperform a uniform allocation approach.
",8. Conclusion,[0],[0]
Our work also suggests several open directions for future research.,8. Conclusion,[0],[0]
"For example, in many crowdsourcing problems, one does not ask workers to perform all tasks at once, but rather one task at a time and, yet, it may be of interest to find workers who excel at a collection of tasks.",8. Conclusion,[0],[0]
This suggests a variant of the feasible arm identification problem where the agent chooses one coordinate of one arm and observes a realization of the corresponding random variable in each round.,8. Conclusion,[0],[0]
This work was supported in part by NSF grant 1422157.,Acknowledgements,[0],[0]
We thank the anonymous reviewers for their very helpful comments and Aditya Modi for his useful feedback.,Acknowledgements,[0],[0]
"We introduce the feasible arm identification problem, a pure exploration multi-armed bandit problem where the agent is given a set of Ddimensional arms and a polyhedron P “ tx :",abstractText,[0],[0]
"Ax ď bu Ă R. Pulling an arm gives a random vector and the goal is to determine, using a fixed budget of T pulls, which of the arms have means belonging to P .",abstractText,[0],[0]
"We propose three algorithms MD-UCBE, MD-SAR, and MD-APT and provide a unified analysis establishing upper bounds for each of them.",abstractText,[0],[0]
We also establish a lower bound that matches up to constants the upper bounds of MDUCBE and MD-APT.,abstractText,[0],[0]
"Finally, we demonstrate the effectiveness of our algorithms on synthetic and real-world datasets.",abstractText,[0],[0]
Feasible Arm Identification,title,[0],[0]
"Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).",1. Introduction,[1.0],"['Deep reinforcement learning has recently enjoyed successes in many domains (Mnih et al., 2015; Schulman et al., 2015; Levine et al., 2015; Mnih et al., 2016; Lillicrap et al., 2015).']"
"Nevertheless, long-term credit assignment remains a major challenge for these methods, especially in environments with sparse reward signals, such as the infamous Montezuma’s Revenge ATARI game.",1. Introduction,[1.0],"['Nevertheless, long-term credit assignment remains a major challenge for these methods, especially in environments with sparse reward signals, such as the infamous Montezuma’s Revenge ATARI game.']"
"It is symptomatic that the standard approach on the ATARI benchmark suite (Bellemare et al., 2012) is to use an actionrepeat heuristic, where each action translates into several (usually 4) consecutive actions in the environment.",1. Introduction,[1.0],"['It is symptomatic that the standard approach on the ATARI benchmark suite (Bellemare et al., 2012) is to use an actionrepeat heuristic, where each action translates into several (usually 4) consecutive actions in the environment.']"
"Yet another dimension of complexity is seen in non-Markovian environments that require memory – these are particularly
1DeepMind, London, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Alexander Sasha Vezhnevets <vezhnick@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"challenging, since the agent has to learn which parts of experience to store for later, using only a sparse reward signal.
",1. Introduction,[0],[0]
"The framework we propose takes inspiration from feudal reinforcement learning (FRL) introduced by Dayan & Hinton (1993), where levels of hierarchy within an agent communicate via explicit goals.",1. Introduction,[0],[0]
"Some key insights from FRL are that goals can be generated in a top-down fashion, and that goal setting can be decoupled from goal achievement; a level in the hierarchy communicates to the level below it what must be achieved, but does not specify how to do so.",1. Introduction,[1.0],"['Some key insights from FRL are that goals can be generated in a top-down fashion, and that goal setting can be decoupled from goal achievement; a level in the hierarchy communicates to the level below it what must be achieved, but does not specify how to do so.']"
"Making higher levels reason at a lower temporal resolution naturally structures the agents behaviour into temporally extended sub-policies.
",1. Introduction,[0],[0]
The architecture explored in this work is a fullydifferentiable neural network with two levels of hierarchy (though there are obvious generalisations to deeper hierarchies).,1. Introduction,[1.0],['The architecture explored in this work is a fullydifferentiable neural network with two levels of hierarchy (though there are obvious generalisations to deeper hierarchies).']
"The top level, the Manager, sets goals at a lower temporal resolution in a latent state-space that is itself learnt by the Manager.",1. Introduction,[1.0],"['The top level, the Manager, sets goals at a lower temporal resolution in a latent state-space that is itself learnt by the Manager.']"
"The lower level, the Worker, operates at a higher temporal resolution and produces primitive actions, conditioned on the goals it receives from the Manager.",1. Introduction,[0],[0]
The Worker is motivated to follow the goals by an intrinsic reward.,1. Introduction,[0],[0]
"However, significantly, no gradients are propagated between Worker and Manager; the Manager receives its learning signal from the environment alone.",1. Introduction,[0],[0]
"In other words, the Manager learns to select latent goals that maximise extrinsic reward.
",1. Introduction,[0],[0]
"The key contributions of our proposal are: (1) A consistent, end-to-end differentiable model that embodies and generalizes the principles of FRL.",1. Introduction,[0],[0]
"(2) A novel, approximate transition policy gradient update for training the Manager, which exploits the semantic meaning of the goals it produces.",1. Introduction,[0],[0]
(3) The use of goals that are directional rather than absolute in nature.,1. Introduction,[0],[0]
"(4) A novel RNN design for the Manager – a dilated LSTM – which extends the longevity of the recurrent state memories and allows gradients to flow through large hops in time, enabling effective back-propagation through hundreds of steps.
",1. Introduction,[0],[0]
Our ablative analysis (Section 5.4) confirms that transitional policy gradient and directional goals are crucial for best performance.,1. Introduction,[0],[0]
"Our experiments on a selection of ATARI games (including the infamous Montezuma’s re-
FeUdal Networks for Hierarchical Reinforcement Learning
venge) and on several memory tasks in the 3D DeepMind Lab environment (Beattie et al., 2016) show that FuN significantly improves long-term credit assignment and memorisation.",1. Introduction,[0],[0]
"Building hierarchical agents is a long standing topic in reinforcement learning (Sutton et al., 1999; Precup, 2000; Dayan & Hinton, 1993; Dietterich, 2000; Boutilier et al., 1997; Dayan, 1993; Kaelbling, 2014; Parr & Russell, 1998; Precup et al., 1997; 1998; Schmidhuber, 1991; Sutton, 1995; Wiering & Schmidhuber, 1997; Vezhnevets et al., 2016; Bacon et al., 2017).",2. Related Work,[0],[0]
"The options framework (Sutton et al., 1999; Precup, 2000) is a popular formulation for considering the problem with a two level hierarchy.",2. Related Work,[0],[0]
"The bottom level – an option – is a sub-policy with a termination condition, which takes in environment observations and outputs actions until the termination condition is met.",2. Related Work,[0],[0]
"An agent picks an option using its policy-over-options (the top level) and subsequently follows it until termination, at which point the policy-over-options is queried again and the process continues.",2. Related Work,[0],[0]
"Options are typically learned using sub-goals and ‘pseudo-rewards’ that are provided explicitly (Sutton et al., 1999; Dietterich, 2000; Dayan & Hinton, 1993).",2. Related Work,[0],[0]
"For a simple, tabular case (Wiering & Schmidhuber, 1997; Schaul et al., 2015), each state can be used as a sub-goal.",2. Related Work,[0],[0]
"Given the options, a policy-over-options can be learned using standard techniques by treating options as actions.",2. Related Work,[0],[0]
"Recently (Tessler et al., 2016; Kulkarni et al., 2016) have demonstrated that combining deep learning with predefined sub-goals delivers promising results in challenging environments like Minecraft and Atari, however sub-goal discovery was not addressed.
",2. Related Work,[0],[0]
"A recent work of (Bacon et al., 2017) shows the possibility of learning options jointly with a policy-over-options in an end-to-end fashion by extending the policy gradient theorem to options.",2. Related Work,[0],[0]
"When options are learnt end-toend, they tend to degenerate to one of two trivial solutions: (i) only one active option that solves the whole task; (ii) a policy-over-options that changes options at every step, micro-managing the behaviour.",2. Related Work,[0],[0]
"Consequently, regularisers (Bacon et al., 2017; Vezhnevets et al., 2016) are usually introduced to steer the solution towards multiple options of extended length.",2. Related Work,[0],[0]
"This is believed to provide an inductive bias towards re-usable temporal abstractions and to help generalisation.
",2. Related Work,[0],[0]
A key difference between our approach and the options framework is that in our proposal the top level produces a meaningful and explicit goal for the bottom level to achieve.,2. Related Work,[0],[0]
Sub-goals emerge as directions in the latent statespace and are naturally diverse.,2. Related Work,[0],[0]
"We also achieve significantly better scores on ATARI than Option-Critic (section 5).
",2. Related Work,[0],[0]
"ℒ(rt+ cos(St+c-St,gt))
Policy gradient
(Rt-V M t) cos(st+c-st,gt)
Transition policy gradient
There has also been a significant progress in nonhierarchical deep RL methods by using auxiliary losses and rewards.",2. Related Work,[0],[0]
"(Bellemare et al., 2016a) have significantly advanced the state-of-the-art on Montezuma’s Revenge by using pseudo-count based auxiliary rewards for exploration, which stimulate agents to explore new parts of the state space.",2. Related Work,[0],[0]
"The recently proposed UNREAL agent (Jaderberg et al., 2016) also demonstrates a strong improvement by using unsupervised auxiliary tasks to help refine its internal representations.",2. Related Work,[0],[0]
"We note that these benefits are orthogonal to those provided by FuN, and that both approaches could be combined with FuN for even greater effect.",2. Related Work,[0],[0]
What is FuN?,3. The model,[0],[0]
FuN is a modular neural-network consisting of two modules – the Worker and the Manager.,3. The model,[0],[0]
The Manager internally computes a latent state representation st and outputs a goal vector gt.,3. The model,[1.0],['The Manager internally computes a latent state representation st and outputs a goal vector gt.']
"The Worker produces actions conditioned on external observation, its own state, and the Managers goal.",3. The model,[0],[0]
The Manager and the Worker share a perceptual module which takes an observation from the environment xt and computes a shared intermediate representation zt.,3. The model,[1.0],['The Manager and the Worker share a perceptual module which takes an observation from the environment xt and computes a shared intermediate representation zt.']
The Manager’s goals gt are trained using an approximate transition policy gradient.,3. The model,[0],[0]
This is a particularly efficient form of policy gradient training that exploits the knowledge that the Worker’s behaviour will ultimately align with the goal directions it has been set.,3. The model,[0],[0]
The Worker is then trained via intrinsic reward to produce actions that cause these goal directions to be achieved.,3. The model,[0],[0]
"Figure 1a illustrates the overall design and the following equations describe the forward dynamics of our network:
zt = f percept(xt); st = fMspace(zt) (1)
hMt , ĝt = f Mrnn(st, hMt−1); gt = ĝt/||ĝt||; (2)
",3. The model,[0],[0]
wt = φ,3. The model,[0],[0]
"( t∑ i=t−c gi) (3)
hW , Ut = f Wrnn(zt, hWt−1);πt = SoftMax(Utwt) (4)
where both the Manager and the Worker are recurrent.",3. The model,[0],[0]
"Here hM and hW correspond to the internal states of the Man-
ager and the Worker respectively.",3. The model,[0],[0]
"A linear transform φ maps a goal gt into an embedding vectorwt ∈ Rk, which is then combined via product with matrixUt (Workers output) to produce policy π – vector of probabilities over primitive actions.",3. The model,[0],[0]
"The next section provides the details on goal embedding and the following sections 3.2,3.3 describes how FuN is trained.",3. The model,[0],[0]
"The goal g modulates the policy via a multiplicative interaction in a low dimensional goal-embedding space Rk, k << d.",3.1. Goal embedding,[0],[0]
"The Worker first produces an embedding vector for every action, represented by rows of matrix U ∈ R|a|×k (eq. 4).",3.1. Goal embedding,[0],[0]
"To incorporate goals from the Manager, the last c goals are first pooled by summation and then embedded into a vector w ∈ Rk using a linear projection φ (eq. 3).",3.1. Goal embedding,[0],[0]
"The projection φ is linear, with no biases, and is learnt with gradients coming from the Worker’s actions.",3.1. Goal embedding,[0],[0]
The embedding matrix U is then combined with the goal embedding w via a matrix-vector product (eq. 4).,3.1. Goal embedding,[0],[0]
Since φ has no biases it can never produce a constant non-zero vector – which is the only way the setup could ignore the Manager’s input.,3.1. Goal embedding,[0],[0]
This makes sure that the goal output by the Manager always influences the final policy.,3.1. Goal embedding,[0],[0]
"Notice how, due to pooling of goals over several time-steps, the conditioning from the Manager varies smoothly.",3.1. Goal embedding,[1.0],"['Notice how, due to pooling of goals over several time-steps, the conditioning from the Manager varies smoothly.']"
We consider a standard reinforcement learning setup.,3.2. Learning,[0],[0]
"At each step t, the agent receives an observation xt from the environment and selects an action at from a finite set of possible actions.",3.2. Learning,[0],[0]
The environment responds with a new observation xt+1 and a scalar reward rt.,3.2. Learning,[0],[0]
"The process continues until the terminal state is reached, after which it restarts.",3.2. Learning,[0],[0]
The goal of the agent is to maximise the discounted return Rt = ∑∞,3.2. Learning,[0],[0]
k=0,3.2. Learning,[0],[0]
"γ
krt+k+1 with γ ∈",3.2. Learning,[0],[0]
"[0, 1].",3.2. Learning,[0],[0]
"The agent’s behaviour is defined by its action-selection policy π. FuN produces a distribution over possible actions (a stochastic policy) as defined in eq. 4.
",3.2. Learning,[0],[0]
The conventional wisdom would be to train the whole architecture monolithically through gradient descent on either the policy directly or via TD-learning.,3.2. Learning,[0],[0]
"Notice, that since FuN is fully differentiable we could train it end-toend using a policy gradient algorithm operating on the actions taken by the Worker.",3.2. Learning,[0],[0]
The outputs g of the Manager would be trained by gradients coming from the Worker.,3.2. Learning,[0],[0]
"This, however would deprive Manager’s goals g of any semantic meaning, making them just internal latent variables of the model.",3.2. Learning,[0],[0]
We propose instead to independently train Manager to predict advantageous directions (transitions) in state space and to intrinsically reward the Worker to follow these directions.,3.2. Learning,[1.0],['We propose instead to independently train Manager to predict advantageous directions (transitions) in state space and to intrinsically reward the Worker to follow these directions.']
"If the Worker can fulfil the goal of moving in these directions (as it is rewarded for doing), then we ought to end up taking advantageous trajectories through
state-space.",3.2. Learning,[0],[0]
"We formalise this in the following update rule for the Manager:
∇gt = AMt ∇θdcos(st+c − st, gt(θ)), (5)
where AMt = Rt − VMt (xt, θ) is the Manager’s advantage function, computed using a value function estimate VMt (xt, θ) from the internal critic; dcos(α, β) = αTβ/(|α||β|) is the cosine similarity between two vectors.",3.2. Learning,[0],[0]
Note: the dependence of s on θ is ignored when computing ∇θdcos – this avoids trivial solutions.,3.2. Learning,[0],[0]
"Notice that now gt acquires a semantic meaning as an advantageous direction in the latent state space at a horizon c, which defines the temporal resolution of the Manager.
",3.2. Learning,[0.9999999327038657],"['Notice that now gt acquires a semantic meaning as an advantageous direction in the latent state space at a horizon c, which defines the temporal resolution of the Manager.']"
"The intrinsic reward that encourages the Worker to follow the goals is defined as:
rIt = 1/c c∑ i=1",3.2. Learning,[0],[0]
"dcos(st − st−i, gt−i) (6)
We use directions because it is more feasible for the Worker to be able to reliably cause directional shifts in the latent state than it is to assume that the Worker can take us to (potentially) arbitrary new absolute locations.",3.2. Learning,[0],[0]
"It also gives a degree of invariance to the goals and allows for structural generalisation – the same directional sub-goal g can invoke a sub-policy that is valid and useful in a large part of the latent state space; e.g. evade an enemy, swim up for air, etc.",3.2. Learning,[0],[0]
"We compare absolute against directional goals empirically in section 5.4.
",3.2. Learning,[0],[0]
The original feudal reinforcement learning formulation of Dayan & Hinton (1993) advocated completely concealing the reward from the environment from lower levels of hierarchy.,3.2. Learning,[0],[0]
"In practice we take a softer approach by adding an intrinsic reward for following the goals, but retaining the environment reward as well.",3.2. Learning,[0],[0]
"The Worker is then trained to maximise a weighted sum Rt + αRIt , where α is a hyperparameter that regulates the influence of the intrinsic reward.",3.2. Learning,[0],[0]
The Workers policy π can be trained to maximise intrinsic reward by using any off-the shelf deep reinforcement learning algorithm.,3.2. Learning,[0],[0]
"Here we use an advantage actor critic (Mnih et al., 2016):
∇πt = ADt ∇θ log π(at|xt; θ) (7)
",3.2. Learning,[0],[0]
The advantage function estimator ADt =,3.2. Learning,[0],[0]
"(Rt + αR I t − V Dt (xt; θ)) is calculated using an internal critic, which estimates the value functions for both rewards.
",3.2. Learning,[0],[0]
Note that the Worker and Manager can potentially have different discount factors γ for computing the return.,3.2. Learning,[0],[0]
"This allows, for instance, the Worker to be more greedy and focus on immediate rewards while the Manager can consider a long-term perspective.",3.2. Learning,[0],[0]
We now motivate our proposed update rule for the Manager as a novel form of policy gradient with respect to a model of the Worker’s behaviour.,3.3. Transition Policy Gradients,[0],[0]
"Consider a high-level policy ot = µ(st, θ) that selects among sub-policies (possibly from a continuous set), where we assume for now that these sub-policies are fixed duration behaviours (lasting for c steps).",3.3. Transition Policy Gradients,[0],[0]
"Corresponding to each sub-policy is a transition distribution, p(st+c|st, ot), that describes the distribution of states that we end up at the end of the sub-policy, given the start state and the sub-policy enacted.",3.3. Transition Policy Gradients,[0],[0]
"The high-level policy can be composed with the transition distribution to give a ‘transition policy’ πTP (st+c|st) = p(st+c|st, µ(st, θ)) describing the distribution over end states given start states.",3.3. Transition Policy Gradients,[0],[0]
It is valid to refer to this as a policy because the original MDP is isomorphic to a new MDP with policy πTP and transition function st+c =,3.3. Transition Policy Gradients,[0],[0]
πTP (st) (i.e. the state always transitions to the end state picked by the transition policy).,3.3. Transition Policy Gradients,[0],[0]
"As a result, we can apply the policy gradient theorem to the transition policy πTP , so as to find the performance gradient with respect to the policy parameters,
∇θπTPt",3.3. Transition Policy Gradients,[0],[0]
= E,3.3. Transition Policy Gradients,[0],[0]
"[(Rt − V (st))∇θ log p(st+c|st, µ(st, θ))] (8)
In general, the Worker may follow a complex trajectory.",3.3. Transition Policy Gradients,[0],[0]
A naive application of policy gradients requires the agent to learn from samples of these trajectories.,3.3. Transition Policy Gradients,[0],[0]
"But if we know where these trajectories are likely to end up, by modelling the transitions, then we can skip directly over the Worker’s behaviour and instead follow the policy gradient of the predicted transition.",3.3. Transition Policy Gradients,[0],[0]
"FuN assumes a particular form for the transition model: that the direction in state-space, st+c−st, follows a von Mises-Fisher distribution.",3.3. Transition Policy Gradients,[0],[0]
"Specifically, if the mean direction of the von Mises-Fisher distribution is given by g(ot) (which for compactness we write as gt) we would have p(st+c|st, ot) ∝",3.3. Transition Policy Gradients,[0],[0]
"edcos(st+c−st,gt).",3.3. Transition Policy Gradients,[0],[0]
"If this functional form were indeed correct, then we see that our proposed update heuristic for the Manager, eqn.5, is in fact the proper form for the transition policy gradient arrived at in eqn.8.
",3.3. Transition Policy Gradients,[0],[0]
Note that the Worker’s intrinsic reward (eqn. 6) is based on the log-likelihood of state trajectory.,3.3. Transition Policy Gradients,[0],[0]
Through that the FuN architecture actively encourages the functional form of the transition model to hold true.,3.3. Transition Policy Gradients,[0],[0]
"Because the Worker is learning to achieve the Manager’s direction, its transitions should, over time, closely follow a distribution around this direction, and hence our approximation for transition policy gradients should hold reasonably well.",3.3. Transition Policy Gradients,[1.0],"['Because the Worker is learning to achieve the Manager’s direction, its transitions should, over time, closely follow a distribution around this direction, and hence our approximation for transition policy gradients should hold reasonably well.']"
This section provides the particular details of the model as described in section 3.,4. Architecture details,[1.0],['This section provides the particular details of the model as described in section 3.']
The perceptual module fpercept is a convolutional network (CNN) followed by a fully connected layer.,4. Architecture details,[0],[0]
"The CNN has a first layer with 16 8x8 fil-
ters of stride 4, followed by a layer with with 32 4x4 filters of stride 2.",4. Architecture details,[0],[0]
The fully connected layer has 256 hidden units.,4. Architecture details,[0],[0]
Each convolutional and fully-connected layer is followed by a rectifier non-linearity1.,4. Architecture details,[0],[0]
"The state space which the Manager implicitly models in formulating its goals is computed via fMspace, which is another fully connected layer followed by a rectifier non-linearity.",4. Architecture details,[1.0],"['The state space which the Manager implicitly models in formulating its goals is computed via fMspace, which is another fully connected layer followed by a rectifier non-linearity.']"
"The dimensionality of the embedding vectors, w, is set as k = 16.",4. Architecture details,[0],[0]
"To encourage exploration in transition policy, at every step with a small probability we emit a random goal sampled from a uni-variate Gaussian.
",4. Architecture details,[0],[0]
"The Worker’s recurrent network fWrnn is a standard LSTM (Hochreiter & Schmidhuber, 1997).",4. Architecture details,[0],[0]
"For the Manager’s recurrent network, fMrnn, we propose a novel design – the dilated LSTM, which is introduced in the next section.",4. Architecture details,[0],[0]
Both fMrnn and fWrnn have 256 hidden units.,4. Architecture details,[0],[0]
"We propose a novel RNN architecture for the Manager, which operates at lower temporal resolution than the data stream.",4.1. Dilated LSTM,[0],[0]
"The main contribution here is the inductive bias towards slowly varying outputs, which have very long-term temporal dependencies.",4.1. Dilated LSTM,[0],[0]
"We define a dilated LSTM analogously to dilated convolutional networks (Yu & Koltun, 2016).",4.1. Dilated LSTM,[0],[0]
"For a dilation radius r let the full state of the network be h = {ĥi}ri=1, i.e. it is composed of r separate groups of sub-states or ‘cores’.",4.1. Dilated LSTM,[0],[0]
"At time t the network is governed by the following equations: ĥt%rt , gt = LSTM(st, ĥt%rt−1; θ
LSTM), where % denotes the modulo operation and allows us to indicate which group of cores is currently being updated.",4.1. Dilated LSTM,[0],[0]
"We make the parameters of the LSTM network θLSTM explicit to stress that the same set of parameters governs the update for each of the r groups within the dLSTM.
",4.1. Dilated LSTM,[0],[0]
At each time step only the corresponding part of the state is updated and the output is pooled across the previous c outputs.,4.1. Dilated LSTM,[0],[0]
"This allows the r groups of cores inside the dLSTM to preserve the memories for long periods, yet the dLSTM as a whole is still able to process and learn from every input experience, and is also able to update its output at every step.",4.1. Dilated LSTM,[0],[0]
"This idea is similar to clockwork RNNs (Koutnı́k et al., 2014), however there the top level “ticks” at a fixed, slow pace, whereas the dLSTM observes all the available training data instead.",4.1. Dilated LSTM,[0],[0]
"In the experiments we set r = 10, and this was also used as the predictions horizon, c.",4.1. Dilated LSTM,[0],[0]
"The goal of our experiments is to demonstrate that FuN learns non-trivial, helpful, and interpretable sub-policies
1This is substantially the same CNN as in (Mnih et al., 2016; 2015), the only difference is that in the pre-processing stage we retain all colour channels.
and sub-goals, and also to validate components of the architecture.",5. Experiments,[0],[0]
We start by describing technical details of the experimental setup and then present results on Montezuma’s revenge – an infamously hard ATARI game – in section 5.1.,5. Experiments,[1.0],['We start by describing technical details of the experimental setup and then present results on Montezuma’s revenge – an infamously hard ATARI game – in section 5.1.']
Section 5.2 presents results on more ATARI games and extensively compares FuN to LSTM baseline with different discount factors and BPTT lengths.,5. Experiments,[0],[0]
In section 5.3 we present results on a set of visual memorisation tasks in 3D environment.,5. Experiments,[0],[0]
"Section 5.4 presents an ablation study of FuN, validating our design choices.
",5. Experiments,[0],[0]
Baseline.,5. Experiments,[0],[0]
Our main baseline is a recurrent LSTM network on top of a representation learned by a CNN.,5. Experiments,[0],[0]
"The LSTM (Hochreiter & Schmidhuber, 1997) architecture is a widely used recurrent network and it was demonstrated to perform very well on a suite of reinforcement learning problems (Mnih et al., 2016).",5. Experiments,[0],[0]
LSTM uses 316 hidden units2 and its inputs are the feature representation of an observation and the previous action of the agent.,5. Experiments,[0],[0]
Action probabilities and the value function estimate are regressed from its hidden state.,5. Experiments,[0],[0]
"All the methods the same CNN architecture, input pre-processing, and an action repeat of 4.
Optimisation.",5. Experiments,[0],[0]
"We use the A3C method (Mnih et al., 2016) for all reinforcement learning experiments.",5. Experiments,[0],[0]
"It was shown to achieve state-of-the-art results on several challenging benchmarks (Mnih et al., 2016).",5. Experiments,[0],[0]
"We cut the trajectory and run backpropagation through time (BPTT) (Mozer, 1989) after K forward passes of a network or if a terminal signal is received.",5. Experiments,[0],[0]
"For FuN K = 400, for LSTM, unless otherwise stated, K = 40.",5. Experiments,[0],[0]
We discuss different choice of K for LSTM in section 5.2.,5. Experiments,[0],[0]
The optimization process runs 32 asynchronous threads using shared RMSProp.,5. Experiments,[0],[0]
"There are 3 hyper-parameters in
2This choice means that FuN and the LSTM baseline to have roughly the same number of total parameters.
FuN and 2 in the LSTM baselines.",5. Experiments,[0],[0]
"For each method, we ran 100 experiments, each using randomly sampled hyper-parameters.",5. Experiments,[0],[0]
"Learning rate and entropy penalty were sampled from a LogUniform(10−4, 10−3) interval for LSTM.",5. Experiments,[0],[0]
"For FuN the learning rate was sampled from LogUniform(10−4.5, 10−3.5), to account for higher gradients due to longer BPTT unrolls.",5. Experiments,[0],[0]
The learning rate was linearly annealed from a sampled value to half the initial rate for all agents.,5. Experiments,[0],[0]
"To explore intrinsic motivation in FuN, we sample its weight α ∼ Uniform(0, 1).",5. Experiments,[0],[0]
We define a training epoch as one million observations.,5. Experiments,[0],[0]
"When reporting learning curves, we plot the average episode score of the top 5 agents (according to the final score) against the training epochs.",5. Experiments,[0],[0]
"For all ATARI experiments we clip the reward to [−1,+1] interval",5. Experiments,[0],[0]
"Montezuma’s revenge is one of the hardest games available through the ALE (Bellemare et al., 2012).",5.1. Montezuma’s revenge,[0],[0]
The game is infamous for challenging agents with lethal traps and sparse rewards.,5.1. Montezuma’s revenge,[0],[0]
We had to broaden and intensify our hyper-parameter search for the LSTM baseline to see any progress at all for that model.,5.1. Montezuma’s revenge,[0],[0]
"We have experimented with many different hyper-parameter configurations for LSTM baseline, for instance expanding learning rate search to LogUniform(10−3, 10−2), and we report on the configuration that worked best.",5.1. Montezuma’s revenge,[0],[0]
We use a small discount 0.99 for LSTM; for FuN we use 0.99 in Worker and 0.999 in Manager.,5.1. Montezuma’s revenge,[0],[0]
Figure 2b,5.1. Montezuma’s revenge,[0],[0]
analyses the sub-goals learnt by FuN in the first room.,5.1. Montezuma’s revenge,[0],[0]
"They turn out to be meaningful milestones, which bridge the agents progress to its first extrinsic reward – picking up the key.",5.1. Montezuma’s revenge,[1.0],"['They turn out to be meaningful milestones, which bridge the agents progress to its first extrinsic reward – picking up the key.']"
"Interestingly, two of the learnt sub-goals correspond to roughly the same locations as the ones hand-crafted in (Kulkarni et al., 2016) (ladder and key), but here they are learnt by the agent itself.",5.1. Montezuma’s revenge,[0],[0]
"Figure 2a
plots the learning curves.",5.1. Montezuma’s revenge,[0],[0]
Notice how FuN starts learning much earlier and achieves much higher scores.,5.1. Montezuma’s revenge,[0],[0]
"It takes > 300 epochs for LSTM to reach the score 400, which corresponds to solving the first room (take the key, open a door); it stagnates at that score until about 900 epochs, when it starts exploring further.",5.1. Montezuma’s revenge,[0],[0]
"FuN solves the first room in less than 200 epochs and immediately moves on to explore further, eventually visiting several other rooms and scoring up to 2600 points.",5.1. Montezuma’s revenge,[0],[0]
Experiments in this section validate that the capabilities of FuN go beyond what standard tools for long-term credit assignment – discount factors and BPTT unroll length – can provide for a baseline LSTM agent.,5.2. ATARI,[0],[0]
We use two discounts 0.99 and 0.95 for both FuN and LSTM agents.,5.2. ATARI,[0],[0]
"(For the experiments on FuN only the discount for the Manager changes, while the Worker’s discount is fixed at
0.95.)",5.2. ATARI,[0],[0]
"For the LSTM we explore BPTT of 40 and 100, while for FuN we use a BPTT unroll of 400.",5.2. ATARI,[0],[0]
"For LSTM with BPTT 100 we search for learning rate in the interval LogUniform(10−4.5, 10−3.5), as for FuN. We use a diverse set of ATARI games, some of which involve longterm credit assignment and some which are more reactive.
",5.2. ATARI,[0],[0]
Figure 3 plots the learning curves.,5.2. ATARI,[0],[0]
A few categories emerge.,5.2. ATARI,[0],[0]
"On Ms. Pacman, Amidar, and Gravitar FuN with a low Manager discount of 0.99 strongly outperforms all other methods.",5.2. ATARI,[0],[0]
All of these games are known to require long-term reasoning to play well.,5.2. ATARI,[0],[0]
Enduro stands out as all the LSTM agents completely fail at it.,5.2. ATARI,[0],[0]
In this game the agent controls a racing car and scores points for overtaking other racers; this requires accelerating and steering for significant amount of time before the first reward is experienced.,5.2. ATARI,[0],[0]
"Frostbite is a hard game (Vezhnevets et al., 2016; Lake et al., 2016) that requires both long-term credit assignment and good exploration.",5.2. ATARI,[0],[0]
"The best-performing frost-
bite agent is FuN with 0.95 Manager discount, which outperforms the rest by a factor of 7.",5.2. ATARI,[0],[0]
On Hero and Space Invaders all agents perform equally well.,5.2. ATARI,[0],[0]
"On Seaquest and Breakout, the baseline LSTM with a more aggressive discount of 0.95 is the best.",5.2. ATARI,[0],[0]
This suggests that in these games long-term credit assignment is not important and the agent is better off optimising more immediate rewards in a greedy fashion.,5.2. ATARI,[0],[0]
"Alien is the only game where using different discounts doesn’t meaningfully influence the agents performance; here we see the baseline LSTM outperforms our FuN model, although both still achieve a satisfactory scores.",5.2. ATARI,[0],[0]
"We provide qualitative analysis of subpolicies learnt on Seaquest in supplementary material.
",5.2. ATARI,[0],[0]
"Note how using an unroll for BPTT=100 in the baseline LSTM significantly hurts its performance (hence we do not explore longer unrolls), while FuN performs very well with BPTT of 400 thanks to its ability to leverage the dLSTM.",5.2. ATARI,[0],[0]
"Being able to train a recurrent network over very long sequences could be an enabling tool for many memory related task, as we demonstrate in section 5.3.
",5.2. ATARI,[0],[0]
"Option-critic architecture (Bacon et al., 2017) is, to the best of our knowledge, the only other end-to-end trainable system with sub-policies.",5.2. ATARI,[0],[0]
"The experimental results for Option-Critic on 4 ATARI (Bacon et al., 2017) games show scores similar those from a flat DQN (Mnih et al., 2015) baseline agent.",5.2. ATARI,[0],[0]
"Notice that our baseline (Mnih et al., 2016) is much stronger than DQN.",5.2. ATARI,[0],[0]
"We also ran FuN on the same games as Option-Critic (Asterix, Ms. Pacman, Seaquest and Zaxxon) and after 200 epochs it achieves a similar score on Seaquest, doubles it on Ms. Pacman, more than triples it on Zaxxon and gets more than 20x improvement on Asterix (see supplementary material for plots).",5.2. ATARI,[0],[0]
"DeepMind Lab (Beattie et al., 2016) is a first-person 3D game platform extended from OpenArena.",5.3. Memory in Labyrinth,[0],[0]
It’s a visually complex 3D environment with agent actions corresponding to movement and orientation.,5.3. Memory in Labyrinth,[1.0],['It’s a visually complex 3D environment with agent actions corresponding to movement and orientation.']
"We use 4 different levels that test long-term credit assignment and visual memory:
Water maze is a reproduction of the Morris water maze experiment (Morris, 1981) from the behavioural science literature.",5.3. Memory in Labyrinth,[0],[0]
An agent is dropped into a circular pool of water with a concealed platform at unknown random location.,5.3. Memory in Labyrinth,[0],[0]
The agent can move around and upon stepping on the platform it receives a reward and the trial restarts.,5.3. Memory in Labyrinth,[0],[0]
"The platform remains in the same location for the rest of the episode, while agent starts each trial at a random location.",5.3. Memory in Labyrinth,[0],[0]
"The walls of the pool are decorated with visual cues to assist localisation.
",5.3. Memory in Labyrinth,[0],[0]
T-maze is another classic animal cognition test.,5.3. Memory in Labyrinth,[0],[0]
The agent spawns in a small T-shaped maze.,5.3. Memory in Labyrinth,[0],[0]
Two objects with randomly chosen shape and colour are spawned at the left and right ”baiting” locations.,5.3. Memory in Labyrinth,[0],[0]
One of them is assigned a reward of +1 and the other a reward of -1.,5.3. Memory in Labyrinth,[0],[0]
"When the agent collects one of the objects, it receives the reward and is respawned at the beginning of the T-maze.",5.3. Memory in Labyrinth,[0],[0]
The objects are also re-instantiated in the same locations and with the same rewards on the re-spawn event.,5.3. Memory in Labyrinth,[0],[0]
The agent should remember which object gives the positive reward across re-spawns and collect it as many times as possible within the fixed time given for the episode.,5.3. Memory in Labyrinth,[0],[0]
"T-maze+ is a modification of T-maze, where at each trial the length of corridors can vary, adding additional dimension of complexity.
",5.3. Memory in Labyrinth,[0],[0]
Non-match is a visual memorisation task.,5.3. Memory in Labyrinth,[0],[0]
Each trial begins in small room with an out of reach object being displayed in one of two display pods.,5.3. Memory in Labyrinth,[0],[0]
"There is a pad in the middle, which upon touching, the agent is rewarded with 1 point, and is teleported to a second room which has two objects in it, one of which matches the object in the previous room.",5.3. Memory in Labyrinth,[0],[0]
"Collecting the matching object gives a reward of -10 points, collecting the non matching object gives a reward of 10 points.",5.3. Memory in Labyrinth,[0],[0]
"Once either is collected, the agent is teleported back to the first room, with the same object being shown.
",5.3. Memory in Labyrinth,[0],[0]
For all agents we include reward as a part of the observation.,5.3. Memory in Labyrinth,[0],[0]
Figure 5 plots the learning curves.,5.3. Memory in Labyrinth,[0],[0]
FuN consitently outperforms the LSTM baseline – it learns faster and also reaches a higher final reward.,5.3. Memory in Labyrinth,[0],[0]
We analyse the FuN agent’s behaviour in more detail in Figure 4b.,5.3. Memory in Labyrinth,[0],[0]
"It demonstrates that FuN learns meaningful sub-policies, which are then efficiently integrated with memory to produce rewarding behaviour.",5.3. Memory in Labyrinth,[0],[0]
"Interestingly, the LSTM agent doesn’t appear to use its memory for water maze task at all, always circling the maze at the roughly the same radius.",5.3. Memory in Labyrinth,[1.0],"['Interestingly, the LSTM agent doesn’t appear to use its memory for water maze task at all, always circling the maze at the roughly the same radius.']"
This section empirically validates the main innovations of this paper: transition policy gradient for training the Manager; relative rather than absolute goals; intrinsic motivation for the Worker.,5.4. Ablative analysis,[1.0],['This section empirically validates the main innovations of this paper: transition policy gradient for training the Manager; relative rather than absolute goals; intrinsic motivation for the Worker.']
"First we consider a ‘non-Feudal’ FuN – it has exactly the same network architecture as FuN, but the Managers output g is trained with gradients coming directly from the Worker and no intrinsic reward is used, much like in Option-Critic architecture (Bacon et al., 2017).",5.4. Ablative analysis,[0],[0]
"Second, g is learnt using a standard policy gradient approach with the Manager emitting the mean of a Gaussian distribution from which goals are sampled (as if the Manager were solving a continuous control problem (Schulman et al., 2016; Mnih et al., 2016; Lillicrap et al., 2015)).",5.4. Ablative analysis,[0],[0]
"Third, we explore a variant of FuN in which g specifies absolute, rather than relative/directional, goals (and the Worker’s intrinsic reward is adjusted accordingly) but otherwise everything is the same.",5.4. Ablative analysis,[0],[0]
"The experiments (Figure 6) reveal that, although alternatives do work to some degree their performance is significantly inferior.",5.4. Ablative analysis,[0],[0]
We also evaluate a purely feudal version of FuN – in which the Worker is trained from the intrinsic reward alone.,5.4. Ablative analysis,[0],[0]
"This ablation performs better than other, but still inferior to the full FuN approach.",5.4. Ablative analysis,[0],[0]
It shows that allowing the Worker to experience the external reward is beneficial.,5.4. Ablative analysis,[0],[0]
One of the advantages of FuN is the clear separation of duties between Manager and Worker.,5.5. ATARI action repeat transfer,[0],[0]
"The Manager learns a transition policy, while the Worker learns to operate primitive actions to enact these transitions.",5.5. ATARI action repeat transfer,[0],[0]
This transition policy is invariant to the underlying embodiment of the agent – the way its primitive actions translate into state space transitions.,5.5. ATARI action repeat transfer,[1.0],['This transition policy is invariant to the underlying embodiment of the agent – the way its primitive actions translate into state space transitions.']
"Potentially, the transition policy can be transferred between agents with different embodiment – e.g. robot models with different bodies or different operational frequency.",5.5. ATARI action repeat transfer,[0],[0]
We provide evidence towards that possibility by transferring policies across agents with different action repeat on ATARI.,5.5. ATARI action repeat transfer,[1.0],['We provide evidence towards that possibility by transferring policies across agents with different action repeat on ATARI.']
"Action repeat is a heuristic used in all successful agents (Mnih et al., 2015; 2016; Bellemare et al., 2016b; Vezhnevets et al., 2016).",5.5. ATARI action repeat transfer,[0],[0]
"It enables better exploration, eases credit assignment, and saves computation by repeating an action chosen by the agent several (= 4) times.
",5.5. ATARI action repeat transfer,[0],[0]
"To perform transfer, we initialise the FuN system with parameters extracted from an agent trained with action repeat of 4 and then make the following adjustments: (i) we accordingly adjust the discounts for all rewards; (ii) we increase the dilation of the dLSTM by a factor of 4; (iii) we increase the Manager’s goal horizon c by a factor of 4.",5.5. ATARI action repeat transfer,[1.0],"['To perform transfer, we initialise the FuN system with parameters extracted from an agent trained with action repeat of 4 and then make the following adjustments: (i) we accordingly adjust the discounts for all rewards; (ii) we increase the dilation of the dLSTM by a factor of 4; (iii) we increase the Manager’s goal horizon c by a factor of 4.']"
(These modifications adapt all the “hard-wired” but explicitly temporally sensitive aspects of the agent.),5.5. ATARI action repeat transfer,[0],[0]
We then train this agent without action repeat.,5.5. ATARI action repeat transfer,[0],[0]
As a baseline we use an LSTM agent transferred in a similar way (with adjusted discounts) as well as FuN and LSTM agents trained without action repeat from scratch.,5.5. ATARI action repeat transfer,[0],[0]
Figure 7 shows the corresponding learning curves.,5.5. ATARI action repeat transfer,[0],[0]
The transferred FuN agent (green curve) significantly outperforms every other method.,5.5. ATARI action repeat transfer,[0],[0]
"Furthermore it shows positive transfer on each environment, whereas LSTM only shows positive transfer on Ms. Pacman.",5.5. ATARI action repeat transfer,[1.0],"['Furthermore it shows positive transfer on each environment, whereas LSTM only shows positive transfer on Ms. Pacman.']"
How to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question.,6. Discussion and future work,[1.0],['How to create agents that can learn to decompose their behaviour into meaningful primitives and then reuse them to more efficiently acquire new behaviours is a long standing research question.']
The solution to this question may be an important stepping stone towards agents with general intelligence and competence.,6. Discussion and future work,[1.0],['The solution to this question may be an important stepping stone towards agents with general intelligence and competence.']
"This paper introduced FeUdal Networks, a novel architecture that formulates sub-goals as directions in latent state space, which, if followed, translate into a meaningful behavioural primitives.",6. Discussion and future work,[0],[0]
FuN clearly separates the module that discovers and sets sub-goals from the module that generates the behaviour through primitive actions.,6. Discussion and future work,[0],[0]
This creates a natural hierarchy that is stable and allows both modules to learn in complementary ways.,6. Discussion and future work,[0],[0]
Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable.,6. Discussion and future work,[1.0],['Our experiments clearly demonstrate that this makes long-term credit assignment and memorisation more tractable.']
"This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability.",6. Discussion and future work,[1.0],"['This also opens many avenues for further research, for instance: deeper hierarchies can be constructed by setting goals at multiple time scales, scaling agents to truly large environments with sparse rewards and partial observability.']"
"The modular structure of FuN is also lends itself to transfer and multitask learning – learnt behavioural primitives can be re-used to acquire new complex skills, or alternatively the transitional policies of the Manager can be transferred to agents with different embodiment.",6. Discussion and future work,[1.0],"['The modular structure of FuN is also lends itself to transfer and multitask learning – learnt behavioural primitives can be re-used to acquire new complex skills, or alternatively the transitional policies of the Manager can be transferred to agents with different embodiment.']"
"We thank Daan Wierstra, Olivier Pietquin, Tejas Kulkarni, Alex Graves, Oriol Vinyals, Joseph Modayil and Vlad Mnih for many helpful discussions, suggestions and comments on the paper.",Acknowledgements,[0],[0]
We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning.,abstractText,[0],[0]
"Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels – allowing it to utilise different resolutions of time.",abstractText,[0],[0]
Our framework employs a Manager module and a Worker module.,abstractText,[0],[0]
The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker.,abstractText,[0],[0]
The Worker generates primitive actions at every tick of the environment.,abstractText,[0],[0]
The decoupled structure of FuN conveys several benefits – in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager.,abstractText,[0],[0]
These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.,abstractText,[0],[0]
FeUdal Networks for Hierarchical Reinforcement Learning,title,[0],[0]
