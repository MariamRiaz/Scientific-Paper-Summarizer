0	28	Ordinal classification (sometimes called ordinal regression) is a prediction task in which the classes to be predicted are discrete and ordered in some fashion.
1	43	This is different from discrete classification in which the classes are not ordered, and different from regression in that we typically do not know the distances between the classes (unlike regression, in which we know the distances because the predictions lie on the real number line).
2	28	Some examples of ordinal classification tasks include predicting the stages of disease for a cancer (Gentry et al., 2015), predicting what star rating a user gave to a movie (Koren & Sill, 2011), or predicting the age of a person (Eidinger et al., 2014).
4	21	The former ignores the inherent ordering between the classes, while the latter takes into account the distances between them (due to the square in the error term) but assumes that the labels are actually real-valued – that is, adjacent classes are equally distant.
5	15	Furthermore, the cross-entropy loss – under a one-hot target encoding – is formulated such that it only ‘cares’ about the ground truth class, and that probability estimates corresponding to the other classes may not necessarily make sense in context.
7	45	Highlighted in orange is the ground truth (i.e. the image is of an adult), and all probability distributions have identical cross-entropy: this is because the loss only takes into account the ground truth class, − log(p(y|x)c), where c = adult, and all three distributions have the same probability mass for the adult class.
10	46	However, A and B are both unusual, because the probability mass does not gradually decrease to the left and right of the ground truth.
11	70	In other words, it seems unusual to place more confidence on ‘schooler’ than ‘teen’ (distribution A) considering that a teenager looks more like an adult than a schooler, and it seems unusual to place more confidence on ’baby’ than ’teen’ considering that again, a teenager looks more like an adult than a baby.
43	21	The Poisson distribution is commonly used to model the probability of the number of events, k ∈ N ∪ 0 occurring in a particular interval of time.
46	15	While we are not actually using this distribution to model the occurrence of events, we can make use of its probability mass function (PMF) to enforce discrete unimodal probability distributions.
49	67	(3) If we let f(x) denote the scalar output of our deep net (where f(x) > 0 which can be enforced with the softplus nonlinearity), then we denote h(x)j to be: j log(f(x))− f(x)− log(j!
55	15	We note that the term in equation (4) can be re-arranged and simplified to h(x)j = j log(f(x))− f(x)− log(j!)
61	25	We can see that all distributions are unimodal and that by gradually increasing f(x) we gradually change which class has the most mass associated with itself.
65	25	An unfortunate side effect of using the Poisson distribution is that the variance is equivalent to the mean, λ.
72	66	The probability mass function for this distribution – for k successes (where 0 ≤ k ≤ K − 1), given K − 1 trials and success probability p – is: p(k;K − 1, p) = ( K − 1 k ) pk(1− p)K−1−k (8) In the context of applying this to a neural network, k denotes the class we wish to predict, K − 1 denotes the number of classes (minus one since we index from zero), and p = f(x) ∈ [0, 1] is the output of the network that we wish to estimate.
73	33	While no normalisation is theoretically needed since the binomial distribution’s support is finite, we still had to take the log of the PMF and normalise with a softmax to address numeric stability issues.
75	17	Just like with the Poisson formulation, we can introduce the temperature term τ into the resulting softmax to control for the variance of the resulting distribution.
81	70	In this dataset, we try and predict from five levels of diabetic retinopathy: no DR (25,810 images), mild DR (2,443 images), moderate DR (5,292 images), severe DR (873 images), or proliferative DR (708 images).
93	23	We conduct the following experiments for both DR and Adience datasets: • (Baseline) cross-entropy loss.
102	35	• EMD loss (equation 1) where ` = 2 (i.e. Euclidean norm) and the entire term is squared (to get rid of the square root induced by the norm) using Poisson and binomial extensions at the end of architecture.
109	68	It is also important to note that in the case of ordinal prediction, there are two ways to compute the final prediction: simply taking the argmax of p(y|x) (which is what is simply done in discrete classification), or taking a ‘smoothed’ prediction which is simply the expectation of the integer labels w.r.t.
113	23	One benefit of the former however is that we can use it to easily rank our predictions, which can be important if we are interested in computing top-k accuracy (rather than top1).
114	25	We also introduce an ordinal evaluation metric – the quadratic weighted kappa (QWK) (Cohen, 1968) – which has seen recent use on ordinal competitions on Kaggle.
115	28	Intuitively, this is a number between [-1,1], where a kappa κ = 0 denotes the model does no better than random chance, κ < 0 denotes worst than random chance, and κ > 0 better than random chance (with κ = 1 being the best score).
116	27	The ‘quadratic’ part of the metric imposes a quadratic penalty on misclassifications, making it an appropriate metric to use for ordinal problems.3 All experiments utilise an `2 norm of 10−4, ADAM optimiser (Kingma & Ba, 2014) with initial learning rate 10−3, and batch size 128.
143	222	We can see that even with the worst-performing model – the Poisson formulation with τ = 1 (orange) – produces a better top-3 accuracy than the cross-entropy baseline (blue).
146	35	We evaluate our technique on two ordinal image datasets and obtain results competitive or superior to the cross-entropy baseline for both the quadratic weighted kappa (QWK) metric and topk accuracy for both cross-entropy and EMD losses, especially under the binomial distribution.
147	19	Lastly, the unimodal constraint can makes the probability distributions behave more sensibly in certain settings.
148	44	However, there may be ordinal problems where a multimodal distribution may be more appropriate.
149	43	We leave an exploration of this issue for future work.
