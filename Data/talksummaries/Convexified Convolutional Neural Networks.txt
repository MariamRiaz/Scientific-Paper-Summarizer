27	16	However, this method is not equipped with the optimality guarantees that we provide for CCNNs in this paper, even for learning one convolutional layer.
29	53	Daniely et al. (2016) show that a randomly initialized CNN can extract features as powerful as kernel methods, but it is not clear how to provably improve the model from random initialization.
31	24	For a rectangular matrix A, let ‖A‖∗ be its nuclear norm, ‖A‖2 be its spectral norm (i.e., maximal singular value), and ‖A‖F be its Frobenius norm.
34	42	In this section, we formalize the class of convolutional neural networks to be learned and describe the associated nonconvex optimization problem.
38	34	• Second, given some choice of activation function σ : R→ R and a collection of weight vectors {wj}rj=1 in Rd1 , we define the functions hj(z) := σ(w > j z) for each patch z ∈ Rd1 .
49	15	We assume that the loss function L is convex and L-Lipschitz in its first argument given any value of its second argument.
61	45	In order to develop intuition for our approach, let us begin by considering the simple case of the linear activation function σ(t) = t. In this case, the filter hj when applied to the patch vector zp(x) outputs a Euclidean inner product of the form hj(zp(x)) = 〈zp(x), wj〉.
62	33	For each x ∈ Rd0 , we first define the P × d1-dimensional matrix Z(x) := z1(x) > ... zP (x) >  .
63	27	(5) We also define the P -dimensional vector αk,j := (αk,j,1, .
64	13	With this notation, we can rewrite equation (2) for the kth output as fk(x) = r∑ j=1 P∑ p=1 αk,j,p〈zp(x), wj〉 = r∑ j=1 α>k,jZ(x)wj = tr ( Z(x) ( r∑ j=1 wjα > k,j )) = tr(Z(x)Ak), (6) where in the final step, we have defined the d1 × P - dimensional matrix Ak := ∑r j=1 wjα > k,j .
67	13	See Figure 1 for a graphical illustration of this model structure.
73	20	The matrix A of rank r can be decomposed as A = UV >, where both U and V have r columns.
75	44	The matrices satisfying constraints (C1) and (C2) form a nonconvex set.
77	31	It is straightforward to verify that any matrix A satisfying the constraints (C1) and (C2) must have nuclear norm bounded as ‖A‖∗ ≤ B1B2r √ d2.
81	79	For nonlinear activation functions σ, we relax the class of CNN filters to a reproducing kernel Hilbert space (RKHS).
82	38	As we will show, this relaxation allows us to reduce the problem to the linear activation case.
85	71	See Section 3.4 for the choice of the kernel function and the activation function.
86	32	Let S := {zp(xi) : p ∈ [P ], i ∈ [n]} be the set of patches in the training dataset.
87	16	The representer theorem then implies that for any patch zp(xi) ∈ S, the function value can be represented by h(zp(xi)) = ∑ (i′,p′)∈[n]×[P ] ci′,p′K(zp(xi), zp′(xi′)) (10) for some coefficients {ci′,p′}(i′,p′)∈[n]×[P ].
88	33	Filters of the form (10) are members of the RKHS, because they are linear combinations of basis functions z 7→ K(z, zp′(xi′)).
96	29	Then the problem reduces to learning a linear filter with coefficient vector w. Carrying out all of Sec- tion 3.1, solving the ERM gives us a parameter matrix A ∈ Rm×Pd2 .
103	14	When we learn multi-layer CCNNs (Section 4), we need to compute the filters hj explicitly in order to form the inputs to the next layer.
106	13	Then set the j-th filter hj to the mapping z 7→ 〈Ûj , Q†v(z)〉 for any patch z ∈ Rd1 , (11) where Ûj ∈ Rm is the j-th column of matrix Û , and Q†v(z) represents the feature vector for patch z.2 The matrix V̂ > encodes parameters of the output layer, thus Algorithm 1 Learning two-layer CCNNs Input: Data {(xi, yi)}ni=1, kernel function K, regularization parameter R > 0, number of filters r. 1.
115	16	It is important to note that the filter retrieval is not unique, because the rank-r approximation of the matrix A is not unique.
116	79	The heuristic we suggest is to form the singular value decomposition A = UΛV >, then define Û to be the first r columns of U .
118	30	The algorithm for learning a two-layer CCNN is summarized in Algorithm 1; it is a formalization of the steps described in Section 3.2.
126	136	Setting m = nP allows us to solve the exact kernelized problem, but to improve the computation efficiency, we can use Nyström approximation (Drineas & Mahoney, 2005) or random feature approximation (Rahimi & Recht, 2007); both are randomized methods to obtain a tall-and-thin matrix Q ∈ RnP×m such thatK ≈ QQ>.
138	13	Let f̂ccnn be the CCNN that minimizes the empirical risk (12) using one of the two kernels above.
150	13	We are ready to state the main theoretical result.
169	57	From an algorithmic perspective, we don’t need to know the activation function for executing Algorithm 1.
