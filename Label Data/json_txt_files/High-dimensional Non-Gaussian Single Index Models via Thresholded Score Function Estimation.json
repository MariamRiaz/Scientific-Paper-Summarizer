{"sections": [{"heading": "1. Introduction", "text": "Estimators for high-dimensional parametric (linear) models have been developed and analyzed extensively in the last two decades (see for example (Bu\u0308hlmann & van de Geer, 2011; Vershynin, 2015) for comprehensive overviews). While being a useful testbed for illustrating conceptual phenomenon, they often suffer from a lack of flexibility in modeling real-world situations. On the other hand, completely nonparametric models, although flexible, suffer from the curse of dimensionality unless restrictive additive sparsity or smoothness assumptions are imposed (Ravikumar et al., 2009; Yuan et al., 2016). An interesting compromise between the parametric and nonparametric models is provided by the so-called semiparametric index models (Horowitz, 2009). Here, the response and the covariate are linked through a low-dimensional nonparametric function that takes in as input a linear transformation of the covariate. The nonparametric component is also called as the link function and the linear components are\n1 Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA. Correspondence to: Han Liu <hanliu@princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ncalled as the indices.\nIn this work, we focus on the simplest family of such models, the single index models (SIMs), which assume that the response Y and the covariate X satisfy Y = f(\u3008X,\u03b2\u2217\u3009) + , where \u03b2\u2217 is the true signal, is the mean-zero random noise, and f is a univariate link function. (see \u00a72 for the precise definition). They form the basis of more complicated models such as Multiple Index Models (MIMs) (Diaconis & Shahshahani, 1984) and Deep Neural Networks (DNNs) (LeCun et al., 2015), which are cascades of MIMs. Moreover, we focus on the task of estimating the parametric (linear) component \u03b2\u2217 without the knowledge of the nonparametric part f in the high-dimensional setting, where the number of samples is much smaller than the dimensionality of \u03b2\u2217.\nEstimating the parametric component without depending on the specific form of the nonparametric part appears naturally in several situations. For example, in one-bit compressed sensing (Boufounos & Baraniuk, 2008) and sparse generalized linear models (Loh & Wainwright, 2015), we are interested in recovering the true signal vector based on nonlinear measurements. Furthermore, in a DNN, the activation function is pre-specified and the task is to estimate the linear components, which are used for prediction in the test stage. Performing nonlinear least-squares in this setting, leads to nonconvex optimization problems that are invariably sub-optimal without further assumptions. Hence, developing estimators for the linear component that are both statistically accurate and computationally efficient for a class of activation functions provide a compelling alternative. Understanding such estimators for SIMs is hence crucial for understanding the more complicated DNNs.\nAlthough SIMs appear to be a simple extension of the standard linear models, most existing work in the highdimensional setting assume X follows a Gaussian distribution for estimating \u03b2\u2217 without the knowledge of the nonparametric part. It is not clear whether those estimation methods are still valid and optimal when X is drawn from a more general class of distributions. To relax the Gaussian assumption, we study the setting where the distribution of X is non-Gaussian but known a priori."}, {"heading": "1.1. Challenges of the Single Index Models", "text": "There are significant challenges that appear when we are dealing with estimators for SIMs. They can be summarized as assumptions on either the link function or the data distribution (for example, non-Gaussian assumption).\n1. Knowledge of link function: Suppose the link function is known, for example, f(u) = u2 which corresponds to the phase retrieval model (see (Jaganathan et al., 2015) for a survey and history of this model). Then using an M-estimator to estimate \u03b2\u2217 is a natural procedure (Jaganathan et al., 2015). But computationally the problem becomes nonconvex and one need to resort to either SDP based convex relaxations that are computationally expensive or do non-convex alternating minimization that require Gaussian assumptions on the data for successful initialization in the highdimensional setting (Cai et al., 2015). Furthermore, if the link function is changed, it might become challenging or impossible to obtain provably computable estimators.\n2. Knowledge of data distribution: Now suppose we want to be agnostic about the link function, i.e., we want to estimate the linear component for a general class of link functions. Then it becomes necessary to make assumptions about the distribution from which the covariates are sampled from. In particular, assuming the covariate has Gaussian and symmetric elliptical distributions respectively, (Plan & Vershynin, 2016) and (Goldstein et al., 2016) propose estimators in the high-dimensional setting for a large class of unknown link functions.\nAs mentioned previously, our estimators are based on Stein\u2019s Lemma for non-Gaussian distributions, which utilizes the score function. Estimating with the score function is challenging due to their heavy tails. In order to illustrate that, consider the univariate histograms provided in Figure1. The dark shaded, more concentrated one corresponds to the histogram of 10000 i.i.d. samples from Gamma distribution with scale and shape parameters set to 5 and 0.2 respectively. The transparent histogram corresponds to the distribution of the score function of the same Gamma distribution. Note that even when the actual Gamma distribution is well concentrated, the distribution of the corresponding score function is well-spread and heavy-tailed. In the high dimensional setting, in order to estimate with the score functions, we require certain vectors or matrices based on the score functions to be well-concentrated in appropriate norms. In order to achieve that, we construct robust estimators via careful truncation arguments to balance the bias (due to thresholding)-variance (of the estimator) tradeoff and achieve the required concentration."}, {"heading": "1.2. Related Work", "text": "There is a significant body of work on SIMs in the lowdimensional setting. They are based on assumptions on either the distribution of the covariate or the link functions. Assuming a monotonic link function, (Han, 1987; Sherman, 1993) propose the maximum rank correlation estimator exploiting the relationship between monotonic functions and rank-correlations. Furthermore, (Li & Duan, 1989) propose an estimator for a wide class of unknown link functions under the assumption that the covariate follows a symmetric elliptical distribution. This assumption is restrictive as often times the covariates are not from a symmetric distribution. For example, in several economic applications where the covariates are usually highly skewed and heavy-tailed (Horowitz, 2009). A line of work for estimation in SIMs is proposed by Ker-Chau Li which is based on sliced inverse regression (Li, 1991) and principal Hessian directions (Li, 1992) . These estimators are based on similar symmetry assumptions and involve computing second-order (conditional and unconditional) moments which are difficult to estimate in high-dimensions without restrictive assumptions.\nThe success of Lasso and related linear estimators in highdimensions (Bu\u0308hlmann & van de Geer, 2011), also enabled the exploration of high-dimensional SIMs. Although, this is very much work in progress. As mentioned previously, (Plan & Vershynin, 2016) show that the Lasso estimator works for the SIMs in high dimensions when the data is Gaussian. A more tighter albeit an asymptotic results under the same setting was proved in (Thrampoulidis et al., 2015). Very recently (Goldstein et al., 2016) extend\nthe results of (Li & Duan, 1989) to the high dimensional setting but it suffers from similar problems as mentioned in the low-dimensional setting. For the case of monotone nonparametric component, (Yang et al., 2015) analyze a non-convex least squares approach under the assumption that the data is sub-Gaussian. However, the success of their method hinges on the knowledge of the link function. Furthermore, (Jiang & Liu, 2014; Lin et al., 2015; Zhu et al., 2006) analyze the sliced inverse regression estimator in the high-dimensional setting concentrating mainly on support recovery and consistency properties. Similar to the low-dimensional case, the assumptions made on the covariate distribution restrict them from several real-world applications involving non-Gaussian or non-symmetric covariate, for example high-dimensional problems in economics (Fan et al., 2011). Furthermore, several results are established on a case-by-case basis for fixed link function. Specifically (Boufounos & Baraniuk, 2008; Ai et al., 2014) and (Davenport et al., 2014) consider 1-bit compressed sensing and matrix completion respectively, where the link is assumed to be the sign function. Also, (Waldspurger et al., 2015) and (Cai et al., 2015) propose and analyze convex and non-convex estimators for phase retrieval respectively, in which the link is the square function. All the above works, except (Ai et al., 2014) make Gaussian assumptions on the data and are specialized for the specific link functions. The non-asymptotic result obtained in (Ai et al., 2014) is under sub-Gaussian assumptions, but the estimator is not consistent. Finally, there is a line of work focussing on estimating both the parametric and the nonparametric component (Kalai & Sastry, 2009; Kakade et al., 2011; Alquier & Biau, 2013; Radchenko, 2015). We do not focus on this situation in this paper as mentioned before.\nTo summarize, all the above works require restrictive assumption on either the data distribution or on the link function. We propose and analyze an estimator for a class of (unknown) link functions for the case when the covariates are drawn from a non-Gaussian distribution \u2013 under the assumption that we know the distribution a priori. Note that in several situations, one could fit specialized distributions, to real-world data that is often times skewed and heavytailed, so that it provides a good generative model of the data. Also, mixture of Gaussian distribution, with the number of components selected appropriately, approximates the set of all square integrable distributions to arbitrary accuracy (see for example (McLachlan & Peel, 2004)). Furthermore, since this is a density estimation problem it is unlabeled and there is no issue of label scarcity. Hence it is possible to get accurate estimate of the distribution in most situations of interest. Thus our work is complementary to the existing literature and provides an estimator for a class of models that is not addressed in the previous works. We conclude this section with a summary of our main contri-\nbutions in this paper:\n\u2022 We propose estimators for the parametric component of a sparse SIM and low-rank SIM for a class of unknown link function under the assumption that the covariate distribution is non-Gaussian but known a priori.\n\u2022 We show that it is possible to recover a s-sparse ddimensional vector and a rank-r, d1\u00d7 d2 dimensional matrix with number of samples of the order of s log d and r(d1 +d2) log(d1 +d2) respectively under significantly mild moment assumptions in the SIM setting.\n\u2022 We provide numerical simulation results that confirm our theoretical predictions."}, {"heading": "2. Single Index Models", "text": "In this section, we introduce the notation and define the single index models. Throughout this work, we use [n] to denote the set {1, . . . , n}. In addition, for a vector v \u2208 Rd, we denote by \u2016v\u2016p the `p-norm of v for any p \u2265 1. We use Sd\u22121 to denote the unit sphere in Rd, which is defined as Sd\u22121 = {v \u2208 Rd : \u2016v\u20162 = 1}. In addition, we define the support of v \u2208 Rd as supp(v) = {j \u2208 [d], vj 6= 0}. Moreover, we denote the nuclear norm, operator norm, and Frobenius norm of a matrixA \u2208 Rd1\u00d7d2 by \u2016\u00b7\u2016?, \u2016\u00b7\u2016op, and \u2016 \u00b7 \u2016fro, respectively. We denote by vec(A) the vectorization of matrix A, which is a vector in Rd1\u00b7d2 . For two matrices A,B \u2208 Rd1\u00d7d2 we define the trace inner product as \u3008A,B\u3009 = Trace(A>B). Note that it can be viewed as the standard inner product between vec(A) and vec(B). In addition, for an univariate function g : R \u2192 R, we denote by g \u25e6 (v) and g \u25e6 (A) the output of applying g to each element of a vector v and a matrixA, respectively. Finally, for a random variable X \u2208 R with density p, we use p\u2297d : Rd \u2192 R to denote the joint density of {X1, \u00b7 \u00b7 \u00b7 , Xd}, which are d identical copies of X .\nNow we are ready to define the statistical model. Let f : R \u2192 R be an univariate function and \u03b2\u2217 be the parameter of interest, which is a structured vector or a matrix. The single index model in general is formulated as\nY = f(\u3008X,\u03b2\u2217\u3009) + , (2.1)\nwhere X is the covariate, Y \u2208 R is the response, and is the exogenous noise that is independent of X . We assume that is centered and has bounded fourth moment, i.e., Ep0( ) = 0 and E( 4) \u2264 C for an absolute constant C > 0. Note in particular that this allows for heavy-tailed noise as well. In addition, we assume that the entries of X are i.i.d. random variables with density p0. This assumption could be further relaxed using more sophisticated concentration arguments; here we focus on the i.i.d. setting to clearly present the main message of this paper.\nLet {(Yi, Xi)}ni=1 be n i.i.d. observations of the SIM. Our goal is to consistently estimate \u03b2\u2217 without the knowledge of f . In particular, we focus on the case when \u03b2\u2217 is either sparse or low-rank, which are defined as follows.\nSparse single index model: In this setting, we assume that \u03b2\u2217 = (\u03b2\u22171 , \u00b7 \u00b7 \u00b7 , \u03b2\u2217d)> is a sparse vector in Rd with s\u2217 nonzero entries, such that s\u2217 n d. Moreover, for the model in (2.1) to be identifiable, we further assume \u03b2\u2217 lies on the unit sphere Sd\u22121 as the norm of \u03b2\u2217 can always be absorbed in the unknown link function f .\nLow-rank single index model: In this setting, we assume that \u03b2\u2217 \u2208 Rd1\u00d7d2 has rank r\u2217 min{d1, d2}. In this scenario, X \u2208 Rd1\u00d7d2 and the inner product in (2.1) is \u3008X,\u03b2\u2217\u3009 = Trace(X>\u03b2\u2217). For model identifiability, we further assume that \u2016\u03b2\u2217\u2016F = 1, similar to the sparse case."}, {"heading": "3. Estimation via Score Functions", "text": "Our estimator is primarily motivated by an interesting phenomenon illustrated in (Plan & Vershynin, 2016) for the Gaussian setting. Below, we first briefly summarize the result from (Plan & Vershynin, 2016) and then provide our alternative justification for the same result via Stein\u2019s Lemma. We mainly leverage this alternative justification and propose our estimators for the more general setting we consider. Assuming for simplicity, we work in the onedimensional setting and are given n i.i.d. samples from the SIM. Consider the least-squares estimator\n\u03b2\u0302LS = argmin \u03b2\u2208R\n1\nn n\u2211 i=1 (Yi \u2212Xi\u03b2)2 .\nNote that the above estimator is the standard least-squares estimator assuming a linear model (i.e., identity link function). The surprising observation from (Plan & Vershynin, 2016) is that, under the crucial assumption that X is standard Gaussian, \u03b2\u0302LS is a good estimator of \u03b2\u2217 (up to a scaling) even when the data is generated from the nonlinear SIM. The same holds true for the high-dimensional setting when the minimization is performed in an appropriately constrained norm-ball (for example, the `1-ball). Hence the theory developed for the linear setting could be leveraged to understand the performance in the SIM setting. Below, we give an alternative justification for the above estimator as an implication of Stein\u2019s Lemma in the Gaussian case, which is summarized as follows. Proposition 3.1 (Gaussian Stein\u2019s Lemma (Stein, 1972)). Let X \u223c N(0, 1) and g : R \u2192 R be a continuos function such that E|g\u2032(X)| \u2264 \u221e. Then we have E[g(X)X] = E[g\u2032(X)].\nNote that in our context for SIMs, we have E[f \u2032(X)] \u221d \u03b2\u2217 and E[f(X)X] = E[Y \u00b7 X]. Now consider the following\nestimator, which is based on performing least-squares on the sample version of the above proposition:\n\u03b2\u0302SL = argmin \u03b2\u2208R\n1\nn n\u2211 i=1 (YiXi \u2212 \u03b2)2\nNote that \u03b2\u0302LS and \u03b2\u0302SL are the same estimators assuming X \u223c N(0, 1), as n \u2192 \u221e. This observation leads to an alternative interpretation of the estimator proposed by (Plan & Vershynin, 2016) via Stein\u2019s Lemma for Gaussian random variables. Thus it provides an alternative justification for why the linear least-squares estimator should work in the SIM setting. This observation naturally leads to leveraging non-Gaussian versions of Stein\u2019s Lemma for dealing with non-Gaussian covariates.\nWe now describe our estimator for the non-Gaussian setting based on the above observation. We first define the score function associate to a density. Let p : Rd \u2192 R be a probability density function defined on Rd. The score function Sp : Rd \u2192 R associated to p is defined as\nSp(x) = \u2212\u2207x[log p(x)] = \u2212\u2207xp(x)/p(x).\nNote that in the above definition, the derivative is taken with respect to x. This is different from the more traditional definition of the score function where the density belongs to a parametrized family and the derivative is taken with respect to the parameters. In the rest of the paper to simplify the notation, we omit the subscript x from \u2207x. We also omit the subscript p from Sp when the underlying density p is clear from the context.\nWe now describe a version of Stein\u2019s Lemma that is applicable for non-Gaussian random variables. Note from the motivating example for the Gaussian case that while utilizing the Stein\u2019s Lemma for SIM estimation, assumptions on the function in Stein\u2019s Lemma translate directly to those on the link function in SIM. We now introduce a version of Stein\u2019s Lemma that applies to non-Gaussian random variables and for continuously differentiable functions from (Stein et al., 2004). A more general version of the Stein\u2019s Lemma that applies to a class of regular functions is available in (Stein et al., 2004). We assume continuously differentiable functions in the Stein\u2019s Lemma below as they cover a wide range of practical SIM such as generalized linear models and single-layer neural networks.\nLemma 3.2 (Non-Gaussian Stein\u2019s Lemma (Stein et al., 2004)). Let g : Rd \u2192 R be continuously differentiable function and X \u2208 Rd be a random vector with density p : Rd \u2192 R, which is also continuously differentiable. Under the assumption that the expectations E[g(X) \u00b7 S(X)] and E[\u2207g(X)] are both well-defined, we have the follow-\ning generalized Stein\u2019s identity E[g(X) \u00b7 S(X)] = \u2212 \u222b Rd g(x) \u00b7 \u2207p(x)dx\n= \u222b Rd \u2207g(x) \u00b7 p(x)dx = E[\u2207g(X)]. (3.1)\nRecall that in the two single index models introduced in \u00a72, X in (2.1) has i.i.d. entries with density p0. To unify both the vector and matrix settings, in the low-rank SIM, we identify X with vec(X) \u2208 Rd where d = d1 \u00b7 d2. In this case, X has density p = p\u2297d0 and the corresponding score function S : Rd \u2192 Rd is given by\nS(x) = \u2212\u2207 log p(x) = \u2212\u2207p(x)/p(x) = s0 \u25e6 (x), (3.2)\nwhere the univariate function s0 = p\u20320/p0 is applied to each entry of x. Thus S(X) has i.i.d. entries. In addition, by Lemma 3.2, we have E[S(X)] = 0 by setting g to be a constant function in (3.1). Moreover, in the context of SIMs specified in (2.1), we have\nE[Y \u00b7 S(X)] = E [ f(\u3008X,\u03b2\u2217\u3009) \u00b7 S(X) ] = E[f \u2032(\u3008X,\u03b2\u2217\u3009)] \u00b7 \u03b2\u2217,\nas long as the density and the link function satisfy the conditions stated in Lemma 3.2. This implies that optimization problem\nminimize \u03b2\u2208Rd\n{ \u3008\u03b2, \u03b2\u3009 \u2212 2E[Y \u00b7 \u3008S(X), \u03b2\u3009] } (3.3)\nhas solution \u03b2 = \u00b5 \u00b7\u03b2\u2217, where \u00b5 = E[f \u2032(\u3008X,\u03b2\u2217\u3009)]. Hence the above program could be used to obtain the unknown \u03b2\u2217 as long as \u00b5 6= 0. Before we proceed to describe the sample version of the above program, we make the following brief remark. The requirement \u00b5 6= 0 rules out in particular the use of our approach for non-Gaussian phase retrieval (where f(u) = u2) as in that case we have \u00b5 = 0 when X is centered. But we emphasize that the same holds true in the Gaussian and elliptical setting as well, as noted in (Plan & Vershynin, 2016) and (Goldstein et al., 2016). Their methods also fail to recover the true \u03b2\u2217 when the SIM model corresponds to phase retrieval. We refer the reader to \u00a76 for a discussion on overcoming this limitation.\nFinally, we use a sample version of the above program as an estimator for the unknown \u03b2\u2217. In order to deal with the high-dimensional setting, we consider a regularized version of the above formulation. More specifically, we use the `1-norm and nuclear norm regularization in the vector and matrix settings respectively. However, a major difficulty in the sample setting for this procedure is that E[Y \u00b7 S(X)] and its empirical counterpart may not be close enough due to a lack of concentration. Recall our discussion from \u00a71.1 that even if the random variable X is light-tailed, its scorefunction S(x) might be arbitrarily heavy-tailed. Furthermore, bounded-fourth moment assumption on the noise, Y\ntoo can be heavy-tailed. Thus the naive method of using the sample version of (3.3) to estimate \u03b2\u2217 leads to sub-optimal statistical rates of convergence.\nTo improve concentration and obtain optimal rates of convergence, we replace Y \u00b7 S(X) with a transformed random variable T (Y,X), which will be defined precisely in \u00a74 for the sparse and low-rank cases. In particular, T (Y,X) is a carefully truncated version of Y \u00b7S(X), introduced and analyzed in (Catoni et al., 2012; Fan et al., 2016) for related problems, that enables us to obtain well-concentrated estimators. Thus our final estimator \u03b2\u0302 is defined as the solution to the following regularized optimization problem\nminimize \u03b2\u2208Rd\nL(\u03b2) + \u03bb \u00b7R(\u03b2), (3.4)\nL(\u03b2) = \u3008\u03b2, \u03b2\u3009 \u2212 2 n n\u2211 i=1 \u3008T (Yi, Xi), \u03b2 \u232a ,\nwhere \u03bb > 0 is the regularization parameter which will be specified later and R(\u00b7) is the `1-norm in the vector case and the nuclear norm in the matrix case."}, {"heading": "4. Theoretical Results", "text": "In this section, we state our main results in Theorem 4.2 and Theorem 4.3,which establish the statistical rates of convergence of the estimator defined in \u00a73. The proof for both Theorems is presented in the supplementary material. Before doing so, we introduce our main moment assumption for the single index model. This assumption is made apart from the assumptions made on the noise and the link function in \u00a72 and \u00a73 respectively. Recall that each entry of the score function defined in (3.2) is equal to s0(u) = \u2212p\u20320(u)/p0(u). We first state the assumption and make a few remarks about it.\nAssumption 4.1. There exists an absolute constant M > 0 such that E(Y 4) \u2264 M and Ep0 [s40(U)] \u2264 M , where random variable U \u2208 R has density p0.\nConsider the assumption E(Y 4) \u2264 M . By CauchySchwarz inequality we have E(Y 4) \u2264 4E( 4) + 4E[f4(\u3008X,\u03b2\u2217)]. Note that we assum to be centered, independent of X and has bounded fourth moment (see \u00a72). If the covariate X has bounded fourth moment along the direction of true parameter, since f(\u00b7) is continuously differentiable, f(\u3008X,\u03b2\u2217\u3009) has bounded fourth moment as well if f(\u00b7) is defined on a compact subset of R. Hence the condition E(Y 4) \u2264 M is relatively easy to satisfy and significantly milder than assuming that Y is bounded or has lighter tails. Furthermore, Ep0 [s40(U)] \u2264 M is relatively mild and satisfied by a wide class of random variables. Specifically random variables that are non-symmetric and non-Gaussian satisfy this property thereby allowing our approach to work with covariates not previously possible.\nWe believe it is highly non-trivial to weaken this condition without losing significantly in the rates of convergence that we discuss below."}, {"heading": "4.1. Sparse Single Index Model", "text": "Under the above assumptions, we first state our theorem on the sparse SIM. As discussed in \u00a73, Y \u00b7 S(X) can by heavy-tailed and hence we apply truncation to achieve concentration. Denote the j-th entry of the score function S in (3.2) as Sj : Rd \u2192 R, j \u2208 [d]. We define the truncated response and score function as\nY\u0303 = sign(Y ) \u00b7 (|Y | \u2227 \u03c4), S\u0303j(x) = sign[Sj(x)] \u00b7 [ |Sj(x)| \u2227 \u03c4 ] , (4.1)\nwhere \u03c4 > 0 is a predetermined threshold value. We define Y\u0303i similarly for all Yi, i \u2208 [n]. Then we define the estimator \u03b2\u0302 as the solution to the optimization problem in (3.4) with T (Yi, Xi) = Y\u0303i \u00b7 S\u0303(Xi) and R(\u03b2) = \u2016\u03b2\u20161. Here we apply elementwise truncation in T to ensure the sample average of T converges to E[Y \u00b7S(X)] in the `\u221e-norm for an appropriately chosen \u03c4 . Note that the `\u221e-norm is the dual norm of the `1-norm. Such a convergence requirement in the dual norm is standard in the analysis of regularized M - estimators (Negahban et al., 2012) to achieve optimal rates. The following theorem characterizes the convergence rates of \u03b2\u0302.\nTheorem 4.2 (Signal recovery for the sparse single index model). For the sparse SIM defined in \u00a72, we assume that \u03b2\u2217 \u2208 Rd has s\u2217 nonzero entries. Under Assumption 4.1, we let \u03c4 = 2(M \u00b7 log d/n)1/4 in (4.1) and set the regularization parameter \u03bb in (3.4) as C \u221a M \u00b7 log d/n, where C > 0 is an absolute constant. Then with probability at least 1\u2212d\u22122, the `1-regularized estimator \u03b2\u0302 defined in (3.4) satisfies\n\u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u20162 \u2264 \u221a s\u2217 \u00b7 \u03bb, \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u20161 \u2264 4s\u2217 \u00b7 \u03bb.\nFrom this theorem, the `1- and `2-convergence rates of \u03b2\u0302 are \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u20161 = O(s\u2217 \u221a log d/n) and \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u20162 =\nO( \u221a s\u2217 log d/n), respectively. These rates match the convergence rates of sparse generalized linear models (Loh & Wainwright, 2015) and sparse single index models with Gaussian and symmetric elliptical covariates (Plan & Vershynin, 2016; Goldstein et al., 2016) which are known to be minimax-optimal for this problem via matching lower bounds."}, {"heading": "4.2. Low-rank Single Index Model", "text": "We next state our theorem for the low-rank SIM. In this case, we apply the nuclear norm regularization to promote low-rankness. Note that by definition, T is matrix-valued.\nSince the dual norm of the nuclear norm is the operator norm, we need the sample average of T to converge to E[Y \u00b7S(X)] in the operator norm rapidly to achieve optimal rates of convergence. To achieve such a goal, we leverage the truncation argument from (Catoni et al., 2012; Minsker, 2016) to construct T (Y,X).\nLet \u03c6 : R\u2192 R be a non-decreasing function such that\n\u2212 log(1\u2212x+x2/2) \u2264 \u03c6(x) \u2264 log(1+x+x2/2), \u2200x \u2208 R.\nBased on \u03c6, we define a linear mapping \u03c8 : Rd1\u00d7d2 \u2192 Rd1\u00d7d2 as follows. For any A \u2208 Rd1\u00d7d2 , let\nA\u0303 = [ 0 A A> 0 ] and let \u03a5\u039b\u03a5> be the eigenvalue decomposition of A\u0303. In addition, let B = \u03a5[\u03c6 \u25e6 (\u039b)]\u03a5>, where \u03c8 is applied elementwisely on \u039b. Then we write B in block from as\nB = [ B11 B12 B21 B22 ] and define \u03c8(A) = B12. Finally, we define T (Y,X) = 1/\u03ba \u00b7 \u03c8 [ \u03ba \u00b7 Y \u00b7 S(X) ] , where \u03ba > 0 will be specified later. Therefore, our final estimator \u03b2\u0302 \u2208 Rd1\u00d7d2 is defined as the solution to the optimization problem in (3.4) with R(\u03b2) = \u2016\u03b2\u2016?. We note here the minimization in (3.4) is taken over Rd1\u00d7d2 . The following theorem quantifies the convergence rates of the proposed estimator.\nTheorem 4.3 (Signal recovery for the low-rank single index model). For the low-rank single index model defined in \u00a72, we assume that rank(\u03b2\u2217) = r\u2217. Under Assumption 4.1, we let\n\u03ba = 2 \u221a n \u00b7 log(d1 + d2)\u221a (d1 + d2)M\nin T (Y,X). Moreover, the regularization parameter \u03bb in (3.4) is set to C \u221a M \u00b7 (d1 + d2) \u00b7 log(d1 + d2)/n, where C > 0 is an absolute constant. Then with probability at least 1\u2212 (d1 +d2)\u22122, the nuclear norm regularized estimator \u03b2\u0302 satisfies\n\u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u2016fro \u2264 3 \u221a r\u2217 \u00b7 \u03bb, \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u2016? \u2264 12r\u2217 \u00b7 \u03bb.\nBy this theorem, we have \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u2016fro = O( \u221a r\u2217(d1 + d2) \u00b7 log(d1 + d2)/n) and \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u2016? =\nO(r\u2217 \u00b7 \u221a\n(d1 + d2) \u00b7 log(d1 + d2)/n). Note that the rate obtained is minimax-optimal up to a logarithmic factor. Furthermore, it matches the rates for low-rank single index models with Gaussian and symmetric elliptical distributions up to a logarithmic factor (Plan & Vershynin, 2016; Goldstein et al., 2016)."}, {"heading": "5. Numerical Experiments", "text": "We assess the finite sample performance of the proposed estimators on simulated data. Throughout this section, we let \u223c N(0, 1) and set the link function in (2.1) as one of f1(u) = 3u+10 sin(u) and f2(u) = \u221a 2u+4 exp(\u22122u2), which are plotted in Figure 2. We set p0 to be one of (i) Gamma distribution with shape parameter 5 and scale parameter 1, (ii) Student\u2019s t-distribution with 5 degrees of freedom, and (iii) Rayleigh distribution with scale parameter 2. To measure the estimation accuracy, we use the cosine distance\ncos \u03b8(\u03b2\u0302, \u03b2\u2217) = 1\u2212 \u2016\u03b2\u0302\u2016\u22121\u2022 |\u3008\u03b2\u0302, \u03b2\u2217\u3009|,\nwhere \u2022 stands for the Euclidean norm in the vector case and the Frobenius norm when \u03b2\u2217 is a matrix. Here we report the cosine distance rather than \u2016\u03b2\u0302 \u2212 \u00b5\u03b2\u2217\u2016\u2022 to compare the performances for X having different distributions, where \u00b5 may have different values.\nFor the vector case, we fix d = 2000, s\u2217 = 5 and vary n. The support of \u03b2\u2217 is chosen uniformly random among all subsets of {1, . . . , d}. For each j \u2208 supp(\u03b2\u2217), we set \u03b2\u2217j = 1/ \u221a s\u2217 \u00b7 \u03b3j , where each \u03b3j is an i.i.d. Rademacher random variable. In addition, the regularization parameter \u03bb is set to 4 \u221a log d/n. We plot the cosine distance against\nthe signal strength \u221a s\u2217 log d/n in Figure 4-(a) and (b) for f1 and f2 respectively, based on 200 independent trials for each n. As shown in this figure, the estimation error grows sublinearly as a function of the signal strength.\nAs for the matrix case, we fix d1 = d2 = 20, r\u2217 = 3 and let n vary. The signal parameter \u03b2\u2217 is equal to USV >, where U, V \u2208 Rd\u00d7d are random orthogonal matrices and S is a diagonal matrix with r\u2217 nonzero entries. Moreover, we set the nonzero diagonal entries of S as 1/ \u221a r\u2217, which implies \u2016\u03b2\u2217\u2016fro = 1. We set the regularization parameter as \u03bb = 2 \u221a\n(d1 + d2) log(d1 + d2)/n. Furthermore, we use the proximal gradient descent algorithm (with the learning rate fixed to 0.05) to solve the nuclear norm regularization problem in (3.4). To present the result, we plot the cosine distant against the signal strength \u221a r\u2217(d1 + d2) log(d1 + d2)/n in Figure 4-(b) based on 200 independent trials. As shown in this figure, the error is bounded by a linear function of the signal strength, which corroborates Theorem 4.3."}, {"heading": "6. Conclusion", "text": "In this paper, we consider SIMs in the high-dimensional non-Gaussian setting and proposed estimators based on Stein\u2019s Lemma for a wider class of unknown link functions and covariate distributions. We consider both sparse and low-rank models and propose minimax rate-optimal estimators under fairly mild assumptions. An interesting avenue of future work is the problem of phase retrieval\nwith non-Gaussian data. Our current approach requires that \u00b5 6= 0 which is not applicable. The main reason this happens is we use a first-order version of Stein\u2019s Lemma. Such a problem could overcome by second-order Stein\u2019s Lemma (Janzamin et al., 2014). Obtaining rate-optimal estimators based on second-order score functions require addressing several challenges. Concentrating on phase retrieval (and sparse phase retrieval) we plan to report our results for the above problem in the near future."}], "year": 2017, "references": [{"title": "One-bit compressed sensing with non-gaussian measurements", "authors": ["Ai", "Albert", "Lapanowski", "Alex", "Plan", "Yaniv", "Vershynin", "Roman"], "venue": "Linear Algebra and its Applications,", "year": 2014}, {"title": "Sparse single-index model", "authors": ["Alquier", "Pierre", "Biau", "G\u00e9rard"], "venue": "The Journal of Machine Learning Research,", "year": 2013}, {"title": "Concentration inequalities: A nonasymptotic theory of independence", "authors": ["Boucheron", "St\u00e9phane", "Lugosi", "G\u00e1bor", "Massart", "Pascal"], "venue": "Oxford university press,", "year": 2013}, {"title": "1-bit compressive sensing", "authors": ["Boufounos", "Petros T", "Baraniuk", "Richard G"], "venue": "In Information Sciences and Systems,", "year": 2008}, {"title": "Statistics for highdimensional data: methods, theory and applications", "authors": ["B\u00fchlmann", "Peter", "van de Geer", "Sara"], "venue": "Springer Science & Business Media,", "year": 2011}, {"title": "Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow", "authors": ["Cai", "T Tony", "Li", "Xiaodong", "Ma", "Zongming"], "venue": "arXiv preprint arXiv:1506.03382,", "year": 2015}, {"title": "Challenging the empirical mean and empirical variance: a deviation study", "authors": ["Catoni", "Olivier"], "venue": "Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "year": 2012}, {"title": "1-bit matrix completion", "authors": ["Davenport", "Mark A", "Plan", "Yaniv", "van den Berg", "Ewout", "Wootters", "Mary"], "venue": "Information and Inference,", "year": 2014}, {"title": "On nonlinear functions of linear combinations", "authors": ["P. Diaconis", "M. Shahshahani"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "year": 1984}, {"title": "Sparse high-dimensional models in economics", "authors": ["J. Fan", "J. Lv", "L. Qi"], "venue": "Annual review of economics,", "year": 2011}, {"title": "Robust low-rank matrix recovery", "authors": ["Fan", "Jianqing", "Wang", "Weichen", "Zhu", "Ziwei"], "venue": "arXiv preprint arXiv:1603.08315,", "year": 2016}, {"title": "Structured signal recovery from non-linear and heavytailed measurements", "authors": ["Goldstein", "Larry", "Minsker", "Stanislav", "Wei", "Xiaohan"], "venue": "arXiv preprint arXiv:1609.01025,", "year": 2016}, {"title": "Non-parametric analysis of a generalized regression model: the maximum rank correlation estimator", "authors": ["Han", "Aaron K"], "venue": "Journal of Econometrics,", "year": 1987}, {"title": "Semiparametric and nonparametric methods in econometrics, volume 12", "authors": ["Horowitz", "Joel L"], "year": 2009}, {"title": "Phase retrieval: An overview of recent developments", "authors": ["Jaganathan", "Kishore", "Eldar", "Yonina C", "Hassibi", "Babak"], "venue": "arXiv preprint arXiv:1510.07713,", "year": 2015}, {"title": "Score function features for discriminative learning: Matrix and tensor framework", "authors": ["Janzamin", "Majid", "Sedghi", "Hanie", "Anandkumar", "Anima"], "venue": "arXiv preprint arXiv:1412.2863,", "year": 2014}, {"title": "Variable selection for general index models via sliced inverse regression", "authors": ["B. Jiang", "J.S. Liu"], "venue": "The Annals of Statistics,", "year": 2014}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "authors": ["Kakade", "Sham M", "Kanade", "Varun", "Shamir", "Ohad", "Kalai", "Adam"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "authors": ["Kalai", "Adam Tauman", "Sastry", "Ravi"], "venue": "In Conference on Learning Theory,", "year": 2009}, {"title": "Sliced inverse regression for dimension reduction", "authors": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "year": 1991}, {"title": "On principal Hessian directions for data visualization and dimension reduction: Another application of Stein\u2019s lemma", "authors": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "year": 1992}, {"title": "Regression analysis under link violation", "authors": ["Li", "Ker-Chau", "Duan", "Naihua"], "venue": "The Annals of Statistics,", "year": 1989}, {"title": "On consistency and sparsity for sliced inverse regression in high dimensions", "authors": ["Q. Lin", "Z. Zhao", "J.S. Liu"], "venue": "arXiv preprint arXiv:1507.03895,", "year": 2015}, {"title": "Regularized mestimators with nonconvexity: Statistical and algorithmic theory for local optima", "authors": ["Loh", "Po-Ling", "Wainwright", "Martin J"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "Finite mixture models", "authors": ["McLachlan", "Geoffrey", "Peel", "David"], "year": 2004}, {"title": "Sub-gaussian estimators of the mean of a random matrix with heavy-tailed entries", "authors": ["Minsker", "Stanislav"], "venue": "arXiv preprint arXiv:1605.07129,", "year": 2016}, {"title": "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers", "authors": ["Negahban", "Sahand N", "Ravikumar", "Pradeep", "Wainwright", "Martin J", "Yu", "Bin"], "venue": "Statistical Science, 27(4):538\u2013557,", "year": 2012}, {"title": "The generalized lasso with non-linear observations", "authors": ["Plan", "Yaniv", "Vershynin", "Roman"], "venue": "IEEE Transactions on information theory,", "year": 2016}, {"title": "High dimensional single index models", "authors": ["Radchenko", "Peter"], "venue": "Journal of Multivariate Analysis,", "year": 2015}, {"title": "Sparse additive models", "authors": ["Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2009}, {"title": "The limiting distribution of the maximum rank correlation estimator", "authors": ["Sherman", "Robert P"], "venue": "Econometrica: Journal of the Econometric Society,", "year": 1993}, {"title": "Use of exchangeable pairs in the analysis of simulations", "authors": ["Stein", "Charles", "Diaconis", "Persi", "Holmes", "Susan", "Reinert", "Gesine"], "venue": "In Stein\u2019s Method. Institute of Mathematical Statistics,", "year": 2004}, {"title": "Lasso with non-linear measurements is equivalent to one with linear measurements", "authors": ["Thrampoulidis", "Christos", "Abbasi", "Ehsan", "Hassibi", "Babak"], "venue": "Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Estimation in high dimensions: a geometric perspective", "authors": ["Vershynin", "Roman"], "venue": "In Sampling theory, a renaissance,", "year": 2015}, {"title": "Phase recovery, maxcut and complex semidefinite programming", "authors": ["Waldspurger", "Ir\u00e8ne", "dAspremont", "Alexandre", "Mallat", "St\u00e9phane"], "venue": "Mathematical Programming,", "year": 2015}, {"title": "Sparse nonlinear regression: Parameter estimation and asymptotic inference", "authors": ["Yang", "Zhuoran", "Wang", "Zhaoran", "Liu", "Han", "Eldar", "Yonina C", "Zhang", "Tong"], "venue": "International Conference on Machine Learning,", "year": 2015}, {"title": "Minimax optimal rates of estimation in high dimensional additive models", "authors": ["Yuan", "Ming", "Zhou", "Ding-Xuan"], "venue": "The Annals of Statistics,", "year": 2016}, {"title": "On sliced inverse regression with high-dimensional covariates", "authors": ["Zhu", "Lixing", "Miao", "Baiqi", "Peng", "Heng"], "venue": "Journal of the American Statistical Association,", "year": 2006}], "id": "SP:d4a31e8416ee4e76e399a69ccb77068646e16e7b", "authors": [{"name": "Zhuoran Yang", "affiliations": []}, {"name": "Krishnakumar Balasubramanian", "affiliations": []}, {"name": "Han Liu", "affiliations": []}], "abstractText": "We consider estimating the parametric component of single index models in high dimensions. Compared with existing work, we do not require the covariate to be normally distributed. Utilizing Stein\u2019s Lemma, we propose estimators based on the score function of the covariate. Moreover, to handle score function and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts. Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators. Extensive numerical experiments are provided to back up our theory.", "title": "High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation"}