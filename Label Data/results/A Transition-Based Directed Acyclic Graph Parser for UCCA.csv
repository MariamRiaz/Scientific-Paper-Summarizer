0,1,label2,summary_sentences
Deep learning has significantly advanced our ability to address a wide range of difficult machine learning and signal processing problems.,1. Introduction,[0],[0]
"Today’s machine learning landscape is dominated by deep (neural) networks (DNs), which are compositions of a large number of simple parameterized linear and nonlinear transforms.",1. Introduction,[0],[0]
"An all-too-common story of late is that of plugging a deep network into an application as a black box, training it on copious training data,
1ECE Department, Rice University, Houston, TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Randall B. <randallbalestriero@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"and then significantly improving performance over classical approaches.
",1. Introduction,[0],[0]
"Despite this empirical progress, the precise mechanisms by which deep learning works so well remain relatively poorly understood, adding an air of mystery to the entire field.",1. Introduction,[0],[0]
"Ongoing attempts to build a rigorous mathematical framework fall roughly into five camps: (i) probing and measuring DNs to visualize their inner workings (Zeiler & Fergus, 2014); (ii) analyzing their properties such as expressive power (Cohen et al., 2016), loss surface geometry (Lu & Kawaguchi, 2017; Soudry & Hoffer, 2017), nuisance management (Soatto & Chiuso, 2016), sparsification (Papyan et al., 2017), and generalization abilities; (iii) new mathematical frameworks that share some (but not all) common features with DNs (Bruna & Mallat, 2013); (iv) probabilistic generative models from which specific DNs can be derived (Arora et al., 2013; Patel et al., 2016); and (v) information theoretic bounds (Tishby & Zaslavsky, 2015).
",1. Introduction,[0],[0]
"In this paper, we build a rigorous bridge between DNs and approximation theory via spline functions and operators.",1. Introduction,[0],[0]
"We prove that a large class of DNs — including convolutional neural networks (CNNs) (LeCun, 1998), residual networks (ResNets) (He et al., 2016; Targ et al., 2016), skip connection networks (Srivastava et al., 2015), fully connected networks (Pal & Mitra, 1992), recurrent neural networks (RNNs) (Graves, 2013), and beyond — can be written as spline operators.",1. Introduction,[0],[0]
"In particular, when these networks employ current standard-practice piecewise-affine, convex nonlinearities (e.g., ReLU, max-pooling, etc.)",1. Introduction,[0],[0]
"they can be written as the composition of max-affine spline operators (MASOs) (Magnani & Boyd, 2009; Hannah & Dunson, 2013).",1. Introduction,[0],[0]
"We focus on such nonlinearities here but note that our framework applies also to non-piecewise-affine nonlinearities through a standard approximation argument.
",1. Introduction,[0],[0]
The max-affine spline connection provides a powerful portal through which to view and analyze the inner workings of a DN using tools from approximation theory and functional analysis.,1. Introduction,[0],[0]
"Here is a summary of our key contributions:
[C1] We prove that a large class of DNs can be written as a composition of MASOs, from which it follows immediately that, conditioned on the input signal, the output of a DN is a simple affine transformation of the input.",1. Introduction,[0],[0]
"We illustrate in Section 4 by deriving a closed-form expression for the
input/output mapping of a CNN.
",1. Introduction,[0],[0]
"[C2] The affine mapping formula enables us to interpret a MASO DN as constructing a set of signal-dependent, classspecific templates against which the signal is compared via a simple inner product.",1. Introduction,[0],[0]
"In Section 5 we relate DNs directly to the classical theory of optimal classification via matched filters and provide insights into the effects of data memorization (Zhang et al., 2016).
",1. Introduction,[0],[0]
[C3] We propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal to each other.,1. Introduction,[0],[0]
"In Section 6, we show that this leads to significantly improved classification performance and reduced overfitting on standard test data sets like CIFAR100 with no change to the DN architecture.
",1. Introduction,[0],[0]
"[C4] The partition of the input space induced by a MASO links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up a new geometric avenue to study how DNs cluster and organize signals in a hierarchical fashion.",1. Introduction,[0],[0]
"Section 7 studies the properties of the MASO partition.
",1. Introduction,[0],[0]
"[C5] Leveraging the fact that a DN considers two signals to be similar if they lie in the same MASO partition region, we develop a new signal distance in Section 7.3 that measures the difference between their partition encodings.",1. Introduction,[0],[0]
"The distance is easily computed via backpropagation.
",1. Introduction,[0],[0]
A number of appendices in the Supplementary Material (SM) contain the mathematical setup and proofs.,1. Introduction,[0],[0]
"A significantly extended account of these events with numerous new results is available in (Balestriero & Baraniuk, 2018).",1. Introduction,[0],[0]
A deep network (DN) is an operator fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC that maps an input signal1 x ∈ RD to an output prediction ŷ ∈ RC as fΘ :,2. Background on Deep Networks,[0],[0]
RD → RC .,2. Background on Deep Networks,[0],[0]
"All current DNs can be written as a composition of L intermediate mappings called layers
fΘ(x) =",2. Background on Deep Networks,[0],[0]
( f (L) θ(L),2. Background on Deep Networks,[0],[0]
◦ · · · ◦,2. Background on Deep Networks,[0],[0]
f (1) θ(1) ),2. Background on Deep Networks,[0],[0]
"(x), (1)
where Θ = { θ(1), . . .",2. Background on Deep Networks,[0],[0]
", θ(L) } is the collection of the network’s parameters from each layer.",2. Background on Deep Networks,[0],[0]
"This composition of mappings is nonlinear and non-commutative, in general.
",2. Background on Deep Networks,[0],[0]
A DN layer at level ` is an operator f (`) θ(`) that takes as input the vector-valued signal z(`−1)(x) ∈ RD(`−1) and produces the vector-valued output z(`)(x) ∈ RD(`) .,2. Background on Deep Networks,[0],[0]
We will assume that x and z(`) are column vectors.,2. Background on Deep Networks,[0],[0]
We initialize with z(0)(x) =,2. Background on Deep Networks,[0],[0]
"x and denote z(L)(x) =: z for convenience.
1",2. Background on Deep Networks,[0],[0]
"For concreteness, we focus here on processing K-channel images x, such as color digital photographs.",2. Background on Deep Networks,[0],[0]
"But our analysis and techniques apply to signals of any index-dimensionality, including speech and audio signals, video signals, etc.
",2. Background on Deep Networks,[0],[0]
"The signals z(`)(x) are typically called feature maps; it is easy to see that
z(`)(x) =",2. Background on Deep Networks,[0],[0]
"( f (`)
θ(`) ◦ · · · ◦ f (1) θ(1)
) (x), ` ∈ {1, . . .",2. Background on Deep Networks,[0],[0]
", L}.",2. Background on Deep Networks,[0],[0]
"(2)
We briefly overview the basic DN operators and layers we consider in this paper; more details and additional layers are provided in (Goodfellow et al., 2016) and (Balestriero & Baraniuk, 2018).",2. Background on Deep Networks,[0],[0]
"A fully connected operator performs an arbitrary affine transformation by multiplying its input by the dense matrix W (`) ∈ RD(`)×D(`−1) and adding the arbitrary bias vector b(`)W ∈ RD (`) , as in f (`)W ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= W (`)z(`−1)(x) + b (`) W .,2. Background on Deep Networks,[0],[0]
"A convolution operator reduces the number of parameters in the affine transformation by replacing the unconstrained W (`) with a multichannel convolution matrix, as in f (`)C ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
:= C(`)z(`−1)(x) +,2. Background on Deep Networks,[0],[0]
"b (`) C .
",2. Background on Deep Networks,[0],[0]
"An activation operator applies a scalar nonlinear activation function σ independently to each entry of its input, as in[ f (`) σ",2. Background on Deep Networks,[0],[0]
( z(`−1)(x) ),2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
k := σ,2. Background on Deep Networks,[0],[0]
"( [z(`−1)(x)]k ) , k = 1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"Nonlinearities are crucial to DNs, since otherwise the entire network would collapse to a single global affine transform.",2. Background on Deep Networks,[0],[0]
Three popular activation functions are the rectified linear unit (ReLU) σReLU(u),2. Background on Deep Networks,[0],[0]
":= max(u, 0), the leaky ReLU σLReLU(u)",2. Background on Deep Networks,[0],[0]
":= max(ηu, u), η > 0, and the absolute value σabs(u) := |u|.",2. Background on Deep Networks,[0],[0]
These three functions are both piecewise affine and convex.,2. Background on Deep Networks,[0],[0]
Other popular activation functions include the sigmoid σsig(u) := 11+e−u and hyperbolic tangent σtanh(u) := 2σsig(2u)−1.,2. Background on Deep Networks,[0],[0]
"These two functions are neither piecewise affine nor convex.
",2. Background on Deep Networks,[0],[0]
"A pooling operator subsamples its input to reduce its dimensionality according to a sub-sampling policy ρ applied over a collection of input indices {Rk}K (`)
k=1 (typically a small patch), e.g., max pooling[ f (`) ρ ( z(`−1)(x) )",2. Background on Deep Networks,[0],[0]
],2. Background on Deep Networks,[0],[0]
"k
:= max d∈R(`)k
[ z(`−1)(x) ]",2. Background on Deep Networks,[0],[0]
"d , k =
1, . . .",2. Background on Deep Networks,[0],[0]
", D(`).",2. Background on Deep Networks,[0],[0]
"See (Balestriero & Baraniuk, 2018) for the definitions of average pooling, channel pooling, skip connections, and recurrent layers.
",2. Background on Deep Networks,[0],[0]
Definition 1.,2. Background on Deep Networks,[0],[0]
"A DN layer f (`) θ(`)
comprises a single nonlinear DN operator (non-affine to be precise) composed with any preceding affine operators lying between it and the preceding nonlinear operator.
",2. Background on Deep Networks,[0],[0]
"This definition yields a single, unique layer decomposition for any DN, and the complete DN is then the composition of its layers per (1).",2. Background on Deep Networks,[0],[0]
"For example, in a standard CNN, there are two different layers types: i) convolution-activation and ii) max-pooling.
",2. Background on Deep Networks,[0],[0]
We form the prediction ŷ by feeding fΘ(x) through a final nonlinearity g : RD(L) → RD(L) as in ŷ = g(fΘ(x)).,2. Background on Deep Networks,[0],[0]
"In classification, g is typically the softmax nonlinearity, which arises naturally from posing the classification inference as a
multinomial logistic regression problem (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"In regression, typically no g is applied.
",2. Background on Deep Networks,[0],[0]
"We learn the DN parameters Θ for a particular prediction task in a supervised setting using a labeled data set D = (xn,yn) N n=1, a loss function, and a learning policy to update the parameters Θ in the predictor fΘ(x).",2. Background on Deep Networks,[0],[0]
"For classification problems, the loss function is typically the negative cross-entropy LCE(x,y) (Bishop, 1995).",2. Background on Deep Networks,[0],[0]
"For regression problems, the loss function is typically is the squared error.",2. Background on Deep Networks,[0],[0]
"Since the layer-by-layer operations in a DN are differentiable almost everywhere with respect to their parameters and inputs, we can use some flavor of first-order optimization such as gradient descent to optimize the parameters Θ with respect to the loss function.",2. Background on Deep Networks,[0],[0]
"Moreover, the gradients for all internal parameters can be computed efficiently by backpropagation (Hecht-Nielsen, 1992), which follows from the chain rule of calculus.",2. Background on Deep Networks,[0],[0]
"Approximation theory is the study of how and how well functions can best be approximated using simpler functions (Powell, 1981).",3. Background on Spline Operators,[0],[0]
"A classical example of a simpler function is a spline s : RD → R (Schmidhuber, 1994).",3. Background on Spline Operators,[0],[0]
"For concreteness, we will work exclusively with affine splines in this paper (aka “linear splines”), but our ideas generalize naturally to higher-order splines.
",3. Background on Spline Operators,[0],[0]
Multivariate Affine Splines.,3. Background on Spline Operators,[0],[0]
"Consider a partition of a domain RD into a set of regions Ω = {ω1, . . .",3. Background on Spline Operators,[0],[0]
", ωR}",3. Background on Spline Operators,[0],[0]
"and a set of local mappings Φ = {φ1, . . .",3. Background on Spline Operators,[0],[0]
", φR} that map each region in the partition to R via φr(x) := 〈[α]r,·,x〉+ [β]r for x ∈ ωr.2",3. Background on Spline Operators,[0],[0]
"The parameters are: α ∈ RR×D, a matrix of hyperplane “slopes,” and β ∈ RR, a vector of hyperplane “offsets” or “biases”.",3. Background on Spline Operators,[0],[0]
We will use the terms offset and bias interchangeably in the sequel.,3. Background on Spline Operators,[0],[0]
"The notation [α]r,· denotes the column vector formed from the rth row of α.
",3. Background on Spline Operators,[0],[0]
"With this setup, the multivariate affine spline is defined as
s[α, β,Ω](x) = R∑ r=1 (〈[α]r,·,x〉+ [β]r)1(x ∈ ωr)
",3. Background on Spline Operators,[0],[0]
"=: 〈α[x],x〉+ β[x], (3)
where 1(x ∈ ωr) is the indicator function.",3. Background on Spline Operators,[0],[0]
The second line of (3) introduces the streamlined notation α[x] =,3. Background on Spline Operators,[0],[0]
"[α]r,· when x ∈ ωr; the definition for β[x] is similar.",3. Background on Spline Operators,[0],[0]
Such a spline is piecewise affine and hence piecewise convex.,3. Background on Spline Operators,[0],[0]
"However, in general, it is neither globally affine nor globally convex unless R = 1, a case we denote as a degenerate spline, since it corresponds simply to an affine mapping.
",3. Background on Spline Operators,[0],[0]
"2 To make the connection between splines and DNs more immediately obvious, here x is interpreted as a point in RD , which plays the rôle of the space of signals in the other sections.
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Functions.,3. Background on Spline Operators,[0],[0]
"A major complication of function approximation with splines in general is the need to jointly optimize both the spline parameters α, β and the input domain partition Ω (the “knots” for a 1D spline) (Bennett & Botkin, 1985).",3. Background on Spline Operators,[0],[0]
"However, if a multivariate affine spline is constrained to be globally convex, then it can always be rewritten as a max-affine spline (Magnani & Boyd, 2009; Hannah & Dunson, 2013)
s[α, β,Ω](x) = max r=1,...,R
〈[α]r,·,x〉+ [β]r .",3. Background on Spline Operators,[0],[0]
"(4)
An extremely useful feature of such a spline is that it is completely determined by its parameters α and β without needing to specify the partition Ω.",3. Background on Spline Operators,[0],[0]
"As such, we denote a max-affine spline simply as s[α, β].",3. Background on Spline Operators,[0],[0]
"Changes in the parameters α, β of a max-affine spline automatically induce changes in the partition Ω, meaning that they are adaptive partitioning splines (Magnani & Boyd, 2009).
",3. Background on Spline Operators,[0],[0]
Max-Affine Spline Operators.,3. Background on Spline Operators,[0],[0]
"A natural extension of an affine spline function is an affine spline operator (ASO) S[A,B,ΩS ] that produces a multivariate output.",3. Background on Spline Operators,[0],[0]
It is obtained simply by concatenating K affine spline functions from (3).,3. Background on Spline Operators,[0],[0]
"The details and a more general development are provided in the SM and (Balestriero & Baraniuk, 2018).
",3. Background on Spline Operators,[0],[0]
"We are particularly interested in the max-affine spline operator (MASO) S[A,B] :",3. Background on Spline Operators,[0],[0]
RD → RK formed by concatenating K independent max-affine spline functions from (4).,3. Background on Spline Operators,[0],[0]
"A MASO with slope parameters A ∈ RK×R×D and offset parameters B ∈ RK×R is defined as
S[A,B](x) =  maxr=1,...,R〈[A]1,r,·,x〉+ [B]1,r... maxr=1,...,R〈[A]K,r,·,x〉+ [B]K,r  =: A[x]x",3. Background on Spline Operators,[0],[0]
+B[x].,3. Background on Spline Operators,[0],[0]
"(5)
The second line of (5) introduces the streamlined notation in terms of the signal-dependent matrix A[x] and signal-dependent vector B[x], where [A[x]]k,· := [A]k,rk(x),· and [B[x]]k := [B]k,rk(x) with rk(x) = arg maxr〈[A]k,r,·,x〉+ [B]k,r.
Max-affine spline functions and operators are always piecewise affine and globally convex (and hence also continuous) with respect to each output dimension.",3. Background on Spline Operators,[0],[0]
"Conversely, any piecewise affine and globally convex function/operator can be written as a max-affine spline.",3. Background on Spline Operators,[0],[0]
"Moverover, using standard approximation arguments, it is easy to show that a MASO can approximate arbitrarily closely any (nonlinear) operator that is convex in each output dimension.",3. Background on Spline Operators,[0],[0]
"While a MASO is appropriate only for approximating convex functions/operators, we now show that virtually all of
today’s DNs can be written as a composition of MASOs, one for each layer.",4. DNs are Compositions of Spline Operators,[0],[0]
"Such a composition is, in general, nonconvex and hence can approximate a much larger class of functions/operators.",4. DNs are Compositions of Spline Operators,[0],[0]
"Interestingly, under certain broad conditions, the composition remains a piecewise affine spline operator, which enables a variety of insights into DNs.",4. DNs are Compositions of Spline Operators,[0],[0]
"We now state our main theoretical results, which are proved in the SM and elaborated in (Balestriero & Baraniuk, 2018).
",4.1. DN Operators are MASOs,[0],[0]
Proposition 1.,4.1. DN Operators are MASOs,[0],[0]
An arbitrary fully connected operator f (`)W is an affine mapping and hence a degenerate MASO S,4.1. DN Operators are MASOs,[0],[0]
"[ A
(`) W , B (`) W ] , with R = 1, [A(`)W ]k,1,· = [ W (`) ]",4.1. DN Operators are MASOs,[0],[0]
"k,· and
[B (`) W ]k,1 =
[ b
(`) W ] k , leading to W (`)z(`−1)(x) + b(`)W",4.1. DN Operators are MASOs,[0],[0]
"=
A (`)",4.1. DN Operators are MASOs,[0],[0]
W,4.1. DN Operators are MASOs,[0],[0]
[x]z (`−1)(x) +B (`) W,4.1. DN Operators are MASOs,[0],[0]
[x].,4.1. DN Operators are MASOs,[0],[0]
"The same is true of a convolution operator with W (`), b(`)W replaced by C (`), b (`) C .",4.1. DN Operators are MASOs,[0],[0]
Proposition 2.,4.1. DN Operators are MASOs,[0],[0]
"Any activation operator f (`)σ using a piecewise affine and convex activation function is a MASO S [ A (`) σ ,B (`) σ ] with R = 2, [ B (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1 = [ B (`) σ ] k,2 =
0 ∀k, and for ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = 0, [ A (`) σ ] k,2,· =
ek ∀k; for leaky ReLU [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,· = νek, [ A (`) σ ] k,2,· =
ek ∀k, ν > 0; and for absolute value [ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,1,·
= −ek,[ A (`) σ ]",4.1. DN Operators are MASOs,[0],[0]
"k,2,· = ek ∀k, where ek represents the kth canonical basis element of RD(`) .
",4.1. DN Operators are MASOs,[0],[0]
Proposition 3.,4.1. DN Operators are MASOs,[0],[0]
"Any pooling operator f (`)ρ that is piecewise affine and convex is a MASO S [ A (`) ρ ,B (`) ρ ]",4.1. DN Operators are MASOs,[0],[0]
.3,4.1. DN Operators are MASOs,[0],[0]
"Max-
pooling has R = #Rk (typically a constant over all output dimensions k), [ A (`) ρ ] k,·,·
= {ei, i ∈ Rk}, and[ B (`) ρ ] k,r = 0 ∀k, r. Average-pooling is a degenerate
MASO with R = 1, [ A (`) ρ ] k,1,· = 1#(Rk) ∑ i∈Rk ei, and[
B (`) ρ ] k,1 = 0 ∀k.
",4.1. DN Operators are MASOs,[0],[0]
Proposition 4.,4.1. DN Operators are MASOs,[0],[0]
"A DN layer constructed from an arbitrary composition of fully connected/convolution operators followed by one activation or pooling operator is a MASO S[A(`), B(`)] such that
f (`)(z(`−1)(x))",4.1. DN Operators are MASOs,[0],[0]
= A(`)[x]z(`−1)(x) +B(`)[x].,4.1. DN Operators are MASOs,[0],[0]
"(6)
Consequently, a large class of DNs boil down to a composition of MASOs.",4.1. DN Operators are MASOs,[0],[0]
"We prove the following in the SM and in (Balestriero & Baraniuk, 2018) for CNNs, ResNets, skip connection nets, fully connected nets, and RNNs.
3",4.1. DN Operators are MASOs,[0],[0]
"This result is agnostic to the pooling type (spatial or channel).
",4.1. DN Operators are MASOs,[0],[0]
Theorem 1.,4.1. DN Operators are MASOs,[0],[0]
"A DN constructed from an arbitrary composition of fully connected/convolution, activation, and pooling operators of the types in Propositions 1–3 is a composition of MASOs that is equivalent to a global affine spline operator.
",4.1. DN Operators are MASOs,[0],[0]
"Note carefully that, while the layers of each of the DNs stated in Theorem 1 are MASOs, the composition of several layers is not necessarily a MASO.",4.1. DN Operators are MASOs,[0],[0]
"Indeed, a composition of MASOs remains a MASO if and only if all of its component operators (except the first) are non-decreasing with respect to each of their output dimensions (Boyd & Vandenberghe, 2004).",4.1. DN Operators are MASOs,[0],[0]
"Interestingly, ReLU and max-pooling are both nondecreasing, while leaky ReLU is strictly increasing.",4.1. DN Operators are MASOs,[0],[0]
"The culprits causing non-convexity of the composition of layers are negative entries in the fully connected or convolution operators, which destroy the required non-increasing property.",4.1. DN Operators are MASOs,[0],[0]
"A DN where these culprits are thwarted is an interesting special case, because it is convex with respect to its input (Amos et al., 2016) and multiconvex (Xu & Yin, 2013) with respect to its parameters.
",4.1. DN Operators are MASOs,[0],[0]
Theorem 2.,4.1. DN Operators are MASOs,[0],[0]
"A DN whose layers ` = 2, . . .",4.1. DN Operators are MASOs,[0],[0]
", L consist of an arbitrary composition of fully connected and convolution operators with nonnegative weights, i.e., W (`)k,j ≥ 0, C (`) k,j ≥ 0; non-decreasing, piecewise-affine, and convex activation operators; and non-decreasing, piecewise-affine, and convex pooling operators is globally a MASO and thus also globally convex with respect to each of its output dimensions.
",4.1. DN Operators are MASOs,[0],[0]
"The above results pertain to DNs using convex, affine operators.",4.1. DN Operators are MASOs,[0],[0]
"Other popular non-convex DN operators (e.g., the sigmoid and arctan activation functions) can be approximated arbitrarily closely by an affine spline operator but not by a MASO.
DNs are Signal-Dependent Affine Transformations.",4.1. DN Operators are MASOs,[0],[0]
"A common theme of the above results is that, for DNs constructed from fully connected/convolution, activation, and pooling operators from Propositions 1–3, the operator/layer outputs z(`)(x) are always a signal-dependent affine function of the input x (recall (5)).",4.1. DN Operators are MASOs,[0],[0]
The particular affine mapping applied to x depends on which partition of the spline it falls in RD.,4.1. DN Operators are MASOs,[0],[0]
"More on this in Section 7 below.
",4.1. DN Operators are MASOs,[0],[0]
DN Learning and MASO Parameters.,4.1. DN Operators are MASOs,[0],[0]
"Given labeled training data (xn,yn)Nn=1, learning in a DN that meets the conditions of Theorem 1 (i.e., optimizing its parameters Θ) is equivalent to optimally approximating the mapping from input x to output ŷ =",4.1. DN Operators are MASOs,[0],[0]
g ( z(L)(x) ),4.1. DN Operators are MASOs,[0],[0]
"using an appropriate cost function (e.g., cross-entropy for classification or squared error for regression) by learning the parameters θ(`) of the layers.",4.1. DN Operators are MASOs,[0],[0]
"In general the overall optimization problem is nonconvex (it is actually piecewise multi-convex in general (Rister, 2016)).
",4.1. DN Operators are MASOs,[0],[0]
"z (L) CNN(x) = W (L)
( 1∏
`=L−1
A(`)ρ",4.1. DN Operators are MASOs,[0],[0]
[x]A (`) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (`) ) ︸,4.1. DN Operators are MASOs,[0],[0]
"︷︷ ︸
ACNN[x]
x + W (L) L−1∑",4.1. DN Operators are MASOs,[0],[0]
`=1  `+1∏ j=L−1 A(j)ρ [x]A (j) σ,4.1. DN Operators are MASOs,[0],[0]
[x]C (j) (A(`)ρ [x]A(`)σ [x]b(`)C )︸ ︷︷ ︸,4.1. DN Operators are MASOs,[0],[0]
"BCNN[x] +b (L) W
(7)",4.1. DN Operators are MASOs,[0],[0]
"Combining Propositions 1–3 and Theorem 1 and substituting (5) into (2), we can write an explicit formula for the output of any layer z(`)(x) of a DN in terms of the input x for a variety of different architectures.",4.2. Application: DN Affine Mapping Formula,[0],[0]
"The formula for a standard CNN (using ReLU activation and max-pooling) is given in (7) above; we derive this formula and analogous formulas for ResNets and RNNs in (Balestriero & Baraniuk, 2018).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"In (7), A(`)σ [x] are the signal-dependent matrices corresponding to the ReLU activations,",4.2. Application: DN Affine Mapping Formula,[0],[0]
A(`)ρ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] are the signal-dependent matrices corresponding to maxpooling, and the biases b(L)W , b (`) C arise directly from the fully connected and convolution operators.",4.2. Application: DN Affine Mapping Formula,[0],[0]
The absence of B (`) σ,4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x], B (`) ρ",4.2. Application: DN Affine Mapping Formula,[0],[0]
"[x] is due to the absence of bias in the ReLU (recall (2)) and max-pooling operators (recall (3)).
",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Inspection of (7) reveals the exact form of the signaldependent, piecewise affine mapping linking x to z(L)CNN(x).",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Moreover, this formula can be collapsed into
z (L) CNN(x) = W (L) ( ACNN[x]x +BCNN[x] )",4.2. Application: DN Affine Mapping Formula,[0],[0]
"+ b (L) W (8)
from which we can recognize
z",4.2. Application: DN Affine Mapping Formula,[0],[0]
(L−1) CNN (x) = ACNN[x]x,4.2. Application: DN Affine Mapping Formula,[0],[0]
"+BCNN[x] (9)
as an explicit, signal-dependent, affine formula for the featurization process that aims to convert x into a set of (hopefully) linearly separable features that are then input to the linear classifier in layer ` = L with parameters W (L) and b
(L) W .",4.2. Application: DN Affine Mapping Formula,[0],[0]
"Of course, the final prediction ŷ is formed by running z (L) CNN(x) through a softmax nonlinearity g, but this merely rescales its entries to create a probability distribution.",4.2. Application: DN Affine Mapping Formula,[0],[0]
We now dig deeper into (8) in order to bridge DNs and classical optimal classification theory.,5. DNs are Template Matching Machines,[0],[0]
"While we focus on CNNs and classification for concreteness, our analysis holds for any DN meeting the conditions of Theorem 1.",5. DNs are Template Matching Machines,[0],[0]
An alternate interpretation of (8) is that z(L)CNN(x) is the output of a bank of linear matched filters (plus a set of biases).,5.1. Template Matching,[0],[0]
"That is, the cth element of z(L)(x) equals the inner product between the signal x and the matched filter for the cth class, which is contained in the cth row of the matrix
W (L)A[x].",5.1. Template Matching,[0],[0]
"The bias W (L)B[x] + b(L)W can be used to account for the fact that some classes might be more likely than others (i.e., the prior probability over the classes).",5.1. Template Matching,[0],[0]
"It is well-known that a matched filterbank is the optimal classifier for deterministic signals in additive white Gaussian noise (Rabiner & Gold, 1975).",5.1. Template Matching,[0],[0]
"Given an input x, the class decision is simply the index of the largest element of z(L)(x).4
Yet another interpretation of (8) is that z(L)(x) is computed not in a single matched filter calculation but hierarchically as the signal propagates through the DN layers.",5.1. Template Matching,[0],[0]
Abstracting (5) to write the per-layer maximization process as z(`)(x) = maxr(`),5.1. Template Matching,[0],[0]
A (`) r(`) z(`−1)(x),5.1. Template Matching,[0],[0]
"+B (`) r(`) and cascading, we obtain a formula for the end-to-end DN mapping
z(L)(x) =",5.1. Template Matching,[0],[0]
"W (L) max r(L−1)
",5.1. Template Matching,[0],[0]
"( A
(L−1) r(L−1)
max r(2)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .
",5.1. Template Matching,[0],[0]
"max r(1)
",5.1. Template Matching,[0],[0]
"( A (1)
r(1) x + B (1) r(1)
) + B (2)
r(2)
) · · ·+ B(L−1)
r(L−1)
) + b
(L) W .
",5.1. Template Matching,[0],[0]
"(10)
",5.1. Template Matching,[0],[0]
"This formula elucidates that a DN performs a hierarchical, greedy template matching on its input, a computationally efficient yet sub-optimal template matching technique.",5.1. Template Matching,[0],[0]
Such a procedure is globally optimal when the DN is globally convex.,5.1. Template Matching,[0],[0]
Corollary 1.,5.1. Template Matching,[0],[0]
"For a DN abiding by the requirements of Theorem 2, the computation (10) collapses to the following globally optimal template matching
z(L−1)(x)",5.1. Template Matching,[0],[0]
"= W (L) max r(L−1),r(2),...,r(1)
( A
(L−1) r(L−1)
",5.1. Template Matching,[0],[0]
"( A (2)
r(2) . . .",5.1. Template Matching,[0],[0]
"(
A (1)
r(1) x + B
(1) r(1)
) +",5.1. Template Matching,[0],[0]
"B (2)
r(2)
) · · ·+ B(L−1
r(L−1)
)",5.1. Template Matching,[0],[0]
"+ b
(L) W .
(11)",5.1. Template Matching,[0],[0]
"Since the complete DN mapping (up to the final softmax) can be expressed as in (8), given a signal x, we can compute the signal-dependent template for class c via A[x]c =",5.2. Template Visualization Examples,[0],[0]
"d[z(L)(x)]c dx , which can be efficiently computed via backpropogation (Hecht-Nielsen, 1992).5",5.2. Template Visualization Examples,[0],[0]
"Once the template A[x]c has been computed, the bias term b[x]c can be computed via b[x]c = z(L)(x)c − 〈A[x]c,·,x〉.",5.2. Template Visualization Examples,[0],[0]
"Figure 1 plots
4Again, since the softmax merely rescales the entries of z(L)(x) into a probability distribution, it does not affect the location of its largest element.
",5.2. Template Visualization Examples,[0],[0]
"5In fact, we can use the same backpropagation procedure used for computing the gradient with respect to a fully connected or
various signal-dependent templates for two CNNs trained on the MNIST and CIFAR10 datasets.",5.2. Template Visualization Examples,[0],[0]
"Under the matched filterbank interpretation of a DN developed in Section 5.1, the optimal template for an image x of class c is a scaled version of x itself.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
But what are the optimal templates for the other (incorrect) classes?,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In an idealized setting, we can answer this question.
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
Proposition 5.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Consider an idealized DN consisting of a composition of MASOs that has sufficient approximation power to span arbitrary MASO matrices A[xn] from (9) for any input xn from the training set.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"Train the DN to classify among C classes using the training data D = (xn, yn)Nn=1 with normalized inputs ‖xn‖2 = 1 ∀n and the cross-entropy loss LCE(yn, fΘ(xn)) with the addition of the regularization constraint that ∑ c ‖A[xn]c,·‖2",5.3. Collinear Templates and Data Set Memorization,[0],[0]
< α with α > 0.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"At the global minimum of this constrained optimization problem, the rows of A?[xn] (the optimal templates) have the form:
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"[A?[xn]]c,· =  + √ (C−1)α C xn, c = yn − √
α C(C−1) xn, c 6=",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"yn
(12)
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"In short, the idealized CNN in the proposition will memorize a set of collinear templates whose bimodal outputs force
convolution weight but instead with the input x.",5.3. Collinear Templates and Data Set Memorization,[0],[0]
"This procedure is becoming increasingly popular in the study of adversarial examples (Szegedy et al., 2013).
",5.3. Collinear Templates and Data Set Memorization,[0],[0]
the softmax output to a Dirac delta function (aka 1-hot representation) that peaks at the correct class.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
Figure 2 confirms this bimodal behavior on the MNIST and CIFAR10 datasets.,5.3. Collinear Templates and Data Set Memorization,[0],[0]
"While a DN’s signal-dependent matched filterbank (8) is optimized for classifying signals immersed in additive white Gaussian noise, such a statistical model is overly simplistic for most machine learning problems of interest.",6. New DNs with Orthogonal Templates,[0],[0]
"In practice, errors will arise not just from random noise but also from nuisance variations in the inputs such as arbitrary rotations, positions, and modalities of the objects of interest.",6. New DNs with Orthogonal Templates,[0],[0]
The effects of these nuisances are only poorly approximated as Gaussian random errors.,6. New DNs with Orthogonal Templates,[0],[0]
"Limited work has been done on filterbanks for classification in nonGaussian noise; one promising direction involves using not matched but rather orthogonal templates (Eldar & Oppenheim, 2001).
",6. New DNs with Orthogonal Templates,[0],[0]
"For a MASO DN’s templates to be orthogonal for all inputs, it is necessary that the rows of the matrix W (L) in the final linear classifier layer be orthogonal.",6. New DNs with Orthogonal Templates,[0],[0]
"This weak constraint on the DN still enables the earlier layers to create a high-performance, class-agnostic, featurized representation (recall the discussion just below (9)).",6. New DNs with Orthogonal Templates,[0],[0]
"To create orthogonal templates during learning, we simply add to the standard (potentially regularized) cross-entropy loss function LCE a term that penalizes non-zero off-diagonal entries in the matrix W (L)(W (L))T leading to the new loss with the additional penalty
LCE + λ ∑ c1 6=c2 ∣∣∣〈[W",6. New DNs with Orthogonal Templates,[0],[0]
"(L)] c1,· , [ W (L) ]",6. New DNs with Orthogonal Templates,[0],[0]
"c2,· 〉∣∣∣2 .",6. New DNs with Orthogonal Templates,[0],[0]
"(13) The parameter λ controls the tradeoff between cross-entropy
minimization and orthogonality preservation.",6. New DNs with Orthogonal Templates,[0],[0]
"Conveniently, when minimizing (13) via backpropagation, the orthogonal rows of W (L) induce orthogonal backpropagation updates for the various classes.
",6. New DNs with Orthogonal Templates,[0],[0]
We now empirically demonstrate that orthogonal templates lead to significantly improved classification performance.,6. New DNs with Orthogonal Templates,[0],[0]
"We conducted a range of experiments with three different conventional DN architectures – smallCNN, largeCNN, and ResNet4-4 – trained on three different datasets – SVHN, CIFAR10, and CIFAR100.",6. New DNs with Orthogonal Templates,[0],[0]
"Each DN employed bias units, ReLU activations, and max-pooling as well as batchnormalization prior each ReLU.",6. New DNs with Orthogonal Templates,[0],[0]
"The full experimental details are given in the (Balestriero & Baraniuk, 2018).",6. New DNs with Orthogonal Templates,[0],[0]
"For learning, we used the Adam optimizer with an exponential learning rate decay.",6. New DNs with Orthogonal Templates,[0],[0]
All inputs were centered to zero mean and scaled to a maximum value of one.,6. New DNs with Orthogonal Templates,[0],[0]
"No further preprocessing was performed, such as ZCA whitening (Nam et al., 2014).",6. New DNs with Orthogonal Templates,[0],[0]
We assessed how the classification performance of a given DN would change as we varied the orthogonality penalty λ in (13).,6. New DNs with Orthogonal Templates,[0],[0]
"For each configuration of DN architecture, training dataset, learning rate, and penalty λ, we averaged over 15 runs to estimate the average performance and standard deviation.
",6. New DNs with Orthogonal Templates,[0],[0]
We report here on only the CIFAR100 with largeCNN experiments.,6. New DNs with Orthogonal Templates,[0],[0]
"(See (Balestriero & Baraniuk, 2018) for detailed results for all three datasets and the other architectures.",6. New DNs with Orthogonal Templates,[0],[0]
The trends for all three datasets are similar and are independent of the learning rate.),6. New DNs with Orthogonal Templates,[0],[0]
The results for CIFAR100 in Figure 3 indicate that the benefits of the orthogonality penalty emerge distinctly as soon as λ > 0.,6. New DNs with Orthogonal Templates,[0],[0]
"In addition to improved final accuracy and generalization performance, we see that template orthogonality reduces the temptation of the DN to overfit.",6. New DNs with Orthogonal Templates,[0],[0]
"(This is is especially visible in the examples in (Balestriero & Baraniuk, 2018).)",6. New DNs with Orthogonal Templates,[0],[0]
One explanation is that the orthogonal weights W (L) positively impact not only the prediction but also the backpropagation via orthogonal gradient updates with respect to each output dimension’s partial derivatives.,6. New DNs with Orthogonal Templates,[0],[0]
"Like any spline, it is the interplay between the (affine) spline mappings and the input space partition that work the magic in a MASO DN.",7. DN’s Intrinsic Multiscale Partition,[0],[0]
Recall from Section 3 that a MASO has the attractive property that it implicitly partitions its input space as a function of its slope and offset parameters.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
The induced partition Ω opens up a new geometric avenue to study how a DN clusters and organizes signals in a hierarchical fashion.,7. DN’s Intrinsic Multiscale Partition,[0],[0]
"A DN operator at level ` directly influences the partitioning of its input space RD(`−1) and indirectly influences the partitioning of the overall signal space RD.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
A ReLU activation operator splits each of its input dimensions into two half-planes depending on the sign of the input in each dimension.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"This partitions RD(`−1) into a combinatorially large number (up to 2D (`)
) of regions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Following a fully connected or convolution operator with a ReLU simply rotates the partition in RD(`−1) .,7.1. Effect of the DN Operators on the Partition,[0],[0]
"A max-pooling operator also partitions RD(`−1) into a combinatorially large number (up to #RD (`)
) of regions, where #R is the size of the pooling region.
",7.1. Effect of the DN Operators on the Partition,[0],[0]
This per-MASO partitioning of each layer’s input space constructs an overall partitioning of the input signal space RD.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"As each MASO is applied, it subdivides the input space RD into finer and finer partitions.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"The final partition corresponds to the intersection of all of the intermediate partitions, and hence we can encode the input in terms of the ordered collection of per-layer partition regions into which it falls.",7.1. Effect of the DN Operators on the Partition,[0],[0]
This overall process can be interpreted as a hierarchical vector quantization (VQ) of the training input signals xn.,7.1. Effect of the DN Operators on the Partition,[0],[0]
"There are thus many potential connections between DNs and optimal quantization, information theory, and clustering that we leave for future research.",7.1. Effect of the DN Operators on the Partition,[0],[0]
"See (Balestriero & Baraniuk, 2018) for some early results.",7.1. Effect of the DN Operators on the Partition,[0],[0]
Unfortunately there is no simple formula for the partition of the signal space.,7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"However, once can obtain the set of inputs signals xn that fall into the same partition region at each layer of a DN.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"At layer `, denote the index of the region selected by the input x (recall (6)) by[ t(`)(x) ]",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"k
= arg max r
〈",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[A(`)]k,r,·, z (`−1)(x) 〉 +",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"[B(`)]k,r.
(14)
Thus, [t(`)]k ∈ {1, . . .",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
", R(`)}, with R(`) the number of partition regions in the layer’s input space.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Encoding the partition as an ordered collection of integers designating the activate hyperplane parameters from (4), we can now visualize which inputs fall into the same or nearby partitions.
",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"Due to the very large number of possible regions (up to 2D (`) for a ReLU at layer `) and the limited amount of training data, in general, many partitions will be empty or contain only a single training data point.",7.2. Inferring a DN Layer’s Intrinsic Partition,[0],[0]
"To validate the utility of the hierarchical intrinsic clustering induced by a DN, we define a new distance function between the signals x1 and x2 that quantifies the similarity of their position encodings t`(x1) and t`(x2) at layer ` via
d ( t(`)(x1), t (`)(x2) )
",7.3. A New Image Distance based on the DN Partition,[0],[0]
"= 1− ∑D(`) k=1 1 ( [t(`)(x1)]k = [t (`)(x2)]k )
D(`) .",7.3. A New Image Distance based on the DN Partition,[0],[0]
"(15)
For a ReLU MASO, this corresponds simply to counting how many entries of the layer inputs for x1 and x2 are positive or negative at the same positions.",7.3. A New Image Distance based on the DN Partition,[0],[0]
"For a max-pooling
MASO, this corresponds to counting how many argmax positions are the same in each patch for x1 and x2.
",7.3. A New Image Distance based on the DN Partition,[0],[0]
Figure 4 provides a visualization of the nearest neighbors of a test image under this partition-based distance measure.,7.3. A New Image Distance based on the DN Partition,[0],[0]
"Visual inspection of the figures highlights that, as we progress through the layers of the DN, similar images become closer in the new distance but further in Euclidean distance.",7.3. A New Image Distance based on the DN Partition,[0],[0]
We have used the theory of splines to build a rigorous bridge between deep networks (DNs) and approximation theory.,8. Conclusions,[0],[0]
"Our key finding is that, conditioned on the input signal, the output of a DN can be written as a simple affine transformation of the input.",8. Conclusions,[0],[0]
"This links DNs directly to the classical theory of optimal classification via matched filters and provides insights into the positive effects of data memorization.
",8. Conclusions,[0],[0]
"There are many avenues for future work, including a more in-depth analysis of the hierarchical MASO partitioning, particularly from the viewpoint of vector quantization and K-means clustering, which are unsupervised learning techniques, and information theory.",8. Conclusions,[0],[0]
The spline viewpoint also could inspire the creation of new DN layers that have certain attractive partitioning or approximation capabilities.,8. Conclusions,[0],[0]
"We have begun exploring some of these directions in (Balestriero & Baraniuk, 2018).6
6 This work was partially supported by ARO grant W911NF-151-0316, AFOSR grant FA9550-14-1-0088, ONR grants N0001417-1-2551 and N00014-18-12571, DARPA grant G001534-7500, and a DOD Vannevar Bush Faculty Fellowship (NSSEFF) grant N00014-18-1-2047.",8. Conclusions,[0],[0]
We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators.,abstractText,[0],[0]
"Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings.",abstractText,[0],[0]
"For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input.",abstractText,[0],[0]
"This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization.",abstractText,[0],[0]
"Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture.",abstractText,[0],[0]
The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion.,abstractText,[0],[0]
"As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.",abstractText,[0],[0]
A Spline Theory of Deep Networks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2263–2270, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"As a fundamental task in natural language processing (NLP), discourse parsing entails the discovery of the latent relational structure in multi-sentence level analysis.",1 Introduction,[0],[0]
"It is also central to many practical tasks such as question answering (Liakata et al., 2013; Jansen et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Meyer and Webber, 2013) and automatic summarization (Murray et al., 2006;
∗Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by Cai Yuanpei Program (CSC No. 201304490199 and No. 201304490171), National Natural Science Foundation of China (No. 61170114, No. 61672343 and No. 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technology Committee (No. 15JC1400103), Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04), and Key Project of National Society Science Foundation of China (No. 15-ZDA041).
",1 Introduction,[0],[0]
"Yoshida et al., 2014).",1 Introduction,[0],[0]
"Discourse parsing is also the shared task of CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016), and many previous works previous on this task (Qin et al., 2016b; Li et al., 2016; Chen et al., 2015; Wang and Lan, 2016).",1 Introduction,[0],[0]
"In a discourse parser, implicit relation recognition has been the bottleneck due to lack of explicit connectives (like “because” or “and”) that can be strong indicators for the senses between adjacent clauses (Qin et al., 2016b; Pitler et al., 2009; Lin et al., 2014).",1 Introduction,[0],[0]
"This work therefore focuses on implicit relation recognition that infers the senses of the discourse relations within adjacent sentence pairs.
",1 Introduction,[0],[0]
"Most previous works on PDTB implicit relation recognition only focus on one-versus-others binary classification problems of the top level four classes (Pitler et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Rutherford and Xue, 2014; Braud and Denis, 2015).",1 Introduction,[0],[0]
"Traditional classification methods directly rely on feature engineering, based on bag-of-words, production rules, and some linguistically-informed features (Zhou et al., 2010; Rutherford and Xue, 2014).",1 Introduction,[0],[0]
"However, discourse relations root in semantics, which may be hard to recover from surface level feature, thus these methods did not report satisfactory performance.",1 Introduction,[0],[0]
"Recently, neural network (NN) models have shown competitive or even better results than traditional linear models with handcrafted sparse features (Wang et al., 2016b; Zhang et al., 2016a; Jia and Zhao, 2014).",1 Introduction,[0],[0]
"They have been proved to be effective for many tasks (Qin et al., 2016a; Wang et al., 2016a; Zhang et al., 2016b; Wang et al., 2015; Wang et al., 2014; Cai and
2263
Zhao, 2016), also including discourse parsing.",1 Introduction,[0],[0]
Ji and Eisenstein (2015) adopt recursive neural network and incorporate with entity-augmented distributed semantics.,1 Introduction,[0],[0]
Zhang et al. (2015) explore a shallow convolutional neural network and achieve competitive performance.,1 Introduction,[0],[0]
"Although simple neural network has been shown effective, the result has not been quite satisfactory which suggests that there is still space for improving.
",1 Introduction,[0],[0]
"The concerned task could be straightforwardly formalized as a sentence-pair classification problem, which needs inferring senses solely based on the two arguments without cues of connectives.",1 Introduction,[0],[0]
Two problems should be carefully handled in this task: how to model sentences and how to capture the interactions between the two arguments.,1 Introduction,[0],[0]
"The former could be addressed by Convolutional Neural Network (CNN) which has been proved effective for sentence modeling (Kalchbrenner et al., 2014; Kim, 2014), while the latter is the key problem, which might need deep semantic analysis for the interaction of two arguments.",1 Introduction,[0],[0]
"To solve the latter problem, we propose collaborative gated neural network (CGNN) which is partially inspired by Highway Network whose gate mechanism achieves success (Srivastava et al., 2015).",1 Introduction,[0],[0]
"Our method will be evaluated on the benchmark dataset against state-of-the-art methods.
",1 Introduction,[0],[0]
"The rest of the paper is organized as follows: Section 2 briefly describes our model, introducing the stacking architecture of CNN and CGNN, Section 3 shows the experiments and analysis, and Section 4 concludes this paper.",1 Introduction,[0],[0]
"The architecture of the model, as shown in Figure 1, is straightforward.",2 Method,[0],[0]
It can be divided into three parts: 1) CNN for modeling arguments; 2) CGNN unit for feature transformation; 3) a conventional softmax layer for the final classification.,2 Method,[0],[0]
"CNN is used to obtain the vector representations for the sentences, CGNN further captures and transforms the features for the final classification.",2 Method,[0],[0]
"As CNN has been broadly adopted for modeling sentences, we will explain it in brevity.",2.1 Convolutional Neural Network,[0],[0]
"For two arguments, typical sentence modeling process
will be applied: sentence embedding (including embeddings for words and part-of-speech (POS) tags) through projection layer, convolution operations (with multiple groups of filters) through the convolution layer, obtaining the sentence representation through one-max-pooling.",2.1 Convolutional Neural Network,[0],[0]
"The two arguments will get their sentence vectors independently without any interfering, and the convolution operation will be the same by sharing parameters.",2.1 Convolutional Neural Network,[0],[0]
The final argument-pair representation will be the vector v which is concatenated from two sentence vectors and this vector will be used as the input of the CGNN unit.,2.1 Convolutional Neural Network,[0],[0]
"For implicit sense classification, the key is how to effectively capture the interactions between the two arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
"The interactions could be word pairs, phrase pairs or even the latent meaning of the two full arguments.",2.2 Collaborative Gated Neural Network,[0],[0]
Pitler et al. (2009) has shown that word pair features are helpful.,2.2 Collaborative Gated Neural Network,[0],[0]
"To model these interactions, we have to make a full use of the sentence vectors obtained from CNN.",2.2 Collaborative Gated Neural Network,[0],[0]
"However, common neural hidden layers might be insufficient to deal with the challenge.",2.2 Collaborative Gated Neural Network,[0],[0]
"We need to seek more powerful neural models, i.e., gated neural network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"In recent years, gated mechanism has gained popularity in neural models.",2.2 Collaborative Gated Neural Network,[0],[0]
"Although it is first introduced in the cells of recurrent neural networks, like Long-Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014), traditional feed-forward neural models such as the Highway Network could also benefit from it (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"The existing studies show that the gated mechanism in highway network serves not only a means for easier training, but also a tool to route information in a trained network.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Motivated by the idea of highway network, we propose a collaborative gated neural network (CGNN) for this task.",2.2 Collaborative Gated Neural Network,[0],[0]
"The architecture of CGNN is illustrated in Figure 1, and it contains a sequence of transformations.",2.2 Collaborative Gated Neural Network,[0],[0]
"First, the inner-cell ĉ is obtained through linear transformation and non-linear activation on the input v, and this process is exactly the operation of an ordinary neural layer.
",2.2 Collaborative Gated Neural Network,[0],[0]
"ĉ = tanh(Wc · v + bc)
",2.2 Collaborative Gated Neural Network,[0],[0]
"Meanwhile, the two gates gi and go are calculated independently because they are only influenced by
the original input through different parameters:
gi = σ(W i · v + bi) go = σ(W o · v + bo)
where the σ denotes sigmoid function which guarantees the values in the gates are in [0,1].",2.2 Collaborative Gated Neural Network,[0],[0]
"Two gated operations are applied sequentially, where a gated operation indicates the element-wise multiplication of an inner-cell and a gate.",2.2 Collaborative Gated Neural Network,[0],[0]
"Between the two gated operations, a non-linear activation operation is applied.",2.2 Collaborative Gated Neural Network,[0],[0]
"The procedure could be formulated as follows:
c = ĉ gi h = tanh(c) go
where denotes element-wise multiplication, c is the second inner-cell and h is the output of CGNN unit.
",2.2 Collaborative Gated Neural Network,[0],[0]
"Although the two gates are generated independently, they will work collaboratively because they control the information flow of the inner-cells sequentially which resembles logical AND operation in a probabilistic version.",2.2 Collaborative Gated Neural Network,[0],[0]
"In fact, the transformations after ĉ will concern only element-wise operations which might give finer controls for each dimension, and the information can only flow on the dimensions where both gates are “open”.",2.2 Collaborative Gated Neural Network,[0],[0]
"This procedure will help select the most crucial features.
",2.2 Collaborative Gated Neural Network,[0],[0]
The gates in this model are mainly used for routing information from sentence-pairs vectors.,2.2 Collaborative Gated Neural Network,[0],[0]
"When there is only one gate in our network, the model works similar to the highway network (Srivastava et al., 2015).",2.2 Collaborative Gated Neural Network,[0],[0]
"After the transformation of the CGNN unit, the transformed vector h will be sent to a conventional softmax for classification.
",2.3 Output and Training,[0],[0]
"The training object J will be the cross-entropy error E with L2 regularization:
E(ŷ, y) =",2.3 Output and Training,[0],[0]
"− l∑
j
yj × log(Pr(ŷj))
",2.3 Output and Training,[0],[0]
J(θ),2.3 Output and Training,[0],[0]
"= 1
m
m∑
k
E(ŷ(k), y(k))",2.3 Output and Training,[0],[0]
+,2.3 Output and Training,[0],[0]
"λ
2 ‖θ‖2
where yj is the gold label and ŷj is the predicted one.",2.3 Output and Training,[0],[0]
"We adopt the diagonal variant of AdaGrad (Duchi et al., 2011) for the optimization process.",2.3 Output and Training,[0],[0]
"As for the benchmark dataset, Penn Discourse Treebank (PDTB) (Prasad et al., 2008) corpus1 is used for evaluation.",3.1 Setting,[0],[0]
"In the PDTB, each discourse relation is annotated between two argument spans.
",3.1 Setting,[0],[0]
"To be consistent with the setups of prior works, we formulate the implicit relation classification task as four one-versus-other binary classification problems only using the four top level classes: COMPARISON (COMP.), CONTINGENCY (CONT.), EXPANSION (EXP.) and TEMPORAL (TEMP.).",3.1 Setting,[0],[0]
"While different works include different relations of varying specificities, all of them include these four core relations (Pitler et al., 2009).",3.1 Setting,[0],[0]
"Following dataset splitting convention of the previous works, we use sections 2-20 for training, sections 21-22 for testing and sections 0-1 for development set.",3.1 Setting,[0],[0]
"The proposed model is possible to be extended for multi-class classification of discourse parsing, but for the comparisons with most of previous works, we will follow them and focus on the binary classification problems.
",3.1 Setting,[0],[0]
"For other hyper-parameters of the model and training process, we fix the lengths of both the input arguments to be 80, and apply truncating or zero-padding when necessary.",3.1 Setting,[0],[0]
"The dimensions for word embeddings and POS embeddings are respectively 300 and 50, and the embedding layer adopts a dropout of 0.2.",3.1 Setting,[0],[0]
"The word embeddings are initialized with pre-trained word vectors using word2vec 2 (Mikolov et al., 2013) and other parameters are randomly initialized including POS embeddings.",3.1 Setting,[0],[0]
"We
1http://www.seas.upenn.edu/˜pdtb/ 2http://www.code.google.com/p/word2vec
set the starting learning rate to 0.001.",3.1 Setting,[0],[0]
"For CNN model, we utilize three groups of filters with window widths of (2, 2, 2) and their filter numbers are all set to 1024.",3.1 Setting,[0],[0]
The hyper-parameters are the same for all models and we do not tune them individually.,3.1 Setting,[0],[0]
"For transformation of sentence vectors, a simple Multilayer Perceptron (MLP) layer could be a straightforward choice, while more complex neural modules, such as LSTM and highway network, could also be considered.",3.2 Model Analysis,[0],[0]
Our model utilizes a CGNN unit with refined gated mechanism for the transformation.,3.2 Model Analysis,[0],[0]
Will the proposed CGNN really bring about further performance improvement?,3.2 Model Analysis,[0],[0]
"We now answer this question empirically.
",3.2 Model Analysis,[0],[0]
"As shown in Table 1, CNN model usually performs well on its own.",3.2 Model Analysis,[0],[0]
"Utilizing an MLP layer or a Highway layer could improve the accuracies on CONTINGENCY, EXPANSION, TEMPORARY except for COMPARISON.",3.2 Model Analysis,[0],[0]
"Though the primary motivation of Highway is to ease gradient-based training of highly deep networks through utilizing gated units, it works merely as an ordinary MLP in the proposed model, which explains the reason that it performs like MLP.",3.2 Model Analysis,[0],[0]
"Despite one of four classes, COMPARISON, not receiving performance improvement, introducing a non-linear transformation layer lets the classification benefit as a whole.",3.2 Model Analysis,[0],[0]
"“CNN+LSTM” denotes the method of using LSTM to read the convolution sequence (without pooling operation), and it even does not perform better than MLP.
",3.2 Model Analysis,[0],[0]
The CGNN achieves the best performance on all classes including COMPARISON.,3.2 Model Analysis,[0],[0]
It gains 3.97% imrovement on average F1 score using CNN only model.,3.2 Model Analysis,[0],[0]
"We assume that CGNN is well-suited to work with CNN, adaptively transforming and combining local features detected by the individual filters.",3.2 Model Analysis,[0],[0]
We show the main results in Tables 2 and 3.,3.3 Results,[0],[0]
"The metrics include precision (P), recall (R), accuracy (Acc) and F1 score.",3.3 Results,[0],[0]
"Since not all of these metrics are reported in previous work, the comparisons are correspondingly in Table 2 and 3.",3.3 Results,[0],[0]
"Some previous work merges Entrel with Expansion, which is also explored in our study and noted as EXP.+.
",3.3 Results,[0],[0]
We compare with best-performed or competitive models including both traditional linear methods and recent neural methods.,3.3 Results,[0],[0]
"For traditional methods: Pitler et al. (2009) use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features; Zhou et al. (2010) improve the performance through predicting connective words as features; Park and Cardie (2012) propose a locallyoptimal feature set and further identify factors for feature extraction that can have a major impact performance, including stemming and lexicon look-up; Biran and McKeown (2013) collect word pairs from arguments of explicit examples to help the learning; Rutherford and Xue (2014) employ Brown cluster pair and coreference patterns for performance enhancement.",3.3 Results,[0],[0]
"Several neural methods have also been included for comparison: Zhang et al. (2015) propose a simplified neural network which has only
three different pooling operations (max, min, average); Ji and Eisenstein (2015) compute distributed semantics representation by composition up the syntactic parse tree through recursive neural network; Braud and Denis (2015) consider shallow lexical features and word embeddings.",3.3 Results,[0],[0]
Chen et al. (2016) replace the original words by word embeddings to overcome the data sparsity problem and they also utilize gated relevance network to capture the semantic interaction between word pairs.,3.3 Results,[0],[0]
"The gated network is different from ours but also works well.
",3.3 Results,[0],[0]
"Our model achieves F-measure improvements of 1.85% on COMPARISON, 1.56% on CONTINGENCY, 1.27% on EXPANSION, 0.94% on EXPANSION+, 4.89% on TEMPORAL, against the state-ofthe-art of each class.",3.3 Results,[0],[0]
We improve by 4.73% on average F1 score when not including ENTREL in EXPANSION as reported in Table 2 and 3.19% on average F1 score otherwise as reported in Table 3.,3.3 Results,[0],[0]
The results show that our model achieves the best performance and especially makes the most remarkable progress on TEMPORAL.,3.3 Results,[0],[0]
"In this paper, we propose a stacking gated neural architecture for implicit discourse relation classification.",4 Conclusion,[0],[0]
Our model includes convolution and collaborative gated neural network.,4 Conclusion,[0],[0]
The analysis and experiments show that CNN performs well on its own and combining CGNN provides further gains.,4 Conclusion,[0],[0]
Our evaluation on PTDB shows that the proposed model outperforms previous state-of-the-art systems.,4 Conclusion,[0],[0]
Discourse parsing is considered as one of the most challenging natural language processing (NLP) tasks.,abstractText,[0],[0]
Implicit discourse relation classification is the bottleneck for discourse parsing.,abstractText,[0],[0]
"Without the guide of explicit discourse connectives, the relation of sentence pairs are very hard to be inferred.",abstractText,[0],[0]
This paper proposes a stacking neural network model to solve the classification problem in which a convolutional neural network (CNN) is utilized for sentence modeling and a collaborative gated neural network (CGNN) is proposed for feature transformation.,abstractText,[0],[0]
Our evaluation and comparisons show that the proposed model outperforms previous state-of-the-art systems.,abstractText,[0],[0]
A Stacking Gated Neural Architecture for Implicit Discourse Relation Classification,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1030–1040, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
A verb plays a primary role in conveying the meaning of a sentence.,1 Introduction,[0],[0]
"Capturing the sense of a verb is essential for natural language processing (NLP), and thus lexical resources for verbs play an important role in NLP.
",1 Introduction,[0],[0]
Verb classes are one such lexical resource.,1 Introduction,[0],[0]
"Manually-crafted verb classes have been developed, such as Levin’s classes (Levin, 1993) and their extension, VerbNet (Kipper-Schuler, 2005), in which verbs are organized into classes on the basis of their syntactic and semantic behavior.",1 Introduction,[0],[0]
"Such verb classes have been used in many NLP applications that need to consider semantics in particular, such as word sense disambiguation (Dang, 2004), semantic parsing (Swier and Stevenson, 2005; Shi and Mihalcea, 2005) and discourse parsing (Subba and Di Eugenio, 2009).
",1 Introduction,[0],[0]
"There have also been many attempts to automatically acquire verb classes with the goal of ei-
ther adding frequency information to an existing resource or of inducing similar verb classes for other languages.",1 Introduction,[0],[0]
"Most of these approaches assume that all target verbs are monosemous (Stevenson and Joanis, 2003; Schulte im Walde, 2006; Joanis et al., 2008; Li and Brew, 2008; Sun et al., 2008; Sun and Korhonen, 2009; Vlachos et al., 2009; Parisien and Stevenson, 2010; Parisien and Stevenson, 2011; Falk et al., 2012; Lippincott et al., 2012; Reichart and Korhonen, 2013; Sun et al., 2013).",1 Introduction,[0],[0]
"This monosemous assumption, however, is not realistic because many frequent verbs actually have multiple senses.",1 Introduction,[0],[0]
"Moreover, to the best of our knowledge, none of the following approaches attempt to quantitatively evaluate soft clusterings of verb classes induced by polysemy-aware unsupervised approaches (Korhonen et al., 2003; Lapata and Brew, 2004; Li and Brew, 2007; Schulte im Walde et al., 2008).
",1 Introduction,[0],[0]
"In this paper, we propose an unsupervised method for inducing verb classes that is aware of verb polysemy.",1 Introduction,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,1 Introduction,[0],[0]
"By taking this step-wise approach, we can not only induce verb classes with frequency information from a massive amount of verb uses in a scalable manner, but also deal with verb polysemy.
",1 Introduction,[0],[0]
"Our novel contributions are summarized as follows:
• induce both semantic frames and verb classes from a massive amount of verb uses by a scalable method,
• explicitly deal with verb polysemy, • discover effective features for each of the
clustering steps, and
• quantitatively evaluate a soft clustering of verbs.
1030",1 Introduction,[0],[0]
"As stated in Section 1, most of the previous studies on verb clustering assume that verbs are monosemous.",2 Related Work,[0],[0]
"A typical method in these studies is to represent each verb as a single data point and apply classification (e.g., Joanis et al. (2008)) or clustering (e.g., Sun and Korhonen (2009)) to these data points.",2 Related Work,[0],[0]
"As a representation for a data point, distributions of subcategorization frames are often used, and other semantic features (e.g., selectional preferences) are sometimes added to improve the performance.
",2 Related Work,[0],[0]
"Among these studies on monosemous verb clustering (i.e., predominant class induction), there have been several Bayesian methods.",2 Related Work,[0],[0]
Vlachos et al. (2009) proposed a Dirichlet process mixture model (DPMM; Neal (2000)) to cluster verbs based on subcategorization frame distributions.,2 Related Work,[0],[0]
"They evaluated their result with a gold-standard test set, where a single class is assigned to a verb.",2 Related Work,[0],[0]
Parisien and Stevenson (2010) proposed a hierarchical Dirichlet process (HDP; Teh et al. (2006)) model to jointly learn argument structures (subcategorization frames) and verb classes by using syntactic features.,2 Related Work,[0],[0]
Parisien and Stevenson (2011) extended their model by adding semantic features.,2 Related Work,[0],[0]
They tried to account for verb learning by children and did not evaluate the resultant verb classes.,2 Related Work,[0],[0]
"Modi et al. (2012) extended the model of Titov and Klementiev (2012), which is an unsupervised model for inducing semantic roles, to jointly induce semantic roles and frames across verbs using the Chinese Restaurant Process (Aldous, 1985).",2 Related Work,[0],[0]
"All of the above methods considered verbs to be monosemous and did not deal with verb polysemy.
",2 Related Work,[0],[0]
"Our approach also uses Bayesian methods, but is designed to capture verb polysemy.
",2 Related Work,[0],[0]
"We summarize a few studies that consider polysemy of verbs in the rest of this section.
",2 Related Work,[0],[0]
Miyao and Tsujii (2009) proposed a supervised method that can handle verb polysemy.,2 Related Work,[0],[0]
"Their method represents a verb’s syntactic and semantic features, and learns a log-linear model from the SemLink corpus (Loper et al., 2007).",2 Related Work,[0],[0]
"Boleda et al. (2007) also proposed a supervised method for Catalan adjectives considering the polysemy of adjectives.
",2 Related Work,[0],[0]
"The most closely related work to our polysemyaware task of unsupervised verb class induction is the work of Korhonen et al. (2003), who used distributions of subcategorization frames to cluster verbs.",2 Related Work,[0],[0]
They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.,2 Related Work,[0],[0]
"In particular, they tried to consider verb polysemy by using the IB method, which is a soft clustering method (Tishby et al., 1999).",2 Related Work,[0],[0]
"However, the verb itself is still represented as a single data point.",2 Related Work,[0],[0]
"After performing soft clustering, they noted that most verbs fell into a single class, and they decided to assign a single class to each verb by hardening the clustering.",2 Related Work,[0],[0]
They considered multiple classes only in the gold-standard data used for their evaluations.,2 Related Work,[0],[0]
"We also evaluate our induced verb classes on this gold-standard data, which was created on the basis of Levin’s classes (Levin, 1993).
",2 Related Work,[0],[0]
Lapata and Brew (2004) and Li and Brew (2007) proposed probabilistic models for calculating prior probabilities of verb classes for a verb.,2 Related Work,[0],[0]
"These models are approximated to condition not
on verbs but on subcategorization frames.",2 Related Work,[0],[0]
"As mentioned in Li and Brew (2007), it is desirable to extend the model to depend on verbs to further improve accuracy.",2 Related Work,[0],[0]
"They conducted several evaluations including predominant class induction and token-level verb sense disambiguation, but did not evaluate multiple classes output by their models.",2 Related Work,[0],[0]
Schulte im Walde et al. (2008) also applied probabilistic soft clustering to verbs by incorporating subcategorization frames and selectional preferences based on WordNet.,2 Related Work,[0],[0]
This model is based on the Expectation-Maximization algorithm and the Minimum Description Length principle.,2 Related Work,[0],[0]
"Since they focused on the incorporation of selectional preferences, they did not evaluate verb classes but evaluated only selectional preferences using a language model-based measure.
",2 Related Work,[0],[0]
"Materna proposed LDA-frames, which are defined across verbs and can be considered to be a kind of verb class (Materna, 2012; Materna, 2013).",2 Related Work,[0],[0]
LDA-frames are probabilistic semantic frames automatically induced from a raw corpus.,2 Related Work,[0],[0]
"He used a model based on latent Dirichlet allocation (LDA; Blei et al. (2003)) and the Dirichlet process to cluster verb instances of a triple (subject, verb, object) to produce semantic frames and roles.",2 Related Work,[0],[0]
Both of these are represented as a probabilistic distribution of words across verbs.,2 Related Work,[0],[0]
"He applied this method to the BNC and acquired 1,200 frames and 400 roles (Materna, 2012).",2 Related Work,[0],[0]
"He did not evaluate the resulting frames as verb classes.
",2 Related Work,[0],[0]
"In sum, there have been no studies that quantitatively evaluate polysemous verb classes automatically induced by unsupervised methods.",2 Related Work,[0],[0]
Our objective is to automatically learn semantic frames and verb classes from a massive amount of verb uses following usage-based approaches.,3.1 Overview,[0],[0]
"Although Bayesian approaches are a possible solution to simultaneously induce frames and verb classes from a corpus as used in previous studies, it has prohibitive computational cost.",3.1 Overview,[0],[0]
"For instance, Parisien and Stevenson applied HDP only to a small-scale child speech corpus that contains 170K verb uses to jointly induce subcategorization frames and verb classes (Parisien and Stevenson, 2010; Parisien and Stevenson, 2011).",3.1 Overview,[0],[0]
"Materna applied an LDA-based method to the BNC, which contains 1.4M verb uses, to induce seman-
tic frames across verbs that can be considered to be verb classes (Materna, 2012; Materna, 2013).",3.1 Overview,[0],[0]
"However, it would take three months for this experiment using this 100 million word corpus.1 Although it is best to use the largest possible corpus for this kind of knowledge acquisition tasks (Sasano et al., 2009), it is infeasible to scale to giga-word corpora using such joint models.
",3.1 Overview,[0],[0]
"In this paper, we propose a two-step approach for inducing semantic frames and verb classes.",3.1 Overview,[0],[0]
"First, we make multiple data points for each verb to deal with verb polysemy (cf. polysemy-aware previous studies still represented a verb as one data point (Korhonen et al., 2003; Miyao and Tsujii, 2009)).",3.1 Overview,[0],[0]
"To do that, we induce verb-specific semantic frames by clustering verb uses.",3.1 Overview,[0],[0]
"Then, we induce verb classes by clustering these verbspecific semantic frames across verbs.",3.1 Overview,[0],[0]
"An interesting point here is that we can use exactly the same method for these two clustering steps.
",3.1 Overview,[0],[0]
"Our procedure to automatically induce verb classes from verb uses is summarized as follows:
1.",3.1 Overview,[0],[0]
"induce verb-specific semantic frames by clustering predicate-argument structures for each verb extracted from automatic parses as shown in the lower part of Figure 1, and
2.",3.1 Overview,[0],[0]
"induce verb classes by clustering the induced semantic frames across verbs as shown in the upper part of Figure 1.
",3.1 Overview,[0],[0]
Each of these two steps is described in the following sections in detail.,3.1 Overview,[0],[0]
We induce verb-specific semantic frames from verb uses based on the method of Kawahara et al. (2014).,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"Our semantic frames consist of case slots, each of which consists of word instances that can be filled.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"The procedure for inducing these semantic frames is as follows:
1.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses,
2.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and
1In our replication experiment, it took a week to perform 70 iterations using Materna’s code and an Intel Xeon E5-2680 (2.7GHz) CPU.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"To reach 1,000 iterations, which are reported to be optimum, it would take three months.
3.",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
"apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames.
",3.2 Inducing Verb-specific Semantic Frames,[0],[0]
These three steps are briefly described below.,3.2 Inducing Verb-specific Semantic Frames,[0],[0]
We apply dependency parsing to a large raw corpus.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"We use the Stanford parser with Stanford dependencies (de Marneffe et al., 2006).2 Collapsed dependencies are adopted to directly extract prepositional phrases.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Then, we extract predicate-argument structures from the dependency parses.",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"Dependents that have the following dependency relations to a verb are extracted as arguments:
nsubj, xsubj, dobj, iobj, ccomp, xcomp, prep ∗
In this process, the verb and arguments are lemmatized, and only the head of an argument is preserved for compound nouns.
",3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
Predicate-argument structures are collected for each verb and the subsequent processes are applied to the predicate-argument structures of each verb.,3.2.1 Extracting Predicate-argument Structures from a Raw Corpus,[0],[0]
"To make the computation feasible, we merge the predicate-argument structures that have the same or similar meaning to get initial frames.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
These initial frames are the input of the subsequent clustering process.,3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicateargument structures.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"For each predicate-argument structure of a verb, we couple the verb and an argument to make a unit for sense disambiguation.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We select an argument in the following order by considering the degree of effect on the verb sense:3
dobj, ccomp, nsubj, prep ∗, iobj.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"Then, the predicate-argument structures that have the same verb and argument pair (slot and word, e.g., “dobj:effect”) are merged into an initial frame.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"After this process, we discard minor initial frames that occur fewer than 10 times.
",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"2http://nlp.stanford.edu/software/lex-parser.shtml 3If a predicate-argument structure has multiple preposi-
tional phrases, one of them is randomly selected.",3.2.2 Constructing Initial Frames from Predicate-argument Structures,[0],[0]
"We cluster initial frames for each verb to produce semantic frames using the Chinese Restaurant Process (Aldous, 1985), regarding each initial frame as an instance.
",3.2.3 Clustering Method,[0],[0]
"We calculate the posterior probability of a cluster cj given an initial frame fi as follows:
P (cj |fi) ∝",3.2.3 Clustering Method,[0],[0]
"{ n(cj) N+α · P (fi|cj) cj ̸= new
α N+α · P (fi|cj) cj",3.2.3 Clustering Method,[0],[0]
"= new,
(1)
where N is the number of initial frames for the target verb and n(cj) is the current number of initial frames assigned to the cluster cj .",3.2.3 Clustering Method,[0],[0]
α is a hyperparameter that determines how likely it is for a new cluster to be created.,3.2.3 Clustering Method,[0],[0]
"In this equation, the first term is the Dirichlet process prior and the second term is the likelihood of fi.
P (fi|cj) is defined based on the DirichletMultinomial distribution as follows:
P (fi|cj) = ∏ w∈V",3.2.3 Clustering Method,[0],[0]
"P (w|cj)count(fi,w), (2)
where V is the vocabulary in all case slots cooccurring with the verb and count(fi, w) is the number of w in the initial frame fi.",3.2.3 Clustering Method,[0],[0]
"The original method in Kawahara et al. (2014) defined w as pairs of slots and words, e.g., “nsubj:child” and “dobj:bird,” but does not consider slot-only features, e.g., “nsubj” and “dobj,” which ignore lexical information.",3.2.3 Clustering Method,[0],[0]
"Here we experiment with both representations and compare the results.
",3.2.3 Clustering Method,[0],[0]
"P (w|cj) is defined as follows:
P (w|cj) =",3.2.3 Clustering Method,[0],[0]
"count(cj , w) + β∑ t∈V",3.2.3 Clustering Method,[0],[0]
"count(cj , t) + |V",3.2.3 Clustering Method,[0],[0]
"| · β , (3)
where count(cj , w) is the current number of w in the cluster cj , and β is a hyper-parameter of Dirichlet distribution.",3.2.3 Clustering Method,[0],[0]
"For a new cluster, this probability is uniform (1/|V |).
",3.2.3 Clustering Method,[0],[0]
"We regard each output cluster as a semantic frame, by merging the initial frames in a cluster into a semantic frame.",3.2.3 Clustering Method,[0],[0]
"In this way, semantic frames for each verb are acquired.
",3.2.3 Clustering Method,[0],[0]
We use Gibbs sampling to realize this clustering.,3.2.3 Clustering Method,[0],[0]
"To induce verb classes across verbs, we apply clustering to the induced verb-specific semantic
frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We can use exactly the same clustering method as described in Section 3.2.3 by using semantic frames for multiple verbs as an input instead of initial frames for a single verb.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"This is because an initial frame has the same structure as a semantic frame, which is produced by merging initial frames.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"We regard each output cluster as a verb class this time.
",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
"For the features, w, in equation (2), we try the two representations again: slot-only features and slot-word pair features.",3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The representation using only slots corresponds to the consideration of only syntactic argument patterns.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
The other representation using the slot-word pairs means that semantic similarity based on word overlap is naturally considered by looking at lexical information.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We will compare in our experiments four possible combinations: two feature representations for each of the two clustering steps.,3.3 Inducing Verb Classes from Semantic Frames,[0],[0]
We first describe our experimental settings and define evaluation metrics to evaluate induced soft clusterings of verb classes.,4 Experiments and Evaluations,[0],[0]
"Then, we conduct type-level multi-class evaluations, type-level single-class evaluations and token-level multiclass evaluations.",4 Experiments and Evaluations,[0],[0]
These two levels of evaluations are performed by considering the work of Reichart et al. (2010) on clustering evaluation.,4 Experiments and Evaluations,[0],[0]
"Finally, we discuss the results of our full experiments.",4 Experiments and Evaluations,[0],[0]
"We use two kinds of large-scale corpora: a web corpus and the English Gigaword corpus.
To prepare a web corpus, we extracted sentences from crawled web pages that are judged to be written in English based on the encoding information.",4.1 Experimental Settings,[0],[0]
"Then, we selected sentences that consist of at most 40 words, and removed duplicated sentences.",4.1 Experimental Settings,[0],[0]
"From this process, we obtained a corpus of one billion sentences, totaling approximately 20 billion words.",4.1 Experimental Settings,[0],[0]
"We focused on verbs whose frequency in the web corpus was more than 1,000.",4.1 Experimental Settings,[0],[0]
"There were 19,649 verbs, including phrasal verbs, and separating passive and active constructions.",4.1 Experimental Settings,[0],[0]
"We extracted 2,032,774,982 predicate-argument structures.
",4.1 Experimental Settings,[0],[0]
We also used the English Gigaword corpus (LDC2011T07; English Gigaword Fifth Edition).,4.1 Experimental Settings,[0],[0]
"This corpus consists of approximately 180 million sentences, which totaling four billion words.
",4.1 Experimental Settings,[0],[0]
"There were 7,356 verbs after applying the same frequency threshold as the web corpus.",4.1 Experimental Settings,[0],[0]
"We extracted 423,778,278 predicate-argument structures from this corpus.
",4.1 Experimental Settings,[0],[0]
We set the hyper-parameters α in (1) and β in (3) to 1.0.,4.1 Experimental Settings,[0],[0]
The cluster assignments for all the components were initialized randomly.,4.1 Experimental Settings,[0],[0]
We took 100 samples for each input frame and selected the cluster assignment that has the highest probability.,4.1 Experimental Settings,[0],[0]
"To measure the precision and recall of a clustering, modified purity and inverse purity (also called collocation or weighted class accuracy) are commonly used in previous studies on verb clustering (e.g., Sun and Korhonen (2009)).",4.2 Evaluation Metrics,[0],[0]
"However, since these measures are only applicable to a hard clustering, it is necessary to extend them to be applicable to a soft clustering, because in our task a verb can belong to multiple clusters or classes.4",4.2 Evaluation Metrics,[0],[0]
We propose a normalized version of modified purity and inverse purity.,4.2 Evaluation Metrics,[0],[0]
"This kind of normalization for soft clusterings was performed for other evaluation metrics as in Springorum et al. (2013).
",4.2 Evaluation Metrics,[0],[0]
"To measure the precision of a clustering, a normalized version of modified purity is defined as follows.",4.2 Evaluation Metrics,[0],[0]
Suppose K is the set of automatically induced clusters and G is the set of gold classes.,4.2 Evaluation Metrics,[0],[0]
Let Ki be the verb vector of the i-th cluster and Gj be the verb vector of the j-th gold class.,4.2 Evaluation Metrics,[0],[0]
"Each component of these vectors is a normalized frequency, which equals a cluster/class attribute probability given a verb.",4.2 Evaluation Metrics,[0],[0]
"Where there is no frequency information available for class distribution, such as the gold-standard data described in Section 4.3, we use a uniform distribution across the verb’s classes.",4.2 Evaluation Metrics,[0],[0]
The core idea of purity is that each cluster Ki is associated with its most prevalent gold class.,4.2 Evaluation Metrics,[0],[0]
"In addition, to penalize clusters that consist of only one verb, such singleton clusters in K are considered as errors, as is usual with modified purity.",4.2 Evaluation Metrics,[0],[0]
"The normalized modified purity (nmPU) can then be written as follows:
nmPU = 1 N ∑ i s.t. |Ki|>1",4.2 Evaluation Metrics,[0],[0]
"max j δKi(Ki ∩ Gj), (4)
δKi(Ki ∩ Gj) = ∑
v∈Ki∩Gj civ, (5)
",4.2 Evaluation Metrics,[0],[0]
4Korhonen et al. (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb.,4.2 Evaluation Metrics,[0],[0]
"They reported only precision measures including modified purity, and avoided extending the evaluation metrics for soft clusterings.
where N denotes the total number of verbs, |Ki| denotes the number of positive components in Ki, and civ denotes the v-th component of Ki.",4.2 Evaluation Metrics,[0],[0]
"δKi(Ki ∩ Gj) means the total mass of the set of verbs in Ki ∩Gj , given by summing up the values in Ki.",4.2 Evaluation Metrics,[0],[0]
"In case of evaluating a hard clustering, this is equal to |Ki ∩",4.2 Evaluation Metrics,[0],[0]
"Gj | because all the values of civ are equal to 1.
",4.2 Evaluation Metrics,[0],[0]
"As usual, the following normalized inverse purity (niPU) is used to measure the recall of a clustering:
niPU = 1 N ∑",4.2 Evaluation Metrics,[0],[0]
j max i δGj (Ki ∩ Gj).,4.2 Evaluation Metrics,[0],[0]
"(6)
Finally, we use the harmonic mean (F1) of nmPU and niPU as a single measure of clustering quality.",4.2 Evaluation Metrics,[0],[0]
"We first evaluate our induced verb classes on the test set created by Korhonen et al. (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin’s classes and the LCS database (Dorr, 1997).",4.3 Type-level Multi-class Evaluations,[0],[0]
"It consists of 62 classes and 110 verbs, out of which 35 verbs are monosemous and 75 verbs are polysemous.",4.3 Type-level Multi-class Evaluations,[0],[0]
The average number of verb classes per verb is 2.24.,4.3 Type-level Multi-class Evaluations,[0],[0]
"An excerpt from this data is shown in Table 1.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"As our baselines, we adopt two previously proposed methods.",4.3 Type-level Multi-class Evaluations,[0],[0]
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al. (2003).,4.3 Type-level Multi-class Evaluations,[0],[0]
They used the information bottleneck (IB) method for assigning probabilities of classes to each verb.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Note that Korhonen et al. (2003) actually hardened the clusterings and left
the evaluations of soft clusterings for their future work.",4.3 Type-level Multi-class Evaluations,[0],[0]
"For input data, we employ VALEX (Korhonen et al., 2006), which is a publicly-available large-scale subcategorization lexicon.5 By following the method of Korhonen et al. (2003), prepositional phrases (pp) are parameterized for two frequent subcategorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"It is necessary to specify the number of clusters, k, for the IB method beforehand, and we adopt 35 and 42 clusters according to their reported high accuracies.",4.3 Type-level Multi-class Evaluations,[0],[0]
"To output multiple classes for each verb, we set a threshold, t, for class attribute probabilities.",4.3 Type-level Multi-class Evaluations,[0],[0]
"That is, classes that have a higher class attribute probability than the threshold are output for each verb.",4.3 Type-level Multi-class Evaluations,[0],[0]
"We report the results of the following threshold values: 0.01, 0.02, 0.05 and 0.10.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"The other baseline is LDA-frames (Materna, 2012).",4.3 Type-level Multi-class Evaluations,[0],[0]
"We use the induced LDA-frames that are
5http://ilexir.co.uk/applications/valex/
available on the web site.6 This frame data was induced from the BNC and consists of 1,200 frames and 400 semantic roles.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Again, we set a threshold for frame attribute probabilities.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We report results using our methods with four feature combinations (slot-only (S) and slot-word pair (SW) features each used for both the framegeneration and verb-class clustering steps) for both the Gigaword and web corpora.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Table 2 lists evaluation results for the baseline methods and our methods.7 The results of the IB baseline and our methods are obtained by averaging five runs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
We can see that “web/SW-S” achieved the best performance and obtained a higher F1 than the baselines by more than nine points.,4.3 Type-level Multi-class Evaluations,[0],[0]
“Web/SWS” uses the combination of slot-word pair features for clustering verb-specific frames and slotonly features for clustering across verbs.,4.3 Type-level Multi-class Evaluations,[0],[0]
"Interestingly, this result indicates that slot distributions are more effective than lexical information in slotword pairs for inducing verb classes similar to the gold standard.",4.3 Type-level Multi-class Evaluations,[0],[0]
"This result is consistent with expectations, given a gold standard based on Levin’s verb classes, which are organized according to the syntactic behavior of verbs.",4.3 Type-level Multi-class Evaluations,[0],[0]
"The use of slot-word pairs for verb class induction generally merged too many frames into each class, apparently due to accidental word overlaps across verbs.
",4.3 Type-level Multi-class Evaluations,[0],[0]
The verb classes induced from the web corpus achieved a higher F1 than those from the Gigaword corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
This can be attributed to the larger size of the web corpus.,4.3 Type-level Multi-class Evaluations,[0],[0]
"The employment of this kind of huge corpus is enabled by our scalable method.
",4.3 Type-level Multi-class Evaluations,[0],[0]
"6http://nlp.fi.muni.cz/projekty/lda-frames/ 7Although we do not think that the classes with very small attribute probabilities are meaningful, the F1 scores for lower thresholds than 0.01 converged to about 66 in the case of LDA-frames.",4.3 Type-level Multi-class Evaluations,[0],[0]
"Since we focus on the handling of verb polysemy, predominant class induction for each verb is not our main objective.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"However, we wish to compare our method with previous work on the induction of a predominant (monosemous) class for each verb.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To output a single class for each verb by using our proposed method, we skip the induction of verb-specific semantic frames and instead create a single frame for each verb by merging all predicate-argument structures of the verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Then, we apply clustering to these frames across verbs.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For clustering features, we again compare two representations: slot-only features (S) and slot-word pair features (SW).
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al. (2003).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This data contains 110 verbs and 33 classes.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We evaluate these single-class outputs in the same manner as Korhonen et al. (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"As we did with the multi-class evaluations, we adopt modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1) as the metrics for the evaluation with predominant classes.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"It is not necessary to normalize these metrics when we treat verbs as monosemous, and evaluate against the predominant sense.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"When we evaluate against the multiple classes in the gold standard, we do normalize the inverse purity.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al. (2003), and LDA-frames proposed by Materna (2012).",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"The
clusterings with the NN and IB methods are obtained by using the VALEX subcategorization lexicon.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"To harden the clusterings of the IB method and the LDA-frames, the class with the highest probability is selected for each verb.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This hardening process is exactly the same as Korhonen et al. (2003).,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"Note that our results of the NN and IB methods are different from those reported in their paper since the data source is different.8
Table 3 lists accuracies of baseline methods and our methods.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
Our proposed method using the web corpus achieved comparable performance with the baseline methods on the predominant class evaluation and outperformed them on the multiple class evaluation.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"More sophisticated methods for predominant class induction, such as the method of Sun and Korhonen (2009) using selectional preferences, could produce better single-class outputs, but have difficulty in producing polysemy-aware verb classes.
",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"From the result, we can see that the induced verb classes based on slot-only features did not achieve a higher F1 than those based on slot-word pair features in many cases.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
This result is different from that of multi-class evaluations in Section 4.3.,4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We speculate that slot distributions are not so different among verbs when all uses of a verb are merged into one frame, and thus their discrimination power is lower than that in the intermediate construction of semantic frames.",4.4 Type-level Single-class Evaluations against Predominant/Multiple Classes,[0],[0]
"We conduct token-level multi-class evaluations using 119 verbs, which appear 100 or more times in sections 02-21 of the SemLink WSJ corpus.",4.5 Token-level Multi-class Evaluations,[0],[0]
"These 119 verbs cover 102 VerbNet classes, and 48 of them are polysemous in the sense of being in more than one VerbNet class.",4.5 Token-level Multi-class Evaluations,[0],[0]
Each instance of these 119 verbs in this corpus belongs to one of 102 VerbNet classes.,4.5 Token-level Multi-class Evaluations,[0],[0]
We first add these instances to the instances from a raw corpus and apply the twostep clustering to these merged instances.,4.5 Token-level Multi-class Evaluations,[0],[0]
"Then, we compare the induced verb classes of the SemLink instances with their gold-standard VerbNet classes.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We report the values of modified purity (mPU), inverse purity (iPU) and their harmonic mean (F1).",4.5 Token-level Multi-class Evaluations,[0],[0]
"It is not necessary to normalize these metrics because the clustering of these instances is hard.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"8Korhonen et al. (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.
",4.5 Token-level Multi-class Evaluations,[0],[0]
"For clustering features, we compare two feature combinations: “S-S” and “SW-S,” which achieved high performance in the type-level multiclass evaluations (Section 4.3).",4.5 Token-level Multi-class Evaluations,[0],[0]
The results of these methods are obtained by averaging five runs.,4.5 Token-level Multi-class Evaluations,[0],[0]
"For a baseline, we use verb-specific semantic frames without clustering across verbs (“S-NIL” and “SW-NIL”), where these frames are considered to be verb classes but not shared across verbs.",4.5 Token-level Multi-class Evaluations,[0],[0]
Table 4 lists accuracies of these methods for the two corpora.,4.5 Token-level Multi-class Evaluations,[0],[0]
"We can see that “SW-S” achieved a higher F1 than “S-S” and the baselines without verb class induction (“S-NIL” and “SW-NIL”).
",4.5 Token-level Multi-class Evaluations,[0],[0]
Modi et al. (2012) induced semantic frames across verbs using the monosemous assumption and reported an F1 of 44.7% (77.9% PU and 31.4% iPU) for the assignment of FrameNet frames to the FrameNet corpus.,4.5 Token-level Multi-class Evaluations,[0],[0]
We also conducted the above evaluation against FrameNet frames for 75 verbs.9,4.5 Token-level Multi-class Evaluations,[0],[0]
"We achieved an F1 of 62.79% (66.97% mPU and 59.09% iPU) for “web/SW-S,” and an F1 of 60.06% (65.58% mPU and 55.39% iPU) for “Gigaword/SW-S.” It is difficult to directly compare these results with Modi et al. (2012), but our induced verb classes seem to have higher F1 accuracy.",4.5 Token-level Multi-class Evaluations,[0],[0]
"We finally induce verb classes from the semantic frames of 1,667 verbs, which appear at least once in sections 02-21 of the WSJ corpus.",4.6 Full Experiments and Discussions,[0],[0]
"Based on the best results in the above evaluations, we induced semantic frames using slot-word pair features, and then induced verb classes using slotonly features.",4.6 Full Experiments and Discussions,[0],[0]
"We ended with 38,481 semantic frames and 699 verb classes from the Gigaword
9Since FrameNet frames are not assigned to all verbs of SemLink, the number of verbs is different from the evaluations against VerbNet classes.
corpus, and 61,903 semantic frames and 840 verb classes from the web corpus.",4.6 Full Experiments and Discussions,[0],[0]
"It took two days to induce verb classes from the Gigaword corpus and three days from the web corpus.
",4.6 Full Experiments and Discussions,[0],[0]
Examples of verb classes and semantic frames induced from the web corpus are shown in Table 5 and Table 6.,4.6 Full Experiments and Discussions,[0],[0]
"While there are many classes with consistent meanings, such as “Class 4” and “Class 16,” some classes have mixed meanings.",4.6 Full Experiments and Discussions,[0],[0]
"For instance, “Class 2” consists of the semantic frames “need:2” and “say:2.”",4.6 Full Experiments and Discussions,[0],[0]
"These frames were merged due to the high syntactic similarity of constituting slot distributions, which are comprised of a subject and a sentential complement.",4.6 Full Experiments and Discussions,[0],[0]
"To improve the quality of verb classes, it is necessary to develop a clustering model that can consider syntactic and lexical similarity in a balanced way.",4.6 Full Experiments and Discussions,[0],[0]
We presented a step-wise unsupervised method for inducing verb classes from instances in gigaword corpora.,5 Conclusion,[0],[0]
This method first clusters predicateargument structures to induce verb-specific semantic frames and then clusters these semantic frames across verbs to induce verb classes.,5 Conclusion,[0],[0]
"Both clustering steps are performed with exactly the same method, which is based on the Chinese Restaurant Process.",5 Conclusion,[0],[0]
"The resulting semantic frames and verb classes are open to the public and also can be searched via our web interface.10
10http://nlp.ist.i.kyoto-u.ac.jp/member/kawahara/cf/crp.en/
From the results, we can see that the combination of the slot-word pair features for clustering verb-specific frames and the slot-only features for clustering across verbs is the most effective and outperforms the baselines by approximately 10 points.",5 Conclusion,[0],[0]
"This indicates that slot distributions are more effective than lexical information in slotword pairs for the induction of verb classes, when Levin-style classes are used for evaluation.",5 Conclusion,[0],[0]
"This is consistent with Levin’s principle of organizing verb classes according to the syntactic behavior of verbs.
",5 Conclusion,[0],[0]
"As applications of the resulting semantic frames and verb classes, we plan to integrate them into syntactic parsing, semantic role labeling and verb sense disambiguation.",5 Conclusion,[0],[0]
"For instance, Kawahara and Kurohashi (2006) improved accuracy of dependency parsing based on Japanese semantic frames automatically induced from a raw corpus.",5 Conclusion,[0],[0]
"It is also valuable and promising to apply the induced verb classes to NLP applications as used in metaphor identification (Shutova et al., 2010) and argumentative zoning (Guo et al., 2011).",5 Conclusion,[0],[0]
This work was supported by Kyoto University John Mung Program and JST CREST.,Acknowledgments,[0],[0]
"We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing.",Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgments,[0],[0]
We present an unsupervised method for inducing verb classes from verb uses in gigaword corpora.,abstractText,[0],[0]
Our method consists of two clustering steps: verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames.,abstractText,[0],[0]
"By taking this step-wise approach, we can not only generate verb classes based on a massive amount of verb uses in a scalable manner, but also deal with verb polysemy, which is bypassed by most of the previous studies on verb clustering.",abstractText,[0],[0]
"In our experiments, we acquire semantic frames and verb classes from two giga-word corpora, the larger comprising 20 billion words.",abstractText,[0],[0]
The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data.,abstractText,[0],[0]
A Step-wise Usage-based Method for Inducing Polysemy-aware Verb Classes,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1243",text,[0],[0]
Neural architectures have taken the field of machine translation by storm and are in the process of replacing phrase-based systems.,1 Introduction,[0],[0]
"Based on the encoder-decoder framework (Sutskever et al., 2014) increasingly complex neural systems are being developed at the moment.",1 Introduction,[0],[0]
"These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions (Gehring et al., 2017) or stacked self-attention layers (Vaswani et al., 2017).",1 Introduction,[0],[0]
"These architectural changes have led to great performance improvements over classical RNN-based neural translation systems (Bahdanau et al., 2014).
∗Code and a workflow that reproduces the experiments are available at https://github.com/philschulz/ stochastic-decoder.
†Work done prior to joining Amazon.
",1 Introduction,[0],[0]
"Surprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures.",1 Introduction,[0],[0]
A notable exception is the work of Zhang et al. (2016) who introduce a sentence-level latent Gaussian variable.,1 Introduction,[0],[0]
"In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of Bahdanau et al. (2014).",1 Introduction,[0],[0]
"Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.).",1 Introduction,[0],[0]
"Neural machine translation (NMT) models are incapable of capturing this variation, however.",1 Introduction,[0],[0]
"This is because their likelihood function incorporates the statistical assumption that there is one (and only one) output1 for a given source sentence, i.e.,
P (yn1 |xm1 ) = n∏
i=1
P (yi|xm1 , y<i) .",1 Introduction,[0],[0]
"(1)
Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data.",1 Introduction,[0],[0]
The noise sources are modelled as Gaussian random variables.,1 Introduction,[0],[0]
"The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data.
",1 Introduction,[0],[0]
• A thorough discussions of issues encountered when training this model.,1 Introduction,[0],[0]
"In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically.
",1 Introduction,[0],[0]
1Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence.,1 Introduction,[0],[0]
"The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.
",1 Introduction,[0],[0]
• An empirical demonstration of the improvements achievable with the proposed model.,1 Introduction,[0],[0]
The NMT system upon which we base our experiments is based on the work of Bahdanau et al. (2014).,2 Neural Machine Translation,[0],[0]
The likelihood of the model is given in Equation (1).,2 Neural Machine Translation,[0],[0]
We briefly describe its architecture.,2 Neural Machine Translation,[0],[0]
"Let xm1 = (x1, . . .",2 Neural Machine Translation,[0],[0]
", xm) be the source sentence and yn1 the target sentence.",2 Neural Machine Translation,[0],[0]
Let RNN (·) be any function computed by a recurrent neural network (we use a bi-LSTM for the encoder and an LSTM for the decoder).,2 Neural Machine Translation,[0],[0]
We call the decoder state at the ith target position ti; 1 ≤ i ≤,2 Neural Machine Translation,[0],[0]
n.,2 Neural Machine Translation,[0],[0]
The computation performed by the baseline system is summarised below.,2 Neural Machine Translation,[0],[0]
"[
h1, . . .",2 Neural Machine Translation,[0],[0]
", hm ] = RNN (xm1 ) (2a)
t̃i = RNN (ti−1, yi−1) (2b)
eij = v ⊤ a tanh ( Wa[t̃i, hj ] ⊤ + ba ) (2c) αij = exp (eij)∑m j=1 exp (eij) (2d)
ci = m∑ j=1 αijhj (2e) ti = Wt[t̃i, ci] ⊤ + bt (2f)
ϕi = softmax(Woti + bo) (2g)
",2 Neural Machine Translation,[0],[0]
"The parameters {Wa,Wt,Wo, ba, bt, bo, va} ⊆ θ are learned during training.",2 Neural Machine Translation,[0],[0]
The model is trained usingmaximum likelihood estimation.,2 Neural Machine Translation,[0],[0]
Thismeans that we employ a cross-entropy loss whose input is the probability vector returned by the softmax.,2 Neural Machine Translation,[0],[0]
This section introduces our stochastic decoder model for capturing word-level variation in translation data.,3 Stochastic Decoder,[0],[0]
Imagine an idealised translator whose translations are always perfectly accurate and fluent.,3.1 Motivation,[0],[0]
"If an MT systemwas providedwith training data from such a translator, it would still encounter variation in that data.",3.1 Motivation,[0],[0]
"After all, there are several perfectly accurate and fluent translations for each source sentence.",3.1 Motivation,[0],[0]
"These can be highly different in both their lexical as well as their syntactic realisations.
",3.1 Motivation,[0],[0]
"In practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors.",3.1 Motivation,[0],[0]
"Even within a single translator variation may occur due to level of stress, tiredness or status of health.",3.1 Motivation,[0],[0]
"That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing onemachinegenerated translation against several human translations (see e.g. Papineni et al., 2002).
",3.1 Motivation,[0],[0]
"Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable.",3.1 Motivation,[0],[0]
"Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations.",3.1 Motivation,[0],[0]
"A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions.",3.1 Motivation,[0],[0]
"Multimodal modelling of these variations is thus called for—and our results confirm this intuition.
",3.1 Motivation,[0],[0]
An example of variation comes from free word order and agreement phenomena in morphologically rich languages.,3.1 Motivation,[0],[0]
An English sentence with rigid word order may be translated into several orderings in German.,3.1 Motivation,[0],[0]
"However, all orderings need to respect the agreement relationship between the main verb and the subject (indicated by underlining) as well as the dative case of the direct object (dashes) and the accusative of the indirect object (dots).",3.1 Motivation,[0],[0]
"The agreement requirements are fixed and independent of word order.
",3.1 Motivation,[0],[0]
1.,3.1 Motivation,[0],[0]
I can’t imagine you naked.,3.1 Motivation,[0],[0]
(a) Ich kann mir . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(b) Ich kann . . . .,3.1 Motivation,[0],[0]
.dich,3.1 Motivation,[0],[0]
mir nicht nackt vorstellen.,3.1 Motivation,[0],[0]
(c) . . . .,3.1 Motivation,[0],[0]
.Dich,3.1 Motivation,[0],[0]
"kann ichmir nicht nackt vorstellen.
",3.1 Motivation,[0],[0]
"Stochastically encoding the word order variation allows the model to learn the same agreement phenomenon from different translation variants as it does not need to encode the word order and agreement relationships jointly in the decoder state.
",3.1 Motivation,[0],[0]
"Further examples of VP and NP variation from an actual translation corpus are shown in Figure 1.
",3.1 Motivation,[0],[0]
We aim to address these word-level variation phenomena with a stochastic decoder model.,3.1 Motivation,[0],[0]
The model contains a latent Gaussian variable for each target position.,3.2 Model formulation,[0],[0]
This variable depends on the previous latent states and the decoder state.,3.2 Model formulation,[0],[0]
"Through the use of recurrent networks, the conditioning context does not need to be restricted and the likelihood factorises exactly.
",3.2 Model formulation,[0],[0]
"P (yn1 |xm1 ) = ∫
dzn0 p(z0|xm1 )× n∏
i=1
p(zi|z<i, y<i, xm1 )P (yi|zi1, y<i, xm1 ) (3)
As can be seen from Equation (3), the model also contains a 0th latent variable that is meant to initialise the chain of latent variables based solely on the source sentence.",3.2 Model formulation,[0],[0]
Contrast this with the model of Zhang et al. (2016) which uses only that 0th variable.,3.2 Model formulation,[0],[0]
A graphical representation of the stochastic decoder model is given in Figure 2a.,3.2 Model formulation,[0],[0]
"Its generative story is as follows
Z0|xm1 ∼ N (µ0, σ20) (4a) Zi|z",3.2 Model formulation,[0],[0]
<,3.2 Model formulation,[0],[0]
"i, y<i, xm1 ∼ N (µi, σ2i ) (4b) Yi|zi0, y<i, xm1 ∼ Cat(ϕi) (4c)
where i = 1, . . .",3.2 Model formulation,[0],[0]
", n and both the Gaussian and the Categorical parameters are predicted by neural network architectures whose inputs vary per time step.",3.2 Model formulation,[0],[0]
This probabilistic formulation can be implemented with a multitude of different architectures.,3.2 Model formulation,[0],[0]
We present ours in the next section.,3.2 Model formulation,[0],[0]
"Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs).",3.3 Neural Architecture,[0],[0]
"We use a reparametrisation of the Gaussian variables (Kingma and Welling, 2014; Rezende et al., 2014; Titsias and Lázaro-Gredilla, 2014) to enable backpropagation inside a stochastic computation graph (Schulman et al., 2015).",3.3 Neural Architecture,[0],[0]
In order to sample ddimensional Gaussian variable z ∈,3.3 Neural Architecture,[0],[0]
Rd,3.3 Neural Architecture,[0],[0]
"with mean µ and variance σ2, we first sample from a standard Gaussian distribution and then transform the sample,
z = µ+ σ ⊙ ϵ ϵ",3.3 Neural Architecture,[0],[0]
"∼ N (0, I) .",3.3 Neural Architecture,[0],[0]
"(5)
Here µ, σ ∈ Rd and ⊙ denotes element-wise multiplication (also known as Hadamard product).",3.3 Neural Architecture,[0],[0]
See the supplement for details on the Gaussian reparametrisation.,3.3 Neural Architecture,[0],[0]
We use neural networks with one hidden layer with a tanh activation to compute the mean and standard deviation of each Gaussian distribution.,3.3 Neural Architecture,[0],[0]
A softplus transformation is applied to the output of the standard deviation’s network to ensure positivity.,3.3 Neural Architecture,[0],[0]
Let us denote the functions that these networks compute by f .,3.3 Neural Architecture,[0],[0]
"For the initial latent state z0 we compute the mean and standard deviation as
µ0 = fµ0 (hm) σ0",3.3 Neural Architecture,[0],[0]
= fσ0 (hm) .,3.3 Neural Architecture,[0],[0]
"(6)
The parameters of all other latent distributions are computed by functions fµ and fσ whose inputs vary per target position.
",3.3 Neural Architecture,[0],[0]
"µi = fµ (ti−1, zi−1) σi",3.3 Neural Architecture,[0],[0]
"= fσ (ti−1, zi−1) (7)
Using these values, each latent variable is sampled according to Equation (5).",3.3 Neural Architecture,[0],[0]
The sampled latent variables are then used to modify the update of the decoder hidden state (Equation (2b)),3.3 Neural Architecture,[0],[0]
"as follows:
t̃i = RNN (ti−1, yi−1, zi) (8)
The remaining computations stay unchanged.",3.3 Neural Architecture,[0],[0]
Notice that the latent values are used directly in updating the decoder state.,3.3 Neural Architecture,[0],[0]
This makes the decoder state a function of a random variable and thus the decoder state is itself random.,3.3 Neural Architecture,[0],[0]
"Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic.",3.3 Neural Architecture,[0],[0]
"We use variational inference (see e.g. Blei et al., 2017) to train the model.",4 Inference and Training,[0],[0]
"In variational inference, we employ a variational distribution q(z) that approximates the true posterior p(z|x) over the latent variables.",4 Inference and Training,[0],[0]
The distribution q(z) has its own set of parameters λ that is disjoint from the set of model parameters θ.,4 Inference and Training,[0],[0]
It is used to maximise the evidence lower bound (ELBO) which is a lower bound on the marginal likelihood p(x).,4 Inference and Training,[0],[0]
The ELBO is maximised with respect to both the model parameters θ and the variational parameters λ.,4 Inference and Training,[0],[0]
"Most NLP models that use DGMs only use one latent variable (e.g. Bowman et al., 2016).",4 Inference and Training,[0],[0]
"Models
that use several variables usually employ a mean field approximation under which all latent variables are independent.",4 Inference and Training,[0],[0]
"This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017).",4 Inference and Training,[0],[0]
"For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables,
q(zn0 )",4 Inference and Training,[0],[0]
= q(z0) n∏ i=1,4 Inference and Training,[0],[0]
q(zi|z<i) .,4 Inference and Training,[0],[0]
"(9)
Our stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another.",4 Inference and Training,[0],[0]
"The ELBO thus consists of nested positional ELBOs,
ELBO0 + Eq(z0)[ELBO1 +Eq(z1)[ELBO2 + . . .",4 Inference and Training,[0],[0]
"]] ,
(10)
where for a given target position i the ELBO is
ELBOi = Eq(zi)",4 Inference and Training,[0],[0]
"[log p(yi|x m 1 , y<i, z<i, zi)]
−KL (q(zi) ||",4 Inference and Training,[0],[0]
"p(zi|xm1 , y<i, z<i)) .",4 Inference and Training,[0],[0]
"(11)
",4 Inference and Training,[0],[0]
The first term is often called reconstruction or likelihood term whereas the second term is called the KL term.,4 Inference and Training,[0],[0]
"Since the KL term is a function of two Gaussian distributions, and the Gaussian is an exponential family, we can compute it analytically (Michalowicz et al., 2014), without the need for sampling.",4 Inference and Training,[0],[0]
This is very similar to the hierarchical latent variable model of Rezende et al. (2014).,4 Inference and Training,[0],[0]
"Following common practice in DGM research, we employ a neural network to compute the variational distributions.",4 Inference and Training,[0],[0]
"To discriminate it from the
generative model, we call this neural net the inference model.",4 Inference and Training,[0],[0]
At training time both the source and target sentence are observed.,4 Inference and Training,[0],[0]
We exploit this by endowing our inference model with a “lookahead” mechanism.,4 Inference and Training,[0],[0]
"Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be processed by the generative decoder.",4 Inference and Training,[0],[0]
This allows the latent distribution to not only encode information about the currently modelled word but also about the target words that follow it.,4 Inference and Training,[0],[0]
The conditioning of the inference network is illustrated graphically in Figure 2b.,4 Inference and Training,[0],[0]
The inference network produces additional representations of the target sentence.,4 Inference and Training,[0],[0]
"One representation encodes the target sentence bidirectionally (12a), in analogy to the source sentence encoding.",4 Inference and Training,[0],[0]
The second representation is built by encoding the target sentence in reverse (12b).,4 Inference and Training,[0],[0]
This reverse encoding can be used to provide information about future context to the decoder.,4 Inference and Training,[0],[0]
"We use the symbols b and r for the bidirectional and reverse target encodings, respectively.",4 Inference and Training,[0],[0]
"In our experiments, we again use LSTMs to compute these encodings.",4 Inference and Training,[0],[0]
"[
b1, . . .",4 Inference and Training,[0],[0]
", bn ] = RNN (yn1 ) (12a)[
r1, . . .",4 Inference and Training,[0],[0]
", rn ] = RNN (yn1 ) (12b)
",4 Inference and Training,[0],[0]
"In analogy to the generativemodel (Section 3.3), the inference network uses single hidden layer networks to compute the mean and standard deviations of the latent variable distributions.",4 Inference and Training,[0],[0]
"We denote these functions g and again employ different functions for the initial latent state and all other latent states.
µ0 = gµ0 (hm, bn) (13a) σ0 = gσ0",4 Inference and Training,[0],[0]
"(hm, bn) (13b) µi = gµ (ti−1, zi−1, ri, yi) (13c)",4 Inference and Training,[0],[0]
σi,4 Inference and Training,[0],[0]
"= gσ (ti−1, zi−1, ri, yi) (13d)
",4 Inference and Training,[0],[0]
"As before, we use Equation (5) to sample from the variational distribution.",4 Inference and Training,[0],[0]
"During training, all samples are obtained from the inference network.",4 Inference and Training,[0],[0]
Only at test time do we sample from the generator.,4 Inference and Training,[0],[0]
"Notice that since the inference network conditions on representations produced by the generator network, a naïve application of backpropagation would update parts of the generator network with gradients computed for
the inference network.",4 Inference and Training,[0],[0]
We prevent this by blocking gradient flow from the inference net into the generator.,4 Inference and Training,[0],[0]
The training procedure as outlined above does not work well empirically.,4.1 Analysis of the Training Procedure,[0],[0]
This is because our model uses a strong generator.,4.1 Analysis of the Training Procedure,[0],[0]
By this we mean that the generation model (that is the baseline NMT model) is a very good density model in and by itself and does not need to rely on latent information to achieve acceptable likelihood values during training.,4.1 Analysis of the Training Procedure,[0],[0]
"DGMs with strong generators have a tendency to not make use of latent information (Bowman et al., 2016).",4.1 Analysis of the Training Procedure,[0],[0]
"This problem went initially unnoticed because early DGMs (Kingma and Welling, 2014; Rezende et al., 2014) used weak generators2, i.e. models that made very strong independence assumptions and were not able to capture contextual information without making use of the information encoded by the latent variable.",4.1 Analysis of the Training Procedure,[0],[0]
WhyDGMswould ignore the latent information can be understood by considering the KL-term of the ELBO.,4.1 Analysis of the Training Procedure,[0],[0]
"In order for the latent variable to be informative about the observed data, we need them to have high mutual information I(Z;Y ).
I(Z;Y ) =",4.1 Analysis of the Training Procedure,[0],[0]
"Ep(z,y) [ log p(Z, Y )
p(Z)p(Y )
]",4.1 Analysis of the Training Procedure,[0],[0]
"(14)
Observe that we can rewrite the mutual information as an expected KL divergence by applying the definition of conditional probability.
I(Z;Y ) = Ep(y)",4.1 Analysis of the Training Procedure,[0],[0]
[KL (p(Z|Y ) ||,4.1 Analysis of the Training Procedure,[0],[0]
p(Z)),4.1 Analysis of the Training Procedure,[0],[0]
"] (15)
Since we cannot compute the posterior p(z|y) exactly, we approximate it with the variational distribution q(z|y) (the joint is approximated by q(z|y)p(y) where the latter factor is the data distribution).",4.1 Analysis of the Training Procedure,[0],[0]
"To the extent that the variational distribution recovers the true posterior, the mutual information can be computed this way.",4.1 Analysis of the Training Procedure,[0],[0]
"In fact, if we take the learned prior p(z) to be an approximation of themarginal ∫ q(z|y)p(y)dy it can easily be shown that the thus computed KL term is an upper bound on mutual information (Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
"The trouble is that the ELBO (Equation (11)) can be trivially maximised by setting the KL-term to 0 and maximising only the reconstruction term.
2The term weak generator has first been coined by Alemi et al. (2017).
",4.1 Analysis of the Training Procedure,[0],[0]
This is especially likely at the beginning of training when the variational approximation does not yet encode much useful information.,4.1 Analysis of the Training Procedure,[0],[0]
We can only hope to learn a useful variational distribution if a) the variational approximation is allowed to move away from the prior and b) the resulting increase in the reconstruction term is higher than the increase in the KL-term (i.e. the ELBO increases overall).,4.1 Analysis of the Training Procedure,[0],[0]
"Several schemes have been proposed to enable better learning of the variational distribution (Bowman et al., 2016; Kingma et al., 2016; Alemi et al., 2017).",4.1 Analysis of the Training Procedure,[0],[0]
Here we use KL scaling and increase the scale gradually until the original objective is recovered.,4.1 Analysis of the Training Procedure,[0],[0]
"This has the following effect: during the initial learning stage, the KL-term barely contributes to the objective and thus the updates to the variational parameters are driven by the signal from the reconstruction term and hardly restricted by the prior.",4.1 Analysis of the Training Procedure,[0],[0]
Once the scale factor approaches 1 the variational distribution will be highly informative to the generator (assuming sufficiently slow increase of the scale factor).,4.1 Analysis of the Training Procedure,[0],[0]
The KL-term can now be minimised by matching the prior to the variational distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Notice that up to this point, the prior has hardly been updated.",4.1 Analysis of the Training Procedure,[0],[0]
Thus moving the variational approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes.,4.1 Analysis of the Training Procedure,[0],[0]
This is in stark contrast to Bowman et al. (2016) whose prior was a fixed standard normal distribution.,4.1 Analysis of the Training Procedure,[0],[0]
"Although they used KL scaling, the KL term could only be decreased by moving the variational approximation back to the fixed prior.",4.1 Analysis of the Training Procedure,[0],[0]
This problem disappears in our model where priors are learned.,4.1 Analysis of the Training Procedure,[0],[0]
Moving the prior towards the variational approximation has another desirable effect.,4.1 Analysis of the Training Procedure,[0],[0]
The prior can now learn to emulate the variational “lookahead” mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens).,4.1 Analysis of the Training Procedure,[0],[0]
At test time we can thus hope to have learned latent variable distributions that encode information not only about the output at the current position but about future outputs as well.,4.1 Analysis of the Training Procedure,[0],[0]
We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations.,5 Experiments,[0],[0]
"We trained models to
translate from English into Arabic, Czech, French and German.",5 Experiments,[0],[0]
The number of sentences for each language after preprocessing is shown in Table 1.,5 Experiments,[0],[0]
"The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings.",5 Experiments,[0],[0]
"As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4.",5 Experiments,[0],[0]
Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2.,5 Experiments,[0],[0]
"We report baselines with and without dropout (Srivastava et al., 2014).",5 Experiments,[0],[0]
For dropout a retention probability of 0.5 was used.,5 Experiments,[0],[0]
As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT).,5 Experiments,[0],[0]
Our implementation differs from theirs in three aspects.,5 Experiments,[0],[0]
"First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the inference network (Zhang et al. (2016) use the average of all states).",5 Experiments,[0],[0]
"Second, the latent variable is smaller in size than the one used by (Zhang et al., 2016).5",5 Experiments,[0],[0]
This was done to make their model and the stochastic decoder proposed here as similar as possible.,5 Experiments,[0],[0]
"Finally, their implementation was based on groundhog whereas ours builds on Sockeye.",5 Experiments,[0],[0]
Our stochastic decoder model (SDEC) is also built on top of the basic Sockeyemodel.,5 Experiments,[0],[0]
It adds the components described in Sections 3 and 4.,5 Experiments,[0],[0]
Recall that the functions that compute the means and standard deviations are implemented by neural nets with a single hidden layer with tanh activation.,5 Experiments,[0],[0]
The width of that layer is twice the size of the latent variable.,5 Experiments,[0],[0]
In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1).,5 Experiments,[0],[0]
"The scale started from 0 and was increased by 1/20,000 after each mini-batch.",5 Experiments,[0],[0]
"Thus, at iteration t the scale is min(t/20,000, 1).",5 Experiments,[0],[0]
"All models use 1028 units for the LSTM hid3https://github.com/google/sentencepiece 4https://github.com/awslabs/sockeye 5We did, however, find that increasing the latent variable size actually hurt performance in our implementation.
",5 Experiments,[0],[0]
den state (or 512 for each direction in the bidirectional LSTMs) and 256 for the attention mechansim.,5 Experiments,[0],[0]
"Training is done with Adam (Kingma and Ba, 2015).",5 Experiments,[0],[0]
In decoding we use a beam of size 5 and output the most likely word at each position.,5 Experiments,[0],[0]
We deterministically set all latent variables to their mean values during decoding.,5 Experiments,[0],[0]
"Monte Carlo decoding (Gal, 2016) is difficult to apply to our setting as it would require sampling entire translations.
",5 Experiments,[0],[0]
Results We show the BLEU scores for all models that we tested on the IWSLT data set in Table 2.,5 Experiments,[0],[0]
"The stochastic decoder dominates the Sockeye baseline across all 4 languages, and outperforms SENT on most languages.",5 Experiments,[0],[0]
"Except on German, there is a trend towards smaller latent variable sizes being more helpful.",5 Experiments,[0],[0]
This is in line with findings by Chung et al. (2015) and Fraccaro et al. (2016) who also used relatively small latent variables.,5 Experiments,[0],[0]
This observation also implies that our model does not improve simply because it has more parameters than the baseline.,5 Experiments,[0],[0]
That the margin between the SDEC and SENT models is not large was to be expected for two reasons.,5 Experiments,[0],[0]
"First, Chung et al. (2015) and Fraccaro et al. (2016) have shown that stochastic RNNs lead to enormous improvements in modelling continuous sequences but only modest increases in performance for discrete sequences (such as natural language).",5 Experiments,[0],[0]
"Second, translation performance is measured in BLEU score.",5 Experiments,[0],[0]
We observed that SDEC often reached better ELBO values than SENT indicating a better model fit.,5 Experiments,[0],[0]
"How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future research.
",5 Experiments,[0],[0]
"Qualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation.",5 Experiments,[0],[0]
"To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled instead of fixed.",5 Experiments,[0],[0]
"Contrary to the BLEU-based evaluation, beam search was not used in this evaluation in order to avoid interaction between different latent variable samples.",5 Experiments,[0],[0]
See Figure 3 for examples of syntactic and lexical variation.,5 Experiments,[0],[0]
It is important to note that we do not sample from the categorical output distribution.,5 Experiments,[0],[0]
For each target position we pick the most likely word.,5 Experiments,[0],[0]
"A non-stochastic NMT system would always yield the same translation in
this scenario.",5 Experiments,[0],[0]
"Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system.",5 Experiments,[0],[0]
"This supports our initial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address.",5 Experiments,[0],[0]
"Like the model of Bowman et al. (2016), SENT presumably tends to ignore the latent variable.",5 Experiments,[0],[0]
The stochastic decoder is strongly influenced by previous work on stochastic RNNs.,6 Related Work,[0],[0]
The first such proposal was made by Bayer and Osendorfer (2015) who introduced i.i.d.,6 Related Work,[0],[0]
Gaussian latent variables at each output position.,6 Related Work,[0],[0]
"Since their model neglects any sequential dependence of the noise sources, it underperformed on several sequence modeling tasks.",6 Related Work,[0],[0]
Chung et al. (2015) made the latent variables depend on previous information by feeding the previous decoder state into the latent variable sampler.,6 Related Work,[0],[0]
Their inference model did not make use of future elements in the sequence.,6 Related Work,[0],[0]
Using a “look-ahead” mechanism in the inference net was proposed by Fraccaro et al. (2016) who had a separate stochastic and deterministic RNN layer which both influence the output.,6 Related Work,[0],[0]
"Since the stochastic layer in their model depends on the deterministic layer but not vice versa, they could first run the deterministic layer at inference time and then condition the inference net’s encoding of the future on the thus obtained features.",6 Related Work,[0],[0]
"Like us, they used KL scaling during training.",6 Related Work,[0],[0]
"More recently, Goyal et al. (2017) proposed an auxiliary loss that has the inference net predict future feature representations.",6 Related Work,[0],[0]
This approach yields state-of-the-art results but is still in need of a theoretical justification.,6 Related Work,[0],[0]
"Within translation, Zhang et al. (2016) were the first to incorporate Gaussian variables into an NMT model.",6 Related Work,[0],[0]
Their approach only uses one sentence-level latent variable (corresponding to our z0) and can thus not deal with word-level variation directly.,6 Related Work,[0],[0]
"Concurrently to our work, Su et al. (2018) have also proposed a recurrent latent variable model for NMT.",6 Related Work,[0],[0]
Their approach differs from ours in that they do not use a 0th latent variable nor a look-ahead mechanism during inference time.,6 Related Work,[0],[0]
"Furthermore, their underlying recurrent model is a GRU.",6 Related Work,[0],[0]
"In the wider field of NLP, deep generative mod-
els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016).",6 Related Work,[0],[0]
Wehave presented a recurrent decoder formachine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora.,7 Conclusion and Future Work,[0],[0]
Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation.,7 Conclusion and Future Work,[0],[0]
"The proposed model consistently outperforms strong baselines
on several language pairs.",7 Conclusion and Future Work,[0],[0]
"As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future.",7 Conclusion and Future Work,[0],[0]
"Here, we list the three which we believe to be most promising.
",7 Conclusion and Future Work,[0],[0]
• Latent factor models: our model only contains one source of variation per word.,7 Conclusion and Future Work,[0],[0]
"A latent factor model such as DARN (Gregor et al., 2014) would consider several sources simultaneously.",7 Conclusion and Future Work,[0],[0]
This would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena.,7 Conclusion and Future Work,[0],[0]
•,7 Conclusion and Future Work,[0],[0]
"Richer prior and variational distributions: The diagonal Gaussian is likely too simple a
distribution to appropriately model the variation in our data.",7 Conclusion and Future Work,[0],[0]
"Richer distributions computed by normalising flows (Rezende and Mohamed, 2015; Kingma et al., 2016) will likely improve our model.",7 Conclusion and Future Work,[0],[0]
"• Extension to other architectures: Introducing latent variables into non-autoregressive translation models such as the transformer (Vaswani et al., 2017) should increase their translation ability further.",7 Conclusion and Future Work,[0],[0]
Philip Schulz and Wilker Aziz were supported by the Dutch Organisation for Scientific Research (NWO) VICI Grant nr. 277-89-002.,8 Acknowledgements,[0],[0]
Trevor Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,8 Acknowledgements,[0],[0]
"The process of translation is ambiguous, in that there are typically many valid translations for a given sentence.",abstractText,[0],[0]
"This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process.",abstractText,[0],[0]
"To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora.",abstractText,[0],[0]
We provide an indepth analysis of the pitfalls encountered in variational inference for training deep generative models.,abstractText,[0],[0]
Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.,abstractText,[0],[0]
A Stochastic Decoder for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4810–4815 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4810",text,[0],[0]
"Community question-answer fora are great resources, collecting answers to frequently and lessfrequently asked questions on specific topics, but these are often not moderated and contain many irrelevant answers.",1 Introduction,[0],[0]
"Community Question Answering (CQA), cast as a question relevancy ranking problem, was the topic of two shared tasks at SemEval 2016-17.",1 Introduction,[0],[0]
"This is a non-trivial retrieval task, typically evaluated using mean average precision (MAP).",1 Introduction,[0],[0]
"We present a strong baseline for this task, on par with or surpassing state-of-the-art systems.
",1 Introduction,[0],[0]
"The English subtasks of the SemEval CQA (Nakov et al., 2015, 2017) consist of QuestionQuestion Similarity, Question-Comment Similarity, and Question-External Comment Similarity.",1 Introduction,[0],[0]
"In this study, we focus on the core subtask of Question-Question similarity, defined as follows:",1 Introduction,[0],[0]
"Given a question, rank other relevant questions by their relevancy to that question.",1 Introduction,[0],[0]
This proved to be a difficult task in both SemEval-16 and SemEval17 as it is the one with the least amount of data available.,1 Introduction,[0],[0]
"The baseline was the ranking retrieved
by performing a Google search, which proved to be a strong baseline beating a large portion of the systems submitted.
",1 Introduction,[0],[0]
Contribution Our baseline is a simple multitask feed-forward neural network taking distance measures between pairs of questions as input.,1 Introduction,[0],[0]
We use a question-answer dataset as auxiliary task; but we also experiment with datasets for pairwise classification tasks such as natural language inference and fake news detection.,1 Introduction,[0],[0]
"This simple, easy-totrain model is on par or better than state-of-theart systems for question relevancy ranking.",1 Introduction,[0],[0]
We also show that this simple model outperforms a more complex model based on recurrent neural networks.,1 Introduction,[0],[0]
We present a simple baseline model for question relevancy ranking.1,2 Our Model,[0],[0]
It is a deep feed-forward network with a hidden layer that is shared with an auxiliary task model.,2 Our Model,[0],[0]
The input to the network is extremely simple and consists of five distance measures of the input question-question pair.,2 Our Model,[0],[0]
"§2.1 discusses these distance measures, and how they relate.",2 Our Model,[0],[0]
§2.2 introduces the multi-task learning architecture that we propose.,2 Our Model,[0],[0]
"We use four similarity metrics and three sentence representations (averaged word embeddings, binary unigram vectors, and trigram vectors).",2.1 Features,[0],[0]
"The cosine distance between the sentence representations of query x and query y is∑
i xiyi√∑",2.1 Features,[0],[0]
"i x 2 + √∑ i y 2
1Code available at http://anavaleriagonzalez/FAQ rank.
",2.1 Features,[0],[0]
The Manhattan distance is∑,2.1 Features,[0],[0]
i |xi,2.1 Features,[0],[0]
"− yi|
The Bhattacharya distance is
− ln( ∑ i √ xiyi)
and is a measure of divergence, and the Euclidean distance is √∑
i
(xi − yi)2
Note that the squared Euclidean distance is proportional to cosine distance and Manhattan distance.",2.1 Features,[0],[0]
"The Bhattacharya and Jaccard metrics, on the other hand, are sensitive to the number of types in the input (the `1 norm of the vector encodings).",2.1 Features,[0],[0]
"So, for example, only the cosine, Euclidean, and Manhattan distances will be the same for
x = 〈1, 1, 0, 0, 1, 0, 1, 1, 0, 1〉,y = 〈0, 0, 1, 0, 1, 0, 0, 0, 1, 1〉
and
x = 〈0, 0, 0, 0, 0, 1, 0, 0, 1, 1〉,y = 〈1, 1, 1, 1, 0, 0, 0, 0, 0, 1〉
The Jaccard index is the only metric that can only be applied to two of our representations, unigrams and n-grams: It is defined over mdimensional binary (indicator) vectors and therefore not applicable to averaged embeddings.",2.1 Features,[0],[0]
"It is defined as
x · y m
We represent each query pair by these 14 numerical features.",2.1 Features,[0],[0]
"Our architecture is a simple feed-forward, multitask learning (MTL) architecture.",2.2 MTL Architecture,[0],[0]
Our architecture is presented in Figure 1 and is a Multi-Layer Perceptron (MLP) that takes a pair of sequences as input.,2.2 MTL Architecture,[0],[0]
The sequences can be sampled from the main task or the auxiliary task.,2.2 MTL Architecture,[0],[0]
"The MLP has one shared hidden layer, a task-specific hidden layer and, finally, a task-specific classification layer for each output.",2.2 MTL Architecture,[0],[0]
"The hyper-parameters, after doing grid search, optimizing performance on the validation data, are given in Figure 2.",2.2 MTL Architecture,[0],[0]
"We compare our MLP ranker to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) model.",2.3 LSTM baseline,[0],[0]
"It takes two sequences inputs: sequence 1 and sequence 2, and a stack of three bidirectional LSTM layers, which encode sequence 1 and sequence 2, respectively.",2.3 LSTM baseline,[0],[0]
"The outputs are then concatenated, to enable representing the differences between the two sequences.",2.3 LSTM baseline,[0],[0]
"Instead of relying only on this presentation (Bowman et al., 2015; Augenstein et al., 2016), we also concatenate our distance features and feed everything into our MLP ranker described above.",2.3 LSTM baseline,[0],[0]
"For our experiments, we use data from SemEval shared tasks, but we also take advantage of potential synergies with other existing datasets for classification of sentence pairs.",3 Datasets,[0],[0]
Below we present the datasets used for our main and auxiliary tasks.,3 Datasets,[0],[0]
"We provide some summary statistics for each dataset in Table 3.
SemEval 2016 and 2017 As our main dataset we use the queries from SemEval’s subtask B which consists of an original query and 10 possibly related queries.",3 Datasets,[0],[0]
"As an auxiliary task, we use the data from subtask A, which is a questionrelated comment ranking task.
",3 Datasets,[0],[0]
"Natural Language Inference Natural Language Inference (NLI), consists in predicting ENTAILMENT, CONTRADICTION or NEUTRAL, given a hypothesis and a premise.",3 Datasets,[0],[0]
"We use the MNLI dataset as opposed to the SNLI data (Bowman et al., 2015; Nangia et al., 2017), since it contains different genres.",3 Datasets,[0],[0]
"Our model is not built to be a strong NLI system; we use the similarity between premise and hypothesis as a weak signal to improve the generalization on our main task.
",3 Datasets,[0],[0]
Fake News Challenge The Fake News Challenge2 (FNC) was introduced to combat misleading and false information online.,3 Datasets,[0],[0]
"This task has been used before in a multi-task setting as a way to utilize general information about pairwise relations (Augenstein et al., 2018).",3 Datasets,[0],[0]
"Formally, the FNC task consists in, given a headline and the body of
2http://www.fakenewschallenge.org/
text which can be from the same news article or not, classify the stance of the body of text relative to what is claimed in the headline.",3 Datasets,[0],[0]
"There are four labels:
• AGREES:",3 Datasets,[0],[0]
The body of the article is in agreement with the headline • DISAGREES:,3 Datasets,[0],[0]
"The body of the article is in dis-
agreement with the headline • DISCUSSES:",3 Datasets,[0],[0]
"The body of the article does not
take a position • UNRELATED: the body of the article dis-
cusses a different topic
We include fake news detection as a weak auxiliary signal that can lead to better generalization of our question-question ranking model.",3 Datasets,[0],[0]
"We evaluate our performance on the main task of question relevancy ranking using the official
SemEval-2017 Task 3 evaluation scripts (Nakov et al., 2017).",3.1 Evaluation,[0],[0]
"The scripts provide a variety of metrics; however, in accordance with the shared task, we report Mean Average Precision (MAP) (the official metric for the SemEval 2016 and 2017 shared tasks); Mean Reciprocal Rank (MRR), which has being thoroughly used for IR and QA; Average Recall; and, finally, the accuracy of predicting relevant documents.",3.1 Evaluation,[0],[0]
The results from our experiments are shown in Table 1.,4 Results,[0],[0]
"We present the official metric from the SemEval task, as well as other common metrics.",4 Results,[0],[0]
"For the SemEval-16 data, our multitask MLP architecture with a question-answer auxiliary task performed best on all metrics, except accuracy, where the multi-task MLP using all auxiliary tasks performed best.",4 Results,[0],[0]
We outperform the winning systems of both the SemEval 2016 and 2017 campaigns.,4 Results,[0],[0]
"In addition, our improvements from single-task to multi-task are significant (p < 0.01).",4 Results,[0],[0]
We also outperform the official IR baseline used in the SemEval 2016 and 2017 shared tasks.,4 Results,[0],[0]
We discuss the STL-LSTM-SIM results in §5.,4 Results,[0],[0]
"Furthermore, in Table 2, we show the performance of our models when training on feature combinations, while in Table 3, we present an ablation test where we remove one feature at a time.
",4 Results,[0],[0]
"Learning curve In Figure 4, we also present our learning curves for the development set when incrementally increasing the training set size.",4 Results,[0],[0]
"We observe that when using an auxiliary task, the learning is more stable across training set size.",4 Results,[0],[0]
"For the SemEval shared tasks on CQA, several authors used complex recurrent and convolutional neural network architectures (Severyn and Moschitti, 2015; Barrón-Cedeno et al., 2016).",5 Discussion,[0],[0]
"For example, Barrón-Cedeno et al. used a convolutional neural network in combination with feature vectors representing lexical, syntactic, and semantic similarity as well as tree kernels.",5 Discussion,[0],[0]
Their performance was slightly lower than the best system (SemEval-Best for 2016 in Table 1).,5 Discussion,[0],[0]
"The best system used lexical and semantic similarity measures in combination with a ranking model based on support vector machines (SVMs) (Filice et al., 2016; Franco-Salvador et al., 2016).",5 Discussion,[0],[0]
Both systems are harder to implement and train than the model we propose here.,5 Discussion,[0],[0]
"For SemEval-17, FrancoSalvador et al. (2016), the winning team used
distributed representations of words, knowledge graphs and frames from FrameNet (Baker et al., 1998) as some of their features, and used SVMs for ranking.
",5 Discussion,[0],[0]
"For a more direct comparison, we also train a more expressive model than the simple MTLbased model we propose.",5 Discussion,[0],[0]
"This architecture is based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997).",5 Discussion,[0],[0]
"For this model, we input sequences of embedded words (using pre-trained word embeddings) from each query into independent BiLSTM blocks and output a vector representation for each query.",5 Discussion,[0],[0]
We then concatenate the vector representations with the similarity features from our MTL model and feed it into a dense layer and a classification layer.,5 Discussion,[0],[0]
"This way we can evaluate the usefulness of the flexible, expressive LSTM network directly (as our MTL model becomes an ablation instance of the full, more complex architecture).",5 Discussion,[0],[0]
We use the same dropout regularization and SGD values as for the MLP.,5 Discussion,[0],[0]
"Tuning all parameters on the development data, we do not manage to outperform our proposed model, however.",5 Discussion,[0],[0]
See lines MTL-LSTM-SIM in Table 1 for results.,5 Discussion,[0],[0]
"We show that simple feature engineering, combined with an auxiliary task and a simple feedfor-
ward neural architecture is appropriate for a small dataset and manages to beat the baseline and the best performing systems for the Semeval task of question relevancy ranking.",6 Conclusion,[0],[0]
We observe that introducing pairwise classification tasks leads to significant improvements in performance and a more stable model.,6 Conclusion,[0],[0]
"Overall, our simple model introduces a new strong baseline which is particularly useful when there is a lack of labeled data.",6 Conclusion,[0],[0]
The first author of this paper is funded by a BotXO PhD Award;3 the last author by an ERC Starting Grant.,Acknowledgments,[0],[0]
We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.,Acknowledgments,[0],[0]
The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks – a task that amounts to question relevancy ranking – involve complex pipelines and manual feature engineering.,abstractText,[0],[0]
"Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google’s search engine.",abstractText,[0],[0]
We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair.,abstractText,[0],[0]
"This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.",abstractText,[0],[0]
A strong baseline for question relevancy ranking,title,[0],[0]
"Understanding temporal information described in natural language text is a key component of natural language understanding (Mani et al., 2006; Verhagen et al., 2007; Chambers et al., 2007; Bethard and Martin, 2007) and, following a series of TempEval (TE) workshops (Verhagen et al., 2007, 2010; UzZaman et al., 2013), it has drawn increased attention.",1 Introduction,[0],[0]
"Time-slot filling (Surdeanu, 2013; Ji et al., 2014), storyline construction (Do et al., 2012; Minard et al., 2015), clinical narratives processing (Jindal and Roth, 2013; Bethard et al., 2016), and temporal question answering (Llorens et al., 2015) are all explicit examples of temporal processing.
",1 Introduction,[0],[0]
"The fundamental tasks in temporal processing, as identified in the TE workshops, are 1) time expression (the so-called “timex”) extraction
and normalization and 2) temporal relation (also known as TLINKs (Pustejovsky et al., 2003a)) extraction.",1 Introduction,[0],[0]
"While the first task has now been well handled by the state-of-the-art systems (HeidelTime (Strötgen and Gertz, 2010), SUTime (Chang and Manning, 2012), IllinoisTime (Zhao et al., 2012), NavyTime (Chambers, 2013), UWTime (Lee et al., 2014), etc.) with end-to-end F1 scores being around 80%, the second task has long been a challenging one; even the top systems only achieved F1 scores of around 35% in the TE workshops.
",1 Introduction,[0],[0]
"The goal of the temporal relation task is to generate a directed temporal graph whose nodes represent temporal entities (i.e., events or timexes) and edges represent the TLINKs between them.",1 Introduction,[0],[0]
"The task is challenging because it often requires global considerations – considering the entire graph, the TLINK annotation is quadratic in the number of nodes and thus very expensive, and an overwhelming fraction of the temporal relations are missing in human annotation.",1 Introduction,[0],[0]
"In this paper, we propose a structured learning approach to temporal relation extraction, where local models are updated based on feedback from global inferences.",1 Introduction,[0],[0]
"The structured approach also gives rise to a semisupervised method, making it possible to take advantage of the readily available unlabeled data.",1 Introduction,[0],[0]
"As a byproduct, this approach further provides a new, effective perspective on handling those missing relations.
",1 Introduction,[0],[0]
"In the common formulations, temporal relations are categorized into three types: the E-E TLINKs (those between a pair of events), the T-T TLINKs (those between a pair of timexes), and the E-T TLINKs (those between an event and a timex).",1 Introduction,[0],[0]
"While the proposed approach can be generally applied to all three types, this paper focuses on the majority type, i.e., the E-E TLINKs.",1 Introduction,[0],[0]
"For example, consider the following snippet taken from the
ar X
iv :1
90 6.
04 94
3v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 Ju
n 20
19
training set provided in the TE3 workshop.",1 Introduction,[0],[0]
"We want to construct a temporal graph as in Fig. 1 for the events in boldface in Ex1.
Ex1 . . .",1 Introduction,[0],[0]
"tons of earth cascaded down a hillside, ripping two houses from their foundations.",1 Introduction,[0],[0]
"No one was hurt, but firefighters ordered the evacuation of nearby homes and said they’ll monitor the shifting ground.. . .
",1 Introduction,[0],[0]
"As discussed in existing work (Verhagen, 2004; Bramsen et al., 2006; Mani et al., 2006; Chambers and Jurafsky, 2008), the structure of a temporal graph is constrained by some rather simple rules:
1.",1 Introduction,[0],[0]
Symmetry.,1 Introduction,[0],[0]
"For example, if A is before B, then B must be after A.
2.",1 Introduction,[0],[0]
Transitivity.,1 Introduction,[0],[0]
"For example, if A is before B and B is before C, then A must be before C.
This particular structure of a temporal graph (especially the transitivity structure) makes its nodes highly interrelated, as can be seen from Fig. 1.",1 Introduction,[0],[0]
"It is thus very challenging to identify the TLINKs between them, even for human annotators: The inter-annotator agreement on TLINKs is usually about 50%-60% (Mani et al., 2006).",1 Introduction,[0],[0]
Fig. 2 shows the actual human annotations provided by TE3.,1 Introduction,[0],[0]
"Among all the ten possible pairs of nodes, only three TLINKs were annotated.",1 Introduction,[0],[0]
"Even if we only look at main events in consecutive sentences and at events in the same sentence, there are still quite a few missing TLINKs, e.g., the one between hurt and cascaded and the one between monitor and ordered.
",1 Introduction,[0],[0]
Early attempts by Mani et al. (2006); Chambers et al. (2007); Bethard et al. (2007); Verhagen and Pustejovsky (2008) studied local methods – learning models that make pairwise decisions between each pair of events.,1 Introduction,[0],[0]
"State-of-the-art local methods, including ClearTK (Bethard, 2013), UTTime
(Laokulrat et al., 2013), and NavyTime (Chambers, 2013), use better designed rules or more features such as syntactic tree paths and achieve better results.",1 Introduction,[0],[0]
"However, the decisions made by these (local) models are often globally inconsistent (i.e., the symmetry and/or transitivity constraints are not satisfied for the entire temporal graph).",1 Introduction,[0],[0]
"Integer linear programming (ILP) methods (Roth and Yih, 2004) were used in this domain to enforce global consistency by several authors including Bramsen et al. (2006); Chambers and Jurafsky (2008); Do et al. (2012), which formulated TLINK extraction as an ILP and showed that it improves over local methods for densely connected graphs.",1 Introduction,[0],[0]
"Since these methods perform inference (“I”) on top of pre-trained local classifiers (“L”), they are often referred to as L+I (Punyakanok et al., 2005).",1 Introduction,[0],[0]
"In a state-of-the-art method, CAEVO (Chambers et al., 2014), many hand-crafted rules and machine learned classifiers (called sieves therein) form a pipeline.",1 Introduction,[0],[0]
The global consistency is enforced by inferring all possible relations before passing the graph to the next sieve.,1 Introduction,[0],[0]
"This best-first architecture is conceptually similar to L+I but the inference is greedy, similar to Mani et al. (2007); Verhagen and Pustejovsky (2008).
",1 Introduction,[0],[0]
"Although L+I methods impose global constraints in the inference phase, this paper argues that global considerations are necessary in the learning phase as well (i.e., structured learning).",1 Introduction,[0],[0]
"In parallel to the work presented here, Leeuwenberg and Moens (2017) also proposed a structured learning approach to extracting the temporal relations.",1 Introduction,[0],[0]
"Their work focuses on a domain-specific dataset from Clinical TempEval (Bethard et al., 2016), so their work does not need to address some of the difficulties of the general problem that our work addresses.",1 Introduction,[0],[0]
"More importantly, they compared structured learning to local baselines, while we find that the comparison between structured learning and L+I is more interesting and important for
understanding the effect of global considerations in the learning phase.",1 Introduction,[0],[0]
"In difference from existing methods, we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way.",1 Introduction,[0],[0]
"Our solution targets on these issues and, as we show, achieves significant improvements on two commonly used evaluation sets.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
"Section 2 clarifies the temporal relation types and the evaluation metric of a temporal graph used in this paper, Section 3 explains the structured learning approach in detail, and Section 4 discusses the practical issue of missing relations.",1 Introduction,[0],[0]
We provide experiments and discussion in Section 5 and conclusion in Section 6.,1 Introduction,[0],[0]
"Existing corpora for temporal processing often follows the interval representation of events proposed in Allen (1984), and makes use of 13 relation types in total.",2.1 Temporal Relation Types,[0],[0]
"In many systems, vague or none is also included as another relation type when a TLINK is not clear or missing.",2.1 Temporal Relation Types,[0],[0]
"However, current systems usually use a reduced set of relation types, mainly due to the following reasons.
1.",2.1 Temporal Relation Types,[0],[0]
The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others (see Table 1 in Mani et al. (2006)).,2.1 Temporal Relation Types,[0],[0]
"For example, relations such as immediately before or immediately after barely exist in a corpus compared to before and after.
2.",2.1 Temporal Relation Types,[0],[0]
"Due to the ambiguity in natural language, determining relations like before and immediately before can be a difficult task itself (Chambers et al., 2014).
",2.1 Temporal Relation Types,[0],[0]
"In this work, we follow the reduced set of temporal relation types used in CAEVO (Chambers et al., 2014): before, after, includes, is included, equal, and vague.",2.1 Temporal Relation Types,[0],[0]
"The most recent evaluation metric in TE3, i.e., the temporal awareness (UzZaman and Allen, 2011), is adopted in this work.",2.2 Quality of A Temporal Graph,[0],[0]
"Specifically, let Gsys and Gtrue be two temporal graphs from the system prediction and the ground truth, respectively.",2.2 Quality of A Temporal Graph,[0],[0]
"The
precision and recall of temporal awareness are defined as follows.
",2.2 Quality of A Temporal Graph,[0],[0]
"P = |G−sys ∩G+true| |G−sys| , R = |G−true ∩G+sys| |G−true|
where G+ is the closure of graph G, G− is the reduction of G, “∩” is the intersection between TLINKs in two graphs, and |G| is the number of TLINKs in G. The temporal awareness metric better captures how “useful” a temporal graph is.",2.2 Quality of A Temporal Graph,[0],[0]
"For example, if system 1 produces ripping is before hurt and hurt is before monitor, and system 2 adds ripping is before monitor on top of system 1.",2.2 Quality of A Temporal Graph,[0],[0]
"Since system 2 is simply a transitive closure of system 1, they would have the same evaluation scores.",2.2 Quality of A Temporal Graph,[0],[0]
Note that vague relations are usually considered as non-existing TLINKs and are not counted during evaluation.,2.2 Quality of A Temporal Graph,[0],[0]
"As shown in Fig. 1, the learning problem in temporal relation extraction is global in nature.",3 A Structured Training Approach,[0],[0]
"Even the top local method in TE3, UTTime (Laokulrat et al., 2013), only achieved F1=56.5 when presented with a pair of temporal entities (Task C– relation only (UzZaman et al., 2013)).",3 A Structured Training Approach,[0],[0]
"Since the success of an L+I method strongly relies on the quality of the local classifiers, a poor local classifier is obviously a roadblock for L+I methods.",3 A Structured Training Approach,[0],[0]
"Following the insights from Punyakanok et al. (2005), we propose to use a structured learning approach (also called “Inference Based Training” (IBT)).
",3 A Structured Training Approach,[0],[0]
"Unlike the current L+I approach, where local classifiers are trained independently beforehand without knowledge of the predictions on neighboring pairs, we train local classifiers with feedback that accounts for other relations, by performing global inference in each round of the learning process.",3 A Structured Training Approach,[0],[0]
"In order to introduce the structured learning algorithm, we first explain its most important component, the global inference step.",3 A Structured Training Approach,[0],[0]
"In a document with n pairs of events, let φi ∈ X ⊆ Rd be the extracted d-dimensional feature and yi ∈ Y be the temporal relation for the i-th pair of events, i = 1, 2, . . .",3.1 Inference,[0],[0]
", n, where Y = {rj}6j=1 is the label set for the six temporal relations we use.",3.1 Inference,[0],[0]
"Moreover, let x = {φ1, . . .",3.1 Inference,[0],[0]
", φn} ∈ X n",3.1 Inference,[0],[0]
"and y = {y1, . . .",3.1 Inference,[0],[0]
", yn} ∈ Yn be more compact representations of all the features and labels in this
document.",3.1 Inference,[0],[0]
"Given the weight vector wr of a linear classifier trained for relation r ∈ Y (i.e., using the one-vs-all scheme), the global inference step is to solve the following constrained optimization problem:
ŷ = arg max y∈C(Yn) f(x,y), (1)
where C(Yn) ⊆ Yn constrains the temporal graph to be symmetrically and transitively consistent, and f(x,y) is the scoring function:
f(x,y) = n∑ i=1",3.1 Inference,[0],[0]
fyi(φi),3.1 Inference,[0],[0]
= n∑ i=1,3.1 Inference,[0],[0]
ew T yi φi∑,3.1 Inference,[0],[0]
"r∈Y e wTr φi .
",3.1 Inference,[0],[0]
"Specifically, fyi(φi) is the probability of the i-th event pair having relation yi.",3.1 Inference,[0],[0]
"f(x, y) is simply the sum of these probabilities over all the event pairs in a document, which we think of as the confidence of assigning y = {y1, ..., yn} to this document and therefore, it needs to be maximized in Eq.",3.1 Inference,[0],[0]
"(1).
Note that when C(Yn) = Yn, Eq. (1) can be solved for each ŷi independently, which is what the so-called local methods do, but the resulting ŷ may not satisfy global consistency in this way.",3.1 Inference,[0],[0]
When C(Yn) 6=,3.1 Inference,[0],[0]
"Yn, Eq.",3.1 Inference,[0],[0]
"(1) cannot be decoupled for each ŷi and is usually formulated as an ILP problem (Roth and Yih, 2004; Chambers and Jurafsky, 2008; Do et al., 2012).",3.1 Inference,[0],[0]
"Specifically, let Ir(ij) ∈ {0, 1} be the indicator function of relation r for event i and event j and fr(ij) ∈",3.1 Inference,[0],[0]
"[0, 1] be the corresponding soft-max score.",3.1 Inference,[0],[0]
"Then the ILP objective for global inference is formulated as follows.
",3.1 Inference,[0],[0]
Î,3.1 Inference,[0],[0]
=,3.1 Inference,[0],[0]
argmax,3.1 Inference,[0],[0]
"I
∑ ij∈E ∑ r∈Y fr(ij)Ir(ij) (2)
s.t. ΣrIr(ij) = 1 (uniqueness) , Ir(ij) = Ir̄(ji), (symmetry)
Ir1(ij)",3.1 Inference,[0],[0]
"+ Ir2(jk)− ΣNm=1Irm3 (ik) ≤ 1, (transitivity)
for all distinct events i, j, and k, where E = {ij | sentence dist(i, j)≤ 1}, r̄ is the reverse of r, and N is the number of possible relations for r3 when r1 and r2 are true.
",3.1 Inference,[0],[0]
Our formulation in Eq.,3.1 Inference,[0],[0]
"(2) is different from previous work (Chambers and Jurafsky, 2008; Do et al., 2012) in two aspects: 1) We restrict our event pairs ij to a smaller set E = {ij | sentence dist(i, j)≤ 1} where pairs that are
more than one sentence away are deleted for computational efficiency and (usually) for better performance.",3.1 Inference,[0],[0]
"In fact, to make better use of global constraints, we should have allowed more event pairs in Eq.",3.1 Inference,[0],[0]
(2).,3.1 Inference,[0],[0]
"However, fr(ij) is usually more reliable when i and j are closer in text.",3.1 Inference,[0],[0]
"Many participating systems in TE3 (UzZaman et al., 2013) have used this pre-filtering strategy to balance the trade-off between confidence in fr(ij) and global constraints.",3.1 Inference,[0],[0]
"We observe that the strategy fits very well to the existing datasets: As shown in Fig. 3, annotated TLINKs barely exist if two events are two sentences away.",3.1 Inference,[0],[0]
"2) Previously, transitivity constraints were formulated as Ir1(ij) + Ir2(jk)",3.1 Inference,[0],[0]
"− Ir3(ik) ≤ 1, which is a special case when N = 1 and can be understood as “r1 and r2 determine a single r3”.",3.1 Inference,[0],[0]
"However, it was overlooked that, although some r1 and r2 cannot uniquely determine r3, they can still constrain the set of labels r3 can take.",3.1 Inference,[0],[0]
"For example, as shown in Fig. 4, when r1=before and r2=is included, r3 is not determined",3.1 Inference,[0],[0]
"but we know that r3 ∈ {before, is included}1.",3.1 Inference,[0],[0]
"This information can be easily exploited by allowing N > 1.
",3.1 Inference,[0],[0]
"With these two differences, the optimization problem (2) can still be efficiently solved using off-the-shelf ILP packages such as GUROBI
1The transitivity table in Allen (1983) shows two more possible relations, overlap and immediately before, which are not in our label set.
",3.1 Inference,[0],[0]
"(Gurobi Optimization, Inc., 2012).",3.1 Inference,[0],[0]
"With the inference solver defined above, we propose to use the structured perceptron (Collins, 2002) as a representative for the inference based training (IBT) algorithm to learn those weight vectors wr.",3.2 Learning,[0],[0]
"Specifically, let L = {xk,yk}Kk=1 be the labeled training set of K instances (usually documents).",3.2 Learning,[0],[0]
The structured perceptron training algorithm for this problem is shown in Algorithm 1.,3.2 Learning,[0],[0]
"The Illinois-SL package (Chang et al., 2010) was used in our experiments for its structured perceptron component.",3.2 Learning,[0],[0]
"In terms of the features used in this work, we adopt the same set of features designed for E-E TLINKs in Sec. 3.1 of Do et al. (2012).
",3.2 Learning,[0],[0]
"In Algorithm 1, Line 6 is the inference step as in Eq.",3.2 Learning,[0],[0]
"(1) or (2), which is augmented with a closure operation on ŷ in the following line.",3.2 Learning,[0],[0]
"In the case in which there is only one pair of events in each instance (thus no structure to take advantage of), Algorithm 1 reduces to the conventional perceptron algorithm and Line 6 simply chooses the top scoring label.",3.2 Learning,[0],[0]
"With a structured instance instead, Line 6 becomes slower to solve, but it can provide valuable information so that the perceptron learner is able to look further at other labels rather than an isolated pair.",3.2 Learning,[0],[0]
"For example in Ex1 and Fig. 1, the fact that (ripping,ordered)=before is established through two other relations: 1) ripping is an adverbial participle and thus included in cascaded and 2) cascaded is before ordered.",3.2 Learning,[0],[0]
"If (ripping,ordered)=before is presented to a local learning algorithm without knowing its predictions on (ripping,cascaded) and (cascaded,ordered), then the model either cannot support it or overfits it.",3.2 Learning,[0],[0]
"In IBT, however, if the classifier was correct in deciding (ripping,cascaded) and (cascaded,ordered), then (ripping,ordered) would be correct automatically and would not contribute to updating the classifier.",3.2 Learning,[0],[0]
The scarcity of training data and the difficulty in annotation have long been a bottleneck for temporal processing systems.,3.3 Semi-supervised Structured Learning,[0],[0]
"Given the inherent global constraints in temporal graphs, we propose to perform semi-supervised structured learning using the constraint-driven learning (CoDL) algorithm (Chang et al., 2007, 2012), as shown in Algorithm 2, where the function “Learn” in Lines 2 and 9 represents any standard learning algorithm
Algorithm 1: Structured perceptron algorithm for temporal relations
Input: Training set L = {xk,yk}Kk=1, learning rate λ
1 Perform graph closure on each yk 2",3.3 Semi-supervised Structured Learning,[0],[0]
"Initialize wr = 0, ∀r ∈ Y 3 while convergence criteria not satisfied do 4 Shuffle the examples in L 5 foreach (x,y) ∈ L do 6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 if ŷ",3.3 Semi-supervised Structured Learning,[0],[0]
6=,3.3 Semi-supervised Structured Learning,[0],[0]
"y then 9 wr = wr + λ( ∑ i:yi=r
φi−∑",3.3 Semi-supervised Structured Learning,[0],[0]
"i:ŷi=r φi), ∀r ∈ Y
10 return {wr}r∈Y
(e.g., perceptron, SVM, or even structured perceptron",3.3 Semi-supervised Structured Learning,[0],[0]
"; here we used the averaged perceptron (Freund and Schapire, 1998)) and subscript “r” means selecting the learned weight vector for relation r ∈ Y .",3.3 Semi-supervised Structured Learning,[0],[0]
"CoDL improves the model learned from a small amount of labeled data by repeatedly generating feedback through labeling unlabeled examples, which is in fact a semi-supervised version of IBT.",3.3 Semi-supervised Structured Learning,[0],[0]
"Experiments show that this scheme is indeed helpful in this problem.
",3.3 Semi-supervised Structured Learning,[0],[0]
Algorithm 2: Constraint-driven learning algorithm,3.3 Semi-supervised Structured Learning,[0],[0]
"Input: Labeled set L, unlabeled set U ,
weighting coefficient γ 1",3.3 Semi-supervised Structured Learning,[0],[0]
"Perform closure on each graph in L 2 Initialize wr = Learn(L)r, ∀ r ∈",3.3 Semi-supervised Structured Learning,[0],[0]
Y 3 while convergence criteria not satisfied do 4 T = ∅ 5 foreach x ∈ U do,3.3 Semi-supervised Structured Learning,[0],[0]
"6 ŷ = arg maxy∈C f(x,y) 7 Perform graph closure on ŷ 8 T = T ∪ {(x, ŷ)} 9 wr = γwr + (1− γ)Learn(T )",3.3 Semi-supervised Structured Learning,[0],[0]
"r,∀ r ∈ Y
10 return {wr}r∈Y",3.3 Semi-supervised Structured Learning,[0],[0]
"Since even human annotators find it difficult to annotate temporal graphs, many of the TLINKs are left unspecified by annotators (compare Fig. 2 to Fig. 1).",4 Missing Annotations,[0],[0]
"While some of these missing TLINKs can be inferred from existing ones, the vast majority still remain unknown as shown in Table 1.",4 Missing Annotations,[0],[0]
"De-
spite the existence of denser annotation schemes (e.g., Cassidy et al. (2014)), the TLINK annotation task is quadratic in the number of nodes, and it is practically infeasible to annotate complete graphs.",4 Missing Annotations,[0],[0]
"Therefore, the problem of identifying these unknown relations in training and test is a major issue that dramatically hurts existing methods.
",4 Missing Annotations,[0],[0]
We could simply use these unknown pairs (or some filtered version of them) to design rules or train classifiers to identify whether a TLINK is vague or not.,4 Missing Annotations,[0],[0]
"However, we propose to exclude both the unknown pairs and the vague classifier from the training process – by changing the structured loss function to ignore the inference feedback on vague TLINKs (see Line 9 in Algorithm 1 and Line 9 in Algorithm 2).",4 Missing Annotations,[0],[0]
"The reasons are discussed below.
",4 Missing Annotations,[0],[0]
"First, it is believed that a lot of the unknown pairs are not really vague but rather pairs that the annotators failed to look at (Bethard et al., 2007; Cassidy et al., 2014; Chambers et al., 2014).",4 Missing Annotations,[0],[0]
"For example, (cascaded, monitor) should be annotated as before but is missing in Fig. 2.",4 Missing Annotations,[0],[0]
It is hard to exclude this noise in the data during training.,4 Missing Annotations,[0],[0]
"Second, compared to the overwhelmingly large number of unknown TLINKs (89.5% as shown in Table 1), the scarcity of non-vague TLINKs makes it hard to learn a good vague classifier.",4 Missing Annotations,[0],[0]
"Third, vague is fundamentally different from the other relation types.",4 Missing Annotations,[0],[0]
"For example, if a before TLINK can be established given a sentence, then it always holds as before regardless of other events around it, but if a TLINK is vague given a sentence, it may still change to other types afterwards if a connection can later be established through other nodes from the context.",4 Missing Annotations,[0],[0]
"This distinction emphasizes that vague is a consequence of lack of background/contextual information, rather than a concrete relation type to be trained on.",4 Missing Annotations,[0],[0]
"Fourth, without the vague classifier, the predicted temporal graph tends to become more densely connected, thus the global transitivity constraints can be more effective in correcting local mistakes (Chambers
and Jurafsky, 2008).",4 Missing Annotations,[0],[0]
"However, excluding the local classifier for vague TLINKs would undesirably assign nonvague TLINKs to every pair of events.",4 Missing Annotations,[0],[0]
"To handle this, we take a closer look at the vague TLINKs.",4 Missing Annotations,[0],[0]
We note that a vague TLINK could arise in two situations if the annotators did not fail to look at it.,4 Missing Annotations,[0],[0]
"One is that an annotator looks at this pair of events and decides that multiple relations can exist, and the other one is that two annotators disagree on the relation (similar arguments were also made in Cassidy et al. (2014)).",4 Missing Annotations,[0],[0]
"In both situations, the annotators first try to assign all possible relations to a TLINK, and then change the relation to vague if more than one can be assigned.",4 Missing Annotations,[0],[0]
"This human annotation process for vague is different from many existing methods, which either identify the existence of a TLINK first (using rules or machinelearned classifiers) and then classify, or directly include vague as a classification label along with other non-vague relations.
",4 Missing Annotations,[0],[0]
"In this work, however, we propose to mimic this mental process by a post-filtering method2.",4 Missing Annotations,[0],[0]
"Specifically, we take each TLINK produced by ILP and determine whether it is vague using its relative entropy (the Kullback-Leibler divergence) to the uniform distribution.",4 Missing Annotations,[0],[0]
"Let {rm}Mm=1 be the set of relations that the i-th pair of events can take, we filter the i-th TLINK given by ILP by:
δi = M∑ m=1 frm(φi) log (Mfrm(φi)),
where frm(φi) is the soft-max score of rm, obtained by the local classifier for rm.",4 Missing Annotations,[0],[0]
"We then compare δi to a fixed threshold τ to determine the vagueness of this TLINK; we accept its originally predicted label if δi > τ , or change it to vague otherwise.",4 Missing Annotations,[0],[0]
Using relative entropy here is intuitively appealing and empirically useful as shown in the experiments section; better metrics are of course yet to be designed.,4 Missing Annotations,[0],[0]
"The TempEval3 (TE3) workshop (UzZaman et al., 2013) provided the TimeBank (TB) (Pustejovsky et al., 2003b), AQUAINT (AQ) (Graff, 2002), Silver (TE3-SV), and Platinum (TE3-PT) datasets,
2Some systems (e.g., TARSQI (Verhagen and Pustejovsky, 2008)) employed a similar idea from a different standpoint, by thresholding TLINKs based on confidence scores.
where TB and AQ are usually for training, and TE3-PT is usually for testing.",5.1 Datasets,[0],[0]
"The TE3-SV dataset is a much larger, machine-annotated and automatically-merged dataset based on multiple systems, with the intention to see if these “silver” standard data can help when included in training (although almost all participating systems saw performance drop with TE3-SV included in training).
",5.1 Datasets,[0],[0]
Two popular augmentations on TB are the VerbClause temporal relation dataset (VC) and TimebankDense dataset (TD).,5.1 Datasets,[0],[0]
"The VC dataset has specially annotated event pairs that follow the socalled Verb-Clause structure (Bethard et al., 2007), which is usually beneficial to be included in training (UzZaman et al., 2013).",5.1 Datasets,[0],[0]
The TD dataset contains 36 documents from TB which were reannotated using the dense event ordering framework proposed in Cassidy et al. (2014).,5.1 Datasets,[0],[0]
The experiments included in this paper will involve the TE3 datasets as well as these augmentations.,5.1 Datasets,[0],[0]
"Therefore, some statistics on them are shown in Table 2 for the readers’ information.",5.1 Datasets,[0],[0]
"In addition to the state-of-the-art systems, another two baseline methods were also implemented for a better understanding of the proposed ones.",5.2 Baseline Methods,[0],[0]
"The first is the regularized averaged perceptron (AP) (Freund and Schapire, 1998) implemented in the LBJava package (Rizzolo and Roth, 2010) and is a local method.",5.2 Baseline Methods,[0],[0]
"On top of the first baseline, we performed global inference in Eq.(2), referred to as the L+I baseline (AP+ILP).",5.2 Baseline Methods,[0],[0]
"Both of them used the same feature set (i.e., as designed in Do et al. (2012))",5.2 Baseline Methods,[0],[0]
as in the proposed structured perceptron (SP) and CoDL for fair comparisons.,5.2 Baseline Methods,[0],[0]
"To clarify,
SP and CoDL are training algorithms and their immediate outputs are the weight vectors {wr}r∈Y for local classifiers.",5.2 Baseline Methods,[0],[0]
"An ILP inference was performed on top of them to yield the final output, and we refer to it as “S+I” (i.e., structured learning+inference) methods.",5.2 Baseline Methods,[0],[0]
"To show the benefit of using structured learning, we first tested one scenario where the gold pairs of events that have a non-vague TLINK were known priori.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup was a standard task presented in TE3, so that the difficulty of detecting vague TLINKs was ruled out.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This setup also helps circumvent the issue that TE3 penalizes systems which assign extra labels that do not exist in the annotated graph, while these extra labels may be actually correct because the annotation itself might be incomplete.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"UTTime (Laokulrat et al., 2013) was the top system in this task in TE3.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"Since UTTime is not available to us, and its performance was reported in TE3 in terms of both E-E and E-T TLINKs together, we locally trained an E-T classifier based on Do et al. (2012) and included its prediction only for fair comparison.
",5.3.1 TE3 Task C - Relation Only,[0],[0]
UTTime is a local method and was trained on TB+AQ and tested on TE3-PT.,5.3.1 TE3 Task C - Relation Only,[0],[0]
We used the same datasets for our local baseline and its performance is shown in Table 3 under the name “AP-1”.,5.3.1 TE3 Task C - Relation Only,[0],[0]
Note that the reported numbers below are the temporal awareness scores obtained from the official evaluation script provided in TE3.,5.3.1 TE3 Task C - Relation Only,[0],[0]
"We can see that UTTime is about 3% better than AP-1 in the absolute value of F1, which is expected since UTTime included more advanced features derived from syntactic parse trees.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"By adding the VC and TD datasets into the training set, we retrained our local baseline and achieved comparable performance to
UTTime (“AP-2” in Table 3).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"On top of AP-2, a global inference step enforcing symmetry and transitivity constraints (“AP+ILP”) can further improve the F1 score by 9.3%, which is consistent with previous observations (Chambers and Jurafsky, 2008; Do et al., 2012).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"SP+ILP further improved the performance in precision, recall, and F1 significantly (per the McNemar’s test (Everitt, 1992; Dietterich, 1998) with p <0.0005), reaching an F1 score of 67.2%.",5.3.1 TE3 Task C - Relation Only,[0],[0]
"This meets our expectation that structured learning can be better when the local problem is difficult (Punyakanok et al., 2005).",5.3.1 TE3 Task C - Relation Only,[0],[0]
"In the first scenario, we knew in advance which TLINKs existed or not, so the “pre-filtering” (i.e., ignoring distant pairs as mentioned in Sec. 3.1 and “post-filtering” methods were not used when generating the results in Table 3.",5.3.2 TE3 Task C,[0],[0]
"We then tested a more practical scenario, where we only knew the events, but did not know which ones are related.",5.3.2 TE3 Task C,[0],[0]
"This setup was Task C in TE3 and the top system was ClearTK (Bethard, 2013).",5.3.2 TE3 Task C,[0],[0]
"Again, for fair comparison, we simply added the E-T TLINKs predicted by ClearTK.",5.3.2 TE3 Task C,[0],[0]
"Moreover, 10% of the training data was held out for development.",5.3.2 TE3 Task C,[0],[0]
"Corresponding results on the TE3-PT testset are shown in Table 4.
",5.3.2 TE3 Task C,[0],[0]
"From lines 2-4, all systems see significant drops in performance if compared with the same entries in Table 3.",5.3.2 TE3 Task C,[0],[0]
It confirms our assertion that how to handle vague TLINKs is a major issue for this temporal relation extraction problem.,5.3.2 TE3 Task C,[0],[0]
"The improvement of SP+ILP (line 4) over AP (line 2) was small and AP+ILP (line 3) was even worse than AP, which necessitates the use of a better approach
towards vague TLINKs.",5.3.2 TE3 Task C,[0],[0]
"By applying the postfiltering method proposed in Sec. 4, we were able to achieve better performances using SP+ILP (line 5), which shows the effectiveness of this strategy.",5.3.2 TE3 Task C,[0],[0]
"Finally, by setting U in Algorithm 2 to be the TE3-SV dataset, CoDL+ILP (line 6) achieved the best F1 score with a relative improvement over ClearTK being 14.8%.",5.3.2 TE3 Task C,[0],[0]
"Note that when using TE3SV in this paper, we did not use its annotations on TLINKs because of its well-known large noise (UzZaman et al., 2013).
",5.3.2 TE3 Task C,[0],[0]
"In UzZaman et al. (2013), we notice that the best performance of ClearTK was achieved when trained on TB+VC (line 7 is higher than its reported values in TE3 because of later changes in ClearTK), so we retrained the proposed systems on the same training set and results are shown on lines 8-9.",5.3.2 TE3 Task C,[0],[0]
"In this case, the improvement of S+I over Local was small, which may be due to the lack of training data.",5.3.2 TE3 Task C,[0],[0]
"Note that line 8 was still significantly different to line 7 per the McNemar’s test, although there was only 0.2% absolute difference in F1, which can be explained from their large differences in precision and recall.",5.3.2 TE3 Task C,[0],[0]
"The proposed structured learning approach was further compared to a recent system, a CAscading EVent Ordering architecture (CAEVO) proposed in Chambers et al. (2014) (lines 10-13).",5.3.3 Comparison with CAEVO,[0],[0]
We used the same training set and test set as CAEVO in the S+I systems.,5.3.3 Comparison with CAEVO,[0],[0]
"Again, we added the E-T TLINKs predicted by CAEVO to both S+I systems.",5.3.3 Comparison with CAEVO,[0],[0]
"In Chambers et al. (2014), CAEVO was reported on the straightforward evaluation metric including the vague TLINKs, but the temporal awareness scores
were used here, which explains the difference between line 11 in Table 4 and what was reported in Chambers et al. (2014).
ClearTK was reported to be outperformed by CAEVO on TD-Test (Chambers et al., 2014), but we observe that ClearTK on line 10 was much worse even than itself on line 7 (trained on TB+VC) and on line 1 (trained on TB+AQ+VC+TD) due to the annotation scheme difference between TD and TB/AQ/VC. ClearTK was designed mainly for TE3, aiming for high precision, which is reflected by its high precision on line 10, but it does not have enough flexibility to cope with two very different annotation schemes.",5.3.3 Comparison with CAEVO,[0],[0]
"Therefore, we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones.",5.3.3 Comparison with CAEVO,[0],[0]
"On the TD-Test dataset, all systems other than ClearTK had better F1 scores compared to their performances on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
"This notable difference (i.e., 48.53 vs 40.3) indicates the better quality of the dense annotation scheme that was used to create TD (Cassidy et al., 2014).",5.3.3 Comparison with CAEVO,[0],[0]
"SP+ILP outperformed CAEVO and if additional unlabeled dataset TE3-SV was used, CoDL+ILP achieved the best score with a relative improvement in F1 score being 6.3%.
",5.3.3 Comparison with CAEVO,[0],[0]
"We notice that the proposed systems often have higher recall than precision, and that this is less an issue on a densely annotated testset (TD-Test), so their low precision on TE3-PT possibly came from the missing annotations on TE3-PT.",5.3.3 Comparison with CAEVO,[0],[0]
It is still under investigation how to control precision and recall in real applications.,5.3.3 Comparison with CAEVO,[0],[0]
We develop a structured learning approach to identifying temporal relations in natural language text and show that it captures the global nature of this problem better than state-of-the-art systems do.,6 Conclusion,[0],[0]
A new perspective towards vague relations is also proved to gain from fully taking advantage of the structured approach.,6 Conclusion,[0],[0]
"In addition, the global nature of this problem gives rise to a better way of making use of the readily available unlabeled data, which further improves the proposed method.",6 Conclusion,[0],[0]
"The improved performance on both TE3-PT and TDTest, two differently annotated datasets, clearly shows the advantage of the proposed method over existing methods.",6 Conclusion,[0],[0]
We plan to build on the notable improvements shown here and expand this study to deal with additional temporal reasoning problems in natural language text.,6 Conclusion,[0],[0]
We thank all the reviewers for providing useful comments.,Acknowledgements,[0],[0]
This research is supported in part by a grant from the Allen Institute for Artificial Intelligence (allenai.org); the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizon Network; by the US Defense Advanced Research Projects Agency (DARPA) under contract FA8750-13-2-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053.,Acknowledgements,[0],[0]
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies of the U.S. Government.,Acknowledgements,[0],[0]
Identifying temporal relations between events is an essential step towards natural language understanding.,abstractText,[0],[0]
"However, the temporal relation between two events in a story depends on, and is often dictated by, relations among other events.",abstractText,[0],[0]
"Consequently, effectively identifying temporal relations between events is a challenging problem even for human annotators.",abstractText,[0],[0]
This paper suggests that it is important to take these dependencies into account while learning to identify these relations and proposes a structured learning approach to address this challenge.,abstractText,[0],[0]
"As a byproduct, this provides a new perspective on handling missing relations, a known issue that hurts existing methods.",abstractText,[0],[0]
"As we show, the proposed approach results in significant improvements on the two commonly used data sets for this problem.",abstractText,[0],[0]
A Structured Learning Approach to Temporal Relation Extraction,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1169–1180 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
A Structured Syntax-Semantics Interface for English-AMR Alignment
Ida Szubert Adam Lopez School of Informatics
University of Edinburgh Edinburgh, Scotland, UK
{k.i.szubert@sms, alopez@inf}.ed.ac.uk
Nathan Schneider Linguistics and Computer Science
Georgetown University Washington, DC, USA
nathan.schneider@georgetown.edu
Abstract
Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested. To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs. Our annotation explains how 97% of AMR edges are evoked by words or syntax. Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%. While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences. We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser. We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",text,[0],[0]
"Abstract Meaning Representation (AMR; Banarescu et al., 2013) is a popular framework for annotating whole sentence meaning.",1 Introduction,[0],[0]
"An AMR annotation is a directed, usually acyclic graph in which nodes represent entities and events, and edges represent relations between them, as on the right in figure 1.1
AMR annotations include no explicit mapping between elements of an AMR and the corresponding elements of the sentence that evoke them, and this presents a challenge to developers of machine learning systems that parse sentences to AMR or generate sentences from AMR, since they must
1For clarity of presentation, we have constructed the sentences and AMRs shown in figures—except for figure 3, which is a simplified version of a sentence in the corpus.
first infer this mapping in the training data (e.g. Flanigan et al., 2014; Wang et al., 2015; Artzi et al., 2015; Flanigan et al., 2016; Pourdamghani et al., 2016; Misra and Artzi, 2016; Damonte et al., 2017; Peng et al., 2017, inter alia).2
This AMR alignment problem was first formalized by Flanigan et al. (2014), who mapped AMR nodes or connected subgraphs to words or sequences of words under the assumption of a oneto-one mapping—we call this JAMR alignment.",1 Introduction,[0],[0]
Pourdamghani et al. (2014) then re-formalized it so that any AMR node or edge can map to any word without a one-to-one assumption—we call this ISI alignment.,1 Introduction,[0],[0]
"In ISI alignments, edges often align to syntactic function words: for example, :location aligns to in in figure 1.",1 Introduction,[0],[0]
"So edge alignments allow ISI to explain more of the AMR structure than JAMR, but in a limited way: only 23% of AMR edges are aligned in the ISI corpus.",1 Introduction,[0],[0]
"This may be be-
2Some recent neural AMR sytems require minimal or no explicit alignments (Konstas et al., 2017; van Noord and Bos, 2017).",1 Introduction,[0],[0]
"But they implicitly learn them in the form of soft attention, and we believe that a clearer understanding of alignment will benefit modeling and error analysis even in these systems.
1169
cause edges are often evoked by syntactic structure rather than words: for instance, the :ARG1 edge in figure 1 is evoked by the fact that cat is the subject of lies and not by any particular word.
",1 Introduction,[0],[0]
"Although it seems sensible to assume that all of the nodes and edges of an AMR are evoked by the words and syntax of a sentence, the existing alignment schemes do not allow for expressing that relationship.",1 Introduction,[0],[0]
We therefore propose a framework expressive enough to align AMR to syntax (§2) and use it to align a corpus of 200 AMRs to dependency parses.,1 Introduction,[0],[0]
"We analyse our corpus and show that the addition of syntactic alignments allows us account for 97% of the AMR content.
",1 Introduction,[0],[0]
"Syntactic-semantic mappings are often assumed by AMR parsing models (e.g. Wang et al., 2015; Artzi et al., 2015; Damonte et al., 2017), which is understandable since these mappings are wellstudied in linguistic theory.",1 Introduction,[0],[0]
But AMR explicitly avoids theoretical commitment to a syntaxsemantics mapping: Banarescu et al. (2013) state that “AMR is agnostic about how we might want to derive meanings from strings.”,1 Introduction,[0],[0]
"If we are going to build such an assumption into our models, we should test it empirically, which we can do by analysing our corpus.",1 Introduction,[0],[0]
"We observe some pervasive structural differences between AMR and dependency syntax (§3), despite the fact that a majority of AMR edges map easily onto dependency edges.
",1 Introduction,[0],[0]
"Since syntactic alignment can largely explain AMRs, we also develop a baseline rule-based aligner for it, and show that this new task is much more difficult than lexical alignment (§4).",1 Introduction,[0],[0]
We also show how our data can be used to analyze errors made by an AMR parser (§5).,1 Introduction,[0],[0]
We make our annotated data and aligner freely available for further research.3,1 Introduction,[0],[0]
"Our syntactic representation is dependency grammar, which represents the sentence as a rooted, directed graph where nodes are words and edges are grammatical relations between them (Kruijff, 2006).",2 Aligning AMR to dependency syntax,[0],[0]
"We use Universal Dependencies (UD), a cross-lingual dependency annotation scheme, as implemented in Stanford CoreNLP (Manning et al., 2014).",2 Aligning AMR to dependency syntax,[0],[0]
"Within the UD framework, we use enhanced dependencies (Schuster and Manning, 2016), in which dependents can have more than one head,
3https://github.com/ida-szubert/amr_ud
resulting in dependency graphs (DGs).4
Our alignment guidelines generalize ideas present in the existing frameworks.",2 Aligning AMR to dependency syntax,[0],[0]
"We want to allow many-to-many alignments, which we motivate by the observation that some phenomena cause an AMR graph to have one structure expressing the same information as multiple DG structures, and vice versa.",2 Aligning AMR to dependency syntax,[0],[0]
"For instance, in figure 2 the AMR subgraph representing Cruella de Vil aligns to two subgraphs in the dependency graph because of pronominal coreference.",2 Aligning AMR to dependency syntax,[0],[0]
"In the other direction, in figure 3 the capabilities node aligns to both capable nodes in the AMR, which is a result of the AMR treating conjoined adjectival modifiers as a case of ellipsis.",2 Aligning AMR to dependency syntax,[0],[0]
The alignments we propose hold between subgraphs of any size.,2 Aligning AMR to dependency syntax,[0],[0]
By aligning subgraphs we gain expressiveness needed to point out correspondences between semantic and syntactic structure.,2 Aligning AMR to dependency syntax,[0],[0]
"If AMR and DG were very similar in how they represent information, such correspondences would probably hold between subgraphs consisting of a single edge, as in figure 1 cat nmod:possÐÐÐÐÐ→my ∼ cat possÐÐ→I.",2 Aligning AMR to dependency syntax,[0],[0]
"However, AMR by design abstracts away from syntax and it should not be assumed that all mappings will be so clean.",2 Aligning AMR to dependency syntax,[0],[0]
"For example, the same figure has lies nmod-inÐÐÐÐ→sun caseÐÐ→in∼ lies locationÐÐÐÐ→sun.",2 Aligning AMR to dependency syntax,[0],[0]
"Moreover, AMR represents the meaning of particular words or phrases with elaborate structures, the result of which might be that the same information is expressed by a single word and a complex AMR subgraph, as in figure 3 where AMR represents general as person
ARG0-ofÐÐÐÐ→have-org-role ARG2ÐÐ→general.",2 Aligning AMR to dependency syntax,[0],[0]
An alignment is a link between subgraphs in an AMR and a DG which represent equivalent information.,2.1 Overview,[0],[0]
Given a sentence’s DG and AMR we define an alignment as a mapping between an AMR subgraph and a DG subgraph.,2.1 Overview,[0],[0]
"Lexical alignments (§2.2) hold between pairs of nodes, and nodes from either graph may participate in multiple lexical alignments.",2.1 Overview,[0],[0]
"Structural alignments (§2.3) hold between pairs of connected subgraphs where at least one of the subgraphs contains an edge.
",2.1 Overview,[0],[0]
"4We chose UD because it emphasises shallow and semantically motivated annotation, by the virtue of which it can be expected to align relatively straightforwardly to a semantic annotation such as AMR.",2.1 Overview,[0],[0]
"Aligning AMR with different versions of dependency grammar (e.g. Prague) or different syntactic frameworks (e.g. CCG, TAG) would be an interesting extension of our work.
",2.1 Overview,[0],[0]
In the following two sections we discuss the types of alignments that our framework allows.,2.1 Overview,[0],[0]
More detailed guidelines regarding how to align particular linguistic constructions can be found in appendix A.,2.1 Overview,[0],[0]
A lexical alignment should hold between a word and an AMR concept if the latter is judged to express the lexical meaning of the former.,2.2 Lexical alignments,[0],[0]
"Node labels usually reflect their lexically aligned word or its lemma, including derivational morphology (e.g. thirsty ∼ thirst-01).",2.2 Lexical alignments,[0],[0]
"Thus, string similarity is a useful heuristic for lexical alignment.5
Most AMR nodes align lexically to a single word.",2.2 Lexical alignments,[0],[0]
"Cases of one-to-many alignments include coreference, when an entity is mentioned multiple times in the sentence, and multiword expressions such as a verb-particle constructions (pay off ∼ pay-off-02) and fixed grammatical expressions (instead of ∼ instead-of-91).",2.2 Lexical alignments,[0],[0]
Occasionally an AMR node does not lexically align to any DG node.,2.2 Lexical alignments,[0],[0]
"This is true for constants indicating sentence mood such as imperative, implicit uses of and to group list items, inferred concept nodes such as entity
5Exceptions include: pronouns with noun antecedents in the sentence; the - indicating negative polarity, which lexically aligns to no, not, and negative prefixes; modal auxiliaries, e.g., can ∼ possible; normalized dates and values such as February ∼ 2 in a date-entity; and amr-unknown, which aligns to wh-words.
",2.2 Lexical alignments,[0],[0]
"types, name in named entities, and -91 frames like have-org-role-91.
",2.2 Lexical alignments,[0],[0]
"Most words are lexically aligned to a single AMR node, if they are aligned at all.",2.2 Lexical alignments,[0],[0]
"A word may align to multiple AMR nodes if it is duplicated in the AMR due to ellipsis or distributive coordination (capabilities aligns to c2 / capable and c3 / capable in figure 3), or if it is morphologically decomposed in the AMR (evildoer aligns to evil and do-02 in figure 2).",2.2 Lexical alignments,[0],[0]
"Many words are not lexically aligned to any AMR node, including punctuation tokens, articles, copulas, nonmodal auxiliaries, expletive subjects, infinitival to, complementizer that, and relative pronouns.",2.2 Lexical alignments,[0],[0]
"Structural alignments primarily reflect compositional grammatical constructions, be they syntactic or morphological.",2.3 Structural alignments,[0],[0]
Note that the structural alignments build upon the lexical ones.,2.3 Structural alignments,[0],[0]
"Structural alignments hold between two subgraphs, at least one of which is larger than a single node.",2.3 Structural alignments,[0],[0]
"If a subgraph includes any edges, it automatically includes nodes adjacent to those edges.",2.3 Structural alignments,[0],[0]
Structural alignments need not be disjoint: an edge can appear in two or more distinct alignments.,2.3 Structural alignments,[0],[0]
Nodes and edges in both AMR and DG may be unaligned.,2.3 Structural alignments,[0],[0]
"The ability to align subgraphs to subgraphs gives considerable flexibility in how the annotation task
can be interpreted.",2.3.1 Constraints on structural alignments,[0],[0]
We establish the following principles to guide the specification of alignment: Connectedness Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"In an alignment d ∼ a, d must be a connected subgraph of the DG, and a must be a connected subgraph of the AMR.",2.3.1 Constraints on structural alignments,[0],[0]
Minimality Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"If two alignments, d ∼ a and d′ ∼ a′, have no dependency or AMR edges in common, then their union d ∪d′ ∼",2.3.1 Constraints on structural alignments,[0],[0]
"a∪a′ is redundant, even if it is valid.",2.3.1 Constraints on structural alignments,[0],[0]
Individual alignments should be as small as possible; we believe compositionality is best captured by keeping structures minimal.,2.3.1 Constraints on structural alignments,[0],[0]
"Therefore, in figure 1 there is no alignment between subgraphs spanning My, cat, lies and i, cat, lie.",2.3.1 Constraints on structural alignments,[0],[0]
"Such subgraphs do express equivalent information, but the alignment between them decomposes neatly into smaller alignments and we record only those.",2.3.1 Constraints on structural alignments,[0],[0]
Subsumption Principle.,2.3.1 Constraints on structural alignments,[0],[0]
This principle expresses the fact that our alignments are hierarchical.,2.3.1 Constraints on structural alignments,[0],[0]
"Structural alignments need to be consistent with lexical alignments: for subgraph a to be aligned to subgraph d, all nodes lexically aligned to nodes in a must be included in d, and vice versa.",2.3.1 Constraints on structural alignments,[0],[0]
"Moreover, structural alignments need to be consistent with other structural alignments.",2.3.1 Constraints on structural alignments,[0],[0]
"A structural alignment d ∼ a is valid only if, for every connected AMR subgraph a< ⊂ a which is aligned to a DG subgraph, d′ ∼ a<, we also have that d′ is a subgraph of d—and vice versa for every d< ⊂",2.3.1 Constraints on structural alignments,[0],[0]
"d.
Further, if a contains a node n which is not lexically aligned but which is part of a structurally aligned subgraph a′ such that d′ ∼ a′, it needs to be the case that a′ ⊂ a ∧ d′ ⊂ d or
a′ ⊃ a ∧ d′ ⊃",2.3.1 Constraints on structural alignments,[0],[0]
d. (And vice versa for nodes in d.),2.3.1 Constraints on structural alignments,[0],[0]
"For example, conceal
nsubj-xsubjÐÐÐÐÐ→Cruella ∼ conceal
ARG0ÐÐ→person nameÐÐ→name op1Ð→Cruella is not a valid alignment, because the AMR side contains nodes person and name, which are not lexically aligned but which are both parts of a structural alignment marked in blue.
",2.3.1 Constraints on structural alignments,[0],[0]
Coordination Principle.,2.3.1 Constraints on structural alignments,[0],[0]
"If an alignment contains a dependency edge between two conjuncts, or between a conjunct and a coordinating conjunction, then it must also include all conjuncts and the conjunction.",2.3.1 Constraints on structural alignments,[0],[0]
This preserves the integrity of coordinate structures in alignments.,2.3.1 Constraints on structural alignments,[0],[0]
"For example, in figure 2 there is no alignment glee ccÐ→and ∼ and op1Ð→glee; only the larger structure which includes the greed nodes is aligned.
",2.3.1 Constraints on structural alignments,[0],[0]
Named Entity Principle.,2.3.1 Constraints on structural alignments,[0],[0]
Any structural alignment containing an AMR name node or any of the strings under it must contain the full subgraph rooted in the name plus the node above it specifying the entity type.,2.3.1 Constraints on structural alignments,[0],[0]
"This means that for example, in figure 2 there is no alignment conceal nsubj-xsubjÐÐÐÐÐ→Cruella ∼ conceal ARG0ÐÐ→person nameÐÐ→name op1Ð→""Cruella"".",2.3.1 Constraints on structural alignments,[0],[0]
Such an alignment would also be stopped by the Subsumption Principle provided that the blue alignment of the whole name was present.,2.3.1 Constraints on structural alignments,[0],[0]
"The Named Entity Principle is superfluous, but is provided to explicitly describe the treatment of such constructions.",2.3.1 Constraints on structural alignments,[0],[0]
"The smallest structure which can participate in a structural alignment is a single node, provided that it is aligned to a subgraph containing at least one edge.",2.3.2 Typology of structural alignments,[0],[0]
"A DG node may align to an AMR subgraph if the word is morphologically decomposed or otherwise analyzed in the AMR (e.g. in figure 2, evildoer ∼ person ARG0-ofÐÐÐÐ→do-02 ARG1ÐÐ→thing modÐ→evil).",2.3.2 Typology of structural alignments,[0],[0]
"Examples of DG structures whose meaning is expressed in a single AMR node include light verb constructions, phrasal verbs, and various other multiword expressions (e.g. in figure 2, makes dobjÐÐ→attempt ∼ attempt-01).
",2.3.2 Typology of structural alignments,[0],[0]
"Conceptually the simplest case of structural alignment is one edge to one edge, as in the blue and green alignments in figure 1.",2.3.2 Typology of structural alignments,[0],[0]
"For such an alignment to be possible, two requirements must be satisfied: nodes which are endpoints of those edges need to be aligned one-to-one; and the AMR relation and the syntactic dependency must map cleanly in terms of the relationship they express.
",2.3.2 Typology of structural alignments,[0],[0]
A one edge to multiple edges alignment arises when either of those requirements is not met.,2.3.2 Typology of structural alignments,[0],[0]
To see what happens in absence of one-to-one endpoint alignments let’s look at the relation between confident and general in figure 3.,2.3.2 Typology of structural alignments,[0],[0]
The DG general node is aligned to an AMR subgraph: general∼ person ARG0-ofÐÐÐÐ→have-org-role ARG2ÐÐ→general.,2.3.2 Typology of structural alignments,[0],[0]
All alignments which involve the general node on the DG side need to include its aligned subgraph on the AMR side.,2.3.2 Typology of structural alignments,[0],[0]
"It necessarily follows that the AMR subgraphs in those alignments will contain more edges that the DG ones; in this case the yellow subgraph in DG has 1 edge, and in AMR 3 edges.",2.3.2 Typology of structural alignments,[0],[0]
"As for the second requirement, it is possible for one graph to use multiple edges to express a relationship when the other graph needs only one.",2.3.2 Typology of structural alignments,[0],[0]
This is the case for lie nmod-inÐÐÐÐ→sun caseÐÐ→in ∼ lie locationÐÐÐÐ→sun in figure 1.,2.3.2 Typology of structural alignments,[0],[0]
"An example which combines both the node- and edge-related issues is marked in red in figure 2.
",2.3.2 Typology of structural alignments,[0],[0]
"Finally, we also allow for many edges to many edges alignments.",2.3.2 Typology of structural alignments,[0],[0]
"This may seem counterintuitive considering the assumption that we want to capture mappings between relations expressed in DG and AMR, and that we want to align minimal subgraphs.",2.3.2 Typology of structural alignments,[0],[0]
"There are cases where an alignment is actually capturing a single relation, but we need to treat a subgraph as an endpoint of the edge both in DG and AMR.",2.3.2 Typology of structural alignments,[0],[0]
"For instance, con-
sider in figure 2 the relationship that holds between Cruella de Vil and concealing, expressed syntactically as an nsubj-xsubj edge and semantically as an ARG0 edge.",2.3.2 Typology of structural alignments,[0],[0]
"One of the entities involved in that relationship, Cruella, is represented by a 2- edge DG subgraph and a 4-edge AMR subgraph.",2.3.2 Typology of structural alignments,[0],[0]
"Consequently, the alignment covering the DG and AMR edges that relate Cruella to concealing must link subgraphs consisting respectively of 3 and 5 edges.",2.3.2 Typology of structural alignments,[0],[0]
A more difficult case of many edges to many edges alignment arises when relationships between nodes are expressed so differently in the DG and AMR that given an edge in one graph it is not possible to find in the other graph a subgraph that would convey the same information without also including some other information.,2.3.2 Typology of structural alignments,[0],[0]
Coordination has this property: e.g. in figure 2 the conj-and dependency between glee and greed has no counterpart in the AMR.,2.3.2 Typology of structural alignments,[0],[0]
"There is no edge between AMR nodes aligned to those words, and the smallest AMR subgraph which contains them also contains and, which is itself lexically aligned.",2.3.2 Typology of structural alignments,[0],[0]
We cannot align glee conj-andÐÐÐÐ→greed ∼ glee op1←Ðand,2.3.2 Typology of structural alignments,[0],[0]
op2Ð→greed because of the rule that all lexically aligned nodes in one subgraph must be aligned to nodes in the other subgraph.,2.3.2 Typology of structural alignments,[0],[0]
Therefore we need to extend the DG side to and cc←Ðglee conj-andÐÐÐÐ→greed.,2.3.2 Typology of structural alignments,[0],[0]
"We annotated a corpus of 200 AMR-sentence pairs (3813 aligned structures) using the guidelines of §2 and appendix A.6
Data selection.",3 Manually aligned corpus,[0],[0]
"To create the corpus we drew a total of 200 AMR-sentence pairs: 135 from the training split of the AMR Annotation Release 1.0 (Knight et al., 2014), 55 from the training split of The Little Prince Corpus v1.6,7 and 10 sentences from the Adam part of the CHILDES Brown corpus (Brown, 1973), for which AMRs were produced by an experienced annotator.",3 Manually aligned corpus,[0],[0]
"Seventy items were selected to illustrate particular linguistic phenomena.8 The remaining 130 were selected at random.
",3 Manually aligned corpus,[0],[0]
"6We followed the precedent of previous AMR-to-sentence alignment corpora (see §4.2) in including 200 sentences in our gold standard, though ours was a different sample.
",3 Manually aligned corpus,[0],[0]
"7https://amr.isi.edu/download/ amr-bank-struct-v1.6.txt
8Namely: relative clauses, reflexive and non-reflexive pronominal anaphora, subject and object control, raising, exceptional case marking, coordination, wh-questions, dosupport questions, ellipsis, expletives, modal verbs, light verbs, comparison constructions, and quantification.
",3 Manually aligned corpus,[0],[0]
Preprocessing.,3 Manually aligned corpus,[0],[0]
"Dependency parses were obtained using Stanford CoreNLP neural network parser9 (Chen and Manning, 2014) and manually corrected.",3 Manually aligned corpus,[0],[0]
"The final parses conform to the enhanced UD guidelines,10 except they lack enhancements for ellipsis.
",3 Manually aligned corpus,[0],[0]
Inter-annotator agreement.,3 Manually aligned corpus,[0],[0]
The corpus was created by one annotator.,3 Manually aligned corpus,[0],[0]
"To assess inter-annotator agreement, a second annotator deeply familiar with UD and AMR annotated a random sample of sentences accounting for 10% of alignments in the corpus.",3 Manually aligned corpus,[0],[0]
"The overall inter-annotator F1-score was 88%, with 96% agreement on lexical alignments and 80% on structural alignments.",3 Manually aligned corpus,[0],[0]
We take this as an indication that our richly structured alignment framework as laid out in §2 is reasonably well-defined for annotators.,3 Manually aligned corpus,[0],[0]
"To assess our attempt to explain as much of the AMR as possible, we computed the proportion of AMR nodes and edges that participate in at least one alignment.",3.1 Coverage,[0],[0]
"Overall, 99.3% of nodes and 97.2% of edges in AMRs are aligned.",3.1 Coverage,[0],[0]
"We found that 81.5% of AMR graphs have full coverage, 18.5% have at least one unaligned edge, and 7.5% have one unaligned node (none had more than one; all unaligned nodes express mood or discourse-related information: interrogative, and, and say).",3.1 Coverage,[0],[0]
"We conclude that nearly all information in an AMR is evoked by lexical items or syntactic structure.
",3.1 Coverage,[0],[0]
We expected coverage of DG to be lower because punctuation and many function words are unaligned in our guidelines (§2.2).,3.1 Coverage,[0],[0]
"Indeed, only 71.4% of words and 65.2% of dependency edges are aligned.",3.1 Coverage,[0],[0]
"The similarity of AMR to syntax in examples like figure 1 invites the assumption of a close mapping, which often seems to be made in AMR parsers (Wang et al., 2015; Artzi et al., 2015; Misra and Artzi, 2016; Damonte et al., 2017) and aligners (Chu and Kurohashi, 2016; Chen and Palmer,
9The corpus is annotated with UD v1; a release of the dataset converted to UD v2 is planned for the future.",3.2 Syntactic-semantic similarity,[0],[0]
"We used the pretrained dependency parsing model provided in CoreNLP with depparse.extradependencies set to MAXIMAL, and used collapsed CCprocessed dependencies.
",3.2 Syntactic-semantic similarity,[0],[0]
10http://universaldependencies.org/u/overview/ enhanced-syntax.html,3.2 Syntactic-semantic similarity,[0],[0]
"2017).11 Such an attitude reflects decades of work in the syntax-semantics interface (Partee, 2014) and the utility of dependency syntax for other forms of semantics (e.g., Oepen et al., 2014; Reddy et al., 2016; Stanovsky et al., 2016; White et al., 2016; Zhang et al., 2017; Hershcovich et al., 2017).",1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, this assumption has not been empirically tested, and as Bender et al. (2015) observe, it is an assumption not guaranteed by the AMR annotation style.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"Having aligned a corpus of AMR-DG pairs, we are in a position to provide empirical evidence.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
Are AMRs and dependency graphs structurally similar?,1:2 16 13.1 2:3 14 16.0,[0],[0]
"We approach the question by analyzing the sizes of subgraphs used to align the two representations of the sentence.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
We define the size of a subgraph as the number of edges it contains.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"If a structure consists of a single node, we say its size is 0.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"The configuration of an alignment is then the pair of sizes for its AMR and DG sides; for example, an alignment with 1 AMR edge and 2 DG edges has configuration 1:2.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"We call an alignment configuration simple if at least one of the subgraphs is a single edge, indicating that there is a single relation which the alignment captures.",1:2 16 13.1 2:3 14 16.0,[0],[0]
Complex configurations cover multiple relations.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"By principle of minimality we infer that some structural difference between the graphs prevented those relations from aligning individually.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
One measure of similarity between AMR and DG graphs is the configuration of the most complex subgraph alignment between them.,1:2 16 13.1 2:3 14 16.0,[0],[0]
Configuration a:b is higher than c:d if a+b > c+d.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, all configurations involving 0 are lower than those which do not.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"A maximum of 1:1 means the graphs have only node-to-node, node-to-edge, and edge-toedge alignments, rendering the graphs isomorphic (ignoring edge directions and unaligned nodes).",1:2 16 13.1 2:3 14 16.0,[0],[0]
"In
11In particular, Chen and Palmer (2017) align dependency paths to AMR edges.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"However, their evaluation only considers node-to-node alignment, and their code and data are not available for comparison at the time of this writing.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"general, if the maximum alignment configuration is a simple one, the graphs could be made isomorphic by collapsing the larger side of the alignment (e.g., in figure 2, the AMR side of the alignment evildoer ∼ person ARG0-ofÐÐÐÐ→do",1:2 16 13.1 2:3 14 16.0,[0],[0]
"ARG1ÐÐ→thing modÐ→evil could be collapsed into a node).
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"In contrast, complex configurations imply serious structural dissimilarity, as in figure 3, where the cyan alignment has configuration 4:4.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
"The numbers in table 1 show that ≈33% of the sentences are simple.
",1:2 16 13.1 2:3 14 16.0,[0],[0]
Table 2 provides a detailed breakdown of alignment configurations in the corpus.,1:2 16 13.1 2:3 14 16.0,[0],[0]
"Phenomena which often trigger complex configurations include coordination, named entities, semantically decomposed words, attachment of negation, and preposition-based concepts encoding location, time, and quantity.12
We observe, comparing tables 1 and 2, that while simple configurations are most frequent in the corpus, the majority of sentences have at least one alignment which is complex.",1:2 16 13.1 2:3 14 16.0,[0],[0]
"It should not be assumed that AMR and DG representations of a sentence are, or could trivially be made to be, isomorphic.",1:2 16 13.1 2:3 14 16.0,[0],[0]
It is worth noting that our analysis suggests that DG and AMR could be made more similar by applying simple transformations targeting problematic constructions like coordination and named entities.,1:2 16 13.1 2:3 14 16.0,[0],[0]
We use our annotations to measure the accuracy of AMR aligners on specific phenomena that were inexpressible in previous annotation schemes.,4 Evaluation of automatic aligners,[0],[0]
"Our experiments evaluate the JAMR heuristic aligner (Flanigan et al., 2014), the ISI statistical aligner (Pourdamghani et al., 2014), and a heuristic rulebased aligner that we developed specifically for
12An AMR concept evoked by a preposition usually dominates the structure (after op1ÐÐ→date-entity decadeÐÐÐ→nineties), which is at odds with UD’s prepositions-as-case-markers policy (nineties caseÐÐ→after).
structural alignment.",4 Evaluation of automatic aligners,[0],[0]
"Our aligner operates in two passes: one for lexical alignment and one for structural alignment.
",4.1 Rule-based aligner,[0],[0]
Lexical alignment algorithm.,4.1 Rule-based aligner,[0],[0]
"AMR concepts are cognate with English words, so we align them by lexical similarity.",4.1 Rule-based aligner,[0],[0]
This algorithm does not make use of the DG.,4.1 Rule-based aligner,[0],[0]
"Before alignment, we remove sense identifiers on AMR node labels, and lemmatize DG node labels.",4.1 Rule-based aligner,[0],[0]
"Then for every pair of nodes a from the AMR and d from the DG we align them if any of the following conditions holds:
1.",4.1 Rule-based aligner,[0],[0]
"The Levenshtein distance of a and d is 15% or less of the length of the longer word.13
2.",4.1 Rule-based aligner,[0],[0]
"The label of a is the morphological negation of d (e.g. prudent ∼ imprudent).14
3.",4.1 Rule-based aligner,[0],[0]
"The label of a is – (AMR’s annotation of negation) and the parent of a aligns to d via rule 2.
4.",4.1 Rule-based aligner,[0],[0]
"The label of a is – and d is one of no, none, not, or never.
5.",4.1 Rule-based aligner,[0],[0]
"The label of a consists of multiple words, and the label of d matches any one of them under rule 1.",4.1 Rule-based aligner,[0],[0]
"(e.g. sit ∼ sit-down, war-torn ∼ war).15
6.",4.1 Rule-based aligner,[0],[0]
Labels of a and d likely have the same morphological root.,4.1 Rule-based aligner,[0],[0]
"We determine this by segmenting each word with Morfessor (Grönroos et al., 2014) trained on Wiki data and applying rule 1 to the first morpheme of each word.
",4.1 Rule-based aligner,[0],[0]
"Note that if a word type is repeated in a sentence, each repetition is aligned to the same AMR nodes under the above rules.
",4.1 Rule-based aligner,[0],[0]
Structural alignment algorithm.,4.1 Rule-based aligner,[0],[0]
"We align subgraphs using the procedure below, first from AMR to DG, then from DG to AMR.",4.1 Rule-based aligner,[0],[0]
"For clarity, the explanation refers to the first case.
",4.1 Rule-based aligner,[0],[0]
"13Threshold was determined empirically on a 10% sample from the dataset.
",4.1 Rule-based aligner,[0],[0]
"14We use a list of morphologically negated words provided by Ulf Hermjakob.
",4.1 Rule-based aligner,[0],[0]
"15This rule misaligns some AMR-specific node types, such as government ∼ government-organization.
",4.1 Rule-based aligner,[0],[0]
Local phase.,4.1 Rule-based aligner,[0],[0]
"For every AMR edge ea whose endpoints are lexically aligned nodes a1 (aligned to d1) and a2 (aligned to d2), we attempt to align minimal and connected AMR and dependency subgraphs, a′ and d′:
1.",4.1 Rule-based aligner,[0],[0]
"If there is a DG edge ed whose endpoints are d1 and d2, then a′← ea and d′← ed .
2.",4.1 Rule-based aligner,[0],[0]
"Otherwise, let πd be the shortest undirected path between d1 and d2.",4.1 Rule-based aligner,[0],[0]
"If all lexically aligned nodes in πd are aligned to a1 or a2, then a′ ← ea and d′← πd .
3.",4.1 Rule-based aligner,[0],[0]
"Otherwise, let a′′ be the smallest subgraph covering all AMR nodes that are lexically aligned to nodes in πd .",4.1 Rule-based aligner,[0],[0]
"If all the nodes in a′′ are aligned only to nodes in πd , then a′← a′′ and d′← πd .
4.",4.1 Rule-based aligner,[0],[0]
"Otherwise, the attempt is abandoned.",4.1 Rule-based aligner,[0],[0]
5.,4.1 Rule-based aligner,[0],[0]
"Finally, if the top node of a′ has a parent node labeled with an entity type concept, extend a′ to include the parent.",4.1 Rule-based aligner,[0],[0]
"(This step is performed only in the AMR-to-DG step.)
",4.1 Rule-based aligner,[0],[0]
Global phase.,4.1 Rule-based aligner,[0],[0]
"The local phase might produce alignments that violate the Subsumption Principle (§2.3.1), so we filter them out heuristically.",4.1 Rule-based aligner,[0],[0]
"For every pair of structural alignments, πd ∼ πa and π ′d ∼ π ′a where πa overlaps with π ′a, or πd with π ′d , if the region of overlap is not itself an aligned subgraph, we prune both alignments.16",4.1 Rule-based aligner,[0],[0]
"We evaluate JAMR, ISI, and our aligner on two distinct tasks.",4.2 Experiments,[0],[0]
Lexical alignment.,4.2 Experiments,[0],[0]
"Lexical alignment involves aligning AMR nodes to words, a task all three systems can perform.",4.2 Experiments,[0],[0]
"We evaluate against three datasets: our own, the JAMR dataset (Flanigan et al., 2014), and the ISI dataset (Pourdamghani et al., 2014).17 Results (table 3) suggest that this task is already well-addressed, but also that there exist marked differences between how lexical alignment is defined in each dataset and that aligners are
16This could be order-dependent since the removal of one alignment could trigger the removal of others, but our aligner does not account for this.
",4.2 Experiments,[0],[0]
"17We remove span alignments in the JAMR dataset and edge alignments in the ISI dataset.
",4.2 Experiments,[0],[0]
fine-tuned to their dataset.,4.2 Experiments,[0],[0]
"For our aligner, errors are due to faulty morphological analysis, duplicated words, and both accidental string similarity between AMR concepts and words and occasional lack of similarity between concepts and words that should be aligned.",4.2 Experiments,[0],[0]
Structural alignment.,4.2 Experiments,[0],[0]
An important goal of our experiments is to establish baselines for the structural alignment task.,4.2 Experiments,[0],[0]
"While we cannot evaluate the JAMR and ISI aligners directly on this task, we can use the lexical alignments they output in place of the first pass of our aligner.",4.2 Experiments,[0],[0]
The only dataset for this task is our own.,4.2 Experiments,[0],[0]
"The results (table 4) evaluate accuracy of structural alignments only and do not count lexical alignments.
",4.2 Experiments,[0],[0]
"The automatic alignments have lower coverage of AMRs than the gold alignments do: our best aligner leaves 13.3% of AMR nodes and 30.0% of AMR edges unaligned, compared to 0.07% and 2.8% in the gold standard.",4.2 Experiments,[0],[0]
"The aligner also leaves 39.2% of DG nodes and 47.7% of DG edges unaligned, compared to 28.6% and 34.8% in the gold standard.",4.2 Experiments,[0],[0]
The relatively low F-score for the gold standard lexical alignments and DGs condition suggests that substantial improvements to our structural alignment algorithm are possible.,4.2 Experiments,[0],[0]
"The two most common reasons for low recall were missing one of the conjuncts in a coordinate structure and aligning structures that violate the principle of minimality.
",4.2 Experiments,[0],[0]
Our corpus gives alignments between AMRs and gold standard dependency parses.,4.2 Experiments,[0],[0]
To see how much performance degrades when such parses are not available we also evaluate on automatic parses.18 Both precision and recall are substantially worse when the aligner relies on automatic syntax.,4.2 Experiments,[0],[0]
"Our corpus of manually aligned AMRs can be used to identify linguistic constructions which cause
18We use the CoreNLP dependency parser with settings as described in §3.",5 Improving error analysis for AMR parsers,[0],[0]
problems for an AMR parser.,UD structure missed mislabeled,[0],[0]
"We parsed the sentences from our corpus with the parser of Damonte et al. (2017).19 We map the nodes of the resulting automatic AMRs to the gold AMRs using the smatch evaluation tool (Cai and Knight, 2013), and on the basis of this mapping identify those nodes and edges of the gold AMRs which are missing or mislabeled in the automatic AMRs.
",UD structure missed mislabeled,[0],[0]
We then measured the number and rate of erroneous AMR fragments associated with each UD relation or construction (table 5).,UD structure missed mislabeled,[0],[0]
"The largest proportion of recall errors were for fragments associated with the subject relation, prepositional phrases, and nominal compounds.",UD structure missed mislabeled,[0],[0]
"Focusing on the subject relation, we can further say that 69% of the missing or mislabeled edges have the gold label ARG0, 19% ARG1, and the rest are distributed amongst domain, ARG2, purpose and mod.",UD structure missed mislabeled,[0],[0]
"Inspecting the errors we see that phenomena underlying them include pronominal coreference, sharing arguments between conjoined predicates, auxiliary verb constructions, and control and raising.20
Our corpus facilitates fine-grained error analysis of AMR parsers with respect to individual syntactic constructions.",UD structure missed mislabeled,[0],[0]
We release the code for the above analysis in order to encourage syntactically-informed comparison and improvement of systems.,UD structure missed mislabeled,[0],[0]
We have presented a new framework and corpus for aligning AMRs to dependency syntax.,6 Conclusion,[0],[0]
"Our data and analysis show that the vast majority of the semantics in AMR graphs can be mapped to the lexical and syntactic structure of a sentence, though current alignment systems do not fully capture this correspondence.",6 Conclusion,[0],[0]
"The syntax–semantics
19The overall smatch score of the parser on this dataset was 0.65.
",6 Conclusion,[0],[0]
"20The missing edge counts include gold edges for which the parser failed to produce one or both endpoints.
",6 Conclusion,[0],[0]
correspondences are often structurally divergent (non-isomorphic).,6 Conclusion,[0],[0]
Simple algorithms for lexical and structural alignment establish baselines for the new alignment task; we expect statistical models will be brought to bear on this task in future work.,6 Conclusion,[0],[0]
Our framework also facilitates syntactically-based analysis of AMR parsers.,6 Conclusion,[0],[0]
We release our data and code for the benefit of the research community.,6 Conclusion,[0],[0]
"This work was supported in part by EU ERC Advanced Fellowship 249520 GRAMPLUS and EU ERC H2020 Advanced Fellowship GA 742137 SEMANTAX.
",Acknowledgments,[0],[0]
"We thank Sameer Bansal, Marco Damonte, Lucia Donatelli, Federico Fancellu, Sharon Goldwater, Andreas Grivas, Yova Kementchedjhieva, Junyi Li, Joana Ribeiro, and the anonymous reviewers for helpful discussion of this work and comments on previous drafts of the paper.",Acknowledgments,[0],[0]
A.1 Lexical alignments Names.,A Details of alignment guidelines,[0],[0]
"In proper names, individual strings denoting words in the name are lexically aligned, but the entity as a whole is structurally aligned.
",A Details of alignment guidelines,[0],[0]
Entity types.,A Details of alignment guidelines,[0],[0]
"If the entity type is based on a common noun which occurs in the sentence, it is lexically aligned: e.g., Jon, a clumsy man, has a cat would involve the alignment man ∼ man.",A Details of alignment guidelines,[0],[0]
"Most often, however, an entity type is not explicitly mentioned in the sentence and is taken from AMR’s ontology of entity types (http://www.isi.edu/ ~ulf/amr/lib/ne-types.html), in which case it will not be lexically aligned.
",A Details of alignment guidelines,[0],[0]
Case marking and prepositions.,A Details of alignment guidelines,[0],[0]
The possessive marker ’s and many prepositions participate in structural but not lexical alignments because they are inherently relational.,A Details of alignment guidelines,[0],[0]
"However, we align a preposition if it carries sufficient lexical content to be included as an AMR node (e.g., the AMR for The cat is under the table would include under
op1Ð→table).",A Details of alignment guidelines,[0],[0]
Wh-questions.,A Details of alignment guidelines,[0],[0]
The special concept amr-unknown aligns lexically to the wh-word whose referent is questioned.,A Details of alignment guidelines,[0],[0]
"For multiword wh-expressions like how much, the expression is aligned structurally (not lexically) to amr-unknown.
Sentence mood.",A Details of alignment guidelines,[0],[0]
"In AMR, non-wh questions are indicated by
modeÐÐ→interrogative, imperatives by modeÐÐ→imperative, and exclamations/interjections by
modeÐÐ→expressive.",A Details of alignment guidelines,[0],[0]
"UD parses do not encode sentence mood, which can be conveyed by noncanonical word order (subject-auxiliary inversion for questions) or argument omission (subject omission for imperatives), rather than the presence of certain relations or words.",A Details of alignment guidelines,[0],[0]
"Sometimes the sentence includes an appropriate alignment point, e.g. complementizers whether and if for interrogative, allowing for a lexical alignment.",A Details of alignment guidelines,[0],[0]
"More often the parse has no obvious alignment point, and the constant interrogative, imperative, or expressive is left unaligned.21
A.2 Structural alignments
Copulas.",A Details of alignment guidelines,[0],[0]
"In UD, copulas are treated as modifiers of a predicate nominal or adjective, which is linked directly to the subject of the sentence via an nsubj dependency.",A Details of alignment guidelines,[0],[0]
We do not align copulas or the cop edge.,A Details of alignment guidelines,[0],[0]
"Thus, in figure 3, there is a structural alignment between general nsubj←ÐÐconfident and the AMR subgraph connecting the lexically aligned nodes.
",A Details of alignment guidelines,[0],[0]
"21Among the UD community there has been discussion of possibly adding sentence-level marking of mood (https:// github.com/UniversalDependencies/docs/issues/458), which could provide a convenient alignment point.
",A Details of alignment guidelines,[0],[0]
Control.,A Details of alignment guidelines,[0],[0]
"The subject of the control verb and the controlled predicate are connected by the nsubjxsubj edge, which can be structurally aligned with the corresponding AMR argument relation, as in e.g. figure 2.
",A Details of alignment guidelines,[0],[0]
Relative clauses.,A Details of alignment guidelines,[0],[0]
"In enhanced UD the noun governing a relative clause and the embedded predicate are linked by edges in both directions: a “surface syntax” acl-relcl edge headed by the noun, and a “deep syntax” edge such as nsubj, dobj, iobj, or nmod headed by the embedded predicate.",A Details of alignment guidelines,[0],[0]
Each participates in a structural alignment with the corresponding AMR subgraph.,A Details of alignment guidelines,[0],[0]
"The relative pronoun is left unaligned.
Coordination.",A Details of alignment guidelines,[0],[0]
"Coordination does not naturally lend itself to analysis with dependencies, and different dependency grammar traditions offer different approaches (Nivre, 2005; Mareček et al., 2013).",A Details of alignment guidelines,[0],[0]
"UD follows the Stanford style, where the first conjunct serves as the head of the remaining conjuncts, and the conjunction is a dependent of one of the conjuncts.22 In AMR the conjunction heads all the conjuncts (Prague style).",A Details of alignment guidelines,[0],[0]
"In light of this mismatch, we use a subgraph alignment to group the conjunction with its conjuncts on each side.",A Details of alignment guidelines,[0],[0]
A simple example is illustrated in figure 2.,A Details of alignment guidelines,[0],[0]
A quirk of UD’s approach to coordination is that it does not distinguish modifiers of the first conjunct from modifiers of the coordinate structure as a whole.,A Details of alignment guidelines,[0],[0]
The basic UD parse of her glee and greed is therefore ambiguous.,A Details of alignment guidelines,[0],[0]
"We rely on an extra edge in the enhanced parse between her and greed to establish an alignment for the AMR edge greed
ARG0ÐÐ→person.",A Details of alignment guidelines,[0],[0]
"The coordination in figure 3 is more complex: the coordinated modifier defense and security distributes over capabilities (i.e., there are two kinds of capabilities).",A Details of alignment guidelines,[0],[0]
"In the enhanced parse, defense and security are both attached as modifiers of capabilities.",A Details of alignment guidelines,[0],[0]
"This is expressed semantically via duplicate AMR nodes labeled capable, each receiving different modifiers corresponding to different conjuncts.",A Details of alignment guidelines,[0],[0]
"Independent of coordination, the two capable nodes also share a common argument, nation.",A Details of alignment guidelines,[0],[0]
"The three syntactic modifiers give rise to three subgraph alignments, and the subgraph alignment covering the coordinate structure (cyan in the figure) envelops two of these.",A Details of alignment guidelines,[0],[0]
"Ellipsis construc-
22In UD version 1, and therefore the examples in this paper, the conjunction attaches to the first conjunct, whereas in version 2 it attaches to the next successive conjunct (http: //universaldependencies.org/v2/summary.html).
",A Details of alignment guidelines,[0],[0]
"tions can also trigger node duplication in AMR, requiring similar structural alignments.",A Details of alignment guidelines,[0],[0]
Named entities.,A Details of alignment guidelines,[0],[0]
"AMR annotates each named entity with a node representing the name, linked to the strings of the name and headed by an entity type.",A Details of alignment guidelines,[0],[0]
This full structure is aligned to the full name in the dependency parse.,A Details of alignment guidelines,[0],[0]
Coreferent mentions.,A Details of alignment guidelines,[0],[0]
Coreference often causes an AMR structure to align to multiple DG subgraphs.,A Details of alignment guidelines,[0],[0]
"For example, in figure 2, both the pronoun her and the name align to the AMR subgraph representing the entity.",A Details of alignment guidelines,[0],[0]
This mechanism suffices to represent coreference between mentions in the sentence.,A Details of alignment guidelines,[0],[0]
Light verbs.,A Details of alignment guidelines,[0],[0]
"Light verbs have no lexical alignment, but a subgraph alignment covers the light verb construction as a unit (e.g. makes dobjÐÐ→attempt∼ attempt-01 in figure 2).",A Details of alignment guidelines,[0],[0]
"All subgraph alignments which involve the light verb or its complement have to involve to whole unit, as shown in the alignment highlighted in red in figure 2.",A Details of alignment guidelines,[0],[0]
Multiword expressions.,A Details of alignment guidelines,[0],[0]
"In verb-particle constructions and fixed grammatical expressions the AMR node lexically aligns to all words in the expression, and additionally to the DG subgraph spanning the whole expression.",A Details of alignment guidelines,[0],[0]
"(e.g. pay ∼ pay-off-02, off ∼ pay-off-02, and pay compound-prtÐÐÐÐÐÐÐ→off ∼ pay-off-02).",A Details of alignment guidelines,[0],[0]
Prepositional phrases.,A Details of alignment guidelines,[0],[0]
"PP modifiers typically involve an extra dependency edge for the preposition attachment, as with lies nmod-inÐÐÐÐ→sun caseÐÐ→in ∼ lie-07 locationÐÐÐÐ→sun.
Semantically decomposed words.",A Details of alignment guidelines,[0],[0]
"When one word has multiple lexical alignments because of morphological decomposition, there also exists a structural alignment between that word and an AMR subgraph representing the decomposition: e.g., in figure 2, evildoer∼ person ARG0-ofÐÐÐÐ→do-02 ARG1ÐÐ→thing modÐ→evil, and in figure 3, general ∼ person
ARG0-ofÐÐÐÐ→have-org-role-91 ARG2ÐÐ→general.",A Details of alignment guidelines,[0],[0]
"AMR decomposes certain words by convention which must always be structurally aligned, such as ago ∼ before op1Ð→now and government ∼ government-organization
ARG0-ofÐÐÐÐ→govern-01.",A Details of alignment guidelines,[0],[0]
"Date, time, and value expressions.",A Details of alignment guidelines,[0],[0]
"These expressions are aligned similarly to named entities, even though the normalized constants may not exactly match the words in the sentence.",A Details of alignment guidelines,[0],[0]
"For example,
the DG structure 9:00 nummod←ÐÐÐÐpm would be represented in the AMR as date-entity timeÐÐ→21:00; tokens 9:00 and pm are treated as a multiword expression: each is lexically aligned to ""21:00"".",A Details of alignment guidelines,[0],[0]
"Moreover, we also align 9:00 nummod←ÐÐÐÐpm ∼ 21:00 and 9:00 nummod←ÐÐÐÐpm ∼ date-entity timeÐÐ→21:00.",A Details of alignment guidelines,[0],[0]
"Abstract Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested.",abstractText,[0],[0]
"To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs.",abstractText,[0],[0]
Our annotation explains how 97% of AMR edges are evoked by words or syntax.,abstractText,[0],[0]
"Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%.",abstractText,[0],[0]
"While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences.",abstractText,[0],[0]
"We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser.",abstractText,[0],[0]
"We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",abstractText,[0],[0]
"Meaning Representation (AMR) annotations are often assumed to closely mirror dependency syntax, but AMR explicitly does not require this, and the assumption has never been tested.",abstractText,[0],[0]
"To test it, we devise an expressive framework to align AMR graphs to dependency graphs, which we use to annotate 200 AMRs.",abstractText,[0],[0]
Our annotation explains how 97% of AMR edges are evoked by words or syntax.,abstractText,[0],[0]
"Previously existing AMR alignment frameworks did not allow for mapping AMR onto syntax, and as a consequence they explained at most 23%.",abstractText,[0],[0]
"While we find that there are indeed many cases where AMR annotations closely mirror syntax, there are also pervasive differences.",abstractText,[0],[0]
"We use our annotations to test a baseline AMR-to-syntax aligner, finding that this task is more difficult than AMRto-string alignment; and to pinpoint errors in an AMR parser.",abstractText,[0],[0]
"We make our data and code freely available for further research on AMR parsing and generation, and the relationship of AMR to syntax.",abstractText,[0],[0]
A Structured Syntax-Semantics Interface for English-AMR Alignment,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612–3621 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3612",text,[0],[0]
"Recently, neural machine translation (NMT) (Bahdanau et al., 2015; Hassan et al., 2018; Wu et al., 2016; He et al., 2017; Xia et al., 2016, 2017; Wu et al., 2018b,a) has become more and more popular given its superior performance without the demand of heavily hand-crafted engineering efforts.",1 Introduction,[0],[0]
"It is usually trained to maximize the likelihood of each token in the target sentence, by taking the source sentence and the preceding (ground-truth) target tokens as inputs.",1 Introduction,[0],[0]
"Such training approach is referred as maximum likelihood estimation (MLE) (Scholz, 1985).",1 Introduction,[0],[0]
"Although easy to implement, the token-level
∗This work was conducted at Microsoft Research Asia.
objective function during training is inconsistent with sequence-level evaluation metrics such as BLEU (Papineni et al., 2002).
",1 Introduction,[0],[0]
"To address the inconsistency issue, reinforcement learning (RL) methods have been adopted to optimize sequence-level objectives.",1 Introduction,[0],[0]
"For example, policy optimization methods such as REINFORCE (Ranzato et al., 2016; Wu et al., 2017b) and actorcritic (Bahdanau et al., 2017) are leveraged for sequence generation tasks including NMT.",1 Introduction,[0],[0]
"In machine translation community, a similar method is proposed with the name ‘minimum risk training’ (Shen et al., 2016).",1 Introduction,[0],[0]
"All these works demonstrate the effectiveness of RL techniques for NMT models (Wu et al., 2016).
",1 Introduction,[0],[0]
"However, effectively applying RL to real-world NMT systems has not been fulfilled by previous works.",1 Introduction,[0],[0]
"First, most of, if not all, previous works verified their methods based on shallow recurrent neural network (RNN) models.",1 Introduction,[0],[0]
"However, to obtain state-of-the-art (SOTA) performance, it is essential to leverage recently derived deep models (Gehring et al., 2017; Vaswani et al., 2017), which are much more powerful.
",1 Introduction,[0],[0]
"Second, it is not easy to make RL practically effective given quite a few widely acknowledged limitations of RL method (Henderson et al., 2018) such as high variance of gradient estimation (Weaver and Tao, 2001), and objective instability (Mnih et al., 2013).",1 Introduction,[0],[0]
"Therefore, several tricks are proposed in previous works.",1 Introduction,[0],[0]
"However, it remains unclear, and no agreement is achieved on how to use these tricks in machine translation.",1 Introduction,[0],[0]
"For example, baseline reward method (Weaver and Tao, 2001) is suggested in (Ranzato et al., 2016; Nguyen et al., 2017; Wu et al., 2016) but not leveraged in (He and Deng, 2012; Shen et al., 2016).
",1 Introduction,[0],[0]
"Third, large-scale datasets, especially monolingual datasets are shown to significantly improve translation quality (Sennrich et al., 2015a; Xia et al.,
2016) with MLE training, while it remains nearly empty on how to combine RL with monolingual data in NMT.
",1 Introduction,[0],[0]
"In this paper, we try to fulfill these gaps and study how to practically apply RL to obtain strong NMT systems with quite competitive, even stateof-the-art performance.",1 Introduction,[0],[0]
"Several comprehensive studies are conducted on different aspects of RL training to figure out how to: 1) set efficient rewards; 2) combine MLE and RL objectives with different weights, which aims to stabilize the training procedure; 3) reduce the variance of gradient estimation.
",1 Introduction,[0],[0]
"In addition, given the effectiveness of leveraging monolingual data in improving translation quality, we further propose a new method to combine the strength of both RL training and source/target monolingual data.",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work that tries to explore the power of monolingual data when training NMT model with RL method.
",1 Introduction,[0],[0]
"We obtain some useful findings through the experiments on WMT17 Chinese-English (Zh-En), WMT17 English-Chinese (En-Zh) and WMT14 English-German (En-De) translation tasks.",1 Introduction,[0],[0]
"For instance, multinomial sampling is better than beam search in reward computation, and the combination of RL and monolingual data significantly enhances the NMT model performance.",1 Introduction,[0],[0]
"Our main contributions are summarized as follows.
",1 Introduction,[0],[0]
"• We provide the first comprehensive study on different aspects of RL training, such as how to setup reward and baseline reward, on top of quite competitive NMT models.
",1 Introduction,[0],[0]
"• We propose a new method that effectively leverages large-scale monolingual data, from both the source and target side, when training NMT models with RL.
",1 Introduction,[0],[0]
"• Combined with several of our findings and method, we obtain the SOTA translation quality on WMT17 Zh-En translation task, surpassing strong baseline (Transformer big model + back translation) by nearly 1.5 BLEU points.",1 Introduction,[0],[0]
"Furthermore, on WMT14 En-De and WMT17 En-Zh translation tasks, we can also obtain strong competitive results.
",1 Introduction,[0],[0]
"We hope that our studies and findings will benefit the community to better understand and leverage reinforcement learning for developing strong
NMT models, especially in real-world scenarios faced with deep models and large amount of training data (including both parallel and monolingual data).",1 Introduction,[0],[0]
"Towards this end, we open source all our codes/dataset at https://github.com/ apeterswu/RL4NMT to provide a clear recipe for performance reproduction.",1 Introduction,[0],[0]
"In this section, we first introduce the attentionbased sequence-to-sequence learning framework for neural machine translation (NMT), and then introduce the basis of applying reinforcement learning to training NMT models.",2 Background,[0],[0]
Typical NMT models are based on the encoderdecoder framework with attention mechanism.,2.1 Neural Machine Translation,[0],[0]
The encoder first maps a source sentence x =,2.1 Neural Machine Translation,[0],[0]
"(x1, x2, ..., xn) to a set of continuous representations z = (z1, z2, ..., zn).",2.1 Neural Machine Translation,[0],[0]
"Given z, the decoder then generates a target sentence y = (y1, y2, ..., ym) of word tokens one by one.",2.1 Neural Machine Translation,[0],[0]
"At each decoding step t of model training, the probability of generating a token yt is maximized conditioned on x and y<t = (y1, ..., yt−1).",2.1 Neural Machine Translation,[0],[0]
"Given N training sentence pairs {xi, yi}Ni=1, maximum likelihood estimation (MLE) is usually adopted to optimize the model, and the training objective is defined as:
Lmle = N∑ i=1",2.1 Neural Machine Translation,[0],[0]
"log p(yi|xi)
=",2.1 Neural Machine Translation,[0],[0]
N∑ i=1,2.1 Neural Machine Translation,[0],[0]
"m∑ t=1 log p(yit|yi1, ..., yit−1, xi), (1)
where m is the length of sentence yi.",2.1 Neural Machine Translation,[0],[0]
"Among all the encoder-decoder models, the recently proposed Transformer (Vaswani et al., 2017) architecture achieves the best translation quality so far.",2.1 Neural Machine Translation,[0],[0]
"The main difference between Transformer and previous RNNSearch (Bahdanau et al., 2015) or ConvS2S (Gehring et al., 2017) is that Transformer relies entirely on self-attention (Lin et al., 2017) to compute representations of source and target side sentences, without using recurrent or convolutional operations.",2.1 Neural Machine Translation,[0],[0]
"As aforementioned, reinforcement learning (RL) is leveraged to bridge the gap between training and
inference of NMT, by directly optimizing the evaluation measure (e.g., BLEU) at training time.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Specifically, NMT model can be viewed as an agent, which interacts with the environment (the previous words y<t and the context vector z available at each step t).",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The parameters of the agent define a policy, i.e., a conditional probability p(yt|x, y<t).",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The agent will pick an action , i.e., a candidate word out from the vocabulary, according to the policy.",2.2 Training NMT with Reinforcement Learning,[0],[0]
A terminal reward is observed once the agent generates a complete sequence ŷ.,2.2 Training NMT with Reinforcement Learning,[0],[0]
"The reward for machine translation is the BLEU (Papineni et al., 2002) score, denoted as R(ŷ, y), which is defined by comparing the generated ŷ with the ground-truth sentence",2.2 Training NMT with Reinforcement Learning,[0],[0]
"y. Note that here the reward R(ŷ, y) is the sentence-level reward, i.e., a scalar for each complete sentence ŷ.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"The goal of the RL training is to maximize the expected reward:
Lrl = N∑ i=1",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Eŷ∼p(ŷ|xi)R(ŷ, y i)
=",2.2 Training NMT with Reinforcement Learning,[0],[0]
"N∑ i=1 ∑ ŷ∈Y p(ŷ|xi)R(ŷ, yi), (2)
where Y is the space of all candidate translation sentences, which is exponentially large due to the large vocabulary size, making it impossible to exactly maximize Lrl.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"In practice, REINFORCE (Williams, 1992) is usually leveraged to approximate the above expectation via sampling ŷ from the policy p(y|x), leading to the objective as maximizing:
L̂rl = N∑ i=1",2.2 Training NMT with Reinforcement Learning,[0],[0]
"R(ŷi, yi), ŷi ∼ p(y|xi),∀i ∈",2.2 Training NMT with Reinforcement Learning,[0],[0]
[N ].,2.2 Training NMT with Reinforcement Learning,[0],[0]
"(3)
Throughout the paper we will use REINFORCE as our policy optimization method for RL training.",2.2 Training NMT with Reinforcement Learning,[0],[0]
"Although training NMT with RL can fill in the gap between training objectives and evaluation metrics, it is not easy to successfully put RL training into practice.",3 Strategies for RL Training,[0],[0]
"A key challenge is that RL methods are highly unstable and inefficient, due to the noise in gradient estimation and reward computation.",3 Strategies for RL Training,[0],[0]
"To our best knowledge, currently there is no consensus, or even a systematic study on how to configure different setups for RL training to avoid such problems, especially for training deep NMT models on large scale datasets.",3 Strategies for RL Training,[0],[0]
"We therefore aim to shed light
on practical applications of RL for NMT training.",3 Strategies for RL Training,[0],[0]
"For this purpose, we provide a comprehensive review of several important methods to stabilize RL training process in this section.",3 Strategies for RL Training,[0],[0]
"It is critical to set up appropriate rewards for RL training, i.e., the R(ŷ, y) in Eqn.",3.1 Reward Computation,[0],[0]
(3).,3.1 Reward Computation,[0],[0]
"There are two important aspects to consider in configuring the reward R(ŷ, y): how to sample training instance ŷ and whether to use reward shaping.
",3.1 Reward Computation,[0],[0]
Generate ŷ,3.1 Reward Computation,[0],[0]
"There are two strategies to sample ŷ for computing the BLEU reward R(ŷ, y).",3.1 Reward Computation,[0],[0]
"The first one is beam search (Sutskever et al., 2014), it is a breadth-first search method that maintains a “beam” of the top-K scoring candidates (prefix hypothesis sentences) at each generation step.",3.1 Reward Computation,[0],[0]
"Then, for each candidate sentence in the beam,K most likely words are appended, resulting in a pool of K ×K new candidates.",3.1 Reward Computation,[0],[0]
"Out from this pool, the top-K translations with largest probabilities are selected, and the beam search process continues.",3.1 Reward Computation,[0],[0]
"The second strategy is multinomial sampling (Chatterjee and Cancedda, 2010), which produces each word one by one through multinomial sampling over the model’s output distribution.",3.1 Reward Computation,[0],[0]
"Both sampling strategies terminate the expansion of a candidate sentence when an ‘end of sentence’ (<EOS>) token is met.
",3.1 Reward Computation,[0],[0]
The choice of different sampling strategies reflects the exploration-exploitation dilemma.,3.1 Reward Computation,[0],[0]
"Beam search strategy generates more accurate ŷ by exploiting the probabilistic space output via current NMT model, while multinomial sampling pays more attention to explore more diverse candidates.
",3.1 Reward Computation,[0],[0]
Whether to Use Reward Shaping From Eqn.,3.1 Reward Computation,[0],[0]
"(3) we can see that for the entire sequence ŷ, there is only one terminal reward R(ŷ, y) available for model training.",3.1 Reward Computation,[0],[0]
"Note that the agent needs to take tens of actions (with the number depending on the length of ŷ) to generate a complete sentence ŷ, but only one reward is available for all those actions.",3.1 Reward Computation,[0],[0]
"Consequently, RL training is inefficient due to the sparsity of rewards, and the model updates each token in the training sentence with the same reward value without distinction.",3.1 Reward Computation,[0],[0]
"Reward shaping (Ng et al., 1999) is a strategy to overcome this shortcoming.",3.1 Reward Computation,[0],[0]
"In reward shaping, intermediate reward at each decoding step t is imposed and denoted as rt(ŷt, y).",3.1 Reward Computation,[0],[0]
"Bahdanau et al. (2017) sets up the intermediate reward as rt(ŷt, y) = R(ŷ1...t, y)",3.1 Reward Computation,[0],[0]
"− R(ŷ1...t−1, y), where R(ŷ1...t, y) is defined as the BLEU score
of ŷ1...t with respect to y. Note that we have R(ŷ, y) = ∑m t=1 rt(ŷt, y), where m is the length
of ŷ. During RL training, the cumulative reward∑m τ=t rτ (ŷτ , y) is used to update the policy at time step t. It is verified that using the shaped reward rt instead of awarding the whole score R(ŷ, y) does not change the optimal policy (Ng et al., 1999).",3.1 Reward Computation,[0],[0]
"As mentioned before, the REINFORCE algorithm suffers from high variance in gradient estimation, mainly caused by using single sample ŷ to estimate the expectation.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"To reduce the variance, Ranzato et al. (2016) subtracts an average reward from the returned reward at each time step t, and the actual reward used to update the policy is
R(ŷ, y)− r̂t, (4)
where r̂t is the estimated average reward at step t, named as baseline reward (Weaver and Tao, 2001).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Together with reward shaping, the updated reward becomes ∑m τ=t rτ (ŷτ , y)− r̂t at step t.
Intuitively speaking, a baseline reward r̂t is established, which either encourages a word choice ŷt if the induced reward R satisfies R > r̂t, or discourages it if R < r̂t.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Here R is either the terminal reward R(ŷ, y) or the cumulative reward∑m
τ=t rτ (ŷτ , y).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"Such estimated baseline reward r̂t is designed to decrease the high variance of the gradient estimator.
",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"In practice, the baseline reward r̂t can be obtained through different approaches.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"For example, one may sample multiple sentences and use the mean terminal reward for these sentences as baseline reward.",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"In our work, we adopt the function learning approach, using simple network (e.g., multi-layer perceptron) to build the learning function, which is the same as used in (Ranzato et al., 2016; Bahdanau et al., 2017).",3.2 Variance Reduction of Gradient Estimation,[0],[0]
"The last important strategy we would like to mention is the combination of MLE training objective with RL objective, which is assumed to further stabilize RL training process (Wu et al., 2016; Li et al., 2017; Wu et al., 2017a).
",3.3 Combine MLE and RL Objectives,[0],[0]
"A simple way is to linearly combine the MLE (Eqn. (1)) and RL (Eqn. (3)) objectives as follows:
Lcom = α ∗",3.3 Combine MLE and RL Objectives,[0],[0]
"Lmle + (1− α) ∗ L̂rl, (5)
where α is the hyperparamter controlling the tradeoff between MLE and RL objectives.",3.3 Combine MLE and RL Objectives,[0],[0]
We will empirically evaluate how different values of α impact the final translation accuracy.,3.3 Combine MLE and RL Objectives,[0],[0]
Previous works typically conduct RL training with only bilingual data for NMT.,4 RL Training with Monolingual Data,[0],[0]
"Monolingual data has been proved to be able to significantly improve the performance of NMT systems (Sennrich et al., 2015a; Xia et al., 2016; Cheng et al., 2016).",4 RL Training with Monolingual Data,[0],[0]
It remains an open problem whether it is possible to combine the benefits of RL training and monolingual data such that even more competitive results can be obtained.,4 RL Training with Monolingual Data,[0],[0]
In this section we provide several solutions for combination and will study them in next section.,4 RL Training with Monolingual Data,[0],[0]
"Note that all the settings discussed in this section are semi-supervised learning, i.e., both bilingual and monolingual data are available.",4 RL Training with Monolingual Data,[0],[0]
We first provide a solution to RL training with source-side monolingual data.,4.1 With Source-Side Monolingual Data,[0],[0]
As shown in Eqn.,4.1 With Source-Side Monolingual Data,[0],[0]
"(3), in RL training we need to calculate the reward signal R(ŷ, y) for each generated sentence ŷ, and therefore the reference sentence y seems to be a must-have, which unfortunately is missing for source-side monolingual data.
",4.1 With Source-Side Monolingual Data,[0],[0]
We tackle this challenge via generating pseudo target reference y by bootstrapping with the model itself.,4.1 With Source-Side Monolingual Data,[0],[0]
"Apparently, for the source-side monolingual data, the pseudo target reference y should have good translation quality.",4.1 With Source-Side Monolingual Data,[0],[0]
"Therefore, for each source-side monolingual sentence, we use the NMT model trained from the bilingual data to beam search a target sentence and treat it as the pseudo target reference y. Afterwards ŷ is obtained via multinomial sampling to calculate the reward.",4.1 With Source-Side Monolingual Data,[0],[0]
"Although multinomial sampling is usually not as good as sampling via beam search, the combination of beam search (to get the pseudo target reference sentence) and the multinomial sampling (to generate the action sequence of the agent) achieves good exploration-exploitation trade-off, since the pseudo target reference exploits the accuracy of current NMT model while ŷ achieves better exploration.",4.1 With Source-Side Monolingual Data,[0],[0]
"For a target-side monolingual sentence, its source sentence x is missing, and consequently ŷ is unavailable since it is sampled based on",4.2 With Target-Side Monolingual Data,[0],[0]
"x. We tackle
this challenge via back translation (Sennrich et al., 2015a).",4.2 With Target-Side Monolingual Data,[0],[0]
We first train a reverse NMT model from the target language to the source language with bilingual data.,4.2 With Target-Side Monolingual Data,[0],[0]
"For each target-side monolingual sentence, using the reverse NMT model, we back translate it to get its pseudo source sentence x.",4.2 With Target-Side Monolingual Data,[0],[0]
"We then pair the target monolingual data and its backtranslated sentence as a pseudo bilingual sentence pair, which can be used for RL training in the same way as the genuine bilingual sentence pairs.",4.2 With Target-Side Monolingual Data,[0],[0]
A natural extension of previous discussions is to combine both the source-side and target-side monolingual data for RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"We consider two combinations, the sequential method and the unified method.",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
The former one sequentially leverages the source-side and target-side monolingual data for RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"Specifically, we first train an MLE model using the bilingual data and source-side (or target-side) monolingual data; based on this MLE model, we then use REINFORCE for training with target-side (or source-side) monolingual data.",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"For unified approach, we pack the paired data out from three domains together: the genuine bilingual data, the source monolingual data with its pseudo target references (introduced in subsection 4.1), and the target monolingual data with its back-translated samples (introduced in subsection 4.2).",4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
Then we treat the combined data as normal bilingual data on which the NMT model is trained via MLE or RL principles.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
Our goal is to investigate the model performance with different training data and find the best recipe of how to use these data in RL training.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
More details are introduced in next section.,4.3 With both Source-Side and Target-Side Monolingual Data,[0],[0]
"In this section, we provide a systematic study on aforementioned RL training strategies and the solutions of leveraging monolingual data.",5 Experiments,[0],[0]
"The RL training strategies are evaluated on bilingual datasets from three translation tasks, WMT14 EnglishGerman (En-De), WMT17 English-Chinese (EnZh) and WMT17 Chinese-English (Zh-En), and we further conduct the experiments to leverage monolingual data in WMT17 Zh-En translation.",5 Experiments,[0],[0]
"For the bilingual datasets, WMT17 (Bojar et al., 2017) En-Zh 1 and WMT17 Zh-En use the same dataset, which contains about 24M sentences pairs, including CWMT Corpus 2017 and UN Parallel Corpus V1.0.",5.1 Experimental Settings,[0],[0]
The Jieba2 segmenter is used to perform Chinese word segmentation.,5.1 Experimental Settings,[0],[0]
"We use byte pair encoding (BPE) (Sennrich et al., 2015b) to preprocess the source and target sentences, forming source-side and target-side dictionary with 40, 000 and 37, 000 types, respectively.",5.1 Experimental Settings,[0],[0]
We use the newsdev2017 as the dev set and newstest2017 as the test set.,5.1 Experimental Settings,[0],[0]
"For the WMT14 En-De dataset, it contains about 4.5M training pairs, newstest2012 and newstest2013 are concatenated as the dev set and newstest2014 acts as test set.",5.1 Experimental Settings,[0],[0]
"Same as (Vaswani et al., 2017), we also perform BPE to process the En-De dataset, the shared source-target vocabulary contains about 37, 000 tokens.
",5.1 Experimental Settings,[0],[0]
"For the monolingual dataset on Zh-En translation task, similar to (Sennrich et al., 2017), the Chinese monolingual data comes from LDC Chinese Gigaword (4th edition) and the English monolingual data comes from News Crawl 2016 articles.",5.1 Experimental Settings,[0],[0]
"After preprocessing (e.g., language detection and filtering sentences with more than 80 words), we keep 4M Chinese sentences and 7M English sentences.
",5.1 Experimental Settings,[0],[0]
"We adopt the Transformer model with transformer big setting as defined in (Vaswani et al., 2017) for Zh-En and En-Zh translations, which achieves SOTA translation quality in several other datasets.",5.1 Experimental Settings,[0],[0]
"For En-De translation, we utilize the transformer base v1 setting.",5.1 Experimental Settings,[0],[0]
"These settings are exactly same as used in the original paper, except we set the layer prepostprocess dropout for Zh-En and En-Zh translation to be 0.05.",5.1 Experimental Settings,[0],[0]
"The optimizer used for MLE training is Adam (Kingma and Ba, 2015) with initial learning rate is 0.1, and we follow the same learning rate schedule in (Vaswani et al., 2017).",5.1 Experimental Settings,[0],[0]
"During training, roughly 4, 096 source tokens and 4, 096 target tokens are paired in one mini batch.",5.1 Experimental Settings,[0],[0]
Each model is trained using 8 NVIDIA Tesla M40 GPUs.,5.1 Experimental Settings,[0],[0]
"For RL training, the model is initialized with parameters of the MLE model (trained with only bilingual data), and we continue training it with learning rate 0.0001.",5.1 Experimental Settings,[0],[0]
"Same as (Bahdanau et al., 2017), to calculate the BLEU reward, we start all n-gram counts from 1 instead of 0 and
1http://www.statmt.org/wmt17/ translation-task.html
2https://github.com/fxsjy/jieba
multiply the resulting score by the length of the target reference sentence.",5.1 Experimental Settings,[0],[0]
"For inference, we use beam search with width 6.",5.1 Experimental Settings,[0],[0]
We run each setting for at least 5 times and report the averaged case sensitive BLEU scores3,5.1 Experimental Settings,[0],[0]
"(Papineni et al., 2002) on test set.",5.1 Experimental Settings,[0],[0]
The test set BLEU is chosen via the best configuration based on the validation set.,5.1 Experimental Settings,[0],[0]
"We first evaluate different strategies for RL training, based only on bilingual datasets from previously introduced three translation tasks.
",5.2 Results of of RL Training Strategies,[0],[0]
"Reward Computation As reviewed in subsection 3.1, for reward computation, we need to consider how to sample ŷ",5.2 Results of of RL Training Strategies,[0],[0]
"and whether to use reward shaping.
",5.2 Results of of RL Training Strategies,[0],[0]
"The results are shown in Table 1, where “RL” stands for RL training with the REINFORCE algorithm.",5.2 Results of of RL Training Strategies,[0],[0]
We also report the performance of the pretrained NMT model with the MLE loss.,5.2 Results of of RL Training Strategies,[0],[0]
"From the table, an interesting finding is that ŷ sampled via beam search strategy is worse than that by multinomial sampling, with a gap of roughly 0.2-0.3 BLEU points on the test set (with significant test score ρ < 0.05).",5.2 Results of of RL Training Strategies,[0],[0]
"We therefore conjecture that exploration is more important than exploitation in reward computing: multinomial sampling brings more data diversity to the training of NMT model, while sentences generated by beam search are usually very similar to each other.",5.2 Results of of RL Training Strategies,[0],[0]
"Furthermore, we find that there is no big difference between the leverage of reward shaping or terminal reward, with only slightly better performance of reward shaping.",5.2 Results of of RL Training Strategies,[0],[0]
"We therefore use multinomial sampling and reward shaping in later experiments.
3Calculated by SacréBLEU toolkit, which produces exactly the same evaluation result as that in WMT17 Zh-En campaign.",5.2 Results of of RL Training Strategies,[0],[0]
https://github.com/awslabs/sockeye/ tree/master/contrib/sacrebleu,5.2 Results of of RL Training Strategies,[0],[0]
Variance Reduction of Gradient Estimation Next we evaluate the strategies for reducing variance of gradient estimation (see section3.2).,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
We want to know whether the baseline reward is necessary.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"To compute the baseline reward, similar to (Ranzato et al., 2016; Bahdanau et al., 2017), we build a two-layer MLP regressor with Relu (Nair and Hinton, 2010) activation units.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The function takes the hidden states from decoder as input, and the parameters of the regressor are trained to minimize the mean squared loss of Eqn. (4).",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"We first pre-train the baseline function for 20k steps/minibatches, and then jointly train NMT model (with RL) and the baseline reward function.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
Table 2 shows that the learning of baseline reward does not help RL training.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"This contradicts with previous observations (Ranzato et al., 2016), and seems to suggest that the variance of gradient estimation in NMT is not as large as we expected.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The reason might be that the probability mass on the target-side language space induced by the NMT model is highly concentrated, making the sampled ŷ representative enough in terms of estimating the expectation.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Therefore, for the economic perspective, it is not necessary to add the additional steps of using baseline reward on RL training for NMT.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Combine MLE and RL Objectives As shown in Eqn. (5), the hyperparameter α controls the trade-off between MLE and RL objectives.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"For comparison, we set α to be [0, 0.1, 0.3, 0.5, 0.7, 0.9] in our experiments.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The results are presented in Figure 1.
",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"The results show that combining the MLE objective with the RL objective achieves better performance (27.48 for En-De, 34.63 for En-Zh and 25.04 for Zh-En with α = 0.3).",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"This indicates that MLE objective is helpful to stabilize the training and improve the model performance, as we expected.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"However, further increasing α does not bring more gain.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
The best trade-off between MLE and RL objectives in our experiment is α = 0.3.,34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"Therefore, we set α = 0.3 in the following experiments.",34.47 34.50 34.63 34.56 34.44 34.40,[0],[0]
"In this subsection, we report the results on both valid and test set of RL training using bilingual and monolingual data in Zh-En translation.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 3 to Table 6, “RL” denotes the model trained with RL using multinomial sampling, reward shaping, no baseline reward, and combined objective, based on the observations in the last subsection.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"“B” denotes bilingual data, “Ms” denotes sourceside monolingual data and “Mt” denotes target-side monolingual data, “&” denotes data combination.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With Source-Side Monolingual Data As discussed before, we use beam search with beam width 4 to sample the pseudo target sentence y for each monolingual sentence x.",5.3 Results of RL Training with Monolingual Data,[0],[0]
We consider several settings for RL training: 1) only source-side monolingual data; 2) the combination of bilingual and source-side monolingual data.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"We first train an MLE model using the augmented dataset combining the genuine bilingual data with the pseudo bilingual data generated from the monolingual data, and then perform RL training on this combined dataset.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are shown in Table 3.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With Target-Side Monolingual Data For target-side monolingual data, we first pre-train a translation model from English to Chinese 4, and use it to back translate target-side monolingual
4The BLEU score of the En-Zh model is 34.12.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
sentence y to get pseudo source sentence,5.3 Results of RL Training with Monolingual Data,[0],[0]
x.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"Similarly, we consider several settings for RL training: 1) only target-side monolingual data; 2) the combination of bilingual data and target-side monolingual data.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"We train an MLE model using both the genuine and the generated pseudo bilingual data, and then perform RL training on this data.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are presented in Table 4.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 3 and 4, we have several observations.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"First, monolingual data helps RL training, improving BLEU score from 25.04 to 25.22 (ρ < 0.05) in Table 3.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"Second, when we only add monolingual data for RL training, the model achieves similar performance compared to MLE training with bilingual and monolingual data (e.g., 25.15 vs. 25.24 (ρ < 0.05) in Table 4).
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"With both Source-Side and Target-Side Monolingual Data We have two approaches to use both source-side and target-side monolingual data, as described in subsection 4.3.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"The results are reported in Table 5 and Table 6.
",5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 5, we can observe that the sequen-
tial training of monolingual data can benefit the model performance.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"Taking the last three rows as an example, the BLEU score of the MLE model trained on the combination of bilingual data and target-side monolingual data is 25.24; based on this model, RL training using the source-side monolingual data further improves the model performance by 0.7 (ρ < 0.01)",5.3 Results of RL Training with Monolingual Data,[0],[0]
BLEU points.,5.3 Results of RL Training with Monolingual Data,[0],[0]
"From Table 6, we can observe on top of a quite strong MLE baseline (26.13), through the unified RL training, we can still improve the test set by 0.6 points to 26.73 (ρ < 0.01), which shows the effectiveness of combining source/target monolingual data and reinforcement learning.",5.3 Results of RL Training with Monolingual Data,[0],[0]
"At last, as a summary of our empirical results, we compare several representataive end-to-end NMT systems to our work in Table 7, which includes the Transformer (Vaswani et al., 2017) model, with/without back-translation (Sennrich et al., 2015a) and the best NMT system in WMT17 Chinese-English translation challenge5 (SougouKnowing-ensemble).",5.4 Comparison with Other Models,[0],[0]
"The results clearly show that after combing both source-side and target-side monolingual data with RL training, we obtain the state-of-the-art BLEU score 26.73, even surpassing the best ensemble model in WMT17 Zh-En translation challenge.",5.4 Comparison with Other Models,[0],[0]
Our work is mainly related with the literature of using reinforcement learning to directly optimize the evaluation measure for neural machine translation.,6 Related Work,[0],[0]
"Several representative works are (Ranzato et al.,
5http://matrix.statmt.org/matrix/ systems_list/1878
2016; Shen et al., 2016; Bahdanau et al., 2017).",6 Related Work,[0],[0]
"In (Ranzato et al., 2016), the authors propose to train a neural translation model with the objective gradually shifting from maximizing token-level likelihood to optimizing the sentence-level BLEU score.",6 Related Work,[0],[0]
"Shen et al. (2016) proposes to adopt minimum risk training (Goel and Byrne, 2000) to minimize the task specific expected loss (i.e., induced by BLEU score) on NMT training data.",6 Related Work,[0],[0]
"Instead of the REINFORCE (Williams, 1992) algorithm used in the above two works, Bahdanau et al. (2017) further optimizes the policy by actor-critic algorithm.",6 Related Work,[0],[0]
"Wu et al. (2016) introduces a simple RL based method to optimize the stacked LSTM model for NMT, achieving better BLEU scores on English-French translation but not on English-German.",6 Related Work,[0],[0]
"Edunov et al. (2017) presents a comparative study of several classical structural prediction losses for NMT model, which also includes sequence-level loss but not exactly the same as RL.
",6 Related Work,[0],[0]
"Our work is also related with the research works that leverage monolingual data for improving NMT models (Zhang and Zong, 2016; Sennrich et al., 2015a; Wang et al., 2018; Xia et al., 2016; Cheng et al., 2016).",6 Related Work,[0],[0]
Zhang and Zong (2016) exploits the source-side monolingual data in NMT.,6 Related Work,[0],[0]
Sennrich et al. (2015a) proposes back-translation method to leverage target-side monolingual data for NMT.,6 Related Work,[0],[0]
"Xia et al. (2016) formulates the machine translation as a communication game, which leverages the power of two directional translation models and source/target monolingual data.",6 Related Work,[0],[0]
Cheng et al. (2016) proposes a similar semi-supervised approach.,6 Related Work,[0],[0]
"However, none of these works have explored the power of monolingual data in the context of training NMT model with reinforcement learning.",6 Related Work,[0],[0]
"In this work, we presented a study of how to effectively train NMT models using reinforcement learning.",7 Conclusion,[0],[0]
"Different RL strategies were evaluated in German-English, English-Chinese and ChineseEnglish translation tasks on large-scale bilingual datasets.",7 Conclusion,[0],[0]
"We found that (1) multinomial sampling is better than beam search, (2) several previous tricks such as reward shaping and baseline reward does not make significant difference, and (3) the combination of the MLE and RL objectives is important.",7 Conclusion,[0],[0]
"In addition, we explored the source/target monolingual data for RL training.",7 Conclusion,[0],[0]
"By combing the power of RL and monolingual data, we achieve the state-of-the-art BLEU score on WMT17 ChineseEnglish translation task.",7 Conclusion,[0],[0]
We hope that our study and results can benefit the community and bring some insights on how to train deep NMT models with reinforcement learning and big data.,7 Conclusion,[0],[0]
Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system.,abstractText,[0],[0]
"However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged.",abstractText,[0],[0]
"In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning.",abstractText,[0],[0]
"We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training.",abstractText,[0],[0]
"Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data.",abstractText,[0],[0]
"By integrating all our findings, we obtain competitive results on WMT14 EnglishGerman, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.",abstractText,[0],[0]
A Study of Reinforcement Learning for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2814–2819 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Automatically analyzing and generating natural language requires capturing not only what is said, but also how to say it.",1 Introduction,[0],[0]
Consider the sentences “anybody hurt?”,1 Introduction,[0],[0]
and “is someone wounded?”.,1 Introduction,[0],[0]
"The first one is less formal than the second one, and carries information beyond its literal meaning, such as the situation in which it might be used.",1 Introduction,[0],[0]
"Such differences in formality have been identified as an important dimension of style (Trudgill, 1992) or tone (Halliday, 1978) variation.
",1 Introduction,[0],[0]
"In this paper, we build on prior computational work that has focused on analyzing formality of texts (Lahiri and Lu, 2011; Brooke and Hirst, 2013; Pavlick and Nenkova, 2015; Pavlick and Tetreault, 2016) with a different aim: modeling formality for the purpose of controlling style in applications that generate language, with a focus on machine translation.",1 Introduction,[0],[0]
"Human translators translate a document for a specific audience (Nida and Taber Charles, 1969), and often ask what is the expected tone of the content when taking a new translation job.",1 Introduction,[0],[0]
We design a machine translation system that operates under similar conditions and explicitly takes an expected level of formality as input.,1 Introduction,[0],[0]
"While ultimately we would like systems to preserve the formality of the source, this is a
challenging task that requires not only automatically inferring the formality of the source, but also understanding how formality differs across languages and cultures.",1 Introduction,[0],[0]
"As a first step, we therefore limit our study to the scenario where the expected output formality is given to the MT system as an additional input.
",1 Introduction,[0],[0]
We first select a formality model providing the most accurate scores on intrinsic formality datasets.,1 Introduction,[0],[0]
We compare existing lexical formality models and novel variants based on inducing formality dimensions or subspaces in vector space models.,1 Introduction,[0],[0]
We then turn to machine translation and show that a lexical formality model can have a positive impact when used to control the formality of machine translation output.,1 Introduction,[0],[0]
"When the expected formality matches the reference, we obtain improvement of translation quality evaluated by automatic metrics (BLEU).",1 Introduction,[0],[0]
A human assessment also verified the effectiveness of our proposed system in generating translations at diverse levels of formality.,1 Introduction,[0],[0]
Our goal is to provide systems with the ability to generate language across a range of formality style.,2 Formality-Sensitive MT,[0],[0]
"We propose a Formality-Sensitive Machine Translation (FSMT) scenario where the system takes two inputs: (1) text in the source language to be translated, and (2) a desired formality level capturing the intended audience of the translation.",2 Formality-Sensitive MT,[0],[0]
We propose to implement it as n-best re-ranking within a standard phrase-based MT architecture.,2 Formality-Sensitive MT,[0],[0]
"Unlike domain adaptation approaches, which aim to produce domain-specific or potentially formality-specific systems, our goal is to obtain a single system trained on diverse data which can adaptively produce output for a range of styles.
",2 Formality-Sensitive MT,[0],[0]
"We therefore introduce a formality-scoring fea-
2814
ture for re-ranking.",2 Formality-Sensitive MT,[0],[0]
"For each translation hypothesis h, given the formality level ` as a parameter:
f(h; `) = |Formality(h)−",2 Formality-Sensitive MT,[0],[0]
"`|
where Formality(h) is the sentence-level formality score for h. f(h; `), along with standard model features, is fed into a standard re-ranking model.",2 Formality-Sensitive MT,[0],[0]
"When training the re-ranking model, the parameter ` is set to the actual formality score of the reference translation for each instance.",2 Formality-Sensitive MT,[0],[0]
"At test time, ` is provided by the user.",2 Formality-Sensitive MT,[0],[0]
The re-scoring weights help promote candidate sentences whose formality scores approach the expected level.,2 Formality-Sensitive MT,[0],[0]
The FSMT system requires quantifying the formality level of a sentence.,3 Formality Modeling,[0],[0]
"Following prior work, we define sentence-level formality based on lexical formality scores (Brooke et al., 2010; Pavlick and Nenkova, 2015).",3 Formality Modeling,[0],[0]
"We conduct an empirical comparison of existing techniques that can be adapted as lexical formality models, and introduce a sentence-level formality scheme based on weighted average.",3 Formality Modeling,[0],[0]
"State-of-the-art lexical formality models (Brooke et al., 2010; Brooke and Hirst, 2014) are based on vector space models of word meaning, and a set of pre-selected seed words that are representative of formal and informal language.
",3.1 Lexical Formality,[0],[0]
SimDiff Brooke et al. (2010) proposed to score the formality of a word w by comparing its meaning to that of seed words of known formality using cosine similarity.,3.1 Lexical Formality,[0],[0]
"Intuitively, w is more likely formal if it is semantically closer to formal seed words than to informal seed words.",3.1 Lexical Formality,[0],[0]
"Formally, given a formal word set Sf and an informal word set Si, SimDiff scores a word w by
score(w) = 1 |Sf | ∑ v∈Sf cos(ew, ev)− 1|Si| ∑ v∈Si cos(ew, ev)
",3.1 Lexical Formality,[0],[0]
Turning this difference into a formality score requires further manipulation.,3.1 Lexical Formality,[0],[0]
A neutral word r has to be manually selected to anchor the midpoint of the formality score range.,3.1 Lexical Formality,[0],[0]
"In other words, the final formality score for r is enforced to be zero:
Formality(w) =",3.1 Lexical Formality,[0],[0]
score(w)− score(r),3.1 Lexical Formality,[0],[0]
"normalizer(w, r)
",3.1 Lexical Formality,[0],[0]
The neutral word is typically selected from function words.,3.1 Lexical Formality,[0],[0]
We select “at” because it appears in nearly every document and appears with nearly equivalent probabilities in formal/informal corpora.,3.1 Lexical Formality,[0],[0]
"Finally, a normalizer which is maximized among the whole vocabulary ensures that scores cover the entire [−1, 1] range.
",3.1 Lexical Formality,[0],[0]
"Instead of using cosine diff as the score function score(w), other standard techniques can be also applied under this framework.
",3.1 Lexical Formality,[0],[0]
SVM,3.1 Lexical Formality,[0],[0]
"As an alternative to the model proposed by Brooke and Hirst (2014), we propose to train an Support Vector Machine (SVM) model to find a hyperplane that separates formal and informal words and define the score function as the distance to the hyperplane.
",3.1 Lexical Formality,[0],[0]
Formality Subspace Another category of methods consists in identifying a subspace that captures formality within the original vector space.,3.1 Lexical Formality,[0],[0]
Lexical scores can then simply be obtained by projecting word representations onto the formality subspace.,3.1 Lexical Formality,[0],[0]
One example is training a Principal Component Analysis (PCA) model on word representations of all seeds.,3.1 Lexical Formality,[0],[0]
This method is based on the assumption that representative formal/informal words principally vary along the direction of formality.,3.1 Lexical Formality,[0],[0]
"Alternatively, inspired by DENSIFIER (Rothe et al., 2016), we can learn a subspace that aims at separating words in Sf vs. words in Si and grouping words in the same set.",3.1 Lexical Formality,[0],[0]
"While previous work scored sentence by averaging word scores (Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), we propose a weighted average scheme for word sequences W to downgrade the formality contribution of neutral words:
Formality(W ) =∑ wi∈W |Formality(wi)| · Formality(wi)∑
wi∈W |Formality(wi)|",3.2 From Word to Sentence Formality,[0],[0]
"Before evaluating our FSMT framework, we evaluate the formality models at the sentence level.",3.3 Evaluation,[0],[0]
"Lahiri (2015) and Pavlick and Tetreault (2016) collected 5-way human scores for 11,263 sentences in the genres of blog, email, answers and news.",3.3 Evaluation,[0],[0]
"Following Pavlick and Tetreault (2016), we averaged human scores for each sentence as the
gold standard.",3.3 Evaluation,[0],[0]
"As in prior work, the score quality was evaluated by the Spearman correlation.
",3.3 Evaluation,[0],[0]
A large mixed-topic corpus is required to train vector space models.,3.3 Evaluation,[0],[0]
"As suggested by Brooke et al. (2010), we used the ICWSM 2009 Spinn3r dataset (English tier-1) which consists of about 1.6 billion words (Burton et al., 2009).",3.3 Evaluation,[0],[0]
We also compared the term-document association model,3.3 Evaluation,[0],[0]
"Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and the term-term association model word2vec (W2V) (Mikolov et al., 2013).",3.3 Evaluation,[0],[0]
"We used the same 105 formal seeds and 138 informal seeds as Brooke et al. (2010).
",3.3 Evaluation,[0],[0]
"Followed Brooke et al. (2010), to achieve best performance, we used a small dimensionality (10) for training LSA and word2vec.",3.3 Evaluation,[0],[0]
"In practice, we normalized the LSA word vectors to make them have unit length for SVM and PCA, but did not applied it to word2vec.",3.3 Evaluation,[0],[0]
"This suggests that the magnitude of LSA word vectors is harmful for formality modeling.
",3.3 Evaluation,[0],[0]
"We also compared formality models based on word representations to a baseline that relies on unigram models to compare word statistics in corpora representative of formal vs. informal language (Pavlick and Nenkova, 2015).",3.3 Evaluation,[0],[0]
This method requires language examples of diverse formality.,3.3 Evaluation,[0],[0]
"Conversational transcripts are generally considered as casual text, so we concatenated corpora such as Fisher (Cieri et al., 2004), Switchboard (Godfrey et al., 1992), SBCSAE (Bois et al., 2000- 2005), CallHome1, CallFriend2, BOLT",3.3 Evaluation,[0],[0]
"SMS/Chat (Song et al., 2014) and NPS Chatroom (Forsythand and Martell, 2007).",3.3 Evaluation,[0],[0]
"As the formal counterpart, we extracted comparable size of text from Europarl (Koehn, 2005).",3.3 Evaluation,[0],[0]
"This results in 30 Million tokens of formal corpora (1.1M segments) and 29 Million tokens of informal corpora (2.7M segments).
",3.3 Evaluation,[0],[0]
Table 1 shows that all models based on the vector space achieve similar performance in terms of Spearman’s ρ (except SVM-W2V which yields lower performance).,3.3 Evaluation,[0],[0]
The baseline method based on unigram models was outperformed by 0.1+ point.,3.3 Evaluation,[0],[0]
"So we select DENSIFIER-LSA as a representative for our FSMT system.
",3.3 Evaluation,[0],[0]
"1https://catalog.ldc.upenn.edu/ LDC97S42
2https://talkbank.org/access/CABank/ CallFriend/",3.3 Evaluation,[0],[0]
Set-up We evaluate this approach on a French to English translation task.,4 Evaluation of the FSMT System,[0],[0]
"Two parallel FrenchEnglish corpora are used: (1) MultiUN (Eisele and Chen, 2010), which is extracted from the United Nations website, and can be considered to be formal text; (2) OpenSubtitles2016 (Lison and Tiedemann, 2016), which is extracted from movie and TV subtitles, covers a wider spectrum of styles, but overall tends to be informal since it primarily contains conversations.",4 Evaluation of the FSMT System,[0],[0]
"Each parallel corpus was split into a training set (100M English tokens), a tuning set (2.5K segments) and a test set (5K segments).",4 Evaluation of the FSMT System,[0],[0]
"Two corpora are then concatenated, such that training, tuning and test sets all contained a diversity of styles.",4 Evaluation of the FSMT System,[0],[0]
"Moses (Koehn et al., 2007) is used to build our phrase-based MT system.",4 Evaluation of the FSMT System,[0],[0]
"We followed the standard training pipeline with default parameters.3 Word alignments were generated using fast align (Dyer et al., 2013), and symmetrized using the grow-diag-final-and heuristic.",4 Evaluation of the FSMT System,[0],[0]
"We used 4-gram language models, trained using KenLM (Heafield, 2011).",4 Evaluation of the FSMT System,[0],[0]
"Model weights were tuned using batch MIRA (Cherry and Foster, 2012).
",4 Evaluation of the FSMT System,[0],[0]
We used constant size n=1000 for n-best lists in all experiments.,4 Evaluation of the FSMT System,[0],[0]
The re-ranking is a log-linear model trained using batch MIRA.,4 Evaluation of the FSMT System,[0],[0]
"4 We report results averaged over 5 random tuning re-starts to compensate for tuning noise (Clark et al., 2011).
",4 Evaluation of the FSMT System,[0],[0]
"FSMT In order to evaluate the impact of different input formality (e.g. low/neutral/high) on translation quality, ideally, we would like to have three human reference translations with different
3http://www.statmt.org/moses/?n=Moses.",4 Evaluation of the FSMT System,[0],[0]
"Baseline
4https://github.com/moses-smt/ mosesdecoder/tree/master/scripts/ nbest-rescore
formality for each source sentence.",4 Evaluation of the FSMT System,[0],[0]
"Since such references are not available, we construct three sets of test data where instances are divided according to the formality level of the available reference translation.",4 Evaluation of the FSMT System,[0],[0]
"The formality distribution in the tuning set shows that 97% reference translations fall into the range of [−0.6, 0.6].",4 Evaluation of the FSMT System,[0],[0]
"We therefore set three formality bins – informal [−1,−0.2), neutral formality",4 Evaluation of the FSMT System,[0],[0]
"[−0.2, 0.2], and formal (0.2, 1] – and split the test set into these bins.",4 Evaluation of the FSMT System,[0],[0]
"We use DENSIFIER-LSA and training setting described above to translate the entire test set three times, with three different formality levels: low (-0.4), neutral (0) and high (0.4).",4 Evaluation of the FSMT System,[0],[0]
"We first report standard automatic evaluation results using the BLEU score to compare FSMT output given different desired formality level on each bins (See Table 2).
",4.1 Automatic Evaluation,[0],[0]
"The best BLEU scores for each formality level are obtained when the level of formality given as input to the MT system matches the nature of the text being translated, as can be seen in the scores along the diagonal in Table 2.",4.1 Automatic Evaluation,[0],[0]
"Comparing with the baseline system, which produces the top translation from each n-best list, translation quality improves by +0.5 BLEU on informal text, +0.3 BLEU on neutral text, and remains constant on formal text.",4.1 Automatic Evaluation,[0],[0]
The impact increases with the distance to formal language increases.,4.1 Automatic Evaluation,[0],[0]
"This can be explained by the fact that more formal sentences tend to be longer, and the impact of alternate lexical choice for a small number of words per sentence is smaller in longer sentences.",4.1 Automatic Evaluation,[0],[0]
"In addition, the formal sentences are mostly drawn from UN data which is sufficiently different from the other genres in the heterogeneous training corpus that the informal examples do not affect baseline per-
formance on formal data.",4.1 Automatic Evaluation,[0],[0]
Automatic evaluation is limited to comparing output to a single reference: lower BLEU scores conflate translation errors and stylistic mismatch.,4.2 Human Assessment,[0],[0]
"Therefore, we conduct a human study of the formality vs. the quality.
",4.2 Human Assessment,[0],[0]
We conducted a manual evaluation of the output of our FSMT system taking low/high formality levels (-0.4/0.4) as parameters.,4.2 Human Assessment,[0],[0]
42 translation pairs were randomly selected and were annotated by 15 volunteers.,4.2 Human Assessment,[0],[0]
"For each pair of segments, the volunteers were asked to select the segment that would be more appropriate in a formal setting (e.g., a job interview) than in a casual setting (e.g., chatting with friends).",4.2 Human Assessment,[0],[0]
"A default option of “neither of them is more formal or hard to say” was also available.
",4.2 Human Assessment,[0],[0]
"By majority voting, 20 pairs were annotated as “N”, indicating the two translations has no distinctions w.r.t. formality.",4.2 Human Assessment,[0],[0]
"For example, “A: how can they do this” vs. “B: how can they do that”.",4.2 Human Assessment,[0],[0]
"Given that the translations were restricted to the nbest list, not all sentences could be translated into stylistically different language.
",4.2 Human Assessment,[0],[0]
"Of the remaining 21 pairs where annotators judged one output more formal than the other, in all but one case the translation produced by our FSMT system with high formality level parameter was judged to be more formal.",4.2 Human Assessment,[0],[0]
"Overall this indicates that our formality scoring and ranking procedure are effective.
To determine whether re-ranking based on formality might have a detrimental effect on quality, we also had annotators rate the fluency and adequacy of the segments.",4.2 Human Assessment,[0],[0]
"Inspired by Graham et al. (2013), annotators were first asked to assess fluency without a reference and separately adequacy with a reference.",4.2 Human Assessment,[0],[0]
Both assessments used a sliding scale.,4.2 Human Assessment,[0],[0]
Each segment was evaluated by an average of 7 annotators.,4.2 Human Assessment,[0],[0]
"After rescaling the ratings into the [0, 1] range, we observed a 0.75 level of fluency for informal translations and 0.70 for formal ones.",4.2 Human Assessment,[0],[0]
This slight difference fits our expectation that more casual language may feel more fluent while more formal language may feel more stilted.,4.2 Human Assessment,[0],[0]
"The adequacy ratings were 0.65 and 0.64 for informal and translations respectively, indicating that adjusting the level of formality had minimal effect on the adequacy of the result.
",4.2 Human Assessment,[0],[0]
Some examples are listed in Table 3.,4.2 Human Assessment,[0],[0]
"Occa-
sionally, the n-best list had no translation hypotheses with diverse formality, so the FSMT system dropped necessary words, appended inessential words, or selected improper or even incorrect words to fit the target formality level.",4.2 Human Assessment,[0],[0]
"In the case of ’how do you do’, the translation that was meant to be more casual was rated more formal.",4.2 Human Assessment,[0],[0]
"Because the system measures formality on the lexical level, it was not able to recognize this idiomatically formal phrase made up of words that are not inherently formal.",4.2 Human Assessment,[0],[0]
"Despite these issues, most of the output were formality-variant translations of the same French source segment, as expected.",4.2 Human Assessment,[0],[0]
"We presented a framework for formality-sensitive machine translation, where a system produces translations at a desired formality level.",5 Conclusion,[0],[0]
Our evaluation shows the effectiveness of this system in controlling language formality without loss in translation quality.,5 Conclusion,[0],[0]
"Stylistic variations of language, such as formality, carry speakers’ intention beyond literal meaning and should be conveyed adequately in translation.",abstractText,[0],[0]
We propose to use lexical formality models to control the formality level of machine translation output.,abstractText,[0],[0]
"We demonstrate the effectiveness of our approach in empirical evaluations, as measured by automatic metrics and human assessments.",abstractText,[0],[0]
A Study of Style in Machine Translation: Controlling the Formality of Machine Translation Output,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 231–240 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
231",text,[0],[0]
"The media and the public are currently discussing the recent phenomenon of “fake news” and its potential role in swaying elections, how it may affect society, and what can and should be done about it.",1 Introduction,[0],[0]
"Prone to misunderstanding and misue, the term “fake news” arose from the observation that, in social media, a certain kind of ‘news’ spreads much more successfully than others, and this kind of ‘news’ is typically extremely one-sided (hyperpartisan), inflammatory, emotional, and often riddled with untruths.",1 Introduction,[0],[0]
"Although traditional yellow press has been spreading ‘news’ of varying de-
grees of truthfulness long before the digital revolution, its amplification over real news within social media gives many people pause.",1 Introduction,[0],[0]
"The fake news hype caused a widespread disillusionment about social media, and many politicians, news publishers, IT companies, activists, and scientists concur that this is where to draw the line.",1 Introduction,[0],[0]
"For all their good intentions, however, it must be drawn very carefully (if at all), since nothing less than free speech is at stake—a fundamental right of every free society.
",1 Introduction,[0],[0]
Many favor a two-step approach where fake news items are detected and then countermeasures are implemented to foreclose rumors and to discourage repetition.,1 Introduction,[0],[0]
"While some countermeasures are already tried in practice, such as displaying warnings and withholding ad revenue, fake news detection is still in its infancy.",1 Introduction,[0],[0]
"At any rate, a nearreal time reaction is crucial: once a fake news item begins to spread virally, the damage is done and undoing it becomes arduous.",1 Introduction,[0],[0]
"Since knowledge-based and context-based approaches to fake news detection can only be applied after publication, i.e., as news events unfold and as social interactions occur, they may not be fast enough.
",1 Introduction,[0],[0]
"We have identified style-based approaches as a viable alternative, allowing for instantaneous reactions, albeit not to fake news, but to hyperpartisanship.",1 Introduction,[0],[0]
"In this regard we contribute (1) a large news corpus annotated by experts with respect to veracity and hyperpartisanship, (2) extensive experiments on discriminating fake news, hyperpartisan news, and satire based solely on writing style, and (3) validation experiments to verify our finding that the writing style of the left and the right have more in common than any of the two have with the mainstream, applying Unmasking in a novel way.
",1 Introduction,[0],[0]
"After a review of related work, Section 3 details the corpus and its construction, Section 4 introduces our methodology, and Section 5 reports the results of the aforementioned experiments.",1 Introduction,[0],[0]
"Approaches to fake news detection divide into three categories (Figure 1): they can be knowledge-based (by relating to known facts), context-based (by analyzing news spread in social media), and stylebased (by analyzing writing style).
",2 Related Work,[0],[0]
Knowledge-based fake news detection.,2 Related Work,[0],[0]
Methods from information retrieval have been proposed early on to determine the veracity of web documents.,2 Related Work,[0],[0]
"For example, Etzioni et al. (2008) propose to identify inconsistencies by matching claims extracted from the web with those of a document in question.",2 Related Work,[0],[0]
"Similarly, Magdy and Wanas (2010) measure the frequency of documents that support a claim.",2 Related Work,[0],[0]
"Both approaches face the challenges of web data credibility, namely expertise, trustworthiness, quality, and reliability (Ginsca et al., 2015).
",2 Related Work,[0],[0]
"Other approaches rely on knowledge bases, including the semantic web and linked open data.",2 Related Work,[0],[0]
"Wu et al. (2014) “perturb” a claim in question to query knowledge bases, using the result variations as indicator of the support a knowledge base offers for the claim.",2 Related Work,[0],[0]
"Ciampaglia et al. (2015) use the shortest path between concepts in a knowledge graph, whereas Shi and Weninger (2016) use a link prediction algorithm.",2 Related Work,[0],[0]
"However, these approaches are unsuited for new claims without corresponding entries in a knowledge base, whereas knowledge bases can be manipulated (Heindorf et al., 2016).
",2 Related Work,[0],[0]
Context-based fake news detection.,2 Related Work,[0],[0]
"Here, fake news items are identified via meta information and spread patterns.",2 Related Work,[0],[0]
"For example, Long et al. (2017) show that author information can be a useful feature for fake news detection, and Derczynski et al. (2017) attempt to determine the veracity of a claim based on the conversation it sparks on Twitter as one of the RumourEval tasks.",2 Related Work,[0],[0]
"The Facebook analysis of Mocanu et al. (2015) shows that unsubstantiated claims spread as widely as well-established ones, and that user groups predisposed to conspiracy theories are more open to sharing the former.",2 Related Work,[0],[0]
"Similarly, Acemoglu et al. (2010), Kwon et al. (2013), Ma et al. (2017), and Volkova et al. (2017) model the spread of (mis-)information, while Budak et al. (2011) and Nguyen et al. (2012) propose algorithms to limit its spread.",2 Related Work,[0],[0]
The efficacy of countermeasures like debunking sites is studied by Tambuscio et al. (2015).,2 Related Work,[0],[0]
"While achieving good results, context-based approaches suffer from working only a posteriori, requiring large amounts of data, and disregarding the actual news content.
",2 Related Work,[0],[0]
"Fake news detection
Style-based fake news detection.",2 Related Work,[0],[0]
"Deception detection originates from forensic linguistics and builds on the Undeutsch hypothesis—a result from forensic psychology which asserts that memories of reallife, self-experienced events differ in content and quality from imagined events (Undeutsch, 1967).",2 Related Work,[0],[0]
The hypothesis led to the development of forensic tools to assess testimonies at the statement level.,2 Related Work,[0],[0]
"Some approaches operationalize deception detection at scale to detect uncertainty in social media posts, for example Wei et al. (2013) and Chen et al. (2015).",2 Related Work,[0],[0]
"In this regard, Rubin et al. (2015) use rhetorical structure theory as a measure of story coherence and as an indicator for fake news.",2 Related Work,[0],[0]
"Recently, Wang (2017) collected a large dataset consisting of sentence-length statements along their veracity from the fact-checking site PolitiFact.com, and then used style features to detect false statements.",2 Related Work,[0],[0]
"A related task is stance detection, where the goal is to detect the relation between a claim about an article, and the article itself (Bourgonje et al., 2017).",2 Related Work,[0],[0]
"Most prominently, stance detection was the task of the Fake News Challenge1 which ran in 2017 and received 50 submissions, albeit hardly any participants published their approach.",2 Related Work,[0],[0]
"1http://www.fakenewschallenge.org/
Where deception detection focuses on single statements, style-based text categorization as proposed by Argamon-Engelson et al. (1998) assesses entire texts.",2 Related Work,[0],[0]
"Common applications are author profiling (age, gender, etc.) and genre classification.",2 Related Work,[0],[0]
"Though susceptible to authors who can modify their writing style, such obfuscations may be detectable (e.g., Afroz et al. (2012)).",2 Related Work,[0],[0]
"As an early precursor to fake news detection, Badaskar et al. (2008) train models to identify news items that were automatically generated.",2 Related Work,[0],[0]
"Currently, text categorization methods for fake news detection focus mostly on satire detection (e.g., Rubin et al. (2016), Yang et al. (2017)).",2 Related Work,[0],[0]
"Rashkin et al. (2017) perform a statistical analysis of the stylistic differences between real, satire, hoax, and propaganda news.",2 Related Work,[0],[0]
"We make use of their results by incorporating the bestperforming style features identified.
",2 Related Work,[0],[0]
"Finally, two preprint papers have been recently shared.",2 Related Work,[0],[0]
Horne and Adali (2017) use style features for fake news detection.,2 Related Work,[0],[0]
"However, the relatively high accuracies reported must be taken with a grain of salt: their two datasets comprise only 70 news articles each, whose ground-truth is based on where an article came from, instead of resulting from a per-article expert review as in our case; their final classifier uses only 4 features (number of nouns, type-token ratio, word count, number of quotes), which can be easily manipulated; and based on their experimental setup, it cannot be ruled out that the classifier simply differentiates news portals rather than fake and real articles.",2 Related Work,[0],[0]
We avoid this problem by testing our classifiers on articles from portals which were not represented in the training data.,2 Related Work,[0],[0]
"Similarly, Pérez-Rosas et al. (2017) also report on constructing two datasets comprising around 240 and 200 news article excerpts (i.e., the 5-sentence lead) with a balanced distribution of fake vs. real.",2 Related Work,[0],[0]
"The former was collected via crowdsourcing, asking workers to write a fake news item based on a real news item, the latter was collected from the web.",2 Related Work,[0],[0]
"For style analysis, the former dataset may not be suitable, since the authors note themselves that “workers succeeded in mimicking the reporting style from the original news”.",2 Related Work,[0],[0]
"The latter dataset encompasses only celebrity news (i.e., yellow press), which introduces a bias.",2 Related Work,[0],[0]
"Their feature selection follows that of Rubin et al. (2016), which is covered by our experiments, but also incorporates topic features, rendering the resulting classifier not generalizable.",2 Related Work,[0],[0]
"This section introduces the BuzzFeed-Webis Fake News Corpus 2016, detailing its construction and annotation by professional journalists employed at BuzzFeed, as well as key figures and statistics.2",3 The BuzzFeed-Webis Fake News Corpus,[0],[0]
"The corpus encompasses the output of 9 publishers on 7 workdays close to the US presidential elections 2016, namely September 19 to 23, 26, and 27.",3.1 Corpus Construction,[0],[0]
Table 1 gives an overview.,3.1 Corpus Construction,[0],[0]
"Among the selected publishers are six prolific hyperpartisan ones (three left-wing and three right-wing), and three mainstream ones.",3.1 Corpus Construction,[0],[0]
"All publishers earned Facebook’s blue checkmark , indicating authenticity and an elevated status within the network.",3.1 Corpus Construction,[0],[0]
"Every post and linked news article has been fact-checked by 4 BuzzFeed journalists, including about 19% of posts forwarded from third parties.",3.1 Corpus Construction,[0],[0]
"Having checked a total of 2,282 posts, 1,145 mainstream, 471 leftwing, and 666 right-wing, Silverman et al. (2016) reported key insights as a data journalism article.",3.1 Corpus Construction,[0],[0]
The annotations were published alongside the article.3,3.1 Corpus Construction,[0],[0]
"However, this data only comprises URLs to the original Facebook posts.",3.1 Corpus Construction,[0],[0]
"To construct our corpus, we archived the posts, the linked articles, and attached media as well as relevant meta data to ensure long-term availability.",3.1 Corpus Construction,[0],[0]
"Due to the rapid pace at which the publishers change their websites, we were able to recover only 1,627 articles, 826 mainstream, 256 left-wing, and 545 right-wing.
",3.1 Corpus Construction,[0],[0]
Manual fact-checking.,3.1 Corpus Construction,[0],[0]
"A binary distinction between fake and real news turned out to be infeasible, since hardly any piece of fake news is entirely false, and pieces of real news may not be flawless.",3.1 Corpus Construction,[0],[0]
"Therefore, posts were rated “mostly true,” “mixture of true and false,” “mostly false,” or, if the post was opinion-driven or otherwise lacked a factual claim, “no factual content.”",3.1 Corpus Construction,[0],[0]
"Four BuzzFeed journalists worked on the manual fact-checks of the news articles: to minimize costs, each article was reviewed only once and articles were assigned round robin.",3.1 Corpus Construction,[0],[0]
"The ratings “mixture of true and false” and “mostly false” had to be justified, and, when in doubt about a rating, a second opinion was collected, whereas disagreements were resolved by a third one.",3.1 Corpus Construction,[0],[0]
"Finally, all news rated “mostly false” underwent a final check to ensure the rating was justified, lest the respective publishers would contest it.",3.1 Corpus Construction,[0],[0]
"2Corpus download: https://doi.org/10.5281/zenodo.1239675 3http://github.com/BuzzFeedNews/2016-10-facebook-fact-check
",3.1 Corpus Construction,[0],[0]
The journalists were given the following guidance: Mostly true: The post and any related link or image are based on factual information and portray it accurately.,3.1 Corpus Construction,[0],[0]
"The authors may interpret the event/info in their own way, so long as they do not misrepresent events, numbers, quotes, reactions, etc., or make information up.",3.1 Corpus Construction,[0],[0]
"This rating does not allow for unsupported speculation or claims.
",3.1 Corpus Construction,[0],[0]
"Mixture of true and false (mix, for short): Some elements of the information are factually accurate, but some elements or claims are not.",3.1 Corpus Construction,[0],[0]
"This rating should be used when speculation or unfounded claims are mixed with real events, numbers, quotes, etc., or when the headline of the link being shared makes a false claim but the text of the story is largely accurate.",3.1 Corpus Construction,[0],[0]
It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link.,3.1 Corpus Construction,[0],[0]
"Finally, use this rating for news articles that are based on unconfirmed information.
",3.1 Corpus Construction,[0],[0]
Mostly false: Most or all of the information in the post or in the link being shared is inaccurate.,3.1 Corpus Construction,[0],[0]
"This should also be used when the central claim being made is false.
",3.1 Corpus Construction,[0],[0]
"No factual content (n/a, for short):",3.1 Corpus Construction,[0],[0]
"This rating is used for posts that are pure opinion, comics, satire, or any other posts that do not make a factual claim.",3.1 Corpus Construction,[0],[0]
This is also the category to use for posts that are of the “Like this if you think...” variety.,3.1 Corpus Construction,[0],[0]
"Given the significant workload (i.e., costs) required to carry out the aforementioned annotations, the corpus is restricted to the given temporal period and biased toward the US culture and political landscape, comprising only English news articles from a limited number of publishers.",3.2 Limitations,[0],[0]
"Annotations were recorded at the article level, not at statement level.",3.2 Limitations,[0],[0]
"For text categorization, this is sufficient.",3.2 Limitations,[0],[0]
"At the time of writing, our corpus is the largest of its kind that has been annotated by professional journalists.",3.2 Limitations,[0],[0]
Table 1 shows the fact-checking results and some key statistics per article.,3.3 Corpus Statistics,[0],[0]
"Unsurprisingly, none of the mainstream articles are mostly false, whereas 8 across all three publishers are a mixture of true and false.",3.3 Corpus Statistics,[0],[0]
"Disregarding non-factual articles, a little more than a quarter of all hyperpartisan left-wing articles were found faulty: 15 articles mostly false, and 51 a mixture of true and false.",3.3 Corpus Statistics,[0],[0]
"Publisher “The Other 98%” sticks out by achieving an almost per-
Orientation Fact-checking results Key statistics per article Publisher
true mix false n/a Σ Paras.",3.3 Corpus Statistics,[0],[0]
"Links Words
extern all quoted all
Mainstream 806 8 0 12 826 20.1 2.2 3.7 18.1 692.0 ABC News 90 2 0 3 95 21.1 1.0 4.8 21.0 551.9 CNN 295 4 0 8 307 19.3 2.4 2.5 15.3 588.3 Politico 421 2 0 1 424 20.5 2.3 4.3 19.9 798.5
Left-wing 182 51 15 8 256 14.6 4.5 4.9 28.6 423.2 Addicting Info 95 25 8 7 135 15.9 4.4 4.5 30.5 430.5 Occupy Democrats 55 23 6 0 91 10.9 4.1 4.7 29.0 421.7",3.3 Corpus Statistics,[0],[0]
"The Other 98% 32 3 1 1 30 20.2 6.4 7.2 21.2 394.5
Right-wing 276 153 72 44 545 14.1 2.5 3.1 24.6 397.4 Eagle Rising 107 47 25 36 214 12.9 2.6 2.8 17.3 388.3 Freedom Daily 48 24 22 4 99 14.6 2.2 2.3 23.5 419.3 Right Wing News 121 82 25 4 232 15.0 2.5 3.6 33.6 396.6
Σ 1264 212 87 64 1627 17.2 2.7 3.7 20.6 551.0
Table 1: The BuzzFeed-Webis Fake News Corpus 2016 at a glance (“Paras.”",3.3 Corpus Statistics,[0],[0]
"short for “paragraphs”).
",3.3 Corpus Statistics,[0],[0]
fect score.,3.3 Corpus Statistics,[0],[0]
"By contrast, almost 45% of the rightwing articles are a mixture of true and false (153) or mostly false (72).",3.3 Corpus Statistics,[0],[0]
"Here, publisher “Right Wing News” sticks out by supplying more than half of mixtures of true and false alone, whereas mostly false articles are equally distributed.
",3.3 Corpus Statistics,[0],[0]
"Regarding key statistics per article, it is interesting that the articles from all mainstream publishers are on average about 20 paragraphs long with word counts ranging from 550 words on average at ABC News to 800 at Politico.",3.3 Corpus Statistics,[0],[0]
"Except for one publisher, left-wing articles and right-wing articles are shorter on average in terms of paragraphs as well as word count, averaging at about 420 words and 400 words, respectively.",3.3 Corpus Statistics,[0],[0]
"Left-wing articles quote on average about 10 words more than the mainstream, and right-wing articles 6 words more.",3.3 Corpus Statistics,[0],[0]
"When articles comprise links, they are usually external ones, whereas ABC News rather uses internal links, and only half of the links found at Politico articles are external.",3.3 Corpus Statistics,[0],[0]
Left-wing news articles stick out by containing almost double the amount of links across publishers than mainstream and right-wing ones.,3.3 Corpus Statistics,[0],[0]
"In our experiments, we operationalize the category of fake news by joining the articles that were rated mostly false with those rated a mixture of true and false.",3.4 Operationalizing Fake News,[0],[0]
"Arguably, the latter may not be exactly what is deemed “fake news” (as in: a complete fabrication), however, practice shows fake news are hardly ever devoid of truth.",3.4 Operationalizing Fake News,[0],[0]
"More often, true facts are misconstrued or framed badly.",3.4 Operationalizing Fake News,[0],[0]
"In our experiments, we hence call mostly true articles real news, mostly false plus mixtures of true and false—except for satire—fake news, and disregard all articles rated non-factual.",3.4 Operationalizing Fake News,[0],[0]
"This section covers our methodology, including our feature set to capture writing style, and a brief recap of Unmasking by Koppel et al. (2007), which we employ for the first time to distinguish genre styles as opposed to author styles.",4 Methodology,[0],[0]
"For sake of reproducibility, all our code has been published.4",4 Methodology,[0],[0]
Our writing style model incorporates common features as well as ones specific to the news domain.,4.1 Style Features and Feature Selection,[0],[0]
"The former are n-grams, n in [1, 3], of characters, stop words, and parts-of-speech.",4.1 Style Features and Feature Selection,[0],[0]
"Further, we employ 10 readability scores5 and dictionary features, each indicating the frequency of words from a tailor-made dictionary in a document, using the General Inquirer Dictionaries as a basis (Stone et al., 1966).",4.1 Style Features and Feature Selection,[0],[0]
"The domain-specific features include ratios of quoted words and external links, the number of paragraphs, and their average length.
",4.1 Style Features and Feature Selection,[0],[0]
"In each of our experiments, we carefully select from the aforementioned features the ones worthwhile using: all features are discarded that are hardly represented in our corpus, namely word tokens that occur in less than 2.5% of the documents, and n-gram features that occur in less than 10% of the documents.",4.1 Style Features and Feature Selection,[0],[0]
"Discarding these features prevents overfitting and improves the chances that our model will generalize.
",4.1 Style Features and Feature Selection,[0],[0]
"If not stated otherwise, our experiments share a common setup.",4.1 Style Features and Feature Selection,[0],[0]
"In order to avoid biases from the respective training sets, we balance them using oversampling.",4.1 Style Features and Feature Selection,[0],[0]
"Furthermore, we perform 3-fold cross-validation where each fold comprises one publisher from each orientation, so that the classifier does not learn a publisher’s style.",4.1 Style Features and Feature Selection,[0],[0]
For nonUnmasking experiments we use WEKA’s random forest implementation with default settings.,4.1 Style Features and Feature Selection,[0],[0]
"Unmasking, as proposed by Koppel et al. (2007), is a meta learning approach for authorship verification.",4.2 Unmasking Genre Styles,[0],[0]
"We study for the first time whether it can be used to assess the similarity of more broadly defined style categories, such as left-wing vs. rightwing vs. mainstream news.",4.2 Unmasking Genre Styles,[0],[0]
"This way, we uncover relations between the writing styles that people may involuntarily adopt as per their political orientation.",4.2 Unmasking Genre Styles,[0],[0]
"4Code download: http://www.github.com/webis-de/ACL-18 5Automated Readability Index, Coleman Liau Index, Flesh Kincaid Grade Level and Reading Ease, Gunning Fog Index, LIX, McAlpine EFLAW Score, RIX, SMOG Grade, Strain Index
Originally, Unmasking takes two documents as input and outputs its confidence whether they have been written by the same author.",4.2 Unmasking Genre Styles,[0],[0]
"Three steps are taken to accomplish this: first, each document is chunked into a set of at least 500-word long chunks; second, classification errors are measured while iteratively removing the most discriminative features of a style model consisting of the 250 most frequent words, separating the two chunk sets with a linear classifier; and third, the resulting classification accuracy curves are analyzed with regard to their slope.",4.2 Unmasking Genre Styles,[0],[0]
"A steep decrease is more likely than a shallow decrease if the two documents have been written by the same author, since there are presumably less discriminating features between documents written by the same author than between documents written by different authors.",4.2 Unmasking Genre Styles,[0],[0]
"Training a classifier on many examples of error curves obtained from same-author document pairs and differentauthor document pairs yields an effective authorship verifier—at least for long documents that can be split up into a sufficient number of chunks.
",4.2 Unmasking Genre Styles,[0],[0]
It turns out that what applies to the style of authors also applies to genre styles.,4.2 Unmasking Genre Styles,[0],[0]
"We adapt Unmasking by skipping its first step and using two sets of documents (e.g., left-wing articles and rightwing articles) as input.",4.2 Unmasking Genre Styles,[0],[0]
"When plotting classification error curves for visual inspection, steeper decreases in these plots, too, indicate higher style similarity of the two input document sets, just as with chunk sets of two documents written by the same author.",4.2 Unmasking Genre Styles,[0],[0]
"We employ four baseline models: a topic-based bag of words model, often used in the literature, but less practical since news topics change frequently and drastically; a model using only the domain-specific news style features to check whether the differences between categories measured as corpus statistics play a significant role; and naive baselines that classify all items into one of the categories in question, relating our results to the class distributions.",4.3 Baselines,[0],[0]
"Classification performance is measured as accuracy, and class-wise precision, recall, and F1.",4.4 Performance Measures,[0],[0]
"We favor these measures over, e.g., areas under the ROC curve or the precision recall curve for simplicity sake.",4.4 Performance Measures,[0],[0]
"Also, the tasks we are tackling are new, so that little is known to date about user preferences.",4.4 Performance Measures,[0],[0]
This is also why we chose the evenly-balanced F1.,4.4 Performance Measures,[0],[0]
"We report on the results of two series of experiments that investigate style differences and similarities between hyperpartisan and mainstream news, and between fake, real, and satire news, shedding light on the following questions:
1.",5 Experiments,[0],[0]
Can (left/right) hyperpartisanship be distinguished from the mainstream?,5 Experiments,[0],[0]
2.,5 Experiments,[0],[0]
Is style-based fake news detection feasible?,5 Experiments,[0],[0]
3.,5 Experiments,[0],[0]
Can fake news be distinguished from satire?,5 Experiments,[0],[0]
"Our first experiment addressing the first question uncovered an odd behavior of our classifier: it would often misjudge left-wing for right-wing news, while being much better at distinguishing both combined from the mainstream.",5 Experiments,[0],[0]
"To explain this behavior, we hypothesized that maybe the writing style of the hyperpartisan left and right are more similar to one another than to the mainstream.",5 Experiments,[0],[0]
"To investigate this hypothesis, we devised two additional validation experiments, yielding three sources of evidence instead of just one.",5 Experiments,[0],[0]
A. Predicting orientation.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Table 2 shows the classification performance of a ternary classifier trained to discriminate left, right, and mainstream—an obvious first experiment for our dataset.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Separating the left and right orientation from the mainstream does not work too well: the topic baseline outperforms the style-based models with regard to accuracy, whereas the results for class-wise precision and recall are a mixed bag.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
The left-wing articles are apparently significantly more difficult to be identified compared to articles from the other two orientations.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"When we inspected the confusion matrix (not shown), it turned out that 66% of misclassifications of left-wing articles are falsely classified as right-wing articles, whereas 60% of all misclassified right-wing articles are classified as mainstream articles.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Misclassified mainstream articles spread almost evenly across the other classes.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The poor performance of the domain-specific news style features by themselves demonstrate that orientation cannot be discriminated based on the basic corpus characteristics observed with respect to paragraphs, quotations, and hyperlinks.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This holds for all subsequent experiments.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
B. Predicting hyperpartisanship.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Given the apparent difficulty of telling apart individual orientations, we did not frantically add features or switch classifiers to make it work.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Rather, we trained a binary
Features Accuracy Precision Recall F1 all left right main.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
left right main.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"left right main.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Style 0.60 0.21 0.56 0.75 0.20 0.59 0.74 0.20 0.57 0.75,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Topic 0.64 0.24 0.62 0.72 0.15 0.54 0.86 0.19 0.58,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"0.79 News style 0.39 0.09 0.35 0.59 0.14 0.36 0.49 0.11 0.36 0.53
classifier to discriminate hyperpartisanship in general from the mainstream.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Table 3 shows the performance values.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This time, the best classification accuracy of 0.75 at a remarkable 0.89 recall for the hyperpartisan class is achieved by the style-based classifier, outperforming the topic baseline.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Comparing Table 2 and Table 3, we were left with a riddle: all other things being equal, how could it be that hyperpartisanship in general can be much better discriminated from the mainstream than individual orientation?",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Attempts to answer this question gave rise to our aforementioned hypothesis that, perhaps, the writing style of hyperpartisan left and right are not altogether different, despite their opposing agendas.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Or put another way, if style and topic are orthogonal concepts, then being an extremist should not exert a different style dependent on political orientation.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Excited, we sought ways to independently disprove the hypothesis, and found two: Experiments C and D.
C. Validation using leave-out classification.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"If leftwing and right-wing articles have a more similar style than either of them compared to mainstream articles, then what class would a binary classifier assign to a left-wing article, if it were trained to distinguish only the right-wing from the mainstream, and vice versa?",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
Table 4 shows the results of this experiment.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"As indicated by proportions well above 0.50, full style-based classifiers have a tendency of clas-
sifying left as right and right as left.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The topic baseline, though, gets confused especially when omitting right articles from the training set with performance close to random.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The fact that the topic baseline works better when omitting left from the training set may be explainable: leading up to the elections, the hyperpartisan left was often merely reacting to topics prompted by the hyperpartisan right, instead of bringing up their own.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
D. Validation using Unmasking.,5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Based on Koppel et al.’s original approach in the context of authorship verification, for the first time, we generalize Unmasking to assess genre styles: just like author style similarity, genre style similarity will be characterized by the slope of a given Unmasking curve, where a steeper decrease indicates higher similarity.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"We apply Unmasking as described in Section 4.2 onto pairs of sets of left, right, and mainstream articles.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Figure 2 shows the resulting Unmasking curves (Unmasking is symmetrical, hence three curves).",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"The curves are averaged over 5 runs, where each run comprised sets of 100 articles from each orientation.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"In case of the left-wing orientation, where less than 500 articles are available in our corpus, once all of them had been used, they were shuffled again to select articles for the remainder of the runs.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"As can be seen, the curve comparing left vs. right has a distinctly steeper slope than either of the others.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"This result hence matches the findings of the previous experiments.
",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"With caution, we conclude that the evidence gained from our three independent experimental setups supports our hypothesis that the hyperpartisan left and the hyperpartisan right have more in common in terms of writing style than any of the two have with the mainstream.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
"Another more tangible (e.g., practical) outcome of Experiment B is the finding that hyperpartisan news can apparently be
discriminated well from the mainstream: in particular the high recall of 0.89 at a reasonable precision of 0.69 gives us confidence that, with some further effort, a practical classifier can be built that detects hyperpartisan news at scale and in real time, since an article’s style can be assessed immediately without referring to external information.",5.1 Hyperpartisanship vs. Mainstream,[0],[0]
This series of experiments targets research questions (2) and (3).,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Again, we conduct three experiments, where the first is about predicting veracity, and the last two about discriminating satire.
",5.2 Fake vs. Real (vs. Satire),[0],[0]
A. Predicting veracity.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"When taking into account that the mainstream news publishers in our corpus did not publish any news items that are mostly false, and only very few instances that are mixtures of true and false, we may safely disregard them for the task of fake news detection.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"A reliable classifier for hyperpartisan news can act as a prefilter for a subsequent, more in-depth fake news detection approach, which may in turn be tailored to a much more narrowly defined classification task.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We hence use only the left-wing articles and the right-wing articles of our corpus for our attempt at a style-based fake news classifier.
Table 5 shows the performance values for a generic classifier that predicts fake news across orientations, and orientation-specific classifiers that have been individually trained on articles from either orientation.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Although all classifiers outperform the naive baselines of classifying everything into one of the classes in terms of precision, the slight increase comes at the cost of a large decrease in recall.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"While the orientation-specific classifiers are slightly better for most metrics, none of them outperform the naive baselines regarding the F - Measure.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We conclude that style-based fake news classification simply does not work in general.
B. Predicting satire.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Yet, not all fake news are the same.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"One should distinguish satire from the rest, which takes the form of news but lies more or less obviously to amuse its readers.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Regardless the problems that spreading fake news may cause, satire should never be filtered, but be discriminated from other fakes.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Table 6 shows the performance values of our classifier in the satire-detection setting used by Rubin et al. (2016) (the S-n-L News DB corpus), distinguishing satire from real news.",5.2 Fake vs. Real (vs. Satire),[0],[0]
This setting uses a balanced 3:1 training-to-test set split over 360 articles (180 per class).,5.2 Fake vs. Real (vs. Satire),[0],[0]
"As can be seen, our style-based model significantly outperforms all baselines across the board, achieving an accuracy of 0.82, and an F score of 0.81.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"It clearly improves over topic classification, but does not outperform Rubin et al.’s classifier, which includes features based on topic, absurdity, grammar, and punctuation.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We argue that incorporating topic into satire detection is not appropriate, since the topics of satire change along the topics of news.",5.2 Fake vs. Real (vs. Satire),[0],[0]
A classifier with topic features therefore does not generalize.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Apparently, a style-based model is competitive, and we believe that satire can be detected at scale this way, so as to prevent other fake news detection technology from falsely filtering it.
",5.2 Fake vs. Real (vs. Satire),[0],[0]
C. Unmasking satire.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Given the above results on stylistic similarities between left and right news, the question remains how satire fits into the picture.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"We assess the style similarity of satire from Rubin et al.’s corpus compared to fake news and real news from ours, again applying Unmasking to compare pairs of the three categories of news as described above.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Figure 3 shows the resulting Un-
masking curves.",5.2 Fake vs. Real (vs. Satire),[0],[0]
The curve for the pair of fake vs. real news drops faster compared to the other two pairs.,5.2 Fake vs. Real (vs. Satire),[0],[0]
"Apparently, the style of fake news has more in common with that of real news than either of the two have with satire.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"These results are encouraging: satire is distinct enough from fake and real news, so that, just like with hyperpartisan news compared to mainstream news, it can be discriminated with reasonable accuracy.",5.2 Fake vs. Real (vs. Satire),[0],[0]
"Fact-checking for fake news detection poses an interdisciplinary challenge: technology is required to extract factual statements from text, to match facts with a knowledge base, to dynamically retrieve and maintain knowledge bases from the web, to reliably assess the overall veracity of an entire article rather than individual statements, to do so in real time as news events unfold, to monitor the spread of fake news within and across social media, to measure the reputation of information sources, and to raise awareness in readers.",6 Conclusion,[0],[0]
"These are only the most salient things that need be done to tackle the problem, and as our cross-section of related work shows, a large body of work must be covered.",6 Conclusion,[0],[0]
"Notwithstanding the many attacks on fake news by developing one way or another of fact-checking, we believe it worthwhile to mount our attack from another angle: writing style.
",6 Conclusion,[0],[0]
We show that news articles conveying a hyperpartisan world view can be distinguished from more balanced news by writing style alone.,6 Conclusion,[0],[0]
"Moreover, for the first time, we found quantifiable evidence that the writing styles of news of the two opposing orientations are in fact very similar: there appears to be a common writing style of left and right extremism.",6 Conclusion,[0],[0]
"We further show that satire can be distinguished well from other news, ensuring that humor will not be outcast by fake news detection technology.",6 Conclusion,[0],[0]
"All of these results offer new, tangible, short-term avenues of development, lest large-scale fact-checking is still far out of reach.",6 Conclusion,[0],[0]
"Employed as pre-filtering technologies to separate hyperpartisan news from mainstream news, our approach allows for directing the attention of human fact checkers to the most likely sources of fake news.",6 Conclusion,[0],[0]
"We thank Craig Silverman, Lauren Strapagiel, Hamza Shaban, Ellie Hall, and Jeremy Singer-Vine from BuzzFeed for making their data available, enabling our research.",Acknowledgements,[0],[0]
We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news.,abstractText,[0],[0]
"A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan.",abstractText,[0],[0]
"We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1=0.78), and satire from both (F1=0.81).",abstractText,[0],[0]
But stylometry is no silver bullet as style-based fake news detection does not work (F1=0.46).,abstractText,[0],[0]
We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream.,abstractText,[0],[0]
"This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way.",abstractText,[0],[0]
Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.,abstractText,[0],[0]
A Stylometric Inquiry into Hyperpartisan and Fake News,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 440–450 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1041",text,[0],[0]
"Every programmer has experienced the situation where they know what they want to do, but do not have the ability to turn it into a concrete implementation.",1 Introduction,[0],[0]
"For example, a Python programmer may want to “sort my list in descending order,” but not be able to come up with the proper syntax sorted(my list, reverse=True) to realize his intention.",1 Introduction,[0],[0]
"To resolve this impasse, it is common for programmers to search the web in natural language (NL), find an answer, and modify it into the desired form (Brandt et al., 2009, 2010).",1 Introduction,[0],[0]
"However, this is time-consuming, and thus the software engineering literature is ripe with methods to directly generate code from NL descriptions, mostly with hand-engineered methods highly tailored to specific programming languages (Balzer, 1985; Little and Miller, 2009; Gvero and Kuncak, 2015).
",1 Introduction,[0],[0]
"In parallel, the NLP community has developed methods for data-driven semantic parsing, which attempt to map NL to structured logical forms executable by computers.",1 Introduction,[0],[0]
"These logical forms can be general-purpose meaning representations (Clark and Curran, 2007; Banarescu et al., 2013), formalisms for querying knowledge bases (Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Berant et al., 2013) and instructions for robots or personal assistants (Artzi and Zettlemoyer, 2013; Quirk et al., 2015; Misra et al., 2015), among others.",1 Introduction,[0],[0]
"While these methods have the advantage of being learnable from data, compared to the programming languages (PLs) in use by programmers, the domain-specific languages targeted by these works have a schema and syntax that is relatively simple.
",1 Introduction,[0],[0]
"Recently, Ling et al. (2016) have proposed a data-driven code generation method for high-level, general-purpose PLs like Python and Java.",1 Introduction,[0],[0]
"This work treats code generation as a sequence-tosequence modeling problem, and introduce methods to generate words from character-level models, and copy variable names from input descriptions.",1 Introduction,[0],[0]
"However, unlike most work in semantic parsing, it does not consider the fact that code has to be well-defined programs in the target syntax.
",1 Introduction,[0],[0]
"In this work, we propose a data-driven syntaxbased neural network model tailored for generation of general-purpose PLs like Python.",1 Introduction,[0],[0]
"In order to capture the strong underlying syntax of the PL, we define a model that transduces an NL statement into an Abstract Syntax Tree (AST; Fig. 1(a), § 2) for the target PL.",1 Introduction,[0],[0]
"ASTs can be deterministically generated for all well-formed programs using standard parsers provided by the PL, and thus give us a way to obtain syntax information with minimal engineering.",1 Introduction,[0],[0]
"Once we generate an AST, we can use deterministic generation tools to convert the AST into surface code.",1 Introduction,[0],[0]
"We hypothesize
440
that such a structured approach has two benefits.",1 Introduction,[0],[0]
"First, we hypothesize that structure can be used to constrain our search space, ensuring generation of well-formed code.",1 Introduction,[0],[0]
"To this end, we propose a syntax-driven neural code generation model.",1 Introduction,[0],[0]
"The backbone of our approach is a grammar model (§ 3) which formalizes the generation story of a derivation AST into sequential application of actions that either apply production rules (§ 3.1), or emit terminal tokens (§ 3.2).",1 Introduction,[0],[0]
The underlying syntax of the PL is therefore encoded in the grammar model a priori as the set of possible actions.,1 Introduction,[0],[0]
"Our approach frees the model from recovering the underlying grammar from limited training data, and instead enables the system to focus on learning the compositionality among existing grammar rules.",1 Introduction,[0],[0]
"Xiao et al. (2016) have noted that this imposition of structure on neural models is useful for semantic parsing, and we expect this to be even more important for general-purpose PLs where the syntax trees are larger and more complex.
",1 Introduction,[0],[0]
"Second, we hypothesize that structural information helps to model information flow within the neural network, which naturally reflects the recursive structure of PLs.",1 Introduction,[0],[0]
"To test this, we extend a standard recurrent neural network (RNN) decoder to allow for additional neural connections which reflect the recursive structure of an AST (§ 4.2).",1 Introduction,[0],[0]
"As an example, when expanding the node ?",1 Introduction,[0],[0]
"in Fig. 1(a), we make use of the information from both its parent and left sibling (the dashed rectangle).",1 Introduction,[0],[0]
"This enables us to locally pass information of relevant code segments via neural network connections, resulting in more confident predictions.
",1 Introduction,[0],[0]
"Experiments (§ 5) on two Python code generation tasks show 11.7% and 9.3% absolute improvements in accuracy against the state-of-the-art system (Ling et al., 2016).",1 Introduction,[0],[0]
"Our model also gives competitive performance on a standard semantic parsing benchmark1.
1Implementation available at https://github.",1 Introduction,[0],[0]
com/neulab/NL2code,1 Introduction,[0],[0]
"Given an NL description x, our task is to generate the code snippet c in a modern PL based on the intent of x. We attack this problem by first generating the underlying AST.",2 The Code Generation Problem,[0],[0]
We define a probabilistic grammar model of generating an AST y given x: p(y|x).,2 The Code Generation Problem,[0],[0]
"The best-possible AST ŷ is then given by
ŷ = arg max y
p(y|x).",2 The Code Generation Problem,[0],[0]
"(1)
ŷ is then deterministically converted to the corresponding surface code c.2",2 The Code Generation Problem,[0],[0]
"While this paper uses examples from Python code, our method is PLagnostic.
",2 The Code Generation Problem,[0],[0]
"Before detailing our approach, we first present a brief introduction of the Python AST and its underlying grammar.",2 The Code Generation Problem,[0],[0]
"The Python abstract grammar contains a set of production rules, and an AST is generated by applying several production rules composed of a head node and multiple child nodes.",2 The Code Generation Problem,[0],[0]
"For instance, the first rule in Tab. 1 is used to generate the function call sorted(·) in Fig. 1(a).",2 The Code Generation Problem,[0],[0]
"It consists of a head node of type Call, and three child nodes of type expr, expr* and keyword*, respectively.",2 The Code Generation Problem,[0],[0]
Labels of each node are noted within brackets.,2 The Code Generation Problem,[0],[0]
"In an AST, non-terminal nodes sketch the general structure of the target code, while terminal nodes can be categorized into two types: operation terminals and variable terminals.",2 The Code Generation Problem,[0],[0]
Operation terminals correspond to basic arithmetic operations like AddOp.,2 The Code Generation Problem,[0],[0]
Variable terminal nodes store values for variables and constants of built-in data types3.,2 The Code Generation Problem,[0],[0]
"For instance, all terminal nodes in Fig. 1(a) are variable terminal nodes.",2 The Code Generation Problem,[0],[0]
"Before detailing our neural code generation method, we first introduce the grammar model at its core.",3 Grammar Model,[0],[0]
Our probabilistic grammar model defines the generative story of a derivation AST.,3 Grammar Model,[0],[0]
"We fac-
2We use astor library to convert ASTs into Python code.",3 Grammar Model,[0],[0]
"3bool, float, int, str.
torize the generation process of an AST into sequential application of actions of two types:
• APPLYRULE[r] applies a production rule r to the current derivation tree;
• GENTOKEN[v] populates a variable terminal node by appending a terminal token v.
Fig. 1(b) shows the generation process of the target AST in Fig. 1(a).",3 Grammar Model,[0],[0]
Each node in Fig. 1(b) indicates an action.,3 Grammar Model,[0],[0]
Action nodes are connected by solid arrows which depict the chronological order of the action flow.,3 Grammar Model,[0],[0]
"The generation proceeds in depth-first, left-to-right order (dotted arrows represent parent feeding, explained in § 4.2.1).
",3 Grammar Model,[0],[0]
"Formally, under our grammar model, the probability of generating an AST y is factorized as:
p(y|x)",3 Grammar Model,[0],[0]
"= TY
t=1
p(at|x, a<t), (2)
where at is the action taken at time step t, and a<t is the sequence of actions before t. We will explain how to compute the action probabilities p(at|·) in Eq.",3 Grammar Model,[0],[0]
"(2) in § 4. Put simply, the generation process begins from a root node at t0, and proceeds by the model choosing APPLYRULE actions to generate the overall program structure from a closed set of grammar rules, then at leaves of the tree corresponding to variable terminals, the model switches to GENTOKEN actions to generate variables or constants from the open set.",3 Grammar Model,[0],[0]
We describe this process in detail below.,3 Grammar Model,[0],[0]
"APPLYRULE actions generate program structure, expanding the current node (the frontier node at
time step t: nft) in a depth-first, left-to-right traversal of the tree.",3.1 APPLYRULE Actions,[0],[0]
"Given a fixed set of production rules, APPLYRULE chooses a rule r from the subset that has a head matching the type of nft , and uses r to expand nft by appending all child nodes specified by the selected production.",3.1 APPLYRULE Actions,[0],[0]
"As an example, in Fig. 1(b), the rule Call 7! expr. . .",3.1 APPLYRULE Actions,[0],[0]
"expands the frontier node Call at time step t4, and its three child nodes expr, expr* and keyword* are added to the derivation.
",3.1 APPLYRULE Actions,[0],[0]
APPLYRULE actions grow the derivation AST by appending nodes.,3.1 APPLYRULE Actions,[0],[0]
"When a variable terminal node (e.g., str) is added to the derivation and becomes the frontier node, the grammar model then switches to GENTOKEN actions to populate the variable terminal with tokens.
",3.1 APPLYRULE Actions,[0],[0]
"Unary Closure Sometimes, generating an AST requires applying a chain of unary productions.",3.1 APPLYRULE Actions,[0],[0]
"For instance, it takes three time steps (t9 t11) to generate the sub-structure expr* 7! expr 7!",3.1 APPLYRULE Actions,[0],[0]
Name 7! str in Fig. 1(a).,3.1 APPLYRULE Actions,[0],[0]
This can be effectively reduced to one step of APPLYRULE action by taking the closure of the chain of unary productions and merging them into a single rule: expr* 7!⇤ str.,3.1 APPLYRULE Actions,[0],[0]
"Unary closures reduce the number of actions needed, but would potentially increase the size of the grammar.",3.1 APPLYRULE Actions,[0],[0]
In our experiments we tested our model both with and without unary closures (§ 5).,3.1 APPLYRULE Actions,[0],[0]
"Once we reach a frontier node nft that corresponds to a variable type (e.g., str), GENTOKEN actions are used to fill this node with values.",3.2 GENTOKEN Actions,[0],[0]
"For generalpurpose PLs like Python, variables and constants have values with one or multiple tokens.",3.2 GENTOKEN Actions,[0],[0]
"For in-
stance, a node that stores the name of a function (e.g., sorted) has a single token, while a node that denotes a string constant (e.g., a=‘hello world’) could have multiple tokens.",3.2 GENTOKEN Actions,[0],[0]
Our model copes with both scenarios by firing GENTOKEN actions at one or more time steps.,3.2 GENTOKEN Actions,[0],[0]
"At each time step, GENTOKEN appends one terminal token to the current frontier variable node.",3.2 GENTOKEN Actions,[0],[0]
A special </n>,3.2 GENTOKEN Actions,[0],[0]
token is used to “close” the node.,3.2 GENTOKEN Actions,[0],[0]
"The grammar model then proceeds to the new frontier node.
",3.2 GENTOKEN Actions,[0],[0]
"Terminal tokens can be generated from a predefined vocabulary, or be directly copied from the input NL.",3.2 GENTOKEN Actions,[0],[0]
This is motivated by the observation that the input description often contains out-ofvocabulary (OOV) variable names or literal values that are directly used in the target code.,3.2 GENTOKEN Actions,[0],[0]
"For instance, in our running example the variable name my list can be directly copied from the the input at t12.",3.2 GENTOKEN Actions,[0],[0]
We give implementation details in § 4.2.2.,3.2 GENTOKEN Actions,[0],[0]
We estimate action probabilities in Eq.,4 Estimating Action Probabilities,[0],[0]
(2) using attentional neural encoder-decoder models with an information flow structured by the syntax trees.,4 Estimating Action Probabilities,[0],[0]
"For an NL description x consisting of n words {wi}ni=1, the encoder computes a context sensitive embedding hi for each wi using a bidirectional Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997), similar to the setting in (Bahdanau et al., 2014).",4.1 Encoder,[0],[0]
See supplementary materials for detailed equations.,4.1 Encoder,[0],[0]
The decoder uses an RNN to model the sequential generation process of an AST defined as Eq.,4.2 Decoder,[0],[0]
(2).,4.2 Decoder,[0],[0]
Each action step in the grammar model naturally grounds to a time step in the decoder RNN.,4.2 Decoder,[0],[0]
"Therefore, the action sequence in Fig. 1(b) can be interpreted as unrolling RNN time steps, with solid arrows indicating RNN connections.",4.2 Decoder,[0],[0]
"The RNN maintains an internal state to track the generation process (§ 4.2.1), which will then be used to compute action probabilities p(at|x, a<t) (§ 4.2.2).",4.2 Decoder,[0],[0]
"Our implementation of the decoder resembles a vanilla LSTM, with additional neural connections (parent feeding, Fig. 1(b)) to reflect the topological structure of an AST.",4.2.1 Tracking Generation States,[0],[0]
"The decoder’s internal hidden state at time step t, st, is given by:
st = fLSTM([at 1 : ct : pt :",4.2.1 Tracking Generation States,[0],[0]
"nft ], st 1), (3)
where fLSTM(·) is the LSTM update function.",4.2.1 Tracking Generation States,[0],[0]
[:] denotes vector concatenation.,4.2.1 Tracking Generation States,[0],[0]
"st will then be used to compute action probabilities p(at|x, a<t) in Eq.",4.2.1 Tracking Generation States,[0],[0]
(2).,4.2.1 Tracking Generation States,[0],[0]
"Here, at 1 is the embedding of the previous action.",4.2.1 Tracking Generation States,[0],[0]
ct is a context vector retrieved from input encodings {hi} via soft attention.,4.2.1 Tracking Generation States,[0],[0]
pt is a vector that encodes the information of the parent action.,4.2.1 Tracking Generation States,[0],[0]
"nft denotes the node type embedding of the current frontier node nft
4.",4.2.1 Tracking Generation States,[0],[0]
"Intuitively, feeding the decoder the information of nft helps the model to keep track of the frontier node to expand.",4.2.1 Tracking Generation States,[0],[0]
"Action Embedding at We maintain two action embedding matrices, WR and WG.",4.2.1 Tracking Generation States,[0],[0]
Each row in WR (WG) corresponds to an embedding vector for an action APPLYRULE[r] (GENTOKEN[v]).,4.2.1 Tracking Generation States,[0],[0]
Context Vector ct,4.2.1 Tracking Generation States,[0],[0]
The decoder RNN uses soft attention to retrieve a context vector ct from the input encodings {hi} pertain to the prediction of the current action.,4.2.1 Tracking Generation States,[0],[0]
We follow Bahdanau et al. (2014) and use a Deep Neural Network (DNN) with a single hidden layer to compute attention weights.,4.2.1 Tracking Generation States,[0],[0]
Parent Feeding pt,4.2.1 Tracking Generation States,[0],[0]
Our decoder RNN uses additional neural connections to directly pass information from parent actions.,4.2.1 Tracking Generation States,[0],[0]
"For instance, when computing s9, the information from its parent action step t4 will be used.",4.2.1 Tracking Generation States,[0],[0]
"Formally, we define the parent action step pt as the time step at which the frontier node nft is generated.",4.2.1 Tracking Generation States,[0],[0]
"As an example, for t9, its parent action step p9 is t4, since nf9 is the node ?, which is generated at t4 by the APPLYRULE[Call7!. .",4.2.1 Tracking Generation States,[0],[0]
.],4.2.1 Tracking Generation States,[0],[0]
"action.
",4.2.1 Tracking Generation States,[0],[0]
"We model parent information pt from two sources: (1) the hidden state of parent action spt , and (2) the embedding of parent action apt .",4.2.1 Tracking Generation States,[0],[0]
pt is the concatenation.,4.2.1 Tracking Generation States,[0],[0]
"The parent feeding schema en-
4We maintain an embedding for each node type.
",4.2.1 Tracking Generation States,[0],[0]
ables the model to utilize the information of parent code segments to make more confident predictions.,4.2.1 Tracking Generation States,[0],[0]
Similar approaches of injecting parent information were also explored in the SEQ2TREE model in Dong and Lapata (2016)5.,4.2.1 Tracking Generation States,[0],[0]
"In this section we explain how action probabilities p(at|x, a<t) are computed based on st.",4.2.2 Calculating Action Probabilities,[0],[0]
"APPLYRULE The probability of applying rule r as the current action at is given by a softmax6:
p(at = APPLYRULE[r]|x, a<t) = softmax(WR ·",4.2.2 Calculating Action Probabilities,[0],[0]
"g(st))| · e(r) (4) where g(·) is a non-linearity tanh(W ·st+b), and e(r)",4.2.2 Calculating Action Probabilities,[0],[0]
"the one-hot vector for rule r. GENTOKEN As in § 3.2, a token v can be generated from a predefined vocabulary or copied from the input, defined as the marginal probability:
p(at = GENTOKEN[v]|x, a<t) =",4.2.2 Calculating Action Probabilities,[0],[0]
"p(gen|x, a<t)p(v|gen, x, a<t)
+ p(copy|x, a<t)p(v|copy, x, a<t).",4.2.2 Calculating Action Probabilities,[0],[0]
The selection probabilities p(gen|·) and p(copy|·) are given by softmax(WS · st).,4.2.2 Calculating Action Probabilities,[0],[0]
"The probability of generating v from the vocabulary, p(v|gen, x, a<t), is defined similarly as Eq. (4), except that we use the GENTOKEN embedding matrix WG, and we concatenate the context vector ct with st as input.",4.2.2 Calculating Action Probabilities,[0],[0]
"To model the copy probability, we follow recent advances in modeling copying mechanism in neural networks (Gu et al., 2016; Jia and Liang, 2016; Ling et al., 2016), and use a pointer network (Vinyals et al., 2015) to compute the probability of copying the i-th word from the input by attending to input representations {hi}:
p(wi|copy, x, a<t) =",4.2.2 Calculating Action Probabilities,[0],[0]
"exp(!(hi, st, ct))Pn
i0=1 exp(!(hi0 , st, ct)) ,
where !",4.2.2 Calculating Action Probabilities,[0],[0]
(·) is a DNN with a single hidden layer.,4.2.2 Calculating Action Probabilities,[0],[0]
"Specifically, if wi is an OOV word (e.g., the variable name my list), which is represented by a special <unk> token during encoding, we then directly copy the actual word wi from the input description to the derivation.",4.2.2 Calculating Action Probabilities,[0],[0]
"Given a dataset of pairs of NL descriptions xi and code snippets ci, we parse ci into its AST yi and
5SEQ2TREE generates tree-structured outputs by conditioning on the hidden states of parent non-terminals, while our parent feeding uses the states of parent actions.
",4.3 Training and Inference,[0],[0]
"6We do not show bias terms for all softmax equations.
",4.3 Training and Inference,[0],[0]
"decompose yi into a sequence of oracle actions, which explains the generation story of yi under the grammar model.",4.3 Training and Inference,[0],[0]
The model is then optimized by maximizing the log-likelihood of the oracle action sequence.,4.3 Training and Inference,[0],[0]
"At inference time, given an NL description, we use beam search to approximate the best AST ŷ in Eq.",4.3 Training and Inference,[0],[0]
(1).,4.3 Training and Inference,[0],[0]
See supplementary materials for the pseudo-code of the inference algorithm.,4.3 Training and Inference,[0],[0]
"HEARTHSTONE (HS) dataset (Ling et al., 2016) is a collection of Python classes that implement cards for the card game HearthStone.",5.1 Datasets and Metrics,[0],[0]
"Each card comes with a set of fields (e.g., name, cost, and description), which we concatenate to create the input sequence.",5.1 Datasets and Metrics,[0],[0]
"This dataset is relatively difficult: input descriptions are short, while the target code is in complex class structures, with each AST having 137 nodes on average.",5.1 Datasets and Metrics,[0],[0]
"DJANGO dataset (Oda et al., 2015) is a collection of lines of code from the Django web framework, each with a manually annotated NL description.",5.1 Datasets and Metrics,[0],[0]
"Compared with the HS dataset where card implementations are somewhat homogenous, examples in DJANGO are more diverse, spanning a wide variety of real-world use cases like string manipulation, IO operations, and exception handling.",5.1 Datasets and Metrics,[0],[0]
"IFTTT dataset (Quirk et al., 2015) is a domainspecific benchmark that provides an interesting side comparison.",5.1 Datasets and Metrics,[0],[0]
"Different from HS and DJANGO which are in a general-purpose PL, programs in IFTTT are written in a domain-specific language used by the IFTTT task automation
App.",5.1 Datasets and Metrics,[0],[0]
"Users of the App write simple instructions (e.g., If Instagram.",5.1 Datasets and Metrics,[0],[0]
"AnyNewPhotoByYou Then Dropbox.AddFileFromURL) with NL descriptions (e.g., “Autosave your Instagram photos to Dropbox”).",5.1 Datasets and Metrics,[0],[0]
"Each statement inside the If or Then clause consists of a channel (e.g., Dropbox) and a function (e.g., AddFileFromURL)7.",5.1 Datasets and Metrics,[0],[0]
This simple structure results in much more concise ASTs (7 nodes on average).,5.1 Datasets and Metrics,[0],[0]
"Because all examples are created by ordinary Apps users, the dataset is highly noisy, with input NL very loosely connected to target ASTs.",5.1 Datasets and Metrics,[0],[0]
"The authors thus provide a high-quality filtered test set, where each example is verified by at least three annotators.",5.1 Datasets and Metrics,[0],[0]
We use this set for evaluation.,5.1 Datasets and Metrics,[0],[0]
"Also note IFTTT’s grammar has more productions (Tab. 2), but this does not imply that its grammar is more complex.",5.1 Datasets and Metrics,[0],[0]
"This is because for HS and DJANGO terminal tokens are generated by GENTOKEN actions, but for IFTTT, all the code is generated directly by APPLYRULE actions.",5.1 Datasets and Metrics,[0],[0]
"Metrics As is standard in semantic parsing, we measure accuracy, the fraction of correctly generated examples.",5.1 Datasets and Metrics,[0],[0]
"However, because generating an exact match for complex code structures is nontrivial, we follow Ling et al. (2016), and use tokenlevel BLEU-4 with as a secondary metric, defined as the averaged BLEU scores over all examples.8",5.1 Datasets and Metrics,[0],[0]
Preprocessing All input descriptions are tokenized using NLTK.,5.2 Setup,[0],[0]
"We perform simple canonicalization for DJANGO, such as replacing quoted strings in the inputs with place holders.",5.2 Setup,[0],[0]
See supplementary materials for details.,5.2 Setup,[0],[0]
We extract unary closures whose frequency is larger than a threshold k,5.2 Setup,[0],[0]
(k = 30 for HS and 50 for DJANGO).,5.2 Setup,[0],[0]
"Configuration The size of all embeddings is 128, except for node type embeddings, which is 64.",5.2 Setup,[0],[0]
"The dimensions of RNN states and hidden layers are 256 and 50, respectively.",5.2 Setup,[0],[0]
"Since our datasets are relatively small for a data-hungry neural model, we impose strong regularization using recurrent
7Like Beltagy and Quirk (2016), we strip function parameters since they are mostly specific to users.
",5.2 Setup,[0],[0]
"8These two metrics are not ideal: accuracy only measures exact match and thus lacks the ability to give credit to semantically correct code that is different from the reference, while it is not clear whether BLEU provides an appropriate proxy for measuring semantics in the code generation task.",5.2 Setup,[0],[0]
"A more intriguing metric would be directly measuring semantic/functional code equivalence, for which we present a pilot study at the end of this section (cf.",5.2 Setup,[0],[0]
Error Analysis).,5.2 Setup,[0],[0]
"We leave exploring more sophisticated metrics (e.g. based on static code analysis) as future work.
dropouts (Gal and Ghahramani, 2016) for all recurrent networks, together with standard dropout layers added to the inputs and outputs of the decoder RNN.",5.2 Setup,[0],[0]
"We validate the dropout probability from {0, 0.2, 0.3, 0.4}.",5.2 Setup,[0],[0]
"For decoding, we use a beam size of 15.",5.2 Setup,[0],[0]
Evaluation results for Python code generation tasks are listed in Tab. 3.,5.3 Results,[0],[0]
Numbers for our systems are averaged over three runs.,5.3 Results,[0],[0]
"We compare primarily with two approaches: (1) Latent Predictor Network (LPN), a state-of-the-art sequenceto-sequence code generation model (Ling et al., 2016), and (2) SEQ2TREE, a neural semantic parsing model (Dong and Lapata, 2016).",5.3 Results,[0],[0]
"SEQ2TREE generates trees one node at a time, and the target grammar is not explicitly modeled a priori, but implicitly learned from data.",5.3 Results,[0],[0]
"We test both the original SEQ2TREE model released by the authors and our revised one (SEQ2TREE–UNK) that uses unknown word replacement to handle rare words (Luong et al., 2015).",5.3 Results,[0],[0]
"For completeness, we also compare with a strong neural machine translation (NMT) system (Neubig, 2015) using a standard encoder-decoder architecture with attention and unknown word replacement9, and include numbers from other baselines used in Ling et al. (2016).",5.3 Results,[0],[0]
"On the HS dataset, which has relatively large ASTs, we use unary closure for our model and SEQ2TREE, and for DJANGO we do not.
",5.3 Results,[0],[0]
"9For NMT, we also attempted to find the best-scoring syntactically correct predictions in the size-5 beam, but this did not yield a significant improvement over the NMT results in Tab. 3.
",5.3 Results,[0],[0]
"System Comparison As in Tab. 3, our model registers 11.7% and 9.3% absolute improvements over LPN in accuracy on HS and DJANGO.",5.3 Results,[0],[0]
This boost in performance strongly indicates the importance of modeling grammar in code generation.,5.3 Results,[0],[0]
"For the baselines, we find LPN outperforms NMT and SEQ2TREE in most cases.",5.3 Results,[0],[0]
"We also note that SEQ2TREE achieves a decent accuracy of 13.6% on HS, which is due to the effect of unknown word replacement, since we only achieved 1.5% without it.",5.3 Results,[0],[0]
"A closer comparison with SEQ2TREE is insightful for understanding the advantage of our syntax-driven approach, since both SEQ2TREE and our system output ASTs: (1) SEQ2TREE predicts one node each time step, and requires additional “dummy” nodes to mark the boundary of a subtree.",5.3 Results,[0],[0]
The sheer number of nodes in target ASTs makes the prediction process error-prone.,5.3 Results,[0],[0]
"In contrast, the APPLYRULE actions of our grammar model allows for generating multiple nodes at a single time step.",5.3 Results,[0],[0]
"Empirically, we found that in HS, SEQ2TREE takes more than 300 time steps on average to generate a target AST, while our model takes only 170 steps.",5.3 Results,[0],[0]
"(2) SEQ2TREE does not directly use productions in the grammar, which possibly leads to grammatically incorrect ASTs and thus empty code outputs.",5.3 Results,[0],[0]
"We observe that the ratio of grammatically incorrect ASTs predicted by SEQ2TREE on HS and DJANGO are 21.2% and 10.9%, respectively, while our system guarantees grammaticality.
",5.3 Results,[0],[0]
Ablation Study We also ablated our bestperforming models to analyze the contribution of each component.,5.3 Results,[0],[0]
“–frontier embed.” removes the frontier node embedding nft from the decoder RNN inputs (Eq.,5.3 Results,[0],[0]
(3)).,5.3 Results,[0],[0]
"This yields worse results on DJANGO while gives slight improvements in ac-
curacy on HS.",5.3 Results,[0],[0]
"This is probably because that the grammar of HS has fewer node types, and thus the RNN is able to keep track of nft without depending on its embedding.",5.3 Results,[0],[0]
"Next, “–parent feed.” removes the parent feeding mechanism.",5.3 Results,[0],[0]
"The accuracy drops significantly on HS, with a marginal deterioration on DJANGO.",5.3 Results,[0],[0]
"This result is interesting because it suggests that parent feeding is more important when the ASTs are larger, which will be the case when handling more complicated code generation tasks like HS.",5.3 Results,[0],[0]
"Finally, removing the pointer network (“–copy terminals”) in GENTOKEN actions gives poor results, indicating that it is important to directly copy variable names and values from the input.
",5.3 Results,[0],[0]
"The results with and without unary closure demonstrate that, interestingly, it is effective on HS but not on DJANGO.",5.3 Results,[0],[0]
"We conjecture that this is because on HS it significantly reduces the number of actions from 173 to 142 (c.f., Tab. 2), with the number of productions in the grammar remaining unchanged.",5.3 Results,[0],[0]
"In contrast, DJANGO has a broader domain, and thus unary closure results in more productions in the grammar (237 for DJANGO vs. 100 for HS), increasing sparsity.",5.3 Results,[0],[0]
Performance by the size of AST We further investigate our model’s performance w.r.t.,5.3 Results,[0],[0]
the size of the gold-standard ASTs in Figs. 3 and 4.,5.3 Results,[0],[0]
"Not surprisingly, the performance drops when the size of the reference ASTs increases.",5.3 Results,[0],[0]
"Additionally, on the HS dataset, the BLEU score still remains at around 50 even when the size of ASTs grows to 200, indicating that our proposed syntax-driven approach is robust for long code segments.",5.3 Results,[0],[0]
Domain Specific Code Generation,5.3 Results,[0],[0]
"Although this is not the focus of our work, evaluation on IFTTT brings us closer to a standard semantic parsing set-
ting, which helps to investigate similarities and differences between generation of more complicated general-purpose code and and more limiteddomain simpler code.",5.3 Results,[0],[0]
"Tab. 4 shows the results, following the evaluation protocol in (Beltagy and Quirk, 2016) for accuracies at both channel and full parse tree (channel + function) levels.",5.3 Results,[0],[0]
"Our full model performs on par with existing neural network-based methods, while outperforming other neural models in full tree accuracy (82.0%).",5.3 Results,[0],[0]
"This score is close to the best classical method (LR), which is based on a logistic regression model with rich hand-engineered features (e.g., brown clusters and paraphrase).",5.3 Results,[0],[0]
Also note that the performance between NMT and other neural models is much closer compared with the results in Tab. 3.,5.3 Results,[0],[0]
"This suggests that general-purpose code generation is more challenging than the simpler IFTTT setting, and therefore modeling structural information is more helpful.",5.3 Results,[0],[0]
Case Studies,5.3 Results,[0],[0]
We present output examples in Tab. 5.,5.3 Results,[0],[0]
"On HS, we observe that most of the time our model gives correct predictions by filling learned code templates from training data with arguments (e.g., cost) copied from input.",5.3 Results,[0],[0]
This is in line with the findings in Ling et al. (2016).,5.3 Results,[0],[0]
"However, we do find interesting examples indicating that the model learns to generalize beyond trivial
copying.",5.3 Results,[0],[0]
"For instance, the first example is one that our model predicted wrong — it generated code block A instead of the gold B (it also missed a function definition not shown here).",5.3 Results,[0],[0]
"However, we find that the block A actually conveys part of the input intent by destroying all, not some, of the minions.",5.3 Results,[0],[0]
"Since we are unable to find code block A in the training data, it is clear that the model has learned to generalize to some extent from multiple training card examples with similar semantics or structure.
",5.3 Results,[0],[0]
The next two examples are from DJANGO.,5.3 Results,[0],[0]
"The first one shows that the model learns the usage of common API calls (e.g., os.path.join), and how to populate the arguments by copying from inputs.",5.3 Results,[0],[0]
"The second example illustrates the difficulty of generating code with complex nested structures like lambda functions, a scenario worth further investigation in future studies.",5.3 Results,[0],[0]
More examples are attached in supplementary materials.,5.3 Results,[0],[0]
"Error Analysis To understand the sources of errors and how good our evaluation metric (exact match) is, we randomly sampled and labeled 100 and 50 failed examples (with accuracy=0) from DJANGO and HS, respectively.",5.3 Results,[0],[0]
We found that around 2% of these examples in the two datasets are actually semantically equivalent.,5.3 Results,[0],[0]
These examples include: (1) using different parameter names when defining a function; (2) omitting (or adding) default values of parameters in function calls.,5.3 Results,[0],[0]
"While the rarity of such examples suggests that our exact match metric is reasonable, more advanced evaluation metrics based on statistical code analysis are definitely intriguing future work.
",5.3 Results,[0],[0]
"For DJANGO, we found that 30% of failed cases were due to errors where the pointer network failed to appropriately copy a variable name into the correct position.",5.3 Results,[0],[0]
25% were because the generated code only partially implemented the required functionality.,5.3 Results,[0],[0]
"10% and 5% of errors were due to malformed English inputs and pre-processing errors, respectively.",5.3 Results,[0],[0]
"The remaining 30% of examples were errors stemming from multiple sources, or errors that could not be easily categorized into the above.",5.3 Results,[0],[0]
"For HS, we found that all failed card examples were due to partial implementation errors, such as the one shown in Table 5.",5.3 Results,[0],[0]
"Code Generation and Analysis Most works on code generation focus on generating code for domain specific languages (DSLs) (Kushman and
Barzilay, 2013; Raza et al., 2015; Manshadi et al., 2013), with neural network-based approaches recently explored (Liu et al., 2016; Parisotto et al., 2016; Balog et al., 2016).",6 Related Work,[0],[0]
"For general-purpose code generation, besides the general framework of Ling et al. (2016), existing methods often use language and task-specific rules and strategies (Lei et al., 2013; Raghothaman et al., 2016).",6 Related Work,[0],[0]
"A similar line is to use NL queries for code retrieval (Wei et al., 2015; Allamanis et al., 2015).",6 Related Work,[0],[0]
"The reverse task of generating NL summaries from source code has also been explored (Oda et al., 2015; Iyer et al., 2016).",6 Related Work,[0],[0]
"Finally, our work falls into the broad field of probabilistic modeling of source code (Maddison and Tarlow, 2014; Nguyen et al., 2013).",6 Related Work,[0],[0]
"Our approach of factoring an AST using probabilistic models is closely related to Allamanis et al. (2015), which uses a factorized model to measure the semantic relatedness between NL and ASTs for code retrieval, while our model tackles the more challenging generation task.
",6 Related Work,[0],[0]
"Semantic Parsing Our work is related to the general topic of semantic parsing, which aims to transform NL descriptions into executable logical forms.",6 Related Work,[0],[0]
The target logical forms can be viewed as DSLs.,6 Related Work,[0],[0]
"The parsing process is often guided by grammatical formalisms like combinatory categorical grammars (Kwiatkowski et al., 2013; Artzi et al., 2015), dependency-based syntax (Liang et al., 2011; Pasupat and Liang, 2015) or taskspecific formalisms (Clarke et al., 2010; Yih et al., 2015; Krishnamurthy et al., 2016; Mei et al., 2016).",6 Related Work,[0],[0]
"Recently, there are efforts in designing neural network-based semantic parsers (Misra and Artzi, 2016; Dong and Lapata, 2016; Neelakantan et al., 2016; Yin et al., 2016).",6 Related Work,[0],[0]
"Several approaches have be proposed to utilize grammar knowledge in a neural parser, such as augmenting the training data by generating examples guided by the grammar (Kociský et al., 2016; Jia and Liang, 2016).",6 Related Work,[0],[0]
Liang et al. (2016) used a neural decoder which constrains the space of next valid tokens in the query language for question answering.,6 Related Work,[0],[0]
"Finally, the structured prediction approach proposed by Xiao et al. (2016) is closely related to our model in using the underlying grammar as prior knowledge to constrain the generation process of derivation trees, while our method is based on a unified grammar model which jointly captures production rule application and terminal symbol generation, and scales to general purpose code generation tasks.",6 Related Work,[0],[0]
This paper proposes a syntax-driven neural code generation approach that generates an abstract syntax tree by sequentially applying actions from a grammar model.,7 Conclusion,[0],[0]
Experiments on both code generation and semantic parsing tasks demonstrate the effectiveness of our proposed approach.,7 Conclusion,[0],[0]
We are grateful to Wang Ling for his generous help with LPN and setting up the benchmark.,Acknowledgment,[0],[0]
We thank I. Beltagy for providing the IFTTT dataset.,Acknowledgment,[0],[0]
We also thank Li Dong for helping with SEQ2TREE and insightful discussions.,Acknowledgment,[0],[0]
We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python.,abstractText,[0],[0]
Existing datadriven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language.,abstractText,[0],[0]
"Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge.",abstractText,[0],[0]
"Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",abstractText,[0],[0]
A Syntactic Neural Model for General-Purpose Code Generation,title,[0],[0]
"Greedy transition-based dependency parsers (Nivre, 2008) incrementally process an input sentence from left to right.",1 Introduction,[0],[0]
"These parsers are very fast and provide competitive parsing accuracies (Nivre et al., 2007).",1 Introduction,[0],[0]
"However, greedy transition-based parsers still fall behind search-based parsers (Zhang and Clark, 2008; Huang and Sagae, 2010) with respect to accuracy.
",1 Introduction,[0],[0]
"The training of transition-based parsers relies on a component called the parsing oracle, which maps parser configurations to optimal transitions with respect to a gold tree.",1 Introduction,[0],[0]
A discriminative model is then trained to simulate the oracle’s behavior.,1 Introduction,[0],[0]
A parsing oracle is deterministic if it returns a single canonical transition.,1 Introduction,[0],[0]
"Furthermore, an oracle is partial if it is defined only for configurations that can reach the gold tree, that is, configurations representing parsing histories with no mistake.",1 Introduction,[0],[0]
Oracles that are both deterministic and partial are called static.,1 Introduction,[0],[0]
"Traditionally, only static oracles have been exploited in training of transition-based parsers.
",1 Introduction,[0],[0]
"Recently, Goldberg and Nivre (2012; 2013) showed that the accuracy of greedy parsers can be substantially improved without affecting their parsing speed.",1 Introduction,[0],[0]
"This improvement relies on the introduction of novel oracles that are nondeterministic
and complete.",1 Introduction,[0],[0]
"An oracle is nondeterministic if it returns the set of all transitions that are optimal with respect to the gold tree, and it is complete if it is well-defined and correct for every configuration that is reachable by the parser.",1 Introduction,[0],[0]
"Oracles that are both nondeterministic and complete are called dynamic.
Goldberg and Nivre (2013) develop dynamic oracles for several transition-based parsers.",1 Introduction,[0],[0]
The construction of these oracles is based on a property of transition-based parsers that they call arc decomposition.,1 Introduction,[0],[0]
"They also prove that the popular arc-standard system (Nivre, 2004) is not arc-decomposable, and they leave as an open research question the construction of a dynamic oracle for the arc-standard system.",1 Introduction,[0],[0]
"In this article, we develop one such oracle (§4) and prove its correctness (§5).
",1 Introduction,[0],[0]
"An extension to the arc-standard parser was presented by Sartorio et al. (2013), which relaxes the bottom-up construction order and allows mixing of bottom-up and top-down strategies.",1 Introduction,[0],[0]
"This parser, called here the LR-spine parser, achieves state-ofthe-art results for greedy parsing.",1 Introduction,[0],[0]
"Like the arc-standard system, the LR-spine parser is not arc-decomposable, and a dynamic oracle for this system was not known.",1 Introduction,[0],[0]
"We extend our oracle for the arc-standard system to work for the LR-spine system as well (§6).
",1 Introduction,[0],[0]
The dynamic oracles developed by Goldberg and Nivre (2013) for arc-decomposable systems are based on local properties of computations.,1 Introduction,[0],[0]
"In contrast, our novel dynamic oracle algorithms rely on arguably more complex structural properties of computations, which are computed through dynamic programming.",1 Introduction,[0],[0]
"This leaves open the question of whether a machine-learning model can learn to effectively simulate such complex processes: will the
119
Transactions of the Association for Computational Linguistics, 2 (2014) 119–130.",1 Introduction,[0],[0]
Action Editor: Ryan McDonald.,1 Introduction,[0],[0]
Submitted 11/2013; Revised 2/2014; Published 4/2014.,1 Introduction,[0],[0]
"c©2014 Association for Computational Linguistics.
benefit of training with the dynamic oracle carry over to the arc-standard and LR-spine systems?",1 Introduction,[0],[0]
"We show experimentally that this is indeed the case (§8), and that using the training-with-exploration method of (Goldberg and Nivre, 2013) with our dynamic programming based oracles yields superior parsing accuracies on many languages.",1 Introduction,[0],[0]
"In this section we introduce the arc-standard parser of Nivre (2004), which is the model that we use in this article.",2 Arc-Standard Parser,[0],[0]
"To keep the notation at a simple level, we only discuss the unlabeled version of the parser; however, a labeled extension is used in §8 for our experiments.",2 Arc-Standard Parser,[0],[0]
The set of non-negative integers is denoted as N0.,2.1 Preliminaries and Notation,[0],[0]
"For i, j ∈ N0 with i ≤ j, we write [i, j] to denote the set {i, i + 1, . . .",2.1 Preliminaries and Notation,[0],[0]
", j}.",2.1 Preliminaries and Notation,[0],[0]
"When i > j, [i, j] denotes the empty set.
",2.1 Preliminaries and Notation,[0],[0]
"We represent an input sentence as a string w = w0 · · ·wn, n ∈ N0, where token w0 is a special root symbol, and each wi with i ∈",2.1 Preliminaries and Notation,[0],[0]
"[1, n] is a lexical token.",2.1 Preliminaries and Notation,[0],[0]
"For i, j ∈",2.1 Preliminaries and Notation,[0],[0]
"[0, n] with i ≤ j, we write w[i, j] to denote the substring wiwi+1 · · ·wj of w.
We write i → j to denote a grammatical dependency of some unspecified type between lexical tokens wi and wj , where wi is the head and wj is the dependent.",2.1 Preliminaries and Notation,[0],[0]
"A dependency tree for w is a directed, ordered tree t =",2.1 Preliminaries and Notation,[0],[0]
"(Vw, A), such that Vw = [0, n] is the set of nodes, A ⊆ Vw×Vw is the set of arcs, and node 0 is the root.",2.1 Preliminaries and Notation,[0],[0]
"Arc (i, j) encodes a dependency i → j, and we will often use the latter notation to denote arcs.",2.1 Preliminaries and Notation,[0],[0]
We assume the reader is familiar with the formal framework of transition-based dependency parsing originally introduced by Nivre (2003); see Nivre (2008) for an introduction.,2.2 Transition-Based Dependency Parsing,[0],[0]
"We only summarize here our notation.
",2.2 Transition-Based Dependency Parsing,[0],[0]
"Transition-based dependency parsers use a stack data structure, where each stack element is associated with a tree spanning (generating) some substring of the input w.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The parser processes the input string incrementally, from left to right, applying at each step a transition that updates the stack and/or
consumes one token from the input.",2.2 Transition-Based Dependency Parsing,[0],[0]
"Transitions may also construct new dependencies, which are added to the current configuration of the parser.
",2.2 Transition-Based Dependency Parsing,[0],[0]
We represent the stack data structure as an ordered sequence σ =,2.2 Transition-Based Dependency Parsing,[0],[0]
"[σd, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", σ1], d ∈ N0, of nodes",2.2 Transition-Based Dependency Parsing,[0],[0]
"σi ∈ Vw, with the topmost element placed at the right.",2.2 Transition-Based Dependency Parsing,[0],[0]
"When d = 0, we have the empty stack σ =",2.2 Transition-Based Dependency Parsing,[0],[0]
[].,2.2 Transition-Based Dependency Parsing,[0],[0]
"Sometimes we use the vertical bar to denote the append operator for σ, and write σ = σ′|σ1 to indicate that σ1 is the topmost element of σ.
",2.2 Transition-Based Dependency Parsing,[0],[0]
The parser also uses a buffer to store the portion of the input string still to be processed.,2.2 Transition-Based Dependency Parsing,[0],[0]
We represent the buffer as an ordered sequence β =,2.2 Transition-Based Dependency Parsing,[0],[0]
"[i, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n] of nodes from Vw, with i the first element of the buffer.",2.2 Transition-Based Dependency Parsing,[0],[0]
In this way β always encodes a (non-necessarily proper) suffix of w. We denote the empty buffer as β =,2.2 Transition-Based Dependency Parsing,[0],[0]
[].,2.2 Transition-Based Dependency Parsing,[0],[0]
"Sometimes we use the vertical bar to denote the append operator for β, and write β = i|β′ to indicate that i is the first token of β; consequently, we have β′ =",2.2 Transition-Based Dependency Parsing,[0],[0]
"[i+ 1, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n",2.2 Transition-Based Dependency Parsing,[0],[0]
"].
When processing w, the parser reaches several states, technically called configurations.",2.2 Transition-Based Dependency Parsing,[0],[0]
"A configuration of the parser relative to w is a triple c = (σ, β,A), where σ and β are a stack and a buffer, respectively, and A ⊆ Vw × Vw is a set of arcs.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The initial configuration for w is ([], [0, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", n], ∅).",2.2 Transition-Based Dependency Parsing,[0],[0]
"For the purpose of this article, a configuration is final if it has the form ([0], [], A), and in a final configuration arc set A always defines a dependency tree for w.
The core of a transition-based parser is the set of its transitions, which are specific to each family of parsers.",2.2 Transition-Based Dependency Parsing,[0],[0]
A transition is a binary relation defined over the set of configurations of the parser.,2.2 Transition-Based Dependency Parsing,[0],[0]
"We use symbol ` to denote the union of all transition relations of a parser.
",2.2 Transition-Based Dependency Parsing,[0],[0]
"A computation of the parser on w is a sequence c0, . . .",2.2 Transition-Based Dependency Parsing,[0],[0]
", cm, m ∈ N0, of configurations (defined relative to w) such that ci−1 ` ci for each",2.2 Transition-Based Dependency Parsing,[0],[0]
i ∈,2.2 Transition-Based Dependency Parsing,[0],[0]
"[1,m].",2.2 Transition-Based Dependency Parsing,[0],[0]
We also use the reflexive and transitive closure relation `∗ to represent computations.,2.2 Transition-Based Dependency Parsing,[0],[0]
A computation is called complete whenever c0 is initial and cm is final.,2.2 Transition-Based Dependency Parsing,[0],[0]
"In this way, a complete computation is uniquely associated with a dependency tree for w.",2.2 Transition-Based Dependency Parsing,[0],[0]
"The arc-standard model uses the three types of transitions formally specified in Figure 1
(σ, i|β,A) `sh (σ|i, β, A) (σ|i|j, β,A) `la (σ|j, β,A ∪ {j → i}) (σ|i|j, β,A) `ra (σ|i, β, A ∪ {i→ j})
",2.3 Arc-Standard Parser,[0],[0]
Notation We sometimes use the functional notation for a transition τ ∈,2.3 Arc-Standard Parser,[0],[0]
"{sh, la, ra}, and write τ(c) = c′ in place of c",2.3 Arc-Standard Parser,[0],[0]
`τ c′.,2.3 Arc-Standard Parser,[0],[0]
"Naturally, sh applies only when the buffer is not empty, and la,ra require two elements on the stack.",2.3 Arc-Standard Parser,[0],[0]
We denote by valid(c) the set of valid transitions in a given configuration.,2.3 Arc-Standard Parser,[0],[0]
"Goldberg and Nivre (2013) show how to derive dynamic oracles for any transition-based parser which has the arc decomposition property, defined below.",2.4 Arc Decomposition,[0],[0]
"They also show that the arc-standard parser is not arc-decomposable.
",2.4 Arc Decomposition,[0],[0]
"For a configuration c, we write Ac to denote the associated set of arcs.",2.4 Arc Decomposition,[0],[0]
"A transition-based parser is arc-decomposable if, for every configuration c and for every set of arcs A that can be extended to a projective tree, we have
∀(i→ j) ∈ A,∃c′[c",2.4 Arc Decomposition,[0],[0]
"`∗ c′ ∧ (i→ j) ∈ Ac′ ] ⇒ ∃c′′[c `∗ c′′ ∧A ⊆ Ac′′ ] .
",2.4 Arc Decomposition,[0],[0]
"In words, if each arc in A is individually derivable from c, then the set A in its entirety can be derived from c as well.",2.4 Arc Decomposition,[0],[0]
"The arc decomposition property is useful for deriving dynamic oracles because it is relatively easy to investigate derivability for single arcs and then, using this property, draw conclusions about the number of gold-arcs that are simultaneously derivable from the given configuration.
",2.4 Arc Decomposition,[0],[0]
"Unfortunately, the arc-standard parser is not arcdecomposable.",2.4 Arc Decomposition,[0],[0]
"To see why, consider a configuration with stack σ",2.4 Arc Decomposition,[0],[0]
=,2.4 Arc Decomposition,[0],[0]
"[i, j, k].",2.4 Arc Decomposition,[0],[0]
"Consider also arc set A = {(i, j), (i, k)}.",2.4 Arc Decomposition,[0],[0]
"The arc (i, j) can be derived through the transition sequence ra, ra, and the arc (i, k) can be derived through the alternative transition sequence la, ra.",2.4 Arc Decomposition,[0],[0]
"Yet, it is easy to see that a configuration containing both arcs cannot be reached.
",2.4 Arc Decomposition,[0],[0]
"As we cannot rely on the arc decomposition property, in order to derive a dynamic oracle for the arcstandard model we need to develop more sophisticated techniques which take into account the interaction among the applied transitions.",2.4 Arc Decomposition,[0],[0]
We aim to derive a dynamic oracle for the arc-standard (and related) system.,3 Configuration Loss and Dynamic Oracles,[0],[0]
"This is a function that takes a configuration c and a gold tree tG and returns a set of transitions that are “optimal” for c with respect to tG. As already mentioned in the introduction, a dynamic oracle can be used to improve training of greedy transition-based parsers.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"In this section we provide a formal definition for a dynamic oracle.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Let t1 and t2 be two dependency trees over the same stringw, with arc setsA1 andA2, respectively.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We define the loss of t1 with respect to t2 as
L(t1, t2) = |A1 \A2| .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(1)
Note that L(t1, t2) = L(t2, t1), since |A1| = |A2|.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Furthermore L(t1, t2) = 0",3 Configuration Loss and Dynamic Oracles,[0],[0]
"if and only if t1 and t2 are the same tree.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
Let c be a configuration of our parser relative to input string w. We write D(c) to denote the set of all dependency trees that can be obtained in a computation of the form c,3 Configuration Loss and Dynamic Oracles,[0],[0]
"`∗ cf , where cf is some final configuration.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We extend the loss function in (1) to configurations by letting
L(c, t2) = min t1∈D(c) L(t1, t2) .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(2)
Assume some reference (desired) dependency tree tG for w, which we call the gold tree.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Quantity L(c, tG) can be used to compute a dynamic oracle relating a parser configuration c to a set of optimal actions by setting
oracle(c, tG)",3 Configuration Loss and Dynamic Oracles,[0],[0]
"=
{τ | L(τ(c), tG)− L(c, tG) = 0} .",3 Configuration Loss and Dynamic Oracles,[0],[0]
"(3)
We therefore need to develop an algorithm for computing (2).",3 Configuration Loss and Dynamic Oracles,[0],[0]
"We will do this first for the arc-standard parser, and then for an extension of this model.
",3 Configuration Loss and Dynamic Oracles,[0],[0]
"Notation We also apply the loss function L(t, tG) in (1) when t is a dependency tree for a substring of w.",3 Configuration Loss and Dynamic Oracles,[0],[0]
"In this case the nodes of t are a subset of the nodes of tG, and L(t, tG) provides a count of the nodes of t that are assigned a wrong head node, when tG is considered as the reference tree.",3 Configuration Loss and Dynamic Oracles,[0],[0]
Throughout this section we assume an arc-standard parser.,4 Main Algorithm,[0],[0]
Our algorithm takes as input a projective gold tree tG and a configuration c =,4 Main Algorithm,[0],[0]
"(σL, β, A).",4 Main Algorithm,[0],[0]
"We call σL the left stack, in contrast with a right stack whose construction is specified below.",4 Main Algorithm,[0],[0]
The algorithm consists of two steps.,4.1 Basic Idea,[0],[0]
"Informally, in the first step we compute the largest subtrees, called here tree fragments, of the gold tree tG that have their span entirely included in the buffer β.",4.1 Basic Idea,[0],[0]
"The root nodes of these tree fragments are then arranged into a stack data structure, according to the order in which they appear in β and with the leftmost root in β being the topmost element of the stack.",4.1 Basic Idea,[0],[0]
"We call this structure the right stack σR. Intuitively, σR can be viewed as the result of pre-computing β by applying all sequences of transitions that match tG and that can be performed independently of the stack in the input configuration c, that is, σL.
",4.1 Basic Idea,[0],[0]
"In the second step of the algorithm we use dynamic programming techniques to simulate all computations of the arc-standard parser starting in a configuration with stack σL and with a buffer consisting of σR, with the topmost token of σR being the first token of the buffer.",4.1 Basic Idea,[0],[0]
"As we will see later, the search space defined by these computations includes the dependency trees for w that are reachable from the input configuration c and that have minimum loss.",4.1 Basic Idea,[0],[0]
"We then perform a Viterbi search to pick up such value.
",4.1 Basic Idea,[0],[0]
"The second step is very similar to standard implementations of the CKY parser for context-free grammars (Hopcroft and Ullman, 1979), running on an input string obtained as the concatenation of σL and σR. The main difference is that we restrict ourselves to parse only those constituents in σLσR that dominate the topmost element of σL (the rightmost ele-
ment, if σL is viewed as a string).",4.1 Basic Idea,[0],[0]
"In this way, we account for the additional constraint that we visit only those configurations of the arc-standard parser that can be reached from the input configuration c. For instance, this excludes the reduction of two nodes in σL that are not at the two topmost positions.",4.1 Basic Idea,[0],[0]
"This would also exclude the reduction of two nodes in σR: this is correct, since the associated tree fragments have been chosen as the largest such fragments in β.
",4.1 Basic Idea,[0],[0]
"The above intuitive explanation will be made mathematically precise in §5, where the notion of linear dependency tree is introduced.",4.1 Basic Idea,[0],[0]
"In the first step we process β and construct a stack σR, which we call the right stack associated with c and tG.",4.2 Construction of the Right Stack,[0],[0]
"Each node of σR is the root of a tree t which satisfies the following properties
• t is a tree fragment of the gold tree tG",4.2 Construction of the Right Stack,[0],[0]
"having span entirely included in the buffer β;
• t is bottom-up complete for tG, meaning that for each node i of t different from t’s root, the dependents of i in tG cannot be in σL;
• t is maximal for tG, meaning that every supertree of t in tG violates the above conditions.
",4.2 Construction of the Right Stack,[0],[0]
The stack σR is incrementally constructed by processig β from left to right.,4.2 Construction of the Right Stack,[0],[0]
"Each node i is copied into σR if it satisfies any of the following conditions
• the parent node of i in tG is not in β;
• some dependent of i in tG is in σL or has already been inserted in σR.
It is not difficult to see that the nodes in σR are the roots of tree fragments of tG that satisfy the condition of bottom-up completeness and the condition of maximality defined above.",4.2 Construction of the Right Stack,[0],[0]
We start with some notation.,4.3 Computation of Configuration Loss,[0],[0]
Let `L = |σL| and `R = |σR|.,4.3 Computation of Configuration Loss,[0],[0]
We write σL[i] to denote the i-th element of σL and t(σL[i]) to denote the corresponding tree fragment; σR[i] and t(σR[i]) have a similar meaning.,4.3 Computation of Configuration Loss,[0],[0]
"In order to simplify the specification of the algorithm, we assume below that σL[1] = σR[1].
",4.3 Computation of Configuration Loss,[0],[0]
Algorithm 1 Computation of the loss function for the arc-standard parser 1: T,4.3 Computation of Configuration Loss,[0],[0]
"[1, 1](σL[1])← L(t(σL[1]), tG) 2: for d← 1 to `L + `R − 1 do .",4.3 Computation of Configuration Loss,[0],[0]
"d is the index of a sub-anti-diagonal 3: for j ← max{1, d− `L + 1} to min{d, `R} do .",4.3 Computation of Configuration Loss,[0],[0]
j is the column index 4: i← d− j + 1 .,4.3 Computation of Configuration Loss,[0],[0]
i is the row index 5:,4.3 Computation of Configuration Loss,[0],[0]
if i <,4.3 Computation of Configuration Loss,[0],[0]
`L then .,4.3 Computation of Configuration Loss,[0],[0]
"expand to the left 6: for each h ∈ ∆i,j do 7: T",4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](h)← min{T",4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](h), T [i, j](h) + δG(h→",4.3 Computation of Configuration Loss,[0],[0]
σL[i+ 1])} 8:,4.3 Computation of Configuration Loss,[0],[0]
T,4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1,",4.3 Computation of Configuration Loss,[0],[0]
j](σL[i+ 1])← min{T,4.3 Computation of Configuration Loss,[0],[0]
"[i+ 1, j](σL[i+ 1]), T [i, j](h) +",4.3 Computation of Configuration Loss,[0],[0]
δG(σL[i+ 1]→ h)} 9: if j < `R then .,4.3 Computation of Configuration Loss,[0],[0]
"expand to the right 10: for each h ∈ ∆i,j do 11:",4.3 Computation of Configuration Loss,[0],[0]
T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j + 1](h)← min{T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j + 1](h), T [i, j](h) + δG(h→ σR[j + 1])} 12: T [i, j+1](σR[j + 1])←",4.3 Computation of Configuration Loss,[0],[0]
min{T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j+1](σR[j + 1]), T [i, j](h)+δG(σR[j + 1]→ h)} 13: return T",4.3 Computation of Configuration Loss,[0],[0]
"[`L, `R](0) + ∑ i∈[1,`L] L(t(σL[i]), tG)
",4.3 Computation of Configuration Loss,[0],[0]
"Therefore the elements of σR which have been constructed in §4.2 are σR[i], i ∈",4.3 Computation of Configuration Loss,[0],[0]
"[2, `R].
Algorithm 1 uses a two-dimensional array T of size `L × `R, where each entry T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j] is an association list from integers to integers.",4.3 Computation of Configuration Loss,[0],[0]
"An entry T [i, j](h) stores the minimum loss among dependency trees rooted at h that can be obtained by running the parser on the first i elements of stack σL and the first j elements of buffer σR. More precisely, let
∆i,j = {σL[k] | k ∈",4.3 Computation of Configuration Loss,[0],[0]
"[1, i]} ∪ {σR[k] | k ∈",4.3 Computation of Configuration Loss,[0],[0]
"[1, j]} .",4.3 Computation of Configuration Loss,[0],[0]
"(4)
",4.3 Computation of Configuration Loss,[0],[0]
"For each h ∈ ∆i,j , the entry T [i, j](h) is the minimum loss among all dependency trees defined as above and with root h.",4.3 Computation of Configuration Loss,[0],[0]
"We also assume that T [i, j](h) is initialized to +∞ (not reported in the algorithm).
",4.3 Computation of Configuration Loss,[0],[0]
"Algorithm 1 starts at the top-left corner of T , visiting each individual sub-anti-diagonal of T in ascending order, and eventually reaching the bottomright corner of the array.",4.3 Computation of Configuration Loss,[0],[0]
For each entry T,4.3 Computation of Configuration Loss,[0],[0]
"[i, j], the left expansion is considered (lines 5 to 8) by combining with tree fragment σL[i+ 1], through a left or a right arc reduction.",4.3 Computation of Configuration Loss,[0],[0]
This results in the update of T,4.3 Computation of Configuration Loss,[0],[0]
"[i + 1, j](h), for each h ∈ ∆i+1,j , whenever a smaller value of the loss is achieved for a tree with root h.",4.3 Computation of Configuration Loss,[0],[0]
The Kronecker-like function used at line 8 provides the contribution of each single arc to the loss of the current tree.,4.3 Computation of Configuration Loss,[0],[0]
"Denoting with AG the set of
arcs of tG, such a function is defined as
δG(i→ j) = {
0, if (i→ j) ∈ AG; 1, otherwise.",4.3 Computation of Configuration Loss,[0],[0]
"(5)
A symmetrical process is implemented for the right expansion of T",4.3 Computation of Configuration Loss,[0],[0]
"[i, j] through tree fragment σR[j + 1] (lines 9 to 12).
",4.3 Computation of Configuration Loss,[0],[0]
"As we will see in the next section, quantity T [`L, `R](0) is the minimal loss of a tree composed only by arcs that connect nodes in σL and σR. By summing the loss of all tree fragments t(σL[i]) to the loss in T",4.3 Computation of Configuration Loss,[0],[0]
"[`L, `R](0), at line 13, we obtain the desired result, since the loss of each tree fragment t(σR[j]) is zero.",4.3 Computation of Configuration Loss,[0],[0]
"Throughout this section we let w, tG, σL, σR and c = (σL, β, A) be defined as in §4, but we no longer assume that σL[1] = σR[1].",5 Formal Properties,[0],[0]
"To simplify the presentation, we sometimes identify the tokens in w with the associated nodes in a dependency tree for w.",5 Formal Properties,[0],[0]
"Algorithm 1 explores all dependency trees that can be reached by an arc-standard parser from configuration c, under the condition that (i) the nodes in the buffer β are pre-computed into tree fragments and collapsed into their root nodes in the right stack σR, and (ii) nodes in σR cannot be combined together prior to their combination with other nodes in the left stack",5.1 Linear Trees,[0],[0]
"σL. This set of dependency trees is char-
acterized here using the notion of linear tree, to be used later in the correctness proof.
",5.1 Linear Trees,[0],[0]
Consider two nodes σL[i] and σL[j] with j >,5.1 Linear Trees,[0],[0]
i,5.1 Linear Trees,[0],[0]
> 1.,5.1 Linear Trees,[0],[0]
"An arc-standard parser can construct an arc between σL[i] and σL[j], in any direction, only after reaching a configuration in which σL[i] is at the top of the stack and σL[j] is at the second topmost position.",5.1 Linear Trees,[0],[0]
In such configuration we have that σL[i] dominates σL[1].,5.1 Linear Trees,[0],[0]
"Furthermore, consider nodes σR[i] and σR[j] with j > i ≥ 1.",5.1 Linear Trees,[0],[0]
"Since we are assuming that tree fragments t(σR[i]) and t(σR[j]) are bottom-up complete and maximal, as defined in §4.2, we allow the construction of an arc between σR[i] and σR[j], in any direction, only after reaching a configuration in which σR[i] dominates node σL[1].
",5.1 Linear Trees,[0],[0]
The dependency trees satisfying the restrictions above are captured by the following definition.,5.1 Linear Trees,[0],[0]
"A linear tree over (σL, σR) is a projective dependency tree t for string σLσR satisfying both of the additional conditions reported below.",5.1 Linear Trees,[0],[0]
"The path from t’s root to node σL[1] is called the spine of t.
•",5.1 Linear Trees,[0],[0]
"Every node of t not in the spine is a dependent of some node in the spine.
",5.1 Linear Trees,[0],[0]
• For each arc,5.1 Linear Trees,[0],[0]
"i → j in t with j in the spine, no dependent of i can be placed in between i and j within string σLσR.
An example of a linear tree is depicted in Figure 2.",5.1 Linear Trees,[0],[0]
"Observe that the second condition above forbids the reduction of two nodes i and j, in case none of these dominates node σL[1].",5.1 Linear Trees,[0],[0]
"For instance, the ra reduction of nodes i3 and i2 would result in arc i3 → i2 replacing arc i1 → i2 in Figure 2.",5.1 Linear Trees,[0],[0]
"The new dependency tree is not linear, because of a violation of the
second condition above.",5.1 Linear Trees,[0],[0]
"Similarly, the la reduction of nodes j3 and j4 would result in arc j4 → j3 replacing arc i3 → j3 in Figure 2, again a violation of the second condition above.
",5.1 Linear Trees,[0],[0]
"Lemma 1 Any tree t ∈ D(c) can be decomposed into trees t(σL[i]), i ∈",5.1 Linear Trees,[0],[0]
"[1, `L], trees tj , j ∈",5.1 Linear Trees,[0],[0]
"[1, q] and q ≥ 1, and a linear tree tl over (σL, σR,t), where σR,t = r1 · · · rq and each rj is the root node of tj . 2 PROOF (SKETCH)",5.1 Linear Trees,[0],[0]
"Trees t(σL[i]) are common to every tree in D(c), since the arc-standard model can not undo the arcs already built in the current configuration c. Similar to the construction in §4.2 of the right stack σR from tG, we let tj , j ∈",5.1 Linear Trees,[0],[0]
"[1, q], be tree fragments of t that cover only nodes associated with the tokens in the buffer β",5.1 Linear Trees,[0],[0]
and that are bottomup complete and maximal for t. These trees are indexed according to their left to right order in β.,5.1 Linear Trees,[0],[0]
"Finally, tl is implicitly defined by all arcs of t that are not in trees t(σL[i]) and tj .",5.1 Linear Trees,[0],[0]
"It is not difficult to see that tl has a spine ending with node σL[1] and is a linear tree over (σL, σR,t).",5.1 Linear Trees,[0],[0]
"Our proof of correctness for Algorithm 1 is based on a specific dependency tree t∗ for w, which we define below.",5.2 Correctness,[0],[0]
Let SL = {σL[i] | i ∈,5.2 Correctness,[0],[0]
"[1, `L]} and letDL be the set of nodes that are descendants of some node in SL.",5.2 Correctness,[0],[0]
"Similarly, let SR = {σR[i] | i ∈",5.2 Correctness,[0],[0]
"[1, `R]} and let DR be the set of descendants of nodes in SR.",5.2 Correctness,[0],[0]
"Note that sets SL, SR, DL and DR provide a partition of Vw.
",5.2 Correctness,[0],[0]
"We choose any linear tree t∗l over (σL, σR) having root 0, such that L(t∗l , tG)",5.2 Correctness,[0],[0]
"= mint L(t, tG), where t ranges over all possible linear trees over (σL, σR) with root 0.",5.2 Correctness,[0],[0]
"Tree t∗ consists of the set of nodes Vw and the set of arcs obtained as the union of the set of arcs of t∗l and the set of arcs of all trees t(σL[i]), i ∈",5.2 Correctness,[0],[0]
"[1, `L], and t(σR[j]), j ∈",5.2 Correctness,[0],[0]
"[1, `R].",5.2 Correctness,[0],[0]
Lemma 2 t∗ ∈ D(c).,5.2 Correctness,[0],[0]
2 PROOF (SKETCH),5.2 Correctness,[0],[0]
All tree fragments t(σL[i]) have already been parsed and are available in the stack associated with c.,5.2 Correctness,[0],[0]
"Each tree fragment t(σR[j]) can later be constructed in the computation, when a configuration c′ is reached with the relevant segment of w at the start of the buffer.",5.2 Correctness,[0],[0]
"Note also that parsing of t(σR[j]) can be done in a way that does not depend on the content of the stack in c′.
Finally, the parsing of the tree fragments t(σR[j]) is interleaved with the construction of the arcs from the linear tree t∗l , which are all of the form (i → j) with i, j ∈ (SL ∪ SR).",5.2 Correctness,[0],[0]
"More precisely, if (i → j) is an arc from t∗l , at some point in the computation nodes i and j will become available at the two topmost positions in the stack.",5.2 Correctness,[0],[0]
"This follows from the second condition in the definition of linear tree.
",5.2 Correctness,[0],[0]
"We now show that tree t∗ is “optimal” within the set D(c) and with respect to tG. Lemma 3 L(t∗, tG)",5.2 Correctness,[0],[0]
"= L(c, tG).",5.2 Correctness,[0],[0]
2 PROOF Consider an arbitrary tree t ∈ D(c).,5.2 Correctness,[0],[0]
"Assume the decomposition of t defined in the proof of Lemma 1, through trees t(σL[i]), i ∈",5.2 Correctness,[0],[0]
"[1, `L], trees tj , j ∈",5.2 Correctness,[0],[0]
"[1, q], and linear tree tl over (σL, σR,t).
",5.2 Correctness,[0],[0]
Recall that an arc,5.2 Correctness,[0],[0]
"i → j denotes an ordered pair (i, j).",5.2 Correctness,[0],[0]
"Let us consider the following partition for the set of arcs of any dependency tree for w
A1 = (SL ∪DL)×DL , A2 = (SR ∪DR)×DR , A3 = (Vw × Vw) \",5.2 Correctness,[0],[0]
"(A1 ∪A2) .
",5.2 Correctness,[0],[0]
"In what follows, we compare the losses L(t, tG) and L(t∗, tG) by separately looking into the contribution to such quantities due to the arcs in A1, A2 and A3.
",5.2 Correctness,[0],[0]
"Note that the arcs of trees t(σL[i]) are all in A1, the arcs of trees t(σR[j]) are all in A2, and the arcs of tree t∗l are all in A3.",5.2 Correctness,[0],[0]
"Since t and t
∗ share trees t(σL[i]), when restricted to arcs in A1 quantities L(t, tG) and L(t∗, tG) are the same.",5.2 Correctness,[0],[0]
"When restricted to arcs in A2, quantity L(t∗, tG) is zero, by construction of the trees t(σR[j]).",5.2 Correctness,[0],[0]
"Thus L(t, tG) can not be smaller thanL(t∗, tG) for these arcs.",5.2 Correctness,[0],[0]
"The difficult part is the comparison of the contribution to L(t, tG) and L(t∗, tG) due to the arcs in A3.",5.2 Correctness,[0],[0]
"We deal with this below.
",5.2 Correctness,[0],[0]
"LetAS,G be the set of all arcs from tG that are also in set (SL × SR) ∪ (SR × SL).",5.2 Correctness,[0],[0]
"In words, AS,G represents gold arcs connecting nodes in SL and nodes in SR, in any direction.",5.2 Correctness,[0],[0]
"Within tree t, these arcs can only be found in the tl component, since nodes in SL are all placed within the spine of tl, or else at the left of that spine.
",5.2 Correctness,[0],[0]
Let us consider an arc (j → i) ∈,5.2 Correctness,[0],[0]
"AS,G with j ∈ SL",5.2 Correctness,[0],[0]
"and i ∈ SR, and let us assume that (j → i) is in t∗l .",5.2 Correctness,[0],[0]
"If token ai does not occur in σR,t, node i is not
in tl",5.2 Correctness,[0],[0]
and (j → i) can not be an arc of t.,5.2 Correctness,[0],[0]
"We then have that (j → i) contributes one unit to L(t, tG) but does not contribute to L(t∗, tG).",5.2 Correctness,[0],[0]
"Similarly, let (i → j) ∈",5.2 Correctness,[0],[0]
"AS,G be such that i ∈ SR and j ∈ SL, and assume that (i→ j) is in t∗l .",5.2 Correctness,[0],[0]
"If token ai does not occur in σR,t, arc (i → j) can not be in t.",5.2 Correctness,[0],[0]
"We then have that (i → j) contributes one unit to L(t, tG) but does not contribute to L(t∗, tG).
",5.2 Correctness,[0],[0]
"Intuitively, the above observations mean that the winning strategy for trees in D(c) is to move nodes from SR as much as possible into the linear tree component tl, in order to make it possible for these nodes to connect to nodes in SL, in any direction.",5.2 Correctness,[0],[0]
"In this case, arcs fromA3 will also move into the linear tree component of a tree inD(c), as it happens in the case of t∗.",5.2 Correctness,[0],[0]
"We thus conclude that, when restricted to the set of arcs in A3, quantity L(t, tG) is not smaller than L(t∗, tG), because stack σR has at least as many tokens corresponding to nodes in SR as stack σR,t, and because t∗l has the minimum loss among all the linear trees over (σL, σR).
",5.2 Correctness,[0],[0]
"Putting all of the above observations together, we conclude that L(t, tG) can not be smaller than L(t∗, tG).",5.2 Correctness,[0],[0]
"This concludes the proof, since t has been arbitrarily chosen in D(c).",5.2 Correctness,[0],[0]
"Theorem 1 Algorithm 1 computes L(c, tG).",5.2 Correctness,[0],[0]
2 PROOF (SKETCH),5.2 Correctness,[0],[0]
"Algorithm 1 implements a Viterbi search for trees with smallest loss among all linear trees over (σL, σR).",5.2 Correctness,[0],[0]
"Thus T [`L, `R](0) = L(t∗l , tG).",5.2 Correctness,[0],[0]
The loss of the tree fragments t(σR[j]) is 0 and the loss of the tree fragments t(σL[i]) is added at line 13 in the algorithm.,5.2 Correctness,[0],[0]
"Thus the algorithm returns L(t∗, tG), and the statement follows from Lemma 2 and Lemma 3.",5.2 Correctness,[0],[0]
"Following §4.2, the right stack σR can be easily constructed in time O(n), n the length of the input string.",5.3 Computational Analysis,[0],[0]
We now analyze Algorithm 1.,5.3 Computational Analysis,[0],[0]
For each entry T,5.3 Computational Analysis,[0],[0]
"[i, j] and for each h ∈ ∆i,j , we update T [i, j](h) a number of times bounded by a constant which does not depend on the input.",5.3 Computational Analysis,[0],[0]
Each updating can be computed in constant time as well.,5.3 Computational Analysis,[0],[0]
We thus conclude that Algorithm 1 runs in time O(`L · `R · (`L + `R)).,5.3 Computational Analysis,[0],[0]
"Quantity `L+`R is bounded by n, but in practice the former is significantly smaller.",5.3 Computational Analysis,[0],[0]
"When measured over the sentences in the Penn
Treebank, the average value of `L+`Rn is 0.29.",5.3 Computational Analysis,[0],[0]
"In terms of runtime, training is 2.3 times slower when using our oracle instead of a static oracle.",5.3 Computational Analysis,[0],[0]
"In this section we consider the transition-based parser proposed by Sartorio et al. (2013), called here the LR-spine parser.",6 Extension to the LR-Spine Parser,[0],[0]
This parser is not arcdecomposable: the same example reported in §2.4 can be used to show this fact.,6 Extension to the LR-Spine Parser,[0],[0]
We therefore extend to the LR-spine parser the algorithm developed in §4.,6 Extension to the LR-Spine Parser,[0],[0]
Let t be a dependency tree.,6.1 The LR-Spine Parser,[0],[0]
"The left spine of t is an ordered sequence 〈i1, . . .",6.1 The LR-Spine Parser,[0],[0]
", ip〉, p ≥ 1, consisting of all nodes in a descending path from the root of t taking the leftmost child node at each step.",6.1 The LR-Spine Parser,[0],[0]
The right spine of t is defined symmetrically.,6.1 The LR-Spine Parser,[0],[0]
"We use ⊕ to denote sequence concatenation.
",6.1 The LR-Spine Parser,[0],[0]
"In the LR-spine parser each stack element σ[i] denotes a partially built subtree t(σ[i]) and is represented by a pair (lsi, rsi), with lsi and rsi the left and the right spine, respectively, of t(σ[i]).",6.1 The LR-Spine Parser,[0],[0]
"We write lsi[k] (rsi[k]) to represent the k-th element of lsi (rsi, respectively).",6.1 The LR-Spine Parser,[0],[0]
"We also write r(σ[i]) to denote the root of t(σ[i]), so that r(σ[i]) =",6.1 The LR-Spine Parser,[0],[0]
"lsi[1] = rsi[1].
Informally, the LR-spine parser uses the same transition typologies as the arc-standard parser.",6.1 The LR-Spine Parser,[0],[0]
"However, an arc (h → d) can now be created with the head node h chosen from any node in the spine of the involved tree.",6.1 The LR-Spine Parser,[0],[0]
"The transition types of the LRspine parser are defined as follows.
",6.1 The LR-Spine Parser,[0],[0]
"• Shift (sh) removes the first node from the buffer and pushes into the stack a new element, consisting of the left and right spines of the associated tree
(σ, i|β,A) `sh (σ|(〈i〉, 〈i〉), β, A) .
•",6.1 The LR-Spine Parser,[0],[0]
Left-Arc k (lak) creates a new arc h → d from the k-th node in the left spine of the topmost tree in the stack to the head of the second element in the stack.,6.1 The LR-Spine Parser,[0],[0]
"Furthermore, the two topmost stack elements are replaced by a new element associated with the resulting tree
(σ′|σ[2]|σ[1], β, A) `lak (σ′|σlak , β, A ∪ {h→ d}) where we have set h = ls1[k], d = r(σ[2]) and σlak = (〈ls1[1], . . .",6.1 The LR-Spine Parser,[0],[0]
", ls1[k]〉 ⊕ ls2, rs1).
",6.1 The LR-Spine Parser,[0],[0]
•,6.1 The LR-Spine Parser,[0],[0]
"Right-Arc k (rak for short) is defined symmetrically with respect to lak
(σ′|σ[2]|σ[1], β, A) `rak (σ′|σrak , β, A ∪ {h→ d})
where we have set h = rs2[k], d = r(σ[1]) and σrak = (ls2, 〈rs2[1], . . .",6.1 The LR-Spine Parser,[0],[0]
", rs2[k]〉 ⊕ rs1).
",6.1 The LR-Spine Parser,[0],[0]
"Note that, at each configuration in the LR-spine parser, there are |ls1| possible lak transitions, one for each choice of a node in the left spine of t(σ[1]); similarly, there are |rs2| possible rak transitions, one for each choice of a node in the right spine of t(σ[2]).",6.1 The LR-Spine Parser,[0],[0]
"We only provide an informal description of the extended algorithm here, since it is very similar to the algorithm in §4.
",6.2 Configuration Loss,[0],[0]
"In the first phase we use the procedure of §4.2 for the construction of the right stack σR, considering only the roots of elements in σL and ignoring the rest of the spines.",6.2 Configuration Loss,[0],[0]
"The only difference is that each element σR[j] is now a pair of spines (lsR,j , rsR,j).",6.2 Configuration Loss,[0],[0]
"Since tree fragment t(σR[j]) is bottom-up complete (see §4.1), we now restrict the search space in such a way that only the root node r(σR[j]) can take dependents.",6.2 Configuration Loss,[0],[0]
"This is done by setting lsR,j = rsR,j = 〈r(σR[j])〉 for each j ∈",6.2 Configuration Loss,[0],[0]
"[1, `R].",6.2 Configuration Loss,[0],[0]
"In order to simplify the presentation we also assume σR[1] = σL[1], as done in §4.3.
In the second phase we compute the loss of an input configuration using a two-dimensional array T , defined as in §4.3.",6.2 Configuration Loss,[0],[0]
"However, because of the way transitions are defined in the LR-spine parser, we now need to distinguish tree fragments not only on the basis of their roots, but also on the basis of their left and right spines.",6.2 Configuration Loss,[0],[0]
"Accordingly, we define each entry T",6.2 Configuration Loss,[0],[0]
"[i, j] as an association list with keys of the form (ls, rs).",6.2 Configuration Loss,[0],[0]
"More specifically, T [i, j](ls, rs) is the minimum loss of a tree with left and right spines ls and rs, respectively, that can be obtained by running the parser on the first i elements of stack σL and the first j elements of buffer σR.
We follow the main idea of Algorithm 1 and expand each tree in T",6.2 Configuration Loss,[0],[0]
"[i, j] at its left side, by combining with tree fragment t(σL[i+ 1]), and at its right side, by combining with tree fragment t(σR[j + 1]).
",6.2 Configuration Loss,[0],[0]
"Tree combination deserves some more detailed discussion, reported below.
",6.2 Configuration Loss,[0],[0]
We consider the combination of a tree ta from T,6.2 Configuration Loss,[0],[0]
"[i, j] and tree t(σL[i+ 1])",6.2 Configuration Loss,[0],[0]
by means of a left-arc transition.,6.2 Configuration Loss,[0],[0]
All other cases are treated symmetrically.,6.2 Configuration Loss,[0],[0]
"Let (lsa, rsa) be the spine pair of ta, so that the loss of ta is stored in T",6.2 Configuration Loss,[0],[0]
"[i, j](lsa, rsa).",6.2 Configuration Loss,[0],[0]
"Let also (lsb, rsb) be the spine pair of t(σL[i+ 1]).",6.2 Configuration Loss,[0],[0]
"In case there exists a gold arc in tG connecting a node from lsa to r(σL[i+ 1]), we choose the transition lak, k ∈",6.2 Configuration Loss,[0],[0]
"[1, |lsa|], that creates such arc.",6.2 Configuration Loss,[0],[0]
"In case such gold arc does not exists, we choose the transition lak with the maximum possible value of k, that is, k = |lsa|.",6.2 Configuration Loss,[0],[0]
"We therefore explore only one of the several possible ways of combining these two trees by means of a left-arc transition.
",6.2 Configuration Loss,[0],[0]
We remark that the above strategy is safe.,6.2 Configuration Loss,[0],[0]
"In fact, in case the gold arc exists, no other gold arc can ever involve the nodes of lsa eliminated by lak (see the definition in §6.1), because arcs can not cross each other.",6.2 Configuration Loss,[0],[0]
"In case the gold arc does not exist, our choice of k = |lsa| guarantees that we do not eliminate any element from lsa.
",6.2 Configuration Loss,[0],[0]
"Once a transition lak is chosen, as described above, the reduction is performed and the spine pair (ls, rs) for the resulting tree is computed from (lsa, rsa) and (lsb, rsb), as defined in §6.1.",6.2 Configuration Loss,[0],[0]
"At the same time, the loss of the resulting tree is computed, on the basis of the loss T",6.2 Configuration Loss,[0],[0]
"[i, j](lsa, rsa), the loss of tree t(σL[i+ 1]), and a Kronecker-like function defined below.",6.2 Configuration Loss,[0],[0]
This loss is then used to update T,6.2 Configuration Loss,[0],[0]
"[i+ 1, j](ls, rs).
",6.2 Configuration Loss,[0],[0]
Let ta and tb be two trees that must be combined in such a way that tb becomes the dependent of some node in one of the two spines of ta.,6.2 Configuration Loss,[0],[0]
"Let also pa = (lsa, rsa) and pb = (lsb, rsb) be spine pairs for ta and tb, respectively.",6.2 Configuration Loss,[0],[0]
Recall that AG is the set of arcs of tG.,6.2 Configuration Loss,[0],[0]
"The new Kronecker-like function for the computation of the loss is defined as
δG(pa, pb) =    0, if r(ta)",6.2 Configuration Loss,[0],[0]
< r(tb)∧,6.2 Configuration Loss,[0],[0]
"∃k[(rska → r(tb)) ∈ AG]; 0, if r(ta) > r(tb)∧",6.2 Configuration Loss,[0],[0]
"∃k[(lska → r(tb)) ∈ AG];
1, otherwise.",6.2 Configuration Loss,[0],[0]
The algorithm in §6.2 has an exponential behaviour.,6.3 Efficiency Improvement,[0],[0]
"To see why, consider trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j].",6.3 Efficiency Improvement,[0],[0]
These trees are produced by the combination of trees in T,6.3 Efficiency Improvement,[0],[0]
"[i − 1, j] with tree t(σL[i]), or by the combination of trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j",6.3 Efficiency Improvement,[0],[0]
− 1] with tree t(σR[j]).,6.3 Efficiency Improvement,[0],[0]
"Since each combination involves either a left-arc or a right-arc transition, we obtain a recursive relation that resolves into a number of trees in T",6.3 Efficiency Improvement,[0],[0]
"[i, j] bounded by 4i+j−2.
",6.3 Efficiency Improvement,[0],[0]
We introduce now two restrictions to the search space of our extended algorithm that result in a huge computational saving.,6.3 Efficiency Improvement,[0],[0]
"For a spine s, we write N (s) to denote the set of all nodes in s. We also let ∆i,j be the set of all pairs (ls, rs) such that T",6.3 Efficiency Improvement,[0],[0]
"[i, j](ls, rs) 6=",6.3 Efficiency Improvement,[0],[0]
"+∞.
• Every time a new pair (ls, rs) is created in ∆[i, j], we remove from ls all nodes different from the root that do not have gold dependents in {r(σL[k])",6.3 Efficiency Improvement,[0],[0]
| k <,6.3 Efficiency Improvement,[0],[0]
"i}, and we remove from rs all nodes different from the root that do not have gold dependents in {r(σR[k])",6.3 Efficiency Improvement,[0],[0]
"| k > j}.
•",6.3 Efficiency Improvement,[0],[0]
"A pair pa = (lsa, rsa) is removed from ∆[i, j] if there exists a pair pb = (lsb, rsb) in ∆[i, j] with the same root node as pa and with (lsa, rsa) 6= (lsb, rsb), such that N (lsa) ⊆ N (lsb), N (rsa) ⊆ N (rsb), and T",6.3 Efficiency Improvement,[0],[0]
"[i, j](pa) ≥ T",6.3 Efficiency Improvement,[0],[0]
"[i, j](pb).
",6.3 Efficiency Improvement,[0],[0]
The first restriction above reduces the size of a spine by eliminating a node if it is irrelevant for the computation of the loss of the associated tree.,6.3 Efficiency Improvement,[0],[0]
"The second restriction eliminates a tree ta if there is a tree tb with smaller loss than ta, such that in the computations of the parser tb provides exactly the same context as ta.",6.3 Efficiency Improvement,[0],[0]
"It is not difficult to see that the above restrictions do not affect the correctness of the algorithm, since they always leave in our search space some tree that has optimal loss.
",6.3 Efficiency Improvement,[0],[0]
A mathematical analysis of the computational complexity of the extended algorithm is quite involved.,6.3 Efficiency Improvement,[0],[0]
"In Figure 3, we plot the worst case size of T",6.3 Efficiency Improvement,[0],[0]
"[i, j] for each value of j + i",6.3 Efficiency Improvement,[0],[0]
"− 1, computed over all configurations visited in the training phase (see §7).",6.3 Efficiency Improvement,[0],[0]
We see that |T,6.3 Efficiency Improvement,[0],[0]
"[i, j]| grows linearly with j + i− 1, leading to the same space requirements of Algorithm 1.",6.3 Efficiency Improvement,[0],[0]
"Empirically, training with the dynamic
Algorithm 2 Online training for greedy transitionbased parsers
1: w← 0 2: for k iterations do 3: shuffle(corpus) 4: for sentencew and gold tree tG in corpus do 5: c← INITIAL(w) 6: while not FINAL(c) do 7: τp ← argmaxτ∈valid(c)w · φ(c, τ) 8: τo ← argmaxτ∈oracle(c,tG)w·φ(c, τ) 9: if τp 6∈ oracle(c, tG) then
10: w←",6.3 Efficiency Improvement,[0],[0]
"w + φ(c, τo)− φ(c, τp)
11: τ",6.3 Efficiency Improvement,[0],[0]
"← { τp if EXPLORE τo otherwise 12: c← τ(c) return averaged(w)
oracle is only about 8 times slower than training with the oracle of Sartorio et al. (2013) without exploring incorrect configurations.",6.3 Efficiency Improvement,[0],[0]
"We follow the training procedure suggested by Goldberg and Nivre (2013), as described in Algorithm 2.",7 Training,[0],[0]
The algorithm performs online learning using the averaged perceptron algorithm.,7 Training,[0],[0]
"A weight vector w (initialized to 0) is used to score the valid transitions in each configuration based on a feature representation φ, and the highest scoring transition τp is predicted.",7 Training,[0],[0]
"If the predicted transition is not optimal according to the oracle, the weights w are updated away from the predicted transition and to-
wards the highest scoring oracle transition τo.",7 Training,[0],[0]
"The parser then moves to the next configuration, by taking either the predicted or the oracle transition.",7 Training,[0],[0]
"In the “error exploration” mode (EXPLORE is true), the parser follows the predicted transition, and otherwise the parser follows the oracle transition.",7 Training,[0],[0]
"Note that the error exploration mode requires the completeness property of a dynamic oracle.
",7 Training,[0],[0]
"We consider three training conditions: static, in which the oracle is deterministic (returning a single canonical transition for each configuration) and no error exploration is performed; nondet, in which we use a nondeterministic partial oracle (Sartorio et al., 2013), but do not perform error exploration; and explore in which we use the dynamic oracle and perform error exploration.",7 Training,[0],[0]
The static setup mirrors the way greedy parsers are traditionally trained.,7 Training,[0],[0]
The nondet setup allows the training procedure to choose which transition to take in case of spurious ambiguities.,7 Training,[0],[0]
"The explore setup increases the configuration space explored by the parser during training, by exposing the training procedure to non-optimal configurations that are likely to occur during parsing, together with the optimal transitions to take in these configurations.",7 Training,[0],[0]
"It was shown by Goldberg and Nivre (2012; 2013) that the nondet setup outperforms the static setup, and that the explore setup outperforms the nondet setup.",7 Training,[0],[0]
"Datasets Performance evaluation is carried out on CoNLL 2007 multilingual dataset, as well as on the Penn Treebank (PTB) (Marcus et al., 1993) converted to Stanford basic dependencies (De Marneffe et al., 2006).",8 Experimental Evaluation,[0],[0]
"For the CoNLL datasets we use gold part-of-speech tags, while for the PTB we use automatically assigned tags.",8 Experimental Evaluation,[0],[0]
"As usual, the PTB parser is trained on sections 2-21 and tested on section 23.
",8 Experimental Evaluation,[0],[0]
"Setup We train labeled versions of the arc-standard (std) and LR-spine (lrs) parsers under the static, nondet and explore setups, as defined in §7.",8 Experimental Evaluation,[0],[0]
In the nondet setup we use a nondeterministic partial oracle and in the explore setup we use the nondeterministic complete oracles we present in this paper.,8 Experimental Evaluation,[0],[0]
In the static setup we resolve oracle ambiguities and choose a canonic transition sequence by attaching arcs as soon as possible.,8 Experimental Evaluation,[0],[0]
"In the explore setup,
from the first round of training onward, we always follow the predicted transition (EXPLORE is true).",8 Experimental Evaluation,[0],[0]
"For all languages, we deal with non-projectivity by skipping the non-projective sentences during training but not during test.",8 Experimental Evaluation,[0],[0]
"For each parsing system, we use the same feature templates across all languages.1 The arc-standard models are trained for 15 iterations and the LR-spine models for 30 iterations, after which all the models seem to have converged.
Results In Table 1 we report the labeled (LAS) and unlabeled (UAS) attachment scores.",8 Experimental Evaluation,[0],[0]
"As expected, the LR-spine parsers outperform the arc-standard parsers trained under the same setup.",8 Experimental Evaluation,[0],[0]
"Training with the dynamic oracles is also beneficial: despite the arguable complexity of our proposed oracles, the trends are consistent with those reported by Goldberg and Nivre (2012; 2013).",8 Experimental Evaluation,[0],[0]
For the arc-standard model we observe that the move from a static to a nondeterministic oracle during training improves the accuracy for most of languages.,8 Experimental Evaluation,[0],[0]
Making use of the completeness of the dynamic oracle and moving to the error exploring setup further improve results.,8 Experimental Evaluation,[0],[0]
"The only exceptions are Basque, that has a small dataset with more than 20% of non-projective sentences, and Chinese.",8 Experimental Evaluation,[0],[0]
"For Chinese we observe a reduction of accuracy in the nondet setup, but an increase in the explore setup.
",8 Experimental Evaluation,[0],[0]
"For the LR-spine parser we observe a practically constant increase of performance by moving from
1Our complete code, together with the description of the feature templates, is available on the second author’s homepage.
",8 Experimental Evaluation,[0],[0]
the static to the nondeterministic and then to the error exploring setups.,8 Experimental Evaluation,[0],[0]
"We presented dynamic oracles, based on dynamic programming, for the arc-standard and the LRspine parsers.",9 Conclusions,[0],[0]
"Empirical evaluation on 10 languages showed that, despite the apparent complexity of the oracle calculation procedure, the oracles are still learnable, in the sense that using these oracles in the error exploration training algorithm presented in (Goldberg and Nivre, 2012) considerably improves the accuracy of the trained parsers.
",9 Conclusions,[0],[0]
Our algorithm computes a dynamic oracle using dynamic programming to explore a forest of dependency trees that can be reached from a given parser configuration.,9 Conclusions,[0],[0]
"For the arc-standard parser, the computation takes cubic time in the size of the largest of the left and right input stacks.",9 Conclusions,[0],[0]
Dynamic programming for the simulation of arc-standard parsers have been proposed by Kuhlmann et al. (2011).,9 Conclusions,[0],[0]
"That algorithm could be adapted to compute minimum loss for a given configuration, but the running time is O(n4), n the size of the input string: besides being asymptotically slower by one order of magnitude, in practice n is also larger than the stack size above.
",9 Conclusions,[0],[0]
Acknowledgments We wish to thank the anonymous reviewers.,9 Conclusions,[0],[0]
"In particular, we are indebted to one of them for two important technical remarks.",9 Conclusions,[0],[0]
The third author has been partially supported by MIUR under project PRIN,9 Conclusions,[0],[0]
No. 2010LYA9RH 006.,9 Conclusions,[0],[0]
"We develop parsing oracles for two transition-based dependency parsers, including the arc-standard parser, solving a problem that was left open in (Goldberg and Nivre, 2013).",abstractText,[0],[0]
We experimentally show that using these oracles during training yields superior parsing accuracies on many languages.,abstractText,[0],[0]
A Tabular Method for Dynamic Oracles in Transition-Based Parsing,title,[0],[0]
"proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.",text,[0],[0]
"Driven by massive data and computational resources, modern convolutional neural networks (CNNs) and other network architectures have achieved many outstanding results, such as image recognition (Krizhevsky et al., 2012), neural machine translation (Sutskever et al., 2014), and playing Go games (Silver et al., 2016), etc.",1. Introduction,[0],[0]
"Despite their extensive applications, these neural networks are always considered as black boxes.",1. Introduction,[0],[0]
"Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions.",1. Introduction,[0],[0]
"The demand for explainable artificial intelligence (XAI) (Gunning, 2017) – human interpretable explanations of model decisions – has driven the development of visualization techniques, including image synthesis via activation
1Department of Electrical and Computer Engineering, Rice University, Houston, USA. 2Department of Computer Science, Rice University, Houston, USA.",1. Introduction,[0],[0]
"3Department of Neuroscience, Baylor College of Medicine, Houston, USA.",1. Introduction,[0],[0]
"Correspondence to: Weili Nie <wn8@rice.edu>, Ankit B. Patel <abp4@rice.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
maximization (Simonyan et al., 2013; Johnson et al., 2016; Nguyen et al., 2016) and backpropagation-based visualizations (Simonyan et al., 2013; Zeiler & Fergus, 2014; Springenberg et al., 2014; Shrikumar et al., 2017; Kindermans et al., 2017).
",1. Introduction,[0],[0]
The basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space.,1. Introduction,[0],[0]
The intensity changes of these pixels have the most significant impact on network decisions.,1. Introduction,[0],[0]
"Specifically, (Simonyan et al., 2013) visualizes the spatial support of a given class in a given image, i.e. saliency map, by using the true gradient which masks out negative entries of bottom data via the forward ReLU.",1. Introduction,[0],[0]
"Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult.",1. Introduction,[0],[0]
"(Zeiler & Fergus, 2014) visualize the reverse mapping from feature activities back to the input pixel space with the deconvolutional network (DeconvNet) method.",1. Introduction,[0],[0]
The basic idea of DeconvNet is to mask out negative entries of the top gradients by resorting to the backward ReLU.,1. Introduction,[0],[0]
"(Springenberg et al., 2014) proposed the Guided Backpropagation (GBP) method which combines the above two methods: by considering both the forward and backward ReLUs, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations.",1. Introduction,[0],[0]
"More recently, DeepLift (Shrikumar et al., 2017) and PatternNet (Kindermans et al., 2017) have been proposed to further improve the visual quality of backpropagation-based methods.
",1. Introduction,[0],[0]
"This class of backpropagation-based visualizations, in particular GBP and DeconvNet, has attracted a lot of attention in both the deep learning community and other fields (Szegedy et al., 2013; Dosovitskiy & Brox, 2016; Selvaraju et al., 2016; Fong & Vedaldi, 2017; Kraus et al., 2016).",1. Introduction,[0],[0]
"Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored.",1. Introduction,[0],[0]
Do the pretty visualizations actually tell us reliably about what the network is doing internally?,1. Introduction,[0],[0]
"Our experiments have confirmed previous observations (Mahendran & Vedaldi, 2016; Selvaraju et al., 2016; Samek et al., 2017) that saliency map is indeed very sensitive to the change of class labels, while GBP and DeconvNet, though their visualization results are much cleaner than saliency map, remain almost the same given different class labels.",1. Introduction,[0],[0]
"It seems that
the visual quality improvement of backpropagation-based methods is sacrificing the ability of highlighting important pixels to a specific output class.",1. Introduction,[0],[0]
"In this sense, GBP and DeconvNet may be unreliable in interpreting how deep neural networks make classification decisions.
",1. Introduction,[0],[0]
"The most commonly used explanation for these visualizations is to approximate the neural networks with a linear function (Simonyan et al., 2013; Kindermans et al., 2017), where the derivative of output with respect to input image is just the weight vector of the model.",1. Introduction,[0],[0]
"In such sense, the backpropagation-based methods can be regarded as visualizing the learned weights.",1. Introduction,[0],[0]
But apparently the approximate linear model is too simplistic to reflect the highly nonlinear property of deep neural networks.,1. Introduction,[0],[0]
"For example, GBP and DeconvNet essentially apply the same algorithm as saliency map, but treat ReLU, the nonlinear activation, differently.",1. Introduction,[0],[0]
"The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model.",1. Introduction,[0],[0]
"Therefore, we need a more complex model, which should at least capture the impact of both forward ReLU and backward ReLU, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
We provide a theoretical explanation for why GBP and DeconvNet generate more humaninterpretable but less class-sensitive visualizations than saliency map.,1. Introduction,[0],[0]
"Specifically, our analysis reveals that GBP and DeconvNet are essentially doing (partial) image recovery instead of highlighting class-relevant pixels or visualizing the learned weights, which means in principle they are unrelated to the decision-making of neural networks.",1. Introduction,[0],[0]
"We also find that it is the backward ReLU introduced by either GBP or DeconvNet, together with the local connections in CNNs that results in crisp visualizations.",1. Introduction,[0],[0]
"In particular, we explain how DeconvNet also relies on the max-pooling to recover the input.",1. Introduction,[0],[0]
"Finally, we do extensive experiments to support our theory and further reveal more detailed properties of these backpropagation-based visualizations1.",1. Introduction,[0],[0]
"In this section, we first give formal definitions of backpropagation-based visualizations: saliency map, DeconvNet and GBP, and then compare their empirical behaviors.",2. Backpropagation-based Visualizations,[0],[0]
"The key difference of backpropagation-based methods is the way they propagate the output score back through the
1Code is available at https://github.com/weilinie/BackpropVis
ReLU activations.",2.1. Formal Definitions,[0],[0]
"As illustrated by Figure 1, we consider the i-th ReLU activation in the l-th layer with its input",2.1. Formal Definitions,[0],[0]
y (l) i and its output,2.1. Formal Definitions,[0],[0]
o,2.1. Formal Definitions,[0],[0]
"(l) i and denote by σ(t) = max(t, 0) the ReLU activation.",2.1. Formal Definitions,[0],[0]
"Also, denote by R (l) i the top gradient before activation, i.e., gradient of the output score with respect to o (l) i and denote by T (l) i the (modified) gradient after activation, i.e., gradient of the output score with respect to y (l) i .",2.1. Formal Definitions,[0],[0]
"Then in the gradient calculations, the corresponding forward ReLU could be formally defined as a function
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i(t) , I
(
y (l) i
)
t
where I(·) is the indicator function and the corresponding backward ReLU could be formally defined as a function
σ",2.1. Formal Definitions,[0],[0]
"(l) b,i(t) , I
(
R (l) i
)
",2.1. Formal Definitions,[0],[0]
"t
Therefore, the formal definition of backpropagation-based methods for propagating the output score back through the i-th ReLU activation in the l-th layer is
T (l) i =

   
   
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i
(
R (l) i
)
for saliency map
σ",2.1. Formal Definitions,[0],[0]
"(l) b,i
(
R (l) i
)
for DeconvNet
σ",2.1. Formal Definitions,[0],[0]
"(l) f,i
(
σ (l) b,i
(
R (l) i
))
",2.1. Formal Definitions,[0],[0]
"for GBP
which can be further uniformly formulated as
T (l) i = h
(
R (l) i
)",2.1. Formal Definitions,[0],[0]
∂g,2.1. Formal Definitions,[0],[0]
"(
y (l) i
)
∂y (l) i
(1)
where the two functions h(·) and g(·) are defined as
h(t) =
{
t for saliency map
σ(t) for DeconvNet and GBP
g(t) =
{
t for DeconvNet
σ(t) for saliency map and GBP
(2)",2.1. Formal Definitions,[0],[0]
"To be a good visualization method, a clean and visually human-interpretable result is very desirable.",2.2. Empirical Observations,[0],[0]
"More importantly, it should also reveal how the neural networks make decisions.",2.2. Empirical Observations,[0],[0]
"Based on this, we provide the empirical behaviors of the backpropagation-based visualizations for a pretrained VGG-16 net (Simonyan & Zisserman, 2014) in Figure 2.",2.2. Empirical Observations,[0],[0]
"Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.
",2.2. Empirical Observations,[0],[0]
"For the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out.",2.2. Empirical Observations,[0],[0]
"For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose.",2.2. Empirical Observations,[0],[0]
"This, together with more experiments, suggests that after introducing the backward ReLU, both DeconvNet and GBP modify the true gradient in a way that they create much cleaner results but their functionality as an indicator of important pixels to a specific class has disappeared.",2.2. Empirical Observations,[0],[0]
"In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map.",2.2. Empirical Observations,[0],[0]
"We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs.",3. Theoretical Explanations,[0],[0]
"Besides, we also investigate their behaviors in well-trained CNNs.",3. Theoretical Explanations,[0],[0]
"Consider a three-layer CNN, consisting of an input layer and a convolutional hidden layer, followed by a ReLU activation function and a fully connected layer of which its output is called class logits.",3.1. A Random Three-Layer CNN,[0],[0]
"Formally, let x ∈ Rd be a normalized input image with dimension d and ‖x‖ = 1, and let W ∈ Rp×N be N convolutional filters where each column w(i) denotes the i-th filter with size p.",3.1. A Random Three-Layer CNN,[0],[0]
"Note that here we use vectors to represent images and filters for simplicity, and the analysis also works for the more practical two-dimensional case.",3.1. A Random Three-Layer CNN,[0],[0]
"Then, we let Y ∈",3.1. A Random Three-Layer CNN,[0],[0]
Rp×J,3.1. A Random Three-Layer CNN,[0],[0]
"be J image patches extracted from x, and each column y(j) with size p is generated by a linear function y(j) =",3.1. A Random Three-Layer CNN,[0],[0]
"Djx where Dj , [ 0p×(j−1)b Ip×p 0p×(d−(j−1)b−p) ] with b being the stride size2.",3.1. A Random Three-Layer CNN,[0],[0]
"For example, given a filter with size 3 and stride 1, the resulting j-th patch y(j) is made of the j-th to (j + 2)-th consecutive pixels.",3.1. A Random Three-Layer CNN,[0],[0]
The weights in the fullyconnected layer can be represented by V ∈ RNJ×K with K being the number of output logits.,3.1. A Random Three-Layer CNN,[0],[0]
"Therefore, the k-th logit is represented by
fk(x) =
N ∑
i=1
J ∑
j=1
Vqij ,kσ(w (i)T y(j))",3.1. A Random Three-Layer CNN,[0],[0]
"(3)
where the index qij denotes the ((i− 1)J + j)-th entry in every column vector of weight matrix V .
Assume every entry of V and W is sampled from an i.i.d.",3.1. A Random Three-Layer CNN,[0],[0]
"Gaussian distribution N (0, c2).",3.1. A Random Three-Layer CNN,[0],[0]
The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN.,3.1. A Random Three-Layer CNN,[0],[0]
"Note that the norm of the final results will be in the range of [0, 1] as we apply the normalization during visualizations.
",3.1. A Random Three-Layer CNN,[0],[0]
Lemma 1.,3.1. A Random Three-Layer CNN,[0],[0]
"The backpropagation-based visualizations for the k-th logit in a random three-layer CNN is formalized as
sk(x) = 1
",3.1. A Random Three-Layer CNN,[0],[0]
"Zk
J ∑
j=1
Dj T
N ∑
i=1
h(Vqij ,k)w̃",3.1. A Random Three-Layer CNN,[0],[0]
"(i,j) (4)
where Zk is the normalization coefficient to ensure ‖sk(x)‖ ∈",3.1. A Random Three-Layer CNN,[0],[0]
"[0, 1], h(·) is given by Eq.",3.1. A Random Three-Layer CNN,[0],[0]
"(2) and
w̃(i,j) =
{
w(i) for DeconvNet w(i)I ( w(i)T y(j) )",3.1. A Random Three-Layer CNN,[0],[0]
"for saliency map and GBP
2Here we assume a VALID padding method implicitly, and other padding methods do not impact our analysis.
",3.1. A Random Three-Layer CNN,[0],[0]
Proof.,3.1. A Random Three-Layer CNN,[0],[0]
"See Appendix A.
Next, we can analyze the different behaviors of these backpropagation-based methods case by case.",3.1. A Random Three-Layer CNN,[0],[0]
"First, the behavior of GBP is given as follows.
Theorem 1.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"In a random three-layer CNN, if the number of filters N is sufficiently large, GBP at the k-th logit can be approximated as
sGBPk (x)",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"≈ x (5)
Proof.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"See Appendix B.
The above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"However, according to the linear model explanation, backpropagation-based methods are visualizing learned weights, which should be random noise as they are all sampled from i.i.d Gaussians.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"Obviously, it is inconsistent with the actual behavior of GBP.
",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As the approximation in Eq. (5) builds on an assumption that the number of filters N is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery?",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"From (Lugosi & Mendelson, 2017), we can set N =",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
Õ( p ǫ2 ),3.1.1. GUIDED BACKPROPAGATION,[0],[0]
such that with high probability ‖ 1 N ∑N i=1,3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"w̃ (i,j) − E[w̃(i,j)]‖",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"< ǫ, where p denotes the filter size and Õ(·) hides some other factors.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size p.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"For example, given a filter size 3× 3× 3, we need at most O(103) filters to achieve an estimation error ǫ less than 0.1.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations.",3.1.1. GUIDED BACKPROPAGATION,[0],[0]
"Here we show the behaviors of saliency map and DeconvNet in a random three-layer CNN are largely different from GBP.
",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
Theorem 2.,3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"In a random three-layer CNN, if the number of filters N is sufficiently large, saliency map and DeconvNet are approximated as Gaussian random variables satisfying
sSalk (x), s Deconv k (x) ∼ N (0, I)
",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
Proof.,3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"See Appendix C.
The above theorem shows that both saliency map and DeconvNet visualizations will yield random noise, conveying
little information about the input image and class logits.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"For saliency map, it is easily understood since saliency map represents the true gradient of the class logit, which heavily depends on the weights.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"For DeconvNet, although its behavior appears similar to saliency map in this simplistic scenario, we will show later on that it behaves more similarly to GBP, in particular with the existence of max-pooling.",3.1.2. SALIENCY MAP AND DECONVNET,[0],[0]
"In this section, we extend our analysis of a simple random three-layer CNN to other more realistic cases, including the max-pooling, deeper nets and trained weights.",3.2. Extensions to More Realistic Models,[0],[0]
"If we add a max-pooling layer between the ReLU and the fully-connected layer, the k-th logit becomes
fk(x) =
N ∑
i=1
J ∑
j=1
Vq̃ij ,kδ(σ(w (i)T y(j)))
where δ(·) denotes the max-pooling, which successively selects the maximum value in a fixed-size pooling window, and the new index q̃ij is the down-sampled version of qij .",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"Then the backpropagation-based visualizations for the k-th logit can be formulated as
sk(x) = 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(δ′(oij)Vq̃ij ,k)w̃",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(i,j) (6)
where oij , σ(w (i)T y(j)) is the output of each ReLU activation and δ′(oij) denotes the derivative of δ(·) evaluated at oij , which is
δ′(oij) =
{
1 if oij is chosen by max-pooling 0",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"otherwise
Since oij ≥ 0 with equality holds for w (i)T y(j) ≤ 0, given a proper pooling window size, it is highly possible that oij is chosen by the max-pooling if and only if w(i)T y(j)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
> 0.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"It means with high probability, Eq. (6) is approximated as
sk(x)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"≈ 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(Vq̃ij ,k)w̃",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(i,j) I(w(i)T y(j))
(7)
For saliency map and GBP, we know w̃(i,j)I(w(i)T y(j))",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"= w̃(i,j) and thus Eq.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(7) is further reduced to Eq.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"(4), which means the behaviors of saliency map and GBP remain the same after introducing the max-pooling.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"However, with high probability, DeconvNet at the k-th logit becomes
sDeconvk (x)",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"≈ 1
Zk
J ∑
j=1
Dj T
N ∑
i=1
σ(Vq̃ij ,k)w (i) I(w(i)T y(j))
",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations
which is exactly the form of GBP in Eq.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(4).,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"Therefore, adding the max-pooling makes the DeconvNet behave like GBP – doing nothing but image recovery.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"This also explains and extends the previous intuitive claims in (Samek et al., 2017; Odena et al., 2016) that the image-specific information in DeconvNet comes from the max-pooling.
",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
Note that that the approximation from Eq. (6) to Eq.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
(7) in DeconvNet with the max-pooling is essentially different from the approximations used in GBP.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"For GBP, the approximate gap can be made arbitrarily small by increasing the hidden layer size N , leading to a perfect recovery of the input.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"However, for DeconvNet, given any pooling window size, there might always exist at least one of the following two contradictory cases: it is possible that aij is chosen by the max-pooling if w(i)T y(j) ≤ 0, and also possible that aij is not chosen if w
(i)T y(j) >",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
0.,3.2.1. CNNS WITH MAX-POOLING,[0],[0]
"This makes DeconvNet (with max-pooling), in theory, never recover input perfectly, which might explain why the unusual texture-like artifacts appear in the DeconvNet visualizations.",3.2.1. CNNS WITH MAX-POOLING,[0],[0]
The analysis for a three-layer CNN can be generalized to the multi-layer (or deeper) case.,3.2.2. DEEP CNNS,[0],[0]
"For clarity, we formulate the k-th logit of an L-layer deep CNN in a matrix form:
fk(x) = Γ (L)T k σ
( Γ(L−1)T · · ·σ ( Γ(1)Tx ))
where Γ(l) ∈ Rdl×dl+1 denotes either the convolutional or fully-connected operator matrix in the l-th layer and Γ (L) k is the k-th column of Γ (L).",3.2.2. DEEP CNNS,[0],[0]
"Denote by o(l) the output of ReLU activations in the l-th layer, i.e. o(l) = σ",3.2.2. DEEP CNNS,[0],[0]
"( Γ(l)T o(l−1) )
, ∀l ∈ {1, · · · , L−1} with o(0) , x.",3.2.2. DEEP CNNS,[0],[0]
"Then backpropagation-based visualizations at the k-th logit in an L-layer deep CNN can be formulated as
sk(x) = 1
Zk
∂õ(1)
∂x · h(V̂
(1) ·,k )
(a) =
1
Zk
J ∑
j=1
Dj T
N ∑
i=1
h(V̂ (1) qij ,k
)",3.2.2. DEEP CNNS,[0],[0]
"w̃(i,j) (8)
with ∀l ∈ {1, · · · , L− 1},
V̂ (l) ·,k =
∂õ(l+1)
∂o(l) ·",3.2.2. DEEP CNNS,[0],[0]
"h
(
∂õ(l+2) ∂o(l+1) · · ·h
(
∂õ(L−1)
∂o(L−2) h ( Γ (L) k )
))
where in (a) we rewrite sk(x) in an expanded form, õ (l) , g ( Γ(l)T o(l−1) )
, w(i) is the i-th filter encoded in Γ(1) and N is the number of filters in the first convolutional layer.",3.2.2. DEEP CNNS,[0],[0]
"Also, h(·), g(·) and w̃(i,j) are defined in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(2) and Lemma 1.
",3.2.2. DEEP CNNS,[0],[0]
"First, the approximate property of V̂ (1) ·,k in the random deep CNN is given in the following proposition.
",3.2.2. DEEP CNNS,[0],[0]
"w(i)
y(j)
",3.2.2. DEEP CNNS,[0],[0]
Proposition 1.,3.2.2. DEEP CNNS,[0],[0]
For a random deep CNN where weights are i.i.d.,3.2.2. DEEP CNNS,[0],[0]
"Gaussians with zero mean, we can also approximate every entry of V̂ (1) ·,k as i.i.d.",3.2.2. DEEP CNNS,[0],[0]
"Gaussian with zero mean.
",3.2.2. DEEP CNNS,[0],[0]
Proof.,3.2.2. DEEP CNNS,[0],[0]
"See Appendix D.
Based on Proposition 1, we can see that the statistical properties of V̂ (1) qij ,k in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(8) are approximately the same with those of Vqij ,k in Eq.",3.2.2. DEEP CNNS,[0],[0]
"(4), which means the analysis of backpropagation-based visualizations in a shallow threelayer CNN also applies to the deep CNN case.",3.2.2. DEEP CNNS,[0],[0]
"Therefore, the behaviors of these visualizations will barely change when increasing the depth of neural networks.",3.2.2. DEEP CNNS,[0],[0]
The previous analysis for random CNNs does not apply to the trained case directly since the weights here may not be i.i.d.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
Gaussian distributed.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For saliency map, which uses the true gradient, the trained weights are likely to impose a stronger bias towards some specific subset of the input pixels, and so they can highlight class-relevant pixels rather than producing random noise.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For GBP and DeconvNet, the analysis is a little more involved.
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"On the one hand, the trained weights w(i) will only lie in a small subspace of the whole image patch space which will create some “dead zones”, as illustrated in Figure 3 (a).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"That
is, all image patches lying in the “dead zone” will be filtered out by the forward ReLU.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"For example, it is well-known that the trained weights in the first convolutional layer are Gabor-like filters to detect the image patches containing edges (Yosinski et al., 2014; Zeiler & Fergus, 2014).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"That is, image patches without edges will probably be filtered out by the first convolutional layer.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Also, the higher convolutional layers keep filtering out more image patches with certain patterns (e.g. Figure 9).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"See the supplementary material for a comparison between GBP and a linear edge detector.
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"On the other hand, as shown in Figure 3 (b) and (c), the histograms of weights connected to the respective one of any two different neurons in the first fully connected layer (called “fc1”) of the trained VGG-16 net are very similar to each other.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Approximately, they form two very similar Gaussians with a small standard deviation, which means the (modified) gradients at any two different neurons in the layer “fc1” with respect to the input image are almost the same.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Namely, ∂õ (fc1)
∂x in Eq. (8) for GBP and DeconvNet
(with max-pooling) satisfies
∂õ (fc1) m
∂x ≈ Fconv(x), ∀m ∈ {1, · · · ,M}
where õ (fc1) m is the m-th entry of õ (fc1) and Fconv(·) :",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
R d → R d denotes the (normalized) overall filtering effect of the convolutional layers and M is the number of neurons in the layer “fc1”.,3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"Thus, Eq. (8) for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as
sk(x) = 1
Zk
∂õ(fc1)
∂x · h(V̂
(fc1) ·,k )
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"= 1
Zk
M ∑
m=1
∂õ (fc1) m
∂x · h(V̂
(fc1) m,k )
(a) ≈ Fconv(x)
(9)
where (a) follows from setting the normalization coefficient to be Zk = 1 ∑
M m=1 h(V̂ (fc1) m,k
) .
",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"It shows that GBP and DeconvNet (with max-pooling) in a trained CNN are actually doing the partial image recovery, where the trained weights control which image patch could form an active path to the class logit.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"More importantly, this filtering process is not class sensitive (e.g. the edge detector).",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"In the end, only these “active” image patches are combined in the first fully connected layer to form the final visualization results.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"As the right side of (9) does not depend on k, it illustrates why the GBP and DecovNet visualizations in the trained VGG are not class-sensitive.",3.2.3. CNNS WITH TRAINED WEIGHTS,[0],[0]
"To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully-
connected network (FCN) and a VGG-16 net.",4. Experiments,[0],[0]
"For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation 0.1.",4. Experiments,[0],[0]
"Unless stated otherwise, the input is the image “tabby” from the ImageNet dataset (Deng et al., 2009) with size 224×224×3.",4. Experiments,[0],[0]
"See the supplementary materials for more results on other images and other neural network such as ResNet (He et al., 2016).",4. Experiments,[0],[0]
"In the three-layer CNN, the filter size is 7× 7× 3, the number of filters is N = 256, and the stride is 2.",4. Experiments,[0],[0]
"In the three-layer FCN, the hidden layer size is set to Nh = 4096.",4. Experiments,[0],[0]
"By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit.",4. Experiments,[0],[0]
"Figure 4 shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively.",4.1. Impact of Local Connections,[0],[0]
"We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section 3.1.",4.1. Impact of Local Connections,[0],[0]
"In contrast, as local connections do not exist in the FCN and the input size (e.g. 224 × 224 × 3) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise.",4.1. Impact of Local Connections,[0],[0]
"Particularly for GBP, the number of hidden neurons Nh = 4096 is still not large enough to recover the image.
",4.1. Impact of Local Connections,[0],[0]
"To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters N in the CNN and the number of hidden neurons Nh in the FCN, respectively, while keep other parameters fixed.",4.1. Impact of Local Connections,[0],[0]
The results are given in Figure 5.,4.1. Impact of Local Connections,[0],[0]
"Note that in the FCN, we have downsampled the input image to be of size 64 × 64 × 3 due to computational limitations.",4.1. Impact of Local Connections,[0],[0]
We can see that as the number of filters N increases (resp.,4.1. Impact of Local Connections,[0],[0]
"the hidden layer size Nh), the vi-
sual quality of GBP in the CNN (resp.",4.1. Impact of Local Connections,[0],[0]
in the FCN) becomes better.,4.1. Impact of Local Connections,[0],[0]
"Interestingly, even by setting Nh = 70000, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with N = 64.",4.1. Impact of Local Connections,[0],[0]
"Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP.",4.1. Impact of Local Connections,[0],[0]
"To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in Figure 6 (top row).",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"As compared with the visualizations in Figure 4 (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section 3.2.1.
",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"To show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
Figure 6 (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations.,4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"Though there are subtle visual differences between the top row and bottom row of Figure 6, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG , which is different from the trained VGG in Figure 2 where only those “active” image patches are preserved.",4.2. Impact of Max-Pooling and Network Depth,[0],[0]
"To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average l2 distance statistics as shown in Figure 7.",4.3. Average l2 Distance Statistics,[0],[0]
Our results are obtained by first calculating the l2 distance of two visualization results given two different class logits for each input image and then taking an average of those l2 distances based on 10K images from the ImageNet test set.,4.3. Average l2 Distance Statistics,[0],[0]
The process is repeated for all backpropagationbased methods in both random and trained cases.,4.3. Average l2 Distance Statistics,[0],[0]
"As we can see, the average l2 distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not.",4.3. Average l2 Distance Statistics,[0],[0]
"Interestingly, in the trained VGG-16 net, the average l2 distance of DeconvNet is slightly larger than that of GBP.",4.3. Average l2 Distance Statistics,[0],[0]
It shows that the class insensitivity is exchanged for further improvement of visual quality.,4.3. Average l2 Distance Statistics,[0],[0]
Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery.,4.4. Adversarial Attack on VGG,[0],[0]
"The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image.",4.4. Adversarial Attack on VGG,[0],[0]
"In this experiment, we first generate an adversarial example “busby” via the fast gradient sign method (FGSM) (Goodfellow et al., 2014) by feeding the image “panda” into the pretrained VGG-16 net.",4.4. Adversarial Attack on VGG,[0],[0]
"Next, we apply the backpropagationbased visualizations to the original image “panda” and its adversary “busby” in the trained VGG-16 net.",4.4. Adversarial Attack on VGG,[0],[0]
"As shown in Figure 8, the saliency map visualization changes significantly whereas the GBP and DeconvNet visualizations remain almost unchanged after replacing “panda” by its adversary “busby”.",4.4. Adversarial Attack on VGG,[0],[0]
"Therefore, it further confirms that saliency map is class-sensitive in that it highlights important pixels in making classification decisions.",4.4. Adversarial Attack on VGG,[0],[0]
"However, GBP and DeconvNet are doing nothing but (partial) image recovery.",4.4. Adversarial Attack on VGG,[0],[0]
"There exist some differences for backpropagation-based visualizations, GBP and DeconvNet in particular, between the random and trained cases.",4.5. VGG with Partly Trained Weights,[0],[0]
"We take GBP as an example here to investigate the contributions of different layers in the trained VGG-16 net to these visual differences.
",4.5. VGG with Partly Trained Weights,[0],[0]
"First, to isolate the impact of later layers, we load the trained weights up to a given layer and leave later layers randomly initialized.",4.5. VGG with Partly Trained Weights,[0],[0]
"As shown in Figure 9 (top row), from “Conv11*” to “Conv5-1*” GBP keeps filtering out more image patches as the number of trained convolutional layers increases.",4.5. VGG with Partly Trained Weights,[0],[0]
"However, from “Conv5-1*” to “FC3*” (i.e., the
fully-trained case) GBP behaves almost the same, no matter weights in the dense layers are random or trained.",4.5. VGG with Partly Trained Weights,[0],[0]
"Therefore, it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches.",4.5. VGG with Partly Trained Weights,[0],[0]
"Also, it further confirms that GBP is class-insensitive.",4.5. VGG with Partly Trained Weights,[0],[0]
"Furthermore, to reveal the impact of each layer, we load the trained weights for the whole VGG-16 net except for a given layer which is randomly initialized instead.",4.5. VGG with Partly Trained Weights,[0],[0]
The results are shown in Figure 9 (bottom row).,4.5. VGG with Partly Trained Weights,[0],[0]
"We can see that the GBP visualization is blurry for “Conv1-1⋄”, clean with much background information for “Conv3-1⋄” and clean without background information for “Conv5-1⋄”, respectively.",4.5. VGG with Partly Trained Weights,[0],[0]
It means that the earlier convolutional layer has more important impact in the GBP visualization than the later convolutional layer.,4.5. VGG with Partly Trained Weights,[0],[0]
"In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases.",5. Conclusions,[0],[0]
"We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties.",5. Conclusions,[0],[0]
"We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs, that is responsible for human-interpretable visualizations.",5. Conclusions,[0],[0]
We also explained how DeconvNet also relies on the max-pooling to recover the input.,5. Conclusions,[0],[0]
Our analysis was supported by extensive experiments.,5. Conclusions,[0],[0]
"Finally, we hope our analysis can provide useful insights into developing better visualization methods for deep neural networks.",5. Conclusions,[0],[0]
A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer.,5. Conclusions,[0],[0]
Thanks to the anonymous reviewers for useful comments.,Acknowledgements,[0],[0]
"WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003.",Acknowledgements,[0],[0]
"Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map.",abstractText,[0],[0]
"Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions.",abstractText,[0],[0]
"Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations.",abstractText,[0],[0]
Extensive experiments are provided that support the theoretical analysis.,abstractText,[0],[0]
A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1127–1138 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1104",text,[0],[0]
"Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004).",1 Introduction,[1.0],"['Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004).']"
"It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015).",1 Introduction,[1.0],"['It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015).']"
"It has also proven useful for machine translation evaluation (Birch et al., 2016).",1 Introduction,[1.0],"['It has also proven useful for machine translation evaluation (Birch et al., 2016).']"
UCCA differs from syntactic schemes in terms of content and formal structure.,1 Introduction,[1.0],['UCCA differs from syntactic schemes in terms of content and formal structure.']
"It exhibits reentrancy,
discontinuous nodes and non-terminals, which no single existing parser supports.",1 Introduction,[1.0000000647906842],"['It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports.']"
"Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses.
",1 Introduction,[0],[0]
"We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA.",1 Introduction,[1.0],"['We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA.']"
"Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016).",1 Introduction,[0],[0]
"We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokgöz and Eryiğit, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016).
",1 Introduction,[0],[0]
"We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings.",1 Introduction,[0],[0]
"To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees.",1 Introduction,[0],[0]
"Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1
The rest of the paper is structured as follows:
1All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa.
1127
Section 2 describes UCCA in more detail.",1 Introduction,[0],[0]
Section 3 introduces TUPA.,1 Introduction,[0],[0]
Section 4 discusses the data and experimental setup.,1 Introduction,[0],[0]
Section 5 presents the experimental results.,1 Introduction,[0],[0]
"Section 6 summarizes related work, and Section 7 concludes the paper.",1 Introduction,[0],[0]
"UCCA graphs are labeled, directed acyclic graphs (DAGs), whose leaves correspond to the tokens of the text.",2 The UCCA Scheme,[0],[0]
A node (or unit) corresponds to a terminal or to several terminals (not necessarily contiguous) viewed as a single entity according to semantic or cognitive considerations.,2 The UCCA Scheme,[1.0],['A node (or unit) corresponds to a terminal or to several terminals (not necessarily contiguous) viewed as a single entity according to semantic or cognitive considerations.']
"Edges bear a category, indicating the role of the sub-unit in the parent relation.",2 The UCCA Scheme,[0],[0]
"Figure 1 presents a few examples.
",2 The UCCA Scheme,[0],[0]
"UCCA is a multi-layered representation, where each layer corresponds to a “module” of semantic distinctions.",2 The UCCA Scheme,[0],[0]
"UCCA’s foundational layer, targeted in this paper, covers the predicate-argument structure evoked by predicates of all grammatical categories (verbal, nominal, adjectival and others), the inter-relations between them, and other major linguistic phenomena such as coordination and multi-word expressions.",2 The UCCA Scheme,[0],[0]
"The layer’s basic notion is the scene, describing a state, action, movement or some other relation that evolves in time.",2 The UCCA Scheme,[0],[0]
"Each scene contains one main relation (marked as either a Process or a State), as well as one or more Participants.",2 The UCCA Scheme,[0],[0]
"For example, the sentence “After graduation, John moved to Paris” (Figure 1a) contains two scenes, whose main relations are “graduation” and “moved”.",2 The UCCA Scheme,[0],[0]
"“John” is a Participant in both scenes, while “Paris” only in the latter.",2 The UCCA Scheme,[0],[0]
"Further categories account for inter-scene relations and the internal structure of complex arguments and relations (e.g. coordination, multi-word expressions and modification).
",2 The UCCA Scheme,[0],[0]
"One incoming edge for each non-root node is marked as primary, and the rest (mostly used for implicit relations and arguments) as remote edges, a distinction made by the annotator.",2 The UCCA Scheme,[1.0],"['One incoming edge for each non-root node is marked as primary, and the rest (mostly used for implicit relations and arguments) as remote edges, a distinction made by the annotator.']"
"The primary edges thus form a tree structure, whereas the remote edges enable reentrancy, forming a DAG.
",2 The UCCA Scheme,[0],[0]
"While parsing technology in general, and transition-based parsing in particular, is wellestablished for syntactic parsing, UCCA has several distinct properties that distinguish it from syntactic representations, mostly UCCA’s tendency to abstract away from syntactic detail that do not affect argument structure.",2 The UCCA Scheme,[0],[0]
"For instance, consider the following examples where the concept of a scene
(a) After
L
graduation P
H
, U
John
A
moved
P
to R
Paris
C
A
H
A
(b) John
A
gave
C
everything up
C
P
A P process A participant H linked scene C center R relator N connector L scene linker U punctuation F function unit
(c)
John
C
and
N
Mary
C
’s
F
A
trip
P
home
A
Figure 1: UCCA structures demonstrating three structural properties exhibited by the scheme.",2 The UCCA Scheme,[0],[0]
"(a) includes a remote edge (dashed), resulting in “John” having two parents.",2 The UCCA Scheme,[0],[0]
(b) includes a discontinuous unit (“gave ... up”).,2 The UCCA Scheme,[0],[0]
(c) includes a coordination construction (“John and Mary”).,2 The UCCA Scheme,[0],[0]
Pre-terminal nodes are omitted for brevity.,2 The UCCA Scheme,[0],[0]
"Right: legend of edge labels.
has a different rationale from the syntactic concept of a clause.",2 The UCCA Scheme,[0],[0]
"First, non-verbal predicates in UCCA are represented like verbal ones, such as when they appear in copula clauses or noun phrases.",2 The UCCA Scheme,[0],[0]
"Indeed, in Figure 1a, “graduation” and “moved” are considered separate events, despite appearing in the same clause.",2 The UCCA Scheme,[0],[0]
"Second, in the same example, “John” is marked as a (remote) Participant in the graduation scene, despite not being overtly marked.",2 The UCCA Scheme,[0],[0]
"Third, consider the possessive construction in Figure 1c.",2 The UCCA Scheme,[0],[0]
"While in UCCA “trip” evokes a scene in which “John and Mary” is a Participant, a syntactic scheme would analyze this phrase similarly to “John and Mary’s shoes”.
",2 The UCCA Scheme,[0],[0]
"These examples demonstrate that a UCCA parser, and more generally semantic parsers, face an additional level of ambiguity compared to their syntactic counterparts (e.g., “after graduation” is formally very similar to “after 2pm”, which does not evoke a scene).",2 The UCCA Scheme,[0],[0]
"Section 6 discusses UCCA in the context of other semantic schemes, such as AMR (Banarescu et al., 2013).
",2 The UCCA Scheme,[0],[0]
"Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6).",2 The UCCA Scheme,[0],[0]
"One such property is reentrancy, namely the sharing of semantic units between predicates.",2 The UCCA Scheme,[0],[0]
"For instance, in Figure 1a, “John” is an argument of both “gradu-
ation” and “moved”, yielding a DAG rather than a tree.",2 The UCCA Scheme,[0],[0]
"A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit.",2 The UCCA Scheme,[0],[0]
"Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014).",2 The UCCA Scheme,[0],[0]
"Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word.",2 The UCCA Scheme,[0],[0]
"The use of non-terminal nodes is motivated by constructions with no clear head, including coordination structures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head).",2 The UCCA Scheme,[0],[0]
"To our knowledge, no existing parser supports all structural properties required for UCCA parsing.",2 The UCCA Scheme,[0],[0]
We now turn to presenting TUPA.,3 Transition-based UCCA Parsing,[0],[0]
"Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties.
",3 Transition-based UCCA Parsing,[0],[0]
"Transition-based parsers (Nivre, 2003) scan the text from start to end, and create the parse incrementally by applying a transition at each step to the parser’s state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V,E, `) of constructed nodes and edges, where V is the set of nodes, E is the set of edges, and ` : E → L is the label function, L being the set of possible labels.",3 Transition-based UCCA Parsing,[0],[0]
"Some states are marked as terminal, meaning that G is the final output.",3 Transition-based UCCA Parsing,[0],[0]
A classifier is used at each step to select the next transition based on features encoding the parser’s current state.,3 Transition-based UCCA Parsing,[1.0],['A classifier is used at each step to select the next transition based on features encoding the parser’s current state.']
"During training, an oracle creates training instances for the classifier, based on gold-standard annotations.
",3 Transition-based UCCA Parsing,[0],[0]
Transition Set.,3 Transition-based UCCA Parsing,[0],[0]
"Given a sequence of tokens w1, . . .",3 Transition-based UCCA Parsing,[0],[0]
", wn, we predict a UCCA graph G over the sequence.",3 Transition-based UCCA Parsing,[1.0],"[', wn, we predict a UCCA graph G over the sequence.']"
"Parsing starts with a single node on the stack (an artificial root node), and the input tokens in the buffer.",3 Transition-based UCCA Parsing,[1.0],"['Parsing starts with a single node on the stack (an artificial root node), and the input tokens in the buffer.']"
"Figure 2 shows the transition set.
",3 Transition-based UCCA Parsing,[0],[0]
"In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), adding the NODE transition for creating new non-terminal nodes.",3 Transition-based UCCA Parsing,[1.0],"['In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), adding the NODE transition for creating new non-terminal nodes.']"
"For every X ∈ L, NODEX creates a new node on the buffer as a par-
ent of the first element on the stack, with an Xlabeled edge.",3 Transition-based UCCA Parsing,[1.0000000562757796],"['For every X ∈ L, NODEX creates a new node on the buffer as a par- ent of the first element on the stack, with an Xlabeled edge.']"
"LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively.",3 Transition-based UCCA Parsing,[1.0],"['LEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively.']"
"As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed if the child node already has an incoming primary edge.",3 Transition-based UCCA Parsing,[0],[0]
"LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is additionally marked as remote.",3 Transition-based UCCA Parsing,[0],[0]
We distinguish between these two pairs of transitions to allow the parser to create remote edges without the possibility of producing invalid graphs.,3 Transition-based UCCA Parsing,[0],[0]
"To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokgöz and Eryiğit, 2015).",3 Transition-based UCCA Parsing,[0],[0]
"REDUCE pops the stack, to allow removing a node once all its edges have been created.",3 Transition-based UCCA Parsing,[0],[0]
"To handle discontinuous nodes, SWAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015).",3 Transition-based UCCA Parsing,[0],[0]
"Finally, FINISH pops the root node and marks the state as terminal.
Classifier.",3 Transition-based UCCA Parsing,[0],[0]
"The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016).",3 Transition-based UCCA Parsing,[0],[0]
"To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models.
1.",3 Transition-based UCCA Parsing,[0],[0]
"Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and MINUPDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2
2.",3 Transition-based UCCA Parsing,[0],[0]
"Changing the model to a feedforward neural network with dense embedding features, TUPAMLP (“multi-layer perceptron”), uses an architecture similar to that of Chen and Manning (2014), but with two rectified linear layers
2We also experimented with a linear model using dense embedding features, trained with the averaged structured perceptron algorithm.",3 Transition-based UCCA Parsing,[0.9909993172113006],"['Changing the model to a feedforward neural network with dense embedding features, TUPAMLP (“multi-layer perceptron”), uses an architecture similar to that of Chen and Manning (2014), but with two rectified linear layers instead of one layer with cube activation.']"
"It performed worse than the sparse perceptron model and was hence discarded.
instead of one layer with cube activation.",3 Transition-based UCCA Parsing,[0],[0]
"The embeddings and classifier are trained jointly.
3.",3 Transition-based UCCA Parsing,[0],[0]
"Finally, TUPABiLSTM uses a bidirectional LSTM for feature representation, on top of the dense embedding features, an architecture similar to Kiperwasser and Goldberg (2016).",3 Transition-based UCCA Parsing,[0],[0]
"The BiLSTM runs on the input tokens in forward and backward directions, yielding a vector representation that is then concatenated with dense features representing the parser state (e.g., existing edge labels and previous parser actions; see below).",3 Transition-based UCCA Parsing,[1.0],"['The BiLSTM runs on the input tokens in forward and backward directions, yielding a vector representation that is then concatenated with dense features representing the parser state (e.g., existing edge labels and previous parser actions; see below).']"
This representation is then fed into a feedforward network similar to TUPAMLP.,3 Transition-based UCCA Parsing,[0],[0]
"The feedforward layers, BiLSTM and embeddings are all trained jointly.
",3 Transition-based UCCA Parsing,[0],[0]
"For all classifiers, inference is performed greedily, i.e., without beam search.",3 Transition-based UCCA Parsing,[0],[0]
"Hyperparameters are tuned on the development set (see Section 4).
",3 Transition-based UCCA Parsing,[0],[0]
Features.,3 Transition-based UCCA Parsing,[0],[0]
"TUPASparse uses binary indicator features representing the words, POS tags, syntactic dependency labels and existing edge labels related to the top four stack elements and the next three buffer elements, in addition to their children and grandchildren in the graph.",3 Transition-based UCCA Parsing,[0],[0]
"We also use bi- and trigram features based on these values (Zhang and Clark, 2009; Zhu et al., 2013), features related to discontinuous nodes (Maier, 2015, including separating punctuation and gap type), features representing existing edges and the number of parents and children, as well as the past actions taken by the parser.",3 Transition-based UCCA Parsing,[0],[0]
"In addition, we use use a novel, UCCAspecific feature: number of remote children.3
For TUPAMLP and TUPABiLSTM, we replace all indicator features by a concatenation of the vector embeddings of all represented elements: words,
3See Appendix A for a full list of used feature templates.
",3 Transition-based UCCA Parsing,[0],[0]
"POS tags, syntactic dependency labels, edge labels, punctuation, gap type and parser actions.",3 Transition-based UCCA Parsing,[0],[0]
These embeddings are initialized randomly.,3 Transition-based UCCA Parsing,[0],[0]
"We additionally use external word embeddings initialized with pre-trained word2vec vectors (Mikolov et al., 2013),4 updated during training.",3 Transition-based UCCA Parsing,[0],[0]
"In addition to dropout between NN layers, we apply word dropout (Kiperwasser and Goldberg, 2016): with a certain probability, the embedding for a word is replaced with a zero vector.",3 Transition-based UCCA Parsing,[0],[0]
"We do not apply word dropout to the external word embeddings.
",3 Transition-based UCCA Parsing,[0],[0]
"Finally, for all classifiers we add a novel realvalued feature to the input vector, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too.
Training.",3 Transition-based UCCA Parsing,[0],[0]
"For training the transition classifiers, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state.",3 Transition-based UCCA Parsing,[0],[0]
"For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created.",3 Transition-based UCCA Parsing,[0],[0]
"The transition predicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions.",3 Transition-based UCCA Parsing,[0],[0]
"Otherwise, a random optimal transition is applied, and for the perceptronbased parser, the classifier’s weights are updated
4https://goo.gl/6ovEhC
according to the perceptron update rule.",3 Transition-based UCCA Parsing,[0],[0]
"POS tags and syntactic dependency labels are extracted using spaCy (Honnibal and Johnson, 2015).5",3 Transition-based UCCA Parsing,[0],[0]
"We use the categorical cross-entropy objective function and optimize the NN classifiers with the Adam optimizer (Kingma and Ba, 2014).",3 Transition-based UCCA Parsing,[0],[0]
Data.,4 Experimental Setup,[0],[0]
"We conduct our experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as outof-domain data.6 Table 1 presents some statistics for the two corpora.",4 Experimental Setup,[1.0],"['We conduct our experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as outof-domain data.6 Table 1 presents some statistics for the two corpora.']"
"We use passages of indices up to 676 of the Wiki corpus as our training set, passages 688–808 as development set, and passages 942–1028 as in-domain test set.",4 Experimental Setup,[0],[0]
"While
5https://spacy.io 6http://cs.huji.ac.il/˜oabend/ucca.html
UCCA edges can cross sentence boundaries, we adhere to the common practice in semantic parsing and train our parsers on individual sentences, discarding inter-relations between them (0.18% of the edges).",4 Experimental Setup,[0],[0]
We also discard linkage nodes and edges (as they often express inter-sentence relations and are thus mostly redundant when applied at the sentence level) as well as implicit nodes.7,4 Experimental Setup,[0],[0]
"In the out-of-domain experiments, we apply the same parsers (trained on the Wiki training set) to the 20K Leagues corpus without parameter re-tuning.
",4 Experimental Setup,[0],[0]
Implementation.,4 Experimental Setup,[0],[0]
"We use the DyNet package (Neubig et al., 2017) for implementing the NN classifiers.",4 Experimental Setup,[0],[0]
"Unless otherwise noted, we use the default values provided by the package.",4 Experimental Setup,[0],[0]
"See Appendix C for the hyperparameter values we found by tuning on the development set.
",4 Experimental Setup,[0],[0]
Evaluation.,4 Experimental Setup,[0],[0]
"We define a simple measure for comparing UCCA structures Gp = (Vp, Ep, `p) and Gg = (Vg, Eg, `g), the predicted and goldstandard graphs, respectively, over the same sequence of terminals W = {w1, . . .",4 Experimental Setup,[0],[0]
", wn}.",4 Experimental Setup,[0],[0]
"For an edge e = (u, v) in either graph, u being the parent and v the child, its yield y(e) ⊆ W is the set of terminals in W that are descendants of v. Define the set of mutual edges between Gp and Gg:
M(Gp, Gg) =
{(e1, e2) ∈",4 Experimental Setup,[0],[0]
Ep × Eg | y(e1),4 Experimental Setup,[0],[0]
= y(e2),4 Experimental Setup,[0],[0]
"∧ `p(e1) = `g(e2)}
Labeled precision and recall are defined by dividing |M(Gp, Gg)| by |Ep| and |Eg|, respectively, and F-score by taking their harmonic mean.
",4 Experimental Setup,[0],[0]
"7Appendix B further discusses linkage and implicit units.
",4 Experimental Setup,[0],[0]
"We report two variants of this measure: one where we consider only primary edges, and another for remote edges (see Section 2).",4 Experimental Setup,[0],[0]
"Performance on remote edges is of pivotal importance in this investigation, which focuses on extending the class of graphs supported by statistical parsers.
",4 Experimental Setup,[0],[0]
We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp and Gg are trees.,4 Experimental Setup,[0],[0]
"Punctuation is excluded from the evaluation, but not from the datasets.
",4 Experimental Setup,[0],[0]
Comparison to bilexical graph parsers.,4 Experimental Setup,[0],[0]
"As no direct comparison with existing parsers is possible, we compare TUPA to bilexical dependency graph parsers, which support reentrancy and discontinuity but not non-terminal nodes.
",4 Experimental Setup,[0.9999999058366377],"['As no direct comparison with existing parsers is possible, we compare TUPA to bilexical dependency graph parsers, which support reentrancy and discontinuity but not non-terminal nodes.']"
"To facilitate the comparison, we convert our training set into bilexical graphs (see examples in Figure 4), train each of the parsers, and evaluate them by applying them to the test set and then reconstructing UCCA graphs, which are compared with the gold standard.",4 Experimental Setup,[1.0],"['To facilitate the comparison, we convert our training set into bilexical graphs (see examples in Figure 4), train each of the parsers, and evaluate them by applying them to the test set and then reconstructing UCCA graphs, which are compared with the gold standard.']"
"The conversion to bilexical graphs is done by heuristically selecting a head terminal for each non-terminal node, and attaching all terminal descendents to the head terminal.",4 Experimental Setup,[0],[0]
"In the inverse conversion, we traverse the bilexical graph in topological order, creating non-terminal parents for all terminals, and attaching them to the previously-created non-terminals corresponding to the bilexical heads.8
",4 Experimental Setup,[0],[0]
"In Section 5 we report the upper bounds on the achievable scores due to the error resulting from the removal of non-terminal nodes.
",4 Experimental Setup,[0],[0]
Comparison to tree parsers.,4 Experimental Setup,[0],[0]
"For completeness, and as parsing technology is considerably more
8See Appendix D for a detailed description of the conversion procedures.
mature for tree (rather than graph) parsing, we also perform a tree approximation experiment, converting UCCA to (bilexical) trees and evaluating constituency and dependency tree parsers on them (see examples in Figure 5).",4 Experimental Setup,[0],[0]
"Our approach is similar to the tree approximation approach used for dependency graph parsing (Agić et al., 2015; Fernández-González and Martins, 2015), where dependency graphs were converted into dependency trees and then parsed by dependency tree parsers.",4 Experimental Setup,[1.0],"['Our approach is similar to the tree approximation approach used for dependency graph parsing (Agić et al., 2015; Fernández-González and Martins, 2015), where dependency graphs were converted into dependency trees and then parsed by dependency tree parsers.']"
"In our setting, the conversion to trees consists simply of removing remote edges from the graph, and then to bilexical trees by applying the same procedure as for bilexical graphs.
",4 Experimental Setup,[0],[0]
Baseline parsers.,4 Experimental Setup,[0],[0]
"We evaluate two bilexical graph semantic dependency parsers: DAGParser (Ribeyre et al., 2014), the leading transition-based parser in SemEval 2014 (Oepen et al., 2014) and TurboParser (Almeida and Martins, 2015), a graph-based parser from SemEval 2015 (Oepen et al., 2015); UPARSE (Maier and Lichte, 2016), a transition-based constituency parser supporting discontinuous constituents; and two bilexical tree parsers: MaltParser (Nivre et al., 2007), and the stack LSTM-based parser of Dyer et al. (2015, henceforce “LSTM Parser”).",4 Experimental Setup,[0],[0]
"Default settings are used in all cases.9 DAGParser and UPARSE use beam search by default, with a beam size of 5 and 4 respectively.",4 Experimental Setup,[0],[0]
The other parsers are greedy.,4 Experimental Setup,[0],[0]
"Table 2 presents our main experimental results, as well as upper bounds for the baseline parsers, re-
9For MaltParser we use the ARCEAGER transition set and SVM classifier.",5 Results,[0],[0]
"Other configurations yielded lower scores.
",5 Results,[0],[0]
"flecting the error resulting from the conversion.10
DAGParser and UPARSE are most directly comparable to TUPASparse, as they also use a perceptron classifier with sparse features.",5 Results,[0],[0]
"TUPASparse considerably outperforms both, where DAGParser does not predict any remote edges in the out-ofdomain setting.",5 Results,[1.0],"['TUPASparse considerably outperforms both, where DAGParser does not predict any remote edges in the out-ofdomain setting.']"
"TurboParser fares worse in this comparison, despite somewhat better results on remote edges.",5 Results,[1.0],"['TurboParser fares worse in this comparison, despite somewhat better results on remote edges.']"
"The LSTM parser of Dyer et al. (2015) obtains the highest primary F-score among the baseline parsers, with a considerable margin.
",5 Results,[0],[0]
"Using a feedforward NN and embedding features, TUPAMLP obtains higher scores than TUPASparse, but is outperformed by the LSTM parser on primary edges.",5 Results,[0],[0]
"However, using better input encoding allowing virtual look-ahead and look-behind in the token representation, TUPABiLSTM obtains substantially higher scores than TUPAMLP and all other parsers, on both primary and remote edges, both in the in-domain and out-of-domain settings.",5 Results,[0],[0]
"Its performance in absolute terms, of 73.5% F-score on primary edges, is encouraging in light of UCCA’s inter-annotator agreement of 80–85% F-score on them (Abend and Rappoport, 2013).
",5 Results,[0],[0]
"The parsers resulting from tree approximation 10The low upper bound for remote edges is partly due to the removal of implicit nodes (not supported in bilexical representations), where the whole sub-graph headed by such nodes, often containing remote edges, must be discarded.
are unable to recover any remote edges, as these are removed in the conversion.11 The bilexical DAG parsers are quite limited in this respect as well.",5 Results,[0],[0]
"While some of the DAG parsers’ difficulty can be attributed to the conversion upper bound of 58.3%, this in itself cannot account for their poor performance on remote edges, which is an order of magnitude lower than that of TUPABiLSTM.",5 Results,[0],[0]
"While earlier work on anchored12 semantic parsing has mostly concentrated on shallow semantic analysis, focusing on semantic role labeling of verbal argument structures, the focus has recently shifted to parsing of more elaborate representations that account for a wider range of phenomena (Abend and Rappoport, 2017).
",6 Related Work,[0],[0]
Grammar-Based Parsing.,6 Related Work,[0],[0]
"Linguistically expressive grammars such as HPSG (Pollard and Sag, 1994), CCG (Steedman, 2000) and TAG (Joshi and Schabes, 1997) provide a theory of the syntax-semantics interface, and have been used as a basis for semantic parsers by defining com-
11We also experimented with a simpler version of TUPA lacking REMOTE transitions, obtaining an increase of up to 2 labeled F-score points on primary edges, at the cost of not being able to predict remote edges.
",6 Related Work,[0],[0]
"12By anchored we mean that the semantic representation directly corresponds to the words and phrases of the text.
",6 Related Work,[0],[0]
"positional semantics on top of them (Flickinger, 2000; Bos, 2005, among others).",6 Related Work,[0],[0]
"Depending on the grammar and the implementation, such semantic parsers can support some or all of the structural properties UCCA exhibits.",6 Related Work,[0],[0]
"Nevertheless, this line of work differs from our approach in two important ways.",6 Related Work,[0],[0]
"First, the representations are different.",6 Related Work,[0],[0]
UCCA does not attempt to model the syntaxsemantics interface and is thus less coupled with syntax.,6 Related Work,[0],[0]
"Second, while grammar-based parsers explicitly model syntax, our approach directly models the relation between tokens and semantic structures, without explicit composition rules.
",6 Related Work,[0],[0]
Broad-Coverage Semantic Parsing.,6 Related Work,[0],[0]
"Most closely related to this work is Broad-Coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014, 2015).",6 Related Work,[0],[0]
"Like UCCA parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and reentrancy.",6 Related Work,[0],[0]
"In SDP, however, bilexical dependencies are used, and a head must be selected for every relation—even in constructions that have no clear head, such as coordination (Ivanova et al., 2012).",6 Related Work,[0],[0]
The use of non-terminal nodes is a simple way to avoid this liability.,6 Related Work,[0],[0]
"SDP also differs from UCCA in the type of distinctions it makes, which are more tightly coupled with syntactic considerations, where UCCA aims to capture purely semantic cross-linguistically applicable notions.",6 Related Work,[0],[0]
"For instance, the “poss” label in the DM target representation is used to annotate syntactic possessive constructions, regardless of whether they correspond to semantic ownership (e.g., “John’s dog”) or other semantic relations, such as marking an argument of a nominal predicate (e.g., “John’s kick”).",6 Related Work,[0],[0]
"UCCA reflects the difference between these constructions.
",6 Related Work,[0],[0]
"Recent interest in SDP has yielded numerous works on graph parsing (Ribeyre et al., 2014; Thomson et al., 2014; Almeida and Martins, 2015; Du et al., 2015), including tree approximation (Agić and Koller, 2014; Schluter et al., 2014) and joint syntactic/semantic parsing (Henderson et al., 2013; Swayamdipta et al., 2016).
",6 Related Work,[0],[0]
Abstract Meaning Representation.,6 Related Work,[0],[0]
"Another line of work addresses parsing into AMRs (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015), which, like UCCA, abstract away from syntactic distinctions and represent meaning directly, using OntoNotes predi-
cates (Weischedel et al., 2013).",6 Related Work,[0],[0]
"Events in AMR may also be evoked by non-verbal predicates, including possessive constructions.
",6 Related Work,[0],[0]
"Unlike in UCCA, the alignment between AMR concepts and the text is not explicitly marked.",6 Related Work,[0],[0]
"While sharing much of this work’s motivation, not anchoring the representation in the text complicates the parsing task, as it requires the alignment to be automatically (and imprecisely) detected.",6 Related Work,[0],[0]
"Indeed, despite considerable technical effort (Flanigan et al., 2014; Pourdamghani et al., 2014; Werling et al., 2015), concept identification is only about 80%–90% accurate.",6 Related Work,[0],[0]
"Furthermore, anchoring allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (Fernández-González and Martins, 2015; Birch et al., 2016).
",6 Related Work,[0],[0]
"Several transition-based AMR parsers have been proposed: CAMR assumes syntactically parsed input, processing dependency trees into AMR (Wang et al., 2015a,b, 2016; Goodman et al., 2016).",6 Related Work,[0],[0]
"In contrast, the parsers of Damonte et al. (2017) and Zhou et al. (2016) do not require syntactic pre-processing.",6 Related Work,[0],[0]
"Damonte et al. (2017) perform concept identification using a simple heuristic selecting the most frequent graph for each token, and Zhou et al. (2016) perform concept identification and parsing jointly.",6 Related Work,[0],[0]
UCCA parsing does not require separately aligning the input tokens to the graph.,6 Related Work,[0],[0]
"TUPA creates non-terminal units as part of the parsing process.
",6 Related Work,[0],[0]
"Furthermore, existing transition-based AMR parsers are not general DAG parsers.",6 Related Work,[0],[0]
"They are only able to predict a subset of reentrancies and discontinuities, as they may remove nodes before their parents have been predicted (Damonte et al., 2017).",6 Related Work,[0],[0]
"They are thus limited to a sub-class of AMRs in particular, and specifically cannot produce arbitrary DAG parses.",6 Related Work,[0],[0]
"TUPA’s transition set, on the other hand, allows general DAG parsing.13",6 Related Work,[0],[0]
"We present TUPA, the first parser for UCCA.",7 Conclusion,[0],[0]
"Evaluated in in-domain and out-of-domain settings, we show that coupled with a NN classifier and BiLSTM feature extractor, it accurately predicts UCCA graphs from text, outperforming a variety of strong baselines by a margin.
",7 Conclusion,[1.0000000694161586],"['Evaluated in in-domain and out-of-domain settings, we show that coupled with a NN classifier and BiLSTM feature extractor, it accurately predicts UCCA graphs from text, outperforming a variety of strong baselines by a margin.']"
"Despite the recent diversity of semantic pars-
13See Appendix E for a proof sketch for the completeness of TUPA’s transition set.
ing work, the effectiveness of different approaches for structurally and semantically different schemes is not well-understood (Kuhlmann and Oepen, 2016).",7 Conclusion,[0],[0]
"Our contribution to this literature is a general parser that supports multiple parents, discontinuous units and non-terminal nodes.
",7 Conclusion,[0],[0]
"Future work will evaluate TUPA in a multilingual setting, assessing UCCA’s cross-linguistic applicability.",7 Conclusion,[1.0],"['Future work will evaluate TUPA in a multilingual setting, assessing UCCA’s cross-linguistic applicability.']"
"We will also apply the TUPA transition scheme to different target representations, including AMR and SDP, exploring the limits of its generality.",7 Conclusion,[1.0],"['We will also apply the TUPA transition scheme to different target representations, including AMR and SDP, exploring the limits of its generality.']"
"In addition, we will explore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation.
",7 Conclusion,[0],[0]
"A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016).",7 Conclusion,[1.0],"['A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016).']"
"We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015).",7 Conclusion,[1.0],"['We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015).']"
"This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).",Acknowledgments,[0],[0]
The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences.,Acknowledgments,[0],[0]
"We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments.",Acknowledgments,[0],[0]
"We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation.",abstractText,[0],[0]
"UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units.",abstractText,[0],[0]
"To our knowledge, the conjunction of these formal properties is not supported by any existing parser.",abstractText,[0],[0]
"Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.",abstractText,[0],[0]
A Transition-Based Directed Acyclic Graph Parser for UCCA,title,[0],[0]
