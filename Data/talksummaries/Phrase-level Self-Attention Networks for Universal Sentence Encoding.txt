0	35	Following the success of word embeddings (Bengio et al., 2003; Mikolov et al., 2013), one of NLP’s next challenges has become the hunt for universal sentence encoders.
1	72	The goal is to learn a general-purpose sentence encoding model on a large corpus, which can be readily transferred to other tasks.
2	16	The learned sentence representations are able to generalize to unseen combination of words, which makes them highly desirable for downstream NLP tasks, especially for those with relatively small datasets.
3	15	Previous models for sentence encoding typically rely on Recurrent Neural Networks (RNNs) (Hochreiter and Schmidhuber, 1997; Chung et al., 2014) or Convolutional Neural Networks (CNNs) (Kalchbrenner et al., 2014; dos Santos and Gatti, 2014; Kim, 2014; Mou et al., 2016) to produce context-aware representation.
4	24	RNNs encode a sentence by reading words in sequential order, they are capable of learning long-term dependencies but are hard to parallelize and not time-efficient.
8	19	Shen et al. (2017) proposed the directional self-attention, they apply forward and backward masks to the alignment score matrix to encode temporal order information, and computed attention at feature level to select the features that can best describe the word’s meaning in given context.
9	15	Effective as their models are, the memory required to store the alignment scores of all the token pairs grows quadratically with the sentence length.
10	14	Furthermore, the syntactic property that is intrinsic to natural language is not considered at all.
11	13	Language is inherently tree structured, and the meaning of a sentence comes largely from composing the meanings of subtrees (Chomsky, 1957).
31	18	At last, an attention mechanism is utilized to summarize all the token representations into a fixed-length sentence vector.
32	32	The phrase structure organizes words into nested constituents which can be successively divided into their parts as we move down the constituencybased parse trees.
33	11	One phrase division shows only one aspect of context dependency.
34	38	In order to capture different levels of context dependencies, we can split a sentence at different granularities.
39	20	In order to filter out information that is semantically or syntactically distant, self-attention is performed at the phrase level instead of the sentence level.
43	16	,pl], where l is the length of the phrase and d is the dimension of word embedding representation, we first compute the alignment score for each token pair in the phrase: aij = σ ( W a1pi +W a2pj + b a ) +Mij Mij = { 0, i 6= j −∞, i = j (1) where σ (·) is an activation function, W a1,W a2 ∈ Rd×d and ba ∈ Rd are parameters to be learned, and M is a diagonal-diabled mask (Hu et al., 2017) that aims to prevent a word from being aligned with itself.
44	14	The output of the attention mechanism is a weighted sum of embeddings from all tokens for each token in the phrase: p̃i = l∑ j=1 [ exp (aij)∑l k=1 exp (aik) pj ] (2) where means point-wise product.
56	18	We propose a novel gated memory updating mechanism to refine each word representation hierarchically with longer-term dependencies captured in a larger granularity.
57	12	Inspired by the idea of adaptive gate in highway networks (Srivastava et al., 2015), our memory mechanism add a gate to original memory networks (Weston et al., 2014; Sukhbaatar et al., 2015).
58	11	This gate has the ability to determine the importance of the new input and the original memory in the memory updating.
67	30	Following Conneau et al. (2017), we train our sentence encoder using the SNLI dataset, and evaluate it across a variety of NLP tasks including sentence classification, natural language inference and sentence textual similarity.
90	14	To show the modeling capacity and robustness of our proposed model, we evaluate our model across a wide range of tasks that can be solved purely based on the encoded semantics.
100	12	Micro and macro accuracies are two composite indicators for evaluating transfer performance of tasks whose metric is classification accuracy.
103	26	PSAN achieves the state-of-the-art performance with considerably fewer parameters, outperforming a RNN-based model, a CNN-based model, a fully attention-based model and a model that utilize syntactic information.
109	13	For thorough comparison, we implement seven extra baselines to analyze the improvements con- tributed by each part of our PSAN model: • PSA on the first/second/third layer only only uses the Phrase-level Self-Attention at the first/second/third layer of phrase division.
124	27	Comparing (7) with (1), (2) and (3), we can find that performing self-attention at the phrase level is generally better than at the sentence level, indicating that reducing attention context into phrase level can effectively filter out words that are syntactically and semantically distant, thus focusing on the interaction with important words.
131	16	This demonstrates that incorporating syntactic information by performing self-attention at the phrase level and refining each word’s representation hierarchically can help to capture long-term dependencies across words in a sentence.
144	35	Finally, nouns and verbs dominate the attention weights, while stop words like “a” and “its”, contribute little to the final sentence representation, this indicates that PSAN can effectively pick out semantically important words that are most representative for the meaning of the whole sentence.
159	17	We propose the Phrase-level Self-Attention Networks (PSAN), a fully attention-based model that can utilize syntactic information for universal sentence encoding.
160	35	By applying self-attention at the phrase level, we can filter out distant and unrelated words and focus on modeling interaction between semantically and syntactically important words, a gated memory updating mechanism is utilized to incorporate different levels of contextual information along the parse tree.
161	15	Empirical results on a wide range of transfer tasks demonstrate the effectiveness of our model.
