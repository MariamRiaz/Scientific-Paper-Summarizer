0	58	Contextual bandits refer to a learning setting where the learner repeatedly observes a context, takes an action and observes a reward for the chosen action in the observed context, but no feedback on any other action.
1	29	An example is movie recommendation, where the context describes a user, actions are candidate movies and the reward measures if the user enjoys the recommended movie.
4	87	By letting the policy choose actions (e.g., recommend movies to users), we can compute its reward.
7	27	Given logs from the existing system, which might be choosing actions according to a very different logging policy than the one we seek to evaluate, can we estimate the expected reward of the target policy?
8	30	There are three classes of approaches to address this question: the direct method (DM), also known as regression adjustment, inverse propensity scoring (IPS) (Horvitz & Thompson, 1952) and doubly robust (DR) estimators (Robins & Rotnitzky, 1995; Bang & Robins, 2005; Dudík et al., 2011; 2014).
9	26	Our first goal in this paper is to study the optimality of these three classes of approaches (or lack thereof), and more fundamentally, to quantify the statistical hardness of offpolicy evaluation.
11	39	In both settings, a major underlying assumption is that rewards can be consistently estimated from the features (i.e., covariates) describing contexts and actions, either via a parametric model or non-parametrically.
13	31	Unfortunately, consistency of a reward model can be difficult to achieve in practice.
20	23	We attribute this to a lower variance of the DR estimator.
22	22	We therefore ask whether it is possible to achieve an even better bias-variance tradeoff than DR. We answer affirmatively and propose a new class of estimators, called the SWITCH estimators, that adaptively interpolate between DM and DR (or IPS).
56	33	What is the smallest MSE that any estimator can achieve in the worst case over a large class of contextual bandit problems?
58	31	In our problem, we fix λ, µ and π, and only take worst case over a class of reward distributions.
63	16	Formally, an estimator is any function v̂ : (X × A × R)n → R that takes n data points collected by µ and outputs an estimate of vπ.
82	34	In general, choosing γ = O ( 1/(n log n) ) excludes the contexts likely to appear multiple times, and ensures that the second term in Theorem 1 remains nontrivial (when µ(x, a) ≤ γ with positive probability).
98	19	Comparison with asymptotic optimality results: As discussed in Section 1, previous work on optimal off-policy evaluation, specifically the average treatment estimation, assumes that it is possible to consistently estimate r∗(x, a) = E[r | x, a].
124	18	We consider two separate problem instances corresponding to the two terms in Theorem 1.
130	26	In order to lower bound the MSE across all problems, it suffices to lower bound Eθ[MSEη(v̂)].
131	21	That is, we can compute the MSE of an estimator for each individual η, and take expectation of the MSEs under the prior prescribed by θ.
134	21	This is powerful, since this new problem instance has stochastic rewards, just like Gaussian mean estimation, and is amenable to standard techniques.
139	40	However, under a large reward noise σ, DR may still suffer from high variance when the importance weights are large, even when given a perfect reward model.
163	16	Bottou et al. (2013) consider a special case of SWITCH with r̂ ≡ 0, meaning that all the actions with large importance weights are eliminated from IPS.
192	16	In our experiments, the automatic tuning using Eq.
201	15	In each data set with n examples, we treat the uniform distribution over the data set itself as a surrogate of the population distribution so that we know the ground truth of the rewards.
209	44	This allows us to get valid confidence intervals for our empirical estimates of this quantity.
211	64	In this section, whenever we refer to “MSE”, we are referring to this truncated version.
212	37	We compare SWITCH and SWITCH-DR against the following baselines: 1.
213	24	DM trained via logistic regression; 3.
228	28	The automatic tuning by MAGIC tends to revert to DM, because its bias estimate is too optimistic and so DM is preferred whenever IPS or DR have some significant variance.
229	46	The gains of SWITCH-DR are even greater in the noisy-reward setting, where we add label noise to UCI data.
232	39	In the first case, SWITCH-DR outperforms both DM and IPS, while DR improves over IPS only moderately.
