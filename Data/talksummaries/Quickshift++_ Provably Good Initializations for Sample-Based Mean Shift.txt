0	25	Quick Shift (Vedaldi & Soatto, 2008) is a mode-seeking based clustering algorithm that has a growing popularity in computer vision.
4	10	The most popular choice of empirical density function is the Kernel Density Estimator (KDE) with Gaussian Kernel.
10	42	The key differences are that Quick Shift restricts the steps to sample points (and thus is a sample-based version of Mean Shift) and has the extra τ parameter which allows it to merge close segments together.
11	56	One of the drawbacks of these two procedures, as well as many mode-seeking based clustering algorithms, is that the point-modes of the density functions are often poor representations of the clusters.
12	98	This will happen when the high-density regions within a cluster are of arbitrary shape and have some variations causing the underlying density function to have possibly many apparent, but not so salient modes.
14	20	To combat this effect, practitioners often increase the kernel bandwidth, which makes the density estimate more smooth.
19	8	We introduce Quickshift++, which first estimates these cluster-cores, and then runs the Quick Shift based hillclimbing procedure on each remaining sample until it reaches a cluster-core.
57	9	Let rk(x) := inf{r > 0 : |B(x, r)∩X[n]| ≥ k}, i.e., the distance from x to its k-th nearest neighbor.
59	40	We define the cluster core with respect to fixed fluctuation parameter β as follows.
65	10	We have that M1 and M2 are CCs of {x ∈ X : f(x) ≥ λ1} and {x ∈ X : f(x) ≥ λ2}, respectively for some λ1, λ2.
72	19	Moreover, the multiplicative fluctuation adapts to clusters at varying density levels more reasonably than a fixed additive fluctuation.
81	13	This follows from the fact that the CCs of G(λ) approximates the CCs of {x ∈ X : f(x) ≥ λ}.
98	33	This step runs in O(nk · α(n)) where α is the Inverse Ackermann function (Cormen, 2009), in addition to the time it takes to compute the k-NN sets for the n sample points.
99	8	To cluster the remaining points, for each sample not in a cluster-core, we must find its nearest sample of higher k-NN density.
100	10	Although this is worst-case O(n) time for each sample point, fortunately we see that in practice (as long as k is not too small) for the vast majority of cases, the nearest sample with higher density is within the k-nearest neighbor set so it only takes O(k) in most cases.
125	13	The assumption is borrowed from (Jiang, 2017b).
127	8	This is adapted from standard analyses of level-set estimation (e.g. Assumption B of Singh et al. (2009)).
152	10	where Cδ,n := 16 log(2/δ) √ d log n. We next need the following uniform concentration bound on balls intersected with level-sets, which says that if such a set has large enough probability mass, then it will contain a sample point with high probability.
153	11	Let E := {B(x, r) ∩ Lf (λ) : x ∈ Rd, r > 0, λ > 0}.
170	14	Figure 3 provides simple verification that Quickshift++ provides reasonable clusterings in a wide variety of situations where other density-based procedures are known to fail.
171	44	For instance, in the two rings dataset (first row), we see that Mean Shift and Quick Shift suffer from the oversegmentation issue coupled with the oversized bandwidth which causes them to recover clusters that have points from both the rings even though the rings are separated.
172	54	In the three Gaussians dataset (third row), we see that DBSCAN fails because the three clusters are of different density levels and thus no matter which density-level we set, DBSCAN will not be able to recover the three clusters.
173	12	In order to apply clustering to image segmentation, we use the following standard approach (see e.g. Felzenszwalb & Huttenlocher (2004)): we transform each pixel into a 5-dimensional vector where two coordinates correspond to the location of the pixel and three correspond to each of the RGB color channels.
176	9	We compare Quickshift++ to Quick Shift, as the latter is often used for segmentation.
178	10	On the other hand Quickshift++ gives us reason- able segmentations in many cases and can capture segments that may be problematic for other procedures.
181	30	We ran Quickshift++ against other clustering algorithms on the various real datasets and scored against the groundtruth using the adjusted rand index and the adjusted mutual information scores.
193	9	Then remaining points are assigned to its appropriate cluster-core using a hill-climbing procedure based on Quick Shift.
195	19	As a result, Quickshift++ enjoys the advantages of the popular density-based clustering algorithms while avoiding many of their respective weaknesses.
196	47	We then gave guarantees for cluster recovery.
197	40	Finally, we showed that the algorithm has strong and robust performance on real datasets and has promising applications to image segmentation.
