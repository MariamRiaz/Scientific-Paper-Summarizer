{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698\u2013707 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1065"}, {"heading": "1 Introduction", "text": "Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016). In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations. After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations. In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs).\n\u2217Contribution during internship at Microsoft Research.\nMany methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015). Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on. All these NMT models employ a sequential recurrent neural network for target generations. Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints. This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies. For example, Figure 1 shows an incorrect translation related to the long-distance dependency. The translation fragment in italic is locally fluent around the word is, but from a global view the translation is ungrammatical. Actually, this part of translation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby.\nFortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees. In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is. This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008). On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models:\n698\na) syntactic trees can be used to model the grammatical validity of translation candidates; b) partial syntactic structures can be used as additional context to facilitate future target word prediction.\nHowever, it is not trivial to build and leverage syntactic structures on the target side in current NMT framework. Several practical challenges arise:\n(1) How to model syntactic structures such as dependency parse trees with recurrent neural network;\n(2) How to efficiently perform both target word generation and syntactic structure construction tasks simultaneously in a single neural network;\n(3) How to effectively leverage target syntactic context to help target word generation.\nTo address these issues, we propose and empirically evaluate a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) model in our paper. An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees. The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004).\nWe will describe in detail how these two RNNs work interactively in Section 3.\nWe evaluate our method on publicly available data sets with Chinese-English and JapaneseEnglish translation tasks. Experimental results show that our model significantly improves translation accuracy over the conventional NMT and SMT baseline systems."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Neural Machine Translation", "text": "As a new paradigm to machine translation, NMT is an end-to-end framework (Sutskever et al., 2014; Bahdanau et al., 2015) which directly models the conditional probability P (Y |X) of target translation Y = y1,y2,...,yn given source sentence X = x1,x2,...,xm. An NMT model consists of two parts: an encoder and a decoder. Both of them utilize recurrent neural networks which can be a Gated Recurrent Unit (GRU) (Cho et al., 2014) or a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) in practice. The encoder bidirectionally encodes a source sentence into a sequence of hidden vectorsH = h1,h2,...,hm with a forward RNN and a backward RNN. Then the decoder predicts target words one by one with probability\nP (Y |X) = n\u220f\nj=1\nP (yj|y<j, H) (1)\nTypically, for the jth target word, the probability P (yj |y<j , H) is computed as\nP (yj|y<j, H) = g(sj, yj\u22121, cj) (2)\nwhere g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state. The context cj is calculated at each timestamp j based on H by the attention network\ncj = m\u2211\nk=1\najkhk (3)\najk = exp(ejk)\u2211m i=1 exp(eji)\n(4)\nejk = v T a tanh(Wasj\u22121 + Uahk) (5)\nwhere va, Wa, Ua are the weight matrices. The attention mechanism is effective to model the correspondences between source and target."}, {"heading": "2.2 Dependency Tree Construction", "text": "We use a shift-reduce transition-based dependency parser to build the syntactic structure for the target language in our work. Specially, we adopt the arcstandard algorithm (Nivre, 2004) to perform incremental parsing during the translation process. In this algorithm, a stack and a buffer are maintained to store the parsing state over which three kinds of transition actions are applied. Let w0 and w1 be two topmost words in the stack, and w\u0304 be the current new word in a sequence of input, three transition actions are described as below.\n\u2022 Shift(SH) : Push w\u0304 to the stack.\n\u2022 Left-Reduce(LR(d)) : Link w0 and w1 with dependency label d as w0\nd\u2212\u2192w1, and reduce them to the head w0.\n\u2022 Right-Reduce(RR(d)) : Link w0 andw1 with dependency label d as w0\nd\u2190\u2212w1, and reduce them to the head w1.\nDuring parsing, an specific structure is used to record the dependency relationship between different words of input sentence. The parsing finishes when the stack is empty and all input words are consumed. As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence (Nivre, 2004). Because each valid transition action sequence corresponds to a unique dependency tree, a dependency tree can also be equivalently represented by a sequence of transition actions."}, {"heading": "3 Sequence-to-Dependency Neural Machine Translation", "text": "An SD-NMT model is an extension to the conventional NMT model augmented with syntactic structural information of target translation. Given a source sentenceX = x1,x2,..,xm, its target translation Y = y1,y2,..,yn and Y \u2019s dependency parse tree T , the goal of the extension is to enable us to compute the joint probability P (Y, T |X). As in most structural learning tasks, the full prediction of Y and T is further decomposed into a chain of smaller predictions. For translation Y , it is generated in the left-to-right order as y1, y2, .., yn following the way in a normal sequence-to-sequence model. For Y \u2019s parse tree T , instead of directly modeling the tree itself, we predict a parsing action sequence A which can map Y to T . Thus at\ntop level our SD-NMT model can be formulated as\nP (Y, T |X) = P (Y,A|X) = P (y1y2..yn, a1, a2..al|X)(6)\nwhere A = a1,a2,..,aj ,..,al 1 with length l (l = 2n), aj \u2208 {SH,RR(d),LR(d)}2.\nTwo recurrent neural networks, Word-RNN and Action-RNN, are used to model generation processes of translation sequence Y and parsing action sequence A respectively. Figure 2 shows an example how translation Y and its parsing actions are predicted step by step.\nBecause the lengths of Word-RNN and ActionRNN are different, they are designed to work in a mutually dependent way: a target word is only allowed to be generated when the SH action is predicted in the action sequence. In this way, we can perform incremental dependency parsing for translation Y and at the same time track the partial parsing status through the translation generation process.\nFor notational clarity, we introduce a virtual translation sequence Y\u0302 =y\u03021,y\u03022,..,y\u0302j ,..,y\u0302l for WordRNN which has the same length l with transition action sequence. y\u0302j is defined as\ny\u0302j = { yvj \u03b4(SH, aj) = 1 yvj\u22121 \u03b4(SH, aj) = 0\nwhere \u03b4(SH, aj) is 1 when aj = SH, otherwise 0. vj is the index of Y , computed by vj =\u2211j\ni=1 \u03b4(SH, ai). Apparently the mapping from Y\u0302 1In the rest of this paper, aj represents the transition action, rather than the attention weight in Equation 4. 2RR(d) refers to a set of RR actions augmented with dependency labels so as to LR(d).\n\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5a\ud835\udc5d 1 2 3 4 \ud835\udc57 \u2212 1\n\ud835\udc8b\nto Y is deterministic, and Y can be easily derived given Y\u0302 and A.\nWith the notation of Y\u0302 , the sequence probability of Y and A can be written as\nP (A|X, Y\u0302<l) = l\u220f\nj=1\nP (aj|a<j, X, Y\u0302<j) (7)\nP (Y\u0302 |X,A\u2264l) = l\u220f\nj=1\nP (y\u0302j|y\u0302<j, X,A\u2264j)\u03b4(SH,aj)\n(8)\nwhere Y\u0302<j refers to the subsequence y\u03021, y\u03022, .., y\u0302j\u22121, and A\u2264j to a1, a2, .., aj . Based on Equation 7 and 8, the overall joint model can be computed as\nP (Y, T |X) = P (A|X, Y\u0302<l)\u00d7 P (Y\u0302 |X,A\u2264l) (9)\nAs we have two RNNs in our model, the termination condition is also different from a conventional NMT model. In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced.\nFigure 3 (a) gives an overview of our SD-NMT model. Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model\nfollows standard bidirectional RNN configuration. At timestamp j during decoding, our model first predicts an action aj by Action-RNN, then WordRNN checks the condition gate \u03b4 according to aj . If aj = SH, the Word-RNN will generate a new state (solid arrow) and predict a new target word yvj , otherwise it just copies previous state (dashed arrow) to the current state. For example, at timestamp 3, a3 6= SH, the state of Word-RNN is copied from its previous one. Meanwhile, y\u03023 = y2 is used as the immediate proceeding word in translation history.\nWhen computing attention scores, we extend Equation 5 by replacing the decoder hidden state with the concatenation of Word-RNN hidden state s and Action-RNN hidden state s\u2032 (gray boxes in Figure 3). The new attention score is then updated as\nejk = v T a tanh(Wa[sj\u22121; s \u2032 j\u22121] + Uahk) (10)"}, {"heading": "3.1 Syntactic Context for Target Word Prediction", "text": "Syntax has been proven useful for sentence generation task (Dyer et al., 2016). We propose to leverage target syntax to help translation generation. In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current\nparsing configuration of Action-RNN. Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree,w0r andw1r their rightmost modifiers respectively. We define two unigram features and four bigram features. The unigram features are w0 and w1 which are represented by the word embedding vectors. The bigram features are w0w0l, w0w0r, w1w1l and w1w1r. Each of them is computed by bhc = tanh(WbEwh + UbEwhc), h \u2208 {0, 1}, c \u2208 {l, r}. These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008). Based on these features, the syntactic context vector Kj is computed as\nKj = tanh(Wk[Ew0;Ew1] + Uk[b0l; b0r; b1l; b1r]) (11)\nwhere Wk, Uk, Wb, Ub are the weight matrices, E stands for the embedding matrix. Figure 2 (b) gives an overview of the construction of Kj . Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation.\nAdding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and 8 are then updated as\nP (aj|a<j, X, Y\u0302<j) = g(s\u2032j, aj\u22121, cj,Kj) (12) P (y\u0302j|y\u0302<j, X,A\u2264j) = g(sj, y\u0302j\u22121, cj,Kj) (13)\nAfter each prediction step in Word-RNN and Action-RNN, the syntax context vector K will be updated accordingly. Note that K is not used to calculate the recurrent states s in this work."}, {"heading": "3.2 Model Training and Decoding", "text": "For SD-NMT model, we use the sum of loglikelihoods of word sequence and action sequence as objective function for training algorithm, so that the joint probability of target translations and their parsing trees can be maximized:\nJ(\u03b8) = \u2211\n(X,Y,A)\u2208D log P (A|X, Y\u0302<l)+\nlog P (Y\u0302 |X,A\u2264l) (14)\nWe also use mini-batch for model training. As the target dependency trees are known in the bilingual corpus during training, we pre-compute the partial tree state and syntactic context at each time\nstamp for each training instance. Thus it is easy for the model to process multiple trees in one batch.\nIn the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length:\nscore = 1\nl\nl\u2211\nj=1\nlog P (aj |a<j , X, Y\u0302<j)+\n1\nn\nl\u2211\nj=1\n\u03b4(SH, aj) log P (y\u0302j |y\u0302<j , X,A\u2264j) (15)\nwhere n is word sequence length and l is action sequence length."}, {"heading": "4 Experiments", "text": "The experiments are conducted on the ChineseEnglish task as well as the Japanese-English translation tasks where the same data set from WAT 2016 ASPEC corpus (Nakazawa et al., 2016) 3 is used for a fair comparison with other work. In addition to evaluate translation performance, we also investigate the quality of dependency parsing as a by-product and the effect of parsing quality against translation quality."}, {"heading": "4.1 Setup", "text": "In the Chinese-English task, the bilingual training data consists of a set of LDC datasets, 4 which has around 2M sentence pairs. We use NIST2003 as the development set, and the testsets contain NIST2005, NIST2006, NIST2008 and NIST2012. All English words are lowercased.\nIn the Japanese-English task, we use top 1M sentence pairs from ASPEC Japanese-English corpus. The development data contains 1,790 sentences, and the test data contains 1,812 sentences with single reference per source sentence.\nTo train SD-NMT model, the target dependency tree references are needed. As there is no golden annotation of parse trees over the target training data, we use pseudo parsing results as the target dependency references, which are got from an in-house developed arc-eager dependency parser based on work in (Zhang and Nivre, 2011).\n3http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ 4LDC2003E14, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2003E07, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07, LDC2004T08, LDC2005T06\nIn the neural network training, the vocabulary size is limited to 30K high frequent words for both source and target languages. All low frequent words are normalized into a special token unk and post-processed by following the work in (Luong et al., 2015b). The size of word embedding and transition action embedding is set to 512. The dimensions of the hidden states for all RNNs are set to 1024. All model parameters are initialized randomly with Gaussian distribution (Glorot and Bengio, 2010) and trained on a NVIDIA Tesla K40 GPU. The stochastic gradient descent (SGD) algorithm is used to tune parameters with a learning rate of 1.0. The batch size is set to 96. In the update procedure, Adadelta (Zeiler, 2012) algorithm is used to automatically adapt the learning rate. The beam sizes for both word prediction and transition action prediction are set to 12 in decoding.\nThe baselines in our experiments are a phrasal system and a neural translation system, denoted by HPSMT and RNNsearch respectively. HPSMT is an in-house implementation of the hierarchical phrase-based model (Chiang, 2005), where a 4- gram language model is trained using the modified Kneser-Ney smoothing (Kneser and Ney, 1995) algorism over the English Gigaword corpus (LDC2009T13) plus the target data from the bilingual corpus. RNNsearch is an in-house implementation of the attention-based neural machine translation model (Bahdanau et al., 2015) using the same parameter settings as our SD-NMT model including word embedding size, hidden vector dimension, beam size, as well as the same mechanism for OOV word processing.\nThe evaluation results are reported with the case-insensitive IBM BLEU-4 (Papineni et al., 2002). A statistical significance test is performed using the bootstrap resampling method proposed by Koehn (2004) with a 95% confidence level. For Japanese-English task, we use the official eval-\nuation procedure provided by WAT 2016.5, where both BLEU and RIBES (Isozaki et al., 2010) are used for evaluation."}, {"heading": "4.2 Evaluation on Chinese-English Translation", "text": "We evaluate our method on the Chinese-English translation task. The evaluation results over all NIST test sets against baselines are listed in Table 1. Generally, RNNsearch outperforms HPSMT by 3.78 BLEU points on average while SD-NMT surpasses RNNsearch 2.03 BLUE point gains on average, which shows that NMT models usually achieve better results than SMT models, and our proposed sequence-to-dependency NMT model performs much better than traditional sequence-tosequence NMT model.\nWe also investigate the effect of syntactic knowledge context by excluding its computation in Equation 12 and 13. The alternative model is denoted by SD-NMT\\K. According to Table 1, SD-NMT\\K outperforms RNNsearch by 0.54 BLEU points but degrades SD-NMT by 1.49 BLEU points on average, which demonstrates that the long distance dependencies captured by the target syntactic knowledge context, such as leftmost/rightmost children together with their dependency relationships, really bring strong positive effects on the prediction of target words.\nIn addition to translation quality, we compare the perplexity (PPL) changes on the development set in terms of numbers of training mini-batches for RNNsearch and SD-NMT in Figure 4. We can see that the PPL of SD-NMT is initially higher than that of RNNsearch, but decreased to be lower over time. This is mainly because the quality of parse tree is too poor at the beginning which degrades translation quality and leads to higher PPL. After some training iterations, the SD-NMT\n5http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index .html\nmodel learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL.\n25 17.26 15.74\n26 18.76 16.58\n27 17.62 15.88\nIn our experiments, the time cost of SD-NMT is two times of that for RNNsearch due to a more complicated model structure. But we think it is a worthy trade to pursue high quality translations."}, {"heading": "4.3 Evaluation on Japanese-English Translation", "text": "In this section, we report results on the JapaneseEnglish translation task. To ensure fair comparisons, we use the same training data and follow the pre-processing steps recommended in WAT 20166. Table 2 shows the comparison results from 8 systems with the evaluation metrics of BLEU and RIBES. The results in the first 3 rows are produced by SMT systems taken from the official WAT 2016. The remaining results are produced by NMT systems, among which the bottom two row results are taken from our in-house NMT systems and others refer to the work in (Cromieres, 2016;\n6http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/data PreparationJE.html\nCromieres et al., 2016) that are the competitive NMT results on WAT 2016. According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results. The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016). Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016). We believe SD-NMT can get more improvements with an ensemble of multiple models in future experiments."}, {"heading": "4.4 Effect of the Parsing Accuracy upon Translation Quality", "text": "The interaction effect between dependency tree conduction and target word generation is investigated in this section. The experiments are conducted on the Chinese-English task over multiple test sets. We evaluate how the quality of dependency trees affect the performance of translation. In the decoding phase of SD-NMT, beam search is applied to the generations of both transition and actions as illustrated in Equation 15. Intuitively, the larger the beam size of action prediction is, the better the dependency tree quality is. We fix the beam size for generating target words to 12, and change the beam size for action prediction to see the difference. Figure 5 shows the evaluation results of all test sets. There is a tendency for BLEU scores to increase with the growth of action prediction beam size. The reason is that the translation quality increases as the quality of dependency tree improves, which shows the construction of dependency trees can boost the generation of target\n4 38.77 40.64 32.06 30.63\n6 38.93 41.32 32.63 31.07\n8 39.34 41.52 32.88 31.32\n10 39.32 41.65 32.82 31.41 12 39.38 41.81 33.06 31.43\nwords, and vice versa we believe."}, {"heading": "4.5 Quality Estimation of Dependency Tree Construction", "text": "As a by-product, the quality of dependency trees not only affects the performance of target word generation, but also influences the possible downstream processors or tasks such as text analyses. The direct evaluation of tree quality is not feasible due to the unavailable golden references. So we resort to estimating the consistency between the by-products and the parsing results of our standalone dependency parser with state-of-the-art performance. The higher the consistency is, the closer the performance of by-product is to the standalone parser. To reduce the influence of ill-formed data as much as possible, we build the evaluation data set by heuristically selecting 360 SD-NMT translation results together with their dependency trees from NIST test sets where both source- and target-side do not contain unk and have a length of 20-30. We then take the parsing results of the stand-alone parser for these translations as references to indirectly estimate the quality of byproducts. We get a UAS (unlabeled attachment score) of 94.96% and a LAS (labeled attachment score) of 93.92%, which demonstrates that the dependency trees produced by SD-NMT are much similar with the parsing results from the standalone parser."}, {"heading": "4.6 Translation Example", "text": "In this section, we give a case study to explain how our method works. Figure 6 shows a translation example from the NIST testsets. SMT and RNNsearch refer to the translation results from the\nbaselines HPSMT and NMT. For our SD-NMT model, we list both the generated translation and its corresponding dependency tree. We find that the translation of SMT is disfluent and ungrammatical, whereas RNNsearch is better than SMT. Although the translation of RNNsearch is locally fluent around word \u201chave\u201d in the rectangle, both its grammar is incorrect and its meaning is inaccurate from a global view. The word \u201chave\u201d should be in a singular form as its subject is \u201csafety\u201d rather than \u201cworkers\u201d. For our SD-NMT model, we can see that the translation is much better than baselines and the dependency tree is reasonable. The reason is that after generating the word \u201cworkers\u201d, the previous subtree in the gray region is transformed to the syntactic context which can guide the generation of the next word as illustrated by the dashed arrow. Thus our model is more likely to generate the correct verb \u201cis\u201d with singular form. In addition, the global structure helps the model correctly identify the inverted sentence pattern of the former translated part and make better choices for the future translation (\u201conly when .. can ..\u201d in our translation, \u201conly when .. will ..\u201d in the reference), which remains a challenge for conventional NMT model."}, {"heading": "5 Related Work", "text": "Incorporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006). Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information. Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information. These methods show promising improvement for SMT.\nRecently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016). In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected. Some effort has been done to incorporate source syntax into NMT. Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement. Intuitively, adding source syntactic information to\n[Source] \u53ea\u6709\u65bd\u5de5\u4eba\u5458\u7684\u5b89\u5168\u5f97\u5230\u4e86\u4fdd\u8bc1 , \u624d\u80fd\u7ee7\u7eed\u65bd\u5de5 . [Reference] only when the safety of the workers is guaranteed will they continue with the project . [HPSMT] only safety is assured of construction personnel , to continue construction . [RNNsearch] only when the safety of construction workers have been guaranteed to continue construction . [SD-NMT] only when the safety of the workers is guaranteed can we continue to work .\nNMT is straightforward, because the source sentence is definitive and easy to attach extra information. However, it is non-trivial to add target syntax as target words are uncertain in decoding process. Up to now, there is few work that attempts to build and leverage target syntactic information for NMT.\nThere has been work that incorporates syntactic information into NLP tasks with neural networks. Dyer et al. (2016) presented a RNN grammar for parsing and language modeling. They replaced SH with a set of generative actions to generate words under a Stack LSTM framework (Dyer et al., 2015), which achieves promising results for language modeling on the Penn Treebank data. In our work, we propose to involve target syntactic trees into NMT model to jointly learn target translation and dependency parsing where target syntactic context over the parse tree is used to improve the translation quality."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we propose a novel string-todependency translation model over NMT. Our model jointly performs target word generation and arc-standard dependency parsing. Experimental results show that our method can boost the two procedures and achieve significant improvements on the translation quality of NMT systems.\nIn future work, along this research direction, we will try to integrate other prior knowledge, such as\nsemantic information, into NMT systems. In addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness."}, {"heading": "Acknowledgments", "text": "We are grateful to the anonymous reviewers for their insightful comments. We also thank Shujie Liu and Zhirui Zhang for the helpful discussions."}], "year": 2017, "references": [{"title": "Neural machine translation by jointly learning to align and translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015 .", "year": 2015}, {"title": "A hierarchical phrase-based model for statistical machine translation", "authors": ["David Chiang."], "venue": "Proceedings of ACL 2005.", "year": 2005}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings", "year": 2014}, {"title": "Kyoto-nmt: a neural machine translation implementation in chainer", "authors": ["Fabien Cromieres."], "venue": "Proceedings of COLING 2016.", "year": 2016}, {"title": "Kyoto university participation to wat 2016", "authors": ["Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi."], "venue": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016). pages 166\u2013174.", "year": 2016}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "authors": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of ACL 2015.", "year": 2015}, {"title": "Recurrent neural network grammars", "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith."], "venue": "Proceedings of the NAACL 2016.", "year": 2016}, {"title": "Tree-to-sequence attentional neural machine translation", "authors": ["Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka."], "venue": "Proceedings of ACL 2016.", "year": 2016}, {"title": "Scalable inference and training of context-rich syntactic translation models", "authors": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer."], "venue": "Proceedings of ACL 2006.", "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats. volume 9, pages 249\u2013256.", "year": 2010}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8).", "year": 1997}, {"title": "Automatic evaluation of translation quality for distant language pairs", "authors": ["Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada."], "venue": "Proceedings of EMNLP.", "year": 2010}, {"title": "On using very large target vocabulary for neural machine translation", "authors": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL 2015.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "authors": ["Reinhard Kneser", "Hermann Ney."], "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on. IEEE, volume 1, pages 181\u2013184.", "year": 1995}, {"title": "Statistical significance tests for machine translation evaluation", "authors": ["Philipp Koehn."], "venue": "EMNLP. Citeseer, pages 388\u2013395.", "year": 2004}, {"title": "Treeto-string alignment template for statistical machine translation", "authors": ["Yang Liu", "Qun Liu", "Shouxun Lin."], "venue": "Proceedings of ACL 2006.", "year": 2006}, {"title": "Effective approaches to attentionbased neural machine translation", "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of EMNLP 2015.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "authors": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL 2015.", "year": 2015}, {"title": "Aspec: Asian scientific paper excerpt corpus", "authors": ["Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara."], "venue": "Nicoletta Calzolari (Conference Chair), Khalid Choukri,", "year": 2016}, {"title": "Lexicons and minimum risk training for neural machine translation: NAISTCMU at WAT2016", "authors": ["Graham Neubig."], "venue": "Proceedings of the 3nd Workshop on Asian Translation (WAT2016). Osaka, Japan.", "year": 2016}, {"title": "Incrementality in deterministic dependency parsing", "authors": ["Joakim Nivre."], "venue": "Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together.", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL 2002.", "year": 2002}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "authors": ["Libin Shen", "Jinxi Xu", "Ralph M Weischedel."], "venue": "ACL. pages 577\u2013585.", "year": 2008}, {"title": "Minimum risk training for neural machine translation", "authors": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of ACL 2016.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "Proceedings of ACL 2016.", "year": 2016}, {"title": "Adadelta: an adaptive learning rate method", "authors": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 .", "year": 2012}, {"title": "Variational neural machine translation", "authors": ["Biao Zhang", "Deyi Xiong", "jinsong su", "Hong Duan", "Min Zhang"], "venue": "In Proceedings of EMNLP", "year": 2016}, {"title": "A tale of two parsers: Investigating and combining graphbased and transition-based dependency parsing", "authors": ["Yue Zhang", "Stephen Clark."], "venue": "EMNLP2008.", "year": 2008}, {"title": "Transition-based dependency parsing with rich non-local features", "authors": ["Yue Zhang", "Joakim Nivre."], "venue": "Proceedings of ACL 2011.", "year": 2011}], "id": "SP:c79d8c3768b8dbde4e9fbbd8924805d4a02a1158", "authors": [{"name": "Shuangzhi Wu", "affiliations": []}, {"name": "Dongdong Zhang", "affiliations": []}, {"name": "Nan Yang", "affiliations": []}, {"name": "Mu Li", "affiliations": []}, {"name": "Ming Zhou", "affiliations": []}], "abstractText": "Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned. Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations. Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks.", "title": "Sequence-to-Dependency Neural Machine Translation"}