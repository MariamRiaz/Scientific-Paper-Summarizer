{"sections": [{"text": "\u21b5 (1 e \u21b5) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications."}, {"heading": "1. Introduction", "text": "Many important problems, such as experimental design and sparse modeling, are naturally formulated as a subset selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e.,\nmax S\u2713V,|S|K F (S), (P)\n1Department of Computer Science, ETH Zurich, Zurich, Switzerland. Correspondence to: Joachim M. Buhmann <jbuhmann@inf.ethz.ch>, Andreas Krause <krausea@ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nwhere V = {v1, . . . , vn} is the ground set. Specifically, in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized. This problem arises naturally in domains where performing experiments is costly. In sparse modeling, the task is to identify sparse representations of signals, enabling interpretability and robustness in high-dimensional statistical problems\u2014properties that are crucial in modern data analysis.\nFrequently, the standard GREEDY algorithm (Alg. 1) is used to (approximately) solve (P). For the case that F (S)\nAlgorithm 1: The GREEDY Algorithm Input: Ground set V , set function F : 2V!R+, budget K S\n0 ; for t = 1, . . . ,K do\nv \u21e4 argmax v2V\\St 1 F (S t 1 [ {v}) F (St 1) S\nt St 1 [ {v\u21e4} Output: SK\nis a monotone nondecreasing submodular set function1, the GREEDY algorithm enjoys the multiplicative approximation guarantee of (1 1/e) (Nemhauser et al., 1978; Vondra\u0301k, 2008; Krause & Golovin, 2014). This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornue\u0301jols, 1984; Vondra\u0301k, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.e., F (S) and F (S) are submodular).\nHowever, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold. In practice, however, the standard GREEDY algorithm often achieves very good performance on these applications, e.g., in subset selection with the R2 (squared multiple correlation) ob-\n1 F (\u00b7) is monotone nondecreasing if 8A \u2713 V, v 2 V , F (A [ {v}) F (A). F (\u00b7) is submodular iff it satisfies the diminishing returns property F (A [ {v}) F (A) F (B [ {v}) F (B) for all A \u2713 B \u2713 V \\{v}. Assume wlog. that F (\u00b7) is normalized, i.e., F (;) = 0.\njective (Das & Kempe, 2011). To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set function is to being submodular.\nAnother important class of non-submodular set functions comes as the auxiliary function when optimizing a continuous function f(x) s.t. combinatorial constraints, i.e., min\nx2C,supp(x)2I f(x), where supp(x) := {i | xi 6= 0} is the support set of x, C is a convex set, and I is the independent sets of the combinatorial structure. One of the most popular ways to solve this problem is to use the GREEDY algorithm to maximize the auxiliary function F (S) : = max\nx2C,supp(x)\u2713S f(x). This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016). Recently, Elenberg et al. (2016) proved that if f(x) has L-restricted smoothness and m-restricted strong convexity, then the submodularity ratio of F (S) is lower bounded by m/L. This result significantly enlarges the domain where the GREEDY algorithm can be applied.\nIn this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant factor approximation guarantees of the GREEDY algorithm. Our guarantees allow us to better characterize the empirical success of applying GREEDY on a significantly larger class of non-submodular functions. Furthermore, we bound these characteristics for important applications, rendering the usage of GREEDY a principled choice rather than a mere heuristic. Our main contributions are:\n- We prove the first tight constant-factor approximation guarantees for GREEDY on maximizing nonsubmodular nondecreasing set functions s.t. a cardinality constraint, characterized by a novel combination of the (generalized) notions of submodularity ratio and curvature \u21b5.\n- By theoretically bounding parameters ( ,\u21b5) for several important objectives, including Bayesian A-optimality in experimental design, the determinantal function of a square submatrix and maximization of LPs with combinatorial constraints, our theory implies the first guarantees for them.\n- Lastly, we experimentally validate our theory on several real-world applications. It is worth noting that for the Bayesian A-optimality objective, GREEDY generates comparable solutions as the classically used semidefinite programming (SDP) based method, but is usually two orders of magnitude faster.\nNotation. We use boldface letters, e.g., x, to represent vectors, and capital boldface letters, e.g., A, to denote matrices. x\ni is the ith entry of the vector x. We refer to V = {v1, ..., vn} as the ground set. We use f(\u00b7) to denote a continuous function, and F (\u00b7) to represent a set function. supp(x) := {i 2 V | x\ni 6= 0} is the support set of the vector x, and [n] := {1, ..., n} for an integer n 1. We denote the marginal gain of a set \u2326 \u2713 V in context of a set S \u2713 V as \u21e2\u2326(S) := F (\u2326 [ S) F (S). For v 2 V , we use the shorthand \u21e2\nv (S) for \u21e2{v}(S)."}, {"heading": "2. Submodularity Ratio and Curvature", "text": "In this section we provide the submodularity ratio and curvature for general, not necessarily submodular functions2, they are natural extensions of the classical ones. Let S 0 = ;, St = {j1, ..., jt}, t = 1, ...,K be the successive sets chosen by GREEDY. For brevity, let \u21e2 t : = \u21e2 jt(S t 1 ) be the marginal gain of GREEDY in step t.\nDefinition 1 (Submodularity ratio (Das & Kempe, 2011)). The submodularity ratio of a non-negative set function F (\u00b7) is the largest scalar s.t.\nX !2\u2326\\S \u21e2 ! (S) \u21e2\u2326(S), 8 \u2326, S \u2713 V.\nThe greedy submodularity ratio is the largest scalar G s.t. X\n!2\u2326\\St \u21e2 ! (S\nt ) G\u21e2\u2326(St), 8|\u2326|=K, t = 0, . . . ,K 1.\nIt is easy to see that G . The submodularity ratio measures to what extent F (\u00b7) has submodular properties. We make the following observations:\nRemark 1. For a nondecreasing function F (\u00b7), it holds a) ,\nG 2 [0, 1]; b) F (\u00b7) is submodular iff = 1. Definition 2 (Generalized curvature). The curvature of a non-negative function F (\u00b7) is the smallest scalar \u21b5 s.t.\n\u21e2\ni (S \\ {i} [ \u2326) (1 \u21b5)\u21e2 i (S \\ {i}), 8 \u2326, S \u2713 V, i 2 S\\\u2326.\nThe greedy curvature is the smallest scalar \u21b5G 0 s.t.\n\u21e2 ji(S i 1 [ \u2326) (1 \u21b5G)\u21e2 ji(S i 1 ),\n8 \u2326 : |\u2326| = K, i : j i 2 SK 1\\\u2326. 2Curvature is commonly defined for submodular functions. Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions. We show in Appendix C the details of these notions and the relations to ours. Additionally, we prove in Remark 3 of Appendix C.2 that our combination of curvature and submodularity ratio is more expressive than that of Sviridenko et al. (2013) in characterizing the maximization of problem (P) using standard GREEDY.\nWhen K = n or 1, SK 1\\\u2326 = ;, it is natural to define \u21b5 G\n= 0. It is easy to observe that \u21b5G  \u21b5. Note that the classical total curvature is \u21b5total := 1 min i2V \u21e2i(V\\{i})\n\u21e2i(;) .\nRemark 2. For a nondecreasing function F (\u00b7), it holds: a) \u21b5,\u21b5G 2 [0, 1]; b) F (\u00b7) is supermodular iff \u21b5 = 0; c) If F (\u00b7) is submodular, then \u21b5G  \u21b5 = \u21b5total.\nSo for a submodular function, our notion of curvature is consistent with \u21b5total. Notably, \u21b5G usually characterizes the problem better than \u21b5total, as will be validated in Section 5."}, {"heading": "3. Approximation Guarantee", "text": "We present approximation guarantee of GREEDY in Theorem 1. Note that both versions of the submodularity ratio and curvature apply in the proof. For brevity, we use and \u21b5 to refer to any of these versions in the sequel. In Section 3.3 we prove tightness of the approximation guarantees. All omitted proofs are given in Appendix B. Theorem 1. Let F (\u00b7) be a non-negative nondecreasing set function with submodularity ratio 2 [0, 1] and curvature \u21b5 2 [0, 1]. The GREEDY algorithm enjoys the following approximation guarantee for solving problem (P):\nF (S\nK ) 1 \u21b5\n\" 1 \u2713 K \u21b5\nK\n\u25c6 K # F (\u2326 \u21e4 )\n1 \u21b5\n(1 e \u21b5 )F (\u2326\u21e4), (1)\nwhere \u2326\u21e4 is the optimal solution of (P) and SK the output of the GREEDY algorithm.3"}, {"heading": "3.1. Interpreting Theorem 1", "text": "Before proving the theorem, we want to give the reader an intuition of the results and show how our results recover and extend several classical guarantees for the GREEDY algorithm. For the case \u21b5 = 0 (i.e., F (\u00b7) is supermodular), the approximation guarantee is lim\n\u21b5!0 1 \u21b5 (1 e \u21b5 ) = , which gives the first guarantee of greedily maximizing a nondecreasing supermodular function with bounded . When = 1, (i.e., F (\u00b7) is submodular), we recover the guarantee of \u21b5 1(1 e \u21b5) (Conforti & Cornue\u0301jols, 1984). For the case \u21b5 = 1, we have a guarantee of (1 e ) (Das & Kempe, 2011). For the case \u21b5 = 1, = 1, we recover the classical guarantee of (1 1/e) (Nemhauser et al., 1978). We plot the constant-factor approximation guarantees for different values of and \u21b5 in Fig. 1. One interesting phenomenon is that and \u21b5 play different roles: Looking at = 0, the approximation factor is always 0, independent of the value \u21b5 takes. In contrast, for \u21b5 = 0, the\n3For the setting that GREEDY is allowed to pick more than K elements, e.g., pick K0 > K elements, our theory can be easily extended to show that F (SK 0 ) \u21b5 1(1 e \u21b5 K 0/K)F (\u2326\u21e4).\napproximation guarantee is (1 e ). This can be interpreted as the curvature boosting the guarantees."}, {"heading": "3.2. Proof of Theorem 1", "text": "The high-level proof framework is based on Conforti & Cornue\u0301jols (1984) (where they derive the approximation guarantee for maximizing a nondecreasing submodular function with bounded curvature). However, adapting the proof to non-submodular functions requires several changes detailed in Section 6.\nProof overview. Let us denote all problem instances of maximizing a non-negative nondecreasing function F (\u00b7) s.t. K-cardinality constraint (max|S|K F (S)) to be P K,\u21b5,\n, where F (\u00b7) is parametrized by submodularity ratio and curvature \u21b5. Let P\u2326\u21e4,SK 2 PK,\u21b5, denote those problem instances with optimal solution \u2326\u21e4 and greedy solution SK . We group all problem instances P\nK,\u21b5, according to the set \u2326\u21e4 \\ SK := {l1 = jm1 , l2 = jm2 , . . . , ls = j\nms}, where jm1 , . . . , jms are consistent with the order of greedy selection. Let us denote the problem instances with \u2326 \u21e4\\SK = {l1, . . . , ls} as the group PK,\u21b5, ({l1, . . . , ls}).\nThe main idea of the proof is to investigate the worst-case approximation ratio of each group of the problem instances P K,\u21b5,\n({l1, . . . , ls}), 8{l1, . . . , ls} \u2713 SK . We do this by constructing LPs based on the properties of the problem instances. By studying the structures of these LPs, we will prove that the worst-case approximation ratio of all problem instances occurs when \u2326\u21e4 \\SK = ;. Thus the desired approximation guarantee corresponds to the worst-case approximation ratio of P\nK,\u21b5,\n(;).\nThe proof. When = 0 or F (\u2326\u21e4) = 0, (1) holds naturally. In the following, let 2 (0, 1] and F (\u2326\u21e4) > 0. First, we present Lemma 1, which will be used to construct the LPs.\nLemma 1. For any \u2326 \u2713 V with |\u2326| = K and any t 2 {0, . . . ,K 1}, let wt := |St \\ \u2326|. It holds that\n\u21b5\nX\ni:ji2St\\\u2326\n\u21e2\ni\n+\nX\ni:ji2St\\\u2326\n\u21e2\ni\n+ 1 (K wt)\u21e2\nt+1 F (\u2326).\nWe now specify the constructing of the LPs: For any problem instance P\u2326\u21e4,SK 2 PK,\u21b5, ({l1, . . . , ls}), we know that F (SK) = P K\ni=1 \u21e2i (telescoping sum). Hence, the approximation ratio is F (S\nK) F (\u2326\u21e4) = P i \u21e2i\nF (\u2326\u21e4) , which we denote as R({l1, . . . , ls}) = P i \u21e2i F (\u2326\u21e4) . Define xi := \u21e2i\nF (\u2326\u21e4) , i 2 [K]. Since F is nondecreasing, x\ni 0. Plugging \u2326 = \u2326\u21e4 into Lemma 1, and considering t = 0, . . . ,K 1, we have in total K constraints over the variables x\ni , which constitute the constraints of the LP. So the worst-case approximation ratio of the group P\nK,\u21b5, ({l1, . . . , ls}) is:\nR({l1, . . . , ls}) = min X K\ni=1 x i\n, s.t. x i 0 and,\nrow (0) row (1)\n... row (l1 1) row (l2 1) row (q = lr) ... row (ls 1) ... row (K 1)\n2\n66666666666666666664\nK\n\u21b5\nK\n... ... . . . \u21b5 \u21b5 \u00b7 \u00b7 \u00b7 K 0 \u21b5 \u21b5 \u00b7 \u00b7 \u00b7 1 K 1 \u21b5 \u21b5 \u00b7 \u00b7 \u00b7 1 1 K r ... ... ... ... ...\n. . . \u21b5 \u21b5 \u00b7 \u00b7 \u00b7 1 1 \u21b5 \u00b7 \u00b7 \u00b7 K s+1 ... ... ... ... ... ... . . . \u21b5 \u21b5 \u00b7 \u00b7 \u00b7 1 1 \u21b5 \u00b7 \u00b7 \u00b7 1 \u00b7 \u00b7 \u00b7 K s\n3\n77777777777777777775 \u00b7\n2\n66666666666666664\nx1\nx2\n... xl1 xl2 xq+1\n... xls\n... xK\n3\n77777777777777775\n2\n66666666666666664 1 1 ... 1 1 1 ... 1 ... 1\n3\n77777777777777775\n(2)\nThe following Lemma presents the key structure of the constructed LPs, which will be used to deduce the relation between the LPs of different problem instance groups. Lemma 2. Assume that the optimal solution of the constructed LP is x\u21e4 2 RK+ and that s = |\u2326\u21e4 \\ SK | 1. For all 1  r  s it holds that x\u21e4\nq  x\u21e4 q+1, where q = lr.\nProof sketch of Lemma 2. Assume by virture of creating a contradiction that x\u21e4\nq\n> x \u21e4 q+1. We can always create a\nnew feasible solution y\u21e4 2 RK+ by decreasing x\u21e4q by some \u270f > 0, while increasing all the x\u21e4\nq+1 to x\u21e4 K by some proper values, s.t. y\u21e4 has smaller LP objective value. Specifically, we define y\u21e4 as: for k = 1, . . . , q 1, y\u21e4\nk\n:= x \u21e4 k ; y\n\u21e4 q := x \u21e4 q \u270f; for k = q + 1, . . . ,K, y\u21e4 k := x \u21e4 k + \u270f k where \u270f\nk s are defined recursively as: \u270f q+1 = \u270f K r , and\n\u270f q+1+u = \u270fq+u K r u+ 1 K r u , 1  u  K q 1.\nClaim 1. a) The new solution y\u21e4 0; b) All of the constraints in (2) are still feasible for y\u21e4.\nAfter that the change of the LP objective is,\nLP = \u270f+ \u270f q+1 + \u270fq+2 + . . .+ \u270fK .\nOne can prove that the LP objective decreases:\nClaim 2. For all K 1, 1  r  q < K, it holds that\nLP  0, 8 2 (0, 1]. Equality is achieved when r = q and = 1. Therefore we reach the contradiction that x\u21e4 is an optimal solution of the constructed LP.\nGiven Lemma 2, we prove in the following Lemma, which states that the worst-case approximation ratio of all problem instances occurs when \u2326\u21e4 \\ SK = ;. Lemma 3. For all {l1, . . . , ls} \u2713 SK , it holds that\nR({l1, . . . , ls}) R(;) = 1 \u21b5\n 1 \u21e3 K \u21b5\nK\n\u2318 K\n.\nSo the greedy solution has objective F (SK) 1 \u21b5  1 \u21e3 K \u21b5 K \u2318 K F (\u2326 \u21e4 ) 1 \u21b5 (1 e \u21b5 )F (\u2326\u21e4).\n3.3. Tightness Result\nWe demonstrate that the approximation guarantee in Theorem 1 is tight, i.e., for every submodularity ratio and every curvature \u21b5, there exist set functions that achieve the bound exactly.\nAssume the ground set V contains the elements in S :=\n{j1, . . . , jK} and the elements in \u2326 := {!1, . . . ,!K} (S \\ \u2326 = ;) and n 2K dummy elements. The objective function we are going to construct will not depend on these dummy elements, i.e., the objective value of a set does not change if dummy elements are removed from or added to that set. Consequently, the dummy elements will not affect the submodularity ratio and the curvature. For the constants \u21b5 2 [0, 1], 2 (0, 1], we define the objective function as,\nF (T ) := f(|\u2326 \\ T |)\nK\n1 \u21b5\nX\ni:ji2S\\T\n\u21e0i\n+\nX\ni:ji2S\\T\n\u21e0i, (3)\nwhere \u21e0 i : = 1 K\n\u21e3 K \u21b5\nK\n\u2318 i 1\n, i 2 [K]; f(x) = 1 1 K 1 x 2 +\nK 1 K 1 x. Note that f(x) is convex nondecreasing over [0,K], and that f(0) = 0, f(1) = 1, f(K) = K/ . It is clear that F (;) = 0 and F (\u00b7) is monotone nondecreasing. The following lemma shows that it is generally nonsubmodular and non-supermodular.\nLemma 4. For the objective in (3): a) When \u21b5 = 0, it is supermodular; b) When = 1, it is submodular; c) F (T ) has submodularity ratio and curvature \u21b5.\nConsidering the problem of max|T |K F (T ), we claim that the GREEDY algorithm may output S. This can be proved by induction. One can see that \u21e2\nj1(;) = \u21e01 = \u21e2\n!1(;), so GREEDY can choose j1 in the first step. Assume in step t 1 GREEDY has chosen St 1 = {j1, . . . , jt 1}, one can verify that the marginal gains coincide, i.e., \u21e2\njt(S t 1 ) = \u21e0 t = \u21e2 !t(S t 1 ). However, the optimal solution is actually \u2326 with function value as F (\u2326) = 1 . So the\napproximation ratio is F (S) F (\u2326) = 1 \u21b5\n 1 \u21e3 K \u21b5\nK\n\u2318 K\n, which\nmatches our approximation guarantee in Theorem 1."}, {"heading": "4. Applications", "text": "We consider several important real-world applications and their corresponding objective functions. We show that the submodularity ratio and the curvature of these functions can be bounded and, hence, the approximation guarantees from our theoretical results are applicable. All the omitted proofs are provided in Appendix D."}, {"heading": "4.1. Bayesian A-optimality in Experimental Design", "text": "In Bayesian experimental design (Chaloner & Verdinelli, 1995), the goal is to select a set of experiments to perform s.t. some statistical criterion is optimized, e.g., the variance of certain parameter estimates is minimized. Krause et al. (2008) investigated several criteria for this purpose, amongst others the Bayesian A-optimality criterion. This criterion is used to maximally reduce the variance in the posterior distribution over the parameters. In general, the criterion is not submodular as shown in Krause et al. (2008, Section 8.4).\nFormally, assume there are n experimental stimuli {x1, . . . ,xn}, each xi 2 Rd, which constitute the data matrix X 2 Rd\u21e5n. Let us arrange a set S \u2713 V of stimuli as a matrix X\nS\n:\n= [x v1 , . . . ,xvs ] 2 Rd\u21e5|S|. Let \u2713 2 Rd be the parameter vector in the linear model y\nS = X> S \u2713 +w, where w is the Gaussian noise with zero mean and variance\n2, i.e., w \u21e0 N (0, 2I), and y S is the vector of dependent variables. Suppose the prior takes the form of an isotropic Gaussian, i.e., \u2713 \u21e0 N (0,\u21e4 1),\u21e4 = 2I. Then,  y\nS\n\u2713\n\u21e0 N (0,\u2303),\u2303 =\n\n2I+X> S \u21e4 1X S X> S \u21e4 1\n\u21e4 1X S\n\u21e4 1\n.\nThis implies that \u2303 \u2713|yS = (\u21e4 + 2X S X> S ) 1. The Aoptimality objective is defined as,\nF\nA\n(S)\n: = tr(\u2303 \u2713 ) tr(\u2303 \u2713|yS ) (4) = tr(\u21e4 1) tr((\u21e4+ 2X S X> S ) 1 ).\nThe following Proposition gives bounds on the submodularity ratio and curvature of (4).\nProposition 1. Assume normalized stimuli, i.e., kx i k = 1, 8i 2 V . Let the spectral norm of X be kXk.4 Then, a) The objective in (4) is monotone nondecreasing. b) Its submodularity ratio can be lower bounded by 2\nkXk2( 2+ 2kXk2) , and its curvature \u21b5 can be upper\nbounded by 1 2\nkXk2( 2+ 2kXk2) ."}, {"heading": "4.2. The Determinantal Function", "text": "The determinantal function of a square submatrix is widely used in many areas, e.g., in determinantal point processes (Kulesza & Taskar, 2012) and active set selection for sparse Gaussian processes. Monotone nondecreasing determinantal functions appear in the second problem. Assume \u2303 is the covariance matrix parameterized by a positive definite kernel. In the Informative Vector Machine (Lawrence et al., 2003), the information gain of a subset of points S \u2713 V is 1 2 logF (S), where\nF (S)\n: = det(I+ 2\u2303 S ), (5)\nwhere is the noise variance in the Gaussian process model, \u2303\nS is the square submatrix with both its rows and columns indexed by S. Although logF (S) is submodular, F (S) is in general not submodular. The approximation guarantee of GREEDY for maximizing logF (S) does not translate to a guarantee for maximizing F (S). The following Proposition characterizes (5).\nProposition 2. a) F (S) in (5) is supermodular, its curvature is 0; b) Let the eigenvalues of A := I + 2\u2303 be 1 \u00b7 \u00b7 \u00b7 n > 1. The greedy submodularity ratio of F (S) can be lower bounded by K( n 1)\n( QK j=1 j) 1 ."}, {"heading": "4.3. LPs with Combinatorial Constraints", "text": "LPs with combinatorial constraints appear frequently in practice. Consider the following example: Suppose that V is the set of all products a company can produce. Given budget constraints on the raw materials needed, companies consider the LP max\nx2Phd,xi, where d is the vector of profits for the individual products and where P is a polytope representing the continuous constraints. The above LP can be used to assess the profit maximizing production plan. Usually the company needs to consider combinatorial constraints as well. For instance, the company has at most K production lines, thus they have to select a subset of K products to produce. Often this kind of problems can be formalized as max\nx2P,supp(x)2Ihd,xi, where I is the independent set of the combinatorial structure. Hence, a natural auxiliary set function is,\nF (S) := maxsupp(x)\u2713S, x2Phd,xi, 8S \u2713 V. (6) 4By Weyl\u2019s inequality, a naive upper bound is kXk  p n.\nLet P = {x 2 Rn | 0  x  \u00afu,Ax  b, \u00afu 2 Rn+,A 2 Rm\u21e5n+ , b 2 Rm+}. In general F (S) in (6) is non-submodular as illustrated by two examples in Appendix D.3. Upper bounding the curvature is equivalent to lower bounding F (S[\u2326) F (S\\{i}[\u2326)\nF (S) F (S\\{i}) , which can be 0 in the worst case. However, the submodularity ratio can be lower bounded by a non-zero scalar.\nProposition 3. a) F (S) in (6) is a normalized nondecreasing set function. b) With regular non-degenerancy assumptions (details in Appendix D.3.2), its submodularity ratio can be lower bounded by 0 > 0."}, {"heading": "4.4. More Applications", "text": "Many real-world applications can benefit from the theory in this work, for instance: subset selection using the R2 objective, sparse modeling and the budget allocation problem with combinatorial constraints. Details on these applications are deferred to Appendix G."}, {"heading": "5. Experimental Results", "text": "We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for several applications. Since it is too time consuming to calculate the full versions of \u21b5 and using exhaustive search, we only calculated the greedy versions (\u21b5G, G). All averaged results are from 20 repeated experiments. Source code is available at https://github.com/bianan/ non-submodular-max.5 More results are put in Appendix H."}, {"heading": "5.1. Bayesian Experimental Design", "text": "We considered the Bayesian A-optimality objective for both synthetic and real-world data. In all experiments, we normalized the data points to have unit `2-norm.\nReal-world results: We used the Boston Housing Data. 5All experiments were implemented using Matlab. We used the SDP solver provided by CVX (Version 2.1).\nThe dataset6 has 14 features (e.g., crime rate, property tax rates, etc.) and 516 samples. To be able to quickly calculate the parameters and optimal solution by exhaustive search, the first n = 14 samples were used. As a baseline, we used an SDP-based algorithm (abbreviated as SDP, details are available in Appendix E). Results are shown in Fig. 2 for varying values of K. In Fig. 2a we can observe that both GREEDY and SDP compute near-optimal solutions. From Fig. 2b we can see that the greedy submodularity ratio G is close to 1, and that the greedy curvature \u21b5G is less than 1, while the classical curvature \u21b5total is always 1 (the worstcase value). This implies that the classical total curvature \u21b5\ntotal characterizes the considered maximization problems less accurate than the greedy curvature.\nSynthetic results: We generated random observations from a multivariate Gaussian distribution with different correlations. To be able to assess the ground truth, we used n = 12 samples with d = 6 features. Fig. 3 shows the results with correlation 0.2 (first column) and 0.6 (second column), respectively: The first row shows the average objective values over the optimal value with error bars, and the second row shows the parameters. One can observe that GREEDY always obtains near-optimal solutions and that these solutions are roughly comparable with those obtained by the SDP. The classical curvature \u21b5total is always close to 1, while \u21b5G take smaller values, and G takes values close to 1, thus characterize the performance of GREEDY better.\nMedium-scale synthetic experiments: To compare the runtime of SDP and GREEDY, we considered mediumscale datasets (we cannot report results on larger datasets because of the huge computational demands of the SDP).\n6https://archive.ics.uci.edu/ml/datasets/ Housing\nFig. 4 shows the objective value achieved by GREEDY and SDP for different numbers of features d and numbers of samples n, as well as the correlations. We can observe that GREEDY computes solutions that are on par or superior to those of SDP. In Table 1 we summarize the runtime of GREEDY and SDP for different values of d and n, for correlation 0.5. Furthermore, we show the ratio of runtimes of the two algorithms. We can observe that GREEDY is usually two orders of magnitude faster than SDP."}, {"heading": "5.2. LPs with Combinatorial Constraints", "text": "We generated synthetic LPs as follows: Firstly, we generated the matrix A 2 Rm\u21e5n+ , Aij 2 [0, 1] by drawing all entries independently from a uniform distribution on\n[0, 1]. We set b = d = 1, and set \u00afu as 1. The first row of Fig. 5 plots the optimal LP objective (calculated using exhaustive search) and the LP objective returned by GREEDY. The second row shows the curvature and submodularity ratio. The first column (Fig. 5a) presents the results for n = 6,m = 20, while the second column (Fig. 5b) presents that for n = 8,m = 30. Note the greedy submodularity ratio takes values between \u21e0 0.15 and 1, and that the curvature is close to the worst-case value of 1. These observations are consistent with the theory in Section 4.3."}, {"heading": "5.3. Determinantal Functions Maximization", "text": "We experimented with synthetic and real-world data: For synthetic data, we generated random covariance matrices \u2303 2 Rn\u21e5n with uniformly distributed eigenvalues in [0, 1]. We set n = 10, = 2. In Fig. 6 (left) we plot the optimal determinantal objective value and the value achieved by GREEDY. Fig. 6 (right) traces the greedy submodularity ratio G. Since the determinantal objective is supermodular, so the approximation guarantee equals to G. We can see that G can reasonably predict the performance of GREEDY.\nFor real-world data, we considered an active set selection task on the CIFAR-107 dataset. The first n = 12 images in the test set were used to calculate the covariance matrix with an squared exponential kernel (k(x\ni\n,x\nj\n) =\nexp( kx i x j k2/h2), h was set to be 1). The results in Fig. 7 shows similar results as with the synthetic data.\n7https://www.cs.toronto.edu/\u02dckriz/cifar. html"}, {"heading": "6. Related Work", "text": "In this section we briefly discuss related work on various notions of non-submodularity and the optimization of nonsubmodular functions (Further details in Appendix F).\nRelation to Conforti & Cornue\u0301jols (1984) in deriving approximation guarantees. In proving Theorem 1 we use the similar proof framework (i.e., utilizing LP formulations to analyze the worst-case approximation ratios of different groups of problem instances) as that in Conforti & Cornue\u0301jols (1984), where they derive guarantees for maximizing submodular functions. However, since we are proving guarantees for non-submodular functions, the specific techniques on how to manipulate these LPs are different. Specifically, 1) The building block to construct LPs (Lemma 1) is different; 2) The technique to prove the structure of the LPs (which corresponds to Lemma 2) is significantly different for a submodular function and a nonsubmodular function, and Lemma 2 is the key to investigate the worst-case approximation ratios of different groups of problem instances. 3) The specific way to prove Lemma 3 is also different since the constraints of the LPs are different for submodular and non-submodular functions.\nSubmodularity ratio and curvature. Curvature is typically defined for submodular functions. Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions. Appendix C provides details of that notion and relates it to our definition. Yoshida (2016) prove an improved approximation ratio for knapsack-constrained maximization of submodular functions with bounded curvature. Submodularity ratio (Das & Kempe, 2011) is a quantity characterizing how close a function is to being submodular.\nApproximate submodularity. Krause et al. (2008) define approximately submodular functions with parameter \u270f 0 as those functions F that satisfy an approximate diminishing returns property, i.e., 8A \u2713 B \u2713 V \\ v it holds that \u21e2\nv (A) \u21e2 v (B) \u270f. GREEDY yields a solution with objective F (SK) (1 e 1)F (\u2326\u21e4) K\u270f, for maximizing a monotone F s.t. a K-cardinality constraint. Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity. Restricted submodularity refers to functions which are submodular only over some collection of subsets of V , and shifted submodularity can be viewed as a special case of the approximate diminishing returns as defined above. Recently, Horel & Singer (2016) study \u270f-approximately submodular functions, which arised from their research on \u201cnoisy\u201d submodular functions. A function F (\u00b7) is \u270f-approximately submodular if there exists a submodular function G s.t. (1 \u270f)G(S)  F (S)  (1 + \u270f)G(S), 8S \u2713 V .\nWeak submodularity. Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (\u00b7) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) |S \\T |F (S [T )+ |S [T |F (S \\T ). For a function F (\u00b7), we show in Remark 4 that the following two facts do not imply each other: i) F (\u00b7) is weakly submodular; ii) The submodularity ratio of F (\u00b7) is strictly larger than 0, and its curvature is strictly smaller than 1.\nOther notions of non-submodularity. Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions. They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree. Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.\nOptimization of non-submodular functions. The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012). Jegelka & Bilmes (2011) present the problem of minimizing \u201ccooperative cuts\u201d, which are non-submodular in general, and propose efficient algorithms for optimization. Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function. Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation factor."}, {"heading": "7. Conclusion", "text": "We analyzed the guarantees for greedy maximization of non-submodular nondecreasing set functions. By combining the (generalized) curvature \u21b5 and submodularity ratio for generic set functions, we prove the first tight approximation bounds in terms of these definitions for greedily maximizing nondecreasing set functions. These approximation bounds significantly enlarge the domain where GREEDY has guarantees. Furthermore, we theoretically bounded the parameters \u21b5 and for several non-trivial applications, and validate our theory in various experiments."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to thank Adish Singla, Kfir Y. Levy and Aurelien Lucchi for valuable discussions. This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing."}], "year": 2017, "references": [{"title": "Greedy column subset selection: New bounds and distributed algorithms", "authors": ["Altschuler", "Jason", "Bhaskara", "Aditya", "Fu", "Gang", "Mirrokni", "Vahab", "Rostamizadeh", "Afshin", "Zadimoghaddam", "Morteza"], "venue": "In ICML,", "year": 2016}, {"title": "Algorithms for optimizing the ratio of submodular functions", "authors": ["Bai", "Wenruo", "Iyer", "Rishabh", "Wei", "Kai", "Bilmes", "Jeff"], "venue": "In ICML,", "year": 2016}, {"title": "Introduction to Linear Optimization", "authors": ["Bertsimas", "Dimitris", "Tsitsiklis", "John"], "venue": "Athena Scientific,", "year": 1997}, {"title": "Guaranteed nonconvex optimization: Submodular maximization over continuous domains", "authors": ["Bian", "Andrew An", "Mirzasoleiman", "Baharan", "Buhmann", "Joachim M", "Krause", "Andreas"], "venue": "In AISTATS,", "year": 2017}, {"title": "Weakly submodular functions", "authors": ["Borodin", "Allan", "Le", "Dai Tri Man", "Ye", "Yuli"], "venue": "arXiv preprint arXiv:1401.6697,", "year": 2014}, {"title": "Submodular maximization with cardinality constraints", "authors": ["Buchbinder", "Niv", "Feldman", "Moran", "Naor", "Joseph", "Schwartz", "Roy"], "venue": "In SODA, pp", "year": 2014}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "authors": ["Candes", "Emmanuel J", "Romberg", "Justin K", "Tao", "Terence"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2006}, {"title": "Bayesian experimental design: A review", "authors": ["Chaloner", "Kathryn", "Verdinelli", "Isabella"], "venue": "Statistical Science,", "year": 1995}, {"title": "Submodular set functions, matroids and the greedy algorithm: tight worst-case bounds and some generalizations of the radoedmonds theorem", "authors": ["Conforti", "Michele", "Cornu\u00e9jols", "G\u00e9rard"], "venue": "Discrete Applied Mathematics,", "year": 1984}, {"title": "Algorithms for subset selection in linear regression", "authors": ["Das", "Abhimanyu", "Kempe", "David"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "year": 2008}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "authors": ["Das", "Abhimanyu", "Kempe", "David"], "venue": "In ICML, pp", "year": 2011}, {"title": "Analysis of greedy approximations with nonsubmodular potential functions", "authors": ["Du", "Ding-Zhu", "Graham", "Ronald L", "Pardalos", "Panos M", "Wan", "Peng-Jun", "Wu", "Weili", "Zhao", "Wenbo"], "venue": "In SODA, pp", "year": 2008}, {"title": "Restricted strong convexity implies weak submodularity", "authors": ["Elenberg", "Ethan R", "Khanna", "Rajiv", "Dimakis", "Alexandros G", "Negahban", "Sahand"], "venue": "arXiv preprint arXiv:1612.00804,", "year": 2016}, {"title": "Welfare maximization and the supermodular degree", "authors": ["Feige", "Uriel", "Izsak", "Rani"], "venue": "In Proceedings of the Fourth Conference on Innovations in Theoretical Computer Science,", "year": 2013}, {"title": "An introduction to variable and feature selection", "authors": ["Guyon", "Isabelle", "Elisseeff", "Andr\u00e9"], "venue": "Journal of machine learning research,", "year": 2003}, {"title": "Maximization of approximately submodular functions", "authors": ["Horel", "Thibaut", "Singer", "Yaron"], "venue": "In NIPS,", "year": 2016}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "authors": ["Iyer", "Rishabh", "Bilmes", "Jeff"], "venue": "In Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,", "year": 2012}, {"title": "Curvature and optimal algorithms for learning and minimizing submodular functions", "authors": ["Iyer", "Rishabh K", "Jegelka", "Stefanie", "Bilmes", "Jeff A"], "year": 2013}, {"title": "On iterative hard thresholding methods for high-dimensional m-estimation", "authors": ["Jain", "Prateek", "Tewari", "Ambuj", "Kar", "Purushottam"], "venue": "In NIPS, pp", "year": 2014}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "authors": ["Jegelka", "Stefanie", "Bilmes", "Jeff"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "year": 1897}, {"title": "On approximate non-submodular minimization via treestructured supermodularity", "authors": ["Kawahara", "Yoshinobu", "Iyer", "Rishabh K", "Bilmes", "Jeff A"], "venue": "In AISTATS, pp", "year": 2015}, {"title": "Submodular dictionary selection for sparse representation", "authors": ["Krause", "Andreas", "Cevher", "Volkan"], "venue": "In ICML, pp", "year": 2010}, {"title": "Submodular function maximization. In Tractability: Practical Approaches to Hard Problems", "authors": ["Krause", "Andreas", "Golovin", "Daniel"], "year": 2014}, {"title": "Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies", "authors": ["Krause", "Andreas", "Singh", "Ajit", "Guestrin", "Carlos"], "venue": "Journal of Machine Learning Research,", "year": 2008}, {"title": "Fast sparse gaussian process methods: The informative vector machine", "authors": ["Lawrence", "Neil", "Seeger", "Matthias", "Herbrich", "Ralf"], "venue": "NIPS, pp", "year": 2003}, {"title": "A submodularsupermodular procedure with applications to discriminative structure learning", "authors": ["Narasimhan", "Mukund", "Bilmes", "Jeff"], "venue": "In Proceedings of the TwentyFirst Conference on Uncertainty in Artificial Intelligence,", "year": 2005}, {"title": "An analysis of approximations for maximizing submodular set functions\u2013i", "authors": ["Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L"], "venue": "Mathematical Programming,", "year": 1978}, {"title": "Optimal budget allocation: Theoretical guarantee and efficient algorithm", "authors": ["Soma", "Tasuku", "Kakimura", "Naonori", "Inaba", "Kazuhiro", "Kawarabayashi", "Ken-ichi"], "venue": "In ICML, pp", "year": 2014}, {"title": "Optimal approximation for submodular and supermodular optimization with bounded curvature", "authors": ["Sviridenko", "Maxim", "Vondr\u00e1k", "Jan", "Ward", "Justin"], "venue": "arXiv preprint arXiv:1311.4728,", "year": 2013}, {"title": "Optimal approximation for the submodular welfare problem in the value oracle model", "authors": ["Vondr\u00e1k", "Jan"], "venue": "In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing,", "year": 2008}, {"title": "Submodularity and curvature: the optimal algorithm", "authors": ["Vondr\u00e1k", "Jan"], "venue": "RIMS Kokyuroku Bessatsu B,", "year": 2010}, {"title": "Maximizing a monotone submodular function with a bounded curvature under a knapsack constraint", "authors": ["Yoshida", "Yuichi"], "venue": "arXiv preprint arXiv:1607.04527,", "year": 2016}, {"title": "Causal meets submodular: Subset selection with directed information", "authors": ["Zhou", "Yuxun", "Spanos", "Costas J"], "venue": "In NIPS,", "year": 2016}], "id": "SP:8833347924da4de06b83873c013b9f48585e29cd", "authors": [{"name": "Andrew An Bian", "affiliations": []}, {"name": "Joachim M. Buhmann", "affiliations": []}, {"name": "Andreas Krause", "affiliations": []}, {"name": "Sebastian Tschiatschek", "affiliations": []}], "abstractText": "We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones. However, GREEDY enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design. We prove theoretical guarantees supporting the empirical performance. Our guarantees are characterized by a combination of the (generalized) curvature \u21b5 and the submodularity ratio . In particular, we prove that GREEDY enjoys a tight approximation guarantee of 1 \u21b5 (1 e \u21b5) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications.", "title": "Guarantees for Greedy Maximization of Non-submodular Functions with Applications"}