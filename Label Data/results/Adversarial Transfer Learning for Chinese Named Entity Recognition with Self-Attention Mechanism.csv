0,1,label2,summary_sentences
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1–10 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1001",text,[0],[0]
Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks.,1 Introduction,[0],[0]
"Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks.
",1 Introduction,[0],[0]
"However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of
some components should be shared.",1 Introduction,[0],[0]
"As shown in Figure 1-(a), the general shared-private model introduces two feature spaces for any task: one is used to store task-dependent features, the other is used to capture shared features.",1 Introduction,[0],[0]
"The major limitation of this framework is that the shared feature space could contain some unnecessary taskspecific features, while some sharable features could also be mixed in private space, suffering from feature redundancy.
",1 Introduction,[0],[0]
"Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews.
",1 Introduction,[0],[0]
The infantile cart is simple and easy to use.,1 Introduction,[0],[0]
"This kind of humour is infantile and boring.
",1 Introduction,[0],[0]
The word “infantile” indicates negative sentiment in Movie task while it is neutral in Baby task.,1 Introduction,[0],[0]
"However, the general shared-private model could place the task-specific word “infantile” in a shared space, leaving potential hazards for other tasks.",1 Introduction,[0],[0]
"Additionally, the capacity of shared space could also be wasted by some unnecessary features.
",1 Introduction,[0],[0]
"To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are in-
1
herently disjoint by introducing orthogonality constraints.",1 Introduction,[0],[0]
"Specifically, we design a generic sharedprivate learning framework to model the text sequence.",1 Introduction,[0],[0]
"To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints.",1 Introduction,[0],[0]
"The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.
",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows.
1.",1 Introduction,[0],[0]
"Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters.",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data.",1 Introduction,[0],[0]
3.,1 Introduction,[0],[0]
"We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks.",1 Introduction,[0],[0]
"There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013).",2 Recurrent Models for Text Classification,[0],[0]
"Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017).
",2 Recurrent Models for Text Classification,[0],[0]
"Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.",2 Recurrent Models for Text Classification,[0],[0]
"While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.
",2 Recurrent Models for Text Classification,[0],[0]
"We define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a
forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units.",2 Recurrent Models for Text Classification,[0],[0]
"The elements of the gating vectors it, ft and ot are in [0, 1].
",2 Recurrent Models for Text Classification,[0],[0]
"The LSTM is precisely specified as follows.
 
",2 Recurrent Models for Text Classification,[0],[0]
"c̃t ot it ft
  =  
tanh σ σ",2 Recurrent Models for Text Classification,[0],[0]
"σ
  ( Wp [ xt
ht−1
] + bp ) , (1)
ct = c̃t it + ct−1 ft, (2) ht = ot tanh (ct) , (3)
",2 Recurrent Models for Text Classification,[0],[0]
where xt ∈,2 Recurrent Models for Text Classification,[0],[0]
Re is the input at the current time step;,2 Recurrent Models for Text Classification,[0],[0]
"Wp ∈ R4d×(d+e) and bp ∈ R4d are parameters of affine transformation; σ denotes the logistic sigmoid function and denotes elementwise multiplication.
",2 Recurrent Models for Text Classification,[0],[0]
"The update of each LSTM unit can be written precisely as follows:
ht = LSTM(ht−1,xt, θp).",2 Recurrent Models for Text Classification,[0],[0]
"(4)
Here, the function LSTM(·, ·, ·, ·) is a shorthand for Eq.",2 Recurrent Models for Text Classification,[0],[0]
"(1-3), and θp represents all the parameters of LSTM.
",2 Recurrent Models for Text Classification,[0],[0]
"Text Classification with LSTM Given a text sequence x = {x1, x2, · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi.",2 Recurrent Models for Text Classification,[0],[0]
"The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.
",2 Recurrent Models for Text Classification,[0],[0]
"ŷ = softmax(WhT + b) (5)
where ŷ is prediction probabilities, W is the weight which needs to be learned, b is a bias term.
",2 Recurrent Models for Text Classification,[0],[0]
"Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.
",2 Recurrent Models for Text Classification,[0],[0]
"L(ŷ, y) =",2 Recurrent Models for Text Classification,[0],[0]
"− N∑
i=1
C∑
j=1
yji log(ŷ j i ), (6)
where yji is the ground-truth label; ŷ",2 Recurrent Models for Text Classification,[0],[0]
"j i is prediction probabilities, and C is the class number.
",2 Recurrent Models for Text Classification,[0],[0]
"softmax Lmtask
LSTM
softmax Lntask
xm xn
(a) Fully Shared Model (FS-MTL)",2 Recurrent Models for Text Classification,[0],[0]
The goal of multi-task learning is to utilizes the correlation among these related tasks to improve classification by learning tasks in parallel.,3 Multi-task Learning for Text Classification,[0],[0]
"To facilitate this, we give some explanation for notations used in this paper.",3 Multi-task Learning for Text Classification,[0],[0]
"Formally, we refer to Dk as a dataset with Nk samples for task k. Specifically,
",3 Multi-task Learning for Text Classification,[0],[0]
"Dk = {(xki , yki )}Nki=1 (7)
where xki and y k",3 Multi-task Learning for Text Classification,[0],[0]
i denote a sentence and corresponding label for task k.,3 Multi-task Learning for Text Classification,[0],[0]
The key factor of multi-task learning is the sharing scheme in latent feature space.,3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"In neural network based model, the latent features can be regarded as the states of hidden neurons.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Therefore, the sharing schemes are different in how to group the shared features.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme.
",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
Fully-Shared Model (FS-MTL),3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"For example, given two tasks m and n, it takes the view that the features of task m can be totally shared by task n and vice versa.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
This model ignores the fact that some features are task-dependent.,3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Figure 2a illustrates the fully-shared model.
",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
Shared-Private Model (SP-MTL),3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"As shown in Figure 2b, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used to capture task-invariant features.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer.",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"Formally, for any sentence in task k, we can compute its shared representation skt and task-specific representation h k t as follows:
skt = LSTM(xt, s k t−1, θs), (8)
hkt = LSTM(xt,h m t−1, θk) (9)
where LSTM(., θ) is defined as Eq. (4).",3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
The final features are concatenation of the features from private space and shared space.,3.1 Two Sharing Schemes for Sentence Modeling,[0],[0]
"For a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks.
",3.2 Task-Specific Output Layer,[0],[0]
The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks.,3.2 Task-Specific Output Layer,[0],[0]
"The loss Ltask can be computed as:
LTask = K∑
k=1
αkL(ŷ (k), y(k))",3.2 Task-Specific Output Layer,[0],[0]
"(10)
where αk is the weights for each task k respectively.",3.2 Task-Specific Output Layer,[0],[0]
"L(ŷ, y) is defined as Eq. 6.",3.2 Task-Specific Output Layer,[0],[0]
"Although the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa.",4 Incorporating Adversarial Training,[0],[0]
"Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information.
",4 Incorporating Adversarial Training,[0],[0]
"Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information.",4 Incorporating Adversarial Training,[0],[0]
"To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL).",4 Incorporating Adversarial Training,[0],[0]
"Adversarial networks have recently surfaced and are first used for generative model (Goodfellow et al., 2014).",4.1 Adversarial Network,[0],[0]
The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x),4.1 Adversarial Network,[0],[0]
"Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x).",4.1 Adversarial Network,[0],[0]
and D learns to determine whether a sample is from pG(x) or Pdata(x).,4.1 Adversarial Network,[0],[0]
"This min-max game can be optimized by the following risk:
φ = min G max D
( Ex∼Pdata [logD(x)]
+ Ez∼p(z)[log(1−D(G(z)))] )
(11)
While originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016).",4.1 Adversarial Network,[0],[0]
"Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network.",4.1 Adversarial Network,[0],[0]
"Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.",4.1 Adversarial Network,[0],[0]
"Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks.",4.2 Task Adversarial Loss for MTL,[0],[0]
"This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features.
",4.2 Task Adversarial Loss for MTL,[0],[0]
"Task Discriminator Discriminator is used to map the shared representation of sentences into a probability distribution, estimating what kinds of tasks the encoded sentence comes from.
D(skT , θD) = softmax(b+Us k T ) (12)
where U ∈ Rd×d is a learnable parameter and b ∈ Rd is a bias.
",4.2 Task Adversarial Loss for MTL,[0],[0]
"Adversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space.",4.2 Task Adversarial Loss for MTL,[0],[0]
The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features.,4.2 Task Adversarial Loss for MTL,[0],[0]
The original loss of adversarial network is limited since it can only be used in binary situation.,4.2 Task Adversarial Loss for MTL,[0],[0]
"To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:
LAdv = min θs ( λmax θD ( K∑
k=1
Nk∑
i=1
dki log[D(E(x k))]) )",4.2 Task Adversarial Loss for MTL,[0],[0]
"(13)
where dki denotes the ground-truth label indicating the type of the current task.",4.2 Task Adversarial Loss for MTL,[0],[0]
"Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator.",4.2 Task Adversarial Loss for MTL,[0],[0]
"At the same time, the discriminator tries its best to make a correct classification on the type of task.",4.2 Task Adversarial Loss for MTL,[0],[0]
"After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks.
",4.2 Task Adversarial Loss for MTL,[0],[0]
Semi-supervised Learning Multi-task Learning,4.2 Task Adversarial Loss for MTL,[0],[0]
"We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning.",4.2 Task Adversarial Loss for MTL,[0],[0]
"Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora.",4.2 Task Adversarial Loss for MTL,[0],[0]
We notice that there is a potential drawback of the above model.,4.3 Orthogonality Constraints,[0],[0]
"That is, the task-invariant features can appear both in shared space and private space.
",4.3 Orthogonality Constraints,[0],[0]
"Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)
on shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs.
",4.3 Orthogonality Constraints,[0],[0]
"After exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:
Ldiff = K∑
k=1
∥∥∥Sk>Hk ∥∥∥ 2
F , (14)
where ‖ · ‖2F is the squared Frobenius norm.",4.3 Orthogonality Constraints,[0],[0]
"Sk and Hk are two matrics, whose rows are the output of shared extractor Es(, ; θs) and task-specific extrator Ek(, ; θk) of a input sentence.",4.3 Orthogonality Constraints,[0],[0]
"The final loss function of our model can be written as:
L = LTask + λLAdv + γLDiff",4.4 Put It All Together,[0],[0]
"(15)
where λ and γ are hyper-parameter.",4.4 Put It All Together,[0],[0]
"The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).",4.4 Put It All Together,[0],[0]
"To make an extensive evaluation, we collect 16 different datasets from several popular review corpora.
",5.1 Dataset,[0],[0]
"The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect.",5.1 Dataset,[0],[0]
The goal is to classify a product review as either positive or negative.,5.1 Dataset,[0],[0]
"These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007).",5.1 Dataset,[0],[0]
"Specifically, we extract the sentences and corresponding labels from the unprocessed original data 2.",5.1 Dataset,[0],[0]
"The only preprocessing operation of these sentences is tokenized using the Stanford tokenizer 3.
",5.1 Dataset,[0],[0]
The remaining two datasets are about movie reviews.,5.1 Dataset,[0],[0]
"The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011).",5.1 Dataset,[0],[0]
One key aspect of this dataset is that each movie review has several sentences.,5.1 Dataset,[0],[0]
"The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005).
",5.1 Dataset,[0],[0]
"All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 20% and 10% respectively.",5.1 Dataset,[0],[0]
The detailed statistics about all the datasets are listed in Table 1.,5.1 Dataset,[0],[0]
The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused.,5.2 Competitor Methods for Multi-task Learning,[0],[0]
"Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.
",5.2 Competitor Methods for Multi-task Learning,[0],[0]
• MT-CNN:,5.2 Competitor Methods for Multi-task Learning,[0],[0]
"This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.
1https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/
2Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.
",5.2 Competitor Methods for Multi-task Learning,[0],[0]
"3http://nlp.stanford.edu/software/ tokenizer.shtml
4https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/unprocessed.tar.gz
5https://www.cs.cornell.edu/people/ pabo/movie-review-data/.
• MT-DNN:",5.2 Competitor Methods for Multi-task Learning,[0],[0]
"The model is proposed by Liu et al. (2015b) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.",5.2 Competitor Methods for Multi-task Learning,[0],[0]
"The word embeddings for all of the models are initialized with the 200d GloVe vectors ((Pennington et al., 2014)).",5.3 Hyperparameters,[0],[0]
"The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1].",5.3 Hyperparameters,[0],[0]
"The mini-batch size is set to 16.
",5.3 Hyperparameters,[0],[0]
"For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], λ ∈",5.3 Hyperparameters,[0],[0]
"[0.01, 0.1], and γ ∈",5.3 Hyperparameters,[0],[0]
"[0.01, 0.1].",5.3 Hyperparameters,[0],[0]
"Finally, we chose the learning rate as 0.01, λ as 0.05 and γ as 0.01.",5.3 Hyperparameters,[0],[0]
Table 2 shows the error rates on 16 text classification tasks.,5.4 Performance Evaluation,[0],[0]
"The column of “Single Task” shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models.",5.4 Performance Evaluation,[0],[0]
The column of “Multiple Tasks” shows the results achieved by corresponding multi-task models.,5.4 Performance Evaluation,[0],[0]
"From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates.",5.4 Performance Evaluation,[0],[0]
"More concretely, compared with SP-MTL, ASP-
MTL achieves 4.1% average improvement surpassing SP-MTL with 1.0%, which indicates the importance of adversarial learning.",5.4 Performance Evaluation,[0],[0]
"It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space.",5.4 Performance Evaluation,[0],[0]
"With the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks.
",5.5 Shared Knowledge Transfer,[0],[0]
"To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model MS with multi-task learning, then the learned shared layer are transferred to a second network MT that is used for the remaining one task.",5.5 Shared Knowledge Transfer,[0],[0]
"The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized.
",5.5 Shared Knowledge Transfer,[0],[0]
"More formally, we investigate two mechanisms towards the transferred shared extractor.",5.5 Shared Knowledge Transfer,[0],[0]
As shown in Figure 4.,5.5 Shared Knowledge Transfer,[0],[0]
"The first one Single Channel (SC) model consists of one shared feature extractor Es from MS , then the extracted representation will be sent to an output layer.",5.5 Shared Knowledge Transfer,[0],[0]
"By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information.",5.5 Shared Knowledge Transfer,[0],[0]
"To evaluate the effectiveness of our introduced adversarial training framework, we also make a compar-
ison with vanilla multi-task learning method.
",5.5 Shared Knowledge Transfer,[0],[0]
"Results and Analysis As shown in Table 3, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL.",5.5 Shared Knowledge Transfer,[0],[0]
"Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better.",5.5 Shared Knowledge Transfer,[0],[0]
The reason is that the task-specific layer introduced in the BiChannel model can store some private features.,5.5 Shared Knowledge Transfer,[0],[0]
"Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task.",5.5 Shared Knowledge Transfer,[0],[0]
"To get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer.",5.6 Visualization,[0],[0]
"More concretely, we refer to htj as the activation of the jneuron at time step t, where t ∈ {1, . . .",5.6 Visualization,[0],[0]
",",5.6 Visualization,[0],[0]
"n} and
j ∈ {1, . .",5.6 Visualization,[0],[0]
.,5.6 Visualization,[0],[0]
", d}.",5.6 Visualization,[0],[0]
"By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on.
",5.6 Visualization,[0],[0]
Figure 5 illustrates this phenomenon.,5.6 Visualization,[0],[0]
"Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model.",5.6 Visualization,[0],[0]
"Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons.",5.6 Visualization,[0],[0]
"For the positive sentence “Five stars, my baby can fall asleep soon in the stroller”, both models capture the informative pattern “Five stars” 6.",5.6 Visualization,[0],[0]
"However, SP-MTL makes a wrong prediction due to misunderstanding of the word “asleep”.
",5.6 Visualization,[0],[0]
"By contrast, our model makes a correct prediction and the reason can be inferred from the activation of Figure 5-(b), where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as ”asleep”, which misleads the final prediction.",5.6 Visualization,[0],[0]
"This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features.
",5.6 Visualization,[0],[0]
"We also list some typical patterns captured by
6For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature “Five stars”.
",5.6 Visualization,[0],[0]
"Model Shared Layer Task-Movie Task-Baby
SP-MTL
good, great bad, love, simple, cut, slow, cheap, infantile good, great, well-directed, pointless, cut, cheap, infantile love, bad, cute, safety, mild, broken simple
ASP-MTL good, great, love, bad poor well-directed, pointless, cut, cheap, infantile cute, safety, mild, broken simple
Table 4: Typical patterns captured by shared layer and task-specific layer of SP-MTL and ASP-MTL models on Movie and Baby tasks.
neurons from shared layer and task-specific layer in Table 4, and we have observed that: 1) for SP-MTL, if some patterns are captured by taskspecific layer, they are likely to be placed into shared space.",5.6 Visualization,[0],[0]
"Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need.",5.6 Visualization,[0],[0]
"Furthermore, some typical taskinvariant features also go into task-specific layer.",5.6 Visualization,[0],[0]
"2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively.",5.6 Visualization,[0],[0]
There are two threads of related work.,6 Related Work,[0],[0]
One thread is multi-task learning with neural network.,6 Related Work,[0],[0]
"Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011).
",6 Related Work,[0],[0]
"Liu et al. (2016c) first utilizes different LSTM layers to construct multi-task learning framwork
for text classification.",6 Related Work,[0],[0]
"Liu et al. (2016b) proposes a generic multi-task framework, in which different tasks can share information by an external memory and communicate by a reading/writing mechanism.",6 Related Work,[0],[0]
"These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space.
",6 Related Work,[0],[0]
Another thread of work is adversarial network.,6 Related Work,[0],[0]
Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks.,6 Related Work,[0],[0]
"Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain.",6 Related Work,[0],[0]
"Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept.
",6 Related Work,[0],[0]
"Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy.",6 Related Work,[0],[0]
"Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained.",6 Related Work,[0],[0]
"In this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks.",7 Conclusion,[0],[0]
We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks.,7 Conclusion,[0],[0]
"We also perform extensive qualitative
analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance.",7 Conclusion,[0],[0]
"We would like to thank the anonymous reviewers for their valuable comments and thank Kaiyu Qian, Gang Niu for useful discussions.",Acknowledgments,[0],[0]
"This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Program of China (No. 2015AA015408), Shanghai Municipal Science and Technology Commission (No. 16JC1420401).",Acknowledgments,[0],[0]
"Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features.",abstractText,[0],[0]
"However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks.",abstractText,[0],[0]
"In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other.",abstractText,[0],[0]
"We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach.",abstractText,[0],[0]
"Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.",abstractText,[0],[0]
The datasets of all 16 tasks are publicly available at http://nlp.fudan.,abstractText,[0],[0]
Adversarial Multi-task Learning for Text Classification,title,[0],[0]
Increasing use of machine learning in adversarial settings has motivated a series of efforts investigating the extent to which learning approaches can be subverted by malicious parties.,1. Introduction,[0],[0]
"An important class of such attacks involves adversaries changing their behaviors, or features of the environment, to effect an incorrect prediction.",1. Introduction,[0],[0]
"Most previous efforts study this problem as an interaction between a single learner and a single attacker (Brückner & Scheffer, 2011; Dalvi et al., 2004; Li & Vorobeychik, 2014; Zhou et al., 2012).",1. Introduction,[0],[0]
"However, in reality attackers often target a broad array of potential victim organizations.",1. Introduction,[0],[0]
"For example, they craft generic spam templates and generic malware, and then disseminate these widely to maximize impact.",1. Introduction,[0],[0]
"The resulting
*Equal contribution 1Department of EECS, Vanderbilt University, Nashville, TN, USA 2Computer Science Department, Amherst College, Amherst, MA, USA.",1. Introduction,[0],[0]
"Correspondence to: Yevgeniy Vorobeychik <eug.vorobey@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"ecology of attack targets reflects not a single learner, but many such learners, all making autonomous decisions about how to detect malicious content, although these decisions often rely on similar training datasets.
",1. Introduction,[0],[0]
"We model the resulting game as an interaction between multiple learners, who simultaneously learn linear regression models, and an attacker, who observes the learned models (as in white-box attacks (Šrndic & Laskov, 2014)), and modifies the original feature vectors at test time in order to induce incorrect predictions.",1. Introduction,[0],[0]
"Crucially, rather than customizing the attack to each learner (as in typical models), the attacker chooses a single attack for all learners.",1. Introduction,[0],[0]
"We term the resulting game a Multi-Learner Stackelberg Game, to allude to its two stages, with learners jointly acting as Stackelberg leaders, and the attacker being the follower.",1. Introduction,[0],[0]
Our first contribution is the formal model of this game.,1. Introduction,[0],[0]
Our second contribution is to approximate this game by deriving upper bounds on the learner loss functions.,1. Introduction,[0],[0]
"The resulting approximation yields a game in which there always exists a symmetric equilibrium, and this equilibrium is unique.",1. Introduction,[0],[0]
"In addition, we prove that this unique equilibrium can be computed by solving a convex optimization problem.",1. Introduction,[0],[0]
"Our third contribution is to show that the equilibrium of the approximate game is robust, both theoretically (by showing it to be equivalent to a particular robust optimization problem), and through extensive experiments, which demonstrate it to be much more robust to attacks than standard regularization approaches.
",1. Introduction,[0],[0]
"Related Work Both attacks on and defenses of machine learning approaches have been studied within the literature on adversarial machine learning (Brückner & Scheffer, 2011; Dalvi et al., 2004; Li & Vorobeychik, 2014; Zhou et al., 2012; Lowd & Meek, 2005).",1. Introduction,[0],[0]
"These approaches commonly assume a single learner, and consider either the problem of finding evasions against a fixed model (Dalvi et al., 2004; Lowd & Meek, 2005; Šrndic & Laskov, 2014), or algorithmic approaches for making learning more robust to attacks (Russu et al., 2016; Brückner & Scheffer, 2011; Dalvi et al., 2004; Li & Vorobeychik, 2014; 2015).",1. Introduction,[0],[0]
"Most of these efforts deal specifically with classification learning, but several consider adversarial tampering with regression models (Alfeld et al., 2016; Grosshans et al., 2013), although still within a single-learner and single-attacker framework.",1. Introduction,[0],[0]
"Stevens & Lowd (2013) study the algorithmic problem of
attacking multiple linear classifiers, but did not consider the associated game among classifiers.
",1. Introduction,[0],[0]
"Our work also has a connection to the literature on security games with multiple defenders (Laszka et al., 2016; Smith et al., 2017; Vorobeychik et al., 2011).",1. Introduction,[0],[0]
"The key distinction with our paper is that in multi-learner games, the learner strategy space is the space of possible models in a given model class, whereas prior research has focused on significantly simpler strategies (such as protecting a finite collection of attack targets).",1. Introduction,[0],[0]
"We investigate the interactions between a collection of learners N = {1, 2, ..., n} and an attacker in regression problems, modeled as a Multi-Learner Stackelberg Game (MLSG).",2. Model,[0],[0]
"At the high level, this game involves two stages: first, all learners choose (train) their models from data, and second, the attacker transforms test data (such as features of the environment, at prediction time) to achieve malicious goals.",2. Model,[0],[0]
"Below, we first formalize the model of the learners and the attacker, and then formally describe the full game.",2. Model,[0],[0]
"At training time, a set of training data (X,y) is drawn from an unknown distribution D. X 2 Rm⇥d is the training sample and y 2 Rm⇥1 is a vector of values of each data in X.",2.1. Modeling the Players,[0],[0]
"We let xj 2 Rd⇥1 denote the jth instance in the training sample, associated with a corresponding value yj 2 R from y. Hence, X =",2.1. Modeling the Players,[0],[0]
"[x1, ...,xm]> and y =",2.1. Modeling the Players,[0],[0]
"[y1, y2, ..., ym]>.",2.1. Modeling the Players,[0],[0]
"On the other hand, test data can be generated either from D, the same distribution as the training data, or from D0 , a modification of D generated by an attacker.",2.1. Modeling the Players,[0],[0]
The nature of such malicious modifications is described below.,2.1. Modeling the Players,[0],[0]
"We let (0   1) represent the probability that a test instance is drawn from D0 (i.e., the malicious distribution), and 1 be the probability that it is generated from D.
The action of the ith learner is to select a d⇥ 1 vector ✓i as the parameter of the linear regression function ˆyi = X✓i, where ˆyi is the predicted values for data X.",2.1. Modeling the Players,[0],[0]
"The expected cost function of the ith learner at test time is then
ci(✓i,D 0 )",2.1. Modeling the Players,[0],[0]
"= E(X0 ,y)⇠D0 [`(X 0",2.1. Modeling the Players,[0],[0]
"✓i,y)]
+ (1 )E(X,y)⇠D[`(X✓i,y)].",2.1. Modeling the Players,[0],[0]
"(1)
where `(ˆy,y) = ||ˆy",2.1. Modeling the Players,[0],[0]
y||22.,2.1. Modeling the Players,[0],[0]
"That is, the cost function of a learner i is a combination of its expected cost from both the attacker and the honest source.
",2.1. Modeling the Players,[0],[0]
"Every instance (x, y) generated according to D is, with probability , maliciously modified by the attacker into another, (x0, y), as follows.",2.1. Modeling the Players,[0],[0]
"We assume that the attacker has an instance-specific target z(x), and wishes that the
prediction made by each learner i on the modified instance, ŷ =",2.1. Modeling the Players,[0],[0]
"✓>i x 0 , is close to this target.",2.1. Modeling the Players,[0],[0]
"We measure this objective for the attacker by `(ˆy, z) = ||ˆy z||22 for a vector of predicted and target values ˆy and z, respectively.",2.1. Modeling the Players,[0],[0]
"In addition, the attacker incurs a cost of transforming a distribution D into D0 , denoted by R(D0 ,D).
",2.1. Modeling the Players,[0],[0]
"After a dataset (X 0 ,y) is generated in this way by the attacker, it is used simultaneously against all the learners.",2.1. Modeling the Players,[0],[0]
"This is natural in most real attacks: for example, spam templates are commonly generated to be used broadly, against many individuals and organizations, and, similarly, malware executables are often produced to be generally effective, rather than custom made for each target.",2.1. Modeling the Players,[0],[0]
The expected cost function of the attacker is then a sum of its total expected cost for all learners plus the cost of transforming D into D0 with coefficient > 0,2.1. Modeling the Players,[0],[0]
":
ca({✓i}ni=1,D 0 )",2.1. Modeling the Players,[0],[0]
"=
nX
i=1
E(X0 ,y)⇠D0 [`(X 0",2.1. Modeling the Players,[0],[0]
"✓i, z)]+ R(D 0 ,D).
",2.1. Modeling the Players,[0],[0]
"(2)
As is typical, we estimate the cost functions of the learners and the attacker using training data (X,y), which is also used to simulate attacks.",2.1. Modeling the Players,[0],[0]
"Consequently, the cost functions of each learner and the attacker are estimated by
ci(✓i,X 0 ) =",2.1. Modeling the Players,[0],[0]
`(X 0,2.1. Modeling the Players,[0],[0]
"✓i,y) +",2.1. Modeling the Players,[0],[0]
"(1 )`(X✓i,y) (3)
and
ca({✓i}ni=1,X 0 )",2.1. Modeling the Players,[0],[0]
"=
nX
i=1
`(X 0",2.1. Modeling the Players,[0],[0]
"✓i, z) +",2.1. Modeling the Players,[0],[0]
"R(X 0 ,X) (4)
where the attacker’s modification cost is measured by R(X
0 ,X) = ||X0 X||2F , the squared Frobenius norm.",2.1. Modeling the Players,[0],[0]
We are now ready to formally define the game between the n learners and the attacker.,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The MLSG has two stages: in the first stage, learners simultaneously select their model parameters ✓i, and in the second stage, the attacker makes its decision (manipulating X 0 ) after observing the learners’ model choices {✓i}ni=1.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"We assume that the proposed game satisfies the following assumptions:
1.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The learners have complete information about parameters , and z.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"This is a strong assumption, and we relax it in our experimental evaluation (Section 6), providing guidance on how to deal with uncertainty about these parameters.
2.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
Each learner has the same action (model parameter) space ⇥,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"✓ Rd⇥1 which is nonempty, compact and convex.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The action space of the attacker is Rm⇥d.
3.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The columns of the training data X are linearly independent.
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"We use Multi-Learner Stackelberg Equilibrium (MLSE) as the solution for the MLSG, defined as follows.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
Definition 1 (Multi-Learner Stackelberg Equilibrium (MLSE)).,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"An action profile ({✓⇤i }ni=1,X⇤) is an MLSE if it satisfies
✓⇤i = argmin ✓i2⇥ ci(✓i,X ⇤ (✓)), 8i 2 N
s.t.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"X⇤(✓) = argmin X02Rm⇥d ca({✓i}ni=1,X 0 ).
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(5)
where ✓ = {✓i}ni=1 constitutes the joint actions of the learners.
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"At the high level, the MLSE is a blend between a Nash equilibrium (among all learners) and a Stackelberg equilibrium (between the learners and the attacker), in which the attacker plays a best response to the observed models ✓ chosen by the learners, and given this behavior by the attacker, all learners’ models",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"✓i are mutually optimal.
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
The following lemma characterizes the best response of the attacker to arbitrary model choices {✓i}ni=1 by the learners.,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
Lemma 1 (Best Response of the Attacker).,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"Given {✓i}ni=1, the best response of the attacker is
X ⇤ =",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"( X+ z
nX
i=1
✓>i )(",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"I+ nX
i=1
✓i✓ > i ) 1.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(6)
Proof.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
We derive the best response of the attacker by using the first order condition.,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The details are included in the supplementary material.
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"Lemma 1 shows that the best response of the attacker, X⇤, has a closed form solution, as a function of learner model parameters {✓i}ni=1.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
Let,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"✓ i = {✓j}j 6=i, then ci(✓i,X⇤) in Eq.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(5) can be rewritten as
ci(✓i,✓ i) =",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"`(X ⇤ (✓i,✓ i)✓i,y) + (1 )`(X✓i,y).
(7) Using Eq. (7), we can then define a Multi-Learner Nash Game (MLNG): Definition 2 (Multi-Learner Nash Game (MLNG)).",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"A static game, denoted as hN ,⇥, (ci)i is a Multi-Learner Nash Game if
1.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"The set of players is the set of learners N , 2.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"the cost function of each learner i is ci(✓i,✓ i) defined
in Eq.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(7), 3.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"all learners simultaneously select ✓i 2 ⇥.
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"We can then define Multi-Learner Nash Equilibrium (MLNE) of the game hN ,⇥, (ci)i: Definition 3 (Multi-Learner Nash Equilibrium (MLNE)).",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"An action profile ✓⇤ = {✓⇤i }ni=1 is a Multi-Learner Nash Equilibrium of the MLNG hN ,⇥, (ci)i if it is the solution of the following set of coupled optimization problem:
min ✓i2⇥ ci(✓i,✓ i), 8i 2 N .",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(8)
Combining the results above, the following result is immediate.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
Theorem 1.,2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"An action profile ({✓⇤i }ni=1,X⇤) is an MLSE of the multi-learner Stackelberg game if and only if {✓⇤i }ni=1 is a MLNE of the multi-learner Nash game hN ,⇥, (ci)i, with X
⇤ defined in Eq.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"(6) for ✓i = ✓⇤i , 8i 2 N .
",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"Theorem 1 shows that we can reduce the original (n+ 1)- player Stackelberg game to an n-player simultaneous-move game hN ,⇥, (ci)i.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"In the remaining sections, we focus on analyzing the Nash equilibrium of this multi-learner Nash game.",2.2. The Multi-Learner Stackerlberg Game,[0],[0]
"In this section, we analyze the game hN ,⇥, (ci)i.",3. Theoretical Analysis,[0],[0]
As presented in Eq.,3. Theoretical Analysis,[0],[0]
"(6), there is an inverse of a complicated matrix to compute the best response of the attacker.",3. Theoretical Analysis,[0],[0]
"Hence, the cost function ci(✓i,✓ i) shown in Eq.",3. Theoretical Analysis,[0],[0]
(7) is intractable.,3. Theoretical Analysis,[0],[0]
"To address this challenge, we first derive a new game, hN ,⇥, (eci)i with tractable cost function for its players, to approximate hN ,⇥, (ci)i.",3. Theoretical Analysis,[0],[0]
"Afterward, we analyze existence and uniqueness of the Nash Equilibirum of hN ,⇥, (eci)i.
3.1.",3. Theoretical Analysis,[0],[0]
"Approximation of hN ,⇥, (ci)i We start our analysis by computing ( I + Pn
i=1",3. Theoretical Analysis,[0],[0]
"✓i✓ > i ) 1
presented in Eq.",3. Theoretical Analysis,[0],[0]
(6).,3. Theoretical Analysis,[0],[0]
Let matrix An = I +,3. Theoretical Analysis,[0],[0]
Pn i=1,3. Theoretical Analysis,[0],[0]
✓i✓ >,3. Theoretical Analysis,[0],[0]
"i ,
and A i = I+ P j 6=i ✓j✓ > j .",3. Theoretical Analysis,[0],[0]
"Then, An = A i + ✓i✓>i .",3. Theoretical Analysis,[0],[0]
"Similarly, let matrix Bn = X+ z",3. Theoretical Analysis,[0],[0]
Pn i=1,3. Theoretical Analysis,[0],[0]
✓ >,3. Theoretical Analysis,[0],[0]
"i , and B i =
X + z P
j 6=i ✓ > j , which implies that Bn = B i + z✓>i
The best response of the attacker can then be rewritten as X ⇤ = BnA 1 n .",3. Theoretical Analysis,[0],[0]
"We then obtain the following results.
",3. Theoretical Analysis,[0],[0]
Lemma 2.,3. Theoretical Analysis,[0],[0]
"An and A i satisfy
1.",3. Theoretical Analysis,[0],[0]
"An and A i are invertible, and the corresponding invertible matrices, A 1n and A 1 i , are positive definite.
",3. Theoretical Analysis,[0],[0]
2.,3. Theoretical Analysis,[0],[0]
A 1n = A 1,3. Theoretical Analysis,[0],[0]
"i
A 1 i✓i✓ >",3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1,3. Theoretical Analysis,[0],[0]
"i
1+✓>i",3. Theoretical Analysis,[0],[0]
"A 1 i✓i
.
3.",3. Theoretical Analysis,[0],[0]
✓>,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1 i✓i  1 ✓ >,3. Theoretical Analysis,[0],[0]
"i ✓i.
Proof.",3. Theoretical Analysis,[0],[0]
"The proof is included in the supplementary document.
",3. Theoretical Analysis,[0],[0]
"Lemma 2 allows us to relax `(X⇤(✓i,✓ i)✓i,y) as follows: Lemma 3.
",3. Theoretical Analysis,[0],[0]
"`(X⇤(✓i,✓ i)✓i,y)  `(B iA 1 i✓",3. Theoretical Analysis,[0],[0]
"i,y)
+
1 2 ||z y||22(✓>i",3. Theoretical Analysis,[0],[0]
✓i)2.,3. Theoretical Analysis,[0],[0]
"(9)
Proof.",3. Theoretical Analysis,[0],[0]
"Firstly, by using Sherman-Morrison formula we have
X ⇤✓i = BnA 1 n",3. Theoretical Analysis,[0],[0]
"✓i
= (B i + z✓ > i )(A 1 i A
1 i✓i✓",3. Theoretical Analysis,[0],[0]
>,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1 i 1 +,3. Theoretical Analysis,[0],[0]
✓>,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1 i✓i ),3. Theoretical Analysis,[0],[0]
"✓i
= B iA 1",3. Theoretical Analysis,[0],[0]
i✓,3. Theoretical Analysis,[0],[0]
"i +
z✓>i",3. Theoretical Analysis,[0],[0]
A 1 i✓i B iA 1,3. Theoretical Analysis,[0],[0]
i✓i✓ >,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
"A 1 i✓i
1 +",3. Theoretical Analysis,[0],[0]
✓>,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
"A 1 i✓i
=
(B i + z✓>i )",3. Theoretical Analysis,[0],[0]
"A 1 i✓i
1 + ✓>i A 1 i✓i
=
BnA 1 i✓i
1 + ✓>i A 1 i✓i
.
",3. Theoretical Analysis,[0],[0]
"Then,
`(X⇤✓i,y) = ||",3. Theoretical Analysis,[0],[0]
"BnA
1",3. Theoretical Analysis,[0],[0]
"i✓i
1 + ✓>i A 1 i✓i
y||22
= ||",3. Theoretical Analysis,[0],[0]
"BnA
1",3. Theoretical Analysis,[0],[0]
i✓i y,3. Theoretical Analysis,[0],[0]
"✓>i A 1 i✓iy
1 + ✓>i",3. Theoretical Analysis,[0],[0]
"A 1 i✓i
||22
 ||BnA 1",3. Theoretical Analysis,[0],[0]
i✓i y,3. Theoretical Analysis,[0],[0]
✓ >,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1 i✓iy|| 2 2 = ||(B i + z✓>i ),3. Theoretical Analysis,[0],[0]
A 1 i✓i y,3. Theoretical Analysis,[0],[0]
✓ >,3. Theoretical Analysis,[0],[0]
i,3. Theoretical Analysis,[0],[0]
A 1 i✓iy|| 2 2 = ||B iA 1,3. Theoretical Analysis,[0],[0]
i✓i y + (z y)✓ >,3. Theoretical Analysis,[0],[0]
i A 1,3. Theoretical Analysis,[0],[0]
"i✓i|| 2 2  `(B iA 1 i✓i,y) + ||z y|| 2 2(✓ > i",3. Theoretical Analysis,[0],[0]
"A 1 i✓i) 2
By using Lemma 2, we have (✓>i",3. Theoretical Analysis,[0],[0]
A 1 i✓i) 2  1 2 (✓ >,3. Theoretical Analysis,[0],[0]
"i ✓i) 2 which completes the proof.
",3. Theoretical Analysis,[0],[0]
Note that in Eq.,3. Theoretical Analysis,[0],[0]
"(9), B i and A i only depend on {✓j}j 6=i. Hence, the RHS of Eq.",3. Theoretical Analysis,[0],[0]
"(9) is a strictly convex function with respect to ✓i. Lemma 3 shows that `(X⇤(✓i,✓ i)✓i,y) can be relaxed by moving ✓i out of X⇤(✓i,✓ i) and adding a regularizer (✓>i ✓i)2 with its coefficient ||z y||22 2 .",3. Theoretical Analysis,[0],[0]
"Motivated by this method, we iteratively relax `(X⇤(✓i,✓ i)✓i,y) by adding corresponding regularizers.",3. Theoretical Analysis,[0],[0]
"We now identify a tractable upper bound function for ci(✓i,✓ i).",3. Theoretical Analysis,[0],[0]
"Theorem 2.
",3. Theoretical Analysis,[0],[0]
"ci(✓i,✓ i)  ",3. Theoretical Analysis,[0],[0]
"c̄i(✓i,✓ i)
= `(X✓i,y) + 2 ||z y||22
nX
j=1
(✓>j",3. Theoretical Analysis,[0],[0]
"✓i) 2 + ✏,
(10)
where ✏ is a positive constant and ✏ < +1.
",3. Theoretical Analysis,[0],[0]
Proof.,3. Theoretical Analysis,[0],[0]
We prove by extending the results in Lemma 3 and iteratively relaxing the cost function.,3. Theoretical Analysis,[0],[0]
"The details are included in the supplementary material.
",3. Theoretical Analysis,[0],[0]
As represented in Eq.,3. Theoretical Analysis,[0],[0]
"(10), c̄i(✓i,✓ i) is strictly convex with respect to ✓i and ✓j(8j 6= i).",3. Theoretical Analysis,[0],[0]
"We then use the game hN ,⇥, (c̄i)i as an approximation of hN ,⇥, (ci)i.",3. Theoretical Analysis,[0],[0]
"Let
eci(✓i,✓ i) = c̄i(✓i,✓ i)",3. Theoretical Analysis,[0],[0]
"✏
= `(X✓i,y) + 2 ||z y||22
nX
j=1
(✓>j",3. Theoretical Analysis,[0],[0]
"✓i) 2,
(11)
then hN ,⇥, (eci)i has the same Nash equilibrium with hN ,⇥, (c̄i)i if one exists, as adding or deleting a constant term does not affect the optimal solution.",3. Theoretical Analysis,[0],[0]
"Hence, we use hN ,⇥, (eci)i to approximate hN ,⇥, (ci)i, and analyze the Nash equilibrium of hN ,⇥, (eci)i in the remaining sections.",3. Theoretical Analysis,[0],[0]
"As introduced in Section 2, each learner has identical action spaces, and they are trained with the same dataset.",3.2. Existence of Nash Equilibrium,[0],[0]
"We exploit this symmetry to analyze the existence of a Nash equilibrium of the approximation game hN ,⇥, (eci)i.
",3.2. Existence of Nash Equilibrium,[0],[0]
"We first define a Symmetric Game (Cheng et al., 2004):",3.2. Existence of Nash Equilibrium,[0],[0]
Definition 4 (Symmetric Game).,3.2. Existence of Nash Equilibrium,[0],[0]
"An n-player game is symmetric if the players have the same action space, and their cost functions",3.2. Existence of Nash Equilibrium,[0],[0]
"ci(✓i,✓ i) satisfy
ci(✓i,✓ i) = cj(✓j ,✓ j), 8i, j 2 N (12)
if ✓i =",3.2. Existence of Nash Equilibrium,[0],[0]
✓j and ✓,3.2. Existence of Nash Equilibrium,[0],[0]
"i = ✓ j .
",3.2. Existence of Nash Equilibrium,[0],[0]
"In a symmetric game hN ,⇥, (eci)i it is natural to consider a Symmetric Equilibrium:",3.2. Existence of Nash Equilibrium,[0],[0]
Definition 5 (Symmetric Equilibrium).,3.2. Existence of Nash Equilibrium,[0],[0]
"An action profile {✓⇤i }ni=1 of hN ,⇥, (eci)i is a symmetric equilibrium if it is a Nash equilibrium and ✓⇤i = ✓⇤j , 8i, j 2 N .
",3.2. Existence of Nash Equilibrium,[0],[0]
"We now show that our approximate game is symmetric, and always has a symmetric Nash equilibrium.",3.2. Existence of Nash Equilibrium,[0],[0]
Theorem 3 (Existence of Nash Equilibrium).,3.2. Existence of Nash Equilibrium,[0],[0]
"hN ,⇥, (eci)i is a symmetric game and it has at least one symmetric equilibrium.
",3.2. Existence of Nash Equilibrium,[0],[0]
Proof.,3.2. Existence of Nash Equilibrium,[0],[0]
"As described above, the players of hN ,⇥, (eci)i use the same action space and complete information of others.",3.2. Existence of Nash Equilibrium,[0],[0]
"Hence, the cost function ci is symmetric, making hN ,⇥, (eci)i a symmetric game.",3.2. Existence of Nash Equilibrium,[0],[0]
"As hN ,⇥, (eci)i has nonempty, compact and convex action space, and the cost function eci is continuous in {✓i}ni=1 and convex in ✓i, according to Theorem 3 in Cheng et al. (2004), hN ,⇥, (eci)i has at least one symmetric Nash equilibrium.",3.2. Existence of Nash Equilibrium,[0],[0]
"While we showed that the approximate game always admits a symmetric Nash equilibrium, it leaves open the possibility that there may be multiple symmetric equilibria, as well as equilibria which are not symmetric.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
We now demonstrate that this game in fact has a unique equilibrium (which must therefore be symmetric).,3.3. Uniqueness of Nash Equilibrium,[0],[0]
Theorem 4 (Uniqueness of Nash Equilibrium).,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"hN ,⇥, (eci)i has a unique Nash equilibrium, and this unique NE is symmetric.
",3.3. Uniqueness of Nash Equilibrium,[0],[0]
Proof.,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"We have known that hN ,⇥, (eci)i has at least one NE, and each learner has an nonempty, compact and convex action space ⇥.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Hence, we can apply Theorem 2 and Theorem 6 of Rosen (1965).",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"That is, for some fixed {ri}ni (0 < ri < 1, Pn i=1 ri = 1), if the matrix in Eq.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"(13) is positive definite, then hN ,⇥, (eci)i has a unique NE.
",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Jr(✓) =
2 64 r1r✓1,✓1ec1(✓) . .",3.3. Uniqueness of Nash Equilibrium,[0],[0]
.,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"r1r✓1,✓nec1(✓) ...",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"...
rnr✓n,✓1ecn(✓) . .",3.3. Uniqueness of Nash Equilibrium,[0],[0]
.,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"rnr✓n,✓necn(✓)
3
75
(13) We first let r1 = r2 = ...",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"= rn = 1n and decompose Jr(✓) as follows,
Jr(✓) = 2
n P+ 2",3.3. Uniqueness of Nash Equilibrium,[0],[0]
||z y||22 2n,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"(Q+ S+T), (14)
where P and Q are block diagonal matrices such that Pii = X
> X, Pij = 0, Qii = 4✓i✓>i +",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"✓>i ✓iI and Qij = 0, 8i, j 2 N , j 6= i. S and T are block symmetric matrices such that Sii = ✓>i ✓iI, Sij = ✓>i ✓jI, Tii = P j 6=i ✓j✓ > j and Tij = ✓j✓>i , 8i, j 2 N ,",3.3. Uniqueness of Nash Equilibrium,[0],[0]
j 6=,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"i.
",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Next, we prove that P is positive definite, and Q, S and T are positive semi-definite.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Hence, Jr(✓) is positive definite, which indicates that hN ,⇥, (eci)i has a unique NE.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"As Theorem 3 points out, the game has at least one symmetric NE.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Therefore, the NE is unique and must be symmetric.",3.3. Uniqueness of Nash Equilibrium,[0],[0]
Due to space limitation the details of this proof are included in the supplementary material.,3.3. Uniqueness of Nash Equilibrium,[0],[0]
"Having shown that hN ,⇥, (eci)i has a unique symmetric Nash equilibrium, we now consider computing its solution.",4. Computing the Equilibrium,[0],[0]
We exploit the symmetry of the game which enables to reduce the search space of the game to only symmetric solutions.,4. Computing the Equilibrium,[0],[0]
"Particularly, we derive the symmetric Nash equilibrium of hN ,⇥, (eci)i by solving a single convex optimization problem.",4. Computing the Equilibrium,[0],[0]
We obtain the following result.,4. Computing the Equilibrium,[0],[0]
Theorem 5.,4. Computing the Equilibrium,[0],[0]
"Let
f(✓) =",4. Computing the Equilibrium,[0],[0]
"`(X✓,y)",4. Computing the Equilibrium,[0],[0]
"+ (n+ 1)
2 2 ||z y||22(✓>✓)2, (15)
",4. Computing the Equilibrium,[0],[0]
"Then, the unique symmetric NE of hN ,⇥, (eci)i, {✓⇤i }ni=1, can be derived by solving the following convex optimization problem
min ✓2⇥ f(✓) (16)
and then letting ✓⇤i = ✓⇤, 8i 2 N , where ✓⇤ is the solution of Eq.",4. Computing the Equilibrium,[0],[0]
"(16).
",4. Computing the Equilibrium,[0],[0]
Proof.,4. Computing the Equilibrium,[0],[0]
We prove this theorem by characterizing the firstorder optimality conditions of each learner’s minimization problem in Eq.,4. Computing the Equilibrium,[0],[0]
(8) with ci being replaced with its approximation eci.,4. Computing the Equilibrium,[0],[0]
"Let {✓⇤i }ni=1 be the NE, then it satisfies
(⌘ ✓⇤i )>",4. Computing the Equilibrium,[0],[0]
"r✓ieci(✓⇤i ,✓⇤ i) 0, 8⌘ 2 ⇥, 8i 2 N (17)
where r✓ieci(✓⇤i ,✓⇤ i) is the gradient of eci(✓i,✓ i) with respect to ✓i and is evaluated at {✓⇤i }ni=1.",4. Computing the Equilibrium,[0],[0]
"Then, Eq. (17) is equivalent to the equations as follows:
( (⌘ ✓⇤1)>r✓1ec1(✓⇤1 ,✓⇤ 1) 0, 8⌘ 2 ⇥,",4. Computing the Equilibrium,[0],[0]
"✓⇤1 = ✓ ⇤ j , 8j 2 N \",4. Computing the Equilibrium,[0],[0]
"{1}
(18)
",4. Computing the Equilibrium,[0],[0]
"The reasons are: first, any solution of Eq.",4. Computing the Equilibrium,[0],[0]
(17) satisfies Eq.,4. Computing the Equilibrium,[0],[0]
"(18), as {✓⇤i }ni=1 is symmetric; Second, any solution of Eq.",4. Computing the Equilibrium,[0],[0]
(18) also satisfies Eq.,4. Computing the Equilibrium,[0],[0]
(17).,4. Computing the Equilibrium,[0],[0]
"By definition of symmetric game, if ✓⇤1 = ✓⇤j , then r✓1ec1(✓⇤1 ,✓⇤ 1) = r✓jecj(✓⇤j ,✓⇤ j), and we have
(⌘ ✓⇤j )>",4. Computing the Equilibrium,[0],[0]
"r✓jecj(✓⇤j ,✓⇤ j), 8⌘ 2 ⇥, 8j 2 N \ {1}
Hence, Eq. (17) and Eq.",4. Computing the Equilibrium,[0],[0]
(18) are equivalent.,4. Computing the Equilibrium,[0],[0]
Eq.,4. Computing the Equilibrium,[0],[0]
"(18) can be further rewritten as
(⌘ ✓⇤1)>r✓1ec1(✓⇤1 ,✓⇤ 1)|✓⇤1=...=✓⇤n 0, 8⌘ 2 ⇥.",4. Computing the Equilibrium,[0],[0]
"(19)
We then let F (✓⇤1) = r",4. Computing the Equilibrium,[0],[0]
"✓1ec1(✓⇤1 ,✓⇤ 1)|✓⇤1=...=✓⇤n
= 2X >",4. Computing the Equilibrium,[0],[0]
"(X✓⇤1 y) +
2 (n+ 1)
2 ||z y||22✓⇤1 >✓⇤1✓ ⇤ 1 .
(20)
",4. Computing the Equilibrium,[0],[0]
"Then, F (✓⇤1) = r✓1f(✓⇤1) where f(·) is defined in Eq. (15).",4. Computing the Equilibrium,[0],[0]
"Hence, we have
(⌘ ✓⇤1)>r✓1f(✓⇤1) 0, 8⌘ 2 ⇥, (21)
",4. Computing the Equilibrium,[0],[0]
"This means that ✓⇤1 is the solution of the optimization problem in Eq.(16) which finally completes the proof.
",4. Computing the Equilibrium,[0],[0]
A deeper look at Eq.,4. Computing the Equilibrium,[0],[0]
"(15) reveals that the Nash equilibrium can be obtained by each learner independently, without knowing others’ actions.",4. Computing the Equilibrium,[0],[0]
This means that the Nash equilibrium can be computed in a distributed manner while the convergence is still guaranteed.,4. Computing the Equilibrium,[0],[0]
"Hence, our proposed approach is highly scalable, as increasing the number of learners does not impact the complexity of finding the Nash equilibrium.",4. Computing the Equilibrium,[0],[0]
We investigate the robustness of this equilibrium both using theoretical analysis and experiments in the remaining sections.,4. Computing the Equilibrium,[0],[0]
"We now draw a connection between the multi-learner equilibrium in the adversarial setting, derived above, and robustness, in the spirit of the analysis by Xu et al. (2009).",5. Robustness Analysis,[0],[0]
"Specifically, we prove the equivalence between Eq.",5. Robustness Analysis,[0],[0]
(16) and a robust linear regression problem where data is maliciously corrupted by some disturbance 4.,5. Robustness Analysis,[0],[0]
"Formally, a robust linear regression solves the following problem:
min ✓2⇥ max 42U ||y (X+4)✓||22, (22)
where the uncertainty set U = {4 2 Rm⇥d |4T4 = G : |Gij |  c|✓i✓j | 8i, j}, with c = (n+1)2 2 ||z y|| 2 2.",5. Robustness Analysis,[0],[0]
"Note that ✓ is a vector and ✓i is the i-th element of ✓.
",5. Robustness Analysis,[0],[0]
"From a game-theoretic point of view, in training phase the defender is simulating an attacker.",5. Robustness Analysis,[0],[0]
The attacker maximizes the training error by adding disturbance to X.,5. Robustness Analysis,[0],[0]
The magnitude of the disturbance is controlled by a parameter c = (n+1)2 2 ||z y|| 2 2.,5. Robustness Analysis,[0],[0]
"Consequently, the robustness of Eq.",5. Robustness Analysis,[0],[0]
(22) is guaranteed if and only if the magnitude reflects the uncertainty interval.,5. Robustness Analysis,[0],[0]
"This sheds some light on how to choose , and z in practice.",5. Robustness Analysis,[0],[0]
"One strategy is to overestimate the attacker’s strength, which amounts to choosing small values of , large values of and exaggerated target z.",5. Robustness Analysis,[0],[0]
The intuition of this strategy is to enlarge the uncertainty set so as to cover potential adversarial behavior.,5. Robustness Analysis,[0],[0]
In Experiments section we will show this strategy works well in practice.,5. Robustness Analysis,[0],[0]
Another insight from Eq.,5. Robustness Analysis,[0],[0]
(22) is that the fundamental reason MLSG is robust is because it proactively takes adversarial behavior into account.,5. Robustness Analysis,[0],[0]
Theorem 6.,5. Robustness Analysis,[0],[0]
The optimal solution ✓⇤ of the problem in Eq.,5. Robustness Analysis,[0],[0]
(16) is an optimal solution to the robust optimization problem in Eq.,5. Robustness Analysis,[0],[0]
"(22).
",5. Robustness Analysis,[0],[0]
Proof.,5. Robustness Analysis,[0],[0]
"Fix ✓⇤, we show that
max 42U ||y (X+4)✓⇤||22 = ||y X✓⇤||22 + c(✓⇤ T✓⇤) 2 .
",5. Robustness Analysis,[0],[0]
"The left-hand side can be expanded as:
max 42U ||y (X+4)✓⇤||22
=max 42U ||y X✓⇤ 4✓⇤||22
max 42U ||y X✓⇤||22 + max42U ||4✓ ⇤||22
=max 42U ||y X✓⇤||22 + max42U ✓ ⇤T4T4✓⇤
(substitute 4T4 = G) =||y X✓⇤||22 +max
G ✓⇤TG✓⇤
=||y X✓⇤||22 +max G
dX
i=1
",5. Robustness Analysis,[0],[0]
"|✓⇤i | 2 Gii + 2
dX
j=1
j 1X
i=1
✓⇤i ✓ ⇤ jGij
||y X✓⇤||22 + c dX
i=1
|✓⇤i | 4 + 2c
dX
j=1
j 1X
i=1
(✓⇤i ✓ ⇤ j ) 2
=||y X✓⇤||22 + c dX
i=1
|✓⇤i |2 2
=||y X✓⇤||22 + c(✓⇤ T✓⇤) 2 .
",5. Robustness Analysis,[0],[0]
Now we define 4⇤ =,5. Robustness Analysis,[0],[0]
"[pc✓⇤1u, · · · , p c✓⇤nu], where ✓⇤i is the i-th element of ✓⇤ and u is defined as:
u , ( y X✓⇤ ||y X✓⇤||2 , if y 6= X✓ ⇤
any vector with unit L2 norm, otherwise (23)
",5. Robustness Analysis,[0],[0]
"Then we have:
max 42U ky (X+4)✓⇤k22
||y (X+4⇤)✓⇤||22 =||y X✓⇤ 4⇤✓⇤||22
=||y X✓⇤ dX
i=1
p c|✓⇤i |2u||22
(u is in the same direction as y X✓⇤ )
=||y X✓⇤||22 + ||",5. Robustness Analysis,[0],[0]
"dX
i=1
p c|✓⇤i |2u||22
=||y X✓⇤||22 + c(✓⇤ T✓⇤)
2
(24)",5. Robustness Analysis,[0],[0]
"As previously discussed, a dataset is represented by (X,y), where X is the feature matrix and y is the vector of labels.",6. Experiments,[0],[0]
"We use (xj ,yj) to denote the j-th instance and its corresponding label.",6. Experiments,[0],[0]
"The dataset is equally divided into a training set (Xtrain,ytrain) and a testing set (Xtest,ytest).",6. Experiments,[0],[0]
"We conducted experiments on three datasets: Wine Quality (redwine),PDF malware (PDF), and Boston Housing Market (boston).",6. Experiments,[0],[0]
The number of learners is set to 5.,6. Experiments,[0],[0]
"Due to space limitation the experimental results for the boston dataset are included in supplement.
",6. Experiments,[0],[0]
"The Wine Quality dataset (Cortez et al., 2009) contains 1599 instances and each instance has 11 features.",6. Experiments,[0],[0]
Those features are physicochemical and sensory measurements for wine.,6. Experiments,[0],[0]
"The response variables are quality scores ranging from 0 to 10, where 10 represents for best quality and 0 for least quality.",6. Experiments,[0],[0]
The PDF malware dataset consists of 18658 PDF files collected from the internet.,6. Experiments,[0],[0]
"We employed an opensourced tool mimicus1 to extract 135 real-valued features
1https://github.com/srndic/mimicus
from PDF files (Šrndic & Laskov, 2014).",6. Experiments,[0],[0]
"We then applied peepdf 2 to score each PDF between 0 and 10, with a higher score indicating greater likelihood of being malicious.
",6. Experiments,[0],[0]
"Throughout, we abbreviate our proposed approach as MLSG, and compare it to three other algorithms: ordinary least squares (OLS) regression, as well as Lasso, and Ridge regression (Ridge).",6. Experiments,[0],[0]
Lasso and Ridge are ordinary least square with L1 and L2 regularizations.,6. Experiments,[0],[0]
"In our evaluation, we simulate the attacker for different values of (the probability that a given instance is maliciously manipulated).",6. Experiments,[0],[0]
The specific attack targets z vary depending on the dataset; we discuss these below.,6. Experiments,[0],[0]
"For our evaluation, we compute model parameters (for the equilibrium, in the case of MLSG) on training data.",6. Experiments,[0],[0]
"We then use test data to compute optimal attacks, characterized by Eq.",6. Experiments,[0],[0]
(6).,6. Experiments,[0],[0]
"Let X
0 test be the test feature matrix after adversarial manipulation, ˆyAtest the associated predicted labels on manipulated test data, ˆytest predicted labels on untainted test data, and ytest the ground truth labels for test data.",6. Experiments,[0],[0]
"We use root expected mean square error (RMSE) as an evaluation metric, where the expectation is with respect to the probability of a particular instance being maliciously manipulated:q
(ŷAtest ytest) T (ŷAtest ytest)+(1 )(ŷtest ytest)T (ŷtest ytest)
N , where N is the size of the test data.
",6. Experiments,[0],[0]
The redwine dataset: Recall that the response variables in redwine dataset are quality scores ranging from 0 to 10.,6. Experiments,[0],[0]
We simulated an attacker whose target is to increase the overall scores of testing data.,6. Experiments,[0],[0]
In practice this could correspond to the scenario that wine sellers try to manipulate the evaluation of third-party organizations.,6. Experiments,[0],[0]
"We formally define the attacker’s target as z = y + , where y is the ground-truth response variables and is a real-valued vector representing the difference between the attacker’s target and the ground-truth.",6. Experiments,[0],[0]
"Since the maximum score is 10, any element of z that is greater than 10 is clipped to 10.",6. Experiments,[0],[0]
We define to be homogeneous (all elements are the same); generalization to heterogeneous values is direct.,6. Experiments,[0],[0]
The mean and standard deviation of y are µr = 5.64 and r = 0.81.,6. Experiments,[0],[0]
"We let = 5 r ⇥ 1, where 1 is a vector with all elements equal to one.",6. Experiments,[0],[0]
The intuition for this definition is to simulate the generating process of adversarial data.,6. Experiments,[0],[0]
"Specifically, by setting the attacker’s target to an unrealistic value (i.e. in current case outside the 3 r of µr), the generated adversarial data X 0 is supposed to be intrinsically different from X. For ease of exposition we use the term defender to refer to MLSG.
Remember that in Eq.(11) there are three hyper-parameters in the defender’s loss function: , , and z. is the regularization coefficient in the attacker’s loss function shown in Eq.(4).",6. Experiments,[0],[0]
"It is negatively proportional to the attacker’s strength.
",6. Experiments,[0],[0]
"2https://github.com/rohit-dua/peePDF
is the probability of a test data being malicious.",6. Experiments,[0],[0]
z is the predication targets of the attacker.,6. Experiments,[0],[0]
In practice these three hyper-parameters are externally set by the attacker.,6. Experiments,[0],[0]
"In the first experiment below we assume the defender knows the values of these three hyper-parameters, which corresponds to the best case.",6. Experiments,[0],[0]
The result is shown in Figure 1.,6. Experiments,[0],[0]
"Each bar is averaged over 50 runs, where at each run we randomly sampled training and test data.",6. Experiments,[0],[0]
The regularization parameters of Lasso and Ridge were selected by cross-validation.,6. Experiments,[0],[0]
"Figure 1 demonstrates that MLSG approximate equilibrium solution is significantly more robust than conventional linear regression learning, with and without regularization.
",6. Experiments,[0],[0]
"In the second experiment we relaxed the assumption that the defender knows , and z, and instead simulated the practical scenario that the defender obtains estimates for these (for example, from historical attack data), but the estimates have error.",6. Experiments,[0],[0]
We denote by ˆ = 0.5 and ˆ = 0.8 the defender’s estimates of the true and .3,6. Experiments,[0],[0]
Remember that is the probability of an instance being malicious and is negatively proportional to the attacker’s strength.,6. Experiments,[0],[0]
So the estimation characterizes a pessimistic defender that is expecting very strong attacks.,6. Experiments,[0],[0]
"We experimented with two kinds of estimation about z: 1) the defender overestimates z: ˆz = y + t1, where t is a random variable sampled from a uniform distribution over [5 r, 10]; and 2) the defender underestimates z: ˆz = y + t1, where t is sampled from [0, 5 r].",6. Experiments,[0],[0]
Due to space limitations we only present the results for the latter; the former can be found in the supplementary materials.,6. Experiments,[0],[0]
"In Figure 2 the y-axis represents the actual values of , and the x-axis represents the actual values of .",6. Experiments,[0],[0]
The color bar on the right of each figure visualizes the average RMSE.,6. Experiments,[0],[0]
Each cell is averaged over 50 runs.,6. Experiments,[0],[0]
"The result shows that even if there is a discrepancy between the defender’s
3We tried alternative values of ̂ and ̂, and the results are consistent.",6. Experiments,[0],[0]
"Due to space limitations we include them in supplemental materials.
estimation and the actual adversarial behavior, MLSG is consistently more robust than the other approaches.
",6. Experiments,[0],[0]
The PDF dataset:,6. Experiments,[0],[0]
The response variables of this dataset are malicious scores ranging between 0 and 10.,6. Experiments,[0],[0]
The mean and standard deviation of y are µp = 5.56 and p = 2.66.,6. Experiments,[0],[0]
"Instead of letting the be non-negative as in previous two datasets, the attacker’s target is to descrease the scores of malicious PDFs.",6. Experiments,[0],[0]
"Consequently, we define = 2 p⇥1M, where M is the set of indices of malicious PDF and 1M is a vector with only those elements indexed by M being one and others being zero.",6. Experiments,[0],[0]
Our experiments were conducted on a subset (3000 malicious PDF and 3000 benign PDF) randomly sampled from the original dataset.,6. Experiments,[0],[0]
We evenly divided the subset into training and testing sets.,6. Experiments,[0],[0]
We applied PCA to reduce dimensionality of the data and selected the top-10 principal components as features.,6. Experiments,[0],[0]
The result for best case is displayed in Figure 3.,6. Experiments,[0],[0]
"Notice that when = 0, MLSG is less robust than Lasso.",6. Experiments,[0],[0]
"This is to be expected, as = 0 corresponds to non-adversarial data.
",6. Experiments,[0],[0]
"Similarly as before we relaxed the assumption that the defender knows , and z and let the defender’s estimation of the true and be ˆ = 1.5 and ˆ = 0.5.",6. Experiments,[0],[0]
We also experimented with both overestimation and underestimation of z. The defender’s estimation is ˆz = y,6. Experiments,[0],[0]
"t1M. For overestimation setting t is sampled from [2 p, 3 p], and for underestimation setting it is sampled from [ p, 2 p].",6. Experiments,[0],[0]
The result for underestimated ˆz is showed in Figure 4.,6. Experiments,[0],[0]
Notice that in the upper left plot of Figure 4 the area inside the blue rectangle corresponds to those cases where ˆ and ˆ are overestimated and they are more robust than the remaining underestimated cases.,6. Experiments,[0],[0]
Similar patterns can be observed in Figure 2.,6. Experiments,[0],[0]
This further supports our claim that it is advantageous to overestimate adversarial behavior.,6. Experiments,[0],[0]
We study the problem of linear regression in adversarial settings involving multiple learners learning from the same or similar data.,7. Conclusion,[0],[0]
"In our model, learners first simultaneously decide on their models (i.e., learn), and an attacker then modifies test instances to cause predictions to err towards the attacker’s target.",7. Conclusion,[0],[0]
"We first derive an upper bound on the cost functions of all learners, and the resulting approximate game.",7. Conclusion,[0],[0]
"We then show that this game has a unique symmetric equilibrium, and present an approach for computing this equilibrium by solving a convex optimization problem.",7. Conclusion,[0],[0]
"Finally, we show that the equilibrium is robust, both theoretically, and through an extensive experimental evaluation.",7. Conclusion,[0],[0]
We would like to thank Shiying Li in the Department of Mathematics at Vanderbilt University for her valuable and constructive suggestions for the proof of Theorem 4.,Acknowledgements,[0],[0]
"This work was partially supported by the National Science Foundation (CNS-1640624, IIS-1526860, IIS-1649972), Office of Naval Research (N00014-15-1-2621), Army Research Office (W911NF-16-1-0069), and National Institutes of Health (R01HG006844-05).",Acknowledgements,[0],[0]
"Despite the considerable success enjoyed by machine learning techniques in practice, numerous studies demonstrated that many approaches are vulnerable to attacks.",abstractText,[0],[0]
An important class of such attacks involves adversaries changing features at test time to cause incorrect predictions.,abstractText,[0],[0]
Previous investigations of this problem pit a single learner against an adversary.,abstractText,[0],[0]
"However, in many situations an adversary’s decision is aimed at a collection of learners, rather than specifically targeted at each independently.",abstractText,[0],[0]
We study the problem of adversarial linear regression with multiple learners.,abstractText,[0],[0]
"We approximate the resulting game by exhibiting an upper bound on learner loss functions, and show that the resulting game has a unique symmetric equilibrium.",abstractText,[0],[0]
"We present an algorithm for computing this equilibrium, and show through extensive experiments that equilibrium models are significantly more robust than conventional regularized linear regression.",abstractText,[0],[0]
Adversarial Regression with Multiple Learners,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 11–21 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
11",text,[0],[0]
"Consider automated systems that are used for determining credit ratings, setting insurance policy rates, or helping in hiring decisions about individuals.",1 Introduction,[0],[0]
"We would like such decisions to not take into account factors such as the gender or the race of the individual, or any other factor which we deem to be irrelevant to the decision.",1 Introduction,[0],[0]
We refer to such irrelevant factors as protected attributes.,1 Introduction,[0],[0]
"The naive solution of not including protected attributes in the features to a Machine Learning system is insufficient: other features may be highly correlated with—and thus predictive of—the protected attributes (Pedreshi et al., 2008).",1 Introduction,[0],[0]
"For example, in Credit Score modeling, text might help in credit
score decisions (Ghailan et al., 2016).",1 Introduction,[0],[0]
"By using the raw text as is, a discrimination issue might arise, as textual information can be predictive of some demographic factors (Hovy et al., 2015) and author’s attributes might correlate with target variables (Zhao et al., 2017).
",1 Introduction,[0],[0]
In this paper we are interested in languagebased features.,1 Introduction,[0],[0]
"It is well established that textual information can be predictive of age, race, gender, and many other social factors of the author (Koppel et al., 2002; Burger et al., 2011; Nguyen et al., 2013; Weren et al., 2014; Verhoeven and Daelemans, 2014; Rangel et al., 2016; Verhoeven et al., 2016; Blodgett et al., 2016), or even the audience of the text (Voigt et al., 2018).
",1 Introduction,[0],[0]
"Thus, any system that incorporates raw text into its decision process is at risk of indirectly conditioning on such signals.",1 Introduction,[0],[0]
Recent advances in representation learning suggest adversarial training as a mean to hide the protected attributes from the decision function (Section 2).,1 Introduction,[0],[0]
"We perform a series of experiments and show that: (1) Information about race, gender and age is indeed encoded into intermediate representations of neural networks, even when training for seemingly unrelated tasks and the training data is balanced in terms of the protected attributes (Section 4); (2) The adversarial training method is indeed effective for reducing the amount of protected encoded information... (3) ...",1 Introduction,[0],[0]
"but in some cases even though the adversarial component seems to be doing a perfect job, a fair amount of protected information still remains, and can be extracted from the encoded representations (Section 5.1).
",1 Introduction,[0],[0]
This suggests that when working with text data it is very easy to condition on sensitive properties by mistake.,1 Introduction,[0],[0]
"Even when explicitly using the adversarial training method to remove such properties, one should not blindly trust the adversary, and be careful to ensure the protected attributes are in-
deed fully removed.",1 Introduction,[0],[0]
"We explore means for improving the effectiveness of the adversarial training procedure (section 5.2).1
However, while successful to some extent, none of the methods fully succeed in removing all demographic information.",1 Introduction,[0],[0]
"Our main message, then, remains cautionary: if the goal is to ensure fairness or invariant representation, do not trust adversarial removal of features from text inputs for achieving it.",1 Introduction,[0],[0]
"We follow a setup in which we have some labeled data D composed of documents x1, ..., xn and task labels y1, ..., yn.",2 Learning Setup,[0],[0]
We wish to train a classifier f that accurately predicts the main task labels yi.,2 Learning Setup,[0],[0]
"Each data point xi is also associated with a protected attribute zi, and we want the decision yi = f(xi) to be oblivious to zi.",2 Learning Setup,[0],[0]
"Following (Ganin and Lempitsky, 2015; Xie et al., 2017), we structure f as an encoder h(x) that maps x into a representation vector hx, and a classifier c(h(x))",2 Learning Setup,[0],[0]
that is used for predicting y based on hx.,2 Learning Setup,[0],[0]
"If hxi is not predictive of zi, then the main task prediction f(xi) = c(h(xi)) does not depend on zi.
",2 Learning Setup,[0],[0]
"We say that a protected attribute z has leaked if we can train a classifier c′(hxi) to predict zi with an accuracy beyond chance level, and that the protected attribute is guarded if we cannot train such a classifier.",2 Learning Setup,[0],[0]
"We say that a classifier f(x) = c(h(x)) is guarded if z is guarded, and that it is leaky with respect to z if z leaked.
",2 Learning Setup,[0],[0]
"Adversarial Training In order to make f oblivious to z, we follow the adversarial training setup (Goodfellow et al., 2014; Ganin and Lempitsky, 2015; Beutel et al., 2017; Xie et al., 2017).",2 Learning Setup,[0],[0]
"During training, an adversarial classifier adv(hx) is trained to predict z, while the encoder h is trained to make adv fail.",2 Learning Setup,[0],[0]
"Concretely, the training procedure tries to jointly optimize both quantities:
arg min adv
L(adv(h(xi)), zi)
arg min h,c
L(c(h(xi)), yi)− L(adv(h(xi)), zi)
where L(y′, y) is the loss function (in our case, cross entropy).",2 Learning Setup,[0],[0]
This objective results in creating the representation hx s.t.,2 Learning Setup,[0],[0]
"it’s maximally informative for the main task, while at the same time
1The code and data acquisition are available in: https: //github.com/yanaiela/demog-text-removal
minimally informative of the protected attribute.",2 Learning Setup,[0],[0]
"The optimization is performed in practice using the gradient-reversal layer (GRL) method (Ganin and Lempitsky, 2015).",2 Learning Setup,[0],[0]
The GRL is a layer gλ that is inserted between the encoded vector hx and the adversarial classifier adv.,2 Learning Setup,[0],[0]
"During the forward pass the layer acts as the identity, while during backpropagation it scales the gradients passed through it by −λ, causing the encoder to receive the opposite gradients from the adversary.",2 Learning Setup,[0],[0]
The metaparameter λ controls the intensity of the reversal layer.,2 Learning Setup,[0],[0]
"This results in the objective:
arg min h,c,adv L(c(h(xi)), yi)+L(adv(gλ(h(xi))), zi)
Attacker Network To test the effectiveness of the adversarial training, we use an attacker network att(hx).",2 Learning Setup,[0],[0]
"After the classifier c(h(x)) is fully trained, we use the encoder to obtain representations h, and train the attacker network to predict z based on h, without access to the encoder or to the original inputs x that resulted in h.",2 Learning Setup,[0],[0]
"If, after training, the attacker can predict z on unseen examples with an accuracy of beyond chance level, then the attribute z leaked to the representation, and the classifier is not guarded.",2 Learning Setup,[0],[0]
Network Architecture,2 Learning Setup,[0],[0]
"In our setup, an example xi is a sequence of tokens w1, ..., wmi and the encoder is a one layer LSTM network that reads in the associated embedding vectors and returns the final state: h = LSTM(w1:m).",2 Learning Setup,[0],[0]
"The classifier c and the adversarial adv are both multi-layer perceptrons with one hidden layer, sharing the same hidden layer size and activation function (tanh).2",2 Learning Setup,[0],[0]
"To perform our experiments, we need a reasonably large dataset in which the data-points x contain textual information, and for which we have both main-task labels y and protected attribute labels z.","3 Data, Tasks, and Protected Attributes",[0],[0]
"While our motivating example used prediction tasks for credit rating, insurance rates or hiring decisions, to the best of our knowledge there are no publicly available datasets for these sensitive tasks that meet our criteria.","3 Data, Tasks, and Protected Attributes",[0],[0]
"We thus opted to use much less sensitive main-tasks, for which we can obtain the needed data.","3 Data, Tasks, and Protected Attributes",[0],[0]
"We focus on Twitter messages, and our protected attributes are binary-race (non-hispanic Whites vs. non-hispanic Blacks),
2Further details regarding the architecture and training parameters can be found in the supplementary materials.
","3 Data, Tasks, and Protected Attributes",[0],[0]
binary-gender (Male vs. Female)3 and binaryage (18-34 vs. 35+).,"3 Data, Tasks, and Protected Attributes",[0],[0]
As main tasks we chose binary emoji-based sentiment prediction and binary tweet-mention prediction.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"Both the sentiment and the mention prediction tasks are not inherently correlated with race, gender or age.","3 Data, Tasks, and Protected Attributes",[0],[0]
"Protected attributes leakage in these seemingly benign main-tasks is a strong indicator that such leakage is likely to occur also in more sensitive tasks.
","3 Data, Tasks, and Protected Attributes",[0],[0]
Main Tasks: Sentiment and Mention-detection Both tasks can be derived automatically from twitter data.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"We construct a binary “sentiment” task by identifying a subset of emojis which are associated with positive and negative sentiment,4 identifying tweets containing these emojis, assigning them with the corresponding sentiment and removing the emojis.","3 Data, Tasks, and Protected Attributes",[0],[0]
Tweets containing emojis from both sentiment lists are discarded.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"The binary mention task is to determine if a tweet mentions another user, i.e, classifying conversational vs. nonconversational tweets.","3 Data, Tasks, and Protected Attributes",[0],[0]
"We derive this dataset by identifying tweets that include @mentions tokens, and removing all such tokens from the tweets.
","3 Data, Tasks, and Protected Attributes",[0],[0]
Protected: Race,"3 Data, Tasks, and Protected Attributes",[0],[0]
"The race annotation is based on the dialectal tweets (DIAL) corpus from (Blodgett et al., 2016), consisting of 59.2 million tweets by 2.8 million users.","3 Data, Tasks, and Protected Attributes",[0],[0]
Each tweet is associated with predicted “race” information which was predicted using a technique that takes into account the geolocation of the author and the words in the tweet.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"We focus on the AAE (African-American English) and SAE (Standard American English) categories, which we use as proxies for non-Hispanic blacks and non-Hispanic whites.
","3 Data, Tasks, and Protected Attributes",[0],[0]
We chose only annotations with confidence (the probability of the authors’ race) of above 80%.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"Due to its construction, the race annotations in this dataset are highly correlated with the language being used.","3 Data, Tasks, and Protected Attributes",[0],[0]
"As such, the data reflects an extreme case in which the underlying language is very predictive of the protected attribute.
","3 Data, Tasks, and Protected Attributes",[0],[0]
"Protected: Age and Gender We use data from the PAN16 dataset (Rangel et al., 2016), containing manually annotated Age and Gender information of 436 Twitter users, along with up to 1k
3While gender is a non-binary construct, many decisions in the real-world are unfortunately still influenced by hard binary gender categories.","3 Data, Tasks, and Protected Attributes",[0],[0]
"We thus consider binary-gender to be a useful approximation in our context.
","3 Data, Tasks, and Protected Attributes",[0],[0]
"4Complete list is available in Appendix C
tweets for each user.","3 Data, Tasks, and Protected Attributes",[0],[0]
User annotation was performed by consulting the user’s LinkedIn profile.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"Gender was determined by considering the user’s name and photograph, discarding unclear cases.","3 Data, Tasks, and Protected Attributes",[0],[0]
"Age range was determined by birth-date which was published on the user’s profile, or by mapping their degree starting date.
","3 Data, Tasks, and Protected Attributes",[0],[0]
"Data-splits From the DIAL corpus we extracted 166K and 10K tweets for training and development purpose respectively (after cleaning and extracting relevant tweets), whereas for the PAN16 dataset we collected 160K tweets for training and 10K for development.","3 Data, Tasks, and Protected Attributes",[0],[0]
The train/development split in both phases of the training (task-training and attacker-training) is the same.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"This is the worst possible scenario for the attacker, as it is training on the exact representations the adversary attempted to remove the protected attribute from.","3 Data, Tasks, and Protected Attributes",[0],[0]
"Each split is balanced with respect to both the main and the protected labels: a random prediction of each variable is likely to result in 50% accuracy.
","3 Data, Tasks, and Protected Attributes",[0],[0]
"Metrics Throughout this paper, we measure leakage using accuracy.","3 Data, Tasks, and Protected Attributes",[0],[0]
"We say that the protected attribute has leaked if an attacker manages to predict the protected attribute with better than 50% accuracy, which is always the probability of that attribute (P (Z) = 0.5).","3 Data, Tasks, and Protected Attributes",[0],[0]
"In Appendix A we relate our metric to more standard fairness metrics, and prove that in our setup a guarded predictor guarantees demographic parity, equality of odds, and equality of opportunity.","3 Data, Tasks, and Protected Attributes",[0],[0]
Note however that we also show empirically that such guarded predictors are very hard to attain in practice.,"3 Data, Tasks, and Protected Attributes",[0],[0]
"In-dataset Accuracy Upper-bounds We begin by examining how well can we perform on each task (both main-tasks and protected attributes) when training the encoder and classifier directly on that task, without any adversarial component.",4 Baselines and Data Leakage,[0],[0]
This provides an upper bound on the protected attribute leakage for the main tasks results.,4 Baselines and Data Leakage,[0],[0]
The results in Table 1 indicate that the classifiers achieve reasonable accuracies for the main tasks.5,4 Baselines and Data Leakage,[0],[0]
"For
5While the sentiment score may seem low, we manually verified the erroneous predictions and found out that many of them are indeed ambiguous with respect to sentiment, e.g. sentences like “I can’t take Amanda seriously ” and “You make me so angry, yet you make me so happy. ”",4 Baselines and Data Leakage,[0],[0]
"which were predicted negative and positive respectively, but their gold label was the opposite.
the protected attributes, race is highly predictable (83.9%) while age and gender can also be recovered at above 64% accuracy.
",4 Baselines and Data Leakage,[0],[0]
"Leakage When training directly for the protected attributes, we can recover them with relatively high accuracies.",4 Baselines and Data Leakage,[0],[0]
But is information about them being encoded when we train on the main tasks?,4 Baselines and Data Leakage,[0],[0]
"In this set of experiments, we encode the training and validation sets using the encoder trained on the main task, and train the attacker network to predict the protected attributes based on these vectors.",4 Baselines and Data Leakage,[0],[0]
This experiment suggests an upper bound on the amount of leakage of protected attributes when we do not actively attempt to prevent it.,4 Baselines and Data Leakage,[0],[0]
The Balanced section in Table 2 summarizes the validation-set accuracies.,4 Baselines and Data Leakage,[0],[0]
"While the numbers are lower than when training directly (Table 1), they are still high enough to extract meaningful and possibly highly sensitive information (e.g. DIAL Race direct prediction is 83.9% while DIAL Race leakage on the balanced Sentiment task is 64.5%).
",4 Baselines and Data Leakage,[0],[0]
Leakage: Unbalanced Data,4 Baselines and Data Leakage,[0],[0]
The datasets we considered were perfectly balanced with respect to both main task and protected attribute labels (Figure 1a).,4 Baselines and Data Leakage,[0],[0]
"Such extreme case is not representative of real-world datasets, in which a dataset may be
well balanced w.r.t.",4 Baselines and Data Leakage,[0],[0]
the main task labels but not the protected attribute.,4 Baselines and Data Leakage,[0],[0]
"For example, when training a classifier to predict a fit for managerial position based on Curriculum Vitae (CV) of candidates, the CV dataset may be perfectly balanced according to the managerial / non-managerial variable, but, because of existing social biases, CVs of females might be under-represented in the managerial category and over-represented in the nonmanagerial one.",4 Baselines and Data Leakage,[0],[0]
"In such a situation, the classifier may perpetuate the bias by learning to favor males over females for managerial positions.",4 Baselines and Data Leakage,[0],[0]
"We simulate this more realistic scenario by constructing unbalanced datasets in which the main tasks (sentiment/mention) remain balanced but the protected class proportions within each main class are not, as demonstrated in Figure 1b.",4 Baselines and Data Leakage,[0],[0]
"For example, in the sentiment/gender case, we set the positivesentiment class to contain 80% male and 20% female tweets, while the negative-sentiment class contains 20% male and 80% female tweets.",4 Baselines and Data Leakage,[0],[0]
We then follow the leakage experiment on the unbalanced datasets.,4 Baselines and Data Leakage,[0],[0]
The attacker is trained and tested on a balanced dataset.,4 Baselines and Data Leakage,[0],[0]
"Otherwise, the attacker can perform quite well on the male/female task simply by learning to predict sentiment, which does not reflect leakage of gender data to the representation.",4 Baselines and Data Leakage,[0],[0]
"When training the attacker on balanced data, its decisions cannot rely on the sentiment information encoded in the vectors, and must look for encoded information about the protected attributes.",4 Baselines and Data Leakage,[0],[0]
"The results in Table 2 indicate that both task accuracy and attribute leakage are stronger in the unbalanced case.
",4 Baselines and Data Leakage,[0],[0]
Leakage: Real-world Example,4 Baselines and Data Leakage,[0],[0]
The above experiments used artificially constructed datasets.,4 Baselines and Data Leakage,[0],[0]
"Here, we demonstrate leakage using a popular encoder trained for emotion detection: the DeepMoji encoder (Felbo et al., 2017) trained to predict the most suitable emoji usage for a sentence (one of 64 in total), based on 1.2 billion tweets.",4 Baselines and Data Leakage,[0],[0]
"The model is advertised as a good encoder for encoding sentences into a representation that is highly predictive of sentiment, mood, emotion and sarcasm.",4 Baselines and Data Leakage,[0],[0]
Does it also capture protected attributes?,4 Baselines and Data Leakage,[0],[0]
"We encode the sentences of the different protected attributes using the DeepMoji encoder and train three different attackers to predict race, gender and age.",4 Baselines and Data Leakage,[0],[0]
"The best scores on the development set are 84.7%, 67.2% and 67.1% respectively.",4 Baselines and Data Leakage,[0],[0]
"This should not come as a surprise, as indeed some
emoji usage is highly correlated with these properties.",4 Baselines and Data Leakage,[0],[0]
Leakage of protected attributes information into the internal representation of the network when training on seemingly unrelated tasks is very common.,5 Mitigating Data Leakage,[0],[0]
We explore the means of mitigating such leakage.,5 Mitigating Data Leakage,[0],[0]
"We repeat the experiments in Table 2 with an adversarial component (Ganin and Lempitsky, 2015) as described in Section 2, in order to actively remove the protected attribute information from the encoded representation during training.",5.1 Adversarial Training,[0],[0]
"Note that the adversarial objective is in odds with the maintask one: by removing the protected attribute information from the encoder, we may also hurt its ability to encode information about the main task.
",5.1 Adversarial Training,[0],[0]
"Figure 2 shows the main task and adversary prediction accuracies on the development set as training progresses, for the Sentiment/Race pair.
",5.1 Adversarial Training,[0],[0]
"After an initial peak in task prediction accuracy, the adversary prediction drops and starts to fluctuate around chance level (50%), as desired, along with a drop in main task accuracy as well.",5.1 Adversarial Training,[0],[0]
"The adversary’s accuracy remain around chance level
throughout the entire training process, suggesting that the adversarial training is indeed effective for removing the protected attribute information from the encoded representation.",5.1 Adversarial Training,[0],[0]
"These trends are persistent for all main-task/protected-attribute pairs we tried.
",5.1 Adversarial Training,[0],[0]
"However, training the attacker network on the resulting encoder vectors reveals a different story.",5.1 Adversarial Training,[0],[0]
"For example, when considering the encoder after 50 training epochs (adversary accuracy of 49.0%), the attacker reaches 56.0% accuracy: substantially higher than the adversarial’s success rate, despite sharing the exact same architecture, and being trained and tested on the exact same dataset.
",5.1 Adversarial Training,[0],[0]
Table 3 summarizes the attacker’s recovery rate on the adversarialy-trained encoders for the different settings.,5.1 Adversarial Training,[0],[0]
"In all cases, the adversarial’s success rate is around 50%, while the attacker’s rate is substantially higher.",5.1 Adversarial Training,[0],[0]
"The attacker’s rates are lower than in Table 1, indicating the adversarial training is effective in removing some of the protected attribute.",5.1 Adversarial Training,[0],[0]
"However, a substantial amount of information managed to leak past the adversary, despite its seemingly perfect performance.
",5.1 Adversarial Training,[0],[0]
Attacker’s Accuracy on Unseen Data We verify that the attacker’s recovery accuracy persists also on the more realistic scenario in which the attacker is applied to encoded sentences that did not participate in the adversarial training.,5.1 Adversarial Training,[0],[0]
We constructed an additional dataset of 166K completely unseen samples from the Sentiment/Race case.,5.1 Adversarial Training,[0],[0]
"As
expected, the attacker works even better in this case, reaching an accuracy of 59.7% Vs.",5.1 Adversarial Training,[0],[0]
56.0% on the original development set.,5.1 Adversarial Training,[0],[0]
"We explore means of strengthening the adversarial component, by tuning its capacity and its weight, as well as by using a novel adversarial-ensemble configuration.
",5.2 Strengthening the Adversarial Component,[0],[0]
"Capacity We increase the capacity of the adversarial component by increasing its hidden dimension, while keeping the attacker’s hidden dimension constant at 300 dimensions.",5.2 Strengthening the Adversarial Component,[0],[0]
"We try hidden dimensions of size 500, 1000, 2000, 5000 and 8000.
",5.2 Strengthening the Adversarial Component,[0],[0]
"Weight We experiment with different weighting of the adversarial component during training by tuning the λ parameter, trying the values 0.5, 1.0 (default), 1.5, 2, 3, 5 (with values above 5 the main task training became extremely unstable, not raising above 50%).
",5.2 Strengthening the Adversarial Component,[0],[0]
Ensemble An alternative to using larger λ values is to introduce several adversaries.,5.2 Strengthening the Adversarial Component,[0],[0]
"The potential benefit of this approach is that rather than focusing harder on removing a single feature, here the different adversaries could each focus on a different aspect of the representation.",5.2 Strengthening the Adversarial Component,[0],[0]
This approach is potentially better suited to deal with language variability.,5.2 Strengthening the Adversarial Component,[0],[0]
"Concretely, we suggest the following adaptation to the adversarial loss to incorporate k adversaries with different random initializations:
Ly(c(h(x)), y) + k∑ j=1 Lz(advj(gλ(h(x))), z)
Other Attempts",5.2 Strengthening the Adversarial Component,[0],[0]
We also experienced with several other techniques: reinitializing the adversarial weights every t epochs; training the adversary without propagating the error to the encoder components for t epochs and only then starting to propagate; using adversaries with more hidden layers; adding dropout on the encoded vectors and within the encoder.,5.2 Strengthening the Adversarial Component,[0],[0]
"None of these yielded improvements over the above methods.
",5.2 Strengthening the Adversarial Component,[0],[0]
"Results All methods are effective to some extent, Table 4 summarizes the results.
",5.2 Strengthening the Adversarial Component,[0],[0]
"Increasing the capacity of the adversarial network helped reduce the protected attribute’s leakage, though different capacities work best on each
setup.",5.2 Strengthening the Adversarial Component,[0],[0]
"On the Sentiment/Race task, none of the higher dimensional adversaries worked better than the 300-dim one, on the PAN16 dataset it did.",5.2 Strengthening the Adversarial Component,[0],[0]
"On PAN16/Gender the 8000-dim adversary performed best, and on PAN16/Age, the 500-dim one.
",5.2 Strengthening the Adversarial Component,[0],[0]
Increasing the weight of the adversary through the λ parameter also has a positive effect on the result (except on the Sentiment/Race pair).,5.2 Strengthening the Adversarial Component,[0],[0]
"However, too large λ values make training unstable, and require many more epochs for the main-task to stabilize around a satisfying accuracy.
",5.2 Strengthening the Adversarial Component,[0],[0]
"The adversarial ensemble method with 2 adversaries achieves 57.4% on Sentiment/Race, as opposed to 56.0% with a single one, but when using 5 different adversaries, we achieve 54.8%.",5.2 Strengthening the Adversarial Component,[0],[0]
On the PAN16 dataset larger ensembles are more effective.,5.2 Strengthening the Adversarial Component,[0],[0]
"However, a potential issue with the ensemble method is that larger ensembles reduces training stability, similar to increasing the λ value.",5.2 Strengthening the Adversarial Component,[0],[0]
"For example, with 5 adversaries, the main-task accuracy remained at random for 5 epochs, and only begun rising at the 6th epoch.",5.2 Strengthening the Adversarial Component,[0],[0]
"Using 10 adversaries, the main task could not be trained.
",5.2 Strengthening the Adversarial Component,[0],[0]
"To summarize, while all methods are effective to some extent, it appears that (a) no method and parameter setting performs equally well across the different setups; and (b) no method succeeds in completely preventing the leakage of the protected attributes.",5.2 Strengthening the Adversarial Component,[0],[0]
"Combining the different methods (ensembles of larger networks, larger networks with larger λ, etc.) did not improve the results.
",5.2 Strengthening the Adversarial Component,[0],[0]
Unbalanced Data Results We repeated the same set of experiments on the unbalanced Sentiment/Race corpus (Table 5).,5.2 Strengthening the Adversarial Component,[0],[0]
"In this setup, the results are somewhat similar: increasing the adversarial capacity and λ is ineffective, and even increases the attacker’s recovery rate.",5.2 Strengthening the Adversarial Component,[0],[0]
"However, using an ensemble of 5 adversaries does manage to reduce the leakage, but it is still far from a satisfying result.",5.2 Strengthening the Adversarial Component,[0],[0]
The gap between the adversary’s dev-set accuracy and the after-the-fact attacker accuracy on the same data is surprising.,6 Analysis,[0],[0]
"To better understand the phenomenon, we perform further analysis on the Sentiment/Race pair with the default single adversary.
",6 Analysis,[0],[0]
Embedding Vs.,6 Analysis,[0],[0]
"RNN Recall that the attacker network tries to extract as much information from
the encoder’s output as possible.",6 Analysis,[0],[0]
The encoder consists of two components: (1) Embedding Matrix and (2) an RNN.,6 Analysis,[0],[0]
"Therefore, the leakage can be caused due to one of them (or due to their combination).
",6 Analysis,[0],[0]
"We conduct the following experiment to determine which part affects the leakage more: we create a new encoder by composing 2 existing encoders: an encoder with high leakage (Leaky, using the baseline encoder) and an encoder with low leakage (Guarded, using the 5-Ensemble adversary).",6 Analysis,[0],[0]
"We fuse the two encoders by combining the embedding matrix of the Leaky encoder with
the RNN module of the Guarded encoder, and vice versa.",6 Analysis,[0],[0]
"This yields two new encoders: an encoder with a “leaky” Embedding Matrix module and a “strong” RNN module (Leaky-EMB), and an encoder with a “strong” Embedding Matrix module and a “leaky” RNN module (Leaky-RNN).",6 Analysis,[0],[0]
We compare encoders Leaky-EMB and Leaky-RNN to gauge which module has a greater contribution to the data leakage.,6 Analysis,[0],[0]
"We train attacker-networks over the encoders’ output to predict the protected attributes.
",6 Analysis,[0],[0]
"Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix.6
6A discrepancy exists to some extent in the new encoders, as their parts originate from different models that were trained separately.",6 Analysis,[0],[0]
"To test if the fusion is valid, we train a different classifier on top of the new encoders to predict the main task.",6 Analysis,[0],[0]
"The combination of the leaked RNN with the guarded embeddings results in 65.4% on the sentiment task and the other combination results in 60.9% as opposed to 67.5% and 63.8% on the leaked and guarded models, respectively.",6 Analysis,[0],[0]
"As the new models are on par with the original ones, we conclude that the new encoders are valid.
",6 Analysis,[0],[0]
Consistent Leakage: Examples Inspection We are interested in tweets whose protected attribute (race) is correctly predicted by the adversary.,6 Analysis,[0],[0]
"However, at accuracy rates below 60%, many of the correct predictions could be attributed to chance.",6 Analysis,[0],[0]
"To identify the relevant examples, we repeated the Sentiment/Race default adversary experiment 10 times with different random seeds.",6 Analysis,[0],[0]
"We then trained 10 attacker networks, and used each of them to label all examples in the development set.",6 Analysis,[0],[0]
We then looked for tweets which are consistently and correctly classified by at least 9 attackers.7 Table 7 shows some of these cases.,6 Analysis,[0],[0]
"Many of them include tokens (Naw, Bestfrand, tan) and syntactic structures (Going over Bae house) which are indeed predictive, though not the most salient features.
",6 Analysis,[0],[0]
"Leakage via Embeddings Even though we found out the RNN is much more responsible to the leakage then the Embedding, those still contribute to the leakage and are easier to inspect.",6 Analysis,[0],[0]
"Therefore, we turn to inspect the encoders’ Embedding.",6 Analysis,[0],[0]
We hypothesize that a possible reason for the adversarial network’s inability to completely remove the protected race information is word frequency.,6 Analysis,[0],[0]
"Namely, rare words, which might be strongly identified with one group, didn’t get enough updates during training and therefore remained predictive towards one of the groups.",6 Analysis,[0],[0]
"To quantify this, we compared two vocabularies: words appearing in tweets where the predictions were consistently predicted (9 or 10 out of 10 times) by the different attackers, and words appearing in tweets that were randomly distributed (50%) between the attackers.",6 Analysis,[0],[0]
"If our hypothesis is correct, we expect words from the second group to be more frequent than words in the first group.",6 Analysis,[0],[0]
"We discard words appearing in both groups, and associate each word with its training set frequency.",6 Analysis,[0],[0]
"One-tailed Mann-Whitney U test (Mann and Whitney, 1947) showed the effect is highly significant with p < e−12.
Data Overfitting?",6 Analysis,[0],[0]
"Standard ML setups often suffer from overfitting on the training data, especially when using neural-networks which tend to memorize the data they encounter.",6 Analysis,[0],[0]
"In the adversarial setup, the overfitting could result in the encoder-adversary pair working together to perfectly clean the attributes from the training data,
7776 correct and 946 consistent examples in total
without generalization.",6 Analysis,[0],[0]
Such overfitting could explain the attacker success.,6 Analysis,[0],[0]
Is this what happened?,6 Analysis,[0],[0]
We test this hypothesis by using the same attacker networks experiments solely on the training data.,6 Analysis,[0],[0]
We train the attackers on 90% of the training data while using the rest 10% as heldout.,6 Analysis,[0],[0]
"If overfitting has occurred, the accuracy is likely to result in 50% accuracy.",6 Analysis,[0],[0]
"Alas, this is not the case.",6 Analysis,[0],[0]
Table 8 summarize the training accuracies of the attacker network.,6 Analysis,[0],[0]
The Mention/Race task achieves the highest score of 64.3% whereas the Mention/Gender task achieves the lowest - 58.1%.,6 Analysis,[0],[0]
"Even though when trained directly to predict these attributes without the adversarial setup, the training accuracies are much higher, a substantial amount of signal is still left, even in the training data.",6 Analysis,[0],[0]
"The fact that intermediary vector representations that are trained for one task are predictive of another is not surprising: it is at the core of the success of NLP methods for deriving “generic” word and sentence representations (e.g. Word2vec (Mikolov et al., 2013), Skipthought vectors (Kiros et al., 2015), Contextualized Word Representations (Melamud et al., 2016; Peters et al., 2018) etc.).",7 Related Work,[0],[0]
"While usually considered a positive feature, it can often have undesired consequences one should be aware of and potentially control for.",7 Related Work,[0],[0]
"Several works document biases and stereotypes that are captured by unsupervised word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) and ways of mitigating them (Bolukbasi et al., 2016; Zhang et al., 2018).",7 Related Work,[0],[0]
"Bias and stereotyping were also documented on a common NLP dataset (Rudinger et al., 2017).",7 Related Work,[0],[0]
"While these work are concerned with the learned representations encoding unwanted biases about the world, our concern is with capturing potentially sensitive demographic information about individual authors of the text.
",7 Related Work,[0],[0]
"Removing sensitive attributes (demographic or otherwise) from intermediate representations in order to achieve fair classification has been explored by solving an optimization problem (Zemel et al., 2013), as well as by employing adversarial training (Edwards and Storkey, 2015; Louizos et al., 2015; Xie et al., 2017; Zhang et al., 2018), focusing on structured features.",7 Related Work,[0],[0]
"Adversarial training was also applied for Image anonymization
(Edwards and Storkey, 2015; Feutry et al., 2018).",7 Related Work,[0],[0]
"In contrast, we consider features that are based on short user-authored text.
",7 Related Work,[0],[0]
"Several works apply adversarial training to textual data, in order to learn encoders that are invariant to some properties of the text (Chen et al., 2016; Conneau et al., 2017; Zhang et al., 2017; Xie et al., 2017).",7 Related Work,[0],[0]
"As their main motivation is to remove information about domain or language in order to improve transfer learning, domain adaptation, or end task accuracy, they were less concerned with the ability to recover information from the resulting representation, and did not evaluate it directly as we do here.
",7 Related Work,[0],[0]
"Recent work on creating private representation in the text domain (Li et al., 2018) share our motivation of removing unintended demographic attributes from the learned representation using adversarial training.",7 Related Work,[0],[0]
"However, they report only the discrimination accuracies of the adversarial component, and do not train another classifier to verify that the representations are indeed clear of the protected attribute.",7 Related Work,[0],[0]
"As our work shows, trusting the adversary is insufficient, and external verification is crucial.
",7 Related Work,[0],[0]
"Finally, our work is motivated by the desire for fairness.",7 Related Work,[0],[0]
"We use a definition in which a fair classification is one that does not condition on a certain
attribute (fairness by blindness), and evaluate the ability to achieve text-derived representations that are blind to a property we wish to protect.",7 Related Work,[0],[0]
"Many other definitions of fairness exist, including demographic parity, equality of odds and equality of opportunity (see e.g. discussion in (Hardt et al., 2016; Beutel et al., 2017)).",7 Related Work,[0],[0]
"Under our setup, blindness guarantees these metrics (Appendix A).",7 Related Work,[0],[0]
We show that demographic information leaks into intermediate representations of neural networks trained on text data.,8 Conclusions,[0],[0]
Systems that train on text data and do not want to condition on demographic information must take active steps against accidental conditioning.,8 Conclusions,[0],[0]
"Our experiments suggest that: (1) Adversarial training is effective for mitigating protected attribute leakage, but, when dealing with text data, may fail to remove it completely.",8 Conclusions,[0],[0]
"(2) When using the adversarial training method, the adversary score during training cannot be trusted, and must be verified with an externallytrained attacker, preferably on unseen data.",8 Conclusions,[0],[0]
"(3) Tuning the capacity and weight of the adversary, as well as using an ensemble of several adversaries, can improve the results.",8 Conclusions,[0],[0]
"However, no single method is the most effective in all cases.",8 Conclusions,[0],[0]
"We would like to thank Moni Shahar, Felix Kreuk, Yova Kementchedjhieva and the BIU NLP lab for fruitful conversation and helpful comments.",Acknowledgments,[0],[0]
We also thank Su Lin Blodgett for her help in supplying the DIAL dataset and clarifications.,Acknowledgments,[0],[0]
"This work was supported in part by the The Israeli Science Foundation (grant number 1555/15) and German Research Foundation via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).",Acknowledgments,[0],[0]
Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation.,abstractText,[0],[0]
We show that demographic information of authors is encoded in—and can be recovered from—the intermediate representations learned by text-based neural classifiers.,abstractText,[0],[0]
The implication is that decisions of classifiers trained on textual data are not agnostic to—and likely condition on—demographic attributes.,abstractText,[0],[0]
"When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data.",abstractText,[0],[0]
"This behavior is consistent across several tasks, demographic properties and datasets.",abstractText,[0],[0]
We explore several techniques to improve the effectiveness of the adversarial component.,abstractText,[0],[0]
Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.,abstractText,[0],[0]
Adversarial Removal of Demographic Attributes from Text Data,title,[0],[0]
"Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing (Zhang et al., 2017), POS tagging (Lample et al., 2016), relation extraction (dos Santos et al., 2015), translation (Bahdanau et al., 2015), and joint tasks (Miwa and Bansal, 2016).",1 Introduction,[0],[0]
"However, Szegedy et al. (2014) observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence).",1 Introduction,[0],[0]
Goodfellow et al. (2015) proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model.,1 Introduction,[0],[0]
"Although AT has recently been applied in NLP tasks (e.g., text classification (Miyato et al., 2017)), this paper — to the best of our knowledge — is the first attempt investigating regularization effects of AT in a joint setting for two related tasks.
",1 Introduction,[0],[0]
We start from a baseline joint model that performs the tasks of named entity recognition (NER) and relation extraction at once.,1 Introduction,[0],[0]
"Previously proposed models (summarized in Section 2) exhibit
several issues that the neural network-based baseline approach (detailed in Section 3.1) overcomes: (i)",1 Introduction,[0],[0]
"our model uses automatically extracted features without the need of external parsers nor manually extracted features (see Gupta et al. (2016); Miwa and Bansal (2016); Li et al. (2017)), (ii) all entities and the corresponding relations within the sentence are extracted at once, instead of examining one pair of entities at a time (see Adel and Schütze (2017)), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see Katiyar and Cardie (2017); Bekoulis et al. (2018a)).",1 Introduction,[0],[0]
"The core contribution of the paper is the use of AT as an extension in the training procedure for the joint extraction task (Section 3.2).
",1 Introduction,[0],[0]
"To evaluate the proposed AT method, we perform a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch).",1 Introduction,[0],[0]
"We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5).",1 Introduction,[0],[0]
"Compared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness.",1 Introduction,[0],[0]
"Joint entity and relation extraction: Joint models (Li and Ji, 2014; Miwa and Sasaki, 2014) that are based on manually extracted features have been proposed for performing both the named entity recognition (NER) and relation extraction subtasks at once.",2 Related work,[0],[0]
"These methods rely on the availability of NLP tools (e.g., POS taggers) or manually designed features leading to additional complexity.",2 Related work,[0],[0]
"Neural network methods have been exploited to overcome this feature design issue and usually involve RNNs and CNNs (Miwa and Bansal,
2016; Zheng et al., 2017).",2 Related work,[0],[0]
"Specifically, Miwa and Bansal (2016) as well as Li et al. (2017) apply bidirectional tree-structured RNNs for different contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers).",2 Related work,[0],[0]
Gupta et al. (2016) propose the use of various manually extracted features along with RNNs.,2 Related work,[0],[0]
"Adel and Schütze (2017) solve the simpler problem of entity classification (EC, assuming entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer.",2 Related work,[0],[0]
Katiyar and Cardie (2017) investigate RNNs with attention without taking into account that relation labels are not mutually exclusive.,2 Related work,[0],[0]
"Finally, Bekoulis et al. (2018a) use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part.",2 Related work,[0],[0]
Our baseline model enables simultaneous extraction of multiple relations from the same input.,2 Related work,[0],[0]
"Then, we further extend this strong baseline using adversarial training.
",2 Related work,[0],[0]
"Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of image recognition.",2 Related work,[0],[0]
"In the context of NLP, several variants have been proposed for different tasks such as text classification (Miyato et al., 2017), relation extraction (Wu et al., 2017) and POS tagging (Yasunaga et al., 2018).",2 Related work,[0],[0]
AT is considered as a regularization method.,2 Related work,[0],[0]
"Unlike other regularization methods (i.e., dropout (Srivastava et al., 2014), word dropout (Iyyer et al., 2015)) that introduce random noise, AT generates perturbations that are variations of examples easily misclassified by the model.",2 Related work,[0],[0]
"The baseline model, described in detail in Bekoulis et al. (2018b), is illustrated in Fig. 1.",3.1 Joint learning as head selection,[0],[0]
It aims to detect (i) the type and the boundaries of the entities and (ii) the relations between them.,3.1 Joint learning as head selection,[0],[0]
"The input is a sequence of tokens (i.e., sentence)",3.1 Joint learning as head selection,[0],[0]
"w = w1, ..., wn.",3.1 Joint learning as head selection,[0],[0]
"We use character level embeddings to implicitly capture morphological features (e.g., prefixes and suffixes), representing each character by a vector (embedding).",3.1 Joint learning as head selection,[0],[0]
The character embeddings are fed to a bidirectional LSTM (BiLSTM) to obtain the character-based representation of the word.,3.1 Joint learning as head selection,[0],[0]
"We also use pre-trained word embeddings.
",3.1 Joint learning as head selection,[0],[0]
"Word and character embeddings are concatenated to form the final token representation, which is then fed to a BiLSTM layer to extract sequential information.
",3.1 Joint learning as head selection,[0],[0]
"For the NER task, we adopt the BIO (Beginning, Inside, Outside) encoding scheme.",3.1 Joint learning as head selection,[0],[0]
"In Fig. 1, the B-PER tag is assigned to the beginning token of a ‘person’ (PER) entity.",3.1 Joint learning as head selection,[0],[0]
"For the prediction of the entity tags, we use: (i) a softmax approach for the entity classification (EC) task (assuming entity boundaries given) or (ii) a CRF approach where we identify both the type and the boundaries for each entity.",3.1 Joint learning as head selection,[0],[0]
"During decoding, in the softmax setting, we greedily detect the entity types of the tokens (i.e., independent prediction).",3.1 Joint learning as head selection,[0],[0]
"Although independent distribution of types is reasonable for EC tasks, this is not the case when there are strong correlations between neighboring tags.",3.1 Joint learning as head selection,[0],[0]
"For instance, the BIO encoding scheme imposes several constraints in the NER task (e.g., the B-PER and ILOC tags cannot be sequential).",3.1 Joint learning as head selection,[0],[0]
"Motivated by this intuition, we use a linear-chain CRF for the NER task (Lample et al., 2016).",3.1 Joint learning as head selection,[0],[0]
"For decoding, in the CRF setting, we use the Viterbi algorithm.",3.1 Joint learning as head selection,[0],[0]
"During training, for both EC (softmax) and NER tasks (CRF), we minimize the cross-entropy loss LNER.",3.1 Joint learning as head selection,[0],[0]
"The entity tags are later fed into the relation extraction layer as label embeddings (see Fig. 1), assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities.
",3.1 Joint learning as head selection,[0],[0]
"We model the relation extraction task as a multi-label head selection problem (Bekoulis
et al., 2018b; Zhang et al., 2017).",3.1 Joint learning as head selection,[0],[0]
"In our model, each word wi can be involved in multiple relations with other words.",3.1 Joint learning as head selection,[0],[0]
"For instance, in the example illustrated in Fig. 1, “Smith” could be involved not only in a Lives in relation with the token “California” (head) but also in other relations simultaneously (e.g., Works for, Born In with some corresponding tokens).",3.1 Joint learning as head selection,[0],[0]
"The goal of the task is to predict for each word wi, a vector of heads ŷi and the vector of corresponding relations r̂i.",3.1 Joint learning as head selection,[0],[0]
"We compute the score s(wj , wi, rk) of word wj to be the head of wi given a relation label rk using a single layer neural network.",3.1 Joint learning as head selection,[0],[0]
"The corresponding probability is defined as: P(wj , rk | wi; θ) =",3.1 Joint learning as head selection,[0],[0]
"σ(s(wj , wi, rk)), where σ(.) is the sigmoid function.",3.1 Joint learning as head selection,[0],[0]
"During training, we minimize the cross-entropy loss Lrel as:
n∑ i=0 m∑ j=0",3.1 Joint learning as head selection,[0],[0]
"− logP(yi,j , ri,j | wi; θ) (1)
where m is the number of associated heads (and thus relations) per word wi.",3.1 Joint learning as head selection,[0],[0]
"During decoding, the most probable heads and relations are selected using threshold-based prediction.",3.1 Joint learning as head selection,[0],[0]
The final objective for the joint task is computed as LJOINT(w; θ) = LNER + Lrel where θ is a set of parameters.,3.1 Joint learning as head selection,[0],[0]
"In the case of multi-token entities, only the last token of the entity can serve as head of another token, to eliminate redundant relations.",3.1 Joint learning as head selection,[0],[0]
"If an entity is not involved in any relation, we predict the auxiliary “N” relation label and the token itself as head.",3.1 Joint learning as head selection,[0],[0]
"We exploit the idea of AT (Goodfellow et al., 2015) as a regularization method to make our model robust to input perturbations.",3.2 Adversarial training (AT),[0],[0]
"Specifically, we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation (Miyato et al., 2017).",3.2 Adversarial training (AT),[0],[0]
This is similar to the concept introduced by Goodfellow et al. (2015) to improve the robustness of image recognition classifiers.,3.2 Adversarial training (AT),[0],[0]
"We generate an adversarial example by adding the worst-case perturbation ηadv to the original embedding w that maximizes the loss function:
ηadv = argmax ‖η‖≤
LJOINT(w + η; θ̂) (2)
where θ̂ is a copy of the current model parameters.",3.2 Adversarial training (AT),[0],[0]
Since Eq.,3.2 Adversarial training (AT),[0],[0]
"(2) is intractable in neural networks, we use the approximation proposed in Goodfellow et al. (2015) defined as: ηadv = g/ ‖g‖ , with g =
∇wLJOINT(w; θ̂), where is a small bounded norm treated as a hyperparameter.",3.2 Adversarial training (AT),[0],[0]
"Similar to Yasunaga et al. (2018), we set to be α √ D (where D is the dimension of the embeddings).",3.2 Adversarial training (AT),[0],[0]
"We train on the mixture of original and adversarial examples, so the final loss is computed as: LJOINT(w; θ̂)",3.2 Adversarial training (AT),[0],[0]
+ LJOINT(w + ηadv; θ̂).,3.2 Adversarial training (AT),[0],[0]
"We evaluate our models on four datasets, using the code as available from our github codebase.1 Specifically, we follow the 5-fold crossvalidation defined by Miwa and Bansal (2016) for the ACE04 (Doddington et al., 2004) dataset.",4 Experimental setup,[0],[0]
"For the CoNLL04 (Roth and Yih, 2004) EC task (assuming boundaries are given), we use the same splits as in Gupta et al. (2016); Adel and Schütze (2017).",4 Experimental setup,[0],[0]
We also evaluate our models on the NER task similar to Miwa and Sasaki (2014) in the same dataset using 10-fold cross validation.,4 Experimental setup,[0],[0]
"For the Dutch Real Estate Classifieds, DREC (Bekoulis et al., 2017) dataset, we use train-test splits as in Bekoulis et al. (2018a).",4 Experimental setup,[0],[0]
"For the Adverse Drug Events, ADE (Gurulingappa et al., 2012), we perform 10-fold cross-validation similar to Li et al. (2017).",4 Experimental setup,[0],[0]
"To obtain comparable results that are not affected by the input embeddings, we use the embeddings of the previous works.",4 Experimental setup,[0],[0]
We employ early stopping in all of the experiments.,4 Experimental setup,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2015) and we fix the hyperparameters (i.e., α, dropout values, best epoch, learning rate) on the validation sets.",4 Experimental setup,[0],[0]
"The scaling parameter α is selected from {5e−2, 1e−2, 1e−3, 1e−4}.",4 Experimental setup,[0],[0]
"Larger values of α (i.e., larger perturbations) lead to consistent performance decrease in our early experiments.",4 Experimental setup,[0],[0]
"This can be explained from the fact that adding more noise can change the content of the sentence as also reported by Wu et al. (2017).
",4 Experimental setup,[0],[0]
"We use three types of evaluation, namely: (i) S(trict): we score an entity as correct if both the entity boundaries and the entity type are correct (ACE04, ADE, CoNLL04, DREC), (ii) B(oundaries): we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account (DREC) and (iii) R(elaxed): a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the
1https://github.com/bekou/multihead_",4 Experimental setup,[0],[0]
"joint_entity_relation_extraction
boundaries are known (CoNLL04), to compare to previous works.",4 Experimental setup,[0],[0]
"In all cases, a relation is considered as correct when both the relation type and the argument entities are correct.",4 Experimental setup,[0],[0]
Table 1 shows our experimental results.,5 Results,[0],[0]
The name of the dataset is presented in the first column while the models are listed in the second column.,5 Results,[0],[0]
"The proposed models are the following: (i) baseline: the baseline model shown in Fig. 1 with the CRF layer and the sigmoid loss, (ii) baseline EC: the proposed model with the softmax layer for EC, (iii) baseline (EC) + AT: the baseline regularized using AT.",5 Results,[0],[0]
The final three columns present the F1 results for the two subtasks and their average performance.,5 Results,[0],[0]
"Bold values indicate the best results among models that use only automatically extracted features.
",5 Results,[0],[0]
"For ACE04, the baseline outperforms Katiyar and Cardie (2017) by ∼2% in both tasks.",5 Results,[0],[0]
"This improvement can be explained by the use of: (i) multi-label head selection, (ii) CRF-layer and (iii) character level embeddings.",5 Results,[0],[0]
"Compared to Miwa and Bansal (2016), who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task.",5 Results,[0],[0]
"On the other hand, Li et al. (2017) use the same model for
the ADE biomedical dataset, where we report a 2.5% overall improvement.",5 Results,[0],[0]
This indicates that NLP tools are not always accurate for various contexts.,5 Results,[0],[0]
"For the CoNLL04 dataset, we use two evaluation settings.",5 Results,[0],[0]
We use the relaxed evaluation similar to Gupta et al. (2016); Adel and Schütze (2017) on the EC task.,5 Results,[0],[0]
"The baseline model outperforms the state-of-the-art models that do not rely on manually extracted features (>4% improvement for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities.",5 Results,[0],[0]
"Moreover, compared to the model of Gupta et al. (2016) that relies on complex features, the baseline model performs within a margin of 1% in terms of overall F1 score.",5 Results,[0],[0]
"We also report NER results on the same dataset and improve overall F1 score with∼1% compared to Miwa and Sasaki (2014), indicating that our automatically extracted features are more informative than the hand-crafted ones.",5 Results,[0],[0]
These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model.,5 Results,[0],[0]
"For the DREC dataset, we use two evaluation methods.",5 Results,[0],[0]
"In the boundaries evaluation, the baseline has an improvement of ∼3% on both tasks compared to Bekoulis et al. (2018a), whose quadratic scoring layer complicates NER.
",5 Results,[0],[0]
Table 1 and Fig. 2 show the effectiveness of the adversarial training on top of the baseline model.,5 Results,[0],[0]
"In all of the experiments, AT improves the predictive performance of the baseline model in the joint setting.",5 Results,[0],[0]
"Moreover, as seen in Fig. 2, the performance of the models using AT is closer to maximum even from the early training epochs.",5 Results,[0],[0]
"Specifically, for ACE04, there is an improvement in both tasks as well as in the overall F1 performance (0.4%).",5 Results,[0],[0]
"For CoNLL04, we note an improvement in the overall F1 of 0.4% for the EC and 0.8% for the NER tasks, respectively.",5 Results,[0],[0]
"For the DREC dataset, in both settings, there is an overall improvement of ∼1%.",5 Results,[0],[0]
"Figure 2 shows that from the first epochs, the model obtains its maximum performance on the DREC validation set.",5 Results,[0],[0]
"Finally, for ADE, our AT model beats the baseline F1 by 0.7%.
",5 Results,[0],[0]
"Our results demonstrate that AT outperforms the neural baseline model consistently, considering our experiments across multiple and more diverse datasets than typical related works.",5 Results,[0],[0]
"The im-
provement of AT over our baseline (depending on the dataset) ranges from∼0.4% to∼0.9% in terms of overall F1 score.",5 Results,[0],[0]
"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component, which is in accordance with the recent advances in NER using neural networks that report similarly small gains (e.g., the performance improvement in Ma and Hovy (2016) and Lample et al. (2016) on the CoNLL-2003 test set is 0.01% and 0.17% F1 percentage points, while in the work of Yasunaga et al. (2018), a 0.07% F1 improvement on CoNLL2000 using AT for NER is reported).",5 Results,[0],[0]
"However, the relation extraction performance increases by∼1% F1 scoring points, except for the ACE04 dataset.",5 Results,[0],[0]
"Further, as seen in Fig. 2, the improvement for CoNLL04 is particularly small on the evaluation set.",5 Results,[0],[0]
"This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models, but this needs further investigation in future work.",5 Results,[0],[0]
We proposed to use adversarial training (AT) for the joint task of entity recognition and relation extraction.,6 Conclusion,[0],[0]
"The contribution of this study is twofold: (i) investigation of the consistent effectiveness of AT as a regularization method over a multi-context baseline joint model, with (ii) a large scale experimental evaluation.",6 Conclusion,[0],[0]
"Experiments show that
AT improves the results for each task separately, as well as the overall performance of the baseline joint model, while reaching high performance already during the first epochs of the training procedure.",6 Conclusion,[0],[0]
"We would like to thank the anonymous reviewers for the time and effort they spent in reviewing our work, and for their valuable feedback.",Acknowledgments,[0],[0]
Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data.,abstractText,[0],[0]
We show how to use AT for the tasks of entity recognition and relation extraction.,abstractText,[0],[0]
"In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).",abstractText,[0],[0]
Adversarial training for multi-context joint entity and relation extraction,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 633–639 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
633",text,[0],[0]
"In natural language processing fields, full neural network based methods are suitable for joint modeling as they can simultaneously utilize multiple task data sets or multiple language data sets to improve the performance achieved for individual tasks or languages (Collobert and Weston, 2008).",1 Introduction,[0],[0]
"It is known that joint modeling can address the data scarcity problem.
",1 Introduction,[0],[0]
"Key natural language processing technologies for spoken dialogue systems include utterance intent classification, which is needed to detect intent labels such as dialogue act (Stolcke et al., 2000; Khanpour et al., 2016), domain (Xu and Sarikaya, 2014), and question type (Wu et al., 2005) from input utterances (Ravuri and Stolcke,
2015a,b, 2016).",1 Introduction,[0],[0]
One problem is that the training data are often limited or unbalanced among different tasks or different languages.,1 Introduction,[0],[0]
"Therefore, our motivation is to leverage both multi-task joint modeling and multi-lingual joint modeling to enhance utterance intent classification.
",1 Introduction,[0],[0]
"The multi-task and multi-lingual joint modeling can be composed by introducing both task-specific networks, which are shared among different languages, and language-specific networks, which are shared among different tasks (Masumura et al., 2018; Lin et al., 2018).",1 Introduction,[0],[0]
"Although joint modeling is mainly intended to improve classification performance in resource-poor tasks or languages, its classification performance is degraded in some minor data sets.",1 Introduction,[0],[0]
"This is because the languagespecific networks often depend on majority tasks, while the task-specific networks often depend on majority languages.",1 Introduction,[0],[0]
"What are needed are taskspecific networks that are invariant to languages, and language-specific networks that are invariant to tasks.
",1 Introduction,[0],[0]
"In order to explicitly improve the invariance of language and task-specific networks, this paper introduces adversarial training (Goodfellow et al., 2014).",1 Introduction,[0],[0]
"Our idea is to train language-specific networks so as to be insensitive to the target task, while training task-specific networks to be insensitive to language.",1 Introduction,[0],[0]
"To this end, we introduce multiple domain adversarial networks (Ganin et al., 2016), language-specific task adversarial networks, and task-specific language adversarial networks, into a state-of-the-art fully neural network based joint modeling; we adopt the bidirectional long short-term memory recurrent neural networks (BLSTM-RNNs) with attention mechanism (Yang et al., 2016; Zhou et al., 2016).",1 Introduction,[0],[0]
"To the best of our knowledge, this paper is the first study to employ adversarial training for multi-input and multi-output joint modeling.
",1 Introduction,[0],[0]
Experiments on Japanese and English data sets demonstrate the effectiveness of the adversarial training proposal.,1 Introduction,[0],[0]
"To support spoken dialogue systems, three different utterance intent classification tasks are examined: dialogue act, topic type, and question type classification.",1 Introduction,[0],[0]
"Joint Modeling: In natural language processing research, joint modeling is usually split into multitask joint modeling and multi-lingual joint modeling.",2 Related Work,[0],[0]
"Multi-task joint modeling has been shown to effectively improve individual tasks (Collobert and Weston, 2008; Liu et al., 2016a,b; Zhang and Weng, 2016; Liu et al., 2016c).",2 Related Work,[0],[0]
"In addition, multi-lingual joint modeling is achieved by learning common semantic representations among different languages (Guo et al., 2016; Duong et al., 2016; Zhang et al., 2016, 2017b).",2 Related Work,[0],[0]
"In addition, a few work have examined multi-task and multilingual joint modeling (Masumura et al., 2018; Lin et al., 2018).",2 Related Work,[0],[0]
"Different from the previous work, our novelty is to introduce adversarial training for multi-task and multi-lingual joint modeling.",2 Related Work,[0],[0]
"Adversarial Training: The concept of adversarial training was first proposed by Goodfellow et al. (2014), and many studies in the machine learning field have focused on adversarial training.",2 Related Work,[0],[0]
"Adversarial training has been well utilized in text classification (Ganin et al., 2016; Chen et al., 2016; Liu et al., 2017; Miyato et al., 2017; Chen and Cardie, 2018).",2 Related Work,[0],[0]
"Most natural language processing papers adopted either the language invariant approach (Chen et al., 2016; Zhang et al., 2017a) or the task invariant approach (Ganin et al., 2016; Liu et al., 2017; Chen and Cardie, 2018).",2 Related Work,[0],[0]
This paper aims to fully utilize both task adversarial training and language adversarial training.,2 Related Work,[0],[0]
"To this end, we simultaneously introduce language-specific task adversarial networks and task-specific language adversarial networks.",2 Related Work,[0],[0]
"This section details our adversarial training method for multi-task and multi-lingual joint modeling of utterance intent classification.
",3 Proposed Method,[0],[0]
"In the j-th task utterance intent classification for the i-th language input utterance, intent label l(j) ∈ {1, · · · ,K(j)} is estimated from input utterance W(i) = {w(i)1 , · · · , w (i) T }",3 Proposed Method,[0],[0]
"where i ∈ {1, · · · , I} and j ∈ {1, · · · , J}.",3 Proposed Method,[0],[0]
"Utter-
ance intent classification is followed by estimation of the probabilities of each intent label given input utterance, P (l(j)|W(i),Θ(i,j)) where Θ(i,j) is the trainable model parameter for the combination of the i-th language and the j-th task.",3 Proposed Method,[0],[0]
"In multi-task and multi-lingual joint modeling, {Θ(1,1), · · · ,Θ(I,J)} are jointly trained from I language and J task data sets.",3 Proposed Method,[0],[0]
"The proposed method is founded on a fully neural network that employs I language-specific networks, J task-specific networks, and J classification networks as well as Masumura et al. (2018).
",3.1 Main Joint Network,[0],[0]
"The language-specific network can be shared between multiple tasks, where words in the input utterance are converted into language-specific hidden representations.",3.1 Main Joint Network,[0],[0]
Each word in the i-th language input utterance W(i) is first converted into a continuous representation.,3.1 Main Joint Network,[0],[0]
"Next, each word representation is converted into a hidden representation that uses BLSTM-RNNs to take neighboring word context information into account.",3.1 Main Joint Network,[0],[0]
"The t-th language-specific hidden representation for the ith language is given by:
w (i) t = EMBED(w (i) t ;θ (i) h ), (1)
h (i) t",3.1 Main Joint Network,[0],[0]
"= BLSTM({w (i) 1 , · · · ,w (i) T }, t;θ (i) h ), (2)
where EMBED() is a linear transformational function for word embedding, BLSTM() is a function of the BLSTM-RNN layer, and θ(i)h is the trainable parameter for the i-th language-specific network.
",3.1 Main Joint Network,[0],[0]
"In addition, task-specific networks can be shared between multiple languages, where the language-specific hidden representations are converted into task-specific hidden representations.",3.1 Main Joint Network,[0],[0]
"The t-th language-specific hidden representation for the j-th task is given by:
u (j) t = BLSTM({h (i) 1 , · · · ,h (i) T }, t;θ (j) u ), (3)
where θ(j)u is the trainable parameter for the j-th task-specific network.
",3.1 Main Joint Network,[0],[0]
"In classification networks for each task, the task-specific hidden representations are summarized as sentence representation s(j) by using a self-attention mechanism that can consider the importance of individual hidden representations (Yang et al., 2016; Zhou et al., 2016; Sawada et al., 2017).",3.1 Main Joint Network,[0],[0]
"Next, predicted probabilities of the j-th
task intent labels, o(j) ∈ RK(j) , are given by:
s(j) = ATTENSUM({h(i)1 , · · · ,h (i) T };θ (j) o ), (4) o(j) = SOFTMAX(s(j);θ(j)o ), (5)
where ATTENSUM() is a weighted sum function with self-attention, SOFTMAX() is a transformational function with softmax activation, and θ(j)o is the trainable parameter for the j-th classification network.",3.1 Main Joint Network,[0],[0]
"In the main joint networks of the proposal, Θ(i,j) corresponds to {θ(i)h , θ (j) u ,θ (j) o }.",3.1 Main Joint Network,[0],[0]
The proposed method combines a languagespecific task adversarial network with a taskspecific language adversarial network.,3.2 Adversarial Networks,[0],[0]
"The task adversarial network is used for training the language-specific networks to be insensitive to target task labels, and the language adversarial network is used for training the task-specific networks to be insensitive to target language labels.",3.2 Adversarial Networks,[0],[0]
"In order to efficiently use stochastic gradient descent based training for optimizing the adversarial networks, we use gradient reversal layers, which allow the input vectors during forward propagation, and sign inversion of the gradients during back propagation, to be utilized (Ganin et al., 2016).
",3.2 Adversarial Networks,[0],[0]
The i-th language-specific task adversarial network estimates task labels from the i-th languagespecific hidden representations.,3.2 Adversarial Networks,[0],[0]
"The predicted probabilities of task labels, x(i) ∈ RJ , are given by:
h̃ (i) t = GRL(h (i) t ), (6) h̃(i) =",3.2 Adversarial Networks,[0],[0]
"ATTENSUM({h̃(i)1 , · · · , h̃ (i) T };θ (i) x ), (7) x(i) = SOFTMAX(h̃(i),θ(i)x ), (8)
where GRL() represents the gradient reversal layer, and θ(i)x is the trainable parameter.",3.2 Adversarial Networks,[0],[0]
The j-th taskspecific language adversarial network estimates language labels from the j-th task-specific hidden representations.,3.2 Adversarial Networks,[0],[0]
"The predicted probabilities of language labels, y(j) ∈ RI , are given by:
ũ (j) t = GRL(u",3.2 Adversarial Networks,[0],[0]
"(j) t ), (9) ũ(j)",3.2 Adversarial Networks,[0],[0]
=,3.2 Adversarial Networks,[0],[0]
"ATTENSUM({ũ(j)1 , · · · , ũ (j) T };θ (j) y ), (10) y(j) = SOFTMAX(ũ(j),θ(j)y ), (11)
where θy is the trainable parameter.
",3.2 Adversarial Networks,[0],[0]
The proposed network structure shown in Figure 1 includes both joint networks and adversarial networks for two tasks and two languages.,3.2 Adversarial Networks,[0],[0]
"The red components are language-specific networks, the orange components are task-specific networks, and the purple components are classification networks.",3.2 Adversarial Networks,[0],[0]
"In addition, green components are language-specific task adversarial networks, and blue components are task-specific language adversarial networks.",3.2 Adversarial Networks,[0],[0]
"Our adversarial training proposal jointly optimizes all parameters in both the main joint networks and the adversarial networks by using all training data sets {D(1,1), · · · ,D(I,J)} where D(i,j) represents the sets of the input utterances and the reference.",3.3 Training,[0],[0]
"The cross-entropy loss functions of each network are defined as:
Lo = − I∑
i=1 J∑",3.3 Training,[0],[0]
"j=1 |D(i,j)|∑ n=1 K(j)∑ k=1 ô (j) n,k log o (j) n,k, (12)
",3.3 Training,[0],[0]
"Lx = − I∑
i=1 J∑ j=1",3.3 Training,[0],[0]
"|D(i,j)|∑ n=1 J∑ j′=1 x̂",3.3 Training,[0],[0]
"(i) n,j′ logx (i) n,j′ , (13)
",3.3 Training,[0],[0]
"Ly = − I∑
i=1 J∑ j=1",3.3 Training,[0],[0]
"|D(i,j)|∑ n=1 I∑ i′=1 ŷ",3.3 Training,[0],[0]
(j) n,3.3 Training,[0],[0]
",i′ log y (j) n,i′ , (14)
where Lo, Lx, and Ly are the cross entropy loss terms for the classification networks, the task adversarial networks, and the language adversarial networks.",3.3 Training,[0],[0]
"ô(j)n,k, x̂ (i) n,j′ , and ŷ (j) n,i′ are the reference probabilities, and on,k, xn,j′ , and yn,i′ are the estimated probabilities of the k-th label in the j-th task classification network, the j′-th task in the ith language-specific task adversarial network, and
the i′-th language in the j-th task-specific language adversarial network forWn, respectively.
",3.3 Training,[0],[0]
"Due to use of gradient reversal layers, individual parameters are gradually updated as follows:
θ(j)o",3.3 Training,[0],[0]
"← θ(j)o − ∂Lo
∂θ (j) o
, (15)
θ(j)y ← θ(j)y − β ∂Ly
∂θ (j) y
, (16)
θ(j)u",3.3 Training,[0],[0]
← θ(j)u,3.3 Training,[0],[0]
"− ( ∂Lo
∂θ (j) u",3.3 Training,[0],[0]
− β,3.3 Training,[0],[0]
∂Ly,3.3 Training,[0],[0]
"∂θ (j) u ), (17)
θ(i)x ← θ(i)x",3.3 Training,[0],[0]
"− α ∂Lx
",3.3 Training,[0],[0]
∂θ,3.3 Training,[0],[0]
"(i) x
, (18)
θ (i) h",3.3 Training,[0],[0]
← θ (i) h,3.3 Training,[0],[0]
"− (
∂Lo ∂θ",3.3 Training,[0],[0]
(i) h,3.3 Training,[0],[0]
− α ∂Lx ∂θ (i) h,3.3 Training,[0],[0]
− β,3.3 Training,[0],[0]
∂Ly,3.3 Training,[0],[0]
"∂θ (i) h ),
(19)
where α and β are hyper parameters of the parameter update, and is the learning rate.",3.3 Training,[0],[0]
Note that adversarial training is suppressed by setting α and β to 0.0.,3.3 Training,[0],[0]
"In training, we prepared optimizers for individual data sets.",3.3 Training,[0],[0]
The individual learning rates fall when the validation loss of the target classification network increases.,3.3 Training,[0],[0]
Our experiments employed Japanese and English data sets created for three different utterance intent classification tasks.,4 Experiments,[0],[0]
"The tasks, dialogue act (DA) classification, topic type (TT) classification, and question type (QT) classification, are intended to support spoken dialogue systems.",4 Experiments,[0],[0]
"For example, the task of English DA classification is to obtain a DA label from an input utterance.",4 Experiments,[0],[0]
We used natural language texts as the input utterances and individual label sets were unified between Japanese and English.,4 Experiments,[0],[0]
"Data sets employed in experiments were corpora that were made for constructing spoken dialogue systems (Masumura et al., 2018).",4 Experiments,[0],[0]
"Each of the data sets were divided into training (Train),
validation (Valid), and test (Test) sets.",4 Experiments,[0],[0]
Table 1 shows the number of utterances in individual data sets where #labels represents the number of labels.,4 Experiments,[0],[0]
Table 2 shows English utterances and label examples for individual tasks.,4 Experiments,[0],[0]
"We examined single-task and mono-lingual modeling, multi-task joint modeling, multi-lingual join modeling, and multi-task and multi-lingual joint modeling with or without adversarial training.
",4.1 Setups,[0],[0]
We unified network configurations as follows.,4.1 Setups,[0],[0]
"Word representation size was set to 128, BLSTMRNN unit size was set to 400, and sentence representation was set to 400.",4.1 Setups,[0],[0]
"Dropout was used for EMBED() and BLSTM(), and the dropout rate was set to 0.5.",4.1 Setups,[0],[0]
Words that appeared only once in the training data sets were treated as unknown words.,4.1 Setups,[0],[0]
"We used mini-batch stochastic gradient descent, in which initial learning rate was set to 0.1.",4.1 Setups,[0],[0]
We optimized hyper-parameters of adversarial training (α and β) for the validation sets by varying them from 0.001 to 1.0.,4.1 Setups,[0],[0]
Other hyper parameters were also optimized for the validation sets.,4.1 Setups,[0],[0]
Table 3 shows the results in terms of utterance classification accuracy.,4.2 Results,[0],[0]
"For each setup, we constructed five models by varying the initial parameters and evaluated the average accuracy.",4.2 Results,[0],[0]
Line (1) shows baseline results: single-task and monolingual modeling.,4.2 Results,[0],[0]
"Lines (2) and (3) show results
with only performing multi-task joint modeling, and lines (4) and (5) show results with only performing multi-lingual joint modeling.",4.2 Results,[0],[0]
Note that lines (3) and (5) show the results achieved with adversarial training.,4.2 Results,[0],[0]
Line (6) shows multi-task and multi-lingual joint modeling results: adversarial training was suppressed by setting both α and β to 0.0.,4.2 Results,[0],[0]
Lines (7)–(9) shows the results achieved with adversarial training.,4.2 Results,[0],[0]
"Note that setting with bold values achieved the highest performance in our evaluation.
",4.2 Results,[0],[0]
"First, in lines (2) and (4), the classification performance deteriorated in some cases, while performance improvements were achieved in other cases.",4.2 Results,[0],[0]
"On the other hand, in lines (3) and (5), classification performance in each data sets was improved by introducing adversarial training.",4.2 Results,[0],[0]
"This indicates that adversarial training was effective in improving the performance of joint modeling.
",4.2 Results,[0],[0]
"Next, line (6) shows that, relative to line 1, multi-task and multi-lingual joint modeling can improve the classification performance for Japanese TT, Japanese QT, and English TT, but classification performance was degraded for English DA and English QT.",4.2 Results,[0],[0]
This indicates that it is difficult to simultaneously improve the classification performance for all data sets because joint modeling often depends on majority tasks or majority languages.,4.2 Results,[0],[0]
"In addition, lines (7) and (8) show the introduction of either task adversarial networks or language adversarial networks yielded better performance than line (6) for all data sets.",4.2 Results,[0],[0]
This indicates that adversarial training was effective in improving the performance of multi-task and multi-lingual joint modeling.,4.2 Results,[0],[0]
"The best results were achieved by using both language-specific task adversarial networks and task-specific language adversarial networks, line (9).",4.2 Results,[0],[0]
"These results confirm that task adversarial
networks and language adversarial networks well complement each other.",4.2 Results,[0],[0]
"Of particular benefit, the proposed method demonstrated greater classification performance improvements when the number of training utterances per label was small.",4.2 Results,[0],[0]
We have proposed an adversarial training method for the multi-task and multi-lingual joint modeling needed to enhance utterance intent classification.,5 Conclusions,[0],[0]
Our adversarial training proposal utilizes both task adversarial networks and language adversarial networks for improving task-invariance in languagespecific networks and language-invariance in taskspecific networks.,5 Conclusions,[0],[0]
Experiments showed that the adversarial training proposal could well realize the benefits of joint modeling in all data sets.,5 Conclusions,[0],[0]
This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification.,abstractText,[0],[0]
"In joint modeling, common knowledge can be efficiently utilized among multiple tasks or multiple languages.",abstractText,[0],[0]
This is achieved by introducing both languagespecific networks shared among different tasks and task-specific networks shared among different languages.,abstractText,[0],[0]
"However, the shared networks are often specialized in majority tasks or languages, so performance degradation must be expected for some minor data sets.",abstractText,[0],[0]
"In order to improve the invariance of shared networks, the proposed method introduces both language-specific task adversarial networks and task-specific language adversarial networks; both are leveraged for purging the task or language dependencies of the shared networks.",abstractText,[0],[0]
The effectiveness of the adversarial training proposal is demonstrated using Japanese and English data sets for three different utterance intent classification tasks.,abstractText,[0],[0]
Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 182–192 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
182",text,[0],[0]
The task of named entity recognition (NER) is to recognize the named entities in given text.,1 Introduction,[1.0],['The task of named entity recognition (NER) is to recognize the named entities in given text.']
"NER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yao and Van Durme, 2014).",1 Introduction,[0],[0]
"In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001).",1 Introduction,[1.0],"['In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001).']"
"Currently, with the development
of deep learning, neural networks (Lample et al., 2016; Peng and Dredze, 2016; Luo and Yang, 2016) have been introduced to NER task.",1 Introduction,[1.0000000544480903],"['Currently, with the development of deep learning, neural networks (Lample et al., 2016; Peng and Dredze, 2016; Luo and Yang, 2016) have been introduced to NER task.']"
"All these methods need to determine entities boundaries and classify them into pre-defined categories.
",1 Introduction,[0.99999998116564],['All these methods need to determine entities boundaries and classify them into pre-defined categories.']
"Although great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed.",1 Introduction,[1.0],"['Although great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed.']"
One significant drawback is that there is only a very small amount of annotated data available.,1 Introduction,[0],[0]
"Weibo NER dataset (Peng and Dredze, 2015; He and Sun, 2017a) and Sighan2006 NER dataset (Levow, 2006) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively.",1 Introduction,[1.0],"['Weibo NER dataset (Peng and Dredze, 2015; He and Sun, 2017a) and Sighan2006 NER dataset (Levow, 2006) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively.']"
"On the two datasets, the highest F1 scores are 48.41% and 89.21%, respectively.",1 Introduction,[1.0],"['On the two datasets, the highest F1 scores are 48.41% and 89.21%, respectively.']"
"As a basic task in NLP area, the performance is not satisfactory.",1 Introduction,[0],[0]
"Fortunately, Chinese word segmentation (CWS) task is to recognize word boundaries and the amount of supervised training data for CWS is abundant compared with NER.",1 Introduction,[0],[0]
"There are many similarities between Chinese NER task and CWS task, which we call task-shared information.",1 Introduction,[0],[0]
"As shown in Figure 1, given a sentence “ » ¯ : : (Hilton leaves Houston Airport)”, the two tasks have the same boundaries for some words such as “ (Hilton)” and “» (leaves)”, while Chinese NER has more coarse-grained boundaries
than CWS task for certain word such as “ ¯ :: (Houston Airport)” in the example of Figure 1, which we call task-specific information.",1 Introduction,[0.9999999722628198],"['As shown in Figure 1, given a sentence “ » ¯ : : (Hilton leaves Houston Airport)”, the two tasks have the same boundaries for some words such as “ (Hilton)” and “» (leaves)”, while Chinese NER has more coarse-grained boundaries than CWS task for certain word such as “ ¯ :: (Houston Airport)” in the example of Figure 1, which we call task-specific information.']"
"In order to incorporate word boundary information from CWS task into NER task, Peng and Dredze (2016) propose a joint model that performs Chinese NER with CWS task.",1 Introduction,[0],[0]
"However, their proposed model only focuses on task-shared information between Chinese NER and CWS, and ignores filtering the specificities of each task, which will bring noise for both of the tasks.",1 Introduction,[0],[0]
"For example, the CWS task splits “ ¯ :: (Houston Airport)” into “ ¯ (Houston)” and “:: (Airport)”, while the NER task takes “ ¯ :: (Houston Airport)” as a whole entity.",1 Introduction,[1.0],"['For example, the CWS task splits “ ¯ :: (Houston Airport)” into “ ¯ (Houston)” and “:: (Airport)”, while the NER task takes “ ¯ :: (Houston Airport)” as a whole entity.']"
"Thus, how to exploit task-shared information and prevent the noise brought by CWS task to Chinese NER task is a challenging problem.
",1 Introduction,[1.00000003887569],"['Thus, how to exploit task-shared information and prevent the noise brought by CWS task to Chinese NER task is a challenging problem.']"
Another issue is that most proposed models cannot explicitly model long range dependencies when predicting entity type.,1 Introduction,[1.0],['Another issue is that most proposed models cannot explicitly model long range dependencies when predicting entity type.']
"Though bidirectional long short term memory (BiLSTM) can learn long-distance dependencies, it cannot conduct direct connections between arbitrary two characters.",1 Introduction,[0],[0]
"As shown in Figure 1, if the model only focuses on the word “ (Hilton)”, it can be a person or organization.",1 Introduction,[0],[0]
"However, when the model explicitly captures the dependencies between “ (Hilton)” and “» (leaves)”, it is easy to classify “ (Hilton)” into “person” category.",1 Introduction,[1.0],"['However, when the model explicitly captures the dependencies between “ (Hilton)” and “» (leaves)”, it is easy to classify “ (Hilton)” into “person” category.']"
Context information is very crucial for determining the entity type.,1 Introduction,[0],[0]
"While in the sentence “ O( (I will be staying at the Hilton)”, the entity type of “ (Hilton)” is “organization”.",1 Introduction,[0],[0]
"Thus, how to better capture the global dependencies of the whole sentence is another challenging problem.
",1 Introduction,[0],[0]
"To address the above problems, we propose an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER task in this paper.",1 Introduction,[0],[0]
The adversarial transfer learning is incorporating adversarial training into transfer learning.,1 Introduction,[0],[0]
"To better capture long range dependencies and synthesize the information of the sentence, we extend self-attention mechanism into the framework.",1 Introduction,[0],[0]
"Specifically, we try to improve Chinese NER task performance by incorporating shared boundary information from CWS task.",1 Introduction,[0],[0]
"To prevent the specific information of CWS task from lowering the performance of the Chinese NER task, we introduce adversarial training to ensure that the Chinese NER task on-
ly exploits task-shared word boundary information.",1 Introduction,[0],[0]
"Then, for tackling the long range dependency problems, we utilize self-attention to synthesize the hidden representation of BiLSTM.",1 Introduction,[0],[0]
"Finally, we evaluate our model on two different widely used Chinese NER datasets.",1 Introduction,[0],[0]
"Experimental results show that our proposed model achieves better performance than other state-of-the-art methods and gains new benchmarks.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are as follows:
• We propose an adversarial transfer learning framework to incorporate task-shared word boundary information from CWS task into Chinese NER task.",1 Introduction,[0],[0]
"To our best knowledge, it is the first work to apply adversarial transfer learning method into NER task.
",1 Introduction,[0],[0]
"• We introduce self-attention mechanism into our model, which aims to capture the global dependencies of the whole sentence and learn inner structure features of sentence.
",1 Introduction,[0],[0]
"• We conduct our experiment on two different widely used Chinese NER datasets, and the experimental results demonstrate that our proposed model significantly and consistently outperforms previous state-of-the-art methods.",1 Introduction,[0],[0]
We release the source code publicly for further research1.,1 Introduction,[0],[0]
NER Many methods have been proposed for NER task.,2 Related Work,[0],[0]
"Early studies on NER often exploit SVMs (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRFs (Lafferty et al., 2001), heavily relying on feature engineering.",2 Related Work,[0],[0]
Zhou et al. (2013) formulate Chinese NER as a joint identification and categorization task.,2 Related Work,[0],[0]
"In recent years, neural network models have been introduced to NER task (Collobert et al., 2011; Huang et al., 2015; Peng and Dredze, 2016).",2 Related Work,[0],[0]
Huang et al. (2015) exploit BiLSTM to extract features and feed them into CRF decoder.,2 Related Work,[0],[0]
"After that, the BiLSTM-CRF model is usually exploited as the baseline.",2 Related Work,[0],[0]
Lample et al. (2016) use a character LSTM to represent spelling characteristics.,2 Related Work,[0],[0]
"In addition, Wang et al. (2017) propose a gated convolutional neural network (GCNN) model for Chinese NER.",2 Related Work,[0],[0]
"Peng and Dredze (2016) propose a joint model for Chinese
1https://github.com/CPF-NLPR/AT4ChineseNER
NER, which are jointly trained with CWS task.",2 Related Work,[0],[0]
"However, the specific features brought by CWS task can lower the performance of the Chinese NER task.
",2 Related Work,[0],[0]
"Adversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015).",2 Related Work,[0],[0]
"In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018).",2 Related Work,[0],[0]
Bousmalis et al. (2016) propose shared-private model in domain separation network.,2 Related Work,[0],[0]
"Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task.",2 Related Work,[0],[0]
"To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task.
",2 Related Work,[0],[0]
Self-Attention Self-attention has been introduced to machine translation by Vaswani et al. (2017) for capturing global dependencies between input and output and achieves state-of-the-art performance.,2 Related Work,[0],[0]
"For language understanding task, Shen et al. (2017) exploit self-attention to learn long range dependencies.",2 Related Work,[0],[0]
Tan et al. (2017) apply self-attention to semantic role labelling task and achieve state-of-the-art results.,2 Related Work,[0],[0]
"We are the first to
introduce self-attention mechanism to Chinese NER task.",2 Related Work,[0],[0]
"In this paper, we propose a novel adversarial transfer learning framework that will learn task-shared word boundary information from CWS task, filter specific information of CWS and explicitly capture the long range dependencies between arbitrary two characters in sentence.",3 Method,[0],[0]
The architecture of our proposed model is illustrated in Figure 2.,3 Method,[1.0],['The architecture of our proposed model is illustrated in Figure 2.']
"The model mainly consists of five components: embedding layer, shared-private feature extractor, self-attention, task-specific CRF and task discriminator.",3 Method,[0],[0]
"In the following sections, we will describe each part of our proposed model in detail.",3 Method,[1.0],"['In the following sections, we will describe each part of our proposed model in detail.']"
"Similar to other neural network models, the first step of our proposed model is to map discrete characters into the distributed representations.",3.1 Embedding Layer,[1.0],"['Similar to other neural network models, the first step of our proposed model is to map discrete characters into the distributed representations.']"
"For a given Chinese sentence x = {c1, c2, . . .",3.1 Embedding Layer,[0],[0]
", cN} from Chinese NER dataset or CWS dataset, we lookup embedding vector from pre-trained embedding matrix for each character ci as xi ∈ Rde .",3.1 Embedding Layer,[0],[0]
"Long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of recurrent neural network (RNN) (Elman, 1990), which enables to address the gradient vanishing and exploding
problems in RNN via introducing gate mechanism and memory cell.",3.2 Shared-Private Feature Extractor,[0],[0]
"The unidirectional LSTM only leverages information from the past, ignoring the future information.",3.2 Shared-Private Feature Extractor,[0],[0]
"In order to incorporate information from both sides of sequence, we adopt BiLSTM to extract features.",3.2 Shared-Private Feature Extractor,[0],[0]
"Specially, the hidden state of BiLSTM could be expressed as follows:
−→ hi = −−−−→",3.2 Shared-Private Feature Extractor,[0],[0]
"LSTM( −→ h i−1, xi) (1) ←−",3.2 Shared-Private Feature Extractor,[0],[0]
"hi = ←−−−− LSTM( ←− h i+1, xi) (2)",3.2 Shared-Private Feature Extractor,[0],[0]
"hi = −→ hi ⊕ ←− hi (3)
",3.2 Shared-Private Feature Extractor,[0],[0]
where −→ hi ∈,3.2 Shared-Private Feature Extractor,[0],[0]
Rdh,3.2 Shared-Private Feature Extractor,[0],[0]
and ←− hi ∈,3.2 Shared-Private Feature Extractor,[0],[0]
"Rdh are the hidden states of the forward and backward LSTM at position i, respectively.",3.2 Shared-Private Feature Extractor,[0],[0]
"⊕ denotes concatenation operation.
",3.2 Shared-Private Feature Extractor,[0],[0]
"As shown in Figure 2, we propose a sharedprivate feature extractor, which assigns a private BiLSTM layer and shared BiLSTM layer for task k ∈ {NER,CWS}.",3.2 Shared-Private Feature Extractor,[1.0],"['As shown in Figure 2, we propose a sharedprivate feature extractor, which assigns a private BiLSTM layer and shared BiLSTM layer for task k ∈ {NER,CWS}.']"
"The private BiLSTM layer is used to extract task-specific features, and the shared BiLSTM layer is used to learn task-shared word boundaries.",3.2 Shared-Private Feature Extractor,[0],[0]
"Formally, for any sentence in dataset of task k, the hidden states of shared and private BiLSTM layer can be computed as follows:
ski = BiLSTM(x k i , s k i−1; θs) (4) hki = BiLSTM(x k i ,h k i−1; θk) (5)
where θs and θk are the shared BiLSTM parameters and private BiLSTM parameters of task k, respectively.",3.2 Shared-Private Feature Extractor,[0],[0]
"Inspired by the self-attention applied to machine translation (Vaswani et al., 2017) and semantic role labelling (Tan et al., 2017), we exploit selfattention to explicitly learn the dependencies between any two characters in sentence and capture the inner structure information of sentence.",3.3 Self-Attention,[1.0],"['Inspired by the self-attention applied to machine translation (Vaswani et al., 2017) and semantic role labelling (Tan et al., 2017), we exploit selfattention to explicitly learn the dependencies between any two characters in sentence and capture the inner structure information of sentence.']"
"In this paper, we adopt the multi-head self-attention mechanism.",3.3 Self-Attention,[0],[0]
"H = {h1,h2, . . .",3.3 Self-Attention,[0],[0]
",hN} denotes the output of private BiLSTM.",3.3 Self-Attention,[0],[0]
"Correspondingly, S = {s1, s2, . . .",3.3 Self-Attention,[0],[0]
", sN} is the output of shared BiLSTM.",3.3 Self-Attention,[0],[0]
We will take the self-attention in private space as example to illustrate how it works.,3.3 Self-Attention,[0],[0]
"The scaled dotproduct attention can be precisely described as follows:
Attention(Q,K,V) = softmax( QKT√
d )V (6)
where Q ∈ RN×2dh , K ∈ RN×2dh and V ∈ RN×2dh are query matrix, keys matrix and value matrix, respectively.",3.3 Self-Attention,[0],[0]
"In our setting, Q = K = V = H. d is the dimension of hidden units of BiLSTM, which equals to 2dh.
",3.3 Self-Attention,[0],[0]
"Multi-head attention first linearly projects the queries, keys and values h times by using different linear projections.",3.3 Self-Attention,[0],[0]
Then h projections perform the scaled dot-product attention in parallel.,3.3 Self-Attention,[0],[0]
"Finally, these results of attention are concatenated and once again projected to get the new representation.",3.3 Self-Attention,[0],[0]
"Formally, the multi-head attention can be expressed as follows:
headi = Attention(QWQi ,KW K i ,VW V i ) (7) H ′ =",3.3 Self-Attention,[0],[0]
(headi ⊕ . .,3.3 Self-Attention,[0],[0]
.⊕,3.3 Self-Attention,[0],[0]
"headh)Wo (8)
where WQi ∈ R2dh×dk , WKi ∈ R2dh×dk and WVi ∈",3.3 Self-Attention,[0],[0]
R2dh×dk,3.3 Self-Attention,[0],[0]
are trainable projection parameters and dk = 2dh/,3.3 Self-Attention,[0],[0]
h.,3.3 Self-Attention,[0],[0]
Wo ∈ R2dh×2dh is also trainable parameter.,3.3 Self-Attention,[0],[0]
"For a sentence in dataset of task k, we compute the final representation via concatenating the representations from private space and shared space after self-attention layer:
H ′′k = H ′k ⊕ S′k (9)
where H′k and S ′k are the outputs of private selfattention and shared self-attention of task k, respectively.
",3.4 Task-Specific CRF,[0],[0]
"Considering the dependencies between successive labels, we exploit CRF (Lafferty et al., 2001) to inference tags instead of making tagging decisions using h′′i independently.",3.4 Task-Specific CRF,[0],[0]
"Due to the difference of labels, we introduce a specific CRF layer for each task.",3.4 Task-Specific CRF,[0],[0]
"Given a sentence x = {c1, c2, . . .",3.4 Task-Specific CRF,[0],[0]
", cN} with a predicted tag sequence y = {y1, y2, . . .",3.4 Task-Specific CRF,[0],[0]
", yN}, the CRF tagging process can be formalized as follows:
oi = Wsh ′′",3.4 Task-Specific CRF,[0],[0]
"i + bs (10)
s(x, y) =",3.4 Task-Specific CRF,[0],[0]
N∑ i=1,3.4 Task-Specific CRF,[0],[0]
"(oi,yi + Tyi−1,yi) (11) ȳ = arg max y∈Yx s(x, y) (12)
where Ws ∈ R|T |×4dh and bs ∈ R|T | are trainable parameters.",3.4 Task-Specific CRF,[0],[0]
|T | denotes the number of output labels.,3.4 Task-Specific CRF,[0],[0]
"oi,yi represents the score of the yi-th tag
of the character ci.",3.4 Task-Specific CRF,[0],[0]
T is a transition score matrix which defines the scores of two successive labels.,3.4 Task-Specific CRF,[0],[0]
Yx represents all candidate tag sequences for given sentence x.,3.4 Task-Specific CRF,[0],[0]
"In decoding, we use Viterbi algorithm to get the predicted tag sequence ȳ.
For training, we exploit negative log-likelihood objective as the loss function.",3.4 Task-Specific CRF,[0],[0]
"The probability of the ground-truth label sequence is computed by:
p(ŷ|x) = e s(x,ŷ)∑
ỹ∈Yx e s(x,̃y) (13)
where ŷ denotes the ground-truth label sequence.",3.4 Task-Specific CRF,[0],[0]
"Given T training examples (x(i); ŷ(i)), the loss function LTask can be defined as follows:
LTask = − T∑ i=1",3.4 Task-Specific CRF,[0],[0]
logp(ŷ(i)|x(i)),3.4 Task-Specific CRF,[0],[0]
"(14)
We use gradient back-propagation method to minimize the loss function.",3.4 Task-Specific CRF,[0],[0]
"Inspired by adversarial networks (Goodfellow et al., 2014), we incorporate adversarial training into shared space to guarantee that specific features of tasks do not exist in shared space.",3.5 Task Discriminator,[0],[0]
We propose a task discriminator to estimate which task the sentence comes from.,3.5 Task Discriminator,[1.0],['We propose a task discriminator to estimate which task the sentence comes from.']
"Formally, the task discriminator can be expressed as follows:
s ′k = Maxpooling(S ′k) (15) D(s ′k; θd) = softmax(Wds ′k + bd) (16)
where θd indicates the parameters of task discriminator.",3.5 Task Discriminator,[0],[0]
Wd ∈,3.5 Task Discriminator,[0],[0]
RK×2dh and bd ∈ RK are trainable parameters.,3.5 Task Discriminator,[0],[0]
"K is the number of tasks.
",3.5 Task Discriminator,[0],[0]
"Besides the task lossLTask, we introduce an adversarial loss LAdv to prevent specific features of CWS task from creeping into shared space.",3.5 Task Discriminator,[0],[0]
The adversarial loss trains the shared model to produce shared features such that the task discriminator cannot reliably recognize which task the sentence comes from.,3.5 Task Discriminator,[0],[0]
"The adversarial loss can be computed as follows:
",3.5 Task Discriminator,[0],[0]
LAdv = min θs (max θd K∑ k=1,3.5 Task Discriminator,[0],[0]
Tk∑ i=1,3.5 Task Discriminator,[0],[0]
"logD(Es(x (i) k )))
(17) where θs denotes the trainable parameters of shared BiLSTM.Es denotes the shared feature extractor.",3.5 Task Discriminator,[0],[0]
"Tk is the number of training examples of
task k. x(i)k is the i-th example of task k.",3.5 Task Discriminator,[0],[0]
"There is a minimax optimization that the shared BiLSTM generates a representation to mislead the task discriminator and the discriminator tries its best to correctly determine the type of task.
",3.5 Task Discriminator,[0],[0]
"We add a gradient reversal layer (Ganin and Lempitsky, 2014) below the softmax layer to address the minimax optimization problem.",3.5 Task Discriminator,[0],[0]
"In the training phrase, we minimize the task discriminator errors, and through gradient reversal layer the gradients will become opposed sign to adversarially encourage the shared feature extractor to learn task-shared word boundary information.",3.5 Task Discriminator,[0],[0]
"After training phrase, the shared feature extractor and task discriminator reach a point where the discriminator cannot differentiate the tasks according to the representations learned from shared feature extractor.",3.5 Task Discriminator,[0],[0]
"The final loss function of our proposed model can be written as follows:
L = LNER · I(x) + LCWS · (1− I(x))",3.6 Training,[0],[0]
+ λLAdv (18) where λ is a hyper-parameter.,3.6 Training,[0],[0]
LNER and LCWS can be computed via Eq.14.,3.6 Training,[0],[0]
I(x) is a switching function to identify which task the input comes from.,3.6 Training,[0],[0]
"It is defined as follows:
I(x) =",3.6 Training,[0],[0]
"{ 1, if x ∈ DNER 0, if x ∈ DCWS
(19)
where DNER and DCWS are Chinese NER training corpora and CWS training corpora, respectively.
",3.6 Training,[0],[0]
"In the training phrase, at each iteration, we first select a task from {NER,CWS} in turn.",3.6 Training,[0],[0]
"Then, we sample a batch of training instances from the given task to update the parameters.",3.6 Training,[0],[0]
"We use Adam (Kingma and Ba, 2014) algorithm to optimize the final loss function.",3.6 Training,[1.0],"['We use Adam (Kingma and Ba, 2014) algorithm to optimize the final loss function.']"
"Since Chinese NER task and CWS task may have different convergence rate, we repeat the above iterations until early stopping according to the Chinese NER task performance.",3.6 Training,[1.0],"['Since Chinese NER task and CWS task may have different convergence rate, we repeat the above iterations until early stopping according to the Chinese NER task performance.']"
"To evaluate our proposed model on Chinese NER, we experiment on two different widely used datasets, including Weibo NER dataset (WeiboNER) (Peng and Dredze, 2015; He and Sun,
2017a) and SIGHAN2006 NER dataset (SighanNER) (Levow, 2006).",4.1 Datasets,[0.9999999298602268],"['To evaluate our proposed model on Chinese NER, we experiment on two different widely used datasets, including Weibo NER dataset (WeiboNER) (Peng and Dredze, 2015; He and Sun, 2017a) and SIGHAN2006 NER dataset (SighanNER) (Levow, 2006).']"
"We use the MSR dataset (from SIGHAN2005) for CWS task.
",4.1 Datasets,[0.9999999801813012],['We use the MSR dataset (from SIGHAN2005) for CWS task.']
"The WeiboNER is annotated with four entity types (person, location, organization and geopolitical entities), including named entities and nominal mentions.",4.1 Datasets,[0],[0]
"The SighanNER is simplified Chinese, which contains three entity types (person, location and organization).",4.1 Datasets,[0],[0]
"For WeiboNER, we use the same training, development and testing splits as Peng and Dredze (2015).",4.1 Datasets,[0],[0]
"Since the SighanNER does not have development set, we sample 10% data of training set as development set.",4.1 Datasets,[0],[0]
We use MSR dataset to improve the performance of the Chinese NER task.,4.1 Datasets,[0],[0]
Table 1 gives the details of the three datasets.,4.1 Datasets,[1.0],['Table 1 gives the details of the three datasets.']
"For evaluation, we use the Precision (P), Recall (R) and F1 score as metrics in our experiment.
",4.2 Settings,[0],[0]
"For hyper-parameter configurations, we adjust them according to the performance on development set of Chinese NER task.",4.2 Settings,[0],[0]
We set the character embedding size de to 100.,4.2 Settings,[0],[0]
The dimensionality of LSTM hidden states dh is 120.,4.2 Settings,[0],[0]
The initial learning rate is set to 0.001.,4.2 Settings,[0],[0]
The loss weight coefficient λ is set to 0.06.,4.2 Settings,[0],[0]
"We set the dropout rate to 0.3.
",4.2 Settings,[0],[0]
The number of projections h is 8.,4.2 Settings,[0],[0]
"We set the batch size of SighanNER and WeiboNER as 64 and 20, respectively.
",4.2 Settings,[0],[0]
"For trainable parameters initialization, we use xavier initializer (Glorot and Bengio, 2010) to initialize parameters.",4.2 Settings,[0],[0]
"The character embeddings used in our experiment are pre-trained on Baidu Encyclopedia corpus and Weibo corpus by using word2vec toolkit (Mikolov et al., 2013).",4.2 Settings,[0],[0]
"In this section, we will give the experimental results of our proposed model and previous stateof-the-art methods on WeiboNER dataset and SighanNER dataset, respectively.",4.3 Compared with State-of-the-art Methods,[0],[0]
We compare our proposed model with the latest models on WeiboNER dataset.,4.3.1 Evaluation on WeiboNER,[0],[0]
"Table 2 shows the experimental results for named entities on the original WeiboNER dataset.
",4.3.1 Evaluation on WeiboNER,[0],[0]
"In the first block of Table 2, we give the performance of the main model and baselines proposed by Peng and Dredze (2015).",4.3.1 Evaluation on WeiboNER,[0.9958611138001376],"['In the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016).']"
"They propose a CRF-based model to jointly train the embeddings with NER task, which achieves better results than pipeline models.",4.3.1 Evaluation on WeiboNER,[0],[0]
"In addition, they consider the po-
sition of each character in a word to train character and position embeddings.
",4.3.1 Evaluation on WeiboNER,[0],[0]
"In the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016).",4.3.1 Evaluation on WeiboNER,[0],[0]
"Aiming to incorporate word boundary information into the NER task, they propose an integrated model that can joint training CWS task, improving the F1 score from 46.20% to 48.41% as compared with pipeline model (Pipeline Seg.Repr.+NER).
",4.3.1 Evaluation on WeiboNER,[0],[0]
"In the last block of Table 2, we give the experimental result of our proposed model (BiLSTM+CRF+adversarial+self-attention).",4.3.1 Evaluation on WeiboNER,[0],[0]
We can observe that our proposed model significantly outperforms other models.,4.3.1 Evaluation on WeiboNER,[0],[0]
"Compared with the model proposed by Peng and Dredze (2016), our method gains 4.67% improvement in F1 score.",4.3.1 Evaluation on WeiboNER,[0],[0]
"Interestingly, WeiboNER dataset and MSR dataset are different domains.",4.3.1 Evaluation on WeiboNER,[0],[0]
"The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain.",4.3.1 Evaluation on WeiboNER,[0],[0]
"The improvement of performance indicates that our proposed adversarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem.
",4.3.1 Evaluation on WeiboNER,[0],[0]
We also conduct an experiment on the updated WeiboNER dataset.,4.3.1 Evaluation on WeiboNER,[0],[0]
Table 3 lists the performance of the latest models and our proposed model on the updated dataset.,4.3.1 Evaluation on WeiboNER,[0],[0]
"In the first block of Table 3,
we report the performance of the latest models.",4.3.1 Evaluation on WeiboNER,[0],[0]
The model proposed by Peng and Dredze (2015) achieves F1 score of 56.05% on overall performance.,4.3.1 Evaluation on WeiboNER,[0],[0]
He and Sun (2017b) propose an unified model for Chinese NER task to exploit the data from out-of-domain corpus and in-domain unlabelled texts.,4.3.1 Evaluation on WeiboNER,[0],[0]
"The unified model improves the F1 score from 54.82% to 58.23% compared with the model proposed by He and Sun (2017a).
",4.3.1 Evaluation on WeiboNER,[0],[0]
"In the second block of Table 3, we give the result of our proposed model.",4.3.1 Evaluation on WeiboNER,[0],[0]
It can be observed that our proposed model achieves a very competitive performance.,4.3.1 Evaluation on WeiboNER,[0],[0]
"Compared with the latest model proposed by He and Sun (2017b), our model improves the F1 score from 58.23% to 58.70% on overall performance.",4.3.1 Evaluation on WeiboNER,[0],[0]
The improvement demonstrates the effectiveness of our proposed model.,4.3.1 Evaluation on WeiboNER,[0],[0]
Table 4 lists the comparisons on SighanNER dataset.,4.3.2 Evaluation on SighanNER,[0],[0]
"We observe that our proposed model achieves new state-of-the-art performance.
",4.3.2 Evaluation on SighanNER,[0],[0]
"In the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset.",4.3.2 Evaluation on SighanNER,[0],[0]
Chen et al. (2006) propose a character-based CRF model for Chinese NER task.,4.3.2 Evaluation on SighanNER,[0],[0]
"Zhou et al. (2006) introduce a pipeline model, which first segments the text with characterlevel CRF model and then applies word-level CRF to tag.",4.3.2 Evaluation on SighanNER,[0],[0]
"Luo and Yang (2016) first train a word segmenter and then use word segmentation as addi-
tional features for sequence tagging.",4.3.2 Evaluation on SighanNER,[0],[0]
"Although the model achieves competitive performance, giving the F1 score of 89.21%, it suffers from the error propagation problem.
",4.3.2 Evaluation on SighanNER,[0],[0]
"In the second block, we report the result of our proposed model.",4.3.2 Evaluation on SighanNER,[0],[0]
"Compared with the state-ofthe-art model proposed by Luo and Yang (2016), our method improves the F1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model.",4.3.2 Evaluation on SighanNER,[0],[0]
Table 5 provides the experimental results of our proposed model and baseline as well as its simplified models on SighanNER dataset and WeiboNER dataset.,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"The simplified models are described as follows:
• BiLSTM+CRF:",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"The model is used as strong baseline in our work, which is trained using Chinese NER training data.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
• BiLSTM+CRF+transfer:,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"We apply transfer learning to BiLSTM+CRF model without adversarial loss and self-attention mechanism.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
• BiLSTM+CRF+adversarial:,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"Compared with BiLSTM+CRF+transfer model, the BiLST-
M+CRF+adversarial model incorporates adversarial training.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
• BiLSTM+CRF+self-attention:,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"The model integrates the self-attention mechanism based on BiLSTM+CRF model.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"From the experimental results of Table 5, we have following observations:
• Effectiveness of transfer learning.",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0.9999999237391183],"['From the experimental results of Table 5, we have following observations: • Effectiveness of transfer learning.']"
"BiLSTM+CRF+transfer improves F1 score from 89.13% to 89.89% as compared with BiLSTM+CRF on SighanNER dataset and achieves 1.08% improvement on WeiboNER dataset, which indicates the word boundary information from CWS is very effective for Chinese NER task.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
• Effectiveness of adversarial training.,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[1.0],['• Effectiveness of adversarial training.']
"By introducing adversarial training, BiLSTM+CRF+adversarial boosts the performance as compared with BiLSTM+CRF+transfer model, showing 0.15% and 0.36% improvement on SighanNER dataset and WeiboNER dataset, respectively.",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"It proves that adversarial training can prevent specific features of CWS task from creeping into shared space.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
• Effectiveness of self-attention mechanism.,4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"When compared with BiLSTM+CRF, the
BiLSTM+CRF+self-attention significantly improves the performance on the two different datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for Chinese NER task.
",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"We observe that our proposed adversarial transfer learning framework and self-attention lead to noticeable improvements over the baseline, improving F1 score from 51.01% to 53.08% on WeiboNER dataset and giving 1.51% improvement on SighanNER dataset.",4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention,[0],[0]
"Word boundary information from CWS task is very important for Chinese NER task, especially when different entities appear together, .",4.5.1 Case Study,[0],[0]
We take a sentence in WeiboNER test set as example for illustrating the effectiveness of our proposed model.,4.5.1 Case Study,[0],[0]
"As shown in Figure 4(a), when two “person” entities appearing together, our proposed method exploits word segmentation information to determine the boundary between them and then make correct taggings.",4.5.1 Case Study,[0],[0]
"In Figure 4(b), when labelling the word “ ø (the boss)”, the self-attention explicitly learns the dependencies with “ Í (respect)”, therefore, our model enables to correctly classify the word into “person” category.",4.5.1 Case Study,[0],[0]
It verifies that the self-attention is very effective for Chinese NER task.,4.5.1 Case Study,[0],[0]
"According to the results of Table 2 and Table 4, our proposed model achieves 4.67% and 1.43% improvement as compared with previous stateof-the-art methods on WeiboNER dataset and SighanNER dataset, respectively.",4.5.2 Error Analysis,[0],[0]
"However, the overall performance on WeiboNER dataset is relatively low.",4.5.2 Error Analysis,[0],[0]
Two reasons can be explained for this issue.,4.5.2 Error Analysis,[0],[0]
One reason is that the number of training examples in WeiboNER dataset is very limited as compared with SighanNER dataset.,4.5.2 Error Analysis,[0],[0]
"There are only 1.3k examples in WeiboNER training corpora, which is not enough to train deep neural networks.",4.5.2 Error Analysis,[0],[0]
"Another reason is that the expression is informal in social media, lowering the performance on WeiboNER dataset.",4.5.2 Error Analysis,[0],[0]
While the greater improvement on WeiboNER dataset proves that our method is helpful to solve the problem.,4.5.2 Error Analysis,[1.0],['While the greater improvement on WeiboNER dataset proves that our method is helpful to solve the problem.']
"In this paper, we propose a novel adversarial transfer learning framework for Chinese NER task, which can exploit task-shared word boundaries features and prevent the specific information of CWS task.",5 Conclusions,[1.0],"['In this paper, we propose a novel adversarial transfer learning framework for Chinese NER task, which can exploit task-shared word boundaries features and prevent the specific information of CWS task.']"
"Besides, we introduce self-attention mechanism to capture the dependencies of arbitrary two characters and learn the inner structure information of sentence.",5 Conclusions,[1.0],"['Besides, we introduce self-attention mechanism to capture the dependencies of arbitrary two characters and learn the inner structure information of sentence.']"
Experiments on two different widely used datasets demonstrate that our method significantly and consistently outperforms previous state-of-the-art models.,5 Conclusions,[1.0],['Experiments on two different widely used datasets demonstrate that our method significantly and consistently outperforms previous state-of-the-art models.']
"The research work is supported by the Natural Science Foundation of China (No.61533018 and No.61702512), and the independent research project of National Laboratory of Pattern Recognition.",Acknowledgments,[0],[0]
"This work is also supported in part by Beijing Unisound Information Technology Co., Ltd.",Acknowledgments,[0],[0]
"Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories.",abstractText,[0],[0]
"For Chinese NER task, there is only a very small amount of annotated data available.",abstractText,[0],[0]
Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries.,abstractText,[0],[0]
There are also specificities in each task.,abstractText,[0],[0]
"However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS.",abstractText,[0],[0]
"In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the taskspecific features of CWS.",abstractText,[0],[0]
"Besides, since arbitrary character can provide important cues when predicting entity type, we exploit selfattention to explicitly capture long range dependencies between two tokens.",abstractText,[0],[0]
Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-ofthe-art methods.,abstractText,[0],[0]
Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism,title,[0],[0]
