0	40	Building a chatbot that can naturally and consistently converse with human-beings on opendomain topics draws increasing research interests in past years.
1	21	One important task in chatbots is response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation.
4	33	Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013).
7	29	Figure 1 illustrates semantic connectivities between segment pairs across context and response.
8	82	As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for example the lexical overlap of words “packages”-“package” and phrases “debian package manager”-“debian pack- age manager”.
10	14	Such as the word “it” in the response, which refers to “dpkg” in the context, as well as the phrase “its just reassurance” in the response, which latently points to “what packages are installed on my system”, the question that speaker A wants to double-check.
13	15	Moreover, Recurrent Neural Networks (RNN) are conveniently used for encoding texts, which is too costly to use for capturing multi-grained semantic representations (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017).
14	56	As an alternative, we propose to match a response with multi-turn context using dependency information based entirely on attention mechanism.
17	41	By hierarchically stacking self-attention from word embeddings, we can gradually construct semantic representations at different granularities.
18	31	cross-attention By making context and response attend to each other, we can generally capture dependencies between those latently matched segment pairs, which is able to provide complementary information to textual relevance for matching response with multi-turn context.
19	14	We jointly introduce self-attention and crossattention in one uniform neural matching network, namely the Deep Attention Matching Network (DAM), for multi-turn response selection.
54	15	We will discuss in detail the implementation of Attentive Module and how we used it to implement both self-attention and cross-attention in following sections.
57	81	The Attentive Module first takes each word in the query sentence to attend to words in the key sentence via Scaled Dot-Product Attention (Vaswani et al., 2017), then applies those attention results upon the value sentence, which is defined as: Att(Q,K) = [ softmax( Q[i] · KT√ d ) ]nQ−1 i=0 (1) Vatt = Att(Q,K) · V ∈ RnQ×d (2) where Q[i] is the ith embedding in the query sentence Q.
58	23	Each row of Vatt, denoted as Vatt[i], stores the fused semantic information of words in the value sentence that possibly have dependencies to the ith word in query sentence.
61	19	A feed-forward network FFN with RELU (LeCun et al., 2015) activation is then applied upon the normalization result, in order to further process the fused embeddings, defined as: FFN(x) = max(0, xW1 + b1)W2 + b2 (3) where, x is a 2D-tensor in the same shape of query sentence Q and W1, b1,W2, b2 are learnt parameters.
63	30	The result FFN(x) is a 2D-tensor that has the same shape as x, FFN(x) is then residually added (He et al., 2016) to x, and the fusion result is then normalized as the final outputs.
73	18	DAM finally aggregates all the segmental matching degrees across each utterance and response into a 3D matching image Q, which is defined as: Q = {Qi,k,t}n×nui×nr (11) where each pixel Qi,k,t is formulated as: Qi,k,t = [Mui,r,lself [k, t]] L l=0 ⊕ [Mui,r,lcross [k, t]]Ll=0 (12) ⊕ is concatenation operation, and each pixel has 2(L + 1) channels, storing the matching degrees between one certain segment pair at different levels of granularity.
79	24	The Ubuntu training set contains 0.5 million multiturn contexts, and each context has one positive response that generated by human and one negative response which is randomly sampled.
96	15	Learning rate is initialized as 1e-3 and gradually decreased during training, and the batch-size is 256.
97	46	We use validation sets to select the best models and report their performances on test sets.
98	37	Table 1 shows the evaluation results of DAM as well as all comparison models.
102	16	Also the absence of self-attention-match brings down the precision, as shown in DAMcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection.
141	23	Also as shown in Figure 5, self-attentionmatch and cros -attention-match capture complementary information in matching utterance with response.
147	25	(2) logical-error, where response candidates are wrong due to logical mismatch, for example, given a conversation context A: “I just want to stay at home tomorrow.”, B: “Why not go hiking?
148	215	I can go with you.”, response candidate like “Sure, I was planning to go out tomorrow.” is logically wrong because it is contradictory to the first utterance of speaker A.
149	30	We believe generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future.
150	114	In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention.
152	31	(2) utilizing cross-attention to match with dependency information.
153	27	Empirical results on two large-scale datasets demonstrate the effectiveness of self-attention and cross-attention in multi-turn response selection.
155	76	We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.
