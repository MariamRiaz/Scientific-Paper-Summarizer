21	36	In this work, we design SWAP-NET a new deep learning model for extractive summarization.
24	14	Salient sentences of a document, that are useful in summaries, often contain key words and, to our knowledge, none of the previous models have explicitly modeled this interaction.
26	23	An attention-based mechanism, similar to that of Pointer Networks, is used to learn important words and sentences from labeled data.
27	33	A switch mechanism is used to select between words and sentences during decoding and the final summary is generated using a combination of selected sentences and words.
28	8	We demonstrate the efficacy of our model on the CNN/Daily Mail corpus where it outperforms state-of-the-art extractive summarizers.
32	19	, wn be the sequence of n words in document D. An extractive summary aims to obtain a subset of the input sentences that forms a salient summary.
38	9	We omit M in the following to simplify notation.
44	9	The attention vector at each output step j is computed as follows: uji = v T tanh(Weei +Wddj), i ∈ (1, .
46	56	In a pointer network, the same attention mechanism is used to select one of the n input vectors with the highest probability, at each decoding step, thus effectively pointing to an input: p(rj |r1, ....rj−1,X) = softmax(uj) Here, v,Wd, and We are learnable parameters of the model.
47	49	We use an encoder-decoder architecture with an attention mechanism similar to that of Pointer Networks.
48	177	To model the interaction between words and sentences in a document we use two encoders and decoders, one at the word level and the other at the sentence level.
49	18	The sentence-level decoder learns to point to important sentences while the word-level decoder learns to point to important words.
50	83	A switch mechanism is trained to select either a word or a sentence at each decoding step.
64	35	Let αskj denote the probability of selecting the kth input sentence at the jth decoding step of sentence decoder: αskj = p(Tj = k|v<j , Qj = 1, D), and let αwij denote the probability of selecting the ith input word at the jth decoding step of word decoder: αwij = p(tj = i|v<j , Qj = 0, D), both conditional on the corresponding switch selection.
66	7	These probabilities are obtained through the attention weight vectors at the word and sentence levels and the switch probabilities: αwij = softmax(v T t φ(whhj + wtei)), αskj = softmax(V T T φ(WHHj +WTEk)).
73	25	Note that at each decoding step, switch is either qwj = 1, q s j = 0 if the j th output is a word or qwj = 0, q s j = 1 if the j th output is a sentence.
77	17	We assign importance scores to the selected sentences based on their probability values during decoding as well as the probabilities of the selected words that are present in the selected sentences.
128	8	We also compare our method with an abstractive summarizer that uses a similar attention-based encoder-decoder architecture (Nallapati et al., 2016), denoted by ABS.
134	12	We have 193,986 training documents, 12,147 validation documents and 10,346 test documents from the DailyMail corpus and 83,568 training documents, 1,220 validation documents and 1,093 test documents from CNN subset with labels for sentences and words.
135	7	We use the ROUGE toolkit (Lin and Hovy, 2003) for evaluation of the generated summaries in comparison to the gold summaries.
139	21	The performance of SWAP-NET is comparable to that of SummaRuNNer and better than NN and other baselines.
152	30	Key word coverage measures the proportion of key words from those in the gold summary present in the generated summary.
155	160	Sentences with key words measures the proportion of sentences containing at least one key word.
166	18	In contrast, the average pairwise distance in Lead-3 summaries is 0.553 indicating higher redundancy.
169	34	We observe the presence of key words in all the overlapping segments of text with the gold summary indicating the importance of key words in finding salient sentences.
170	48	Modeling this interaction, we believe, is the reason for the superior performance of SWAP-NET in our experiments.
172	10	We present SWAP-NET, a neural sequence-tosequence model for extractive summarization that outperforms state-of-the-art extractive summarizers SummaRuNNer (Nallapati et al., 2017) and NN (Cheng and Lapata, 2016) on large scale benchmark datasets.
173	8	The architecture of SWAPNET is simpler than that of NN but due to its effective modeling of interaction between salient sentences and key words in a document, SWAPNET achieves superior performance.
174	10	SWAP-NET models this interaction using a new two-level pointer network based architecture with a switching mechanism.
175	15	Our experiments also suggest that modeling sentence-keyword interaction has the desirable property of less semantic redundancy in summaries generated by SWAP-NET.
