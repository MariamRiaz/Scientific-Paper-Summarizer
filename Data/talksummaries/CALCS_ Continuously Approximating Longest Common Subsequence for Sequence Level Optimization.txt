1	64	Specifically, neural text generation models, central focus of this work, have led to great progress in central downstream NLP tasks like text summarization, machine translation, and image captioning.
8	25	While, at test time, the models need to generate the entire sequence by feeding its own predictions at previous time steps.
10	13	The second drawback, called wrong objective, is due yet another discrepancy between training and testing.
18	19	We essentially introduce a continuous approximation to the discrete LCS metric which can be directly optimized against using standard gradient-based methods.
28	33	In this work, we explore the potential use of longest common subsequence (LCS) metric from an algorithmic point of view to address the aforementioned wrong objective and exposure bias problems.
30	81	To this end, we propose a way to continuously approximate LCS metric and use this differentiable approximation as the objective to train text generation models rather than the exact LCS measure, which is hard to optimize for due to non-differentiability of the corresponding loss function.
31	31	Although such differentiable approximation provides a unique advantage from modeling and optimization perspective, the difficulty of controlling its tightness might be a potential drawback in terms of its applicability.
41	14	Given two sequences y and z of tokens, longest common subsequence LCS(y, z) is defined as the longest sequence of tokens that appear left-to-right (but not necessarily in a contiguous block) in both sequences.
44	55	Let ri,j denote the longest common subsequence between prefix sequences y[:i] = (y1, y2, .
54	46	Intuitively, CALCS replaces the hard token comparison 1 [yi = zj ] in Eq.
55	29	1 with the probability p(yi)j as in Eq.
60	11	Characterization of this bound will enable us to theoretically argue about the feasibilty of using the proposed surrogate reward function for our objective as well as controlling its tightness.
79	20	More precisely, upper bound on the approximation error is represented as a sum of 1−max(pj)’s, hence the more peaked the model’s output probability distributions on average, the smaller the approximation error we are guaranteed by the established bounds.
87	30	One can further attempt to use Corollary 1 as a guide to pinpoint a range of α values to force the approximation error within certain desired limits.
104	12	We use pointergenerator network (See et al., 2017) as our baseline sequence-to-sequence model for text summarization.
107	17	On each decoding time step t, the decoder LSTM is fed the word embedding of the previous word, and computes a decoder state st, an attention distribution at over the words of input article, and a probability Pvocab(w) of generating word w for summary from output vocabulary V , which is then softly combined with the copy mode’s probability distribution Pcopy(w) via soft switch probability pgen ∈ [0, 1] by p (w) t = pgenPvocab(w) + (1− pgen)Pcopy(w) and Pcopy(w) = ∑ {i:wi=w} ati where ati indicates the attention probability on ith word of the input article.
114	38	Let {(x(l),y(l))}Nl=1 denote the set of training examples, where x(l)’s are input sequences, and y(l)’s are their corresponding ground-truth output sequences.
119	17	During the training with the joint loss, we compute JCALCS(x,y;Θ), defined in Eq.
125	14	We use a modified version of the CNN/Daily Mail dataset (Hermann et al., 2015) that is first used for summarization by (Nallapati et al., 2016).
126	35	However, we follow the processing script provided by (See et al., 2017) to obtain non-anonymized version of the data that contains 287,226 training pairs, 13,368 validation pairs, and 11,490 test pairs of news articles (781 tokens on average) and their corresponding ground-truth summaries (56 tokens on average).
128	26	For training our baseline model, we use single layer LSTM encoder (bi-directional) and decoder with hidden dimensions of 512 and 1024, respectively.
135	16	In Table 1, we report our main results on the summarization task.
140	16	The reason why ROUGE-L scores of our models are lower than previously reported is that we evaluate ROUGE-L score by taking the entire summary as a single sequence instead of splitting it into sentences, which is also the way we compute CALCS objective during the model training process.
141	54	The main motivation behind this approach is to encourage the model to preserve the sentence order within a summary, and evaluate its performance in the same way.
143	12	When POINTGEN*+SS and POINTGEN*+SS+CALCS are evaluated by splitting the generated summaries into sentences, their corresponding ROUGE-L scores become 35.38 and 35.12, respectively.
144	25	We also observe a nice sideimprovement of 1.0 point in ROUGE-1 score over the baseline, which achieves a comparable performance with the long-overdue LEAD-3 baseline score.
145	18	It might also be comparable to the recently reported state-of-the-art ROUGE-1 result on CNN/DailyMail dataset by Paulus et al. (2018) as they used a different dataset processing pipeline, which makes it difficult to directly compare with ours.
157	50	As a final remark, we would like to note that our proposed approach is orthogonal to advancements in more expressive and powerful architecture designs.
174	21	In this work we explored an alternative approach for training text generation models with sequencelevel optimization to combat wrong objective and exposure bias problems.
178	13	Our proposed approach suggests a promising alternative to policy-gradient methods to side step the difficulty of differentiating w.r.t reward function while directly optimizing for sequence-level discrete metrics.
