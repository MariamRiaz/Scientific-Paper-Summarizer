{"sections": [{"heading": "1. Introduction", "text": "Deep Neural Networks (LeCun et al., 2015) have been successful on numerous difficult machine learning tasks, including image recognition(Krizhevsky et al., 2012; Donahue et al., 2015), speech recognition(Hinton et al., 2012) and natural language processing(Collobert et al., 2011;\n*Equal contribution 1Massachusetts Institute of Technology 2New York University, Facebook AI Research. Correspondence to: Li Jing <ljing@mit.edu>, Yichen Shen <ycshen@mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nBahdanau et al., 2014; Sutskever et al., 2014). However, deep neural networks can suffer from vanishing and exploding gradient problems(Hochreiter, 1991; Bengio et al., 1994), which are known to be caused by matrix eigenvalues far from unity being raised to large powers. Because the severity of these problems grows with the the depth of a neural network, they are particularly grave for Recurrent Neural Networks (RNNs), whose recurrence can be equivalent to thousands or millions of equivalent hidden layers.\nSeveral solutions have been proposed to solve these problems for RNNs. Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997), which help RNNs contain information inside hidden layers with gates, remains one of the the most popular RNN implementations. Other recently proposed methods such as GRUs(Cho et al., 2014) and Bidirectional RNNs (Berglund et al., 2015) also perform well in numerous applications. However, none of these approaches has fundamentally solved the vanishing and exploding gradient problems, and gradient clipping is often required to keep gradients in a reasonable range.\nA recently proposed solution strategy is using orthogonal hidden weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015; Henaff et al., 2016), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers. This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al., 2015) or by alternating gradient-descent steps with projections onto the unitary subspace (Wisdom et al., 2016).\nIn this paper, we will first present an Efficient Unitary Neural Network (EUNN) architecture that parametrizes the entire space of unitary matrices in a complete and computationally efficient way, thereby eliminating the need for time-consuming unitary subspace-projections. Our architecture has a wide range of capacity-tunability to represent subspace unitary models by fixing some of our parameters; the above-mentioned unitary subspace models correspond to special cases of our architecture. We also implemented\nan EUNN with an earlier introduced FFT-like architecture which efficiently approximates the unitary space with minimum number of required parameters(Mathieu & LeCun, 2014b).\nWe then benchmark EUNN\u2019s performance on both simulated and real tasks: the standard copying task, the pixelpermuted MNIST task, and speech prediction with the TIMIT dataset (Garofolo et al., 1993). We show that our EUNN algorithm with an O(N) hidden layer size can compute up to the entire N \u00d7 N gradient matrix using O(1) computational steps and memory access per parameter. This is superior to theO(N) computational complexity of the existing training method for a full-space unitary network (Wisdom et al., 2016) and O(logN) more efficient than the subspace Unitary RNN(Arjovsky et al., 2015)."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. Basic Recurrent Neural Networks", "text": "A recurrent neural network takes an input sequence and uses the current hidden state to generate a new hidden state during each step, memorizing past information in the hidden layer. We first review the basic RNN architecture.\nConsider an RNN updated at regular time intervals t = 1, 2, ... whose input is the sequence of vectors x(t) whose hidden layer h(t) is updated according to the following rule:\nh(t) = \u03c3(Ux(t) +Wh(t\u22121)), (1)\nwhere \u03c3 is the nonlinear activation function. The output is generated by\ny(t) = Wh(t) + b, (2)\nwhere b is the bias vector for the hidden-to-output layer. For t = 0, the hidden layer h(0) can be initialized to some special vector or set as a trainable variable. For convenience of notation, we define z(t) = Ux(t) + Wh(t\u22121) so that h(t) = \u03c3(z(t))."}, {"heading": "2.2. The Vanishing and Exploding Gradient Problems", "text": "When training the neural network to minimize a cost function C that depends on a parameter vector a, the gradient descent method updates this vector to a \u2212 \u03bb\u2202C\u2202a , where \u03bb is a fixed learning rate and \u2202C\u2202a \u2261 \u2207C. For an RNN, the vanishing or exploding gradient problem is most significant during back propagation from hidden to hidden layers, so we will only focus on the gradient for hidden layers. Training the input-to-hidden and hidden-to-output matrices is relatively trivial once the hidden-to-hidden matrix has been successfully optimized.\nIn order to evaluate \u2202C\u2202Wij , one first computes the derivative\n\u2202C \u2202h(t) using the chain rule:\n\u2202C\n\u2202h(t) =\n\u2202C \u2202h(T ) \u2202h(T ) \u2202h(t) (3)\n= \u2202C\n\u2202h(T ) T\u22121\u220f k=t \u2202h(k+1) \u2202h(k) (4)\n= \u2202C\n\u2202h(T ) T\u22121\u220f k=t D(k)W, (5)\nwhere D(k) = diag{\u03c3\u2032(Ux(k) + Wh(k\u22121))} is the Jacobian matrix of the pointwise nonlinearity. For large times T , the term \u220f W plays a significant role. As long as the eigenvalues of D(k) are of order unity, then if W has eigenvalues \u03bbi 1, they will cause gradient explosion | \u2202C \u2202h(T )\n| \u2192 \u221e, while if W has eigenvalues \u03bbi 1, they can cause gradient vanishing, | \u2202C\n\u2202h(T ) | \u2192 0. Either situation\nprevents the RNN from working efficiently."}, {"heading": "3. Unitary RNNs", "text": ""}, {"heading": "3.1. Partial Space Unitary RNNs", "text": "In a breakthrough paper, Arjovsky, Shah & Bengio (Arjovsky et al., 2015) showed that unitary RNNs can overcome the exploding and vanishing gradient problems and perform well on long term memory tasks if the hiddento-hidden matrix in parametrized in the following unitary form:\nW = D3T2F\u22121D2\u03a0T1FD1. (6)\nHere D1,2,3 are diagonal matrices with each element ei\u03c9j , j = 1, 2, \u00b7 \u00b7 \u00b7 , n. T1,2 are reflection matrices, and T = I \u2212 2 v\u0302v\u0302 \u2020\n||v\u0302||2 , where v\u0302 is a vector with each of its entries as a parameter to be trained. \u03a0 is a fixed permutation matrix. F and F\u22121 are Fourier and inverse Fourier transform matrices respectively. Since each factor matrix here is unitary, the product W is also a unitary matrix.\nThis model uses O(N) parameters, which spans merely a part of the whole O(N2)-dimensional space of unitary N \u00d7 N matrices to enable computational efficiency. Several subsequent papers have tried to expand the space to O(N2) in order to achieve better performance, as summarized below."}, {"heading": "3.2. Full Space Unitary RNNs", "text": "In order to maximize the power of Unitary RNNs, it is preferable to have the option to optimize the weight matrix W over the full space of unitary matrices rather than a subspace as above. A straightforward method for implementing this is by simply updating W with standard backpropagation and then projecting the resulting matrix (which will typically no longer be unitary) back onto to the space\nof unitary matrices. Defining Gij \u2261 \u2202C\u2202Wij as the gradient with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016):\nA(t) \u2261 G(t) \u2020 W(t) \u2212W(t) \u2020 G(k), (7) W(t+1) \u2261 ( I + \u03bb\n2 A(t)\n)\u22121( I\u2212 \u03bb\n2 A(t)\n) W(t).(8)\nThis method shows that full space unitary networks are superior on many RNN tasks (Wisdom et al., 2016). A key limitation is that the back-propation in this method cannot avoid N -dimensional matrix multiplication, incurring O(N3) computational cost."}, {"heading": "4. Efficient Unitary Neural Network (EUNN) Architectures", "text": "In the following, we first describe a general parametrization method able to represent arbitrary unitary matrices with up to N2 degrees of freedom. We then present an efficient algorithm for this parametrization scheme, requiring only O(1) computational and memory access steps to obtain the gradient for each parameter. Finally, we show that our scheme performs significantly better than the above mentioned methods on a few well-known benchmarks."}, {"heading": "4.1. Unitary Matrix Parametrization", "text": "Any N \u00d7 N unitary matrix WN can be represented as a product of rotation matrices {Rij} and a diagonal matrix D, such that WN = D \u220fN i=2 \u220fi\u22121 j=1 Rij , where Rij is defined as the N -dimensional identity matrix with the elements Rii, Rij , Rji and Rjj replaced as follows (Reck et al., 1994; Clements et al., 2016):(\nRii Rij Rji Rjj\n) = ( ei\u03c6ij cos \u03b8ij \u2212ei\u03c6ij sin \u03b8ij\nsin \u03b8ij cos \u03b8ij\n) . (9)\nwhere \u03b8ij and \u03c6ij are unique parameters corresponding to Rij. Each of these matrices performs a U(2) unitary transformation on a two-dimensional subspace of the Ndimensional Hilbert space, leaving an (N\u22122)-dimensional subspace unchanged. In other words, a series of U(2) rotations can be used to successively make all off-diagonal elements of the given N \u00d7 N unitary matrix zero. This generalizes the familiar factorization of a 3D rotation matrix into 2D rotations parametrized by the three Euler angles. To provide intuition for how this works, let us briefly describe a simple way of doing this that is similar to Gaussian elimination by finishing one column at a time. There are infinitely many alternative decomposition schemes as well; Fig. 1 shows two that are particularly convenient to implement in software (and even in neuromorphic hardware (Shen et al., 2016)). The unitary matrix WN is multiplied from the right by a succession of unitary matrices\nRNj for j = N \u2212 1, \u00b7 \u00b7 \u00b7 , 1. Once all elements of the last row except the one on the diagonal are zero, this row will not be affected by later transformations. Since all transformations are unitary, the last column will then also contain only zeros except on the diagonal:\nWNRN,N\u22121RN,N\u22122 \u00b7 \u00b7RN,1 = (\nWN\u22121 0 0 eiwN\n) (10)\nThe effective dimensionality of the the matrix W is thus reduced toN\u22121. The same procedure can then be repeated N \u2212 1 times until the effective dimension of W is reduced to 1, leaving us with a diagonal matrix:1\nWNRN,N\u22121RN,N\u22122 \u00b7 \u00b7 \u00b7Ri,jRi,j\u22121 \u00b7 \u00b7 \u00b7R3,1R2,1 = D, (11)\nwhere D is a diagonal matrix whose diagonal elements are eiwj , from which we can write the direct representation of WN as\nWN = DR \u22121 2,1R \u22121 3,1 . . .R \u22121 N,N\u22122R \u22121 N,N\u22121\n= DR\u20322,1R \u2032 3,1 . . .R \u2032 N,N\u22122R \u2032 N,N\u22121. (12)\nwhere\nR\u2032ij = R(\u2212\u03b8ij ,\u2212\u03c6ij) = R(\u03b8ij , \u03c6ij)\u22121 = R\u22121ij (13)\n1Note that Gaussian Elimination would make merely the upper triangle of a matrix vanish, requiring a subsequent series of rotations (complete Gauss-Jordan Elimination) to zero the lower triangle. We need no such subsequent series because since W is unitary: it is easy to show that if a unitary matrix is triangular, it must be diagonal.\nThis parametrization thus involves N(N \u2212 1)/2 different \u03b8ij-values, N(N \u2212 1)/2 different \u03c6ij-values and N different wi-values, combining to N2 parameters in total and spans the entire unitary space. Note we can always fix a portion of our parameters, to span only a subset of unitary space \u2013 indeed, our benchmark test below will show that for certain tasks, full unitary space parametrization is not necessary. 2"}, {"heading": "4.2. Tunable space implementation", "text": "The representation in Eq. 12 can be made more compact by reordering and grouping specific rotational matrices, as was shown in the optical community (Reck et al., 1994; Clements et al., 2016) in the context of universal multiport interferometers. For example (Clements et al., 2016), a unitary matrix can be decomposed as\nWN = D ( R (1) 1,2R (1) 3,4 . . .R (1) N/2\u22121,N/2 ) \u00d7 ( R\n(2) 2,3R (2) 4,5 . . .R (2) N/2\u22122,N/2\u22121 ) \u00d7 . . .\n= DF (1) A F (2) B . . .F (L) B , (14)\nwhere every\nF (l) A = R (l) 1,2R (l) 3,4 . . .R (l) N/2\u22121,N/2\nis a block diagonal matrix, with N angle parameters in total, and\nF (l) B = R (l) 2,3R (l) 4,5 . . .R (l) N/2\u22122,N/2\u22121\nwithN\u22121 parameters, as is schematically shown in Fig. 1a. By choosing different values for L , WN will span a different subspace of the unitary space. Specifically,when L = N , WN will span the entire unitary space.\nFollowing this physics-inspired scheme, we decompose our unitary hidden-to-hidden layer matrix W as\nW = DF (1) A F (2) B F (3) A F (4) B \u00b7 \u00b7 \u00b7F (L) B . (15)"}, {"heading": "4.3. FFT-style approximation", "text": "Inspired by (Mathieu & LeCun, 2014a), an alternative way to organize the rotation matrices is implementing an FFTstyle architecture. Instead of using adjacent rotation matrices, each F here performs a certain distance pairwise rotations as shown in Fig. 1b:\nW = DF1F2F3F4 \u00b7 \u00b7 \u00b7Flog(N). (16)\nThe rotation matrices in Fi are performed between pairs of coordinates\n(2pk + j, p(2k + 1) + j) (17)\n2Our preliminary experimental tests even suggest that a fullcapacity unitary RNN is even undesirable for some tasks.\nwhere p = N2i , k \u2208 {0, ..., 2 i\u22121} and j \u2208 {1, ..., p}. This requires only log(N) matrices, so there are a total of N log(N)/2 rotational pairs. This is also the minimal number of rotations that can have all input coordinates interacting with each other, providing an approximation of arbitrary unitary matrices."}, {"heading": "4.4. Efficient implementation of rotation matrices", "text": "To implement this decomposition efficiently in an RNN, we apply vector element-wise multiplications and permutations: we evaluate the product Fx as\nFx = v1 \u2217 x + v2 \u2217 permute(x) (18)\nwhere \u2217 represents element-wise multiplication, F refers to general rotational matrices such as FA/B in Eq. 14 and Fi in Eq. 16. For the case of the tunable-space implementation, if we want to implement F(l)A in Eq. 14, we define v and the permutation as follows:\nv1 = (e i\u03c6 (l) 1 cos \u03b8 (l) 1 , cos \u03b8 (l) 1 , e i\u03c6 (l) 2 cos \u03b8 (l) 2 , cos \u03b8 (l) 2 , \u00b7 \u00b7 \u00b7 )\nv2 = (\u2212ei\u03c6 (l) 1 sin \u03b8 (l) 1 , sin \u03b8 (l) 1 ,\u2212ei\u03c6 (l) 2 sin \u03b82, sin \u03b8 (l) 2 , \u00b7 \u00b7 \u00b7 )\npermute(x) = (x2, x1, x4, x3, x6, x5, \u00b7 \u00b7 \u00b7 ).\nFor the FFT-style approach, if we want to implement F1 in Eq 16, we define v and the permutation as follows:\nv1 = (e i\u03c6 (l) 1 cos \u03b8 (l) 1 , e i\u03c6 (l) 2 cos \u03b8 (l) 2 , \u00b7 \u00b7 \u00b7 , cos \u03b8 (l) 1 , \u00b7 \u00b7 \u00b7 )\nv2 = (\u2212ei\u03c6 (l) 1 sin \u03b8 (l) 1 ,\u2212ei\u03c6 (l) 2 sin \u03b82, \u00b7 \u00b7 \u00b7 , sin \u03b8(l)1 , \u00b7 \u00b7 \u00b7 )\npermute(x) = (xn 2 +1 , xn 2 +2 \u00b7 \u00b7 \u00b7xn, x1, x2 \u00b7 \u00b7 \u00b7 ).\nIn general, the pseudocode for implementing operation F is as follows:\nAlgorithm 1 Efficient implementation for F with parameter \u03b8i and \u03c6i.\nInput: input x, size N ; parameters \u03b8 and \u03c6, size N/2; constant permuatation index list ind1 and ind2. Output: output y, size N . v1\u2190 concatenate(cos \u03b8, cos \u03b8 * exp(i\u03c6)) v2\u2190 concatenate(sin \u03b8, - sin \u03b8 * exp(i\u03c6)) v1\u2190 permute(v1, ind1) v2\u2190 permute(v2, ind1) y\u2190 v1 \u2217 x + v2 \u2217 permute(x, ind2)\nNote that ind1 and ind2 are different for different F.\nFrom a computational complexity viewpoint, since the operations \u2217 and permute take O(N) computational steps, evaluating Fx only requires O(N) steps. The product Dx is trivial, consisting of an element-wise vector multiplication. Therefore, the product Wx with the total unitary\nmatrix W can be computed in only O(NL) steps, and only requires O(NL) memory access (for full-space implementation L = N , for FFT-style approximation gives L = logN ). A detailed comparison on computational complexity of the existing unitary RNN architectures is given in Table 1."}, {"heading": "4.5. Nonlinearity", "text": "We use the same nonlinearity as (Arjovsky et al., 2015):\n(modReLU(z,b))i = zi |zi| \u2217 ReLU(|zi|+ bi) (19)\nwhere the bias vector b is a shared trainable parameter, and |zi| is the norm of the complex number zi.\nFor real number input, modReLU can be simplified to:\n(modReLU(z,b))i = sign(zi) \u2217 ReLU(|zi|+ bi) (20)\nwhere |zi| is the absolute value of the real number zi.\nWe empirically find that this nonlinearity function performs the best. We believe that this function possibly also serves as a forgetting filter that removes the noise using the bias threshold."}, {"heading": "5. Experimental tests of our method", "text": "In this section, we compare the performance of our Efficient Unitary Recurrent Neural Network (EURNN) with\n1. an LSTM RNN (Hochreiter & Schmidhuber, 1997),\n2. a Partial Space URNN (Arjovsky et al., 2015), and\n3. a Projective full-space URNN (Wisdom et al., 2016).\nAll models are implemented in both Tensorflow and Theano, available from https://github.com/ jingli9111/EUNN-tensorflow and https: //github.com/iguanaus/EUNN-theano."}, {"heading": "5.1. Copying Memory Task", "text": "We compare these networks by applying them all to the well defined Copying Memory Task (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016). The copying task is a synthetic task that is commonly used to test the network\u2019s ability to remember information seen T time steps earlier.\nSpecifically, the task is defined as follows (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016). An alphabet consists of symbols {ai}, the first n of which represent data, and the remaining two representing \u201cblank\u201d and \u201cstart recall\u201d, respectively; as illustrated by the following example where T = 20 and M = 5:\nInput: BACCA--------------------:---Output: -------------------------BACCA\nIn the above example, n = 3 and {ai} = {A,B,C,\u2212, :}. The input consists of M random data symbols (M = 5 above) followed by T \u2212 1 blanks, the \u201cstart recall\u201d symbol and M more blanks. The desired output consists of M +T blanks followed by the data sequence. The cost function C is defined as the cross entropy of the input and output sequences, which vanishes for perfect performance.\nWe use n = 8 and input length M = 10. The symbol for each input is represented by an n-dimensional one-hot vector. We trained all five RNNs for T = 1000 with the same batch size 128 using RMSProp optimization with a learning rate of 0.001. The decay rate is set to 0.5 for EURNN, and 0.9 for all other models respectively. (Fig. 2). This results show that the EURNN architectures introduced in both Sec.4.2 (EURNN with N=512, selecting L=2) and Sec.4.3 (FFT-style EURNN with N=512) outperform the LSTM model (which suffers from long term memory problems and only performs well on the copy task for small time delays T ) and all other unitary RNN models, both in-terms of learnability and in-terms of convergence rate. Note that the only other unitary RNN model that is able to beat the baseline for T = 1000 (Wisdom et al., 2016) is significantly slower than our method.\nMoreover, we find that by either choosing smaller L or by using the FFT-style method (so that W spans a smaller unitary subspace), the EURNN converges toward optimal performance significantly more efficiently (and also faster in wall clock time) than the partial (Arjovsky et al., 2015) and projective (Wisdom et al., 2016) unitary methods. The EURNN also performed more robustly. This means that a fullcapacity unitary matrix is not necessary for this particular task."}, {"heading": "5.2. Pixel-Permuted MNIST Task", "text": "The MNIST handwriting recognition problem is one of the classic benchmarks for quantifying the learning ability of neural networks. MNIST images are formed by a 28\u00d728 grayscale image with a target label between 0 and 9.\nTo test different RNN models, we feed all pixels of the MNIST images into the RNN models in 28\u00d728 time steps, where one pixel at a time is fed in as a floating-point number. A fixed random permutation is applied to the order of input pixels. The output is the probability distribution quantifying the digit prediction. We used RMSProp with a learning rate of 0.0001 and a decay rate of 0.9, and set the batch size to 128.\nAs shown in Fig. 3, EURNN significantly outperforms LSTM with the same number of parameters. It learns faster, in fewer iteration steps, and converges to a higher classifi-\nModel hidden size number of validation test (capacity) parameters accuracy accuracy LSTM 80 16k 0.908 0.902 URNN 512 16k 0.942 0.933\nPURNN 116 16k 0.922 0.921 EURNN (tunable style) 1024 (2) 13.3k 0.940 0.937\nEURNN (FFT style) 512 (FFT) 9.0k 0.928 0.925\ncation accuracy. In addition, the EURNN reaches a similar accuracy with fewer parameters. In Table. 2, we compare the performance of different RNN models on this task."}, {"heading": "5.3. Speech Prediction on TIMIT dataset", "text": "We also apply our EURNN to real-world speech prediction task and compare its performance to LSTM. The main task we consider is predicting the log-magnitude of future frames of a short-time Fourier transform (STFT) (Wisdom et al., 2016; Sejdi et al., 2009). We use the TIMIT dataset (Garofolo et al., 1993) sampled at 8 kHz. The audio .wav file is initially diced into different time frames (all frames have the same duration referring to the Hann analysis window below). The audio amplitude in each frame is then\nFourier transformed into the frequency domain. The logmagnitude of the Fourier amplitude is normalized and used as the data for training/testing each model. In our STFT operation we uses a Hann analysis window of 256 samples (32 milliseconds) and a window hop of 128 samples (16 milliseconds). The frame prediction task is as follows: given all the log-magnitudes of STFT frames up to time t, predict the log-magnitude of the STFT frame at time t+ 1 that has the minimum mean square error (MSE). We use\na training set with 2400 utterances, a validation set of 600 utterances and an evaluation set of 1000 utterances. The training, validation, and evaluation sets have distinct speakers. We trained all RNNs for with the same batch size 32 using RMSProp optimization with a learning rate of 0.001, a momentum of 0.9 and a decay rate of 0.1.\nThe results are given in Table. 3, in terms of the meansquared error (MSE) loss function. Figure. 4 shows prediction examples from the three types of networks, illustrat-\ning how EURNNs generally perform better than LSTMs. Furthermore, in this particular task, full-capacity EURNNs outperform small capacity EURNNs and FFT-style EURNNs."}, {"heading": "6. Conclusion", "text": "We have presented a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational cost is merely O(1) per parameter, which is O(logN) more efficient than the other methods discussed above. It significantly outperforms existing RNN architectures on the standard Copying Task, and the pixel-permuted MNIST Task using a comparable parameter count, hence demonstrating the highest recorded ability to memorize sequential information over long time periods.\nIt also performs well on real tasks such as speech prediction, outperforming an LSTM on TIMIT data speech prediction.\nWe want to emphasize the generality and tunability of our method. The ordering of the rotation matrices we presented in Fig. 1 are merely two of many possibilities; we used it simply as a concrete example. Other ordering options that can result in spanning the full unitary matrix space can be used for our algorithm as well, with identical speed and memory performance. This tunability of the span of the unitary space and, correspondingly, the total number of parameters makes it possible to use different capacities for different tasks, thus opening the way to an optimal performance of the EUNN. For example, as we have shown, a small subspace of the full unitary space is preferable for the copying task, whereas the MNIST task and TIMIT task are better performed by EUNN covering a considerably larger unitary space. Finally, we note that our method remains applicable even if the unitary matrix is decomposed into a different product of matrices (Eq. 12).\nThis powerful and robust unitary RNN architecture also might be promising for natural language processing because of its ability to efficiently handle tasks with long-term correlation and very high dimensionality."}, {"heading": "Acknowledgment", "text": "We thank Hugo Larochelle and Yoshua Bengio for helpful discussions and comments.\nThis work was partially supported by the Army Research Office through the Institute for Soldier Nanotechnologies under contract W911NF-13-D0001, the National Science Foundation under Grant No. CCF-1640012 and the Rothberg Family Fund for Cognitive Science."}], "year": 2017, "references": [{"title": "Unitary evolution recurrent neural networks", "authors": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1511.06464,", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "authors": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.0473,", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "authors": ["Bengio", "Yoshua", "Simard", "Patrice", "Frasconi", "Paolo"], "venue": "IEEE transactions on neural networks,", "year": 1994}, {"title": "Bidirectional recurrent neural networks as generative models", "authors": ["Berglund", "Mathias", "Raiko", "Tapani", "Honkala", "Mikko", "K\u00e4rkk\u00e4inen", "Leo", "Vetek", "Akos", "Karhunen", "Juha T"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "authors": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "year": 2014}, {"title": "An optimal design for universal multiport interferometers, 2016", "authors": ["Clements", "William R", "Humphreys", "Peter C", "Metcalf", "Benjamin J", "Kolthammer", "W. Steven", "Walmsley", "Ian A"], "year": 2016}, {"title": "Natural language processing (almost) from scratch", "authors": ["Collobert", "Ronan", "Weston", "Jason", "Bottou", "L\u00e9on", "Karlen", "Michael", "Kavukcuoglu", "Koray", "Kuksa", "Pavel"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Darpa timit acousticphonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "authors": ["Garofolo", "John S", "Lamel", "Lori F", "Fisher", "William M", "Fiscus", "Jonathon G", "Pallett", "David S"], "venue": "NASA STI/Recon technical report n,", "year": 1993}, {"title": "Orthogonal rnns and long-memory tasks", "authors": ["Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1602.06662,", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "authors": ["N Tara"], "venue": "IEEE Signal Processing Magazine,", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "authors": ["Hochreiter", "Sepp"], "venue": "Diploma, Technische Universita\u0308t Mu\u0308nchen, pp", "year": 1991}, {"title": "Long shortterm memory", "authors": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "authors": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "arXiv preprint arXiv:1504.00941,", "year": 2015}, {"title": "Fast approximation of rotations and hessians matrices", "authors": ["Mathieu", "Michael", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1404.7195,", "year": 2014}, {"title": "Fast approximation of rotations and hessians matrices", "authors": ["Mathieu", "Michal", "LeCun", "Yann"], "venue": "CoRR, abs/1404.7195,", "year": 2014}, {"title": "Experimental realization of any discrete unitary operator", "authors": ["Reck", "Michael", "Zeilinger", "Anton", "Bernstein", "Herbert J", "Bertani", "Philip"], "venue": "Phys. Rev. Lett.,", "year": 1994}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "authors": ["Saxe", "Andrew M", "McClelland", "James L", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1312.6120,", "year": 2013}, {"title": "Timefrequency feature representation using energy concentration: An overview of recent advances", "authors": ["Sejdi", "Ervin", "Djurovi", "Igor", "Jiang", "Jin"], "venue": "Digital Signal Processing,", "year": 2009}, {"title": "Deep learning with coherent nanophotonic circuits", "authors": ["Shen", "Yichen", "Harris", "Nicholas C", "Skirlo", "Scott", "Prabhu", "Mihika", "Baehr-Jones", "Tom", "Hochberg", "Michael", "Sun", "Xin", "Zhao", "Shijie", "Larochelle", "Hugo", "Englund", "Dirk"], "venue": "arXiv preprint arXiv:1610.02365,", "year": 2016}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "authors": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "year": 2014}, {"title": "Full-capacity unitary recurrent neural networks", "authors": ["Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John", "Le Roux", "Jonathan", "Atlas", "Les"], "venue": "In Advances In Neural Information Processing Systems,", "year": 2016}], "id": "SP:c5b8e636c64b7244bc81188fd874a3a8f5ab4141", "authors": [{"name": "Li Jing", "affiliations": []}, {"name": "Yichen Shen", "affiliations": []}, {"name": "Tena Dubcek", "affiliations": []}, {"name": "John Peurifoy", "affiliations": []}, {"name": "Scott Skirlo", "affiliations": []}, {"name": "Yann LeCun", "affiliations": []}, {"name": "Max Tegmark", "affiliations": []}, {"name": "Marin Solja\u010di\u0107", "affiliations": []}], "abstractText": "Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merelyO(1) per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixelpermuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.", "title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs"}