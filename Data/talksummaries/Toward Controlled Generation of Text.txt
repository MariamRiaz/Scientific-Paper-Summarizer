0	26	There is a surge of research interest in deep generative models (Hu et al., 2017), such as Variational Autoencoders (VAEs) (Kingma & Welling, 2013), Generative Adversarial Nets (GANs) (Goodfellow et al., 2014), and autoregressive models (van den Oord et al., 2016).
3	27	Previous work have been mostly limited to task-specific applications in supervised settings, including machine translation (Bahdanau et al., 2014) and image captioning (Vinyals et al., 2015).
25	29	We apply our model to generate sentences with controlled sentiment and tenses.
54	49	For instance, to control sentence sentiment, our model allocates one dimension of the latent representation to encode “positive” and “negative” semantics, and generates samples with desired sentiment by simply specifying a particular code.
58	17	We build our framework starting from variational autoencoders (§2) which have been used for text generation (Bowman et al., 2015), where sentence x̂ is generated conditioned on latent code z.
59	35	The vanilla VAE employs an unstructured vector z in which the dimensions are entangled.
60	33	To model and control the attributes of interest in an interpretable way, we augment the unstructured variables z with a set of structured variables c each of which targets a salient and independent semantic feature of sentences.
61	12	We want our sentence generator to condition on the combined vector (z, c), and generate samples that fulfill the attributes as specified in the structured code c. Conditional generation in the context of VAEs (e.g., semi-supervised VAEs (Kingma et al., 2014)) is often learned by reconstructing observed examples given their feature code.
67	26	Intuitively, having an interpretable representation would imply that each structured code in c can independently control its target feature, without entangling with other attributes, especially those not explicitly modeled.
68	11	We encourage the independency by enforcing those irrelevant at- tributes to be completely captured in the unstructured code z and thus be separated from c that we will manipulate.
69	27	To this end, we reuse the VAE encoder as an additional discriminator for recognizing the attributes modeled in z, and train the generator so that these unstructured attributes can be recovered from the generated samples.
70	12	As a result, varying different attribute codes will keep the unstructured attributes invariant as long as z is unchanged.
71	9	Figure 1 shows the overall model structure.
72	34	Our complete model incorporates VAEs and attribute discriminators, in which the VAE component trains the generator to reconstruct real sentences for generating plausible text, while the discriminators enforce the generator to produce attributes coherent with the conditioned code.
75	11	The collaborative optimization resembles wake-sleep algorithm.
92	27	The resulting “soft” generated sentence, denoted as eG⌧ (z, c), is fed into the discriminator1 to measure the fitness to the target attribute, leading to the following loss for improving G: LAttr,c(✓G) = Ep(z)p(c) h log qD(c| eG⌧ (z, c)) i .
93	12	(6) The temperature ⌧ (Eq.2) is set to ⌧ !
99	26	To address this, we introduce the independency constraint which separates these attributes with c by enforcing them to be fully captured by the unstructured part z.
110	14	The discriminator is learned in a different way compared to the VAE encoder, since the target attributes can be discrete which are not supported in the VAE framework.
117	36	To learn specified semantic meaning, we use a set of labeled examples XL = {(xL, cL)} to train the discriminator D with the following objective: Ls(✓D) = EXL [log qD(cL|xL)] .
118	30	(9) Besides, the conditional generator G is also capable of synthesizing (noisy) sentence-attribute pairs (x̂, c) which can be used to augment training data for semi-supervised learning.
134	18	Training of the discriminators need supervised data to impose designated semantics.
140	11	We apply our model to generate short sentences (length  15) with controlled sentiment and tense.
150	13	To study the size of labeled data required in the semi-supervised learning for accurate attribute control, we sample a small subset from SST-full, containing only 250 labeled sentences for training.
152	24	The lexicon from (Wilson et al., 2005) contains 2700 words with sentiment labels.
153	11	We use the lexicon for training by treating the words as sentences, and evaluate on the SST-full test set.
159	17	The lexicon mainly consists of verbs in different tenses (e.g., “was”, “will be”) as well as time expressions (e.g., “in the future”).
163	35	To avoid vanishingly small KL term in the VAE module (Eq.4) (Bowman et al., 2015), we use a KL term weight linearly annealing from 0 to 1 during training.
165	50	At test time sentences are generated with Eq.
166	10	We quantitatively measure sentence attribute control by evaluating the accuracy of generating designated sentiment, and the effect of using samples for training classifiers.
