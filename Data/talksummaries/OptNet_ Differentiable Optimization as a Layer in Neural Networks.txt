26	26	Energy-based learning methods These methods can be used for tasks like (structured) prediction where the training method shapes the energy function to be low around the observed data manifold and high elsewhere (LeCun et al., 2006).
40	24	Argmin differentiation Most closely related to our own work, there have been several papers that propose some form of differentiation through argmin operators.
48	27	In this paper, we use implicit differentiation (Dontchev & Rockafellar, 2009; Griewank & Walther, 2008) and techniques from matrix differential calculus (Magnus & Neudecker, 1988) to derive the gradients from the KKT matrix of the problem we are interested in.
50	25	We have also developed methods to make this approach practical and reasonably scalable within the context of deep architectures.
51	58	Although in the most general form, an OptNet layer can be any optimization problem, in this paper we will study OptNet layers defined by a quadratic program minimize z 1 2 zTQz + qT z subject to Az = b, Gz ≤ h (2) where z ∈ Rn is our optimization variable Q ∈ Rn×n 0 (a positive semidefinite matrix), q ∈ Rn, A ∈ Rm×n, b ∈ Rm, G ∈ Rp×n and h ∈ Rp are problem data, and leaving out the dependence on the previous layer zi as we showed in (1) for notational convenience.
53	15	In the neu- ral network setting, the optimal solution (or more generally, a subset of the optimal solution) of this optimization problems becomes the output of our layer, denoted zi+1, and any of the problem data Q, q,A, b,G, h can depend on the value of the previous layer zi.
54	19	The forward pass in our OptNet architecture thus involves simply setting up and finding the solution to this optimization problem.
68	13	∂b ∈ R n×m, we would simply substitute db = I (and set all other differential terms in the right hand side to zero), solve the equation, and the resulting value of dz would be the desired Jacobian.
74	37	We note that some of these parameters should depend on the previous layer zi and the gradients with respect to the previous layer can be obtained through the chain rule.
76	16	Deep networks are typically trained in mini-batches to take advantage of efficient data-parallel GPU operations.
77	18	Without mini-batching on the GPU, many modern deep learning architectures become intractable for all practical purposes.
78	42	However, today’s state-of-the-art QP solvers like Gurobi and CPLEX do not have the capability of solving multiple optimization problems on the GPU in parallel across the entire minibatch.
80	15	To overcome this performance bottleneck in our quadratic program layers, we have implemented a GPU-based primal-dual interior point method (PDIPM) based on Mattingley & Boyd (2012) that solves a batch of quadratic programs, and which provides the necessary gradients needed to train these in an end-to-end fashion.
82	33	Following the method of Mattingley & Boyd (2012), our solver introduces slack variables on the inequality constraints and iteratively minimizes the residuals from the KKT conditions over the primal variable z ∈ Rn, slack variable s ∈ Rp, and dual variables ν ∈ Rm associated with the equality constraints and λ ∈ Rp associated with the inequality constraints.
129	23	Each layer maps this input to an output of the same dimension; the linear layer does this with a batched matrix-vector multiplication and the OptNet layer does this by taking the argmin of a random QP that has the same number of inequality constraints as the dimensionality of the problem.
130	30	Figure 1 shows the profiling results (averaged over 10 tri- als) of the forward and backward passes.
132	34	Our next experiment illustrates why standard baseline QP solvers like CPLEX and Gurobi without batch support are too computationally expensive for QP OptNet layers to be tractable.
133	18	We set up random QP of the form (1) that have 100 variables and 100 inequality constraints in Gurobi and in the serialized and batched versions of our solver qpth and vary the batch size.4 Figure 2 shows the means and standard deviations of running each trial 10 times, showing that our batched solver outperforms Gurobi, itself a highly tuned solver for reasonable batch sizes.
134	20	For the minibatch size of 128, we solve all problems in an average of 0.18 seconds, whereas Gurobi tasks an average of 4.7 seconds.
139	30	Specifically, the total variation denoising approach attempts to smooth some noisy observed signal y by solving the optimization problem argmin z 1 2 ||y − z||+ λ||Dz||1 (12) where D is the first-order differencing operation, which can be expressed in matrix form by a matrix with rows Di = ei− ei+1 Penalizing the `1 norm of the signal difference encourages this difference to be sparse, i.e., the number of changepoints of the signal is small, and we end up approximating y by a (roughly) piecewise constant function.
159	13	One compelling use case of an OptNet layer is to learn constraints and dependencies over the output or latent space of a model.
162	43	Sudoku is a popular logical puzzle, where a (typically 9x9) grid of points must be arranged given some initial point, so that each row, each column, and each 3x3 grid of points must contain one of each number 1 through 9.
164	27	Sudoku is fundamentally a constraint satisfaction problem, and is trivial for computers to solve when told the rules of the game.
173	14	We know that Sudoku can be approximated well with a linear program (indeed, integer programming is a typical solution method for such problems), but the model here is told nothing about the rules of Sudoku.
174	15	We trained these models using ADAM (Kingma & Ba, 2014) to minimize the MSE (which we refer to as “loss”) on a dataset we created consisting of 9000 training puzzles, and we then tested the models on 1000 different heldout puzzles.
175	21	The error rate is the percentage of puzzles solved correctly if the cells are assigned to whichever index is largest in the prediction.
177	29	We contrast this with the performance of the OptNet network, which learns most of the correct hard constraints within the first three epochs and is able to generalize much better to unseen examples.
178	14	We have presented OptNet, a neural network architecture where we use optimization problems as a single layer in the network.
181	21	Our experiments highlight the potential power of these networks, showing that they can solve problems where existing networks are very poorly suited, such as learning Sudoku problems purely from data.
182	47	There are many future directions of research for these approaches, but we feel that they add another important primitive to the toolbox of neural network practitioners.
