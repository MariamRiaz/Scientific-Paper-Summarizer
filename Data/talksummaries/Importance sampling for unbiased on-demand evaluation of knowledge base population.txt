25	26	We believe the public availability of this service will speed the pace of progress in developing KBP systems.
26	59	In knowledge base population, each relation is a triple (SUBJECT, PREDICATE, OBJECT) where SUBJECT and OBJECT are some globally unique entity identifiers (e.g. Wikipedia page titles) and PREDICATE belongm to a specified schema.1 A KBP system returns an output in the form of relation instances (SUBJECT, PREDICATE, OBJECT, PROVENANCE), where PROVENANCE is a description of where exactly in the document corpus the relation was found.
28	34	The provenance also identifies that CARRIE FISHER is referenced by Fisher within the sentence.
42	50	In total, there are 70 system submissions from 18 teams for 317 evaluation entities (E) and the evaluation set consists of 11,008 labeled relation instances.3 The original evaluation dataset gives us a good measure of the true scores for the participating systems.
43	29	Similar to Zobel (1998), which studied pooling bias in information retrieval, we simulate the condition of a team not being part of the pooling process by removing any predictions that are unique to its systems from the evaluation dataset.
44	27	The pooling bias is then the difference between the true and unpooled scores.
45	59	Figure 3 shows the results of measuring pooling bias on the TAC KBP 2015 evaluation on the F1 metric using the official and anydoc scores.45 We observe that even with lenient anydoc heuristic, the median bias (2.05% F1) is much larger than largest difference between adjacently ranked systems (1.5% F1).
46	114	This experiment shows that pooling evaluation is significantly and systematically biased against systems that make novel predictions!
48	20	5 The outlier at rank 36 corresponds to a University of Texas, Austin system that only filtered predictions from other systems and hence has no unique predictions itself.
50	129	We could of course sidestep the problem by exhaustively annotating the entire document corpus, by annotating all mentions of entities and checking relations between all pairs of mentions.
51	44	However, that would be a laborious and prohibitively expensive task: using the interfaces we’ve developed (Section 6), it costs about $15 to annotate a single document by non-expert crowdworkers, resulting in an estimated cost of at least $1,350,000 for a reasonably large corpus of 90,000 documents (Dang, 2016).
52	39	The annotation effort would cost significantly more with expert annotators.
53	22	In contrast, labeling relation instances from system predictions can be an order of magnitude cheaper than finding them in documents: using our interfaces, it costs only about $0.18 to verify each relation instance compared to $1.60 per instance extracted through exhaustive annotations.
54	69	We propose a new paradigm called on-demand evaluation which takes a lazy approach to dataset construction by annotating predictions from systems only when they are underrepresented, thus correcting for pooling bias as it arises.
55	76	In this section, we’ll formalize the problem solved by ondemand evaluation independent of KBP and describe a cost-effective solution that allows us to accurately estimate evaluation scores without bias using importance sampling.
56	29	We’ll then instantiate the framework for KBP in Section 5.
57	74	Let X be the universe of (relation) instances, Y ⊆ X be the unknown subset of correct instances, X1, .
59	66	Let f(x) def = I[x ∈ Y] and gi(x) = I[x ∈ Xi], then the precision, πi, and recall, ri, of the set of predictions Xi is πi def = Ex∼pi [f(x)] ri def = Ex∼p0 [gi(x)], where pi is a distribution over Xi and p0 is a distribution over Y .
60	333	We assume that pi is known, e.g. the uniform distribution overXi and that we know p0 up to normalization constant and can sample from it.
64	24	Our goal is to estimate πi and ri for each system i = 1, .
66	76	, Xm respectively, and let Ŷ0 be a multiset of n0 samples drawn from Y .
68	27	4.3 Joint estimators6 The simple estimators are unbiased but have wastefully large variance because evaluating a new system does not leverage labels acquired for previous systems.
78	15	, X̂m with Ŷ0 when estimating recall ri?
80	20	Intuitively, if two systems have very similar predictions Xi and Xj , we should be able to use samples from one to estimate precision on the other.
81	27	However, it might also be the case that Xi and Xj only overlap on a small region, in which case the samples from Xj do not accurately represent instances in Xi and could lead to a biased estimate.
82	15	We address this problem by using importance sampling (Owen, 2013), a standard statistical technique for estimating properties of one distribution using samples from another distribution.
84	134	We would like the proposal distribution qi to both leverage samples from all m systems and be tailored towards system i.
88	22	Unfortunately, the standard importance sampling procedure requires us to draw and use samples from each distribution qi(x) independently and thus can not effectively reuse samples drawn from different distributions.
89	23	To this end, we introduce a practical refinement to the importance sampling procedure: we independently draw nj samples according to pj(x) from each of the m systems independently and then numerically integrate over these samples using the weights wij to “mix” them appropriately to produce and unbiased estimate of πi while reducing variance.
90	27	Formally, we define the joint precision estimator: π̂ (joint) i def = m∑ j=1 wij nj ∑ x∈X̂j pi(x)f(x) qi(x) , where each X̂j consists of nj i.i.d.
