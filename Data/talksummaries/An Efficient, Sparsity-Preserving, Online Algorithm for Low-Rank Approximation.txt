0	26	Low-rank approximation is an essential data processing technique for understanding large or noisy data in diverse areas including data compression, image and pattern recognition, signal processing, compressed sensing, latent semantic indexing, anomaly detection, and recommendation systems.
3	63	In this work, we introduce a novel low-rank approximation algorithm called Spectrum-Revealing LU (SRLU) that can be efficiently computed and updated.
4	31	Furthermore, SRLU preserves sparsity and can identify important data variables and observations.
5	67	Our algorithm works on any data matrix, and achieves an approximation accuracy that only differs from the accuracy of the best approximation possible for any given rank by a constant factor.1 The major innovation in SRLU is the efficient calculation of a truncated LU factorization of the form Π1AΠ T 2 = ( k m − k k L11 m − k L21 In−k ) ( k n − k U11 U12 S ) ≈ ( L11 L21 )( U11 U12 ) def = L̂Û, where Π1 and Π2 are judiciously chosen permutation matrices.
26	37	This work repurposes the LU factorization to create a novel efficient and effective low-rank approximation algorithm using modern randomization technology.
29	25	Randomized low-rank approximation algorithms generally fall into one of two categories: sampling algorithms and black box algorithms.
30	14	Sampling algorithms form data approximations from a random selection of rows and/or columns of the data.
33	38	Black box algorithms typically approximate a data matrix in the form A ≈ QTQA, where Q is an orthonormal basis of the random projection (usually using SVD, QR, or ID).
34	17	The result of (Johnson & Lindenstrauss, 1984) provided the theoretical groundwork for these algorithms, which have been extensively studied (Clarkson & Woodruff, 2012; Halko et al., 2011; Martinsson et al., 2006; Papadimitriou et al., 2000; Sarlos, 2006; Woolfe et al., 2008; Liberty et al., 2007; Gu, 2015).
39	30	Rank-revealing algorithms (Chan, 1987) are LRMA algorithms that guarantee the approximation is of high quality by also capturing the rank of the data within a tolerance (see supplementary materials for definitions).
44	14	A key advancement of this work is a new definition of high quality low-rank approximation: Definition 1 A rank-k truncated LU factorization is spectrum-revealing if∥∥∥A− L̂Û∥∥∥ 2 ≤ q1(k,m, n)σk+1 (A) and σj ( L̂Û ) ≥ σj (A) q2(k,m, n) for 1 ≤ j ≤ k and q1(k,m, n) and q2(k,m, n) are bounded by a low degree polynomial in k, m, and n. Definition 1 has precisely what we desire in an LRMA, and no additional requirements.
46	115	This work shows theoretically and numerically that our algorithm, SRLU, is spectrumrevealing in that it always finds such q1 and q2, often with q1, q2 = O(1) in practice.
47	50	Algorithm 3 TRLUCP 1: Inputs: Data matrix A ∈ Rm×n, target rank k, block size b, oversampling parameter p ≥ b, random Gaussian matrix Ω ∈ Rp×m, L̂ and Û are initially 0 matrices 2: Calculate random projection R = ΩA 3: for j = 0, b, 2b, · · · , k − b do 4: Perform column selection algorithm on R and swap columns of A 5: Update block column of L̂ 6: Perform block LU with partial row pivoting and swap rows of A 7: Update block row of Û 8: Update R 9: end for
56	16	The ordering is reasoned as follows: LU with partial row pivoting cannot be performed until the needed columns are selected, and so column selection must first occur at each iteration.
67	37	The projection R can be updated efficiently to become a random projection of the Schur complement for the next iteration.
73	26	Because TRLUCP does not provide an updated Schur complement, the largest element in the Schur complement can be approximated by finding the column of R with largest norm, performing a Schur update of that column, and then picking the largest element in that column.
74	19	Let α be this element, and, without loss of generality, assume it is the first entry of the Schur complement.
85	17	We will discuss spectrumrevealing properties of this factorization in Section 4.2.
96	14	Extra computational time, nevertheless, is needed to calculate M. A more efficient, approximate CUR decomposition can be obtained by replacing A with a high quality approximation (such as an SRLU factorization of high rank) in the calculation of M.
104	43	Theorem 1 simply concludes that the approximation is accurate if the Schur complement is small, but the singular value bounds of Theorem 2 are needed to guarantee that the approximation retains structural properties of the original data, such as an accurate approximation of the rank and the spectrum.
105	15	Furthermore, singular values bounds can be significantly stronger than the more familiar norm error bounds that appear in Theorem 1.
125	15	Stronger results are achieved with the CUR version of SRLU: Theorem 6 ‖Π1AΠT2 − L̂MÛ‖2 ≤ 2γσk+1 (A) and ‖Π1AΠT2 − L̂MÛ‖F ≤ ωσk+1 (A) , where γ = O (fk √ mn) is the same as in Theorem 4, and ω = O (fkmn).
133	23	Note that both methods exhibits a convergence rate similar to that of the truncated SVD (TSVD), and so only a constant amount of extra work is needed to achieve the same accuracy.
136	22	Note that for Subspace Iteration, we choose iteration parameter q = 0 and do not measure the time of applying the random projection, in acknowledgement that fast methods exist to apply a random projection to a data matrix.
138	91	See supplement for additional details.
150	20	S80PIn1 is 4,028 by 4,028, deter3 is 7,647 by 21,777, and lp ceria3d (abbreviated lc3d) is 3,576 by 4,400.
151	19	Note that SRLU, a more efficient algorithm, provides a better approximation in two of the three experiments.
155	21	Figure 3 shows the sparsity patterns of the factors of an SRLU factorization of a sparse data matrix representing a circuit simulation (oscil dcop), as well as a full LU decomposition of the data.
160	17	Note that the rows and columns chosen overlap with the astronaut and the planet, implying that minimal storage is needed to capture the black background, which composes approximately two thirds of the image.
161	37	While this result cannot be called feature selection per se, the rows and columns selected highlight where to look for features: rows and/or columns are selected in a higher density around the astronaut, the curvature of the planet, and the storm front on the planet.
