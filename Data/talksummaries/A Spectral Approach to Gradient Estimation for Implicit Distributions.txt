1	37	Popular examples include Generative adversarial networks (GAN) (Goodfellow et al., 2014; Mohamed & Lakshminarayanan, 2016).
2	32	Compared to the explicit likelihoods (e.g., Gaussian) in other deep generative models such as variational autoencoders (VAE) (Kingma & Welling, 2013), implicit distributions are shown able to capture the complex data manifold that lies in a high dimensional space, leading to more realistic samples generated by GAN than other models.
3	28	Besides, as the constraint of requiring an explicit density is removed, implicit distributions are treated as more flexible variants of variational families for approximate inference (Ranganath et al., 2016; Liu & Feng, 2016; Mescheder et al., 2017; Tran et al., 2017; Huszár, 2017; Li & Turner, 2018; Shi et al., 2018).
11	6	Li & Turner (2018) propose the Stein gradient estimator for the log density of an implicit distribution.
19	23	Moreover, there is no principled way to obtain gradient estimates at positions out of the sample points.
21	72	To approximate the gradient function of the log density (i.e., ∇x log q(x)), SSGE expands it in terms of the eigenfunctions of a kernel-based operator.
22	15	These eigenfunctions are orthogonal with respect to the underlying distribution.
23	7	By setting the test functions in the Stein’s identity to these eigenfunctions, we can take advantage of their orthogonality to obtain a simple solution.
36	6	,xM} from q and applying the equation to these samples, we obtain 1 M Kψ ≈ µψ, (3) where K is the Gram matrix: Kij = k(xi,xj), and ψ =[ ψ(x1), .
47	6	Central to these works is an equation that generalizes the original Stein’s identity (Stein, 1981), shown in the following theorem.
50	8	h : X → Rd′ is a smooth vectorvalued function h(x) = [h1(x), h2(x), .
52	40	(7) Then the following identity holds: Eq[h(x)∇x log q(x)> +∇xh(x)] = 0.
81	13	In this section we derive a gradient estimator for implicit distributions called the Spectral Stein Gradient Estimator (SSGE).
86	19	We denote the target gradient function to estimate by g : X → Rd: g(x) = ∇x log q(x).
101	10	Now truncating the series expansion to the first J terms and plugging in the Nyström approximations of {ψj}Jj=1, we get our estimator: ĝi(x) = J∑ j=1 β̂ijψ̂j(x), (16) β̂ij = − 1 M M∑ m=1 ∇xi ψ̂j(xm), (17) where ψ̂j is the Nyström approximation of ψj as in Section 2.1.
121	7	(18) are the sample errors caused by the Nyström approximation, which we call the estimation error.
123	9	The last term is caused by the bias introduced by the truncation, which we call the approximation error.
161	14	In Figure 1 we plot the gradients estimates produced by the Stein gradient estimator, its out-of-sample extension (Stein+) (see Section 2.2), and our approach (SSGE).
162	10	Since the original Stein estimator only gives estimates at the sample points, we plot them as individual points (in red).
167	6	We follow the settings in Sejdinovic et al. (2014); Strathmann et al. (2015) and consider a Gaussian Process classification problem on the UCI Glass dataset.
168	6	The goal is to infer the posterior over hyperparameters under a fully Bayesian treatment.
199	23	Note that the original Stein gradient estimator can also be used here.
205	20	We use a BNN with 1 hidden layer and 20 units to model the normalized inputs and targets.
207	6	We compare SSGE with implicit posteriors, Hamiltonian Monte Carlo (HMC) (Neal et al., 2011) and Bayes-by-backprop (BBB) (Blundell et al., 2015).
228	14	We can see that without the entropy term, the Implicit VAE tends to overfit and produces visually bad generations, while if we retain the entropy term and use SSGE to estimate its gradients, the Implicit VAE can produce realistic samples.
256	7	As KPCA embeddings are known to automatically adapt to the geometry of the samples, given a suitable kernel is chosen, it can reduce the curse of dimensionality when the estimator is applied to high dimensional spaces, which helps explain the effectiveness of SSGE.
257	9	Connection to Manifold-modeling Dimension Reduction Methods It has been pointed out in previous works (Williams, 2001; Ham et al., 2004; Bengio et al., 2004a;b) that many successful manifold-modeling dimension reduction methods (e.g., MDS, LLE, Laplacian eigenmaps, and Spectral clustering) can be viewed as KPCA with different ways of constructing data-dependent kernels.
259	24	We propose the Spectral Stein Gradient Estimator (SSGE) for implicit distributions.
260	47	Unlike previous methods, SSGE directly estimates the gradient function and thus has a principled out-of-sample extension.
261	50	Future work may include learning kernels or eigenfunctions in the estimator, as indicated by the error bound as well as the connection to dimension reduction methods.
