29	1	To overcome this difficulty, in Section 5, we remove the continuity assumption from g and rely on combinatorial tools from VapnikChervonenkis (VC) theory in order to present bounds valid for any bounded, lower semicontinuous function g. Whenever possible, the sample complexity bounds presented here are compared to similar ones in literature.
59	1	We do not provide robustness analysis of dictionary learning, since this would require a detailed mathematical definition of the notion “outlier”.
79	1	Unfortunately, this approach does not work when g is the indicator function of all k-sparse vectors; see Section 5.
108	1	Let us mention that factor dĝ −1(eh(T )) 2 appearing on the right hand side (rhs) of (22) is an upper bound for the Lipschitz constant of the aforementioned map.
129	2	For this purpose, we require that ĝ satisfies the additional (strict) inequality eh(T ) < sup t∈R ĝ(t).
134	1	Now assumption (29) reads as sup t∈R ĝmcp(t;λ2, γ) > 1 2 min{T 2, λ21} (30) or after some simple algebraic calculations, 1 2 λ22γ > 1 2 min{T 2, λ21} ⇔ λ2 > √ 1 γ min{T 2, λ21}.
143	1	Using combinatorial tools from VC theory, we remove the spurious condition on the coherence of D ∈ D appearing in previous works (Gribonval et al., 2015b; Vainsencher et al., 2011) and prove the UCEM property when g is bounded and lsc.
145	4	Next is presented Proposition 2, a modification of Theorem 20 in (Vainsencher et al., 2011): it states that map F from metric space (D, || · ||1,2) to metric space (FD, || · ||∞), FD := { min a∈Σk eh(||x−Da||2); D ∈ D } , (33) is not uniformly Lipschitz for any Lipschitz constant.3 This is the main reason we resign (ourselves) from previous proof techniques.
152	1	In such a way, the authors restrict their analysis on a subspace of original space of all unit-norm column dictionaries.
156	1	A direct use of Proposition’s 3 bound in the popular VapnikChervonenkis’s theorem (Theorem 12.5, (Devroye et al., 1997)) generates Theorem 2 and its byproduct Proposition 4.
159	1	Let fD : BRm(T ) → [0, eh(T )] for each fD 3 Although Proposition 2 has the same formulation as Theorem 20 of (Vainsencher et al., 2011), the latter cannot apply directly in our case except for k = 2.
166	1	(38) When fD(x) = eh(||x||2) and eh(t) = t2, the bounds for the absolute difference in the rhs of (38) in (Gribonval et al., 2015b) and (Vainsencher et al., 2011) are of order O (√ logn n ) and O (√ log( √ n) n ) respectively.
178	1	The approximation error does not depend on the sample size n; it is determined by the family of losses under study and the probability distribution of the data.
180	1	The authors in (Vovk, 2016), Section 24, show that the two objectives, of good data approximation and of sparsity of the combination vector a, are incompatible if the data distribution puts its mass far from any low dimensional subspace and in such cases app 6= 0.
181	1	In this section, assuming m d, app is considered a function of d. An upper bound for app as d→ m, valid for any probability measure µ ∈ P̄ , gives insights to the problem of approximating points in Rm with combinations of points lying on subspaces of dimension d. Following the approach in (Liu & Tao, 2016), we relate the optimal population risk R(D∗) to the quantization error of probability measure µ.
182	1	Next proposition is meaningful only in the case where g is the indicator function of special compact subsets of Rd, i.e., g(a) = χK(a) withK ⊂ Rd; χK(a) alternates between zero and infinity according to whether its actual argument belongs in K or not.
183	3	Specifically, K is assumed to contain the basis vectors of the positive orthant.
189	1	(46) The bound in (46) depends on m and d. Despite being “weak”, as m−2/m → 1, this upper bound provides an insight to the problem: when m is fixed, but sufficiently large, and d → m, the approximation error decreases as d increases, at rate O(d−2/m).
191	7	This article is a theoretical analysis on the sample complexity of dictionary learning when the loss function to be minimized is the sum of the Moreau envelope of some univariate lsc function h on the real line and a regularization function g. We derive generalization bounds for a wide range of g, including the case of the indicator function of all k-sparse vectors.
192	11	As a byproduct of this analysis is provided some intuition behind the popularity of loss functions under study in the context of “gross outliers”, that is, samples with arbitrary “large” values.
193	19	Finally, we comment on the approximation error of an ideal family of losses when the dimension m d, where d is the size of the dictionary.
194	162	In the future, it would be interesting to characterize the differentiability properties of the losses under study.
195	163	Such an analysis would have direct practical applications on the design of numerical optimization algorithms.
