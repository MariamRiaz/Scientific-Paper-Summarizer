22	3	Neither underfitting nor overfitting are desirable when learning a probabilistic representation of the data.
24	50	This paper addresses model selection issues in variational inference.
25	25	Our key contributions are: (i) An extension of the concept of evidence score and its use as a new modelselection criterion.
26	38	(ii) Derivation of novel importanceweighted evidence bounds, and proof of their theoretical properties.
27	1	(iii) Development of a novel variational inference procedure that favors more plausible models, compared to existing VI-based approaches.
28	42	(iv) A new low bias-variance variational estimator with an improved update rule for optimization.
29	111	Let pd(x) be the true and unknown data-generating distribution, which we seek to model with pα(x) = ∫ pα(x, z)dz where pα(x, z) = pα(x|z)p(z).
30	210	Here z ∈ Rd represents latent variables responsible for x ∈ Rp, p(z) is a specified prior on z, and pα(x|z) is the conditional distribution of the data with model parameters α.
31	63	The joint distribution may also be expressed pα(x, z) = pα(x)pα(z|x), where pα(z|x) is the model conditional distribution of latent z given x.
32	20	The posterior pα(z|x) is typically difficult to compute, and therefore in variational inference it is approximated by qβ(z|x), a distribution with parameters β.
33	96	The evidence lower bound (ELBO) is defined ELBO(pα(x, z), qβ(z|x)) = Eqβ(z|x) log [pα(x, z) qβ(z|x) ] .
34	93	(1) It is well known that ELBO(pα(x, z), qβ(z|x)) = log pα(x) − KL(qβ(z|x)‖pα(z|x)) ≤ log pα(x), where KL(p‖q) is the Kullback-Leibler divergence between distributions p and q.
35	13	Hence, the ELBO, characterized by cumulative parameters θ = (α, β), serves as a lower bound on evidence log pα(x).
43	23	Our key observation is that existing bounds are almost exclusively based on the Jensen inequality, which implies the variational gap can be improved with a less convex transform.
46	9	Let φ(u) : R+ → R be a non-decreasing function defined on the non-negative real line, referred to as the evidence function.
47	19	Further, assume (i) φ(u) is concave, (ii) ψ(u) is a convex and monotonically increasing function, and (iii) h(u) , ψ(φ(u)) is concave.
50	25	The K-sample Generalized Evidence Lower Bound (GLBO) is defined as GLBO(x;K) , ψ−1 ( EZ1:K∼qβ [ h ( 1 K K∑ k=1 pα(x, Zk) qβ(Zk|x) )]) , (2) where Z1:K = {Zk}Kk=1 are K iid samples from qβ(z|x).
53	4	Concerning intuitions for the above assumptions, the concavity of φ(u) in (i) reflects that, in general, we want to use a φ(u) that is monotonically increasing.
55	10	Assumption (ii) from above introduces a convex auxiliary function ψ(u), and (iii) states that the concavity of φ(u) dominates the convexity of ψ(u).
56	13	As discussed below, the additional convexity from ψ(u) generally improves our variational bound.
65	8	Note this is for the special case where φ = log(u).
66	10	We now consider a few concrete examples.
67	8	Letting T ≥ 1 be a temperature parameter, we define the K-sample χ-Evidence Lower Bound (CLBO) to be CLBO(x;K,T ) , GLBO(x;K,φ = log(u), ψ = exp(T−1u)).
69	18	Further, our K-sample CLBO is superior to the K-sample bound used in (Dieng et al., 2017; Li & Turner, 2016).
70	6	Specifically, our lower bound is guaranteed to be sharper than RVB (see Section 5.1, Theorem 8), and our upper bound is guaranteed to be an upper bound (see SM), while for RVB this property only holds in the asymptotic limit.
71	38	See additional discussion in Section 5.1 and experimental results in Figure 1.
72	38	Stronger results can be established for the CLBO in (4).
74	9	Let 1 ≤ T1 ≤ T2, then CLBO(x;K,T2) ≤ CLBO(x;K,T1).
75	42	While in theory as T → 1 the bound gets sharper, we note that in practice the empirical estimator becomes more unstable as the bound gets sharper.
