{"sections": [{"heading": "1. Introduction", "text": "Given a function f : Rd \u2192 R, a gradient descent aims to minimize the function via the following iteration:\nxt+1 = xt \u2212 \u03b7\u2207f(xt),\nwhere \u03b7 > 0 is a step size. Gradient descent and its variants (e.g., stochastic gradient) are widely used in machine learning applications due to their favorable computational properties. This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation (Rumelhart et al., 1988).\nGradient descent is especially useful in high-dimensional settings because the number of iterations required to reach\n1University of California, Berkeley 2Duke University 3Microsoft Research India 4University of Washington. Correspondence to: Chi Jin <chijin@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\na point with small gradient is independent of the dimension (\u201cdimension-free\u201d). More precisely, for a function that is `- gradient Lipschitz (see Definition 1), it is well known that gradient descent finds an -first-order stationary point (i.e., a point x with \u2016\u2207f(x)\u2016 \u2264 ) within `(f(x0) \u2212 f?)/ 2 iterations (Nesterov, 1998), where x0 is the initial point and f? is the optimal value of f . This bound does not depend on the dimension of x. In convex optimization, finding an -first-order stationary point is equivalent to finding an approximate global optimum.\nIn non-convex settings, however, convergence to first-order stationary points is not satisfactory. For non-convex functions, first-order stationary points can be global minima, local minima, saddle points or even local maxima. Finding a global minimum can be hard, but fortunately, for many non-convex problems, it is sufficient to find a local minimum. Indeed, a line of recent results show that, in many problems of interest, all local minima are global minima (e.g., in tensor decomposition (Ge et al., 2015), dictionary learning (Sun et al., 2016a), phase retrieval (Sun et al., 2016b), matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016), matrix completion (Ge et al., 2016), and certain classes of deep neural networks (Kawaguchi, 2016)). Moreover, there are suggestions that in more general deep networks most of the local minima are as good as global minima (Choromanska et al., 2014).\nOn the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems (see, e.g., Jain et al., 2015; Sun et al., 2016b). Furthermore, Dauphin et al. (2014) argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks. Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible. Ge et al. (2015) showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2). Lee et al. (2016) proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding\nAlgorithm 1 Perturbed Gradient Descent (Meta-algorithm) for t = 0, 1, . . . do\nif perturbation condition holds then xt \u2190 xt + \u03bet, \u03bet uniformly \u223c B0(r) xt+1 \u2190 xt \u2212 \u03b7\u2207f(xt)\nnoise. However, this result does not bound the number of steps needed to reach a local minimum.\nPrevious work explains why gradient descent avoids saddle points in the nonconvex setting, but not why it is efficient\u2014all of them have runtime guarantees with high polynomial dependency in dimension d. For instance, the number of iterations required in Ge et al. (2015) is at least \u2126(d4), which is prohibitive in high dimensional setting such as deep learning (typically with millions of parameters). Therefore, we wonder whether gradient descent type of algorithms are fundamentally slow in escaping saddle points, or it is the lack of our theoretical understanding while gradient descent is indeed efficient. This motivates the following question: Can gradient descent escape saddle points and converge to local minima in a number of iterations that is (almost) dimension-free?\nIn order to answer this question formally, this paper investigates the complexity of finding -second-order stationary points. For \u03c1-Hessian Lipschitz functions (see Definition 5), these points are defined as (Nesterov & Polyak, 2006):\n\u2016\u2207f(x)\u2016 \u2264 , and \u03bbmin(\u22072f(x)) \u2265 \u2212 \u221a \u03c1 .\nUnder the assumption that all saddle points are strict (i.e., for any saddle point xs, \u03bbmin(\u22072f(xs)) < 0), all second-order stationary points ( = 0) are local minima. Therefore, convergence to second-order stationary points is equivalent to convergence to local minima.\nThis paper studies a simple variant of gradient descent (with phasic perturbations, see Algorithm 1). For `-smooth functions that are also Hessian Lipschitz, we show that perturbed gradient descent will converge to an -second-order stationary point in O\u0303(`(f(x0)\u2212 f?)/ 2), where O\u0303(\u00b7) hides polylog factors. This guarantee is almost dimension free (up to polylog(d) factors), answering the above highlighted question affirmatively. Note that this rate is exactly the same as the well-known convergence rate of gradient descent to first-order stationary points (Nesterov, 1998), up to log factors. Furthermore, our analysis admits a maximal step size of up to \u2126(1/`), which is the same as that in analyses for first-order stationary points.\nAs many real learning problems present strong local geometric properties, similar to strong convexity in the global setting (see, e.g. Bhojanapalli et al., 2016; Sun & Luo, 2016; Zheng & Lafferty, 2016), it is important to note that our analysis naturally takes advantage of such local struc-\nture. We show that when local strong convexity is present, the -dependence goes from a polynomial rate, 1/ 2, to linear convergence, log(1/ ). As an example, we show that sharp global convergence rates can be obtained for matrix factorization as a direct consequence of our analysis."}, {"heading": "1.1. Our Contributions", "text": "This paper presents the first sharp analysis that shows that (perturbed) gradient descent finds an approximate secondorder stationary point in at most polylog(d) iterations, thus escaping all saddle points efficiently. Our main technical contributions are as follows:\n\u2022 For `-gradient Lipschitz, \u03c1-Hessian Lipschitz functions (possibly non-convex), gradient descent with appropriate perturbations finds an -second-order stationary point in O\u0303(`(f(x0)\u2212f?)/ 2) iterations. This rate matches the well-known convergence rate of gradient descent to first-order stationary points up to log factors.\n\u2022 Under a strict-saddle condition (see Assumption A2), the same convergence result applies for local minima. This means that gradient descent can escape all saddle points with only logarithmic overhead in runtime.\n\u2022 When the function has local structure, such as local strong convexity (see Assumption A3.a), the above results can be further improved to linear convergence. We give sharp rates that are comparable to previous problem-specific local analysis of gradient descent with smart initialization (see Section 1.2).\n\u2022 All the above results rely on a new characterization of the geometry around saddle points: points from where gradient descent gets stuck at a saddle point constitute a thin \u201cband.\u201d We develop novel techniques to bound the volume of this band. As a result, we can show that after a random perturbation the current point is very unlikely to be in the \u201cband\u201d; hence, efficient escape from the saddle point is possible (see Section 5)."}, {"heading": "1.2. Related Work", "text": "Over the past few years, there have been many problemspecific convergence results for non-convex optimization. One line of work requires a smart initialization algorithm to provide a coarse estimate lying inside a local neighborhood, from which popular local search algorithms enjoy fast local convergence (see, e.g., Netrapalli et al., 2013; Candes et al., 2015; Sun & Luo, 2016; Bhojanapalli et al., 2016). While there are not many results that show global convergence for non-convex problems, Jain et al. (2015) show that gradient descent yields global convergence rates for matrix square-root problems. Although these results\ngive strong guarantees, the analyses are heavily tailored to specific problems, and it is unclear how to generalize them to a wider class of non-convex functions.\nFor general non-convex optimization, there are a few previous results on finding second-order stationary points. These results can be divided into the following three categories, where, for simplicity of presentation, we only highlight dependence on dimension d and , assuming that all other problem parameters are constant from the point of view of iteration complexity:\nHessian-based: Traditionally, only second-order optimization methods were known to converge to second-order stationary points. These algorithms rely on computing the Hessian to distinguish between first- and second-order stationary points. Nesterov & Polyak (2006) designed a cubic regularization algorithm which converges to an -secondorder stationary point in O(1/ 1.5) iterations. Trust region algorithms (Curtis et al., 2014) can also achieve the same performance if the parameters are chosen carefully. These algorithms typically require the computation of the inverse of the full Hessian per iteration, which can be very expensive.\nHessian-vector-product-based: A number of recent papers have explored the possibility of using only Hessianvector products instead of full Hessian information in order to find second-order stationary points. These algorithms require a Hessian-vector product oracle: given a function f , a point x and a direction u, the oracle returns \u22072f(x) \u00b7 u. Agarwal et al. (2016) and Carmon et al. (2016) presented accelerated algorithms that can find an -second-order stationary point in O(log d/ 7/4) steps. Also, Carmon & Duchi (2016) showed by running gradient descent as a subroutine to solve the subproblem of cubic regularization\n(which requires Hessian-vector product oracle), it is possible to find an -second-order stationary pointinO(log d/ 2) iterations. In many applications such an oracle can be implemented efficiently, in roughly the same complexity as the gradient oracle. Also, when the function has a Hessian Lipschitz property such an oracle can be approximated by differentiating the gradients at two very close points (although this may suffer from numerical issues, thus is seldom used in practice).\nGradient-based: Another recent line of work shows that it is possible to converge to a second-order stationary point without any use of the Hessian. These methods feature simple computation per iteration (only involving gradient operations), and are closest to the algorithms used in practice. Ge et al. (2015) showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/ ) iterations, with polynomial of order at least four. This was improved in Levy (2016) to O(d3 \u00b7 poly(1/ )) using normalized gradient descent. The current paper improves on both results by showing that perturbed gradient descent can actually find an -second-order stationary point in O(polylog(d)/ 2) steps, which matches the guarantee for converging to first-order stationary points up to polylog factors."}, {"heading": "2. Preliminaries", "text": "In this section, we will first introduce our notation, and then present some definitions and existing results in optimization which will be used later."}, {"heading": "2.1. Notation", "text": "We use bold upper-case letters A,B to denote matrices and bold lower-case letters x,y to denote vectors. Aij means the (i, j)th entry of matrix A. For vectors we use \u2016\u00b7\u2016 to denote the `2-norm, and for matrices we use \u2016\u00b7\u2016 and \u2016\u00b7\u2016F to denote spectral norm and Frobenius norm respectively. We use \u03c3max(\u00b7), \u03c3min(\u00b7), \u03c3i(\u00b7) to denote the largest, the smallest and the i-th largest singular values respectively, and \u03bbmax(\u00b7), \u03bbmin(\u00b7), \u03bbi(\u00b7) for corresponding eigenvalues.\nFor a function f : Rd \u2192 R, we use \u2207f(\u00b7) and \u22072f(\u00b7) to denote its gradient and Hessian, and f? to denote the global minimum of f(\u00b7). We use notation O(\u00b7) to hide only absolute constants which do not depend on any problem parameter, and notation O\u0303(\u00b7) to hide only absolute constants and log factors. We let B(d)x (r) denote the d-dimensional ball centered at x with radius r; when it is clear from context, we simply denote it as Bx(r). We use PX (\u00b7) to denote projection onto the set X . Distance and projection are always defined in a Euclidean sense."}, {"heading": "2.2. Gradient Descent", "text": "The theory of gradient descent often takes its point of departure to be the study of convex optimization. Definition 1. A differentiable function f(\u00b7) is `-smooth (or `-gradient Lipschitz) if:\n\u2200x1,x2, \u2016\u2207f(x1)\u2212\u2207f(x2)\u2016 \u2264 `\u2016x1 \u2212 x2\u2016.\nDefinition 2. A twice-differentiable function f(\u00b7) is \u03b1strongly convex if \u2200x, \u03bbmin(\u22072f(x)) \u2265 \u03b1\nSuch smoothness guarantees imply that the gradient can not change too rapidly, and strong convexity ensures that there is a unique stationary point (and hence a global minimum). Standard analysis using these two properties shows that gradient descent converges linearly to a global optimum x? (see e.g. (Bubeck et al., 2015)). Theorem 1. Assume f(\u00b7) is `-smooth and \u03b1-strongly convex. For any > 0, if we run gradient descent with step size \u03b7 = 1` , iterate xt will be -close to x ? in iterations:\n2` \u03b1 log \u2016x0 \u2212 x?\u2016\nIn a more general setting, we no longer have convexity, let alone strong convexity. Though global optima are difficult to achieve in such a setting, it is possible to analyze convergence to first-order stationary points. Definition 3. For a differentiable function f(\u00b7), we say that x is a first-order stationary point if \u2016\u2207f(x)\u2016 = 0; we also say x is an -first-order stationary point if \u2016\u2207f(x)\u2016 \u2264 .\nUnder an `-smoothness assumption, it is well known that by choosing the step size \u03b7 = 1` , gradient descent converges to first-order stationary points. Theorem 2 ((Nesterov, 1998)). Assume that the function f(\u00b7) is `-smooth. Then, for any > 0, if we run gradient descent with step size \u03b7 = 1` and termination condition \u2016\u2207f(x)\u2016 \u2264 , the output will be -first-order stationary point, and the algorithm will terminate within the following number of iterations:\n`(f(x0)\u2212 f?) 2 .\nNote that the iteration complexity does not depend explicitly on intrinsic dimension; in the literature this is referred to as \u201cdimension-free optimization.\u201d\nNote that a first-order stationary point can be either a local minimum or a saddle point or a local maximum. For minimization problems, saddle points and local maxima are undesirable, and we abuse nomenclature to call both of them \u201csaddle points\u201d in this paper. The formal definition is as follows:\nDefinition 4. For a differentiable function f(\u00b7), we say that x is a local minimum if x is a first-order stationary point, and there exists > 0 so that for any y in the - neighborhood of x, we have f(x) \u2264 f(y); we also say x is a saddle point if x is a first-order stationary point but not a local minimum. For a twice-differentiable function f(\u00b7), we further say a saddle point x is strict (or nondegenerate) if \u03bbmin(\u22072f(x)) < 0.\nFor a twice-differentiable function f(\u00b7), we know a saddle point x must satify \u03bbmin(\u22072f(x)) \u2264 0. Intuitively, for saddle point x to be strict, we simply rule out the undetermined case \u03bbmin(\u22072f(x)) = 0, where Hessian information alone is not enough to check whether x is a local minimum or saddle point. In most non-convex problems, saddle points are undesirable.\nTo escape from saddle points and find local minima in a general setting, we move both the assumptions and guarantees in Theorem 2 one order higher. In particular, we require the Hessian to be Lipschitz:\nDefinition 5. A twice-differentiable function f(\u00b7) is \u03c1Hessian Lipschitz if:\n\u2200x1,x2, \u2016\u22072f(x1)\u2212\u22072f(x2)\u2016 \u2264 \u03c1\u2016x1 \u2212 x2\u2016.\nThat is, Hessian can not change dramatically in terms of spectral norm. We also generalize the definition of firstorder stationary point to higher order:\nDefinition 6. For a \u03c1-Hessian Lipschitz function f(\u00b7), we say that x is a second-order stationary point if \u2016\u2207f(x)\u2016 = 0 and \u03bbmin(\u22072f(x)) \u2265 0; we also say x is -second-order stationary point if:\n\u2016\u2207f(x)\u2016 \u2264 , and \u03bbmin(\u22072f(x)) \u2265 \u2212 \u221a \u03c1\nSecond-order stationary points are very important in nonconvex optimization because when all saddle points are strict, all second-order stationary points are exactly local minima.\nNote that the literature sometime defines -second-order stationary point by two independent error terms; i.e., letting \u2016\u2207f(x)\u2016 \u2264 g and \u03bbmin(\u22072f(x)) \u2265 \u2212 H . We instead follow the convention of Nesterov & Polyak (2006) by choosing H = \u221a \u03c1 g to reflect the natural relations between the gradient and the Hessian."}, {"heading": "3. Main Result", "text": "In this section we show that it possible to modify gradient descent in a simple way so that the resulting algorithm will provably converge quickly to a second-order stationary point.\nAlgorithm 2 Perturbed Gradient Descent: PGD(x0, `, \u03c1, , c, \u03b4,\u2206f )\n\u03c7\u2190 3 max{log(d`\u2206fc 2\u03b4 ), 4}, \u03b7 \u2190 c ` , r \u2190\n\u221a c \u03c72 \u00b7 `\ngthres \u2190 \u221a c \u03c72 \u00b7 , fthres \u2190 c \u03c73 \u00b7\n\u221a 3\n\u03c1 , tthres \u2190 \u03c7 c2 \u00b7 \u221a\u0300\u03c1\ntnoise \u2190 \u2212tthres \u2212 1 for t = 0, 1, . . . do\nif \u2016\u2207f(xt)\u2016 \u2264 gthres and t\u2212 tnoise > tthres then x\u0303t \u2190 xt, tnoise \u2190 t xt \u2190 x\u0303t + \u03bet, \u03bet uniformly \u223c B0(r) if t \u2212 tnoise = tthres and f(xt) \u2212 f(x\u0303tnoise) > \u2212fthres then\nreturn x\u0303tnoise xt+1 \u2190 xt \u2212 \u03b7\u2207f(xt)\nThe algorithm that we analyze is a perturbed form of gradient descent (see Algorithm 2). The algorithm is based on gradient descent with step size \u03b7. When the norm of the current gradient is small (\u2264 gthres) (which indicates that the current iterate x\u0303t is potentially near a saddle point), the algorithm adds a small random perturbation to the gradient. The perturbation is added at most only once every tthres iterations.\nTo simplify the analysis we choose the perturbation \u03bet to be uniformly sampled from a d-dimensional ball1. The use of the threshold tthres ensures that the dynamics are mostly those of gradient descent. If the function value does not decrease enough (by fthres) after tthres iterations, the algorithm outputs x\u0303tnoise . The analysis in this section shows that under this protocol, the output x\u0303tnoise is necessarily \u201cclose\u201d to a second-order stationary point.\nWe first state the assumptions that we require. Assumption A1. Function f(\u00b7) is both `-smooth and \u03c1Hessian Lipschitz.\nThe Hessian Lipschitz condition ensures that the function is well-behaved near a saddle point, and the small perturbation we add will suffice to allow the subsequent gradient updates to escape from the saddle point. More formally, we have: Theorem 3. Assume that f(\u00b7) satisfies A1. Then there exists an absolute constant cmax such that, for any \u03b4 > 0, \u2264 ` 2\n\u03c1 , \u2206f \u2265 f(x0) \u2212 f ?, and constant c \u2264\ncmax, PGD(x0, `, \u03c1, , c, \u03b4,\u2206f ) will output an -secondorder stationary point, with probability 1\u2212\u03b4, and terminate in the following number of iterations:\nO\n( `(f(x0)\u2212 f?)\n2 log4 ( d`\u2206f 2\u03b4 )) .\n1Note that uniform sampling from a d-dimensional ball can be done efficiently by sampling U\n1 d \u00d7 Y\u2016Y\u2016 where U \u223c\nUniform([0, 1]) and Y \u223c N (0, Id) (Harman & Lacko, 2010).\nStrikingly, Theorem 3 shows that perturbed gradient descent finds a second-order stationary point in almost the same amount of time that gradient descent takes to find first-order stationary point. The step size \u03b7 is chosen as O(1/`) which is in accord with classical analyses of convergence to first-order stationary points. Though we state the theorem with a certain choice of parameters for simplicity of presentation, our result holds even if we vary the parameters up to constant factors.\nWithout loss of generality, we can focus on the case \u2264 `2/\u03c1, as in Theorem 3. In the case > `2/\u03c1, standard gradient descent without perturbation\u2014Theorem 2\u2014easily solves the problem. This is because by A1, we always have \u03bbmin(\u22072f(x)) \u2265 \u2212` \u2265 \u2212 \u221a \u03c1 , which means that all - second-order stationary points are -first order stationary points.\nWe believe that the dependence on at least one log d factor in the iteration complexity is unavoidable in the nonconvex setting, as our result can be directly applied to the principal component analysis problem, for which the best known runtimes (for the power method or Lanczos method) incur a log d factor due to random initialization. Establishing this formally is still an open question however.\nTo provide some intuition for Theorem 3, consider an iterate xt which is not yet an -second-order stationary point. By definition, either (1) the gradient \u2207f(xt) is large, or (2) the Hessian \u22072f(xt) has a significant negative eigenvalue. Traditional analysis works in the first case. The crucial step in the proof of Theorem 3 involves handling the second case: when the gradient is small \u2016\u2207f(xt)\u2016 \u2264 gthres and the Hessian has a significant negative eigenvalue \u03bbmin(\u22072f(x\u0303t)) \u2264 \u2212 \u221a \u03c1 , then adding a perturbation, followed by standard gradient descent for tthres steps, decreases the function value by at least fthres, with high probability. The proof of this fact relies on a novel characterization of geometry around saddle points (see Section 5)\nIf we are able to make stronger assumptions on the objective function we are able to strengthen our main result. This further analysis is presented in the next section."}, {"heading": "3.1. Functions with Strict Saddle Property", "text": "In many real applications, objective functions further admit the property that all saddle points are strict (Ge et al., 2015; Sun et al., 2016a;b; Bhojanapalli et al., 2016; Ge et al., 2016). In this case, all second-order stationary points are local minima and hence convergence to second-order stationary points (Theorem 3) is equivalent to convergence to local minima.\nTo state this result formally, we introduce a robust version of the strict saddle property (cf. Ge et al., 2015): Assumption A2. Function f(\u00b7) is (\u03b8, \u03b3, \u03b6)-strict saddle.\nThat is, for any x, at least one of following holds:\n\u2022 \u2016\u2207f(x)\u2016 \u2265 \u03b8.\n\u2022 \u03bbmin(\u22072f(x)) \u2264 \u2212\u03b3.\n\u2022 x is \u03b6-close to X ? \u2014 the set of local minima.\nIntuitively, the strict saddle assumption states that the Rd space can be divided into three regions: 1) a region where the gradient is large; 2) a region where the Hessian has a significant negative eigenvalue (around saddle point); and 3) the region close to a local minimum. With this assumption, we immediately have the following corollary: Corollary 4. Let f(\u00b7) satisfy A1 and A2. Then, there exists an absolute constant cmax such that, for any \u03b4 > 0,\u2206f \u2265 f(x0) \u2212 f?, constant c \u2264 cmax, and letting \u0303 = min(\u03b8, \u03b32/\u03c1), PGD(x0, `, \u03c1, \u0303, c, \u03b4,\u2206f ) will output a point \u03b6-close to X ?, with probability 1 \u2212 \u03b4, and terminate in the following number of iterations:\nO\n( `(f(x0)\u2212 f?)\n\u03032 log4 ( d`\u2206f \u03032\u03b4 )) .\nCorollary 4 shows after finding \u0303-second-order stationary point by Theorem 3 where \u0303 = min(\u03b8, \u03b32/\u03c1), the output is also in the \u03b6-neighborhood of some local minimum.\nNote although Corollary 4 only explicitly asserts that the output will lie within some fixed radius \u03b6 from a local minimum. In many real applications, we further have that \u03b6 can be written as a function \u03b6(\u03b8) which decreases linearly or polynomially depending on \u03b8, while \u03b3 will be nondecreasing w.r.t \u03b8. In these cases, the above corollary further gives a convergence rate to a local minimum."}, {"heading": "3.2. Functions with Strong Local Structure", "text": "The convergence rate in Theorem 3 is polynomial in , which is similar to that of Theorem 2, but is worse than the rate of Theorem 1 because of the lack of strong convexity. Although global strong convexity does not hold in the nonconvex setting that is our focus, in many machine learning problems the objective function may have a favorable local structure in the neighborhood of local minima (Ge et al., 2015; Sun et al., 2016a;b; Sun & Luo, 2016). Exploiting this property can lead to much faster convergence (linear convergence) to local minima. One such property that ensures such convergence is a local form of smoothness and strong convexity: Assumption A3.a. In a \u03b6-neighborhood of the set of local minima X ?, the function f(\u00b7) is \u03b1-strongly convex, and \u03b2-smooth.\nHere we use different letter \u03b2 to denote the local smoothness parameter (in contrast to the global smoothness parameter `). Note that we always have \u03b2 \u2264 `.\nAlgorithm 3 Perturbed Gradient Descent with Local Improvement: PGDli(x0, `, \u03c1, , c, \u03b4,\u2206f , \u03b2)\nx0 \u2190 PGD(x0, `, \u03c1, , c, \u03b4,\u2206f ) for t = 0, 1, . . . do xt+1 \u2190 xt \u2212 1\u03b2\u2207f(xt)\nHowever, often even local \u03b1-strong convexity does not hold. We thus introduce the following relaxation:\nAssumption A3.b. In a \u03b6-neighborhood of the set of local minima X ?, the function f(\u00b7) satisfies a (\u03b1, \u03b2)-regularity condition if for any x in this neighborhood:\n\u3008\u2207f(x),x\u2212PX?(x)\u3009 \u2265 \u03b1\n2 \u2016x\u2212 PX?(x)\u20162+\n1\n2\u03b2 \u2016\u2207f(x)\u20162.\n(1)\nHerePX?(\u00b7) is the projection on to the setX ?. Note (\u03b1, \u03b2)regularity condition is more general and is directly implied by standard \u03b2-smooth and \u03b1-strongly convex conditions. This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. (2016); Zheng & Lafferty (2016), where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.\nGradient descent naturally exploits local structure very well. In Algorithm 3, we first run Algorithm 2 to output a point within the neighborhood of a local minimum, and then perform standard gradient descent with step size 1\u03b2 . We can then prove the following theorem:\nTheorem 5. Let f(\u00b7) satisfy A1, A2, and A3.a (or A3.b). Then there exists an absolute constant cmax such that, for any \u03b4 > 0, > 0,\u2206f \u2265 f(x0) \u2212 f?, constant c \u2264 cmax, and letting \u0303 = min(\u03b8, \u03b32/\u03c1), PGDli(x0, `, \u03c1, \u0303, c, \u03b4,\u2206f , \u03b2) will output a point that is - close to X ?, with probability 1\u2212\u03b4, in the following number of iterations:\nO\n( `(f(x0)\u2212 f?)\n\u03032 log4 ( d`\u2206f \u03032\u03b4 ) + \u03b2 \u03b1 log \u03b6 ) .\nTheorem 5 says that if strong local structure is present, the convergence rate can be boosted to linear convergence (log 1 ). In this theorem we see that sequence of iterations can be decomposed into two phases. In the first phase, perturbed gradient descent finds a \u03b6-neighborhood by Corollary 4. In the second phase, standard gradient descent takes us from \u03b6 to -close to a local minimum. Standard gradient descent and Assumption A3.a (or A3.b) make sure that the iterate never steps out of a \u03b6-neighborhood in this second phase, giving a result similar to Theorem 1 with linear convergence."}, {"heading": "4. Example \u2014 Matrix Factorization", "text": "As a simple example to illustrate how to apply our general theorems to specific non-convex optimization problems, we consider a symmetric low-rank matrix factorization problem, based on the following objective function:\nmin U\u2208Rd\u00d7r\nf(U) = 1\n2 \u2016UU> \u2212M?\u20162F, (2)\nwhere M? \u2208 Rd\u00d7d. For simplicity, we assume rank(M?) = r, and denote \u03c3?1 := \u03c31(M\n?), \u03c3?r := \u03c3r(M\n?). Clearly, in this case the global minimum of function value is zero, which is achieved at V? = TD1/2 where TDT> is the SVD of the symmetric real matrix M?.\nThe following two lemmas show that the objective function in Eq. (2) satisfies the geometric assumptions A1, A2,and A3.b. Moreover, all local minima are global minima. Lemma 6. For any \u0393 \u2265 \u03c3?1 , the function f(U) defined in Eq. (2) is 8\u0393-smooth and 12\u03931/2-Hessian Lipschitz, inside the region {U|\u2016U\u20162 < \u0393}. Lemma 7. For function f(U) defined in Eq. (2), all local minima are global minima. The set of global minima is X ? = {V?R|RR> = R>R = I}. Furthermore, f(U) is ( 124 (\u03c3 ? r ) 3/2, 13\u03c3 ? r , 1 3 (\u03c3 ? r )\n1/2)-strict saddle; and satisfies a ( 23\u03c3 ? r , 10\u03c3 ? 1)-regularity condition in a 1 3 (\u03c3 ? r )\n1/2- neighborhood of X ?.\nOne caveat is that since the objective function is actually a fourth-order polynomial with respect to U, the smoothness and Hessian Lipschitz parameters from Lemma 6 naturally depend on \u2016U\u2016. Fortunately, we can further show that gradient descent (even with perturbation) does not increase \u2016U\u2016 beyond O(max{\u2016U0\u2016, (\u03c3?1)1/2}). Then, applying Theorem 5 gives: Theorem 8. There exists an absolute constant cmax such that the following holds. For the objective function in Eq. (2), for any \u03b4 > 0 and constant c \u2264 cmax, and for \u03931/2 := 2 max{\u2016U0\u2016, 3(\u03c3?1)1/2}, the output of PGDli(U0, 8\u0393, 12\u03931/2, (\u03c3?r ) 2\n108\u03931/2 , c, \u03b4, r\u0393\n2\n2 , 10\u03c3 ? 1), will be -\nclose to the global minimum set X ?, with probability 1\u2212 \u03b4, after the following number of iterations:\nO ( r ( \u0393\n\u03c3?r\n)4 log4 ( d\u0393\n\u03b4\u03c3?r ) + \u03c3?1 \u03c3?r log \u03c3?r ) .\nTheorem 8 establishes global convergence of perturbed gradient descent from an arbitrary initial point U0, including exact saddle points. Suppose we initialize at U0 = 0, then our iteration complexity becomes:\nO ( r(\u03ba?)4 log4(d\u03ba?/\u03b4) + \u03ba? log(\u03c3?r/ ) ) ,\nwhere \u03ba? = \u03c3?1/\u03c3 ? r is the condition number of the matrix M?. We see that in the first phase, to move from a neighborhood of the solution, our method requires a number of\niterations scaling as O\u0303(r(\u03ba?)4). We suspect that this strong dependence on condition number arises from our generic assumption that the Hessian Lipschitz is uniformly upper bounded; it may well be the case that this dependence can be reduced in the special case of matrix factorization via a finer analysis of the geometric structure of the problem."}, {"heading": "5. Proof Sketch for Theorem 3", "text": "In this section we will present the key ideas underlying the main result of this paper (Theorem 3). We will first argue the correctness of Theorem 3 given two important intermediate lemmas. Then we turn to the main lemma, which establishes that gradient descent can escape from saddle points quickly. We present full proofs of all these results in Appendix A. Throughout this section, we use \u03b7, r, gthres, fthres and tthres as defined in Algorithm 2."}, {"heading": "5.1. Exploiting Large Gradient or Negative Curvature", "text": "Recall that an -second-order stationary point is a point with a small gradient, and where the Hessian does not have a significant negative eigenvalue. Suppose we are currently at an iterate xt that is not an -second-order stationary point; i.e., it does not satisfy the above properties. There are two possibilities: (1) The gradient is large: \u2016\u2207f(xt)\u2016 \u2265 gthres; or (2) Around the saddle point we have \u2016\u2207f(xt)\u2016 \u2264 gthres and \u03bbmin(\u22072f(xt)) \u2264 \u2212 \u221a \u03c1 .\nThe following two lemmas address these two cases respectively. They guarantee that perturbed gradient descent will decrease the function value in both scenarios.\nLemma 9 (Gradient). Assume that f(\u00b7) satisfies A1. Then for gradient descent with stepsize \u03b7 < 1` , we have f(xt+1) \u2264 f(xt)\u2212 \u03b72\u2016\u2207f(xt)\u2016 2.\nLemma 10 (Saddle). (informal) Assume that f(\u00b7) satisfies A1, If xt satisfies \u2016\u2207f(xt)\u2016 \u2264 gthres and \u03bbmin(\u22072f(xt)) \u2264 \u2212 \u221a \u03c1 , then adding one perturbation step followed by tthres steps of gradient descent, we have f(xt+tthres)\u2212 f(xt) \u2264 \u2212fthres with high probability.\nWe see that Algorithm 2 is designed so that Lemma 10 can be directly applied. According to these two lemmas, perturbed gradient descent will decrease the function value either in the case of a large gradient, or around strict saddle points. Computing the average decrease in function value yields the total iteration complexity. Since Algorithm 2 only terminate when the function value decreases too slowly, this guarantees that the output must be -second-order stationary point (see Appendix A for formal proofs)."}, {"heading": "5.2. Escaping from Saddle Points Quickly", "text": "The proof of Lemma 9 is straightforward and follows from traditional analysis. The key technical contribution of this paper is the proof of Lemma 10, which gives a new characterization of the geometry around saddle points.\nConsider a point x\u0303 that satisfies the the preconditions of Lemma 10 (\u2016\u2207f(x\u0303)\u2016 \u2264 gthres and \u03bbmin(\u22072f(x\u0303)) \u2264 \u2212\u221a\u03c1 ). After adding the perturbation (x0 = x\u0303+\u03be), we can view x0 as coming from a uniform distribution over Bx\u0303(r), which we call the perturbation ball. We can divide this perturbation ball Bx\u0303(r) into two disjoint regions: (1) an escaping region Xescape which consists of all the points x \u2208 Bx\u0303(r) whose function value decreases by at least fthres after tthres steps; (2) a stuck region Xstuck = Bx\u0303(r)\u2212Xescape. Our general proof strategy is to show that Xstuck consists of a very small proportion of the volume of perturbation ball. After adding a perturbation to x\u0303, point x0 has a very small chance of falling in Xstuck, and hence will escape from the saddle point efficiently.\nLet us consider the nature of Xstuck. For simplicity, let us imagine that x\u0303 is an exact saddle point whose Hessian has only one negative eigenvalue, and d \u2212 1 positive eigenvalues. Let us denote the minimum eigenvalue direction as e1. In this case, if the Hessian remains constant (and we\nhave a quadratic function), the stuck region Xstuck consists of points x such that x\u2212 x\u0303 has a small e1 component. This is a straight band in two dimensions and a flat disk in high dimensions. However, when the Hessian is not constant, the shape of the stuck region is distorted. In two dimensions, it forms a \u201cnarrow band\u201d as plotted in Figure 2 on top of the gradient flow. In three dimensions, it forms a \u201cthin pancake\u201d as shown in Figure 1.\nThe major challenge here is to bound the volume of this high-dimensional non-flat \u201cpancake\u201d shaped region Xstuck. A crude approximation of this \u201cpancake\u201d by a flat \u201cdisk\u201d loses polynomial factors in the dimensionalilty, which gives a suboptimal rate. Our proof relies on the following crucial observation: Although we do not know the explicit form of the stuck region, we know it must be very \u201cthin,\u201d therefore it cannot have a large volume. The informal statement of the lemma is as follows:\nLemma 11. (informal) Suppose x\u0303 satisfies the precondition of Lemma 10, and let e1 be the smallest eigendirection of \u22072f(x\u0303). For any \u03b4 \u2208 (0, 1/3] and any two points w,u \u2208 Bx\u0303(r), if w \u2212 u = \u00b5re1 and \u00b5 \u2265 \u03b4/(2 \u221a d), then at least one of w,u is not in the stuck region Xstuck.\nUsing this lemma it is not hard to bound the volume of the stuck region: we can draw a straight line along the e1 direction which intersects the perturbation ball (shown as purple line segment in Figure 2). For any two points on this line segment that are at least \u03b4r/(2 \u221a d) away from each other (shown as red points w,u in Figure 2), by Lemma 11, we know at least one of them must not be in Xstuck. This implies if there is one point u\u0303 \u2208 Xstuck on this line segment, then Xstuck on this line can be at most an interval of length \u03b4r/ \u221a d around u\u0303. This establishes the \u201cthickness\u201d of Xstuck in the e1 direction, which is turned into an upper bound on the volume of the stuck region Xstuck by standard calculus."}, {"heading": "6. Conclusion", "text": "This paper presents the first (nearly) dimension-free result for gradient descent in a general non-convex setting. We present a general convergence result and show how it can be further strengthened when combined with further structure such as strict saddle conditions and/or local regularity/convexity.\nThere are still many related open problems. First, in the presence of constraints, it is worthwhile to study whether gradient descent still admits similar sharp convergence results. Another important question is whether similar techniques can be applied to accelerated gradient descent. We hope that this result could serve as a first step towards a more general theory with strong, almost dimension free guarantees for non-convex optimization."}], "year": 2017, "references": [{"title": "Finding approximate local minima for nonconvex optimization in linear time", "authors": ["Agarwal", "Naman", "Allen-Zhu", "Zeyuan", "Bullins", "Brian", "Hazan", "Elad", "Ma", "Tengyu"], "venue": "arXiv preprint arXiv:1611.01146,", "year": 2016}, {"title": "Global optimality of local search for low rank matrix recovery", "authors": ["Bhojanapalli", "Srinadh", "Neyshabur", "Behnam", "Srebro", "Nathan"], "venue": "arXiv preprint arXiv:1605.07221,", "year": 2016}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "authors": ["Candes", "Emmanuel J", "Li", "Xiaodong", "Soltanolkotabi", "Mahdi"], "venue": "IEEE Transactions on Information Theory,", "year": 1985}, {"title": "Gradient descent efficiently finds the cubic-regularized non-convex newton step", "authors": ["Carmon", "Yair", "Duchi", "John C"], "venue": "arXiv preprint arXiv:1612.00547,", "year": 2016}, {"title": "Accelerated methods for non-convex optimization", "authors": ["Carmon", "Yair", "Duchi", "John C", "Hinder", "Oliver", "Sidford", "Aaron"], "venue": "arXiv preprint arXiv:1611.00756,", "year": 2016}, {"title": "The loss surface of multilayer networks", "authors": ["Choromanska", "Anna", "Henaff", "Mikael", "Mathieu", "Michael", "Arous", "G\u00e9rard Ben", "LeCun", "Yann"], "year": 2014}, {"title": "A trust region algorithm with a worst-case iteration complexity of\\ mathcal {O}(\\ epsilon\u02c6{-3/2}) for nonconvex optimization", "authors": ["Curtis", "Frank E", "Robinson", "Daniel P", "Samadi", "Mohammadreza"], "venue": "Mathematical Programming,", "year": 2014}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "authors": ["Ge", "Rong", "Huang", "Furong", "Jin", "Chi", "Yuan", "Yang"], "venue": "In COLT,", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "authors": ["Ge", "Rong", "Lee", "Jason D", "Ma", "Tengyu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "On decompositional algorithms for uniform sampling from n-spheres and n-balls", "authors": ["Harman", "Radoslav", "Lacko", "Vladim\u0131\u0301r"], "venue": "Journal of Multivariate Analysis,", "year": 2010}, {"title": "Computing matrix squareroot via non convex local search", "authors": ["Jain", "Prateek", "Jin", "Chi", "Kakade", "Sham M", "Netrapalli", "Praneeth"], "venue": "arXiv preprint arXiv:1507.05854,", "year": 2015}, {"title": "Deep learning without poor local minima", "authors": ["Kawaguchi", "Kenji"], "venue": "In Advances In Neural Information Processing Systems,", "year": 2016}, {"title": "Gradient descent only converges to minimizers", "authors": ["Lee", "Jason D", "Simchowitz", "Max", "Jordan", "Michael I", "Recht", "Benjamin"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "The power of normalization: Faster evasion of saddle points", "authors": ["Levy", "Kfir Y"], "venue": "arXiv preprint arXiv:1611.04831,", "year": 2016}, {"title": "Introductory lectures on convex programming volume", "authors": ["Nesterov", "Yu"], "venue": "i: Basic course. Lecture notes,", "year": 1998}, {"title": "Cubic regularization of newton method and its global performance", "authors": ["Nesterov", "Yurii", "Polyak", "Boris T"], "venue": "Mathematical Programming,", "year": 2006}, {"title": "Phase retrieval using alternating minimization", "authors": ["Netrapalli", "Praneeth", "Jain", "Prateek", "Sanghavi", "Sujay"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Non-square matrix sensing without spurious local minima via the burermonteiro approach", "authors": ["Park", "Dohyung", "Kyrillidis", "Anastasios", "Caramanis", "Constantine", "Sanghavi", "Sujay"], "venue": "arXiv preprint arXiv:1609.03240,", "year": 2016}, {"title": "Learning representations by back-propagating errors", "authors": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": "Cognitive modeling,", "year": 1988}, {"title": "Complete dictionary recovery over the sphere i: Overview and the geometric picture", "authors": ["Sun", "Ju", "Qu", "Qing", "Wright", "John"], "venue": "IEEE Transactions on Information Theory,", "year": 2016}, {"title": "A geometric analysis of phase retrieval", "authors": ["Sun", "Ju", "Qu", "Qing", "Wright", "John"], "venue": "In Information Theory (ISIT),", "year": 2016}, {"title": "Guaranteed matrix completion via non-convex factorization", "authors": ["Sun", "Ruoyu", "Luo", "Zhi-Quan"], "venue": "IEEE Transactions on Information Theory,", "year": 2016}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "authors": ["Zheng", "Qinqing", "Lafferty", "John"], "venue": "arXiv preprint arXiv:1605.07051,", "year": 2016}], "id": "SP:79c3282f781ced03b737ab57a20528f0b884ff8a", "authors": [{"name": "Chi Jin", "affiliations": []}, {"name": "Rong Ge", "affiliations": []}, {"name": "Praneeth Netrapalli", "affiliations": []}, {"name": "Sham M. Kakade", "affiliations": []}, {"name": "Michael I. Jordan", "affiliations": []}], "abstractText": "This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost \u201cdimension-free\u201d). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.", "title": "How to Escape Saddle Points Efficiently"}