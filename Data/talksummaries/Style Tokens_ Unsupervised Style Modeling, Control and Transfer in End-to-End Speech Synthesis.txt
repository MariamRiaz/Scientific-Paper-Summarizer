9	47	First, there is no objective measure of “correct” prosodic style, making both modeling and evaluation difficult.
10	37	Acquiring annotations for large datasets can be costly and similarly problematic, since human raters often disagree.
11	23	Second, the high dynamic range in expressive voices is difficult to model.
12	45	Many TTS models, including recent end-to-end systems, only learn an averaged prosodic distribution over their input data, generating less expressive speech – especially for long-form phrases.
13	20	Furthermore, they often lack the ability to control the expression with which speech is synthesized.
14	18	This work1 attempts to address the above issues by introducing “global style tokens” (GSTs) to Tacotron (Wang et al., 2017a; Shen et al., 2017), a state-of-the-art end-to-end TTS model.
18	22	Our model is based on Tacotron (Wang et al., 2017a; Shen et al., 2017), a sequence-to-sequence (seq2seq) model that predicts mel spectrograms directly from grapheme or phoneme inputs.
22	85	During training, information flows through the model as follows: • The reference encoder, proposed in (Skerry-Ryan et al., 2018), compresses the prosody of a variablelength audio signal into a fixed-length vector, which we call the reference embedding.
24	20	• The reference embedding is passed to a style token layer, where it is used as the query vector to a contentbased attention module.
30	18	• The style token layer weights (including token em- beddings) are jointly trained with the rest of the model, driven only by the reconstruction loss from the Tacotron decoder.
45	19	The reference encoder is made up of a convolutional stack, followed by an RNN.
66	24	GST embeddings can also be viewed as an external memory that stores style information extracted from training data.
87	18	In this section, we measure the ability of GSTs to control and transfer speaking style, using the inference methods from Section 2.2.
95	61	At inference time, we simply replace the style embedding with a specific, optionally scaled token.
96	24	Conditioning in this manner has several benefits.
105	20	In addition to providing interpretability, style token conditioning can also improve synthesis quality.
122	22	While the same style embedding is added to all text encoder states during training, this doesn’t need to be the case in inference mode.
124	77	Style transfer is an active area of research that aims to synthesize a phrase in the prosodic style of a reference signal (Wu et al., 2013; Nakashika et al., 2016; Kinnunen et al., 2017).
125	77	The property that a GST model can be conditioned on any convex combination of style tokens lends itself well to this task; at inference time (method 2 of Section 2.2), we can simply feed a reference signal to guide the choice of token combination weights.
129	34	We see that, given only text input, the baseline Tacotron model does not closely match the prosodic style of the reference signal.
134	23	We next show results for a non-parallel transfer task, in which a TTS system must synthesize arbitrary text in the prosodic style of a reference signal.
152	18	In this section, we demonstrate how GSTs can be used to train robust models directly from noisy found data, without modifications.
163	29	This means that we can synthesize clean speech for arbitrary text input by conditioning the model on a single, clean style token.
166	30	Table 2 shows MOS results for both a baseline Tacotron and a “clean-token" GST model.
168	19	Because the model has no prior knowledge of speech or noise, it blindly models all statistics in the training set, resulting in substantial amounts of noise during synthesis.
212	59	Second, style attributes, such as emotion, are often very difficult to label for large-scale noisy data.
214	37	This work has introduced Global Style Tokens, a powerful method for modeling style in end-to-end TTS systems.
216	20	We have shown that, in addition to learning interpretable embeddings that can be used to control and transfer style, GSTs are a general technique for uncovering latent variations in data.
218	22	Finally, while this work adds GSTs only to Tacotron, we believe the method can be readily used by other types of end-to-end TTS models.
219	27	More generally, we envision that GSTs can be applied to models in other domains – such as text-to-image and neural machine translation systems – that would benefit from interpretability, controllability and robustness.
