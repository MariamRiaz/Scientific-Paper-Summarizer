{"sections": [{"heading": "1. Introduction", "text": "Given their impressive performance on machine learning and pattern recognition tasks, Deep Neural Networks (DNNs) have recently attracted a considerable deal of attention in several applied domains such as computer vision and natural language processing; see, e.g., LeCun et al. (2015) and references therein. Deep Gaussian Processes (DGPs; Damianou & Lawrence, 2013) alleviate the outstanding issue characterizing DNNs of having to specify the number of units in hidden layers by implicitly working with infinite representations at each layer. From a generative perspective, DGPs transform the inputs using a cascade of Gaussian Processes (GPs; Rasmussen & Williams,\n1Department of Data Science, EURECOM, France 2School of Computer Science and Engineering, University of New South Wales, Australia. Correspondence to: Kurt Cutajar <kurt.cutajar@eurecom.fr>, Pietro Michiardi <pietro.michiardi@eurecom.fr>, Edwin V. Bonilla Cutajar <e.bonilla@unsw.edu.au>, Maurizio Filippone <maurizio.filippone@eurecom.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\n2006) such that the output of each layer of GPs forms the input to the GPs at the next layer, effectively implementing a deep probabilistic nonparametric model for compositions of functions (Neal, 1996; Duvenaud et al., 2014).\nBecause of their probabilistic formulation, it is natural to approach the learning of DGPs through Bayesian inference techniques; however, the application of such techniques to learn DGPs leads to various forms of intractability. A number of contributions have been proposed to recover tractability, extending or building upon the literature on approximate methods for GPs. Nevertheless, only few works leverage one of the key features that arguably make DNNs so successful, that is being scalable through the use of minibatch-based learning (Hensman & Lawrence, 2014; Dai et al., 2016; Bui et al., 2016). Even among these works, there does not seem to be an approach that is truly applicable to large-scale problems, and practical beyond only a few hidden layers.\nIn this paper, we develop a practical learning framework for DGP models that significantly improves the state-of-the-art on those aspects. In particular, our proposal introduces two sources of approximation to recover tractability, while (i) scaling to large-scale problems, (ii) being able to work with moderately deep architectures, and (iii) being able to accurately quantify uncertainty. The first is a model approximation, whereby the GPs at all layers are approximated using random feature expansions (Rahimi & Recht, 2008); the second approximation relies upon stochastic variational inference to retain a probabilistic and scalable treatment of the approximate DGP model.\nWe show that random feature expansions for DGP models yield Bayesian DNNs with low-rank weight matrices, and the expansion of different covariance functions results in different DNN activation functions, namely trigonometric for the Radial Basis Function (RBF) covariance, and Rectified Linear Unit (ReLU) functions for the ARC-COSINE covariance. In order to retain a probabilistic treatment of the model we adapt the work on variational inference for DNNs and variational autoencoders (Graves, 2011; Kingma & Welling, 2014) using mini-batch-based stochastic gradient optimization, which can exploit GPU and distributed computing. In this respect, we can view the probabilistic treatment of DGPs approximated through random feature\nexpansions as a means to specify sensible and interpretable priors for probabilistic DNNs. Furthermore, unlike popular inducing points-based approximations for DGPs, the resulting learning framework does not involve any matrix decompositions in the size of the number of inducing points, but only matrix products. We implement our model in TensorFlow (Abadi et al., 2015), which allows us to rely on automatic differentiation to apply stochastic variational inference.\nAlthough having to select the appropriate number of random features goes against the nonparametric formulation favored in GP models, the level of approximation can be tuned based on constraints on running time or hardware. Most importantly, the random feature approximation enables us to develop a learning framework for DGPs which significantly advances the state-of-the-art. We extensively demonstrate the effectiveness of our proposal on a variety of regression and classification problems by comparing it with DNNs and other state-of-the-art approaches to infer DGPs. The results indicate that for a given DGP architecture, our proposal is consistently faster at achieving better generalization compared to the competitors. Another key observation is that the proposed DGP outperforms DNNs trained with dropout when quantifying uncertainty.\nWe focus part of the experiments on large-scale problems, such as MNIST8M digit classification and the AIRLINE dataset, which contain over 8 and 5 million observations, respectively. Only very recently there have been attempts to demonstrate performance of GP models on such large data sets (Wilson et al., 2016; Krauth et al., 2016), and our proposal is on par with these latest GP methods. Furthermore, we obtain impressive results when employing our learning framework to DGPs with moderate depth (few tens of layers) on the AIRLINE dataset. We are not aware of any other DGP models having such depth that can achieve comparable performance when applied to datasets with millions of observations. Crucially, we obtain all these results by running our algorithm on a single machine without GPUs, but our proposal is designed to be able to exploit GPU and distributed computing to significantly accelerate our deep probabilistic learning framework (see supplement for experiments in distributed mode).\nIn summary, the most significant contributions of this work are as follows: (i) we propose a novel approximation of DGPs based on random feature expansions that we study in connection with DNNs; (ii) we demonstrate the ability of our proposal to systematically outperform state-of-theart methods to carry out inference in DGP models, especially for large-scale problems and moderately deep architectures; (iii) we validate the superior quantification of uncertainty offered by DGPs compared to DNNs."}, {"heading": "1.1. Related work", "text": "Following the original proposal of DGP models in Damianou & Lawrence (2013), there have been several attempts to extend GP inference techniques to DGPs. Notable examples include the extension of inducing point approximations (Hensman & Lawrence, 2014; Dai et al., 2016) and Expectation Propagation (Bui et al., 2016). Sequential inference for training DGPs has also been investigated in Wang et al. (2016). A recent example of a DGP \u201cnatively\u201d formulated as a variational model appears in Tran et al. (2016). Our work is the first to employ random feature expansions to approximate DGPs as DNNs. The expansion of the squared exponential covariance for DGPs leads to trigonometric DNNs, whose properties were studied in Sopena et al. (1999). Meanwhile, the expansion of the arccosine covariance is inspired by Cho & Saul (2009), and it allows us to show that DGPs with such covariance can be approximated with DNNs having ReLU activations.\nThe connection between DGPs and DNNs has been pointed out in several papers, such as Neal (1996) and Duvenaud et al. (2014), where pathologies with deep nets are investigated. The approximate DGP model described in our work becomes a DNN with low-rank weight matrices, which have been used in, e.g., Novikov et al. (2015); Sainath et al. (2013); Denil et al. (2013) as a regularization mechanism. Dropout is another technique to speed-up training and improve generalization of DNNs that has recently been linked to variational inference (Gal & Ghahramani, 2016).\nRandom Fourier features for large scale kernel machines were proposed in Rahimi & Recht (2008), and their application to GPs appears in La\u0301zaro-Gredilla et al. (2010). In the case of squared exponential covariances, variational learning of the posterior over the frequencies was proposed in Gal & Turner (2015) to avoid potential overfitting caused by optimizing these variables. These approaches are special cases of our DGP model when using no hidden layers.\nIn our work, we learn the proposed approximate DGP model using stochastic variational inference. Variational learning for DNNs was first proposed in Graves (2011), and later extended to include the reparameterization trick to clamp randomness in the computation of the gradient with respect to the posterior over the weights (Kingma & Welling, 2014; Rezende et al., 2014), and to include a Gaussian mixture prior over the weights (Blundell et al., 2015)."}, {"heading": "2. Preliminaries", "text": "Consider a supervised learning scenario where a set of input vectors X = [x1, . . . ,xn]\u22a4 is associated with a set of (possibly multivariate) labels Y = [y1, . . . ,yn]\u22a4, where xi \u2208 RDin and yi \u2208 RDout . We assume that there is an underlying function fo(xi) characterizing a mapping from the\ninputs to a latent representation, and that the labels are a realization of some probabilistic process p(yio|fo(xi)) which is based on this latent representation.\nIn this work, we consider modeling the latent functions using Deep Gaussian Processes (DGPs; Damianou & Lawrence, 2013). Let variables in layer l be denoted by the (l) superscript. In DGP models, the mapping between inputs and labels is expressed as a composition of functions\nf(x) = ( f (Nh\u22121) \u25e6 . . . \u25e6 f (0) ) (x),\nwhere each of the Nh layers is composed of a (possibly transformed) multivariate Gaussian process (GP). Formally, a GP is a collection of random variables such that any subset of these are jointly Gaussian distributed (Rasmussen & Williams, 2006). In GPs, the covariance between variables at different inputs is modeled using the so-called covariance function.\nGiven the relationship between GPs and single-layered neural networks with an infinite number of hidden units (Neal, 1996), the DGP model has an obvious connection with DNNs. In contrast to DNNs, where each of the hidden layers implements a parametric function of its inputs, in DGPs these functions are assigned a GP prior, and are therefore nonparametric. Furthermore, because of their probabilistic formulation, it is natural to approach the learning of DGPs through Bayesian inference techniques that lead to principled approaches for both determining the optimal settings of architecture-dependent parameters, such as the number of hidden layers, and quantification of uncertainty.\nWhile DGPs are attractive from a theoretical standpoint, inference is extremely challenging. Denote by F (l) the set of latent variables with entries f (l)io = f (l) o (xi), and let p(Y |F (Nh)) be the conditional likelihood. Learning and making predictions with DGPs requires solving integrals that are generally intractable. For example, computing the marginal likelihood to optimize covariance parameters \u03b8(l) at all layers entails solving\np(Y |X,\u03b8) = \u222b p ( Y |F (Nh) ) p ( F (Nh)|F (Nh\u22121),\u03b8(Nh\u22121) ) \u00d7 . . .\u00d7 p ( F (1)|X,\u03b8(0) ) dF (Nh) . . . dF (1).\nIn the following section we use random feature approximations to the covariance function in order to develop a scalable algorithm for inference in DGPs."}, {"heading": "2.1. Random Feature Expansions for GPs", "text": "We start by describing how random feature expansions can be used to approximate the covariance of a single GP model. Such approximations have been considered previously, for example by Rahimi & Recht (2008) in the context of non-probabilistic kernel machines. Here we focus\non random feature expansions for the radial basis function (RBF) covariance and the ARC-COSINE covariance, which we will use in our experiments.\nFor the sake of clarity, we will present the covariances without any explicit scaling of the features or the covariance itself. After explaining the random feature expansion associated with each covariance, we will generalize these results in the context of DGPs to include scaling the covariance by a factor \u03c32, and scaling the features for Automatic Relevance Determination (ARD) (Mackay, 1994)."}, {"heading": "2.1.1. RADIAL BASIS FUNCTION COVARIANCE", "text": "A popular example of a covariance function, which we consider here, is the Radial Basis Function (RBF) covariance\nkrbf(x,x \u2032) = exp [ \u22121 2 \u2225x\u2212 x\u2032\u2225\u22a4 ] . (1)\nAppealing to Bochner\u2019s theorem, any continuous shiftinvariant normalized covariance function k(xi,xj) = k(xi\u2212xj) is positive definite if and only if it can be rewritten as the Fourier transform of a non-negative measure p(\u03c9) (Rahimi & Recht, 2008). Denoting the spectral frequencies by \u03c9, while assigning \u03b9 = \u221a \u22121 and \u03b4 = xi\u2212xj , in the case of the RBF covariance in equation (1), this yields:\nkrbf(\u03b4) =\n\u222b p(\u03c9) exp ( \u03b9\u03b4\u22a4\u03c9 ) d\u03c9, (2)\nwith a corresponding non-negative measure p(\u03c9) = N (0, I). Because the covariance function and the nonnegative measures are real, we can drop the unnecessary complex part of the argument of the expectation, keeping cos(\u03b4\u22a4\u03c9) = cos((xi \u2212 xj)\u22a4\u03c9) that can be rewritten as cos(x\u22a4i \u03c9) cos(x \u22a4 j \u03c9) + sin(x \u22a4 i \u03c9) sin(x \u22a4 j \u03c9).\nThe importance of the expansion above is that it allows us to interpret the covariance function as an expectation that can be estimated using Monte Carlo. Defining z(x|\u03c9) = [cos(x\u22a4\u03c9), sin(x\u22a4\u03c9)]\u22a4, the covariance function can be therefore unbiasedly approximated as\nkrbf(xi,xj) \u2248 1\nNRF NRF\u2211 r=1 z(xi|\u03c9\u0303r)\u22a4z(xj |\u03c9\u0303r), (3)\nwith \u03c9\u0303r \u223c p(\u03c9). This has an important practical implication, as it provides the means to access an approximate explicit representation of the mapping induced by the covariance function that, in the RBF case, is infinite dimensional (Shawe-Taylor & Cristianini, 2004). Various results have been established on the accuracy of the random Fourier feature approximation; see, e.g., Rahimi & Recht (2008)."}, {"heading": "2.1.2. ARC-COSINE COVARIANCE", "text": "We also consider the ARC-COSINE covariance of order p\nk(p)arc (x,x \u2032) =\n1 \u03c0 (\u2225x\u2225 \u2225x\u2032\u2225)p Jp\n( cos\u22121 ( x\u22a4x\u2032\n\u2225x\u2225\u2225x\u2032\u2225\n)) ,\n(4) where we have defined\nJp(\u03b1) = (\u22121)p(sin\u03b1)2p+1 ( 1\nsin\u03b1\n\u2202\n\u2202\u03b1 )p ( \u03c0 \u2212 \u03b1 sin\u03b1 ) .\nLet H(\u00b7) be the Heaviside function. Following Cho & Saul (2009), an integral representation of this covariance is:\nk(p)arc (x,x \u2032) = 2\n\u222b H(\u03c9\u22a4x) ( \u03c9\u22a4x )p H(\u03c9\u22a4x\u2032) ( \u03c9\u22a4x\u2032 )p \u00d7N (\u03c9|0, I)d\u03c9. (5)\nThis integral formulation immediately suggests a random feature approximation for the ARC-COSINE covariance in equation (4), noting that it can be seen as an expectation of the product of the same function applied to the inputs to the covariance. As before, this provides an approximate explicit representation of the mapping induced by the covariance function. Interestingly, for the ARC-COSINE covariance of order p = 1, this yields an approximation based on popular Rectified Linear Unit (ReLU) functions. We note that when p = 0 the resulting Heaviside activations are unsuitable for our inference scheme, given that they yield systematically zero gradients."}, {"heading": "3. Random Feature Expansions for DGPs", "text": "In this section, we present our approximate formulation of DGPs which, as we illustrate in the experiments, leads to a practical learning algorithm for these deep probabilistic\nnonparametric models. We propose to employ the random feature expansion at each layer, and by doing so we obtain an approximation to the original DGP model as a DNN (Figure 1).\nAssume that the GPs have zero mean, and define F (0) := X . Also, assume that the GP covariances at each layer are parameterized through a set of parameters \u03b8(l). The parameter set \u03b8(l) comprises the layer-wise GP marginal variances (\u03c32)(l) and lengthscale parameters \u039b(l) = diag((\u211321)\n(l), . . . , (\u21132 D\n(l) F\n)(l)).\nConsidering a DGP with RBF covariances, taking a \u201cweightspace view\u201d of the GPs at each layer, and extending the results in the previous section, we have that\n\u03a6 (l) rbf =\n\u221a (\u03c32)(l)\nN (l) RF\n[ cos ( F (l)\u2126(l) ) , sin ( F (l)\u2126(l) )] ,\n(6) and F (l+1) = \u03a6(l)rbfW (l). At each layer, the priors over the\nweights are p ( \u2126\n(l) \u00b7j\n) = N ( 0, ( \u039b(l) )\u22121) and p ( W\n(l) \u00b7i\n) =\nN (0, I). Each matrix \u2126(l) has dimensions DF (l) \u00d7 N (l) RF. On the other hand, the weight matrices W (l) have dimensions 2N (l)RF \u00d7 DF (l+1) (weighting of sine and cosine random features), with the constraint that DF (Nh) = Dout.\nSimilarly, considering a DGP with ARC-COSINE covariances of order p = 1, the application of the random feature approximation leads to DNNs with ReLU activations:\n\u03a6(l)arc =\n\u221a 2(\u03c32)(l)\nN (l) RF\nmax ( 0, F (l)\u2126(l) ) , (7)\nwith \u2126(l)\u00b7j \u223c N ( 0, ( \u039b(l) )\u22121) , which are cheaper to evaluate and differentiate than the trigonometric functions required in the RBF case. As in the RBF case, we allowed the covariance and the features to be scaled by (\u03c32)(l) and \u039b(l), respectively. The dimensions of the weight matrices \u2126(l) are the same as in the RBF case, but the dimensions of the W (l) matrices are N (l)RF \u00d7DF (l+1) ."}, {"heading": "3.1. Low-rank weights in the resulting DNN", "text": "Our formulation of an approximate DGP using random feature expansions reveals a close connection with DNNs. In our formulation, the design matrices at each layer are \u03a6(l+1) = \u03b3 ( \u03a6(l)W (l)\u2126(l+1) ) , where \u03b3(\u00b7) denotes the element-wise application of covariance-dependent functions, i.e., sine and cosine for the RBF covariance and ReLU for the ARC-COSINE covariance. Instead, for the DNN case, the design matrices are computed as \u03a6(l+1) = g(\u03a6(l)\u2126(l)), where g(\u00b7) is a so-called activation function. In light of this, we can view our approximate DGP model as a DNN. From a probabilistic standpoint, we can interpret our approximate\nDGP model as a DNN with specific Gaussian priors over the \u2126(l) weights controlled by the covariance parameters \u03b8(l), and standard Gaussian priors over the W (l) weights. Covariance parameters act as hyper-priors over the weights \u2126(l), and the objective is to optimize these during training.\nAnother observation about the resulting DGP approximation is that, for a given layer l, the transformations given by W (l) and \u2126(l+1) are both linear. If we collapsed the two transformations into a single one, by introducing weights \u039e(l) = W (l)\u2126(l+1), we would have to learn O ( N\n(l) RF \u00d7N (l+1) RF\n) weights at each layer, which is con-\nsiderably more than learning the two separate sets of weights. As a result, we can view the proposed approximate DGP model as a way to impose a low-rank structure on the weights of DNNs, which is a form of regularization proposed in the literature of DNNs (Novikov et al., 2015; Sainath et al., 2013; Denil et al., 2013)."}, {"heading": "3.2. Variational inference", "text": "In order to keep the notation uncluttered, let \u0398 be the collection of all covariance parameters \u03b8(l) at all layers. Also, consider the case of a DGP with fixed spectral frequencies \u2126(l) collected into \u2126, and let W be the collection of the weight matrices W (l) at all layers. For W we have a product of standard normal priors stemming from the approximation of the GPs at each layer p(W) = \u220fNh\u22121 l=0 p(W\n(l)), and we propose to treat W using variational inference following Kingma & Welling (2014) and Graves (2011), and optimize all covariance parameters \u0398. We will consider \u2126 to be fixed here, but we will discuss alternative ways to treat \u2126 in the next section. In the supplement we also assess the quality of the variational approximation over W, with \u2126 and \u0398 fixed, by comparing it with MCMC techniques.\nThe marginal likelihood p(Y |X,\u2126,\u0398) involves intractable integrals, but we can obtain a tractable lower bound using variational inference. Defining L = log [p(Y |X,\u2126,\u0398)] and E = Eq(W) (log [p (Y |X,W,\u2126,\u0398)]), we obtain\nL \u2265 E \u2212DKL [q(W)\u2225p (W)] , (8)\nwhere q(W) acts as an approximation to the posterior over all the weights p(W|Y,X,\u2126,\u0398).\nWe are interested in optimizing q(W), i.e. finding an optimal approximate distribution over the parameters according to the bound above. The first term can be interpreted as a model fitting term, whereas the second as a regularization term. In the case of a Gaussian distribution q(W) and a Gaussian prior p(W), it is possible to compute the DKL term analytically (see supplementary material), whereas the remaining term needs to be estimated. Assume a Gaussian approximating distribution that factorizes across layers\nand weights:\nq(W) = \u220f ijl q ( W (l) ij ) = \u220f ijl N ( m (l) ij , (s 2) (l) ij ) . (9)\nThe variational parameters are the mean and the variance of each of the approximating factors m(l)ij , (s 2) (l) ij , and we aim to optimize the lower bound with respect to these as well as all covariance parameters \u0398.\nIn the case of a likelihood that factorizes across observations, an interesting feature of the expression of the lower bound is that it is amenable to fast stochastic optimization. In particular, we derive a doubly-stochastic approximation of the expectation in the lower bound as follows. First, E can be rewritten as a sum over the input points, which allows us to estimate it in an unbiased fashion using minibatches, selecting m points indexed by Im:\nE \u2248 n m \u2211 k\u2208Im Eq(W)(log[p(yk|xk,W,\u2126,\u0398)]). (10)\nSecond, each of the elements of the sum can be estimated using NMC Monte Carlo samples, yielding:\nE \u2248 n m \u2211 k\u2208Im 1 NMC NMC\u2211 r=1 log[p(yk|xk,W\u0303r,\u2126,\u0398)], (11)\nwith W\u0303r \u223c q(W). In order to facilitate the optimization, we reparameterize the weights as follows:\n(W\u0303 (l)r )ij = s (l) ij \u03f5 (l) rij +m (l) ij . (12)\nBy differentiating the lower bound with respect to \u0398 and the mean and variance of the approximate posterior over W, we obtain an unbiased estimate of the gradient for the lower bound. The reparameterization trick ensures that the randomness in the computation of the expectation is fixed when applying stochastic gradient ascent moves to the parameters of q(W) and \u0398 (Kingma & Welling, 2014). Automatic differentiation tools enabled us to compute stochastic gradients automatically, which is why we opted to implement our model in TensorFlow (Abadi et al., 2015)."}, {"heading": "3.3. Treatment of the spectral frequencies \u2126", "text": "So far, we have assumed the spectral frequencies \u2126 to be sampled from the prior and fixed throughout, whereby we employ the reparameterization trick to obtain \u2126(l)ij = (\u03b22) (l) ij \u03b5 (l) rij + \u00b5 (l) ij , with (\u03b2 2) (l) ij and \u00b5 (l) ij determined by the\nprior p ( \u2126\n(l) \u00b7j\n) = N ( 0, ( \u039b(l) )\u22121) . We then draw the\n\u03b5 (l) rij\u2019s and fix them from the outset, such that covariance parameters \u0398 can be optimized along with q(W). We refer to this variant as PRIOR-FIXED.\nInspired by previous work on random feature expansions for GPs, we can think of alternative ways to treat these parameters, e.g., La\u0301zaro-Gredilla et al. (2010); Gal & Turner (2015). In particular, we study a variational treatment of \u2126; we refer the reader to the supplementary material for details on the derivation of the lower bound in this case. When being variational about \u2126, we introduce an approximate posterior q(\u2126) which also has a factorized form. We use the reparameterization trick once again, but \u2126 are now sampled from the posterior, which in general has different mean and variances to the prior. We report two variations of this treatment, namely VAR-FIXED and VAR-RESAMPLED. In VAR-FIXED, we fix \u03b5(l)rij in computing \u2126 throughout the learning of the model, whereas in VAR-RESAMPLED we resample these at each iteration. We note that one can also be variational about \u0398, but we leave this for future work.\nIn Figure 2, we illustrate the differences between the strategies discussed in this section; we report the accuracy of the proposed one-layer DGP with RBF covariances with respect to the number of random features on one of the datasets that we consider in the experiment section (EEG dataset). For PRIOR-FIXED, more random features result in a better approximation of the GP priors at each layer, and this results in better generalization. When we resample \u2126 from the approximate posterior (VAR-RESAMPLED), we notice that the model quickly struggles with the optimization when increasing the number of random features. We attribute this to the fact that the factorized form of the posterior over \u2126 and W is unable to capture posterior correlations between the coefficients for the random features and the weights of the corresponding linearized model. Being deterministic about the way spectral frequencies are computed (VARFIXED) offers the best performance among the three learning strategies, and this is what we employ throughout the rest of this paper."}, {"heading": "3.4. Computational complexity", "text": "When estimating the lower bound, there are two main operations performed at each layer, that is F (l)\u2126(l) and \u03a6(l)W (l). Recalling that this matrix product is done for\nsamples from the posterior over W (and \u2126 when treated variationally) and given the mini-batch formulation, the former costs O ( mD\n(l) F N (l) RFNMC\n) , while the latter costs\nO ( mN\n(l) RFD (l) F NMC\n) .\nBecause of feature expansions and stochastic variational inference, the resulting algorithm does not involve any Cholesky decompositions. This is in sharp contrast with stochastic variational inference using inducing-point approximations (see e.g. Dai et al., 2016; Bui et al., 2016), where such operations could significantly limit the number of inducing points that can be employed."}, {"heading": "4. Experiments", "text": "We evaluate our model by comparing it against relevant alternatives for both regression and classification, and assess its performance when applied to large-scale datasets. We also investigate the extent to which such deep compositions continue to yield good performance when the number of layers is significantly increased."}, {"heading": "4.1. Model Comparison", "text": "We primarily compare our model to the state-of-the-art DGP inference method presented in the literature, namely DGPs using expectation propagation (DGP-EP; Bui et al., 2016). We originally intended to include results for the variational auto-encoded DGP (Damianou & Lawrence, 2013); however, the results obtained using the available code were not competitive with DGP-EP and we thus decided to exclude them from the figures. We also omitted DGP training using sequential inference (Wang et al., 2016) given that we could not find an implementation of the method and, in any case, the performance reported in the paper is inferior to more recent approaches. We also compare against DNNs in order to present the results in a wider context, and demonstrate that DGPs lead to better quantification of uncertainty. Finally, to substantiate the benefits of using a deep model, we compare against the shallow sparse variational GP (Hensman et al., 2015b) implemented in GPflow (Matthews et al., 2016).\nWe use the same experimental set-up for both regression and classification tasks using datasets from the UCI repository (Asuncion & Newman, 2007), for models having one hidden layer. The results for architectures with two hidden layers are included in the supplementary material. The specific configurations for each model are detailed below:\nDGP-RBF, DGP-ARC : In the proposed DGP with an RBF kernel, we use 100 random features at every hidden layer to construct a multivariate GP with D(l)F = 3, and set the batch size to m = 200. We initially only use a single\nMonte Carlo sample, and halfway through the allocated optimization time, this is then increased to 100 samples. We employ the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.01, and in order to stabilize the optimization procedure, we fix the parameters \u0398 for 12, 000 iterations, before jointly optimizing all parameters. As discussed in Section 3.3, \u2126 are optimized variationally with fixed randomness. The same set-up is used for DGP-ARC, the variation of our model using the ARC-COSINE kernel;\nDGP-EP 1: For this technique, we use the same architecture and optimizer as for DGP-RBF and DGP-ARC, a batch size of 200 and 100 inducing points at each hidden layer. For the classification case, we use 100 samples for approximating the Softmax likelihood;\nDNN : We construct a DNN configured with a dropout rate of 0.5 at each hidden layer in order to provide regularization during training. In order to preserve a degree of fairness, we set the number of hidden units in such a way as to ensure that the number of weights to be optimized match those in the DGP-RBF and DGP-ARC models when the random features are taken to be fixed.\nWe assess the performance of each model using the error rate (RMSE in the regression case) and mean negative loglikelihood (MNLL) on withheld test data. The results are averaged over 3 folds for every dataset. The experiments were launched on single nodes of a cluster of Intel Xeon E5-2630 CPUs having 32 cores and 128GB RAM.\nFigure 3 shows that DGP-RBF and DGP-ARC consistently outperform competing techniques both in terms of convergence speed and predictive accuracy. This is particu-\n1Code obtained from: github.com/thangbui/deepGP_approxEP\nlarly significant for larger datasets where other techniques take considerably longer to converge to a reasonable error rate, although DGP-EP converges to superior MNLL for the PROTEIN dataset. The results are also competitive (and sometimes superior) to those obtained by the variational GP (VAR-GP) in Hensman et al. (2015b). It is striking to see how inferior uncertainty quantification provided by the DNN (which is inherently limited to the classification case, so no MNLL reported on regression datasets) is compared to DGPs, despite the error rate being comparable.\nBy virtue of its higher dimensionality, larger configurations were used for MNIST. For DGP-RBF and DGP-ARC, we use 500 random features, 50 GPs in the hidden layers, batch size of 1000, and Adam with a 0.001 learning rate. Similarly for DGP-EP, we use 500 inducing points, with the only difference being a slightly smaller batch size to cater for issues with memory requirements. Following Simard et al. (2003), we employ 800 hidden units at each layer of the DNN. The DGP-RBF peaks at 98.04% and 97.93% for 1 and 2 hidden layers respectively. It was observed that the model performance degrades noticeably when more than 2 hidden layers are used (without feeding forward the inputs). This is in line with what is reported in the literature on DNNs (Neal, 1996; Duvenaud et al., 2014). By simply re-introducing the original inputs in the hidden layer, the accuracy improves to 98.2% for the one hidden layer case.\nRecent experiments on MNIST using a variational GP with MCMC report overall accuracy of 98.04% (Hensman et al., 2015a), while the AutoGP architecture has been shown to give 98.45% accuracy (Krauth et al., 2016). Using a finer-tuned configuration, DNNs were also shown to obtain 98.5% accuracy (Simard et al., 2003), whereas 98.6% has been reported for SVMs (Scho\u0308lkopf, 1997). In view of this wider scope of inference techniques, it can be confirmed\nthat the results obtained using the proposed architecture are comparable to the state-of-the-art, even if further extensions may be required for obtaining a proper edge. Note that this comparison focuses on approaches without preprocessing, and excludes convolutional neural nets."}, {"heading": "4.2. Large-scale Datasets", "text": "One of the defining characteristics of our model is the ability to scale up to large datasets without compromising on performance and accuracy in quantifying uncertainty. As a demonstrative example, we evaluate our model on two large-scale problems which go beyond the scale of datasets to which GPs and especially DGPs are typically applied.\nWe first consider MNIST8M, which artificially extends the original MNIST dataset to 8+ million observations. We trained this model using the same configuration described for standard MNIST, and we obtained 99.14% accuracy on the test set using one hidden layer. Given the size of this dataset, there are only few reported results for other GP models. Most notably, Krauth et al. (2016) recently obtained 99.11% accuracy with the AutoGP framework, which is comparable to the result obtained by our model.\nMeanwhile, the AIRLINE dataset contains flight information for 5+ million US flights. Following the procedure described in Hensman et al. (2013) and Wilson et al. (2016), we use this 8-dimensional dataset for classification, where the task is to determine whether a flight has been delayed or not. We construct the test set using the scripts provided in Wilson et al. (2016), where 100, 000 data points are heldout for testing. We construct our DGP models using 100 random features at each layer, and set the dimensionality to DF (l) = 3. As shown in Table 1, our model works significantly better when using the RBF kernel. In addition, the results are also directly comparable to those obtained by Wilson et al. (2016), which reports accuracy and MNLL of 78.1% and 0.457, respectively."}, {"heading": "4.3. Model Depth", "text": "Finally, we assess the scalability of our model with respect to additional hidden layers in the constructed model. In particular, we re-consider the AIRLINE dataset and evaluate the performance of DGP-RBF models constructed using up to 30 layers. In order to cater for the increased depth in the\nmodel, we feed-forward the original input to each hidden layer, as suggested in Duvenaud et al. (2014).\nFigure 4 reports the progression of error rate and MNLL over time for different number of hidden layers, using the results obtained in Wilson et al. (2016) as a baseline (reportedly obtained in about 3 hours). As expected, the model takes longer to train as the number of layers increases. However, the model converges to an optimal state in every case in less than a couple of hours, with an improvement being noted in the case of 10 and 20 layers over the shallower 2-layer model. The box plot within the same figure indicates that the negative lower bound is a suitable objective function for carrying out model selection."}, {"heading": "5. Conclusions", "text": "In this work, we have proposed a novel formulation of DGPs which exploits the approximation of covariance functions using random features, as well as stochastic variational inference for preserving the probabilistic representation of a regular GP. We demonstrated how inference using this model is not only faster, but also frequently superior to other state-of-the-art methods, with particular emphasis on competing DGP models. The results obtained for both the AIRLINE dataset and the MNIST8M digit recognition problem are particularly impressive since such large datasets have been generally considered to be beyond the computational scope of DGPs. We perceive this to be a considerable step forward in the direction of scaling and accelerating DGPs.\nThe results obtained on higher-dimensional datasets strongly suggest that approximations such as Fastfood (Le et al., 2013) could be instrumental in the interest of using more random features. We are also currently investigating ways to mitigate the decline in performance observed when optimizing \u2126 variationally with resampling. The obtained results also encourage the extension of our model to include convolutional layers suitable for computer vision applications."}, {"heading": "ACKNOWLEDGEMENTS", "text": "MF gratefully acknowledges support from the AXA Research Fund. PM was partially supported by the EU project H2020-644182 \u201cIOStack\u201d."}], "year": 2017, "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "authors": ["Abadi", "Mart\u0131\u0301n", "Agarwal", "Ashish", "Barham", "Paul"], "year": 2015}, {"title": "Weight Uncertainty in Neural Network", "authors": ["Blundell", "Charles", "Cornebise", "Julien", "Kavukcuoglu", "Koray", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "year": 2015}, {"title": "Kernel methods for deep learning", "authors": ["Cho", "Youngmin", "Saul", "Lawrence K"], "venue": "In Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems", "year": 2009}, {"title": "Variational auto-encoded deep Gaussian processes", "authors": ["Dai", "Zhenwen", "Damianou", "Andreas", "Gonz\u00e1lez", "Javier", "Lawrence", "Neil"], "venue": "In Proceedings of the Fourth International Conference on Learning Representations,", "year": 2016}, {"title": "Deep Gaussian Processes", "authors": ["Damianou", "Andreas C", "Lawrence", "Neil D"], "venue": "In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics,", "year": 2013}, {"title": "Predicting Parameters in Deep Learning", "authors": ["Denil", "Misha", "Shakibi", "Babak", "Dinh", "Laurent", "Ranzato", "Marc\u2019Aurelio", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems", "year": 2013}, {"title": "Avoiding pathologies in very deep networks", "authors": ["Duvenaud", "David K", "Rippel", "Oren", "Adams", "Ryan P", "Ghahramani", "Zoubin"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "year": 2014}, {"title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "authors": ["Gal", "Yarin", "Ghahramani", "Zoubin"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "year": 2016}, {"title": "Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs", "authors": ["Gal", "Yarin", "Turner", "Richard"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "year": 2015}, {"title": "Practical Variational Inference for Neural Networks", "authors": ["Graves", "Alex"], "venue": "Advances in Neural Information Processing Systems", "year": 2011}, {"title": "Nested Variational Compression in Deep Gaussian Processes", "authors": ["Hensman", "James", "Lawrence", "Neil D"], "year": 2014}, {"title": "Gaussian processes for big data", "authors": ["Hensman", "James", "Fusi", "Nicol\u00f3", "Lawrence", "Neil D"], "venue": "In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,", "year": 2013}, {"title": "Scalable variational Gaussian process classification", "authors": ["Hensman", "James", "de G. Matthews", "Alexander G", "Ghahramani", "Zoubin"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "year": 2015}, {"title": "Auto-Encoding Variational Bayes", "authors": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the Second International Conference on Learning Representations,", "year": 2014}, {"title": "AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models", "authors": ["Krauth", "Karl", "Bonilla", "Edwin V", "Cutajar", "Kurt", "Filippone", "Maurizio"], "venue": "arXiv preprint 1610.05392,", "year": 2016}, {"title": "Sparse Spectrum Gaussian Process Regression", "authors": ["M. L\u00e1zaro-Gredilla", "J. Quinonero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Fastfood - computing Hilbert space expansions in loglinear time", "authors": ["Le", "Quoc V", "Sarls", "Tams", "Smola", "Alexander J"], "venue": "In ICML (3),", "year": 2013}, {"title": "Bayesian methods for backpropagation networks", "authors": ["D.J.C. Mackay"], "venue": "Models of Neural Networks III,", "year": 1994}, {"title": "GPflow: A Gaussian process library using TensorFlow", "authors": ["Matthews", "Alexander G. de G", "van der Wilk", "Mark", "Nickson", "Tom", "Fujii", "Keisuke", "Boukouvalas", "Alexis", "Le\u00f3nVillagr\u00e1", "Pablo", "Ghahramani", "Zoubin", "Hensman", "James"], "venue": "arXiv preprint 1610.08733,", "year": 2016}, {"title": "Bayesian Learning for Neural Networks (Lecture Notes in Statistics). Springer, 1 edition", "authors": ["Neal", "Radford M"], "year": 1996}, {"title": "Tensorizing Neural Networks", "authors": ["Novikov", "Alexander", "Podoprikhin", "Dmitry", "Osokin", "Anton", "Vetrov", "Dmitry P"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "year": 2015}, {"title": "Random Features for Large-Scale Kernel Machines", "authors": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "Advances in Neural Information Processing Systems", "year": 2008}, {"title": "Gaussian Processes for Machine Learning", "authors": ["Rasmussen", "Carl E", "Williams", "Christopher"], "year": 2006}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "authors": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "year": 2014}, {"title": "Support vector learning", "authors": ["Sch\u00f6lkopf", "Bernhard"], "venue": "PhD thesis, Berlin Institute of Technology,", "year": 1997}, {"title": "Kernel Methods for Pattern Analysis", "authors": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "year": 2004}, {"title": "Neural networks with periodic and monotonic activation functions: a comparative study in classification problems", "authors": ["J.M. Sopena", "E. Romero", "R. Alquezar"], "venue": "In Artificial Neural Networks,", "year": 1999}, {"title": "The Variational Gaussian Process", "authors": ["Tran", "Dustin", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "In Proceedings of the Fourth International Conference on Learning Representations,", "year": 2016}, {"title": "Sequential inference for deep Gaussian process", "authors": ["Wang", "Yali", "Brubaker", "Marcus A", "Chaib-draa", "Brahim", "Urtasun", "Raquel"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Stochastic variational deep kernel learning", "authors": ["Wilson", "Andrew Gordon", "Hu", "Zhiting", "Salakhutdinov", "Ruslan", "Xing", "Eric P"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems", "year": 2016}], "id": "SP:31b12d92d3a26d4a81bba1f73bbfd4ba315ce855", "authors": [{"name": "Kurt Cutajar", "affiliations": []}, {"name": "Edwin V. Bonilla", "affiliations": []}, {"name": "Pietro Michiardi", "affiliations": []}, {"name": "Maurizio Filippone", "affiliations": []}], "abstractText": "The composition of multiple Gaussian Processes as a Deep Gaussian Process (DGP) enables a deep probabilistic nonparametric approach to flexibly tackle complex machine learning problems with sound quantification of uncertainty. Existing inference approaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work we introduce a novel formulation of DGPs based on random feature expansions that we train using stochastic variational inference. This yields a practical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and performance of our proposal on several datasets with up to 8 million observations, and various DGP architectures with up to 30 hidden layers.", "title": "Random Feature Expansions for Deep Gaussian Processes"}