0	30	Coherence is a key factor that distinguishes well-written texts from random collections of sentences.
1	39	A potential application of coherence models is text quality assessment.
2	18	Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010).
6	18	Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016).
20	15	Our model encodes salient information that relates two adja- cent sentences based on the two most similar RNN states in sentences.
76	19	For sentence si, the lookup table returns matrix Ei whose rows are embeddings of words in si.
77	17	A weakness of former lexical coherence models (Somasundaran et al., 2014; Mesgar and Strube, 2016) is that they only rely on semantic relations between words in sentences, regardless of the current context of words.
84	14	Given two adjacent sentences, two of their LSTM states that have the highest similarity are selected to connect them.
93	14	Since averaging in the vector space is an effective way to accumulate information represented in some vectors (Iyyer et al., 2015; Wieting et al., 2016), we compute the average of two identified vectors among the LSTM states of two adjacent sentences to represent semantic information shared by the sentences.
96	16	Since sentences in a coherent text are about similar topics and share some semantic information, we compute semantic similarity between adjacent information states, i.e. �fis, to capture how they are changing through a text.
108	22	CohEmb has no RNN layer, so the model is built directly on word embeddings.
129	23	Texts that are more coherent are supposed to be faster to read and easier to understand.
136	26	Given the graph representation of a text, its coherence is encoded as a vector whose elements are frequencies of different subgraphs in the graph.
147	18	For this task, we use an output layer to map coherence vector �p to score s which quantifies the degree of the perceived coherence of document d. Formally, the output layer is sd = �u · �p + b where �u and b are the weight vector and bias, respectively.
159	14	The quality of a model is measured in terms of accuracy, which is the fraction of pairs that are correctly ranked by a model divided by the total number of document-pairs.
170	29	An essay scoring system assigns an essay a score reflecting the quality of the essay.
171	17	The quality of an essay depends on various factors including coherence.
180	13	This system uses a neural regression method as described above.
183	27	This model combines the feature vector computed by EASE, �f , and the coherence vector produced by CohEmb, �p, to have a more reliable representation of an essay.
211	38	To gain a better insight, we ablate EASE feature vectors and compare the performance of the coherence models, i.e., CohLSTM, and CohEmb.
217	17	The largest improvement for CohLSTM, with respect to EASE, is obtained on prompt 7 and 8.
219	26	The guidelines of these two prompts, which are publicly available in the Kaggle data, ask human annotators to assign the highest score to essays that are coherent and hold the attention of readers through an essay.
223	18	Essays are expected to contain specific infor- mation from the memoir so that an essay with the highest score has the highest coverage of all relevant and specific information from the memoir.
228	25	In this sense, our model goes beyond entity-based coherence models, which need extra dependencies such as coreference resolution systems.
230	50	Our model relates sentences by means of distant relations between word representations.
231	13	The most similar LSTM states in two adjacent sentences are selected to encode the salient semantic concept that relates the sentences.
232	14	The model finally employs a convolutional layer to extract and represent patterns of topic changes across sentences in a text as a coherence vector.
235	64	On the latter, it significantly improves the performance of a strong essay scorer.
236	118	We believe the reason that our system works is that it learns which semantic concepts of sentences should be used to relate sentences, and which information about concepts is required to model sentence-to-sentence transitions.
237	84	In future work we intend to run qualitative experiments on patterns that are extracted by our model to see if they are also linguistically interpretable.
