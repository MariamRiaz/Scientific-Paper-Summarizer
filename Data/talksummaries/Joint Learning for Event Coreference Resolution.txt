0	56	Within-document event coreference resolution is the task of determining which event mentions in a text refer to the same real-world event.
1	31	Compared to entity coreference resolution, event coreference resolution is not only much less studied, but it is arguably more challenging.
3	42	One such component is the trigger detection system, which is responsible for identifying event triggers and determining their event subtypes.
4	30	As is commonly known, trigger detection is another challenging task that is far from being solved.
7	23	The best-performing KBP 2016 system on English trigger detection achieved only an F-score of 47 (Lu and Ng, 2016).1 Given the difficulty of trigger detection, it is conceivable that many errors will propagate from the trigger detection component to the event coreference component in any pipeline architecture where trigger detection precedes event coreference resolution.
9	74	For instance, two event mentions could be wrongly posited as coreferent if the underlying triggers were wrongly predicted to have the same subtype.
10	12	Nevertheless, the top-performing systems in the KBP 2016 event coreference task all adopted the aforementioned pipeline architecture (Liu et al., 2016; Lu and Ng, 2016; Nguyen et al., 2016).
20	19	Note that in our joint model, anaphoricity serves as an auxiliary task: its intended use is to improve trigger detection and event coreference, potentially mediating the interaction between trigger detection and event coreference.
27	19	First, we present a joint model of event coreference, trigger detection, and anaphoricity that is novel in terms of the choice of tasks and the features used to capture cross-task interactions.
66	20	Each ti takes a value in the set of 18 event subtypes defined in KBP 2016 or NONE, which indicates that the event mention is not a trigger.
68	34	In other words, the value of each ci is the id of its antecedent, which can be one of the preceding event mentions or NEW (if the event mention underlying ci starts a new cluster).
70	27	Our model induces the following log-linear probability distribution over these variables: p(t,a, c|x; Θ) ∝ exp( ∑ i θifi(t,a, c,x)) where θi ∈ Θ is the weight associated with feature function fi and x is the input document.
79	17	Each candidate trigger t is represented using t’s word, t’s lemma, word bigrams formed with a window size of three from t, as well as feature conjunctions created by pairing t’s lemma with each of the following features: the head word of the entity syntactically closest to t, the head word of the entity textually closest to t, the entity type of the entity that is syntactically closest to t, and the entity type of the entity that is textually closest to t.4 In addition, for event mentions with verb triggers, we use the head words and the entity types of their subjects and objects as features, where the subjects and objects are extracted from the dependency parse trees obtained using Stanford CoreNLP (Manning et al., 2014).
86	11	Specifically, we create a conjunction between mj’s lemma and the number of sentences preceding mj , as well as a conjunction between mj’s lemma and the number of mentions preceding mj in the document.
91	51	We fire features that conjoin each candidate event mention’s event subtype, the lemma of its trigger and its anaphoricity.
92	103	We define our joint coreference and trigger detection factors such that the features defined on subtype variables ti and tj are fired only if current mention mj is coreferent with preceding mention mi.
105	31	Next, we learn the model parameters to maximize the following conditional likelihood of the training data with L1 regularization: L(Θ) = d∑ i=1 log ∑ c∗∈A(C∗i ) p′(t∗i ,a ∗ i , c ∗|xi; Θ)+λ‖Θ‖1 In this objective, p′ is obtained by augmenting the distribution p (defined in Section 3.1) with task-specific parameterized loss functions: p′(t,a, c|xi; Θ) ∝ p(t,a, c|xi; Θ) exp[αtlt(t, t∗) + αala(a,a ∗) + αclc(c, C∗)] where lt, la and lc are task-specific loss functions, and αt, αa and αc are the associated weight parameters that specify the relative importance of the three tasks in the objective function.
126	10	We perform training and evaluation on the KBP 2016 English and Chinese corpora.
127	10	For English, we train models on 509 of the training documents, tune parameters on 139 training documents, and report results on the official KBP 2016 English test set.6 For Chinese, we train models on 302 of the training documents, tune parameters on 81 training documents, and report results on the official KBP 2016 Chinese test set.
133	13	The top half of the table shows the results on the English evaluation set.
134	10	Specifically, row 1 shows the performance of the best event coreference system participating in KBP 2016 (Lu and Ng, 2016).
139	10	Row 2 shows the performance of the independent models, each of which is trained independently of the other models.
141	12	As we can see, the independent models outperform the top KBP 2016 system by 1.2 points in AVG-F for event coreference and 1.83 points for trigger detection.
150	40	While the top KBP system outperforms the independent models for both tasks (by 0.59 points in AVG-F for event coreference and 0.19 points for trigger detection), our joint model outperforms the independent models for all three tasks: by 1.95 points for event coreference, 4.02 points for anaphoricity determination, and 0.71 points for trigger detection.
154	23	For both datasets, the joint model’s superior performance to the independent coreference model stems primarily from considerable improvements in MUC F-score.
197	15	We proposed a joint model of event coreference resolution, trigger detection, and event anaphoricity determination.
205	19	This appendix describes how we (partially) address this issue that involves allowing each event mention to be associated with multiple event subtypes.
207	34	First, for each word triggering multiple event mentions (with different event subtypes), we merge their event mentions into one event mention having the combined subtype.
211	21	To train our joint model, however, the trigger annotations and the event coreference annotations in the training data must be consistent.
213	12	First, let C1 and C2 be two event coreference chains in a training document such that the set of words triggering the event mentions in C1 (with subtype t1) is the same as that triggering the event mentions in C2 (with subtype t2).
