0	33	Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information.
2	46	Extractive methods generate summary for a document by directly selecting salient sentences from the original document.
4	78	Recent neural models enable an end-to-end framework for natural language generation, which inspires the research on abstractive document summarization.
5	20	Most existing work directly apply the neural encoding-decoding framework, which first encodes the input into an abstract representation and then decodes the output based on the encoded information.
10	26	Although the encodingdecoding framework has implicitly modeled the information selection process via end-to-end training, we argue that abstractive document summarization shall benefit from explicitly modeling and optimizing it by capturing both the global document information and local inter-sentence relations.
11	43	In this paper, we propose to extend the encoding-decoding framework to model the information selection process explicitly.
14	15	In our model, both the document and summary are processed sentence by sentence, to better capture the inter-sentence relations.
15	45	The information selection layer consists of two parts: gated global information filtering and local sentence selection.
16	26	Unnecessary information in the original document are first globally filtered by a gated network, then important sentences are selected locally while generating each summary sentence sequentially.
22	47	Then the information selection layer selects and filters the sentence representa- tions based on the global document representation.
23	47	A sentence selection RNN is used to select salient and relevant sentences while generating each summary sentence sequentially based on the tailored sentence representations.
24	16	At last, the summary decoder produces the output summary to paraphrase and generalize the selected sentences.
53	42	The representation of the selected source sentences is computed by: qt = ∑ j αjtfj (8) which is used as initial state of the summary decoder to generate a summary sentence to paraphrase and generalize the selected sentences.
60	23	Despite the end-to-end training for the performance of generated summary, we also directly optimize the sentence selection decisions by importing supervision for the sentence selection vector αt in Equation 5.
62	21	To simulate the sentence selection process on human-written abstracts, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and corresponding source document sentences and normalize them into distantly-labelled sentence selection vector pt.
64	17	The sentence selection loss is imported into the final loss function to be optimized with the summary generation component together.
97	15	Three data annotators were asked to compare the generated summaries with the human summaries, and assess each summary from four independent perspectives: (1) Informative: How informative the summary is?
109	24	The summary generated by our method obviously contains more salient information, which shows the effectiveness of the information selection component in our model.
114	32	The visualization of the sentence selection vectors of the gold reference summary and the three abstractive models when generating the presented examples in Table 3 are shown in Figure 22.
116	20	Coverage learns to reduce repetitions, but fails to detect all the salient information.
126	19	Our method much outperforms all the comparison systems and removing each component of our model one by one will leads to sustained significant performance declining, which verifies the effectiveness of each component in our model.
131	17	The sentence extractor extracts the source sentence with the largest weight in each sentence generation step.
134	16	Results in Table 5 show that our simple extractive method OurExtractive significantly outperforms state-of-the-art neural extractive baselines, which demonstrates the effectiveness of the information selection component in our model.
151	21	These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines.
153	36	The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions.
155	76	Distraction-based attention model (Chen et al., 2016) and coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem.
157	55	Recently, Tan et al. (2017a) propose to leverage the hierarchical encoder-decoder architecture on generating multi-sentence summaries, and incorporate sentence-ranking into the summary generation process based on the graph-based attention mechanism.
160	29	In this paper, we have analyzed the necessity of explicitly modeling and optimizing of the information selection process in document summarization, and verified its effectiveness by extending the basic neural encoding-decoding framework with an information selection layer and optimizing it with distantly-supervised training.
162	31	Experimental results demonstrate that both of them are effective for helping select salient information during the summary generation process, which significantly improves the document summarization performance.
163	25	Our model combines the strengths of extractive methods and abstractive methods, which can generate more informative and concise summaries, and thus achieves state-of-the-art abstractive document summarization performance and is also competitive with state-of-the-art extractive models.
