0,1,label2,summary_sentences
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 603–612 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1056",text,[0],[0]
Entering a new group is rarely easy.,1 Introduction,[0],[0]
"Adjusting to unfamiliar behavioral norms and donning a new identity can be cognitively and emotionally taxing, and failure to do so can lead to exclusion.",1 Introduction,[0],[0]
"But successful enculturation to the group often yields significant rewards, especially in organizational contexts.",1 Introduction,[0],[0]
"Fitting in has been tied to positive career outcomes such as faster time-to-promotion, higher performance ratings, and reduced risk of being fired (O’Reilly et al., 1991; Goldberg et al., 2016).
",1 Introduction,[0],[0]
"A major challenge for enculturation research is distinguishing between internalization and self-
regulation.",1 Introduction,[0],[0]
"Internalization, a more inwardly focused process, involves identifying as a group member and accepting group norms, while selfregulation, a more outwardly oriented process, entails deciphering the group’s normative code and adjusting one’s behavior to comply with it.",1 Introduction,[0],[0]
"Existing approaches, which generally rely on selfreports, are subject to various forms of reporting bias and typically yield only static snapshots of this process.",1 Introduction,[0],[0]
"Recent computational approaches that use language as a behavioral signature of group integration uncover dynamic traces of enculturation but cannot distinguish between internalization and self-regulation.
",1 Introduction,[0],[0]
"To overcome these limitations, we introduce a dynamic measure of directed linguistic accommodation between a newcomer and existing group members.",1 Introduction,[0],[0]
Our approach differentiates between an individual’s (1) base rate of word use and (2) linguistic alignment to interlocutors.,1 Introduction,[0],[0]
"The former corresponds to internalization of the group’s linguistic norms, whereas the latter reflects the capacity to regulate one’s language in response to peers’ language use.",1 Introduction,[0],[0]
"We apply this language model to a corpus of internal email communications and personnel records, spanning a seven-year period, from a mid-sized technology firm.",1 Introduction,[0],[0]
"We show that changes in base rates and alignment, especially with respect to pronoun use, are consistent with successful assimilation into a group and can predict eventual employment outcomes— continued employment, involuntary exit, or voluntary exit—at levels above chance.",1 Introduction,[0],[0]
We use this predictive problem to investigate the nature of linguistic alignment.,1 Introduction,[0],[0]
"Our results suggest that the common formulation of alignment as a lexical-level phenomenon is incomplete.
603",1 Introduction,[0],[0]
Linguistic alignment Linguistic alignment is the tendency to use the same or similar words as one’s conversational partner.,2 Linguistic Alignment and Group Fit,[0],[0]
"Alignment is an instance of a widespread and socially important human behavior: communication accommodation, the tendency of two interacting people to nonconsciously adopt similar behaviors.",2 Linguistic Alignment and Group Fit,[0],[0]
"Evidence of accommodation appears in many behavioral dimensions, including gestures, postures, speech rate, self-disclosure, and language or dialect choice (see Giles et al. (1991) for a review).",2 Linguistic Alignment and Group Fit,[0],[0]
"More accommodating people are rated by their interlocutors as more intelligible, attractive, and cooperative (Feldman, 1968; Ireland et al., 2011; Triandis, 1960).",2 Linguistic Alignment and Group Fit,[0],[0]
"These perceptions have material consequences—for example, high accommodation requests are more likely to be fulfilled, and pairs who accommodate more in how they express uncertainty perform better in lab-based tasks (Buller and Aune, 1988; Fusaroli et al., 2012).
",2 Linguistic Alignment and Group Fit,[0],[0]
"Although accommodation is ubiquitous, individuals vary in their levels of accommodation in ways that are socially informative.",2 Linguistic Alignment and Group Fit,[0],[0]
"Notably, more powerful people are accommodated more strongly in many settings, including trials (Gnisci, 2005), online forums (Danescu-Niculescu-Mizil et al., 2012), and Twitter (Doyle et al., 2016).",2 Linguistic Alignment and Group Fit,[0],[0]
"Most relevant for this work, speakers may increase their accommodation to signal camaraderie or decrease it to differentiate from the group.",2 Linguistic Alignment and Group Fit,[0],[0]
"For example, Bourhis and Giles (1977) found that Welsh English speakers increased their use of the Welsh accent and language in response to an English speaker who dismissed it.
",2 Linguistic Alignment and Group Fit,[0],[0]
Person-group fit and linguistic alignment These findings suggest that linguistic alignment is a useful avenue for studying how people assimilate into a group.,2 Linguistic Alignment and Group Fit,[0],[0]
"Whereas traditional approaches to studying person-group fit rely on self-reports that are subject to various forms of reporting bias and cannot feasibly be collected with high granularity across many points in time, recent studies have proposed language-based measures as a means to tracing the dynamics of person-group fit without having to rely on self-reports.",2 Linguistic Alignment and Group Fit,[0],[0]
"Building on Danescu-Niculescu-Mizil et al. (2013)’s research into language use similarities as a proxy for social distance between individuals, Srivastava et al. (forthcoming) and Goldberg et al. (2016) devel-
oped a measure of cultural fit based on the similarity in linguistic style between individuals and their colleagues in an organization.",2 Linguistic Alignment and Group Fit,[0],[0]
"Their timevarying measure highlights linguistic compatibility as an important facet of cultural fit and reveals distinct trajectories of enculturation for employees with different career outcomes.
",2 Linguistic Alignment and Group Fit,[0],[0]
"While this approach can help uncover the dynamics and consequences of an individual’s fit with her colleagues in an organization, it cannot disentangle the underlying reasons for this alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
"For two primary reasons, it cannot distinguish between fit that arises from internalization and fit produced by self-regulation.",2 Linguistic Alignment and Group Fit,[0],[0]
"First, Goldberg et al. (2016) and Srivastava et al. (forthcoming) define fit using a symmetric measure, the Jensen-Shannon divergence, which does not take into account the direction of alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
Yet the distinction between an individual adapting to peers versus peers adapting to the individual would appear to be consequential.,2 Linguistic Alignment and Group Fit,[0],[0]
"Second, this prior work considers fit across a wide range of linguistic categories but does not interrogate the role of particular categories, such as pronouns, that can be especially informative about enculturation.",2 Linguistic Alignment and Group Fit,[0],[0]
"For example, a person’s base rate use of the first-person singular (I) or plural (we) might indicate the degree of group identity internalization, whereas adjustment to we usage in response to others’ use of the pronoun might reveal the degree of self-regulation to the group’s normative expectations.
",2 Linguistic Alignment and Group Fit,[0],[0]
"Modeling fit with WHAM To address these limitations, we build upon and extend the WHAM alignment framework (Doyle and Frank, 2016) to analyze the dynamics of internalization and selfregulation using the complete corpus of email communications and personnel records from a mid-sized technology company over a seven-year period.",2 Linguistic Alignment and Group Fit,[0],[0]
"WHAM uses a conditional measure of alignment, separating overall homophily (unconditional similarity in people’s language use, driven by internalized similarity) from in-the-moment adaptation (adjusting to another’s usage, corresponding to self-regulation).",2 Linguistic Alignment and Group Fit,[0],[0]
"WHAM also provides a directed measure of alignment, in that it estimates a replier’s adaptation to the other conversational participant separately from the participant’s adaptation to the replier.
Level(s) of alignment The convention within linguistic alignment research, dating back to early
work on Linguistic Style Matching (Niederhoffer and Pennebaker, 2002), is to look at lexical alignment: the repetition of the same or similar words across conversation participants.",2 Linguistic Alignment and Group Fit,[0],[0]
"From a communication accommodation standpoint, this is justified by assuming that one’s choice of words represents a stylistic signal that is partially independent of the meaning one intends to express—similar to the accommodation on paralinguistic signals discussed above.",2 Linguistic Alignment and Group Fit,[0],[0]
"The success of previous linguistic alignment research shows that this is valid.
",2 Linguistic Alignment and Group Fit,[0],[0]
"However, words are difficult to divorce from their meanings, and sometimes repeating a word conflicts with repeating its referent.",2 Linguistic Alignment and Group Fit,[0],[0]
"In particular, pronouns often refer to different people depending on who uses the pronoun.",2 Linguistic Alignment and Group Fit,[0],[0]
"While there is evidence that one person using a first-person singular pronoun increases the likelihood that her conversation partner will as well (Chung and Pennebaker, 2007), we may also expect that one person using first-person singular pronouns may cause the other to use more second-person pronouns, so that both people are referring to the same person.",2 Linguistic Alignment and Group Fit,[0],[0]
"This is especially important under the Interactive Alignment Model view (Pickering and Garrod, 2004), where conversants align their entire mental representations, which predicts both lexical and referential alignment behaviors will be observed.",2 Linguistic Alignment and Group Fit,[0],[0]
"Discourse-strategic explanations for alignment also predict alignment at multiple levels (Doyle and Frank, 2016).
",2 Linguistic Alignment and Group Fit,[0],[0]
"Since we have access to a high-quality corpus with meaningful outcome measures, we can investigate the relative importance of these two types of alignment.",2 Linguistic Alignment and Group Fit,[0],[0]
"We will show that referential alignment is more predictive of employment outcomes than is lexical alignment, suggesting a need for alignment research to consider both levels rather than just the latter.",2 Linguistic Alignment and Group Fit,[0],[0]
"We use the complete corpus of internal emails exchanged among full-time employees at a midsized US-based technology company between 2009 to 2014 (Srivastava et al., forthcoming).",3 Data: Corporate Email Corpus,[0],[0]
Each email was summarized as a count of word categories in its text.,3 Data: Corporate Email Corpus,[0],[0]
"These categories are a subset of the Linguistic Information and Word Count system (Pennebaker et al., 2007).",3 Data: Corporate Email Corpus,[0],[0]
"The categories were chosen because they are likely to be indica-
tive of one’s standing/role within a group.1
We divided email chains into message-reply pairs to investigate conditional alignment between a message and its reply.",3 Data: Corporate Email Corpus,[0],[0]
"To limit these pairs to cases where the reply was likely related to the preceding message, we removed all emails with more than one sender or recipient (including CC/BCC), identical sender and recipient, or where the sender or recipient was an automatic notification system or any other mailbox that was not specific to a single employee.",3 Data: Corporate Email Corpus,[0],[0]
"We also excluded emails with no body text or more than 500 words in the body text, and pairs with more than a week’s latency between message and reply.
",3 Data: Corporate Email Corpus,[0],[0]
"Finally, because our analyses involve enculturation dynamics over the first six months of employment, we excluded replies sent by an employee whose overall tenure was less than six months.",3 Data: Corporate Email Corpus,[0],[0]
"This resulted in a collection of 407,779 messagereply pairs, with 485 distinct replying employees.",3 Data: Corporate Email Corpus,[0],[0]
We combined this with monthly updates of employees joining and leaving the company and whether they left voluntarily or involuntarily.,3 Data: Corporate Email Corpus,[0],[0]
"Of the 485, 66 left voluntarily, 90 left involuntarily, and 329 remained employed at the end of the observation period.
",3 Data: Corporate Email Corpus,[0],[0]
Privacy protections and ethical considerations Research based on employees’ archived electronic communications in organizational settings poses potential threats to employee privacy and company confidentiality.,3 Data: Corporate Email Corpus,[0],[0]
"To address these concerns, and following established ethical guidelines for the conduct of such research (Borgatti and Molina, 2003), we implemented the following procedures: (a) raw data were stored on secure research servers behind the company’s firewall; (b) messages exchanged with individuals outside the firm were eliminated; (c) all identifying information such as email addresses was transformed into hashed identifiers, with the company retaining access to the key code linking identifying information to hashed identifiers; and (d) raw message content was transformed into linguistic categories so that identities could not be inferred from message content.",3 Data: Corporate Email Corpus,[0],[0]
"Per terms of the non-disclosure agreement we signed with the firm, we are not able to share the data underlying the analyses reported below.
",3 Data: Corporate Email Corpus,[0],[0]
"1Six pronoun categories (first singular (I), first plural (we), second (you), third singular personal (he, she), third singular impersonal (it, this), and third plural (they)) and five time/certainty categories (past tense, present tense, future tense, certainty, and tentativity).
",3 Data: Corporate Email Corpus,[0],[0]
"We can, however, share the code and dummy test data, both of which can be accessed at http: //github.com/gabedoyle/acl2017.",3 Data: Corporate Email Corpus,[0],[0]
"To assess alignment, we use the Word-Based Hierarchical Alignment Model (WHAM) framework (Doyle and Frank, 2016).",4 Model: An Extended WHAM Framework,[0],[0]
"The core principle of WHAM is that alignment is a change, usually an increase, in the frequency of using a word category in a reply when the word category was used in the preceding message.",4 Model: An Extended WHAM Framework,[0],[0]
"For instance, a reply to the message What will we discuss at the meeting?, is likely to have more instances of future tense than a reply to the message What did we discuss at the meeting?",4 Model: An Extended WHAM Framework,[0],[0]
"Under this definition, alignment is the log-odds shift from the baseline reply frequency, the frequency of the word in a reply when the preceding message did not contain the word.
",4 Model: An Extended WHAM Framework,[0],[0]
"WHAM is a hierarchical generative modeling framework, so it uses information from related observations (e.g., multiple repliers with similar demographics) to improve its robustness on sparse data (Doyle et al., 2016).",4 Model: An Extended WHAM Framework,[0],[0]
"There are two key parameters, shown in Figure 2: ηbase, the log-odds of a given word category c when the preceding message did not contain c, and ηalign, the increase in the log-odds of c when the preceding message did contain c.
A dynamic extension To understand enculturation, we need to track changes in both the alignment and baseline over time.",4 Model: An Extended WHAM Framework,[0],[0]
"We add a month-bymonth change term to WHAM, yielding a piecewise linear model of these factors over the course of an employee’s tenure.",4 Model: An Extended WHAM Framework,[0],[0]
"Each employee’s tenure is broken into two or three segments: their first six months after being hired, their last six months before leaving (if they leave), and the rest of their tenure.2 The linear segments for their alignment are fit as an intercept term ηalign, based at their first month (for the initial period) or their last month (for the final period), and per-month slopes α.",4 Model: An Extended WHAM Framework,[0],[0]
"Baseline segments are fit similarly, with parameters ηbase and β.3",4 Model: An Extended WHAM Framework,[0],[0]
"To visualize the align-
2Within each segment, the employee’s alignment model is similar to that of Yurovsky et al. (2016), who introduced a constant by-month slope parameter to model changes in parent-child alignment during early linguistic development.
3The six month timeframe was chosen as previous research has found it to be a critical period for early enculturation (Bauer et al., 1998).",4 Model: An Extended WHAM Framework,[0],[0]
"Pilot investigations into the change
ment behaviors and the parameter values, we create “sawhorse” plots, with an example in Figure 1.
",4 Model: An Extended WHAM Framework,[0],[0]
"In our present work, we are focused on changes in cultural fit during the transitions into or out of the group, so we collapse observations outside the first/last six months into a stable point estimate, constraining their slopes to be zero.",4 Model: An Extended WHAM Framework,[0],[0]
"This simplification also circumvents the issue of different employees having different middle-period lengths.4
Model structure The graphical model for our instantiation of WHAM is shown in Figure 2.",4 Model: An Extended WHAM Framework,[0],[0]
"For each word category c, WHAM’s generative model represents each reply as a series of tokenby-token independent draws from a binomial distribution.",4 Model: An Extended WHAM Framework,[0],[0]
"The binomial probability µ is dependent on whether the preceding message did (µalign) or did not (µbase) contain a word from category c, and the inferred alignment value is the difference between these probabilities in log-odds space (ηalign).
",4 Model: An Extended WHAM Framework,[0],[0]
"The specific values of these variables depend on three hierarchical features: the word category c, the group g that a given employee falls into, and the time period t (a piece of the piece-wise
in baseline usage over time showed roughly linear changes over the first/last six months, but our linearity assumption may mask interesting variation in the enculturation trajectories.
",4 Model: An Extended WHAM Framework,[0],[0]
"4As shown in Figure 1, the pieces do not need to define a continuous function.",4 Model: An Extended WHAM Framework,[0],[0]
"Alignment behaviors continue to change in the middle of an employee’s tenure (Srivastava et al., forthcoming), so alignment six months in to the job is unlikely to be equal to alignment six months from leaving, or the average alignment over the middle tenure.
linear function: beginning, middle, or end).",4 Model: An Extended WHAM Framework,[0],[0]
"Note that the hierarchical ordering is different for the η chains and the α/β chains; c is above g and t for the η chains, but below them for the α/β chains.",4 Model: An Extended WHAM Framework,[0],[0]
"This is because we expect the static (η) values for a given word category to be relatively consistent across different groups and at different times, but we expect the values to be independent across the different word categories.",4 Model: An Extended WHAM Framework,[0],[0]
"Conversely, we expect that the enculturation trajectories across word categories (α/β) will be similar, while the trajectories may vary substantially across different groups and different times.",4 Model: An Extended WHAM Framework,[0],[0]
"Lastly, the month m in which a reply is written (measured from the start of the time period t) has a linear effect on the η value, as described below.
",4 Model: An Extended WHAM Framework,[0],[0]
"To estimate alignment, we first divide the replies up by group, time period, and calendar month.",4 Model: An Extended WHAM Framework,[0],[0]
We separate the replies into two sets based on whether the preceding message contained the category c (the “alignment” set) or not (the “baseline” set).,4 Model: An Extended WHAM Framework,[0],[0]
"All replies within a set are then aggregated in a single bag-of-words representation, with category token counts Calignc,g,t,m and C base c,g,t,m, and total token counts N basec,g,t,m and N base c,g,t,m comprising the observed variables on the far right of the model.",4 Model: An Extended WHAM Framework,[0],[0]
"Moving from right to left, these counts are assumed to come from binomial draws with prob-
ability µalignc,g,t,m or µ base c,g,t,m. The µ values are then in turn generated from η values in log-odds space by an inverse-logit transform, similar to linear predictors in logistic regression.
",4 Model: An Extended WHAM Framework,[0],[0]
"The ηbase variables are representations of the baseline frequency of a marker in log-odds space, and µbase is simply a conversion of ηbase to probability space, the equivalent of an intercept term in a logistic regression.",4 Model: An Extended WHAM Framework,[0],[0]
"ηalign is an additive value, with µalign = logit−1(ηbase + ηalign), the equivalent of a binary feature coefficient in a logistic regression.",4 Model: An Extended WHAM Framework,[0],[0]
"The specific month’s η variables are calculated as a linear function: ηalignc,g,t,m = η align c,g,t +",4 Model: An Extended WHAM Framework,[0],[0]
"mαc,g,t, and similarly with β for the baseline.",4 Model: An Extended WHAM Framework,[0],[0]
The remainder of the model is a hierarchy of normal distributions that integrate social structure into the analysis.,4 Model: An Extended WHAM Framework,[0],[0]
"In the present work, we have three levels in the hierarchy: category, group, and time period.",4 Model: An Extended WHAM Framework,[0],[0]
"In Analysis 1, employees are grouped by their employment outcome (stay, leave voluntarily, leave involuntarily); in Analyses 2 & 3, where we predict the employment outcomes, each group is a single employee.",4 Model: An Extended WHAM Framework,[0],[0]
The normal distributions that connect these levels have identical standard deviations σ2 = .25.5,4 Model: An Extended WHAM Framework,[0],[0]
"The hierarchies
5The deviation is not a theoretically motivated choice, and was chosen as a good empirical balance between reasonable parameter convergence (improved by smaller σ2) and good model log-probability (improved by larger σ2).
are headed by a normal distribution centered at 0, except for the ηbase hierarchy, which has a Cauchy(0, 2.5) distribution.6
",4 Model: An Extended WHAM Framework,[0],[0]
Message and reply length can affect alignment estimates; the WHAM model was developed in part to reduce this effect.,4 Model: An Extended WHAM Framework,[0],[0]
"As different employees had different email length distributions, we further accounted for length by dividing all replies into five quintile length bins, and treated each bin as separate observations for each employee.",4 Model: An Extended WHAM Framework,[0],[0]
"This design choice adds an additional control factor, but results were qualitatively similar without it.",4 Model: An Extended WHAM Framework,[0],[0]
"All of our analyses are based on parameter estimates from RStan fits of WHAM with 500 iterations over four chains.
",4 Model: An Extended WHAM Framework,[0],[0]
"While previous research on cultural fit has emphasized either its internalization (O’Reilly et al., 1991) or self-regulation (Goldberg et al., 2016) components, our extension to the WHAM framework helps disentangle them by estimating them as separate baseline and alignment trajectories.",4 Model: An Extended WHAM Framework,[0],[0]
"For example, we can distinguish between an archetypal individual who initially aligns to her colleagues and then internalizes this style of communication such that her baseline use also shifts and another archetypal person who aligns to her colleagues but does not change her baseline usage.",4 Model: An Extended WHAM Framework,[0],[0]
"The former exhibits high correspondence between internalization and self-regulation, whereas the latter demonstrates an ability to decouple them.",4 Model: An Extended WHAM Framework,[0],[0]
We perform three analyses on this data.,5 Analyses,[0],[0]
"First, we examine the qualitative behaviors of pronoun alignment and how they map onto employee outcomes in the data.",5 Analyses,[0],[0]
"Second, we show that these qualitative differences in early enculturation are meaningful, with alignment behaviors predicting employment outcome above chance.",5 Analyses,[0],[0]
"Lastly, we consider lexical versus referential levels of alignment and show that predictions are improved under the referential formulation, suggesting that alignment is not limited to low-level wordrepetition effects.
",5 Analyses,[0],[0]
"6As ηbase is the log-odds of each word in a reply being a part of the category c, it is expected to be substantially negative.",5 Analyses,[0],[0]
"For example, second person pronouns (you), are around 2% of the words in replies, approximately −4 in log-odds space.",5 Analyses,[0],[0]
We follow Gelman et al. (2008)’s recommendation of the Cauchy prior as appropriate for parameter estimation in logistic regression.,5 Analyses,[0],[0]
"We begin with descriptive analyses of the behavior of pronouns, which are likely to reflect incorporation into the company.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"In particular, we look at first-person singular (I), first-person plural (we), and second-person pronouns (you).",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"We expect that increases in we usage will occur as the employee is integrated into the group, while I and you usage will decrease, and want to understand whether these changes manifest on baseline usage (i.e., internalization), alignment (i.e., self-regulation), or both.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Design We divided each employee’s emails by calendar month, and separated them into the employee’s first six months, their last six months (if an employee left the company within the observation period), and the middle of their tenure.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees with fewer than twelve months at the company were excluded from this analysis, so that their first and last months did not overlap.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
We fit two WHAM models in this analysis.,5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The first aggregated all employees, regardless of employment outcome, to minimize noise; the second separated them by outcome to analyze cultural fit differences.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Outcome-aggregated model We start with the aggregated behavior of all employees, shown in Figure 3.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"For baselines, we see decreased use of I
and you over the first six months, with we usage increasing over the same period, confirming the expected result that incorporating into the group is accompanied by more inclusive pronoun usage.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Despite the baseline changes, alignment is fairly stable through the first six months.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Alignment on first-person singular and second-person pronouns is lower than first-person plural pronouns, likely due to the fact that I or you have different referents when used by the two conversants, while both conversants could use we to refer to the same group.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
We will consider this referential alignment in more detail in Analysis 3.,5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Since employees with different outcomes have much different experiences over their last six months, we will not discuss them in aggregate, aside from noting the sharp decline in we alignment near the end of the employees’ tenures.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Outcome-separated model Figure 4 shows outcome-specific trajectories, with green lines showing involuntary leavers (i.e., those who are fired or downsized), blue showing voluntary leavers, and orange showing employees who remained at the company through the final month of the data.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The use of I and you is similar to the aggregates in Figure 3, regardless of group.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The last six months of I usage show an interesting difference, where involuntary leavers align more on I but retain a stable baseline while voluntary leavers retain a stable alignment but increase I overall, which is consistent with group separation.
",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"The most compelling result we see here, though, is the changes in we usage by different groups of employees.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees who eventually leave the
company involuntarily show signs of more selfregulation than internalization over the first six months, increasing their alignment while decreasing their baseline use (though they return to more similar levels as other employees later in their tenure).",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"Employees who stay at the company, as well as those who later leave voluntarily, show signs of internalization, increasing their baseline usage to the company average, as well as adapting their alignment levels to the mean.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"This finding suggests that how quickly the employees internalize culturally-standard language use predicts their eventual employment outcome, even if they eventually end up near the average.",5.1 Analysis 1: Dynamic Qualitative Changes,[0],[0]
"This analysis tests the hypothesis that there are meaningful differences in employees’ initial enculturation, captured by alignment behaviors.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
We examine the first six months of communications and attempt to predict whether the employee will leave the company.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We find that, even with a simple classifier, alignment behaviors are predictive of employment outcome.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
Design We fit the WHAM model to only the first six months of email correspondence for all employees who had at least six months of email.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"The model estimated the initial level of baseline use (ηbase) and alignment (ηalign) for each employee, as well as the slope (α, β) for baseline and alignment over those first six months, over all 11 word categories mentioned in Section 3.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We then created logistic regression classifiers, using the parameter estimates to predict whether an employee would leave the company.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
We fit separate classifiers for leaving voluntarily or involuntarily.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Our results show that early alignment behaviors are better at identifying employees who will leave involuntarily than voluntarily, consistent with Srivastava et al.’s (forthcoming) findings that voluntary leavers are similar to stayers until late in their tenure.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We fit separate classifiers using the alignment parameters and the baseline parameters to investigate their relative informativity.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"For each model, we report the area under the curve (AUC).",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This value is estimated from the receiver operating characteristic (ROC) curve, which plots the true positive rate against the false positive rate over different classification thresholds.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
An AUC of 0.5 represents chance performance.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We use balanced, stratified cross-
validation to reduce AUC misestimation due to unbalanced outcome frequencies and high noise (Parker et al., 2007).
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Results The left column of Figure 5 shows the results over 10 runs of 10-fold balanced logistic classifiers with stratified cross-validation in R. The alignment-based classifiers are both above chance at predicting that an employee will leave the company, whether involuntarily or voluntarily.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"The baseline-based classifiers perform worse, especially on voluntary leavers.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This finding is consistent with the idea that voluntary leavers resemble stayers (who form the bulk of the employees) until late in their tenure when their cultural fit declines.
",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"We fit a model using both alignment and baseline parameters, but this model yielded an AUC value below the alignment-only classifier.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
"This suggests that where alignment and baseline behaviors are both predictive, they do not provide substantially different predictive power and lead to overfitting.",5.2 Analysis 2: Predicting Outcomes,[0],[0]
A more sophisticated classifier may overcome these challenges; our goal here was not to achieve maximal classification performance but to test whether alignment provided any useful information about employment outcomes.,5.2 Analysis 2: Predicting Outcomes,[0],[0]
"Our final analysis investigates the nature of linguistic alignment: specifically, whether there is an effect of referential alignment beyond that of the more commonly used lexical alignment.
",5.3 Analysis 3: Types of Alignment,[0],[0]
Testing this hypothesis requires a small change to the alignment calculations.,5.3 Analysis 3: Types of Alignment,[0],[0]
"Lexical alignment is based on the conditional probability of the replier using a word category c given that the preceding message used that same category c. For referential alignment, we examine the conditional probability of the replier using a word category cj given that the preceding message used the category ci, where ci and cj are likely to be referentially linked.",5.3 Analysis 3: Types of Alignment,[0],[0]
"We also consider cases where ci is likely to transition to cj throughout the course of the conversation, such as present tense verbs turning into past tense as the event being described recedes into the past.",5.3 Analysis 3: Types of Alignment,[0],[0]
"The pairs of categories that are likely to be referentially or transitionally linked are: (you, I); (we, I); (you, we); (past, present); (present, future); and (certainty, tentativity).",5.3 Analysis 3: Types of Alignment,[0],[0]
"We include both directions of these pairs, so this provides approximately the same number of predictor variables for both situa-
tions to maximize comparability (12 for the referential alignments, 11 for the lexical).",5.3 Analysis 3: Types of Alignment,[0],[0]
"This modification does not change the structure of the WHAM model, but rather changes its C and N counts by reclassifying replies between the baseline or alignment pathways.
",5.3 Analysis 3: Types of Alignment,[0],[0]
Results Figure 5 plots the differences in predictive model performance using lexical versus referential alignment parameters.,5.3 Analysis 3: Types of Alignment,[0],[0]
We find that the semantic parameters provide more accurate classification than the lexical both for voluntarily and involuntarily-leaving employees.,5.3 Analysis 3: Types of Alignment,[0],[0]
"This suggests that while previous work looking at lexical alignment successfully captures social structure, referential alignment may reflect a deeper and more accurate representation of the social structure.",5.3 Analysis 3: Types of Alignment,[0],[0]
"It is unclear if this behavior holds in less formal situations or with weaker organizational structure and shared goals, but these results suggest that the traditional alignment approach of only measuring lexical alignment should be augmented with referential alignment measures for a more complete analysis.",5.3 Analysis 3: Types of Alignment,[0],[0]
"A key finding from this work is that pronoun usage behaviors in employees’ email communication are consistent with social integration into the group; employees use “I” pronouns less and
“we” pronouns more as they integrate.",6 Discussion,[0],[0]
"Furthermore, we see the importance of using an alignment measure such as WHAM for distinguishing the base rate and alignment usage of words.",6 Discussion,[0],[0]
"Employees who leave the company involuntarily show increased “we” usage through greater alignment, using “we” more when prompted by a colleague, but introducing it less of their own accord.",6 Discussion,[0],[0]
"This suggests that these employees do not feel fully integrated into the group, although they are willing to identify as a part of it when a more fully-integrated group member includes them, corresponding to self-regularization over internalization.",6 Discussion,[0],[0]
"The fact that these alignment measures alone, without any job productivity or performance metrics, have some predictive capability for employees’ leaving the company suggests the potential for support or intervention programs to help highperforming but poorly-integrated employees integrate into the company better.
",6 Discussion,[0],[0]
"More generally, the prominence of pronominally-driven communication changes suggest that alignment analyses can provide insight into a range of social integration settings.",6 Discussion,[0],[0]
"This may be especially helpful in cases where there is great pressure to integrate smoothly, and people would be likely to adopt a self-regulating approach even if they do not internalize their group membership.",6 Discussion,[0],[0]
"Such settings not only include the high-stakes situation of keeping one’s job, but of transitioning from high school to college or moving to a new country or region.",6 Discussion,[0],[0]
Maximizing the chances for new members to become comfortable within a group is critical both for spreading useful aspects of the group’s existing culture to new members and for integrating new ideas from the new members’ knowledge and practices.,6 Discussion,[0],[0]
Alignment-based approaches can be a useful tool in separating effective interventions that cause internalization of the group dynamics from those that lead to more superficial self-regularization changes.,6 Discussion,[0],[0]
This paper described an effort to use directed linguistic alignment as a measure of cultural fit within an organization.,7 Conclusions,[0],[0]
"We adapted a hierarchical alignment model from previous work to estimate fit within corporate email communications, focusing on changes in language during employees’ entry to and exit from the company.",7 Conclusions,[0],[0]
"Our results
showed substantial changes in the use of pronouns, with pronoun patterns varying by employees’ outcomes within the company.",7 Conclusions,[0],[0]
The use of the firstperson plural “we” during an employee’s first six months is particularly instructive.,7 Conclusions,[0],[0]
"Whereas stayers exhibited increased baseline use, indicating internalization, those eventually departing involuntarily were on the one hand decreasingly likely to introduce “we” into conversation, but increasingly responsive to interlocutors’ use of the pronoun.",7 Conclusions,[0],[0]
"While not internalizing a shared identity with their peers, involuntarily departed employees were overly self-regulating in response to its invocation by others.
",7 Conclusions,[0],[0]
"Quantitatively, rates of usage and alignment in the first six months of employment carried information about whether employees left involuntarily, pointing towards fit within the company culture early on as an indicator of eventual employment outcomes.",7 Conclusions,[0],[0]
"Finally, we saw ways in which the application of alignment to cultural fit might help to refine ideas about alignment itself: preliminary analysis suggested that referential, rather than lexical, alignment was more predictive of employment outcomes.",7 Conclusions,[0],[0]
"More broadly, these results suggest ways that quantitative methods can be used to make precise application of concepts like “cultural fit” at scale.",7 Conclusions,[0],[0]
"This work was supported by NSF Grant #1456077; The Garwood Center for Corporate Innovation at the Haas School of Business, University of California, Berkeley; the Stanford Data Science Initiative; and the Stanford Graduate School of Business.",8 Acknowledgments,[0],[0]
Cultural fit is widely believed to affect the success of individuals and the groups to which they belong.,abstractText,[0],[0]
"Yet it remains an elusive, poorly measured construct.",abstractText,[0],[0]
Recent research draws on computational linguistics to measure cultural fit but overlooks asymmetries in cultural adaptation.,abstractText,[0],[0]
"By contrast, we develop a directed, dynamic measure of cultural fit based on linguistic alignment, which estimates the influence of one person’s word use on another’s and distinguishes between two enculturation mechanisms: internalization and selfregulation.",abstractText,[0],[0]
"We use this measure to trace employees’ enculturation trajectories over a large, multi-year corpus of corporate emails and find that patterns of alignment in the first six months of employment are predictive of individuals downstream outcomes, especially involuntary exit.",abstractText,[0],[0]
Further predictive analyses suggest referential alignment plays an overlooked role in linguistic alignment.,abstractText,[0],[0]
Alignment at Work: Using Language to Distinguish the Internalization and Self-Regulation Components of Cultural Fit in Organizations,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1165–1174, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"In instruction-following tasks, an agent executes a sequence of actions in a real or simulated environment, in response to a sequence of natural language commands.",1 Introduction,[0],[0]
Examples include giving navigational directions to robots and providing hints to automated game-playing agents.,1 Introduction,[0],[0]
Plans specified with natural language exhibit compositionality both at the level of individual actions and at the overall sequence level.,1 Introduction,[0],[0]
"This paper describes a framework for learning to follow instructions by leveraging structure at both levels.
",1 Introduction,[0],[0]
"Our primary contribution is a new, alignmentbased approach to grounded compositional semantics.",1 Introduction,[0],[0]
"Building on related logical approaches (Reddy et al., 2014; Pourdamghani et al., 2014), we recast instruction following as a pair of nested, structured alignment problems.",1 Introduction,[0],[0]
"Given instructions and a candidate plan, the model infers a sequenceto-sequence alignment between sentences and
atomic actions.",1 Introduction,[0],[0]
"Within each sentence–action pair, the model infers a structure-to-structure alignment between the syntax of the sentence and a graphbased representation of the action.
",1 Introduction,[0],[0]
"At a high level, our agent is a block-structured, graph-valued conditional random field, with alignment potentials to relate instructions to actions and transition potentials to encode the environment model (Figure 3).",1 Introduction,[0],[0]
"Explicitly modeling sequenceto-sequence alignments between text and actions allows flexible reasoning about action sequences, enabling the agent to determine which actions are specified (perhaps redundantly) by text, and which actions must be performed automatically (in order to satisfy pragmatic constraints on interpretation).",1 Introduction,[0],[0]
"Treating instruction following as a sequence prediction problem, rather than a series of independent decisions (Branavan et al., 2009; Artzi and Zettlemoyer, 2013), makes it possible to use general-purpose planning machinery, greatly increasing inferential power.
",1 Introduction,[0],[0]
"The fragment of semantics necessary to complete most instruction-following tasks is essentially predicate–argument structure, with limited influence from quantification and scoping.",1 Introduction,[0],[0]
Thus the problem of sentence interpretation can reasonably be modeled as one of finding an alignment between language and the environment it describes.,1 Introduction,[0],[0]
We allow this structure-to-structure alignment— an “overlay” of language onto the world—to be mediated by linguistic structure (in the form of dependency parses) and structured perception (in what we term grounding graphs).,1 Introduction,[0],[0]
"Our model thereby reasons directly about the relationship between language and observations of the environment, without the need for an intermediate logical representation of sentence meaning.",1 Introduction,[0],[0]
"This, in turn, makes it possible to incorporate flexible feature representations that have been difficult to integrate with previous work in semantic parsing.
",1 Introduction,[0],[0]
"We apply our approach to three established
1165
2 1 3
instruction-following benchmarks: the map reading task of Vogel and Jurafsky (2010), the maze navigation task of MacMahon et al. (2006), and the puzzle solving task of Branavan et al. (2009).",1 Introduction,[0],[0]
An example from each is shown in Figure 1.,1 Introduction,[0],[0]
"These benchmarks exhibit a range of qualitative properties—both in the length and complexity of their plans, and in the quantity and quality of accompanying language.",1 Introduction,[0],[0]
"Each task has been studied in isolation, but we are unaware of any published approaches capable of robustly handling all three.",1 Introduction,[0],[0]
"Our general model outperforms strong, task-specific baselines in each case, achieving relative error reductions of 15–20% over several state-of-the-art results.",1 Introduction,[0],[0]
Experiments demonstrate the importance of our contributions in both compositional semantics and search over plans.,1 Introduction,[0],[0]
We have released all code for this project at github.com/jacobandreas/instructions.,1 Introduction,[0],[0]
"Existing work on instruction following can be roughly divided into two families: semantic parsers and linear policy estimators.
",2 Related work,[0],[0]
"Semantic parsers Parser-based approaches (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Kim and Mooney, 2013) map from text into a formal language representing commands.",2 Related work,[0],[0]
"These take familiar structured prediction models for semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), and train them with task-provided supervision.",2 Related work,[0],[0]
"Instead of attempting to match the structure of a manually-annotated semantic parse, semantic parsers for instruction following are trained to maximize a reward signal
provided by black-box execution of the predicted command in the environment.",2 Related work,[0],[0]
"(It is possible to think of response-based learning for question answering (Liang et al., 2013) as a special case.)
",2 Related work,[0],[0]
"This approach uses a well-studied mechanism for compositional interpretation of language, but is subject to certain limitations.",2 Related work,[0],[0]
"Because the environment is manipulated only through black-box execution of the completed semantic parse, there is no way to incorporate current or future environment state into the scoring function.",2 Related work,[0],[0]
It is also in general necessary to hand-engineer a task-specific formal language for describing agent behavior.,2 Related work,[0],[0]
"Thus it is extremely difficult to work with environments that cannot be modeled with a fixed inventory of predicates (e.g. those involving novel strings or arbitrary real quantities).
",2 Related work,[0],[0]
Much of contemporary work in this family is evaluated on the maze navigation task introduced by MacMahon et al. (2006).,2 Related work,[0],[0]
"Dukes (2013) also introduced a “blocks world” task for situated parsing of spatial robot commands.
",2 Related work,[0],[0]
"Linear policy estimators An alternative family of approaches is based on learning a policy over primitive actions directly (Branavan et al., 2009; Vogel and Jurafsky, 2010).1 Policybased approaches instantiate a Markov decision process representing the action domain, and apply standard supervised or reinforcement-learning approaches to learn a function for greedily selecting among actions.",2 Related work,[0],[0]
"In linear policy approximators, natural language instructions are incorporated directly into state observations, and reading order
1This is distinct from semantic parsers in which greedy inference happens to have an interpretation as a policy (Vlachos and Clark, 2014).
becomes part of the action selection process.",2 Related work,[0],[0]
"Almost all existing policy-learning approaches make use of an unstructured parameterization, with a single (flat) feature vector representing all text and observations.",2 Related work,[0],[0]
Such approaches are thus restricted to problems that are simple enough (and have small enough action spaces) to be effectively characterized in this fashion.,2 Related work,[0],[0]
"While there is a great deal of flexibility in the choice of feature function (which is free to inspect the current and future state of the environment, the whole instruction sequence, etc.), standard linear policy estimators have no way to model compositionality in language or actions.
",2 Related work,[0],[0]
"Agents in this family have been evaluated on a variety of tasks, including map reading (Anderson et al., 1991) and gameplay (Branavan et al., 2009).
",2 Related work,[0],[0]
"Though both families address the same class of instruction-following problems, they have been applied to a totally disjoint set of tasks.",2 Related work,[0],[0]
"It should be emphasized that there is nothing inherent to policy learning that prevents the use of compositional structure, and nothing inherent to general compositional models that prevents more complicated dependence on environment state.",2 Related work,[0],[0]
"Indeed, previous work (Branavan et al., 2011; Narasimhan et al., 2015) uses aspects of both to solve a different class of gameplay problems.",2 Related work,[0],[0]
"In some sense, our goal in this paper is simply to combine the strengths of semantic parsers and linear policy estimators for fully general instruction following.",2 Related work,[0],[0]
"As we shall see, however, this requires changes to many aspects of representation, learning and inference.",2 Related work,[0],[0]
We wish to train a model capable of following commands in a simulated environment.,3 Representations,[0],[0]
"We do so by presenting the model with a sequence of training pairs (x,y), where each x is a sequence of natural language instructions (x1, x2, . . .",3 Representations,[0],[0]
", xm), e.g.:
(Go down the yellow hall., Turn left., . . . )
",3 Representations,[0],[0]
"and each y is a demonstrated action sequence (y1, y2, . . .",3 Representations,[0],[0]
", yn), e.g.:
(rotate(90), move(2), . . . )
",3 Representations,[0],[0]
"Given a start state, y can equivalently be characterized by a sequence of (state, action, state)
triples resulting from execution of the environment model.",3 Representations,[0],[0]
An example instruction is shown in Figure 2a.,3 Representations,[0],[0]
"An example action, situated in the environment where it occurs, is shown in Figure 2e.
",3 Representations,[0],[0]
Our model performs compositional interpretation of instructions by leveraging existing structure inherent in both text and actions.,3 Representations,[0],[0]
"Thus we interpret xi and yj not as raw strings and primitive actions, but rather as structured objects.
",3 Representations,[0],[0]
"Linguistic structure We assume access to a pretrained parser, and in particular that each of the instructions xi is represented by a tree-structured dependency parse.",3 Representations,[0],[0]
"An example is shown in Figure 2b.
",3 Representations,[0],[0]
"Action structure By analogy to the representation of instructions as parse trees, we assume that each (state, action, state) triple (provided by the environment model) can be characterized by a grounding graph.",3 Representations,[0],[0]
The structure and content of this representation is task-specific.,3 Representations,[0],[0]
"An example grounding graph for the maze navigation task is
shown in Figure 2d.",3 Representations,[0],[0]
"The example contains a node corresponding to the primitive action move(2) (in the upper left), and several nodes corresponding to locations in the environment that are visible after the action is performed.
",3 Representations,[0],[0]
"Each node in the graph (and, though not depicted, each edge) is decorated with a list of features.",3 Representations,[0],[0]
"These features might be simple indicators (e.g. whether the primitive action performed was move or rotate), real values (the distance traveled) or even string-valued (English-language names of visible landmarks, if available in the environment description).",3 Representations,[0],[0]
"Formally, a grounding graph consists of a tuple (V,E,L, fV , fE), with
– V a set of vertices
– E ∈ V × V a set of (directed) edges – L a space of labels (numbers, strings, etc.) – fV : V → 2L a vertex feature function – fE : E → 2L an edge feature function In this paper we have tried to remain agnostic to details of graph construction.",3 Representations,[0],[0]
Our goal with the grounding graph framework is simply to accommodate a wider range of modeling decisions than allowed by existing formalisms.,3 Representations,[0],[0]
"Graphs might be constructed directly, given access to a structured virtual environment (as in all experiments in this paper), or alternatively from outputs of a perceptual system.",3 Representations,[0],[0]
"For our experiments, we have remained as close as possible to task representations described in the existing literature.",3 Representations,[0],[0]
"Details for each task can be found in the accompanying software package.
",3 Representations,[0],[0]
"Graph-based representations are extremely common in formal semantics (Jones et al., 2012; Reddy et al., 2014), and the version presented here corresponds to a simple generalization of familiar formal methods.",3 Representations,[0],[0]
"Indeed, if L is the set of all atomic entities and relations, fV returns a unique label for every v ∈ V , and fE always returns a vector with one active feature, we recover the existentially-quantified portion of first order logic exactly, and in this form can implement large parts of classical neo-Davidsonian semantics (Parsons, 1990) using grounding graphs.
",3 Representations,[0],[0]
"Crucially, with an appropriate choice of L this formalism also makes it possible to go beyond settheoretic relations, and incorporate string-valued features (like names of entities and landmarks) and real-valued features (like colors and positions) as well.
",3 Representations,[0],[0]
Lexical semantics We must eventually combine features provided by parse trees with features provided by the environment.,3 Representations,[0],[0]
"Examples here might include simple conjunctions (word=yellow ∧ rgb=(0.5, 0.5, 0.0)) or more complicated computations like edit distance between landmark names and lexical items.",3 Representations,[0],[0]
"Features of the latter kind make it possible to behave correctly in environments containing novel strings or other features unseen during training.
",3 Representations,[0],[0]
"This aspect of the syntax–semantics interface has been troublesome for some logic-based approaches: while past work has used related machinery for selecting lexicon entries (Berant and Liang, 2014) or for rewriting logical forms (Kwiatkowski et al., 2013), the relationship between text and the environment has ultimately been mediated by a discrete (and indeed finite) inventory of predicates.",3 Representations,[0],[0]
"Several recent papers have investigated simple grounded models with realvalued output spaces (Andreas and Klein, 2014; McMahan and Stone, 2015), but we are unaware of any fully compositional system in recent literature that can incorporate observations of these kinds.
",3 Representations,[0],[0]
"Formally, we assume access to a joining feature function φ : (2L × 2L)→ Rd.",3 Representations,[0],[0]
"As with grounding graphs, our goal is to make the general framework as flexible as possible, and for individual experiments have chosen φ to emulate modeling decisions from previous work.",3 Representations,[0],[0]
"As noted in the introduction, we approach instruction following as a sequence prediction problem.",4 Model,[0],[0]
Thus we must place a distribution over sequences of actions conditioned on instructions.,4 Model,[0],[0]
"We decompose the problem into two components, describing interlocking models of “path structure” and “action structure”.",4 Model,[0],[0]
"Path structure captures how sequences of instructions give rise to sequences of actions, while action structure captures the compositional relationship between individual utterances and the actions they specify.
",4 Model,[0],[0]
"Path structure: aligning utterances to actions
The high-level path structure in the model is depicted in Figure 3.",4 Model,[0],[0]
"Our goal here is to permit both under- and over-specification of plans, and to expose a planning framework which allows plans to be computed with lookahead (i.e. non-greedily).
",4 Model,[0],[0]
These goals are achieved by introducing a sequence of latent alignments between instructions and actions.,4 Model,[0],[0]
Consider the multi-step example in Figure 1b.,4 Model,[0],[0]
"If the first instruction go down the yellow hall were interpreted immediately, we would have a presupposition failure—the agent is facing a wall, and cannot move forward at all.",4 Model,[0],[0]
"Thus an implicit rotate action, unspecified by text, must be performed before any explicit instructions can be followed.
",4 Model,[0],[0]
"To model this, we take the probability of a (text, plan, alignment) triple to be log-proportional to the sum of two quantities:
1.",4 Model,[0],[0]
"a path-only score ψ(n; θ) + ∑
j ψ(yj ; θ)
2.",4 Model,[0],[0]
"a path-and-text score, itself the sum of all pair scores ψ(xi, yj ; θ) licensed by the alignment
(1) captures our desire for pragmatic constraints on interpretation, and provides a means of encoding the inherent plausibility of paths.",4 Model,[0],[0]
"We take ψ(n; θ) and ψ(y; θ) to be linear functions of θ. (2) provides context-dependent interpretation of text by means of the structured scoring function ψ(x, y; θ), described in the next section.
",4 Model,[0],[0]
"Formally, we associate with each instruction xi a sequence-to-sequence alignment variable ai ∈ 1 . . .",4 Model,[0],[0]
n,4 Model,[0],[0]
"(recalling that n is the number of actions).
",4 Model,[0],[0]
"Then we have2 p(y,a|x; θ) ∝ exp { ψ(n) + n∑ j=1 ψ(yj)
+",4 Model,[0],[0]
"m∑ i=1 n∑ j=1 1[aj = i] ψ(xi, yj) } (1)
We additionally place a monotonicity constraint on the alignment variables.",4 Model,[0],[0]
"This model is globally normalized, and for a fixed alignment is equivalent to a linear-chain CRF.",4 Model,[0],[0]
"In this sense it is analogous to IBM Model I (Brown et al., 1993), with the structured potentials ψ(xi, yj) taking the place of lexical translation probabilities.",4 Model,[0],[0]
"While alignment models from machine translation have previously been used to align words to fragments of semantic parses (Wong and Mooney, 2006; Pourdamghani et al., 2014), we are unaware of such models being used to align entire instruction sequences to demonstrations.
",4 Model,[0],[0]
"Action structure: aligning words to percepts Intuitively, this scoring function ψ(x, y) should capture how well a given utterance describes an action.",4 Model,[0],[0]
"If neither the utterances nor the actions had structure (i.e. both could be represented with simple bags of features), we would recover something analogous to the conventional policy-learning approach.",4 Model,[0],[0]
"As structure is essential for some of our tasks, ψ(x, y) must instead fill the role of a semantic parser in a conventional compositional model.
",4 Model,[0],[0]
"Our choice of ψ(x, y) is driven by the following fundamental assumptions: Syntactic relations approximately represent semantic relations.",4 Model,[0],[0]
Syntactic proximity implies relational proximity.,4 Model,[0],[0]
"In this view, there is an additional hidden structure-tostructure alignment between the grounding graph and the parsed text describing it.",4 Model,[0],[0]
"3 Words line up with nodes, and dependencies line up with relations.",4 Model,[0],[0]
"Visualizations are shown in Figure 2c and the zoomed-in portion of Figure 3.
",4 Model,[0],[0]
"As with the top-level alignment variables, this approach can viewed as a simple relaxation of a familiar model.",4 Model,[0],[0]
"CCG-based parsers assume that syntactic type strictly determines semantic type,
2Here and the remainder of this paper, we suppress the dependence of the various potentials on θ in the interest of readability.
",4 Model,[0],[0]
3It is formally possible to regard the sequence-tosequence and structure-to-structure alignments as a single (structured) random variable.,4 Model,[0],[0]
"However, the two kinds of alignments are treated differently for purposes of inference, so it is useful to maintain a notational distinction.
and that each lexical item is associated with a small set of functional forms.",4 Model,[0],[0]
"Here we simply allow all words to license all predicates, multiple words to specify the same predicate, and some edges to be skipped.",4 Model,[0],[0]
We instead rely on a scoring function to impose soft versions of the hard constraints typically provided by a grammar.,4 Model,[0],[0]
"Related models have previously been used for question answering (Reddy et al., 2014; Pasupat and Liang, 2015).
",4 Model,[0],[0]
For the moment let us introduce variables b to denote these structure-to-structure alignments.,4 Model,[0],[0]
"(As will be seen in the following section, it is straightforward to marginalize over all choices of b.",4 Model,[0],[0]
"Thus the structure-to-structure alignments are never explicitly instantiated during inference, and do not appear in the final form of ψ(x, y).)",4 Model,[0],[0]
"For a fixed alignment, we define ψ(x, y, b) according to a recurrence relation.",4 Model,[0],[0]
"Let xi be the ith word of the sentence, and let yj be the jth node in the action graph (under some topological ordering).",4 Model,[0],[0]
Let c(i) and c(j) give the indices of the dependents of xi and children of yj respectively.,4 Model,[0],[0]
"Finally, let xik and yjl denote the associated dependency type or relation.",4 Model,[0],[0]
"Define a “descendant” function:
d(i, j) = { (k, l) : k ∈ c(i), l ∈ c(j), (k, l) ∈ b}
Then, ψ(xi, yj , b) = exp { θ>φ(xi, yj)
+ ∑
(k,l)∈d(x,y)
",4 Model,[0],[0]
"[ θ>φ ( xik, yjl ) · ψ(xk, yl, b)]}
This is just an unnormalized synchronous derivation between x and y—at any aligned (node, word) pair, the score for the entire derivation is the score produced by combining that word and node, times the scores at all the aligned descendants.",4 Model,[0],[0]
"Observe that as long as there are no cycles in the dependency parse, it is perfectly acceptable for the relation graph to contain cycles and even self-loops— the recurrence still bottoms out appropriately.",4 Model,[0],[0]
"Given a sequence of training pairs (x,y), we wish to find a parameter setting that maximizes p(y|x; θ).",5 Learning and inference,[0],[0]
"If there were no latent alignments a or b, this would simply involve minimization of a convex objective.",5 Learning and inference,[0],[0]
The presence of latent variables complicates things.,5 Learning and inference,[0],[0]
"Ideally, we would like
Algorithm 1 Computing structure-to-structure alignments
xi are words in reverse topological order yj are grounding graph nodes (root last) chart is an m× n array for i = 1 to |x| do
for j = 1 to |y| do score← exp{θ>φ(xi, yj)} for (k, l) ∈ d(i, j) do
s←∑l∈c(j) [ exp{θ>φ(xik, yjl)} · chart[k, l]
]",5 Learning and inference,[0],[0]
"score← score · s
end for chart[i, j]← score
end for end for return chart[n,m]
to sum over the latent variables, but that sum is intractable.",5 Learning and inference,[0],[0]
"Instead we make a series of variational approximations: first we replace the sum with a maximization, then perform iterated conditional modes, alternating between maximization of the conditional probability of a and θ.",5 Learning and inference,[0],[0]
"We begin by initializing θ randomly.
",5 Learning and inference,[0],[0]
"As noted in the preceding section, the variable b does not appear in these equations.",5 Learning and inference,[0],[0]
"Conditioned on a, the sum over structure-to-structure ψ(x, y) = ∑ b ψ(x, y, b) can be performed exactly using a simple dynamic program which runs in time O(|x||y|)",5 Learning and inference,[0],[0]
"(assuming out-degree bounded by a constant, and with |x| and |y| the number of words and graph nodes respectively).",5 Learning and inference,[0],[0]
"This is Algorithm 1.
",5 Learning and inference,[0],[0]
"In our experiments, θ is optimized using LBFGS (Liu and Nocedal, 1989).",5 Learning and inference,[0],[0]
"Calculation of the gradient with respect to θ requires computation of a normalizing constant involving the sum over p(x,y′,a) for all y′.",5 Learning and inference,[0],[0]
"While in principle the normalizing constant can be computed using the forward algorithm, in practice the state spaces under consideration are so large that even this is intractable.",5 Learning and inference,[0],[0]
"Thus we make an additional approximation, constructing a set Ỹ of alternative actions and taking p(y,a|x)",5 Learning and inference,[0],[0]
≈ n∑ j=1 exp { ψ(yj)+ ∑m i=1,5 Learning and inference,[0],[0]
"1[ai=j]ψ(xi,yi) } ∑ ỹ∈Ỹ exp { ψ(ỹ)+ ∑m i=1",5 Learning and inference,[0],[0]
"1[ai=j]ψ(xi,ỹ) }
Ỹ is constructed by sampling alternative actions from the environment model.",5 Learning and inference,[0],[0]
"Meanwhile, maximization of a can be performed exactly using the Viterbi algorithm, without computation of normalizers.
",5 Learning and inference,[0],[0]
Inference at test time involves a slightly different pair of optimization problems.,5 Learning and inference,[0],[0]
"We again perform iterated conditional modes, here on the alignments a and the unknown output path y. Maximization of a is accomplished with the Viterbi algorithm, exactly as before; maximization of y also uses the Viterbi algorithm, or a beam search when this is computationally infeasible.",5 Learning and inference,[0],[0]
"If bounds on path length are known, it is straightforward to adapt these dynamic programs to efficiently consider paths of all lengths.",5 Learning and inference,[0],[0]
"As one of the main advantages of this approach is its generality, we evaluate on several different benchmark tasks for instruction following.",6 Evaluation,[0],[0]
These exhibit great diversity in both environment structure and language use.,6 Evaluation,[0],[0]
We compare our full system to recent state-of-the-art approaches to each task.,6 Evaluation,[0],[0]
"In the introduction, we highlighted two core aspects of our approach to semantics: compositionality (by way of grounding graphs and structure-to-structure alignments) and planning (by way of inference with lookahead and sequence-to-sequence alignments).",6 Evaluation,[0],[0]
"To evaluate these, we additionally present a pair of ablation experiments: no grounding graphs (an agent with an unstructured representation of environment state), and no planning (a reflex agent with no lookahead).
",6 Evaluation,[0],[0]
"Map reading Our first application is the map navigation task established by Vogel and Jurafsky (2010), based on data collected for a psychological experiment by Anderson et al. (1991) (Figure 1a).",6 Evaluation,[0],[0]
"Each training datum consists of a map with a designated starting position, and a collection of landmarks, each labeled with a spatial coordinate and a string name.",6 Evaluation,[0],[0]
"Names are not always unique, and landmarks in the test set are never observed during training.",6 Evaluation,[0],[0]
This map is accompanied by a set of instructions specifying a path from the starting position to some (unlabeled) destination point.,6 Evaluation,[0],[0]
"These instruction sets are informal and redundant, involving as many as a hundred utterances.",6 Evaluation,[0],[0]
"They are transcribed from spoken text, so grammatical errors, disfluencies, etc. are common.",6 Evaluation,[0],[0]
"This is a
prime example of a domain that does not lend itself to logical representation—grammars may be too rigid, and previously-unseen landmarks and real-valued positions are handled more easily with feature machinery than predicate logic.
",6 Evaluation,[0],[0]
"The map task was previously studied by Vogel and Jurafsky (2010), who implemented SARSA with a simple set of features.",6 Evaluation,[0],[0]
"By combining these features with our alignment model and search procedure, we achieve state-of-the-art results on this task by a substantial margin (Table 1).
",6 Evaluation,[0],[0]
Some learned feature values are shown in Table 2.,6 Evaluation,[0],[0]
The model correctly infers cardinal directions (the example shows the preferred side of a destination landmark modified by the word top).,6 Evaluation,[0],[0]
"Like Vogel et al., we see support for both allocentric references (you are on top of the hill) and egocentric references (the hill is on top of you).",6 Evaluation,[0],[0]
"We can also see pragmatics at work: the model learns useful text-independent constraints—in this case, that near destinations should be preferred to far ones.
",6 Evaluation,[0],[0]
Maze navigation The next application we consider is the maze navigation task of MacMahon et al. (2006) (Figure 1b).,6 Evaluation,[0],[0]
"Here, a virtual agent is sit-
uated in a maze (whose hallways are distinguished with various wallpapers, carpets, and the presence of a small set of standard objects), and again given instructions for getting from one point to another.",6 Evaluation,[0],[0]
"This task has been the subject of focused attention in semantic parsing for several years, resulting in a variety of sophisticated approaches.
",6 Evaluation,[0],[0]
"Despite superficial similarity to the previous navigation task, the language and plans required for this task are quite different.",6 Evaluation,[0],[0]
"The proportion of instructions to actions is much higher (so redundancy much lower), and the interpretation of language is highly compositional.
",6 Evaluation,[0],[0]
"As can be seen in Table 3, we outperform a number of systems purpose-built for this navigation task.",6 Evaluation,[0],[0]
"We also outperform both variants of our system, most conspicuously the variant without grounding graphs.",6 Evaluation,[0],[0]
This highlights the importance of compositional structure.,6 Evaluation,[0],[0]
"Recent work by Kim and Mooney (2013) and Artzi et al. (2014) has achieved better results; these systems make use of techniques and resources (respectively, discriminative reranking and a seed lexicon of handannotated logical forms) that are largely orthogonal to the ones used here, and might be applied to improve our own results as well.
",6 Evaluation,[0],[0]
Puzzle solving The last task we consider is the Crossblock task studied by Branavan et al. (2009) (Figure 1c).,6 Evaluation,[0],[0]
"Here, again, natural language is used to specify a sequence of actions, in this case the solution to a simple game.",6 Evaluation,[0],[0]
"The environment is simple enough to be captured with a flat feature
4We specifically targeted the single-sentence version of this evaluation, as an alternative full-sequence evaluation does not align precisely with our data condition.
",6 Evaluation,[0],[0]
"representation, so there is no distinction between the full model and the variant without grounding graphs.
",6 Evaluation,[0],[0]
"Unlike the other tasks we consider, Crossblock is distinguished by a challenging associated search problem.",6 Evaluation,[0],[0]
Here it is nontrivial to find any sequence that eliminates all the blocks (the goal of the puzzle).,6 Evaluation,[0],[0]
"Thus this example allows us measure the effectiveness of our search procedure.
",6 Evaluation,[0],[0]
Results are shown in Table 4.,6 Evaluation,[0],[0]
"As can be seen, our model achieves state-of-the-art performance on this task when attempting to match the humanspecified plan exactly.",6 Evaluation,[0],[0]
"If we are purely concerned with task completion (i.e. solving the puzzle, perhaps not with the exact set of moves specified in the instructions) we can measure this directly.",6 Evaluation,[0],[0]
"Here, too, we substantially outperform a no-text baseline.",6 Evaluation,[0],[0]
"Thus it can be seen that text induces a useful heuristic, allowing the model to solve a considerable fraction of problem instances not solved by naı̈ve beam search.
",6 Evaluation,[0],[0]
"The problem of inducing planning heuristics from side information like text is an important one in its own right, and future work might focus specifically on coupling our system with a more sophisticated planner.",6 Evaluation,[0],[0]
"Even at present, the results in this section demonstrate the importance of lookahead and high-level reasoning in instruction following.",6 Evaluation,[0],[0]
"We have described a new alignment-based compositional model for following sequences of natural language instructions, and demonstrated the effectiveness of this model on a variety of tasks.",7 Conclusion,[0],[0]
"A fully general solution to the problem of contextual interpretation must address a wide range of wellstudied problems, but the work we have described
here provides modular interfaces for the study of a number of fundamental linguistic issues from a machine learning perspective.",7 Conclusion,[0],[0]
"These include:
Pragmatics How do we respond to presupposition failures, and choose among possible interpretations of an instruction disambiguated only by context?",7 Conclusion,[0],[0]
"The mechanism provided by the sequence-prediction architecture we have described provides a simple answer to this question, and our experimental results demonstrate that the learned pragmatics aid interpretation of instructions in a number of concrete ways: ambiguous references are resolved by proximity in the map reading task, missing steps are inferred from an environment model in the maze navigation task, and vague hints are turned into real plans by knowledge of the rules in Crossblock.",7 Conclusion,[0],[0]
"A more comprehensive solution might explicitly describe the process by which instruction-givers’ own beliefs (expressed as distributions over sequences) give rise to instructions.
",7 Conclusion,[0],[0]
Compositional semantics,7 Conclusion,[0],[0]
"The graph alignment model of semantics presented here is an expressive and computationally efficient generalization of classical logical techniques to accommodate environments like the map task, or those explored in our previous work (Andreas and Klein, 2014).",7 Conclusion,[0],[0]
"More broadly, our model provides a compositional approach to semantics that does not require an explicit formal language for encoding sentence meaning.",7 Conclusion,[0],[0]
"Future work might extend this approach to tasks like question answering, where logicbased approaches have been successful.
",7 Conclusion,[0],[0]
Our primary goal in this paper has been to explore methods for integrating compositional semantics and the pragmatic context provided by sequential structures.,7 Conclusion,[0],[0]
"While there is a great deal of work left to do, we find it encouraging that this general approach results in substantial gains across multiple tasks and contexts.",7 Conclusion,[0],[0]
The authors would like to thank S.R.K. Branavan for assistance with the Crossblock evaluation.,Acknowledgments,[0],[0]
The first author is supported by a National Science Foundation Graduate Fellowship.,Acknowledgments,[0],[0]
This paper describes an alignment-based model for interpreting natural language instructions in context.,abstractText,[0],[0]
"We approach instruction following as a search over plans, scoring sequences of actions conditioned on structured observations of text and the environment.",abstractText,[0],[0]
"By explicitly modeling both the low-level compositional structure of individual actions and the high-level structure of full plans, we are able to learn both grounded representations of sentence meaning and pragmatic constraints on interpretation.",abstractText,[0],[0]
"To demonstrate the model’s flexibility, we apply it to a diverse set of benchmark tasks.",abstractText,[0],[0]
"On every task, we outperform strong task-specific baselines, and achieve several new state-of-the-art results.",abstractText,[0],[0]
Alignment-Based Compositional Semantics for Instruction Following,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1348–1358, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"With more than one hundred thousand new scholarly articles being published each year, there is a rapid growth in the number of citations for the relevant scientific articles.",1 Introduction,[0],[0]
"In this context, we highlight the following interesting facts about the process of citing scientific articles: (i) the most commonly cited paper by Gerard Salton, titled “A Vector Space Model for Information Retrieval” (alleged to have been published in 1975) does not actually exist in reality (Dubin, 2004), (ii) the scientific authors read only 20% of the works they cite (Simkin and Roychowdhury, 2003), (iii) one third of the refer-
ences in a paper are redundant and 40% are perfunctory (Moravcsik and Murugesan, 1975), (iv) 62.7% of the references could not be attributed a specific function (definition, tool etc.)",1 Introduction,[0],[0]
"(Teufel et al., 2006).",1 Introduction,[0],[0]
"Despite these facts, the existing bibliographic metrics consider that all citations are equally significant.
",1 Introduction,[0],[0]
"In this paper, we would emphasize the fact that all the references of a paper are not equally influential.",1 Introduction,[0],[0]
"For instance, we believe that for our current paper, (Wan and Liu, 2014) is more influential reference than (Garfield, 2006), although the former has received lower citations (9) than the latter (1650) so far1.",1 Introduction,[0],[0]
"Therefore the influence of a cited paper completely depends upon the context of the citing paper, not the overall citation count of the cited paper.",1 Introduction,[0],[0]
"We further took the opinion of the original authors of few selective papers and realized that around 16% of the references in a paper are highly influential, and the rest are trivial (Section 4).",1 Introduction,[0],[0]
"This motivates us to design a prediction model, GraLap to automatically label the influence of a cited paper with respect to a citing paper.",1 Introduction,[0],[0]
"Here, we label paper-reference pairs rather than references alone, because a reference that is influential for one citing paper may not be influential with equal extent for another citing paper.
",1 Introduction,[0],[0]
"We experiment with ACL Anthology Network (AAN) dataset and show that GraLap along with the novel feature set, quite efficiently, predicts the intensity of references of papers, which achieves (Pearson) correlation of 0.90 with the human annotations.",1 Introduction,[0],[0]
"Finally, we present four interesting appli-
1The statistics are taken from Google Scholar on June 2, 2016.
1348
cations to show the efficacy of considering unequal intensity of references, compared to the uniform intensity.
",1 Introduction,[0],[0]
"The contributions of the paper are four-fold: (i) we acquire a rich annotated dataset where paperreference pairs are labeled based on the influence scores (Section 4), which is perhaps the first goldstandard for this kind of task; (ii) we propose a graph-based label propagation model GraLap for semi-supervised learning which has tremendous potential for any task where the training set is less in number and labels are non-uniformly distributed (Section 3); (iii) we propose a diverse set of features (Section 3.3); most of them turn out to be quite effective to fit into the prediction model and yield improved results (Section 5); (iv) we present four applications to show how incorporating the reference intensity enhances the performance of several stateof-the-art systems (Section 6).",1 Introduction,[0],[0]
All the references of a paper usually do not carry equal intensity/strength with respect to the citing paper because some papers have influenced the research more than others.,2 Defining Intensity of References,[0],[0]
"To pin down this intuition, here we discretize the reference intensity by numerical values within the range of 1 to 5, (5: most influential, 1: least influential).",2 Defining Intensity of References,[0],[0]
"The appropriate definitions of different labels of reference intensity are presented in Figure 1, which are also the basis of building the annotated dataset (see Section 4):
Note that “reference intensity” and “reference similarity” are two different aspects.",2 Defining Intensity of References,[0],[0]
It might happen that two similar reference are used with different intensity levels in a citing paper – while one is just mentioned somewhere in the paper and other is used as a baseline.,2 Defining Intensity of References,[0],[0]
"Here, we address the former problem as a semi-supervised learning problem with clues taken from content of the citing and cited papers.",2 Defining Intensity of References,[0],[0]
"In this section, we formally define the problem and introduce our prediction model.",3 Reference Intensity Prediction Model,[0],[0]
"We are given a set of papers P = {P1, P2, ..., PM} and a sets of references R = {R1, R2, ..., RM}, where Ri corresponds to the set of references (or cited papers) of Pi.",3.1 Problem Definition,[0],[0]
"There is a set of papers PL ∈ P whose references RL ∈ R are already labeled by ` ∈ L = {1, ..., 5} (each reference is labeled with exactly one value).",3.1 Problem Definition,[0],[0]
"Our objective is to define a predictive function f that labels the references RU ∈ {R \ RL} of the papers PU ∈ {P \ PL} whose reference intensities are unknown, i.e., f : (P,R, PL, RL, PU , RL) −→ L.
Since the size of the annotated (labeled) data is much smaller than unlabeled data (|PL| |PU |), we consider it as a semi-supervised learning problem.
",3.1 Problem Definition,[0],[0]
Definition 1.,3.1 Problem Definition,[0],[0]
(Semi-supervised Learning),3.1 Problem Definition,[0],[0]
"Given a set of entries X and a set of possible labels YL, let us assume that (x1, y1), (x2, y2),..., (xl, yl) be the set of labeled data where xi is a data point and yi ∈ YL is its corresponding label.",3.1 Problem Definition,[0],[0]
"We assume that at least one instance of each class label
is present in the labeled dataset.",3.1 Problem Definition,[0],[0]
"Let (xl+1, yl+1), (xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled data points where YU = {yl+1, yl+2, ...yl+u} are unknown.",3.1 Problem Definition,[0],[0]
"Each entry x ∈ X is represented by a set of features {f1, f2, ..., fD}.",3.1 Problem Definition,[0],[0]
"The problem is to determine the unknown labels using X and YL.
3.2 GraLap: A Prediction Model We propose GraLap, a variant of label propagation (LP) model proposed by (Zhu et al., 2003) where a node in the graph propagates its associated label to its neighbors based on the proximity.",3.1 Problem Definition,[0],[0]
We intend to assign same label to the vertices which are closely connected.,3.1 Problem Definition,[0],[0]
"However unlike the traditional LP model where the original values of the labels continue to fade as the algorithm progresses, we systematically handle this problem in GraLap.",3.1 Problem Definition,[0],[0]
"Additionally, we follow a post-processing in order to handle “classimbalance problem”.",3.1 Problem Definition,[0],[0]
Graph Creation.,3.1 Problem Definition,[0],[0]
"The algorithm starts with the creation of a fully connected weighted graph G = (X,E) where nodes are data points and the weight wij of each edge eij ∈ E is determined by the radial basis function as follows:
wij = exp
( − ∑D
d=1(x",3.1 Problem Definition,[0],[0]
d i,3.1 Problem Definition,[0],[0]
"− xdj )2 σ2
) (1)
The weight is controlled by a parameter σ.",3.1 Problem Definition,[0],[0]
"Later in this section, we shall discuss how σ is selected.",3.1 Problem Definition,[0],[0]
"Each node is allowed to propagate its label to its neighbors through edges (the more the edge weight, the easy to propagate).",3.1 Problem Definition,[0],[0]
Transition Matrix.,3.1 Problem Definition,[0],[0]
"We create a probabilistic transition matrix T|X|×|X|, where each entry Tij indicates the probability of jumping from j to i based on the following: Tij = P (j → i) =",3.1 Problem Definition,[0],[0]
"wij∑|X|
k=1",3.1 Problem Definition,[0],[0]
"wkj .
",3.1 Problem Definition,[0],[0]
Label Matrix.,3.1 Problem Definition,[0],[0]
"Here, we allow a soft label (interpreted as a distribution of labels) to be associated with each node.",3.1 Problem Definition,[0],[0]
"We then define a label matrix Y|X|×|L|, where ith row indicates the label distribution for node xi.",3.1 Problem Definition,[0],[0]
"Initially, Y contains only the values of the labeled data; others are zero.",3.1 Problem Definition,[0],[0]
Label Propagation Algorithm.,3.1 Problem Definition,[0],[0]
"This algorithm works as follows:
After initializing Y and T , the algorithm starts by disseminating the label from one node to its neighbors (including self-loop) in one step (Step 3).",3.1 Problem Definition,[0],[0]
"Then we normalize each entry of Y by the sum of its cor-
1: Initialize T and Y 2: while (Y does not converge) do 3: Y ← TY 4: Normalize rows of Y , yij =
yij∑ k yik
5: Reassign original labels to XL
responding row in order to maintain the interpretation of label probability (Step 4).",3.1 Problem Definition,[0],[0]
Step 5 is crucial; here we want the labeled sources XL to be persistent.,3.1 Problem Definition,[0],[0]
"During the iterations, the initial labeled nodes XL may fade away with other labels.",3.1 Problem Definition,[0],[0]
"Therefore we forcefully restore their actual label by setting yil = 1 (if xi ∈ XL is originally labeled as l), and other entries (∀j 6=lyij) by zero.",3.1 Problem Definition,[0],[0]
We keep on “pushing” the labels from the labeled data points which in turn pushes the class boundary through high density data points and settles in low density space.,3.1 Problem Definition,[0],[0]
"In this way, our approach intelligently uses the unlabeled data in the intermediate steps of the learning.",3.1 Problem Definition,[0],[0]
Assigning Final Labels.,3.1 Problem Definition,[0],[0]
"Once YU is computed, one may take the most likely label from the label distribution for each unlabeled data.",3.1 Problem Definition,[0],[0]
"However, this approach does not guarantee the label proportion observed in the annotated data (which in this case is not well-separated as shown in Section 4).",3.1 Problem Definition,[0],[0]
"Therefore, we adopt a label-based normalization technique.",3.1 Problem Definition,[0],[0]
"Assume that the label proportions in the labeled data are c1, ..., c|L| (s.t. ∑|L| i=1",3.1 Problem Definition,[0],[0]
ci = 1).,3.1 Problem Definition,[0],[0]
"In case of YU , we try to balance the label proportion observed in the ground-truth.",3.1 Problem Definition,[0],[0]
"The label mass is the column sum of YU , denoted by YU.1 , ..., YU.|L| , each of which is scaled in such a way that YU.1 : ... : YU.|L| = c1 : ... : c|L|.",3.1 Problem Definition,[0],[0]
The label of an unlabeled data point is finalized as the label with maximum value in the row of Y .,3.1 Problem Definition,[0],[0]
Convergence.,3.1 Problem Definition,[0],[0]
Here we briefly show that our algorithm is guaranteed to converge.,3.1 Problem Definition,[0],[0]
"Let us combine Steps 3 and 4 as Y ← T̂ Y , where T̂ = Tij/ ∑ k Tik.",3.1 Problem Definition,[0],[0]
"Y is composed of YLl×|L| and YUu×|L| , where YU never changes because of the reassignment.",3.1 Problem Definition,[0],[0]
"We can split T̂ at the boundary of labeled and unlabeled data as follows:
F̂ =",3.1 Problem Definition,[0],[0]
"[ T̂ll T̂lu T̂ul T̂uu ]
Therefore, YU ← T̂uuYU+ T̂ulYL, which can lead to YU = limn→∞ T̂nuuY 0",3.1 Problem Definition,[0],[0]
+,3.1 Problem Definition,[0],[0]
[ ∑n i=1,3.1 Problem Definition,[0],[0]
"T̂ (i−1) uu ]T̂ulYL, where Y 0 is the shape of Y at iteration 0.",3.1 Problem Definition,[0],[0]
"We need
to show T̂nuuijY 0",3.1 Problem Definition,[0],[0]
← 0.,3.1 Problem Definition,[0],[0]
"By construction, T̂ij ≥ 0, and since T̂ is row-normalized, and T̂uu is a part of T̂ , it leads to the following condition: ∃γ < 1, ∑u
j=1 T̂uuij ≤",3.1 Problem Definition,[0],[0]
"γ, ∀i = 1, ..., u.",3.1 Problem Definition,[0],[0]
"So, ∑
j
T̂nuuij = ∑
j
∑
k
T̂ (n−1) uuik T̂uukj
= ∑
k
T̂ (n−1) uuik
∑
j
T̂uuik
≤ ∑
k
T̂ (n−1) uuik",3.1 Problem Definition,[0],[0]
"γ
≤ γn
Therefore, the sum of each row in T̂nuuij converges to zero, which indicates T̂nuuijY
0",3.1 Problem Definition,[0],[0]
← 0.,3.1 Problem Definition,[0],[0]
Selection of σ.,3.1 Problem Definition,[0],[0]
"Assuming a spatial representation of data points, we construct a minimum spanning tree using Kruskal’s algorithm (Kruskal, 1956) with distance between two nodes measured by Euclidean distance.",3.1 Problem Definition,[0],[0]
"Initially, no nodes are connected.",3.1 Problem Definition,[0],[0]
We keep on adding edges in increasing order of distance.,3.1 Problem Definition,[0],[0]
"We choose the distance (say, df ) of the first edge which connects two components with different labeled points in them.",3.1 Problem Definition,[0],[0]
"We consider df as a heuristic to the minimum distance between two classes, and arbitrarily set σ = d0/3, following 3σ rule of normal distribution (Pukelsheim, 1994).",3.1 Problem Definition,[0],[0]
"We use a wide range of features that suitably represent a paper-reference pair (Pi, Rij), indicating Pi refers to Pj through reference Rij .",3.3 Features for Learning Model,[0],[0]
These features can be grouped into six general classes.,3.3 Features for Learning Model,[0],[0]
"3.3.1 Context-based Features (CF)
",3.3 Features for Learning Model,[0],[0]
The “reference context” of Rij in Pi is defined by three-sentence window (sentence where Rij occurs and its immediate previous and next sentences).,3.3 Features for Learning Model,[0],[0]
"For multiple occurrences, we calculate its average score.",3.3 Features for Learning Model,[0],[0]
We refer to “reference sentence” to indicate the sentence where Rij appears.,3.3 Features for Learning Model,[0],[0]
(i) CF:Alone.,3.3 Features for Learning Model,[0],[0]
It indicates whether Rij is mentioned alone in the reference context or together with other references.,3.3 Features for Learning Model,[0],[0]
(ii) CF:First.,3.3 Features for Learning Model,[0],[0]
"When Rij is grouped with others, this feature indicates whether it is mentioned first (e.g., “[2]” is first in “[2,4,6]”).
",3.3 Features for Learning Model,[0],[0]
"Next four features are based on the occurrence of words in the corresponding lists created manually (see Table 1) to understand different aspects.
",3.3 Features for Learning Model,[0],[0]
(iii) CF:Relevant.,3.3 Features for Learning Model,[0],[0]
It indicates whether Rij is explicitly mentioned as relevant in the reference context (Rel in Table 1).,3.3 Features for Learning Model,[0],[0]
(iv) CF:Recent.,3.3 Features for Learning Model,[0],[0]
It tells whether the reference context indicates that Rij is new (Rec in Table 1).,3.3 Features for Learning Model,[0],[0]
(v) CF:Extreme.,3.3 Features for Learning Model,[0],[0]
It implies that Rij is extreme in some way (Ext in Table 1).,3.3 Features for Learning Model,[0],[0]
(vi) CF:Comp.,3.3 Features for Learning Model,[0],[0]
"It indicates whether the reference context makes some kind of comparison with Rij (Comp in Table 1).
",3.3 Features for Learning Model,[0],[0]
"Note we do not consider any sentiment-based features as suggested by (Zhu et al., 2015).",3.3 Features for Learning Model,[0],[0]
"3.3.2 Similarity-based Features (SF)
",3.3 Features for Learning Model,[0],[0]
It is natural that the high degree of semantic similarity between the contents of Pi and Pj indicates the influence of Pj in Pi.,3.3 Features for Learning Model,[0],[0]
"We assume that although the full text of Pi is given, we do not have access to the full text of Pj (may be due to the subscription charge or the unavailability of the older papers).",3.3 Features for Learning Model,[0],[0]
"Therefore, we consider only the title of Pj as a proxy of its full text.",3.3 Features for Learning Model,[0],[0]
Then we calculate the cosine-similarity2 between the title (T) of Pj and (i) SF:TTitle.,3.3 Features for Learning Model,[0],[0]
"the title, (ii) SF:TAbs.",3.3 Features for Learning Model,[0],[0]
"the abstract, SF:TIntro.",3.3 Features for Learning Model,[0],[0]
"the introduction, (iv) SF:TConcl.",3.3 Features for Learning Model,[0],[0]
"the conclusion, and (v) SF:TRest.",3.3 Features for Learning Model,[0],[0]
"the rest of the sections (sections other than abstract, introduction and conclusion) of Pi.
",3.3 Features for Learning Model,[0],[0]
We further assume that the “reference context” (RC) of Pj in Pi might provide an alternate way of summarizing the usage of the reference.,3.3 Features for Learning Model,[0],[0]
"Therefore, we take the same similarity based approach mentioned above, but replace the title of Pj with its RC and obtain five more features: (vi) SF:RCTitle, (vii) SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and (x) SF:RCRest.",3.3 Features for Learning Model,[0],[0]
"If a reference appears multiple times in a citing paper, we consider the aggregation of all RCs together.",3.3 Features for Learning Model,[0],[0]
"The underlying assumption of these features is that a reference which occurs more frequently in a citing paper is more influential than a single occurrence (Singh et al., 2015).",3.3.3 Frequency-based Feature (FF),[0],[0]
We count the frequency of Rij in (i),3.3.3 Frequency-based Feature (FF),[0],[0]
FF:Whole.,3.3.3 Frequency-based Feature (FF),[0],[0]
"the entire content, (ii) FF:Intro.",3.3.3 Frequency-based Feature (FF),[0],[0]
"the introduction, (iii) FF:Rel. the related work, (iv) FF:Rest.",3.3.3 Frequency-based Feature (FF),[0],[0]
"the rest of the sections (as
2We use the vector space based model (Turney and Pantel, 2010) after stemming the words using Porter stammer (Porter, 1997).
mentioned in Section 3.3.2) of Pi.",3.3.3 Frequency-based Feature (FF),[0],[0]
We also introduce (v) FF:Sec. to measure the fraction of different sections of Pi where Rij occurs (assuming that appearance of Rij in different sections is more influential).,3.3.3 Frequency-based Feature (FF),[0],[0]
These features are further normalized using the number of sentences in Pi in order to avoid unnecessary bias on the size of the paper.,3.3.3 Frequency-based Feature (FF),[0],[0]
"Position of a reference in a paper might be a predictive clue to measure the influence (Zhu et al., 2015).",3.3.4 Position-based Features (PF),[0],[0]
"Intuitively, the earlier the reference appears in the paper, the more important it seems to us.",3.3.4 Position-based Features (PF),[0],[0]
"For the first two features, we divide the entire paper into two parts equally based on the sentence count and then see whether Rij appears (i) PF:Begin.",3.3.4 Position-based Features (PF),[0],[0]
in the beginning or (ii) PF:End.,3.3.4 Position-based Features (PF),[0],[0]
in the end of Pi.,3.3.4 Position-based Features (PF),[0],[0]
"Importantly, if Rij appears multiple times in Pi, we consider the fraction of times it occurs in each part.
",3.3.4 Position-based Features (PF),[0],[0]
"For the other two features, we take the entire paper, consider sentences as atomic units, and measure position of the sentences where Rij appears, including (iii) PF:Mean.",3.3.4 Position-based Features (PF),[0],[0]
"mean position of appearance, (iv) PF:Std. standard deviation of different appearances.",3.3.4 Position-based Features (PF),[0],[0]
"These features are normalized by the total length (number of sentences) of Pi. , thus ranging from 0 (indicating beginning of Pi) to 1 (indicating the end of Pi).",3.3.4 Position-based Features (PF),[0],[0]
The linguistic evidences around the context ofRij sometimes provide clues to understand the intrinsic influence of Pj on Pi.,3.3.5 Linguistic Features (LF),[0],[0]
Here we consider word level and structural features.,3.3.5 Linguistic Features (LF),[0],[0]
(i) LF:NGram.,3.3.5 Linguistic Features (LF),[0],[0]
"Different levels of n-grams (1- grams, 2-grams and 3-grams) are extracted from the reference context to see the effect of different word combination (Athar and Teufel, 2012).
",3.3.5 Linguistic Features (LF),[0],[0]
(ii) LF:POS.,3.3.5 Linguistic Features (LF),[0],[0]
"Part-of-speech (POS) tags of the words in the reference sentence are used as features (Jochim and Schütze, 2012).",3.3.5 Linguistic Features (LF),[0],[0]
(iii) LF:Tense.,3.3.5 Linguistic Features (LF),[0],[0]
"The main verb of the reference sentence is used as a feature (Teufel et al., 2006).",3.3.5 Linguistic Features (LF),[0],[0]
(iv) LF:Modal.,3.3.5 Linguistic Features (LF),[0],[0]
"The presence of modal verbs (e.g., “can”, “may”) often indicates the strength of the claims.",3.3.5 Linguistic Features (LF),[0],[0]
"Hence, we check the presence of the modal verbs in the reference sentence.",3.3.5 Linguistic Features (LF),[0],[0]
(v) LF:MainV. We use the main-verb of the reference sentence as a direct feature in the model.,3.3.5 Linguistic Features (LF),[0],[0]
(vi) LF:hasBut.,3.3.5 Linguistic Features (LF),[0],[0]
"We check the presence of conjunction “but”, which is another clue to show less confidence on the cited paper.",3.3.5 Linguistic Features (LF),[0],[0]
(vii) LF:DepRel.,3.3.5 Linguistic Features (LF),[0],[0]
"Following (Athar and Teufel, 2012)",3.3.5 Linguistic Features (LF),[0],[0]
"we use all the dependencies present in the reference context, as given by the dependency parser (Marneffe et al., 2006).",3.3.5 Linguistic Features (LF),[0],[0]
(viii) LF:POSP.,3.3.5 Linguistic Features (LF),[0],[0]
"(Dong and Schfer, 2011) use seven regular expression patterns of POS tags to capture syntactic information; then seven boolean features mark the presence of these patterns.",3.3.5 Linguistic Features (LF),[0],[0]
"We also utilize the same regular expressions as shown below 3 with the examples (the empty parenthesis in each example indicates the presence of a reference token Rij in the corresponding sentence; while few examples are complete sentences, few are not):
• “.*\\(\\) VV[DPZN].*”: Chen () showed that cohesion is held in the vast majority of cases for English-French.
",3.3.5 Linguistic Features (LF),[0],[0]
• “.*(VHP|VHZ),3.3.5 Linguistic Features (LF),[0],[0]
"VV.*”: while Cherry and Lin () have shown it to be a strong feature for word alignment...
• “.*VH(D|G|N|P|Z) (RB )*VBN.*”: Inducing features for taggers by clustering has been tried by several researchers ().
",3.3.5 Linguistic Features (LF),[0],[0]
"• “.*MD (RB )*VB(RB )* VVN.*”: For example, the likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair ().
3The meaning of each POS tag can be found in http://nlp.stanford.edu/software/tagger.",3.3.5 Linguistic Features (LF),[0],[0]
"shtml(Toutanova and Manning, 2000).
",3.3.5 Linguistic Features (LF),[0],[0]
"• “[ IW.]*VB(D|P|Z) (RB )*VV[ND].*”: Our experimental set-up is modeled after the human evaluation presented in ().
",3.3.5 Linguistic Features (LF),[0],[0]
• “(RB )*PP (RB )*V.*”: We use CRF () to perform this tagging.,3.3.5 Linguistic Features (LF),[0],[0]
• “.*VVG (NP )*(CC )*(NP ).,3.3.5 Linguistic Features (LF),[0],[0]
"*”: Following (), we provide the an-
notators with only short sentences: those with source sentences between 10 and 25 tokens long.
",3.3.5 Linguistic Features (LF),[0],[0]
These are all considered as Boolean features.,3.3.5 Linguistic Features (LF),[0],[0]
"For each feature, we take all the possible evidences from all paper-reference pairs and prepare a vector.",3.3.5 Linguistic Features (LF),[0],[0]
"Then for each pair, we check the presence (absence) of tokens for the corresponding feature and mark the vector accordingly (which in turn produces a set of Boolean features).",3.3.5 Linguistic Features (LF),[0],[0]
This group provides other factors to explain why is a paper being cited.,3.3.6 Miscellaneous Features (MS),[0],[0]
(i) MS:GCount.,3.3.6 Miscellaneous Features (MS),[0],[0]
"To answer whether a highly-cited paper has more academic influence on the citing paper than the one which is less cited, we measure the number of other papers (except Pi) citing Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
"(ii) MS:SelfC. To see the effect of self-citation, we check whether at least one author is common in both Pi and Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
(iii) MG:Time.,3.3.6 Miscellaneous Features (MS),[0],[0]
"The fact that older papers are rarely cited, may not stipulate that these are less influential.",3.3.6 Miscellaneous Features (MS),[0],[0]
"Therefore, we measure the difference of the publication years of Pi and Pj .",3.3.6 Miscellaneous Features (MS),[0],[0]
(iv) MG:CoCite.,3.3.6 Miscellaneous Features (MS),[0],[0]
"It measures the co-citation counts of Pi and Pj defined by
|Ri∩Rj | |Ri∪Rj | , which in turn an-
swers the significance of reference-based similarity driving the academic influence (Small, 1973).
",3.3.6 Miscellaneous Features (MS),[0],[0]
"Following (Witten and Frank, 2005), we further make one step normalization and divide each feature by its maximum value in all the entires.",3.3.6 Miscellaneous Features (MS),[0],[0]
"We use the AAN dataset (Radev et al., 2009) which is an assemblage of papers included in ACL related venues.",4 Dataset and Annotation,[0],[0]
"The texts are preprocessed where sentences, paragraphs and sections are properly separated using different markers.",4 Dataset and Annotation,[0],[0]
"The filtered dataset contains 12,843 papers (on average 6.21 references per paper) and 11,092 unique authors.
",4 Dataset and Annotation,[0],[0]
"Next we use Parscit (Councill et al., 2008) to identify the reference contexts from the dataset and then extract the section headings from all the papers.",4 Dataset and Annotation,[0],[0]
"Then each section heading is mapped into one
of the following broad categories using the method proposed by (Liakata et al., 2012):",4 Dataset and Annotation,[0],[0]
"Abstract, Introduction, Related Work, Conclusion and Rest.",4 Dataset and Annotation,[0],[0]
Dataset Labeling.,4 Dataset and Annotation,[0],[0]
The hardest challenge in this task is that there is no publicly available dataset where references are annotated with the intensity value.,4 Dataset and Annotation,[0],[0]
"Therefore, we constructed our own annotated dataset in two different ways.",4 Dataset and Annotation,[0],[0]
(i) Expert Annotation: we requested members of our research group4 to participate in this survey.,4 Dataset and Annotation,[0],[0]
"To facilitate the labeling process, we designed a portal where all the papers present in our dataset are enlisted in a drop-down menu.",4 Dataset and Annotation,[0],[0]
"Upon selecting a paper, its corresponding references were shown with five possible intensity values.",4 Dataset and Annotation,[0],[0]
The citing and cited papers are also linked to the original texts so that the annotators can read the original papers.,4 Dataset and Annotation,[0],[0]
A total of 20 researchers participated and they were asked to label as many paperreference pairs as they could based on the definitions of the intensity provided in Section 2.,4 Dataset and Annotation,[0],[0]
The annotation process went on for one month.,4 Dataset and Annotation,[0],[0]
"Out of total 1640 pairs annotated, 1270 pairs were taken such that each pair was annotated by at least two annotators, and the final intensity value of the pair was considered to be the average of the scores.",4 Dataset and Annotation,[0],[0]
The Pearson correlation and Kendell’s τ among the annotators are 0.787 and 0.712 respectively.,4 Dataset and Annotation,[0],[0]
(ii) Author Annotation: we believe that the authors of a paper are the best experts to judge the intensity of references present in the paper.,4 Dataset and Annotation,[0],[0]
"With this intension, we launched a survey where we requested the authors whose papers are present in our dataset with significant numbers.",4 Dataset and Annotation,[0],[0]
We designed a web portal in similar fashion mentioned earlier; but each author was only shown her own papers in the drop-down menu.,4 Dataset and Annotation,[0],[0]
"Out of 35 requests, 22 authors responded and total 196 pairs are annotated.",4 Dataset and Annotation,[0],[0]
This time we made sure that each paper-reference pair was annotated by only one author.,4 Dataset and Annotation,[0],[0]
"The percentages of labels in the overall annotated dataset are as follows: 1: 9%, 2: 74%, 3: 9%, 4: 3%, 5: 4%.",4 Dataset and Annotation,[0],[0]
"In this section, we start with analyzing the importance of the feature sets in predicting the reference
4All were researchers with the age between 25-45 working on document summarization, sentiment analysis, and text mining in NLP.
intensity, followed by the detailed results.",5 Experimental Results,[0],[0]
Feature Analysis.,5 Experimental Results,[0],[0]
"In order to determine which features highly determine the gold-standard labeling, we measure the Pearson correlation between various features and the ground-truth labels.",5 Experimental Results,[0],[0]
"Figure 2(a) shows the average correlation for each feature group, and in each group the rank of features based on the correlation is shown in Figure 2(b).",5 Experimental Results,[0],[0]
"Frequencybased features (FF) turn out to be the best, among which FF:Rest is mostly correlated.",5 Experimental Results,[0],[0]
This set of features is convenient and can be easily computed.,5 Experimental Results,[0],[0]
Both CF and LF seem to be equally important.,5 Experimental Results,[0],[0]
"However, PF tends to be less important in this task.
",5 Experimental Results,[0],[0]
Results of Predictive Models.,5 Experimental Results,[0],[0]
"For the purpose of evaluation, we report the average results after 10- fold cross-validation.",5 Experimental Results,[0],[0]
"Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in (Wan and Liu, 2014), (iii) SVR+O:",5 Experimental Results,[0],[0]
"SVR model with our feature set, (iv) C4.5SSL: C4.5 semisupervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set (Zhu et al., 2003).",5 Experimental Results,[0],[0]
"Three metrics are used to compare the results of the competing models with the annotated labels: Root Mean Square Error (RMSE), Pearson’s correlation coeffi-
cient (ρ), and coefficient of determination (R2)5.",5 Experimental Results,[0],[0]
Table 2 shows the performance of the competing models.,5 Experimental Results,[0],[0]
We incrementally include each feature set into GraLap greedily on the basis of ranking shown in Figure 2(a).,5 Experimental Results,[0],[0]
We observe that GraLap with only FF outperforms SVR+O with 41% improvement of ρ.,5 Experimental Results,[0],[0]
"As expected, the inclusion of PF into the model improves the model marginally.",5 Experimental Results,[0],[0]
"However, the overall performance of GraLap is significantly higher than any of the baselines (p < 0.01).",5 Experimental Results,[0],[0]
"In this section, we provide four different applications to show the use of measuring the intensity of references.",6 Applications of Reference Intensity,[0],[0]
"To this end, we consider all the labeled entries for training and run GraLap to predict the intensity of rest of the paper-reference pairs.",6 Applications of Reference Intensity,[0],[0]
Influential papers in a particular area are often discovered by considering equal weights to all the citations of a paper.,6.1 Discovering Influential Articles,[0],[0]
We anticipate that considering the reference intensity would perhaps return more meaningful results.,6.1 Discovering Influential Articles,[0],[0]
"To show this, Here we use the following measures individually to compute the influence of a paper: (i) RawCite: total number of citations per paper, (ii) RawPR: we construct a citation network (nodes: papers, links: citations), and measure PageRank (Page et al., 1998) of each node n: PR(n) = 1−qN + q ∑ m∈M(n) PR(m)",6.1 Discovering Influential Articles,[0],[0]
"|L(m)| ; where, q, the damping factor, is set to 0.85, N is the total number of nodes, M(n) is the set of nodes that have edges to n, and L(m) is the set of nodes that m has an edge to, (iii) InfCite: the weighted version of RawCite, measured by the sum of intensities of all citations of a paper, (iv) InfPR: the weighted version of RawPR: PR(n) =",6.1 Discovering Influential Articles,[0],[0]
"1−qN + q ∑
m∈M(n) Inf(m→n)PR(m)∑
a∈L(m)Inf(m→a) , where Inf indicates
the influence of a reference.",6.1 Discovering Influential Articles,[0],[0]
We rank all the articles based on these four measures separately.,6.1 Discovering Influential Articles,[0],[0]
Table 3(a) shows the Spearman’s rank correlation between pair-wise measures.,6.1 Discovering Influential Articles,[0],[0]
"As expected, (i) and (ii) have high correlation (same for (iii) and (iv)), whereas across two types of measures the correlation is less.",6.1 Discovering Influential Articles,[0],[0]
"Further, in order to know which mea-
5The less (resp. more) the value of RMSE and R2 (resp.",6.1 Discovering Influential Articles,[0],[0]
"ρ), the better the performance of the models.
",6.1 Discovering Influential Articles,[0],[0]
"sure is more relevant, we conduct a subjective study where we select top ten papers from each measure and invite the experts (not authors) who annotated the dataset, to make a binary decision whether a recommended paper is relevant.",6.1 Discovering Influential Articles,[0],[0]
6.,6.1 Discovering Influential Articles,[0],[0]
"The average pairwise inter-annotator’s agreement (based on Cohen’s kappa (Cohen, 1960)) is 0.71.",6.1 Discovering Influential Articles,[0],[0]
"Table 3(b) presents that out of 10 recommendations of InfPR, 7 (5) papers are marked as influential by majority (all) of the annotators, which is followed by InfCite.",6.1 Discovering Influential Articles,[0],[0]
These results indeed show the utility of measuring reference intensity for discovering influential papers.,6.1 Discovering Influential Articles,[0],[0]
Top three papers based on InfPR from the entire dataset are shown in Table 4.,6.1 Discovering Influential Articles,[0],[0]
"H-index, a measure of impact/influence of an author, considers each citation with equal weight (Hirsch, 2005).",6.2 Identifying Influential Authors,[0],[0]
"Here we incorporate the notion of reference intensity into it and define hif-index.
",6.2 Identifying Influential Authors,[0],[0]
Definition 2.,6.2 Identifying Influential Authors,[0],[0]
"An author A with a set of papers P (A) has an hif-index equals to h, if h is the largest value such that |{p ∈ P (A)|Inf(p) ≥ h}| ≥ h; where Inf(p) is the sum of intensities of all citations of p.
",6.2 Identifying Influential Authors,[0],[0]
We consider 37 ACL fellows as the list of goldstandard influential authors.,6.2 Identifying Influential Authors,[0],[0]
"For comparative evaluation, we consider the total number of papers (TotP), total number of citations (TotC) and average citations per paper (AvgC) as three competing measures along with h-index and hif-index.",6.2 Identifying Influential Authors,[0],[0]
We arrange all the authors in our dataset in decreasing order of each measure.,6.2 Identifying Influential Authors,[0],[0]
Figure 3(a) shows the Spearman’s rank correlation among the common elements across pair-wise rankings.,6.2 Identifying Influential Authors,[0],[0]
Figure 3(b) shows the Precision@k for five competing measures at identifying ACL fellows.,6.2 Identifying Influential Authors,[0],[0]
"We observe that hif-index performs significantly well with an overall precision of 0.54, followed by AvgC (0.37),
6We choose papers from the area of “sentiment analysis” on which experts agree on evaluating the papers.
",6.2 Identifying Influential Authors,[0],[0]
"h-index (0.35), TotC (0.32) and TotP (0.34).",6.2 Identifying Influential Authors,[0],[0]
This result is an encouraging evidence that the referenceintensity could improve the identification of the influential authors.,6.2 Identifying Influential Authors,[0],[0]
Top three authors based on hif-index are shown in Table 4.,6.2 Identifying Influential Authors,[0],[0]
Here we show the effectiveness of referenceintensity by applying it to a real paper recommendation system.,6.3 Effect on Recommendation System,[0],[0]
"To this end, we consider FeRoSA7 (Chakraborty et al., 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facetwise recommendations with each facet representing the purpose of recommendation (Chakraborty et al., 2016).",6.3 Effect on Recommendation System,[0],[0]
The methodology is based on random walk with restarts (RWR) initiated from a query paper.,6.3 Effect on Recommendation System,[0],[0]
The model is built on AAN dataset and considers both the citation links and the content information to produce the most relevant results.,6.3 Effect on Recommendation System,[0],[0]
"Instead of using the unweighted citation network, here we use the weighted network with each edge labeled by the intensity score.",6.3 Effect on Recommendation System,[0],[0]
The final recommendation of FeRoSA is obtained by performing RWR with the transition probability proportional to the edge-weight (we call it Inf-FeRoSA).,6.3 Effect on Recommendation System,[0],[0]
"We observe that Inf-FeRoSA achieves an average precision of 0.81 at top 10 recommendations, which is 14% higher then FeRoSA while considering the flat version and 12.34% higher than FeRoSA while considering the faceted version.",6.3 Effect on Recommendation System,[0],[0]
"Recently, Thomson Reuters began screening for journals that exchange large number of anomalous citations with other journals in a cartel-like arrangement, often known as “citation stacking” (Jump, 2013; Hardcastle, 2015).",6.4 Detecting Citation Stacking,[0],[0]
"This sort of citation stacking is much more pernicious and difficult to detect.
",6.4 Detecting Citation Stacking,[0],[0]
"7www.ferosa.org
We anticipate that this behavior can be detected by the reference intensity.",6.4 Detecting Citation Stacking,[0],[0]
"Since the AAN dataset does not have journal information, we use DBLP dataset (Singh et al., 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al., 2014)).",6.4 Detecting Citation Stacking,[0],[0]
"From this dataset, we extract all the features mentioned in Section 3.3 except the ones that require full text, and run our model using the existing annotated dataset as training instances.",6.4 Detecting Citation Stacking,[0],[0]
We measure the traditional impact factor (IF ) of the journals and impact factor after considering the reference intensity (IFif ).,6.4 Detecting Citation Stacking,[0],[0]
"Figure 4(a) shows that there are few journals whose IFif significantly deviates (3σ from the mean) from IF ; out of the suspected journals 70% suffer from the effect of self-journal citations as well (shown in Figure 4(b)), example including Expert Systems with Applications (current IF of 2.53).",6.4 Detecting Citation Stacking,[0],[0]
One of the future work directions would be to predict such journals as early as possible after their first appearance.,6.4 Detecting Citation Stacking,[0],[0]
"Although the citation count based metrics are widely accepted (Garfield, 2006; Hirsch, 2010), the belief that mere counting of citations is dubious has also been a subject of study (Chubin and Moitra, 1975).",7 Related Work,[0],[0]
"(Garfield, 1964) was the first who explained the reasons of citing a paper.",7 Related Work,[0],[0]
"(Pham and Hoffmann, 2003) introduced a method for the rapid development of complex rule bases for classifying text segments.
",7 Related Work,[0],[0]
"(Dong and Schfer, 2011) focused on a less manual approach by learning domain-insensitive features from textual, physical, and syntactic aspects To address concerns about h-index, different alternative measures are proposed (Waltman and van Eck, 2012).",7 Related Work,[0],[0]
However they too could benefit from filtering or weighting references with a model of influence.,7 Related Work,[0],[0]
"Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al., 2011).",7 Related Work,[0],[0]
"Recently, (Wan and Liu, 2014) proposed a SVR based approach to measure the intensity of citations.",7 Related Work,[0],[0]
"Our methodology differs from this approach in at lease four significant ways: (i) they used six very shallow level features; whereas we consider features from different dimensions, (ii) they labeled the dataset by the help of independent annotators; here we additionally ask the authors of the citing papers to identify the influential references which is very realistic (Gilbert, 1977); (iii) they adopted SVR for labeling, which does not perform well for small training instances; here we propose GraLap , designed specifically for small training instances; (iv) four applications of reference intensity mentioned here are completely new and can trigger further to reassessing the existing bibliometrics.",7 Related Work,[0],[0]
"We argued that the equal weight of all references might not be a good idea not only to gauge success of a research, but also to track follow-up work or recommending research papers.",8 Conclusion,[0],[0]
The annotated dataset would have tremendous potential to be utilized for other research.,8 Conclusion,[0],[0]
"Moreover, GraLap can be used for any semi-supervised learning problem.",8 Conclusion,[0],[0]
Each application mentioned here needs separate attention.,8 Conclusion,[0],[0]
"In future, we shall look into more linguistic evidences to improve our model.",8 Conclusion,[0],[0]
"Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for.",abstractText,[0],[0]
"Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications.",abstractText,[0],[0]
"To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semisupervised model, GraLap to label the intensity of references.",abstractText,[0],[0]
Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation).,abstractText,[0],[0]
"Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.",abstractText,[0],[0]
All Fingers are not Equal: Intensity of References in Scientific Articles,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 237–244, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics
Our method involves clustering the document sentences into topics using a fuzzy clustering algorithm. Then each sentence is scored according to how well it covers the various topics. This is done using statistical features such as TF, sentence length, etc. Finally, the summary is constructed from the highest scoring sentences, while avoiding overlap between the summary sentences. This makes it language-independent, but we have to afford preprocessed data first (tokenization, stemming, etc.).",text,[0],[0]
"A document summary can be regarded as domainspecific or general-purpose, using the specificity as classification criterion (Hovy and Lin, 1998).",1 Introduction,[0],[0]
"We can, also, look at this criterion from language angle: language-specific or language-independent summarization.",1 Introduction,[0],[0]
Language-independent systems can handle more than one language.,1 Introduction,[0],[0]
"They can be partially language-independent, which means they use language-related resources, and therefore you can’t add a new language so easily.",1 Introduction,[0],[0]
"Inversely, they can be fully language-independent.
",1 Introduction,[0],[0]
"Recently, multilingual summarization has received the attention of the summarization community, such as Text Analysis Conference (TAC).",1 Introduction,[0],[0]
"The TAC 2011 workshop included a task called “MultiLing task”, which aims to evaluate languageindependent summarization algorithms on a variety of languages (Giannakopoulos et al., 2011).",1 Introduction,[0],[0]
"In
the task’s pilot, there were seven languages covering news texts: Arabic, Czech, English, French, Greek, Hebrew and Hindi, where each system has to participate for at least two languages.",1 Introduction,[0],[0]
MultiLing 2013 workshop is a community-driven initiative for testing and promoting multilingual summarization methods.,1 Introduction,[0],[0]
It aims to evaluate the application of (partially or fully) language-independent summarization algorithms on a variety of languages.,1 Introduction,[0],[0]
"There were three tasks: “Multi-document multilingual summarization”(Giannakopoulos, 2013), “Multilingual single document summarization” (Kubina et al., 2013) and “Multilingual summary evaluation”.",1 Introduction,[0],[0]
"The multi-document task uses the 7 past languages along with three new languages: Chinese, Romanian and Spanish.",1 Introduction,[0],[0]
"The single document task introduces 40 languages.
",1 Introduction,[0],[0]
"This paper contains a description of our method (Aries et al., 2013) which uses sentences’ clustering to define topics, and then trains on these topics to score each sentence.",1 Introduction,[0],[0]
"We will explain each task in the system (AllSummarizer), especially the preprocessing task which is languagedependent.",1 Introduction,[0],[0]
"Then, we will discuss how we fixed the summarization’s hyper-parameters (threshold and features) for each language.",1 Introduction,[0],[0]
The next section (Section 5) is reserved to discuss the experiments conducted in the MultiLing workshop.,1 Introduction,[0],[0]
"Finally, we will conclude by discussing possible improvements.",1 Introduction,[0],[0]
"Clustering has been used for summarization in many systems, either using documents as units, sentences or words.",2 Related works,[0],[0]
The resulted clusters are used to extract the summary.,2 Related works,[0],[0]
Some systems use just the biggest cluster to score sentences and get the top ones.,2 Related works,[0],[0]
"Others take from each cluster a representative sentence, in order to cover all topics.",2 Related works,[0],[0]
"While there are systems, like ours, which score sentences according to all clusters.
237
“CIST” (Liu et al., 2011; Li et al., 2013) is a system which uses hierarchical Latent Dirichlet Allocation topic (hLDA) model to cluster sentences into sub-topics.",2 Related works,[0],[0]
A sub,2 Related works,[0],[0]
-topic containing more sentences is more important and therefore those containing just one or two sentences can be neglected.,2 Related works,[0],[0]
The sentences are scored using hLDA model combined with some traditional features.,2 Related works,[0],[0]
"The system participated for multi-document summarization task, where all documents of the same topic are merged into a big text document.
",2 Related works,[0],[0]
"Likewise, “UoEssex” (El-Haj et al., 2011) uses a clustering method (K-Means) to regroup similar sentences.",2 Related works,[0],[0]
"The biggest cluster is used to extract the summary, while other clusters are ignored.",2 Related works,[0],[0]
"Then, the sentences are scored using their cosine similarities to the cluster’s centroid.",2 Related works,[0],[0]
"The use of the biggest cluster is justified by the assumption that a single cluster will give a coherent summary.
",2 Related works,[0],[0]
"The scoring functions of these two systems are based on statistical features like frequencies of words, cosine similarity, etc.",2 Related works,[0],[0]
"In the contrary, systems like those of Conroy et al. (2011) (“CLASSY”), Varma et al. (2011) (“SIEL IIITH”), El-Haj and Rayson (2013), etc. are corpus-based summarizers, which can make it hard to introduce new languages.",2 Related works,[0],[0]
“CLASSY” uses naı̈ve Bayes to estimate the probability that a term may be included in the summary.,2 Related works,[0],[0]
The classifier was trained on DUC 2005-2007 data.,2 Related works,[0],[0]
"As for backgrounds of each language, Wikinews are used to compute Dunning G-statistic.",2 Related works,[0],[0]
“SIEL IIITH” uses a probabilistic Hyperspace Analogue to Language model.,2 Related works,[0],[0]
"Given a word, it estimates the probability of observing another word with it in a window of size K, using a sufficiently large corpus.",2 Related works,[0],[0]
El-Haj and Rayson (2013) calculate the log-likelihood of each word using a corpus of words frequencies and the multiLing’13 dataset.,2 Related works,[0],[0]
"The score of each sentence is the sum of its words’ log-likelihoods.
",2 Related works,[0],[0]
"In our method (Aries et al., 2013), we use a simple fuzzy clustering algorithm.",2 Related works,[0],[0]
"We assume that a sentence can express many topics, and therefore it can belong to many clusters.",2 Related works,[0],[0]
"Also, we believe that a summary must take in consideration other topics than the main one (the biggest cluster).",2 Related works,[0],[0]
"To score sentences, we use a scoring function based on Naı̈ve Bayes classification.",2 Related works,[0],[0]
"It uses the clusters for training rather than a corpus, in order to avoid the problem of language dependency.",2 Related works,[0],[0]
One of multilingual summarization’s problem is the lack of resources such as labeled corpus used for learning.,3 System overview,[0],[0]
"Learning algorithms were used either to select the sentences that should be in the summary, or to estimate the features’ weights.",3 System overview,[0],[0]
Both cases need a training corpus given the language and the domain we want to adapt the summarizer to.,3 System overview,[0],[0]
"To design a language-neutral summarization system, either we adapt a system for input languages (Partly language-neutral), or we design a system that can process any language (Fully language-neutral).
",3 System overview,[0],[0]
"Our sentence extraction method can be applied to any language without any modifications, affording the pre-process step of the input language.",3 System overview,[0],[0]
"To do this, we had to find a new method to train our system other than using a corpus (language and topic dependent).",3 System overview,[0],[0]
The idea was to find different topics in the input text using similarity between sentences.,3 System overview,[0],[0]
"Then, we train the system using a scoring function based on Bayes classification algorithm and a set of features to find the probability of a feature given the topic.",3 System overview,[0],[0]
"Finally, we calculate for each sentence a score that reflects how it can represent all the topics.
",3 System overview,[0],[0]
"In our previous work (Aries et al., 2013), our system used only two features which have the same nature (TF: uni-grams and bi-grams).",3 System overview,[0],[0]
"When we add new features, this can affect the final result (summary).",3 System overview,[0],[0]
"Also, our clustering method lies on the clustering threshold which has to be estimated somehow.",3 System overview,[0],[0]
"To handle multi-document summarization, we just fuse all documents in the same topic and consider them as one document.",3 System overview,[0],[0]
Figure 1 represents the general architecture of AllSummarizer1.,3 System overview,[0],[0]
"This is the language-dependent part, which can be found in many information retrieval (IR) works.",3.1 Preprocessing,[0],[0]
"In our system, we are interested in four preprocessing tasks:
• Normalizer: in this step, we can delete special characters.",3.1 Preprocessing,[0],[0]
"For Arabic, we can delete diacritics (Tashkiil) if we don’t need them in the process (which is our case).
",3.1 Preprocessing,[0],[0]
"• Segmenter: The segmenter defines two func1 https://github.com/kariminf/AllSummarizer
tions: sentence segmentation and word tokenization.
",3.1 Preprocessing,[0],[0]
•,3.1 Preprocessing,[0],[0]
"Stemmer: The role of this task is to delete suffixes and prefixes so we can get the stem of a word.
",3.1 Preprocessing,[0],[0]
"• Stop-Words eliminator: It is used to remove the stop words, which are the words having no signification added to the text.
",3.1 Preprocessing,[0],[0]
"In this work, normalization is used just for Arabic and Persian to delete diacritics (Tashkiil).",3.1 Preprocessing,[0],[0]
"Concerning stop-word elimination, we use precompiled word-lists available on the web.",3.1 Preprocessing,[0],[0]
Table 1 shows each language and the tools used in the remaining pre-processing tasks.,3.1 Preprocessing,[0],[0]
"Each text contains many topics, where a topic is a set of sentences having some sort of relationship between each other.",3.2 Topics clustering,[0],[0]
"In our case, this relationship is the cosine similarity between each two sentences.",3.2 Topics clustering,[0],[0]
"It means, the sentences that have many terms in common are considered in the same topic.",3.2 Topics clustering,[0],[0]
"Given two sentences X and Y , the cosine similar-
2 https://opennlp.apache.org/ 3 https://github.com/mojtaba-khallash/JHazm 4 https://lucene.apache.org/ 5 http://zeus.cs.pacificu.edu/shereen/research.htm 6 http://code972.com/hebmorph 7 http://snowball.tartarus.org/
ity between them is expressed by equation 1.
",3.2 Topics clustering,[0],[0]
"cos(X,Y )",3.2 Topics clustering,[0],[0]
=,3.2 Topics clustering,[0],[0]
"∑ i xi.yi√∑
i(xi)2.",3.2 Topics clustering,[0],[0]
"√∑ i(yi)2 (1)
Where xi (yi) denotes frequencies for each term in the sentence X (Y ).
",3.2 Topics clustering,[0],[0]
"To generate topics, we use a simple algorithm (see algorithm 1) which uses cosine similarity and a clustering threshold th to cluster n sentences.
",3.2 Topics clustering,[0],[0]
Algorithm 1: clustering method Data: Pre-processed sentences Result: clusters of sentences (C) foreach sentence,3.2 Topics clustering,[0],[0]
"Si / i = 1 to n do
Ci += Si ; // Ci: ith cluster foreach sentence",3.2 Topics clustering,[0],[0]
"Sj / j = i + 1 to n do
Sim = cosine similarity(Si, Sj) ; if sim > th then
Ci += Sj ; end
end C += Ci ;
end foreach cluster Ci / i=n to 1 do
foreach cluster Cj / j=i-1 to 1 do if Ci is included in Cj then
C -= Ci ; break ;
end end
end",3.2 Topics clustering,[0],[0]
"A summary is a short text that is supposed to represent most information in the source text, and cover most of its topics.",3.3 Scoring function,[0],[0]
"Therefore, we assume that a sentence si can be in the summary when it is most probable to represent all topics (clusters) cj ∈ C using a set of features fk ∈ F .",3.3 Scoring function,[0],[0]
"We used Naı̈ve Bayes, assuming independence between different classes and different features (a sentence can have multiple classes).",3.3 Scoring function,[0],[0]
"So, the score of a sentence si is the product over classes of the product over features of its score in a specific class and feature (see equation. 2).
",3.3 Scoring function,[0],[0]
"Score(si, ⋂ j cj , F ) = ∏ j ∏ k Score(si, cj , fk)
(2)
",3.3 Scoring function,[0],[0]
The score of a sentence si in a specific class cj and feature fk is the sum of probability of the feature’s observations when si ∈ cj (see equation. 3).,3.3 Scoring function,[0],[0]
"We add one to the sum, to avoid multiplying by a features’ score of zero.
Score(si, cj , fk) = 1 + ∑ φ∈si
P (fk = φ|si ∈ cj) (3)
Where φ is an observation of the feature fk in the sentence si.",3.3 Scoring function,[0],[0]
"For example, assuming the feature f1 is term frequency, and we have a sentence: “I am studying at home.”.",3.3 Scoring function,[0],[0]
"The sentence after pre-processing would be: s1 = {“studi”(stem of “study”), “home”}.",3.3 Scoring function,[0],[0]
"So, φ may be “studi” or “home”, or any other term.",3.3 Scoring function,[0],[0]
"If we take another feature f2 which is sentence position, the observation φ may take 1st, 2nd, 3rd, etc. as values.",3.3 Scoring function,[0],[0]
"We use 5 statistical features to score the sentences: unigram term frequency (TFU), bigram term frequency (TFB), sentence position (Pos) and sentence length (Rleng, PLeng).
",3.4 Statistical features,[0],[0]
Each feature divides the sentences to several categories.,3.4 Statistical features,[0],[0]
"For example, if we have a text written just with three characters: a, b and c, and the feature is the characters of the text, then we will have three categories.",3.4 Statistical features,[0],[0]
"Each category has a probability to occur in a cluster, which is the number of its appearance in this cluster divided by all cluster’s terms, as shown in equation 4.
",3.4 Statistical features,[0],[0]
Pf (f = φ|cj) =,3.4 Statistical features,[0],[0]
|φ ∈ cj |∑,3.4 Statistical features,[0],[0]
"cl∈C |φ′ ∈ cl|
(4)
",3.4 Statistical features,[0],[0]
Where f is a given feature.,3.4 Statistical features,[0],[0]
φ and φ′ are observations (categories) of the feature f .,3.4 Statistical features,[0],[0]
C is the set of clusters.,3.4 Statistical features,[0],[0]
This feature is used to calculate the sentence pertinence depending on its terms.,3.4.1 Unigram term frequency,[0],[0]
Each term is considered as a category.,3.4.1 Unigram term frequency,[0],[0]
"This feature is similar to unigram term frequency, but instead of one term we use two consecutive terms.",3.4.2 Bigram term frequency,[0],[0]
We want to use sentence positions in the original texts as a feature.,3.4.3 Sentence position,[0],[0]
"The position feature used by Osborne (2002) divides the sentences into three
sets: the ones in the 8 first paragraphs, those in last 3 paragraphs and the others in between.",3.4.3 Sentence position,[0],[0]
"Following the assumption that the first sentences and last ones are more important than the others.
",3.4.3 Sentence position,[0],[0]
Three categories of sentence positions seem very small to express the diversity between the clusters.,3.4.3 Sentence position,[0],[0]
"Instead of just three categories, we divided the position space into 10 categories.",3.4.3 Sentence position,[0],[0]
"So, if we have 20 sentences, we will have 2 sentences per category.",3.4.3 Sentence position,[0],[0]
"One other feature applied in our system is the sentence length (number of words), which is used originally to penalize the short sentences.",3.4.4 Sentence length,[0],[0]
"Following a sentence’s length, we can put it in one of three categories: sentences with length less than 6 words, those with length more than 20 words, and those with length in between Osborne (2002).
",3.4.4 Sentence length,[0],[0]
"Like sentence position, three categories is a small number.",3.4.4 Sentence length,[0],[0]
"Therefore, we used each length as a category.",3.4.4 Sentence length,[0],[0]
"Suppose we have 4 sentences which the lengths are: 5, 6, 5 and 7, then we will have 3 categories of lengths: 5, 6 and 7.
",3.4.4 Sentence length,[0],[0]
"In our work, we use two types of sentence length:
• Real length (RLeng): which is the length of the sentence without removing stop-words.
",3.4.4 Sentence length,[0],[0]
• Pre-processed length (PLeng): which is the length of the sentence after pre-processing.,3.4.4 Sentence length,[0],[0]
"To extract sentences, we reorder them decreasingly using their scores.",3.5 Summary extraction,[0],[0]
Then we extract the first non similar sentences until we get the wanted size (see algorithm 2).,3.5 Summary extraction,[0],[0]
"In this section, we describe how the summarization parameters have been chosen.
",4 Summarization parameters,[0],[0]
"The first parameter is the clustering threshold, which will lead to few huge clusters if it is small, and inversely.",4 Summarization parameters,[0],[0]
The clustering threshold is used with sentences’ similarities to decide if two sentences are similar or not.,4 Summarization parameters,[0],[0]
Our idea is to use statistic measures over those similarities to estimate the clustering threshold.,4 Summarization parameters,[0],[0]
"Eight measures have been used:
• The median
Algorithm 2: extraction method Data: input text Result: a summary add the first sentence to the summary; foreach sentence in the text do
calculate cosine similarity between this sentence and the last accepted one; if the simularity is under the threshold then
add this sentence to the summary; end if the sum of the summary size and the current sentence’s is above the maximum size then
delete this sentence from the summary;
end end
•",4 Summarization parameters,[0],[0]
The mean •,4 Summarization parameters,[0],[0]
"The mode which can be divided to two: lower
mode and higher mode, since we can have many modes.
",4 Summarization parameters,[0],[0]
"• The variance • sDn = ∑ |s|
|D|∗n
• Dsn = |D| n∗ ∑ |s|
• Ds = |D|∑ |s|",4 Summarization parameters,[0],[0]
"Where, |s| is the number of different terms in a sentence s. |D| is the number of different terms in the document D. n is the number of sentences in this document.
",4 Summarization parameters,[0],[0]
"The second parameter is the features’ set, which is the combination of at least one of the five features described in section 3.4.",4 Summarization parameters,[0],[0]
"We want to know which features are useful and which are not for a given language.
",4 Summarization parameters,[0],[0]
"To fix the problem of the clustering threshold and the set of features, we used the training sets provided by the workshop organizers.",4 Summarization parameters,[0],[0]
"For each document (or topic in multi-document), we generated summaries using the 8 measures of th, and different combinations of the scoring features.",4 Summarization parameters,[0],[0]
"Then, we calculated the average ROUGE-2 score for each language.",4 Summarization parameters,[0],[0]
"The threshold measure and the set of features that maximize this average will be used as parameters for the trained language.
",4 Summarization parameters,[0],[0]
Table 2 represents an example of the 10 languages and their parameters used for both tasks: MSS and MMS.,4 Summarization parameters,[0],[0]
We have to point out that the average is not always the best choice for the individual documents (or topic in multi-document).,4 Summarization parameters,[0],[0]
"For example, in MSS, there is a document which gives a ROUGE-2 score of 0.28 when we use the parameters based on average scores.",4 Summarization parameters,[0],[0]
"When we use the mean as threshold and just TFB as feature for the same document, we get a ROUGE-2 score of 0.31.",4 Summarization parameters,[0],[0]
"We participated in all workshop’s languages, either in single document or multi-document tasks.",5 Experiments,[0],[0]
"To compare our system to others participated systems, we followed these steps (for every evaluation metric):
• For each system, calculate the average scores of all used languages.
",5 Experiments,[0],[0]
"• For our system, calculate the average scores of used languages by others.",5 Experiments,[0],[0]
"For example, BGU-SCE-M team uses Arabic, English and Hebrew; We calculate the average of scores of these languages for this system and ours.
",5 Experiments,[0],[0]
"• Then, we calculate the relative improvement using the averages oursystem−othersystemothersystem .",5 Experiments,[0],[0]
"In “Single document summarization” task, ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004) is used to evaluate the participated systems.",5.1 Evaluation metrics,[0],[0]
It allows us to evaluate automatic text summaries against human made abstracts.,5.1 Evaluation metrics,[0],[0]
The principle of this method is to compare N-grams of two summaries based on the number of matches between these two based on the recall measure.,5.1 Evaluation metrics,[0],[0]
"Five metrics are used: ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4 and ROUGE-SU4.
",5.1 Evaluation metrics,[0],[0]
"In “Multi-document summarization” task, Three metrics are officially used: AutoSummENG, MeMoG (Giannakopoulos and Karkaletsis, 2011) and NPowER (Giannakopoulos and Karkaletsis, 2013).",5.1 Evaluation metrics,[0],[0]
"Besides our system (AllSummarizer), there are two more systems which participated in all 38 languages (EXB and CCS).",5.2 Single document summarization,[0],[0]
"Table 3 shows the comparison between our system and the other systems
in single document task, using the relative improvement.
",5.2 Single document summarization,[0],[0]
"Looking at these results, our system took the fifth place out of seven participants.",5.2 Single document summarization,[0],[0]
It outperforms the Lead baseline.,5.2 Single document summarization,[0],[0]
It took the last place out of three participants in all 38 languages.,5.2 Single document summarization,[0],[0]
"Besides our system (AllSummarizer), there are 4 systems that participated with all the 10 languages.",5.3 Multi-document summarization,[0],[0]
"Table 4 shows a comparison between our system and the other systems in multi-document task, using the relative improvement.",5.3 Multi-document summarization,[0],[0]
"We used the parameters fixed for single document summarization to see if the same parameters are applicable for both single and multi-document summarizations.
",5.3 Multi-document summarization,[0],[0]
"Looking to the results, our system took the seventh place out of ten participants.",5.3 Multi-document summarization,[0],[0]
"When we use single document parameters, we can see that it doesn’t outperform the results when using the parameters fixed for multi-document summarization.",5.3 Multi-document summarization,[0],[0]
This shows that we can’t use the same parameters for both single and multi-document summarization.,5.3 Multi-document summarization,[0],[0]
Our intension is to create a method which is language and domain independent.,6 Conclusion,[0],[0]
"So, we consider the input text as a set of topics, where a sentence can belong to many topics.",6 Conclusion,[0],[0]
We calculated how much a sentence can represent all the topics.,6 Conclusion,[0],[0]
"Then, the score is used to reorder the sentences and extract the first non redundant ones.
",6 Conclusion,[0],[0]
"We tested our system using the average score of all languages, in single and multi-document summarization.",6 Conclusion,[0],[0]
"Compared to other systems, it affords fair results, but more improvements have to be done in the future.",6 Conclusion,[0],[0]
We have to point out that our system participated in all languages.,6 Conclusion,[0],[0]
"Also, it is easy to add new languages when you can afford tokenization and stemming.
",6 Conclusion,[0],[0]
We fixed the parameters (threshold and features) based on the average score of ROUGE-2 of all training documents.,6 Conclusion,[0],[0]
Further investigations must be done to estimate these parameters for each document based on statistical criteria.,6 Conclusion,[0],[0]
We want to investigate the effect of the preprocessing step and the clustering methods on the resulted summaries.,6 Conclusion,[0],[0]
"Finally, readability remains a challenge for extractive methods, especially when we want to use a multilingual method.",6 Conclusion,[0],[0]
"In this paper, we evaluate our automatic text summarization system in multilingual context.",abstractText,[0],[0]
We participated in both single document and multi-document summarization tasks of MultiLing 2015 workshop.,abstractText,[0],[0]
Our method involves clustering the document sentences into topics using a fuzzy clustering algorithm.,abstractText,[0],[0]
Then each sentence is scored according to how well it covers the various topics.,abstractText,[0],[0]
"This is done using statistical features such as TF, sentence length, etc.",abstractText,[0],[0]
"Finally, the summary is constructed from the highest scoring sentences, while avoiding overlap between the summary sentences.",abstractText,[0],[0]
"This makes it language-independent, but we have to afford preprocessed data first (tokenization, stemming, etc.).",abstractText,[0],[0]
AllSummarizer system at MultiLing 2015: Multilingual single and multi-document summarization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 20–25 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2004
MT evaluation metrics are tested for correlation with human judgments either at the sentence- or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives.",text,[0],[0]
"Ever since BLEU (Papineni et al., 2002) many proposals for an improved automatic evaluation metric for Machine Translation (MT) have been made.",1 Introduction,[0],[0]
"Some proposals use additional information for extracting quality indicators, like paraphrasing (Denkowski and Lavie, 2011), syntactic trees (Liu and Gildea, 2005; Stanojević and Sima’an, 2015) or shallow semantics (Rios et al., 2011; Lo et al., 2012) etc.",1 Introduction,[0],[0]
"Whereas others use different matching strategies, like n-grams (Papineni et al., 2002), treelets (Liu and Gildea, 2005) and skip-bigrams (Lin and Och, 2004).",1 Introduction,[0],[0]
"Most metrics use several indicators of translation quality which are often combined in a linear model whose weights are estimated on a training set of human judgments.
",1 Introduction,[0],[0]
"Because the most widely available type of human judgments are relative ranking (RR) judgments, the main machine learning method used for training the metrics were based on the learningto-rank framework (Li, 2011).",1 Introduction,[0],[0]
"While the effectiveness of this framework for training evaluation metrics has been confirmed many times, e.g., (Ye et al., 2007; Duh, 2008; Stanojević and Sima’an, 2014; Ma et al., 2016), so far there is no prior work exploring alternative objective functions for training learning-to-rank models.",1 Introduction,[0],[0]
"Without exception, all existing learning-to-rank models are trained to rank sentences while completely ignoring the corpora judgments, likely because human judgments come in the form of sentence rankings.
",1 Introduction,[0],[0]
It might seem that sentence and corpus level tasks are very similar but that is not the case.,1 Introduction,[0],[0]
Empirically it has been shown that many metrics that perform well on the sentence level do not perform well on the corpus level and vice versa.,1 Introduction,[0],[0]
"By training to rank sentences the model does not necessarily learn to give scores that are well scaled, but only to give higher scores to better translations.",1 Introduction,[0],[0]
"Training for the corpus level score would force the metric to give well scaled scores on the sentence level.
",1 Introduction,[0],[0]
Human judgments of sentences can be aggregated in different ways to hypothesize human judgments of full corpora.,1 Introduction,[0],[0]
"However, this fact has not been used so far to train learning-to-rank models that are good for ranking different corpora.
",1 Introduction,[0],[0]
This work fills-in this gap by exploring the merits of different objective functions that take corpus level judgments into consideration.,1 Introduction,[0],[0]
We first create a learning-to-rank model for ranking corpora and compare it to the standard learning-to-rank model that is trained for ranking sentences.,1 Introduction,[0],[0]
This comparison shows that performance of these two objectives can vary radically depending on the chosen meta-evaluation method.,1 Introduction,[0],[0]
"To tackle this prob-
20
lem we contribute a new objective function, inspired by multi-task learning, in which we train for both objectives simultaneously.",1 Introduction,[0],[0]
This multiobjective model behaves a lot more stable over all methods of meta-evaluation and achieves a higher correlation than both single objective models.,1 Introduction,[0],[0]
"All the models that we define have one basic function in common, we call it a forward(·) function, that maps the features of any sentence to a single real number.",2 Models,[0],[0]
"That function can be any differentiable function including multi-layer neural networks as in (Ma et al., 2016), but here we will stick with the standard linear model:
forward(φ) = φTw + b
Here φ is a vector with feature values of a sentence, w is a weight vector and b is a bias term.",2 Models,[0],[0]
"Usually in training we would like to process a mini-batch of feature vectors Φ, where Φ is a matrix in which each column is a feature vector of individual sentence in the mini-batch or in the corpus.",2 Models,[0],[0]
"By using broadcasting we can rewrite the previous definition of the forward(·) function as:
forward(Φ) = ΦTw + b
Now we can define the score of a sentence as a sigmoid function applied over the output of the forward(·) function because we want to get a score between 0 and 1:
sentScore(φ) = σ(forward(φ))
",2 Models,[0],[0]
"As the corpus level score we will use just the average of sentence level scores:
corpScore(Φ) = 1
m
∑ sentScore(Φ)
where m is the number of sentences in the corpus.",2 Models,[0],[0]
Next we present several objective functions that are illustrated by the computation graph in Figure 1.,2 Models,[0],[0]
"Here we use the training objective very similar to BEER (Stanojević and Sima’an, 2014) which is a learning-to-rank framework that finds a separating hyper-plane between “good” and “bad” translations.",2.1 Training for Sentence Level Accuracy,[0],[0]
"Unlike BEER, we use a max-margin objective instead of logistic regression.
",2.1 Training for Sentence Level Accuracy,[0],[0]
For each mini-batch we randomly select m human relative ranking pairwise judgments and after extracting features for all the sentences taking part in these judgments we put features in two matrices Φswin and Φslos.,2.1 Training for Sentence Level Accuracy,[0],[0]
"These matrices are structured in such a way that for judgment i the column i in Φswin contains the features of the “good” translation in the judgment and the column i in Φslos the features of the “bad” translation.
",2.1 Training for Sentence Level Accuracy,[0],[0]
We would like to maximize the average margin that would separate sentence level scores of pairs of translations in each judgment.,2.1 Training for Sentence Level Accuracy,[0],[0]
"Because the squashing sigmoid function does not influence the ranking we can directly optimize on the unsquashed forward pass and require that the margin between “good” and “bad” translation is at least 1:
∆sent = forward(Φswin)− forward(Φslos)
",2.1 Training for Sentence Level Accuracy,[0],[0]
"LossSent = 1
m
∑ max(0, 1−∆sent)",2.1 Training for Sentence Level Accuracy,[0],[0]
At the corpus level we would like to do a similar thing as on the sentence level: maximize the distance between the scores of “good” and “bad” corpora.,2.2 Training for Corpus Level Accuracy,[0],[0]
"In this case we have additional information that is not present on the sentence level: we know not only which corpus is (according to humans) better, but also by how much it is better.",2.2 Training for Corpus Level Accuracy,[0],[0]
"For
that we can use one of the heuristics such as the Expected Wins (Koehn, 2012).",2.2 Training for Corpus Level Accuracy,[0],[0]
"We can use this information to guide the learning model by how much it should separate the scores of two corpora.
",2.2 Training for Corpus Level Accuracy,[0],[0]
"For doing this we use an approach similar to Max-Margin Markov Networks (Taskar et al., 2003) where for each training instance we dynamically scale the margin that should be enforced.",2.2 Training for Corpus Level Accuracy,[0],[0]
We want the margin between the scores ∆corp to be at least as big as the margin between the human scores ∆human assigned to these systems.,2.2 Training for Corpus Level Accuracy,[0],[0]
In one mini-batch we will use only a randomly chosen pair of corpora with feature matrices Φcwin and Φclos for which we have a human comparison.,2.2 Training for Corpus Level Accuracy,[0],[0]
"The corpus level loss function is given by:
∆corp = corpScore(Φcwin)− corpScore(Φclos) LossCorp = max(0,∆human −∆corp)",2.2 Training for Corpus Level Accuracy,[0],[0]
"In this model we optimize both objectives jointly in the style of multi-task learning (Caruana, 1997).",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
"Here we employ the simplest approach of just tasking the interpolation of the previously introduced loss functions.
",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
LossJoint = α · LossSent + (1− α) ·,2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
"LossCorp
The interpolation is controlled by the hyperparameter α which could in principle be tuned for good performance, but here we just fix it to 0.5 to give both objectives equal importance.",2.3 Training Jointly for Sentence and Corpus Level Accuracy,[0],[0]
The feature functions that are used are reimplementation of many (but not all) feature functions of BEER.,2.4 Feature Functions,[0],[0]
"Because the point of this paper is about the exploration of different objective functions we did not try to experiment with more complex feature functions based on paraphrasing, function words or permutation trees.
",2.4 Feature Functions,[0],[0]
"We use just simple precision, recall and 3 types of F-score (with β parameters 1, 2 and 0.5) over different “pieces” of translation:
• character n-grams of orders 1,2,3,4 and 5 • word n-grams of orders 1,2,3 and 4 • skip-bigrams of maximum skip 2 and ∞
(similar to ROUGE-S2 and ROUGE-S* (Lin and Och, 2004))
",2.4 Feature Functions,[0],[0]
One final feature deals with length-disbalance.,2.4 Feature Functions,[0],[0]
"If the length of the system and reference translation are a and b respectively then this feature is computed as max(a,b)−min(a,b)min(a,b) .",2.4 Feature Functions,[0],[0]
It is computed both for word and character length.,2.4 Feature Functions,[0],[0]
"Experiments are conducted on WMT13 (Macháček and Bojar, 2013), WMT14 (Machacek and Bojar, 2014) and WMT16 (Bojar et al., 2016) datasets which were used as training, validation and testing datasets respectively.
",3 Experiments,[0],[0]
All of the models are implemented using TensorFlow1 and trained with L2 regularization λ = 0.001 and ADAM optimizer with learning rate 0.001.,3 Experiments,[0],[0]
The mini-batch size for sentence level judgments is 2000 and for the corpus level is one comparison.,3 Experiments,[0],[0]
"Each model is trained for 200 epochs out of which the one performing best on the validation set for the objective function being optimized is used during the test time.
",3 Experiments,[0],[0]
We show the results for the relative ranking (RR) judgments correlation in Table 1.,3 Experiments,[0],[0]
"For all language pairs that are of the form en-X we show it under the column X and for all the language pairs that have English on the target side we present their average under the column en.
",3 Experiments,[0],[0]
"RR corpus vs. sentence objective The corpusobjective is better than the sentence-objective for both corpus and sentence level RR judgments on 5 out of 7 languages and also on average correlation.
",3 Experiments,[0],[0]
"RR joint vs. single-objectives Training for the joint objective improves even more on both levels of RR correlation and outperforms both singleobjective models on average and on 4 out of 7 languages.
",3 Experiments,[0],[0]
"Making confident conclusions from these results is difficult because, to the best of our knowledge, there is no principled way of measuring statistical significance on the RR judgments.",3 Experiments,[0],[0]
That is why we also tested on direct assessment (DA) judgments available from WMT16.,3 Experiments,[0],[0]
"On DA we can measure statistical significance on the sentence level using Williams test (Graham et al., 2015) and on the corpus level using combination of hybrid-supersampling and Williams test (Graham and Liu, 2016).",3 Experiments,[0],[0]
"The results of correlation with human judgment are for sentence and corpus level are shown in Table 2.
1https://www.tensorflow.org/
DA corpus vs. other objectives On DA judgments the results for corpus level objective are completely different than on the RR judgments.",3 Experiments,[0],[0]
"On DA judgments the corpus-objective model is significantly outperformed on both levels and on all languages by both of the other objectives.
",3 Experiments,[0],[0]
This shows that gambling on one objective function (being that sentence or corpus level objective) could give unpredictable results.,3 Experiments,[0],[0]
"This is precisely the motivation for creating the joint model with multi-objective training.
",3 Experiments,[0],[0]
DA joint vs. single objectives By choosing to jointly optimize both objectives we get a much more stable model that performs well both on DA and RR judgments and on both levels of judgment.,3 Experiments,[0],[0]
"On the DA sentence level, the joint model was not outperformed by any other model and on 3 out of 7 language pairs it significantly outperforms both alternative objectives.",3 Experiments,[0],[0]
"On the corpus level results are
a bit mixed, but still joint objective outperforms both other models on 4 out of 7 language pairs and also it gives higher correlation on average.",3 Experiments,[0],[0]
In this work we found that altering the objective function for training MT metrics can have radical effects on performance.,4 Conclusion,[0],[0]
Also the effects of the objective functions can sometimes be unexpected: the sentence objective might not be good for sentence level correlation (in case of RR judgments) and the corpus objective might not be good for corpus level correlation (in case of DA judgments).,4 Conclusion,[0],[0]
"The difference among objectives is better explained by different types of human judgments: the corpus objective is better for RR while sentence objective is better for DA judgments.
",4 Conclusion,[0],[0]
"Finally, the best results are achieved by training for both objectives at the same time.",4 Conclusion,[0],[0]
"This gives
an evaluation metric that is far more stable in its performance over all methods of meta-evaluation.",4 Conclusion,[0],[0]
"This work is supported by NWO VICI grant nr. 277-89-002, DatAptor project STW grant nr. 12271 and QT21 project H2020 nr. 645452.",Acknowledgments,[0],[0]
MT evaluation metrics are tested for correlation with human judgments either at the sentenceor the corpus-level.,abstractText,[0],[0]
Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only.,abstractText,[0],[0]
"We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized.",abstractText,[0],[0]
"To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentencelevel exemplifying how their performance may vary per language pair, type and level of judgment.",abstractText,[0],[0]
Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than–and on average outperforms– both models on both objectives.,abstractText,[0],[0]
Alternative Objective Functions for Training MT Evaluation Metrics,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1831–1841 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1831",text,[0],[0]
"Over the past few years, Abstract Meaning Representations (AMRs, Banarescu et al. (2013)) have become a popular target representation for semantic parsing.",1 Introduction,[0],[0]
AMRs are graphs which describe the predicate-argument structure of a sentence.,1 Introduction,[0],[0]
"Because they are graphs and not trees, they can capture reentrant semantic relations, such as those induced by control verbs and coordination.",1 Introduction,[0],[0]
"However, it is technically much more challenging to parse a string into a graph than into a tree.",1 Introduction,[0],[0]
"For instance, grammar-based approaches (Peng et al., 2015; Artzi et al., 2015) require the induction of a grammar from the training corpus, which is hard because graphs can be decomposed into smaller pieces in far more ways than trees.",1 Introduction,[0],[0]
"Neural sequence-to-sequence models, which do very well on string-to-tree parsing (Vinyals et al., 2014), can be applied to AMRs but face the challenge that graphs cannot easily be represented as sequences (van Noord and Bos, 2017a,b).
",1 Introduction,[0],[0]
"In this paper, we tackle this challenge by making the compositional structure of the AMR explicit.",1 Introduction,[0],[0]
"As in our previous work, Groschwitz et al. (2017), we view an AMR as consisting of atomic graphs representing the meanings of the individual words,
which were combined compositionally using linguistically motivated operations for combining a head with its arguments and modifiers.",1 Introduction,[0],[0]
We represent this structure as terms over the AM algebra as defined in Groschwitz et al. (2017).,1 Introduction,[0],[0]
"This previous work had no parser; here we show that the terms of the AM algebra can be viewed as dependency trees over the string, and we train a dependency parser to map strings into such trees, which we then evaluate into AMRs in a postprocessing step.",1 Introduction,[0],[0]
"The dependency parser relies on type information, which encodes the semantic valencies of the atomic graphs, to guide its decisions.
",1 Introduction,[0],[0]
"More specifically, we combine a neural supertagger for identifying the elementary graphs for the individual words with a neural dependency model along the lines of Kiperwasser and Goldberg (2016) for identifying the operations of the algebra.",1 Introduction,[0],[0]
One key challenge is that the resulting term of the AM algebra must be semantically well-typed.,1 Introduction,[0],[0]
This makes the decoding problem NP-complete.,1 Introduction,[0],[0]
"We present two approximation algorithms: one which takes the unlabeled dependency tree as given, and one which assumes that all dependencies are projective.",1 Introduction,[0],[0]
"We evaluate on two data sets, achieving state-of-the-art results on one and near state-of-theart results on the other (Smatch f-scores of 71.0 and 70.2 respectively).",1 Introduction,[0],[0]
"Our approach clearly outperforms strong but non-compositional baselines.
",1 Introduction,[0],[0]
Plan of the paper.,1 Introduction,[0],[0]
"After reviewing related work in Section 2, we explain the AM algebra in Section 3 and extend it to a dependency view in Section 4.",1 Introduction,[0],[0]
We explain model training in Section 5 and decoding in Section 6.,1 Introduction,[0],[0]
Section 7 evaluates a number of variants of our system.,1 Introduction,[0],[0]
"Recently, AMR parsing has generated considerable research activity, due to the availability of large-
scale annotated data (Banarescu et al., 2013) and two successful SemEval Challenges (May, 2016; May and Priyadarshi, 2017).
",2 Related Work,[0],[0]
Methods from dependency parsing have been shown to be very successful for AMR parsing.,2 Related Work,[0],[0]
"For instance, the JAMR parser (Flanigan et al., 2014, 2016) distinguishes concept identification (assigning graph fragments to words) from relation identification (adding graph edges which connect these fragments), and solves the former with a supertagging-style method and the latter with a graph-based dependency parser.",2 Related Work,[0],[0]
"Foland and Martin (2017) use a variant of this method based on an intricate neural model, yielding state-of-the-art results.",2 Related Work,[0],[0]
"We go beyond these approaches by explicitly modeling the compositional structure of the AMR, which allows the dependency parser to combine AMRs for the words using a small set of universal operations, guided by the types of these AMRs.
",2 Related Work,[0],[0]
"Other recent methods directly implement a dependency parser for AMRs, e.g. the transitionbased model of Damonte et al. (2017), or postprocess the output of a dependency parser by adding missing edges (Du et al., 2014; Wang et al., 2015).",2 Related Work,[0],[0]
"In contrast to these, our model makes no strong assumptions on the dependency parsing algorithm that is used; here we choose that of Kiperwasser and Goldberg (2016).
",2 Related Work,[0],[0]
"The commitment of our parser to derive AMRs compositionally mirrors that of grammar-based AMR parsers (Artzi et al., 2015; Peng et al., 2015).",2 Related Work,[0],[0]
"In particular, there are parallels between the types we use in the AM algebra and CCG categories (see Section 3 for details).",2 Related Work,[0],[0]
"As a neural system, our parser struggles less with coverage issues than these, and avoids the complex grammar induction process these models require.
",2 Related Work,[0],[0]
"More generally, our use of semantic types to restrict our parser is reminiscent of Kwiatkowski et al. (2010), Krishnamurthy et al. (2017) and Zhang et al. (2017), and the idea of deriving semantic representations from dependency trees is also present in Reddy et al. (2017).",2 Related Work,[0],[0]
"A core idea of this paper is to parse a string into a graph by instead parsing a string into a dependencystyle tree representation of the graph’s compositional structure, represented as terms of the ApplyModify (AM) algebra (Groschwitz et al., 2017).
",3 The AM algebra,[0],[0]
"The values of the AM algebra are annotated s-
graphs, or as-graphs: directed graphs with node and edge labels in which certain nodes have been designated as sources (Courcelle and Engelfriet, 2012) and annotated with type information.",3 The AM algebra,[0],[0]
Some examples of as-graphs are shown in Fig. 1.,3 The AM algebra,[0],[0]
"Each as-graph has exactly one root, indicated by the bold outline.",3 The AM algebra,[0],[0]
"The sources are indicated by red labels; for instance, Gwant has an S-source and an O-source.",3 The AM algebra,[0],[0]
"The annotations, written in square brackets behind the red source names, will be explained below.",3 The AM algebra,[0],[0]
"We use these sources to mark open argument slots; for example, Gsleep in Fig. 1 represents an intransitive verb, missing its subject, which will be added at the S-source.
",3 The AM algebra,[0],[0]
The AM algebra can combine as-graphs with each other using two linguistically motivated operations: apply and modify.,3 The AM algebra,[0],[0]
Apply (APP) adds an argument to a predicate.,3 The AM algebra,[0],[0]
"For example, we can add a subject – the graph Gwriter in Fig. 1 – to the graph GVP in Fig. 2d using APPS, yielding the complete AMR in Fig. 2b.",3 The AM algebra,[0],[0]
"Linguistically, this is like filling the subject (S) slot of the predicate wants to sleep soundly with the argument the writer.",3 The AM algebra,[0],[0]
"In general, for a source a, APPa(GP , GA), combines the asgraph GP representing a predicate, or head, with the as-graph GA, which represents an argument.",3 The AM algebra,[0],[0]
"It does this by plugging the root node of GA into the a-source u of GP – that is, the node u of GP marked with source a.",3 The AM algebra,[0],[0]
"The root of the resulting as-graph G is the root of GP , and we remove the a marking on u, since that slot is now filled.
",3 The AM algebra,[0],[0]
The modify operation (MOD) adds a modifier to a graph.,3 The AM algebra,[0],[0]
"For example, we can combine two elementary graphs from Fig. 1 with MODm (Gsleep, Gsound), yielding the graph in Fig. 2c.",3 The AM algebra,[0],[0]
The Msource of the modifier Gsoundly attaches to the root of Gsleep.,3 The AM algebra,[0],[0]
The root of the result is the same as the root of Gsleep in the same sense that a verb phrase with an adverb modifier is still a verb phrase.,3 The AM algebra,[0],[0]
"In general, MODa(GH , GM ), combines a head GH with a modifier GM .",3 The AM algebra,[0],[0]
It plugs the root of GH into the a-source u of GM .,3 The AM algebra,[0],[0]
"Although this may add incoming edges to the root of GH , that node is still
the root of the resulting graph G. We remove the a marking from GM .
",3 The AM algebra,[0],[0]
"In both APP and MOD, if there is any other source b which is present in both graphs, the nodes marked with b are unified with each other.",3 The AM algebra,[0],[0]
"For example, when Gwant is O-applied to t1 in Fig. 2d, the S-sources of the graphs for “want” and “sleep soundly” are unified into a single node, creating a reentrancy.",3 The AM algebra,[0],[0]
"This falls out of the definition of merge for s-graphs which formally underlies both operations (see (Courcelle and Engelfriet, 2012)).
",3 The AM algebra,[0],[0]
"Finally, the AM algebra uses types to restrict its operations.",3 The AM algebra,[0],[0]
"Here we define the type of an as-graph as the set of its sources with their annotations1; thus for example, in Fig. 1, the graph for “writer” has the empty type [ ],Gsleep has type [S], andGwant has type [S, O[S]].",3 The AM algebra,[0],[0]
Each source in an as-graph specifies with its annotation the type of the as-graph which is plugged into it via APP.,3 The AM algebra,[0],[0]
"In other words, for a source a, we may only a-apply GP with GA if the annotation of the a-source in GP matches the type of GA.",3 The AM algebra,[0],[0]
"For example, the O-source of Gwants (Fig. 1) requires that we plug in an as-graph of type [S]; observe that this means that the reentrancy in Fig.",3 The AM algebra,[0],[0]
2b is lexically specified by the control verb “want”.,3 The AM algebra,[0],[0]
"All other source nodes in Fig. 1 have no annotation, indicating a type requirement of [ ].
",3 The AM algebra,[0],[0]
"Linguistically, modification is optional; we therefore want the modified graph to be derivationally just like the unmodified graph, in that exactly the same operations can apply to it.",3 The AM algebra,[0],[0]
"In a typed algebra, this means MOD should not change the type of the head.",3 The AM algebra,[0],[0]
"MODa therefore requires that the modifier GM have no sources not already present in the head GH , except a, which will be deleted anyway.
",3 The AM algebra,[0],[0]
"As in any algebra, we can build terms from constants (denoting elementary as-graphs) by recursively combining them with the operations of the AM algebra.",3 The AM algebra,[0],[0]
"By evaluating the operations bottomup, we obtain an as-graph as the value of such a term; see Fig. 2 for an example.",3 The AM algebra,[0],[0]
"However, as discussed above, an operation in the term may be undefined due to a type mismatch.",3 The AM algebra,[0],[0]
We call an AMterm well-typed if all its operations are defined.,3 The AM algebra,[0],[0]
Every well-typed AM-term evaluates to an as-graph.,3 The AM algebra,[0],[0]
"Since the applicability of an AM operation depends only on the types, we also write τ = f(τ1, τ2) if as-graphs of type τ1 and τ2 can be combined with the operation f and the result has type τ .
1See (Groschwitz et al., 2017) for a more formally complete definition.
",3 The AM algebra,[0],[0]
Relationship to CCG.,3 The AM algebra,[0],[0]
There is close relationship between the types of the AM algebra and the categories of CCG.,3 The AM algebra,[0],[0]
"A type [S, O] specifies that the as-graph needs to be applied to two arguments to be semantically complete, similar a CCG category such as S\NP/NP, where a string needs to be applied to two NP arguments to be syntactically complete.",3 The AM algebra,[0],[0]
"However, AM types govern the combination of graphs, while CCG categories control the combination of strings.",3 The AM algebra,[0],[0]
"This relieves AM types of the need to talk about word order; there are no “forward” or “backward” slashes in AM types, and a smaller set of operations.",3 The AM algebra,[0],[0]
"Also, the AM algebra spells out raising and control phenomena more explicitly in the types.",3 The AM algebra,[0],[0]
"In this paper, we connect AM terms to the input string w for which we want to produce a graph.",4 Indexed AM terms,[0],[0]
"We do this in an indexed AM term, exemplified in Fig. 3a.",4 Indexed AM terms,[0],[0]
"We assume that every elementary as-graph G at a leaf represents the meaning of an individual word token wi in w, and write G[i] to annotate the leaf G with the index i of this token.",4 Indexed AM terms,[0],[0]
"This induces a connection between the nodes of the AMR and the tokens of the string, in that the label of each node was contributed by the elementary as-graph of exactly one token.
",4 Indexed AM terms,[0],[0]
We define the head index of a subtree t to be the index of the token which contributed the root of the as-graph to which t evaluates.,4 Indexed AM terms,[0],[0]
"For a leaf with annotation i, the head index is i; for an APP or MOD node, the head index is the head index of the left child, i.e. of the head argument.",4 Indexed AM terms,[0],[0]
We annotate each APP and MOD operation with the head index of the left and right subtree.,4 Indexed AM terms,[0],[0]
"We can represent indexed AM terms more compactly as AM dependency trees, as shown in Fig.",4.1 AM dependency trees,[0],[0]
3b.,4.1 AM dependency trees,[0],[0]
The nodes of such a dependency tree are the tokens of w. We draw an edge with label f from i to k if there is a node with label f,4.1 AM dependency trees,[0],[0]
"[i, k] in the indexed AM term.",4.1 AM dependency trees,[0],[0]
"For example, the tree in 3b has an edge labeled MODm from 5 (Gsleep) to 6 (Gsoundly) because there is a node in the term in 3a labeled MODm[5, 6].",4.1 AM dependency trees,[0],[0]
"The same AM dependency tree may represent multiple indexed AM terms, because the order of apply and modify operations is not specified in the dependency tree.",4.1 AM dependency trees,[0],[0]
"However, it can be shown that all well-typed AM terms that map to
APPs[3,2]
Gwant[3]
APPo[3,5]
MODm[5,6]
Gsleep[5] Gsoundly[6]
Gwriter[2]
(a)
2: Gwriter
6: Gsoundly
4: ⊥5: Gsleep
AP P s AP P o
IGNORE
M O
D m
(b)
1: ⊥
IGNORE
3: Gwant
Figure 3: (a) An indexed AM term and (b) an AM dependency tree, linking the term in Fig.",4.1 AM dependency trees,[0],[0]
"2;a to the sentence “The writer wants to sleep soundly”.
",4.1 AM dependency trees,[0],[0]
the same AM dependency tree evaluate to the same as-graph.,4.1 AM dependency trees,[0],[0]
"We define a well-typed AM dependency tree as one that represents a well-typed AM term.
",4.1 AM dependency trees,[0],[0]
"Because not all words in the sentence contribute to the AMR, we include a mechanism for ignoring words in the input.",4.1 AM dependency trees,[0],[0]
"As a special case, we allow the constant ⊥, which represents a dummy as-graph (of type ⊥) which we use as the semantic value of words without a semantic value in the AMR.",4.1 AM dependency trees,[0],[0]
"We furthermore allow the edge label IGNORE in an AM dependency tree, where IGNORE(τ1, τ2) = τ1 if τ2 = ⊥ and is undefined otherwise; in particular, an AM dependency tree with IGNORE edges is only well-typed if all IGNORE edges point into ⊥ nodes.",4.1 AM dependency trees,[0],[0]
"We keep all other operations f(τ1, τ2) as is, i.e. they are undefined if either τ1 or τ2 is⊥, and never yield ⊥ as a result.",4.1 AM dependency trees,[0],[0]
"When reconstructing an AM term from the AM dependency tree, we skip IGNORE edges, such that the subtree below them will not contribute to the overall AMR.",4.1 AM dependency trees,[0],[0]
"In order to train a model that parses sentences into AM dependency trees, we need to convert an AMR corpus – in which sentences are annotated with AMRs – into a treebank of AM dependency trees.",4.2 Converting AMRs to AM terms,[0],[0]
"We do this in three steps: first, we break each AMR up into elementary graphs and identify their roots; second, we assign sources and annotations to make elementary as-graphs out of them; and third, combine them into indexed AM terms.
",4.2 Converting AMRs to AM terms,[0],[0]
"For the first step, an aligner uses hand-written heuristics to identify the string token to which each
node in the AMR corresponds (see Section C in the Supplementary Materials for details).",4.2 Converting AMRs to AM terms,[0],[0]
"We proceed in a similar fashion as the JAMR aligner (Flanigan et al., 2014), i.e. by starting from high-confidence token-node pairs and then extending them until the whole AMR is covered.",4.2 Converting AMRs to AM terms,[0],[0]
"Unlike the JAMR aligner, our heuristics ensure that exactly one node in each elementary graph is marked as the root, i.e. as the node where other graphs can attach their edges through APP and MOD.",4.2 Converting AMRs to AM terms,[0],[0]
"When an edge connects nodes of two different elementary graphs, we use the “blob decomposition” algorithm of Groschwitz et al. (2017) to decide to which elementary graph it belongs.",4.2 Converting AMRs to AM terms,[0],[0]
"For the example AMR in Fig. 2b, we would obtain the graphs in Fig. 1 (without source annotations).",4.2 Converting AMRs to AM terms,[0],[0]
"Note that ARG edges belong with the nodes at which they start, whereas the “manner” edge in Gsoundly goes with its target.
",4.2 Converting AMRs to AM terms,[0],[0]
In the second step we assign source names and annotations to the unlabeled nodes of each elementary graph.,4.2 Converting AMRs to AM terms,[0],[0]
Note that the annotations are crucial to our system’s ability to generate graphs with reentrancies.,4.2 Converting AMRs to AM terms,[0],[0]
"We mostly follow the algorithm of Groschwitz et al. (2017), which determines necessary annotations based on the structure of the given graph.",4.2 Converting AMRs to AM terms,[0],[0]
The algorithm chooses each source name depending on the incoming edge label.,4.2 Converting AMRs to AM terms,[0],[0]
"For instance, the two leaves of Gwant can have the source labels S and O because they have incoming edges labeled ARG0 and ARG1.",4.2 Converting AMRs to AM terms,[0],[0]
"However, the Groschwitz algorithm is not deterministic: It allows object promotion (the sources for an ARG3 edge may be O3, O2, or O), unaccusative subjects (promoting the minimal object to S if the elementary graph contains an ARGi-edge (i > 0) but no ARG0-edge (Perlmutter, 1978)), and passive alternation (swapping O and S).",4.2 Converting AMRs to AM terms,[0],[0]
"To make our as-graphs more consistent, we prefer constants that promote objects as far as possible, use unaccusative subjects, and no passive alternation, but still allow constants that do not satisfy these conditions if necessary.",4.2 Converting AMRs to AM terms,[0],[0]
"This increased our Smatch score significantly.
",4.2 Converting AMRs to AM terms,[0],[0]
"Finally, we choose an arbitrary AM dependency
tree that combines the chosen elementary as-graphs into the annotated AMR; in practice, the differences between the trees seem to be negligible.2",4.2 Converting AMRs to AM terms,[0],[0]
"We can now model the AMR parsing task as the problem of computing the best well-typed AM dependency tree t for a given sentence w. Because t is well-typed, it can be decoded into an (indexed) AM term and thence evaluated to an as-graph.
",5 Training,[0],[0]
We describe t in terms of the elementary asgraphs G[i],5 Training,[0],[0]
it uses for each token i and of its edges,5 Training,[0],[0]
f,5 Training,[0],[0]
"[i, k].",5 Training,[0],[0]
"We assume a node-factored, edge-factored model for the score ω(t) of t:
ω(t) = ∑
1≤i≤n ω(G[i])",5 Training,[0],[0]
+ ∑,5 Training,[0],[0]
f,5 Training,[0],[0]
"[i,k]∈E ω(f",5 Training,[0],[0]
"[i, k]), (1)
where the edge weight further decomposes into the sum ω(f",5 Training,[0],[0]
"[i, k])",5 Training,[0],[0]
= ω(i → k) + ω(f,5 Training,[0],[0]
| i → k) of a score ω(i→ k) for the presence of an edge from i to k and a score ω(f | i→ k) for this edge having label f .,5 Training,[0],[0]
"Our aim is to compute the well-typed t with the highest score.
",5 Training,[0],[0]
We present three models for ω: one for the graph scores and two for the edge scores.,5 Training,[0],[0]
"All of these are based on a two-layer bidirectional LSTM, which reads inputs x =",5 Training,[0],[0]
"(x1, . . .",5 Training,[0],[0]
", xn) token by token, concatenating the hidden states of the forward and the backward LSTMs in each layer.",5 Training,[0],[0]
"On the second layer, we thus obtain vector representations vi = BiLSTM(x, i) for the individual input tokens (see Fig. 4).",5 Training,[0],[0]
Our models differ in the inputs x and the way they predict scores from the vi.,5 Training,[0],[0]
"We construe the prediction of the as-graphs G[i] for each input position i as a supertagging task (Lewis et al., 2016).",5.1 Supertagging for elementary as-graphs,[0],[0]
"The supertagger reads inputs xi = (wi, pi, ci), where wi is the word token, pi its POS tag, and ci is a character-based LSTM encoding of wi.",5.1 Supertagging for elementary as-graphs,[0],[0]
"We use pretrained GloVe embeddings (Pennington et al., 2014) concatenated with learned embeddings for wi, and learned embeddings for pi.
",5.1 Supertagging for elementary as-graphs,[0],[0]
"To predict the score for each elementary as-graph out of a set of K options, we add a K-dimensional output layer as follows:
ω(G[i]) = log softmax(W · vi + b) 2Indeed, we conjecture that for a fixed set of constants and
a fixed AMR, there is only one dependency tree.
and train the neural network using a cross-entropy loss function.",5.1 Supertagging for elementary as-graphs,[0],[0]
This maximizes the likelihood of the elementary as-graphs in the training data.,5.1 Supertagging for elementary as-graphs,[0],[0]
Predicting the edge scores amounts to a dependency parsing problem.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We chose the dependency parser of Kiperwasser and Goldberg (2016), henceforth K&G, to learn them, because of its accuracy and its fit with our overall architecture.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"The K&G parser scores the potential edge from i to k and its label from the concatenations of vi and vk:
MLPθ(v) = W2 · tanh(W1 · v + b1) + b2 ω(i→ k) = MLPE(vi ◦ vk)
ω(f | i→ k) = MLPLBL(vi ◦ vk)
",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We use inputs xi = (wi, pi, τi) including the type τi of the supertag G[i] at position i, using trained embeddings for all three.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"At evaluation time, we use the best scoring supertag according to the model of Section 5.1.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"At training time, we sample from q, where q(τi) =",5.2 Kiperwasser & Goldberg edge model,[0],[0]
(1− δ),5.2 Kiperwasser & Goldberg edge model,[0],[0]
"+ δ · p(τi|pi, pi−1), q(τ) = δ · p(τ |pi, pi−1) for any τ 6= τi",5.2 Kiperwasser & Goldberg edge model,[0],[0]
and δ is a hyperparameter controlling the bias towards the aligned supertag.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
We train the model using K&G’s original DyNet implementation.,5.2 Kiperwasser & Goldberg edge model,[0],[0]
"Their algorithm uses a hinge loss function, which maximizes the score difference between the gold dependency tree and the best predicted dependency tree, and therefore requires parsing each training instance in each iteration.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"Because the AM dependency trees are highly non-projective, we replaced the projective parser used in the off-the-shelf implementation by the Chu-Liu-Edmonds algorithm implemented in the TurboParser (Martins et al., 2010), improving the LAS on the development set by 30 points.",5.2 Kiperwasser & Goldberg edge model,[0],[0]
"We also trained a local edge score model, which uses a cross-entropy rather than a hinge loss and therefore avoids the repeated parsing at training
time.",5.3 Local edge model,[0],[0]
"Instead, we follow the intuition that every node in a dependency tree has at most one incoming edge, and train the model to score the correct incoming edge as high as possible.",5.3 Local edge model,[0],[0]
"This model takes inputs xi = (wi, pi).
",5.3 Local edge model,[0],[0]
"We define the edge and edge label scores as in Section 5.2, with tanh replaced by ReLU.",5.3 Local edge model,[0],[0]
"We further add a learned parameter v⊥ for the “LSTM embedding” of a nonexistent node, obtaining scores ω(⊥",5.3 Local edge model,[0],[0]
"→ k) for k having no incoming edge.
",5.3 Local edge model,[0],[0]
"To train ω(i → k), we collect all scores for edges ending at the same node k into a vector ω(• → k).",5.3 Local edge model,[0],[0]
"We then minimize the cross-entropy loss for the gold edge into k under softmax(ω(• → k)), maximizing the likelihood of the gold edges.",5.3 Local edge model,[0],[0]
"To train the labels ω(f | i → k), we simply minimize the cross-entropy loss of the actual edge labels f of the edges which are present in the gold AM dependency trees.
",5.3 Local edge model,[0],[0]
The PyTorch code for this and the supertagger are available at bitbucket.org/tclup/ amr-dependency.,5.3 Local edge model,[0],[0]
"Given learned estimates for the graph and edge scores, we now tackle the challenge of computing the best well-typed dependency tree t for the input string w, under the score model (equation (1)).",6 Decoding,[0],[0]
"The requirement that t must be well-typed is crucial to ensure that it can be evaluated to an AMR graph, but as we show in the Supplementary Materials (Section A), makes the decoding problem NP-complete.",6 Decoding,[0],[0]
"Thus, an exact algorithm is not practical.",6 Decoding,[0],[0]
"In this section, we develop two different approximation algorithms for AM dependency parsing: one which assumes the (unlabeled) dependency tree structure as known, and one which assumes that the AM dependency tree is projective.",6 Decoding,[0],[0]
"The projective decoder assumes that the AM dependency tree is projective, i.e. has no crossing dependency edges.",6.1 Projective decoder,[0],[0]
"Because of this assumption, it can recursively combine adjacent substrings using dynamic programming.",6.1 Projective decoder,[0],[0]
"The algorithm is shown in Fig. 5 as a parsing schema (Shieber et al., 1995), which derives items of the form ([i, k], r, τ) with scores s. An item represents a well-typed derivation of the substring from i to k with head index r, and which evaluates to an as-graph of type τ .
",6.1 Projective decoder,[0],[0]
"The parsing schema consists of three types of
rules.",6.1 Projective decoder,[0],[0]
"First, the Init rule generates an item for each graph fragment G[i] that the supertagger predicted for the token wi, along with the score and type of that graph fragment.",6.1 Projective decoder,[0],[0]
"Second, given items for adjacent substrings [i, j] and [j, k], the Arc rules apply an operation f to combine the indexed AM terms for the two substrings, with Arc-R making the left-hand substring the head and the right-hand substring the argument or modifier, and Arc-L the other way around.",6.1 Projective decoder,[0],[0]
We ensure that the result is well-typed by requiring that the types can be combined with f .,6.1 Projective decoder,[0],[0]
"Finally, the Skip rules allow us to extend a substring such that it covers tokens which do not correspond to a graph fragment (i.e., their AM term is ⊥), introducing IGNORE edges.",6.1 Projective decoder,[0],[0]
"After all possible items have been derived, we extract the best well-typed tree from the item of the form ([1, n], r, τ) with the highest score, where τ =",6.1 Projective decoder,[0],[0]
"[ ].
",6.1 Projective decoder,[0],[0]
"Because we keep track of the head indices, the projective decoder is a bilexical parsing algorithm, and shares a parsing complexity of O(n5) with other bilexical algorithms such as the Collins parser.",6.1 Projective decoder,[0],[0]
It could be improved to a complexity of O(n4) using the algorithm of Eisner and Satta (1999).,6.1 Projective decoder,[0],[0]
"The fixed-tree decoder computes the best unlabeled dependency tree tr for w, using the edge scores ω(i→ k), and then computes the best AM dependency tree forw whose unlabeled version is tr.",6.2 Fixed-tree decoder,[0],[0]
"The Chu-Liu-Edmonds algorithm produces a forest of dependency trees, which we want to combine into tr.",6.2 Fixed-tree decoder,[0],[0]
"We choose the tree whose root r has the highest score for being the root of the AM dependency tree and make the roots of all others children of r.
At this point, the shape of tr is fixed.",6.2 Fixed-tree decoder,[0],[0]
"We choose
supertags for the nodes and edge labels for the edges by traversing tr bottom-up, computing types for the subtrees as we go along.",6.2 Fixed-tree decoder,[0],[0]
"Formally, we apply the parsing schema in Fig. 6.",6.2 Fixed-tree decoder,[0],[0]
"It uses items of the form (i, C, τ) : s, where 1 ≤ i ≤ n is a node of tr, C is the set of children of i for which we have already chosen edge labels, and τ is a type.",6.2 Fixed-tree decoder,[0],[0]
"We write Ch(i) for the set of children of i in tr.
",6.2 Fixed-tree decoder,[0],[0]
"The Init rule generates an item for each graph that the supertagger can assign to each token i in w, ensuring that every token is also assigned ⊥ as a possible supertag.",6.2 Fixed-tree decoder,[0],[0]
"The Edge rule labels an edge from a parent node i in tr to one of its children k, whose children already have edge labels.",6.2 Fixed-tree decoder,[0],[0]
"As above, this rule ensures that a well-typed AM dependency tree is generated by locally checking the types.",6.2 Fixed-tree decoder,[0],[0]
"In particular, if all types τ2 that can be derived for k are incompatible with τ1, we fall back to an item for k with τ2 = ⊥ (which always exists), along with an IGNORE edge from i to k.
The complexity of this algorithm is O(n · 2d · d), where d is the maximal arity of the nodes in tr.",6.2 Fixed-tree decoder,[0],[0]
We evaluate our models on the LDC2015E86 and LDC2017T103 datasets (henceforth “2015” and “2017”).,7 Evaluation,[0],[0]
Technical details and hyperparameters of our implementation can be found in Sections B to D of the Supplementary Materials.,7 Evaluation,[0],[0]
The original LDC datasets pair strings with AMRs.,7.1 Training data,[0],[0]
"We convert each AMR in the training and development set into an AM dependency tree, using the procedure of Section 4.2.",7.1 Training data,[0],[0]
About 10% of the training instances cannot be split into elementary as-graphs by our aligner; we removed these from the training data.,7.1 Training data,[0],[0]
"Of the remaining AM dependency trees, 37% are non-projective.
",7.1 Training data,[0],[0]
"Furthermore, the AM algebra is designed to handle short-range reentrancies, modeling grammati-
3https://catalog.ldc.upenn.edu/ LDC2017T10, identical to LDC2016E25.
cal phenomena such as control and coordination, as in the derivation in Fig. 2.",7.1 Training data,[0],[0]
"It cannot easily handle the long-range reentrancies in AMRs which are caused by coreference, a non-compositional phenomenon.4 We remove such reentrancies from our training data (about 60% of the roughly 20,000 reentrant edges).",7.1 Training data,[0],[0]
"Despite this, our model performs well on reentrant edges (see Table 2).",7.1 Training data,[0],[0]
We use simple pre- and postprocessing steps to handle rare words and some AMR-specific patterns.,7.2 Pre- and postprocessing,[0],[0]
"In AMRs, named entities follow a pattern shown in Fig. 7.",7.2 Pre- and postprocessing,[0],[0]
"Here the named entity is of type “person”, has a name edge to a “name” node whose children spell out the tokens of “Agatha Christie”, and a link to a wiki entry.",7.2 Pre- and postprocessing,[0],[0]
"Before training, we replace each “name” node, its children, and the corresponding span in the sentence with a special NAME token, and we completely remove wiki edges.",7.2 Pre- and postprocessing,[0],[0]
"In this example, this leaves us with only a “person” and a NAME node.",7.2 Pre- and postprocessing,[0],[0]
"Further, we replace numbers and some date patterns with NUMBER and DATE tokens.",7.2 Pre- and postprocessing,[0],[0]
"On the training data this is straightforward, since names and dates are explicitly annotated in the AMR.",7.2 Pre- and postprocessing,[0],[0]
"At evaluation time, we detect dates and numbers with regular expressions, and names with Stanford CoreNLP (Manning et al., 2014).",7.2 Pre- and postprocessing,[0],[0]
"We also use Stanford CoreNLP for our POS tags.
",7.2 Pre- and postprocessing,[0],[0]
Each elementary as-graph generated by the procedure of Section 4.2 has a unique node whose label corresponds most closely to the aligned word (e.g. the “want” node in Gwant and the “write” node in Gwriter).,7.2 Pre- and postprocessing,[0],[0]
"We replace these node labels with LEX in preprocessing, reducing the number of different elementary as-graphs from 28730 to 2370.",7.2 Pre- and postprocessing,[0],[0]
"We factor the supertagger model of Section 5.1 such that the unlexicalized version of G[i] and the label for LEX are predicted separately.
",7.2 Pre- and postprocessing,[0],[0]
"At evaluation, we re-lexicalize all LEX nodes in the predicted AMR.",7.2 Pre- and postprocessing,[0],[0]
"For words that were frequent in the training data (at least 10 times), we take the supertagger’s prediction for the label.",7.2 Pre- and postprocessing,[0],[0]
"For rarer words, we use simple heuristics, explained in the Supplementary Materials (Section D).",7.2 Pre- and postprocessing,[0],[0]
"For names, we just look up name nodes with their children and wiki entries observed for the name string in the training data, and for unseen names use the literal tokens as the name, and no wiki entry.",7.2 Pre- and postprocessing,[0],[0]
"Similarly,
4As Damonte et al. (2017) comment: “A valid criticism of AMR is that these two reentrancies are of a completely different type, and should not be collapsed together.”
we collect the type for each encountered name (e.g. “person” for “Agatha Christie”), and correct it in the output if the tagger made a different prediction.",7.2 Pre- and postprocessing,[0],[0]
We recover dates and numbers straightforwardly.,7.2 Pre- and postprocessing,[0],[0]
All of our models rely on the supertagger to predict elementary as-graphs; they differ only in the edge scores.,7.3 Supertagger accuracy,[0],[0]
"We evaluated the accuracy of the supertagger on the converted development set (in which each token has a supertag) of the 2015 data set, and achieved an accuracy of 73%.",7.3 Supertagger accuracy,[0],[0]
"The correct supertag is within the supertagger’s 4 best predictions for 90% of the tokens, and within the 10 best for 95%.
",7.3 Supertagger accuracy,[0],[0]
"Interestingly, supertags that introduce grammatical reentrancies are predicted quite reliably, although they are relatively rare in the training data.",7.3 Supertagger accuracy,[0],[0]
"The elementary as-graph for subject control verbs (see Gwant in Fig. 1) accounts for only 0.8% of supertags in the training data, yet 58% of its occurrences in the development data are predicted correctly (84% in 4-best).",7.3 Supertagger accuracy,[0],[0]
"The supertag for VP coordination (with type [OP1[S], OP2[S]]) makes up for 0.4% of the training data, but 74% of its occurrences are recognized correctly (92% in 4-best).",7.3 Supertagger accuracy,[0],[0]
Thus the prediction of informative types for individual words is feasible.,7.3 Supertagger accuracy,[0],[0]
Type-unaware fixed-tree baseline.,7.4 Comparison to Baselines,[0],[0]
The fixed-tree decoder is built to ensure well-typedness of the predicted AM dependency trees.,7.4 Comparison to Baselines,[0],[0]
"To investigate to what extent this is required, we consider a baseline which just adds the individually highest-scoring supertags and edge labels to the unlabeled dependency tree tu, ignoring types.",7.4 Comparison to Baselines,[0],[0]
This leads to AM dependency trees which are not well-typed for 75% of the sentences (we fall back to the largest welltyped subtree in these cases).,7.4 Comparison to Baselines,[0],[0]
"Thus, an off-theshelf dependency parser can reliably predict the tree structure of the AM dependency tree, but correct supertag and edge label assignment requires a decoder which takes the types into account.
",7.4 Comparison to Baselines,[0],[0]
JAMR-style baseline.,7.4 Comparison to Baselines,[0],[0]
"Our elementary asgraphs differ from the elementary graphs used in JAMR-style algorithms in that they contain explicit source nodes, which restrict the way in which they can be combined with other as-graphs.",7.4 Comparison to Baselines,[0],[0]
We investigate the impact of this choice by implementing a strong JAMR-style baseline.,7.4 Comparison to Baselines,[0],[0]
"We adapt the AMR-todependency conversion of Section 4.2 by removing all unlabeled nodes with source names from the
elementary graphs.",7.4 Comparison to Baselines,[0],[0]
"For instance, the graph Gwant in Fig. 1 now only consists of a single “want” node.",7.4 Comparison to Baselines,[0],[0]
"We then aim to directly predict AMR edges between these graphs, using a variant of the local edge scoring model of Section 5.3 which learns scores for each edge in isolation.",7.4 Comparison to Baselines,[0],[0]
"(The assumption for the original local model, that each node has only one incoming edge, does not apply here.)
",7.4 Comparison to Baselines,[0],[0]
"When parsing a string, we choose the highestscoring supertag for each word; there are only 628 different supertags in this setting, and 1-best supertagging accuracy is high at 88%.",7.4 Comparison to Baselines,[0],[0]
We then follow the JAMR parsing algorithm by predicting all edges whose score is over a threshold (we found -0.02 to be optimal) and then adding edges until the graph is connected.,7.4 Comparison to Baselines,[0],[0]
"Because we do not predict which node is the root of the AMR, we evaluated this model as if it always predicted the root correctly, overestimating its score slightly.",7.4 Comparison to Baselines,[0],[0]
"Table 1 shows the Smatch scores (Cai and Knight, 2013) of our models, compared to a selection of previously published results.",7.5 Results,[0],[0]
Our results are averages over 4 runs with 95% confidence intervals (JAMR-style baselines are single runs).,7.5 Results,[0],[0]
"On the 2015 dataset, our best models (local + projective, K&G + fixed-tree) outperform all previous work, with the exception of the Foland and Martin (2017) model; on the 2017 set we match state of the art results (though note that van Noord and Bos (2017b) use 100k additional sentences of silver data).",7.5 Results,[0],[0]
"The fixed-tree decoder seems to work well with either edge model, but performance of the projective decoder drops with the K&G edge scores.",7.5 Results,[0],[0]
"It may be that, while the hinge loss used in the K&G edge scoring model is useful to finding the correct un-
2015 2017 Metric W’15 F’16 D’17 PD FTD vN’17 PD FTD Smatch 67 67 64 70 70 71 71 70 Unlabeled 69 69 69 73 73 74 74 74 No WSD 64 68 65 71 70 72 72 70 Named Ent.",7.5 Results,[0],[0]
"75 79 83 79 78 79 78 77 Wikification 0 75 64 71 72 65 71 71 Negations 18 45 48 52 52 62 57 55 Concepts 80 83 83 83 84 82 84 84 Reentrancies 41 42 41 46 44 52 49 46 SRL 60 60 56 63 61 66 64 62
Table 2: Details for the LDC2015E86 and LDC2017T10 test sets
Agatha_Christiename
person
na me
wiki
Agatha Christie
op 1 op2
Figure 7: A named entity
labeled dependency tree in the fixed-tree decoder, scores for bad edges – which are never used when computing the hinge loss – are not trained accurately.",7.5 Results,[0],[0]
"Thus such edges may be erroneously used by the projective decoder.
",7.5 Results,[0],[0]
"As expected, the type-unaware baseline has low recall, due to its inability to produce well-typed trees.",7.5 Results,[0],[0]
"The fact that our models outperform the JAMR-style baseline so clearly is an indication that they indeed gain some of their accuracy from the type information in the elementary as-graphs, confirming our hypothesis that an explicit model of the compositional structure of the AMR can help the parser learn an accurate model.
",7.5 Results,[0],[0]
"Table 2 analyzes the performance of our two best systems (PD = projective, FTD = fixed-tree) in more detail, using the categories of Damonte et al. (2017), and compares them to Wang’s, Flanigan’s, and Damonte’s AMR parsers on the 2015 set and , and van Noord and Bos (2017b) for the 2017 dataset.",7.5 Results,[0],[0]
(Foland and Martin (2017) did not publish such results.),7.5 Results,[0],[0]
"The good scores we achieve on reentrancy identification, despite removing a large amount of reentrant edges from the training data, indicates that our elementary as-graphs successfully encode phenomena such as control and coordination.
",7.5 Results,[0],[0]
"The projective decoder is given 4, and the fixedtree decoder 6, supertags for each token.",7.5 Results,[0],[0]
We trained the supertagging and edge scoring models of Section 5 separately; joint training did not help.,7.5 Results,[0],[0]
"Not sampling the supertag types τi during training of the K&G model, removing them from the input, and removing the character-based LSTM encodings ci from the input of the supertagger, all reduced our models’ accuracy.",7.5 Results,[0],[0]
"Although the Smatch scores for our two best models are close, they sometimes struggle with different sentences.",7.6 Differences between the parsers,[0],[0]
"The fixed-tree parser is at the mercy of
the fixed tree; the projective parser cannot produce non-projective AM dependency trees.",7.6 Differences between the parsers,[0],[0]
"It is remarkable that the projective parser does so well, given the prevalence of non-projective trees in the training data.",7.6 Differences between the parsers,[0],[0]
"Looking at its analyses, we find that it frequently manages to find a projective tree which yields an (almost) correct AMR, by choosing supertags with unusual types, and by using modify rather than apply (or vice versa).",7.6 Differences between the parsers,[0],[0]
"We presented an AMR parser which applies methods from supertagging and dependency parsing to map a string into a well-typed AM term, which it then evaluates into an AMR.",8 Conclusion,[0],[0]
"The AM term represents the compositional semantic structure of the AMR explicitly, allowing us to use standard treebased parsing techniques.
",8 Conclusion,[0],[0]
The projective parser currently computes the complete parse chart.,8 Conclusion,[0],[0]
"In future work, we will speed it up through the use of pruning techniques.",8 Conclusion,[0],[0]
We will also look into more principled methods for splitting the AMRs into elementary as-graphs to replace our hand-crafted heuristics.,8 Conclusion,[0],[0]
"In particular, advanced methods for alignments, as in Lyu and Titov (2018), seem promising.",8 Conclusion,[0],[0]
"Overcoming the need for heuristics also seems to be a crucial ingredient for applying our method to other semantic representations.
",8 Conclusion,[0],[0]
Acknowledgements We would like to thank the anonymous reviewers for their comments.,8 Conclusion,[0],[0]
"We thank Stefan Grünewald for his contribution to our PyTorch implementation, and want to acknowledge the inspiration obtained from Nguyen et al. (2017).",8 Conclusion,[0],[0]
We also extend our thanks to the organizers and participants of the Oslo CAS Meaning Construction workshop on Universal Dependencies.,8 Conclusion,[0],[0]
This work was supported by the DFG grant KO 2916/2-1 and a Macquarie University Research Excellence Scholarship for Jonas Groschwitz.,8 Conclusion,[0],[0]
We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph.,abstractText,[0],[0]
"This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system.",abstractText,[0],[0]
"We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.",abstractText,[0],[0]
AMR Dependency Parsing with a Typed Semantic Algebra,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2002
AMR-to-text Generation with Synchronous Node Replacement Grammar
Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang and Daniel Gildea Department of Computer Science, University of Rochester, Rochester, NY 14627
IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 Singapore University of Technology and Design
Abstract
This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result.",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph.",1 Introduction,[0],[0]
"AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts.",1 Introduction,[0],[0]
"Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015).
",1 Introduction,[0],[0]
"AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations.",1 Introduction,[0],[0]
"Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016).
",1 Introduction,[0],[0]
"Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer.",1 Introduction,[0],[0]
"Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string.",1 Introduction,[0],[0]
"However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them.",1 Introduction,[0],[0]
Information loss in the graph-to-tree transformation step cannot be recovered.,1 Introduction,[0],[0]
Song et al. (2016) directly generate sentences using graphfragment-to-string rules.,1 Introduction,[0],[0]
"They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences.",1 Introduction,[0],[0]
"However, their method does not learn hierarchical structural correspondences between AMR graphs and strings.
",1 Introduction,[0],[0]
We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules.,1 Introduction,[0],[0]
"As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs.",1 Introduction,[0],[0]
"At test time, we apply a graph transducer to collapse input
7
AMR graphs and generate output strings according to the learned grammar.",1 Introduction,[0],[0]
"Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding.",1 Introduction,[0],[0]
"It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset.",1 Introduction,[0],[0]
"A synchronous node replacement grammar (NRG) is a rewriting formalism: G = 〈N,Σ,∆, P, S〉, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively.",2.1 Grammar Definition,[0],[0]
"S ∈ N is the start symbol, and P is a finite set of productions.",2.1 Grammar Definition,[0],[0]
"Each instance of P takes the form Xi → (〈F,E〉,∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪∆ and ∼ denotes the alignment of nonterminal symbols between F and E. A classic NRG (Engelfriet and Rozenberg, 1997, Chapter 1) also defines C, which is an embedding mechanism defining how F is connected to the rest of the graph when replacing Xi with F on the graph.",2.1 Grammar Definition,[0],[0]
Here we omit defining C and allow arbitrary connections.1,2.1 Grammar Definition,[0],[0]
"Following Chiang
1This may over generate, but does not affect our case, as in our bottom-up decoding procedure (section 3) when F is replaced with Xi, nodes previously connected to F are reconnected to Xi
Data:",2.1 Grammar Definition,[0],[0]
"training corpus C Result: rule instances R
1 R←",2.1 Grammar Definition,[0],[0]
"[]; 2 for (Sent,AMR,∼) in C do 3 Rcur ← FRAGMENTEXTRACT(Sent,AMR,∼); 4 for ri in Rcur do 5 R.APPEND(ri) ; 6 for rj in Rcur/{ri} do 7 if ri.CONTAINS(rj) then 8 rij ← ri.COLLAPSE(rj); 9 R.APPEND(rij) ;
10 end 11 end 12 end 13 end
Algorithm 1: Rule extraction
(2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances.
",2.1 Grammar Definition,[0],[0]
Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1.,2.1 Grammar Definition,[0],[0]
"Given the start symbol S, which is first replaced with X1, rule (c) is applied to generate “X2 to go” and its AMR counterpart.",2.1 Grammar Definition,[0],[0]
Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2.,2.1 Grammar Definition,[0],[0]
"Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3.",2.1 Grammar Definition,[0],[0]
"Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013).",2.1 Grammar Definition,[0],[0]
"There are three types of rules in our system, namely induced rules, concept rules and graph glue rules.",2.2 Induced Rules,[0],[0]
"Here we first introduce induced rules, which are obtained by a two-step procedure on a training corpus.",2.2 Induced Rules,[0],[0]
"Shown in Algorithm 1, the first step is to extract a set of initial rules from training 〈sentence, AMR, ∼〉2 pairs (Line 2) using the phrase-to-graph-fragment extraction algorithm of Peng et al. (2015) (Line 3).",2.2 Induced Rules,[0],[0]
"Here an initial rule
2∼ denotes alignment between words and AMR labels.
",2.2 Induced Rules,[0],[0]
"contains only terminal symbols in both F and E. As a next step, we match between pairs of initial rules ri and rj , and generate rij by collapsing ri with rj , if ri contains rj (Line 6-8).",2.2 Induced Rules,[0],[0]
"Here ri contains rj , if rj .F is a subgraph of ri.F and rj .E is a sub-phrase of ri.",2.2 Induced Rules,[0],[0]
E.,2.2 Induced Rules,[0],[0]
"When collapsing ri with rj , we replace the corresponding subgraph in ri.F with a new non-terminal node, and the sub-phrase in ri.E with the same non-terminal.",2.2 Induced Rules,[0],[0]
"For example, we obtain rule (b) by collapsing (d) with (a) in Table 1.",2.2 Induced Rules,[0],[0]
"All initial and generated rules are stored in a rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set.",2.2 Induced Rules,[0],[0]
"In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations.",2.3 Concept Rules and Glue Rules,[0],[0]
"For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of the node concept.",2.3 Concept Rules and Glue Rules,[0],[0]
A concept rule is used in case no induced rule can cover the node.,2.3 Concept Rules and Glue Rules,[0],[0]
We refer to the verbalization list3 and AMR guidelines4 for creating more complex concept rules.,2.3 Concept Rules and Glue Rules,[0],[0]
"For example, one concept rule created from the verbalization list is “(k / keep-01 :ARG1 (p / peace)) |||",2.3 Concept Rules and Glue Rules,[0],[0]
"peacekeeping”.
",2.3 Concept Rules and Glue Rules,[0],[0]
"Inspired by Chiang (2005), we define graph glue rules to concatenate non-terminal nodes connected with an edge, when no induced rules can be applied.",2.3 Concept Rules and Glue Rules,[0],[0]
Three glue rules are defined for each type of edge label.,2.3 Concept Rules and Glue Rules,[0],[0]
"Taking the edge label “ARG0” as an example, we create the following glue rules:
ID.",2.3 Concept Rules and Glue Rules,[0],[0]
F E r1 (X1 / #X1# :ARG0 (X2 / #X2#)),2.3 Concept Rules and Glue Rules,[0],[0]
#X1# #X2# r2,2.3 Concept Rules and Glue Rules,[0],[0]
(X1 / #X1# :ARG0 (X2 / #X2#)),2.3 Concept Rules and Glue Rules,[0],[0]
"#X2# #X1# r3 (X1 / #X1# :ARG0 X1) #X1#
where for both r1 and r2, F contains two nonterminal nodes with a directed edge connecting them, and E is the concatenation the two nonterminals in either the monotonic or the inverse order.",2.3 Concept Rules and Glue Rules,[0],[0]
"For r3, F contains one non-terminal node with a self-pointing edge, and E is the nonterminal.",2.3 Concept Rules and Glue Rules,[0],[0]
"With concept rules and glue rules in our final rule set, it is easily guaranteed that there are legal derivations for any input AMR graph.",2.3 Concept Rules and Glue Rules,[0],[0]
We adopt a log-linear model for scoring search hypotheses.,3 Model,[0],[0]
"Given an input AMR graph, we find
3http://amr.isi.edu/download/lists/verbalization-listv1.06.txt
4https://github.com/amrisi/amr-guidelines
the highest scored derivation t∗ from all possible derivations t:
t∗ = argmax t
exp ∑
i
wifi(g, t), (1)
where g denotes the input AMR, fi(·, ·) and wi represent a feature and the corresponding weight, respectively.",3 Model,[0],[0]
"The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3).",3 Model,[0],[0]
"The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005).
",3 Model,[0],[0]
We perform bottom-up search to transduce input AMRs to surface strings.,3 Model,[0],[0]
"Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score.",3 Model,[0],[0]
"Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam.",3 Model,[0],[0]
Production rules serve as a basis for scoring hypotheses.,3.1 Translation Probabilities,[0],[0]
We associate each synchronous NRG rule n →,3.1 Translation Probabilities,[0],[0]
"(〈F,E〉,∼) with a set of probabilities.",3.1 Translation Probabilities,[0],[0]
"First, phrase-to-fragment translation probabilities are defined based on maximum likelihood estimation (MLE), as shown in Equation 2, where c〈F,E〉 is the fractional count of 〈F,E〉.
p(F |E) = c〈F,E〉∑ F ′ c〈F ′,E〉
(2)
",3.1 Translation Probabilities,[0],[0]
"In addition, lexicalized translation probabilities are defined as:
pw(F |E) = ∏
l∈F
∑ w∈E p(l|w) (3)
",3.1 Translation Probabilities,[0],[0]
"Here l is a label (including both edge labels such as “ARG0” and concept labels such as “want-01”) in the AMR fragment F , and w is a word in the phrase E. Equation 3 can be regarded as a “soft” version of the lexicalized translation probabilities adopted by SMT, which picks the alignment yielding the maximum lexicalized probability for each translation rule.",3.1 Translation Probabilities,[0],[0]
"In addition to p(F |E) and pw(F |E), we use features in the reverse direction, namely p(E|F ) and pw(E|F ), the definitions of which are omitted as they are consistent with
Equations 2 and 3, respectively.",3.1 Translation Probabilities,[0],[0]
The probabilities associated with concept rules and glue rules are manually set to 0.0001.,3.1 Translation Probabilities,[0],[0]
"Although the word order is defined for induced rules, it is not the case for glue rules.",3.2 Reordering Model,[0],[0]
We learn a reordering model that helps to decide whether the translations of the nodes should be monotonic or inverse given the directed connecting edge label.,3.2 Reordering Model,[0],[0]
"The probabilistic model using smoothed counts is defined as:
p(M |h, l, t) = 1.0 + ∑ h ∑ t c(h, l, t,M)
2.0 + ∑ o∈{M,I} ∑ h ∑ t c(h, l, t, o) (4)
c(h, l, t,M) is the count of monotonic translations of head h and tail t, connected by edge l.",3.2 Reordering Model,[0],[0]
"The moving distance feature captures the distances between the subgraph roots of two consecutive rule matches in the decoding process, which controls a bias towards collapsing nearby subgraphs consecutively.",3.3 Moving Distance,[0],[0]
"We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances.",4.1 Setup,[0],[0]
"Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner.",4.1 Setup,[0],[0]
"Rules are extracted from the training data, and model parameters are tuned on the dev set.",4.1 Setup,[0],[0]
"For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances.",4.1 Setup,[0],[0]
"We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric.",4.1 Setup,[0],[0]
"MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50.
",4.1 Setup,[0],[0]
"We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model.",4.1 Setup,[0],[0]
"Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate
existing translation fragments into a final translation, and if a subgraph can not be translated, the empty string is used as the output.",4.1 Setup,[0],[0]
"We also compare our method with previous works, in particular JAMR-gen (Flanigan et al., 2016) and TSP-gen (Song et al., 2016), on the same dataset.",4.1 Setup,[0],[0]
The results are shown in Table 2.,4.2 Main results,[0],[0]
"First, All outperforms all baselines.",4.2 Main results,[0],[0]
"NoInducedRule leads to the greatest performance drop compared with All, demonstrating that induced rules play a very important role in our system.",4.2 Main results,[0],[0]
"On the other hand, NoConceptRule does not lead to much performance drop.",4.2 Main results,[0],[0]
This observation is consistent with the observation of Song et al. (2016) for their TSP-based system.,4.2 Main results,[0],[0]
"NoMovingDistance leads to a significant performance drop, empirically verifying the fact that the translations of nearby subgraphs are also close.",4.2 Main results,[0],[0]
"Finally, NoReorderingModel does not affect the performance significantly, which can be because the most important reordering patterns are already covered by the hierarchical induced rules.",4.2 Main results,[0],[0]
"Compared with TSP-gen and JAMR-gen, our final model All improves the BLEU from 22.44 and 23.00 to 25.62, showing the advantage of our model.",4.2 Main results,[0],[0]
"To our knowledge, this is the best result reported so far on the task.",4.2 Main results,[0],[0]
We have shown the effectiveness of our synchronous node replacement grammar (SNRG) on the AMR-to-text generation task.,4.3 Grammar analysis,[0],[0]
"Here we further analyze our grammar as it is relatively less studied than the hyperedge replacement grammar (HRG) (Drewes et al., 1997).
",4.3 Grammar analysis,[0],[0]
"Statistics on the whole rule set We first categorize our rule set by the number of terminals and nonterminals in the AMR fragment F , and show the percentages of each type in Figure 3.",4.3 Grammar analysis,[0],[0]
"Each rule contains at most 1 nonterminal, as we collapse each initial rule only once.",4.3 Grammar analysis,[0],[0]
"First
of all, the percentage of rules containing nonterminals are much more than those without nonterminals, as we collapse each pair of initial rules (in Algorithm 1) and the results can be quadratic the number of initial rules.",4.3 Grammar analysis,[0],[0]
"In addition, most rules are small containing 1 to 3 terminals, meaning that they represent small pieces of meaning and are easier to matched on a new AMR graph.",4.3 Grammar analysis,[0],[0]
"Finally, there are a few large rules, which represent complex meaning.
",4.3 Grammar analysis,[0],[0]
"Statistics on the rules used for decoding In addition, we collect the rules that our well-tuned system used for generating the 1-best output on the testset, and categorize them into 3 types: (1) glue rules, (2) nonterminal rules, which are not glue rules but contain nonterminals on the righthand side and (3) terminal rules, whose right-hand side only contain terminals.",4.3 Grammar analysis,[0],[0]
"Over the rules used on the 1-best result, more than 30% are non-terminal rules, showing that the induced rules play an important role.",4.3 Grammar analysis,[0],[0]
"On the other hand, 30% are glue rules.",4.3 Grammar analysis,[0],[0]
"The reason is that the data sparsity for graph grammars is more severe than string-based grammars (such as CFG), as the graph structures are more complex than strings.",4.3 Grammar analysis,[0],[0]
"Finally, terminal rules take the largest percentage, while most are induced rules, but not concept rules.
",4.3 Grammar analysis,[0],[0]
"Rule examples Finally, we show some rules in Table 4, where F and E are the right-hand-side AMR fragment and phrase, respectively.",4.3 Grammar analysis,[0],[0]
"For the first rule, the root of F is a verb (“give-01”) whose subject is a nonterminal and object is a AMR fragment “(p / person :ARG0-of (u / use-01))”, which means “user”.",4.3 Grammar analysis,[0],[0]
So it is easy to see that the corresponding phrase E conveys the same meaning.,4.3 Grammar analysis,[0],[0]
"For the second rule, “(s3 / stay-01 :accompanier (i / i))” means “stay
with me”, which is also covered by its phrase.",4.3 Grammar analysis,[0],[0]
"Finally, we show an example in Table 5, where the top is the input AMR graph, and the bottom is the generation result.",4.4 Generation example,[0],[0]
"Generally, most of the meaning of the input AMR are correctly translated, such as “:example”, which means “such as”, and “thing”, which is an abstract concept and should not be translated, while there are a few errors, such as “that” in the result should be “what”, and there should be an “in” between “tmt” and “fairfax”.",4.4 Generation example,[0],[0]
"We showed that synchronous node replacement grammar is useful for AMR-to-text generation by developing a system that learns a synchronous NRG in the training time, and applies a graph transducer to collapse input AMR graphs and generate output strings according to the learned grammar at test time.",5 Conclusion,[0],[0]
"Our method performs better than the previous systems, empirically proving the advantages of our graph-to-string rules.",5 Conclusion,[0],[0]
This work was funded by a Google Faculty Research Award.,Acknowledgement,[0],[0]
Yue Zhang is funded by NSFC61572245 and T2MOE201301 from Singapore Ministry of Education.,Acknowledgement,[0],[0]
This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar.,abstractText,[0],[0]
"During training, graph-to-string rules are learned using a heuristic extraction algorithm.",abstractText,[0],[0]
"At test time, a graph transducer is applied to collapse input AMRs and generate output sentences.",abstractText,[0],[0]
"Evaluated on a standard benchmark, our method gives the state-of-the-art result.",abstractText,[0],[0]
AMR-to-text Generation with Synchronous Node Replacement Grammar,title,[0],[0]
"We consider the design of adaptive, nonparametric statistical tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals PxPy with the null hypothesis that H0 : X and Y are independent.",1. Introduction,[0],[0]
"While classical tests of dependence, such as Pearson’s correlation and Kendall’s τ , are able to detect monotonic relations between univariate variables, more modern tests can address complex interactions, for instance changes in variance of X with the value of Y .",1. Introduction,[0],[0]
Key to many recent tests is to examine covariance or correlation between data features.,1. Introduction,[0],[0]
"These interactions become significantly harder to detect, and the features are more difficult to design, when the data reside in high dimensions.
",1. Introduction,[0],[0]
Zoltán Szabó’s ORCID ID: 0000-0001-6183-7603.,1. Introduction,[0],[0]
Arthur Gretton’s ORCID ID: 0000-0003-3169-7624.,1. Introduction,[0],[0]
"1Gatsby Unit, University College London, UK. 2CMAP, École Polytechnique, France.",1. Introduction,[0],[0]
"Correspondence to: Wittawat Jitkrittum <wittawatj@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"A basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the Hilbert-Schmidt norm of the covariance operator between feature mappings of the random variables (Gretton et al., 2005; 2008).",1. Introduction,[0],[0]
Each random variable X and Y is mapped to a respective reproducing kernel Hilbert space Hk and Hl.,1. Introduction,[0],[0]
"For sufficiently rich mappings, the covariance operator norm is zero if and only if the variables are independent.",1. Introduction,[0],[0]
"A second basic nonlinear dependence measure is the smoothed difference between the characteristic function of the joint distribution, and that of the product of marginals.",1. Introduction,[0],[0]
"When a particular smoothing function is used, the statistic corresponds to the covariance between distances ofX and Y variable pairs (Feuerverger, 1993; Székely et al., 2007; Székely & Rizzo, 2009), yielding a simple test statistic based on pairwise distances.",1. Introduction,[0],[0]
It has been shown by Sejdinovic et al. (2013) that the distance covariance (and its generalization to semi-metrics) is an instance of HSIC for an appropriate choice of kernels.,1. Introduction,[0],[0]
"A disadvantage of these feature covariance statistics, however, is that they require quadratic time to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo & Székely (2016) achieve an O(n log n) cost).",1. Introduction,[0],[0]
"Moreover, the feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of an expensive eigenvalue problem (e.g. Zhang et al., 2011) is required for consistent estimation of the quantiles.",1. Introduction,[0],[0]
Several approaches were proposed by Zhang et al. (2017) to obtain faster tests along the lines of HSIC.,1. Introduction,[0],[0]
"These include computing HSIC on finite-dimensional feature mappings chosen as random Fourier features (RFFs) (Rahimi & Recht, 2008), a block-averaged statistic, and a Nyström approximation to the statistic.",1. Introduction,[0],[0]
"Key to each of these approaches is a more efficient computation of the statistic and its threshold under the null distribution: for RFFs, the null distribution is a finite weighted sum of χ2 variables; for the block-averaged statistic, the null distribution is asymptotically normal; for Nyström, either a permutation approach is employed, or the spectrum of the Nyström approximation to the kernel matrix is used in approximating the null distribution.",1. Introduction,[0],[0]
"Each of these methods costs significantly less than theO(n2) cost of the full HSIC (the cost is linear in n, but also depends quadratically on the number of features retained).",1. Introduction,[0],[0]
"A potential disadvantage of the Nyström and Fourier approaches is that the features are not optimized to maximize test power,
but are chosen randomly.",1. Introduction,[0],[0]
"The block statistic performs worse than both, due to the large variance of the statistic under the null (which can be mitigated by observing more data).
",1. Introduction,[0],[0]
"In addition to feature covariances, correlation measures have also been developed in infinite dimensional feature spaces: in particular, Bach & Jordan (2002); Fukumizu et al. (2008) proposed statistics on the correlation operator in a reproducing kernel Hilbert space.",1. Introduction,[0],[0]
"While convergence has been established for certain of these statistics, their computational cost is high at O(n3), and test thresholds have relied on permutation.",1. Introduction,[0],[0]
"A number of much faster approaches to testing based on feature correlations have been proposed, however.",1. Introduction,[0],[0]
"For instance, Dauxois & Nkiet (1998) compute statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order B-splines.",1. Introduction,[0],[0]
The cost of this approach is O(n).,1. Introduction,[0],[0]
"This idea was extended by Lopez-Paz et al. (2013), who computed the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they performed a copula transform on the inputs, with a total cost of O(n log n).",1. Introduction,[0],[0]
"Finally, space partitioning approaches have also been proposed, based on statistics such as the KL divergence, however these apply only to univariate variables (Heller et al., 2016), or to multivariate variables of low dimension (Gretton & Györfi, 2010) (that said, these tests have other advantages of theoretical interest, notably distribution-independent test thresholds).
",1. Introduction,[0],[0]
The approach we take is most closely related to HSIC on a finite set of features.,1. Introduction,[0],[0]
"Our simplest test statistic, the Finite Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each of X and Y .",1. Introduction,[0],[0]
A normalized version of the statistic (NFSIC) yields a distribution-independent asymptotic test threshold.,1. Introduction,[0],[0]
"We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. (2015).",1. Introduction,[0],[0]
"As in recent work on two-sample testing by Jitkrittum et al. (2016), our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power.",1. Introduction,[0],[0]
"The design of features for independence testing turns out to be quite different to the case of two-sample testing, however: the task is to find correlated feature pairs on the respective marginal domains, rather than attempting to find a single, high-dimensional feature representation on the tensor product of the marginals, as we would need to do if we were comparing distributions Pxy and Qxy .",1. Introduction,[0],[0]
"While the use of coupled feature pairs on the marginals entails a smaller feature space dimension, it introduces significant complications in the proof of the lower bound, compared with the two-sample case.",1. Introduction,[0],[0]
"We demonstrate the performance of our tests on several challenging artificial and real-world datasets, including detection of dependence between music and its year of appearance, and between videos and captions.
",1. Introduction,[0],[0]
"In these experiments, we outperform competing linear and O(n log n) time tests.",1. Introduction,[0],[0]
"We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle that dependence can be measured in terms of the covariance between data features.",2. Independence Criteria and Statistical Tests,[0],[0]
"Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution when Pxy = PxPy.",2. Independence Criteria and Statistical Tests,[0],[0]
We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test.,2. Independence Criteria and Statistical Tests,[0],[0]
"We begin by recalling the Hilbert-Schmidt Independence Criterion (HSIC) as proposed in Gretton et al. (2005), since our unnormalized statistic is built along similar lines.",2.1. The Finite Set Independence Criterion,[0],[0]
Consider two random variables X ∈ X ⊆ Rdx and Y ∈,2.1. The Finite Set Independence Criterion,[0],[0]
Y ⊆ Rdy .,2.1. The Finite Set Independence Criterion,[0],[0]
Denote by Pxy the joint distribution betweenX and Y ; Px and Py are the marginal distributions of X and Y .,2.1. The Finite Set Independence Criterion,[0],[0]
"Let⊗ denote the tensor product, such that (a⊗ b) c = a 〈b, c〉.",2.1. The Finite Set Independence Criterion,[0],[0]
Assume that k :,2.1. The Finite Set Independence Criterion,[0],[0]
"X × X → R and l : Y × Y → R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS)Hk andHl, respectively.",2.1. The Finite Set Independence Criterion,[0],[0]
Let ‖ · ‖HS be the norm on the space ofHl →,2.1. The Finite Set Independence Criterion,[0],[0]
Hk Hilbert-Schmidt operators.,2.1. The Finite Set Independence Criterion,[0],[0]
"Then, HSIC between X and Y is defined as
HSIC(X,Y ) = ∥∥µxy",2.1. The Finite Set Independence Criterion,[0],[0]
"− µx ⊗ µy∥∥2HS
= E(x,y),(x′,y′)",2.1. The Finite Set Independence Criterion,[0],[0]
"[k(x,x′)l(y,y′)]",2.1. The Finite Set Independence Criterion,[0],[0]
"+ ExEx′ [k(x,x′)]EyEy′",2.1. The Finite Set Independence Criterion,[0],[0]
"[l(y,y′)]",2.1. The Finite Set Independence Criterion,[0],[0]
"− 2E(x,y) [Ex′ [k(x,x′)]Ey′",2.1. The Finite Set Independence Criterion,[0],[0]
"[l(y,y′)]] , (1)
where Ex := Ex∼Px , Ey := Ey∼Py , Exy := E(x,y)∼Pxy , and x′ is an independent copy of x.",2.1. The Finite Set Independence Criterion,[0],[0]
"The mean embedding of Pxy belongs to the space of Hilbert-Schmidt operators from Hl to Hk, µxy := ∫ X×Y k(x, ·) ⊗ l(y, ·) dPxy(x,y) ∈
HS(Hl,Hk), and the marginal mean embeddings are µx :=∫ X k(x, ·) dPx(x) ∈ Hk and µy := ∫ Y l(y, ·) dPy(y)",2.1. The Finite Set Independence Criterion,[0],[0]
"∈ Hl (Smola et al., 2007).",2.1. The Finite Set Independence Criterion,[0],[0]
"Gretton et al. (2005, Theorem 4) show that if the kernels k and l are universal (Steinwart & Christmann, 2008) on compact domains X and Y , then HSIC(X,Y )",2.1. The Finite Set Independence Criterion,[0],[0]
= 0,2.1. The Finite Set Independence Criterion,[0],[0]
if and only if X and Y are independent.,2.1. The Finite Set Independence Criterion,[0],[0]
"Given a joint sample Zn = {(xi,yi)}ni=1 ∼ Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.
",2.1. The Finite Set Independence Criterion,[0],[0]
"We now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC).",2.1. The Finite Set Independence Criterion,[0],[0]
Let X ⊆ Rdx and Y ⊆ Rdy be open sets.,2.1. The Finite Set Independence Criterion,[0],[0]
"Let µxµy(x,y) := µx(x)µy(y)",2.1. The Finite Set Independence Criterion,[0],[0]
"The idea is to see µxy(v,w) = Exy[k(x,v)l(y,w)], µx(v) = Ex[k(x,v)] and µy(w) = Ey[l(y,w)] as smooth functions, and consider a new dis-
tance between µxy and µxµy instead of a Hilbert-Schmidt distance as in HSIC (Gretton et al., 2005).",2.1. The Finite Set Independence Criterion,[0],[0]
"The new measure is given by the average of squared differences between µxy and µxµy, evaluated at J random test locations VJ := {(vi,wi)}Ji=1 ⊂",2.1. The Finite Set Independence Criterion,[0],[0]
"X × Y .
",2.1. The Finite Set Independence Criterion,[0],[0]
"FSIC2(X,Y ) := 1
J J∑ i=1",2.1. The Finite Set Independence Criterion,[0],[0]
"[µxy(vi,wi)− µx(vi)µy(wi)]2
= 1
J J∑ i=1 u2(vi,wi) = 1 J ‖u‖22,
where
u(v,w) := µxy(v,w)− µx(v)µy(w) = Exy[k(x,v)l(y,w)]− Ex[k(x,v)]Ey[l(y,w)], (2) = covxy[k(x,v), l(y,w)],
u := (u(v1,w1), . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", u(vJ ,wJ))",2.1. The Finite Set Independence Criterion,[0],[0]
">, and {(vi,wi)}Ji=1 are realizations from an absolutely continuous distribution (wrt the Lebesgue measure).
",2.1. The Finite Set Independence Criterion,[0],[0]
"Our first result in Proposition 2 states that FSIC(X,Y ) almost surely defines a dependence measure for the random variables X and Y , provided that the product kernel on the joint space X × Y is characteristic and analytic (see Definition 1).
",2.1. The Finite Set Independence Criterion,[0],[0]
"Definition 1 (Analytic kernels (Chwialkowski et al., 2015)).",2.1. The Finite Set Independence Criterion,[0],[0]
Let X be an open set in Rd.,2.1. The Finite Set Independence Criterion,[0],[0]
"A positive definite kernel k : X ×X → R is said to be analytic on its domain X ×X if for all v ∈ X , f(x) := k(x,v) is an analytic function on X .",2.1. The Finite Set Independence Criterion,[0],[0]
Assumption A.,2.1. The Finite Set Independence Criterion,[0],[0]
"The kernels k : X × X → R and l : Y × Y → R are bounded by Bk and Bl respectively [supx,x′∈X k(x,x
′) ≤",2.1. The Finite Set Independence Criterion,[0],[0]
"Bk, supy,y′∈Y l(y,y′) ≤",2.1. The Finite Set Independence Criterion,[0],[0]
"Bl] , and the product kernel g((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
":= k(x,x′)l(y,y′) is characteristic (Sriperumbudur et al., 2010, Definition 6), and analytic (Definition 1) on (X ×",2.1. The Finite Set Independence Criterion,[0],[0]
Y)× (X × Y).,2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 2 (FSIC is a dependence measure).,2.1. The Finite Set Independence Criterion,[0],[0]
"Assume that assumption A holds, and that the test locations VJ = {(vi,wi)}Ji=1 are drawn from an absolutely continuous distribution η.",2.1. The Finite Set Independence Criterion,[0],[0]
"Then, η-almost surely, it holds that FSIC(X,Y ) =",2.1. The Finite Set Independence Criterion,[0],[0]
"1√
J ‖u‖2 = 0",2.1. The Finite Set Independence Criterion,[0],[0]
"if and only if X and Y are
independent.
",2.1. The Finite Set Independence Criterion,[0],[0]
Proof.,2.1. The Finite Set Independence Criterion,[0],[0]
"Since g is characteristic,",2.1. The Finite Set Independence Criterion,[0],[0]
"the mean embedding map Πg : P 7→ E(x,y)∼P",2.1. The Finite Set Independence Criterion,[0],[0]
"[g((x,y), ·)] is injective (Sriperumbudur et al., 2010, Section 3), where P is a probability distribution on X × Y .",2.1. The Finite Set Independence Criterion,[0],[0]
"Since g is analytic, by Lemma 10 (Appendix), µxy and µxµy are analytic functions.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, Lemma 11 (Appendix, setting Λ = Πg) guarantees that FSIC(X,Y ) = 0",2.1. The Finite Set Independence Criterion,[0],[0]
⇐⇒,2.1. The Finite Set Independence Criterion,[0],[0]
"Pxy = PxPy ⇐⇒ X and Y are independent almost surely.
",2.1. The Finite Set Independence Criterion,[0],[0]
"FSIC uses µxy as a proxy for Pxy , and µxµy as a proxy for PxPy.",2.1. The Finite Set Independence Criterion,[0],[0]
"Proposition 2 states that, to detect the dependence between X and Y , it is sufficient to evaluate the difference of the population joint embedding µxy and the embedding of the product of the marginal distributions µxµy at a finite number of locations (defined by VJ ).",2.1. The Finite Set Independence Criterion,[0],[0]
The intuitive explanation of this property is as follows.,2.1. The Finite Set Independence Criterion,[0],[0]
"If Pxy = PxPy, then u(v,w) = 0 everywhere, and FSIC(X,Y ) = 0 for any VJ .",2.1. The Finite Set Independence Criterion,[0],[0]
"If Pxy 6= PxPy, then u will not be a zero function, since the mean embedding map is injective (requires the product kernel to be characteristic).",2.1. The Finite Set Independence Criterion,[0],[0]
"Using the same argument as in Chwialkowski et al. (2015), since k and l are analytic, u is also analytic, and the set of roots",2.1. The Finite Set Independence Criterion,[0],[0]
"Ru := {(v,w) | u(v,w) = 0} has Lebesgue measure zero.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, it is sufficient to draw (v,w) from an absolutely continuous distribution to have (v,w) /∈",2.1. The Finite Set Independence Criterion,[0],[0]
"Ru η-almost surely, and hence FSIC(X,Y ) >",2.1. The Finite Set Independence Criterion,[0],[0]
0.,2.1. The Finite Set Independence Criterion,[0],[0]
We note that a characteristic kernel which is not analytic may produce u such thatRu has a positive Lebesgue measure.,2.1. The Finite Set Independence Criterion,[0],[0]
"In this case, there is a positive probability",2.1. The Finite Set Independence Criterion,[0],[0]
"that (v,w) ∈ Ru, resulting in a potential failure to detect the dependence.
",2.1. The Finite Set Independence Criterion,[0],[0]
"The next proposition shows that Gaussian kernels k and l yield a product kernel which is characteristic and analytic; in other words, this is an example when Assumption A holds.",2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 3 (A product of Gaussian kernels is characteristic and analytic).,2.1. The Finite Set Independence Criterion,[0],[0]
"Let k(x,x′) = exp ( −(x− x′)>A(x− x′) ) and l(y,y′)",2.1. The Finite Set Independence Criterion,[0],[0]
"=
exp ( −(y − y′)>B(y",2.1. The Finite Set Independence Criterion,[0],[0]
"− y′) ) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
"= k(x,x′)l(y,y′) is characteristic and analytic on (Rdx × Rdy )×",2.1. The Finite Set Independence Criterion,[0],[0]
(Rdx × Rdy ).,2.1. The Finite Set Independence Criterion,[0],[0]
Proof (sketch).,2.1. The Finite Set Independence Criterion,[0],[0]
"The main idea is to use the fact that a Gaussian kernel is analytic, and a product of Gaussian kernels is a Gaussian kernel on the pair of variables.",2.1. The Finite Set Independence Criterion,[0],[0]
"See the full proof in Appendix D.
Plug-in Estimator Assume that we observe a joint sample Zn := {(xi,yi)}ni=1
i.i.d.∼ Pxy.",2.1. The Finite Set Independence Criterion,[0],[0]
"Unbiased estimators of µxy(v,w) and µxµy(v,w) are µ̂xy(v,w) :",2.1. The Finite Set Independence Criterion,[0],[0]
= 1n,2.1. The Finite Set Independence Criterion,[0],[0]
∑n i=1,2.1. The Finite Set Independence Criterion,[0],[0]
"k(xi,v)l(yi,w) and µ̂xµy(v,w) := 1 n(n−1)",2.1. The Finite Set Independence Criterion,[0],[0]
∑n i=1,2.1. The Finite Set Independence Criterion,[0],[0]
∑,2.1. The Finite Set Independence Criterion,[0],[0]
"j 6=i k(xi,v)l(yj ,w), respectively.",2.1. The Finite Set Independence Criterion,[0],[0]
"A straightforward empirical estimator of FSIC2 is then given by
F̂SIC2(Zn) = 1
J J∑ i=1",2.1. The Finite Set Independence Criterion,[0],[0]
"û(vi,wi) 2,
û(v,w) := µ̂xy(v,w)− µ̂xµy(v,w) (3)
",2.1. The Finite Set Independence Criterion,[0],[0]
"= 2 n(n− 1) ∑ i<j h(v,w)((xi,yi), (xj ,yj)), (4)
where h(v,w)((x,y), (x′,y′))",2.1. The Finite Set Independence Criterion,[0],[0]
":= 12 (k(x,v)",2.1. The Finite Set Independence Criterion,[0],[0]
"− k(x′,v))(l(y,w) − l(y′,w)).",2.1. The Finite Set Independence Criterion,[0],[0]
"For conciseness, we
define û := (û1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", ûJ)> ∈ RJ where ûi := û(vi,wi) so that F̂SIC2(Zn) =",2.1. The Finite Set Independence Criterion,[0],[0]
"1J û >û.
F̂SIC2 can be efficiently computed inO((dx+dy)Jn) time which is linear in n",2.1. The Finite Set Independence Criterion,[0],[0]
"[see (3) which does not have nested double sums], assuming that the runtime complexity of evaluating k(x,v) is O(dx) and that of l(y,w) is O(dy).",2.1. The Finite Set Independence Criterion,[0],[0]
"Since FSIC satisfies FSIC(X,Y ) = 0 ⇐⇒ X ⊥ Y , in principle its empirical estimator can be used as a test statistic for an independence test proposing a null hypothesis H0 : “X and Y are independent” against an alternative H1 : “X and Y are dependent.”",2.1. The Finite Set Independence Criterion,[0],[0]
"The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however, and depends on the unknown Pxy.",2.1. The Finite Set Independence Criterion,[0],[0]
This prompts us to consider a normalized version of FSIC whose asymptotic null distribution takes a more convenient form.,2.1. The Finite Set Independence Criterion,[0],[0]
"We first derive the asymptotic distribution of û in Proposition 4, which we use to derive the normalized test statistic in Theorem 5.",2.1. The Finite Set Independence Criterion,[0],[0]
"As a shorthand, we write z := (x,y), t := (v,w), covz is covariance,Vz stands for variance.",2.1. The Finite Set Independence Criterion,[0],[0]
Proposition 4 (Asymptotic distribution of û).,2.1. The Finite Set Independence Criterion,[0],[0]
"Define u := (u(t1), . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", u(tJ))
",2.1. The Finite Set Independence Criterion,[0],[0]
">, k̃(x,v) :",2.1. The Finite Set Independence Criterion,[0],[0]
"= k(x,v)",2.1. The Finite Set Independence Criterion,[0],[0]
"− Ex′k(x′,v), and l̃(y,w) := l(y,w)",2.1. The Finite Set Independence Criterion,[0],[0]
"− Ey′ l(y′,w).",2.1. The Finite Set Independence Criterion,[0],[0]
Let Σ =,2.1. The Finite Set Independence Criterion,[0],[0]
"[Σij ] ∈ RJ×J be the positive semi-definite matrix with entries Σij = covz(û(ti), û(tj))",2.1. The Finite Set Independence Criterion,[0],[0]
"= Exy[k̃(x,vi)l̃(y,wi)k̃(x,vj)l̃(y,wj)]−u(ti)u(tj).",2.1. The Finite Set Independence Criterion,[0],[0]
"Then, under both H0 and H1, for any fixed test locations {t1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", tJ} for which Σ is full rank, and 0 < Vz[htj (z)]",2.1. The Finite Set Independence Criterion,[0],[0]
"< ∞ for j = 1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", J , it holds that √ n(û − u) d→ N (0,Σ).
",2.1. The Finite Set Independence Criterion,[0],[0]
Proof.,2.1. The Finite Set Independence Criterion,[0],[0]
"For a fixed {t1, . . .",2.1. The Finite Set Independence Criterion,[0],[0]
", tJ}, û is a one-sample secondorder multivariate U-statistic with a U-statistic kernel ht.",2.1. The Finite Set Independence Criterion,[0],[0]
"Thus, by Lehmann (1999, Theorem 6.1.6) and Kowalski & Tu (2008, Section 5.1, Theorem 1), it follows directly that √ n(û − u) d→ N (0,Σ) where we note that Exy[k̃(x,v)l̃(y,w)]",2.1. The Finite Set Independence Criterion,[0],[0]
"= u(v,w).
",2.1. The Finite Set Independence Criterion,[0],[0]
Recall from Proposition 2 that u = 0 holds almost surely under H0.,2.1. The Finite Set Independence Criterion,[0],[0]
The asymptotic normality described in Proposition 4 implies that nF̂SIC2 = nJ,2.1. The Finite Set Independence Criterion,[0],[0]
"û
>û converges in distribution to a sum of J dependent weighted χ2 random variables.",2.1. The Finite Set Independence Criterion,[0],[0]
The dependence comes from the fact that the coordinates û1 . . .,2.1. The Finite Set Independence Criterion,[0],[0]
", ûJ of û all depend on the sample Zn.",2.1. The Finite Set Independence Criterion,[0],[0]
"This null distribution is not analytically tractable, and requires a large number of simulations to compute the rejection threshold",2.1. The Finite Set Independence Criterion,[0],[0]
Tα for a given significance value α.,2.1. The Finite Set Independence Criterion,[0],[0]
"For the purpose of an independence test, we will consider a normalized variant of F̂SIC2, which we call N̂FSIC2, whose tractable asymptotic null distribution is χ2(J), the
chi-squared distribution with J degrees of freedom.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We then show that the independence test defined by N̂FSIC2 is consistent.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"These results are given in Theorem 5.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 5 (Independence test based on N̂FSIC2 is consistent).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn, where Σ is defined in Proposition 4.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that VJ = {(vi,wi)}Ji=1 ∼ η where η is absolutely continuous wrt the Lebesgue measure.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The N̂FSIC2 statistic is
defined as λ̂n := nû> ( Σ̂ + γnI )−1",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û where γn ≥ 0 is a
regularization parameter.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assumption A holds.
2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Σ is invertible η-almost surely.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
3.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"limn→∞ γn = 0.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, for any k, l and VJ satisfying the assumptions,
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, λ̂n d→ χ2(J) as n→∞. 2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H1, for any r ∈ R, limn→∞ P ( λ̂n ≥ r ) = 1
η-almost surely.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"That is, the independence test based on N̂FSIC2 is consistent.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proof (sketch) .,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, nû>(Σ̂ + γnI)−1û asymptotically follows χ2(J) because √ nû is asymptotically normally distributed (see Proposition 4).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Claim 2 builds on the result in Proposition 2 stating that u 6= 0 under H1; it follows using the convergence of û to u.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The full proof can be found in Appendix E.
Theorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for any fixed threshold.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Asymptotically the test threshold,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Tα is given by the (1− α)-quantile of χ2(J) and is independent of n.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The assumption on the consistency of Σ̂ is required to obtain the asymptotic chi-squared distribution.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The regularization parameter γn is to ensure that (Σ̂ + γnI)−1 can be stably computed.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, γn requires no tuning, and can be set to be a very small constant.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We emphasize that J need not increase with n for test consistency.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The next proposition states that the computational complexity of the N̂FSIC2 estimator is linear in both the input dimension and sample size, and that it can be expressed in terms of the K =[Kij ] =",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[k(vi,xj)] ∈ RJ×n,L =",2.2. Normalized FSIC and Adaptive Test,[0],[0]
[Lij ] =,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[l(wi,yj)] ∈ RJ×n matrices.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In contrast to typical kernel methods, a large Gram matrix of size n × n is not needed to compute N̂FSIC2.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proposition 6 (An empirical estimator of N̂FSIC2).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let 1n := (1, . . .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
", 1)
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
> ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Rn.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Denote by ◦ the element-wise matrix product.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then,
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û = (K◦L)1nn−1 − (K1n)◦(L1n) n(n−1) .
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
2.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"A consistent estimator for Σ is Σ̂ = ΓΓ >
n where
Γ := (K− n−1K1n1>n ) ◦",2.2. Normalized FSIC and Adaptive Test,[0],[0]
(L− n−1L1n1>n ),2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− ûb1>n , ûb = n−1 (K ◦ L)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
1n,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− n−2 (K1n) ◦ (L1n) .
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Assume that the complexity of the kernel evaluation is linear in the input dimension.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then the test statistic λ̂n =
nû> ( Σ̂ + γnI )−1",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"û can be computed in O(J3 + J2n +
(dx + dy)Jn) time.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Proof (sketch).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Claim 1 for û is straightforward.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The expression for Σ̂ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
The consistency of Σ̂ can be obtained by noting that the finite sample bound for P(‖Σ̂−Σ‖F > t) decreases as n increases.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"This is implicitly shown in Appendix F.2.2 and its following sections.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Although the dependency of the estimator on J is cubic, we empirically observe that only a small value of J is required (see Section 3).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The number of test locations J relates to the number of regions in X × Y of pxy and pxpy that differ (see Figure 1).
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 5 asserts the consistency of the test for any test locations VJ drawn from an absolutely continuous distribution.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, VJ can be further optimized to increase the test power for a fixed sample size.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Our final theoretical result gives a lower bound on the test power of N̂FSIC2 i.e., the probability of correctly rejecting H0.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We will use this lower bound as the objective function to determine VJ and the kernel parameters.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Let ‖ · ‖F be the Frobenius norm.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
Theorem 7 (A lower bound on the test power).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let NFSIC2(X,Y ) :",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"= λn := nu
>Σ−1u.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Assume that
1.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
There exist finite,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bk and Bl such that supk∈K supx,x′∈X |k(x,x′)| ≤",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bk and supl∈L supy,y′∈Y |l(y,y′)| ≤",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Bl.
2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
c̃,2.2. Normalized FSIC and Adaptive Test,[0],[0]
:,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"= supk∈K supl∈L supVJ∈V ‖Σ −1‖F <∞.
Then, for any k ∈ K, l ∈ L, VJ ∈ V , and λn ≥ r, the test power satisfies P ( λ̂n ≥ r ) ≥ L(λn) where
L(λn) = 1− 62e−ξ1γ 2 n(λn−r) 2/n",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− 2e−b0.5nc(λn−r)2/[ξ2n2]
− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ2nn(n−1)] 2 /[ξ4n2(n−1)],
b·c is the floor function, ξ1 := 132c21J2B∗ , B ∗ is a constant depending on onlyBk andBl, ξ2 := 72c22JB 2,B := BkBl,
ξ3 := 8c1B 2J , c3 := 4B2Jc̃2, ξ4 := 28B4J2c21, c1 :=
4B2J √ Jc̃, and c2 := 4B √ Jc̃.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Moreover, for sufficiently large fixed n, L(λn) is increasing in λn.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We provide the proof in Appendix F. To put Theorem 7 into perspective, assume that K ={ (x,v) 7→ exp ( −‖x−v‖ 2
2σ2x
)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
| σ2x ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[σ2x,l, σ2x,u] } =:",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Kg
for some 0 < σ2x,l < σ 2 x,u < ∞ and L ={ (y,w) 7→ exp ( −‖y−w‖ 2
2σ2y
)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
| σ2y ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"[σ2y,l, σ2y,u] }",2.2. Normalized FSIC and Adaptive Test,[0],[0]
=:,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Lg
for some 0 < σ2y,l < σ",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"2 y,u < ∞ are Gaussian kernel classes.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The assumption c̃ < ∞ is a technical condition to guarantee that the test power lower bound is finite for all θ defined by the feasible sets K,L, and V .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Let V ,r := { VJ | ‖vi‖2, ‖wi‖2 ≤
r and ‖vi−vj‖22 + ‖wi−wj‖22 ≥ , for all i 6= j }
.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c̃ <∞ as Kg,Lg, and V ,r are compact.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"We show in Appendix C that the objective function used to choose VJ will discourage any two locations to be in the same neighborhood.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Parameter Tuning Let θ be the collection of all tuning parameters of the test.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
If k ∈ Kg and l ∈,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Lg (i.e., Gaussian kernels), then θ = {σ2x, σ2y, VJ}.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The test power lower bound L(λn) in Theorem 7 is a function of λn = nu>Σ−1u which is the population counterpart of the test statistic λ̂n.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"As in FSIC, it can be shown that λn = 0",2.2. Normalized FSIC and Adaptive Test,[0],[0]
if and only if X are Y are independent (from Proposition 2).,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"According to Theorem 7, for a sufficiently large n, the test power lower bound is increasing in λn.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
One can therefore think of λn (a function of θ) as representing how easily the test rejects H0 given a problem Pxy .,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The higher the λn, the greater the lower bound on the test power, and thus the more likely it is that the test will reject H0 when it is false.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In light of this reasoning, we propose to set θ by maximizing the lower bound on the test power i.e., set θ to θ∗ = arg maxθ L(λn).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
Assume that n is sufficiently large so that λn 7→ L(λn) is an increasing function.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Then, arg maxθ L(λn) = arg maxθ λn.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
That this procedure is also valid under H0 can be seen as follows.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Under H0, θ∗ = arg maxθ 0 will be arbitrary.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Since Theorem 7 guarantees that λ̂n
d→ χ2(J) as n→∞ for any θ, the asymptotic null distribution does not change by using θ∗.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In practice, λn is a population quantity which is unknown.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
We propose dividing the sample Zn into two disjoint sets: training and test sets.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The training set is used to compute λ̂n (an estimate of λn) to optimize for θ∗, and the test set is used for the actual independence test with the optimized θ∗.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"The splitting is to guarantee the independence of θ∗ and the test sample to avoid overfitting.
",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"To better understand the behaviour of N̂FSIC2, we visualize µ̂xy(v,w), µ̂xµy(v,w) and Σ̂(v,w) as a function of one test location (v,w) on a simple toy problem.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this problem, Y = −X + Z where Z ∼ N (0, 0.32) is an independent noise variable.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"As we consider only one location (J = 1), Σ̂(v,w) is a scalar.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The statistic can be written as λ̂n = n,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"(µ̂xy(v,w)−µ̂xµy(v,w))",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"2
Σ̂(v,w) .",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"These components are
shown in Figure 1, where we use Gaussian kernels for both X and Y , and the horizontal and vertical axes correspond to v ∈ R and w ∈ R, respectively.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Intuitively, û(v,w) = µ̂xy(v,w)",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"− µ̂xµy(v,w) captures the difference of the joint distribution and the product of the marginals as a function of (v,w).",2.2. Normalized FSIC and Adaptive Test,[0],[0]
"Squaring û(v,w) and dividing it by the variance shown in Figure 1c gives the statistic (also the parameter tuning objective) shown in Figure 1d.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
The latter figure illustrates that the parameter tuning objective function can be non-convex: non-convexity arises since there are multiple ways to detect the difference between the joint distribution and the product of the marginals.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this case, the lower left and upper right regions equally indicate the largest difference.",2.2. Normalized FSIC and Adaptive Test,[0],[0]
A convex objective would not be able to capture this phenomenon.,2.2. Normalized FSIC and Adaptive Test,[0],[0]
"In this section, we empirically study the performance of the proposed method on both toy (Section 3.1) and real problems (Section 3.2).",3. Experiments,[0],[0]
"We are interested in challenging problems requiring a large number of samples, where a quadratic-time test might be computationally infeasible.",3. Experiments,[0],[0]
Our goal is not to outperform a quadratic-time test with a linear-time test uniformly over all testing problems.,3. Experiments,[0],[0]
"We will find, however, that our test does outperform the quadratic-time test in some cases.",3. Experiments,[0],[0]
"Code is available at https://github.com/wittawatj/fsic-test.
",3. Experiments,[0],[0]
We compare the proposed NFSIC with optimization (NFSICopt) to five multivariate nonparametric tests.,3. Experiments,[0],[0]
"The N̂FSIC2 test without optimization (NFSIC-med) acts as a baseline, allowing the effect of parameter optimization to be clearly
seen.",3. Experiments,[0],[0]
"For pedagogical reason, we consider the original HSIC test of Gretton et al. (2005) denoted by QHSIC, which is a quadratic-time test.",3. Experiments,[0],[0]
Nyström HSIC (NyHSIC) uses a Nyström approximation to the kernel matrices ofX and Y when computing the HSIC statistic.,3. Experiments,[0],[0]
"FHSIC is another variant of HSIC in which a random Fourier feature approximation (Rahimi & Recht, 2008) to the kernel is used.",3. Experiments,[0],[0]
NyHSIC,3. Experiments,[0],[0]
"and FHSIC are studied in Zhang et al. (2017) and can be computed in O(n), with quadratic dependency on the number of inducing points in NyHSIC, and quadratic dependency on the number of random features in FHSIC.",3. Experiments,[0],[0]
"Finally, the Randomized Dependence Coefficient (RDC) proposed in Lopez-Paz et al. (2013) is also considered.",3. Experiments,[0],[0]
The RDC can be seen as the primal form (with random Fourier features) of the kernel canonical correlation analysis of Bach & Jordan (2002) on copula-transformed data.,3. Experiments,[0],[0]
We consider RDC as a linear-time test even though preprocessing by an empirical copula transform costs O((dx,3. Experiments,[0],[0]
+ dy)n log n).,3. Experiments,[0],[0]
We use Gaussian kernel classes Kg and Lg for both X and Y in all the methods.,3. Experiments,[0],[0]
"Except NFSIC-opt, all other tests use full sample to conduct the independence test, where the Gaussian widths σx and σy are set according to the widely used median heuristic i.e., σx = median ({‖xi − xj‖2 | 1 ≤",3. Experiments,[0],[0]
"i < j ≤ n}), and σy is set in the same way using {yi}ni=1.",3. Experiments,[0],[0]
The J locations for NFSICmed are randomly drawn from the standard multivariate normal distribution in each trial.,3. Experiments,[0],[0]
"For a sample of size n, NFSIC-opt uses half the sample for parameter tuning, and the other disjoint half for the test.",3. Experiments,[0],[0]
We permute the sample 300 times in RDC1 and HSIC to simulate from the null distribution and compute the test threshold.,3. Experiments,[0],[0]
The null distributions for FHSIC and NyHSIC are given by a finite sum of weighted χ2(1) random variables given in Eq. 8 of Zhang et al. (2017).,3. Experiments,[0],[0]
"Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1 − α)-quantile of χ2(J).",3. Experiments,[0],[0]
"To provide a fair comparison, we set J = 10, use 10 inducing points in NyHSIC, and 10 random Fourier features in FHSIC and RDC.
Optimization of NFSIC-opt The parameters of NFSIC-opt are σx, σy, and J locations of size (dx + dy)J .",3. Experiments,[0],[0]
We treat all the parameters as a long vector in R2+(dx+dy)J and use gradient ascent to optimize λ̂n/2.,3. Experiments,[0],[0]
We observe that initializing VJ by randomly picking J points from the training sample yields good performance.,3. Experiments,[0],[0]
"The regularization parameter γn in NFSIC is fixed to a small value, and is not optimized.",3. Experiments,[0],[0]
"It is worth emphasizing that the complexity of the optimization procedure is still linear-time.2
1We use a permutation test for RDC, following the authors’ implementation (https://github.com/lopezpaz/ randomized_dependence_coefficient, referred commit: b0ac6c0).
",3. Experiments,[0],[0]
2Our claim on linear runtime (with respect to n) is for the gradient ascent procedure to find a local optimum for θ.,3. Experiments,[0],[0]
"We do not
Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n.",3. Experiments,[0],[0]
"By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed.",3. Experiments,[0],[0]
We refer the reader to Appendix C for a brief investigation of the test power vs. increasing J .,3. Experiments,[0],[0]
The test power does not necessarily monotonically increase with J .,3. Experiments,[0],[0]
"We consider three toy problems.
",3.1. Toy Problems,[0],[0]
1.,3.1. Toy Problems,[0],[0]
Same Gaussian (SG).,3.1. Toy Problems,[0],[0]
"The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx) and Y ∼ N (0, Idy ) where Id is the d× d identity matrix.",3.1. Toy Problems,[0],[0]
"This problem represents a case in which H0 holds.
2.",3.1. Toy Problems,[0],[0]
Sinusoid (Sin).,3.1. Toy Problems,[0],[0]
Let pxy be the probability density of Pxy .,3.1. Toy Problems,[0],[0]
"In the Sinusoid problem, the dependency ofX and Y is characterized by (X,Y ) ∼ pxy(x, y) ∝",3.1. Toy Problems,[0],[0]
"1 + sin(ωx) sin(ωy), where the domains of X ,Y = (−π, π) and ω is the frequency of the sinusoid.",3.1. Toy Problems,[0],[0]
"As the frequency ω increases, the drawn sample becomes more similar to a sample drawn from Uniform((−π, π)2).",3.1. Toy Problems,[0],[0]
"That is, the higher ω, the harder to detect the dependency between X and Y .",3.1. Toy Problems,[0],[0]
This problem was studied in Sejdinovic et al. (2013).,3.1. Toy Problems,[0],[0]
Plots of the density for a few values of ω are shown in Figures 6 and 7 in the appendix.,3.1. Toy Problems,[0],[0]
The main characteristic of interest in this problem is the local change in the density function.,3.1. Toy Problems,[0],[0]
3.,3.1. Toy Problems,[0],[0]
Gaussian Sign (GSign).,3.1. Toy Problems,[0],[0]
"In this problem, Y = |Z|∏dxi=1 sgn(Xi), where X ∼ N (0, Idx), sgn(·) is the sign function, and Z ∼ N (0, 1) serves as a source of noise.",3.1. Toy Problems,[0],[0]
"The full interaction of X = (X1, . . .",3.1. Toy Problems,[0],[0]
", Xdx) is what makes the problem challenging.",3.1. Toy Problems,[0],[0]
"That is, Y is dependent on X , yet it is independent of any proper subset of {X1, . . .",3.1. Toy Problems,[0],[0]
", Xd}.",3.1. Toy Problems,[0],[0]
"Thus, simultaneous consideration of all the coordinates of X is required to successfully detect the dependency.
",3.1. Toy Problems,[0],[0]
We fix n = 4000 and vary the problem parameters.,3.1. Toy Problems,[0],[0]
"Each problem is repeated for 300 trials, and the sample is redrawn each time.",3.1. Toy Problems,[0],[0]
The significance level α is set to 0.05.,3.1. Toy Problems,[0],[0]
"The re-
claim a linear runtime to find a global optimum.
sults are shown in Figure 2.",3.1. Toy Problems,[0],[0]
"It can be seen that in the SG problem (Figure 2b) where H0 holds, all the tests achieve roughly correct type-I errors at α = 0.05.",3.1. Toy Problems,[0],[0]
"In particular, we point out that NFSIC-opt’s rejection rate is well controlled as the sample used for testing and the sample used for parameter tuning are independent.",3.1. Toy Problems,[0],[0]
"The rejection rate would have been much higher had we done the optimization and testing on the same sample (i.e., overfitting).",3.1. Toy Problems,[0],[0]
"In the Sin problem, NFSIC-opt achieves high test power for all considered ω = 1, . . .",3.1. Toy Problems,[0],[0]
", 6, highlighting its strength in detecting local changes in the joint density.",3.1. Toy Problems,[0],[0]
The performance of NFSIC-med is significantly lower than that of NFSIC-opt.,3.1. Toy Problems,[0],[0]
This phenomenon clearly emphasizes the importance of the optimization to place the locations at the relevant regions in X×Y .,3.1. Toy Problems,[0],[0]
"RDC has a remarkably high performance in both Sin and GSign (Figure 2c, 2d) despite no parameter tuning.",3.1. Toy Problems,[0],[0]
"The ability to simultaneously consider interacting features of NFSIC-opt is indicated by its superior test power in GSign, especially at the challenging settings of dx = 5, 6.
NFSIC vs. QHSIC.",3.1. Toy Problems,[0],[0]
We observe that NFSIC-opt outperforms the quadratic-time QHSIC in these two problems.,3.1. Toy Problems,[0],[0]
QHSIC is defined as the RKHS norm of the witness function u (see (2)).,3.1. Toy Problems,[0],[0]
"Intuitively, one can think of the RKHS norm as taking into account all the locations (v,w).",3.1. Toy Problems,[0],[0]
"By contrast, the proposed NFSIC evaluates the witness function at J locations.",3.1. Toy Problems,[0],[0]
"If the differences in pxy and pxpy are local (e.g., Sin problem), or there are interacting features (e.g., GSign problem), then only small regions in the space of (X,Y ) are relevant in detecting the difference of pxy and pxpy.",3.1. Toy Problems,[0],[0]
"In these cases, pinpointing exact test locations by the optimization of NFSIC performs well.",3.1. Toy Problems,[0],[0]
"On the other hand, taking into account all possible test locations as done implicitly in QHSIC also integrates over regions where the difference between pxy and pxpy is small, resulting in a weaker indication of dependence.",3.1. Toy Problems,[0],[0]
"Whether QHSIC is better than NFSIC depends heavily on the problem, and there is no one best answer.",3.1. Toy Problems,[0],[0]
"If the difference between pxy and pxpy is large only in localized regions, then the proposed linear time statistic has an advantage.",3.1. Toy Problems,[0],[0]
"If the difference is spatially diffuse, then QHSIC has an advantage.",3.1. Toy Problems,[0],[0]
"No existing work has proposed a procedure to optimally tune kernel parameters for QHSIC; by contrast, NFSIC has a clearly defined objective for parameter tuning.
",3.1. Toy Problems,[0],[0]
"To investigate the sample efficiency of all the tests, we fix dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and increase n. Figure 3 shows the results.",3.1. Toy Problems,[0],[0]
The quadratic dependency on n in QHSIC makes it infeasible both in terms of memory and runtime to consider n larger than 6000 (Figure 3a).,3.1. Toy Problems,[0],[0]
"By constrast, although not the most time-efficient, NFSIC-opt has the highest sample-efficiency for GSign, and for Sin in the low-sample regime, significantly outperforming QHSIC.",3.1. Toy Problems,[0],[0]
"Despite the small additional overhead from the optimization, we are yet able to conduct an accurate test with n = 105, dx = dy = 250 in less than 100 seconds.",3.1. Toy Problems,[0],[0]
We observe in Figure 3b that the two NFSIC variants have correct type-I errors across all sample sizes.,3.1. Toy Problems,[0],[0]
We recall from Theorem 5 that the NFSIC test with random test locations will asymptotically reject H0 if it is false.,3.1. Toy Problems,[0],[0]
"A demonstration of this property is given in Figure 3c, where the test power of NFSIC-med eventually reaches 1 with n higher than 105.",3.1. Toy Problems,[0],[0]
"We now examine the performance of our proposed test on real problems.
",3.2. Real Problems,[0],[0]
Million Song Data (MSD),3.2. Real Problems,[0],[0]
"We consider a subset of the Million Song Data3 (Bertin-Mahieux et al., 2011), in which each song (X) out of 515,345 is represented by 90 features, of which 12 features are timbre average (over all segments) of the song, and 78 features are timbre covariance.",3.2. Real Problems,[0],[0]
Most of the songs are western commercial tracks from 1922 to 2011.,3.2. Real Problems,[0],[0]
The goal is to detect the dependency between each song and its year of release (Y ).,3.2. Real Problems,[0],[0]
"We set α = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial.",3.2. Real Problems,[0],[0]
Other settings are the same as in the toy problems.,3.2. Real Problems,[0],[0]
"To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold.",3.2. Real Problems,[0],[0]
Figure 4b shows the test powers as n increases from 500 to 2000.,3.2. Real Problems,[0],[0]
"To simulate the case whereH0 holds in the problem, we permute the sample to break the dependency of X and Y .",3.2. Real Problems,[0],[0]
"The results are shown in Figure 5 in the appendix.
",3.2. Real Problems,[0],[0]
"Evidently, NFSIC-opt has the highest test power among all
3Million Song Data subset: https://archive.ics.",3.2. Real Problems,[0],[0]
"uci.edu/ml/datasets/YearPredictionMSD.
the linear-time tests for all the sample sizes.",3.2. Real Problems,[0],[0]
Its test power is second to only QHSIC.,3.2. Real Problems,[0],[0]
We recall that NFSIC-opt uses half of the sample for parameter tuning.,3.2. Real Problems,[0],[0]
"Thus, at n = 500, the actual sample for testing is 250, which is relatively small.",3.2. Real Problems,[0],[0]
"The fact that there is a vast power gain from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that the optimization procedure can perform well even at a lower sample sizes.
",3.2. Real Problems,[0],[0]
Videos and Captions,3.2. Real Problems,[0],[0]
"Our last problem is based on the VideoStory46K4 dataset (Habibian et al., 2014).",3.2. Real Problems,[0],[0]
"The dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding text captions (Y ) uploaded by the users.",3.2. Real Problems,[0],[0]
Each video is represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors of Wang & Schmid (2013).,3.2. Real Problems,[0],[0]
Each caption is represented as a bag of words with each feature being the frequency of one word.,3.2. Real Problems,[0],[0]
"After filtering only words which occur in at least six video captions, we obtain dy = 1878 words.",3.2. Real Problems,[0],[0]
We examine the test powers as n increases from 2000 to 8000.,3.2. Real Problems,[0],[0]
The results are given in Figure 4.,3.2. Real Problems,[0],[0]
The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000.,3.2. Real Problems,[0],[0]
"QHSIC performs exceptionally well on this problem, achieving a maximum power throughout.",3.2. Real Problems,[0],[0]
"NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting.
",3.2. Real Problems,[0],[0]
4VideoStory46K dataset: https://ivi.fnwi.uva.nl/ isis/mediamill/datasets/videostory.php.,3.2. Real Problems,[0],[0]
We thank the Gatsby Charitable Foundation for the financial support.,Acknowledgement,[0],[0]
"The major part of this work was carried out while Zoltán Szabó was a research associate at the Gatsby Computational Neuroscience Unit, University College London.",Acknowledgement,[0],[0]
"A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed.",abstractText,[0],[0]
"The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features).",abstractText,[0],[0]
"These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n).",abstractText,[0],[0]
"The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most.",abstractText,[0],[0]
"Consistency of the independence test is established, for an appropriate choice of features.",abstractText,[0],[0]
"In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.",abstractText,[0],[0]
An Adaptive Test of Independence with Analytic Kernel Embeddings,title,[0],[0]
There is a fundamental tension in decision making between choosing the action that has highest expected utility and avoiding “starving” the other actions.,1. Introduction,[0],[0]
"The issue arises in the context of the exploration–exploitation dilemma (Thrun, 1992), non-stationary decision problems (Sutton, 1990), and when interpreting observed decisions (Baker et al., 2007).
",1. Introduction,[0],[0]
"In reinforcement learning, an approach to addressing the tension is the use of softmax operators for value-function optimization, and softmax policies for action selection.",1. Introduction,[0],[0]
"Examples include value-based methods such as SARSA (Rummery & Niranjan, 1994) or expected SARSA (Sutton & Barto, 1998; Van Seijen et al., 2009), and policy-search methods such as REINFORCE (Williams, 1992).
",1. Introduction,[0],[0]
"1Brown University, USA.",1. Introduction,[0],[0]
"Correspondence to: Kavosh Asadi <kavosh@brown.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"An ideal softmax operator is a parameterized set of operators that:
1. has parameter settings that allow it to approximate maximization arbitrarily accurately to perform reward-seeking behavior;
2. is a non-expansion for all parameter settings ensuring convergence to a unique fixed point;
3. is differentiable to make it possible to improve via gradient-based optimization; and
4.",1. Introduction,[0],[0]
"avoids the starvation of non-maximizing actions.
",1. Introduction,[0],[0]
"Let X = x1, . . .",1. Introduction,[0],[0]
", xn be a vector of values.",1. Introduction,[0],[0]
"We define the following operators:
max(X) = max i∈{1,...,n} xi ,
mean(X) = 1
n
n∑
i=1
xi ,
eps (X) = mean(X) + (1− ) max(X) ,
boltzβ(X) =",1. Introduction,[0],[0]
∑n i=1,1. Introduction,[0],[0]
"xi e βxi
∑n i=1 e βxi .
",1. Introduction,[0],[0]
"The first operator, max(X), is known to be a non-expansion (Littman & Szepesvári, 1996).",1. Introduction,[0],[0]
"However, it is non-differentiable (Property 3), and ignores non-maximizing selections (Property 4).
",1. Introduction,[0],[0]
"The next operator, mean(X), computes the average of its inputs.",1. Introduction,[0],[0]
"It is differentiable and, like any operator that takes a fixed convex combination of its inputs, is a non-expansion.",1. Introduction,[0],[0]
"However, it does not allow for maximization (Property 1).
",1. Introduction,[0],[0]
"The third operator eps (X), commonly referred to as epsilon greedy (Sutton & Barto, 1998), interpolates between max and mean.",1. Introduction,[0],[0]
"The operator is a non-expansion, because it is a convex combination of two non-expansion operators.",1. Introduction,[0],[0]
"But it is non-differentiable (Property 3).
",1. Introduction,[0],[0]
The Boltzmann operator boltzβ(X) is differentiable.,1. Introduction,[0],[0]
"It also approximates max as β → ∞, and mean as β → 0.",1. Introduction,[0],[0]
"However, it is not a non-expansion (Property 2), and therefore, prone to misbehavior as will be shown in the next section.
",1. Introduction,[0],[0]
"In the following section, we provide a simple example illustrating why the non-expansion property is important, especially in the context of planning and on-policy learning.",1. Introduction,[0],[0]
We then present a new softmax operator that is similar to the Boltzmann operator yet is a non-expansion.,1. Introduction,[0],[0]
"We prove several critical properties of this new operator, introduce a new softmax policy, and present empirical results.",1. Introduction,[0],[0]
We first show that boltzβ can lead to problematic behavior.,2. Boltzmann Misbehaves,[0],[0]
"To this end, we ran SARSA with Boltzmann softmax policy (Algorithm 1) on the MDP shown in Figure 1.",2. Boltzmann Misbehaves,[0],[0]
The edges are labeled with a transition probability (unsigned) and a reward number (signed).,2. Boltzmann Misbehaves,[0],[0]
"Also, state s2 is a terminal state, so we only consider two action values, namely Q̂(s1, a) and Q̂(s2, b).",2. Boltzmann Misbehaves,[0],[0]
"Recall that the Boltzmann softmax policy assigns the following probability to each action:
π(a|s) = e βQ̂(s,a)
∑ a e βQ̂(s,a) .
",2. Boltzmann Misbehaves,[0],[0]
"Algorithm 1 SARSA with Boltzmann softmax policy Input: initial Q̂(s, a) ∀s ∈",2. Boltzmann Misbehaves,[0],[0]
S ∀a ∈,2. Boltzmann Misbehaves,[0],[0]
"A, α, and β for each episode do
Initialize s a ∼ Boltzmann with parameter β repeat
Take action a, observe r, s′ a ′",2. Boltzmann Misbehaves,[0],[0]
"∼ Boltzmann with parameter β Q̂(s, a)← Q̂(s, a) + α",2. Boltzmann Misbehaves,[0],[0]
"[ r + γQ̂(s′, a′)− Q̂(s, a) ]
s← s′ , a← a′ until s is terminal
end for
",2. Boltzmann Misbehaves,[0],[0]
"In Figure 2, we plot state–action value estimates at the end of each episode of a single run (smoothed by averaging over ten consecutive points).",2. Boltzmann Misbehaves,[0],[0]
We set α = .1 and β = 16.55.,2. Boltzmann Misbehaves,[0],[0]
"The value estimates are unstable.
",2. Boltzmann Misbehaves,[0],[0]
"SARSA is known to converge in the tabular setting using -greedy exploration (Littman & Szepesvári, 1996), under decreasing exploration (Singh et al., 2000), and to a region in the function-approximation setting (Gordon, 2001).",2. Boltzmann Misbehaves,[0],[0]
"There are also variants of the SARSA update rule that converge more generally (Perkins & Precup, 2002; Baird & Moore, 1999; Van Seijen et al., 2009).",2. Boltzmann Misbehaves,[0],[0]
"However, this example is the first, to our knowledge, to show that SARSA fails to converge in the tabular setting with Boltzmann policy.",2. Boltzmann Misbehaves,[0],[0]
The next section provides background for our analysis of the example.,2. Boltzmann Misbehaves,[0],[0]
"A Markov decision process (Puterman, 1994), or MDP, is specified by the tuple 〈S,A,R,P, γ〉, where S is the set of states and A is the set of actions.",3. Background,[0],[0]
"The functions R : S ×A → R and P : S × A× S → [0, 1] denote the reward and transition dynamics of the MDP.",3. Background,[0],[0]
"Finally, γ ∈",3. Background,[0],[0]
"[0, 1), the discount rate, determines the relative importance of immediate reward as opposed to the rewards received in the future.
",3. Background,[0],[0]
A typical approach to finding a good policy is to estimate how good it is to be in a particular state—the state value function.,3. Background,[0],[0]
"The value of a particular state s given a policy π and initial action a is written Qπ(s, a).",3. Background,[0],[0]
"We define the optimal value of a state–action pair Q?(s, a) =",3. Background,[0],[0]
"maxπ Qπ(s, a).",3. Background,[0],[0]
"It is possible to defineQ?(s, a) recursively and as a function of the optimal value of the other state–action pairs:
Q?(s, a) =",3. Background,[0],[0]
"R(s, a)+ ∑
s′∈S γ P(s, a, s′) max a′",3. Background,[0],[0]
"Q?(s′, a′) .
",3. Background,[0],[0]
"Bellman equations, such as the above, are at the core of many reinforcement-learning algorithms such as Value Iteration (Bellman, 1957).",3. Background,[0],[0]
"The algorithm computes the
value of the best policy in an iterative fashion:
Q̂(s, a)←",3. Background,[0],[0]
"R(s, a) +",3. Background,[0],[0]
"γ ∑
s′∈S P(s, a, s′) max a′ Q̂(s′, a′).
",3. Background,[0],[0]
"Regardless of its initial value, Q̂ will converge to Q∗.
Littman & Szepesvári (1996) generalized this algorithm by replacing the max operator by any arbitrary operator ⊗ , resulting in the generalized value iteration (GVI) algorithm with the following update rule:
Q̂(s, a)←",3. Background,[0],[0]
"R(s, a)+γ ∑
s′∈S γP(s, a, s′)
⊗
a′
Q̂(s′, a′).",3. Background,[0],[0]
"(1)
Algorithm 2 GVI algorithm Input: initial Q̂(s, a) ∀s ∈ S",3. Background,[0],[0]
∀a ∈,3. Background,[0],[0]
"A and δ ∈ R+ repeat
diff← 0 for each s ∈ S do
for each a ∈",3. Background,[0],[0]
"A do Qcopy ← Q̂(s, a) Q̂(s, a)←∑s′∈S R(s, a, s′)
+ γP(s, a, s′)⊗ Q̂(s′, .)",3. Background,[0],[0]
"diff← max { diff, |Qcopy − Q̂(s, a)| }
end for end for
until diff < δ
Crucially, convergence of GVI to a unique fixed point follows if operator ⊗ is a non-expansion with respect to the infinity norm: ∣∣∣ ⊗
a
Q̂(s, a)− ⊗
a
Q̂′(s, a)",3. Background,[0],[0]
∣∣∣ ≤,3. Background,[0],[0]
"max
a
∣∣∣Q̂(s, a)− Q̂′(s, a) ∣∣∣,
for any Q̂, Q̂′ and s. As mentioned earlier, the max operator is known to be a non-expansion, as illustrated in Figure 3.",3. Background,[0],[0]
mean and eps operators are also non-expansions.,3. Background,[0],[0]
"Therefore, each of these operators can play the role of ⊗ in GVI, resulting in convergence to the corresponding unique
fixed point.",3. Background,[0],[0]
"However, the Boltzmann softmax operator, boltzβ , is not a non-expansion (Littman, 1996).",3. Background,[0],[0]
Note that we can relate GVI to SARSA by observing that SARSA’s update is a stochastic implementation of GVI’s update.,3. Background,[0],[0]
"Under a Boltzmann softmax policy π, the target of the (expected) SARSA update is the following:
E π
[ r + γQ̂(s′, a′) ∣∣s, a ] =
R(s, a) + γ ∑
s′∈S P(s, a, s′)
∑
a′∈A π(a′|s′)Q̂(s′, a′) ︸",3. Background,[0],[0]
"︷︷ ︸ boltzβ ( Q̂(s′,·) ) .
",3. Background,[0],[0]
This matches the GVI update (1) when ⊗ = boltzβ .,3. Background,[0],[0]
"Although it has been known for a long time that the Boltzmann operator is not a non-expansion (Littman, 1996), we are not aware of a published example of an MDP for which two distinct fixed points exist.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"The MDP presented in Figure 1 is the first example where, as shown in Figure 4, GVI under boltzβ has two distinct fixed points.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"We also show, in Figure 5, a vector field visualizing GVI updates under boltzβ=16.55.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
The updates can move the current estimates farther from the fixed points.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
The behavior of SARSA (Figure 2) results from the algorithm stochastically bouncing back and forth between the two fixed points.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
"When the learning algorithm performs a sequence of noisy updates, it moves from a fixed point to the other.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
"As we will show later, planning will also progress extremely slowly near the fixed points.",4. Boltzmann Has Multiple Fixed Points,[0],[0]
The lack of the non-expansion property leads to multiple fixed points and ultimately a misbehavior in learning and planning.,4. Boltzmann Has Multiple Fixed Points,[0],[0]
"We advocate for an alternative softmax operator defined as follows:
mmω(X) = log( 1n
∑n i=1 e ωxi)
ω ,
which can be viewed as a particular instantiation of the quasi-arithmetic mean (Beliakov et al., 2016).",5. Mellowmax and its Properties,[0],[0]
"It can also
be derived from information theoretical principles as a way of regularizing policies with a cost function defined by KL divergence (Todorov, 2006; Rubin et al., 2012; Fox et al., 2016).",5. Mellowmax and its Properties,[0],[0]
"Note that the operator has previously been utilized in other areas, such as power engineering (Safak, 1993).
",5. Mellowmax and its Properties,[0],[0]
"We show that mmω , which we refer to as mellowmax, has the desired properties and that it compares quite favorably to boltzβ in practice.",5. Mellowmax and its Properties,[0],[0]
"We prove that mmω is a non-expansion (Property 2), and therefore, GVI and SARSA under mmω are guaranteed to converge to a unique fixed point.
",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Let X = x1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", xn and Y = y1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", yn be two vectors of values.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Let ∆i = xi − yi for i ∈ {1, . . .",5.1. Mellowmax is a Non-Expansion,[0],[0]
", n} be the difference of the ith components of the two vectors.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Also, let i∗ be the index with the maximum component-wise difference, i∗ = argmaxi ∆i.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"For simplicity, we assume that i∗ is unique and ω > 0.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"Also, without loss of generality, we assume that xi∗ − yi∗ ≥ 0.",5.1. Mellowmax is a Non-Expansion,[0],[0]
"It follows that:
∣∣mmω(X)−mmω(Y) ∣∣
= ∣∣ log( 1
n
n∑
i=1
",5.1. Mellowmax is a Non-Expansion,[0],[0]
"eωxi)/ω − log( 1 n
n∑
i=1
eωyi)/ω ∣∣
= ∣∣ log 1 n
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωxi
1 n",5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ωyi /ω ∣∣
= ∣∣ log
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ω",5.1. Mellowmax is a Non-Expansion,[0],[0]
( yi+∆i ),5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi /ω ∣∣
≤ ∣∣ log
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ω ( yi+∆i∗ )",5.1. Mellowmax is a Non-Expansion,[0],[0]
∑n i=1,5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi /ω ∣∣
= ∣∣ log e
ω∆i∗ ∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e ωyi
∑n i=1",5.1. Mellowmax is a Non-Expansion,[0],[0]
"e
ωyi /ω ∣∣
= ∣∣ log(eω∆i∗ )/ω ∣∣ = ∣∣∆i∗ ∣∣ = max i ∣∣xi − yi ∣∣ ,
allowing us to conclude that mellowmax is a non-expansion under the infinity norm.",5.1. Mellowmax is a Non-Expansion,[0],[0]
Mellowmax includes parameter settings that allow for maximization (Property 1) as well as for minimization.,5.2. Maximization,[0],[0]
"In particular, as ω goes to infinity, mmω acts like max.
Let m = max(X) and let W = |{xi = m|i ∈",5.2. Maximization,[0],[0]
"{1, . . .",5.2. Maximization,[0],[0]
", n}}|.",5.2. Maximization,[0],[0]
"Note that W ≥ 1 is the number of maximum values (“winners”) in X. Then:
lim ω→∞ mmω(X) = lim ω→∞
log( 1n ∑n i=1 e ωxi)
ω
= lim ω→∞
log( 1ne ωm",5.2. Maximization,[0],[0]
∑n i=1,5.2. Maximization,[0],[0]
"e ω(xi−m))
",5.2. Maximization,[0],[0]
"ω
= lim ω→∞
log( 1ne ωmW )
ω
= lim ω→∞ log(eωm)− log(n) + log(W ) ω
= m+ lim ω→∞",5.2. Maximization,[0],[0]
− log(n),5.2. Maximization,[0],[0]
"+ log(W ) ω = m = max(X) .
",5.2. Maximization,[0],[0]
"That is, the operator acts more and more like pure maximization as the value of ω is increased.",5.2. Maximization,[0],[0]
"Conversely, as ω goes to −∞, the operator approaches the minimum.",5.2. Maximization,[0],[0]
"We can take the derivative of mellowmax with respect to each one of the arguments xi and for any non-zero ω:
∂mmω(X) ∂xi = eωxi∑n i=1 e ωxi ≥ 0 .
",5.3. Derivatives,[0],[0]
"Note that the operator is non-decreasing in each component of X.
Moreover, we can take the derivative of mellowmax with respect to ω.",5.3. Derivatives,[0],[0]
We define nω(X) = log(,5.3. Derivatives,[0],[0]
1n ∑n i=1,5.3. Derivatives,[0],[0]
"e
ωxi) and dω(X)",5.3. Derivatives,[0],[0]
= ω.,5.3. Derivatives,[0],[0]
"Then:
∂nω(X)",5.3. Derivatives,[0],[0]
∂ω,5.3. Derivatives,[0],[0]
"=
∑n i=1",5.3. Derivatives,[0],[0]
"xie ωxi
∑n i=1 e ωxi and ∂dω(X) ∂ω = 1 ,
and so:
∂mmω(X) ∂ω
= ∂nω(X)",5.3. Derivatives,[0],[0]
"∂ω dω(X)− nω(X) ∂dω(X) ∂ω
dω(X)2 ,
ensuring differentiablity of the operator (Property 3).",5.3. Derivatives,[0],[0]
"Because of the division by ω in the definition of mmω , the parameter ω cannot be set to zero.",5.4. Averaging,[0],[0]
"However, we can examine the behavior of mmω as ω approaches zero and show that the operator computes an average in the limit.
",5.4. Averaging,[0],[0]
"Since both the numerator and denominator go to zero as ω goes to zero, we will use L’Hôpital’s rule and the derivative given in the previous section to derive the value in the limit:
lim ω→0 mmω(X) =",5.4. Averaging,[0],[0]
"lim ω→0
log( 1n ∑n i=1 e ωxi)
ω
L’Hôpital = lim
ω→0
1 n",5.4. Averaging,[0],[0]
∑n i=1,5.4. Averaging,[0],[0]
"xie ωxi
1 n",5.4. Averaging,[0],[0]
"∑n i=1 e ωxi
= 1
n
n∑
i=1
xi = mean(X) .
",5.4. Averaging,[0],[0]
"That is, as ω gets closer to zero, mmω(X) approaches the mean of the values in X.",5.4. Averaging,[0],[0]
"As described, mmω computes a value for a list of numbers somewhere between its minimum and maximum.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"However, it is often useful to actually provide a probability distribution over the actions such that (1) a non-zero probability mass is assigned to each action, and (2) the resulting expected value equals the computed value.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Such a probability distribution can then be used for action selection in algorithms such as SARSA.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"In this section, we address the problem of identifying such a probability distribution as a maximum entropy problem—over all distributions that satisfy the properties above, pick the one that maximizes information entropy (Cover & Thomas, 2006; Peters et al., 2010).",6. Maximum Entropy Mellowmax Policy,[0],[0]
"We formally define the maximum entropy mellowmax policy of a state s as:
πmm(s) = argmin π
∑ a∈A π(a|s) log ( π(a|s) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"(2)
subject to { ∑ a∈A π(a|s)Q̂(s, a) = mmω(Q̂(s, .))
π(a|s) ≥ 0∑ a∈A π(a|s) = 1 .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Note that this optimization problem is convex and can be solved reliably using any numerical convex optimization library.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"One way of finding the solution, which leads to an interesting policy form, is to use the method of Lagrange
multipliers.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Here, the Lagrangian is:
L(π, λ1, λ2) = ∑
a∈A π(a|s) log
( π(a|s) )
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"−λ1 (∑
a∈A π(a|s)− 1
)
−λ2 (∑
a∈A π(a|s)Q̂(s, a)−mmω
( Q̂(s, .) )) .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Taking the partial derivative of the Lagrangian with respect to each π(a|s) and setting them to zero, we obtain:
∂L ∂π(a|s) = log ( π(a|s) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"+1−λ1−λ2Q̂(s, a) = 0 ∀",6. Maximum Entropy Mellowmax Policy,[0],[0]
"a ∈ A .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"These |A| equations, together with the two linear constraints in (2), form |A| + 2 equations to constrain the |A| + 2 variables π(a|s) ∀a ∈ A and the two Lagrangian multipliers λ1 and λ2.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Solving this system of equations, the probability of taking an action under the maximum entropy mellowmax policy has the form:
πmm(a|s) = eβQ̂(s,a)∑ a∈A",6. Maximum Entropy Mellowmax Policy,[0],[0]
"e βQ̂(s,a) ∀a ∈ A ,
where β is a value for which:
∑ a∈A eβ ( Q̂(s,a)−mmωQ̂(s,.) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"( Q̂(s, a)−mmωQ̂(s, .) )",6. Maximum Entropy Mellowmax Policy,[0],[0]
"= 0 .
",6. Maximum Entropy Mellowmax Policy,[0],[0]
The argument for the existence of a unique root is simple.,6. Maximum Entropy Mellowmax Policy,[0],[0]
"As β → ∞ the term corresponding to the best action dominates, and so, the function is positive.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Conversely, as β → −∞ the term corresponding to the action with lowest utility dominates, and so the function is negative.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Finally, by taking the derivative, it is clear that the function is monotonically increasing, allowing us to conclude that there exists only a single root.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Therefore, we can find β easily using any root-finding algorithm.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"In particular, we use Brent’s method (Brent, 2013) available in the Numpy library of Python.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"This policy has the same form as Boltzmann softmax, but with a parameter β whose value depends indirectly on ω.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"This mathematical form arose not from the structure of mmω , but from maximizing the entropy.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"One way to view the use of the mellowmax operator, then, is as a form of Boltzmann policy with a temperature parameter chosen adaptively in each state to ensure that the non-expansion property holds.
",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Finally, note that the SARSA update under the maximum entropy mellowmax policy could be thought of as a
stochastic implementation of the GVI update under the mmω operator:
E πmm
[ r + γQ̂(s′, a′) ∣∣s, a ] =
∑ s′∈S",6. Maximum Entropy Mellowmax Policy,[0],[0]
"R(s, a, s′) + γP(s, a, s′) ∑ a′∈A πmm(a ′|s′)Q̂(s′, a′) ]
︸ ︷︷ ︸ mmω",6. Maximum Entropy Mellowmax Policy,[0],[0]
"( Q̂(s′,.) )
",6. Maximum Entropy Mellowmax Policy,[0],[0]
due to the first constraint of the convex optimization problem (2).,6. Maximum Entropy Mellowmax Policy,[0],[0]
"Because mellowmax is a non-expansion, SARSA with the maximum entropy mellowmax policy is guaranteed to converge to a unique fixed point.",6. Maximum Entropy Mellowmax Policy,[0],[0]
"Note also that, similar to other variants of SARSA, the algorithm simply bootstraps using the value of the next state while implementing the new policy.",6. Maximum Entropy Mellowmax Policy,[0],[0]
We observed that in practice computing mellowmax can yield overflow if the exponentiated values are large.,7. Experiments on MDPs,[0],[0]
"In this case, we can safely shift the values by a constant before exponentiating them due to the following equality:
log( 1n ∑n i=1 e ωxi)
ω =",7. Experiments on MDPs,[0],[0]
"c+
log( 1n ∑n i=1",7. Experiments on MDPs,[0],[0]
"e ω(xi−c))
",7. Experiments on MDPs,[0],[0]
"ω .
",7. Experiments on MDPs,[0],[0]
"A value of c = maxi xi usually avoids overflow.
",7. Experiments on MDPs,[0],[0]
We repeat the experiment from Figure 5 for mellowmax with ω = 16.55 to get a vector field.,7. Experiments on MDPs,[0],[0]
"The result, presented in Figure 6, show a rapid and steady convergence towards the unique fixed point.",7. Experiments on MDPs,[0],[0]
"As a result, GVI under mmω can terminate significantly faster than GVI under boltzβ , as illustrated in Figure 7.
",7. Experiments on MDPs,[0],[0]
We present three additional experiments.,7. Experiments on MDPs,[0],[0]
The first experiment investigates the behavior of GVI with the softmax operators on randomly generated MDPs.,7. Experiments on MDPs,[0],[0]
The second experiment evaluates the softmax policies when used in SARSA with a tabular representation.,7. Experiments on MDPs,[0],[0]
"The last
experiment is a policy gradient experiment where a deep neural network, with a softmax output layer, is used to directly represent the policy.",7. Experiments on MDPs,[0],[0]
The example in Figure 1 was created carefully by hand.,7.1. Random MDPs,[0],[0]
It is interesting to know whether such examples are likely to be encountered naturally.,7.1. Random MDPs,[0],[0]
"To this end, we constructed 200 MDPs as follows: We sampled |S| from {2, 3, ..., 10} and |A| from {2, 3, 4, 5} uniformly at random.",7.1. Random MDPs,[0],[0]
"We initialized the transition probabilities by sampling uniformly from [0, .01].",7.1. Random MDPs,[0],[0]
"We then added to each entry, with probability 0.5, Gaussian noise with mean 1 and variance 0.1.",7.1. Random MDPs,[0],[0]
"We next added, with probability 0.1, Gaussian noise with mean 100 and variance 1.",7.1. Random MDPs,[0],[0]
"Finally, we normalized the raw values to ensure that we get a transition matrix.",7.1. Random MDPs,[0],[0]
"We did a similar process for rewards, with the difference that we divided each entry by the maximum entry and multiplied by 0.5 to ensure that Rmax = 0.5 .
",7.1. Random MDPs,[0],[0]
We measured the failure rate of GVI under boltzβ and mmω by stopping GVI when it did not terminate in 1000 iterations.,7.1. Random MDPs,[0],[0]
We also computed the average number of iterations needed before termination.,7.1. Random MDPs,[0],[0]
A summary of results is presented in the table below.,7.1. Random MDPs,[0],[0]
"Mellowmax outperforms Boltzmann based on the three measures provided below.
",7.1. Random MDPs,[0],[0]
"MDPs, no terminate MDPs, > 1 fixed points average iterations
boltzβ 8 of 200 3 of 200 231.65 mmω 0 0 201.32",7.1. Random MDPs,[0],[0]
We evaluated SARSA on the multi-passenger taxi domain introduced by Dearden et al. (1998).,7.2. Multi-passenger Taxi Domain,[0],[0]
"(See Figure 8.)
",7.2. Multi-passenger Taxi Domain,[0],[0]
One challenging aspect of this domain is that it admits many locally optimal policies.,7.2. Multi-passenger Taxi Domain,[0],[0]
Exploration needs to be set carefully to avoid either over-exploring or under-exploring the state space.,7.2. Multi-passenger Taxi Domain,[0],[0]
"Note also that Boltzmann softmax performs remarkably well on this domain, outperforming sophisticated Bayesian
reinforcement-learning algorithms (Dearden et al., 1998).",7.2. Multi-passenger Taxi Domain,[0],[0]
"As shown in Figure 9, SARSA with the epsilon-greedy policy performs poorly.",7.2. Multi-passenger Taxi Domain,[0],[0]
"In fact, in our experiment, the algorithm rarely was able to deliver all the passengers.",7.2. Multi-passenger Taxi Domain,[0],[0]
"However, SARSA with Boltzmann softmax and SARSA with the maximum entropy mellowmax policy achieved significantly higher average reward.",7.2. Multi-passenger Taxi Domain,[0],[0]
"Maximum entropy mellowmax policy is no worse than Boltzmann softmax, here, suggesting that the greater stability does not come at the expense of less effective exploration.",7.2. Multi-passenger Taxi Domain,[0],[0]
"In this section, we evaluate the use of the maximum entropy mellowmax policy in the context of a policy-gradient algorithm.",7.3. Lunar Lander Domain,[0],[0]
"Specifically, we represent a policy by a neural network (discussed below) that maps from states to probabilities over actions.",7.3. Lunar Lander Domain,[0],[0]
A common choice for the activation function of the last layer is the Boltzmann softmax policy.,7.3. Lunar Lander Domain,[0],[0]
"In contrast, we can use maximum entropy mellowmax policy, presented in Section 6, by treating the inputs of the activation function as Q̂ values.
",7.3. Lunar Lander Domain,[0],[0]
"We used the lunar lander domain, from OpenAI Gym (Brockman et al., 2016) as our benchmark.",7.3. Lunar Lander Domain,[0],[0]
A screenshot of the domain is presented in Figure 10.,7.3. Lunar Lander Domain,[0],[0]
"This domain has a continuous state space with 8 dimensions, namely x-y coordinates, x-y velocities, angle and angular velocities, and leg-touchdown sensors.",7.3. Lunar Lander Domain,[0],[0]
There are 4 discrete actions to control 3 engines.,7.3. Lunar Lander Domain,[0],[0]
"The reward is +100 for a safe landing in the designated area, and −100 for a crash.",7.3. Lunar Lander Domain,[0],[0]
There is a small shaping reward for approaching the landing area.,7.3. Lunar Lander Domain,[0],[0]
Using the engines results in a negative reward.,7.3. Lunar Lander Domain,[0],[0]
An episode finishes when the spacecraft crashes or lands.,7.3. Lunar Lander Domain,[0],[0]
"Solving the domain is defined as maintaining mean episode return higher than 200 in 100 consecutive episodes.
",7.3. Lunar Lander Domain,[0],[0]
"The policy in our experiment is represented by a neural network with a hidden layer comprised of 16 units with RELU activation functions, followed by a second layer with 16 units and softmax activation functions.",7.3. Lunar Lander Domain,[0],[0]
We used REINFORCE to train the network.,7.3. Lunar Lander Domain,[0],[0]
"A batch episode size
of 10 was used, as we had stability issues with smaller episode batch sizes.",7.3. Lunar Lander Domain,[0],[0]
"We used the Adam algorithm (Kingma & Ba, 2014) with α = 0.005 and the other parameters as suggested by the paper.",7.3. Lunar Lander Domain,[0],[0]
"We used Keras (Chollet, 2015) and Theano (Team et al., 2016) to implement the neural network architecture.",7.3. Lunar Lander Domain,[0],[0]
"For each softmax policy, we present in Figure 11 the learning curves for different values of their free parameter.",7.3. Lunar Lander Domain,[0],[0]
We further plot average return over all 40000 episodes.,7.3. Lunar Lander Domain,[0],[0]
Mellowmax outperforms Boltzmann at its peak.,7.3. Lunar Lander Domain,[0],[0]
"Softmax operators play an important role in sequential decision-making algorithms.
",8. Related Work,[0],[0]
"In model-free reinforcement learning, they can help strike
a balance between exploration (mean) and exploitation (max).",8. Related Work,[0],[0]
"Decision rules based on epsilon-greedy and Boltzmann softmax, while very simple, often perform surprisingly well in practice, even outperforming more advanced exploration techniques (Kuleshov & Precup, 2014) that require significant approximation for complex domains.",8. Related Work,[0],[0]
"When learning “on policy”, exploration steps can (Rummery & Niranjan, 1994) and perhaps should (John, 1994) become part of the value-estimation process itself.",8. Related Work,[0],[0]
"On-policy algorithms like SARSA can be made to converge to optimal behavior in the limit when the exploration rate and the update operator is gradually moved toward max (Singh et al., 2000).",8. Related Work,[0],[0]
"Our use of softmax in learning updates reflects this point of view and shows that the value-sensitive behavior of Boltzmann exploration can be maintained even as updates are made stable.
",8. Related Work,[0],[0]
Analyses of the behavior of human subjects in choice experiments very frequently use softmax.,8. Related Work,[0],[0]
"Sometimes referred to in the literature as logit choice (Stahl & Wilson, 1994), it forms an important part of the most accurate predictor of human decisions in normal-form games (Wright & Leyton-Brown, 2010), quantal level-k reasoning (QLk).",8. Related Work,[0],[0]
Softmax-based fixed points play a crucial role in this work.,8. Related Work,[0],[0]
"As such, mellowmax could potentially make a good replacement.
",8. Related Work,[0],[0]
"Algorithms for inverse reinforcement learning (IRL), the problem of inferring reward functions from observed behavior (Ng & Russell, 2000), frequently use a Boltzmann operator to avoid assigning zero probability to non-optimal actions and hence assessing an observed sequence as impossible.",8. Related Work,[0],[0]
"Such methods include Bayesian IRL (Ramachandran & Amir, 2007), natural gradient IRL (Neu & Szepesvári, 2007), and maximum likelihood IRL (Babes et al., 2011).",8. Related Work,[0],[0]
"Given the recursive nature of value defined in these problems, mellowmax could be a more stable and efficient choice.
",8. Related Work,[0],[0]
"In linearly solvable MDPs (Todorov, 2006), an operator similar to mellowmax emerges when using an alternative characterization for cost of action selection in MDPs.",8. Related Work,[0],[0]
Inspired by this work Fox et al. (2016) introduced an off-policy G-learning algorithm that uses the operator to perform value-function updates.,8. Related Work,[0],[0]
"Instead of performing off-policy updates, we introduced a convergent variant of SARSA with Boltzmann policy and a state-dependent temperature parameter.",8. Related Work,[0],[0]
This is in contrast to Fox et al. (2016) where an epsilon greedy behavior policy is used.,8. Related Work,[0],[0]
We proposed the mellowmax operator as an alternative to the Boltzmann softmax operator.,9. Conclusion and Future Work,[0],[0]
We showed that mellowmax has several desirable properties and that it works favorably in practice.,9. Conclusion and Future Work,[0],[0]
"Arguably, mellowmax could be used in place of Boltzmann throughout reinforcement-learning research.
",9. Conclusion and Future Work,[0],[0]
"A future direction is to analyze the fixed point of planning, reinforcement-learning, and game-playing algorithms when using the mellowmax operators.",9. Conclusion and Future Work,[0],[0]
"In particular, an interesting analysis could be one that bounds the sub-optimality of the fixed points found by GVI.
",9. Conclusion and Future Work,[0],[0]
"An important future work is to expand the scope of our theoretical understanding to the more general function approximation setting, in which the state space or the action space is large and abstraction techniques are used.",9. Conclusion and Future Work,[0],[0]
Note that the importance of non-expansion in the function approximation case is well-established.,9. Conclusion and Future Work,[0],[0]
"(Gordon, 1995)
",9. Conclusion and Future Work,[0],[0]
"Finally, due to the convexity of mellowmax (Boyd & Vandenberghe, 2004), it is compelling to use it in a gradient-based algorithm in the context of sequential decision making.",9. Conclusion and Future Work,[0],[0]
IRL is a natural candidate given the popularity of softmax in this setting.,9. Conclusion and Future Work,[0],[0]
"The authors gratefully acknowledge the assistance of George D. Konidaris, as well as anonymous ICML reviewers for their outstanding feedback.",10. Acknowledgments,[0],[0]
A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average.,abstractText,[0],[0]
"In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision.",abstractText,[0],[0]
"The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior.",abstractText,[0],[0]
"In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning.",abstractText,[0],[0]
"We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter.",abstractText,[0],[0]
We show that the algorithm is convergent and that it performs favorably in practice.,abstractText,[0],[0]
An Alternative Softmax Operator for Reinforcement Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2422–2430 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2422
An AMR Aligner Tuned by Transition-based Parser
Yijia Liu, Wanxiang Che∗, Bo Zheng, Bing Qin, Ting Liu Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China {yjliu,car,bzheng,qinb,tliu}@ir.hit.edu.cn
Abstract
In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017).",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example).",1 Introduction,[0],[0]
"Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017).
",1 Introduction,[0],[0]
The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser.,1 Introduction,[0],[0]
"A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the
∗*",1 Introduction,[0],[0]
"Email corresponding.
alignment output is then used as reference to train the AMR parser.",1 Introduction,[0],[0]
"In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al., 2014; Wang and Xue, 2017).
",1 Introduction,[0],[0]
The rule-based aligner – JAMR aligner proposed by Flanigan et al. (2014) is widely used in previous works thanks to its flexibility of incorporating additional linguistic resources like WordNet.,1 Introduction,[0],[0]
"However, achieving good alignments with the JAMR aligner still faces some difficult challenges.",1 Introduction,[0],[0]
The first challenge is deriving an optimal alignment in ambiguous situations.,1 Introduction,[0],[0]
"Taking the sentence-AMR-graph pair in Figure 1 for example, the JAMR aligner doesn’t distinguish between the two “nuclear”s in the sentence and can yield sub-optimal alignment in which the first “nuclear” is aligned to the nucleus˜2 concept.",1 Introduction,[0],[0]
"The second challenge is recalling more semantically matched word-concept pair without harming the
alignment precision.",1 Introduction,[0],[0]
"The JAMR aligner adopts a rule that aligns the word-concept pair which at least have a common longest prefix of 4 characters, but omitting the shorter cases like aligning the word “actions” to the concept act-01 and the semantically matched cases like aligning the word “example” to the concept exemplify-01.",1 Introduction,[0],[0]
The final challenge which is faced by both the rule-based and unsupervised aligners is tuning the alignment with downstream parser learning.,1 Introduction,[0],[0]
Previous works treated the alignment as a fixed input.,1 Introduction,[0],[0]
Its quality is never evaluated and its alternatives are never explored.,1 Introduction,[0],[0]
"All these challenges make the JAMR aligner achieve only an alignment F1 score of about 90% and influence the performance of the trained AMR parsers.
",1 Introduction,[0],[0]
"In this paper, we propose a novel method to solve these challenges and improve the word-toconcept alignment, which further improves the AMR parsing performance.",1 Introduction,[0],[0]
A rule-based aligner and a transition-based oracle AMR parser lie in the core of our method.,1 Introduction,[0],[0]
"For the aligner part, we incorporate rich semantic resources into the JAMR aligner to recall more word-concept pairs and cancel its greedily aligning process.",1 Introduction,[0],[0]
This leads to multiple alignment outputs with higher recall but lower precision.,1 Introduction,[0],[0]
"For the parser part, we propose a new transition system that can parse the raw sentence into AMR graph directly.",1 Introduction,[0],[0]
"Meanwhile, a new oracle algorithm is proposed which produces the best achievable AMR graph from an alignment.",1 Introduction,[0],[0]
"Our aligner is tuned by our oracle parser by feeding the alignments to the oracle parser and picking the one which leads to the highest Smatch F1 score (Cai and Knight, 2013).",1 Introduction,[0],[0]
The chosen alignment is used in downstream training of the AMR parser.,1 Introduction,[0],[0]
"Based on the newly proposed aligner and transition system, we develop a transition-based parser that directly parses a sentence into its AMR graph and it can be easily improved through ensemble thanks to its simplicity.
",1 Introduction,[0],[0]
We conduct experiments on LDC2014T12 dataset.1 Both intrinsic and extrinsic evaluations are performed on our aligner.,1 Introduction,[0],[0]
"In the intrinsic evaluation, our aligner achieves an alignment F1 score of 95.2%.",1 Introduction,[0],[0]
"In the extrinsic evaluation, we replace the JAMR aligner with ours in two opensourced AMR parsers, which leads to consistent improvements on both parsers.",1 Introduction,[0],[0]
"We also evaluate our transition-based parser on the same dataset.
1catalog.ldc.upenn.edu/ldc2014t12
Using both our aligner and ensemble, a score of 68.1 Smatch F1 is achieved without any additional resources, which is comparable to the parser of Wang and Xue (2017).",1 Introduction,[0],[0]
"With additional part-ofspeech (POS) tags, our ensemble parser achieves 68.4 Smatch F1 score and outperforms that of Wang and Xue (2017).
",1 Introduction,[0],[0]
"The contributions of this paper come in two folds:
• We propose a new AMR aligner (§3) which recalls more semantically matched pairs and produces multiple alignments.",1 Introduction,[0],[0]
We also propose a new transition system for AMR parsing (§4.1) and use its oracle (§4.2) to pick the alignment that leads to the highest-scored achievable AMR graph (§4.3).,1 Introduction,[0],[0]
"Both intrinsic and extrinsic evaluations (§5) show the effectiveness of our aligner by achieving higher F1 score and consistently improving two opensourced AMR parsers.
",1 Introduction,[0],[0]
• We build a new transition-based parser (§4.4) upon our aligner and transition system which directly parses a raw sentence into its AMR graph.,1 Introduction,[0],[0]
"Through simple ensemble, our parser achieves 68.4 Smatch F1 score with only words and POS tags as input (§6) and outperforms the parser of Wang and Xue (2017).
",1 Introduction,[0],[0]
Our code and the alignments for LDC2014T12 dataset are publicly available at https:// github.com/Oneplus/tamr,1 Introduction,[0],[0]
AMR Parsers.,2 Related Work,[0],[0]
AMR parsing maps a natural language sentence into its AMR graph.,2 Related Work,[0],[0]
"Most current parsers construct the AMR graph in a two-staged manner which first identifies concepts (nodes in the graph) from the input sentence, then identifies relations (edges in the graph) between the identified concepts.",2 Related Work,[0],[0]
"Flanigan et al. (2014) and their follow-up works (Flanigan et al., 2016; Zhou et al., 2016) model the parsing problem as finding the maximum spanning connected graph.",2 Related Work,[0],[0]
"Wang et al. (2015b) proposes to greedily transduce the dependency tree into AMR graph and a bunch of works (Wang et al., 2015a; Goodman et al., 2016; Wang and Xue, 2017) further improve the transducer’s performance with rich features and imitation learning.2 Transition-based methods
2Wang et al. (2015b) and the follow-up works refer their transducing process as “transition-based”.",2 Related Work,[0],[0]
"However, to dis-
that directly parse an input sentence into its AMR graph have also been studied (Ballesteros and AlOnaizan, 2017; Damonte et al., 2017).",2 Related Work,[0],[0]
"In these works, the concept identification and relation identification are performed jointly.
",2 Related Work,[0],[0]
"An aligner which maps a span of words into its concept serves to the generation of training data for the concept identifier, thus is important to the parser training.",2 Related Work,[0],[0]
"Missing or incorrect alignments lead to poor concept identification, which then hurt the overall AMR parsing performance.",2 Related Work,[0],[0]
"Besides the typical two-staged methods, the aligner also works in some other AMR parsing algorithms like that using syntax-based machine translation (Pust et al., 2015), sequence-to-sequence (Peng et al., 2017; Konstas et al., 2017), Hyperedge Replacement Grammar (Peng et al., 2015) and Combinatory Category Grammar (Artzi et al., 2015).
",2 Related Work,[0],[0]
Previous aligner works solve the alignment problem in two different ways.,2 Related Work,[0],[0]
"The rule-based aligner (Flanigan et al., 2014) defines a set of heuristic rules which align a span of words to the graph fragment and greedily applies these rules.",2 Related Work,[0],[0]
"The unsupervised aligner (Pourdamghani et al., 2014; Wang and Xue, 2017) uncovers the word-toconcept alignment from the linearized AMR graph through EM.",2 Related Work,[0],[0]
"All these approaches yield a single alignment for one sentence and its effect on the downstream parsing is not considered.
",2 Related Work,[0],[0]
"JAMR Aligner (Flanigan et al., 2014).",2 Related Work,[0],[0]
"Two components exist in the JAMR aligner: 1) a set of heuristic rules and 2) a greedy search process.
",2 Related Work,[0],[0]
"The heuristic rules in the JAMR aligner are a set of indicator functions ρ(c, ws,e) which take a concept c and a span of words ws,e starting from s and ending with e as input and return whether they should be aligned.",2 Related Work,[0],[0]
These rules can be categorized into matching rules and updating rules.,2 Related Work,[0],[0]
"The matching rules directly compare c with ws,e and determine if they should be aligned.",2 Related Work,[0],[0]
"The updating rules first retrieve the concept c′ that ws,e aligns, then determine if c and ws,e should be aligned by checking whether c and c′ meet some conditions.",2 Related Work,[0],[0]
"Here, we illustrate how update rules work by applying a rule named Entity Type on the AMR graph in Figure 1 as an example.",2 Related Work,[0],[0]
"When determining if the entity type concept country should be aligned to “North Korea”, the Entity
tinguish their work with that of Damonte et al. (2017) and Ballesteros and Al-Onaizan (2017), we use the term “transduce” instead.
",2 Related Work,[0],[0]
"Type rule first retrieve that this span is aligned to the fragment (name :op1 ""North"" :op2 ""Korea""), then determine if they are aligned by checking if name is the tail concept of country.
",2 Related Work,[0],[0]
The greedy search process applies rules in a manually defined order.,2 Related Work,[0],[0]
"The results are mutually exclusive which means once a graph fragment is aligned by one rule, it cannot be realigned.",2 Related Work,[0],[0]
"By doing so, conflicts between the alignments produced by different rules are resolved.",2 Related Work,[0],[0]
"Flanigan et al. (2014) didn’t talk about the principle of orders but it generally follows the principle that 1) the matching rules have higher priorities than the updating rules, and 2) exact matching rules have higher priorities than the fuzzy matching rules.",2 Related Work,[0],[0]
Error propagates in the greedy search process.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
An alignment error can lead to future errors because of the dependencies and mutual exclusions between rules.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"In the JAMR aligner, rules that recall more alignments but introduce errors are carefully opted out and it influences the aligner’s performance.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Our motivation is to use rich semantic resources to recall more alignments.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Instead of resolving the resulted conflicts and errors by greedy search, we keep the multiple alignments produced by the aligner and let a parser decide the best alignment.
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"In this paper, we use two kinds of semantic resources to recall more alignments, which include the similarity drawn from Glove embedding (Pennington et al., 2014)3 and the morphosemantic database (Fellbaum et al., 2009) in the WordNet project4.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Two additional matching schemes semantic match and morphological match are proposed as:
Semantic Match.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Glove embedding encodes a word into its vector representation.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"We define semantic match of a concept as a word in the sentence that has a cosine similarity greater than 0.7 in the embedding space with the concept striping off trailing number (e.g. run-01→ run).
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
Morphological Match.,3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Morphosemantic is a database that contains links among derivational
3nlp.stanford.edu/projects/glove/ 4wordnet.princeton.edu/wordnet/
download/standoff/
links connecting noun and verb senses (e.g., “example” and exemplify).",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"We define morphological match of a concept as a word in the sentence having the (word, concept) link in the database.
",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"By defining the semantic match and morphological match, we extend the rules in Flanigan et al. (2014) with four additional matching rules as shown in Table 1.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"These rules are intended to recall the concepts or entities which either semantically resemble a span of words but differ in the surface form, or match a span of words in their morphological derivation.",3.1 Enhancing Aligner with Rich Semantic Resources,[0],[0]
"Using the rules in the JAMR aligner along with our four extended matching rules, we propose an algorithm to draw multiple alignments from a pair of sentence and AMR graph and it is shown in Algorithm 1.",3.2 Producing Multiple Alignments,[0],[0]
"In this algorithm, Ac denotes the set of candidate alignments for a graph fragment c, in which each alignment is represented as a tuple (s, e, c′) where s denotes the starting position, e denotes the ending position, and c′ denotes the concept that lead to this alignment.",3.2 Producing Multiple Alignments,[0],[0]
"At the beginning, Ac is initialized as an empty set (line 1 to 2).",3.2 Producing Multiple Alignments,[0],[0]
Then all the matching rules are tried to align a span of words to that fragment (line 3 to 7).,3.2 Producing Multiple Alignments,[0],[0]
"After applying all the matching rules, all the updating rules are repeatedly applied until no new alignment is generated in one iteration (line 8 to 16).",3.2 Producing Multiple Alignments,[0],[0]
"During applying the updating rules, we keep track of the dependencies between fragments.",3.2 Producing Multiple Alignments,[0],[0]
"Finally, all the possible combination of the alignments are enumerated without considering the one that violates the fragment dependencies (line 17 to 26).",3.2 Producing Multiple Alignments,[0],[0]
"Our enhanced rule-based aligner produces multiple alignments, and we would like to use our
Algorithm 1: Our alignment algorithm.",4 Transition-based AMR Parser,[0],[0]
"Input: An AMR graph with a set of graph fragments C;
a sentence W ; a set of matching rules PM ; and a set of updating rules PU .
",4 Transition-based AMR Parser,[0],[0]
"Output: a set of alignments A. 1 for c ∈ C do 2 Ac ← ∅; 3 for ρM ∈ PM do 4 for ws,e ← spans(W ) do 5 for c ∈ C do 6 if ρM (c, ws,e) then 7 Ac ← Ac ∪ (s, e, nil);
8 updated← true ; 9 while updated is true do
10 updated← false; 11 for ρU ∈ PU do 12 for c, c′ ∈ C × C do 13 for (s, e, d) ∈ A′c do 14 if ρU (c, ws,e)",4 Transition-based AMR Parser,[0],[0]
"∧ (s, e, c′) /∈",4 Transition-based AMR Parser,[0],[0]
"Ac then 15 Ac ← Ac ∪ (s, e, c′); 16 updated← true;
17 A ← ∅ ; 18 for (a1, ..., ac) ∈ CartesianProduct(A1, ..., A|C|) do 19 legal← true; 20 for a ∈ (a1, ..., ac) do 21 (s, e, c′)← a; 22 (s′, e′, d)← ac′ ; 23 if s 6= s′ ∧ e 6= e′",4 Transition-based AMR Parser,[0],[0]
"then 24 legal← false ;
25 if legal then 26 A ← A∪ (a1, ..., ac);
parser to evaluate their qualities.",4 Transition-based AMR Parser,[0],[0]
A parameterized parser does not accomplish such goal because training its parameters depends on the aligner’s outputs.,4 Transition-based AMR Parser,[0],[0]
A deterministic parser works in this situation but is required to consider the association between concepts and spans.,4 Transition-based AMR Parser,[0],[0]
"This stops the deterministic parsers which build AMR graph only from the derived concepts5 from being used because they do not distinguish alignments that yields to the same set of concepts.6
This discussion shows that to evaluate the quality of an alignment, we need a deterministic (oracle) parser which builds the AMR graph from the raw sentence.",4 Transition-based AMR Parser,[0],[0]
Ballesteros and Al-Onaizan (2017) presented a transition-based parser that directly parses a sentence into its AMR graph.,4 Transition-based AMR Parser,[0],[0]
"A transition system which extends the swap-based dependency parsing system to handle AMR non-projectivities (Damonte et al., 2017) was proposed in their work.
5e.g.",4 Transition-based AMR Parser,[0],[0]
"the reference relation identifier in Flanigan et al. (2014) and the oracle transducer in Wang et al. (2015b).
",4 Transition-based AMR Parser,[0],[0]
"6recall the “nuclear” example in Section 1.
",4 Transition-based AMR Parser,[0],[0]
"Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly.",4 Transition-based AMR Parser,[0],[0]
"What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system.",4 Transition-based AMR Parser,[0],[0]
"Their extension to transition-based AMR parsing is worth studying.
",4 Transition-based AMR Parser,[0],[0]
"In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser.",4 Transition-based AMR Parser,[0],[0]
The oracle parser is used for tuning our aligner and training our parser.,4 Transition-based AMR Parser,[0],[0]
We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6.3.,4 Transition-based AMR Parser,[0],[0]
"We follow Choi and McCallum (2013) and define a state in our transition system as a quadruple s = (σ, δ, β,A), where σ is a stack holding processed words, δ is a deque holding words popped out of σ that will be pushed back in the future, and β is a buffer holding unprocessed words.",4.1 List-based Extension for AMR Parsing,[0],[0]
A is a set of labeled relations.,4.1 List-based Extension for AMR Parsing,[0],[0]
A set of actions is defined to parse sentence into AMR graph.,4.1 List-based Extension for AMR Parsing,[0],[0]
Table 2 gives a formal illustration of these actions and how they work.,4.1 List-based Extension for AMR Parsing,[0],[0]
"The first five actions in Table 2 are our ex-
tended actions, and they are used to deriving concepts from the input sentence.",4.1 List-based Extension for AMR Parsing,[0],[0]
"Given an alignment and the gold standard AMR graph, we can build the best AMR graph by repeatedly applying one of these actions and this is what we called oracle parser.",4.2 Oracle Parser,[0],[0]
"Before running the oracle parser, we first remove the concepts which aren’t aligned with any span of words from the AMR graph.",4.2 Oracle Parser,[0],[0]
"During running the oracle parser, for a state s = (σ|s0, δ, b0|b1|β, A), our oracle parser decides which action to apply by checking the following conditions one by one.
1.",4.2 Oracle Parser,[0],[0]
"If b0 is a word and it doesn’t align to any concept, perform DROP.
2.",4.2 Oracle Parser,[0],[0]
"If b1 is within a span in the alignment, perform MERGE.
3.",4.2 Oracle Parser,[0],[0]
"If b0 is a word or span and it only aligns to one entity concept c, perform ENTITY(c).
4.",4.2 Oracle Parser,[0],[0]
"If b0 is a word or span and it aligns to one or more concepts, perform CONFIRM(c) where c is the concept b0 aligns and has the longest graph distance to the root.
5.",4.2 Oracle Parser,[0],[0]
"If b0 is a concept and its head concept c has the same alignment as b0, perform NEW(c).
6.",4.2 Oracle Parser,[0],[0]
"If b0 is a concept and there is an unprocessed edge r between s0 and t0, perform LEFT(r) or RIGHT(r) according to r’s direction.
7.",4.2 Oracle Parser,[0],[0]
"If s0 has unprocessed edge, perform CACHE.
8.",4.2 Oracle Parser,[0],[0]
"If s0 doesn’t have unprocessed edge, perform REDUCE.
9. perform SHIFT.
",4.2 Oracle Parser,[0],[0]
"We test our oracle parser on the hand-align data created by Flanigan et al. (2014) and it achieves 97.4 Smatch F1 score.7 Besides the errors resulted from incorrect manual alignments, entity errors made by the limitation of our ENTITY(c) action count a lot.",4.2 Oracle Parser,[0],[0]
Since our ENTITY action directly converts the surface form of a word span into an entity.,4.2 Oracle Parser,[0],[0]
"It cannot correctly generate entity names when they require derivation,8 or where tokenization errors exist.9",4.2 Oracle Parser,[0],[0]
"Using our oracle parser, we tune the aligner by picking the alignment which leads to the highestscored AMR graph from the set of candidates (see Figure 2 for the workflow).",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"When more than one alignment achieve the highest score, we choose the one with the smallest number of actions.",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"Intuitively, choosing the one with the smallest number of actions will encourage structurally coherent alignment10 because coherent alignment requires fewer CACHE actions.",4.3 Tune the Aligner with Oracle Parser,[0],[0]
"Based on our aligner and transition system, we propose a transition-based parser which parse the
7 Since some alignments in hand-align were created on incorrect AMR annotations, we filter out them and only use the correct subset which has 136 pairs of alignment and AMR graph.",4.4 Parsing Model,[0],[0]
"This data is also used in our intrinsic evaluation.
8e.g., “North Koreans” cannot be parsed into (name :op1 ""North"" :op2 ""Korea"")
9e.g., “Wi Sung - lac” cannot be parsed into (name :op1 ""Wi"" :op2 ""Sung-lac"")
10e.g.",4.4 Parsing Model,[0],[0]
"the first “nuclear” aligned to nucleus˜1 in Fig. 1
raw sentence directly into its AMR graph.",4.4 Parsing Model,[0],[0]
"In this paper, we follow Ballesteros and Al-Onaizan (2017) and use StackLSTM (Dyer et al., 2015) to model the states.",4.4 Parsing Model,[0],[0]
The score of a transition action a on state s is calculated as p(a|s) = exp{ga · STACKLSTM(s) +,4.4 Parsing Model,[0],[0]
"ba}∑ a′ exp{ga′ · STACKLSTM(s) + ba′} ,
where STACKLSTM(s) encodes the state s into a vector and ga is the embedding vector of action a.",4.4 Parsing Model,[0],[0]
"We encourage the reader to refer Ballesteros and Al-Onaizan (2017) for more details.
",4.4 Parsing Model,[0],[0]
Ensemble.,4.4 Parsing Model,[0],[0]
"Ensemble has been shown as an effective way of improving the neural model’s performance (He et al., 2017).",4.4 Parsing Model,[0],[0]
"Since the transitionbased parser directly parse a sentence into its AMR graph, ensemble of several parsers is easier compared to the two-staged AMR parsers.",4.4 Parsing Model,[0],[0]
"In this paper, we ensemble the parsers trained with different initialization by averaging their probability distribution over the actions.",4.4 Parsing Model,[0],[0]
We evaluate our aligner on the LDC2014T12 dataset.,5.1 Settings,[0],[0]
"Two kinds of evaluations are carried out including the intrinsic and extrinsic evaluations.
",5.1 Settings,[0],[0]
"For the intrinsic evaluation, we follow Flanigan et al. (2014) and evaluate the F1 score of the alignments produced by our aligner against the manually aligned data created in their work (handalign).",5.1 Settings,[0],[0]
"We also use our oracle parser’s performance as an intrinsic evaluation assuming that better alignment leads to higher scored oracle parser.
",5.1 Settings,[0],[0]
"For the extrinsic evaluation, we plug our alignment into two open-sourced AMR parsers: 1) JAMR (Flanigan et al., 2014, 2016) and 2) CAMR (Wang et al., 2015b,a) and evaluate the final performances of the AMR parsers on both the newswire proportion and the entire dataset of LDC2014T12.",5.1 Settings,[0],[0]
We use the configuration in Flanigan et al. (2016) for JAMR and the configuration in Wang et al. (2015a) without semantic role labeling (SRL) features for CAMR.,5.1 Settings,[0],[0]
Intrinsic Evaluation.,5.2 Results,[0],[0]
"Table 3 shows the intrinsic evaluation results, in which our alignment intrinsically outperforms JAMR aligner by achieving better alignment F1 score and leading to a higher scored oracle parser.
",5.2 Results,[0],[0]
Extrinsic Evaluation.,5.2 Results,[0],[0]
Table 4 shows the results.,5.2 Results,[0],[0]
"From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7.",5.2 Results,[0],[0]
Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner.,5.2 Results,[0],[0]
"To have a better understanding of our aligner, we conduct ablation test by removing the semantic matching and oracle parser tuning respectively and retrain the JAMR parser on the newswire proportion.",5.3 Ablation,[0],[0]
The results are shown in Table 5.,5.3 Ablation,[0],[0]
"From this table, we can see that removing either of these components harms the performance.",5.3 Ablation,[0],[0]
Removing oracle parser tuning leads to severe performance drop and the score is even lower than that with JAMR aligner.,5.3 Ablation,[0],[0]
We address this observation to that alignment noise is introduced by the semantic matching especially by the word embedding similarity component.,5.3 Ablation,[0],[0]
"Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance.",5.3 Ablation,[0],[0]
We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser.,6.1 Settings,[0],[0]
"For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input.",6.1 Settings,[0],[0]
"Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014).",6.1 Settings,[0],[0]
Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017).,6.1 Settings,[0],[0]
"To opt
out the effect of different initialization in training the neural network, we run 10 differently seeded runs and report their average performance following Reimers and Gurevych (2017).",6.1 Settings,[0],[0]
Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works.,6.2 Results,[0],[0]
"When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment.",6.2 Results,[0],[0]
The same trend is witnessed using words and POS tags as input.,6.2 Results,[0],[0]
"When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms the effectiveness of our aligner.
",6.2 Results,[0],[0]
"The second block in Table 6 shows the results of our ensemble parser, in which ensemble significantly improves the performance and more parsers ensembled, more improvements are achieved.",6.2 Results,[0],[0]
An ensemble of 10 parsers with only words as input achieves 68.1 Smatch F1 score which is comparable to the AMR parser of Wang and Xue (2017).,6.2 Results,[0],[0]
"Using the minimal amount of additional syntactic information – POS tags, the performance of the ensemble of 10 parsers is further pushed to 68.4, which surpasses that of Wang and Xue (2017) which relied on named entity recognition (NER) and dependency parsing (DEP).
",6.2 Results,[0],[0]
A further study on the speed shows that our 10 parser ensemble can parse 43 tokens per second which is faster than JAMR (7 tokens/sec.) and CAMR (24 tokens/sec.),6.2 Results,[0],[0]
"thanks to the simplicity of our model and independence of preprocessing, like NER and DEP.11",6.2 Results,[0],[0]
"Al-Onaizan (2017)
",6.3 Comparison to Ballesteros and,[0],[0]
"To explain the improved performance against Ballesteros and Al-Onaizan (2017) in Table 6, we
11In our speed comparison, we also count the time of preprocessing for JAMR and CAMR.",6.3 Comparison to Ballesteros and,[0],[0]
"All the comparison is performed in the same single-threaded settings.
give a comprehensive comparison between our transition system and that of Ballesteros and AlOnaizan (2017).
",6.3 Comparison to Ballesteros and,[0],[0]
Capability.,6.3 Comparison to Ballesteros and,[0],[0]
"In both these two systems, a span of words can only be derived into concept for one time.",6.3 Comparison to Ballesteros and,[0],[0]
“Patch” actions are required to generate new concepts from the one that is aligned to the same span.12 Ballesteros and Al-Onaizan (2017) uses a DEPENDENT action to generate one tail concept for one hop and cannot deal with the cases which have a chain of more than two concepts aligned to the same span.,6.3 Comparison to Ballesteros and,[0],[0]
Our list-based system differs theirs by using a NEW action to deal these cases.,6.3 Comparison to Ballesteros and,[0],[0]
"Since the new concept is pushed onto the buffer, NEW action can be repeatedly applied and used to generate arbitrary concepts that aligned to the same
12 e.g., three concepts in the fragment (person :source (country :name (name :op1 ""North"" :op2 ""Korea""))) are aligned to “North Koreans”.
span.",6.3 Comparison to Ballesteros and,[0],[0]
"On the development set of LDC2014T12, our oracle achieves 91.7 Smatch F1 score over the JAMR alignment, which outperforms Ballesteros and Al-Onaizan (2017)’s oracle (89.5 in their paper) on the same alignment.",6.3 Comparison to Ballesteros and,[0],[0]
"This result confirms that our list-based system is more powerful.
",6.3 Comparison to Ballesteros and,[0],[0]
Number of Actions.,6.3 Comparison to Ballesteros and,[0],[0]
Our list-based system also differs theirs in the number of oracle actions required to parse the same AMR graphs.,6.3 Comparison to Ballesteros and,[0],[0]
We use the oracles from two systems to parse the development set of LDC2014T12 on the same JAMR alignments.,6.3 Comparison to Ballesteros and,[0],[0]
Figure 3 shows the comparison in which our system clearly uses fewer actions (the average number of our system is 63.7 and that of Ballesteros and Al-Onaizan (2017) is 86.4).,6.3 Comparison to Ballesteros and,[0],[0]
Using fewer actions makes the parser learned from the oracle less prone to error propagation.,6.3 Comparison to Ballesteros and,[0],[0]
We attribute the improved performance in Table 6 to this advantage of transition system.,6.3 Comparison to Ballesteros and,[0],[0]
"In this paper, we propose a new AMR aligner which is tuned by a novel transition-based AMR oracle parser.",7 Conclusion,[0],[0]
Our aligner is also enhanced by rich semantic resource and recalls more alignments.,7 Conclusion,[0],[0]
Both the intrinsic and extrinsic evaluations show the effectiveness of our aligner by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers.,7 Conclusion,[0],[0]
We also develop transition-based AMR parser based on our aligner and transition system and it achieves a performance of 68.4 Smatch F1 score via ensemble with only words and POS tags as input.,7 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
"This work was
supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772153.",Acknowledgments,[0],[0]
"In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser.",abstractText,[0],[0]
Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph.,abstractText,[0],[0]
Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers.,abstractText,[0],[0]
"Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly.",abstractText,[0],[0]
"An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017).",abstractText,[0],[0]
An AMR Aligner Tuned by Transition-based Parser,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 64–71 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2011",text,[0],[0]
Action recognition is the task of identifying the action being depicted in a video or still image.,1 Introduction,[0],[0]
"The task is useful for a range of applications such as generating descriptions, image/video retrieval, surveillance, and human–computer interaction.",1 Introduction,[0],[0]
"It has been widely studied in computer vision, often on videos (Nagel, 1994; Forsyth et al., 2005), where motion and temporal information provide cues for recognizing actions (Taylor et al., 2010).",1 Introduction,[0],[0]
"However, many actions are recognizable from still images, see the examples in Figure 1.",1 Introduction,[0],[0]
"Due to the absence of motion cues and temporal features (Ikizler et al., 2008) action recognition from stills is more challenging.",1 Introduction,[0],[0]
Most of the existing work can be categorized into four tasks: (a) action classification (AC); (b) determining human–object interaction (HOI); (c) visual verb sense disambiguation (VSD); and (d) visual semantic role labeling (VSRL).,1 Introduction,[0],[0]
"In Figure 2 we illustrate each of these
tasks and show how they are related to each other.",1 Introduction,[0],[0]
"Until recently, action recognition was studied as action classification on small-scale datasets with a limited number of predefined actions labels (Ikizler et al., 2008; Gupta et al., 2009; Yao and FeiFei, 2010; Everingham et al., 2010; Yao et al., 2011).",1 Introduction,[0],[0]
"Often the labels in action classification tasks are verb phrases or a combination of verb and object such as playing baseball, riding horse.",1 Introduction,[0],[0]
"These datasets have helped in building models and understanding which aspects of an image are important for classifying actions, but most methods are not scalable to larger numbers of actions (Ramanathan et al., 2015).",1 Introduction,[0],[0]
"Action classification models are trained on images annotated with mutually exclusive labels, i.e., the assumption is that only a single label is relevant for a given image.",1 Introduction,[0],[0]
This ignores the fact that actions such as holding bicycle and riding bicycle can co-occur in the same image.,1 Introduction,[0],[0]
"To address these issues and also to understand the range of possible interactions between humans and objects, the human–object interaction (HOI) detection task has been proposed, in which all possible interactions between a human and a given object have to be identified (Le et al., 2014; Chao et al., 2015; Lu et al., 2016).
",1 Introduction,[0],[0]
"However, both action classification and HOI detection do not consider the ambiguity that arises when verbs are used as labels, e.g., the verb play has multiple meanings in different contexts.",1 Introduction,[0],[0]
"On the other hand, action labels consisting of verbobject pairs can miss important generalizations:
64
riding horse and riding elephant both instantiate the same verb semantics, i.e., riding animal.",1 Introduction,[0],[0]
"Thirdly, existing action labels miss generalizations across verbs, e.g., the fact that fixing bike and repairing bike are semantically equivalent, in spite of the use of different verbs.",1 Introduction,[0],[0]
These observations have led authors to argue that actions should be analyzed at the level of verb senses.,1 Introduction,[0],[0]
"Gella et al. (2016) propose the new task of visual verb sense disambiguation (VSD), in which a verb– image pair is annotated with a verb sense taken from an existing lexical database (OntoNotes in this case).",1 Introduction,[0],[0]
"While VSD handles distinction between different verb senses, it does not identify or localize the objects that participate in the action denoted by the verb.",1 Introduction,[0],[0]
"Recent work (Gupta and Malik, 2015; Yatskar et al., 2016) has filled this gap by proposing the task of visual semantic role labeling (VSRL), in which images are labeled with verb frames, and the objects that fill the semantic roles of the frame are identified in the image.
",1 Introduction,[0],[0]
"In this paper, we provide a unified view of action recognition tasks, pointing out their strengths and weaknesses.",1 Introduction,[0],[0]
We survey existing literature and provide insights into existing datasets and models for action recognition tasks.,1 Introduction,[0],[0]
We give an overview of commonly used datasets for action recognition tasks in Table 1 and group them according to subtask.,2 Datasets for Action Recognition,[0],[0]
"We observe that the number of verbs covered in these datasets is often smaller than the number of action labels reported (see Table 1, columns #V and #L) and in many cases the action label involves object reference.",2 Datasets for Action Recognition,[0],[0]
"A few of the first action recognition datasets such as the Ikizler and Willow datasets (Ikizler et al.,
2008; Delaitre et al., 2010) had action labels such as throwing and running; they were taken from the sports domain and exhibited diversity in camera view point, background and resolution.",2 Datasets for Action Recognition,[0],[0]
"Then datasets were created to capture variation in human poses in the sports domain for actions such as tennis serve and cricket bowling; typically features based on poses and body parts were used to build models (Gupta et al., 2009).",2 Datasets for Action Recognition,[0],[0]
"Further datasets were created based on the intuition that object information helps in modeling action recognition (Li and Fei-Fei, 2007; Ikizler-Cinbis and Sclaroff, 2010), which resulted in the use of action labels such as riding horse or riding bike (Everingham et al., 2010; Yao et al., 2011).",2 Datasets for Action Recognition,[0],[0]
"Not only were most of these datasets domain specific, but the labels were also manually selected and mutually exclusive, i.e., two actions cannot co-occur in the same image.",2 Datasets for Action Recognition,[0],[0]
"Also, most of these datasets do not localize objects or identify their semantic roles.",2 Datasets for Action Recognition,[0],[0]
"The limitations with early datasets (small scale, domain specificity, and the use of ad-hoc labels that combine verb and object) have been recently addressed in a number of broad-coverage datasets that offer linguistically motivated labels.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"Often these datasets use existing linguistic resources such as VerbNet (Schuler, 2005), OntoNotes (Hovy et al., 2006) and FrameNet (Baker et al., 1998) to classify verbs and their senses.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"This allows for a more general, semantically motivated treatment of verbs and verb phrases, and also takes into account that not all verbs are depictable.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"For example, abstract verbs such as presuming and acquiring are not depictable at all, while other verbs have both depictable and non-depictable senses: play is non-depictable in playing with emotions, but depictable in playing instrument and playing sport.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"The process of identifying depictable verbs or verb senses is used by Ronchi and Perona (2015), Gella et al. (2016) and Yatskar et al. (2016) to identify visual verbs, visual verb senses, and the semantic roles of the participating objects respectively.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
In all the cases the process of identifying visual verbs or senses is carried out by human annotators via crowd-sourcing platforms.,2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
"Visualness labels for 935 OntoNotes verb senses corresponding to 154 verbs is provided by Gella et al. (2016), while Yatskar et al. (2016) provides visualness labels for 9683 FrameNet verbs.",2.1 Identifying Visual Verbs and Verb Senses,[0],[0]
Over the last few years tasks that combine language and vision such as image description and visual question answering have gained much attention.,2.2 Datasets Beyond Action Classification,[0],[0]
"This has led to the creation of new, large datasets such as MSCOCO (Chen et al., 2015) and the VQA dataset (Antol et al., 2015).",2.2 Datasets Beyond Action Classification,[0],[0]
"Although these datasets are not created for action recognition, a number of attempts have been made to use the verbs present in image descriptions to annotate actions.",2.2 Datasets Beyond Action Classification,[0],[0]
"The COCO-a, VerSe and VCOCO-SRL datasets all use the MSCOCO image descriptions to annotate fine-grained aspects of interaction and semantic roles.
HICO:",2.2 Datasets Beyond Action Classification,[0],[0]
The HICO dataset has 47.8k images annotated with 600 categories of human-object interactions with 111 verbs applying to 80 object categories of MSCOCO.,2.2 Datasets Beyond Action Classification,[0],[0]
It is annotated to include diverse interactions for objects and has an average of 6.5 distinct interactions per object category.,2.2 Datasets Beyond Action Classification,[0],[0]
"Unlike other HOI datasets such as TUHOI which label interactions as verbs and ignore senses, the HOI categories of HICO are based on WordNet (Miller, 1995) verb senses.",2.2 Datasets Beyond Action Classification,[0],[0]
The HICO dataset also has multiple annotations per object and it incorporates the information that certain interactions such as riding a bike and holding a bike often co-occur.,2.2 Datasets Beyond Action Classification,[0],[0]
"However, it fails to include annotations to distinguish between multiple senses of a verb.
",2.2 Datasets Beyond Action Classification,[0],[0]
"Visual Genome: The dataset created by Krishna et al. (2016) has dense annotations of objects, at-
tributes, and relationships between objects.",2.2 Datasets Beyond Action Classification,[0],[0]
The Visual Genome dataset contains 105k images with 40k unique relationships between objects.,2.2 Datasets Beyond Action Classification,[0],[0]
"Unlike other HOI datasets such as HICO, visual genome relationships also include prepositions, comparative and prepositional phrases such as near and taller than, making the visual relationship task more generic than action recognition.",2.2 Datasets Beyond Action Classification,[0],[0]
"Krishna et al. (2016) combine all the annotations of objects, relationships, and attributes into directed graphs known as scene graphs.
",2.2 Datasets Beyond Action Classification,[0],[0]
"COCO-a: Ronchi and Perona (2015) present Visual VerbNet (VVN), a list of 140 common visual verbs manually mined from English VerbNet (Schuler, 2005).",2.2 Datasets Beyond Action Classification,[0],[0]
"The coverage of visual verbs in this dataset is not complete, as many visual verbs such as dive, perform and shoot are not included.",2.2 Datasets Beyond Action Classification,[0],[0]
This also highlights a bias in this dataset as the authors relied on occurrence in MSCOCO as a verification step to consider a verb as visual.,2.2 Datasets Beyond Action Classification,[0],[0]
"They annotated 10k images containing human subjects with one of the 140 visual verbs, for 80 MSCOCO objects.",2.2 Datasets Beyond Action Classification,[0],[0]
"This dataset has better coverage of human-object interactions than the HICO dataset despite of missing many visual verbs.
",2.2 Datasets Beyond Action Classification,[0],[0]
VerSe: Gella et al. (2016) created a dataset of 3.5k images sampled from the MSCOCO and TUHOI datasets and annotated it with 90 verbs and their OntoNotes senses to distinguish different verb senses using visual context.,2.2 Datasets Beyond Action Classification,[0],[0]
"This is the first dataset that aims to annotate all visual senses
of a verb.",2.2 Datasets Beyond Action Classification,[0],[0]
"However, the total number of images annotated and number of images for some senses is relatively small, which makes it difficult to use this dataset to train models.",2.2 Datasets Beyond Action Classification,[0],[0]
"The authors further divided their 90 verbs into motion and non-motion verbs according to Levin (1993) verb classes and analyzed visual ambiguity in the task of visual sense disambiguation.
",2.2 Datasets Beyond Action Classification,[0],[0]
VCOCO-SRL: Gupta and Malik (2015) annotated a dataset of 16k person instances in 10k images with 26 verbs and associated objects in the scene with the semantic roles for each action.,2.2 Datasets Beyond Action Classification,[0],[0]
The main aim of the dataset is to build models for visual semantic role labeling in images.,2.2 Datasets Beyond Action Classification,[0],[0]
"This task involves identifying the actions depicted in an image, along with the people and objects that instantiate the semantic roles of the actions.",2.2 Datasets Beyond Action Classification,[0],[0]
"In the VCOCO-SRL dataset, each person instance is annotated with a mean of 2.8 actions simultaneously.
",2.2 Datasets Beyond Action Classification,[0],[0]
"imSitu: Yatskar et al. (2016) annotated a large dataset of 125k images with 504 verbs, 1.7k semantic roles and 11k objects.",2.2 Datasets Beyond Action Classification,[0],[0]
"They used FrameNet verbs, frames and associated objects or scenes with roles to develop the dataset.",2.2 Datasets Beyond Action Classification,[0],[0]
They annotate every image with a single verb and the semantic roles of the objects present in the image.,2.2 Datasets Beyond Action Classification,[0],[0]
VCOCOSRL,2.2 Datasets Beyond Action Classification,[0],[0]
"the is dataset most similar to imSitu, however VCOCO-SRL includes localization information of agents and all objects and provides multiple action annotations per image.",2.2 Datasets Beyond Action Classification,[0],[0]
"On the other hand, imSitu is the dataset that covers highest number of verbs, while also omitting many commonly studied polysemous verbs such as play.",2.2 Datasets Beyond Action Classification,[0],[0]
"With the exception of a few datasets such as COCO-a, VerSe, imSitu all action recognition datasets have manually picked labels or focus on covering actions in specific domains such as sports.",2.3 Diversity in Datasets,[0],[0]
"Alternatively, many datasets only cover actions relevant to specific object categories such as musical instruments, animals and vehicles.",2.3 Diversity in Datasets,[0],[0]
"In the real world, people interact with many more objects and perform actions relevant to a wide range of domains such as personal care, household activities, or socializing.",2.3 Diversity in Datasets,[0],[0]
This limits the diversity and coverage of existing action recognition datasets.,2.3 Diversity in Datasets,[0],[0]
Recently proposed datasets partly handle this issue by using generic linguistic resources to extend the vocabulary of verbs in action labels.,2.3 Diversity in Datasets,[0],[0]
"The diversity issue has also been high-
lighted and addressed in recent video action recognition datasets (Caba Heilbron et al., 2015; Sigurdsson et al., 2016), which include generic household activities.",2.3 Diversity in Datasets,[0],[0]
An analysis of various image description and question answering datasets by Ferraro et al. (2015) shows the bias in the distribution of word categories.,2.3 Diversity in Datasets,[0],[0]
"Image description datasets have a higher distribution of nouns compared to other word categories, indicating that the descriptions are object specific, limiting their usefulness for action-based tasks.",2.3 Diversity in Datasets,[0],[0]
"Template based description generation systems for both videos and images rely on identifying subject–verb–object triples and use language modeling to generate or rank descriptions (Yang et al., 2011; Thomason et al., 2014; Bernardi et al., 2016).",3 Relevant Language and Vision Tasks,[0],[0]
"Understanding actions also plays an important role in question answering, especially when the question is pertaining to an action depicted in the image.",3 Relevant Language and Vision Tasks,[0],[0]
"There are some specifically curated question answering datasets which target human activities or relationships between a pair of objects (Yu et al., 2015).",3 Relevant Language and Vision Tasks,[0],[0]
Mallya and Lazebnik (2016) have shown that systems trained on action recognition datasets could be used to improve the accuracy of visual question answering systems that handle questions related to human activity and human–object relationships.,3 Relevant Language and Vision Tasks,[0],[0]
"Action recognition datasets could be used to learn actions that are visually similar such as interacting with panda and feeding a panda or tickling a baby and calming a baby, which cannot be learned from text alone (Ramanathan et al., 2015).",3 Relevant Language and Vision Tasks,[0],[0]
"Visual semantic role labeling is a crucial step for grounding actions in the physical world (Yang et al., 2016).",3 Relevant Language and Vision Tasks,[0],[0]
"Most of the models proposed for action classification and human–object interaction tasks rely on identifying higher-level visual cues present in the image, including human bodies or body parts (Ikizler et al., 2008; Gupta et al., 2009; Yao et al., 2011; Andriluka et al., 2014), objects (Gupta et al., 2009), and scenes (Li and Fei-Fei, 2007).",4 Action Recognition Models,[0],[0]
"Higherlevel visual cues are obtained through low-level features extracted from the image such as Scale Invariant Feature Transforms (SIFT), Histogram of Oriented Gradients (HOG), and Spatial Envelopes (Gist) features (Lowe, 1999; Dalal and Triggs,
2005).",4 Action Recognition Models,[0],[0]
"These are useful in identifying key points, detecting humans, and scene or background information in images, respectively.",4 Action Recognition Models,[0],[0]
"In addition to identifying humans and objects, the relative position or angle between a human and an object is useful in learning human–object interactions (Le et al., 2014).",4 Action Recognition Models,[0],[0]
"Most of the existing approaches rely on learning supervised classifiers over low-level features to predict action labels.
",4 Action Recognition Models,[0],[0]
"More recent approaches are based on end-toend convolutional neural network architectures which learn visual cues such as objects and image features for action recognition (Chao et al., 2015; Zhou et al., 2016; Mallya and Lazebnik, 2016).",4 Action Recognition Models,[0],[0]
"While most of the action classification models rely solely on visual information, models proposed for human–object interaction or visual relationship detection sometimes combine human and object identification (using visual features) with linguistic knowledge (Le et al., 2014; Krishna et al., 2016; Lu et al., 2016).",4 Action Recognition Models,[0],[0]
"Other work on identifying actions, especially methods that focus on relationships that are infrequent or unseen, utilize word vectors learned on large text corpora as an additional source of information (Lu et al., 2016).",4 Action Recognition Models,[0],[0]
"Similarly, Gella et al. (2016) show that embeddings generated from textual data associated with images (object labels, image descriptions) is useful for visual verb sense disambiguation, and is complementary to visual information.",4 Action Recognition Models,[0],[0]
"Linguistic resources such as WordNet, OntoNotes, and FrameNet play a key role in textual sense disambiguation and semantic role labeling.",5 Discussion,[0],[0]
"The visual action disambiguation and visual semantic role labeling tasks are extensions of their textual counterparts, where context is provided as an image instead of as text.",5 Discussion,[0],[0]
Linguistic resources therefore have to play a key role if we are to make rapid progress in these language and vision tasks.,5 Discussion,[0],[0]
"However, as we have shown in this paper, only a few of the existing datasets for action recognition and related tasks are based on linguistic resources (Chao et al., 2015; Gella et al., 2016; Yatskar et al., 2016).",5 Discussion,[0],[0]
"This is despite the fact that the WordNet noun hierarchy (for example) has played an important role in recent progress in object recognition, by virtue of underlying the ImageNet database, the de-facto standard for this task (Russakovsky et al., 2015).",5 Discussion,[0],[0]
"The success of ImageNet for objects has
in turn helped NLP tasks such as bilingual lexicon induction (Vulić et al., 2016).",5 Discussion,[0],[0]
"In our view, language and vision datasets that are based on the WordNet, OntoNotes, or FrameNet verb sense inventories can play a similar role for tasks such as action recognition or visual semantic role labeling, and ultimately be useful also for more distantly related tasks such as language grounding.
",5 Discussion,[0],[0]
Another argument for linking language and vision datasets with linguistic resources is that this enables us to deploy the datasets in a multilingual setting.,5 Discussion,[0],[0]
"For example a polysemous verb such as ride in English has multiple translations in German and Spanish, depending on the context and the objects involved.",5 Discussion,[0],[0]
"Riding a horse is translated as reiten in German and cabalgar in Spanish, whereas riding a bicycle is translated as fahren in German and pedalear in Spanish.",5 Discussion,[0],[0]
"In contrast, some polysemous verb (e.g., English play) are always translated as the same verb, independent of sense (spielen in German).",5 Discussion,[0],[0]
"Such sense mappings are discoverable from multilingual lexical resources (e.g., BabelNet, Navigli and Ponzetto 2010), which makes it possible to construct language and vision models that are applicable to multiple languages.",5 Discussion,[0],[0]
"This opportunity is lost if language and vision dataset are constructed in isolation, instead of using existing linguistic resources.",5 Discussion,[0],[0]
"In this paper, we have shown the evolution of action recognition datasets and tasks from simple ad-hoc labels to the fine-grained annotation of verb semantics.",6 Conclusions,[0],[0]
"It is encouraging to see the recent increase in datasets that deal with sense ambiguity and annotate semantic roles, while using standard linguistic resources.",6 Conclusions,[0],[0]
"One major remaining issue with existing datasets is their limited coverage, and the skewed distribution of verbs or verb senses.",6 Conclusions,[0],[0]
Another challenge is the inconsistency in annotation schemes and task definitions across datasets.,6 Conclusions,[0],[0]
"For example Chao et al. (2015) used WordNet senses as interaction labels, while Gella et al. (2016) used the more coarsegrained OntoNotes senses.",6 Conclusions,[0],[0]
"Yatskar et al. (2016) used FrameNet frames for semantic role annotation, while Gupta and Malik (2015) used manually curated roles.",6 Conclusions,[0],[0]
"If we are to develop robust, domain independent models, then we need to standardize annotation schemes and use the same linguistic resources across datasets.",6 Conclusions,[0],[0]
"A large amount of recent research has focused on tasks that combine language and vision, resulting in a proliferation of datasets and methods.",abstractText,[0],[0]
"One such task is action recognition, whose applications include image annotation, scene understanding and image retrieval.",abstractText,[0],[0]
"In this survey, we categorize the existing approaches based on how they conceptualize this problem and provide a detailed review of existing datasets, highlighting their diversity as well as advantages and disadvantages.",abstractText,[0],[0]
We focus on recently developed datasets which link visual information with linguistic resources and provide a fine-grained syntactic and semantic analysis of actions in images.,abstractText,[0],[0]
An Analysis of Action Recognition Datasets for Language and Vision Tasks,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 95–105, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
structions has been observed to have a substantial effect on language processing. This begs the question of what causes certain constructions to be more or less frequent. A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs. This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times. Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.",text,[0],[0]
"Frequency effects in language have been isolated and observed in many studies (Trueswell, 1996; Jurafsky, 1996; Hale, 2001; Demberg and Keller, 2008).",1 Introduction,[0],[0]
"These effects are important because they illuminate the ontogeny of language (how individual speakers have acquired language), but they do not answer questions about the phylogeny of language (how the language came to its current form).
",1 Introduction,[0],[0]
Phillips (2010) has hypothesized that grammar rule probabilities may be grounded in memory limitations.,1 Introduction,[0],[0]
"Increased delays in processing centerembedded sentences as the number of embeddings increases, for example, are often explained in terms of a complexity cost associated with maintaining incomplete dependencies in working memory (Gibson, 2000; Lewis and Vasishth, 2005).",1 Introduction,[0],[0]
"Other studies have shown a link between processing delays
and the low frequency of center-embedded constructions like object relatives (Hale, 2001), but they have not explored the source of this low frequency.",1 Introduction,[0],[0]
A grounding hypothesis would claim that the low probability of generating such a structure may arise from an associated memory load.,1 Introduction,[0],[0]
"In this account, while these complexity costs may involve languagespecific concepts such as referent or argument linking, the underlying explanation would be one of memory limitations (Gibson, 2000) or neural activation (Lewis and Vasishth, 2005).
",1 Introduction,[0],[0]
"This paper seeks to explore the different predictions made by these theories on a broad-coverage corpus of eye-tracking data (Kennedy et al., 2003).",1 Introduction,[0],[0]
"In addition, the current experiment seeks to isolate memory effects from frequency effects in the same task.",1 Introduction,[0],[0]
"The results show that memory load measures are a significant factor even when frequency measures are residualized out.
",1 Introduction,[0],[0]
The remainder of this paper is organized as follows: Sections 2 and 3 describe several frequency and memory measures.,1 Introduction,[0],[0]
Section 4 describes a probabilistic hierarchic sequence model that allows all of these measures to be directly computed.,1 Introduction,[0],[0]
Section 5 describes how these measures were used to predict reading time durations on the Dundee eye-tracking corpus.,1 Introduction,[0],[0]
Sections 6 and 7 present results and discuss.,1 Introduction,[0],[0]
"One of the strongest predictors of processing complexity is surprisal (Hale, 2001).",2.1 Surprisal,[0],[0]
"It has been shown in numerous studies to have a strong correlation with reading time durations in eye-tracking and selfpaced reading studies when calculated with a variety
95
of models (Levy, 2008; Roark et al., 2009; Wu et al., 2010).
",2.1 Surprisal,[0],[0]
"Surprisal predicts the integration difficulty that a word xt at time step t presents given the preceding context and is calculated as follows:
surprisal(xt) =",2.1 Surprisal,[0],[0]
"− log2
( ∑
s∈S(x1...xt) P (s)
∑
s∈S(x1...xt−1) P (s)
)
(1)
where S(x1 . . .",2.1 Surprisal,[0],[0]
xt) is the set of syntactic trees whose leaves have x1 . . .,2.1 Surprisal,[0],[0]
xt,2.1 Surprisal,[0],[0]
as a prefix.,2.1 Surprisal,[0],[0]
"1
In essence, surprisal measures how unexpected constructions are in a given context.",2.1 Surprisal,[0],[0]
What it does not provide is an explanation for why certain constructions would be less common and thus more surprising.,2.1 Surprisal,[0],[0]
"Processing difficulty can also be measured in terms of entropy (Shannon, 1948).",2.2 Entropy Reduction,[0],[0]
A larger entropy over a random variable corresponds to greater uncertainty over the observed value it will take.,2.2 Entropy Reduction,[0],[0]
The entropy of a syntactic derivation over the sequence x1 . . .,2.2 Entropy Reduction,[0],[0]
"xt is calculated as:2
H(x1...t) =",2.2 Entropy Reduction,[0],[0]
"∑
s∈S(x1...xt)
−P (s) · log2 P (s) (2)
Reduction in entropy has been found to predict processing complexity (Hale, 2003; Hale, 2006; Roark et al., 2009; Wu et al., 2010; Hale, 2011):
∆H(x1...t) = max(0, H(x1...t−1)−H(x1...t))",2.2 Entropy Reduction,[0],[0]
"(3)
This measures the change in uncertainty about the discourse as each new word is processed.",2.2 Entropy Reduction,[0],[0]
"In Dependency Locality Theory (DLT) (Gibson, 2000), complexity arises from intervening referents introduced between a predicate and its argument.",3.1 Dependency Locality,[0],[0]
"Under the original formulation of DLT, there is a
1The parser in this study uses a beam.",3.1 Dependency Locality,[0],[0]
"However, given high parser accuracy, Roark (2001) showed that calculating complexity metrics over a beam should obtain similar results to the full complexity calculation.",3.1 Dependency Locality,[0],[0]
"2The incremental formulation used here was first proposed in Wu et al. (2010).
",3.1 Dependency Locality,[0],[0]
storage cost for each new referent introduced and an integration cost for each referent intervening in a dependency projection.,3.1 Dependency Locality,[0],[0]
"This is a simplification made for ease of computation, and subsequent work has found DLT to be more accurate cross-linguistically if the intervening elements are structurally defined rather than defined in terms of referents (Kwon et al., 2010).",3.1 Dependency Locality,[0],[0]
"That is, simply having a particular referent intervene in a dependency projection may not have as great an effect on processing complexity as the syntactic construction the referent appears in.",3.1 Dependency Locality,[0],[0]
"Therefore, this work reinterprets the costs of dependency locality to be related to the events of beginning a center embedding (storage) and completing a center embedding (integration).",3.1 Dependency Locality,[0],[0]
"Note that antilocality effects (where longer dependencies are easier to process) have also been observed in some languages, and DLT is unable to account for these phenomena (Vasishth and Lewis, 2006).",3.1 Dependency Locality,[0],[0]
"Processing complexity has also been attributed to confusability (Lewis and Vasishth, 2005) as defined in domain-general cognitive models like ACT-R (Anderson et al., 2004).
",3.2 ACT-R,[0],[0]
ACT-R is based on theories of neural activation.,3.2 ACT-R,[0],[0]
Each new word is encoded and stored in working memory until it is retrieved at a later point for modification before being re-encoded into the parse.,3.2 ACT-R,[0],[0]
"A newly observed sign (word) associatively activates any appropriate arguments from working memory, so multiple similarly appropriate arguments would slow processing as the parser must choose between the highly activated hypotheses.",3.2 ACT-R,[0],[0]
Any intervening signs (words or phrases) that modify a previously encoded sign re-activate it and raise its resting activation potential.,3.2 ACT-R,[0],[0]
"This can ease later retrieval of that sign in what is termed an anti-locality effect, contra predictions of DLT.",3.2 ACT-R,[0],[0]
"In this way, returning out of an embedded clause can actually speed processing by having primed the retrieved sign before it was needed.",3.2 ACT-R,[0],[0]
ACT-R attributes locality phenomena to frequency effects (e.g. unusual constructions) overriding such priming and to activation decay if embedded signs do not prime the target sign through modification (as in parentheticals).,3.2 ACT-R,[0],[0]
"Finally, ACT-R predicts something like DLT’s storage cost due to the need to differentiate each newly encoded sign from

those previously encoded (similarity-based encoding interference) (Lewis et al., 2006).",3.2 ACT-R,[0],[0]
"Current models of working memory in structured tasks are defined in terms of hierarchies of sequential processes, in which superordinate sequences can be interrupted by subordinate sequences and resume when the subordinate sequences have concluded (Botvinick, 2007).",3.3 Hierarchic Sequential Prediction,[0],[0]
"These models rely on temporal cueing as well as content-based cueing to explain how an interrupted sequence may be recalled for continuation.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Temporal cueing is based on a context of temporal features for the current state (Howard and Kahana, 2002).",3.3 Hierarchic Sequential Prediction,[0],[0]
The temporal context in which the subordinate sequence concludes must be similar enough to the temporal context in which it was initiated to recall where in the superordinate sequence the subordinate sequence occurred.,3.3 Hierarchic Sequential Prediction,[0],[0]
"For example, the act of making breakfast may be interrupted by a phone call.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Once the call is complete, the temporal context is sufficiently similar to when the call began that one is able to continue preparing breakfast.",3.3 Hierarchic Sequential Prediction,[0],[0]
"The association between the current temporal context and the temporal context prior to the interruption is strong enough to cue the next action.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Temporal cueing is complemented by sequential (content-based) cueing (Botvinick, 2007) in which the content of an individual element is associated with, and thus cues, the following element.",3.3 Hierarchic Sequential Prediction,[0],[0]
"For example, recalling the 20th note of a song is difficult, but when playing the song, each note cues the fol-
lowing note, leading one to play the 20th note without difficulty.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"Hierarchic sequential prediction may be directly applicable to processing syntactic center embeddings (van Schijndel et al., in press).",3.3 Hierarchic Sequential Prediction,[0],[0]
An ongoing parse may be viewed graph-theoretically as one or more connected components of incomplete phrase structure trees (see Figure 1).,3.3 Hierarchic Sequential Prediction,[0],[0]
"Beginning a new subordinate sequence (a center embedding) introduces a new connected component, disjoint from that of the superordinate sequence.",3.3 Hierarchic Sequential Prediction,[0],[0]
"As the subordinate sequence proceeds, the new component gains associated discourse referents, each sequentially cued from the last, until finally it merges with the superordinate connected component at the end of the embedded clause, forming a single connected component representing the parse up to that point.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Since it is not connected to the subordinate connected component prior to merging, the superordinate connected component must be recalled through temporal cueing.
",3.3 Hierarchic Sequential Prediction,[0],[0]
"McElree (2001; 2006) has found that retrieval of any non-focused (or in this case, unconnected) element from memory leads to slower processing.",3.3 Hierarchic Sequential Prediction,[0],[0]
"Therefore, integrating two disjoint connected components should be expected to incur a processing cost due to the need to recall the current state of the superordinate sequence to continue the parse.",3.3 Hierarchic Sequential Prediction,[0],[0]
Such a cost would corroborate a DLT-like theory where integration slows processing.,3.3 Hierarchic Sequential Prediction,[0],[0]
Language processing is typically centered in the left hemisphere of the brain (for right-handed individuals).,3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
Just and Varma (2007) provide fMRI results suggesting readers dynamically recruit additional processing resources such as the right-side homologues of the language processing areas of the brain when processing center-embedded constructions.,3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Once an embedded construction terminates, the reader may still have temporary access to these extra processing resources, which may briefly speed processing.
",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"This hypothesis would, therefore, predict an encoding cost when a center embedding is initiated.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"The resulting inhibition would trigger recruitment of additional processing resources, which would then
allow the rest of the embedded structure to be processed at the usual speed.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Upon completing an embedding, the difficulty arising from memory retrieval (McElree, 2001) would be ameliorated by these extra processing resources, and the reduced processing complexity arising from reduced memory load would yield a temporary facilitation in processing.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"No longer requiring the additional resources to cope with the increased embedding, the processor would release them, returning the processor to its usual speed.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Unlike anti-locality, where processing is facilitated in longer passages due to accumulating probabilistic evidence, a model of dynamic recruitment of additional processing resources would predict universal facilitation after a center embedding of any length, modulo frequency effects.",3.4 Dynamic Recruitment of Additional Processing Resources,[0],[0]
"Wu et al. (2010) propose an explicit measure of the difficulty associated with processing centerembedded constructions, which is similar to the predictions of dynamic recruitment and is defined in terms of changes in memory load.",3.5 Embedding Difference,[0],[0]
"They calculate a probabilistically-weighted average embedding depth as follows:
µemb(x1 . . .",3.5 Embedding Difference,[0],[0]
"xt) = ∑
s∈S(x1...xt)
d(s) ·",3.5 Embedding Difference,[0],[0]
"P (s) (4)
where d(s) returns the embedding depth of the derivation s at xt in a variant of a left-corner parsing process.3 Embedding difference may then be derived as:
EmbDiff (x1 . . .",3.5 Embedding Difference,[0],[0]
xt) =µemb(x1 . . .,3.5 Embedding Difference,[0],[0]
"xt)− (5)
µemb(x1 . . .",3.5 Embedding Difference,[0],[0]
"xt−1)
",3.5 Embedding Difference,[0],[0]
This is hypothesized to correlate positively with processing load: increasing the embedding depth increases processing load and decreasing it reduces processing load.,3.5 Embedding Difference,[0],[0]
Note that embedding difference makes the opposite prediction from DLT in that integrating an embedded clause is predicted to speed processing.,3.5 Embedding Difference,[0],[0]
"In fact, the predictions of embedding
3As pointed out by Wu et al. (2010), in practice this can be computed over a beam of potential parses in which case it must be normalized by the total probability of the beam.
difference are such that it may be viewed as an implementation of the predictions of a hierarchic sequential processing model with dynamic recruitment of additional resources.",3.5 Embedding Difference,[0],[0]
"This paper uses a hierarchic sequence model implementation of a left-corner parser variant (van Schijndel et al., in press), which represents connected components of phrase structure trees in hierarchies of hidden random variables.",4 Model,[0],[0]
"This requires, at each time step t:
• a hierarchically-organized set of N connected component states qnt , each consisting of an active sign of category aqn
t , and an awaited sign
of category bqn t , separated by a slash ‘/’; and
• an observed word xt.
",4 Model,[0],[0]
"Each connected component state in this model then represents a contiguous portion of a phrase structure tree (see Figure 1 on preceding page).
",4 Model,[0],[0]
"The operations of this parser can be defined as a deductive system (Shieber et al., 1995) with an input sequence consisting of a top-level connected component state ⊤/⊤, corresponding to an existing discourse context, followed by a sequence of observed words x1, x2, . . .",4 Model,[0],[0]
4,4 Model,[0],[0]
"If an observation xt can attach as the awaited sign of the most recent (most subordinate) connected component a/b, it is hypothesized to do so, turning this incomplete sign into a complete sign a (F–, below); or if the observation can serve as a lower descendant of this awaited sign, it is hypothesized to form the first complete sign a′ in a newly initiated connected component (F+):
a/b xt a b → xt (F–)
a/b xt a/b a′ b + → a′ ... ; a′ → xt (F+)
Then, if either of these complete signs (a or a′ above, matched to a′′ below) can attach as an initial
4A deductive system consists of inferences or productions
of the form: P
Q R, meaning premise P entails conclusion Q ac-
cording to rule R.
⊤/⊤",4 Model,[0],[0]
"the ⊤/⊤, D F+
⊤/⊤, NP/N L– studio
⊤/⊤, NP F–
⊤/⊤, S/VP L–
bought
⊤/⊤, S/VP, V F+
⊤/⊤, S/NP L+
the
⊤/⊤, S/NP, D F+
⊤/⊤, S/NP, NP/N L–
publisher
⊤/⊤, S/NP, NP F–
⊤/⊤, S/NP, D/G L–
’s
⊤/⊤, S/NP, D F–
child of the awaited sign of the immediately superordinate connected component state a/b, it is hypothesized to do so and terminate the subordinate connected component state, with xt as the last observation of the terminated connected component (L+); or if the observation can serve as a lower descendant of this awaited sign, it is hypothesized to remain disjoint and form its own connected component (L–):
a/b a′′
a/b′′ b → a′′ b′′ (L+)
a/b a′′
a/b a′/b′′ b
+ → a′ ... ; a′ → a′′ b′′ (L–)
",4 Model,[0],[0]
These operations can be made probabilistic.,4 Model,[0],[0]
"The probability σ of a transition at time step t is defined in terms of (i) a probability φ of initiating a new connected component state with xt as its first observation, multiplied by (ii) the probability λ of terminating a connected component state with xt as its last observation, multiplied by (iii) the probabilities α and β of generating categories for active and awaited signs aqn
t and bqn t in the resulting most subordinate
connected component state qnt .",4 Model,[0],[0]
"This kind of model can be defined directly on PCFG probabilities and trained to produce state-of-the-art accuracy by using the latent variable annotation of Petrov et al. (2006) (van Schijndel et al., in press).5
",4 Model,[0],[0]
An example parse is shown in Figure 2.,4 Model,[0],[0]
"Since two binary structural decisions (F and L) must be made in order to generate each word, there are four possible structures that may be generated (see Table 1).",4 Model,[0],[0]
The F+L– transition initiates a new level of embedding at word xt and so requires the superordinate state to be encoded for later retrieval (e.g. on observing the in Figure 2).,4 Model,[0],[0]
"The F–L+ transition completes the deepest level of embedding and therefore requires the recall of the current superordinate connected component state with which the
5The model has been shown to achieve an F-score of 87.8, within .2 points of the Petrov and Klein (2007) parser, which obtains an F-score of 88.0 on the same task.",4 Model,[0],[0]
"Because the sequence model is defined over binary-branching phrase structure, both parsers were evaluated on binary-branching phrase structure trees to provide a fair comparison.
",4 Model,[0],[0]
subordinate connected component state will be integrated.,4 Model,[0],[0]
"For example, in Figure 2, upon observing ’s, the parser must use temporal cueing to recall that it is in the middle of processing an NP (to complete an S), which sequentially cues a prediction of N. F–L– transitions complete the awaited sign of the most subordinate state and so sequentially cue a following connected component state at the same tier of the hierarchy.",4 Model,[0],[0]
"For example, in Figure 2, after observing studio, the parser uses the completed NP to sequentially cue the prediction that it has finished the left child of an S. F+L+ transitions locally expand the awaited sign of the most subordinate state and so should also not require any recall or encoding.",4 Model,[0],[0]
"For example, in Figure 2, observing bought while awaiting a VP sequentially cues a prediction of NP.
F+L–, then, loosely corresponds to a storage action under DLT as more hierarchic levels must now be maintained at each future step of the parse.",4 Model,[0],[0]
"As stated before, it differs from DLT in that it is sensitive to the depth of embedding rather than a particular subset of syntactic categories.",4 Model,[0],[0]
Wu et al. (2010) found that increasing the embedding depth led to longer reading times in a self-paced reading experiment.,4 Model,[0],[0]
"In ACT-R terms, F+L– corresponds to an encoding action, potentially causing processing difficulty resulting from the similarity of the current sign to previously encoded signs.
",4 Model,[0],[0]
"F–L+, by contrast, is similar to DLT’s integration action since a subordinate connected component is integrated into the rest of the parse structure.",4 Model,[0],[0]
"This represents a temporal cueing event in which the awaited category of the superordinate connected
component is recalled.",4 Model,[0],[0]
"In contrast to DLT, embedding difference and dynamic recruitment would predict a shorter reading time in the F–L+ case because of the reduction in memory load.",4 Model,[0],[0]
"In an ACT-R framework, reading time durations can increase at the retrieval site because the retrieval causes competition among similarly encoded signs in the context set.",4 Model,[0],[0]
"While it is possible for reading times to decrease when completing a center embedding in ACT-R (Vasishth and Lewis, 2006), this would be expressed as a frequency effect due to certain argument types commonly foreshadowing their predicates (Jaeger et al., 2008).",4 Model,[0],[0]
"Since frequency effects are factored separately from memory effects in this study, ACT-R would predict longer residual (memory-based) reading times when completing an embedding.
",4 Model,[0],[0]
"Predicted correlations to reading times for the F
and L transitions are summarized in Table 2.",4 Model,[0],[0]
"Eye-tracking and reading time data are often used to test complexity measures (Gibson, 2000; Demberg and Keller, 2008; Roark et al., 2009) under the assumption that readers slow down when reading more complex passages.",5 Eye-tracking,[0],[0]
"Readers saccade over portions of text and regress back to preceding text in complex patterns, but studies have correlated certain measures with certain processing constraints (see Clifton et al. 2007 for a review).",5 Eye-tracking,[0],[0]
"For example, the initial length of time fixated on a single word is correlated with word identification time; whereas regression durations after a word is fixated (but prior to a fixation in a new region) are hypothesized to correlate
with integration difficulty.
",5 Eye-tracking,[0],[0]
"Since this work focuses on incremental processing, all processing that occurs up to a given point in the sentence is of interest.",5 Eye-tracking,[0],[0]
"Therefore, in this study, predictions will be compared to go-past durations.",5 Eye-tracking,[0],[0]
"Go-past durations are calculated by summing all fixations in a region of text, including regressions, until a new region is fixated, which accounts for additional processing that may take place after initial lexical access, but before the next region is processed.",5 Eye-tracking,[0],[0]
"For example, if one region ends at word 5 in a sentence, and the next fixation lands on word 8, then the go-past region consists of words 6-8 and the go-past duration sums all fixations until a fixation occurs after word 8.",5 Eye-tracking,[0],[0]
"The measures presented in this paper were evaluated on the Dundee eye-tracking corpus (Kennedy et al., 2003).",6 Evaluation,[0],[0]
The corpus consists of 2388 sentences of naturally occurring news text written in standard British English.,6 Evaluation,[0],[0]
"The corpus also includes eye-tracking data from 10 native English speakers, which provides a test corpus of 260,124 subject-duration pairs of reading time data.",6 Evaluation,[0],[0]
"Of this, any fixated words appearing fewer than 5 times in the training data were considered unknown and were filtered out to obtain accurate predictions.",6 Evaluation,[0],[0]
Fixations on the first or last words of a line were also filtered out to avoid any ‘wrap-up’ effects resulting from preparing to saccade to the beginning of the next line or resulting from orienting to a new line.,6 Evaluation,[0],[0]
"Additionally, following Demberg and Keller (2008), any fixations that skip more than 4 words were attributed to track loss by the eyetracker or lack of attention of the reader and so were excluded from the analysis.",6 Evaluation,[0],[0]
"This left the final evaluation corpus with 151,331 subject-duration pairs.
",6 Evaluation,[0],[0]
"The evaluation consisted of fitting a linear mixedeffects model (Baayen et al., 2008) to reading time durations using the lmer function of the lme4 R package (Bates et al., 2011; R Development Core Team, 2010).",6 Evaluation,[0],[0]
"This allowed by-subject and by-item variation to be included in the initial regression as random intercepts in addition to several baseline predictors.6 Before fitting, the durations extracted from
6Each fixed effect was centered to reduce collinearity.
",6 Evaluation,[0],[0]
"the corpus were log-transformed, producing more normally distributed data to obey the assumptions of linear mixed effects models.7
Included among the fixed effects were the position in the sentence that initiated the go-past region (SENTPOS) and the number of characters in the initiating word (NRCHAR).",6 Evaluation,[0],[0]
"The difficulty of integrating a word may be seen in whether the immediately following word was fixated (NEXTISFIX), and similarly if the immediately previous word was fixated (PREVISFIX)",6 Evaluation,[0],[0]
the current word probably need not be fixated for as long.,6 Evaluation,[0],[0]
"Finally, unigram (LOGPROB) and bigram probabilities are included.",6 Evaluation,[0],[0]
The bigram probabilities are those of the current word given the previous word (LOGFWPROB) and the current word given the following word (LOGBWPROB).,6 Evaluation,[0],[0]
"Fossum and Levy (2012) showed that for n-gram probabilities to be effective predictors on the Dundee corpus, they must be calculated from a wide variety of texts, so following them, this study used the Brown corpus (Francis and Kucera, 1979), the WSJ Sections 02-21 (Marcus et al., 1993), the written text portion of the British National Corpus (BNC Consortium, 2007), and the Dundee corpus (Kennedy et al., 2003).",6 Evaluation,[0],[0]
This amounted to an n-gram training corpus of roughly 87 million words.,6 Evaluation,[0],[0]
"These statistics were smoothed using the SRILM (Stolcke, 2002) implementation of modified Kneser-Ney smoothing (Chen and Goodman, 1998).",6 Evaluation,[0],[0]
"Finally, total surprisal (SURP) was included to account for frequency effects in the baseline.
",6 Evaluation,[0],[0]
"The preceding measures are commonly used in baseline models to fit reading time data (Demberg and Keller, 2008; Frank and Bod, 2011; Fossum and Levy, 2012) and were calculated from the final word of each go-past region.",6 Evaluation,[0],[0]
The following measures create a more sophisticated baseline by accumulating over the entire go-past region to capture what must be integrated into the discourse to continue the parse.,6 Evaluation,[0],[0]
One factor (CWDELTA) simply counts the number of words in each go-past region.,6 Evaluation,[0],[0]
"Cumula-
",6 Evaluation,[0],[0]
"7In particular, these models assume the noise in the data is normally distributed.",6 Evaluation,[0],[0]
Initial exploratory trials showed that the residuals of fitting any sensible baseline also become more normally distributed if the response variable is log-transformed.,6 Evaluation,[0],[0]
"Finally, the directions of the effects remain the same whether or not the reading times are log-transformed, though significance cannot be ascertained without the transform.
",6 Evaluation,[0],[0]
"tive total surprisal (CUMUSURP) and cumulative entropy reduction (ENTRED) give the surprisal (Hale, 2001) and entropy reduction (Hale, 2003) summed over the go-past region.",6 Evaluation,[0],[0]
"To avoid convergence issues, each of the cumulative measures is residualized from the next simpler model in the following order: CWDELTA from the standard baseline, CUMUSURP from the baseline with CWDELTA, and ENTRED from the baseline with all other effects.
",6 Evaluation,[0],[0]
Residualization was accomplished by using the simpler mixed-effects model to fit the measure of interest.,6 Evaluation,[0],[0]
The residuals from that model fit were then used in place of the factor of interest.,6 Evaluation,[0],[0]
All joint interactions were included in the baseline model as well.,6 Evaluation,[0],[0]
"Finally, to account for spillover effects (Just et al., 1982) where processing from a previous region contributes to the following duration, the above baseline predictors from the previous go-past region were included as factors for the current region.
",6 Evaluation,[0],[0]
"Having SURP as a predictor with CUMUSURP may seem redundant, but initial analyses showed SURP was a significant predictor over CUMUSURP when CWDELTA was a separate factor in the baseline (current: p = 2.2 · 10−16 spillover: p = 2 · 10−15) and vice versa (current: p = 2.2 · 10−16 spillover: p = 6 · 10−5).",6 Evaluation,[0],[0]
One reason for this could be that go-past durations conflate complexity experienced when initially fixating on a region with the difficulty experienced during regressions.,6 Evaluation,[0],[0]
"By including both versions of surprisal, the model is able to account for frequency effects occurring in both conditions.
",6 Evaluation,[0],[0]
"This study is only interested in how well the proposed memory-based measures fit the data over the baseline, so to avoid fitting to the test data or weakening the baseline by overfitting to training data, the full baseline was used in the final evaluation.
",6 Evaluation,[0],[0]
Each measure proposed in this paper was summed over go-past regions to make it cumulative and was residualized from all non-spillover factors before being included on top of the full baseline as a main effect.,6 Evaluation,[0],[0]
"Likewise, the spillover version of each proposed measure was residualized from the other spillover factors before being included as a main effect.",6 Evaluation,[0],[0]
Only a single proposed measure (or its spillover corrollary) was included in each model.,6 Evaluation,[0],[0]
The results shown in Table 3 reflect the probability of the full model fit being obtained by the model lacking each factor of interest.,6 Evaluation,[0],[0]
"This was found via posterior sam-
pling of each factor using the Markov chain Monte Carlo implementation of the languageR R package (Baayen, 2008).
",6 Evaluation,[0],[0]
The results indicate that the F+L– and F–L+ measures were both significant predictors of duration as expected.,6 Evaluation,[0],[0]
"Further, F–L– and F+L+, which both simply reflect sequential cueing, were not significant predictors of go-past duration, also as expected.",6 Evaluation,[0],[0]
The fact that F+L– was strongly predictive over the baseline is encouraging as it suggests that memory limitations could provide at least a partial explanation of why certain constructions are less frequent in corpora and thus yield a high surprisal.,7 Discussion and Conclusion,[0],[0]
"Moreover, it indicates that the model corroborates the shared prediction of most of the memory-based models that initiating a new connected component slows processing.
",7 Discussion and Conclusion,[0],[0]
"The fact that F–L+ is predictive but has a negative coefficient could be evidence of anti-locality, or it could be an indication of some sort of processing momentum due to dynamic recruitment of additional processing resources (Just and Varma, 2007).",7 Discussion and Conclusion,[0],[0]
"Since anti-locality is an expectation-based frequency effect, and since this study controlled for frequency effects with n-grams, surprisal, and entropy reduction, an anti-locality explanation would rely on either (i) more precise variants of the metrics used in this study or (ii) other frequency metrics altogether.",7 Discussion and Conclusion,[0],[0]
"Future work could investigate the possibility of anti-locality by looking at the distance between an encoding operation and its corresponding
integration action to see if the integration facilitation observed in this study is driven by longer embeddings or if there is simply a general facilitation effect when completing embeddings.
",7 Discussion and Conclusion,[0],[0]
"The finding of a negative integration cost was previously observed by Wu et al. (2010) as well as Demberg and Keller (2008), although Demberg and Keller calculated it using the original referent-based definitions of Gibson (1998; 2000) and varied which parts of speech counted for calculating integration cost.",7 Discussion and Conclusion,[0],[0]
"Ultimately, Demberg and Keller (2008) concluded that the negative coefficient was evidence that integration cost was not a good broad-coverage predictor of reading times; however, this study has replicated the effect and showed it to be a very strong predictor of reading times, albeit one that is correlated with facilitation rather than inhibition.
",7 Discussion and Conclusion,[0],[0]
"It is interesting that many studies have found negative integration cost using naturalistic stimuli while others have consistently found positive integration cost when using constructed stimuli with multiple center embeddings presented without context (Gibson, 2000; Chen et al., 2005; Kwon et al., 2010).",7 Discussion and Conclusion,[0],[0]
It may be the case that any dynamic recruitment is overwhelmed by the memory demands of multiply center-embedded stimuli.,7 Discussion and Conclusion,[0],[0]
"Alternatively, it may be that the difficulty of processing multiply center-embedded sentences containing ambiguities produces anxiety in subjects, which slows processing at implicit prosodic boundaries (Fodor, 2002; Mitchell et al., 2008).",7 Discussion and Conclusion,[0],[0]
"In any case, the source of this discrepancy presents an attractive target for future research.
",7 Discussion and Conclusion,[0],[0]
"In general, sequential prediction does not seem to present people with any special ease or difficulty as evidenced by the lack of significance of F–L– and F+L+ predictions when frequency effects are factored out.",7 Discussion and Conclusion,[0],[0]
"This supports a theory of sequential, content-based cueing (Botvinick, 2007) that predicts that certain states would directly cue other states and thus avoid recall difficulty.",7 Discussion and Conclusion,[0],[0]
An example of this may be seen in the case of a transitive verb triggering the prediction of a direct object.,7 Discussion and Conclusion,[0],[0]
"This kind of cueing would show up as a frequency effect predicted by surprisal rather than as a memory-based cost, due to frequent occurrences becoming ingrained as a learned skill.",7 Discussion and Conclusion,[0],[0]
"Future work could use these sequential cueing operations to investigate further claims
of the dynamic recruitment hypothesis.",7 Discussion and Conclusion,[0],[0]
"One of the implications of the hypothesis is that recruitment of resources alleviates the initial encoding cost, which allows the parser to continue on as before the embedding.",7 Discussion and Conclusion,[0],[0]
"DLT, on the other hand, predicts that there is a storage cost for maintaining unresolved dependencies during a parse (Gibson, 2000).",7 Discussion and Conclusion,[0],[0]
"By weighting each of the sequential cueing operations with the embedding depth at which it occurs, an experiment may be able to test these two predictions.
",7 Discussion and Conclusion,[0],[0]
This study has shown that measures based on working memory operations have strong predictivity over other previously proposed measures including those associated with frequency effects.,7 Discussion and Conclusion,[0],[0]
This suggests that memory limitations may provide a partial explanation of what gives rise to frequency effects.,7 Discussion and Conclusion,[0],[0]
"Lastly, this paper provides evidence that there is a robust facilitation effect in English that arises from completing center embeddings.
",7 Discussion and Conclusion,[0],[0]
"The hierarchic sequence model, all evaluation scripts, and regression results for all baseline predictors used in this paper are freely available at http://sourceforge.net/projects/modelblocks/.",7 Discussion and Conclusion,[0],[0]
"Thanks to Peter Culicover, Micha Elsner, and three anonymous reviewers for helpful suggestions.",Acknowledgements,[0],[0]
This work was funded by an OSU Department of Linguistics Targeted Investment for Excellence (TIE) grant for collaborative interdisciplinary projects conducted during the academic year 2012-13.,Acknowledgements,[0],[0]
The frequency of words and syntactic constructions has been observed to have a substantial effect on language processing.,abstractText,[0],[0]
This begs the question of what causes certain constructions to be more or less frequent.,abstractText,[0],[0]
"A theory of grounding (Phillips, 2010) would suggest that cognitive limitations might cause languages to develop frequent constructions in such a way as to avoid processing costs.",abstractText,[0],[0]
This paper studies how current theories of working memory fit into theories of language processing and what influence memory limitations may have over reading times.,abstractText,[0],[0]
Measures of such limitations are evaluated on eye-tracking data and the results are compared with predictions made by different theories of processing.,abstractText,[0],[0]
An Analysis of Frequency- and Memory-Based Processing Costs,title,[0],[0]
"erties of training a two-layered ReLU network g(x;w) = ∑K
j=1 σ(w ⊺
j x) with centered d-dimensional spherical Gaussian input x (σ=ReLU). We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w∗. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors. First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case. On the other hand, convergence to w∗ for one ReLU node is guaranteed with at least (1 − ǫ)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by
O(ǫ/ √ d), consistent with empirical practice. For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w∗ (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics. We assume no independence of ReLU activations. Simulation verifies our findings.",text,[0],[0]
"Despite empirical success of deep learning (e.g., Computer Vision (He et al., 2016; Simonyan & Zisserman, 2015; Szegedy et al., 2015; Krizhevsky et al., 2012), Natural Language Processing (Sutskever et al., 2014) and Speech Recognition (Hinton et al., 2012)), it remains elusive how
1Facebook AI Research.",1. Introduction,[0],[0]
Correspondence to: Yuandong Tian,1. Introduction,[0],[0]
"<yuandong@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
and why simple methods like gradient descent can solve the complicated non-convex optimization during training.,1. Introduction,[0],[0]
"In this paper, we focus on a two-layered ReLU network:
g(x;w) =
K ∑
j=1
σ(w⊺j x), (1)
Here σ(x) = max(x, 0) is the ReLU nonlinearity.",1. Introduction,[0],[0]
"We consider the setting that a student network is optimized to minimize the l2 distance between its prediction and the supervision provided by a teacher network of the same architecture with fixed parameters w∗. Note that although the network prediction (Eqn. 1) is convex, when coupled with loss (e.g., l2 loss Eqn. 2), the optimization becomes highly non-convex and has exponential number of critical points.
",1. Introduction,[0],[0]
"To analyze it, we introduce a simple analytic formula for population gradient in the case of l2 loss, when inputs x are sampled from zero-mean spherical Gaussian.",1. Introduction,[0],[0]
"Using this formula, critical point and convergence analysis follow.
",1. Introduction,[0],[0]
"For critical points, we show that critical points outside the principal hyperplane (the subspace spanned by w∗) form manifolds.",1. Introduction,[0],[0]
"We also characterize the region in the principal hyperplane that has no critical points, in two ReLU case.
",1. Introduction,[0],[0]
We also analyze the convergence behavior under the population gradient.,1. Introduction,[0],[0]
"Using Lyapunov method (LaSalle & Lefschetz, 1961), for single ReLU case we prove that gradient descent converges to w∗ with at least (1 − ǫ)/2 probability, if initialized randomly with standard deviation
upper-bounded by O(ǫ/ √ d), verifying common initialization techniques (Bottou, 1988; Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012).",1. Introduction,[0],[0]
"For multiple ReLU case, when the teacher parameters {wj}Kj=1 form an orthonormal basis, we prove that (1) a symmetric weight initialization gets stuck at a saddle point and (2) a particular infinitesimal perturbation of (1) leads to convergence towards w∗ or its permutation.",1. Introduction,[0],[0]
"The behavior that the population gradient field is invariant under certain symmetry but the solution breaks it, is known as spontaneous symmetry breaking in physics.",1. Introduction,[0],[0]
"Although such behaviors are known practically, to our knowledge, we first formally characterize them in 2-layered ReLU network.",1. Introduction,[0],[0]
"Codes are available 1.
",1. Introduction,[0],[0]
1github.com/yuandong-tian/ICML17_ReLU,1. Introduction,[0],[0]
"For multilayer linear network, many works analyze its critical points and convergence behaviors.",2. Related Works,[0],[0]
"(Saxe et al., 2013) analyzes its dynamics of gradient descent and (Kawaguchi, 2016) shows every local minimum is global.",2. Related Works,[0],[0]
"On the other hand, very few theoretical works have been done for nonlinear networks.",2. Related Works,[0],[0]
"(Mei et al., 2016) shows the global convergence for a single nonlinear node whose derivatives of activation σ′, σ′′, σ′′′ are bounded and σ′ > 0.",2. Related Works,[0],[0]
"Similar to our approach, (Saad & Solla, 1996) also uses the student-teacher setting and analyzes the student dynamics when the teacher’s parameters w∗ are orthonormal.",2. Related Works,[0],[0]
"However, their activation is Gaussian error function erf(x), and only the local behaviors of the two critical points (the initial saddle point near the origin and w∗) are analyzed.",2. Related Works,[0],[0]
"Recent paper (Zhang et al., 2017) analyzes a similar teacher-student setting on 2-layered network when the involved function is harmonic, but it is unclear how the conclusion is generalized to ReLU case.",2. Related Works,[0],[0]
"To our knowledge, our close-form formula for 2-layered ReLU network is novel, as well as the critical point and convergence analysis.",2. Related Works,[0],[0]
"Concurrent work (Brutzkus & Globerson, 2017) proposes the same formula with a different approach, and provides similar convergence analysis for one node.",2. Related Works,[0],[0]
"For multiple nodes, they assume non-overlapping shared weights, a special case of our assumption (Sec. 6.2) that weights are cyclically symmetric and orthonormal.
",2. Related Works,[0],[0]
Many previous works analyze nonlinear network based on the assumption of independent activations: the activations of ReLU (or other nonlinear) nodes are independent of the input and/or mutually independent.,2. Related Works,[0],[0]
"For example, (Choromanska et al., 2015a;b) relates the nonlinear ReLU network with spin-glass models when several assumptions hold, including the assumption of independent activations (A1p and A5u).",2. Related Works,[0],[0]
"(Kawaguchi, 2016) proves that every local minimum in nonlinear network is global based on similar assumptions.",2. Related Works,[0],[0]
"(Soudry & Carmon, 2016) shows the global
optimality of the local minimum in a two-layered ReLU network, when independent multiplicative Bernoulli noise is applied to the activations.",2. Related Works,[0],[0]
"In practice, activations that share the input are highly dependent.",2. Related Works,[0],[0]
"Ignoring such dependency misses important behaviors, and may lead to misleading conclusions.",2. Related Works,[0],[0]
"In this paper, no assumption of independent activations is made.",2. Related Works,[0],[0]
"Instead, we assume input to follow spherical Gaussian distribution, which gives more realistic and interdependent activations during training.
",2. Related Works,[0],[0]
"For sigmoid activation, (Fukumizu & Amari, 2000) gives complicated conditions for a local minimum to be global when adding a new node to a 2-layered network.",2. Related Works,[0],[0]
"(Janzamin et al., 2015) gives guarantees for parameter recovery of a 2-layered network learnt with tensor decomposition.",2. Related Works,[0],[0]
"In comparison, we analyze ReLU networks trained with gradient descent, which is more popular in practice.",2. Related Works,[0],[0]
Denote N as the number of samples and d as the input dimension.,3. Problem Definition,[0],[0]
The N -by-d matrix X is the input data and w∗ is the fixed parameter of the teacher network.,3. Problem Definition,[0],[0]
"Given the current estimation w, we have the following l2 loss:
J(w) = 1
2 ‖g(X;w∗)− g(X;w)‖2, (2)
Here we focus on population loss",3. Problem Definition,[0],[0]
"EX [J ], where the input X is assumed to follow spherical Gaussian distribution N (0, I).",3. Problem Definition,[0],[0]
Its gradient is the population gradient EX [∇Jw(w)] (abbrev.,3. Problem Definition,[0],[0]
E,3. Problem Definition,[0],[0]
[∇J ]).,3. Problem Definition,[0],[0]
"In this paper, we study critical points E",3. Problem Definition,[0],[0]
[∇J ] = 0 and vanilla gradient dynamics w t+1 = wt − ηE,3. Problem Definition,[0],[0]
"[∇J(wt)], where η is the learning rate.",3. Problem Definition,[0],[0]
Properties of ReLU.,4. The Analytical Formula,[0],[0]
ReLU nonlinearity has useful properties.,4. The Analytical Formula,[0],[0]
We define the gating function D(w) ≡,4. The Analytical Formula,[0],[0]
diag(Xw > 0) as an N -by-N binary diagonal matrix.,4. The Analytical Formula,[0],[0]
"Its l-th diagonal element is a binary variable showing whether the neuron is activated for sample l. Using this notation, σ(Xw) = D(w)Xw which means D(w) selects the output of a linear neuron, based on their activations.",4. The Analytical Formula,[0],[0]
"Note that D(w) only depends on the direction of w but not its magnitude.
",4. The Analytical Formula,[0],[0]
D(w) is also “transparent” with respect to derivatives.,4. The Analytical Formula,[0],[0]
"For example, at differentiable regions, Jacobianw[σ(Xw)] = σ′(Xw)X = D(w)X .",4. The Analytical Formula,[0],[0]
"This gives a very concise rule for gradient descent update in ReLU networks.
",4. The Analytical Formula,[0],[0]
One ReLU node.,4. The Analytical Formula,[0],[0]
"Given the properties of ReLU, the population gradient E",4. The Analytical Formula,[0],[0]
"[∇J ] can be written as:
E",4. The Analytical Formula,[0],[0]
[∇J ] = EX [X⊺D(w) (D(w)Xw −D(w∗)Xw∗)],4. The Analytical Formula,[0],[0]
"(3) Intuitively, this term vanishes when w → w∗, and should
be around N2 (w − w∗) if the data are evenly distributed, since roughly half of the samples are blocked.",4. The Analytical Formula,[0],[0]
"However, such an estimation fails to capture the nonlinear behavior.
",4. The Analytical Formula,[0],[0]
"If we define Population Gating (PG) function F (e,w) ≡",4. The Analytical Formula,[0],[0]
"X⊺D(e)D(w)Xw, then",4. The Analytical Formula,[0],[0]
"E [∇J ] can be written as:
E",4. The Analytical Formula,[0],[0]
"[∇J ] = E [F (w/‖w‖,w)]− E [F (w/‖w‖,w∗)] .",4. The Analytical Formula,[0],[0]
"(4)
Interestingly, F (e,w) has an analytic formula if the data X follow spherical Gaussian distribution:
Theorem 1",4. The Analytical Formula,[0],[0]
"Denote F (e,w) =",4. The Analytical Formula,[0],[0]
"X⊺D(e)D(w)Xw where e is a unit vector, X =",4. The Analytical Formula,[0],[0]
"[x1,x2, · · · ,xN ]⊺ is the N -by-d data matrix and D(w) = diag(Xw > 0) is a binary diagonal matrix.",4. The Analytical Formula,[0],[0]
"If xi ∼ N (0, I) (and thus bias-free), then:
E [F (e,w)]",4. The Analytical Formula,[0],[0]
"= N
2π",4. The Analytical Formula,[0],[0]
"[(π − θ)w + ‖w‖ sin θe] (5)
where θ = ∠(e,w) ∈",4. The Analytical Formula,[0],[0]
"[0, π] is the angle between e and w.
See the link2 for the proof of all theorems.",4. The Analytical Formula,[0],[0]
Note that we do not require X to be independent between samples.,4. The Analytical Formula,[0],[0]
"Intuitively, the first mass term N2π (π−θ)w aligns with w and is proportional to the amount of activated data whose ReLU are on.",4. The Analytical Formula,[0],[0]
"When θ = 0, the gating function is fully on and half of the data contribute to the term; when θ = π, the gating function is completely switched off.",4. The Analytical Formula,[0],[0]
The gate is controlled by the angle between w and the control signal e.,4. The Analytical Formula,[0],[0]
"The second asymmetric term is aligned with e, and is proportional to the asymmetry of the activated data samples (Fig. 2).
",4. The Analytical Formula,[0],[0]
"Note that the expectation analysis smooths out ReLU and leaves only one singularity at the origin, where E",4. The Analytical Formula,[0],[0]
[∇J ] is not continuous.,4. The Analytical Formula,[0],[0]
"That is, if approaching from different directions towards w = 0, E",4. The Analytical Formula,[0],[0]
[∇J ] is different.,4. The Analytical Formula,[0],[0]
"With the close form of F , E",4. The Analytical Formula,[0],[0]
"[∇J ] also has a close form:
E",4. The Analytical Formula,[0],[0]
"[∇J ] = N 2 (w−w∗)+N 2π
( θw∗ − ‖w ∗‖",4. The Analytical Formula,[0],[0]
"‖w‖ sin θw ) (6)
where θ = ∠(w,w∗) ∈",4. The Analytical Formula,[0],[0]
"[0, π].",4. The Analytical Formula,[0],[0]
"The first term is from linear approximation, while the second term shows the nonlinear behavior.
",4. The Analytical Formula,[0],[0]
"For linear case, D ≡ I (no gating) and thus ∇J ∝",4. The Analytical Formula,[0],[0]
X⊺X(w − w∗).,4. The Analytical Formula,[0],[0]
"For spherical Gaussian input X , EX [X
⊺X] = I and E",4. The Analytical Formula,[0],[0]
[∇J ] ∝ w−w∗.,4. The Analytical Formula,[0],[0]
"Therefore, the dynamics has only one critical point and global convergence follows, which is consistent with its convex nature.
",4. The Analytical Formula,[0],[0]
Extension to other distributions.,4. The Analytical Formula,[0],[0]
"From its definition, E [F (e,w)]",4. The Analytical Formula,[0],[0]
"= E [X⊺D(e)D(w)Xw] is linear to ‖w‖, regardless of the distribution of X .",4. The Analytical Formula,[0],[0]
"On the other hand, isotropy in spherical Gaussian distribution leads to the fact
2http://yuandong-tian.com/ssb-supp.pdf
that E",4. The Analytical Formula,[0],[0]
"[F (e,w)] only depends on angles between vectors.",4. The Analytical Formula,[0],[0]
"For other isotropic distributions, we could similarly derive:
E [F (e,w)] = A(θ)w + ‖w‖B(θ)e (7)
where A(0) = N/2 (gating fully on), A(π) = 0",4. The Analytical Formula,[0],[0]
"(gating fully off), and B(0) = B(π) = 0",4. The Analytical Formula,[0],[0]
(no asymmetry when w and e are aligned).,4. The Analytical Formula,[0],[0]
"Although we focus on spherical Gaussian case, many following analysis, in particular critical point analysis, can also be applied to Eqn. 7.
",4. The Analytical Formula,[0],[0]
Multiple ReLU node.,4. The Analytical Formula,[0],[0]
"For Eqn. 1 that contains K ReLU node, we could similarly write down the population gradient with respect to wj (note that ej = wj/‖wj‖):
E",4. The Analytical Formula,[0],[0]
"[ ∇wjJ ] =
K ∑
j′=1
E [F (ej ,wj′)]− K ∑
j′=1
E [ F (ej ,w ∗ j′) ]
(8)",4. The Analytical Formula,[0],[0]
"By solving Eqn. 8 (the normal equation, E [ ∇wjJ ]
= 0), we could identify all critical points of g(x).",5. Critical Point Analysis,[0],[0]
"However, it is highly nonlinear and cannot be solved easily.",5. Critical Point Analysis,[0],[0]
"In this paper, we provide conditions for critical points using the structure of Eqn. 8.",5. Critical Point Analysis,[0],[0]
"The case study for K = 2 gives examples for saddle points and regions without critical points.
",5. Critical Point Analysis,[0],[0]
"For convenience, we define Π∗ as the Principal Hyperplane spanned by K ground truth weight vectors.",5. Critical Point Analysis,[0],[0]
Note that Π∗ is at most K dimensional.,5. Critical Point Analysis,[0],[0]
"{wj}Kj=1 is said to be in-plane, if all wj ∈ Π∗.",5. Critical Point Analysis,[0],[0]
Otherwise it is out-of-plane.,5. Critical Point Analysis,[0],[0]
"The normal equation {E [ ∇wjJ ] = 0}Kj=1 contain Kd scalar equations and can be written as the following:
Y E⊺ = B∗W ∗⊺ (9)
where Y = diag(sinΘ⊺w̄ − sinΘ∗⊺w̄∗) + (π11⊺ − Θ⊺)diagw̄ and B∗ = π11⊺ − (Θ∗)⊺. Here θ∗j ′
j ≡ ∠(wj ,w ∗ j′), θ j′
j ≡ ∠(wj ,wj′), Θ =",5.1. Normal Equation,[0],[0]
"[θij ] (i-th row, j-th column of Θ is θij) and Θ ∗ =",5.1. Normal Equation,[0],[0]
"[θ∗ij ].
Note that Y and B∗ are both K-by-K matrices that only depend on angles and magnitudes, and hence rotational invariant.",5.1. Normal Equation,[0],[0]
"This leads to the following theorem characterizing the structure of out-of-plane critical points:
Theorem 2 If d ≥ K+2, then out-of-plane critical points (solutions of Eqn. 9) are non-isolated and lie in a manifold.
",5.1. Normal Equation,[0],[0]
The intuition is to construct a rotational matrix that is not identity matrix but keeps Π∗ invariant.,5.1. Normal Equation,[0],[0]
Such matrices form a Lie group L that transforms critical points to critical points.,5.1. Normal Equation,[0],[0]
"Then for any out-of-plane critical point, there is one matrix in L that changes at least one of its weights, yielding a non-isolated different critical point.
",5.1. Normal Equation,[0],[0]
"Note that Thm. 2 also works for any general isotropic distribution, in which E [F (e,w)] has the form of Eqn. 7.",5.1. Normal Equation,[0],[0]
"This is due to the symmetry of the input X , which in turn affects the geometry of critical points.",5.1. Normal Equation,[0],[0]
"The theorem also explains why we have flat minima (Hochreiter et al., 1995; Dauphin et al., 2014) often occuring in practice.",5.1. Normal Equation,[0],[0]
"To analyze in-plane critical points, it suffices to study gradient projections on Π∗.",5.2. In-Plane Normal Equation,[0],[0]
"When {wj} is full-rank, the projections could be achieved by right-multiplying both sides by {ej′}, which gives K2 equations:
M(Θ)w̄",5.2. In-Plane Normal Equation,[0],[0]
"= M∗(Θ,Θ∗)w̄∗ (10)
",5.2. In-Plane Normal Equation,[0],[0]
"This again shows decomposition of angles and magnitudes, and linearity with respect to the norms of weight vectors.",5.2. In-Plane Normal Equation,[0],[0]
Here w̄ =,5.2. In-Plane Normal Equation,[0],[0]
"[‖w1‖, ‖w2‖, . . .",5.2. In-Plane Normal Equation,[0],[0]
", ‖wK‖]⊺ and similarly for w̄
∗. M and M∗ are K2-by-K matrices that only depend on angles.",5.2. In-Plane Normal Equation,[0],[0]
"Entries of M and M∗ are:
mjj′,k =",5.2. In-Plane Normal Equation,[0],[0]
"(π − θkj ) cos θkj′ + sin θkj cos θjj′ (11) m∗jj′,k =",5.2. In-Plane Normal Equation,[0],[0]
(π − θ∗kj ),5.2. In-Plane Normal Equation,[0],[0]
cos,5.2. In-Plane Normal Equation,[0],[0]
"θ∗kj′ + sin θ∗kj cos θjj′ (12)
",5.2. In-Plane Normal Equation,[0],[0]
"Here index j is the j-th column of Eqn. 9, j′ is from projection vector ej′ and k is the k-th weight magnitude.
",5.2. In-Plane Normal Equation,[0],[0]
Diagnoal constraints.,5.2. In-Plane Normal Equation,[0],[0]
"For “diagonal” constraints (j, j) of Eqn. 10, we have cos θjj = 1 and mjj,k = h(θ",5.2. In-Plane Normal Equation,[0],[0]
"k j ),",5.2. In-Plane Normal Equation,[0],[0]
"m ∗ jj,k = h(θ∗kj ), where h(θ) =",5.2. In-Plane Normal Equation,[0],[0]
(π− θ) cos θ+ sin θ.,5.2. In-Plane Normal Equation,[0],[0]
"Therefore, we arrive at the following subset of the constraints:
Mrw̄ = M ∗ r w̄ ∗",5.2. In-Plane Normal Equation,[0],[0]
"(13)
where Mr = h(Θ ⊺) and M∗r = h(Θ ∗⊺) are both K-byK matrices.",5.2. In-Plane Normal Equation,[0],[0]
"Note that if Mr is full-rank, then we could solve w̄ from Eqn. 13 and plug it back in Eqn. 10 to check whether it is indeed a critical point.",5.2. In-Plane Normal Equation,[0],[0]
"This gives necessary conditions for critical points that only depend on angles.
",5.2. In-Plane Normal Equation,[0],[0]
Separable Property.,5.2. In-Plane Normal Equation,[0],[0]
"Interestingly, the plugging back operation leads to conditions that are separable with respect to ground truth weight (Fig. 3).",5.2. In-Plane Normal Equation,[0],[0]
"To see this, we first define the following quantity Ljj′ which is a function between a single (rather than K) ground truth unit weight vector e∗ and all current unit weights {el}Kl=1:
Ljj′({θ∗l },Θ) = m∗jj′",5.2. In-Plane Normal Equation,[0],[0]
"− v⊺M−1r mjj′ (14)
where θ∗l = ∠(e ∗, el) is the angle between e ∗ and el, v = v({θ∗l }) =",5.2. In-Plane Normal Equation,[0],[0]
"[h(θ∗1), . . .",5.2. In-Plane Normal Equation,[0],[0]
", h(θ∗K)]⊺, and m∗jj′ = (π − θ∗j ) cos θ∗j′ + sin θ∗j cos θjj′ (like Eqn. 12).",5.2. In-Plane Normal Equation,[0],[0]
Note that v({θ∗jl }) is the j-th column of M∗r .,5.2. In-Plane Normal Equation,[0],[0]
Fig. 3 illustrates the case when K = 2.,5.2. In-Plane Normal Equation,[0],[0]
"Ljj′ has the following properties:
Proposition 1 Ljj′({θ∗l },Θ) = 0",5.2. In-Plane Normal Equation,[0],[0]
when there exists l so that e∗ = el.,5.2. In-Plane Normal Equation,[0],[0]
"In addition, Ljj({θ∗l },Θ) = 0 always.
Intuitively, Ljj′ characterizes the relative geometric relationship among e∗ and {el}.",5.2. In-Plane Normal Equation,[0],[0]
"It is like determinant of a matrix whose columns are {el} and e∗. With Ljj′ , we have the following necessary conditions for critical points:
Theorem 3",5.2. In-Plane Normal Equation,[0],[0]
"If w̄∗ 6= 0, and for a given parameter w, Ljj′({θ∗kl },Θ) > 0 (or < 0) for all 1 ≤ k ≤ K, then w cannot be a critical point.",5.2. In-Plane Normal Equation,[0],[0]
"In this case, Mr and M ∗ r are 2-by-2 matrices.",5.3. Case study: K = 2 network,[0],[0]
"Here we discuss the case that both w1 and w2 are in Π∗.
Saddle points.",5.3. Case study: K = 2 network,[0],[0]
When θ12 = 0,5.3. Case study: K = 2 network,[0],[0]
"(w1 and w2 are collinear), Mr = π11 ⊺ is singular since e1 and e2 are identical.",5.3. Case study: K = 2 network,[0],[0]
"From Eqn. 9, if θ∗11 = θ ∗2 1 , i.e., they are both aligned with the bisector angle of w∗1 and w ∗ 2 , and πw̄ ⊺ 1 = h (
θ∗1∗2/2 ) (w̄∗)⊺1, then the current solution is a saddle point.",5.3. Case study: K = 2 network,[0],[0]
"Note that this gives one constraint for two weight magnitudes, and thus there exist infinite solutions.
",5.3. Case study: K = 2 network,[0],[0]
Region without critical points.,5.3. Case study: K = 2 network,[0],[0]
We rely on the following conjecture that is verified empirically in an exhaustive manner (Sec. 7.2).,5.3. Case study: K = 2 network,[0],[0]
"It characterizes zero-crossings of a 2D function on a closed region [0, 2π]× [0, π].",5.3. Case study: K = 2 network,[0],[0]
"In comparison, in-plane 2 ReLU network has 6 parameters and is more difficult to handle: 8 for w1, w2, w ∗ 1 and w ∗ 2 , minus the rotational and scaling symmetries.
∗ 1
Conjecture 1",5.3. Case study: K = 2 network,[0],[0]
"If e∗ is in the interior of Cone(e1, e2), then L12(θ ∗ 1 , θ ∗ 2 , θ 1 2) > 0.",5.3. Case study: K = 2 network,[0],[0]
"If e ∗ is in the exterior, then L12 < 0.
",5.3. Case study: K = 2 network,[0],[0]
This is also empirically true for L21.,5.3. Case study: K = 2 network,[0],[0]
"Combined with Thm. 3, we know that (Fig. 4):
Theorem 4 If Conjecture 1 is correct, then for 2 ReLU network, (w1,w2) (w1 6= w2) is not a critical point, if they both are in Cone(w∗1,w ∗ 2), or both out of it.
",5.3. Case study: K = 2 network,[0],[0]
"When exact one w∗ is inside Cone(w1,w2), whether (w1,w2) is a critical point remains open.",5.3. Case study: K = 2 network,[0],[0]
Application of Eqn. 5 also yields interesting convergence analysis.,6. Convergence Analysis,[0],[0]
"We focus on infinitesimal analysis, i.e., when learning rate η → 0 and the gradient update becomes a first-order differential equation:
dw/dt = −EX [∇wJ(w)] (15)
Then the populated objective EX [J ] does not increase:
dE",6. Convergence Analysis,[0],[0]
[J ] /dt =,6. Convergence Analysis,[0],[0]
−E [∇J ]⊺ dw/dt = −E,6. Convergence Analysis,[0],[0]
[∇J ]⊺ E,6. Convergence Analysis,[0],[0]
[∇J ] ≤ 0,6. Convergence Analysis,[0],[0]
"(16)
The goal of convergence analysis is to determine specific weight initializations w0 that leads to convergence to w∗ following the gradient descent dynamics (Eqn. 15).",6. Convergence Analysis,[0],[0]
"Using Lyapunov method (LaSalle & Lefschetz, 1961), we show that the gradient dynamics (Eqn. 15) converges to w∗ when w0 ∈",6.1. Single ReLU case,[0],[0]
Ω,6.1. Single ReLU case,[0],[0]
"= {w : ‖w −w∗‖ < ‖w∗‖}:
Theorem 5",6.1. Single ReLU case,[0],[0]
When w0 ∈,6.1. Single ReLU case,[0],[0]
Ω,6.1. Single ReLU case,[0],[0]
"= {w : ‖w −w∗‖ < ‖w∗‖}, following the dynamics of Eqn. 15, the Lyapunov function V (w) = 12‖w",6.1. Single ReLU case,[0],[0]
− w∗‖2 has dV/dt < 0,6.1. Single ReLU case,[0],[0]
"and the system is asymptotically stable and thus wt → w∗ when t → +∞.
The intuition is to represent dV/dt as a 2-by-2 bilinear form of vector [‖w‖, ‖w∗‖], and the bilinear coefficient matrix, as a function of angles, is negative definite (except for w = w∗).",6.1. Single ReLU case,[0],[0]
"Note that similar approaches do not apply to regions including the origin because at the origin, the population gradient is discontinuous.",6.1. Single ReLU case,[0],[0]
"Ω does not include the
origin and for any initialization w0 ∈ Ω, we could always find a slightly smaller subset Ω′δ =",6.1. Single ReLU case,[0],[0]
{w : ‖w − w∗‖ ≤ ‖w∗‖−δ} with δ > 0,6.1. Single ReLU case,[0],[0]
"that covers w0, and apply Lyapunov method within.",6.1. Single ReLU case,[0],[0]
"Note that the global convergence claim in (Mei et al., 2016) for l2 loss does not apply to ReLU, since it requires σ′(x) >",6.1. Single ReLU case,[0],[0]
"0.
",6.1. Single ReLU case,[0],[0]
Random Initialization.,6.1. Single ReLU case,[0],[0]
How to sample w0 ∈ Ω without knowing w∗?,6.1. Single ReLU case,[0],[0]
Uniform sampling around origin with radius r ≥ γ‖w∗‖ for any γ > 1 results in exponentially small success rate (r/‖w∗‖)d ≤ γ−d in high-dimensional space.,6.1. Single ReLU case,[0],[0]
"A better idea is to sample around the origin with very small radius (but not at w = 0), so that Ω looks like a hyperplane near the origin, and thus almost half samples are useful (Fig. 5(a)), as shown in the following theorem:
Theorem 6 The dynamics in Eqn. 6 converges to w∗ with probability at least (1 − ǫ)/2, if the initial value w0 is sampled uniformly from Br = {w : ‖w‖ ≤ r} with r ≤",6.1. Single ReLU case,[0],[0]
"ǫ √
2π d+1‖w∗‖.
",6.1. Single ReLU case,[0],[0]
The idea is to lower-bound the probability of the shaded area (Fig. 5(b)).,6.1. Single ReLU case,[0],[0]
"Thm. 6 gives an explanation for common initialization techniques (Glorot & Bengio, 2010; He et al., 2015; LeCun et al., 2012; Bottou, 1988) that uses random
variables with O(1/ √ d) standard deviation.",6.1. Single ReLU case,[0],[0]
"For multiple ReLUs, Lyapunov method on Eqn. 8 yields no decisive conclusion.",6.2. Multiple ReLU case,[0],[0]
"Here we focus on the symmetric property of Eqn. 8 and discuss a special case, that the teacher parameters {w∗j}Kj=1 and the initial weights {w0j}Kj=1 respect the following symmetry: wj = Pjw and w
∗",6.2. Multiple ReLU case,[0],[0]
"j = Pjw ∗, where Pj is an orthogonal matrix whose collection P ≡ {Pj}Kj=1 forms a group.",6.2. Multiple ReLU case,[0],[0]
"Without loss of generality, we set P1 as the identity.",6.2. Multiple ReLU case,[0],[0]
"Then from Eqn. 8 the population gradient becomes:
E [ ∇wjJ ] =",6.2. Multiple ReLU case,[0],[0]
"PjE [∇w1J ] (17) This means that if all wj and w ∗ j are symmetric under group actions, so does their population gradients.",6.2. Multiple ReLU case,[0],[0]
"There-
fore, the trajectory {wt} also respects the symmetry (i.e., Pjw t 1 = w t j) and we only need to solve one equation for E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] instead of K (here e = w/‖w‖):
E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] = K ∑
j′=1
E [F (e, Pj′w)]− E [F (e, Pj′w∗)]",6.2. Multiple ReLU case,[0],[0]
"(18)
Eqn. 18 has interesting properties, known as Spontaneous Symmetric-Breaking (SSB) in physics (Brading & Castellani, 2003), in which the equations of motion respect a certain symmetry but its solution breaks it (Fig. 6).",6.2. Multiple ReLU case,[0],[0]
"In our language, despite that the population gradient field E",6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] and the objective E [J ] are invariant to the group transformation P , i.e., for w∗ → Pjw∗, E",6.2. Multiple ReLU case,[0],[0]
[J ] and E,6.2. Multiple ReLU case,[0],[0]
"[∇wJ ] remain the same, its solution is not (Pjw 6= w).",6.2. Multiple ReLU case,[0],[0]
"Furthermore, since P is finite, as we will see, the final solution converges to different permutations of w∗ due to infinitesimal perturbations of initialization.
",6.2. Multiple ReLU case,[0],[0]
"To illustrate such behaviors, consider the following example in which {w∗j}Kj=1 forms an orthonormal basis and under this basis, P is a cyclic group in which Pj circularly shifts dimension by j − 1 (e.g., P2[1, 2, 3]⊺ =",6.2. Multiple ReLU case,[0],[0]
"[3, 1, 2]⊺).",6.2. Multiple ReLU case,[0],[0]
"In this case, if we start with w0 = x0w∗ + ∑
j 6=1 Pjw ∗ j =
[x0, y0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0] under the basis of w∗, then Eqn. 18 is further reduced to a convergent 2D nonlinear dynamics and Thm. 7 holds (Please check Supplementary Materials for the associated close-form of the 2D dynamics):
Theorem 7 For a bias-free two-layered ReLU network g(x;w) = ∑
j σ(w ⊺
j x) that takes spherical Gaussian inputs, if the teacher’s parameters {w∗j} form orthnomal bases, then (1) when the student parameters is initialized to be [x0, y0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0] under the basis of w∗, where (x0, y0) ∈ Ω",6.2. Multiple ReLU case,[0],[0]
"= {x ∈ (0, 1], y ∈",6.2. Multiple ReLU case,[0],[0]
"[0, 1], x > y}, then Eqn. 8 converges to teacher’s parameters {w∗j} (or (x, y) = (1, 0)); (2) when x0 = y0 ∈ (0, 1], then it converges to a saddle point x = y = 1 πK",6.2. Multiple ReLU case,[0],[0]
"( √ K − 1− arccos(1/ √ K) + π).
",6.2. Multiple ReLU case,[0],[0]
Thm. 7 suggests that when w0 =,6.2. Multiple ReLU case,[0],[0]
"[y0, x0, . . .",6.2. Multiple ReLU case,[0],[0]
", y0], the system converges to P2w
∗, etc.",6.2. Multiple ReLU case,[0],[0]
"Since |x0 − y0| can be arbitrarily small, a slightest perturbation around x0 = y0 leads
to a different fixed point Pjw ∗ for some j. Unlike single ReLU case, the initialization in Thm. 7 is w∗-dependent, and serves as an example for the branching behavior.
",6.2. Multiple ReLU case,[0],[0]
"Thm. 7 also suggests that for convergence, x0 and y0 can be arbitrarily small, regardless of the magnitude of w∗, showing a global convergence behavior.",6.2. Multiple ReLU case,[0],[0]
"In comparison, (Saad & Solla, 1996) uses Gaussian error function (σ = erf) as the activation, and only analyzes local behaviors near the two fixed points (origin and w∗).
",6.2. Multiple ReLU case,[0],[0]
"In practice, even with noisy initialization, Eqn. 18 and the original dynamics (Eqn. 8) still converge to w∗ (and its transformations).",6.2. Multiple ReLU case,[0],[0]
"We leave it as a conjecture, whose proof may lead to an initialization technique for 2-layered ReLU that is w∗-independent.
Conjecture 2",6.2. Multiple ReLU case,[0],[0]
"If the initialization w0 = x0w∗ + y0 ∑
j 6=1 Pjw ∗ + ǫ, where ǫ is noise and (x0, y0) ∈ Ω,
then Eqn. 8 also converges to w∗ with high probability.",6.2. Multiple ReLU case,[0],[0]
7.1.,7. Simulations,[0],[0]
"The analytical solution to F (e,w)
We verify E [F (e,w)]",7. Simulations,[0],[0]
= E [X⊺D(e)D(w)Xw] (Eqn. 5) with simulation.,7. Simulations,[0],[0]
"We randomly pick e and w so that their angle ∠(e,w) is uniformly distributed in [0, π].",7. Simulations,[0],[0]
"The analytical formula E [F (e,w)] is compared with F (e,w), which is computed via sampling on the input X that follows spherical Gaussian distribution.",7. Simulations,[0],[0]
"We use relative RMS error: err = ‖E [F (e,w)]",7. Simulations,[0],[0]
"− F (e,w)‖/‖F (e,w)‖. Fig. 7(a) shows the error distribution with respect to angles.",7. Simulations,[0],[0]
"For small θ, the gating function D(w) and D(e) mostly overlap and give a reliable estimation.",7. Simulations,[0],[0]
"When θ → π, D(w) and D(e)overlap less and the variance grows.",7. Simulations,[0],[0]
Note that our convergence analysis operate on θ ∈,7. Simulations,[0],[0]
"[0, π/2] and is not affected.",7. Simulations,[0],[0]
"In the following, we sample angles from [0, π/2].
",7. Simulations,[0],[0]
Fig. 7(a) shows that the formula is more accurate with more samples.,7. Simulations,[0],[0]
"We also examine other zero-mean distributions of X , e.g., U [−1/2, 1/2].",7. Simulations,[0],[0]
"As shown in Fig. 7(d), the formula still works for large d. Note that the error is computed up to a global scale, due to different normalization constants in probability distributions.",7. Simulations,[0],[0]
Whether Eqn. 5 applies for more general distributions remains open.,7. Simulations,[0],[0]
Conjecture 1 can be reduced to enumerate a complicated but 2D function via exhaustive sampling.,7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"In comparison, a full optimization of 2-ReLU network constrained on principal hyperplane Π∗ involves 6 parameters (8 parameters minus 2 degrees of symmetry) and is more difficult to handle.",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
Fig. 10 shows that empirically L12 has no extra zerocrossing other than e∗ = e1 or e2.,7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"As shown in Fig. 10(c), we have densely enumerated θ12 ∈",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
"[0, π] and e∗ on a
104 × 104 grid without finding any counterexamples.",7.2. Empirical Results in critical point analysis K = 2,[0],[0]
Fig. 8(a) and (b) shows the 2D vector field in Thm 7.,7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Fig. 8(c) shows the 2D trajectory towards convergence to the teacher’s parameters w∗. Interestingly, even when we initialize the weights as [10−3, 0]⊺, whose direction is aligned with w∗ at [1, 0]⊺, the gradient descent still takes detours to reach the destination.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"This is because at the beginning of optimization, all ReLU nodes explain the training error in the same way (both x and y increases); when the “obvious” component is explained, the error pushes some nodes to explain other components.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Hence, specialization follows (x increases but y decreases).
",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Fig. 9 shows empirical convergence for K ≥ 2, when the initialization deviates from initialization",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"[x, y, . . .",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
", y] in Thm. 7.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"Unless the deviation is large, w converges to w ∗. For more general network g2(x) = ∑K j=1 ajσ(w ⊺
j x), when aj > 0 convergence follows.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
"When some aj is negative, the network fails to converge to w∗, even when the student is initialized with the true values {a∗j}Kj=1.",7.3. Convergence analysis for multiple ReLU nodes,[0],[0]
A natural question is whether the proposed method can be extended to multilayer ReLU network.,8. Extension to multilayer ReLU network,[0],[0]
"In this case, there is similar subtraction structure for gradient as Eqn. 3:
Proposition 2",8. Extension to multilayer ReLU network,[0],[0]
Denote,8. Extension to multilayer ReLU network,[0],[0]
"[c] as all nodes in layer c. Denote u ∗ j and uj as the output of node j at layer c of the teacher and student network, then the gradient of the parameters wj immediate under node j ∈",8. Extension to multilayer ReLU network,[0],[0]
"[c] is:
∇wjJ = X⊺c",8. Extension to multilayer ReLU network,[0],[0]
"DjQj ∑
j′∈[c]
(Qj′uj′ −Q∗j′u∗j′) (19)
where Xc is the data fed into node j, Qj and Q ∗",8. Extension to multilayer ReLU network,[0],[0]
j are N - by-N diagonal matrices.,8. Extension to multilayer ReLU network,[0],[0]
For any node k ∈,8. Extension to multilayer ReLU network,[0],[0]
"[c + 1], Qk = ∑
j∈[c] wjkDjQj and similarly for Q ∗",8. Extension to multilayer ReLU network,[0],[0]
"k.
The 2-layered network in this paper is a special case with Qj = Q ∗",8. Extension to multilayer ReLU network,[0],[0]
j = I .,8. Extension to multilayer ReLU network,[0],[0]
"Despite the difficulty that Qj is now depends on the weights of upper layers, and the input Xc is not necessarily Gaussian distributed, Proposition 2 gives a mathematical framework to explore the structure of gradient.",8. Extension to multilayer ReLU network,[0],[0]
"For example, a similar definition of Population Gradi-
ent function is possible.",8. Extension to multilayer ReLU network,[0],[0]
"In this paper, we study the gradient descent dynamics of a 2-layered bias-free ReLU network.",9. Conclusion and Future Work,[0],[0]
The network is trained using gradient descent to reproduce the output of a teacher network with fixed parameters w∗ in the sense of l2 norm.,9. Conclusion and Future Work,[0],[0]
We propose a novel analytic formula for population gradient when the input follows zero-mean spherical Gaussian distribution.,9. Conclusion and Future Work,[0],[0]
This formula leads to interesting critical point and convergence analysis.,9. Conclusion and Future Work,[0],[0]
"Specifically, we show that critical points out of the hyperplane spanned by w∗ are not isolated and form manifolds.",9. Conclusion and Future Work,[0],[0]
"For two ReLU case, we characterize regions that contain no critical points.",9. Conclusion and Future Work,[0],[0]
"For convergence analysis, we show guaranteed convergence for a single ReLU case with random initialization whose stan-
dard deviation is on the order of O(1/ √ d).",9. Conclusion and Future Work,[0],[0]
"For multiple ReLU case, we show that an infinitesimal change of weight initialization leads to convergence to different optima.
",9. Conclusion and Future Work,[0],[0]
Our work opens many future directions.,9. Conclusion and Future Work,[0],[0]
"First, Thm. 2 characterizes the non-isolating nature of critical points in the case of isotropic input distribution, which explains why often practical solutions of NN are degenerated.",9. Conclusion and Future Work,[0],[0]
What if the input distribution has different symmetries?,9. Conclusion and Future Work,[0],[0]
Will such symmetries determine the geometry of critical points?,9. Conclusion and Future Work,[0],[0]
"Second, empirically we see convergence cases that are not covered by the theorems, suggesting the conditions imposed by the theorems can be weaker.",9. Conclusion and Future Work,[0],[0]
"Finally, how to apply similar analysis to broader distributions and how to generalize the analysis to multiple layers are also open problems.
",9. Conclusion and Future Work,[0],[0]
"Acknowledgement We thank Léon Bottou, Ruoyu Sun, Jason Lee, Yann Dauphin and Nicolas Usunier for discussions and insightful suggestions.",9. Conclusion and Future Work,[0],[0]
"In this paper, we explore theoretical properties of training a two-layered ReLU network g(x;w) = ∑K j=1 σ(w ⊺ j x) with centered d-dimensional spherical Gaussian input x (σ=ReLU).",abstractText,[0],[0]
"We train our network with gradient descent on w to mimic the output of a teacher network with the same architecture and fixed parameters w. We show that its population gradient has an analytical formula, leading to interesting theoretical analysis of critical points and convergence behaviors.",abstractText,[0],[0]
"First, we prove that critical points outside the hyperplane spanned by the teacher parameters (“out-of-plane“) are not isolated and form manifolds, and characterize inplane critical-point-free regions for two ReLU case.",abstractText,[0],[0]
"On the other hand, convergence to w for one ReLU node is guaranteed with at least (1 − ǫ)/2 probability, if weights are initialized randomly with standard deviation upper-bounded by O(ǫ/ √ d), consistent with empirical practice.",abstractText,[0],[0]
"For network with many ReLU nodes, we prove that an infinitesimal perturbation of weight initialization results in convergence towards w (or its permutation), a phenomenon known as spontaneous symmetric-breaking (SSB) in physics.",abstractText,[0],[0]
We assume no independence of ReLU activations.,abstractText,[0],[0]
Simulation verifies our findings.,abstractText,[0],[0]
An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis,title,[0],[0]
"Low-rank approximation is an essential data processing technique for understanding large or noisy data in diverse areas including data compression, image and pattern recognition, signal processing, compressed sensing, latent semantic indexing, anomaly detection, and recommendation systems.",1. Introduction,[0],[0]
"Recent machine learning applications include training neural networks (Jaderberg et al., 2014; Kirkpatrick et al., 2017), second order online learning (Luo et al., 2016), representation learning (Wang et al., 2016), and reinforcement learning (Ghavamzadeh et al., 2010).
",1. Introduction,[0],[0]
"*Equal contribution 1University of California, Berkeley.",1. Introduction,[0],[0]
Correspondence to: David Anderson,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
davidanderson@berkeley.edu,1. Introduction,[0],[0]
">, Ming Gu <mgu@berkeley.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Additionally, a recent trend in machine learning is to include an approximation of second order information for better accuracy and faster convergence (Krummenacher et al., 2016).
",1. Introduction,[0],[0]
"In this work, we introduce a novel low-rank approximation algorithm called Spectrum-Revealing LU (SRLU) that can be efficiently computed and updated.",1. Introduction,[0],[0]
"Furthermore, SRLU preserves sparsity and can identify important data variables and observations.",1. Introduction,[0],[0]
"Our algorithm works on any data matrix, and achieves an approximation accuracy that only differs from the accuracy of the best approximation possible for any given rank by a constant factor.1
The major innovation in SRLU is the efficient calculation of a truncated LU factorization of the form
Π1AΠ T 2 =",1. Introduction,[0],[0]
( k m − k k L11 m − k L21 In−k ),1. Introduction,[0],[0]
( k n,1. Introduction,[0],[0]
− k U11 U12 S ),1. Introduction,[0],[0]
"≈ ( L11 L21 )( U11 U12
) def = L̂Û,
where Π1 and Π2 are judiciously chosen permutation matrices.",1. Introduction,[0],[0]
"The LU factorization is unstable, and in practice is implemented by pivoting (interchanging) rows during factorization, i.e. choosing permutation matrix Π1.",1. Introduction,[0],[0]
"For the truncated LU factorization to have any significance, nevertheless, complete pivoting (interchanging rows and columns) is necessary to guarantee that the factors L̂ and Û are well-defined and that their product accurately represents the original data.",1. Introduction,[0],[0]
"Previously, complete pivoting was impractical as a matrix factorization technique because it requires accessing the entire data matrix at every iteration, but SRLU efficiently achieves complete pivoting through randomization and includes a deterministic follow-up procedure to ensure a hight quality low-rank matrix approximation, as supported by rigorous theory and numeric experiments.
1The truncated SVD is known to provide the best low-rank matrix approximation, but it is rarely used for large scale practical data analysis.",1. Introduction,[0],[0]
See a brief discussion of the SVD in supplemental material.,1. Introduction,[0],[0]
"Algorithm 1 presents a basic implementation of the LU factorization, where the result is stored in place such that the upper triangular part of A becomes U and the strictly lower triangular part becomes the strictly lower part of L, with the diagonal of L implicitly known to contain all ones.",1.1. Background on the LU factorization,[0],[0]
LU with partial pivoting finds the largest entry in the ith column from row i tom and pivots the row with that entry to the ith row.,1.1. Background on the LU factorization,[0],[0]
"LU with complete pivoting finds the largest entry in the submatrix Ai+1:m,i+1:n and pivots that entry to Ai,i.",1.1. Background on the LU factorization,[0],[0]
"It is generally known and accepted that partial pivoting is sufficient for general, real-world data matrices in the context of linear equation solving.
",1.1. Background on the LU factorization,[0],[0]
Algorithm 1,1.1. Background on the LU factorization,[0],[0]
"The LU factorization
1: Inputs: Data matrix A ∈ Rm×n 2: for i = 1, 2, · · · ,min(m,n) do 3: Perform row and/or column pivots 4: for k = i+ 1, · · · ,m do 5:",1.1. Background on the LU factorization,[0],[0]
"Ak,i = Ak,i/Ai,i 6: end for 7: Ai+1:m,i+1:n −=",1.1. Background on the LU factorization,[0],[0]
Ai+1:,1.1. Background on the LU factorization,[0],[0]
"m,1:i ·A1:i,i+1:n 8: end for
Algorithm 2 Crout LU
1: Inputs: Data matrix A ∈ Rm×n, block size b 2: for j = 0, b, 2b, · · · ,min(m,n)/b− 1",1.1. Background on the LU factorization,[0],[0]
"do 3: Perform column pivots 4: Aj+1:m,j+1:j+b− = 5: Aj+1:m,1:j ·A1:j,j+1:j+b. 6: Apply Algorithm 1 on Aj+1:m,j+1:j+b 7:",1.1. Background on the LU factorization,[0],[0]
"Apply the row pivots to other columns of A 8: Aj+1:j+b,j+b+1:n −= 9: Aj+1:j+b,1:j ·A1:j,j+b+1:n
10: end for
Line 7 of Algorithm 1 is known as the Schur update.",1.1. Background on the LU factorization,[0],[0]
"Given a sparse input, this is the only step of the LU factorization that causes fill.",1.1. Background on the LU factorization,[0],[0]
"As the algorithm progresses, fill will compound and may become dense, but the LU factorization, and truncated LU in particular, generally preserves some, if not most, of the sparsity of a sparse input.",1.1. Background on the LU factorization,[0],[0]
"A numeric illustration is presented below.
",1.1. Background on the LU factorization,[0],[0]
There are many variations of the LU factorization.,1.1. Background on the LU factorization,[0],[0]
In Algorithm 2 the Crout version of LU is presented in block form.,1.1. Background on the LU factorization,[0],[0]
The column pivoting entails selecting the next b columns so that the in-place LU step is performed on a non-singular matrix (provided the remaining entries are not all zero).,1.1. Background on the LU factorization,[0],[0]
"Note that the matrix multiplication steps are the bottleneck of this algorithm, requiring O(mnb) operations each in general.
",1.1. Background on the LU factorization,[0],[0]
"The LU factorization has been studied extensively since long before the invention of computers, with notable results from many mathematicians, including Gauss, Turing, and Wilkinson.",1.1. Background on the LU factorization,[0],[0]
"Current research on LU factorizations includes communication-avoiding implementations, such as tournament pivoting (Khabou et al., 2013), sparse implementations (Grigori et al., 2007), and new computation of preconditioners (Chow & Patel, 2015).",1.1. Background on the LU factorization,[0],[0]
"A randomized approach to efficiently compute the LU factorization with complete pivoting recently appeared in (Melgaard & Gu, 2015).",1.1. Background on the LU factorization,[0],[0]
"These results are all in the context of linear equation solving, either directly or indirectly through an incomplete factorization used to precondition an iterative method.",1.1. Background on the LU factorization,[0],[0]
This work repurposes the LU factorization to create a novel efficient and effective low-rank approximation algorithm using modern randomization technology.,1.1. Background on the LU factorization,[0],[0]
"Previous work on low-rank data approximation includes the Interpolative Decomposition (ID) (Cheng et al., 2005), the truncated QR with column pivoting factorization (Gu & Eisenstat, 1996), and other deterministic column selection algorithms, such as in (Batson et al., 2012).
",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
Randomized algorithms have grown in popularity in recent years because of their ability to efficiently process large data matrices and because they can be supported with rigorous theory.,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
Randomized low-rank approximation algorithms generally fall into one of two categories: sampling algorithms and black box algorithms.,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
Sampling algorithms form data approximations from a random selection of rows and/or columns of the data.,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"Examples include (Deshpande et al., 2006; Deshpande & Vempala, 2006; Frieze et al., 2004; Mahoney & Drineas, 2009).",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"(Drineas et al., 2008) showed that for a given approximate rank k, a randomly drawn subset C of c =",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
O ( k log(k) −2,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"log (1/δ)
) columns of the data, a randomly drawn subset R of r =",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
O ( c log(c) −2,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"log (1/δ) ) rows of the data, and setting U = C†AR†, then the matrix approximation error ‖A−CUR‖F is at most a factor of 1+ from the optimal rank k approximation with probability at least 1− δ.",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"Black box algorithms typically approximate a data matrix in the form
A ≈ QTQA,
where Q is an orthonormal basis of the random projection (usually using SVD, QR, or ID).",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"The result of (Johnson & Lindenstrauss, 1984) provided the theoretical groundwork for these algorithms, which have been extensively studied (Clarkson & Woodruff, 2012; Halko et al., 2011; Martinsson et al., 2006; Papadimitriou et al., 2000; Sarlos, 2006; Woolfe et al., 2008; Liberty et al., 2007; Gu, 2015).",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"Note
that the projection of an m-by-n data matrix is of size mby-`, for some oversampling parameter ` ≥ k, and k is the target rank.",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"Thus the computational challenge is the orthogonalization of the projection (the random projection can be applied quickly, as described in these works).",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"A previous result on randomized LU factorizations for low-rank approximation was presented in (Aizenbud et al., 2016), but is uncompetitive in terms of theoretical results and computational performance with the work presented here.
",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"For both sampling and black box algorithms the tuning parameter cannot be arbitrarily small, as the methods become meaningless if the number of rows and columns sampled (in the case of sampling algorithms) or the size of the random projection (in the case of black box algorithms) surpasses the size of the data.",2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
A common practice is ≈ 12 .,2.1. Low-Rank Matrix Approximation (LRMA),[0],[0]
"Rank-revealing algorithms (Chan, 1987) are LRMA algorithms that guarantee the approximation is of high quality by also capturing the rank of the data within a tolerance (see supplementary materials for definitions).",2.2. Guaranteeing Quality,[0],[0]
"These methods, nevertheless, attempt to build an important submatrix of the data, and do not directly compute a low-rank approximation.",2.2. Guaranteeing Quality,[0],[0]
"Furthermore, they do not attempt to capture all positive singular values of the data.",2.2. Guaranteeing Quality,[0],[0]
"(Miranian & Gu, 2003) introduced a new type of high-quality LRMA algorithms that can capture all singular values of a data matrix within a tolerance, but requires extra computation to bound approximations of the left and right null spaces of the data matrix.",2.2. Guaranteeing Quality,[0],[0]
"Rank-revealing algorithms in general are designed around a definition that is not specifically appropriate for LRMA.
",2.2. Guaranteeing Quality,[0],[0]
"A key advancement of this work is a new definition of high quality low-rank approximation:
",2.2. Guaranteeing Quality,[0],[0]
"Definition 1 A rank-k truncated LU factorization is spectrum-revealing if∥∥∥A− L̂Û∥∥∥
2 ≤ q1(k,m, n)σk+1 (A)
and
σj
( L̂Û ) ≥ σj (A)
q2(k,m, n)
for 1 ≤ j ≤ k and q1(k,m, n) and q2(k,m, n) are bounded by a low degree polynomial in k, m, and n.
Definition 1 has precisely what we desire in an LRMA, and no additional requirements.",2.2. Guaranteeing Quality,[0],[0]
"The constants, q1(k,m, n) and q2(k,m, n) are at least 1 for any rank-k approximation by (Eckart & Young, 1936).",2.2. Guaranteeing Quality,[0],[0]
"This work shows theoretically and numerically that our algorithm, SRLU, is spectrumrevealing in that it always finds such q1 and q2, often with q1, q2 = O(1) in practice.
",2.2. Guaranteeing Quality,[0],[0]
"Algorithm 3 TRLUCP
1: Inputs: Data matrix A ∈ Rm×n, target rank k, block size b, oversampling parameter p ≥ b, random Gaussian matrix Ω ∈",2.2. Guaranteeing Quality,[0],[0]
"Rp×m, L̂ and Û are initially 0 matrices 2: Calculate random projection R = ΩA 3: for j = 0, b, 2b, · · · , k − b do 4: Perform column selection algorithm on R and swap columns of A 5: Update block column of L̂ 6:",2.2. Guaranteeing Quality,[0],[0]
Perform block LU with partial row pivoting and swap rows of A 7: Update block row of Û 8: Update R 9: end for,2.2. Guaranteeing Quality,[0],[0]
Low-rank and other approximation algorithms have appeared recently in a variety of machine learning applications.,2.3. Low-Rank and Other Approximations in Machine Learning,[0],[0]
"In (Krummenacher et al., 2016), randomized lowrank approximation is applied directly to the adaptive optimization algorithm ADAGRAD to incorporate variable dependence during optimization to approximate the full matrix version of ADAGRAD with a significantly reduced computational complexity.",2.3. Low-Rank and Other Approximations in Machine Learning,[0],[0]
"In (Kirkpatrick et al., 2017), a diagonal approximation of the posterior distribution of previous data is utilized to alleviate catastrophic forgetting.",2.3. Low-Rank and Other Approximations in Machine Learning,[0],[0]
"LU (SRLU)
",3. Main Contribution: Spectrum-Revealing,[0],[0]
Our algorithm for computing SRLU is composed of two subroutines: partially factoring the data matrix with randomized complete pivoting (TRLUCP) and performing swaps to improve the quality of the approximation (SRP).,3. Main Contribution: Spectrum-Revealing,[0],[0]
"The first provides an efficient algorithm for computing a truncated LU factorization, whereas the second ensures the resulting approximation is provably reliable.",3. Main Contribution: Spectrum-Revealing,[0],[0]
"Intuitively, TRLUCP performs deterministic LU with partial row pivoting for some initial data with permuted columns.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
TRLUCP uses a random projection of the Schur complement to cheaply find and move forward columns that are more likely to be representative of the data.,3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"To accomplish this, Algorithm 3 performs an iteration of block LU factorization in a careful order that resembles Crout LU reduction.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"The ordering is reasoned as follows: LU with partial row pivoting cannot be performed until the needed
columns are selected, and so column selection must first occur at each iteration.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Once a block column is selected, a partial Schur update must be performed on that block column before proceeding.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"At this point, an iteration of block LU with partial row pivoting can be performed on the current block.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Once the row pivoting is performed, a partial Schur update of the block of pivoted rows of U can be performed, which completes the factorization up to rank j+ b. Finally, the projection matrix R can be cheaply updated to prepare for the next iteration.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Note that any column selection method may be used when picking column pivots from R, such as QR with column pivoting, LU with row pivoting, or even this algorithm can again be run on the subproblem of column selection of R. The flop count of TRLUCP is dominated by the three matrix multiplication steps (lines 2, 5, and 7).",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"The total number of flops is
F TRLUCP = 2pmn+ (m+ n)k2 +O (k(m+ n)) .
",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Note the transparent constants, and, because matrix multiplication is the bottleneck, this algorithm can be implemented efficiently in terms of both computation as well as memory usage.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Because the output of TRLUCP is only written once, the total number of memory writes is (m + n",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
− k)k.,3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
Minimizing the number of data writes by only writing data once significantly improves efficiency because writing data is typically one of the slowest computational operations.,3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"Also worth consideration is the simplicity of the LU decomposition, which only involves three types of operations: matrix multiply, scaling, and pivoting.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"By contrast, state-of-the-art calculation of both the full and truncated SVD requires a more complex process of bidiagonalization.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
The projection R can be updated efficiently to become a random projection of the Schur complement for the next iteration.,3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"This calculation involves the current progress of the LU factorization and the random matrix Ω, and is described in detail in the appendix.",3.1. Truncated Randomized LU with Complete Pivoting (TRLUCP),[0],[0]
"TRLUCP produces high-quality data approximations for almost all data matrices, despite the lack of theoretical guarantees, but can miss important rows or columns of the data.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Next, we develop an efficient variant of the existing rank-revealing LU algorithms (Gu & Eisenstat, 1996; Miranian & Gu, 2003) to rapidly detect and, if necessary, correct any possible matrix approximation failures of TRLUCP.
",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Intuitively, the quality of the factorization can be tested by searching for the next choice of pivot in the Schur complement if the factorization continued and determining if the addition of that element would significantly improve the approximation quality.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"If so, then the row and column with this element should be included in the approximation and another row and column should be excluded to maintain rank.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Because TRLUCP does not provide an updated
Schur complement, the largest element in the Schur complement can be approximated by finding the column of R with largest norm, performing a Schur update of that column, and then picking the largest element in that column.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Let α be this element, and, without loss of generality, assume it is the first entry of the Schur complement.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Denote:
Π1AΠ T 2 = L11`T 1",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
L31 I U11 u U13α sT12 s21 S22  .,3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"(1) Next, we must find the row and column that should be replaced if the row and column containing α are important.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Note that the smallest entry of L11U11 may still lie in an important row and column, and so the largest element of the inverse should be examined instead.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Thus we propose defining
A11 def = ( L11 `T 1 )( U11 u α ) and testing
‖A−111 ‖max ≤",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"f
|α| (2)
for a tolerance parameter f > 1 that provides a control of accuracy versus the number of swaps needed.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Should the test fail, the row and column containing α are swapped with the row and column containing the largest element in A −1 11 .",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Note that this element may occur in the last row or last column of A −1 11 , indicating only a column swap or row swap respectively is needed.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"When the swaps are performed, the factorization must be updated to maintain truncated LU form.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"We have developed a variant of the LU updating algorithm of (Gondzio, 2007) to efficiently update the SRLU factorization.
",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"SRP can be implemented efficiently: each swap requires at most O (k(m+ n)) operations, and ‖A−111 ‖max can be quickly and reliably estimated using (Higham & Relton, 2015).",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"An argument similar to that used in (Miranian & Gu, 2003) shows that each swap will increase
∣∣det (A11)∣∣ by a factor at least f , hence will never repeat.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"At termination, SRP will ensure a partial LU factorization of the form (1) that satisfies condition (2).",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"We will discuss spectrumrevealing properties of this factorization in Section 4.2.
",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"It is possible to derive theoretical upper bounds on the worst number of swaps necessary in SRP, but in practice, this number is zero for most matrices, and does not exceed 3− 5 in the most pathological data matrix of dimension at most 1000 we can contrive.
",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
SRLU can be used effectively to approximate second order information in machine learning.,3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"SRLU can be used as a modification to ADAGRAD in a manner similar to the
Algorithm 4 Spectrum-Revealing Pivoting (SRP)
1: Input: Truncated LU factorization A ≈ L̂Û, tolerance f > 1 2: while ‖A−111 ‖max > f |α| do 3: Set α to be the largest element in S (or find an approximate α using R) 4: Swap row and column containing α with row and column of largest element in A −1 11 5:",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Update truncated LU factorization 6: end while
low-rank approximation method in (Krummenacher et al., 2016).",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Applying the initialization technique in this work, SRLU would likely provide an efficient and accurate adaptive stochastic optimization algorithm.",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
SRLU can also become a full-rank approximation (low-rank plus diagonal) by adding a diagonal approximation of the Schur complement.,3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"Such an approximation could be appropriate for improving memory in artificial intelligence, such as in (Kirkpatrick et al., 2017).",3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
SRLU is also a freestanding compression algorithm.,3.2. Spectrum-Revealing Pivoting (SRP),[0],[0]
"A natural extension of truncated LU factorizations is a CUR-type decomposition for increased accuracy (Mahoney & Drineas, 2009):
Π1AΠ T 2 ≈ L̂
( L̂†AÛ† ) Û def = L̂MÛ.
As with standard CUR, the factors L̂ and Û retain (much of) the sparsity of the original data, while M is a small, k-by-k matrix.",3.3. The CUR Decomposition with LU,[0],[0]
The CUR decomposition can improve the accuracy of an SRLU with minimal extra needed memory.,3.3. The CUR Decomposition with LU,[0],[0]
"Extra computational time, nevertheless, is needed to calculate M. A more efficient, approximate CUR decomposition can be obtained by replacing A with a high quality approximation (such as an SRLU factorization of high rank) in the calculation of M.",3.3. The CUR Decomposition with LU,[0],[0]
"Given a factored data matrix A ∈ Rm×n and new observations BΠT2 = ( k m − k B1 B2 ) ∈ Rs×m, an augmented LU decomposition takes the form( Π1AΠ T 2
BΠT2
) =",3.4. The Online SRLU Factorization,[0],[0]
"L11L21 I L31 I U11 U12S Snew  , where L31 = B1U−111 and S
new = B2 −B1U−111 U12.",3.4. The Online SRLU Factorization,[0],[0]
An SRLU factorization can then be obtained by simply performing correcting swaps.,3.4. The Online SRLU Factorization,[0],[0]
"For a rank-1 update, at most 1
swap is expected (although examples can be constructed that require more than one swap), which requires at most O (k (m+ n)) flops.",3.4. The Online SRLU Factorization,[0],[0]
"By contrast, the URV decomposition of (Stewart, 1992) is O ( n2 ) , while SVD updating
requires O ( (m+ n) min2 (m,n) )",3.4. The Online SRLU Factorization,[0],[0]
"operations in general,
or O ( (m+ n) min (m,n) log22 ) for a numerical approximation with the fast multipole method.",3.4. The Online SRLU Factorization,[0],[0]
Theorem 1 Let (·)s denote the rank-s truncated SVD for s ≤,4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"k m,n.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Then for any truncated LU factorization with Schur complement S:
‖Π1AΠT2 − L̂Û‖ = ‖S‖
for any norm, and ‖Π1AΠT2 − ( L̂Û ) s ‖2 ≤ 2‖S‖2",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"+ σs+1 (A) .
",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
Theorem 2,4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"For a general rank-k truncated LU decomposition, we have for all 1 ≤ j ≤ k,
σj (A) ≤ σj ( L̂Û )1 + 1 + ‖S‖2 σk ( L̂Û )  ‖S‖2",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
σj (A)  .,4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Theorem 3 CUR Error Bounds.
",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"‖Π1AΠT2 − L̂MÛ‖2 ≤ 2‖S‖2
and
‖Π1AΠT2 − L̂MÛ‖F ≤ ‖S‖F .
",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Theorem 1 simply concludes that the approximation is accurate if the Schur complement is small, but the singular value bounds of Theorem 2 are needed to guarantee that the approximation retains structural properties of the original data, such as an accurate approximation of the rank and the spectrum.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Furthermore, singular values bounds can be significantly stronger than the more familiar norm error bounds that appear in Theorem 1.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Theorem 2 provides a general framework for singular value bounds, and bounding the terms in this theorem provided guidance in the design and development of SRLU.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Just as in the case of deterministic LU with complete pivoting, the sizes of ‖S‖2
σk(L̂Û)
and ‖S‖2 σj(L̂Û) range from moderate to small for almost all data matrices of practical interest.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"They, nevertheless, cannot be effectively bounded for a general TRLUCP factorization, implying the need for Algorithm 4 to ensure that
these terms are controlled.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"While the error bounds in Theorem 3 for the CUR decomposition do not improve upon the result in Theorem 1, CUR bounds for SRLU specifically will be considerably stronger.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
"Next, we present our main theoretical contributions.",4.1. Analysis of General Truncated LU Decompositions,[0],[0]
Theorem 4 (SRLU Error Bounds.),4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"For j ≤ k and γ = O (fk √ mn), SRP produces a rank-k SRLU factorization with
‖Π1AΠT2 − L̂Û‖2 ≤",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
γσk+1,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"(A) , ‖Π1AΠT2 − ( L̂Û )",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
j ‖2 ≤ σj+1 (A) ( 1 + 2γ σk+1(A)σj+1(A) ),4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
Theorem 4 is a special case of Theorem 1 for SRLU factorizations.,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"For a data matrix with a rapidly decaying spectrum, the right-hand side of the second inequality is close to σj+1 (A), a substantial improvement over the sharpness of the bounds in (Drineas et al., 2008).
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
Theorem 5 (SRLU Spectral Bound).,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"For 1 ≤ j ≤ k, SRP produces a rank-k SRLU factorization with
σj (A)
1 + τ",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"σk+1(A)σj(A)
≤ σj ( L̂Û ) ≤",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
σj (A) ( 1 + τ σk+1,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"(A)
σj (A) ) for τ ≤",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"O ( mnk2f3 ) .
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"While the worst case upper bound on τ is large, it is dimension-dependent, and j and k may be chosen so that σk+1(A) σj(A)
is arbitrarily small compared to τ .",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"In particular, if k is the numeric rank of A, then the singular values of the approximation are numerically equal to those of the data.
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"These bounds are problem-specific bounds because their quality depends on the spectrum of the original data, rather than universal constants that appear in previous results.",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
The benefit of these problem-specific bounds is that an approximation of data with a rapidly decaying spectrum is guaranteed to be high-quality.,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Furthermore, if σk+1 (A) is not small compared to σj (A), then no high-quality low-rank approximation is possible in the 2 and Frobenius norms.",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Thus, in this sense, the bounds presented in Theorems 4 and 5 are optimal.
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Given a high-quality rank-k truncated LU factorization, Theorem 5 ensures that a low-rank approximation of rank ` with ` < k of the compressed data is an accurate rank-` approximation of the full data.",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
The proof of this theorem centers on bounding the terms in Theorems 1 and 2.,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Experiments will show that τ is small in almost all cases.
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Stronger results are achieved with the CUR version of SRLU:
Theorem 6
‖Π1AΠT2 − L̂MÛ‖2 ≤",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"2γσk+1 (A)
and
‖Π1AΠT2 − L̂MÛ‖F ≤ ωσk+1 (A) ,
where γ = O",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"(fk √ mn) is the same as in Theorem 4, and ω = O (fkmn).
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
Theorem 7,4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
If σ2j (A) > 2‖S‖22 then σj (A) ≥ σj ( L̂MÛ ),4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"≥ σj (A) √ 1− 2γ ( σk+1 (A)
σj (A) )2 for γ = O ( mnk2f2 ) and f is an input parameter controlling a tradeoff of quality vs. speed as before.
",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"As before, the constants are small in practice.",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"Observe that for most real data matrices, their singular values decay with increasing j. For such matrices this result is significantly stronger than Theorem 5.",4.2. Analysis of the Spectrum-Revealing LU Decomposition,[0],[0]
"In Figure 1, the accuracy of our method is compared to the accuracy of the truncated SVD.",5.1. Speed and Accuracy Tests,[0],[0]
Note that SRLU did not perform any swaps in these experiments.,5.1. Speed and Accuracy Tests,[0],[0]
“CUR” is the CUR version of the output of SRLU.,5.1. Speed and Accuracy Tests,[0],[0]
"Note that both methods exhibits a convergence rate similar to that of the truncated SVD (TSVD), and so only a constant amount of extra work is needed to achieve the same accuracy.",5.1. Speed and Accuracy Tests,[0],[0]
"When the singular values decay slowly, the CUR decomposition provides a greater accuracy boost.",5.1. Speed and Accuracy Tests,[0],[0]
"In Figure 2, the runtime of SRLU is compared to that of the truncated SVD, as well as Subspace Iteration",5.1. Speed and Accuracy Tests,[0],[0]
"(Gu, 2015).",5.1. Speed and Accuracy Tests,[0],[0]
"Note that for Subspace Iteration, we choose iteration parameter q = 0 and do not measure the time of applying the random projection, in acknowledgement that fast methods exist to apply a random projection to a data matrix.",5.1. Speed and Accuracy Tests,[0],[0]
"Also, the block size implemented in SRLU is significantly smaller than the block size used by the standard software LAPACK, as the size of the block size affects the size of the projection.",5.1. Speed and Accuracy Tests,[0],[0]
See supplement for additional details.,5.1. Speed and Accuracy Tests,[0],[0]
All numeric experiments were run on NERSC’s Edison.,5.1. Speed and Accuracy Tests,[0],[0]
"For timing experiments, the truncated SVD is calculated with PROPACK.
",5.1. Speed and Accuracy Tests,[0],[0]
"Even more impressive, the factorization stage of SRLU becomes arbitrarily faster than the standard implementation of the LU decomposition.",5.1. Speed and Accuracy Tests,[0],[0]
"Although the standard LU decomposition is not a low-rank approximation algorithm, it is known to be roughly 10 times faster than the SVD (Demmel, 1997).",5.1. Speed and Accuracy Tests,[0],[0]
"See appendix for details.
",5.1. Speed and Accuracy Tests,[0],[0]
"Next, we compare SRLU against competing algorithms.",5.1. Speed and Accuracy Tests,[0],[0]
"In (Ubaru et al., 2015), error-correcting codes are introduced to yield improved accuracy over existing random projection low-rank approximation algorithms.",5.1. Speed and Accuracy Tests,[0],[0]
"Their algorithm, denoted Dual BCH, is compared against SRLU as well as two other random projection methods: Gaus., which uses a Gaussian random projection, and SRFT, which uses a Fourier transform to apply a random projection.",5.1. Speed and Accuracy Tests,[0],[0]
"We test the spectral norm error of these algorithms on matrices from the sparse matrix collection in (Davis & Hu, 2011).
",5.1. Speed and Accuracy Tests,[0],[0]
"In Table 1, results for SRLU are averaged over 5 experiments.",5.1. Speed and Accuracy Tests,[0],[0]
"Using tuning parameter f = 5, no swaps were needed in all cases.",5.1. Speed and Accuracy Tests,[0],[0]
The matrices being tested are sparse matrices from various engineering problems.,5.1. Speed and Accuracy Tests,[0],[0]
"S80PIn1 is 4,028 by 4,028, deter3 is 7,647 by 21,777, and lp ceria3d (abbreviated lc3d) is 3,576 by 4,400.",5.1. Speed and Accuracy Tests,[0],[0]
"Note
that SRLU, a more efficient algorithm, provides a better approximation in two of the three experiments.",5.1. Speed and Accuracy Tests,[0],[0]
"With a little extra oversampling, a practical assumption due to the speed advantage, SRLU achieves a competitive quality approximation.",5.1. Speed and Accuracy Tests,[0],[0]
"The oversampling highlights an additional and unique advantage of SRLU over competing algorithms: if more accuracy is desired, then the factorization can simply continue as needed.",5.1. Speed and Accuracy Tests,[0],[0]
"The SRLU factorization is tested on sparse, unsymmetric matrices from (Davis & Hu, 2011).",5.2. Sparsity Preservation Tests,[0],[0]
"Figure 3 shows the sparsity patterns of the factors of an SRLU factorization of a sparse data matrix representing a circuit simulation (oscil dcop), as well as a full LU decomposition of the data.",5.2. Sparsity Preservation Tests,[0],[0]
"Note that the LU decomposition preserves the sparsity of the data initially, but the full LU decomposition becomes dense.",5.2. Sparsity Preservation Tests,[0],[0]
Several more experiments are shown in the supplement.,5.2. Sparsity Preservation Tests,[0],[0]
An image processing example is now presented to illustrate the benefit of highlighting important rows and columns selection.,5.3. Towards Feature Selection,[0],[0]
In Figure 4 an image is compressed to a rank50 approximation using SRLU.,5.3. Towards Feature Selection,[0],[0]
"Note that the rows and columns chosen overlap with the astronaut and the planet, implying that minimal storage is needed to capture the
black background, which composes approximately two thirds of the image.",5.3. Towards Feature Selection,[0],[0]
"While this result cannot be called feature selection per se, the rows and columns selected highlight where to look for features: rows and/or columns are selected in a higher density around the astronaut, the curvature of the planet, and the storm front on the planet.",5.3. Towards Feature Selection,[0],[0]
"Online SRLU is tested here on the Enron email corpus (Lichman, 2013).",5.4. Online Data Processing,[0],[0]
"The documents were initially reversesorted by the usage of the most common word, and then reverse-sorted by the second most, and this process was repeated for the five most common words (the top five words were used significantly more than any other), so that the most common words occurred most at the end of the corpus.",5.4. Online Data Processing,[0],[0]
"The data contains 39,861 documents and 28,102
words/terms, and an initial SRLU factorization of rank 20 was performed on the first 30K documents.",5.4. Online Data Processing,[0],[0]
"The initial factorization contained none of the top five words, but, after adding the remaining documents and updating, the top three were included in the approximation.",5.4. Online Data Processing,[0],[0]
"The fourth and fifth words ‘market’ and ‘california’ have high covariance with at least two of the three top words, and so their inclusion may be redundant in a low-rank approximation.",5.4. Online Data Processing,[0],[0]
"We have presented SRLU, a low-rank approximation method with many desirable properties: efficiency, accuracy, sparsity-preservation, the ability to be updated, and the ability to highlight important data features and variables.",6. Conclusion,[0],[0]
Extensive theory and numeric experiments have illustrated the efficiency and effectiveness of this method.,6. Conclusion,[0],[0]
This research was supported in part by NSF Award CCF1319312.,Acknowledgements,[0],[0]
"Low-rank matrix approximation is a fundamental tool in data analysis for processing large datasets, reducing noise, and finding important signals.",abstractText,[0],[0]
"In this work, we present a novel truncated LU factorization called Spectrum-Revealing LU (SRLU) for effective low-rank matrix approximation, and develop a fast algorithm to compute an SRLU factorization.",abstractText,[0],[0]
We provide both matrix and singular value approximation error bounds for the SRLU approximation computed by our algorithm.,abstractText,[0],[0]
"Our analysis suggests that SRLU is competitive with the best low-rank matrix approximation methods, deterministic or randomized, in both computational complexity and approximation quality.",abstractText,[0],[0]
"Numeric experiments illustrate that SRLU preserves sparsity, highlights important data features and variables, can be efficiently updated, and calculates data approximations nearly as accurately as possible.",abstractText,[0],[0]
To the best of our knowledge this is the first practical variant of the LU factorization for effective and efficient low-rank matrix approximation.,abstractText,[0],[0]
"An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation",title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 369–378, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011).",1 Introduction,[0],[0]
"Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text.
",1 Introduction,[0],[0]
"More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014).",1 Introduction,[0],[0]
"Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality.",1 Introduction,[0],[0]
"However, Syntactic
models require annotated syntactic structures for training, which are expensive to obtain manually.",1 Introduction,[0],[0]
"In addition, they can be slower compared to Ngram models.
",1 Introduction,[0],[0]
"In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence.",1 Introduction,[0],[0]
"The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014).
",1 Introduction,[0],[0]
We choose the model of Liu et al.(2015) as the syntactic language model.,1 Introduction,[0],[0]
"There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012).",1 Introduction,[0],[0]
"As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks.",1 Introduction,[0],[0]
"The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015).",1 Introduction,[0],[0]
"We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering.
",1 Introduction,[0],[0]
We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm.,1 Introduction,[0],[0]
• What is the influence of automaticallyparsed training data on the performance of syntactic models.,1 Introduction,[0],[0]
"Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models.",1 Introduction,[0],[0]
"As a result, the syntactic structures that a word ordering system learns can be inaccurate.",1 Introduction,[0],[0]
"However, this might not affect
369
the quality of the synthesized output, which is a string only.",1 Introduction,[0],[0]
We quantitatively study the influence of parsing accuracy of syntactic training data on word ordering output.,1 Introduction,[0],[0]
• What is the influence of data scale on the performance.,1 Introduction,[0],[0]
N-gram language models can be trained efficiently over large numbers of raw sentences.,1 Introduction,[0],[0]
"In contrast, syntactic language models can be much slower to train due to rich features.",1 Introduction,[0],[0]
"We compare the output quality of the two models on different scales of training data, and also on different amounts of training time.",1 Introduction,[0],[0]
•What are the errors characteristics of each model.,1 Introduction,[0],[0]
Syntactic language models can potentially be better in capturing larger constituents and overall sentence structures.,1 Introduction,[0],[0]
"However, compared with N-gram models, little work has been done to quantify the difference between the two models.",1 Introduction,[0],[0]
"We characterise the outputs using a set of different measures, and show empirically the relative strength and weakness of each model.",1 Introduction,[0],[0]
• What is the effect of model combination.,1 Introduction,[0],[0]
"Finally, because the two models make different types of errors, they can be combined to give better outputs.",1 Introduction,[0],[0]
"We develop a combined model by discretizing probability from N-gram model, and using them as features in the syntactic model.",1 Introduction,[0],[0]
The combined model gives the best results in a standard benchmark.,1 Introduction,[0],[0]
Syntactic word ordering algorithms take a multiset of input words constructing an output sentence and its syntactic derivation simultaneously.,2.1 Syntactic word ordering,[0],[0]
"Transition-based syntactic word ordering can be modelled as an extension to transition-based parsing (Liu et al., 2015), with the main difference be-
ing that the order of words is not given in the input, which leads to a much larger search space.
",2.1 Syntactic word ordering,[0],[0]
"We take the system of Liu, et al.1, which gives state-of-the-art performance and efficiencies in standard word ordering benchmark.",2.1 Syntactic word ordering,[0],[0]
"It maintains outputs in stack σ, and orders the unprocessed incoming words in a set ρ.",2.1 Syntactic word ordering,[0],[0]
"Given an input bag of words, ρ is initialized to the input and σ is initialized as empty.",2.1 Syntactic word ordering,[0],[0]
"The system repeatedly applies transition actions to consume words from ρ and construct output on σ.
",2.1 Syntactic word ordering,[0],[0]
"Figure 1 shows the deduction system, where ρ is unordered and any word in ρ can be shifted onto the stack σ.",2.1 Syntactic word ordering,[0],[0]
"The set of actions are SHIFT, L-ARC and R-ARC.",2.1 Syntactic word ordering,[0],[0]
The SHIFT actions add a word to the stack.,2.1 Syntactic word ordering,[0],[0]
"For the L-ARC and R-ARC actions, new arcs {j ← i} and {j → i} are constructed respectively.",2.1 Syntactic word ordering,[0],[0]
"Under these possible actions, the unordered word set “potatoes0 Tom1 likes2” is generated as shown in Figure 2, and the result is “Tom1 ←likes2→potatoes0”.
",2.1 Syntactic word ordering,[0],[0]
We apply the learning and search framework of Zhang and Clark (2011a).,2.1 Syntactic word ordering,[0],[0]
Pseudocode of the search algorithm is shown in Algorithm 1.,2.1 Syntactic word ordering,[0],[0]
"[] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words.",2.1 Syntactic word ordering,[0],[0]
"candidates stores possible states, and agenda stores temporary states transited from possible actions.",2.1 Syntactic word ordering,[0],[0]
GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N-BEST produces the top k candidates in agenda.,2.1 Syntactic word ordering,[0],[0]
"Finally, the algorithm returns the highest-score state best in the agenda.
",2.1 Syntactic word ordering,[0],[0]
A global linear model is used to score search hypotheses.,2.1 Syntactic word ordering,[0],[0]
"Given a hypothesis h, its score is calculated by:
Score(h) = Φ(h) · ~θ, 1http://sourceforge.net/projects/zgen/
Algorithm 1 Transition-based linearisation Input: W, a set of input word Output: the highest-scored final state 1: candidates← ([], set(1..n),Ø) 2: agenda← Ø 3: N ← 2n 4: for i← 1..N do 5: for s in candidates do 6: for action in GETACTIONS(s) do 7: agenda← APPLY(s, action) 8: end for 9: end for 10: candidates← N-BEST(agenda) 11:",2.1 Syntactic word ordering,[0],[0]
"agenda← Ø 12: end for 13: best← BEST(candidates) 14: return best
where Φ(h) is the feature vector of h, extracted by using the same feature templates as Liu et al.(2015), which are shown in Table 1 and ~θ is the parameter vector of the model.",2.1 Syntactic word ordering,[0],[0]
The feature templates essentially represents a syntactic language model.,2.1 Syntactic word ordering,[0],[0]
"As shown in Figure 2, from the hypotheses produced in steps 2 and 4, the features “Tom1 ← likes2” and “likes2 → potatoes0” are extracted, which corresponds to P (Tom1|likes2) and P (potatoes0|likes2) respectively in the dependency language model of Chen et al.,(2012).",2.1 Syntactic word ordering,[0],[0]
Training.,2.1 Syntactic word ordering,[0],[0]
"We apply perceptron with early-update (Collins and Roark, 2004), and iteratively tune related parameters on a set of development data.",2.1 Syntactic word ordering,[0],[0]
"For each iteration, we measure the performance on the development data, and choose best parameters for final tests.
2.2 N-gram word ordering
We build an N-gram word ordering system under the same beam-search framework as the syntactic word ordering system.",2.1 Syntactic word ordering,[0],[0]
"In particular, search is performed incrementally, from left to right, adding one word at each step.",2.1 Syntactic word ordering,[0],[0]
"The decoding process can be regarded as a simplified version of Algorithm 1, with only SHIFT being returned by GETACTIONS, and the score of each transition is given by a standard N-gram language model.",2.1 Syntactic word ordering,[0],[0]
We use the same beam size for both N-gram and the syntactic word ordering.,2.1 Syntactic word ordering,[0],[0]
"Compared with the syntactic model, the N-gram model has less information for disambiguation, but also has less structural ambiguities, and therefore a smaller search space.
",2.1 Syntactic word ordering,[0],[0]
Training.,2.1 Syntactic word ordering,[0],[0]
We train N-gram language models from raw text using modified Kneser-Ney smoothing without pruning.,2.1 Syntactic word ordering,[0],[0]
"The text is true-case tokenized, and we train 4-gram language modes using KenLM2, which gives high efficiencies in standard N-gram language model construction.",2.1 Syntactic word ordering,[0],[0]
"For training data, we use the Wall Street Journal (WSJ) sections 1-22 of the Penn Treebank (Mar-
2https://kheafield.com/code/kenlm/
cus et al., 1993), and the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English Giga Word Fifth Edition (Parker et al., 2011).",3.1 Data,[0],[0]
"As the development data, we use WSJ section 0 for parameter tuning.",3.1 Data,[0],[0]
"For testing, we use data from various domain, which consist of WSJ section 23, Washington Post/Bloomberg(WPB) subsets of the English Giga Word Fifth Edition and SANCL blog data, as shown in Table 2.",3.1 Data,[0],[0]
Example sentence in various test domains are shown in Table 3.,3.1 Data,[0],[0]
"We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation.",3.2 Evaluation metrics,[0],[0]
"Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models.",3.2 Evaluation metrics,[0],[0]
"We additionally use METEOR3(Denkowski and Lavie, 2010) to evaluate the system performances.",3.2 Evaluation metrics,[0],[0]
The BLEU metric measures the fluency of generated sentence without considering long range ordering.,3.2 Evaluation metrics,[0],[0]
The METEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion.,3.2 Evaluation metrics,[0],[0]
"The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is
(1)",3.2 Evaluation metrics,[0],[0]
"[The document is necessary for developer ,]0",3.2 Evaluation metrics,[0],[0]
[so you can not follow this document to get right options .]1,3.2 Evaluation metrics,[0],[0]
"and the generated output sentence is
(2)",3.2 Evaluation metrics,[0],[0]
[so you can not follow this document to get right options .]1,3.2 Evaluation metrics,[0],[0]
"[The document is necessary for developer ,]0 .",3.2 Evaluation metrics,[0],[0]
There is a big distortion in the output.,3.2 Evaluation metrics,[0],[0]
"The BLEU metric gives a score of 90.09 out of 100, while
3http://www.cs.cmu.edu/∼alavie/METEOR/
the METEOR gives a score of 61.34 out of 100.",3.2 Evaluation metrics,[0],[0]
This is because that METEOR is based on explicit word-to-word matches over the whole sentence.,3.2 Evaluation metrics,[0],[0]
"For word ordering, word-to-word matches are unique, which facilitates METEOR evaluation between generated sentences and references.",3.2 Evaluation metrics,[0],[0]
"As can bee seen from the example, long range distortion can highly influence the METEOR scores making the METEOR metric more suitable for evaluating word ordering distortions.",3.2 Evaluation metrics,[0],[0]
"For all the experiments, we assume that the input is a bag of words without order, and the output is a fully ordered sentence.",3.3 Data preparation,[0],[0]
"Following previous work (Wan et al., 2009; Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as ‘Pierre Vinken’ and ‘a big cat’) as a single word.",3.3 Data preparation,[0],[0]
"This avoids unnecessary ambiguities in combination between their subcomponents.
",3.3 Data preparation,[0],[0]
The syntactic model requires that the training sentences have syntactic dependency structure.,3.3 Data preparation,[0],[0]
"However, only the WSJ data contains goldstandard annotations.",3.3 Data preparation,[0],[0]
"In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data.",3.3 Data preparation,[0],[0]
"The results are turned into dependency trees using Penn2Malt4, after base noun phrases are extracted.",3.3 Data preparation,[0],[0]
"In our experiments, we use ZPar5 (Zhu et al., 2013) for automatic constituent parsing.
",3.3 Data preparation,[0],[0]
"In order to study the influence of parsing accuracy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies.",3.3 Data preparation,[0],[0]
"The data is randomly split into ten equal-size subsets, and each subset is automatically parsed with a parser trained on the other
4http://stp.lingfil.uu.se/∼nivre/research/Penn2Malt.html 5http://people.sutd.edu.sg/∼yue zhang/doc/doc/con-
parser.html
nine subset.",3.3 Data preparation,[0],[0]
"In order to obtain datasets with different parsing accuracies, we randomly sample a small number of sentences from each training subset, as shown in Table 4.",3.3 Data preparation,[0],[0]
The dependency trees of each set are derived from these bracketed sentences using Penn2Malt after base noun phrase are extracted as a single word.,3.3 Data preparation,[0],[0]
We train the syntactic models on the WSJ training parsing data with different accuracies.,4.1 In-domain word ordering,[0],[0]
"The WSJ development data are used to find out the optimal number of training iterations for each experiments, and the WSJ test results are shown in Table 5.
",4.1 In-domain word ordering,[0],[0]
Table 5 shows that the parsing accuracy can affect the performance of the syntactic model.,4.1 In-domain word ordering,[0],[0]
A higher parsing accuracy can lead to a better syntactic language model.,4.1 In-domain word ordering,[0],[0]
It conforms to the intuition that syntactic quality affects the fluency of surface texts.,4.1 In-domain word ordering,[0],[0]
"On the other hand, the influence is not huge, the BLEU scores decrease by 1.0 points as the parsing accuracy decreases from 88.10% to 57.31%",4.1 In-domain word ordering,[0],[0]
"The influence of parsing accuracy of the training data on cross-domain word ordering is measured by using the same training settings, but testing on the WPB and SANCL test sets.",4.2 Cross-domain word ordering,[0],[0]
Table 5 shows that the performance on cross-domain word ordering cannot reach that of in-domain word ordering using the syntactic models.,4.2 Cross-domain word ordering,[0],[0]
"Compared with the cross-domain experiments, the influence of parsing accuracy becomes smaller.",4.2 Cross-domain word ordering,[0],[0]
"In the WPB test, the fluctuation of performance decline to about 0.9 BLEU points, and in the SANCL test, the fluctuation is about 1.1 BLEU points.
",4.2 Cross-domain word ordering,[0],[0]
"In conclusion, the experiments show that pars-
ing accuracies have a relatively small influence on the syntactic models.",4.2 Cross-domain word ordering,[0],[0]
This suggests that it is possible to use large automatically-parsed data to train syntactic models.,4.2 Cross-domain word ordering,[0],[0]
"On the other hand, when the training data scale increases, syntactic models can become much slower to train compared with Ngram models.",4.2 Cross-domain word ordering,[0],[0]
"The influence on data scale, which includes output quality and training time, is further studied in the next section.",4.2 Cross-domain word ordering,[0],[0]
We use the AFP news data as the training data for the experiments of this section.,5 Influence of data scale,[0],[0]
"The syntactic models are trained using automatically-parsed trees derived from ZPar, as described in Section 3.3.",5 Influence of data scale,[0],[0]
"The WPB test data is used to measure indomain performance, and the SANCL blog data is used to measure cross-domain performance.",5 Influence of data scale,[0],[0]
"The Figure 3 and 4 shows that using both the BLEU and the METEOR metrics, the performance of the syntactic model is better than that of the N-gram models.",5.1 Influence on BLEU and METEOR,[0],[0]
It suggests that sentences generated by the syntactic model have both better fluency and better ordering.,5.1 Influence on BLEU and METEOR,[0],[0]
"The performance of the syntactic models is not highly weakened in cross-domain tests.
",5.1 Influence on BLEU and METEOR,[0],[0]
"The grey dot in each figure shows the performance of the syntactic model trained on the gold WSJ training data, and evaluated on the same WPB and SANCL test data sets.",5.1 Influence on BLEU and METEOR,[0],[0]
A comparison between the grey dots and the dashed lines shows that the syntactic model trained on the WSJ data perform better than the syntactic model trained on similar amounts of AFP data.,5.1 Influence on BLEU and METEOR,[0],[0]
"This again shows the effect of syntactic quality of the training data.
",5.1 Influence on BLEU and METEOR,[0],[0]
"On the other hand, as the scale of automaticallyparsed AFP data increases, the performance of the
syntactic model rapidly increases, surpassing the syntactic model trained on the high-quality WSJ data.",5.1 Influence on BLEU and METEOR,[0],[0]
"This observation is important, showing that large-scale data can be used to alleviate the problem of lower syntactic quality in automaticallyparsed data, which can be leveraged to address the scarcity issue of manually annotated data in both in-domain and cross-domain settings.",5.1 Influence on BLEU and METEOR,[0],[0]
The training time of both syntactic models and N-gram models increases as the size of training data increases.,5.2 Influence on training time,[0],[0]
Figure 5 shows the BLEU of the two systems under different amounts of training time.,5.2 Influence on training time,[0],[0]
"There is no result reported for the syntactic model beyond 1 million training sentences, because training becomes infeasibly slow 6.",5.2 Influence on training time,[0],[0]
"On the
6Our experiments are carried on a single thread of 3.60GHz CPU.",5.2 Influence on training time,[0],[0]
"If the training time is over 90 hours for a model, we consider it infeasible.
other hand, the N-gram model can be trained using all the WSJ, AFP, XIN training sentences, which are 53 millions, within 103.2 seconds.",5.2 Influence on training time,[0],[0]
"As a result, there is no overlap between the syntactic model and the N-gram model curves.
",5.2 Influence on training time,[0],[0]
"As can be seen from the figure, the syntactic model is much slower to train.",5.2 Influence on training time,[0],[0]
"However, it benefits more from the scale of the training data, with the slope of the dashed curve being steeper than that of the solid curve.",5.2 Influence on training time,[0],[0]
The N-gram model can be trained with more data thanks to the fast training speed.,5.2 Influence on training time,[0],[0]
"However, the performance of the Ngram model flattens when the training data size reaches beyond 3 million.",5.2 Influence on training time,[0],[0]
Projection of the solid curve suggests that the performance of the N-gram model may not surpass that of the syntactic model even if sufficiently large data is available for training the N-gram model in more time.,5.2 Influence on training time,[0],[0]
"Although giving overall better performance, the syntactic model does not perform better than the N-gram model in all cases.",6 Error analysis,[0],[0]
"Here we analyze the strength of each model via more fine-grained comparison.
",6 Error analysis,[0],[0]
"In this set of experiments, the syntactic model is trained using gold-standard annotated WSJ training parse trees, and the N-gram model is trained using the data containing WSJ training data, AFP and XIN.",6 Error analysis,[0],[0]
"The WSJ test data, which contains
golden constituent trees, is used to analyze errors in different aspects.",6 Error analysis,[0],[0]
The BLEU and METEOR scores of the two systems on various sentence lengths are shown in Figure 6.,6.1 Sentence length,[0],[0]
"The results are measured by binning sentences according to their lengths, so that each bin contains about the same number of sentences.",6.1 Sentence length,[0],[0]
"As shown by the figure, the N-gram model performs better on short sentences (less than 8 tokens), and the syntactic model performs better on longer sentences.",6.1 Sentence length,[0],[0]
"This can be explained by the fact that longer sentences have richer underlying syntactic structures, which can better captured by the syntactic model.",6.1 Sentence length,[0],[0]
"In contrast, for shorter sentences, the syntactic structure is relatively simple, and therefore the N-gram model can give better performance based on string patterns, which form smaller search spaces.",6.1 Sentence length,[0],[0]
"We measure the average distortion rate of output word w using the following metric:
distortion(w) = |iw",6.2 Distortion range,[0],[0]
"− i′w| len(Sw) ,
where iw is index of wordw in the output sentence Sw, i′w is the index of the word w in the reference sentence.",6.2 Distortion range,[0],[0]
"len(Sw) is the number of tokens in
sentence Sw.",6.2 Distortion range,[0],[0]
Figure 7 shows distributions of distortion respectively by the syntactic and N-gram model.,6.2 Distortion range,[0],[0]
"The N-gram model makes relatively fewer short-range distortions, but more long-range distortions.",6.2 Distortion range,[0],[0]
This can be explained by the local scoring nature of the N-gram model.,6.2 Distortion range,[0],[0]
"In contrast, the syntactic model makes less long-range distortions, which can suggest better sentence structure.",6.2 Distortion range,[0],[0]
"We further evaluate sentence structure correctness by evaluating the recalls of discovered constituent span in output two systems, respectively.",6.3 Constituent span,[0],[0]
As shown in Figure 8.,6.3 Constituent span,[0],[0]
The syntactic model performs better in most constituent labels.,6.3 Constituent span,[0],[0]
"However, the N-gram model performs better in WHPP, SBARQ and WHNP.
",6.3 Constituent span,[0],[0]
"In the test data, WHPP, SBARQ and WHNP are much less than PP, NP, VP, ADJP, ADVP and CONJP, on which the syntactic model gives better recalls.",6.3 Constituent span,[0],[0]
WHNP spans are small and most of them consist of a question word (WP$) and one or two nouns (e.g. “whose (WP$) parents (NNS)”).,6.3 Constituent span,[0],[0]
WHPP spans are also small and usually consist of a preposition (IN) and a WHNP span (e.g “at (IN) what level (WHNP)”).,6.3 Constituent span,[0],[0]
The N-gram model performs better on these small spans.,6.3 Constituent span,[0],[0]
"The syntactic model also performs better on S, which covers the whole sentence structure.",6.3 Constituent span,[0],[0]
"This verifies the hypothesis introduce that syntactic language models better capture overall sentential grammaticality.
7",6.3 Constituent span,[0],[0]
"Combining the syntactic and N-gram models
The results above show the respective error characteristics of each model, which are complimentary.",6.3 Constituent span,[0],[0]
"This suggests that better results can be achieved by model combination.
7.1 N-gram language model feature
We integrate the two types of models by using N-gram language model probabilities as features in the syntactic model.",6.3 Constituent span,[0],[0]
"N-gram language model probabilities, which ranges from 0 to 1.",6.3 Constituent span,[0],[0]
"Direct use of real value probabilities as features does not work well in our experiments, and we use discretized features instead.",6.3 Constituent span,[0],[0]
"For the L-ARC and RARC actions, because no words are pushed onto the stack, The NLM feature is set to NULL by default.",6.3 Constituent span,[0],[0]
"For the SHIFT action, different feature values are extracted depending on the NLM from 0 to 1.
",6.3 Constituent span,[0],[0]
"In order to measure the N-gram probabilities on our data, we train the 4-gram language model WSJ, AFP and XIN data, and randomly sample 4- gram probabilities from the syntactic model output on the WSJ development data, finding that most of 4-gram probabilities p are larger than 10−12.5.",6.3 Constituent span,[0],[0]
"In this way, if p lower than 10−12.5, NLM feature value is set to LOW.",6.3 Constituent span,[0],[0]
"As for p larger than 10−12.5, we extract the discrete features by assigning them into different bins.",6.3 Constituent span,[0],[0]
We bin the 4-gram probabilities with different granularities without overlap features.,6.3 Constituent span,[0],[0]
"As shown in Table 6, NLM-20, NLM10, NLM-5 and NLM-2 respectively use 20, 10, 5
and 2 bins to capture NLM feature values.",6.3 Constituent span,[0],[0]
"We use the WSJ, AFP and XIN for training the Ngram model7.",7.2 Final results,[0],[0]
"The same WSJ, WPB and SANCL test data are used to measure performances on different domains.
",7.2 Final results,[0],[0]
The experimental results are shown in Tables 7 and 8.,7.2 Final results,[0],[0]
"In both in-domain and cross-domain test data, the combined system outperforms all other systems, with a BLEU score of 52.38 been achieved in the WSJ domain.",7.2 Final results,[0],[0]
It would be overly expensive to obtain a human oracle on discusses.,7.2 Final results,[0],[0]
"However, according to Papineni (2002), a BLEU
7For the combined model, we used the WSJ training data for training, because the syntactic model is slower to train using large data.",7.2 Final results,[0],[0]
"However, we did a set of experiments to scale up the training data by sampling 900k sentences from AFP.",7.2 Final results,[0],[0]
"Results show that the combined model gives BLEU scores of 42.86 and 44.44 on the WPB and SANCL tests, respectively.",7.2 Final results,[0],[0]
"Cross-domain BLEU on WSJ, however falls to 49.84.
score of over 52.38 indicate an easily understood sentence.",7.2 Final results,[0],[0]
"Some sample outputs with different BLEU scores are shown in Table 9
In addition, Table 7 shows that the N-gram model is the fastest among the models due to its small search space.",7.2 Final results,[0],[0]
"The running time of the combined system is larger than the pure syntactic system, because of N-gram probability computation.",7.2 Final results,[0],[0]
Table 8 compare our results with different previous methods on word ordering.,7.2 Final results,[0],[0]
Our combined model gives the best reported performance on this standard benchmarks.,7.2 Final results,[0],[0]
"We empirically compared the strengths and error distributions of syntactic and N-gram language models on word ordering, showing that both can benefit from large-scale raw text.",8 Conclusion,[0],[0]
"The influ-
ence of parsing accuracies has relatively small impact on the syntactic language model trained on automatically-parsed data, which enables scaling up of training data for syntactic language models.",8 Conclusion,[0],[0]
"However, as the size of training data increases, syntactic language models can become intolerantly slow to train, making them benefit less from the scale of training data, as compared with N-gram models.
",8 Conclusion,[0],[0]
"Syntactic models give better performance compared with N-gram models, despite trained with less data.",8 Conclusion,[0],[0]
"On the other hand, the two models lead to different error distributions in word ordering.",8 Conclusion,[0],[0]
"As a result, we combined the advantages of both systems by integrating a syntactic model trained with relatively small data and an N-gram model trained with relatively large data.",8 Conclusion,[0],[0]
"The resulting model gives better performance than both single models and achieves the best reported scores in a standard benchmark for word ordering.
",8 Conclusion,[0],[0]
We release our code under GPL at https:// github.com/SUTDNLP/ZGen.,8 Conclusion,[0],[0]
Future work includes application of the system on text-to-text problem such as machine translation.,8 Conclusion,[0],[0]
The research is funded by the Singapore ministry of education (MOE) ACRF Tier 2 project T2MOE201301.,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their detailed comments.,Acknowledgments,[0],[0]
Syntactic language models and N-gram language models have both been used in word ordering.,abstractText,[0],[0]
"In this paper, we give an empirical comparison between N-gram and syntactic language models on word order task.",abstractText,[0],[0]
Our results show that the quality of automatically-parsed training data has a relatively small impact on syntactic models.,abstractText,[0],[0]
Both of syntactic and N-gram models can benefit from large-scale raw text.,abstractText,[0],[0]
"Compared with N-gram models, syntactic models give overall better performance, but they require much more training time.",abstractText,[0],[0]
"In addition, the two models lead to different error distributions in word ordering.",abstractText,[0],[0]
"A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark.",abstractText,[0],[0]
An Empirical Comparison Between N-gram and Syntactic Language Models for Word Ordering,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 253–263, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
253",text,[0],[0]
"Humans employ different strategies during a conversation in pursuit of their social goals (Tracy and Coupland, 1990).",1 Introduction,[0],[0]
"The contributions to a conversation can be categorized as those which serve propositional functions by adding new information to the dialog, those which serve interactional functions by driving the interaction and those which serve interpersonal functions, by building up the relationship between the involved parties.",1 Introduction,[0],[0]
"When fulfilling interpersonal functions, people either consciously or sub-consciously employ social conversational strategies in order to connect and build relationships with each other (Laurenceau et al., 1998; Won-Doornink, 1985).",1 Introduction,[0],[0]
"This feeling of
rapport, of connecting and having common ground with another human being is one of the fundamental aspects of good human conversation.",1 Introduction,[0],[0]
"Maintaining conversational harmony has shown to be effective in several domains such as education (Ogan et al., 2012; Sinha and Cassell, 2015a,b; Frisby and Martin, 2010; Zhao et al., 2016) and negotiation (Drolet and Morris, 2000; Nadler, 2003, 2004).
",1 Introduction,[0],[0]
Self-disclosure is the conversational act of disclosing information about oneself to others.,1 Introduction,[0],[0]
"We consider the definition of self-disclosure within the theoretical framework of social penetration theory, where it is defined as the voluntary sharing of opinions, thoughts, beliefs, experiences, preferences, values and personal history (Altman and Taylor, 1973).",1 Introduction,[0],[0]
"The effect of self-disclosure has been well-studied in the psychology community, in particular it’s ability to induce reciprocity in dyadic interaction (Jourard, 1971; Derlega et al.,
1Real interaction data withheld for confidentiality.",1 Introduction,[0],[0]
"Conversation data shown here is not real interaction data but follows similar patterns.
1973).",1 Introduction,[0],[0]
"Several studies have shown that selfdisclosure reciprocity characterizes initial social interactions between people (Ehrlich and Graeven, 1971; Sprecher and Hendrick, 2004) and further, that disclosure promotes disclosure (Dindia et al., 2002).
",1 Introduction,[0],[0]
This brings us to a natural question: how does such behavior manifest itself in interactions with dialog systems?,1 Introduction,[0],[0]
"A subtle but crucial aspect is that humans are aware that machines do not have feelings or experiences of their own, so any attempt at self-disclosure on the part of the machine is inherently disingenuous.",1 Introduction,[0],[0]
"However, Nass et al. (1994) suggests that humans tend to view computers as social actors, and interact with them in much the same way they do with humans.",1 Introduction,[0],[0]
"Disclosure reciprocity in such a setting would have far-reaching implications for dialog systems which aim to elicit information from the user in order to offer more personalized experiences for example, or to better achieve task completion (Bickmore and Cassell, 2001; Bickmore and Picard, 2005; Goldstein and Benassi, 1994; Lee and Choi, 2017).
",1 Introduction,[0],[0]
"In this work, we study this phenomena by building an open-domain chatbot (§3) which engages in social conversation with hundreds of Amazon Alexa users (Figure 1.), and gains insights into two aspects of human-machine self-disclosure.",1 Introduction,[0],[0]
"First, self-disclosure by the dialog agent is strongly correlated with instances of self-disclosure by the user indicating disclosure reciprocity in interactions with spoken dialog systems (§4.1).",1 Introduction,[0],[0]
"Second, initial self-disclosure by the user can characterize user behavior throughout the conversation (§4.2).",1 Introduction,[0],[0]
"We additionally study the effect of self-disclosure and likability, but find no reliable linear relationship with the amount of self-disclosure in the conversation (§4.3).",1 Introduction,[0],[0]
"To the best of our knowledge, this work is the first large-scale study of reciprocity and self-disclosure between users in the real world and spoken dialog systems.",1 Introduction,[0],[0]
Self-disclosure as a social phenomena is the act of revealing information about oneself to others.,2 Background,[0],[0]
"It has been of particular interest to study what factors makes humans self-disclose (Miller et al., 1983; Dindia and Allen, 1992; Hill and Stull, 1987; Buhrmester and Prager, 1995; Stokes, 1987; Qian and Scott, 2007; Jourard and Friedman, 1970; Ko and Kuo, 2009), how do they do it (Chen, 1995;
Greene et al., 2006; Chelune, 1975; Sprecher and Hendrick, 2004) and what are the effects of selfdisclosing (Gibbs et al., 2006; Mazer et al., 2009; Forest and Wood, 2012; Turner et al., 2007; Knox et al., 1997; Vittengl and Holt, 2000).
",2 Background,[0],[0]
"One such effect is disclosure reciprocity, which has been shown to be one of the most significant effects of self-disclosure (Jourard, 1971).",2 Background,[0],[0]
Reciprocity is the phenomenon by which selfdisclosure by one participant in a dyadic social interaction results in self-disclosure from the other participant in response.,2 Background,[0],[0]
"A substantial amount of research has shown that when one party selfdiscloses, the other party is much more likely to self-disclose (Jourard, 1971; Jourard and Friedman, 1970; Dindia et al., 2002; Derlega et al., 1973).",2 Background,[0],[0]
"While the exact cause of this phenomena is not known, it has been suggested that selfdisclosure can be viewed as a social exchange, where the party receiving self-disclosure feels obligated to self-disclose in return (Archer, 1979), or as a social conversational norm (Derlega et al., 1993), or from the point of view of social trustattraction (Vittengl and Holt, 2000) where people self-disclose to people who disclose to them, as they consider self-disclosure to be a sign of trust and liking.",2 Background,[0],[0]
"Additionally, Sprecher and Hendrick (2004) find that people who consider themselves to be high self-disclosers are likely to be much better at eliciting self-disclosure as well.",2 Background,[0],[0]
"Derlega et al. (1973) observe that self-disclosure is a positive function of self-disclosure received, regardless of liking for the initial discloser.",2 Background,[0],[0]
"Mikulincer and Nachshon (1991) analyze personality types and self-disclosure, and find that secure people are more likely to both self-disclose and reciprocate self-disclosure.",2 Background,[0],[0]
Cozby (1972) study the relationship between disclosure and liking and suggest that this relationship is not linear.,2 Background,[0],[0]
"In this work, we attempt to combine these perspectives to gain insights into the nature of self-disclosure in humanmachine dialog.",2 Background,[0],[0]
"In this work, we consider the definition of selfdisclosure within the theoretical framework of social penetration theory (Altman and Taylor, 1973) where it is defined to be the voluntary sharing of information which could include amongst other things one’s personal history, thoughts, opinions,
beliefs, feelings, preferences, attitudes, aspirations, likes, dislikes and favorites.",3.1 Coding Self Disclosure,[0],[0]
"In a humanmachine context, we define self-disclosure as the conversational act of revealing aspects of oneself voluntarily, which would otherwise not be possible to be known by the dialog system.",3.1 Coding Self Disclosure,[0],[0]
"A general rule-of-thumb we follow is, self-disclosure is proportional to the amount of extraneous information that is added to a conversation.",3.1 Coding Self Disclosure,[0],[0]
"For example, we do not identify a direct response to a question as self-disclosure as it is not strictly voluntary.",3.1 Coding Self Disclosure,[0],[0]
We show examples of our definition of human selfdisclosure and non-disclosure in the context of our dialog system in Figure.,3.1 Coding Self Disclosure,[0],[0]
2.,3.1 Coding Self Disclosure,[0],[0]
The data for this study was collected by having users from the real-world interact with our open-domain dialog agent.,3.2 Dataset Preparation,[0],[0]
"The dialog agent was hosted on Amazon Alexa devices as part of the AlexaPrize competition (Ram et al., 2018) and was one of sixteen socialbots that could be invoked by any user within the United States through the command ‘Let’s chat!’.",3.2 Dataset Preparation,[0],[0]
"The users that interacted with our socialbot were randomly chosen, and did not know which of the sixteen systems they were interacting with.",3.2 Dataset Preparation,[0],[0]
Users who interacted with our bot over a span of three days (N=1507) were randomly assigned to two groups: one received a bot that self-disclosed at high depth from the beginning of the conversation while the other group interacted with a socialbot that self-disclosed only later about superficial topics like movies and TV shows.,3.2 Dataset Preparation,[0],[0]
"At the end, both socialbots engaged in free-form conversation with the user, where the initiative of the interaction was on the user and both bots were free to self-disclose at any depth.",3.2 Dataset Preparation,[0],[0]
"The users were also free to end the interaction at any time, and thus had no motivation for continuing the conversation besides their own enter-
2Not real interaction data, however very similar to actual utterances found in the interaction data
tainment.",3.2 Dataset Preparation,[0],[0]
"To control the direction of the conversation and bot utterance, we utilize a finite state transducer-based dialog system that chats with the user about movies and TV shows, as well as plays games and supports open-domain conversation (Prabhumoye et al., 2017).",3.2 Dataset Preparation,[0],[0]
"State transitions are decided based on sentiment analysis of user utterances, in order to gauge interest in a particular topic.",3.2 Dataset Preparation,[0],[0]
"Initially the dialog system takes initiative in the conversation and steers the topic of discussion, however later there is a handoff to the user whereby the user can determine the focus of the conversation.",3.2 Dataset Preparation,[0],[0]
"In this way, the socialbot leads the user through the following topics, conditioned on user interest as shown in Figure 3:",3.2 Dataset Preparation,[0],[0]
"Greeting : In this phase, our dialog agent greets the user and asks them about their day.",3.2 Dataset Preparation,[0],[0]
The bot which performs high self-disclosure initially also responds with information about it’s day and a personal anecdote.,3.2 Dataset Preparation,[0],[0]
TV Shows:,3.2 Dataset Preparation,[0],[0]
The next phase involves chit chat about popular TV shows.,3.2 Dataset Preparation,[0],[0]
The dialog agent asks the user if they are an enthusiast of a recent popular TV show and moves on to the next phase of the conversation if they aren’t.,3.2 Dataset Preparation,[0],[0]
Movie:,3.2 Dataset Preparation,[0],[0]
"In this phase, the dialog agent attempts to engage the user in conversations about movies, asking them if they have seen any of the recent ones.",3.2 Dataset Preparation,[0],[0]
Word Game:,3.2 Dataset Preparation,[0],[0]
"In this phase, the dialog agent requests the user to play a word game.",3.2 Dataset Preparation,[0],[0]
Participation in the game is completely optional and the user can move on to the next phase by stating that they do not wish to play.,3.2 Dataset Preparation,[0],[0]
CQA:,3.2 Dataset Preparation,[0],[0]
The last phase supports uninhibited freeform conversation.,3.2 Dataset Preparation,[0],[0]
The initiative of the exchange is now on the user and conversation is stateless.,3.2 Dataset Preparation,[0],[0]
The dialog system response is determined by a retrieval model.,3.2 Dataset Preparation,[0],[0]
"For each utterance, the socialbot attempts to retrieve the most relevant response from the Yahoo L6 dataset (yl6, 2017), a dataset containing approximately 4 million questions and
their corresponding answers from the Community Question-Answering (CQA) website, Yahoo Answers 3.
",3.2 Dataset Preparation,[0],[0]
"The users were then allowed to rate the interaction on a scale of 1-5, based on the question ‘Would you interact with this socialbot again?’.",3.2 Dataset Preparation,[0],[0]
319 users rated the socialbot (Group 1) and 1507 users interacted with our system in total (Group 2).,3.2 Dataset Preparation,[0],[0]
"Following this, to preserve confidentiality of the interaction data, one annotator annotated all turns of conversation from Group 1 for self-disclosure.",3.2 Dataset Preparation,[0],[0]
Annotator reliability was determined by calculating inter-annotator agreement from three external annotators on a carefully prepared anonymized subset of the data amounting to 62 interactions comprising of over 816 turns.,3.2 Dataset Preparation,[0],[0]
"The Fleiss’ kappa from the four annotators was 63.8, indicating substantial agreement.",3.2 Dataset Preparation,[0],[0]
Atleast two of three annotators agreed on 93.6% of the reference annotations.,3.2 Dataset Preparation,[0],[0]
"The full dataset contains a total of 319 conversations, spanning 10751 conversational turns.",3.2 Dataset Preparation,[0],[0]
"Out of the 5216 human dialog utterances, 13.8% featured some form of self-disclosure.
",3.2 Dataset Preparation,[0],[0]
Since our agent is a spoken dialog system in the real world there is some amount of noise in the dataset caused due to ASR errors.,3.2 Dataset Preparation,[0],[0]
"To estimate this, we randomly sample 100 utterances from the dataset and annotate these utterances for whether they contained an ASR mistake, and if the sentence meaning was still apparent either from context or from the utterance itself.",3.2 Dataset Preparation,[0],[0]
"We find that at least one ASR error occurs in 13% of user utterances, but 46.1% of utterances with ASR mistakes can still be understood.",3.2 Dataset Preparation,[0],[0]
"Since our dialog agent relies on sentiment-based FST transitions during the initial stages of the conversation, we also analyze the rate of false transitions in the data.",3.2 Dataset Preparation,[0],[0]
"We randomly sample 100 utterances from across choice points of all conversations and find that 11% of them consisted of incorrect responses, either due to mistakes in sentiment analysis or due to nu-
3answers.yahoo.com
ance in the user utterances which rendered a response from the dialog agent unusable.",3.2 Dataset Preparation,[0],[0]
"Finally, we analyze how many users had multiple interactions with our dialog agent during the course of our study.",3.2 Dataset Preparation,[0],[0]
This is relevant as user behavior during a second interaction with the system might differ from initial interaction.,3.2 Dataset Preparation,[0],[0]
Users are identifiable only by an anonymized hash key provided by Amazon along with the conversation data.,3.2 Dataset Preparation,[0],[0]
"We find that out of 316 users who interacted with our dialog agent and left a rating, only 3 interacted with our agent twice and none of them interacted with our agent more than two times, largely allowing us to disregard this effect.",3.2 Dataset Preparation,[0],[0]
We utilize the annotations of 319 conversations to train and evaluate a Machine Learning model to identify user self-disclosure.,3.3 Feature Space Design,[0],[0]
"We categorize the features for this model at two levels, utterancelevel features wherein the user utterance is taken standalone and analyzed for self-disclosure and conversational-level features which consider the utterance in context of the current conversation.",3.3 Feature Space Design,[0],[0]
This represents a class of features that only consider the current utterance.,3.3.1 Utterance Features,[0],[0]
"These include-
1.",3.3.1 Utterance Features,[0],[0]
"Bag-of-words Features TF-IDF features from the user utterance.
",3.3.1 Utterance Features,[0],[0]
2.,3.3.1 Utterance Features,[0],[0]
"Linguistic Style Features This class of features attempts to characterize the linguistic style of user utterances, including lexical choices that might be indicative of selfdisclosure (Doell, 2013).",3.3.1 Utterance Features,[0],[0]
These include- i),3.3.1 Utterance Features,[0],[0]
"Length of the user utterance, ii)",3.3.1 Utterance Features,[0],[0]
"Presence of negation words, iii) Part-of-speech tags such as nouns and adjectives in the user utterance in order to represent users revealing emotion or discussing topics, iv) Presence of filler words in utterance, v) Number",3.3.1 Utterance Features,[0],[0]
These features are broadly based on dialog structure or the language-based features from local conversational context.,3.3.2 Conversation Features,[0],[0]
"These include i) TF-IDF features from the user utterance concatenated with the bot utterance5, to help capture the difference between direct responses to questions and voluntary self-disclosure, ii) dialog system self-disclosing in previous turn, iii) dialog system asking a question in the previous turn, iv)",3.3.2 Conversation Features,[0],[0]
"Amount of word overlap with previous machine utterance, which is defined as the number of words that overlap with the previous dialog system utterance normalized by the length of the dialog system utterance, v) Number of content words6 that overlap with previous machine utterance.
",3.3.2 Conversation Features,[0],[0]
"4Includes phrases such as ”I’m fine”, ”I’m ok”, ”I’m good”, ”I’m doing ok”, ”I’m doing good”, ”how are you” for conversational responses, ”delightful”, ”favorite”, ”amazing”, ”awesome”, ”fantastic”, ”brilliant”, ”the best”, ”really great” etc. for strongly positive, ”boring”, ”tired”, ”bored”, ”sad”, ”lonely”, ”disgusting”, ”hate”,”awful” etc.",3.3.2 Conversation Features,[0],[0]
"for strongly negative and ”rain”, ”summer”, ”winter”, ”cold”, ”wind” etc. for strongly neutral (as users tend to discuss weather while making small talk).
",3.3.2 Conversation Features,[0],[0]
"5Each word of the bot utterance is encapsulated within a <bot></bot> tag
6where we determine content words following the usual definition of nouns, main verbs, adjectives and adverbs.
",3.3.2 Conversation Features,[0],[0]
"Model Accuracy Precision Recall F1 First Person 86.6% 68.0% 6.0% 10.9%
Utterance Features
89.8% 69.8% 46.5% 55.5%
Utterance + Conversation Features 91.7% 74.4% 60.5% 66.67%",3.3.2 Conversation Features,[0],[0]
The combination of the three categories of features results in a 234-dimensional vector which acts as input to an SVM with a linear kernel.,3.4 Results of Identification,[0],[0]
We utilize truncated SVD with 100 components for dimensionality reduction of all bag-of-words based feature classes.,3.4 Results of Identification,[0],[0]
"We compare against two baselines, the first is a baseline consisting of only personal voiced features (including all LIWC features) and the second attempts to classify self-disclosure independent of dialog context (only conditioned on the current user utterance).",3.4 Results of Identification,[0],[0]
We perform 10-fold cross validation and describe our results in Table.,3.4 Results of Identification,[0],[0]
1.,3.4 Results of Identification,[0],[0]
We observe that considering user utterances in context of the conversation considerably improves our ability to predict self-disclosure.,3.4 Results of Identification,[0],[0]
"To perform more detailed error analysis on a larger test set, we randomly sample 1044 utterances from 5216 utterances to be a held-out test set.",3.4 Results of Identification,[0],[0]
This test set consists of 134 utterances of self-disclosure.,3.4 Results of Identification,[0],[0]
"Our classifier achieves an accuracy of 93.4% at
recognizing self-disclosure on this test set, with a F1-score of 72.7% (Precision: 77.3%, Recall: 68.6%).",3.4 Results of Identification,[0],[0]
The test distribution contains 12.8% examples of self-disclosure and 87.2% examples of no disclosure.,3.4 Results of Identification,[0],[0]
We further perform an ablation study of each dialog-context feature as shown in Figure 4.,3.4 Results of Identification,[0],[0]
"We observe that considering the word in the context of the machine utterance is most helpful in identifying self-disclosure, indicating possibly that it helps us capture the notion of selfdisclosure being a voluntary phenomena whereby the user reveals information about himself or herself, by separating instances of direct answers to questions from turns where users disclose more than what is asked.",3.4 Results of Identification,[0],[0]
"We next conduct a careful manual error analysis of the mistakes made by our classifier, in an attempt to identify what cases are particularly hard or ambiguous.",3.4 Results of Identification,[0],[0]
"We observe that 85% of user turns which our model wrongly labeled as containing self-disclosure had personal pronouns, suggesting that our model considers these as a very strong signal for self-disclosure.",3.4 Results of Identification,[0],[0]
"However many of these utterances were in fact direct responses to questions, or questions to the bot itself prefaced with a personal pronoun, and thus not really instances of self-disclosure.",3.4 Results of Identification,[0],[0]
"25.9% of the mistakes were not well-formed or meaningful sentences, possibly due to ASR errors, speech disfluencies or user phrasing.",3.4 Results of Identification,[0],[0]
We also examine the user turns our model failed to predict as being self-disclosure.,3.4 Results of Identification,[0],[0]
19.5% of these mistakes were not well-formed sentences and 12.1% were statements about the bots performance.,3.4 Results of Identification,[0],[0]
"A further 21.9% of errors contained rare words which might not have been seen before in the training data along with an absence of the linguistic markers of self-disclosure identified by us (for example, M: Anything special today?",3.4 Results of Identification,[0],[0]
H: Really wanna grab a smoke).,3.4 Results of Identification,[0],[0]
"In the future, real world knowledge and a larger amount of training data might help mitigate some of these error classes.",3.4 Results of Identification,[0],[0]
"We analyze common markers of reciprocity (Jourard and Jaffe, 1970; Harper and Harper, 2006), such as the usage of personal pronouns, word overlap with the previous sentence (normalized by length of previous utterance) and average user utterance length between two groups of usersones who were shown a bot that self-disclosed ini-
tially and a bot which only self-disclosed later (Table 2.).
",4.1 Reciprocity,[0],[0]
"Within the data which consists of only rated conversations, we observe how many turns where the machine self-disclosed were also met with human self-disclosure (“Rated” in Table. 3).",4.1 Reciprocity,[0],[0]
We then tag all user utterances 7 with our SVM classifier as either being instances of self-disclosure or not being instances of self disclosure (“All” in Table. 3).,4.1 Reciprocity,[0],[0]
"We find that 10.6% of all user utterances contain self disclosure, and 21.6% of machine utterances that contained self-disclosure were followed by a human utterance that contained self-disclosure, compared to the 7.4% of cases where a user selfdisclosed without the machine self-disclosing (p < 0.05).",4.1 Reciprocity,[0],[0]
These results are shown in Table.,4.1 Reciprocity,[0],[0]
"3.
",4.1 Reciprocity,[0],[0]
"Next, we observe the utterance after initial selfdisclosure for a group where the socialbot selfdiscloses compared to the equivalent dialog turn for a group where the bot doesn’t self-disclose, to analyze if self-disclosure has immediate effects.",4.1 Reciprocity,[0],[0]
These results are shown in Table.,4.1 Reciprocity,[0],[0]
4.,4.1 Reciprocity,[0],[0]
"We observe that when the bot self-discloses, the user self-discloses in response in 56.5% of all cases.",4.1 Reciprocity,[0],[0]
"However if the bot does not self-disclose and asks the same question, the user self discloses only in 35.5% of all cases (p < 0.0001).",4.1 Reciprocity,[0],[0]
"Our findings suggest that it is possible user behavior is affected by
7from 811 conversations of length greater than three turns.
",4.1 Reciprocity,[0],[0]
"the self-disclosing behavior of our dialog agent, and that such an effect can be seen immediately.",4.1 Reciprocity,[0],[0]
"We next examine conversation-wide characteristics and self-disclosure patterns of users based on their initial self-disclosing behavior.
",4.2 Initial Self-Disclosure and User behavior,[0],[0]
Are Conversations With Initial Self-Disclosure Longer?,4.2 Initial Self-Disclosure and User behavior,[0],[0]
We analyze whether whether initial occurrences of user self-disclosure lead to users prolonging the conversation by examining average conversational length for two groups of users : those who decided to self-disclose at the very beginning of the conversation itself and those who didn’t.,4.2 Initial Self-Disclosure and User behavior,[0],[0]
"We find that users who self-disclose initially tend to have significantly longer conversation than users who do not (p<0.05), with an average conversational length of 37.19 turns compared to an average of 32.4 turns for users who chose not to self-disclose.
Does not self-disclosing initially imply reduced self-disclosure throughout the conversation?",4.2 Initial Self-Disclosure and User behavior,[0],[0]
We next examine the hypothesis that users who do not self-disclose initially tend to self-disclose less throughout.,4.2 Initial Self-Disclosure and User behavior,[0],[0]
"This is based on the notion of openness and guardedness in personality (Stokes, 1987; Sermat and Smyth, 1973) indicating that some individuals are more likely to self-disclose than others.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
"For this study, we do not consider interactions involving the word game as it prolongs the conversation without giving opportunities for self-disclosure.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
"We examine to what extent do individuals who refuse to self-disclose initially, self-disclose later in the conversation compared to users who self-disclose from the beginning of the conversation itself.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
"We find that on average, users who do not choose to self-disclose initially are significantly less likely to self-disclose (p<0.05) even later on in the conversation, only revealing information in 9% of their turns as compared to the 24.6% of turns of other users.
",4.2 Initial Self-Disclosure and User behavior,[0],[0]
Do users who choose not to self-disclose initially exhibit less interest in following machine interests?,4.2 Initial Self-Disclosure and User behavior,[0],[0]
"To analyze openness to conversation, we invite users to play a long-winded word game with the dialog system.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
We analyze how much self-disclosure correlates with willingness to play the game and length of game playing.,4.2 Initial Self-Disclosure and User behavior,[0],[0]
"We find that on average users who self-disclose initially are also significantly more open to game-playing than those who don’t (p<0.05), playing on average 4.75 turns of the game compared to an average gameplay of 3.16 turns by other users.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
"They are also significantly more likely to attempt to play the game (p<0.05), with 34.7% of self-disclosing users attempting to play the game and only 25.1% of non-disclosing users attempting to do so.",4.2 Initial Self-Disclosure and User behavior,[0],[0]
"Motivated by Cozby (1972), we attempt to analyze whether self-disclosure increases likability in human-machine interaction.",4.3 Does Self-Disclosure Increase Likability,[0],[0]
"We utilize the user ratings based on the question ‘Would you talk to this socialbot again’ as a proxy for likability of the dialog agent, and examine whether conversations where the user self-disclosed often were given higher ratings than ones where they didn’t.",4.3 Does Self-Disclosure Increase Likability,[0],[0]
We find that there is negligible correlation in general between user ratings and the amount of selfdisclosure (pearson’s r = 0.01).,4.3 Does Self-Disclosure Increase Likability,[0],[0]
"We then examine the differences in user ratings between the top 20% and bottom 20% of self-disclosing conversations, once more excluding interactions with the game.",4.3 Does Self-Disclosure Increase Likability,[0],[0]
"We observe that while more self-disclosing conversations get higher ratings in general, the results are not statistically significant (average rating of conversations with higher self-disclosure is 3.14 compared to 3.13 for conversations with lesser self-disclosure).",4.3 Does Self-Disclosure Increase Likability,[0],[0]
"Lastly, we analyze the effect of reciprocity and self-disclosure, by analyzing the ratings of users who self-disclosed in response to bot disclosure but find no significant difference in the ratings of such users (3.34 to 3.27).",4.3 Does Self-Disclosure Increase Likability,[0],[0]
Thus we are unable to find any conclusive linear relationship between self-disclosure and likability.,4.3 Does Self-Disclosure Increase Likability,[0],[0]
There has been significant prior interest in computationally analyzing various forms of selfdisclosure online,5 Discussion and Related Work,[0],[0]
"(Yang et al., 2017; Wang et al., 2016; Stutzman et al., 2012; Yin et al., 2016;
Bak et al., 2014; De Choudhury and De, 2014).",5 Discussion and Related Work,[0],[0]
"Bickmore et al. (2009) study the effect of machine ‘backstories’ in dialog, and find that users rate their interactions to be more enjoyable when the dialog system has a backstory.",5 Discussion and Related Work,[0],[0]
Zhao et al. (2016) identify self-disclosure in peer tutoring between humans.,5 Discussion and Related Work,[0],[0]
Han et al. (2015); Meguro et al. (2010) identify self-disclosure as a user intention in a natural language understanding system.,5 Discussion and Related Work,[0],[0]
Oscar J. Romero (2017) use self-disclosure as one strategy amongst others to build a socially-aware conversational agent.,5 Discussion and Related Work,[0],[0]
"Higashinaka et al. (2008) study if users self-disclose on topics they like rather than ones they don’t, with a focus on text-based chat rather than spoken dialog.",5 Discussion and Related Work,[0],[0]
"Similarly, Lee and Choi (2017) study the relation between self-disclosure and liking for a movie recommendation system, using a Wizard-of-Oz approach instead of constructing a dialog agent.",5 Discussion and Related Work,[0],[0]
"Perhaps closest to our work is the work of Moon (2000), which studies the phenomena of reciprocity in human-machine self-disclosure.",5 Discussion and Related Work,[0],[0]
"However, this phenomena is not studied for dialog, and similar to previous work, relies on a text-based series of interview questions.
",5 Discussion and Related Work,[0],[0]
"In this work, we are interested in realizing selfdisclosure in a real-time, large-scale spoken dialogue system.",5 Discussion and Related Work,[0],[0]
We depart from previous work in three main ways.,5 Discussion and Related Work,[0],[0]
"First, we have the opportunity of deploying a dialog agent in the wild, and studying hundreds of interactions with real users in US households.",5 Discussion and Related Work,[0],[0]
"Second, we study reciprocity of self-disclosure in human-machine dialog, and find markers of reciprocity even in conversations with a dialog agent.",5 Discussion and Related Work,[0],[0]
"Third, we characterize users by their initial self-disclosing behavior and study conversation-level behavioral differences.",5 Discussion and Related Work,[0],[0]
"We believe this work to be a step towards better understanding the effect of dialog agents deployed in the real-world employing self-disclosure as a social strategy, as well as better understanding the implications of self-disclosing user behavior with dialog agents.
",5 Discussion and Related Work,[0],[0]
We acknowledge limitations of our current approach.,5 Discussion and Related Work,[0],[0]
"In this work, our definition of selfdisclosure is binary.",5 Discussion and Related Work,[0],[0]
"A more nuanced version that considers both magnitude and valence of selfdisclosure would open up several further research directions, such as analyzing reciprocity matching in depth of disclosure and analyzing user behavior based on the valence of disclosure.",5 Discussion and Related Work,[0],[0]
"It would also be interesting to analyze how agent behavior
can significantly influence non-disclosing users, as our results find that users who do not initially selfdisclose continue to self-disclose at reduced levels throughout the conversation.",5 Discussion and Related Work,[0],[0]
"Another immediate research direction would be to study the effect of other social conversational strategies such as praise (Fogg and Nass, 1997; Zhao et al., 2016) at a large scale in spoken-dialog systems.",5 Discussion and Related Work,[0],[0]
"In the future, one could imagine dialog agents that reason over both social strategies and their magnitude, conditioned on user behavior, in service of their conversational goals.",5 Discussion and Related Work,[0],[0]
"In this work, we empirically study the effect of self-disclosure in a large-scale experiment involving real-world users of Amazon Alexa.",6 Conclusion,[0],[0]
"We find that indicators of reciprocity occur even in conversations with dialog systems, and that user behavior can be characterized by self-disclosure patterns in the initial stages of the conversation.",6 Conclusion,[0],[0]
"We hope that these findings inspire more user-centric research in dialog systems, with an emphasis on dialog agents that attempt to build a relationship and maintain rapport with the user when eliciting information.",6 Conclusion,[0],[0]
This work has partially been supported by the National Science Foundation under Grant No.,Acknowledgements,[0],[0]
CNS 13-30596.,Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the NSF, or the US Government.",Acknowledgements,[0],[0]
"The authors are grateful to the CMU Magnus team for their hard work through the AlexaPrize competition, with special thanks to Shrimai Prabhumoye and Chaitanya Malaviya for helping with this effort.",Acknowledgements,[0],[0]
"The authors would like to express their gratitude to Diyi Yang, Alexander Rudnicky and Dan Jurafsky for interesting discussions related to this work.",Acknowledgements,[0],[0]
"The authors would like to especially thank Shruti Rijhwani, Shivani Poddar and Aditya Potukuchi for their time and support through this effort.",Acknowledgements,[0],[0]
"Finally, the authors are immensely grateful to the Amazon Alexa team for facilitating universities to do dialog research with real user data, through the Alexa Prize competition, as well as to all the Amazon Alexa users who were willing to interact with our system.",Acknowledgements,[0],[0]
Self-disclosure is a key social strategy employed in conversation to build relations and increase conversational depth.,abstractText,[0],[0]
"It has been heavily studied in psychology and linguistic literature, particularly for its ability to induce self-disclosure from the recipient, a phenomena known as reciprocity.",abstractText,[0],[0]
"However, we know little about how self-disclosure manifests in conversation with automated dialog systems, especially as any self-disclosure on the part of a dialog system is patently disingenuous.",abstractText,[0],[0]
"In this work, we run a large-scale quantitative analysis on the effect of selfdisclosure by analyzing interactions between real-world users and a spoken dialog system in the context of social conversation.",abstractText,[0],[0]
"We find that indicators of reciprocity occur even in human-machine dialog, with far-reaching implications for chatbots in a variety of domains including education, negotiation and social dialog.",abstractText,[0],[0]
An Empirical Study of Self-Disclosure in Spoken Dialogue Systems,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2883–2889 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2883",text,[0],[0]
An important learning question in morphology— both for NLP and models of language acquisition—is the so-called Paradigm Cell Filling Problem (PCFP).,1 Introduction,[0],[0]
"So dubbed by Ackerman et al. (2009), this problem asks how it is that speakers of a language can reliably produce inflectional forms of most lexemes without ever witnessing those forms before.",1 Introduction,[0],[0]
"For example, a Finnish noun or adjective can be inflected in 2,263 ways if one includes case forms, number, and clitics (Karlsson, 2008).",1 Introduction,[0],[0]
"However, it is unlikely that a Finnish speaker would have heard all forms for even a single, highly frequent lexical item.",1 Introduction,[0],[0]
"It is also unlikely that all 2,263 forms are found in the aggregate of all the witnessed inflected forms over different lexemes and speakers must be able to assess the felicity of, and possibly produce such inflectional combinations they have never witnessed for any noun or adjective.",1 Introduction,[0],[0]
"Figure 1 illustrates the PCFP.
",1 Introduction,[0],[0]
"This paper investigates PCFP in three different settings: (1) when we know n > 1 randomly selected forms in each of a number of inflection tables, (2) when we know a set of frequent word forms in each table (this most closely resembles an L1 language learning setting), and finally (3)
1https://github.com/mpsilfve/pcfp-data
when we know exactly n = 1 word form from each table.
",1 Introduction,[0],[0]
"We treat settings (1) and (2) as traditional morphological reinflection tasks (Cotterell et al., 2016) as explained in Section 2.",1 Introduction,[0],[0]
"In contrast, setting (3) is substantially more challenging because it cannot be handled using a traditional reinflection approach.",1 Introduction,[0],[0]
"To overcome this problem, we utilize an adaptive dropout mechanism which will be discussed in Section 2.",1 Introduction,[0],[0]
"This allows us to train the reinflection system in a manner reminiscent of denoising autoencoders (Vincent et al., 2008).
",1 Introduction,[0],[0]
"Related Work Neural models have recently been shown to be highly competitive in many different tasks of learning supervised morphological inflection (Faruqui et al., 2016; Kann and Schütze, 2016; Makarov et al., 2017; Aharoni and Goldberg, 2017) and derivation (Cotterell et al., 2017b).",1 Introduction,[0],[0]
"Most current architectures are based on encoderdecoder models (Sutskever et al., 2014), and usually contain an attention component (Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"The SIGMORPHON (Cotterell et al., 2016) and CoNLL-SIGMORPHON (Cotterell et al., 2017a, 2018) shared tasks in recent years have explored morphological inflection but not explicitly the PCFP.",1 Introduction,[0],[0]
"In the 2017 task, participants were given full paradigms—i.e. a listing of all forms—of
lexemes during training after which they were given incomplete paradigms which had to be completed at test time.",1 Introduction,[0],[0]
"This is a slightly unrealistic setting in an L1-style learning scenario (Blevins and Blevins, 2009) where arguably very few full paradigms are ever witnessed and where generalization has to proceed on a number of very gappy paradigms.",1 Introduction,[0],[0]
"Of course, such gaps form a distribution where frequently used lexemes have fewer gaps than infrequent ones, which we will attempt to model in this work.
",1 Introduction,[0],[0]
"Silfverberg et al. (2018) evaluate an extension to a linguistically informed symbolic paradigm model based on stem extraction from the longest common subsequence (LCS) shared among related forms (Ahlberg et al., 2014, 2015).",1 Introduction,[0],[0]
"While the original LCS paradigm extraction method was intended to learn from complete inflection tables (Hulden, 2014), Silfverberg et al. (2018) present modifications to allow learning from incomplete paradigms as well, and apply it to the PCFP.",1 Introduction,[0],[0]
"Comparing against their results, shows that our neural model consistently outperforms such a subsequence-based learning model.
",1 Introduction,[0],[0]
Kann et al. (2017) report results on so-called multi-source reinflection in which several input forms are used to generate one output form.,1 Introduction,[0],[0]
"This task is related to the PCFP; however, Kann et al. (2017) use full inflection tables for training.",1 Introduction,[0],[0]
"Moreover, their approach is applicable for PCFP only when 3 or more forms are given in the input tables.",1 Introduction,[0],[0]
"Since this mostly excludes our experimental settings, we do not compare to their system.",1 Introduction,[0],[0]
"Malouf (2016, 2017) documents an experiment with a generator LSTM in completing inflection tables in up to seven languages with either 10% or 40% of table entries missing.",1 Introduction,[0],[0]
"Our work differs from this in that Malouf gives as input a two-hot encoding of both the lexeme and the desired slot during training and testing for which an inflection table is to be completed, which means the system cannot complete paradigms which it has not seen examples of in the training data.",1 Introduction,[0],[0]
"By contrast, our system has no notion of lexeme and we simply work from the symbol strings which are collections of inflected forms of a lexeme given in the test data which may in principle be completely disjoint from training data lexemes.",1 Introduction,[0],[0]
We use the Malouf system as a baseline to compare against.,1 Introduction,[0],[0]
We explore two different models for paradigm filling.,2 Encoder-Decoder Models for PCFP,[0],[0]
The first model is applicable when n > 1 forms are given in each inflection table.,2 Encoder-Decoder Models for PCFP,[0],[0]
"When exactly one (n = 1) form is given, we use another model.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"Case n>1 When more than one form is given in training tables, PCFP can be treated as a morphological reinflection task (Cotterell et al., 2016), where the aim is to translate inflected word forms and their tags into target word forms.",2 Encoder-Decoder Models for PCFP,[0],[0]
"For example, a model would translate tried+PAST into the present participle (PRES,PCPLE) form trying.",2 Encoder-Decoder Models for PCFP,[0],[0]
"We adopt a common approach employed by Kann and Schütze (2016) and many others: we build a model which translates an input word form, its tag and a target tag, for example tried+PAST+PRES,PCPLE, into the target word form trying.
",2 Encoder-Decoder Models for PCFP,[0],[0]
Our model closely follows the formulation of the encoder-decoder LSTM model for morphological reinflection proposed by Kann and Schütze (2016).,2 Encoder-Decoder Models for PCFP,[0],[0]
"We use a 1-layer bidirectional LSTM encoder for encoding the input word form into a sequence of state vectors and a 1-layer LSTM decoder with an attention mechanism over encoder states for generating the output word form.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"We form training pairs by using the given forms in each table, i.e. take the cross-product of the given forms and learn to reinflect each given form in a table to another given form in the same table as demonstrated in Figure 2.2 During test time, we predict forms for missing slots based on each of the given forms in the table and take a majority vote of the results.3
Case n=1",2 Encoder-Decoder Models for PCFP,[0],[0]
"When only one form is given in each inflection table, we cannot train the model as a traditional reinflection model.",2 Encoder-Decoder Models for PCFP,[0],[0]
"The best we can do is to train a model to reinflect forms into the same form walked+PAST+PAST 7→ walked and then try to apply this model for reinflection to fill in missing forms walked+PAST+PRES,PCPLE 7→ walking.",2 Encoder-Decoder Models for PCFP,[0],[0]
"According to preliminary experiments, this however leads to massive over-fitting and the model simply learns to only copy input forms.
2Note that the CoNLL-SIGMORPHON data provides a ‘citation form’ that identifies each table; we do not use this form and the model has no knowledge of it.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"3When only two forms are given in the partial inflection table, we randomly choose one of the resulting output forms since the vote is always tied.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"The idea for our approach in case n = 1 is to first learn to segment word forms into a stem and an affix, for example walk+ed.",2 Encoder-Decoder Models for PCFP,[0],[0]
We then hide the affix in the input form and learn to inflect.,2 Encoder-Decoder Models for PCFP,[0],[0]
"In other words, we map the word form walked into walk$$ and then learn a mapping walk$$+PAST 7→ walked.",2 Encoder-Decoder Models for PCFP,[0],[0]
"This model suffers less from overfitting and we can use it to find missing forms in partial inflection tables.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"Since we do not have access to segmented training data, we cannot directly train a segmentation model.",2 Encoder-Decoder Models for PCFP,[0],[0]
"Instead, we use the forms in the training data to train an LSTM language model conditioned on morphological tags.",2 Encoder-Decoder Models for PCFP,[0],[0]
"We then use the language model for identifying which characters belong to stems and which characters belong to affixes.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"As shown in Figure 3, the language model in general gives higher confidence for predictions of characters in the affix than in the word stem.",2 Encoder-Decoder Models for PCFP,[0],[0]
"Nevertheless, it only gives a probabilistic segmentation into a stem and affix(es).",2 Encoder-Decoder Models for PCFP,[0],[0]
"Therefore, we do not perform a deterministic segmentation.",2 Encoder-Decoder Models for PCFP,[0],[0]
Instead we use the language model to guide a character dropout mechanism in our word inflection model.,2 Encoder-Decoder Models for PCFP,[0],[0]
"When the language model is very confident, as in the case of affix characters, we frequently drop characters.",2 Encoder-Decoder Models for PCFP,[0],[0]
"In contrast, when the language model
is less confident, as in the case of stem characters, we typically keep the character.",2 Encoder-Decoder Models for PCFP,[0],[0]
"Apart from this adaptive dropout applied during training, our inflection system in case n = 1 is exactly the same as in case n >",2 Encoder-Decoder Models for PCFP,[0],[0]
"1.
More precisely, given an input word form, which is a sequence of characters x = x1, ..., xT , the LSTM language model emits a probability p(xt+1,ht,Ext ,Ey) for the next character xt+1 based on the entire previous input sequence x1, ..., xt.",2 Encoder-Decoder Models for PCFP,[0],[0]
"Here ht is the hidden state vector of the language model at position t, E a joint tag and character embedding and y the morphological tag of the input word form.",2 Encoder-Decoder Models for PCFP,[0],[0]
The embedding vector Ey is in fact a sum of sub-tag embeddings.,2 Encoder-Decoder Models for PCFP,[0],[0]
"For example, EPAST+PCPLE denotes EPAST+EPCPLE.",2 Encoder-Decoder Models for PCFP,[0],[0]
This allows us to handle combinations of subtags which we have not seen in the training data.,2 Encoder-Decoder Models for PCFP,[0],[0]
"Guided by the language model, we replace input characters xt+1 during training of the reinflection system with a dropout character $ with probability equal to language model confidence p(xt+1,ht,Ext ,Ey).",2 Encoder-Decoder Models for PCFP,[0],[0]
"4
Baseline Model As a baseline model, we use the neural system presented by Malouf (2016, 2017) for solving PCFP.",2 Encoder-Decoder Models for PCFP,[0],[0]
It is an LSTM generator which is conditioned on the table number of the partial inflection tables and the morphological tag index.,2 Encoder-Decoder Models for PCFP,[0],[0]
The model is trained to generate training word forms in inflection tables.,2 Encoder-Decoder Models for PCFP,[0],[0]
"During testing, it can then generate missing forms by conditioning on morphological tags for the missing forms.
",2 Encoder-Decoder Models for PCFP,[0],[0]
"In order to assure fair comparison, we perform the paradigm completion experiment described in Malouf (2017), where 90% of the word forms in the data set is used for training and the remaining 10% for testing.",2 Encoder-Decoder Models for PCFP,[0],[0]
5,2 Encoder-Decoder Models for PCFP,[0],[0]
"As the results in Table 1 show,
4In practice, we pad input forms with end-of-sequence characters in order to be able to drop x1 if needed.
5We perform the the experiments on the original data sets,
our results very closely replicate those reported by Malouf (2017).
",2 Encoder-Decoder Models for PCFP,[0],[0]
Implementation details,2 Encoder-Decoder Models for PCFP,[0],[0]
"We use 1-layer bidirectional LSTM encoders, decoders and generators with embeddings and hidden states of size 100.",2 Encoder-Decoder Models for PCFP,[0],[0]
We train the language model for case n >,2 Encoder-Decoder Models for PCFP,[0],[0]
1 for 20 epochs and all other models for 60 epochs without batching.,2 Encoder-Decoder Models for PCFP,[0],[0]
We train 10 models for every language and part-of-speech and apply majority voting to get the final output forms.,2 Encoder-Decoder Models for PCFP,[0],[0]
"All models were implemented using DyNet (Neubig et al., 2017).",2 Encoder-Decoder Models for PCFP,[0],[0]
"We use UniMorph morphological paradigm data in our experiments (Kirov et al., 2018).",3 Data,[0],[0]
Unimorph data sets are crowd-sourced collections of morphological inflection tables based on Wiktionary.,3 Data,[0],[0]
"We conduct experiments on noun and verb paradigms from eight languages.6 Not all languages have 1,000 noun and verb tables.",3 Data,[0],[0]
"Hence, our selection is not complete as seen in Table 3.
",3 Data,[0],[0]
"We conduct experiments on two different sets of tables: (1) we randomly sample 1,000 tables for each language and part-of-speech, and (2) we select Unimorph tables including some of the 10,000 most common word forms according to Wikipedia frequency.",3 Data,[0],[0]
"The Wikipedia word frequencies are based on plain Wikipedia text dumps from the Polyglot project (Al-Rfou et al., 2013).",3 Data,[0],[0]
Georgian and Latin did not have a Polyglot Wikipedia,3 Data,[0],[0]
so we excluded those.,3 Data,[0],[0]
"Moreover, we excluded Latvian verbs because there was very little overlap between the most frequent Wikipedia word forms and Unimorph table entries (< 200 forms occurred in both).",3 Data,[0],[0]
"Details for both types of data sets are given in Tables 3 and 2.
",3 Data,[0],[0]
"however, we did not have access to the exact splits into training and test data used by Malouf (2017).",3 Data,[0],[0]
"This may influence results.
",3 Data,[0],[0]
"6Finnish (fin), French (fre), Georgian (geo), German (ger), Latin (lat), Latvian (lav), Spanish (spa) and Turkish (tur).",3 Data,[0],[0]
We perform two experiments.,4 Experiments and Results,[0],[0]
"In the first one, we take the set of 1,000 randomly sampled inflection tables for each language and part-of-speech and then randomly select n=1, 2 or 3 training forms from each table.",4 Experiments and Results,[0],[0]
We then train a reinflection system on these forms and use the resulting system to predict the missing forms.,4 Experiments and Results,[0],[0]
We report accuracy on correctly predicted missing forms and on reconstructing the entire paradigm correctly.,4 Experiments and Results,[0],[0]
"In our second experiment, we consider Unimorph tables which contain entries from a list of 10,000 most common word tokens compiled using a Wikipedia dump of the language as explained above.",4 Experiments and Results,[0],[0]
"We take the forms in the top-10,000 list as given and train a model which is used to reconstruct the remaining forms in each table.",4 Experiments and Results,[0],[0]
We train an identical model as in the case n > 1 on tables with more than one given form.,4 Experiments and Results,[0],[0]
"As in the first task, we evaluate with regard to accuracy for reconstructed forms and full tables.",4 Experiments and Results,[0],[0]
"Results are presented in Tables 4 and 5, and Figure 4.",4 Experiments and Results,[0],[0]
Table 4 shows results for completing tables for common lexemes.,5 Discussion and Conclusions,[0],[0]
"Our system significantly out-
French Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Finnish Nouns
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Finnish Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
German Verbs
.
.
.
1 2 3 4 > 4
French Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Finnish Nouns
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Finnish Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
German Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
German Nouns
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Spanish Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
German
.
.
.
4 4
Spanish Verbs
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Latvian Verbs
.
.
.
",5 Discussion and Conclusions,[0],[0]
"Turkish Nouns
0
0.25
0.5
0.75
1
1 2 3 4 > 4
Latvi
0
0.25
0.5
0.75
1
1 2 3 4
Turk sh",5 Discussion and Conclusions,[0],[0]
"Noun
.
.
.
",5 Discussion and Conclusions,[0],[0]
"Figure 4: Detailed results for filling in missing forms when the 10,000 most frequent forms are given in the inflection tables.",5 Discussion and Conclusions,[0],[0]
The blue bars (on the left) denote accuracy for our system and green bars (on the right) accuracy for the baseline system.,5 Discussion and Conclusions,[0],[0]
"The graphs show accuracy separately for tables where 1, 2, 3, 4, and > 4 forms are given.
performs the baseline on all other datasets apart from German nouns.",5 Discussion and Conclusions,[0],[0]
We believe that the reason for the German outlier is the high degree of syncretism in German noun tables.,5 Discussion and Conclusions,[0],[0]
"To see why syncretism is harmful, consider the German noun Gräben.",5 Discussion and Conclusions,[0],[0]
Its paradigm consists of eight forms but four of those are identical: Gräben.,5 Discussion and Conclusions,[0],[0]
"Only this form is observed among the top 10,000 forms in the German Wikipedia.",5 Discussion and Conclusions,[0],[0]
"Following Section 2, this gives rise to 12 training examples where both the input and output form are Gräben.",5 Discussion and Conclusions,[0],[0]
This strongly biases the system to copying input forms into the output.,5 Discussion and Conclusions,[0],[0]
"However, this will never give the correct output because, by design, missing forms cannot be Gräben.7 This can be seen as a problem with our datasets rather than the model itself.",5 Discussion and Conclusions,[0],[0]
"Consequently, an important future work in addressing the PCFP from an acquisition perspective is to create realistic and accurate data sets that model
7If the same word form occurs in multiple slots, all of them are considered known.
learner exposure both in word types and frequencies to enable assessment of the true difficulty of the PCFP.
",5 Discussion and Conclusions,[0],[0]
There is a notable transition from witnessing one form in each inflection table to witnessing two forms.,5 Discussion and Conclusions,[0],[0]
"With only two forms given, we already approach accuracies reported in earlier work (Malouf, 2016, 2017) that used almost complete tables to train—only 10% of the forms were missing.",5 Discussion and Conclusions,[0],[0]
"Additionally, our encoder-decoder model strongly outperforms that generator model designed for the same task with the same amount of training data on nearly all of our datasets.",5 Discussion and Conclusions,[0],[0]
The first author was supported by The Society of Swedish Literature in Finland (SLS).,Acknowledgements,[0],[0]
NVIDIA Corp. donated the Titan Xp GPU used for this research.,Acknowledgements,[0],[0]
The Paradigm Cell Filling Problem in morphology asks to complete word inflection tables from partial ones.,abstractText,[0],[0]
"We implement novel neural models for this task, evaluating them on 18 data sets in 8 languages, showing performance that is comparable with previous work with far less training data.",abstractText,[0],[0]
We also publish a new dataset for this task and code implementing the system described in this paper.1,abstractText,[0],[0]
An Encoder-Decoder Approach to the Paradigm Cell Filling Problem,title,[0],[0]
"This paper presents a novel framework that enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model.",1. Introduction,[0],[0]
"The Rasch model was proposed in 1960 for modeling the responses of students/users to test/survey items (Rasch, 1960), and has enjoyed great success in applications including (but not limited to) psychometrics (van der Linden & Hambleton, 2013), educational tests (Lan et al., 2016), crowdsourcing (Whitehill et al., 2009), public health (Cappelleri et al., 2014), and even market and financial research (Schellhorn & Sharma, 2013; Brzezińska, 2016).",1. Introduction,[0],[0]
"Mathematically, the (dichotomous) Rasch model, also known as the 1PL item
1Department of Electrical Engineering, Princeton University 2Purdue University 3School of Electrical and Computer Engineering, Cornell University.",1. Introduction,[0],[0]
"Correspondence to: Andrew S. Lan <andrew.lan@princeton.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"response theory (IRT) model (Lord, 1980), is given by
p(Yu,i = 1) =",1. Introduction,[0],[0]
"Φ(au − di), (1)
where Yu,i ∈ {−1,+1} denotes the response of user u to item",1. Introduction,[0],[0]
"i, where +1 stands for a correct response and−1 stands for an incorrect response.",1. Introduction,[0],[0]
"The parameters au ∈ R model the scalar abilities of users u = 1, . . .",1. Introduction,[0],[0]
", U and the parameters di ∈ R model the scalar difficulties of items i = 1, . . .",1. Introduction,[0],[0]
", Q. The function Φ(x) = ∫ x −∞N (t; 0, 1)dt, often referred to as the inverse probit link function1, is the cumulative distribution function of a standard normal random variable, where N (t; 0, 1) denotes the probability density function of a standard normal random variable evaluated at t.
The literature describes a range of parameter estimation methods under the Rasch model and related IRT models; see (Baker & Kim, 2004) for an overview.",1. Introduction,[0],[0]
"However, existing analytical results for the associated parameter estimation error are limited; see (Tsutakawa & Johnson, 1990) for an example.",1. Introduction,[0],[0]
"The majority of existing results have been proposed in the psychometrics and educational measurement literature; see, e.g., (Carroll et al., 2006) for a survey.",1. Introduction,[0],[0]
"The proposed analysis tools rely, for example, on multiple imputation (Yang et al., 2012) or Markov chain Monte Carlo (MCMC) techniques (Patz & Junker, 1999), and are thus not analytical.",1. Introduction,[0],[0]
"Hence, their accuracy strongly depends on the available data.
",1. Introduction,[0],[0]
"Other analysis tools use the Fisher information matrix (Zhang et al., 2011; Yang et al., 2012) to obtain lower bounds on the estimation error.",1. Introduction,[0],[0]
"Such methods are of asymptotic nature, i.e., they yield accurate results only when the number of users and items tend to infinity.",1. Introduction,[0],[0]
"For real-world settings with limited data, these bounds are typically loose; As an example, in computerized adaptive testing (CAT) (Chang & Ying, 2009), a user enters the system and starts responding to items.",1. Introduction,[0],[0]
"The system maintains an estimate of their ability parameter, and adaptively selects the next-best item to assign to the user that is most informative of the ability estimate.",1. Introduction,[0],[0]
"Calculating the informativeness of each item requires an analysis of the uncertainty in the ability
1While some publications assume the inverse logit link function, i.e., the sigmoid Φ(x) = 1
1+e−x , in most real-world applications the choice of the link function has no significant performance impact.",1. Introduction,[0],[0]
"In what follows, we will focus on the inverse probit link function for reasons that will be discussed in Section 3.
estimate.",1. Introduction,[0],[0]
"Initially, after the user has only responded to a few items, these asymptotic methods lead to highly inaccurate analyses, which may lead to poor item selections.
",1. Introduction,[0],[0]
"Another family of analysis tools relies on concentration inequalities and yield probabilistic bounds, i.e., bounds that hold with high probability (Bunea, 2008; Filippi et al., 2010).",1. Introduction,[0],[0]
Such results are often impractical in real-world applications.,1. Introduction,[0],[0]
"However, an exact analysis of the estimation error of the Rasch model is critical to ensure the a certain degree of reliability of assessment scores in tests (Thompson, 2002).",1. Introduction,[0],[0]
"We propose a novel framework for the Rasch model that enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error.",1.1. Contributions,[0],[0]
"To this end, we generalize a recently-proposed linear estimator for binary regression (Lan et al., 2018) to the Rasch model, which enables us to derive a sharp upper bound on the mean squared error (MSE) of model parameter estimates.",1.1. Contributions,[0],[0]
"Our analytical results are in stark contrast to existing analytical results which either provide loose lower bounds or are asymptotic in nature, rendering them impractical in real-world applications.
",1.1. Contributions,[0],[0]
"To demonstrate the efficacy of our framework, we provide experimental results on both synthetic and real-world data.",1.1. Contributions,[0],[0]
"First, using synthetic data, we show that our upper bound on the MSE is (often significantly) tighter than the Fisher information-based lower bound, especially when the problem size is small and when the data is noisy.",1.1. Contributions,[0],[0]
"Therefore, our framework enables a more accurate analysis of the estimation error in real-world settings.",1.1. Contributions,[0],[0]
"Second, using real-world student question response and user movie rating datasets, we show that our linear estimator achieves competitive predictive performance to more sophisticated, nonlinear estimators for which sharp performance guarantees are unavailable.",1.1. Contributions,[0],[0]
"The Rasch model in (1) can be written in equivalent matrixvector form as follows (Hoff, 2009):
y = sign(Dx + w).",2. Rasch Model and Probit Regression,[0],[0]
"(2)
Here, the UQ-dimensional vector y ∈ {−1,+1}UQ contains all user responses to all items, the Rasch model matrix D =",2. Rasch Model and Probit Regression,[0],[0]
"[1Q⊗ IU×U , IQ×Q⊗1U ] is constructed with the Kronecker product operator ⊗, identity matrices I, all-ones vectors 1, and the vector xT =",2. Rasch Model and Probit Regression,[0],[0]
"[aT ,−dT ] to be estimated consists of the user abilities a ∈ RU and item difficulties d ∈ RQ.",2. Rasch Model and Probit Regression,[0],[0]
The “noise” vector w contains i.i.d.,2. Rasch Model and Probit Regression,[0],[0]
standard normal random variables.,2. Rasch Model and Probit Regression,[0],[0]
"In this equivalent form, parameter estimation under the Rasch model can be casted as a probit regression problem (Bliss, 1935), for which numerous estimators have been proposed in the past.",2. Rasch Model and Probit Regression,[0],[0]
"The two most prominent estimators for probit regression are the posterior mean (PM) estimator, given by
x̂PM = Ex[x|y] = ∫ RN xp(x|y)dx, (3)
and the maximum a-posteriori (MAP) estimator, given by
x̂MAP = arg min x∈RN
− ∑M m=1 log(Φ(ymd T mx))",2.1. Estimators for Probit Regression,[0],[0]
"+ 1 2x TC−1x x.
Here, p(x|y) denotes the posterior probability of the vector x given the observations y under the model (2), dTm denotes the mth row of the matrix of covariates D, and Cx denotes the covariance matrix of the multivariate Gaussian prior on x. A special case of the MAP estimator is the wellknown maximum likelihood (ML) estimator, which does not impose a prior distribution on x.
The PM estimator is optimal in terms of minimizing the MSE of the estimated parameters, which is defined as
MSE(x̂) = Ex,w",2.1. Estimators for Probit Regression,[0],[0]
[ ‖x− x̂‖2 ] .,2.1. Estimators for Probit Regression,[0],[0]
"(4)
However, there are no simple methods to evaluate the expectation in (3) under the probit model.",2.1. Estimators for Probit Regression,[0],[0]
"Thus, one typically resorts to Markov chain Monte Carlo (MCMC) methods (Albert & Chib, 1993) to perform PM estimation, which can be computationally intensive.",2.1. Estimators for Probit Regression,[0],[0]
"In contrast to the PM estimator, MAP and ML estimation is generally less complex since it can be implemented using standard convex optimization algorithms (Nocedal & Wright, 2006; Hastie et al., 2010; Goldstein et al., 2014).",2.1. Estimators for Probit Regression,[0],[0]
"On the flipside, MAP and ML estimation is not optimal in terms of minimizing the MSE in (4).",2.1. Estimators for Probit Regression,[0],[0]
"In contrast to such well-established, nonlinear estimators, we build our framework on the family of linear estimators recently proposed in (Lan et al., 2018).",2.1. Estimators for Probit Regression,[0],[0]
"There, a linear minimum MSE (L-MMSE) estimator was proposed for a certain class of probit regression problems.",2.1. Estimators for Probit Regression,[0],[0]
"This L-MMSE estimator was found to perform on par with the PM estimator and outperforms the MAP estimator in terms of the MSE for certain settings, while enabling an exact and nonasymptotic analysis of the MSE.",2.1. Estimators for Probit Regression,[0],[0]
"In the statistical estimation literature, there exists numerous analytical results characterizing the estimation errors for binary regression problems in the asymptotic setting.",2.2. Analytical Performance Guarantees,[0],[0]
"For example, (Brillinger, 1982) shows that least squares estimation is particularly effective when the design matrix D has i.i.d.",2.2. Analytical Performance Guarantees,[0],[0]
"Gaussian entries and the number of observations approaches infinity; in this case, its performance was shown to differ from that of the PM estimator only by a constant factor.",2.2. Analytical Performance Guarantees,[0],[0]
"Recently, (Thrampoulidis et al., 2015) provides a related analysis in the case that the parameter vector x is
sparse.",2.2. Analytical Performance Guarantees,[0],[0]
"Another family of probabilistic results relies on the asymptotic normality property of ML estimators, either in the standard (dense) setting (Gourieroux & Monfort, 1981; Fahrmeir & Kaufmann, 1985) or the sparse setting (Bunea, 2008; Bach, 2010; Ravikumar et al., 2010; Plan & Vershynin, 2013), providing bounds on the MSE with high probability.",2.2. Analytical Performance Guarantees,[0],[0]
"Since numerous real-world applications, such as the Rasch model, rely on deterministic, structured matrices and have small problem dimensions, existing analytical performance bounds are often loose; see Section 4 for experiments that support this claim.",2.2. Analytical Performance Guarantees,[0],[0]
"Our main result is as follows; the proof is given in Appendix A.
Theorem 1.",3. Main Results,[0],[0]
"Assume that x ∼ N (x̄,Cx) with mean vector x̄ and positive definite covariance matrix Cx, and assume that the vector w contains i.i.d.",3. Main Results,[0],[0]
standard normal random variables.,3. Main Results,[0],[0]
"Consider the general probit regression model
y = sign(Dx + m + w), (5)
where D is a given matrix of covariates and m is a given bias vector.",3. Main Results,[0],[0]
"Then, the L-MMSE estimate is given by
x̂L-MMSE = ETC−1y y + b,
where we use the following quantities:
E=2diag(N (c; 0,1)",3. Main Results,[0],[0]
"diag(Cz)− 1 2 )DCx
c = z̄ diag(Cz)−1/2
z̄ = Dx̄ + m
",3. Main Results,[0],[0]
Cz = DCxD T +,3. Main Results,[0],[0]
"I
Cy = 2(Φ2(c1 T ,1cT ;R) + Φ2(−c1T ,−1cT ;R))
",3. Main Results,[0],[0]
"− 1M×M − ȳȳT
R = diag(diag(Cz) −1/2)Czdiag(diag(Cz) −1/2)
ȳ=Φ(c)− Φ(−c) b=",3. Main Results,[0],[0]
"x̄−ETC−1y ȳ.
Here, Φ2(x, y, ρ) denotes the cumulative density of a twodimensional zero-mean Gaussian distribution with covariance matrix [1 ρ; ρ 1] with ρ ∈",3. Main Results,[0],[0]
"[0, 1), defined as
Φ2(x, y; ρ) = ∫ x −∞ ∫ y −∞
1 2π √ 1− ρ2 e",3. Main Results,[0],[0]
− s 2−2ρst+t2 2(1−ρ2),3. Main Results,[0],[0]
"dtds
and is applied element-wise on matrices.",3. Main Results,[0],[0]
"Furthermore, the associated estimation MSE is given by
MSE(x̂L-MMSE) =",3. Main Results,[0],[0]
"tr(Cx −ETC−1y E).
",3. Main Results,[0],[0]
"We note that the linear estimator developed in (Lan et al., 2018, Thm. 1) is a special case of our result with x̄ = 0 and
m = 0.",3. Main Results,[0],[0]
"As we will show below, including both of these terms will be essential for our analysis.
",3. Main Results,[0],[0]
Remark 1.,3. Main Results,[0],[0]
We exclusively focus on probit regression since the matrices E and Cy exhibit tractable expressions under this model.,3. Main Results,[0],[0]
"We are unaware of any closed-form expressions for these quantities in the logistic regression case.
",3. Main Results,[0],[0]
"As an immediate consequence of the fact that the PM estimator minimizes the MSE, we can use Theorem 1 to obtain the following upper bound on the MSE of the PM estimator.
",3. Main Results,[0],[0]
Corollary 2.,3. Main Results,[0],[0]
"The MSE of the PM estimator is upperbounded as follows:
MSE(xPM)",3. Main Results,[0],[0]
≤ MSE(x̂L-MMSE).,3. Main Results,[0],[0]
"(6)
As we will demonstrate in Section 4, this upper bound on the MSE turns out to be surprisingly sharp for a broad range of parameters and problem settings.
",3. Main Results,[0],[0]
We now specialize Theorem 1 for the Rasch model and use Corollary 2 to analyze the associated MSE.,3. Main Results,[0],[0]
We divide our results into two cases: (i) both the user abilities and item difficulties are unknown and (ii) one of the two sets of parameters is known and the other is unknown.,3. Main Results,[0],[0]
"Due to symmetry in the Rasch model, we will present our results with unknown/known item difficulties while the user abilities are unknown and to be estimated; a corresponding analysis on the estimation error of item parameters follows immediately.",3. Main Results,[0],[0]
We now analyze the case in which both the user abilities and item difficulties are unknown and need to be estimated.,3.1. First Case: Unknown Item Parameters,[0],[0]
"In practice, this scenario is relevant if a new set of items are deployed with little or no prior knowledge on their difficulty parameters.",3.1. First Case: Unknown Item Parameters,[0],[0]
"We assume that there is no missing data, i.e., we observe all user responses to all items.2 In the psychometrics literature (see, e.g., (Linacre, 1999)), one typically assumes that the entries of the ability a and difficulty vectors d are i.i.d. zero-mean Gaussian with variance σ2a and σ2d, respectively, i.e., au ∼ N (0, σ2a) and di ∼ N (0, σ2d), which can be included in our model assumptions.",3.1. First Case: Unknown Item Parameters,[0],[0]
"Thus, we can leverage the special structure of the Rasch model, since it corresponds to a special case of the generic probit regression model in (5) with D =",3.1. First Case: Unknown Item Parameters,[0],[0]
"[1Q⊗ IU×U , IQ×Q⊗1U ] and m = 0.",3.1. First Case: Unknown Item Parameters,[0],[0]
"We have the following result on the MSE of the L-MMSE estimator; the proof is given in Appendix B.
Theorem 3.",3.1. First Case: Unknown Item Parameters,[0],[0]
Assume that σ2a = σ2d = σ2x and the covariance matrix of x is Cx = σ2xI(U+Q)×(U+Q).,3.1. First Case: Unknown Item Parameters,[0],[0]
"Let
s = 2
π arcsin
( σ2x
2σ2x + 1
) .
",3.1. First Case: Unknown Item Parameters,[0],[0]
"2Our analysis can readily be generalized to missing data; the results, however, depend on the missing data pattern.
",3.1. First Case: Unknown Item Parameters,[0],[0]
"Then, the MSE of the L-MMSE estimator of user abilities under the Rasch model is given by
MSEa = Ex,w [ (au − âu)2 ] =
σ2x
( 1− 2
π σ2x 2σ2x + 1 sQ(Q+ U − 3) + 1 (s(Q− 2) + 1)(s(Q+ U − 2) + 1)
) .
(7)
To the best of our knowledge, Theorem 3 is the first exact, nonasymptotic, and closed-form analysis of the MSE of a parameter estimation method for the Rasch model.",3.1. First Case: Unknown Item Parameters,[0],[0]
"From (7), we see that if σ2x is held constant, then the relationship between MSEa and the numbers of users (U ) and items (Q) is given by the ratio of two second-order polynomials.",3.1. First Case: Unknown Item Parameters,[0],[0]
"If the signal-to-noise ratio (SNR) is low (or, equivalently, the data is noisy), i.e., σ2x σ2n, then we have σ2x 2σ2x+1 ≈ 0 and hence, s = 2π arcsin( σ2x 2σ2x+1 )",3.1. First Case: Unknown Item Parameters,[0],[0]
≈ 0.,3.1. First Case: Unknown Item Parameters,[0],[0]
"In this case, we have MSEa ≈ σ2x, i.e., increasing the number of users/items does not affect the accuracy of the ability and difficulty parameters of users and items; this behavior is as expected.
",3.1. First Case: Unknown Item Parameters,[0],[0]
"When U,Q→∞, the MSE satisfies MSEa → σ2x ( 1− σ 2 x
2σ2x + 1 arcsin−1
( σ2x
2σ2x + 1
))",3.1. First Case: Unknown Item Parameters,[0],[0]
", (8)
which is a non-negative quantity.",3.1. First Case: Unknown Item Parameters,[0],[0]
This result implies that the L-MMSE estimator has a residual MSE even as the number of users/items grows large.,3.1. First Case: Unknown Item Parameters,[0],[0]
"More specifically, since x ≤ arcsin(x) for x ∈",3.1. First Case: Unknown Item Parameters,[0],[0]
"[0, 1], this residual error approaches σ2x(1 − 3π ) at high values of SNR.",3.1. First Case: Unknown Item Parameters,[0],[0]
"We note, however, this result does not imply that the L-MMSE estimator is not consistent under the Rasch model, since the number of parameters to be estimated (U +Q) grows with the number of the observations (UQ) instead of remaining constant.
",3.1. First Case: Unknown Item Parameters,[0],[0]
Remark 2.,3.1. First Case: Unknown Item Parameters,[0],[0]
"The above MSE analysis is data-independent, in contrast to error estimates that rely on the responses y (which is, for example, the case for method in (Carroll et al., 2006)).",3.1. First Case: Unknown Item Parameters,[0],[0]
This fact implies that our result provides an error estimate before observing y.,3.1. First Case: Unknown Item Parameters,[0],[0]
"Thus, Theorem 3 provides guidelines on how many items to include and how many users to recruit for a study, given a desired MSE level on the user ability and item difficulty parameter estimates.",3.1. First Case: Unknown Item Parameters,[0],[0]
We now analyze the case in which the user abilities are unknown and need to be estimated; the item difficulties (d) are given.,3.2. Second Case: Known Item Difficulties,[0],[0]
"In practice, this scenario is relevant if a large number of users previously responded to a set of items so that a good estimate of the item difficulties is available.",3.2. Second Case: Known Item Difficulties,[0],[0]
Let a denote the scalar ability parameter of an user.,3.2. Second Case: Known Item Difficulties,[0],[0]
"Then, their responses to items are modeled as
p(y = 1) = Φ(1Qa− d).
",3.2. Second Case: Known Item Difficulties,[0],[0]
"The following result follows from Theorem 1 by setting x = a, x̄ = x̄, Cx = σ2x, D = 1Q, and m = −d.",3.2. Second Case: Known Item Difficulties,[0],[0]
Corollary 4.,3.2. Second Case: Known Item Difficulties,[0],[0]
"Assume that a ∼ N (x̄, σ2x).",3.2. Second Case: Known Item Difficulties,[0],[0]
"Then, the LMMSE estimate of user ability is given by
â = eTC−1y y + b,
where
e=2 σ2x√ σ2x + 1 N",3.2. Second Case: Known Item Difficulties,[0],[0]
"(c; 0, 1) c = z̄ diag(Cz)−1/2
z̄ =",3.2. Second Case: Known Item Difficulties,[0],[0]
x̄1Q,3.2. Second Case: Known Item Difficulties,[0],[0]
− d Cz = σ 2 x1Q×Q +,3.2. Second Case: Known Item Difficulties,[0],[0]
"I
ȳ = Φ(c)− Φ(−c)",3.2. Second Case: Known Item Difficulties,[0],[0]
"Cy = 2(Φ2(c1
T,1cT ,R) + Φ2(−c1T ,−1cT ,R))",3.2. Second Case: Known Item Difficulties,[0],[0]
"− 1M×M − ȳȳT
R = diag(diag(Cz) −1/2)Czdiag(diag(Cz) −1/2.
",3.2. Second Case: Known Item Difficulties,[0],[0]
The MSE of the user ability estimate is given by MSE(â) =,3.2. Second Case: Known Item Difficulties,[0],[0]
σ2x − eTC−1y e.,3.2. Second Case: Known Item Difficulties,[0],[0]
We now experimentally demonstrate the efficacy of the proposed framework.,4. Numerical Results,[0],[0]
"First, we use synthetically generated data to numerically compare our L-MMSE-based upper bound on the MSE of the PM estimator to the widely-used lower bound based on Fisher information (Zhang et al., 2011; Yang et al., 2012).",4. Numerical Results,[0],[0]
We then use several real-world collaborative filtering datasets to show that the L-MMSE estimator achieves comparable predictive performance to that of the PM and MAP estimators.,4. Numerical Results,[0],[0]
We start with synthetic data to demonstrate the exact and nonasymptotic nature of our analytical MSE expressions.,4.1. Experiments with Synthetic Data,[0],[0]
"Experimental Setup We vary the number of users U ∈ {20, 50, 100} and the number of items Q ∈ {20, 50, 100, 200}.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
We generate the user ability and item difficulty parameters from zero-mean Gaussian distributions with variance σ2x = σ 2 a = σ 2 d.,4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
We vary σ 2 x,4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"so that the signalto-noise ratio (SNR) corresponds to {−10, 0, 10} decibels (dB).",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"We then randomly generate the response from each user to each item, Yu,i, according to (1).",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"We repeat these experiments for 1, 000 random instances of user and item parameters and responses, and report the averaged results.
",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"We compute the L-MMSE-based upper bound on the MSE of the PM estimator using Theorem 1 and the Fisher
information-based lower bound using the method detailed in (Zhang et al., 2011; Yang et al., 2012).",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"Since the calculation of the Fisher information matrix requires the true values of the user ability and item difficulty parameters (which are to be estimated in practice), we use the PM estimates of these parameters instead.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
We also calculate the empirical parameter estimation MSEs of the L-MMSE and PM estimators.,4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"To this end, we use a standard Gibbs sampling procedure (Albert & Chib, 1993); we use the mean of the generated samples over 20, 000 iterations as the PM estimate after a burn-in phase of 10, 000 iterations.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"We then use these estimates to calculate the empirical MSE.
Results and Discussion Fig. 1 shows the empirical MSEs of the L-MMSE and PM estimators, together with the L-MMSE-based upper bound and the Fisher informationbased lower bound on the MSE of the PM estimator, for every problem size and every SNR.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"First, we see that the analytical and empirical MSEs of the L-MMSE estimator match perfectly, which confirms that our analytical MSE expressions are exact.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"We also see that for low SNR (i.e., the first row of Fig. 1), our L-MMSE upper bound on the MSE of the PM estimator is tight.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"Moreover, at all noise levels,
the L-MMSE-based upper bound is tighter at small problem sizes, while the Fisher information-based lower bound is tighter at very large problem sizes and at high SNR.
",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"These results confirm that our L-MMSE-based upper bound on the MSE is nonasymptotic, while the Fisher informationbased lower bound is asymptotic and thus only tight at very large problem sizes.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"Therefore, the L-MMSE-based upper bound is more practical than the Fisher informationbased lower bound in real-world applications, especially for situations like the initial phase of CAT when the number of items a user has responded to is small.",4.1.1. FIRST CASE: UNKNOWN ITEM PARAMETERS,[0],[0]
"Experimental Setup In this experiment, we randomly generate the item parameters from the standard normal distribution (σ2d = 1) and treat these parameters as known; we then estimate the user ability parameters via Theorem 4.",4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
"The rest of the experimental setup remains unchanged.
",4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
"Results and Discussion Fig. 2 shows the empirical MSEs of the L-MMSE and PM estimators, together with the L-MMSE-based upper bound and the Fisher information-
based lower bound on the MSE of the PM estimator, for every problem size and every SNR.",4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
We see that the analytical and empirical MSEs of the L-MMSE estimator match.,4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
"We also see that the L-MMSE-based upper bound on the MSE is tighter than the Fisher information-based lower bound at low SNR levels (−10 dB and 1 dB), and especially when the problem size is small (less than 50 items).",4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
"These results further confirm that our L-MMSE-based upper bound on the MSE is nonasymptotic, and is thus practical in the “cold-start” setting of recommender systems.",4.1.2. CASE TWO: KNOWN ITEM PARAMETERS,[0],[0]
We now test the performance of the proposed L-MMSE estimator using a variety of real-world datasets.,4.2. Experiments with Real-World Data,[0],[0]
"Since the noise model in real-world datasets is generally unknown, we also consider the performance of MAP estimation using the inverse logit link function (Logit-MAP).
",4.2. Experiments with Real-World Data,[0],[0]
Datasets We perform our experiments using a range of collaborative filtering datasets.,4.2. Experiments with Real-World Data,[0],[0]
These datasets are matrices that contain binary-valued ratings (or graded responses) of users (or students) to movies (or items).,4.2. Experiments with Real-World Data,[0],[0]
"For these datasets, we use the probit Rasch model.",4.2. Experiments with Real-World Data,[0],[0]
"The datasets include (i) “MT”, which consists of students’ binary-valued (correct/incorrect) graded responses to questions in a highschool algebra test, with U = 99 students’ 3, 366 responses to Q = 34 questions, (ii) “SS”, which consists of student responses in a signals and systems course, with U = 92 students’ 5, 692 responses to Q = 203 questions, (iii) “edX”, which consists of student responses in an edX course, with U = 3241 students’ 177, 181 responses to Q = 191 questions, and (iv) “ML”, a processed version of the ml-100k dataset from the Movielens project (Herlocker et al., 1999), with 37, 175 integer-valued ratings by U = 943 users to Q = 1152 movies.",4.2. Experiments with Real-World Data,[0],[0]
"We adopt the procedure used in (Davenport et al., 2014) to transform the dataset into binary values by comparing each rating to the overall average rating.
",4.2. Experiments with Real-World Data,[0],[0]
"Experimental Setup We evaluate the prediction performance of the L-MMSE, MAP, PM, and Logit-MAP estimators using ten-fold cross validation.",4.2. Experiments with Real-World Data,[0],[0]
"We randomly divide the
entire dataset into ten equally-partitioned folds (of user-item response pairs), leave out one fold as the held-out testing set and use the other folds as the training set.",4.2. Experiments with Real-World Data,[0],[0]
"We then use the training set to estimate the learner abilities au and item difficulties di, and use these estimates to predict user responses on the test set.",4.2. Experiments with Real-World Data,[0],[0]
We tune the prior variance parameter σ2x using a separate validation set (one fold in the training set).,4.2. Experiments with Real-World Data,[0],[0]
"To assess the performance of these estimators, we use two common metrics in binary classification problems: prediction accuracy (ACC), which is simply the portion of correct predictions, and area under the receiver operating characteristic curve (AUC) (Jin & Ling, 2005).",4.2. Experiments with Real-World Data,[0],[0]
"Both metrics have range in [0, 1], with larger values indicating better predictive performance.
",4.2. Experiments with Real-World Data,[0],[0]
Results and Discussion Tables 1 and 2 show the mean and standard deviation of the performance of each estimator on both metrics across each fold.,4.2. Experiments with Real-World Data,[0],[0]
"We observe that the performance of the considered estimators are comparable on the ACC metric, while the L-MMSE estimator performs slightly worse than the MAP, PM, and Logit-MAP estimators for most datasets on the AUC metric.
",4.2. Experiments with Real-World Data,[0],[0]
We find it quite surprising that a well-designed linear estimator performs on par with more sophisticated nonlinear estimators on these real-world datasets.,4.2. Experiments with Real-World Data,[0],[0]
We also note that the L-MMSE estimator is more computationally efficient than the PM estimator.,4.2. Experiments with Real-World Data,[0],[0]
"As an example, on the MT and ML datasets, one run of the L-MMSE estimator takes 0.23s and 79s, respectively, while one run of the PM estimator takes 1.9s and 528s (2, 000 and 10, 000 iterations required for convergence) on a standard laptop computer.",4.2. Experiments with Real-World Data,[0],[0]
These observations suggest that the L-MMSE estimator is computationally efficient and thus scales favorably to large datasets.,4.2. Experiments with Real-World Data,[0],[0]
We have generalized a recently proposed linear estimator for probit regression and applied the method to the classic Rasch model in item response analysis.,5. Conclusions,[0],[0]
"We have shown that the L-MMSE estimator enables an exact, closed-form, and nonasymptotic MSE analysis, which is in stark contrast to existing analytical results which are asymptotic, probabilis-
L-MMSE MAP PM Logit-MAP
MT 0.840± 0.016 0.843± 0.015 0.843± 0.015 0.842± 0.015 SS 0.800± 0.014 0.803± 0.013 0.803± 0.013 0.802± 0.013 edX 0.900± 0.004 0.909± 0.004 0.909± 0.004 0.909± 0.004",5. Conclusions,[0],[0]
"ML 0.755± 0.005 0.756± 0.004 0.756± 0.004 0.756± 0.004
tic, or loose.",5. Conclusions,[0],[0]
"As a result, we have shown that the nonasymptotic, L-MMSE-based upper bound on the parameter estimation error of the PM estimator under the Rasch model can be tighter than the common Fisher information-based asymptotic lower bound, especially in practical settings.",5. Conclusions,[0],[0]
"An avenue of future work is to apply our analysis to models that are more sophisticated than the Rasch model, e.g., the latent factor model in (Lan et al., 2014).",5. Conclusions,[0],[0]
Let z = Dx+m+w.,A. Proof of Theorem 4,[0],[0]
"Thus, z ∼ N (Dx̄+m,DCxDT + I) := N (z̄,Cz).",A. Proof of Theorem 4,[0],[0]
"The L-MMSE estimator for x has the general form of x̂L-MMSE = Wy + b, where W = EC−1y and b = x̄−Wȳ, with
Cy =E [ (y−ȳ)(y−ȳ)T ]",A. Proof of Theorem 4,[0],[0]
=E [ yyT ] −ȳȳT,A. Proof of Theorem 4,[0],[0]
:,A. Proof of Theorem 4,[0],[0]
"=C̃y−ȳȳT
and E = E [ (y − ȳ)(x− x̄)T ]",A. Proof of Theorem 4,[0],[0]
=E [ yxT ] −ȳx̄T :,A. Proof of Theorem 4,[0],[0]
"=Ẽ−ȳx̄T .
",A. Proof of Theorem 4,[0],[0]
"We need to evaluate three quantities, ȳ, C̃y, and Ẽ.
We start with ȳ.",A. Proof of Theorem 4,[0],[0]
"Its ith entry is given by
ȳi = ∫ ∞ −∞ sign(zi)N (zi; z̄i, [Cz]i,i)dzi
=− ∫ 0 −∞ N (zi; z̄i, [Cz]i,i)dzi+ ∫ ∞ 0 N (zi; z̄i, [Cz]i,i)dzi
= Φ ( z̄i√
[Cz]i,i
)",A. Proof of Theorem 4,[0],[0]
"− Φ ( − z̄i√
[Cz]i,i
) .
",A. Proof of Theorem 4,[0],[0]
"Next, we calculate C̃y.",A. Proof of Theorem 4,[0],[0]
"Its (i, j)th entry is given by [C̃y]i,j =∫ ∞ −∞ ∫ ∞ −∞ sign(zi) sign(zj)N ([ zi zj
] ;[ z̄i
z̄j
] , [ [Cz]i,i [Cz]i,j
[Cz]j,i [Cz]j,j
]) dzjdzi
(a) = ∫ ∞ −∞ ∫ ∞ −∞ sign ( zi + z̄i√",A. Proof of Theorem 4,[0],[0]
"[Cz]i,i ) sign ( zj + z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j )
N ([ zi
zj
] ;0, [ 1 ρ ρ 1 ]) dzjdzi
=
∫",A. Proof of Theorem 4,[0],[0]
− z̄i√,A. Proof of Theorem 4,[0],[0]
"[Cz]i,i
−∞
∫ − z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j −∞ N ([ zi zj ] ;0, [ 1 ρ ρ 1 ]) dzjdzi︸ ︷︷ ︸
v1
+ ∫ ∞",A. Proof of Theorem 4,[0],[0]
"− z̄i√
[Cz]i,i
∫ ∞ −
z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j
N ([
zi zj
] ;0, [
1 ρ ρ 1 ]) dzjdzi︸ ︷︷ ︸
v2 − ∫",A. Proof of Theorem 4,[0],[0]
− z̄i√,A. Proof of Theorem 4,[0],[0]
"[Cz]i,i
−∞
∫ ∞ −
z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j
N ([ zi
zj
] ;0, [ 1 ρ ρ 1 ]) dzjdzi︸ ︷︷ ︸
v3 − ∫ ∞",A. Proof of Theorem 4,[0],[0]
"− z̄i√
[Cz]i,i
∫ − z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j −∞ N ([ zi zj ] ;0, [ 1 ρ ρ 1 ]) dzjdzi︸ ︷︷ ︸
v4
(b) = 2(v1 + v2)− 1
= 2 ( Φ2 ( z̄i√
[Cz]i,i , z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j , ρ
)
+ Φ2
( − z̄i√
[Cz]i,i ,− z̄j√",A. Proof of Theorem 4,[0],[0]
"[Cz]j,j , ρ
))",A. Proof of Theorem 4,[0],[0]
"− 1,
where we have used (a) change of variable zi−z̄i√",A. Proof of Theorem 4,[0],[0]
"[Cz]i,i → zi and (b) the fact that v1 +v2 +v3 +v4 = 1.",A. Proof of Theorem 4,[0],[0]
"The computation of Ẽ follows from that in (Lan et al., 2018) and is omitted.",A. Proof of Theorem 4,[0],[0]
"Recall that the expression for the MSE is tr(Cx − ETC−1y E), the critical part is to evaluate E
TC−1y E. We begin by evaluating C−1y .",B. Proof of Theorem 3,[0],[0]
"For the Rasch model, we have
D =",B. Proof of Theorem 3,[0],[0]
"[1Q⊗ IU×U , IQ×Q⊗1U ].",B. Proof of Theorem 3,[0],[0]
"Therefore, since Cx = σ2xIU+Q, we have
Cz = DCxD T + IUQ×UQ = σ",B. Proof of Theorem 3,[0],[0]
2,B. Proof of Theorem 3,[0],[0]
xDD T + IUQ×UQ = σ2x[1Q⊗ IU×U IQ×Q⊗1U ],B. Proof of Theorem 3,[0],[0]
"[
1TQ⊗ IU×U IQ×Q⊗1TU ] + IUQ×UQ
= σ2x(1Q×Q⊗ IU×U",B. Proof of Theorem 3,[0],[0]
+ IQ×Q⊗1U×U ),B. Proof of Theorem 3,[0],[0]
+,B. Proof of Theorem 3,[0],[0]
"IUQ×UQ,
where 1U×U denotes an all-one matrix with size U × U .",B. Proof of Theorem 3,[0],[0]
"Therefore, we see that the UQ× UQ matrix Cz consists of three parts: (i) Q copies of the all-ones matrix σ2x1U×U in its diagonal U ×U blocks, (ii) copies of the matrix σ2xIU×U in every other off-diagonal U×U block, plus (iii) a diagonal matrix IUQ×UQ.",B. Proof of Theorem 3,[0],[0]
"Therefore, its diagonal elements are 2σ2x + 1 and its non-zero off-diagonal elements are σ2x.
",B. Proof of Theorem 3,[0],[0]
"As detailed in (Lan et al., 2018, (7)), one can show that
Cy = 2
π arcsin(diag(diag(Cz)
−1/2)Cz
× diag(diag(Cz)−1/2)),
we have that the term inside the arcsin function has the same structure as Cz, with diagonal entries of 1 and nonzero off-diagonal entries as σ 2 x
2σ2x+1 .",B. Proof of Theorem 3,[0],[0]
"Therefore, Cy also has
the same structure, with diagonal entries of 1 and non-zero off-diagonal entries as
s = 2
π arcsin
( σ2x
2σ2x + 1
) .
",B. Proof of Theorem 3,[0],[0]
Since C−1y satisfies CyC −1,B. Proof of Theorem 3,[0],[0]
"y = IUQ×UQ, it is easy to see that the entries of C−1y only contain three distinct values (denoted by a, b, and c), and consists of two parts: (i) Q copies of a U × U matrix with a on its diagonal, b everywhere else, in its diagonal blocks, and (ii) copies of a U ×U matrix with c on its diagonal, d everywhere else, in its other blocks.",B. Proof of Theorem 3,[0],[0]
"We next compute a, b, c, and d.
The first column of C−1y is given by
[a, b11×U−1, c, d11×U−1, c, d11×U−1, . . .]",B. Proof of Theorem 3,[0],[0]
"T .
",B. Proof of Theorem 3,[0],[0]
Since its inner product with the first row of Cy is one (since CyC −1,B. Proof of Theorem 3,[0],[0]
"y = IUQ×UQ), we get
a+ (U − 1)sb+ (Q− 1)sc = 1.
",B. Proof of Theorem 3,[0],[0]
"Similarly, its inner products with the second, (U + 1)− th, and (U + 2)-th rows are all zero; this gives
sa+ ((U − 2)s+ 1)b+ (Q− 1)sd = 0, sa+ ((Q− 2)s+ 1)c+ (U − 1)sd = 0,
sb+ sc+ ((U +Q− 4)s+ 1)d = 0.
",B. Proof of Theorem 3,[0],[0]
"Solving the linear system given by these four equations results in
a= (3U2+3Q2−U2Q−UQ2+8UQ−15U−15Q+20)s3
r
+ (−U2 −Q2 − 3UQ+ 11U + 11Q− 22)s2
r
+ (−2U − 2Q+ 8)s− 1
r
b= (UQ+Q2 − 3U − 5Q+ 8)s3+(U + 2Q− 6)s2+s
r
c= (UQ+ U2 − 5U − 3Q+ 8)s3+(2U +Q− 6)s2+s
r
d= −(U +Q− 4)s3 − 2s2
r , (9)
where
r=(2s−1)((U−2)s+1)((Q−2)s+1)((Q+U−2)s+1).
",B. Proof of Theorem 3,[0],[0]
"Now, let A be the N ×N matrix with c on its diagonal and d everywhere else, B denote the matrix with a − c on its diagonal and b− d everywhere else, we can write C−1y as
C−1y = 1Q×Q⊗A + IQ×Q⊗B. (10)
",B. Proof of Theorem 3,[0],[0]
"Our second task is to evaluate E. Since
E =
√ 2
π diag(diag(Cz)
−1/2)DCx =
√ 2
π σ2x√ 2σ2x + 1 D
= σ2x√
2σ2x + 1",B. Proof of Theorem 3,[0],[0]
"[1Q⊗ IU×U IQ×Q⊗1U ],
we have
ETC−1y E = 2
π σ4x 2σ2x + 1",B. Proof of Theorem 3,[0],[0]
[ 1TQ⊗ IU×U IQ×Q⊗1TU ] × (1Q×Q⊗A + IQ×Q⊗B)[1Q⊗ IU×U,B. Proof of Theorem 3,[0],[0]
"IQ×Q⊗1U ]
",B. Proof of Theorem 3,[0],[0]
"= 2
π σ4x 2σ2x + 1 Q(QA + B),
where we have used (X⊗Y)(U⊗V) = (XU)⊗(YV).
",B. Proof of Theorem 3,[0],[0]
"Therefore, the value of entry (1, 1) in ETC−1y E, i.e., the MSE of the user ability parameter estimates, is given by
2
π σ4x 2σ2x + 1 Q(a+ (Q− 1)c) =
σ2x
( 1− 2
π σ2x 2σ2x + 1 sQ(Q+ U − 3) + 1 (s(Q− 2) + 1)(s(Q+ U − 2) + 1)
) ,
where we have used (9), thus completing the proof.",B. Proof of Theorem 3,[0],[0]
"C. Studer was supported in part by Xilinx, Inc. and by the US National Science Foundation (NSF) under grants ECCS1408006, CCF-1535897, CCF-1652065, and CNS-1717559.",Acknowledgments,[0],[0]
"The Rasch model is widely used for item response analysis in applications ranging from recommender systems to psychology, education, and finance.",abstractText,[0],[0]
"While a number of estimators have been proposed for the Rasch model over the last decades, the available analytical performance guarantees are mostly asymptotic.",abstractText,[0],[0]
"This paper provides a framework that relies on a novel linear minimum mean-squared error (L-MMSE) estimator which enables an exact, nonasymptotic, and closed-form analysis of the parameter estimation error under the Rasch model.",abstractText,[0],[0]
The proposed framework provides guidelines on the number of items and responses required to attain low estimation errors in tests or surveys.,abstractText,[0],[0]
"We furthermore demonstrate its efficacy on a number of real-world collaborative filtering datasets, which reveals that the proposed L-MMSE estimator performs on par with state-of-the-art nonlinear estimators in terms of predictive performance.",abstractText,[0],[0]
An Estimation and Analysis Framework for the Rasch Model,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 42–50, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Human-human conversation has flexible turntaking behavior: back channeling, overlapping speech and smooth turn transitions.",1 Introduction,[0],[0]
Imitating human-like turn-taking in a spoken dialog system (SDS) is challenging due to the degradation in quality of the dialog when overlapping speech is produced in the wrong place.,1 Introduction,[0],[0]
"For this, a traditional SDS often uses a simplified turn-taking model with rigid turn taking.",1 Introduction,[0],[0]
They only respond when users have finished speaking.,1 Introduction,[0],[0]
"Thus past research has mostly focused on end-of-turn detection, finding the end of the user utterance as quickly as possible while minimizing the chance of wrongly interrupting the users.",1 Introduction,[0],[0]
"We refer here to the interruption issue as false cut-ins (FCs).
",1 Introduction,[0],[0]
"Recent research in incremental dialog processing promises more flexible turn-taking behavior (Atterer et al., 2008; Breslin et al., 2013).",1 Introduction,[0],[0]
"Here, the automatic speech recognizer (ASR) and natural language understanding (NLU) incrementally
produce partial decoding/understating messages for decision-making.",1 Introduction,[0],[0]
"This allows for system barge-in (SB), starting to respond before end-of-utterance.",1 Introduction,[0],[0]
"Although this framework has shown promising results in creating flexible SDSs, the following two fundamental issues remain:
1.",1 Introduction,[0],[0]
We need a model that unifies incremental processing and traditional turn-taking behavior.,1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"We also need a systematic procedure that trains a system to produce meaningful SBs.
",1 Introduction,[0],[0]
This paper first proposes a finite state machine (FSM) that both shows superior performance in end-of-turn detection compared to previous methods and is compatible with incremental processing.,1 Introduction,[0],[0]
"Then we propose a systematic procedure to endow a system with meaningful SB by combining the theory of optimal stopping with reinforcement learning.
",1 Introduction,[0],[0]
"Section 2 of the paper discusses related work; Section 3 describes the finite state machine; Sections 4, 5, and 6 describe how to produce meaningful SB; Section 7 gives experimental results of an evaluation using the CMU Let’s Go Live system and simulation results on the Dialog State Tracking Challenging (DTSC) Corpus and Section 8 concludes.",1 Introduction,[0],[0]
"This work is closely related to end-of-turn detection and incremental processing (IP) dialog systems.
",2 Related Work and Limitations,[0],[0]
There are several methods for detecting the endof-turn.,2 Related Work and Limitations,[0],[0]
Raux (2008) built a decision tree for final pause duration using ASR and NLU features.,2 Related Work and Limitations,[0],[0]
"At runtime, the system first dynamically chooses the final pause duration threshold based on the dialog state and then predicts end-of-turn if final pause duration is longer than that threshold.",2 Related Work and Limitations,[0],[0]
Other work explored predicting end-of-turn within a user’s speech.,2 Related Work and Limitations,[0],[0]
"This showed substantial improvement in speed of response (Raux and Eske-
42
nazi, 2009).",2 Related Work and Limitations,[0],[0]
"Another approach examined prosodic and semantic features such as pitch and speaking rate in human-human conversation for turn-yielding cues (Gravano, 2009).
",2 Related Work and Limitations,[0],[0]
"The key limitation of those methods is that the decision made by the end-of-turn detector is treated as a “hard” decision, obliging developers to compromise in a tradeoff between response latency and FC rate (Raux and Eskenazi, 2008).",2 Related Work and Limitations,[0],[0]
"Although adding more complex prosodic and semantic features can improve the performance of the detector, it also increases computation cost and requires significant knowledge of the SDS, which can limit the accessibility for non-expert developers.
",2 Related Work and Limitations,[0],[0]
"For IP, Kim (2014) has demonstrated the possibility of learning turn-taking from human dialogs using inverse reinforcement learning.",2 Related Work and Limitations,[0],[0]
"Other work has focused on incremental NLU (DeVault et al., 2009), showing that the correct interpretation of users’ meaning can be predicted before end-of-turn.",2 Related Work and Limitations,[0],[0]
Another topic is modeling user and system barge-in.,2 Related Work and Limitations,[0],[0]
Selfridge (2013) has presented a FSM that predicts users’ barge-ins.,2 Related Work and Limitations,[0],[0]
"Also, Ghigi (2014) has shown that allowing SB when users produce lengthy speech increases robustness and task success.
",2 Related Work and Limitations,[0],[0]
"Different from Kim’s work that learns humanlike turn-taking, our approach is more related to Ghigi’s method, which tries to improve dialog efficiency from a system-centric perspective.",2 Related Work and Limitations,[0],[0]
We take one step further by optimizing the turn-taking using all available features based on a global objective function with machine learning methods.,2 Related Work and Limitations,[0],[0]
Our model has two distinct modes: passive and active.,3.1 Model Description,[0],[0]
The passive mode exhibits traditional rigid turn-taking behavior while the active mode has the system respond in the middle of a user turn.,3.1 Model Description,[0],[0]
"We first describe how these two modes operate, and then show how they are compatible with existing incremental dialog approaches.
",3.1 Model Description,[0],[0]
The idea is to combine an aggressive speaker with a patient listener.,3.1 Model Description,[0],[0]
The speaker consists of the Text-to-Speech (TTS) and Natural Language Generation (NLG) modules.,3.1 Model Description,[0],[0]
The listener is composed of the ASR and Voice Activity Detection (VAD) modules.,3.1 Model Description,[0],[0]
The system attempts to respond to a user every time it detects a short pause (e.g. 100ms).,3.1 Model Description,[0],[0]
"But before a long pause (e.g. 1000ms) is detected, the user’s continued speech will stop the system from
responding, as shown on Figure 1:
Most of the system’s attempts to respond will thus be FCs.",3.1 Model Description,[0],[0]
"However, since the listener can stop the system from speaking, the FCs have no effect on the conversation (users may hear the false start of the system’s prompt, but often the respond state is cancelled before the synthesized speech begins).",3.1 Model Description,[0],[0]
"If the attempt is correct, however, the system responds with almost 0-latency, as shown in Figure 2.",3.1 Model Description,[0],[0]
"Furthermore, because the dialog manager (DM) can receive partial ASR output whenever there is a short pause, this model produces relatively stable partial ASR output and supports incremental dialog processing.
",3.1 Model Description,[0],[0]
"We then define the short pause as the action threshold (AT) and the long pause as the listening threshold (LT), where 0 < AT ≤ LT, which can be interpreted respectively as the “aggression” and “patience” of the system.",3.1 Model Description,[0],[0]
"By changing the value of each of these thresholds we can modify the system’s behavior from rigid turn taking to active SB.
1.",3.1 Model Description,[0],[0]
"Passive Agent: act fast and listen patiently (AT = small value, LT = large value)
2.",3.1 Model Description,[0],[0]
Active Agent: act and listen impatiently.,3.1 Model Description,[0],[0]
"(AT = LT = small value)
",3.1 Model Description,[0],[0]
"This abstraction simplifies the challenge: “when the system should barge in” as the following transition: PassiveAgent Φ(dialog state)−−−−−−−−−→ ActiveAgent where Φ(·) : dialog State → {true, false} is a function that outputs true whenever the agent should take the floor, regardless of the current state of the floor.",3.1 Model Description,[0],[0]
"For example, this function could output true when the current dialog states fulfill certain rules in a hand-crafted system, or could output true when the system has reached its maximal understanding of the user’s intention (DeVault et al., 2009).",3.1 Model Description,[0],[0]
"A natural next step is to use statistical techniques to learn an optimized Φ(·) based on all features related to the dialog states, in order to support more complex SB behavior.",3.1 Model Description,[0],[0]
"First our model solves end-of-turn detection by using a combination of VAD and TTS control, instead of trying to build a perfect classifier.",3.2 Advantages over Past Methods,[0],[0]
This avoids the tradeoff between response latency and FC.,3.2 Advantages over Past Methods,[0],[0]
"Under the assumption that the TTS can operate at high speed, the proposed system can achieve almost 0-lag and 0-FC by setting AT to be small (e.g. 100ms).",3.2 Advantages over Past Methods,[0],[0]
"Second, the model does not require expensive prosodic and semantic turn-yielding cue detectors, thus simplifying the implementation.",3.2 Advantages over Past Methods,[0],[0]
"In state-of-the-art SDS, the DM uses explicit/implicit confirmation to fill each slot and carries out an error recovery strategy for incorrectly recognized slots (Bohus and Rudnicky, 2009).",4 Toward Active System Barge-in,[0],[0]
"The system should receive many correctly-recognized slots, thus avoiding lengthy error recovery.",4 Toward Active System Barge-in,[0],[0]
"While a better ASR and NLU could help, Ghigh (2014) has shown that allowing the system to actively respond to users also leads to more correct slots.
",4 Toward Active System Barge-in,[0],[0]
Table 1 demonstrates three cases where active SB can help.,4 Toward Active System Barge-in,[0],[0]
The first two rows show the first half of the user’s speech being correctly recognized while the second half is not.,4 Toward Active System Barge-in,[0],[0]
"In this scenario, if, in the middle of the utterance, the system can tell that the existing ASR hypothesis is sufficient and actively barges on the user, it can potentially avoid the poorly-recognized speech that follows.",4 Toward Active System Barge-in,[0],[0]
The third example has noise at the beginning of the user turn.,4 Toward Active System Barge-in,[0],[0]
The system could back channel in the middle of the utterance to ask the user to go to a quieter place or to repeat an answer.,4 Toward Active System Barge-in,[0],[0]
"In these examples active SB can help improve robustness:
1.",4 Toward Active System Barge-in,[0],[0]
Barge in when the current hypothesis has high confidence and contains sufficient information to move the dialog along.,4 Toward Active System Barge-in,[0],[0]
2. Barge in when the hypothesis confidence is low and the predicted future hypothesis will not get better.,4 Toward Active System Barge-in,[0],[0]
"This can avoid recovering from a large number of incorrect slots.
",4 Toward Active System Barge-in,[0],[0]
A natural choice of objective function to train such a system is to maximize the expected quality of information in the users’ utterances.,4 Toward Active System Barge-in,[0],[0]
The quality of the recognized information is positively correlated to number of correctly recognized slots (CS) and inversely correlated to the number of incorrectly recognized slots (ICS).,4 Toward Active System Barge-in,[0],[0]
"In the next section, we describe how we transform CS and ICS into a real-value reward.",4 Toward Active System Barge-in,[0],[0]
We first design a cost model that defines a reward function.,5 A Cost Model for System Barge-in,[0],[0]
This model is based on the assumption that the system will use explicit confirmation for every slot.,5 A Cost Model for System Barge-in,[0],[0]
We choose this because it is the most basic dialog strategy.,5 A Cost Model for System Barge-in,[0],[0]
"A sample dialog for this strategy is as follows:
Sys: Where do you want to leave from?",5 A Cost Model for System Barge-in,[0],[0]
User:,5 A Cost Model for System Barge-in,[0],[0]
Leaving from X. Sys: Do you mean leaving from Y?,5 A Cost Model for System Barge-in,[0],[0]
User:,5 A Cost Model for System Barge-in,[0],[0]
No.,5 A Cost Model for System Barge-in,[0],[0]
Sys: Where do you want to leave from?,5 A Cost Model for System Barge-in,[0],[0]
User:,5 A Cost Model for System Barge-in,[0],[0]
<No Parse> Sys: Where do you want to leave from?,5 A Cost Model for System Barge-in,[0],[0]
User:,5 A Cost Model for System Barge-in,[0],[0]
I am leaving from X. Sys: Do you mean X?,5 A Cost Model for System Barge-in,[0],[0]
User:,5 A Cost Model for System Barge-in,[0],[0]
"Yes.
Given this dialog strategy the system spends one turn asking the question, and k turns confirming k slots in the user response.",5 A Cost Model for System Barge-in,[0],[0]
"Also, for no-parse (0 slot) input, the system asks the same question again.",5 A Cost Model for System Barge-in,[0],[0]
"Therefore, the minimum number of turns required
to acquire n slots is 2n.",5 A Cost Model for System Barge-in,[0],[0]
"However, because user responses contain ICS and no-parses, the system takes more than 2n turns to obtain all the slot information (assume confirmation are never misrecognized).
",5 A Cost Model for System Barge-in,[0],[0]
We denote csi and icsi as the number of correctly/incorrectly recognized slots in the user response.,5 A Cost Model for System Barge-in,[0],[0]
"So the quality of the user response is captured by a tuple, (csi, icsi).",5 A Cost Model for System Barge-in,[0],[0]
"The goal is to obtain a reward function that maps from a given user response (csi, icsi) to a reward value ri ∈",5 A Cost Model for System Barge-in,[0],[0]
<.,5 A Cost Model for System Barge-in,[0],[0]
"This reward value should correlate with the overall efficiency of a dialog, which is inversely correlated with the number of turns needed for task completion.
",5 A Cost Model for System Barge-in,[0],[0]
"Then for a dialog task that has n slots to fill, we can denote hi as the number of turns already spent, fi as the estimated number of future turns needed for task completion and E[S] as the expected number of turns needed to fill 1 slot.",5 A Cost Model for System Barge-in,[0],[0]
"Then for each new user response (csi, icsi), we update the following recursive formulas:
Initialization: h0 = 0, f0 = nE[s] Update Rules:
hi = hi−1 + 1︸︷︷︸",5 A Cost Model for System Barge-in,[0],[0]
"question + csi + icsi︸ ︷︷ ︸ confirm
(1)
fi",5 A Cost Model for System Barge-in,[0],[0]
"= fi−1 − csiE[S]︸ ︷︷ ︸ acquired slots
(2)
",5 A Cost Model for System Barge-in,[0],[0]
"Based on the above setup, it is clear that hi + fi equals the estimated total number of turns needed to fill n slots.",5 A Cost Model for System Barge-in,[0],[0]
"Then the reward, ri, associated with each user response can be expressed as the difference between the previous and current estimates:
ri = (hi−1 + fi−1)− (hi + fi) (3) = −1",5 A Cost Model for System Barge-in,[0],[0]
"+ (E[S]− 1)︸ ︷︷ ︸
weight to CS
csi − icsi (4)
Therefore, a positive reward means the new user response reduces the estimated number of turns for task completion while a negative reward means the opposite.",5 A Cost Model for System Barge-in,[0],[0]
"Another interpretation of this reward function is that for no-parse user response (csi = 0, icsi = 0), the cost is to waste 1 turn asking the same question again.",5 A Cost Model for System Barge-in,[0],[0]
"When there is a parse, each correct slot can save E[S] turns in the future, while each slot, regardless of its correctness, needs a 1- turn confirmation.",5 A Cost Model for System Barge-in,[0],[0]
"As a result, this rewards function is correlated with the global efficiency of a dialog because it assigns a corpus-dependent weight to csi, based on E[S] estimated from historical dialogs.",5 A Cost Model for System Barge-in,[0],[0]
"After modeling the cost of a user turn, we learn a turn-taking policy that can maximize the expected reward in user turns, namely the Φ(dialog state) that controls the switching between passive and active agent of our FSM in Section 3.1.",6 Learning Active Turn-taking Policy,[0],[0]
"Before going into detail, we first introduce the optimal stopping problem and reinforcement learning.",6 Learning Active Turn-taking Policy,[0],[0]
"The theory of optimal stopping is an area of mathematics that addresses the decision of when to take a given action based on a set of sequentially observed random variables, in order to maximize an expected payoff (Ferguson, 2012).",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"A formal description is as follows:
1.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"A sequence of random variables X1, X2... 2.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"A sequence of real-valued reward functions, y0, y1(x1), y2(x1, x2)...
",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"The decider may observe the sequence x1, x2... and after observing X1 = x1, ...",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Xn = xn, the decider may stop and receive the reward yn(x1, ...xn), or continue and observe Xn+1.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"The optimal stopping problem searches for an optimal stopping rule that maximizes the expected reward.
",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
Reinforcement learning models are based on the Markov decision process (MDP).,6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"A (finite) MDP is a tuple (S,A, {Psa}, γ, R), where: • S is a finite set of N states • A = a1, ...ak is a set of k actions •",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Psa(·) are the state transition probabilities on
taking action a in state s. •",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
γ ∈,6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"[0, 1) is the discount factor •",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
R : S → < is the rewards function.,6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Then a policy, π , is a mapping from each state, s ∈ S and action a ∈ A, to the probability π(s, a) of taking action awhen in state s (Sutton and Barto, 1998).",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Then, for MDPs, the Q-function, is the expected return starting from s taking action a and thereafter following policy π and has the Bellman equation:",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Qπ(s, a) = R(s) + γ ∑ s′ P (s′|s, a)V π(s′).",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"(5)
The goal of reinforcement learning is to find the optimal policy π∗, such that Qπ(s, a) can be maximized.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Thus the optimal stopping problem can be formulated as an MDP, where the action space contains two actions {wait, stop}.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Also, solving the optimal stopping rule is equivalent to finding the optimal policy, π∗.",6.1 Optimal Stopping Problem and Reinforcement Learning,[0],[0]
"Equipped with the above two frameworks, we first show that SB can be formulated as an optimal stopping problem.",6.2 Solving Active Turn-taking,[0],[0]
"Then we propose a novel, noniterative, model-free method for solving for the optimal policy.
",6.2 Solving Active Turn-taking,[0],[0]
An SDS dialog contains N user utterances.,6.2 Solving Active Turn-taking,[0],[0]
"Each user utterance contains K partial hypotheses and each partial hypothesis, pi, is associated with a tuple (csi, icsi) and a feature vector, xi ∈ <f×1, where f is the dimension of the feature vector.",6.2 Solving Active Turn-taking,[0],[0]
We also assume that every user utterance is independent of every other utterance.,6.2 Solving Active Turn-taking,[0],[0]
"We will call one user utterance an episode.
",6.2 Solving Active Turn-taking,[0],[0]
"In an episode, the turn-taking decider will see each partial hypothesis sequentially over time, At each hypothesis it takes an action from {wait, stop}.",6.2 Solving Active Turn-taking,[0],[0]
Wait means it continues to listen.,6.2 Solving Active Turn-taking,[0],[0]
Stop means it takes the floor.,6.2 Solving Active Turn-taking,[0],[0]
"The turn-taking decider receives 0 reward for taking the action wait and receives the reward ri from (csi, icsi) according to our cost model for taking the action stop.",6.2 Solving Active Turn-taking,[0],[0]
"This is an optimal stopping problem that can be formulated as an MDP:
• S = {x1, ...{x1...xK}} • A = {wait, stop} • R = −1 + (E[S]− 1)csi − icsi",6.2 Solving Active Turn-taking,[0],[0]
"Then the Bellman equations are: Qπ(s, stop) =",6.2 Solving Active Turn-taking,[0],[0]
"R(s) = r(s) (6)
",6.2 Solving Active Turn-taking,[0],[0]
"Qπ(s, wait) =",6.2 Solving Active Turn-taking,[0],[0]
"γ ∑
s′ P (s′|s, a)V π(s′) (7)
",6.2 Solving Active Turn-taking,[0],[0]
"The first equation shows that the Q-value for any state, s, with action, stop, is simply the immediate reward for s. The second equation shows that the Q-value for any state s, with action, wait, only depends on the future return by following policy π.",6.2 Solving Active Turn-taking,[0],[0]
"This result is crucial because it means that Qπ(s, stop) for any state, s, can be directly calculated based on the cost model, independent of the policy π.",6.2 Solving Active Turn-taking,[0],[0]
"Also, given a policy π, Qπ(s, wait)can also be directly calculated as the discounted reward the first time that the policy chooses to stop.
",6.2 Solving Active Turn-taking,[0],[0]
"Meanwhile, for a given episode with known reward ri for each partial hypothesis pi, optimal stopping means always to stop at the largest reward, meaning that we can obtain the oracle action for the training corpus.",6.2 Solving Active Turn-taking,[0],[0]
"Given a sequence of reward (ri, ...rK) , the optimal policy, π, chooses to stop at partial pm if m = arg maxj∈(i,K] rj .
",6.2 Solving Active Turn-taking,[0],[0]
"The Bellman equations become: Qπ(si, stop) = ri (8)
Qπ(si, wait) = γm−irm (9) and the oracle action at any s can be obtained by : a∗i = wait if Q
∗(si, stop) < Q∗(si, wait) a∗i = stop if Q
∗(si, stop)",6.2 Solving Active Turn-taking,[0],[0]
"≥ Q∗(si, wait)",6.2 Solving Active Turn-taking,[0],[0]
"This special property of optimal stopping problem allows us to use supervised learning methods directly modeling the optimal Q function, by finding a mapping from the input state space, si, into the Q-value for both actions: Q(si, stop)∗ and Q(si, wait)∗.",6.2 Solving Active Turn-taking,[0],[0]
"Further, inspired by the work of reinforcement learning as classification (Lagoudakis and Parr, 2003), we decide to map directly from the input state space into the action space: S → A∗, using a Support Vector Machine (SVM).
",6.2 Solving Active Turn-taking,[0],[0]
"Advantages of solving this problem as a classification rather than a regression include: 1) it explicitly models sign(Q(si, stop)∗−Q(si, wait)∗), which sufficiently determines the behavior of the agent.",6.2 Solving Active Turn-taking,[0],[0]
"2) SVM is known as a state-of-the-art modeler for the binary classification task, due to its ability to find the separating hyperplane in nonlinear space.",6.2 Solving Active Turn-taking,[0],[0]
"Since SVM requires a fixed input dimension size, while the available features will continue to increase as the turn-taking decider observes more partial hypotheses, we adopt the functional idea used by the openSMILE toolkit (Eyben et al., 2010).",6.3 Feature Construction,[0],[0]
"There are three categories of features: immediate feature, delta feature and long-term feature.",6.3 Feature Construction,[0],[0]
Immediate features come from the ASR and the NLU in the latest partial hypothesis.,6.3 Feature Construction,[0],[0]
Delta features are the first-order derivate of immediate features with respect to the previous observed feature.,6.3 Feature Construction,[0],[0]
"Long-term features are global statistics associated with all the observed features.
",6.3 Feature Construction,[0],[0]
"Table 2 shows that we have 18 immediate features, 18 delta features and 18× 7 = 126 long-term features.",6.3 Feature Construction,[0],[0]
"Then we apply F-score feature selection as described in (Chen and Lin, 2006).",6.3 Feature Construction,[0],[0]
The final feature set contains 138 features.,6.3 Feature Construction,[0],[0]
We conducted a live study and a simulation study.,7 Experiments and Results,[0],[0]
The live study evaluates the model’s end-of-turn detection.,7 Experiments and Results,[0],[0]
The simulated study evaluates the active SB behavior.,7 Experiments and Results,[0],[0]
"The finite state machine was implemented in the Interaction Manager of the CMU Lets Go system that provides bus information in Pittsburgh (Raux et al., 2005).",7.1 Live Study,[0],[0]
"We compared base system data from November 1-30, 2014 (773 dialogs), to data from our system from December 1-31, 2014 (565 dialogs).
",7.1 Live Study,[0],[0]
"The base system used the decision tree endof-turn detector described in (Raux and Eskenazi, 2008) and the active SB algorithm described in (Ghigi et al., 2014).",7.1 Live Study,[0],[0]
The action threshold (AT) in the new system was set at 60% of the decision tree output in the former system and the listening threshold (LT) was empirically set at 1200ms.,7.1 Live Study,[0],[0]
We observed that FCs result in several users’ utterances having overlapping timestamps due to a builtin 500ms padding before an utterances in PocketSphinx.,7.2 Live Study Metrics,[0],[0]
This means that we consider two consecutive utterances with a pause less than 500ms as one utterance.,7.2 Live Study Metrics,[0],[0]
"Figure 4 shows that when the end-of-turn detector produces an FC, the continued flow of user
speech instantiates a new user utterance which overlaps with the previous one.",7.2 Live Study Metrics,[0],[0]
"In this example, utterances 0 and 1 have overlaps while utterance 2 does not.",7.2 Live Study Metrics,[0],[0]
"So users actually produce two utterances, while the system thinks there are three due to FC.
",7.2 Live Study Metrics,[0],[0]
"Thus, we can automatically calculate the FC rate of every dialog, by counting the number of user utterances with overlaps.",7.2 Live Study Metrics,[0],[0]
"We define an utterance fragment ratio (UFR) that measures the FC rate in a dialog.
",7.2 Live Study Metrics,[0],[0]
"UFR = Number of user utterances with overlapsTotal number of user utterances
We also manually label task success (TS) of all the dialogs.",7.2 Live Study Metrics,[0],[0]
We define TS as: a dialog is successful if and only if the system conducted a back-end search for bus information with all required slots correctly recognized.,7.2 Live Study Metrics,[0],[0]
"In summary, we use the following metrics to evaluate the new system:
1.",7.2 Live Study Metrics,[0],[0]
Task success rate 2.,7.2 Live Study Metrics,[0],[0]
Utterance fragment ratio (UFR) 3.,7.2 Live Study Metrics,[0],[0]
Average number of system barge-in (ANSB) 4.,7.2 Live Study Metrics,[0],[0]
"Proportion of long user utterances interrupted
by system barge-in (PLUISB) 5.",7.2 Live Study Metrics,[0],[0]
Average response delay (ARD) 6.,7.2 Live Study Metrics,[0],[0]
Average user utterance duration over time,7.2 Live Study Metrics,[0],[0]
Table 3 shows that the TS rate of the new system is 7.5% higher than the previous system (p-value < 0.01).,7.3 Live Study Results,[0],[0]
Table 4 shows that overall UFR decreased by 37.1%.,7.3 Live Study Results,[0],[0]
UFR for successful and for failed dialogs indicates that the UFR decreases more in failed dialogs than in successful ones.,7.3 Live Study Results,[0],[0]
One explanation is that failed dialogs usually have a noisier environment.,7.3 Live Study Results,[0],[0]
"The UFR reduction explains the increase in success rate since UFRs are positively correlated with TS rate, as reported in (Zhao and Eskenazi, 2015)
",7.3 Live Study Results,[0],[0]
Table 5 shows that the SB algorithm was activated more often in the new system.,7.3 Live Study Results,[0],[0]
"This is because the SB algorithm described in (Ghigi et al., 2014) only activates for user utterances longer than 3 seconds.",7.3 Live Study Results,[0],[0]
"FCs will therefore hinder the ability of this algorithm to reliably measure user utterance dura-
tion.",7.3 Live Study Results,[0],[0]
This is an example of how reliable end-of-turn detection can benefit other SDS modules.,7.3 Live Study Results,[0],[0]
Table 5 also shows that the new system is 32.5% more responsive than the old system.,7.3 Live Study Results,[0],[0]
"We purposely set the action threshold to 60% of the threshold in the old system, which demonstrates that the new model can have an response speed equals to action threshold that is independent of the FC rate.
",7.3 Live Study Results,[0],[0]
Figure 5 shows how average user utterance duration evolves in a dialog.,7.3 Live Study Results,[0],[0]
Utterance duration is more stable in the new system than in the old one.,7.3 Live Study Results,[0],[0]
"Two possible explanations are: 1) since UFR is much higher in the old system, the system is more likely to cut in at the wrong time, possibly making users abandon their normal turn-taking behavior and talk over the system.",7.3 Live Study Results,[0],[0]
2) more frequent activation of the SB algorithm entrains the users to produce more concise utterances.,7.3 Live Study Results,[0],[0]
"This part of the experiment uses the DSTC corpus training2 (643 dialogs) (Black et al., 2013).",7.4 Simulation Study,[0],[0]
The data was manually transcribed.,7.4 Simulation Study,[0],[0]
"The reported 1-best word error rate (WER) is 58.2% (Williams et al., 2013).",7.4 Simulation Study,[0],[0]
"This study focuses on all user responses to:“Where are you leaving from?” and “Where are you going?” which have 688 and 773 utterances respectively.
",7.4 Simulation Study,[0],[0]
"An automatic script, based on the manual transcription, labels the number of correct and incorrect
slots (csi, icsi) for each partial hypothesis, pi.",7.4 Simulation Study,[0],[0]
"Also from the training data, the expected number of turns needed to obtain 1 slot, E[S], is 3.82.",7.4 Simulation Study,[0],[0]
"For simplicity, E[S] is set to be 4.",7.4 Simulation Study,[0],[0]
"So the reward function discussed in Section 5 is: ri = −1 + 3csi − icsi.
",7.4 Simulation Study,[0],[0]
"After obtaining the reward value for each hypothesis, the oracle action at each partial hypothesis is calculated based on the procedure discussed in Section 6.3 with γ = 1.
",7.4 Simulation Study,[0],[0]
"We set the SVM kernel as RBF kernel and use a grid search to choose the best parameters for cost and kernel width using 5-fold cross validation on the training data (Hsu et al., 2003).",7.4 Simulation Study,[0],[0]
The optimization criterion is the F-measure.,7.4 Simulation Study,[0],[0]
The evaluation metrics have two parts: classification-related (precision and recall) and dialog-related.,7.5 Simulation Study Metrics,[0],[0]
"Dialog related metrics are:
1.",7.5 Simulation Study Metrics,[0],[0]
Accuracy of system barge-in 2.,7.5 Simulation Study Metrics,[0],[0]
"Average decrease in utterance duration com-
pared to no system barge-in 3.",7.5 Simulation Study Metrics,[0],[0]
Percentage of no-parse utterance 4.,7.5 Simulation Study Metrics,[0],[0]
Average CS per utterance 5.,7.5 Simulation Study Metrics,[0],[0]
Average ICS per utterance 6.,7.5 Simulation Study Metrics,[0],[0]
"Average reward = 1/T ∑ i ri , where T is the
number of utterances in the test set.",7.5 Simulation Study Metrics,[0],[0]
The learned policy is compared to two reference systems: the oracle and the baseline system.,7.5 Simulation Study Metrics,[0],[0]
The oracle directly follows optimal policy obtained from the ground-truth label.,7.5 Simulation Study Metrics,[0],[0]
"The baseline system always waits for the last partial (no SB).
",7.5 Simulation Study Metrics,[0],[0]
"Furthermore, a simple smoothing algorithm is applied to the SVM output for comparison.",7.5 Simulation Study Metrics,[0],[0]
This algorithm confirms the stop action after two consecutive stop outputs from the classifier.,7.5 Simulation Study Metrics,[0],[0]
This increases the classifier’s precision.,7.5 Simulation Study Metrics,[0],[0]
10-fold cross validation was conducted on the two datasets.,7.6 Simulation Study Results,[0],[0]
"Instead of using the SVM binary output, we apply a global threshold of 0.4 on the SVM decision function for output to achieve the best average reward.",7.6 Simulation Study Results,[0],[0]
"The threshold is determined based on cross-validation on training data.
",7.6 Simulation Study Results,[0],[0]
Table 6 shows that the SVM classifier can achieve very high precision and high recall in predicting the correct action.,7.6 Simulation Study Results,[0],[0]
"The F-measure (after smoothing) is 84.46% for departure question responses and 85.99% for arrival questions.
",7.6 Simulation Study Results,[0],[0]
Table 7 shows that learned policy increases the average reward by 27.7% and 14.9% compared to the baseline system for the departure and arrival responses respectively.,7.6 Simulation Study Results,[0],[0]
We notice that the average reward of the baseline arrival responses is significantly higher.,7.6 Simulation Study Results,[0],[0]
"A possible reason is that by this second question the users are adapting to the system.
",7.6 Simulation Study Results,[0],[0]
The decrease in average utterance duration shows some interesting results.,7.6 Simulation Study Results,[0],[0]
"For responses to both questions, the oracle system utterance duration is about 55% shorter than the baseline one.",7.6 Simulation Study Results,[0],[0]
"The learned policy is also 45% shorter, which means that at about the middle of a user utterance, the system can already predict that the user either has expressed enough information or that the ASR is so wrong that there is no point of continuing to listen.
",7.6 Simulation Study Results,[0],[0]
"Table 8 expands our understanding of the oracle
and learned policy behaviors.",7.6 Simulation Study Results,[0],[0]
"We see that the oracle produces a much higher percentage of no-parse utterances in order to maximize the average reward, which, at first, seems counter-intuitive.",7.6 Simulation Study Results,[0],[0]
The reason is that some utterances contain a large number of incorrect slots at the end and the oracle chooses to barge in at the beginning of the utterance to avoid the large negative reward for waiting until the end.,7.6 Simulation Study Results,[0],[0]
This is the expected behavior discussed in Section 4.,7.6 Simulation Study Results,[0],[0]
The learned policy is more conservative in producing no-parse utterances because it cannot cheat like the oracle to access future information and know that all future hypotheses will contain only incorrect information.,7.6 Simulation Study Results,[0],[0]
"However, although the learned policy only has access to historical information, it manages to predict future return by increasing CS and reducing ICS compared to the baseline.",7.6 Simulation Study Results,[0],[0]
This paper describes a novel turn-taking model that unifies the traditional rigid turn-taking model with incremental dialog processing.,8 Conclusions and Future Directions,[0],[0]
It also illustrates a systematic procedure of constructing a cost model and teaching a dialog system to actively grab the conversation floor in order to improve system robustness.,8 Conclusions and Future Directions,[0],[0]
The turn-taking model was tested for end-of-turn detection and active SB.,8 Conclusions and Future Directions,[0],[0]
The proposed model has shown superior performance in reducing FC rate and response delay.,8 Conclusions and Future Directions,[0],[0]
"Also, the proposed SB algorithm has shown promise in increasing the average reward in user responses.
",8 Conclusions and Future Directions,[0],[0]
"Future studies will include constructing a more comprehensive cost model that not only takes into account of CS/ICS, but also includes other factors such as conversational behavior.",8 Conclusions and Future Directions,[0],[0]
"Further, since E[S] will decrease after applying the learned policy, it invalidates the previous reward function.",8 Conclusions and Future Directions,[0],[0]
Future work should investigate how the change inE[S] impacts the optimality of the policy.,8 Conclusions and Future Directions,[0],[0]
"Also, we will add more complex actions to the system such as back channeling, clarifications etc.",8 Conclusions and Future Directions,[0],[0]
This paper deals with an incremental turntaking model that provides a novel solution for end-of-turn detection.,abstractText,[0],[0]
It includes a flexible framework that enables active system barge-in.,abstractText,[0],[0]
"In order to accomplish this, a systematic procedure of teaching a dialog system to produce meaningful system barge-in is presented.",abstractText,[0],[0]
This procedure improves system robustness and success rate.,abstractText,[0],[0]
It includes constructing cost models and learning optimal policy using reinforcement learning.,abstractText,[0],[0]
Results show that our model reduces false cut-in rate by 37.1% and response delay by 32.5% compared to the baseline system.,abstractText,[0],[0]
Also the learned system barge-in strategy yields a 27.7% increase in average reward from user responses.,abstractText,[0],[0]
An Incremental Turn-Taking Model with Active System Barge-in for Spoken Dialog Systems,title,[0],[0]
"The hierarchical Dirichlet process hidden Markov model (HDP-HMM) (Beal et al., 2001; Teh et al., 2006) is a Bayesian model for time series data that generalizes the conventional hidden Markov Model to allow a countably infinite state space.",1. Introduction and Background,[0],[0]
"The hierarchical structure ensures that, despite the infinite state space, a common set of destination states will be reachable with positive probability from each source state.",1. Introduction and Background,[0],[0]
"The HDP-HMM can be characterized by the following generative process.
",1. Introduction and Background,[0],[0]
"Each state, indexed by j, has parameters, θj , drawn from a base measure, H .",1. Introduction and Background,[0],[0]
"A top-level sequence of state weights, β = (β1, β2, . . . )",1. Introduction and Background,[0],[0]
", is drawn by iteratively break-
1 Oberlin College, Oberlin, OH, USA 2The University of Arizona, Tucson, AZ, USA.",1. Introduction and Background,[0],[0]
"Correspondence to: Colin Reimer Dawson <cdawson@oberlin.edu>.
",1. Introduction and Background,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction and Background,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction and Background,[0],[0]
"ing a “stick” off of the remaining weight according to a Beta (1, γ) distribution.",1. Introduction and Background,[0],[0]
"The parameter γ > 0 is known as the concentration parameter and governs how quickly the weights tend to decay, with large γ corresponding to slow decay, and hence more weights needed before a given cumulative weight is reached.",1. Introduction and Background,[0],[0]
"This stick-breaking process is denoted by GEM (Ewens, 1990; Sethuraman, 1994) for Griffiths, Engen and McCloskey.",1. Introduction and Background,[0],[0]
"We thus have a discrete probability measure, G0, with weights βj at locations θj , j = 1, 2, . . .",1. Introduction and Background,[0],[0]
", defined by
θj i.i.d.∼ H β ∼ GEM(γ).",1. Introduction and Background,[0],[0]
"(1)
G0 drawn in this way is a Dirichlet Process (DP) random measure with concentration γ and base measure H .
",1. Introduction and Background,[0],[0]
"The actual transition distribution, πj , from state j, is drawn from another DP with concentration α and base measure G0:
πj i.i.d.∼ DP(αG0) j = 0, 1, 2, . . .",1. Introduction and Background,[0],[0]
"(2)
where π0 represents the initial distribution.",1. Introduction and Background,[0],[0]
"The hidden state sequence, z1, z2, . . .",1. Introduction and Background,[0],[0]
"zT is then generated according to z1 |π0 ∼ Cat(π0), and
zt | zt−1,πzt−1 ∼ Cat(πzt−1) t = 1, 2, . . .",1. Introduction and Background,[0],[0]
", T (3)
",1. Introduction and Background,[0],[0]
"Finally, the emission distribution for state j is a function of θj , so that observation yt is drawn according to
yt | zt, θzt ∼ F (θzt) (4)
",1. Introduction and Background,[0],[0]
"A shortcoming of the HDP prior on the transition matrix is that it does not use the fact that the source and destination states are the same set: that is, each πj has a special element which corresponds to a self-transition.",1. Introduction and Background,[0],[0]
"In the HDPHMM, however, self-transitions are no more likely a priori than transitions to any other state.",1. Introduction and Background,[0],[0]
"The Sticky HDP-HMM (Fox et al., 2008) addresses this issue by adding an extra mass κ at location j to the base measure of the DP that generates πj .",1. Introduction and Background,[0],[0]
"That is, (2) is replaced by
πj ∼ DP(αG0 + κδθj ).",1. Introduction and Background,[0],[0]
"(5)
An alternative approach that treats self-transitions as special is the HDP Hidden Semi-Markov Model (HDPHSMM; Johnson & Willsky (2013)), wherein state duration distributions are modeled separately, and ordinary selftransitions are ruled out.",1. Introduction and Background,[0],[0]
"However, while both of these
models have the ability to privilege self-transitions, they contain no notion of similarity for pairs of states that are not identical: in both cases, when the transition matrix is integrated out, the prior probability of transitioning to state j′ depends only on the top-level stick weight associated with state j′, and not on the identity or parameters of the previous state j.
The two main contributions of this paper are (1) a generalization of the HDP-HMM, which we call the HDP-HMM with local transitions (HDP-HMM-LT) that allows for a geometric structure to be defined on the latent state space, so that “nearby” states are a priori more likely to have transitions between them, and (2) a simple Gibbs sampling algorithm for this model.",1. Introduction and Background,[0],[0]
The “LT” property is introduced by elementwise rescaling and then renormalizing of the HDP transition matrix.,1. Introduction and Background,[0],[0]
"Two versions of the similarity structure are illustrated: in one case, two states are similar to the extent that their emission distributions are similar.",1. Introduction and Background,[0],[0]
"In another, the similarity structure is inferred separately.",1. Introduction and Background,[0],[0]
"In both cases, we give augmented data representations that restore conditional conjugacy and thus allow a simple Gibbs sampling algorithm to be used for inference.
",1. Introduction and Background,[0],[0]
"A rescaling and renormalization approach similar to the one used in the HDP-HMM-LT is used by Paisley et al. (2012) to define their Discrete Infinite Logistic Normal (DILN) model, an instance of a correlated random measure (Ranganath & Blei, 2016), in the setting of topic modeling.",1. Introduction and Background,[0],[0]
"There, however, the contexts and the mixture components (topics) are distinct sets, and there is no notion of temporal dependence.",1. Introduction and Background,[0],[0]
Zhu et al. (2016) developed an HMM based directly on the DILN model1.,1. Introduction and Background,[0],[0]
"Both Paisley et al. and Zhu et al. employ variational approximations, whereas we present a Gibbs sampler, which converges asymptotically to the true posterior.",1. Introduction and Background,[0],[0]
"We discuss additional differences between our model and the DILN-HMM in Sec. 2.2.
",1. Introduction and Background,[0],[0]
"One class of application in which it is useful to incorporate a notion of locality occurs when the latent state sequence consists of several parallel chains, so that the global state changes incrementally, but where these increments are not independent across chains.",1. Introduction and Background,[0],[0]
"Factorial HMMs (Ghahramani et al., 1997) are commonly used in this setting, but this ignores dependence among chains, and hence may do poorly when some combinations of states are much more probable than suggested by the chain-wise dynamics.
",1. Introduction and Background,[0],[0]
"Another setting where the LT property is useful is when there is a notion of state geometry that licenses syllogisms: e.g., if A frequently leads to B and C and B frequently leads to D and E, then it may be sensible to infer that A and C may lead to D and E as well.",1. Introduction and Background,[0],[0]
"This property is arguably
1We thank an anonymous ICML reviewer for bringing this paper to our attention.
present in musical harmony, where consecutive chords are often (near-)neighbors in the “circle of fifths”, and small steps along the circle are more common than large ones.
",1. Introduction and Background,[0],[0]
The paper is structured as follows:,1. Introduction and Background,[0],[0]
In section 2 we define the model.,1. Introduction and Background,[0],[0]
"In section 3, we develop a Gibbs sampling algorithm based on an augmented data representation, which we call the Markov Jump Process with Failed Transitions (MJP-FT).",1. Introduction and Background,[0],[0]
"In section 4 we test two versions of the model: one on a speaker diarization task in which the speakers are inter-dependent, and another on a fourpart chorale corpus, demonstrating performance improvements over state-of-the-art models when “local transitions” are more common in the data.",1. Introduction and Background,[0],[0]
"Using sythetic data from an HDP-HMM, we show that the LT variant can learn not to use its similarity bias when the data does not support it.",1. Introduction and Background,[0],[0]
"Finally, in section 5, we conclude and discuss the relationships between the HDP-HMM-LT and existing HMM variants.",1. Introduction and Background,[0],[0]
Code and additional details are available at http://colindawson.net/hdp-hmm-lt/,1. Introduction and Background,[0],[0]
"We wish to add to the transition model the concept of a transition to a “nearby” state, where transitions between states j and j′ are more likely a priori to the extent that they are “nearby” in some similarity space.",2. An HDP-HMM With Local Transitions,[0],[0]
"In order to accomplish this, we first consider an alternative construction of the transition distributions, based on the Normalized Gamma Process representation of the DP (Ishwaran & Zarepour, 2002; Ferguson, 1973).",2. An HDP-HMM With Local Transitions,[0],[0]
"The Dirichlet Process is an instance of a normalized completely random measure (Kingman, 1967; Ferguson, 1973), that can be defined as G = ∑∞",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"k=1 π̃kδθk , where
πk ind.∼ Gamma(αβk, 1) T = ∞∑ k=1 πk π̃k = πk T , (6)
δθ is a measure assigning 1 to sets if they contain θ and 0 otherwise, and subject to the constraint that ∑ k≥1 βk = 1 and 0",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
< α <∞.,2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"It has been shown (Ferguson, 1973; Paisley et al., 2012; Favaro et al., 2013) that the normalization constant T is positive and finite almost surely, and thatG is distributed as a DP with base measure G0 = ∑∞",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
k=1 βkδθk .,2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"If we draw β = (β1, β2, . . . )",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"from the GEM(γ) stickbreaking process, draw an i.i.d. sequence of θk from a base measure H , and then draw an i.i.d. sequence of random measures, {Gj}, j = 1, 2, . .",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
.,2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
", from the above process, this defines a Hierarchical Dirichlet Process (HDP).",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"If each Gj is associated with the hidden states of an HMM, π is the infinite matrix where entry πjj′ is the j′th mass associated
with the jth random measure, and Tj is the sum of row j, then we obtain the prior for the HDP-HMM, where
p(zt | zt−1,π) =",2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
π̃zt−1zt = πjj′/Tj (7),2.1. A Normalized Gamma Process representation of the HDP-HMM,[0],[0]
"In the HDP prior, the rows of the transition matrix are conditionally independent.",2.2. Promoting “Local” Transitions,[0],[0]
"We wish to relax this assumption, to incorporate possible prior knowledge that certain pairs of states are “nearby” in some sense and thus more likely than others to produce large transition weights between them (in both directions); that is, transitions are likely to be “local”.",2.2. Promoting “Local” Transitions,[0],[0]
"We accomplish this by associating each latent state j with a location `j in some space Ω, introducing a “similarity function” φ :",2.2. Promoting “Local” Transitions,[0],[0]
"Ω × Ω → (0, 1], and scaling each element πjj′ by φjj′ = φ(`j , `j′).",2.2. Promoting “Local” Transitions,[0],[0]
"For example, we might wish to define a (possibly asymmetric) divergence function d : Ω× Ω→ [0,∞) and set φ(`j , `j) = exp{−d(`j , `j′)} so that transitions are less likely the farther apart two states are.",2.2. Promoting “Local” Transitions,[0],[0]
"By setting φ ≡ 1, we obtain the standard HDP-HMM.",2.2. Promoting “Local” Transitions,[0],[0]
"The DILN-HMM (Zhu et al., 2016), employs a similar rescaling of transition probabilities via an exponentiated Gaussian Process, following (Paisley et al., 2012), but the scaling function must be positive semi-definite, and in particular symmetric, whereas in the HDP-HMM-LT, φ need only take values in (0, 1].",2.2. Promoting “Local” Transitions,[0],[0]
"Moreover, the DILN-HMM does not allow the scales to be tied to other state parameters, and hence encode an independent notion of similarity.
",2.2. Promoting “Local” Transitions,[0],[0]
"Letting ` = (`1, `2, . . . )",2.2. Promoting “Local” Transitions,[0],[0]
", we can replace (6) for j ≥ 1 by
πjj′ |β, ` ∼ Gamma(αβj′ , 1), Tj = ∞∑ j′=1 πjj′φjj′
",2.2. Promoting “Local” Transitions,[0],[0]
"π̃jj′ = πjj′φjj′/Tj , p(zt | zt−1,π, `) = π̃zt−1zt .
(8)
Since the φjj′ are positive and bounded above by 1, 0 < πj1φj1 ≤",2.2. Promoting “Local” Transitions,[0],[0]
"Tj ≤ ∑ j′ πjj′ <∞ (9)
almost surely, where the last inequality carries over from the original HDP.",2.2. Promoting “Local” Transitions,[0],[0]
"The prior means of the unnormalized transition distributions, πj are then proportional (for each j) to αβφj where φj = (φj1, φj2, . . . ).
",2.2. Promoting “Local” Transitions,[0],[0]
"The distribution of the latent state sequence z given π and ` is now
p(z |π, `) = T∏ t=1 πzt−1ztφzt−1ztT −nzt−1· zt−1
= ∞∏ j=1 T−1j ∞∏",2.2. Promoting “Local” Transitions,[0],[0]
j′=1,2.2. Promoting “Local” Transitions,[0],[0]
π,2.2. Promoting “Local” Transitions,[0],[0]
"njj′ jj′ φ njj′ jj′
(10)
where njj′ = ∑T t=1 I(zt−1 = j, zt = j
′) is the number of transitions from state j to state j′ in the sequence z
and nj· = ∑ j′ njj′ is the total number of visits to state j. Since Tj is a sum over products of πjj′ and φjj′ terms, the posterior for π is no longer a DP.",2.2. Promoting “Local” Transitions,[0],[0]
"However, conditional conjugacy can be restored by a data-augmentation process with a natural interpretation, which is described next.",2.2. Promoting “Local” Transitions,[0],[0]
"In this section, we define a stochastic process that we call the Markov Jump Process with Failed Transitions (MJP-FT), from which we obtain the HDP-HMM-LT by marginalizing over some of the variables.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"By reinstating these auxiliary variables, we obtain a simple Gibbs sampling algorithm over the full MJP-FT, which can be used to sample from the marginal posterior of the variables used by the HDP-HMM-LT.
Let β, π, ` and Tj , j = 1, 2, . . .",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
be defined as in the last section.,2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Consider a continuous-time Markov Process over the states j = 1, 2, . . .",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
", and suppose that if the process makes a jump to state zt at time τt, the next jump, which is to state zt+1, occurs at time τt + ũt, where ũt ∼ Exp( ∑ j′ πjj′), and p(zt+1 = j
′",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
| zt = j) ∝,2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"πjj′ , independent of ũt.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Note that in this formulation, unlike in standard formulations of Markov Jump Processes, we are assuming that self-jumps are possible.
",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"If we only observe the jump sequence z and not the holding times ũt, this is an ordinary Markov chain with transition matrix row-proportional to π.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"If we do not observe the jumps directly, but instead an observation is generated once per jump from a distribution that depends on the state being jumped to, then we have an ordinary HMM whose transition matrix is obtained by normalizing π; that is, we have the HDP-HMM.
",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
We modify this process as follows.,2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Suppose each jump attempt from state j to state j′ has probability (1 − φjj′) of failing, in which case no transition occurs and no observation is generated.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Assuming independent failures, the rates of successful and failed jumps from j to j′ are πjj′φjj′ and πjj′(1 − φjj′), respectively.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"The probability that the first successful jump is to state j′ (that is, that zt+1 = j′) is proportional to the rate of successful jump attempts to j′, which is πjj′φjj′ .",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Conditioned on zt, the holding time, ũt, is independent of zt+1 and is distributed as Exp(Tzt).",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"We denote the total time spent in state j by uj = ∑ t:zt=j
ũt, where, as the sum of i.i.d.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Exponentials,
uj | z,π,θ ind.∼ Gamma(nj·, Tj) (11)
",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"During this period there will be qjj′ failed attempts to jump to state j′, where qjj′ ∼ Poisson(ujπjj′(1− φjj′)) are independent.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"This data augmentation bears some conceptual similarity to the Geometrically distributed ρ auxiliary variables introduced to the HDP-HSMM (Johnson & Willsky,
2013) to restore conditional conjugacy.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"However, there are key differences: first, ρ measure how many steps the chain would have remained in state j under Markovian dynamics, whereas our u represents putative continuous holding times between each transition, and second ρ allows for the restoration of a zeroed out entry in each row, whereas u allows us to work with unnormalized π entries, avoiding the need to restore zeroed out entries in the HSMM-LT
Incorporating u = {uj} and Q = {qjj′} as augmented data simplifies the likelihood for π, yielding
p(z,u,Q |π) =",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"p(z |π)p(u | z,π)p(Q |u,π) (12)
where dependence on ` has been omitted for conciseness.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"After grouping terms and omitting terms that do not depend on π, this proportional (as a function of π) to∏
j ∏ j′",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
π njj′+qjj′ jj′ φ njj′ jj′ (1− φjj′) qjj′,2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"e−πjj′uj (13)
",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"Conveniently, the Tj have canceled, and the exponential terms involving πjj′ and φjj′ in the Gamma and Poisson distributions of uj and qjj′ combine to cause φjj′ to vanish.",2.3. The HDP-HMM-LT as the Marginalization of a Markov Jump Process with “Failed” Transitions,[0],[0]
"We note that the local transition property of the HDPHMM-LT can be combined with the Sticky property of the Sticky HDP-HMM (Fox et al., 2008), or the nongeometric duration distributions of the HDP-HSMM (Johnson & Willsky, 2013), to add additional prior weight on self-transitions.",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
"In the former case, no changes to inference are needed; one can simply add the the extra mass κ to the shape parameter of the Gamma prior on the πjj , and employ the same auxiliary variable method used by Fox et al. to distinguish “Sticky” from “regular” self-transitions.",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
"For the semi-Markov case, we can fix the diagonal elements of π to zero, and allow Dt observations to be emitted i.i.d.",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
"according to a state-specific duration distribution, and sample the latent state sequence using a suitable semi-Markov message passing algorithm (Johnson & Willsky, 2013).",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
"Inference for the φ matrix is not affected, since the diagonal elements are assumed to be 1.",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
"Unlike in the original representation of the HDP-HSMM, no further dataaugmentation is needed, as the (continuous) durations u already account for the normalization of the π.",2.4. Sticky and Semi-Markov Generalizations,[0],[0]
One setting in which a local transition property is desirable is the case where the latent states encode multiple hidden features at time t as a vector of categories.,2.5. Obtaining the Factorial HMM as a Limiting Case,[0],[0]
"Such problems are often modeled using factorial HMMs (Ghahramani et al., 1997).",2.5. Obtaining the Factorial HMM as a Limiting Case,[0],[0]
"In fact, the HDP-HMM-LT yields the factorial HMM in the limit as α, γ →∞, fixing each row of π to be
uniform with probability 1, so the dynamics are controlled entirely by φ.",2.5. Obtaining the Factorial HMM as a Limiting Case,[0],[0]
"If A(d) is the transition matrix for chain d, then setting φ(`j , `j′) = exp−d(`j , `j′) with asymmetric “divergences” d(`j , `j′) =",2.5. Obtaining the Factorial HMM as a Limiting Case,[0],[0]
"− ∑ d log(A (d) `jd,`j′d
) yields the factorial transition model.",2.5. Obtaining the Factorial HMM as a Limiting Case,[0],[0]
"Nonparametric extensions of the factorial HMM, such as the infinite factorial hidden Markov Model (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al., 2015), have been developed in recent years by making use of the Indian Buffet Process (Ghahramani & Griffiths, 2005) as a state prior.",2.6. An Infinite Factorial HDP-HMM-LT,[0],[0]
"It would be conceptually straightforward to combine the IBP state prior with the similarity bias of the LT model, provided the chosen similarity function is uniformly bounded above on the space of infinite length binary vectors (for example, take φ(u, v) to be the exponentiated negative Hamming distance between u and v).",2.6. An Infinite Factorial HDP-HMM-LT,[0],[0]
"Since the number of differences between two draws from the IBP is finite with probability 1, this yields a reasonable similarity metric.",2.6. An Infinite Factorial HDP-HMM-LT,[0],[0]
"We develop a Gibbs sampling algorithm based on the MJPFT representation described in Sec. 2.3, augmenting the data with the duration variables u, the failed jump attempt count matrix, Q, as well as additional auxiliary variables which we will define below.",3. Inference,[0],[0]
"In this representation the transition matrix is not represented directly, but is a deterministic function of the unscaled transition “rate” matrix, π, and the similarity matrix, φ.",3. Inference,[0],[0]
"The full set of variables is partitioned into blocks: {γ, α, β,π}, {z,u,Q,Λ}, {θ, `}, and {ξ}, where Λ represents a set of auxiliary variables that will be introduced below, θ represents the emission parameters (which may be further blocked depending on the specific choice of model), and ξ represents additional parameters such as any free parameters of the similarity function, φ, and any hyperparameters of the emission distribution.",3. Inference,[0],[0]
"The joint posterior over γ, α, β and π given the augmented data D = (z,u,Q,Λ) will factor as
p(γ,α,β,π | D) = p(γ",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"| D)p(α | D)p(β | γ,D)p(π |α, β,D) (14)
We describe these four factors in reverse order.
",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
Sampling π,3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"Having used data augmentation to simplify the likelihood for π to the factored conjugate form in (13), the individual πjj′ are a posteriori independent
Gamma(αβj′ + njj′ + qjj′ , 1 + uj) distributed.
",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"Sampling β To enable joint sampling of z, we employ a weak limit approximation to the HDP (Johnson & Willsky, 2013), approximating the stick-breaking process for β using a finite Dirichlet distribution with a J components, where J is larger than we expect to need.",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"Due to the product-of-Gammas form, we can integrate out π analytically to obtain the marginal likelihood:
",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"p(β | γ) = Γ(γ/J) J
Γ(γ)",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
∏,3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
j β,3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"γ J−1 j (15)
p(D |β, α) ∝",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
J∏ j=1 (1 + uj) −α ∏ j′ Γ(αβj′,3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"+ njj′ + qjj′) Γ(αβj′)
where we have used the fact that the βj sum to 1 to pull out terms of the form (1 + uj)−αβj′ from the inner product in the likelihood.",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"Following Teh et al. (2006), we can introduce auxiliary variables M = {mjj′}, with
p(mjj′ |βj′ , α,D) ind∝",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"snjj′+qjj′ ,mjj′α mjj′β mjj′ j′",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"(16)
for integer mjj′ ranging between 0 and njj′ + qjj′ ,",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"where sn,m is an unsigned Stirling number of the first kind.",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"The normalizing constant in this distribution cancels the ratio of Gamma functions in the β likelihood, so, letting m·j′ = ∑ jmjj′ and m·· = ∑ j′ m·j′ , the posterior for (the truncated) β is a Dirichlet whose jth mass parameter is γJ +m·j .
",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"Sampling Concentration Parameters Incorporating M into D, we can integrate out β to obtain
p(D |α, γ) ∝ αm··e−",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"∑ j′′ log(1+uj′′ )α
Γ(γ) Γ(γ",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
+m··),3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"× ∏ j Γ( γJ +m·j) Γ( γJ )
(17)
",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
Assuming that α,3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"and γ have Gamma priors with shape and rate parameters aα, bα and aγ , bγ , then
α | D ∼ Gamma(aα +m··, bα + ∑ j log(1 + uj)).",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"(18)
To simplify the likelihood for γ, we can introduce a final set of auxiliary variables, r = (r1, . . .",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
", rJ), rj′ ∈ {0, . . .",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
",m·j′} and w ∈ (0, 1) with the following distributions:
p(rj′ =",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"r |m·j′ , γ) ∝ s(m·j′ , r) ( γ J )r (19) p(w |m··γ) ∝",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"wγ−1(1− w)m··−1 (20)
The normalizing constants are ratios of Gamma functions, which cancel those in (17), so that
γ | D, r, w ∼ Gamma(aγ + r·, bγ − log(w))",3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
(21),3.1. Sampling Transition Parameters and Hyperparameters,[0],[0]
"We sample the hidden state sequence, z, jointly with the auxiliary variables, which consist of u, Q, M, r and w.",3.2. Sampling z and the auxiliary variables,[0],[0]
"The joint conditional distribution of these variables is defined directly by the generative model:
p(D) = p(z)p(u | z)p(Q |u)p(M | z,Q)p(r |M)p(w |M)
",3.2. Sampling z and the auxiliary variables,[0],[0]
"Since we are conditioning on the transition matrix, we can sample the entire sequence z jointly with the forwardbackward algorithm, as in an ordinary HMM.",3.2. Sampling z and the auxiliary variables,[0],[0]
"Since we are sampling the labels jointly, this step requiresO(TJ2) computation per iteration, which is the bottleneck of the inference algorithm for reasonably large T or J (other updates are constant in T or in J).",3.2. Sampling z and the auxiliary variables,[0],[0]
"Having done this, we can sample u, Q, M, r andw from their forward distributions.",3.2. Sampling z and the auxiliary variables,[0],[0]
"It is also possible to employ a variant on beam sampling (Van Gael et al., 2008) to speed up each iteration, at the cost of slower mixing, but we did not use this variant here.",3.2. Sampling z and the auxiliary variables,[0],[0]
"Depending on the application, the locations ` may or may not depend on the emission parameters, θ.",3.3. Sampling state and emission parameters,[0],[0]
"If not, sampling θ conditional on z is unchanged from the HDP-HMM.",3.3. Sampling state and emission parameters,[0],[0]
"There is no general-purpose method for sampling `, or for sampling θ in the dependent case, due to the dependence on the form of φ and on the emission model, but specific instances are illustrated in the experiments below.",3.3. Sampling state and emission parameters,[0],[0]
"The parameter space for the hidden states, the associated prior H on θ, and the similarity function φ, is applicationspecific; we consider here two cases.",4. Experiments,[0],[0]
"The first is a speakerdiarization task, where each state consists of a finite Ddimensional binary vector whose entries indicate which speakers are currently speaking.",4. Experiments,[0],[0]
"In this experiment, the state vectors both determine the pairwise similarities and partially determine the emission distributions via a linearGaussian model.",4. Experiments,[0],[0]
"In the second experiment, the data consists of Bach chorales, and the latent states can be thought of as harmonic contexts.",4. Experiments,[0],[0]
"There, the components of the states that govern similarities are modeled as independent of the emission distributions, which are categorical distributions over four-voice chords.",4. Experiments,[0],[0]
The Data The data was constructed using audio signals collected from the PASCAL 1st Speech Separation Challenge2.,4.1. Cocktail Party,[0],[0]
"The underlying signal consisted ofD = 16 speaker channels recorded at each of T = 2000 time steps, with the
2 http://laslab.org/SpeechSeparationChallenge/
resulting T × D signal matrix, denoted by θ∗, mapped to K = 12 microphone channels via a weight matrix, W. The 16 speakers were grouped into 4 conversational groups of 4, where speakers within a conversation took turns speaking (see Fig. 2).",4.1. Cocktail Party,[0],[0]
"In such a task, there are naively 2D possible states (here, 65536).",4.1. Cocktail Party,[0],[0]
"However, due to the conversational grouping, if at most one speaker in a conversation is speaking at any given time, the state space is constrained, with only ∏ c(sc + 1) states possible, where sc is the number of speakers in conversation c (in this case sc ≡ 4, for a total of 625 possible states).
",4.1. Cocktail Party,[0],[0]
"Each “turn” within a conversation consisted of a single sentence (average duration ∼ 3s) and turn orders within a conversation were randomly generated, with random pauses distributed as N (1/4s, (1/4s)2) inserted between sentences.",4.1. Cocktail Party,[0],[0]
"Every time a speaker has a turn, the sentence is drawn randomly from the 500 sentences uttered by that speaker in the data.",4.1. Cocktail Party,[0],[0]
"The conversations continued for 40s, and the signal was down-sampled to length 2000.",4.1. Cocktail Party,[0],[0]
The ’on’ portions of each speaker’s signal were normalized to have amplitudes with mean 1 and standard deviation 0.5.,4.1. Cocktail Party,[0],[0]
"An additional column of 1s was added to the speaker signal matrix, θ∗, representing background noise.",4.1. Cocktail Party,[0],[0]
"The resulting signal matrix, denoted θ∗, was thus 2000× 17 and the weight matrix, W, was 17× 12.",4.1. Cocktail Party,[0],[0]
"Following Gael et al. (2009) and Valera et al. (2015), the weights were drawn independently from a Unif(0, 1) distribution, and independentN (0, 0.32) noise was added to each entry of the observation matrix.
",4.1. Cocktail Party,[0],[0]
The Model,4.1. Cocktail Party,[0],[0]
"The latent states, θj , are the D-dimensional binary vectors whose dth entry indicates whether or not speaker d is speaking.",4.1. Cocktail Party,[0],[0]
"The locations `j are identified with the binary vectors, `j := θj .",4.1. Cocktail Party,[0],[0]
"We use a Laplacian similarity function on Hamming distance, d0, so that φjj′ := exp(−λd0(`j , `j′)), λ ≥ 0.",4.1. Cocktail Party,[0],[0]
"The emission model is linearGaussian as in the data, with (D + 1) ×K weight matrix W, and T × (D + 1) signal matrix θ∗ whose tth row is θt := (1,θzt), so that yt | z ∼ N",4.1. Cocktail Party,[0],[0]
"(WTθ ∗ t ,Σ).",4.1. Cocktail Party,[0],[0]
"For the experiments discussed here, we assume that Σ is independent of j, but this assumption is easily relaxed if appropriate.
",4.1. Cocktail Party,[0],[0]
"For finite-length binary vector states, the set of possible states is finite, and so it may seem that a nonparametric model is unnecessary.",4.1. Cocktail Party,[0],[0]
"However, if D is reasonably large, likely most of the 2D possible states are vanishingly unlikely (and the number of observations may well be less than 2D), and so we would like to encourage the selection of a sparse set of states.",4.1. Cocktail Party,[0],[0]
"Moreover, there could be more than one state with the same emission parameters, but with different transition dynamics.",4.1. Cocktail Party,[0],[0]
"Next we describe the additional inference steps needed for this version of the model.
",4.1. Cocktail Party,[0],[0]
"Sampling θ / ` Since θj and `j are identified, influencing both the transition matrix and the emission distributions,
both the state sequence z and the observation matrix Y are used in the update.",4.1. Cocktail Party,[0],[0]
"We put independent Beta-Bernoulli priors on each coordinate of θ, and Gibbs sample each coordinate θjd conditioned on all the others and the coordinatewise prior means, {µd}, which we sample in turn conditioned on θ.",4.1. Cocktail Party,[0],[0]
"Details are in the supplement.
",4.1. Cocktail Party,[0],[0]
Sampling λ,4.1. Cocktail Party,[0],[0]
The λ parameter of the similarity function governs the connection between ` and φ.,4.1. Cocktail Party,[0],[0]
"Substituting the definition of φ into (13) yields
p(z,Q | `, λ) ∝",4.1. Cocktail Party,[0],[0]
∏ j ∏ j′,4.1. Cocktail Party,[0],[0]
"e−λdjj′njj′ (1− e−λdjj′ )qjj′ (22)
",4.1. Cocktail Party,[0],[0]
"We put an Exp(bλ) prior on λ, which yields a posterior density
p(λ | z,Q, `) ∝",4.1. Cocktail Party,[0],[0]
e−(bλ+ ∑ j ∑ j′ djj′njj′ ),4.1. Cocktail Party,[0],[0]
"λ (23)
",4.1. Cocktail Party,[0],[0]
× ∏ j ∏ j′,4.1. Cocktail Party,[0],[0]
(1− e−λdjj′ ),4.1. Cocktail Party,[0],[0]
"qjj′
",4.1. Cocktail Party,[0],[0]
"This density is log-concave, and so we use Adaptive Rejection Sampling (Gilks & Wild, 1992) to sample from it.
",4.1. Cocktail Party,[0],[0]
"Sampling W and Σ Conditioned on Y and θ∗, W and Σ can be sampled as in Bayesian linear regression.",4.1. Cocktail Party,[0],[0]
"If each column of W has a multivariate Normal prior, then the columns are a posteriori independent multivariate Normals.",4.1. Cocktail Party,[0],[0]
"For the experiments reported here, we fix W to its ground truth value so that θ∗ can be compared directly with the ground truth signal matrix, and we constrain Σ to be diagonal, with Inverse Gamma priors on the variances, resulting in conjugate updates.
",4.1. Cocktail Party,[0],[0]
"Results We attempted to infer the binary speaker matrices using five models: (1) a binary-state Factorial HMM (Ghahramani et al., 1997), where individual binary speaker sequences are modeled as independent, (2) an ordinary HDP-HMM without local transitions (Teh et al., 2006), where the latent states are binary vectors, (3) a Sticky HDPHMM (Fox et al., 2008), (4) our HDP-HMM-LT model, and (5) a model that combines the Sticky and LT properties3.",4.1. Cocktail Party,[0],[0]
"For all models, all concentration and noise precision parameters are given Gamma(0.1, 0.1) priors.",4.1. Cocktail Party,[0],[0]
"For the Sticky models, the ratio κα+κ is given a Unif(0, 1) prior.",4.1. Cocktail Party,[0],[0]
We evaluated the models at each iteration using both the Hamming distance between inferred and ground truth state matrices and F1 score.,4.1. Cocktail Party,[0],[0]
"We also plot the inferred decay rate λ, and the number of states used by the LT and Sticky-LT models.",4.1. Cocktail Party,[0],[0]
The results for the five models are in Fig. 1.,4.1. Cocktail Party,[0],[0]
"In
3We attempted to add a comparison to the DILN-HMM (Zhu et al., 2016) as well, but code could not be obtained, and the paper did not provide enough detail to reproduce their inference algorithm.
",4.1. Cocktail Party,[0],[0]
"Fig. 2, we plot the ground truth state matrix against the average state matrix, η∗, averaged over runs and post-burn-in iterations.
",4.1. Cocktail Party,[0],[0]
"The LT and Sticky-LT models outperform the others, while the regular Sticky model exhibits only a small advantage over the vanilla HDP-HMM.",4.1. Cocktail Party,[0],[0]
"Both converge on a nonnegligible λ value of about 1.6 (see Fig. 1), suggesting that the local transition structure explains the data well.",4.1. Cocktail Party,[0],[0]
"The LT models also use more states than the non-LT models, perhaps owing to the fact that the weaker transition prior of the non-LT model is more likely to explain nearby similar observations as a single persisting state, whereas the LT model places a higher probability on transitioning to a new state with a similar latent vector.",4.1. Cocktail Party,[0],[0]
"We generated data directly from the ordinary HDP-HMM used in the cocktail experiment as a sanity check, to examine the performance of the LT model in the absence of a similarity bias.",4.2. Synthetic Data Without Local Transitions,[0],[0]
The results are in Fig. 3.,4.2. Synthetic Data Without Local Transitions,[0],[0]
"When the λ parameter is large, the LT model has worse performance than the non-LT model on this data; however, the λ parameter settles near zero as the model learns that local transitions are not more probable.",4.2. Synthetic Data Without Local Transitions,[0],[0]
"When λ = 0, the HDP-HMM-LT is an ordinary HDP-HMM.",4.2. Synthetic Data Without Local Transitions,[0],[0]
"The LT model does not make entirely the same inferences as the non-LT model, however; in particular, the α concentration parameter is larger.",4.2. Synthetic Data Without Local Transitions,[0],[0]
"To some extent, α and λ trade off: sparsity of the transition matrix can be achieved either by beginning with a sparse rate matrix prior to rescaling (α small), or by beginning with a less sparse rate matrix which becomes sparser through rescaling
(larger α and non-zero λ).",4.2. Synthetic Data Without Local Transitions,[0],[0]
"To test a version of the HDP-HMM-LT model in which the components of the latent state governing similarity are unrelated to the emission distributions, we used our model to do unsupervised “grammar” learning from a corpus of Bach chorales.",4.3. Bach Chorales,[0],[0]
"The data was a corpus of 217 four-voice major key chorales by J.S. Bach from music214, 200 of which were randomly selected as a training set, with the other 17 used as a test set to evaluate surprisal (marginal log likelihood per observation) by the trained models.",4.3. Bach Chorales,[0],[0]
"All chorales were transposed to C-major, and each distinct four-voice chord (with voices ordered) was encoded as a single integer.",4.3. Bach Chorales,[0],[0]
"In total there were 3307 distinct chord types and 20401 chord tokens in the 217 chorales, with 3165 types and 18818 tokens in the 200 training chorales, and 143 chord types that were unique to the test set.
",4.3. Bach Chorales,[0],[0]
"Modifications to Model and Inference Since the chords were encoded as integers, the emission distribution for each state is Cat(θj).",4.3. Bach Chorales,[0],[0]
"We use a symmetric Dirichlet prior for each θj , resulting in conjugate updates to θ conditioned on the latent state sequence, z.
In this experiment, the locations, `j , are independent of the θj , withN (0, I) priors.",4.3. Bach Chorales,[0],[0]
"We use a Gaussian similarity function, φjj := exp{−λd2(`j , `j′)2} where d2 is Euclidean distance.",4.3. Bach Chorales,[0],[0]
"Since the latent states are continuous, we use
4 http://web.mit.edu/music21
a Hamiltonian Monte Carlo (HMC) update (Duane et al., 1987; Neal et al., 2011) to update the `j simultaneously, conditioned on z and π (see the supplement for details).
",4.3. Bach Chorales,[0],[0]
"Results We ran 5 Gibbs chains for 10,000 iterations each using the HDP-HMM-LT, Sticky-HDP-HMM-LT, HDPHMM and Sticky-HDP-HMM models on the 200 training chorales, which were modeled as conditionally independent of one another.",4.3. Bach Chorales,[0],[0]
We evaluated the marginal log likelihood on the 17 test chorales (integrating out z) at every 50th iteration.,4.3. Bach Chorales,[0],[0]
The training and test log likelihoods are in Fig. 4.,4.3. Bach Chorales,[0],[0]
"Although the LT model does not achieve as close a fit to the training data, its generalization performance is better, suggesting that the vanilla HDP-HMM is overfitting.",4.3. Bach Chorales,[0],[0]
"This is perhaps counterintuitive, since the LT model is more flexible, and might be expected to be more prone to overfitting.",4.3. Bach Chorales,[0],[0]
"However, the similarity bias induces greater information sharing across parameters, as in a hierarchical model: instead of each entry of the transition matrix being informed mainly by transitions directly involving the corresponding states, it is informed to some extent by all transitions, as they all inform the similarity structure.",4.3. Bach Chorales,[0],[0]
"We have defined a new probabilistic model, the Hierarchical Dirichlet Process Hidden Markov Model with Local Transitions (HDP-HMM-LT), which generalizes the HDP-HMM by allowing state space geometry to be represented via a similarity kernel, making transitions between “nearby” pairs of states (“local” transitions), more likely a priori.",5. Discussion,[0],[0]
"By introducing an augmented data representation,
which we call the Markov Jump Process with Failed Transitions (MJP-FT), we obtain a Gibbs sampling algorithm that simplifies inference in both the LT and ordinary HDPHMM.",5. Discussion,[0],[0]
"When multiple latent chains are interdependent, as in speaker diarization, the HDP-HMM-LT model combines the HDP-HMM’s capacity to discover a small set of joint states with the Factorial HMM’s ability to encode the property that most transitions involve a small number of chains.",5. Discussion,[0],[0]
"The HDP-HMM-LT outperforms both, as well as outperforming the Sticky-HDP-HMM, on a speaker diarization task in which speakers form conversational groups.",5. Discussion,[0],[0]
"Despite the addition of the similarity kernel, the HDP-HMM-LT is able to suppress its local transition prior when the data does not support it, achieving identical performance to the HDPHMM on data generated directly from the latter.
",5. Discussion,[0],[0]
"The local transition property is particularly clear when transitions occur at different times for different latent features, as with binary vector-valued states in the cocktail party setting, but the model can be used with any state space equipped with a suitable similarity kernel.",5. Discussion,[0],[0]
"Similarities need not be defined in terms of emission parameters; state “locations” can be represented and inferred separately, which we demonstrate using Bach chorale data.",5. Discussion,[0],[0]
"There, the LT model achieves better predictive performance on a held-out test set, while the ordinary HDP-HMM overfits the training set: the LT property here acts to encourage a concise harmonic representation where chord contexts are arranged in bidirectional functional relationships.
",5. Discussion,[0],[0]
"We focused on fixed-dimension binary vectors for the cocktail party and synthetic data experiments, but it would be straightforward to add the LT property to a model with nonparametric latent states, such as the iFHMM (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al., 2015), both of which use the Indian Buffet Process (IBP) (Ghahramani & Griffiths, 2005) as a state prior.",5. Discussion,[0],[0]
"The similarity function used here could be employed without changes: since only finitely many coordinates are non-zero in the IBP, the distance between any two states is finite.",5. Discussion,[0],[0]
This work was funded in part by DARPA grant W911NF14-1-0395 under the Big Mechanism Program and DARPA grant W911NF-16-1-0567 under the Communicating with Computers Program.,ACKNOWLEDGMENTS,[0],[0]
We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDPHMM) which is able to encode prior information that state transitions are more likely between “nearby” states.,abstractText,[0],[0]
"This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pairwise similarities, thereby inducing correlations among the transition distributions.",abstractText,[0],[0]
"We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states.",abstractText,[0],[0]
This augmentation restores conditional conjugacy and admits a simple Gibbs sampler.,abstractText,[0],[0]
"We evaluate the model and inference method on a speaker diarization task and a “harmonic parsing” task using fourpart chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.",abstractText,[0],[0]
An Infinite Hidden Markov Model With Similarity-Biased Transitions,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 950–962 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1088",text,[0],[0]
"Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2015), are useful resources for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al., 2009).",1 Introduction,[0],[0]
"However, knowledge bases suffer from incompleteness despite their formidable sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction.
",1 Introduction,[0],[0]
The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base.,1 Introduction,[0],[0]
"By discovering gener-
alizable regularities in known facts, missing ones may be recovered in a faithful way.",1 Introduction,[0],[0]
"Due to its excellent generalization capability, distributed representations, a.k.a. embeddings, have been popularized to address the KBC task (Nickel et al., 2011; Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b).
",1 Introduction,[0],[0]
"As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding.",1 Introduction,[0],[0]
"Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior.",1 Introduction,[0],[0]
"To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space.",1 Introduction,[0],[0]
"Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation.",1 Introduction,[0],[0]
"As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection.",1 Introduction,[0],[0]
"For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity.
",1 Introduction,[0],[0]
"Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem.",1 Introduction,[0],[0]
"Concretely, since the projection spaces are unique to each relation, projection matrices associated with rare relations can only be exposed to very few facts during training, resulting in poor generalization.",1 Introduction,[0],[0]
"For common relations, a similar issue exists.",1 Introduction,[0],[0]
"Without any restrictions on the number of projection matrices, logically related or conceptually similar relations may have distinct projection spaces, hindering the discovery, sharing, and generalization of statistical regularities.
",1 Introduction,[0],[0]
"950
Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem.",1 Introduction,[0],[0]
"In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garcı́a-Durán et al., 2015; Lin et al., 2015a; Shen et al., 2016).",1 Introduction,[0],[0]
"Since the number of paths grows exponentially with its length, as a side effect, path-based models enjoy much more training cases, suffering less from the problem.
",1 Introduction,[0],[0]
"In this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of relations and alleviates the data sparsity problem.",1 Introduction,[0],[0]
"At the core of ITransF is a sparse attention mechanism, which learns to compose shared concept matrices into relation-specific projection matrices, leading to a better generalization property.",1 Introduction,[0],[0]
"Without any external resources, ITransF improves mean rank and Hits@10 on two benchmark datasets, over all previous approaches of the same kind.",1 Introduction,[0],[0]
"In addition, the parameter sharing is clearly indicated by the learned sparse attention vectors, enabling us to interpret how knowledge transfer is carried out.",1 Introduction,[0],[0]
"To induce the desired sparsity during optimization, we further introduce a block iterative optimization algorithm.
",1 Introduction,[0],[0]
"In summary, the contributions of this work are: (i) proposing a novel knowledge embedding model which enables knowledge transfer by learning to discover shared regularities; (ii) introducing a learning algorithm to directly optimize a sparse representation from which the knowledge transferring procedure is interpretable; (iii) showing the effectiveness of our model by outperforming baselines on two benchmark datasets for knowledge base completion task.",1 Introduction,[0],[0]
Let E denote the set of entities and R denote the set of relations.,2 Notation and Previous Models,[0],[0]
"In knowledge base completion, given a training set P of triples (h, r, t) where h, t ∈ E are the head and tail entities having a relation r ∈ R, e.g., (Steve Jobs, FounderOf, Apple), we want to predict missing facts such as (Steve Jobs, Profession, Businessperson).
",2 Notation and Previous Models,[0],[0]
"Most of the embedding models for knowledge base completion define an energy function fr(h, t)
according to the fact’s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b).",2 Notation and Previous Models,[0],[0]
"The models are learned to minimize energy fr(h, t) of a plausible triple (h, r, t) and to maximize energy fr(h′, t′) of an implausible triple (h′, r, t′).
",2 Notation and Previous Models,[0],[0]
"Motivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t ∈",2 Notation and Previous Models,[0],[0]
"Rn respectively, which were trained so that h+r ≈ t.",2 Notation and Previous Models,[0],[0]
"They define the energy function as
fr(h, t) = ‖h+",2 Notation and Previous Models,[0],[0]
"r− t‖` where ` = 1 or 2, which means either the `1 or the `2 norm of the vector h + r",2 Notation and Previous Models,[0],[0]
"− t will be used depending on the performance on the validation set.
",2 Notation and Previous Models,[0],[0]
"To better model relation-specific aspects of the same entity, TransR",2 Notation and Previous Models,[0],[0]
"(Lin et al., 2015b) uses projection matrices and projects the head entity and the tail entity to a relation-dependent space.",2 Notation and Previous Models,[0],[0]
"STransE (Nguyen et al., 2016b) extends TransR by employing different matrices for mapping the head and the tail entity.",2 Notation and Previous Models,[0],[0]
"The energy function is
fr(h, t) =",2 Notation and Previous Models,[0],[0]
"‖Wr,1h+ r−Wr,2t‖` However, not all relations have abundant data to estimate the relation specific matrices as most of the training samples are associated with only a few relations, leading to the data sparsity problem for rare relations.",2 Notation and Previous Models,[0],[0]
"As discussed above, a fundamental weakness in TransR and STransE is that they equip each relation with a set of unique projection matrices, which not only introduces more parameters but also hinders knowledge sharing.",3.1 Model,[0],[0]
"Intuitively, many relations share some concepts with each other, although they are stored as independent symbols in KB.",3.1 Model,[0],[0]
"For example, the relation “(somebody) won award for (some work)” and “(somebody) was nominated for (some work)”",3.1 Model,[0],[0]
both describe a person’s high-quality work which wins an award or a nomination respectively.,3.1 Model,[0],[0]
"This phenomenon suggests that one relation actually represents a collection of real-world concepts, and one concept
can be shared by several relations.",3.1 Model,[0],[0]
"Inspired by the existence of such lower-level concepts, instead of defining a unique set of projection matrices for every relation, we can alternatively define a small set of concept projection matrices and then compose them into customized projection matrices.",3.1 Model,[0],[0]
"Effectively, the relation-dependent translation space is then reduced to the smaller concept spaces.
",3.1 Model,[0],[0]
"However, in general, we do not have prior knowledge about what concepts exist out there and how they are composed to form relations.",3.1 Model,[0],[0]
"Therefore, in ITransF, we propose to learn this information simultaneously from data, together with all knowledge embeddings.",3.1 Model,[0],[0]
"Following this idea, we first present the model details, then discuss the optimization techniques for training.
",3.1 Model,[0],[0]
"Energy function Specifically, we stack all the concept projection matrices to a 3-dimensional tensor D ∈ Rm×n×n, wherem is the pre-specified number of concept projection matrices and n is the dimensionality of entity embeddings and relation embeddings.",3.1 Model,[0],[0]
"We let each relation select the most useful projection matrices from the tensor, where the selection is represented by an attention vector.",3.1 Model,[0],[0]
"The energy function of ITransF is defined as:
fr(h, t) = ‖αHr",3.1 Model,[0],[0]
"·D · h+ r−αTr ·D · t‖` (1)
where αHr ,α T r ∈",3.1 Model,[0],[0]
"[0, 1]m, satisfying ∑ iα
H r,i =∑
iα T r,i = 1, are normalized attention vectors used to compose all concept projection matrices in D by a convex combination.",3.1 Model,[0],[0]
It is obvious that STransE can be expressed as a special case of our model when we use m = 2|R| concept matrices and set attention vectors to disjoint one-hot vectors.,3.1 Model,[0],[0]
"Hence our model space is a generalization of STransE. Note that we can safely use fewer concept matrices in ITransF and obtain better performance (see section 4.3), though STransE always requires 2|R| projection matrices.
",3.1 Model,[0],[0]
"We follow previous work to minimize the following hinge loss function:
L = ∑
(h,r,t)∼P, (h′,r,t′)∼N
[ γ + fr(h, t)− fr(h′, t′) ]",3.1 Model,[0],[0]
"+ (2)
where P is the training set consisting of correct triples, N is the distribution of corrupted triples defined in section 3.3, and [·]+ = max(·, 0).",3.1 Model,[0],[0]
"Note that we have omitted the dependence of N on (h, r, t) to avoid clutter.",3.1 Model,[0],[0]
"We normalize the entity vectors h, t, and the projected entity vectors
αHr ·D · h and αTr ·D · t to have unit length after each update, which is an effective regularization method that benefits all models.
",3.1 Model,[0],[0]
Sparse attention vectors In Eq.,3.1 Model,[0],[0]
"(1), we have defined αHr ,α T r to be some normalized vectors used for composition.",3.1 Model,[0],[0]
"With a dense attention vector, it is computationally expensive to perform the convex combination of m matrices in each iteration.",3.1 Model,[0],[0]
"Moreover, a relation usually does not consist of all existing concepts in practice.",3.1 Model,[0],[0]
"Furthermore, when the attention vectors are sparse, it is often easier to interpret their behaviors and understand how concepts are shared by different relations.
",3.1 Model,[0],[0]
"Motivated by these potential benefits, we further hope to learn sparse attention vectors in ITransF. However, directly posing `1 regularization (Tibshirani, 1996) on the attention vectors fails to produce sparse representations in our preliminary experiment, which motivates us to enforce `0 constraints on αTr ,α H r .
",3.1 Model,[0],[0]
"In order to satisfy both the normalization condition and the `0 constraints, we reparameterize the attention vectors in the following way:
αHr = SparseSoftmax(v H r , I H r )",3.1 Model,[0],[0]
"αTr = SparseSoftmax(v T r , I T r )
",3.1 Model,[0],[0]
"where vHr ,v T r ∈",3.1 Model,[0],[0]
"Rm are the pre-softmax scores, IHr , I T r ∈ {0, 1}m are the sparse assignment vectors, indicating the non-zero entries of attention vectors, and the SparseSoftmax is defined as
SparseSoftmax(v, I)i = exp(vi/τ)Ii∑",3.1 Model,[0],[0]
"j exp(vj/τ)Ij
with τ being the temperature of Softmax.",3.1 Model,[0],[0]
"With this reparameterization, vHr ,v T r and IHr , I T r replace α T r ,α H r to become the real parameters of the model.",3.1 Model,[0],[0]
"Also, note that it is equivalent to pose the `0 constraints on IHr , I T r instead of αTr ,α H r .",3.1 Model,[0],[0]
"Putting these modifications together, we can rewrite the optimization problem as
minimize L subject to ‖IHr ‖0 ≤ k, ‖ITr ‖0 ≤",3.1 Model,[0],[0]
"k
(3)
where L is the loss function defined in Eq.",3.1 Model,[0],[0]
(2).,3.1 Model,[0],[0]
"Though sparseness is favorable in practice, it is generally NP-hard to find the optimal solution under `0 constraints.",3.2 Block Iterative Optimization,[0],[0]
"Thus, we resort to an approximated algorithm in this work.
",3.2 Block Iterative Optimization,[0],[0]
"For convenience, we refer to the parameters with and without the sparse constraints as the sparse partition and the dense partition, respectively.",3.2 Block Iterative Optimization,[0],[0]
"Based on this notion, the high-level idea of the approximated algorithm is to iteratively optimize one of the two partitions while holding the other one fixed.",3.2 Block Iterative Optimization,[0],[0]
"Since all parameters in the dense partition, including the embeddings, the projection matrices, and the pre-softmax scores, are fully differentiable with the sparse partition fixed, we can simply utilize SGD to optimize the dense partition.",3.2 Block Iterative Optimization,[0],[0]
"Then, the core difficulty lies in the step of optimizing the sparse partition (i.e. the sparse assignment vectors), during which we want the following two properties to hold
1.",3.2 Block Iterative Optimization,[0],[0]
"the sparsity required by the `0 constaint is maintained, and
2.",3.2 Block Iterative Optimization,[0],[0]
the cost define by Eq.,3.2 Block Iterative Optimization,[0],[0]
"(2) is decreased.
",3.2 Block Iterative Optimization,[0],[0]
Satisfying the two criterion seems to highly resemble the original problem defined in Eq.,3.2 Block Iterative Optimization,[0],[0]
(3).,3.2 Block Iterative Optimization,[0],[0]
"However, the dramatic difference here is that with parameters in the dense partition regarded as constant, the cost function is decoupled w.r.t.",3.2 Block Iterative Optimization,[0],[0]
each relation r.,3.2 Block Iterative Optimization,[0],[0]
"In other words, the optimal choice of IHr , I T r is independent of I H r′ , I T r′ for any r
′",3.2 Block Iterative Optimization,[0],[0]
6=,3.2 Block Iterative Optimization,[0],[0]
"r. Therefore, we only need to consider the optimization for a single relation r, which is essentially an assignment problem.",3.2 Block Iterative Optimization,[0],[0]
"Note that, however, IHr and ITr are still coupled, without which we basically reach the situation in a backpack problem.",3.2 Block Iterative Optimization,[0],[0]
"In principle, one can explore combinatorial optimization techniques to optimize IHr′ , I T r′ jointly, which usually involve some iterative procedure.",3.2 Block Iterative Optimization,[0],[0]
"To avoid adding another inner loop to our algorithm, we turn to a simple but fast approximation method based on the following single-matrix cost.
",3.2 Block Iterative Optimization,[0],[0]
"Specifically, for each relation r, we consider the induced cost LHr,i where only a single projection matrix i is used for the head entity:
LHr,i = ∑
(h,r,t)∼Pr, (h′,r,t′)∼Nr
[ γ + fHr,i(h, t)− fHr,i(h′, t′) ]",3.2 Block Iterative Optimization,[0],[0]
"+
where fHr,i(h, t) = ‖Di · h + r",3.2 Block Iterative Optimization,[0],[0]
"− αTr · D · t‖ is the corresponding energy function, and the subscript in Pr and Nr denotes the subsets with relation r. Intuitively, LHr,i measures, given the current tail attention vector αTr , if only one project matrix could be chosen for the head entity, how implausible Di would be.",3.2 Block Iterative Optimization,[0],[0]
"Hence, i∗ = argmini LHr,i gives
us the best single projection matrix on the head side given αTr .
",3.2 Block Iterative Optimization,[0],[0]
"Now, in order to choose the best k matrices, we basically ignore the interaction among projection matrices, and update IHr in the following way:
IHr,i ← { 1, i ∈ argpartitioni(LHr,i, k) 0, otherwise
where the function argpartitioni(xi, k) produces the index set of the lowest-k values of xi.
",3.2 Block Iterative Optimization,[0],[0]
"Analogously, we can define the single-matrix cost LTr,i and the energy function fTr,i(h, t) on the tail side in a symmetric way.",3.2 Block Iterative Optimization,[0],[0]
"Then, the update rule for IHr follows the same derivation.",3.2 Block Iterative Optimization,[0],[0]
"Admittedly, the approximation described here is relatively crude.",3.2 Block Iterative Optimization,[0],[0]
"But as we will show in section 4, the proposed algorithm yields good performance empirically.",3.2 Block Iterative Optimization,[0],[0]
We leave the further improvement of the optimization method as future work.,3.2 Block Iterative Optimization,[0],[0]
"Recall that we need to sample a negative triple (h′, r, t′) to compute hinge loss shown in Eq. 2, given a positive triple (h, r, t) ∈ P .",3.3 Corrupted Sample Generating Method,[0],[0]
"The distribution of negative triple is denoted by N(h, r, t).",3.3 Corrupted Sample Generating Method,[0],[0]
"Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.
",3.3 Corrupted Sample Generating Method,[0],[0]
"However, uniformly sampling corrupted entities may not be optimal.",3.3 Corrupted Sample Generating Method,[0],[0]
"Often, the head and tail entities associated a relation can only belong to a specific domain.",3.3 Corrupted Sample Generating Method,[0],[0]
"When the corrupted entity comes from other domains, it is very easy for the model to induce a large energy gap between true triple and corrupted one.",3.3 Corrupted Sample Generating Method,[0],[0]
"As the energy gap exceeds γ, there will be no training signal from this corrupted triple.",3.3 Corrupted Sample Generating Method,[0],[0]
"In comparison, if the corrupted entity comes from the same domain, the task becomes harder for the model, leading to more consistent training signal.
",3.3 Corrupted Sample Generating Method,[0],[0]
"Motivated by this observation, we propose to sample corrupted head or tail from entities in the same domain with a probability pr and from the whole entity set with probability 1 − pr.",3.3 Corrupted Sample Generating Method,[0],[0]
The choice of relation-dependent probability pr is specified in Appendix A.1.,3.3 Corrupted Sample Generating Method,[0],[0]
"In the rest of the paper, we refer to the new proposed sampling method as ”domain sampling”.",3.3 Corrupted Sample Generating Method,[0],[0]
"To evaluate link prediction, we conduct experiments on the WN18 (WordNet) and FB15k (Freebase) introduced by Bordes et al. (2013) and use the same training/validation/test split as in (Bordes et al., 2013).",4.1 Setup,[0],[0]
"The information of the two datasets is given in Table 1.
",4.1 Setup,[0],[0]
"In knowledge base completion task, we evaluate model’s performance of predicting the head entity or the tail entity given the relation and the other entity.",4.1 Setup,[0],[0]
"For example, to predict head given relation r and tail t in triple (h, r, t), we compute the energy function fr(h′, t) for each entity h′ in the knowledge base and rank all the entities according to the energy.",4.1 Setup,[0],[0]
"We follow Bordes et al. (2013) to report the filter results, i.e., removing all other correct candidates h′ in ranking.",4.1 Setup,[0],[0]
The rank of the correct entity is then obtained and we report the mean rank (mean of the predicted ranks) and Hits@10 (top 10 accuracy).,4.1 Setup,[0],[0]
Lower mean rank or higher Hits@10 mean better performance.,4.1 Setup,[0],[0]
"We initialize the projection matrices with identity matrices added with a small noise sampled from normal distribution N (0, 0.0052).",4.2 Implementation Details,[0],[0]
"The entity and relation vectors of ITransF are initialized by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garcı́a-Durán et al. (2016, 2015); Lin et al. (2015a).",4.2 Implementation Details,[0],[0]
We ran minibatch SGD until convergence.,4.2 Implementation Details,[0],[0]
"We employ the “Bernoulli” sampling method to generate incorrect triples as used in Wang et al. (2014), Lin et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a).
",4.2 Implementation Details,[0],[0]
"STransE (Nguyen et al., 2016b) is the most similar knowledge embedding model to ours except that they use distinct projection matrices for each relation.",4.2 Implementation Details,[0],[0]
"We use the same hyperparameters as used in STransE and no significant improvement is ob-
served when we alter hyperparameters.",4.2 Implementation Details,[0],[0]
"We set the margin γ to 5 and dimension of embedding n to 50 for WN18, and γ = 1, n = 100 for FB15k.",4.2 Implementation Details,[0],[0]
We set the batch size to 20 for WN18 and 1000 for FB15k.,4.2 Implementation Details,[0],[0]
The learning rate is 0.01 on WN18 and 0.1 on FB15k.,4.2 Implementation Details,[0],[0]
We use 30 matrices on WN18 and 300 matrices on FB15k.,4.2 Implementation Details,[0],[0]
"All the models are implemented with Theano (Bergstra et al., 2010).",4.2 Implementation Details,[0],[0]
The Softmax temperature is set to 1/4.,4.2 Implementation Details,[0],[0]
The overall link prediction results1 are reported in Table 2.,4.3 Results & Analysis,[0],[0]
Our model consistently outperforms previous models without external information on both the metrics of WN18 and FB15k.,4.3 Results & Analysis,[0],[0]
"On WN18, we even achieve a much better mean rank with comparable Hits@10 than current state-of-the-art model IRN employing external information.
",4.3 Results & Analysis,[0],[0]
We can see that path information is very helpful on FB15k and models taking advantage of path information outperform intrinsic models by a significant margin.,4.3 Results & Analysis,[0],[0]
"Indeed, a lot of facts are easier to recover with the help of multi-step inference.",4.3 Results & Analysis,[0],[0]
"For example, if we know Barack Obama is born in Honolulu, a city in the United States, then we easily know the nationality of Obama is the United States.",4.3 Results & Analysis,[0],[0]
"An straightforward way of extending our proposed model to k-step path P = {ri}ki=1 is to define a path energy function ‖αHP · D · h +∑
ri∈P ri",4.3 Results & Analysis,[0],[0]
"− αTP · D · t‖`, αHP is a concept association related to the path.",4.3 Results & Analysis,[0],[0]
"We plan to extend our model to multi-step path in the future.
",4.3 Results & Analysis,[0],[0]
"To provide a detailed understanding why the proposed model achieves better performance, we present some further analysis in the sequel.
",4.3 Results & Analysis,[0],[0]
"Performance on Rare Relations In the proposed ITransF, we design an attention mechanism to encourage knowledge sharing across different relations.",4.3 Results & Analysis,[0],[0]
"Naturally, facts associated with rare relations should benefit most from such sharing, boosting the overall performance.",4.3 Results & Analysis,[0],[0]
"To verify this hypothesis, we investigate our model’s performance on relations with different frequency.
",4.3 Results & Analysis,[0],[0]
"The overall distribution of relation frequencies resembles that of word frequencies, subject to the zipf’s law.",4.3 Results & Analysis,[0],[0]
"Since the frequencies of relations approximately follow a power distribution, their log
1Note that although IRN (Shen et al., 2016) does not explicitly exploit path information, it performs multi-step inference through the multiple usages of external memory.",4.3 Results & Analysis,[0],[0]
"When IRN is allowed to access memory once for each prediction, its Hits@10 is 80.7, similar to models without path information.
",4.3 Results & Analysis,[0],[0]
frequencies are linear.,4.3 Results & Analysis,[0],[0]
The statistics of relations on FB15k and WN18 are shown in Figure 1.,4.3 Results & Analysis,[0],[0]
"We can clearly see that the distributions exhibit long tails, just like the Zipf’s law for word frequency.
",4.3 Results & Analysis,[0],[0]
"In order to study the performance of relations with different frequencies, we sort all relations by their frequency in the training set, and split them into 3 buckets evenly so that each bucket has a similar interval length of log frequency.
",4.3 Results & Analysis,[0],[0]
"Within each bucket, we compare our model with STransE, as shown in Figure 2.2 As we can see, on WN18, ITransF outperforms STransE by a significant margin on rare relations.",4.3 Results & Analysis,[0],[0]
"In particular, in the last bin (rarest relations), the average Hits@10 increases from 55.2 to 93.8, showing the great benefits of transferring statistical strength from common relations to rare ones.",4.3 Results & Analysis,[0],[0]
The comparison on each relation is shown in Appendix A.2.,4.3 Results & Analysis,[0],[0]
"On FB15k, we can also observe a similar pattern, although the degree of improvement is less significant.",4.3 Results & Analysis,[0],[0]
"We conjecture the difference roots in the fact that many rare relations on FB15k have disjoint domains, knowledge transfer through common concepts is harder.
",4.3 Results & Analysis,[0],[0]
"Interpretability In addition to the quantitative evidence supporting the effectiveness of knowledge sharing, we provide some intuitive examples to show how knowledge is shared in our model.",4.3 Results & Analysis,[0],[0]
"As
2Domain sampling is not employed.
",4.3 Results & Analysis,[0],[0]
"we mentioned earlier, the sparse attention vectors fully capture the association between relations and concepts and hence the knowledge transfer among relations.",4.3 Results & Analysis,[0],[0]
"Thus, we visualize the attention vectors for several relations on both WN18 and FB15K in Figure 3.
",4.3 Results & Analysis,[0],[0]
"For WN18, the words “hyponym” and “hypernym” refer to words with more specific or general meaning respectively.",4.3 Results & Analysis,[0],[0]
"For example, PhD is a hyponym of student and student is a hypernym of PhD.",4.3 Results & Analysis,[0],[0]
"As we can see, concepts associated with the head entities in one relation are also associated with the tail entities in its reverse relation.",4.3 Results & Analysis,[0],[0]
"Further, “instance hypernym” is a special hypernym with the head entity being an instance, and the tail entity being an abstract notion.",4.3 Results & Analysis,[0],[0]
"A typical example is (New York,instance hypernym, city).",4.3 Results & Analysis,[0],[0]
"This connection has also been discovered by our model, indicated by the fact that “instance hypernym(T)” and “hypernym(T)” share a common concept matrix.",4.3 Results & Analysis,[0],[0]
"Finally, for symmetric relations like “similar to”, we see the head attention is identical to the tail attention, which well matches our intuition.
",4.3 Results & Analysis,[0],[0]
"On FB15k, we also see the sharing between reverse relations, as in “(somebody) won award for (some work)” and “(some work) award winning work (somebody)”.",4.3 Results & Analysis,[0],[0]
"What’s more, although relation “won award for” and “was nominated for” share the same concepts,
their attention distributions are different, suggesting distinct emphasis.",4.3 Results & Analysis,[0],[0]
"Finally, symmetric relations like spouse behave similarly as mentioned before.
",4.3 Results & Analysis,[0],[0]
Model Compression A byproduct of parameter sharing mechanism employed by ITransF is a much more compact model with equal performance.,4.3 Results & Analysis,[0],[0]
"Figure 5 plots the average performance of ITransF against the number of projection matrices m, together with two baseline models.",4.3 Results & Analysis,[0],[0]
"On FB15k, when we reduce the number of matrices from 2200 to 30 (∼ 90× compression), our model performance decreases by only 0.09% on Hits@10, still outperforming STransE. Similarly, on WN18, ITransF continues to achieve the best performance when we reduce the number of concept project matrices to 18.",4.3 Results & Analysis,[0],[0]
Sparseness is desirable since it contribute to interpretability and computational efficiency of our model.,5 Analysis on Sparseness,[0],[0]
"We investigate whether enforcing sparseness would deteriorate the model performance and compare our method with another sparse encoding methods in this section.
",5 Analysis on Sparseness,[0],[0]
"Dense Attention w/o `1 regularization Although `0 constrained model usually enjoys many practical advantages, it may deteriorate the model performance when applied improperly.",5 Analysis on Sparseness,[0],[0]
"Here, we show that our model employing sparse attention can achieve similar results with dense attention with a significantly less computational burden.",5 Analysis on Sparseness,[0],[0]
We also compare dense attention with `1 regularization.,5 Analysis on Sparseness,[0],[0]
We set the `1 coefficient to 0.001 in our experiments and does not apply Softmax since the `1 of a vector after Softmax is always 1.,5 Analysis on Sparseness,[0],[0]
"We compare models in a setting where the computation time of
dense attention model is acceptable3.",5 Analysis on Sparseness,[0],[0]
"We use 22 weight matrices on WN18 and 15 weight matrices on FB15k and train both the models for 2000 epochs.
",5 Analysis on Sparseness,[0],[0]
The results are reported in Table 3.,5 Analysis on Sparseness,[0],[0]
"Generally, ITransF with sparse attention has slightly better or comparable performance comparing to dense attention.",5 Analysis on Sparseness,[0],[0]
"Further, we show the attention vectors of
3With 300 projection matrices, it takes 1h1m to run one epoch for a model with dense attention.
model with `1 regularized dense attention in Figure 4.",5 Analysis on Sparseness,[0],[0]
"We see that `1 regularization does not produce a sparse attention, especially on FB15k.
",5 Analysis on Sparseness,[0],[0]
"Nonnegative Sparse Encoding In the proposed model, we induce the sparsity by a carefully designed iterative optimization procedure.",5 Analysis on Sparseness,[0],[0]
"Apart from this approach, one may utilize sparse encoding techniques to obtain sparseness based on the pretrained projection matrices from STransE. Concretely, stacking |2R| pretrained projection
matrices into a 3-dimensional tensor X ∈ R2|R|×n×n, similar sparsity can be induced by solving an `1-regularized tensor completion problem minA,D ||X − DA||22 + λ‖A‖`1 .",5 Analysis on Sparseness,[0],[0]
"Basically, A plays the same role as the attention vectors in our model.",5 Analysis on Sparseness,[0],[0]
"For more details, we refer readers to (Faruqui et al., 2015).
",5 Analysis on Sparseness,[0],[0]
"For completeness, we compare our model with the aforementioned approach4.",5 Analysis on Sparseness,[0],[0]
The comparison is summarized in table 4.,5 Analysis on Sparseness,[0],[0]
"On both benchmarks, ITransF achieves significant improvement against sparse encoding on pretrained model.",5 Analysis on Sparseness,[0],[0]
This performance gap should be expected since the objective function of sparse encoding methods is to minimize the reconstruction loss rather than optimize the criterion for link prediction.,5 Analysis on Sparseness,[0],[0]
"In KBC, CTransR (Lin et al., 2015b) enables relation embedding sharing across similar relations, but they cluster relations before training rather than learning it in a principled way.",6 Related Work,[0],[0]
"Further, they do not solve the data sparsity problem because there is no sharing of projection matrices which have a lot more parameters.",6 Related Work,[0],[0]
"Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al., 2015).
",6 Related Work,[0],[0]
Data sparsity is a common problem in many fields.,6 Related Work,[0],[0]
"Transfer learning (Pan and Yang, 2010) has been shown to be promising to transfer knowl-
4We use the toolkit provided by (Faruqui et al., 2015).
edge and statistical strengths across similar models or languages.",6 Related Work,[0],[0]
"For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition.",6 Related Work,[0],[0]
"Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages.
",6 Related Work,[0],[0]
"Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values.",6 Related Work,[0],[0]
"However, the sorting operation in these works is not GPU-friendly.
",6 Related Work,[0],[0]
"The block iterative optimization algorithm in our work is inspired by LightRNN (Li et al., 2016).",6 Related Work,[0],[0]
They allocate every word in the vocabulary in a table.,6 Related Work,[0],[0]
A word is represented by a row vector and a column vector depending on its position in the table.,6 Related Work,[0],[0]
They iteratively optimize embeddings and allocation of words in tables.,6 Related Work,[0],[0]
"In summary, we propose a knowledge embedding model which can discover shared hidden concepts, and design a learning algorithm to induce the interpretable sparse representation.",7 Conclusion and Future Work,[0],[0]
"Empirically, we show our model can improve the performance on two benchmark datasets without external resources, over all previous models of the same kind.
",7 Conclusion and Future Work,[0],[0]
"In the future, we plan to enable ITransF to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters.",7 Conclusion and Future Work,[0],[0]
"In addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks.",7 Conclusion and Future Work,[0],[0]
We thank anonymous reviewers and Graham Neubig for valuable comments.,Acknowledgments,[0],[0]
"We thank Yulun Du, Paul Mitchell, Abhilasha Ravichander, Pengcheng Yin and Chunting Zhou for suggestions on the draft.",Acknowledgments,[0],[0]
"We are also appreciative for the great working environment provided by staff in LTI.
",Acknowledgments,[0],[0]
This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program.,Acknowledgments,[0],[0]
"A.1 Domain Sampling Probability In this section, we define the probability pr to generate a negative sample from the same domain mentioned in Section 3.3.",A Appendix,[0],[0]
"The probability cannot be too high to avoid generating negative samples that are actually correct, since there are generally a lot of facts missing in KBs.
",A Appendix,[0],[0]
"Specifically, let MHr = {h | ∃t(h, r, t) ∈ P} and MTr = {t | ∃h(h, r, t) ∈ P} denote the head or tail domain of relation r. Suppose",A Appendix,[0],[0]
"Nr = {(h, r, t) ∈ P} is the induced set of edges with relation r.",A Appendix,[0],[0]
"We define the probability pr as
pr = min( λ|MTr ||MHr | |Nr| , 0.5) (4)
Our motivation of such a formulation is as follows:",A Appendix,[0],[0]
"Suppose Or is the set that contains all truthful fact triples on relation r, i.e., all triples in training set and all other missing correct triples.",A Appendix,[0],[0]
"If we assume all fact triples within the domain has uniform probability of being true, the probability of a random triple being correct is Pr((h, r, t) ∈",A Appendix,[0],[0]
"Or | h ∈ MHr , t ∈ MTr )",A Appendix,[0],[0]
"= |Or||MHr ||MTr |
Assume that all facts are missing with a probability λ, then |Nr| = λ|Or| and the above probability can be approximated by |Nr|
λ|MHr ||MTr | .",A Appendix,[0],[0]
"We
want the probability of generating a negative sample from the domain to be inversely proportional to the probability of the sample being true, so we define the probability as Eq. 4.",A Appendix,[0],[0]
"The results in section 4 are obtained with λ set to 0.001.
",A Appendix,[0],[0]
We compare how different value of λ would influence our model’s performance in Table.,A Appendix,[0],[0]
5.,A Appendix,[0],[0]
"With large λ and higher domain sampling probability, our model’s Hits@10 increases while mean rank also increases.",A Appendix,[0],[0]
"The rise of mean rank is due to higher probability of generating a valid triple as a negative sample causing the energy of a valid triple to increase, which leads to a higher overall rank of a correct entity.",A Appendix,[0],[0]
"However, the reasoning capability is boosted with higher Hits@10 as shown in the table.
",A Appendix,[0],[0]
"A.2 Performance on individual relations of WN18
We plot the performance of ITransF and STransE on each relation.",A Appendix,[0],[0]
We see that the improvement is greater on rare relations.,A Appendix,[0],[0]
Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness.,abstractText,[0],[0]
"We propose a novel embedding model, ITransF, to perform knowledge base completion.",abstractText,[0],[0]
"Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts.",abstractText,[0],[0]
"Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily.",abstractText,[0],[0]
"We evaluate ITransF on two benchmark datasets— WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.",abstractText,[0],[0]
An Interpretable Knowledge Transfer Model for Knowledge Base Completion,title,[0],[0]
"In statistics and machine learning, ridge regression (Gunst & Mason, 1977; Hoerl & Kennard, 1970) (also known as Tikhonov regularization or weight decay) is a variant of regularized least squares problems where the choice of the penalty function is the squared `2-norm.",1. Introduction,[0],[0]
"Formally, let A ∈ Rn×d be the design matrix and let b ∈",1. Introduction,[0],[0]
Rn be the response vector.,1. Introduction,[0],[0]
"Then, the linear algebraic formulation of the ridge regression problem is as follows:
Z∗ = min x∈Rd
{ ‖Ax− b‖22 + λ‖x‖22 } , (1)
where λ > 0 is the regularization parameter.",1. Introduction,[0],[0]
"There are two fundamental motivations underlying the use of ridge regres-
1Department of Statistics, Purdue University, West Lafayette, IN 2Department of Computer Science, Purdue University, West Lafayette, IN.",1. Introduction,[0],[0]
"Correspondence to: Agniva Chowdhury <chowdhu5@purdue.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
sion.",1. Introduction,[0],[0]
"First, when d n, i.e., the number of predictor variables d greatly exceeds the number of observations n, fitting the full model without regularization (i.e., setting λ to zero) will result in large prediction intervals and a non-unique regression estimator.",1. Introduction,[0],[0]
"Second, if the design matrix A is ill-conditioned, solving the standard least-squares problem without regularization would depend on (ATA)−1.",1. Introduction,[0],[0]
This inversion would be problematic if ATA were singular or nearly singular and thus adding even a little noise to the elements of A could result in large changes in (ATA)−1.,1. Introduction,[0],[0]
"Due to these two considerations, solving standard least-squares problems without regularization may provide a good fit to the training data but may not generalize well to test data.
",1. Introduction,[0],[0]
Ridge regression abandons the requirement of an unbiased estimator in order to address the aforementioned problems.,1. Introduction,[0],[0]
"At the cost of introducing bias, ridge regression reduces the variance and thus might reduce the overall mean squared error (MSE).",1. Introduction,[0],[0]
The minimizer of eqn.,1. Introduction,[0],[0]
"(1) is
x∗ =",1. Introduction,[0],[0]
( ATA + λId )−1,1. Introduction,[0],[0]
"ATb, (2)
or, equivalently (see Saunders et al. (1998) and Lemma 9 in Appendix A),
x∗ = AT ( AAT + λIn )−1",1. Introduction,[0],[0]
"b. (3)
",1. Introduction,[0],[0]
"Both formulations work for any λ > 0 for either underconstrained or over-constrained ridge regression problems, regardless of the rank of the design matrix A.",1. Introduction,[0],[0]
"It is easy to see that x∗ can be computed in time
O(ndmin{n, d}+ min{n3, d3}) =",1. Introduction,[0],[0]
"O(ndmin{n, d}).
",1. Introduction,[0],[0]
"In our work, we will focus on design matrices A ∈ Rn×d with d n, which is the most common setting for ridge regression.",1. Introduction,[0],[0]
"For simplicity of exposition, we will assume that the rank of A is equal to n.1",1. Introduction,[0],[0]
"In the context of ridge regression, a much more important quantity than the rank of the design matrix is the effective degrees of freedom:
dλ = n∑ i=1",1. Introduction,[0],[0]
"σ2i σ2i + λ ≤ n, (4)
where σi are the singular values of A.
1Our results can be slightly improved to depend on the rank ρ of the matrix A instead of n.
The recent flurry of activity on Randomized Linear Algebra (RLA) (Drineas & Mahoney, 2016) and the widespread use of sketching as a tool for matrix computations (Woodruff, 2014), resulted in many novel results for ridge regression.",1. Introduction,[0],[0]
In Section 1.2 we discuss relevant prior work.,1. Introduction,[0],[0]
"We present a novel iterative algorithm (Algorithm 1) for sketched ridge regression and two simple sketching-based structural conditions under which Algorithm 1 guarantees highly accurate approximations to the optimal solution x∗. More precisely, Algorithm 1 guarantees that, as long as a simple structural constraint is satisfied, the resulting approximate solution vector x̂∗ satisfies (after t iterations)
‖x∗",1.1. Our Contributions,[0],[0]
− x̂∗‖2 ≤ εt‖x∗‖2.,1.1. Our Contributions,[0],[0]
"(5)
Prior to discussing the aforementioned constraint, we note that error guarantees of the above form are highly desirable.",1.1. Our Contributions,[0],[0]
"Indeed, beyond being a relative error guarantee, the dependency on ε drops exponentially fast as the number of iterations increases.",1.1. Our Contributions,[0],[0]
"It is easy to see that by setting εt = ε′, O(ln(1/ε′))",1.1. Our Contributions,[0],[0]
iterations would suffice to provide a relative error guarantee with accuracy parameter ε′.,1.1. Our Contributions,[0],[0]
"This means that converging to, say, ten decimal digits of accuracy would necessitate only a constant number of iterations.",1.1. Our Contributions,[0],[0]
"See Section 1.2 for a comparison of this bound with prior work.
",1.1. Our Contributions,[0],[0]
Let V ∈ Rd×n be the matrix of right singular vectors of A; recall that A has rank n. For eqn.,1.1. Our Contributions,[0],[0]
"(5) to hold, a sketching matrix S ∈ Rd×s is to be constructed such that (for an appropriate choice of the sketching dimension s d)
",1.1. Our Contributions,[0],[0]
"‖VTSSTV − In‖2 ≤ ε
2 .",1.1. Our Contributions,[0],[0]
"(6)
We note that the constraint of eqn.",1.1. Our Contributions,[0],[0]
(6) has been the topic of intense research in the RLA literature; this is precisely the reason why we use eqn.,1.1. Our Contributions,[0],[0]
(6) as the building block in our analysis.,1.1. Our Contributions,[0],[0]
"Indeed, assuming that n d, one can use the (exact or approximate) column leverage scores (Mahoney & Drineas, 2009; Mahoney, 2011) of A to satisfy the aforementioned constraint, in which case S is a samplingand-rescaling matrix.",1.1. Our Contributions,[0],[0]
"Perhaps more interestingly, a variety of oblivious sketching matrix constructions for S can be used to satisfy eqn.",1.1. Our Contributions,[0],[0]
(6).,1.1. Our Contributions,[0],[0]
"We discuss various constructions for S in Section 2.1.
",1.1. Our Contributions,[0],[0]
One deficiency of the structural constraint of eqn.,1.1. Our Contributions,[0],[0]
"(6) is that all known constructions for S that satisfy the constraint need a number of columns s that is proportional to n. As a result, the running time of any algorithm that computes the sketch AS is also proportional to n. It would be much better to design algorithms whose running time depends on the degrees of freedom dλ, which is upper bounded by n, but could be significantly smaller depending on the distribution of the singular values and the choice of λ.
",1.1. Our Contributions,[0],[0]
"Towards that end, we analyze Algorithm 1 under a second structural constraint.",1.1. Our Contributions,[0],[0]
We define a diagonal matrix Σλ ∈ Rn×n,1.1. Our Contributions,[0],[0]
"whose i-th diagonal entry is given by
(Σλ)ii =
√ σ2i
σ2i + λ , i = 1, . . .",1.1. Our Contributions,[0],[0]
", n. (7)
Notice that ‖Σλ‖2F = dλ.",1.1. Our Contributions,[0],[0]
"Our second structural condition is given by
‖ΣλVTSSTVΣλ −Σ2λ‖2 ≤ ε
4 √ 2 .",1.1. Our Contributions,[0],[0]
"(8)
Similarly to the constraint of eqn.",1.1. Our Contributions,[0],[0]
"(6), the constraint of eqn.",1.1. Our Contributions,[0],[0]
"(8) can also be satisfied by, for example, sampling with respect to the ridge leverage scores of Alaoui & Mahoney (2015); Cohen et al. (2017) or by oblivious sketching matrix constructions for S.",1.1. Our Contributions,[0],[0]
"The difference is that, instead of having the column size s of the matrix S depend on n, it now depends on dλ, which could be considerably smaller.",1.1. Our Contributions,[0],[0]
"Indeed, it follows that by sampling-and-rescaling O(dλ ln dλ) predictor variables from the design matrix A (using either exact or approximate ridge leverage scores (Alaoui & Mahoney, 2015; Cohen et al., 2017) we can satisfy the constraint of eqn.",1.1. Our Contributions,[0],[0]
(8).,1.1. Our Contributions,[0],[0]
"Similarly, oblivious sketching matrix constructions for S can be used to satisfy eqn.",1.1. Our Contributions,[0],[0]
(8).,1.1. Our Contributions,[0],[0]
"We discuss constructions for S in Section 2.1.
",1.1. Our Contributions,[0],[0]
"However, this improved dependency on dλ instead of n comes with a mild loss in accuracy.",1.1. Our Contributions,[0],[0]
"For simplicity, we only state a result when λ satisfies σ2k+1 ≤ λ",1.1. Our Contributions,[0],[0]
≤,1.1. Our Contributions,[0],[0]
"σ2k for some integer k, 1 ≤ k ≤ n.2",1.1. Our Contributions,[0],[0]
"In words, λ can be thought of as “regularizing” the bottom n−k singular values of the design matrix A, since it dominates them.",1.1. Our Contributions,[0],[0]
"In this case, we prove that the approximation x̂∗ returned by Algorithm 1 satisfies
‖x∗",1.1. Our Contributions,[0],[0]
− x̂∗‖2 ≤,1.1. Our Contributions,[0],[0]
"εt
2
( ‖x∗‖2 +
1√ 2λ ∥∥UTk,⊥b∥∥2) .",1.1. Our Contributions,[0],[0]
"(9) Here Uk,⊥ ∈ Rn×(n−k) denotes the matrix of the bottom n",1.1. Our Contributions,[0],[0]
− k left singular vectors of the design matrix A.,1.1. Our Contributions,[0],[0]
"In words, we achieve an additive-relative error approximation, where the additive error part depends on the norm of the “piece” of the response vector b that lies on the regularized component of the design matrix A.",1.1. Our Contributions,[0],[0]
"As this piece grows, the quality of the approximation worsens.",1.1. Our Contributions,[0],[0]
"The error decreases exponentially fast with the number of iterations.
",1.1. Our Contributions,[0],[0]
"Another contribution of our work is Theorem 4, which proves that the mean-square-error (MSE) of the approximate solution x̂∗ is a relative error approximation to the MSE of x∗, under the structural assumptions of eqns.",1.1. Our Contributions,[0],[0]
"(6) or (8), even after a single iteration.
2The bound of eqn.",1.1. Our Contributions,[0],[0]
(9) can be easily generalized to hold when c1σ 2 k+1 ≤,1.1. Our Contributions,[0],[0]
"λ ≤ c2σ2k for some constants c1, c2 > 0.",1.1. Our Contributions,[0],[0]
"For simplicity of exposition, we assume that both c1 and c2 equal one.
",1.1. Our Contributions,[0],[0]
"To the best of our knowledge, our bounds are a first attempt to provide general structural results that guarantee highquality approximations to the optimal solution vector of ridge regression.",1.1. Our Contributions,[0],[0]
Our first structural result can be satisfied by sampling with respect to the leverage scores or by the use of oblivious sketching matrices whose size depends on the rank of the design matrix and guarantees relative error approximations.,1.1. Our Contributions,[0],[0]
Our second structural result presents the first accuracy analysis for ridge regression when the ridge leverage scores are used to sample predictor variables.,1.1. Our Contributions,[0],[0]
"Interestingly, the ridge leverage scores have been used in a number of applications involving matrix approximation, cost-preserving projections, clustering, etc.",1.1. Our Contributions,[0],[0]
"(Cohen et al., 2017), but their performance in the context of ridge regression has not been analyzed in prior work.",1.1. Our Contributions,[0],[0]
Our work here argues that the second structural condition can be satisfied by sampling with respect to the ridge leverage scores.,1.1. Our Contributions,[0],[0]
"The number of predictor variables to be sampled depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix, and results in a relative-additive error guarantee.",1.1. Our Contributions,[0],[0]
"In this section, we discuss our contributions in the context of the large and ever-growing body of prior work on sketching-based algorithms for regression and ridge regression.",1.2. Prior Work,[0],[0]
"The work more closely related to ours is Chen et al. (2015), which (in our notation) returns an approximation x̂∗ to x∗ that satisfies (with high probability) a relative error guarantee of the form
‖x∗",1.2. Prior Work,[0],[0]
"− x̂∗‖2 ≤ ε‖x∗‖2.
",1.2. Prior Work,[0],[0]
The running time of the proposed approach is O(nnz(A),1.2. Prior Work,[0],[0]
+,1.2. Prior Work,[0],[0]
ε−2n3 ln(n/ε)).,1.2. Prior Work,[0],[0]
The proposed approach is also based on sketching A using RLA tools such as the count-min sketch of Clarkson & Woodruff (2013) and the sub-sampled Randomized Hadamard Transform of Ailon & Chazelle (2009); Sarlós (2006); Drineas et al. (2011).,1.2. Prior Work,[0],[0]
"Compared to our work, notice that their dependency on ε is exponentially higher: our approach has a running time that grows with ln(1/ε) whereas the above bound grows proportionally to 1/ε2.",1.2. Prior Work,[0],[0]
"Additionally, our analysis can be made to depend on the degrees of freedom of the ridge-regression problem (see Theorem 2 and Section 2.1).",1.2. Prior Work,[0],[0]
"Finally, we complement the bounds on the MSE for the response vector presented in Theorem 6 of Chen et al. (2015) with a relative-error guarantee on the MSE of the solution vector (see Theorem 4).",1.2. Prior Work,[0],[0]
"We should also mention that prior to Chen et al. (2015); Lu et al. (2013) proposed a fast approximation algorithm for the computation of the kernel matrix using the sub-sampled randomized Hadamard transformation (SRHT).
",1.2. Prior Work,[0],[0]
"Recently, Wang et al. (2017) presented many results on ridge-regression problems assuming n d.",1.2. Prior Work,[0],[0]
"In this setting,
the main motivation for ridge regression is to deal with the potential ill-conditioning of the design matrix A. Wang et al. (2017) presented sketching-based approaches that guarantee relative error approximations to the value of the objective Z∗, as opposed to the actual solution vector.",1.2. Prior Work,[0],[0]
Our approach and analysis is quite different and is applicable where d n; the results of Wang et al. (2017) do not generalize to this setting.,1.2. Prior Work,[0],[0]
"However, recent work by Avron et al. (2017a;b) also focused on d n: for example, Theorem 17 of Avron et al. (2017b) presents structural conditions under which the value of the objective Z∗ can be estimated up to relative error accuracy, but no bounds are presented for the approximate solution vector.",1.2. Prior Work,[0],[0]
This last result seems to necessitate two structural conditions: the first one is identical to the condition of eqn.,1.2. Prior Work,[0],[0]
"(6), but the second one is on the spectral norm of an approximate matrix product that is not needed in our analysis.
",1.2. Prior Work,[0],[0]
"Our work was partially motivated by Pilanci & Wainwright (2016), where an iterative algorithm (the so-called Iterative Hessian Sketch) was presented for standard (i.e., λ = 0), over-constrained (n d) regression problems.",1.2. Prior Work,[0],[0]
"Indeed, the authors provide strong motivation that clarifies the need for algorithms for regression problems whose running times depends on ln(1/ε) in order to achieve ε-relative-error approximations.",1.2. Prior Work,[0],[0]
We emphasize that the transition from standard to regularized regression problems as well as from the overto the under-constrained case is far from trivial.,1.2. Prior Work,[0],[0]
"Indeed, algorithms and structural results for over-constrained regression problems date back to 2006 (Drineas et al., 2006b), whereas the analogous results for ridge-regression problems appeared after 2015.",1.2. Prior Work,[0],[0]
"Similarly, the only result that we know for under-constrained regression problems (λ = 0, n d) appeared in Section 6.2 of Drineas et al. (2012).
",1.2. Prior Work,[0],[0]
"Another line of research that motivated our approach was the recent introduction of ridge leverage scores (Alaoui & Mahoney, 2015; Cohen et al., 2017).",1.2. Prior Work,[0],[0]
"Indeed, our Theorem 2 presents a structural result that can be satisfied (with high probability) by sampling columns of A with probabilities proportional to (exact or approximate) ridge leverage scores (see Section 2.1).",1.2. Prior Work,[0],[0]
The number of sampled predictor variables (columns of A) is proportional to O(dλ ln dλ).,1.2. Prior Work,[0],[0]
"To the best of our knowledge, this is the first result showing a strong accuracy guarantee for ridge regression problems when the ridge leverage scores are used to sample predictor variables, in one or more iterations.",1.2. Prior Work,[0],[0]
"We also note a recent application of ridge leverage scores (Calandriello et al., 2017a;b) where the authors presented a row sampling algorithm in order to construct a kernel sketch which is eventually used in a second-order gradient-based method for online kernel convex optimization.
",1.2. Prior Work,[0],[0]
"In yet another relevant line of work, much research recently focused on the computation and inversion of the kernel ma-
trix AAT (or ATA).",1.2. Prior Work,[0],[0]
"A number of recent papers have considered the problem of fast kernel approximation for large datasets (Zhang et al., 2015; Avron et al., 2017b; Musco & Musco, 2017; Calandriello et al., 2017c; Wang et al., 2017).",1.2. Prior Work,[0],[0]
"However, direct comparison of the bounds presented in the aforementioned papers and our work is not straightforward, since our objective (accuracy of the approximate solution vector) is different than the objective of the above papers.",1.2. Prior Work,[0],[0]
"In this context, there are also several recent works (Cutajar et al., 2016; Rudi et al., 2017; Ma & Belkin, 2017) that considered preconditioned gradient-based methods to develop fast and scalable approaches for approximating kernels.
",1.2. Prior Work,[0],[0]
"Finally, Gonen et al. (2016) presented a sketching-based preconditioned SVRG approach for ridge regression problems that converges to the optimal solution in a number of iterations that depends on ln(1/ε), returning an ε-relative-error approximation to the objective value Z∗. However, no such bounds were presented for the actual solution vector.",1.2. Prior Work,[0],[0]
"We use a,b, . . .",1.3. Notation,[0],[0]
"to denote vectors and A,B, . . .",1.3. Notation,[0],[0]
to denote matrices.,1.3. Notation,[0],[0]
"For a matrix A, A∗i (Ai∗) denotes the i-th column (row) of A as a column (row) vector.",1.3. Notation,[0],[0]
"For vector a, ‖a‖2 denotes its Euclidean norm; for a matrix A, ‖A‖2 denotes its spectral norm and ‖A‖F denotes its Frobenius norm.",1.3. Notation,[0],[0]
We refer the reader to Golub & Van Loan (1996) for properties of norms that will be quite useful in our work.,1.3. Notation,[0],[0]
"For a matrix A ∈ Rn×d with d > n of rank n, its (thin) Singular Value Decomposition (SVD) is equal to the product UΣVT, with U ∈ Rn×n (the matrix of the left singular vectors), V ∈ Rd×n (the matrix of the right singular vectors), and Σ ∈ Rn×n a diagonal matrix whose diagonal entries are the singular values of A. Computation of the SVD takes, in this setting, O(n2d) time.",1.3. Notation,[0],[0]
"We will use the notation Uk ∈ Rn×k to denote the matrix of the top k left singular vectors and Uk,⊥ ∈ Rn×(n−k) to denote the matrix of the bottom n−k left singular vectors.",1.3. Notation,[0],[0]
We will often use σi to denote the singular values of a matrix implied by context.,1.3. Notation,[0],[0]
Additional notation will be introduced as needed.,1.3. Notation,[0],[0]
Algorithm 1 iteratively computes a sequence of vectors x̃(j),"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"∈ Rd for j = 1, . . .","2. Iterative, Sketching-based Ridge Regression",[0],[0]
", t and returns the estimator","2. Iterative, Sketching-based Ridge Regression",[0],[0]
x̂∗ =∑t j=1 x̃ (j) to the true solution vector x∗ of eqn.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(3).
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"In words, Algorithm 1 is quite simple: roughly, it solves ridge regression problems with the residual vector b(j) (i.e., the part of the vector b(j−1) that was not captured in the previous iteration) as the new response vector for i = 1, . . .","2. Iterative, Sketching-based Ridge Regression",[0],[0]
", t. Our main quality-of-approximation results (Theorems 1 and 2) argue that returning the sum of those intermediate solutions results in a highly accurate approximation
Algorithm 1 Iterative, sketching-based ridge regression
Input: A ∈ Rn×d, b ∈ Rn, λ > 0; number of iterations t > 0; sketching matrix S ∈ Rd×s; Initialize: b(0) ← b, x̃(0)","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"← 0d, y(0) ← 0n; for j = 1 to t do
b(j) ← b(j−1)","2. Iterative, Sketching-based Ridge Regression",[0],[0]
− λy(j−1) −Ax̃(j−1); y(j),"2. Iterative, Sketching-based Ridge Regression",[0],[0]
← (ASSTAT + λIn)−1b(j); x̃(j),"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"← ATy(j);
end for Output: Approximate solution vector","2. Iterative, Sketching-based Ridge Regression",[0],[0]
x̂∗ = ∑t j=1,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"x̃ (j);
to the optimal solution vector x∗. Theorem 1 presents a quality-of-approximation result under the assumption that the sketching matrix S satisfies the constraint of eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(6).
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
Theorem 1.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
Assume that for some constant 0 < ε,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"< 1, the sketching matrix S ∈ Rd×s satisfies the constraint of eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
(6).,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Then, the estimator x̂∗ returned by Algorithm 1 satisfies
‖x̂∗","2. Iterative, Sketching-based Ridge Regression",[0],[0]
− x∗‖2 ≤ ε t,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"‖x∗‖2 .
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Here x∗ is the true solution of the ridge regression problem.
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Similarly, Theorem 2 presents a quality-of-approximation result under the assumption that the sketching matrix S satisfies the constraint of eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(8).
Theorem 2.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
Assume that for some constant 0 < ε,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"< 1, the sketching matrix S ∈ Rd×s satisfies the constraint of eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
(8).,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Then, the estimator x̂∗ returned by Algorithm 1 satisfies
‖x̂∗","2. Iterative, Sketching-based Ridge Regression",[0],[0]
− x∗‖2 ≤,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"εt
2
( ‖x∗‖2 +
1√ 2λ ∥∥UTk,⊥b∥∥2) .","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Here, k ∈ {1, . . .","2. Iterative, Sketching-based Ridge Regression",[0],[0]
", n} is an integer with σ2k+1 ≤ λ ≤ σ2k","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"and x∗ is the true solution of the ridge regression problem.
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"As we have already discussed, the bound of Theorem 2 is weaker.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"However, the structural condition of eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(8) on which the above theorem depends, can be satisfied with a sketching matrix S whose dimensionality depends only on the degrees of freedom dλ of the underlying ridge regression problem, as opposed to the dimensions of the design matrix.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"This could result in significant savings (see Section 2.1).
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Our algorithm can also be viewed as a preconditioned Richardson iteration (see e.g., Chapter 2 of Quarteroni & Valli (1994)) for solving the linear system (AAT+λIn)y = b with pre-conditioner P−1 =","2. Iterative, Sketching-based Ridge Regression",[0],[0]
(ASSTAT + λIn)−1 and step-size equal to one.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"More precisely, Algorithm 1 can be formulated as
ȳ(j) = ȳ(j−1)","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"+ P−1 ( b− (AAT + λIn)ȳ(j−1) ) ,
where ȳ(j) = ∑j k=1 y
(k) (see Appendix D for the derivation).","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Further, subject to the structural conditions of eqns.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(6) and (8), it can be shown that ȳ(t) converges to the true solution y∗ = (AAT + λIn)−1b in O(ln(1/ε)) steps (see Appendix D) and, consequently, the output of Algorithm 1 (which can be expressed as x̂∗ = ATȳ(t)) also converges to x∗ = AT(AAT + λIn)−1b, the true solution of the ridge regression problem.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
Our analysis offers several advantages over preconditioned Richardson iteration.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"In our case, P−1(AAT + λIn) is not symmetric positive definite which, according to existing literature, implies that the convergence of Richardson’s method is monotone in terms of the energy-norm induced by AAT + λIn, but not the Euclidean norm (see eqn.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
(2.4.17) in Quarteroni & Valli (1994)).,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Additionally, standard convergence analysis of the Richardson iteration is with respect to ȳ(t), whereas our vector of interest is x̂∗ (which is ȳ(t) premultiplied by AT).","2. Iterative, Sketching-based Ridge Regression",[0],[0]
The equality ‖ȳ(t) − y∗‖2 = ‖x̂∗,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"− x∗‖2 holds if A has orthonormal rows, which is not true in general.
","2. Iterative, Sketching-based Ridge Regression",[0],[0]
We now discuss the running time of Algorithm 1.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"First, we need to compute Ax̃(j−1) which takes time O(nnz(A)).","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Next, computing the sketch AS ∈ Rn×s takes T (A,S) time and depends on the particular construction of S (see Section 2.1).","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Then, in order to invert the matrix Θ = ASSTAT +","2. Iterative, Sketching-based Ridge Regression",[0],[0]
λIn it suffices to compute the SVD of the matrix AS.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
Notice that given the singular values of AS we can compute the singular values of Θ; also note that the left and right singular vectors of Θ are the same as the left singular vectors of AS.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Interestingly, we do not need to compute Θ−1: we can store it implicitly by storing its left (and right) singular vectors UΘ and its singular values ΣΘ.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Then, we can compute all necessary matrix-vector products using this implicit representation of Θ−1.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Thus, inverting Θ takes O(sn2) time.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Updating the vectors b(j), y(j), and x̃(j) is dominated by the aforementioned running times, as all updates amount to just matrix-vector products.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"Thus, summing over all t iterations, the running time of Algorithm 1 is given by
O(t · nnz(A))","2. Iterative, Sketching-based Ridge Regression",[0],[0]
+O(sn2),"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"+ T (A,S).","2. Iterative, Sketching-based Ridge Regression",[0],[0]
"(10)
We conclude this section by noting that our results remain valid when different sampling matrices Sj are used in each iteration j = 1, . . .","2. Iterative, Sketching-based Ridge Regression",[0],[0]
", t, as long as they satisfy the constraints of eqns.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
(6) or (8).,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
"As a matter of fact, the sketching matrices Sj do not even need to have the same number of columns.","2. Iterative, Sketching-based Ridge Regression",[0],[0]
See Section 5 for an interesting open problem in this setting.,"2. Iterative, Sketching-based Ridge Regression",[0],[0]
The conditions of eqns.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(6) and (8) essentially boil down to randomized, approximate matrix multiplication (Drineas & Kannan, 2001; Drineas et al., 2006a), a task that has received much attention in the RLA community.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"We start by discussing sketching-based approaches: a particularly useful
result for our purposes appeared in Cohen et al. (2016).",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Using our notation, Cohen et al. (2016) proved that for X ∈ Rd×n and for a (suitably constructed) sketching matrix S ∈ Rd×s, with probability at least 1− δ,
∥∥XTSSTX−XTX∥∥ 2 ≤ ε ( ‖X‖22 + ‖X‖2F r ) , (11)
for any arbitrary r ≥ 1.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
The above bound holds for a very broad family of constructions for the sketching matrix S (see Cohen et al. (2016) for details).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"In particular, Cohen et al. (2016) demonstrated a construction for S with s = O(r/ε2) columns such that, for any n × d matrix A, the product AS can be computed in timeO(nnz(A))+Õ((r3+ r2n)/εγ) for some constant γ.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Thus, starting with eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(6) and using this particular construction for S, let X = V and note that ‖V‖2F = n and ‖V‖2 = 1. Setting r = n, eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
(11) implies that∥∥VTSSTV,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
− In∥∥2 ≤ 2 ε.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"In this case, the running time of the sketch computation is equal to T (A,S) = O(nnz(A))",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
+ Õ(n3/εγ).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
The running time of the overall algorithm follows from eqn.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(10) and our choices for s and r:
O(t · nnz(A))",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"+ Õ(n3/εmax{2,γ}).
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
The failure probability (hidden in the polylogarithmic terms) can be easily controlled using a union bound.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Finally, a simple change of variables (using ε/4 instead of ε) suffices to satisfy the structural condition of eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(6) without changing the above running time.
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Similarly, starting with eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(8), let X = VΣλ and note that ‖VΣλ‖2F = dλ and ‖VΣλ‖2 ≤ 1.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Setting r = dλ, eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
(11) implies that ∥∥ΣλVTSSTVΣλ −Σ2λ∥∥2 ≤ 2ε.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"In this case, the running time of the sketch computation is equal to T (A,S) = O(nnz(A))",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
+ Õ(d2λn/εγ).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
The running time of the overall algorithm follows from eqn.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(10) and our choices for s and r:
O(t · nnz(A))",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
+,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Õ(dλn2/εmax{2,γ}).
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Again, a change of variables suffices to satisfy the structural condition of eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(8) without changing the running time.
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
We now discuss how to satisfy the conditions of eqns.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(6) or (8) by sampling, i.e., by selecting a small number of predictor variables.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Towards that end, consider Algorithm 2 for the construction of the sampling-and-rescaling matrix S.
The following theorem (see Appendix G for its proof) is of independent interest and is a strengthening of Theorem 4.2 of Holodnak & Ipsen (2015), since the sampling complexity s is improved to depend only on ‖X‖2F instead of the stable rank of X for the special case where ‖X‖2 ≤ 1.3
3We do note that Theorem 3 is implicit in Cohen et al. (2017).
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Algorithm 2 Construct sampling-and-rescaling matrix Input: Probabilities pi, i = 1, . . .",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
", d; integer s d; S← 0d×s; for j = 1 to s do
Pick ij ∈ {1, . . .",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
", d} with P(ij = i) = pi; Sijj ← (s pij )",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"− 1 2 ;
end for Output: Sampling-and-rescaling matrix S;
Theorem 3.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Let X ∈ Rd×n with ‖X‖2 ≤ 1 and let S be constructed by Algorithm 2 with pi = ‖Xi∗‖22 / ‖X‖ 2 F for i = 1, . . .",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
", d. Let δ be a failure probability and let ε ∈ (0, 1] be an accuracy parameter.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"If the number of sampled columns s satisfies
s ≥ 8 ‖X‖2F
3 ε2 ln
( 4 (1 + ‖X‖2F )
δ
) ,
then, with probability at least 1− δ,∥∥XTSSTX−XTX∥∥ 2 ≤ ε.
Using Theorem 3 with X = V we can satisfy the condition of eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"(6) by simply using the sampling probabilities pi = ‖Vi∗‖22 /n (recall that ‖V‖ 2 F = n and ‖V‖2 = 1), which are the column leverage scores of the design matrix A. Setting s = O(ε−2n lnn) suffices to satisfy the condition of eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
(6).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"We note that approximate leverage scores also suffice and that their computation can be done efficiently without computing V (Drineas et al., 2012).
",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Finally, using Theorem 3 with X = VΣλ we can satisfy the condition of eqn.",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
(8) using the sampling probabilities pi = ‖(VΣλ)i∗‖22 /dλ (recall that ‖VΣλ‖ 2 F = dλ and ‖VΣλ‖2 ≤ 1).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
It is easy to see that these probabilities are proportional to the column ridge leverage scores of the design matrix A (see Lemma 21 in Appendix F).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
Setting s = O(ε−2dλ ln dλ) suffices to satisfy the condition of eqn.,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
(8).,2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"We note that approximate ridge leverage scores also suffice and that their computation can be done efficiently without computing V (Cohen et al., 2017).",2.1. Satisfying the Conditions of Eqns. (6) or (8),[0],[0]
"Consider the data-generation model
b = Ax0 + ε, (12)
where b ∈ Rn is the response vector, A ∈ Rn×d is the design matrix, x0 ∈",2.2. Bounding the MSE of x̂∗,[0],[0]
"Rn is the “true” parameter vector, and ε ∈",2.2. Bounding the MSE of x̂∗,[0],[0]
Rn is the noise satisfying E(ε) = 0 and E(εεT),2.2. Bounding the MSE of x̂∗,[0],[0]
"= σ2In, σ > 0.",2.2. Bounding the MSE of x̂∗,[0],[0]
"Then, the ridge regression estimator x∗ of the parameter vector x0 can be expressed as in eqn.",2.2. Bounding the MSE of x̂∗,[0],[0]
"(3), with mean squared error (MSE) given by (see Lemma 16 in Appendix E for the derivation)
MSE(x∗) = σ2 ∥∥(AAT + λIn)−1A∥∥2F
+ ∥∥(AT(AAT + λIn)−1A− Id)x0∥∥22 .",2.2. Bounding the MSE of x̂∗,[0],[0]
"(13)
Similarly, we can prove that the MSE of x̂∗ for the special case where t = 1 in Algorithm 1 is equal to
MSE(x̂∗) = σ2 ∥∥(ASSTAT + λIn)−1A∥∥2F
+ ∥∥(AT(ASSTAT",2.2. Bounding the MSE of x̂∗,[0],[0]
+ λIn)−1A− Id)x0∥∥22 .,2.2. Bounding the MSE of x̂∗,[0],[0]
"(14)
We present bounds on the MSE of x̂∗ for the special case where Algorithm 1 is run for a single iteration (t = 1) under the assumptions of eqns.",2.2. Bounding the MSE of x̂∗,[0],[0]
(6) or (8).,2.2. Bounding the MSE of x̂∗,[0],[0]
"Bounds for t > 1 (more than one iteration) are delegated to future work.
",2.2. Bounding the MSE of x̂∗,[0],[0]
Theorem 4.,2.2. Bounding the MSE of x̂∗,[0],[0]
Let A ∈ Rn×d be the design matrix and let x̂∗ be the output of Algorithm 1 for t = 1.,2.2. Bounding the MSE of x̂∗,[0],[0]
If the condition of eqn.,2.2. Bounding the MSE of x̂∗,[0],[0]
(6) is satisfied for some constant 0,2.2. Bounding the MSE of x̂∗,[0],[0]
"< ε < 1, then,
MSE(x̂∗) ≤ (1 + 3εγ21) MSE(x∗),
where γ1 = 1 + σ21 λ .",2.2. Bounding the MSE of x̂∗,[0],[0]
If the condition of eqn.,2.2. Bounding the MSE of x̂∗,[0],[0]
(8) is satisfied for some constant 0,2.2. Bounding the MSE of x̂∗,[0],[0]
"< ε < 1, then,
MSE(x̂∗) ≤ (1 + 3εγ22)",2.2. Bounding the MSE of x̂∗,[0],[0]
"MSE(x∗),
where γ2 = max { 1 + σ21/λ, √ 1 + λ/σ2n } .",2.2. Bounding the MSE of x̂∗,[0],[0]
"Due to space considerations, essentially all our proofs have been deferred to the Appendix.",3. Sketching the Proof of Theorem 2,[0],[0]
"However, to give a flavor of the mathematical derivations underlying our contributions, we present an outline of the proof of Theorem 2, starting with the special case where Algorithm 1 is run for a single iteration (t = 1).
",3. Sketching the Proof of Theorem 2,[0],[0]
"Using the quantities defined in Algorithm 1, let
x∗(j) =",3. Sketching the Proof of Theorem 2,[0],[0]
"AT(AAT + λIn) −1b(j) (15)
",3. Sketching the Proof of Theorem 2,[0],[0]
"for j = 1, . .",3. Sketching the Proof of Theorem 2,[0],[0]
.,3. Sketching the Proof of Theorem 2,[0],[0]
", t. Notice that x∗ = x∗(1).",3. Sketching the Proof of Theorem 2,[0],[0]
Our next result expresses the intermediate vectors x̃(j) of Algorithm 1 in terms of the vectors x∗(j).,3. Sketching the Proof of Theorem 2,[0],[0]
"We remind the reader that U ∈ Rn×n and Σ ∈ Rn×n are, respectively, the matrices of the left singular vectors and singular values of A. We will make extensive use of the matrix Σλ defined in eqn.",3. Sketching the Proof of Theorem 2,[0],[0]
(7).,3. Sketching the Proof of Theorem 2,[0],[0]
Lemma 5.,3. Sketching the Proof of Theorem 2,[0],[0]
"Let A ∈ Rn×d, b ∈ Rn, and λ > 0 be the inputs of the ridge regression problem.",3. Sketching the Proof of Theorem 2,[0],[0]
"Let S ∈ Rd×s be the sketching matrix and define
E = ΣλV TSSTVΣλ −Σ2λ.",3. Sketching the Proof of Theorem 2,[0],[0]
x̃(j),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
= x∗(j),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
+,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"VΣλRΣλΣ −1UTb(j), (16)
where R = ∑∞","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"`=1(−1)`E`.
Now, consider the case when t = 1.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
Algorithm 1 returns x̂∗ = x̃(1); also recall that x∗ = x∗(1) and b = b(1).,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Therefore, applying Lemma 5 yields
x̂∗ = x∗ + VΣλRΣλΣ −1UTb.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(17)
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Further, for any j = 1, . . .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
", t,
‖R‖2 = ∥∥","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
∞∑ `=1 (−1)`E` ∥∥ 2 ≤ ∞∑ `=1 ‖E`‖2 ≤ ∞∑,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"`=1 ‖E‖`2
≤ ∞∑","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
`=1 ( ε 4 √ 2 )` = ε 4 √ 2 1− ε 4 √ 2 ≤ ε 2 √ 2 .,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(18)
where we used the triangle inequality, sub-multiplicativity of the spectral norm, and the fact that ε
4 √ 2 ≤ 12 .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Now, using
eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(17), we have
‖x̂∗","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− x∗‖2 = ‖VΣλRΣλΣ−1UTb‖2 ≤ ‖Σλ‖2‖R‖2‖ΣλΣ−1UTb‖2
≤ ε 2 √ 2 ‖ΣλΣ−1UTb‖2 = ε
2 √ 2 ‖Σ−1λ Σ 2 λΣ −1UTb‖2.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(19)
where the first inequality follows from the unitary invariance and sub-multiplicativity of the spectral norm, and the second inequality is due to eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(18) and the fact that ‖Σλ‖2 ≤ 1.
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Now, let (Σ−1λ )k denote the diagonal matrix whose first k diagonal entries are equal to the first k diagonal entries of Σ−1λ and the bottom n− k diagonal entries are set to zero.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Let (Σ−1λ )k,⊥ = Σ −1 λ","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
− (Σ −1 λ ),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"k. Then, we have
‖Σ−1λ Σ 2 λΣ −1UTb‖2 ≤ ‖(Σ−1λ )","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"k Σ 2 λΣ −1UTb‖2︸ ︷︷ ︸
∆1
+ ‖(Σ−1λ )","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"k,⊥ Σ 2 λΣ −1UTb‖2︸ ︷︷ ︸
∆2
.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(20)
where eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(20) follows from the triangle inequality and the fact that Σ−1λ =,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(Σ −1 λ ),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
k +,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(Σ −1 λ )k,⊥.
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Next, we bound ∆1 and ∆2 separately using eqns.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(60) and (62) in Appendix C:
∆1 ≤ √
2 ‖x∗‖2 , ∆2 ≤ 1√ λ ∥∥UTk,⊥b∥∥2 .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(21)
Finally, combining eqns.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(19), (20) and (21), we obtain
‖x̂∗","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− x∗‖2 ≤ ε
2 √ 2
(√ 2 ‖x∗‖2","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"+
1√ λ ‖UTk,⊥b‖2 )","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"= ε
2
( ‖x∗‖2 +
1√ 2λ ‖UTk,⊥b‖2
) , (22)
which concludes the proof for the t = 1 case.
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Interestingly, the eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(22) holds more generally and can be used to bound the distance between the intermediate approximate solution vectors x̃(j) and the intermediate true solution vectors x∗(j)of eqn.,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(15).,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Indeed, for j = 1, . . .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
", t, we have
‖x̃(j)","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− x∗(j)‖2 ≤ ε
2
( ‖x∗(j)‖2 +
1√ 2λ ‖UTk,⊥b(j)‖2
) .
(23)
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"The next lemma (see Appendix C for its proof) presents a structural result for the optimal solution x∗.
Lemma 6.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Let x̃(j), j = 1, . . .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
", t be the sequence of vectors introduced in Algorithm 1 and let x∗(t) ∈ Rd be defined as in eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(15).,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Then,
x∗ = x∗(t) + t−1∑ j=1 x̃(j), (24)
where x∗ is the true solution of the ridge regression problem.
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
Repeated application of eqns.,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(23) and (24) yields
‖x̂∗ − x∗‖2 = ∥∥","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
t∑ j=1 x̃(j),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− x∗ ∥∥ 2
= ∥∥x̃(t)","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− (x∗ − t−1∑
j=1
x̃(j) )∥∥ 2 = ∥∥x̃(t)","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"− x∗(t)∥∥ 2
≤ ε 2
( ‖x∗(t)‖2 +
1√ 2λ ‖UTk,⊥b(t)‖2
) .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(25)
The next bound (see Appendix C for its proof) provides a critical inequality that can be used recursively in order to establish Theorem 2.
Lemma 7.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"Let b(j), j = 1, . . .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
", t, be the intermediate response vectors of Algorithm 1 and let x∗(j) be the vector defined in eqn.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(15) for j = 1, . . .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
", t − 1.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
If the structural condition of eqn.,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(8) is satisfied, then
‖x∗(j+1)‖2 + 1√ 2λ","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"‖UTk,⊥b(j+1)‖2
≤ ε ( ‖x∗(j)‖2 +
1√ 2λ ‖UTk,⊥b(j)‖2
) .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(26)
","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
Applying eqn.,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(26) iteratively, we obtain
‖x∗(t)‖2 + 1√ 2λ ‖UTk,⊥b(t)‖2
≤ ε","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"( ‖x∗(t−1)‖2 +
1√ 2λ ‖UTk,⊥b(t−1)‖2 ) ≤ · · · ≤","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"εt−1 ( ‖x∗‖2 +
1√ 2λ ‖UTk,⊥b‖2
) .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(27)
Finally, combining eqns.","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"(25) and (27), we conclude that
‖x̂∗","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
− x∗‖2 ≤,"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"εt
2
( ‖x∗‖2 +
1√ 2λ ‖UTk,⊥b‖2
) .","If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
(28),"If ‖E‖2 < 1, then for all j = 1, . . . , t,",[0],[0]
"We perform experiments on the ARCENE dataset (Guyon et al., 2005) from the UCI repository (Lichman, 2013).",4. Empirical Evaluation,[0],[0]
"The design matrix contains 200 samples with 10, 000 real-valued features; we normalize the entries to be within the interval",4. Empirical Evaluation,[0],[0]
"[0, 1].",4. Empirical Evaluation,[0],[0]
The response vector consists of ±1 labels.,4. Empirical Evaluation,[0],[0]
"We also perform experiments on synthetic data generated as in Chen et al. (2015); see Appendix H for details.
",4. Empirical Evaluation,[0],[0]
"In our experiments, we compare three different choices of sampling probabilities: selecting columns (i) uniformly at random, (ii) proportional to their leverage scores, or (iii) proportional to their ridge leverage scores.",4. Empirical Evaluation,[0],[0]
"For each sampling method, we run Algorithm 1 for 50 iterations with a variety of sketch sizes, and measure (i) the relative error of the solution vector ‖x̂
∗−x∗‖2 ‖x∗‖2 , where x ∗ is the true optimal solu-
tion and (ii) the objective sub-optimality f(x̂ ∗)
f(x∗)",4. Empirical Evaluation,[0],[0]
"− 1, where f(x) = ‖Ax− b‖22 + λ‖x‖22 is the objective function for the ridge-regression problem.
",4. Empirical Evaluation,[0],[0]
The results are shown in Figure 1.,4. Empirical Evaluation,[0],[0]
Figures 1a and 1b plot the relative error of the solution vector and the objective suboptimality (for a fixed sketch size) as the iterative algorithm progresses.,4. Empirical Evaluation,[0],[0]
Figure 1c plots the relative error of the solution with respect to varying sketch sizes (the plots for objective sub-optimality are analogous and thus omitted).,4. Empirical Evaluation,[0],[0]
"We observe that both the solution error and the objective sub-optimality decay exponentially as our iterative algorithm progresses.4
4For these experiments, we have set the regularization parameter λ = 10 in the ridge regression objective as well as when computing the ridge leverage score sampling probabilities.
",4. Empirical Evaluation,[0],[0]
"Next, we show that the approximation quality depends directly on the degrees of freedom dλ of the ridge-regression problem (eqn.",4. Empirical Evaluation,[0],[0]
"(4)), rather than the dimensions of the design matrix.",4. Empirical Evaluation,[0],[0]
"To this end, we keep the design matrix unchanged (n remains fixed), and vary the regularization parameter λ ∈ {1, 2, 5, 10, 20, 50}.",4. Empirical Evaluation,[0],[0]
Figure 1d plots the relative solution error against the degrees of freedom dλ,4. Empirical Evaluation,[0],[0]
(for a fixed sketch size and number of iterations); we observe that the relative error decreases roughly exponentially as dλ decreases (as λ increases).,4. Empirical Evaluation,[0],[0]
"Thus, the sketch size or number of iterations necessary to achieve a certain precision in the solution also decreases with dλ, even though n remains fixed.",4. Empirical Evaluation,[0],[0]
We have presented simple structural results that guarantee high-quality approximations to the optimal solution vector of ridge regression.,5. Conclusion and Open Problems,[0],[0]
"In particular, our second structural result presents the first accuracy analysis for ridge regression when the ridge leverage scores are used to sample predictor variables.",5. Conclusion and Open Problems,[0],[0]
The sample size depends on the degrees of freedom of the ridge regression problem and not the dimensions of the design matrix.,5. Conclusion and Open Problems,[0],[0]
An obvious open problem is to either improve the sample size or present lower bounds showing that our bounds are tight.,5. Conclusion and Open Problems,[0],[0]
"Additionally, the results of Theorem 4 should be generalized to cover the t > 1 case.
",5. Conclusion and Open Problems,[0],[0]
"Finally, an interesting open problem would be to investigate whether the use of different sampling matrices in each iteration of Algorithm 1 (i.e., introducing new “randomness” in each iteration) could lead to provably improved bounds for our main theorems.",5. Conclusion and Open Problems,[0],[0]
"We conjecture that this is indeed the case, and we present further experiment results in Appendix H which support our conjecture.",5. Conclusion and Open Problems,[0],[0]
"In particular, the results show that using a newly sampled sketching matrix at every iteration enables faster convergence as the iterations progress, and also reduces the minimum sketch size necessary for Algorithm 1 to converge.
Acknowledgements.",5. Conclusion and Open Problems,[0],[0]
We thank an anonymous reviewer for pointing out the connection between our method and the preconditioned Richardson iteration.,5. Conclusion and Open Problems,[0],[0]
AC and PD were partially supported by NSF IIS-1661760 and IIS-1661756.,5. Conclusion and Open Problems,[0],[0]
JY was supported by NSF IIS-1149789 and IIS-1618690.,5. Conclusion and Open Problems,[0],[0]
Ridge regression is a variant of regularized least squares regression that is particularly suitable in settings where the number of predictor variables greatly exceeds the number of observations.,abstractText,[0],[0]
"We present a simple, iterative, sketching-based algorithm for ridge regression that guarantees highquality approximations to the optimal solution vector.",abstractText,[0],[0]
"Our analysis builds upon two simple structural results that boil down to randomized matrix multiplication, a fundamental and wellunderstood primitive of randomized linear algebra.",abstractText,[0],[0]
An important contribution of our work is the analysis of the behavior of sub-sampled ridge regression problems when the ridge leverage scores are used: we prove that accurate approximations can be achieved by a sample whose size depends on the degrees of freedom of the ridge-regression problem rather than the dimensions of the design matrix.,abstractText,[0],[0]
Our empirical evaluations verify our theoretical results on both real and synthetic data.,abstractText,[0],[0]
"An Iterative, Sketching-based Framework for Ridge Regression",title,[0],[0]
"The problem of training deep feed-forward neural networks is often studied as a nonlinear programming problem (Bazaraa et al., 2013; Bertsekas, 1999; Kuhn & Tucker, 2014)
min θ J(θ)
where θ represents the set of trainable parameters and J is the empirical loss function.",1. Introduction,[0],[0]
"In the general unconstrained case, necessary optimality conditions are given by the condition∇θJ(θ∗) = 0 for an optimal set of training parameters θ∗.",1. Introduction,[0],[0]
"This is largely the basis for (stochastic) gradient-descent based optimization algorithms in deep learning (Robbins & Monro, 1951; Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2014).",1. Introduction,[0],[0]
"When there are additional constraints, e.g. on the trainable parameters, one can instead employ projected
1Institute of High Performance Computing, Singapore.",1. Introduction,[0],[0]
"Correspondence to: Qianxiao Li <liqix@ihpc.a-star.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
versions of the above algorithms.",1. Introduction,[0],[0]
"More broadly, necessary conditions for optimality can be derived in the form of the Karush-Kuhn-Tucker conditions (Kuhn & Tucker, 2014).",1. Introduction,[0],[0]
Such approaches are quite general and typically do not rely on the structures of the objectives encountered in deep learning.,1. Introduction,[0],[0]
"However, in deep learning, the objective function J often has a specific structure; It is derived from feeding a batch of inputs recursively through a sequence of trainable transformations, which can be adjusted so that the final outputs are close to some fixed target set.",1. Introduction,[0],[0]
"This process resembles an optimal control problem (Bryson, 1975; Bertsekas, 1995; Athans & Falb, 2013) that originates from the study of the calculus of variations.
",1. Introduction,[0],[0]
"In this paper, we exploit this optimal control viewpoint of deep learning.",1. Introduction,[0],[0]
"First, we introduce the discrete-time Pontryagin’s maximum principle (PMP) (Halkin, 1966), which is an extension the central result in optimal control due to Pontryagin and coworkers (Boltyanskii et al., 1960; Pontryagin, 1987).",1. Introduction,[0],[0]
"This is an alternative set of necessary conditions characterizing optimality, and we discuss the extent of its validity in the context of deep learning.",1. Introduction,[0],[0]
"Next, we introduce the discrete method of successive approximations (MSA) based on the PMP to optimize deep neural networks.",1. Introduction,[0],[0]
"A rigorous error estimate is proved that elucidates the dynamics of the MSA, and aids us in designing optimization algorithms under rather general conditions.",1. Introduction,[0],[0]
"We apply our method to train a class of unconventional networks, i.e. those with discretevalued weights, to illustrate the usefulness of this approach.",1. Introduction,[0],[0]
"In the process, we discover that in the case of ternary networks, our training algorithm obtains trained models that are very sparse, which is an attractive feature in practice.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows:,1. Introduction,[0],[0]
"In Sec. 2, we introduce the optimal control viewpoint and the discretetime Pontryagin’s maximum principle.",1. Introduction,[0],[0]
"We then introduce the method of successive approximation in Sec. 3 and prove our main estimate, Theorem 2.",1. Introduction,[0],[0]
"In Sec. 4, we derive algorithms based on the developed theory to train binary and ternary neural networks.",1. Introduction,[0],[0]
"Finally, we end with a discussion on related work and a conclusion in Sec. 5 and 6 respectively.",1. Introduction,[0],[0]
"Various details on proofs and algorithms are provided in Appendix A-D, which also contains a link to a software implementation of our algorithms that reproduces all exper-
iments in this paper.
",1. Introduction,[0],[0]
"Hereafter, we denote the usual Euclidean norm by ‖ · ‖ and the corresponding induced matrix norm by ‖ · ‖2.",1. Introduction,[0],[0]
The Frobenius norm is written as ‖ · ‖F .,1. Introduction,[0],[0]
"Throughout this work, we use a bold-faced version of a variable to represent a collection of the same variable, but indexed additionally by t, e.g. θ := {θt : t = 0, . . .",1. Introduction,[0],[0]
", T − 1}.",1. Introduction,[0],[0]
"In this section, we formalize the problem of training a deep neural network as an optimal control problem.",2. The Optimal Control Viewpoint,[0],[0]
"Let T ∈ Z+ denote the number of layers and {xs,0 ∈ Rd0 : s = 0, . . .",2. The Optimal Control Viewpoint,[0],[0]
", S} represent a collection of fixed inputs (images, time-series).",2. The Optimal Control Viewpoint,[0],[0]
"Here, S ∈ Z+ is the sample size.",2. The Optimal Control Viewpoint,[0],[0]
"Consider the dynamical system
xs,t+1 = ft(xs,t, θt), t = 0, 1, . . .",2. The Optimal Control Viewpoint,[0],[0]
", T − 1, (1)
where for each t, ft : Rdt × Θt → Rdt+1 is a transformation on the state.",2. The Optimal Control Viewpoint,[0],[0]
"For example, in typical neural networks, it can represent a trainable affine transformation or a nonlinear activation (in which case it is not trainable and ft does not depend on θ).",2. The Optimal Control Viewpoint,[0],[0]
We assume that each trainable parameter set Θt is a subset of an Euclidean space.,2. The Optimal Control Viewpoint,[0],[0]
"The goal of training a neural network is to adjust the weights θ := {θt : t = 0, . . .",2. The Optimal Control Viewpoint,[0],[0]
", T − 1} so as to minimize some loss function that measures the difference between the final network output xs,T and some true targets ys of xs,0, which are fixed.",2. The Optimal Control Viewpoint,[0],[0]
"Thus, we may define a family of real-valued functions Φs : RdT → R acting on xs,T (ys are absorbed into the definition of Φs) and the average loss function is∑ s Φs(xs,T )/S.",2. The Optimal Control Viewpoint,[0],[0]
"In addition, we may consider some regularization terms for each layer Lt : Rdt × Θt → R that has to be simultaneously minimized.",2. The Optimal Control Viewpoint,[0],[0]
"In typical applications, regularization is only performed for the trainable parameters so that Lt(x, θ) ≡ Lt(θ), but here we will consider the slightly more general case where it is also possible to regularize the state at each layer.",2. The Optimal Control Viewpoint,[0],[0]
"In summary, we wish to solve the following problem
min θ∈Θ
J(θ) := 1
S S∑ s=1 Φs(xs,T ) + 1 S S∑ s=1 T−1∑ t=0 Lt(xs,t, θt)
subject to: xs,t+1 = ft(xs,t, θt), t = 0, . . .",2. The Optimal Control Viewpoint,[0],[0]
", T − 1, s ∈",2. The Optimal Control Viewpoint,[0],[0]
"[S] (2)
where we have defined for shorthand Θ := {Θ0 × · · · × ΘT−1} and [S] := {1, . . .",2. The Optimal Control Viewpoint,[0],[0]
", S}.",2. The Optimal Control Viewpoint,[0],[0]
"One may recognize problem (2) as a classical fixed-time, variable-terminal-state optimal control problem in discrete time (Ogata, 1995), in fact a special one with almost decoupled dynamics across samples in [S].",2. The Optimal Control Viewpoint,[0],[0]
"Maximum principles of the Pontryagin type (Boltyanskii et al., 1960; Pontryagin, 1987) usually consist of necessary conditions for optimality in the form of the maximization of a certain Hamiltonian function.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
The distinguishing feature is that it does not assume differentiability (or even continuity) of ft with respect to θ.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
Consequently the optimality condition and the algorithms based on it need not rely on gradient-descent type updates.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"This is an attractive feature for certain classes of applications.
",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Let θ∗ = {θ0, . . .",2.1. The Pontryagin’s Maximum Principle,[0],[0]
", θT−1} ∈ Θ be a solution of (2).",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"We now outline informally the Pontryagin’s maximum principle (PMP) that characterizes θ∗. First, for each t we define the Hamiltonian function",2.1. The Pontryagin’s Maximum Principle,[0],[0]
Ht : Rdt × Rdt+1,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"×Θt → R by
Ht(x, p, θ) := p · ft(x, θ)− 1SLt(x, θ).",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"(3)
One can show the following necessary conditions.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Theorem 1 (Discrete PMP, Informal Statement).",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Let ft and Φs, s = 1, . . .",2.1. The Pontryagin’s Maximum Principle,[0],[0]
", S be sufficiently smooth in",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"x. Assume further that for each t and x ∈ Rdt , the sets {ft(x, θ) : θ ∈ Θt} and {Lt(x, θ) :",2.1. The Pontryagin’s Maximum Principle,[0],[0]
θ ∈ Θt} are convex.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Then, there exists co-state processes p∗s := {p∗s,t : t = 0, . . .",2.1. The Pontryagin’s Maximum Principle,[0],[0]
", T}, such that following holds for t = 0, . . .",2.1. The Pontryagin’s Maximum Principle,[0],[0]
", T − 1 and s ∈",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"[S]:
x∗s,t+1 = ∇pHt(x∗s,t, p∗s,t+1, θ∗t ), x∗s,0 = xs,0 (4) p∗s,t = ∇xHt(x∗s,t, p∗s,t+1, θ∗t ), p∗s,T = − 1S∇Φs(x ∗ s,T ) (5)
S∑ s=1",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Ht(x ∗ s,t, p ∗ s,t+1, θ ∗ t ) ≥ S∑ s=1",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Ht(x ∗ s,t, p ∗ s,t+1, θ), ∀θ ∈",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Θt
(6)
",2.1. The Pontryagin’s Maximum Principle,[0],[0]
The full statement of Theorem 1 involve explicit smoothness assumptions and additional technicalities (such as the inclusion of an abnormal multiplier).,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"In Appendix A, we state these assumptions and give a sketch of its proof based on Halkin (1966).
",2.1. The Pontryagin’s Maximum Principle,[0],[0]
Let us discuss the PMP in detail.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
The state equation (4) is simply the forward propagation equation (1) under the optimal parameters θ∗. Eq.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
(5) defines the evolution of the co-state p∗s .,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"To draw an analogy with nonlinear programming, the co-state can be interpreted as a set of Lagrange multipliers that enforces the constraint (1) when the optimization problem (2) is regarded as a joint optimization problem in θ and xs, s ∈",2.1. The Pontryagin’s Maximum Principle,[0],[0]
[S].,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"In the optimal control and PMP viewpoint, it is perhaps more appropriate to think of the dynamics (5) as the evolution of the normal vector of a separating hyper-plane, which separates the set of reachable states and the set of states where the objective function takes on values smaller than the optimum (see Appendix A).
",2.1. The Pontryagin’s Maximum Principle,[0],[0]
The Hamiltonian maximization condition (6) is the centerpiece of the PMP.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"It says that an optimal solution θ∗ must globally maximize the (summed) Hamiltonian for each layer t = 0, . . .",2.1. The Pontryagin’s Maximum Principle,[0],[0]
", T",2.1. The Pontryagin’s Maximum Principle,[0],[0]
− 1.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Let us contrast this statement with usual
first-order optimality conditions of the form ∇θJ(θ∗) = 0.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"A key observation is that in Theorem 1, we did not make reference to the derivative of any quantity with respect θ.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"In fact, the PMP holds even if ft is not differentiable, or even continuous, with respect to θ, as long as the convexity assumptions are satisfied.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"On the other hand, if we assume for each t: 1) ft is differentiable with respect to θ; 2)",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"Ht is concave in θ; and 3) θ∗t lies in the interior of Θt, then the Hamiltonian maximization condition (6) is equivalent to the condition",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"∇θ ∑ sHt = 0 for all t, which one can then show is equivalent to ∇θJ = 0",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"(See Appendix C, proof of Prop. C.1).",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"In other words, the PMP can be viewed as a stronger set of necessary conditions (at optimality, Ht is not just stationary, but globally maximized) and has meaning in more general scenarios, e.g. when stationarity with respect to θ is not achievable due to constraints, or not defined due to non-differentiability.",2.1. The Pontryagin’s Maximum Principle,[0],[0]
Remark 1.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"It may occur that ∑ sHt(x ∗ s,t, p ∗ s,t+1, θ) is constant for all θ ∈ Θt, in which case the problem is singular (Athans & Falb, 2013).",2.1. The Pontryagin’s Maximum Principle,[0],[0]
"In such cases, the PMP is trivially satisfied by any θ",2.1. The Pontryagin’s Maximum Principle,[0],[0]
and so the PMP does not tell us anything useful.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
This may arise especially in the case where there are no regularization terms.,2.1. The Pontryagin’s Maximum Principle,[0],[0]
"The most crucial assumption in Theorem 1 is the convexity of the sets {ft(x, θ) : θ ∈ Θt} and {Lt(x, θ) : θ ∈ Θt} for each fixed x 1.",2.2. The Convexity Assumption,[0],[0]
We now discuss how restrictive these assumptions are with regard to deep neural networks.,2.2. The Convexity Assumption,[0],[0]
Let us first assume that the admissable sets Θt are convex.,2.2. The Convexity Assumption,[0],[0]
"Then, the assumption with respect to Lt is not restrictive since most regularizers (e.g. `1, `2) satisfy it.",2.2. The Convexity Assumption,[0],[0]
"Let us consider the convexity of {ft(x, θ) : θ ∈ Θt}.",2.2. The Convexity Assumption,[0],[0]
"In classical feed-forward neural networks, there are two types of layers: trainable ones and non-trainable ones.",2.2. The Convexity Assumption,[0],[0]
"Suppose layer t is non-trainable (e.g. f(xt, θt) = σ(xt) where σ is a non-linear activation function), then for each x the set {ft(x, θ) : θ ∈ Θt} is a singleton, and hence trivially convex.",2.2. The Convexity Assumption,[0],[0]
"On the other hand, in trainable layers ft is usually affine in θ.",2.2. The Convexity Assumption,[0],[0]
"This includes fully connected layers, convolution layers and batch normalization layers (Ioffe & Szegedy, 2015).",2.2. The Convexity Assumption,[0],[0]
"In these cases, as long as the admissable set Θt is convex, we again satisfy the convexity assumption.",2.2. The Convexity Assumption,[0],[0]
Residual networks also satisfy the convexity constraint if one introduces auxiliary variables (see Appendix A.1).,2.2. The Convexity Assumption,[0],[0]
"When the set Θt is not convex, then it is in general not true that the PMP constitute necessary conditions.
",2.2. The Convexity Assumption,[0],[0]
"1Note that this is in general unrelated to the convexity, in the sense of functions, of ft with respect to either x or θ.",2.2. The Convexity Assumption,[0],[0]
"For example, the scalar function f(x, θ) = θ3 sin(x) is evidently non-convex in both arguments, but {f(x, θ) : θ ∈ R} is convex for each x.",2.2. The Convexity Assumption,[0],[0]
"On the other hand {θx : θ ∈ {−1, 1}} is non-convex because of a non-convex admissible set.
",2.2. The Convexity Assumption,[0],[0]
"Finally, we remark that in the original derivation of the PMP for continuous-time control systems (Boltyanskii et al., 1960) (i.e. ẋs,t = ft(xs,t, θt), t ∈",2.2. The Convexity Assumption,[0],[0]
"[0, T ] in place of Eq.",2.2. The Convexity Assumption,[0],[0]
"(1)), the convexity condition can be removed due to the “convexifying” effect of integration with respect to time (Halkin, 1966; Warga, 1962).",2.2. The Convexity Assumption,[0],[0]
"Hence, the convexity condition is purely an artifact of discrete-time dynamical systems.",2.2. The Convexity Assumption,[0],[0]
The PMP (Eq. (4)-(6)) gives us a set of necessary conditions an optimal solution to (2) must satisfy.,3. The Method of Successive Approximations,[0],[0]
"However, it does not tell us how to find one such solution.",3. The Method of Successive Approximations,[0],[0]
"The goal of this section is to discuss algorithms for solving (2) based on the maximum principle.
",3. The Method of Successive Approximations,[0],[0]
On closer inspection of Eq.,3. The Method of Successive Approximations,[0],[0]
"(4)-(6), one can see that they each represent a manifold in solution space consisting of all possible θ, {xs, s ∈",3. The Method of Successive Approximations,[0],[0]
"[S]} and {ps, s ∈",3. The Method of Successive Approximations,[0],[0]
"[S]}, and the intersection of these three manifolds must contain an optimal solution, if one exists.",3. The Method of Successive Approximations,[0],[0]
"Consequently, an iterative projection method that successively projects a guessed solution onto each of the manifolds is natural.",3. The Method of Successive Approximations,[0],[0]
"This is the method of successive approximations (MSA), which was first introduced to solve continuous-time optimal control problems (Krylov & Chernousko, 1962; Chernousko & Lyubushin, 1982).",3. The Method of Successive Approximations,[0],[0]
"Let us now outline a discrete-time version.
",3. The Method of Successive Approximations,[0],[0]
"Start from an initial guess θ0 := {θ0t , t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T − 1}.",3. The Method of Successive Approximations,[0],[0]
"For each sample s, we define xθ 0
s := {xθ 0 s,t : t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T} by the dynamics
xθ 0 s,t+1 = ft(x θ0 s,t, θ 0 t ), x θ0 s,0 = xs,0, (7)
for t = 0, . .",3. The Method of Successive Approximations,[0],[0]
.,3. The Method of Successive Approximations,[0],[0]
", T − 1.",3. The Method of Successive Approximations,[0],[0]
"Intuitively, this is a projection onto the manifold defined by Eq.",3. The Method of Successive Approximations,[0],[0]
(4).,3. The Method of Successive Approximations,[0],[0]
"Next, we perform the projection onto the manifold defined by Eq.",3. The Method of Successive Approximations,[0],[0]
"(5), i.e. we define pθ 0
s := {pθ 0 s,t : t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T} by the backward dynamics
pθ 0 s,t = ∇xH(xθ 0 s,t, p θ0 s,t+1, θ 0 t ), p θ0 s,T = − 1S∇Φs(x θ0
s,T ), (8)
for t = T",3. The Method of Successive Approximations,[0],[0]
"− 1, . . .",3. The Method of Successive Approximations,[0],[0]
", 0.",3. The Method of Successive Approximations,[0],[0]
"Finally, we project onto manifold defined by Eq.",3. The Method of Successive Approximations,[0],[0]
"(6) by performing Hamiltonian maximization to obtain θ1 := {θ1t : t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T − 1} with
θ1t = arg max θ∈Θt S∑ s=1",3. The Method of Successive Approximations,[0],[0]
"Ht(x θ0 s,t, p θ0 s,t+1, θ).",3. The Method of Successive Approximations,[0],[0]
"t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T",3. The Method of Successive Approximations,[0],[0]
− 1. (9) The steps (7)-(9) are then repeated until convergence.,3. The Method of Successive Approximations,[0],[0]
"We summarize the basic MSA algorithm in Alg. 1.
",3. The Method of Successive Approximations,[0],[0]
Let us contrast the MSA with gradient-descent based methods.,3. The Method of Successive Approximations,[0],[0]
"Similar to the formulation of the PMP, at no point did we take the derivative of any quantity with respect to θ.",3. The Method of Successive Approximations,[0],[0]
"Hence, we can in principle apply this to problems that
Algorithm 1 Basic MSA Initialize: θ0 = {θ0t ∈",3. The Method of Successive Approximations,[0],[0]
Θt : t = 0 . . .,3. The Method of Successive Approximations,[0],[0]
", T − 1}; for k = 0 to #Iterations do xθ k s,t+1 = ft(x θk s,t, θ k t ), xθ k
s,0 = xs,0, ∀s, t; pθ k s,t = ∇xHt(xθ k s,t, p θk s,t+1, θ k t ), p θk
s,T =",3. The Method of Successive Approximations,[0],[0]
"− 1S∇Φs(xs,T ), ∀s, t; θk+1t = arg maxθ∈Θt ∑S s=1",3. The Method of Successive Approximations,[0],[0]
"Ht(x θk s,t, p θk
s,t+1, θ) for t = 0, . . .",3. The Method of Successive Approximations,[0],[0]
", T − 1;
end for
are not differentiable with respect to θ.",3. The Method of Successive Approximations,[0],[0]
"However, the catch is that the Hamiltonian maximization step (9) may not be trivial to evaluate.",3. The Method of Successive Approximations,[0],[0]
"Nevertheless, observe that the maximization step is decoupled across different layers of the neural network, and hence it is a much smaller problem than the original optimization problem, and its solution method can be parallelized.",3. The Method of Successive Approximations,[0],[0]
"Alternatively, as seen in Sec. 4, one can exploit cases where the maximization step has explicit solutions.
",3. The Method of Successive Approximations,[0],[0]
"The basic MSA (Alg. 1 can be shown to converge for problems where ft is linear and the costs Φs, Lt are quadratic (Aleksandrov, 1968).",3. The Method of Successive Approximations,[0],[0]
"In general, however, unless a good initial condition is given, the MSA may diverge.",3. The Method of Successive Approximations,[0],[0]
Let us understand the nature of such phenomena by obtaining rigorous error estimates per-iteration of Eq. (7)-(9).,3. The Method of Successive Approximations,[0],[0]
"In this section, we derive a rigorous error estimate for the MSA, which can help us understand its dynamics.",3.1. An Error Estimate for the MSA,[0],[0]
Let us define Wt := conv{x ∈,3.1. An Error Estimate for the MSA,[0],[0]
"Rdt : ∃θ and s s.t. xθs,t = x}, where xθt is defined according to Eq.",3.1. An Error Estimate for the MSA,[0],[0]
(7).,3.1. An Error Estimate for the MSA,[0],[0]
This is the convex hull of all states reachable at layer t by some initial sample and some choice of the values for the trainable parameters.,3.1. An Error Estimate for the MSA,[0],[0]
"Let us now make the following assumptions:
(A1) Φs is twice continuously differentiable, with Φs and ∇Φs satisfying a Lipschitz condition, i.e. there exists K > 0",3.1. An Error Estimate for the MSA,[0],[0]
"such that for all x, x′ ∈WT and s ∈",3.1. An Error Estimate for the MSA,[0],[0]
"[S]
|Φs(x)− Φs(x′)|+",3.1. An Error Estimate for the MSA,[0],[0]
‖∇Φs(x)−∇Φs(x′)‖ ≤,3.1. An Error Estimate for the MSA,[0],[0]
"K‖x− x′‖
(A2) ft(·, θ) and Lt(·, θ) are twice continuously differentiable in x, with ft,∇xft, Lt,∇xLt",3.1. An Error Estimate for the MSA,[0],[0]
"satisfying Lipschitz conditions in x uniformly in t and θ, i.e. there exists K > 0",3.1. An Error Estimate for the MSA,[0],[0]
"such that
‖ft(x, θ)− ft(x′, θ)‖+ ‖∇xft(x, θ)−∇xft(x′, θ)‖2 + |Lt(x, θ)− Lt(x′, θ)|+ ‖∇xLt(x, θ)−∇xLt(x′, θ)‖ ≤ K‖x− x′‖
for all x, x′ ∈Wt, θ ∈ Θt and t = 0, . . .",3.1. An Error Estimate for the MSA,[0],[0]
", T",3.1. An Error Estimate for the MSA,[0],[0]
"− 1.
Again, let us discuss these assumptions with respect to neural networks.",3.1. An Error Estimate for the MSA,[0],[0]
"Note that both assumptions are more easily
satisfied if each Wt is bounded, which is usually implied by the boundedness of Θt.",3.1. An Error Estimate for the MSA,[0],[0]
"Although this is not typically true in principle, we can safely assume this in practice by truncating weights that are too large in magnitude.",3.1. An Error Estimate for the MSA,[0],[0]
"Consequently, (A1) is not very restrictive, since many commonly employed loss functions (mean-square, soft-max with crossentropy) satisfy these assumptions.",3.1. An Error Estimate for the MSA,[0],[0]
"In (A2), the regularity assumption on Lt is again not an issue, because we mostly take Lt to be independent of x.",3.1. An Error Estimate for the MSA,[0],[0]
"On the other hand, the regularity of ft with respect to x is sometimes restrictive.",3.1. An Error Estimate for the MSA,[0],[0]
"For example, ReLU activations does not satisfy (A2) due to non-differentiability.",3.1. An Error Estimate for the MSA,[0],[0]
"Nevertheless, any suitably mollified version (like Soft-plus) does satisfy it.",3.1. An Error Estimate for the MSA,[0],[0]
"Moreover, tanh and sigmoid activations also satisfy (A2).",3.1. An Error Estimate for the MSA,[0],[0]
"Finally, unlike in Theorem 1, we do not assume the convexity of the sets {ft(x, θ) : θ ∈ Θt} and {Lt(x, θ) : θ ∈ Θt}, and hence the results in this section applies to discrete-weight neural networks considered in Sec. 4.",3.1. An Error Estimate for the MSA,[0],[0]
"With the above assumptions, we prove the following estimate.",3.1. An Error Estimate for the MSA,[0],[0]
Theorem 2 (Error Estimate for Discrete MSA).,3.1. An Error Estimate for the MSA,[0],[0]
Let assumptions (A1) and (A2) be satisfied.,3.1. An Error Estimate for the MSA,[0],[0]
"Then, there exists a constant C > 0, independent of S, θ and φ, such that for any θ,φ ∈ Θ, we have
J(φ)− J(θ)
≤− T−1∑ t=0 S∑ s=1",3.1. An Error Estimate for the MSA,[0],[0]
"Ht(x θ s,t, p θ s,t+1, φt)−Ht(xθs,t, pθs,t+1, θt) (10)
+ C
S T−1∑ t=0 S∑ s=1",3.1. An Error Estimate for the MSA,[0],[0]
"‖ft(xθs,t, φt)−",3.1. An Error Estimate for the MSA,[0],[0]
"ft(xθs,t, θt)‖2 (11)
+ C
S T−1∑ t=0 S∑ s=1 ‖∇xft(xθs,t, φt)−∇xft(xθs,t, θt)‖22, (12)
+ C
S T−1∑ t=0 S∑ s=1",3.1. An Error Estimate for the MSA,[0],[0]
"‖∇xLt(xθs,t, φt)−∇xLt(xθs,t, θt)‖2, (13)
where xθs , p θ s are defined by Eq. (7) and (8).
",3.1. An Error Estimate for the MSA,[0],[0]
Proof.,3.1. An Error Estimate for the MSA,[0],[0]
The proof follows from elementary estimates and a discrete Gronwall’s lemma.,3.1. An Error Estimate for the MSA,[0],[0]
"See Appendix B.
Theorem 2 relates the decrement of the total objective function J with respect to the iterative projection steps of the MSA.",3.1. An Error Estimate for the MSA,[0],[0]
"Intuitively, Theorem 2 says that the Hamiltonian maximization step (9) is generally the right direction, because a large magnitude of (10) results in higher loss improvement.",3.1. An Error Estimate for the MSA,[0],[0]
"However, whenever we change the parameters from θ to φ (e.g. during the maximization step (9)), we incur nonnegative penalty terms (11)-(13).",3.1. An Error Estimate for the MSA,[0],[0]
"Observe that these penalty terms vanish if φ = θ, or more generally, when the state and co-state equations (Eq. (7), (8)) are still satisfied when θ is replaced by φ.",3.1. An Error Estimate for the MSA,[0],[0]
"In other words, these terms measure the distance from manifolds defined by the state and co-state equations when the parameter changes.",3.1. An Error Estimate for the MSA,[0],[0]
"Alg. 1 diverges
when these penalty terms dominate the gains from (10).",3.1. An Error Estimate for the MSA,[0],[0]
This insight can point us in the right direction of developing convergent modifications of the basic MSA.,3.1. An Error Estimate for the MSA,[0],[0]
We shall now discuss this in the context of some specific applications.,3.1. An Error Estimate for the MSA,[0],[0]
"We now turn to the application of the theory developed in the previous section on the MSA, which is a PMP-based numerical method for training deep neural networks.",4. Neural Networks with Discrete Weights,[0],[0]
"As discussed previously, the main strength of the PMP and MSA formalism is that we do not rely on gradient-descent type updates.",4. Neural Networks with Discrete Weights,[0],[0]
This is particularly useful when one considers neural networks with (some) trainable parameters that can only take values in a discrete set.,4. Neural Networks with Discrete Weights,[0],[0]
"Then, any small gradient update to the parameters will almost always be infeasible.",4. Neural Networks with Discrete Weights,[0],[0]
"In this section, we will consider two such cases: binary networks, where weights are restricted to {−1,+1}; and ternary networks, where weights are selected from {−1,+1, 0}.",4. Neural Networks with Discrete Weights,[0],[0]
These networks are potentially useful for low-memory devices as storing the trained weights requires less memory.,4. Neural Networks with Discrete Weights,[0],[0]
"In this section, we will modify the MSA so that we can train these networks in a principled way.",4. Neural Networks with Discrete Weights,[0],[0]
"Binary neural networks are those with binary trainable layers, e.g. in the fully connected case,
ft(x, θ) = θx (14)
",4.1. Binary Networks,[0],[0]
where θ ∈,4.1. Binary Networks,[0],[0]
"Θt = {−1,+1}dt×dt+1 is a binary matrix.",4.1. Binary Networks,[0],[0]
"A similar form of ft holds for convolution neural networks after reshaping, except that Θt is now the set of Toeplitz binary matrices.",4.1. Binary Networks,[0],[0]
"Hereafter, we will consider the fully connected case for simplicity of exposition.",4.1. Binary Networks,[0],[0]
It is also natural to set the regularization to 0 since there is in general no preference between +1 or −1.,4.1. Binary Networks,[0],[0]
"Thus, the Hamiltonian has the form
Ht(x, p, θ) = p · θx.
",4.1. Binary Networks,[0],[0]
"Consequently, the Hamiltonian maximization step (9) has explicit solution, given by
arg max θ∈Θt S∑ s=1",4.1. Binary Networks,[0],[0]
"Ht(x θk s,t, p θk s,t+1, θ) = sign(M θk t )
",4.1. Binary Networks,[0],[0]
"where Mθt := ∑S s=1 p θ s,t+1(x θ s,t)
T .",4.1. Binary Networks,[0],[0]
Note that the sign function is applied element-wise.,4.1. Binary Networks,[0],[0]
"If [Mθt ]ij = 0, then the argmax is arbitrary.",4.1. Binary Networks,[0],[0]
"Using Theorem 2 with the form of ft given
by (14) and the fact that Lt ≡ 0, we get
J(φ)− J(θ) ≤− T−1∑ t=0 S∑ s=1",4.1. Binary Networks,[0],[0]
"Ht(x θk s,t, p θk s,t+1, θ)
+ C
S T−1∑ t=0 (1 + S∑ s=1 ‖xθs,t‖2)‖φt − θt‖2F ,
Note that we have used the inequality ‖ · ‖2 ≤ ‖ · ‖F .",4.1. Binary Networks,[0],[0]
"Assuming that ‖xθs,t‖ is O(1), we may then decrease J by not only maximizing the Hamiltonian, but also penalizing the difference ‖φt − θt‖F , i.e. for each k and t we set
θk+1t = arg max",4.1. Binary Networks,[0],[0]
"θ∈Θt [ S∑ s=1 Ht(x θk s,t, p θk s,t+1, θ)− ρk,t‖θ − θk‖2F ] (15)
for some penalization parameters ρk,t > 0.",4.1. Binary Networks,[0],[0]
"This again has the explicit solution
[θk+1t ]ij =
{ sign([Mθ k
t ]ij) |[Mθ k",4.1. Binary Networks,[0],[0]
"t ]ij | ≥ 2ρk,t",4.1. Binary Networks,[0],[0]
"[θkt ]ij otherwise (16)
",4.1. Binary Networks,[0],[0]
"Therefore, we simply replace the parameter update step in Alg. 1 with (16).",4.1. Binary Networks,[0],[0]
"Furthermore, to deal with mini-batches, we keep a moving average of Mθ k
t across different minibatches and use the averaged value to update our parameters.",4.1. Binary Networks,[0],[0]
It is found empirically that this further stabilizes the algorithm.,4.1. Binary Networks,[0],[0]
"Note that the assumption ‖xθs,t‖ is O(1) can be achieved by normalization, e.g. batch-normalization (Ioffe & Szegedy, 2015).",4.1. Binary Networks,[0],[0]
We summarize the algorithm in Alg. 2.,4.1. Binary Networks,[0],[0]
"Further algorithmic details are found in Appendix D, where we also discuss the choice of hyper-parameters and the convergence of the algorithm for a simple binary regression problem.",4.1. Binary Networks,[0],[0]
"A rigorous proof of convergence in the general case is beyond the scope of this work, but we demonstrate via experiments below that the algorithm performs well on the tested benchmarks.
",4.1. Binary Networks,[0],[0]
We apply Alg.,4.1. Binary Networks,[0],[0]
"2 to train binary neural networks on various benchmark datasets and compare the results from previous work on training binary-weight neural networks (Courbariaux et al., 2015).",4.1. Binary Networks,[0],[0]
"We consider a fully-connected neural network on MNIST (LeCun, 1998), as well as (shallow) convolutional networks on CIFAR-10 (Krizhevsky & Hinton, 2009) and SVHN (Netzer et al., 2011) datasets.",4.1. Binary Networks,[0],[0]
The network structures are mostly identical to those considered in Courbariaux et al. (2015) for ease of comparison.,4.1. Binary Networks,[0],[0]
Complete implementation and model details are found in Appendix D. The graphs of training/testing loss and error rates are shown in Fig. 1.,4.1. Binary Networks,[0],[0]
"We observe that our algorithm performs well in terms of an optimization algorithm, as measured by the training loss and error rates.",4.1. Binary Networks,[0],[0]
"For the harder datasets
Algorithm 2 Binary MSA Initialize: θ0, M 0 ;
Hyper-parameters: ρk,t, αk,t; for k = 0 to #Iterations do xθ k s,t+1 = ft(x θk s,t, θ k t ) ∀s, t
with xθ k
s,0 = xs,0;
pθ k s,t = ∇xHt(xθ k s,t, p θk s,t+1, θ k t ) ∀s, t
with pθ k
s,T =",4.1. Binary Networks,[0],[0]
"− 1S∇Φs(xs,T ); M k+1 t = αk,tM k t + (1− αk,t) ∑S s=1",4.1. Binary Networks,[0],[0]
"p θk s,t+1(x θk s,t) T
[θk+1t ]ij =
{ sign([M k+1 t ]ij) |[M k+1",4.1. Binary Networks,[0],[0]
"t ]ij | ≥ 2ρk,t
[θkt ]ij otherwise ∀t, i, and j;
end for
(CIFAR-10 and SVHN), we have rapid convergence but worse test loss and error rates at the end, possibly due to overfitting.",4.1. Binary Networks,[0],[0]
"We note that in (Courbariaux et al., 2015), many regularization strategies are performed.",4.1. Binary Networks,[0],[0]
We expect that similar techniques must be employed to improve the testing performance.,4.1. Binary Networks,[0],[0]
"However, these issues are out of the scope of the optimization framework of this paper.",4.1. Binary Networks,[0],[0]
"Note that we also compared the results of BinaryConnect without regularization strategies such as stochastic binarization, but the results are similar in that our algorithm converges very fast with very low training losses, but sometimes overfits.",4.1. Binary Networks,[0],[0]
"We shall consider another case where the network weights are allowed to take on values in {−1,+1, 0}.",4.2. Ternary Networks,[0],[0]
"In this case, our goal is to explore the sparsification of the network.",4.2. Ternary Networks,[0],[0]
"To this end, we shall take Lt(x, θ) = λt‖θ‖2F for some parameter λt.",4.2. Ternary Networks,[0],[0]
"Note that since the weights are restricted to the ternary set, any component-wise `p regularization for p > 0 are identical.",4.2. Ternary Networks,[0],[0]
"The higher the λt values, the more sparse the solution will be.
",4.2. Ternary Networks,[0],[0]
"As in Sec. 4.1, we can write down the Hamiltonian for a fully connected ternary layer as
Ht(x, p, θ) = p · θx− 1Sλt‖θ‖ 2 F .
",4.2. Ternary Networks,[0],[0]
"The derivation of the ternary algorithm then follows directly from those in Sec. 4.1, but with the new form of Hamiltonian above and that Θt = {−1,+1, 0}dt×dt+1 .",4.2. Ternary Networks,[0],[0]
"Maximizing the augmented Hamiltonian (15) with Ht as defined above, we obtain the ternary update rule
[θk+1t ]ij =  +1",4.2. Ternary Networks,[0],[0]
"[Mθ k t ]ij ≥ ρk,t(1− 2[θkt ]ij)",4.2. Ternary Networks,[0],[0]
+,4.2. Ternary Networks,[0],[0]
λt −1,4.2. Ternary Networks,[0],[0]
"[Mθkt ]ij ≤ −ρk,t(1 + 2[θkt ]ij)−",4.2. Ternary Networks,[0],[0]
λt 0,4.2. Ternary Networks,[0],[0]
"otherwise.
(17) We replace the parameter update step in Alg.",4.2. Ternary Networks,[0],[0]
2 by (17) to obtain the MSA algorithm for ternary networks.,4.2. Ternary Networks,[0],[0]
"For completeness, we give the full ternary algorithm in Alg. 3.",4.2. Ternary Networks,[0],[0]
"We
now test the ternary algorithm on the same benchmarks used in Sec. 4.1 and the results are shown in Fig. 2.",4.2. Ternary Networks,[0],[0]
"Observe that the performance on training and testing datasets is similar to the binary case (Fig. 1), but the ternary networks achieve high degrees of sparsity in the weights, with only 0.5-2.5% of the trained weights being non-zero, depending on the dataset.",4.2. Ternary Networks,[0],[0]
This potentially offers significant memory savings compared to its binary or full floating precision counterparts.,4.2. Ternary Networks,[0],[0]
We begin with a discussion of the results presented thus far.,5. Discussion and Related Work,[0],[0]
We first introduced the viewpoint that deep learning can be regarded as a discrete-time optimal control problem.,5. Discussion and Related Work,[0],[0]
"Consequently, an important result in optimal control theory, the Pontryagin’s maximum principle, can be applied to give a set of necessary conditions for optimality.",5. Discussion and Related Work,[0],[0]
These are in general stronger conditions than the usual optimality conditions based on the vanishing of first-order partial derivatives.,5. Discussion and Related Work,[0],[0]
"Moreover, they apply to broader contexts such as problems with constraints on the trainable parameters or problems that are non-differentiable in the trainable parameters.",5. Discussion and Related Work,[0],[0]
"However, we note that specific assumptions regarding the convexity
Algorithm 3 Ternary MSA Initialize: θ0, M 0 ;
Hyper-parameters: ρk,t, αk,t; for k = 0 to #Iterations do xθ k s,t+1 = ft(x θk s,t, θ k t ) ∀s, t
with xθ k
s,0 = xs,0;
pθ k s,t = ∇xHt(xθ k s,t, p θk s,t+1, θ k t ) ∀s, t
with pθ k
s,T = − 1S∇Φs(xs,T ); M k+1 t = αk,tM k t + (1− αk,t) ∑S s=1",5. Discussion and Related Work,[0],[0]
"p θk s,t+1(x θk s,t) T
[θk+1t ]ij =  +1",5. Discussion and Related Work,[0],[0]
"[M k+1 t ]ij ≥ ρk,t(1− 2[θkt ]ij) +",5. Discussion and Related Work,[0],[0]
"λt
−1",5. Discussion and Related Work,[0],[0]
"[Mk+1t ]ij ≤ −ρk,t(1 + 2[θkt ]ij)−",5. Discussion and Related Work,[0],[0]
"λt 0 otherwise.
",5. Discussion and Related Work,[0],[0]
"∀t, i, and j; end for
of some sets must be satisfied.",5. Discussion and Related Work,[0],[0]
"We showed that they are justified for conventional neural networks, but not necessarily so for all neural networks (e.g. binary, ternary networks).
",5. Discussion and Related Work,[0],[0]
"Next, based on the PMP, we introduced an iterative projection technique, the discrete method of successive approximations (MSA), to find an optimal solution of the learning problem.",5. Discussion and Related Work,[0],[0]
"A rigorous error estimate (Theorem 2) is derived for the discrete MSA, which can be used to both understand its dynamics and to derive useful algorithms.",5. Discussion and Related Work,[0],[0]
This should be viewed as the main theoretical result of the present paper.,5. Discussion and Related Work,[0],[0]
"Note that the usual back-propagation with gradient descent can be regarded as a simple modification of the MSA, if differentiability conditions are assumed (see Appendix C).",5. Discussion and Related Work,[0],[0]
"Nevertheless, we note that Theorem 2 itself does not assume any regularity conditions with respect to the trainable parameters.",5. Discussion and Related Work,[0],[0]
"Moreover, neither does it require the convexity conditions in Theorem 1, and hence applies to a wider range of neural networks, including those in Sec. 4.",5. Discussion and Related Work,[0],[0]
"All results up to this point apply to general neural networks (assuming that the respective conditions are satisfied), and are not specific to the applications presented subsequently.
",5. Discussion and Related Work,[0],[0]
"In the last part of this work, we apply our results to devise training algorithms for discrete-weight neural networks, i.e. those with trainable parameters that can only take values in a discrete set.",5. Discussion and Related Work,[0],[0]
"Besides potential applications in model deployment in low-memory devices, the main reasons for choosing such applications are two-fold.",5. Discussion and Related Work,[0],[0]
"First, gradientdescent updates are not applicable by itself because small updates to parameters are prohibited by the discrete equality constraint on the trainable parameters.",5. Discussion and Related Work,[0],[0]
"However, our method based on the MSA is applicable since it does not perform gradient-descent updates.",5. Discussion and Related Work,[0],[0]
"Second, in such applications the potentially expensive Hamiltonian maximization steps in the MSA have explicit solutions.",5. Discussion and Related Work,[0],[0]
This makes MSA an attractive optimization method for problems of this nature.,5. Discussion and Related Work,[0],[0]
"In Sec 4, we demonstrate the effectiveness of our methods on various benchmark datasets.",5. Discussion and Related Work,[0],[0]
"Interestingly, the ternary
network exhibits extremely sparse weights that perform almost as well as its binary counter-part (see Fig. 2).",5. Discussion and Related Work,[0],[0]
"Also, the phenomena of overfitting in Fig. 1 and 2 are interesting as overfitting is generally less common in stochastic gradient based optimization approaches.",5. Discussion and Related Work,[0],[0]
"This seems to suggest that the MSA based methods optimize neural networks in a rather different way.
",5. Discussion and Related Work,[0],[0]
Let us now put our work in the context of the existing literature.,5. Discussion and Related Work,[0],[0]
"First, the optimal control approach we adopt is quite different from the prevailing viewpoint of nonlinear programming (Bertsekas, 1999; Bazaraa et al., 2013; Kuhn & Tucker, 2014) and the analysis of the derived gradient-based algorithms (Moulines, 2011; Shamir & Zhang, 2013; Bach & Moulines, 2013; Xiao & Zhang, 2014; Shalev-Shwartz & Zhang, 2014) for the training of deep neural networks.",5. Discussion and Related Work,[0],[0]
"In particular, the PMP (Thm. 1) and the MSA error estimate (Thm. 2) do not assume differentiability and do not characterize optimality via gradients (or sub-gradients) with respect to trainable parameters.",5. Discussion and Related Work,[0],[0]
"In this sense, it is a stronger and more robust condition, albeit sometimes requiring different assumptions.",5. Discussion and Related Work,[0],[0]
"The optimal control and dynamical
systems viewpoint has been discussed in the context of deep learning in E (2017); Li et al. (2018) and dynamical systems based discretization schemes has been introduced in Haber & Ruthotto (2017); Chang et al. (2017).",5. Discussion and Related Work,[0],[0]
Most of these works have theoretical basis in continuous-time dynamical systems.,5. Discussion and Related Work,[0],[0]
"In particular, Li et al. (2018) analyzed continuous-time analogues of neural networks in the optimal control framework and derived MSA-based algorithms in continuous time.",5. Discussion and Related Work,[0],[0]
"In contrast, the present work presents a discrete-time formulation, which is natural in the usual context of deep learning.",5. Discussion and Related Work,[0],[0]
"The discrete PMP turns out to be more subtle, as it requires additional assumptions of convexity of reachable sets (Thm. 1).",5. Discussion and Related Work,[0],[0]
"Note also that unlike the estimates derived in Li et al. (2018), Thm. 2 holds rigorously for discrete-time neural networks.",5. Discussion and Related Work,[0],[0]
"The present method for stabilizing the MSA is also different from that in Li et al. (2018), where augmented Lagrangian type of modifications are employed.",5. Discussion and Related Work,[0],[0]
The latter would not be effective here because weights cannot be updated infinitesimally without violating the binary/ternary constraint.,5. Discussion and Related Work,[0],[0]
"Moreover, the present methods that rely on explicit solutions of Hamiltonian maximization are fast (comparable to SGD) on a wall-clock basis.
",5. Discussion and Related Work,[0],[0]
"In the deep learning literature, the connection between optimal control and deep learning has been qualitative discussed in LeCun (1988) and applied to the development of automatic differentiation and back-propagation (Bryson, 1975; Baydin et al., 2015).",5. Discussion and Related Work,[0],[0]
"However, there are relatively fewer works relating optimal control algorithms to training neural networks beyond the classical gradient-descent with back-propagation.",5. Discussion and Related Work,[0],[0]
"Optimal control based strategies in hyper-parameter tuning has been discussed in Li et al. (2017b).
",5. Discussion and Related Work,[0],[0]
"In the continuous-time setting, the Pontryagin’s maximum principle and the method of successive approximations have a long history, with a large body of relevant literature including, but not limited to Boltyanskii et al. (1960); Pontryagin (1987); Bryson (1975); Bertsekas (1995); Athans & Falb (2013); Krylov & Chernousko (1962); Aleksandrov (1968); Krylov & Chernousko (1972); Chernousko & Lyubushin (1982); Lyubushin (1982).",5. Discussion and Related Work,[0],[0]
"The discrete-time PMP have been studied in Halkin (1966); Holtzman (1966a); Holtzman & Halkin (1966); Holtzman (1966b); Canon et al. (1970), where Theorem 1 and its extensions are proved.",5. Discussion and Related Work,[0],[0]
"To the best of our knowledge, the discrete-time MSA and its quantitative analysis have not been performed in either the deep learning or the optimal control literature.
",5. Discussion and Related Work,[0],[0]
"Sec. 4 concerns the application of the MSA, in particular Thm. 2, to develop training algorithms for binary and ternary neural networks.",5. Discussion and Related Work,[0],[0]
"There are a number of prior work exploring the training of similar neural networks, such as Courbariaux et al. (2015); Hubara et al. (2016); Rastegari et al.
(2016); Tang et al. (2017); Li et al. (2016); Zhu et al. (2016).",5. Discussion and Related Work,[0],[0]
Theoretical analysis for the case of convex loss functions is carried out in Li et al. (2017a).,5. Discussion and Related Work,[0],[0]
"Our point of numerical comparison for the binary MSA algorithm is Courbariaux et al. (2015), where optimization of binary networks is based on shadow variables with full floating-point precision that is iteratively truncated to obtain gradients.",5. Discussion and Related Work,[0],[0]
"We showed in Sec. 4.1 that the binary MSA is competitive as a training algorithm, but is in need of modifications to reduce overfitting for certain datasets.",5. Discussion and Related Work,[0],[0]
Training ternary networks has been discussed in Hwang & Fan (1967); Kim et al. (2014); Li et al. (2016); Zhu et al. (2016).,5. Discussion and Related Work,[0],[0]
The difference in our ternary formulation is that we explore the sparsification of networks using a regularization parameter.,5. Discussion and Related Work,[0],[0]
"In this sense it is related to compression of neural networks (e.g. Han et al. (2015)), but our approach trains a network that is naturally ternary, and compression is achieved during training by a regularization term.",5. Discussion and Related Work,[0],[0]
"Generally, a contrasting aspect of our approach from the aforementioned literature is that the theory of optimal control, together with Theorem. 2, provide a theoretical basis for the development of our algorithms.",5. Discussion and Related Work,[0],[0]
"Nevertheless, further work is required to rigorously establish the convergence of these algorithms.",5. Discussion and Related Work,[0],[0]
"We also mention a recent work (Yin et al., 2018) which analyzes quantized networks and develops algorithms based on relaxing the discrete-weight constraints into continuous regularizers.",5. Discussion and Related Work,[0],[0]
"Lastly, there are also analyses of quantized networks from a statistical-mechanical viewpoint (Baldassi et al., 2015; 2016a;b; 2017).",5. Discussion and Related Work,[0],[0]
"In this paper, we have introduced the discrete-time optimal control viewpoint of deep learning.",6. Conclusion and Outlook,[0],[0]
"In particular, the PMP and the MSA form an alternative theoretical and algorithmic basis for deep learning that may apply to broader contexts.",6. Conclusion and Outlook,[0],[0]
"As an application of our framework, we considered the training of binary and ternary neural networks, in which we develop effective algorithms based on optimal control.
",6. Conclusion and Outlook,[0],[0]
There are certainly many avenues of future work.,6. Conclusion and Outlook,[0],[0]
"An interesting mathematical question is the applicability of the PMP for discrete-weight neural networks, which does not satisfy the convexity assumptions in Theorem 1.",6. Conclusion and Outlook,[0],[0]
It will be desirable to find the condition under which rigorous statements can be made.,6. Conclusion and Outlook,[0],[0]
Another question is to establish the convergence of the algorithms presented.,6. Conclusion and Outlook,[0],[0]
Deep learning is formulated as a discrete-time optimal control problem.,abstractText,[0],[0]
This allows one to characterize necessary conditions for optimality and develop training algorithms that do not rely on gradients with respect to the trainable parameters.,abstractText,[0],[0]
"In particular, we introduce the discrete-time method of successive approximations (MSA), which is based on the Pontryagin’s maximum principle, for training neural networks.",abstractText,[0],[0]
"A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm.",abstractText,[0],[0]
"The developed methods are applied to train, in a rather principled way, neural networks with weights that are constrained to take values in a discrete set.",abstractText,[0],[0]
"We obtain competitive performance and interestingly, very sparse weights in the case of ternary networks, which may be useful in model deployment in low-memory devices.",abstractText,[0],[0]
An Optimal Control Approach to Deep Learning and  Applications to Discrete-Weight Neural Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 388–397 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1036",text,[0],[0]
Aspect extraction is one of the key tasks in sentiment analysis.,1 Introduction,[0],[0]
"It aims to extract entity aspects on which opinions have been expressed (Hu and Liu, 2004; Liu, 2012).",1 Introduction,[0],[0]
"For example, in the sentence “The beef was tender and melted in my mouth”, the aspect term is “beef”.",1 Introduction,[0],[0]
"Two sub-tasks are performed in aspect extraction: (1) extracting all aspect terms (e.g., “beef”) from a review corpus, (2) clustering aspect terms with similar meaning into categories where each category represents a single
aspect (e.g., cluster “beef”, “pork”, “pasta”, and “tomato” into one aspect food).
",1 Introduction,[0],[0]
"Previous works for aspect extraction can be categorized into three approaches: rule-based, supervised, and unsupervised.",1 Introduction,[0],[0]
Rule-based methods usually do not group extracted aspect terms into categories.,1 Introduction,[0],[0]
Supervised learning requires data annotation and suffers from domain adaptation problems.,1 Introduction,[0],[0]
"Unsupervised methods are adopted to avoid reliance on labeled data needed for supervised learning.
",1 Introduction,[0],[0]
"In recent years, Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Titov and McDonald, 2008; Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012) have become the dominant unsupervised approach for aspect extraction.",1 Introduction,[0],[0]
"LDA models the corpus as a mixture of topics (aspects), and topics as distributions over word types.",1 Introduction,[0],[0]
"While the mixture of aspects discovered by LDA-based models may describe a corpus fairly well, we find that the individual aspects inferred are of poor quality – aspects often consist of unrelated or loosely-related concepts.",1 Introduction,[0],[0]
This may substantially reduce users’ confidence in using such automated systems.,1 Introduction,[0],[0]
There could be two primary reasons for the poor quality.,1 Introduction,[0],[0]
"Conventional LDA models do not directly encode word co-occurrence statistics which are the primary source of information to preserve topic coherence (Mimno et al., 2011).",1 Introduction,[0],[0]
"They implicitly capture such patterns by modeling word generation from the document level, assuming that each word is generated independently.",1 Introduction,[0],[0]
"Furthermore, LDA-based models need to estimate a distribution of topics for each document.",1 Introduction,[0],[0]
"Review documents tend to be short, thus making the estimation of topic distributions more difficult.
",1 Introduction,[0],[0]
"In this work, we present a novel neural approach to tackle the weaknesses of LDA-based methods.",1 Introduction,[0],[0]
"We start with neural word embeddings that al-
388
ready map words that usually co-occur within the same context to nearby points in the embedding space (Mikolov et al., 2013).",1 Introduction,[0],[0]
"We then filter the word embeddings within a sentence using an attention mechanism (Bahdanau et al., 2015) and use the filtered words to construct aspect embeddings.",1 Introduction,[0],[0]
"The training process for aspect embeddings is analogous to autoencoders, where we use dimension reduction to extract the common factors among embedded sentences and reconstruct each sentence through a linear combination of aspect embeddings.",1 Introduction,[0],[0]
"The attention mechanism deemphasizes words that are not part of any aspect, allowing the model to focus on aspect words.",1 Introduction,[0],[0]
"We call our proposed model Attention-based Aspect Extraction (ABAE).
",1 Introduction,[0],[0]
"In contrast to LDA-based models, our proposed method explicitly encodes word-occurrence statistics into word embeddings, uses dimension reduction to extract the most important aspects in the review corpus, and uses an attention mechanism to remove irrelevant words to further improve coherence of the aspects.
",1 Introduction,[0],[0]
We have conducted extensive experiments on large review data sets.,1 Introduction,[0],[0]
The results show that ABAE is effective in discovering meaningful and coherent aspects.,1 Introduction,[0],[0]
It substantially outperforms baseline methods on multiple evaluation tasks.,1 Introduction,[0],[0]
"In addition, ABAE is intuitive and structurally simple.",1 Introduction,[0],[0]
It can also easily scale to a large amount of training data.,1 Introduction,[0],[0]
"Therefore, it is a promising alternative to LDA-based methods proposed previously.",1 Introduction,[0],[0]
The problem of aspect extraction has been well studied in the past decade.,2 Related Work,[0],[0]
"Initially, methods were mainly based on manually defined rules.",2 Related Work,[0],[0]
Hu and Liu (2004) proposed to extract different product features through finding frequent nouns and noun phrases.,2 Related Work,[0],[0]
They also extracted opinion terms by finding the synonyms and antonyms of opinion seed words through WordNet.,2 Related Work,[0],[0]
"Following this, a number of methods have been proposed based on frequent item mining and dependency information to extract product aspects (Zhuang et al., 2006; Somasundaran and Wiebe, 2009; Qiu et al., 2011).",2 Related Work,[0],[0]
"These models heavily depend on predefined rules which work well only when the aspect terms are restricted to a small group of nouns.
",2 Related Work,[0],[0]
"Supervised learning approaches generally model aspect extraction as a standard sequence
labeling problem.",2 Related Work,[0],[0]
"Jin and Ho (2009) and Li et al. (2010) proposed to use hidden Markov models (HMM) and conditional random fields (CRF), respectively with a set of manually-extracted features.",2 Related Work,[0],[0]
"More recently, different neural models (Yin et al., 2016; Wang et al., 2016) were proposed to automatically learn features for CRF-based aspect extraction.",2 Related Work,[0],[0]
Rule-based models are usually not refined enough to categorize the extracted aspect terms.,2 Related Work,[0],[0]
"On the other hand, supervised learning requires large amounts of labeled data for training purposes.
",2 Related Work,[0],[0]
"Unsupervised approaches, especially topic models, have been proposed subsequently to avoid reliance on labeled data.",2 Related Work,[0],[0]
"Generally, the outputs of those models are word distributions or rankings for each aspect.",2 Related Work,[0],[0]
Aspects are naturally obtained without separately performing extraction and categorization.,2 Related Work,[0],[0]
"Most existing works (Brody and Elhadad, 2010; Zhao et al., 2010; Mukherjee and Liu, 2012; Chen et al., 2014) are based on variants and extensions of LDA (Blei et al., 2003).",2 Related Work,[0],[0]
"Recently, Wang et al. (2015) proposed a restricted Boltzmann machine (RBM)-based model to simultaneously extract aspects and relevant sentiments of a given review sentence, treating aspects and sentiments as separate hidden variables in RBM.",2 Related Work,[0],[0]
"However, the RBM-based model proposed in (Wang et al., 2015) relies on a substantial amount of prior knowledge such as part-of-speech (POS) tagging and sentiment lexicons.",2 Related Work,[0],[0]
"A biterm topic model (BTM) that generates co-occurring word pairs was proposed in (Yan et al., 2013).",2 Related Work,[0],[0]
"We experimentally compare ABAE and BTM on multiple tasks in this paper.
",2 Related Work,[0],[0]
"Attention models (Mnih et al., 2014) have recently gained popularity in training neural networks and have been applied to various natural language processing tasks, including machine translation (Bahdanau et al., 2015; Luong et al., 2015), sentence summarization (Rush et al., 2015), sentiment classification (Chen et al., 2016; Tang et al., 2016), and question answering (Hermann et al., 2015).",2 Related Work,[0],[0]
"Rather than using all available information, attention mechanism aims to focus on the most pertinent information for a task.",2 Related Work,[0],[0]
"Unlike previous works, in this paper, we apply attention to an unsupervised neural model.",2 Related Work,[0],[0]
Our experimental results demonstrate its effectiveness under an unsupervised setting for aspect extraction.,2 Related Work,[0],[0]
We describe the Attention-based Aspect Extraction (ABAE) model in this section.,3 Model Description,[0],[0]
"The ultimate goal is to learn a set of aspect embeddings, where each aspect can be interpreted by looking at the nearest words (representative words) in the embedding space.",3 Model Description,[0],[0]
We begin by associating each word w in our vocabulary with a feature vector ew ∈ Rd.,3 Model Description,[0],[0]
"We use word embeddings for the feature vectors as word embeddings are designed to map words that often co-occur in a context to points that are close by in the embedding space (Mikolov et al., 2013).",3 Model Description,[0],[0]
"The feature vectors associated with the words correspond to the rows of a word embedding matrix E ∈ RV×d, where V is the vocabulary size.",3 Model Description,[0],[0]
"We want to learn embeddings of aspects, where aspects share the same embedding space with words.",3 Model Description,[0],[0]
"This requires an aspect embedding matrix T ∈ RK×d, where K, the number of aspects defined, is much smaller than V .",3 Model Description,[0],[0]
"The aspect embeddings are used to approximate the aspect words in the vocabulary, where the aspect words are filtered through an attention mechanism.
",3 Model Description,[0],[0]
Each input sample to ABAE is a list of indexes for words in a review sentence.,3 Model Description,[0],[0]
"Given such an input, two steps are performed as shown in Figure 1.",3 Model Description,[0],[0]
"First, we filter away non-aspect words by down-weighting them using an attention mechanism, and construct a sentence embedding zs from weighted word embeddings.",3 Model Description,[0],[0]
"Then, we try to reconstruct the sentence embedding as a linear combination of aspect embeddings from T. This process of dimension reduction and reconstruction, where ABAE aims to transform sentence embeddings of the filtered sentences (zs) into their reconstructions (rs) with the least possible amount of distortion, preserves most of the information of the aspect words in the K embedded aspects.",3 Model Description,[0],[0]
We next describe the process in detail.,3 Model Description,[0],[0]
We construct a vector representation zs for each input sentence s in the first step.,3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"In general, we want the vector representation to capture the most relevant information with regards to the aspect (topic) of the sentence.",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"We define the sentence embedding zs as the weighted summation of word embeddings ewi , i = 1, ..., n corresponding to the
word indexes in the sentence.
",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"zs =
n∑
i=1
aiewi .",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"(1)
For each word wi in the sentence, we compute a positive weight ai which can be interpreted as the probability that wi is the right word to focus on in order to capture the main topic of the sentence.",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"The weight ai is computed by an attention model, which is conditioned on the embedding of the word ewi as well as the global context of the sentence:
ai = exp(di)∑n j=1 exp(dj)
(2)
",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"di = e > wi ·M · ys (3)
ys = 1
n
n∑
i=1
ewi (4)
where ys is simply the average of the word embeddings, which we believe captures the global context of the sentence.",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
M ∈ Rd×d is a matrix mapping between the global context embedding ys and the word embedding ew and is learned as part of the training process.,3.1 Sentence Embedding with Attention Mechanism,[0],[0]
We can think of the attention mechanism as a two-step process.,3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"Given a sentence, we first construct its representation by averaging all the word representations.",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
Then the weight of a word is assigned by considering two things.,3.1 Sentence Embedding with Attention Mechanism,[0],[0]
"First, we filter the word through the transformation M which is able to capture the relevance of the word to the K aspects.",3.1 Sentence Embedding with Attention Mechanism,[0],[0]
Then we capture the relevance of the filtered word to the sentence by taking the inner product of the filtered word to the global context ys.,3.1 Sentence Embedding with Attention Mechanism,[0],[0]
We have obtained the sentence embedding.,3.2 Sentence Reconstruction with Aspect Embeddings,[0],[0]
Now we describe how to compute the reconstruction of the sentence embedding.,3.2 Sentence Reconstruction with Aspect Embeddings,[0],[0]
"As shown in Figure 1, the reconstruction process consists of two steps of transitions, which is similar to an autoencoder.",3.2 Sentence Reconstruction with Aspect Embeddings,[0],[0]
"Intuitively, we can think of the reconstruction as a linear combination of aspect embeddings from T:
rs = T > · pt (5)
where rs is the reconstructed vector representation, pt is the weight vector overK aspect embeddings, where each weight represents the probability that the input sentence belongs to the related aspect.",3.2 Sentence Reconstruction with Aspect Embeddings,[0],[0]
"pt can simply be obtained by reducing zs from d dimensions to K dimensions and then applying a softmax non-linearity that yields normalized non-negative weights:
pt = softmax (W · zs + b) (6)
where W, the weighted matrix parameter, and b, the bias vector, are learned as part of the training process.",3.2 Sentence Reconstruction with Aspect Embeddings,[0],[0]
ABAE is trained to minimize the reconstruction error.,3.3 Training Objective,[0],[0]
"We adopted the contrastive max-margin objective function used in previous work (Weston et al., 2011; Socher et al., 2014; Iyyer et al., 2016).",3.3 Training Objective,[0],[0]
"For each input sentence, we randomly sample m sentences from our training data as negative samples.",3.3 Training Objective,[0],[0]
We represent each negative sample as ni which is computed by averaging its word embeddings.,3.3 Training Objective,[0],[0]
Our objective is to make the reconstructed embedding rs similar to the target sentence embedding zs while different from those negative samples.,3.3 Training Objective,[0],[0]
"Therefore, the unregularized objective J is formulated as a hinge loss that maximize the inner product between rs and zs and simultaneously minimize the inner product between rs and the negative samples:
J(θ) = ∑
s∈D
m∑
i=1
max(0, 1− rszs + rsni) (7)
where D represents the training data set and θ = {E,T,M,W,b} represents the model parameters.",3.3 Training Objective,[0],[0]
We hope to learn vector representations of the most representative aspects for a review dataset.,3.4 Regularization Term,[0],[0]
"However, the aspect embedding matrix T may suffer from redundancy problems during training.",3.4 Regularization Term,[0],[0]
"To ensure the diversity of the resulting aspect embeddings, we add a regularization term to the objective function J to encourage the uniqueness of each aspect embedding:
U(θ) = ‖Tn ·T>n",3.4 Regularization Term,[0],[0]
"− I‖ (8)
where I is the identity matrix, and Tn is T with each row normalized to have length 1.",3.4 Regularization Term,[0],[0]
Any nondiagonal element tij(i 6= j) in the matrix Tn ·T>n corresponds to the dot product of two different aspect embeddings.,3.4 Regularization Term,[0],[0]
U reaches its minimum value when the dot product between any two different aspect embeddings is zero.,3.4 Regularization Term,[0],[0]
Thus the regularization term encourages orthogonality among the rows of the aspect embedding matrix T and penalizes redundancy between different aspect vectors.,3.4 Regularization Term,[0],[0]
"Our final objective function L is obtained by adding J and U :
L(θ) = J(θ) + λU(θ) (9)
where λ is a hyperparameter that controls the weight of the regularization term.",3.4 Regularization Term,[0],[0]
We evaluate our method on two real-word datasets.,4.1 Datasets,[0],[0]
"The detailed statistics of the datasets are summarized in Table 1.
(1) Citysearch corpus: This is a restaurant review corpus widely used by previous works (Ganu et al., 2009; Brody and Elhadad, 2010; Zhao et al., 2010), which contains over 50,000 restaurant reviews from Citysearch New York.",4.1 Datasets,[0],[0]
"Ganu et al. (2009) also provided a subset of 3,400 sentences from the corpus with manually labeled aspects.",4.1 Datasets,[0],[0]
These annotated sentences are used for evaluation of aspect identification.,4.1 Datasets,[0],[0]
"There are six manually defined aspect labels: Food, Staff, Ambience, Price, Anecdotes, and Miscellaneous.
(2) BeerAdvocate:",4.1 Datasets,[0],[0]
"This is a beer review corpus introduced in (McAuley et al., 2012), containing over 1.5 million reviews.",4.1 Datasets,[0],[0]
"A subset of 1,000 reviews, corresponding to 9,245 sentences, are annotated with five aspect labels: Feel, Look, Smell, Taste, and Overall.",4.1 Datasets,[0],[0]
"To validate the performance of ABAE, we compare it against a number of baselines:
(1) LocLDA (Brody and Elhadad, 2010):",4.2 Baseline Methods,[0],[0]
This method uses a standard implementation of LDA.,4.2 Baseline Methods,[0],[0]
"In order to prevent the inference of global topics and direct the model towards rateable aspects, each sentence is treated as a separate document.
",4.2 Baseline Methods,[0],[0]
(2) k-means: We initialize the aspect matrix T by using the k-means centroids of the word embeddings.,4.2 Baseline Methods,[0],[0]
"To show the power of ABAE, we compare its performance with using the kmeans centroids directly.
",4.2 Baseline Methods,[0],[0]
"(3) SAS (Mukherjee and Liu, 2012):",4.2 Baseline Methods,[0],[0]
This is a hybrid topic model that jointly discovers both aspects and aspect-specific opinions.,4.2 Baseline Methods,[0],[0]
"This model has been shown to be competitive among topic models in discovering meaningful aspects (Mukherjee and Liu, 2012; Wang et al., 2015).
",4.2 Baseline Methods,[0],[0]
"(4) BTM (Yan et al., 2013):",4.2 Baseline Methods,[0],[0]
This is a biterm topic model that is specially designed for short texts such as texts from social media and review sites.,4.2 Baseline Methods,[0],[0]
The major advantage of BTM over conventional LDA models is that it alleviates the problem of data sparsity in short documents by directly modeling the generation of unordered word-pair co-occurrences (biterms) over the corpus.,4.2 Baseline Methods,[0],[0]
It has been shown to perform better than conventional LDA models in discovering coherent topics.,4.2 Baseline Methods,[0],[0]
"Review corpora are preprocessed by removing punctuation symbols, stop words, and words appearing less than 10 times.",4.3 Experimental Settings,[0],[0]
"For LocLDA, we use the open-source implementation GibbsLDA++1 and for BTM, we use the implementation released by (Yan et al., 2013)2.",4.3 Experimental Settings,[0],[0]
"We tune the hyperparameters of all topic model baselines on a held-out set
1http://gibbslda.sourceforge.net 2http://code.google.com/p/btm/
with grid search using the topic coherence metric to be introduced later in Eq 10: for LocLDA, the Dirichlet priors α = 0.05 and β = 0.1; for SAS and BTM, α = 50/K and β = 0.1.",4.3 Experimental Settings,[0],[0]
"We run 1,000 iterations of Gibbs sampling for all topic models.
",4.3 Experimental Settings,[0],[0]
"For the ABAE model, we initialize the word embedding matrix E with word vectors trained by word2vec with negative sampling on each dataset, setting the embedding size to 200, window size to 10, and negative sample size to 5.",4.3 Experimental Settings,[0],[0]
The parameters we use for training word embeddings are standard with no specific tuning to our data.,4.3 Experimental Settings,[0],[0]
We also initialize the aspect embedding matrix T with the centroids of clusters resulting from running k-means on word embeddings.,4.3 Experimental Settings,[0],[0]
Other parameters are initialized randomly.,4.3 Experimental Settings,[0],[0]
"During the training process, we fix the word embedding matrix E and optimize other parameters using Adam (Kingma and Ba, 2014) with learning rate 0.001 for 15 epochs and batch size of 50.",4.3 Experimental Settings,[0],[0]
"We set the number of negative samples per input sample m to 20, and the orthogonality penalty weight λ to 1 by tuning the hyperparameters on a held-out set with grid search.",4.3 Experimental Settings,[0],[0]
"The results reported for all models are the average over 10 runs.
",4.3 Experimental Settings,[0],[0]
"Following (Brody and Elhadad, 2010; Zhao et al., 2010), we set the number of aspects for the restaurant corpus to 14.",4.3 Experimental Settings,[0],[0]
We experimented with different number of aspects from 10 to 20 for the beer corpus.,4.3 Experimental Settings,[0],[0]
"The results showed no major difference, so we also set it to 14.",4.3 Experimental Settings,[0],[0]
"As in previous work (Brody and Elhadad, 2010; Zhao et al., 2010), we manually mapped each inferred aspect to one of the gold-standard aspects according to its top ranked representative words.",4.3 Experimental Settings,[0],[0]
"In ABAE, representative words of an aspect can be found by looking at its nearest words in the embedding space using cosine as the similarity metric.",4.3 Experimental Settings,[0],[0]
We describe the evaluation tasks and report the experimental results in this section.,5 Evaluation and Results,[0],[0]
"We evaluate ABAE on two criteria:
• Is it able to find meaningful and semantically coherent aspects?
",5 Evaluation and Results,[0],[0]
• Is it able to improve aspect identification performance on real-world review datasets?,5 Evaluation and Results,[0],[0]
Table 2 presents all 14 aspects inferred by ABAE for the restaurant domain.,5.1 Aspect Quality Evaluation,[0],[0]
"Compared to gold-
standard labels, the inferred aspects are more finegrained.",5.1 Aspect Quality Evaluation,[0],[0]
"For example, it can distinguish main dishes from desserts, and drinks from food.",5.1 Aspect Quality Evaluation,[0],[0]
"In order to objectively measure the quality of aspects, we use coherence score as a metric which has been shown to correlate well with human judgment (Mimno et al., 2011).",5.1.1 Coherence Score,[0],[0]
"Given an aspect z and a set of top N words of z, Sz = {wz1, ..., wzN}, the coherence score is calculated as follows:
C(z;Sz) =",5.1.1 Coherence Score,[0],[0]
"N∑
n=2
n−1∑
l=1
log D2(w
z",5.1.1 Coherence Score,[0],[0]
"n,",5.1.1 Coherence Score,[0],[0]
w z,5.1.1 Coherence Score,[0],[0]
l ),5.1.1 Coherence Score,[0],[0]
"+ 1
D1(wzl )",5.1.1 Coherence Score,[0],[0]
"(10)
where D1(w) is the document frequency of word w and D2(w1, w2) is the co-document frequency of words w1 and w2.",5.1.1 Coherence Score,[0],[0]
"A higher coherence score indicates a better aspect interpretability, i.e., more meaningful and semantically coherent.
",5.1.1 Coherence Score,[0],[0]
"Figure 2 shows the average coherence score of each model which is computed as 1 K ∑K k=1C(zk;S
zk) on both the restaurant domain and beer domain.",5.1.1 Coherence Score,[0],[0]
"From the results, we make the following observations: (1) ABAE outperforms previous models for all ranked buckets.",5.1.1 Coherence Score,[0],[0]
(2) BTM performs slightly better than LocLDA and SAS.,5.1.1 Coherence Score,[0],[0]
"This may be because BTM directly models the generation of biterms, while conventional LDA just implicitly captures such patterns by modeling word generation from the document level.",5.1.1 Coherence Score,[0],[0]
"(3) It is interesting to note that performing k-means on the word embeddings is sufficient to perform better than all topic model baselines, including BTM.",5.1.1 Coherence Score,[0],[0]
"This indicates that neural word embedding is a better model for capturing co-occurrence than LDA, even for BTM which specifically models the generation of co-occurring word pairs.",5.1.1 Coherence Score,[0],[0]
"As we want to discover a set of aspects that the human user finds agreeable, it is also necessary
to carry out user evaluation directly.",5.1.2 User Evaluation,[0],[0]
"Following the experimental setting in (Chen et al., 2014), we recruited three human judges.",5.1.2 User Evaluation,[0],[0]
Each aspect is labeled as coherent if the majority of judges assess that most of its top 50 terms coherently represent a product aspect.,5.1.2 User Evaluation,[0],[0]
The numbers of coherent aspects discovered by each model are shown in Table 3.,5.1.2 User Evaluation,[0],[0]
"ABAE discovers the most number of coherent aspects compared with other models.
",5.1.2 User Evaluation,[0],[0]
"For a coherent aspect, each of its top terms is labeled as correct if and only if the majority of judges assess that it reflects the related aspect.",5.1.2 User Evaluation,[0],[0]
"We adopt precision@n (or p@n) to evaluate the results, which was also used in (Mukherjee and Liu, 2012; Chen et al., 2014).",5.1.2 User Evaluation,[0],[0]
Figure 3 shows the average p@n results over all coherent aspects for each domain.,5.1.2 User Evaluation,[0],[0]
"We can see that the user evaluation results correlate well with the coherence scores shown in Figure 2, where ABAE substantially outperforms all other models for all ranked buckets, especially for large values of n.",5.1.2 User Evaluation,[0],[0]
We evaluate the performance of sentence-level aspect identification on both domains using the annotated sentences shown in Table 1.,5.2 Aspect Identification,[0],[0]
"The evaluation criterion is to judge how well the predictions match the true labels, measured by precision, recall, and F1 scores.",5.2 Aspect Identification,[0],[0]
"The results4 are shown in Table 4 and Table 5.
",5.2 Aspect Identification,[0],[0]
"Given a review sentence, ABAE first assigns an inferred aspect label which corresponds to the highest weight in pt calculated as shown in Equation 6 .",5.2 Aspect Identification,[0],[0]
"And we then assign the gold-standard label to the sentence according to the mapping between inferred aspects and gold-standard labels.
",5.2 Aspect Identification,[0],[0]
"3k-means assigns a sentence an inferred aspect whose embedding is the closest to the averaged word embeddings of the sentence.
",5.2 Aspect Identification,[0],[0]
4Note that the values of P/R/F1 reported are the average over 10 runs (except some values taken from published results in Table 4).,5.2 Aspect Identification,[0],[0]
"Thus the F1 values cannot be computed directly from corresponding P/R values
For the restaurant domain, we follow the experimental settings of previous work (Brody and Elhadad, 2010; Zhao et al., 2010; Wang et al., 2015) to make our results comparable.",5.2 Aspect Identification,[0],[0]
"To do that, (1) we only used the single-label sentences for evaluation to avoid ambiguity (about 83% of labeled sentences have a single label), and (2) we only evaluated on three major aspects, namely Food, Staff, and Ambience.",5.2 Aspect Identification,[0],[0]
"The other aspects do not show clear patterns in either word usage or writing style, which makes these aspects very hard for even humans to identify.",5.2 Aspect Identification,[0],[0]
"Besides the baseline models, we also compare the results with other published models, including MaxEnt-LDA (ME-LDA) (Zhao et al., 2010) and SERBM (Wang et al., 2015).",5.2 Aspect Identification,[0],[0]
SERBM has reported state-of-the-art results for aspect identification on the restaurant corpus to date.,5.2 Aspect Identification,[0],[0]
"However, SERBM relies on a substantial amount of prior knowledge.
",5.2 Aspect Identification,[0],[0]
We make the following observations from Table 4: (1) ABAE outperforms all other models on F1 score for aspects Staff and Ambience.,5.2 Aspect Identification,[0],[0]
(2) The F1 score of ABAE for Food is worse than SERBM while its precision is very high.,5.2 Aspect Identification,[0],[0]
We analyzed the errors and found that most of the sentences we failed to recognize as Food are general descriptions without specific food words appearing.,5.2 Aspect Identification,[0],[0]
"For example, the true label for the sentence “The food is prepared quickly and efficiently.”",5.2 Aspect Identification,[0],[0]
is Food.,5.2 Aspect Identification,[0],[0]
ABAE assigns Staff to it as the highly focused words according to the attention mechanism are quickly and efficiently which are more related to Staff.,5.2 Aspect Identification,[0],[0]
"In fact, although this sentence contains the word food, we think it is a rather general description of service.",5.2 Aspect Identification,[0],[0]
(3) ABAE substantially outperforms k-means for this task although both methods perform well for extracting coherent aspects as shown in Figure 2 and Figure 3.,5.2 Aspect Identification,[0],[0]
"This shows the power brought by the attention mechanism, which is able to capture the main topic of a sentence by only focusing on aspect-related words.
",5.2 Aspect Identification,[0],[0]
"For the beer domain, in addition to the five goldstandard aspect labels, we also combined Taste and Smell to form a single aspect – Taste+Smell.",5.2 Aspect Identification,[0],[0]
"This is because these two aspects are very similar
and many words can be used to describe both aspects.",5.2 Aspect Identification,[0],[0]
"For example, the words spicy, bitter, fresh, sweet, etc. are top ranked representative words in both aspects, which makes it very hard even for humans to distinguish them.",5.2 Aspect Identification,[0],[0]
"Since Taste and Smell are highly correlated and difficult to separate in real life, a natural way to evaluate is to treat them as a single aspect.
",5.2 Aspect Identification,[0],[0]
"We can see from Table 5 that due to the issue described above, all models perform poorly on Taste and Smell.",5.2 Aspect Identification,[0],[0]
ABAE outperforms previous models in F1 scores on all aspects except for Taste.,5.2 Aspect Identification,[0],[0]
The results demonstrate the capability of ABAE in identifying separable aspects.,5.2 Aspect Identification,[0],[0]
Figure 4 shows the weights of words assigned by the attention model for some example sentences.,5.3 Validating the Effectiveness of Attention Model,[0],[0]
"As we can see, the weights learned by the model correspond very strongly with human intuition.",5.3 Validating the Effectiveness of Attention Model,[0],[0]
"In order to evaluate how attention model affects the overall performance of ABAE, we conduct experiments to compare ABAE and ABAE− on aspect identification, where ABAE− denotes the model in which the attention layer is switched off and sentence embedding is calculated by averaging its word embeddings: zs = 1n",5.3 Validating the Effectiveness of Attention Model,[0],[0]
∑n i=1,5.3 Validating the Effectiveness of Attention Model,[0],[0]
ewi .,5.3 Validating the Effectiveness of Attention Model,[0],[0]
The results on the restaurant domain are shown in Table 6.,5.3 Validating the Effectiveness of Attention Model,[0],[0]
"ABAE achieves substantially higher precision and recall on all aspects compared with
ABAE−, which demonstrates the effectiveness of the attention mechanism.",5.3 Validating the Effectiveness of Attention Model,[0],[0]
"We have presented ABAE, a simple yet effective neural attention model for aspect extraction.",6 Conclusion,[0],[0]
"In contrast to LDA models, ABAE explicitly captures word co-occurrence patterns and overcomes the problem of data sparsity present in review corpora.",6 Conclusion,[0],[0]
"Our experimental results demonstrated that ABAE not only learns substantially higher quality aspects, but also more effectively captures the aspects of reviews than previous methods.",6 Conclusion,[0],[0]
"To the best of our knowledge, we are the first to propose an unsupervised neural approach for aspect extraction.",6 Conclusion,[0],[0]
"ABAE is intuitive and structurally simple, and also scales up well.",6 Conclusion,[0],[0]
All these benefits make it a promising alternative to LDA-based methods in practice.,6 Conclusion,[0],[0]
This research is partially funded by the Economic Development Board and the National Research Foundation of Singapore.,Acknowledgements,[0],[0]
Aspect extraction is an important and challenging task in aspect-based sentiment analysis.,abstractText,[0],[0]
Existing works tend to apply variants of topic models on this task.,abstractText,[0],[0]
"While fairly successful, these methods usually do not produce highly coherent aspects.",abstractText,[0],[0]
"In this paper, we present a novel neural approach with the aim of discovering coherent aspects.",abstractText,[0],[0]
The model improves coherence by exploiting the distribution of word co-occurrences through the use of neural word embeddings.,abstractText,[0],[0]
"Unlike topic models which typically assume independently generated words, word embedding models encourage words that appear in similar contexts to be located close to each other in the embedding space.",abstractText,[0],[0]
"In addition, we use an attention mechanism to de-emphasize irrelevant words during training, further improving the coherence of aspects.",abstractText,[0],[0]
"Experimental results on real-life datasets demonstrate that our approach discovers more meaningful and coherent aspects, and substantially outperforms baseline methods on several evaluation tasks.",abstractText,[0],[0]
An Unsupervised Neural Attention Model for Aspect Extraction,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"For many low-resource languages, speech data is easier to obtain than textual data.",1 Introduction,[0],[0]
"And because speech transcription is a costly and slow process, speech is more likely to be annotated with translations than with transcriptions.",1 Introduction,[0],[0]
"This translated speech is a potentially valuable source of information – for example, for documenting endangered languages or for training speech translation systems.
",1 Introduction,[0],[0]
"In language documentation, data is usable only if it is interpretable.",1 Introduction,[0],[0]
"To make a collection of speech data usable for future studies of the language, something resembling interlinear glossed text (transcription, morphological analysis, word glosses, free translation) would be needed at minimum.",1 Introduction,[0],[0]
"New
technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016).",1 Introduction,[0],[0]
"As for the other annotation layers, one might hope that a first pass could be done automatically.",1 Introduction,[0],[0]
"A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured by word glosses.
",1 Introduction,[0],[0]
"In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training.",1 Introduction,[0],[0]
"Therefore, producing alignments between speech and text would be a natural first step towards MT systems operating directly on speech.
",1 Introduction,[0],[0]
"We present a model that, in order to learn such alignments, adapts and combines two components: Dyer et al.’s reparameterization of IBM Model 2 (Dyer et al., 2013), more commonly known as fast_align, and k-means clustering using Dynamic Time Warping (Berndt and Clifford, 1994) as a distance measure.",1 Introduction,[0],[0]
"The two components are trained jointly using expectation-maximization.
",1 Introduction,[0],[0]
We experiment on two language pairs.,1 Introduction,[0],[0]
"One is Spanish-English, using the CALLHOME and Fisher corpora.",1 Introduction,[0],[0]
"The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available)1 gold-standard translations and word alignments (Lekakou et al., 2013).",1 Introduction,[0],[0]
"In all cases, our model outperforms both a naive but strong baseline and a neural model (Duong et al., 2016).
1https://www3.nd.edu/∼aanastas/griko/griko-data.tar.gz
1255",1 Introduction,[0],[0]
"In this section, we briefly describe the existing models that the two components of our model are based on.",2 Background,[0],[0]
"In the next section, we will describe how we adapt and combine them to the present task.
2.1 IBM Model 2 and fast_align
The IBM translation models (Brown et al., 1993) aim to model the distribution p(e | f) for an English sentence e = e1 · · · el, given a French sentence f = f1 · · · em.",2 Background,[0],[0]
"They all introduce a hidden variable a = a1 · · · al that gives the position of the French word to which each English word is aligned.
",2 Background,[0],[0]
"The general form of IBM Models 1, 2 and fast_align is
p(e, a | f) = p(l) l∏
i=1
t(ei | fai) δ(ai |",2 Background,[0],[0]
"i, l,m)
where t(e | f ) is the probability of translating French word f to English word e, and δ(ai =",2 Background,[0],[0]
j,2 Background,[0],[0]
|,2 Background,[0],[0]
"i, l,m) is the probability of aligning the i-th English word with the j-th French word.
",2 Background,[0],[0]
"In Model 1, δ is uniform; in Model 2, it is a categorical distribution.",2 Background,[0],[0]
"Dyer et al. (2013) propose a reparameterization of Model 2, known as fast_align:
h(i, j, l,m) =",2 Background,[0],[0]
− ∣∣∣∣∣,2 Background,[0],[0]
i,2 Background,[0],[0]
"l − j m ∣∣∣∣∣
δ(ai |",2 Background,[0],[0]
"i, l,m) = 
p0 ai = 0",2 Background,[0],[0]
"(1 − p0) exp λh(i,ai,l,m)Zλ(i,l,m) ai > 0
where the null alignment probability p0 and precision λ ≥ 0 are hyperparameters optimized by grid search.",2 Background,[0],[0]
"As λ → 0, the distribution gets closer to the distribution of IBM Model 1, and as λ gets larger, the model prefers monotone word alignments more strongly.",2 Background,[0],[0]
"Dynamic Time Warping (DTW) (Berndt and Clifford, 1994) is a dynamic programming method for measuring distance between two temporal sequences of variable length, as well as computing an alignment based on this distance.",2.2 DTW and DBA,[0],[0]
"Given two sequences φ, φ′ of length m and m′ respectively, DTW
constructs an m×m′ matrix w.",2.2 DTW and DBA,[0],[0]
"The warping path can be found by evaluating the following recurrence:
wi, j = d(φi, φ′j) + min{wi−1, j,wi−1, j−1,wi, j−1}
where d is a distance measure.",2.2 DTW and DBA,[0],[0]
"In this paper, we normalize the cost of the warping path:
DTW(φ, φ′) = wm,m′
m + m′
which lies between zero and one.",2.2 DTW and DBA,[0],[0]
"DTW Barycenter Averaging (DBA) (Petitjean et al., 2011) is an iterative approximate method that attempts to find a centroid of a set of sequences, minimizing the sum of squared DTW distances.
",2.2 DTW and DBA,[0],[0]
"In the original definition, given a set of sequences, DBA chooses one sequence randomly to be a “skeleton.”",2.2 DTW and DBA,[0],[0]
"Then, at each iteration, DBA computes the DTW between the skeleton and every sequence in the set, aligning each of the skeleton’s points with points in all the sequences.",2.2 DTW and DBA,[0],[0]
"The skeleton is then refined using the found alignments, by updating each frame in the skeleton to the mean of all the frames aligned to it.",2.2 DTW and DBA,[0],[0]
"In our implementation, in order to avoid picking a skeleton that is too short or too long, we randomly choose one of the sequences with median length.",2.2 DTW and DBA,[0],[0]
We use a generative model from a source-language speech segment consisting of feature frames φ = φ1 · · · φm to a target-language segment consisting of words e = e1 . . .,3 Model,[0],[0]
el.,3 Model,[0],[0]
We chose to model p(e | φ) rather than p(φ | e) because it makes it easier to incorporate DTW.,3 Model,[0],[0]
"The other direction is also possible, and we plan to explore it in future work.
",3 Model,[0],[0]
"In addition to the target-language sentence e, our model hypothesizes a sequence f = f1 · · · fl of source-language clusters (intuitively, sourcelanguage words), and spans (ai, bi) of the source signal that each target word ei is aligned to.",3 Model,[0],[0]
"Thus, the clusters f = f1 · · · fl and the spans a = a1, . . .",3 Model,[0],[0]
", al and b = b1, . . .",3 Model,[0],[0]
", bl are the hidden variables of the model:
p(e | φ) =",3 Model,[0],[0]
"∑
a,b,f p(e, a,b, f | φ).
",3 Model,[0],[0]
"The model generates e, a,b, and f from φ as follows.
1.",3 Model,[0],[0]
"Choose l, the number of target words, with uniform probability.",3 Model,[0],[0]
"(Technically, this assumes a maximum target sentence length, which we can just set to be very high.)
",3 Model,[0],[0]
2.,3 Model,[0],[0]
"For each target word position i = 1, . . .",3 Model,[0],[0]
", l:
(a) Choose a cluster fi.",3 Model,[0],[0]
"(b) Choose a span of source frames (ai, bi) for
ei to be aligned to.",3 Model,[0],[0]
"(c) Generate a target word ei from fi.
Accordingly, we decompose p(e, a,b, f | φ) into several submodels:
p(e, a,b, f | φ) = p(l) l∏
i=1
u( fi) ×
s(ai, bi | fi,φ) × δ(ai, bi | i, l, |φ|) ×",3 Model,[0],[0]
"t(ei | fi).
",3 Model,[0],[0]
"Note that submodels δ and s both generate spans (corresponding to step 2b), making the model deficient.",3 Model,[0],[0]
"We could make the model sum to one by replacing u( fi)s(ai, bi | fi,φ) with s( fi | ai, bi,φ), and this was in fact our original idea, but the model as defined above works much better, as discussed in Section 7.4.",3 Model,[0],[0]
"We describe both δ and s in detail below.
",3 Model,[0],[0]
Clustering model,3 Model,[0],[0]
"The probability over clusters, u( f ), is just a categorical distribution.",3 Model,[0],[0]
"The submodel s assumes that, for each cluster f , there is a “prototype” signal φ",3 Model,[0],[0]
f,3 Model,[0],[0]
(cf.,3 Model,[0],[0]
"Ristad and Yianilos, 1998).",3 Model,[0],[0]
"Technically, the φ f are parameters of the model, and will be recomputed during the M step.",3 Model,[0],[0]
"Then we can define:
s(a, b | f ,φ) = exp(−DTW(φ f , φa · · · φb)2)∑m
a,b=1 exp(−DTW(φ f , φa · · · φb)2)
where DTW is the distance between the prototype and the segment computed using Dynamic Time Warping.",3 Model,[0],[0]
"Thus s assigns highest probability to spans of φ that are most similar to the prototype φ f .
Distortion model The submodel δ controls the reordering of the target words relative to the source frames.",3 Model,[0],[0]
"It is an adaptation of fast_align to our
setting, where there is not a single source word position ai, but a span (ai, bi).",3 Model,[0],[0]
"We want the model to prefer the middle of the word to be close to the diagonal, so we need the variable a to be somewhat to the left and b to be somewhat to the right.",3 Model,[0],[0]
"Therefore, we introduce an additional hyperparameter µ which is intuitively the number of frames in a word.",3 Model,[0],[0]
"Then we define
ha(i, j, l,m, µ) =",3 Model,[0],[0]
− ∣∣∣∣∣,3 Model,[0],[0]
i,3 Model,[0],[0]
l − j m − µ ∣∣∣∣∣,3 Model,[0],[0]
"hb(i, j, l,m, µ) =",3 Model,[0],[0]
− ∣∣∣∣∣,3 Model,[0],[0]
i,3 Model,[0],[0]
l,3 Model,[0],[0]
"− j − µ m − µ ∣∣∣∣∣
δa(ai | i, l,m) = 
p0 ai = 0 (1 − p0) exp λha(i,ai,l,m)Zλ(i,l,m) ai > 0
δb(bi",3 Model,[0],[0]
|,3 Model,[0],[0]
"i, l,m) = 
p0 bi = 0 (1 − p0) exp λhb(i,bi,l,m)Zλ(i,l,m) bi > 0
δ(ai, bi | i, l,m) = δa(ai | i, l,m) δb(bi",3 Model,[0],[0]
"| i, l,m)
where the Zλ(i, l,m) are set so that all distributions sum to one.",3 Model,[0],[0]
"Figure 1 shows an example visualisation of the the resulting distributions for the two variables of our model.
",3 Model,[0],[0]
We set µ differently for each word.,3 Model,[0],[0]
"For each i, we set µi to be proportional to the number of characters in ei, such that ∑ i µi = m.
Translation model The translation model t(e | f ) is just a categorical distribution, in principle allowing a many-to-many relation between source clusters and target words.",3 Model,[0],[0]
"To speed up training (with nearly no change in accuracy, in our experiments), we restrict this relation so that there are k source clusters for each target word, and a source cluster uniquely determines its target word.",3 Model,[0],[0]
"Thus, t(e | f ) is fixed to
either zero or one, and does not need to be reestimated.",3 Model,[0],[0]
"In our experiments, we set k = 2, allowing each target word to have up to two source-language translations/pronunciations.",3 Model,[0],[0]
"(If a source word has more than one target translation, they are treated as distinct clusters with distinct prototypes.)",3 Model,[0],[0]
"We use the hard (Viterbi) version of the ExpectationMaximization (EM) algorithm to estimate the parameters of our model, because calculating expected counts in full EM would be prohibitively expensive, requiring summations over all possible alignments.
",4 Training,[0],[0]
"Recall that the hidden variables of the model are the alignments (ai, bi) and the source words ( fi).",4 Training,[0],[0]
The parameters are the translation probabilities t(ei | f ) and the prototypes (φ f ).,4 Training,[0],[0]
"The (hard) E step uses the current model and prototypes to find, for each target word, the best source segment to align it to and the best source word.",4 Training,[0],[0]
The M step reestimates the probabilities t(e | f ) and the prototypes φ f .,4 Training,[0],[0]
"We describe each of these steps in more detail below.
",4 Training,[0],[0]
"Initialization Initialization is especially important since we are using hard EM.
",4 Training,[0],[0]
"To initialize the parameters, we initialize the hidden variables and then perform an M step.",4 Training,[0],[0]
"We associate each target word type e with k = 2 source clusters, and for each occurrence of e, we randomly assign it one of the k source clusters.
",4 Training,[0],[0]
"The alignment variables ai, bi are initialized to
ai, bi = arg max a,b
δ(a, b",4 Training,[0],[0]
"| i, l,m).
",4 Training,[0],[0]
"M step The M step reestimates the probabilities t(e | f ) using relative-frequency estimation.
",4 Training,[0],[0]
The prototypes φ f are more complicated.,4 Training,[0],[0]
"Theoretically, the M step should recompute each φ f so as to maximize that part of the log-likelihood that depends on φ f :",4 Training,[0],[0]
"Lφ f = ∑
φ
∑
i| fi= f log s(ai, bi | f ,φ)
= ∑
φ
∑
i| fi= f log
exp(−DTW(φ f , φai · · · φbi)2) Z( f ,φ)
= ∑
φ
∑
i| fi= f −DTW(φ f , φai · · ·",4 Training,[0],[0]
"φbi)2 − log Z( f ,φ)
where the summation over φ is over all source signals in the training data.",4 Training,[0],[0]
"This is a hard problem, but note that the first term is just the sum-of-squares of the DTW distance between φ f and all source segments that are classified as f .",4 Training,[0],[0]
"This is what DBA is supposed to approximately minimize, so we simply set φ f using DBA, ignoring the denominator.
",4 Training,[0],[0]
"E step The (hard) E step uses the current model and prototypes to find, for each target word, the best source segment to align it to and the best source cluster.
",4 Training,[0],[0]
"In order to reduce the search space for a and b, we use the unsupervised phonetic boundary detection method of Khanagha et al. (2014).",4 Training,[0],[0]
"This method operates directly on the speech signal and provides us with candidate phone boundaries, on which we restrict the possible values for a and b, creating a list of candidate utterance spans.
",4 Training,[0],[0]
"Furthermore, we use a simple silence detection method.",4 Training,[0],[0]
"We pass the envelope of the signal through a low-pass filter, and then mark as “silence” time spans of 50ms or longer in which the magnitude is below a threshold of 5% relative to the maximum of the whole signal.",4 Training,[0],[0]
"This method is able to detect about 80% of the total pauses, with a 90% precision in a 50ms window around the correct silence boundary.",4 Training,[0],[0]
"We can then remove from the candidate list the utterance spans that include silence, on the assumption that a word should not include silences.",4 Training,[0],[0]
"Finally, in case one of the span’s boundaries happens to be within a silence span, we also move it so as to not include the silence.
",4 Training,[0],[0]
"Hyperparameter tuning The hyperparameters p0, λ, and µ are not learned.",4 Training,[0],[0]
"We simply set p0 to zero (disallowing unaligned target words) and set µ as described above.
",4 Training,[0],[0]
For λwe perform a grid search over candidate values to maximize the alignment F-score on the development set.,4 Training,[0],[0]
We obtain the best scores with λ = 0.5.,4 Training,[0],[0]
"A first step towards modelling parallel speech can be performed by modelling phone-to-word alignment, instead of directly working on continuous speech.",5 Related Work,[0],[0]
"For example, Stahlberg et al. (2012) extend IBM Model 3 to align phones to words in order to build
cross-lingual pronunciation lexicons.",5 Related Work,[0],[0]
"Pialign (Neubig et al., 2012) aligns characters and can be applied equally well to phones.",5 Related Work,[0],[0]
"Duong et al. (2016) use an extension of the neural attentional model of Bahdanau et al. (2015) for aligning phones to words and speech to words; we discuss this model below in Section 6.2.
",5 Related Work,[0],[0]
There exist several supervised approaches that attempt to integrate speech recognition and machine translation.,5 Related Work,[0],[0]
"However, they rely heavily on the abundance of training data, pronunciation lexicons, or language models, and therefore cannot be applied in a low- or zero-resource setting.
",5 Related Work,[0],[0]
"A task somewhat similar to ours, which operates at a monolingual level, is the task of zero-resource spoken term discovery, which aims to discover repeated words or phrases in continuous speech.",5 Related Work,[0],[0]
"Various approaches (Ten Bosch and Cranen, 2007; Park and Glass, 2008; Muscariello et al., 2009; Zhang and Glass, 2010; Jansen et al., 2010) have been tried, in order to spot keywords, using segmental DTW to identify repeated trajectories in the speech signal.
",5 Related Work,[0],[0]
"Kamper et al. (2016) try to discover word segmentation and a pronunciation lexicon in a zero-resource setting, combining DTW with acoustic embeddings; their methods operate in a very low-vocabulary setting.",5 Related Work,[0],[0]
"Bansal (2015) attempts to build a speech translation system in a low-resource setting, by using as source input the simulated output of an unsupervised term discovery system.",5 Related Work,[0],[0]
"We evaluate our method on two language pairs, Spanish-English and Griko-Italian, against two baseline methods, a naive baseline, and the model of Duong et al. (2016).",6 Experiments,[0],[0]
"For each language pair, we require a sentencealigned parallel corpus of source-language speech and target-language text.",6.1 Data,[0],[0]
A subset of these sentences should be annotated with span-to-word alignments for use as a gold standard.,6.1 Data,[0],[0]
"For Spanish-English, we use the Spanish CALLHOME corpus (LDC96S35) and the Fisher corpus
(LDC2010T04), which consist of telephone conversations between Spanish native speakers based in the US and their relatives abroad, together with English translations produced by Post et al. (2013).",6.1.1 Spanish-English,[0],[0]
"Spanish is obviously not a low-resource language, but we pretend that it is low-resource by not making use of any Spanish ASR or resources like transcribed speech or pronunciation lexicons.
",6.1.1 Spanish-English,[0],[0]
"Since there do not exist gold standard alignments between the Spanish speech and English words, we use the “silver” standard alignments produced by Duong et al. (2016) for the CALLHOME corpus, and followed the same procedure for the Fisher corpus as well.",6.1.1 Spanish-English,[0],[0]
"In order to obtain them, they first used a forced aligner to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic to align the Spanish transcription to the English translation.",6.1.1 Spanish-English,[0],[0]
"They then combined the two alignments to produce “silver” standard alignments between the Spanish speech and the English words.
",6.1.1 Spanish-English,[0],[0]
"The CALLHOME dataset consists of 17532 Spanish utterances, based on the dialogue turns.",6.1.1 Spanish-English,[0],[0]
"We first use a sample of 2000 sentences, out of which we use 200 as a development set and the rest as a test set.",6.1.1 Spanish-English,[0],[0]
"We also run our experiments on the whole dataset, selecting 500 utterances for a development set, using the rest as a test set.",6.1.1 Spanish-English,[0],[0]
The Fisher dataset consists of 143355 Spanish utterances.,6.1.1 Spanish-English,[0],[0]
We use 1000 of them as a development set and the rest as a test set.,6.1.1 Spanish-English,[0],[0]
"We also run our model on a corpus that consists of about 20 minutes of speech in Griko, an endangered minority dialect of Greek spoken in south Italy, along with text translations into Italian (Lekakou et al., 2013).2",6.1.2 Griko-Italian,[0],[0]
The corpus consists of 330 mostly prompted utterances by nine native speakers.,6.1.2 Griko-Italian,[0],[0]
"Although the corpus is very small, we use it to showcase the effectiveness of our method in a hard setting with extremely low resources.
",6.1.2 Griko-Italian,[0],[0]
"All utterances were manually annotated and transcribed by a trained linguist and bilingual speaker of both languages, who produced the Griko transcriptions and Italian glosses.",6.1.2 Griko-Italian,[0],[0]
We created full translations into Italian and manually aligned the translations with the Griko transcriptions.,6.1.2 Griko-Italian,[0],[0]
"We then com-
2http://griko.project.uoi.gr
bined the two alignments (speech-to-transcription and transcription-to-translation) to produce speechto-translation alignments.",6.1.2 Griko-Italian,[0],[0]
"Therefore, our comparison is done against an accurate “gold” standard alignment.",6.1.2 Griko-Italian,[0],[0]
"We split the data into a development set of just 30 instances, and a test set of the remaining 300 instances.",6.1.2 Griko-Italian,[0],[0]
"In both data settings, we treat the speech data as a sequence of 39-dimensional Perceptual Linear Prediction (PLP) vectors encoding the power spectrum of the speech signal (Hermansky, 1990), computed at 10ms intervals.",6.1.3 Preprocessing,[0],[0]
"We also normalize the features at the utterance level, shifting and scaling them to have zero mean and unit variance.",6.1.3 Preprocessing,[0],[0]
"Our naive baseline assumes that there is no reordering between the source and target language, and aligns each target word ei to a source span whose length in frames is proportional to the length of ei in characters.",6.2 Baselines,[0],[0]
"This actually performs very well on language pairs that show minimal or no reordering, and language pairs that have shared or related vocabularies.
",6.2 Baselines,[0],[0]
"The other baseline that we compare against is the neural network attentional model of Duong et al. (2016), which extends the attentional model of Bahdanau et al. (2015) to be used for aligning and translating speech, and, along with several modifications, achieve good results on the phone-to-word alignment task, and almost match the baseline performance on the speech-to-word alignment task.",6.2 Baselines,[0],[0]
"To evaluate an automatic alignment between the speech and its translation against the gold/silver standard alignment, we compute alignment precision, recall, and F-score as usual, but on links between source-language frames and target-language words.",7 Results,[0],[0]
"Table 1 shows the precision, recall, and balanced Fscore of the three models on the Spanish-English CALLHOME corpus (both the 2000-sentence subset
and the full set), the Spanish-English Fisher corpus, and the Griko-Italian corpus.
",7.1 Overview,[0],[0]
"In all cases, our model outperforms both the naive baseline and the neural attentional model.",7.1 Overview,[0],[0]
"Our model, when compared to the baselines, improves greatly on precision, while slightly underperforming the naive baseline on recall.",7.1 Overview,[0],[0]
"In certain applications, higher precision may be desirable: for example, in language documentation, it’s probably better to err on the side of precision; in phrase-based translation, higher-precision alignments lead to more extracted phrases.
",7.1 Overview,[0],[0]
"The rest of the section provides a further analysis of the results, focusing on the extremely lowresource Griko-Italian dataset.",7.1 Overview,[0],[0]
Figure 2 shows the alignments produced by our model for three utterances of the same sentence from the Griko-Italian dataset by three different speakers.,7.2 Speaker robustness,[0],[0]
Our model’s performance is roughly consistent across these utterances.,7.2 Speaker robustness,[0],[0]
"In general, the model does not seem significantly affected by speaker-specific variations, as shown in Table 2.
",7.2 Speaker robustness,[0],[0]
"We do find, however, that the performance on male speakers is slightly higher compared to the female speakers.",7.2 Speaker robustness,[0],[0]
"This might be because the female speakers’ utterances are, on average, longer by about 2 words than the ones uttered by males.",7.2 Speaker robustness,[0],[0]
We also compute F-scores for each Italian word type.,7.3 Word level analysis,[0],[0]
"As shown in Figure 3, the longer the word’s utterance, the easier it is for our model to correctly align it.",7.3 Word level analysis,[0],[0]
Longer utterances seem to carry enough information for our DTW-based measure to function properly.,7.3 Word level analysis,[0],[0]
"On the other hand, shorter utterances are harder to align.",7.3 Word level analysis,[0],[0]
"The vast majority of Griko utterances that have less than 20 frames and are less accurately aligned correspond to monosyllabic determiners (o, i,a, to, ta) or conjunctions and prepositions (ka, ce, en, na, an).",7.3 Word level analysis,[0],[0]
"For such short utterances, there could be several parts of the signal that possibly match the prototype, leading the clustering component to prefer to align to wrong spans.
",7.3 Word level analysis,[0],[0]
"Furthermore, we note that rare word types tend to be correctly aligned.",7.3 Word level analysis,[0],[0]
"The average F-score for hapax legomena (on the Italian side) is 63.2, with 53% of them being aligned with an F-score higher than 70.0.",7.3 Word level analysis,[0],[0]
"As mentioned in Section 3, our model is deficient, but it performs much better than the model that sums to one (henceforth, the “proper” model):",7.4 Comparison with proper model,[0],[0]
"In the Spanish-English dataset (2000 sentences sample) the proper model yields an F-score of 32.1, performing worse than the naive baseline; in the Griko-
Italian dataset, it achieves an F-score of 44.3, which is better than the baselines, but still worse than our model.
",7.4 Comparison with proper model,[0],[0]
"In order to further examine why this happens, we performed three EM iterations on the Griko-Italian dataset with our model (in our experience, three iterations are usually enough for convergence), and then computed one more E step with both our model and the proper model, so as to ensure that the two models would align the dataset using the exact same prototypes and that their outputs will be comparable.
",7.4 Comparison with proper model,[0],[0]
"In this case, the proper model achieved an overall F-score of 44.0, whereas our model achieved an F-score of 53.6.",7.4 Comparison with proper model,[0],[0]
Figures 4 and 5 show the resulting alignments for two sentences.,7.4 Comparison with proper model,[0],[0]
"In both of these examples, it is clear that the proper model prefers extreme spans: the selected spans are either much too short or
(less frequently) much too long.",7.4 Comparison with proper model,[0],[0]
This is further verified by examining the statistics of the alignments: the average span selected by the proper model has a length of about 30 ± 39 frames whereas the average span of the alignments produced by our deficient model is 37 ± 24 frames.,7.4 Comparison with proper model,[0],[0]
"This means that the alignments of the deficient model are much closer to the gold ones, whose average span is 42 ± 26 frames.
",7.4 Comparison with proper model,[0],[0]
We think that this is analogous to the “garbage collection” problem in word alignment.,7.4 Comparison with proper model,[0],[0]
"In the IBM word alignment models, if a source word f occurs in only one sentence, then EM can align many target words to f and learn a very peaked distribution t(e | f ).",7.4 Comparison with proper model,[0],[0]
"This can happen in our model and the proper model as well, of course, since IBM Model 2 is embedded in them.",7.4 Comparison with proper model,[0],[0]
"But in the proper model, something similar can also happen with s( f | a, b): EM can make the span (a, b) large or small, and evidently making the span small allows it to learn a very peaked distribution s( f | a, b).",7.4 Comparison with proper model,[0],[0]
"By contrast, our model has s(a, b | f ), which seems less susceptible to this kind of effect.",7.4 Comparison with proper model,[0],[0]
"Alignment of speech to text translations is a relatively new task, one with particular relevance for low-resource or endangered languages.",8 Conclusion,[0],[0]
"The model we propose here, which combines fast_align and k-means clustering using DTW and DBA, outperforms both a very strong naive baseline and a neural attentional model, on three tasks of various sizes.
",8 Conclusion,[0],[0]
"The language pairs used here do not have very much word reordering, and more divergent language
pairs should prove more challenging.",8 Conclusion,[0],[0]
"In that case, the naive baseline should be much less competitive.",8 Conclusion,[0],[0]
"Similarly, the fast_align-based distortion model may become less appopriate; we plan to try incorporating IBM Model 3 or the HMM alignment model (Vogel et al., 1996) instead.",8 Conclusion,[0],[0]
"Finally, we will investigate downstream applications of our alignment methods, in the areas of both language documentation and speech translation.",8 Conclusion,[0],[0]
"We would like to thank Steven Bird, Eamonn Keogh, and the anonymous reviewers for their helpful feedback.",Acknowledgements,[0],[0]
This research was supported in part by NSF Award 1464553.,Acknowledgements,[0],[0]
"For many low-resource languages, spoken language resources are more likely to be annotated with translations than with transcriptions.",abstractText,[0],[0]
Translated speech data is potentially valuable for documenting endangered languages or for training speech translation systems.,abstractText,[0],[0]
A first step towards making use of such data would be to automatically align spoken words with their translations.,abstractText,[0],[0]
We present a model that combines Dyer et al.’s reparameterization of IBM Model 2 (fast_align) and k-means clustering using Dynamic Time Warping as a distance measure.,abstractText,[0],[0]
The two components are trained jointly using expectationmaximization.,abstractText,[0],[0]
"In an extremely low-resource scenario, our model performs significantly better than both a neural model and a strong baseline.",abstractText,[0],[0]
An Unsupervised Probability Model for Speech-to-Translation Alignment of Low-Resource Languages,title,[0],[0]
"Multi-relational embedding, or knowledge graph embedding, is the task of finding the latent representations of entities and relations for better inference over knowledge graphs.",1. Introduction,[0],[0]
"This problem has become increasingly important in recent machine learning due to the broad range of important applications of large-scale knowledge bases, such as Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007) and Google’s Knowledge Graph (Singhal, 2012), including question-answering (Ferrucci et al., 2010), information retrieval (Dalton et al., 2014) and natural language processing (Gabrilovich & Markovitch, 2009).
",1. Introduction,[0],[0]
A knowledge base (KB) typically stores factual information as subject-relation-object triplets.,1. Introduction,[0],[0]
The collection of such triplets forms a directed graph whose nodes are entities and whose edges are the relations among entities.,1. Introduction,[0],[0]
"Real-
1Carnegie Mellon University, Pittsburgh, PA 15213, USA.",1. Introduction,[0],[0]
"Correspondence to: Hanxiao Liu <hanxiaol@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"world knowledge graph is both extremely large and highly incomplete by nature (Min et al., 2013).",1. Introduction,[0],[0]
"How can we use the observed triplets in an incomplete graph to induce the unobserved triples in the graph presents a tough challenge for machine learning research.
",1. Introduction,[0],[0]
"Various statistical relational learning methods (Getoor, 2007; Nickel et al., 2015) have been proposed for this task, among which vector-space embedding models are most particular due to their advantageous performance and scalability (Bordes et al., 2013).",1. Introduction,[0],[0]
"The key idea in those approaches is to find dimensionality reduced representations for both the entities and the relations, and hence force the models to generalize during the course of compression.",1. Introduction,[0],[0]
"Representative models of this kind include tensor factorization (Singhal, 2012; Nickel et al., 2011), neural tensor networks (Socher et al., 2013; Chen et al., 2013), translationbased models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015b), bilinear models and its variants (Yang et al., 2014; Trouillon et al., 2016), pathwise methods (Guu et al., 2015), embeddings based on holographic representations (Nickel et al., 2016), and product graphs that utilizes additional site information for the predictions of unseen edges in a semi-supervised manner (Liu & Yang, 2015; 2016).
",1. Introduction,[0],[0]
"Learning the embeddings of entities and relations can be viewed as a knowledge induction process, as those induced latent representations can be used to make inference about new triplets that have not been seen before.
",1. Introduction,[0],[0]
"Despite the substantial efforts and great successes so far in the research on multi-relational embedding, one important aspect is missing, i.e., to study the solutions of the problem from the analogical inference point of view, by which we mean to rigorously define the desirable analogical properties for multi-relational embedding of entities and relations, and to provide algorithmic solution for optimizing the embeddings w.r.t.",1. Introduction,[0],[0]
the analogical properties.,1. Introduction,[0],[0]
"We argue that analogical inference is particularly desirable for knowledge base completion, since for instance if system A (a subset of entities and relations) is analogous to system B (another subset of entities and relations), then the unobserved triples inB could be inferred by mirroring their counterparts",1. Introduction,[0],[0]
"inA. Figure 1 uses a toy example to illustrate the intuition, where system A corresponds to the solar system with three concepts (entities), and systemB corresponds the atom system with another three concepts.",1. Introduction,[0],[0]
"An analogy exists between the two systems because B is a “miniature” of A. As a result, knowing how the entities are related to each other in system A allows us to make inference about how the entities are related to each other in system B by analogy.
",1. Introduction,[0],[0]
"Although analogical reasoning was an active research topic in classic AI (artificial intelligence), early computational models mainly focused on non-differentiable rulebased reasoning (Gentner, 1983; Falkenhainer et al., 1989; Turney, 2008), which can hardly scale to very large KBs such as Freebase or Google’s Knowledge Graph.",1. Introduction,[0],[0]
"How to leverage the intuition of analogical reasoning via statistical inference for automated embedding of very large knowledge graphs has not been studied so far, to our knowledge.
",1. Introduction,[0],[0]
"It is worth mentioning that analogical structures have been observed in the output of several word/entity embedding models (Mikolov et al., 2013; Pennington et al., 2014).",1. Introduction,[0],[0]
"However, those observations stopped there as merely empirical observations.",1. Introduction,[0],[0]
Can we mathematically formulate the desirable analogical structures and leverage them in our objective functions to improve multi-relational embedding?,1. Introduction,[0],[0]
"In this case, can we develop new algorithms for tractable inference for the embedding of very large knowledge graphs?",1. Introduction,[0],[0]
"These questions present a fundamental challenge which has not been addressed by existing work, and answering these questions are the main contributions we aim in this paper.",1. Introduction,[0],[0]
"We name this open challenge as the analogical inference problem, for the distinction from rule-based analogical reasoning in classic AI.
",1. Introduction,[0],[0]
"Our specific novel contributions are the following:
1.",1. Introduction,[0],[0]
"A new framework that, for the first time, explicitly
models analogical structures in multi-relational embedding, and that improves the state-of-the-art performance on benchmark datasets;
2.",1. Introduction,[0],[0]
"The algorithmic solution for conducting analogical inference in a differentiable manner, whose implementation is as scalable as the fastest known relational embedding algorithms;
3.",1. Introduction,[0],[0]
"The theoretical insights on how our framework provides a unified view of several representative methods as its special (and restricted) cases, and why the generalization of such cases lead to the advantageous performance of our method as empirically observed.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows: §2 introduces related background where multi-relational embedding is formulated as linear maps.,1. Introduction,[0],[0]
§3 describes our new optimization framework where the desirable analogical structures are rigorously defined by the the commutative property of linear maps.,1. Introduction,[0],[0]
"§4 offers an efficient algorithm for scalable inference by exploiting the special structures of commutative linear maps, §5 shows how our framework subsumes several representative approaches in a principled way, and §6 reports our experimental results, followed by the concluding remarks in §7.",1. Introduction,[0],[0]
Let E and R be the space of all entities and their relations.,2.1. Notations,[0],[0]
"A knowledge base K is a collection of triplets (s, r, o) ∈ K",2.1. Notations,[0],[0]
"where s ∈ E , o ∈",2.1. Notations,[0],[0]
"E , r ∈",2.1. Notations,[0],[0]
"R stand for the subject, object and their relation, respectively.",2.1. Notations,[0],[0]
"Denote by v ∈ R|E|×m a look-up table where ve ∈ Rm is the vector embedding for entity e, and denote by tensor W ∈ R|R|×m×m another look-up table where Wr ∈ Rm×m is the matrix embedding for relation r. Both v",2.1. Notations,[0],[0]
and W are to be learned from K.,2.1. Notations,[0],[0]
"We formulate each relation r as a linear map that, for any given (s, r, o) ∈ K, transforms the subject s from its original position in the vector space to somewhere near the object o.",2.2. Relations as Linear Maps,[0],[0]
"In other words, we expect the latent representations for any valid (s, r, o) to satisfy
v>s",2.2. Relations as Linear Maps,[0],[0]
"Wr ≈ v>o (1)
",2.2. Relations as Linear Maps,[0],[0]
The degree of satisfaction in the approximated form of (1) can be quantified using the inner product of v>s Wr and vo.,2.2. Relations as Linear Maps,[0],[0]
"That is, we define a bilinear score function as:
φ(s, r, o) = 〈v>s Wr, vo〉 = v>s Wrvo (2)
",2.2. Relations as Linear Maps,[0],[0]
"Our goal is to learn v and W such that φ(s, r, o) gives high scores to valid triples, and low scores to the invalid ones.
",2.2. Relations as Linear Maps,[0],[0]
"In contrast to some previous models (Bordes et al., 2013) where relations are modeled as additive translating operators, namely vs + wr",2.2. Relations as Linear Maps,[0],[0]
"≈ vo, the multiplicative formulation in (1) offers a natural analogy to the first-order logic where each relation is treated as a predicate operator over input arguments (subject and object in our case).",2.2. Relations as Linear Maps,[0],[0]
"Clearly, the linear transformation defined by a matrix, a.k.a.",2.2. Relations as Linear Maps,[0],[0]
"a linear map, is a richer operator than the additive transformation defined by a vector.",2.2. Relations as Linear Maps,[0],[0]
"Multiplicative models are also found to substantially outperform additive models empirically (Nickel et al., 2011; Yang et al., 2014).",2.2. Relations as Linear Maps,[0],[0]
"Instead of allowing arbitrary linear maps to be used for representing relations, a particular family of matrices has been studied for “well-behaved” linear maps.",2.3. Normal Transformations,[0],[0]
This family is named as the normal matrices.,2.3. Normal Transformations,[0],[0]
Definition 2.1 (Normal Matrix).,2.3. Normal Transformations,[0],[0]
"A real matrixA is normal if and only if A>A = AA>.
",2.3. Normal Transformations,[0],[0]
"Normal matrices have nice theoretical properties which are often desirable form relational modeling, e.g., they are unitarily diagonalizable and hence can be conveniently analyzed by the spectral theorem (Dunford et al., 1971).",2.3. Normal Transformations,[0],[0]
"Representative members of the normal family include:
• Symmetric Matrices for which WrW>r = W>r Wr = W 2r .",2.3. Normal Transformations,[0],[0]
"These includes all diagonal matrices and positive semi-definite matrices, and the symmetry implies φ(s, r, o) = φ(o, r, s).",2.3. Normal Transformations,[0],[0]
"They are suitable for modeling symmetric relations such as is identical.
",2.3. Normal Transformations,[0],[0]
"• Skew-/Anti-symmetric Matrices for which WrW>r = W>r Wr = −W 2r , which implies φ(s, r, o) = −φ(o, r, s).",2.3. Normal Transformations,[0],[0]
"These matrices are suitable for modeling asymmetric relations such as is parent of .
",2.3. Normal Transformations,[0],[0]
"• Rotation Matrices for which WrW>r = W>r Wr = Im, which suggests that the relation r is invertible as W−1r always exists.",2.3. Normal Transformations,[0],[0]
"Rotation matrices are suitable for modeling 1-to-1 relationships (bijections).
",2.3. Normal Transformations,[0],[0]
"• Circulant Matrices (Gray et al., 2006), which have been implicitly used in recent work on holographic representations (Nickel et al., 2016).",2.3. Normal Transformations,[0],[0]
"These matrices are usually related to the learning of latent representations in the Fourier domain (see §5 for more details).
",2.3. Normal Transformations,[0],[0]
"In the remaining parts of this paper, we denote all the real normal matrices in Rm×m as Nm(R).",2.3. Normal Transformations,[0],[0]
"Analogical reasoning is known to play a central role in human induction about knowledge (Gentner, 1983; Minsky,
1988; Holyoak et al., 1996; Hofstadter, 2001).",3. Proposed Analogical Inference Framework,[0],[0]
"Here we provide a mathematical formulation of the analogical structures of interest in multi-relational embedding in a latent semantic space, to support algorithmic inference about the embeddings of entities and relations in a knowledge graph.",3. Proposed Analogical Inference Framework,[0],[0]
"Consider the famous example in the word embedding literature (Mikolov et al., 2013; Pennington et al., 2014), for the following entities and relations among them:
“man is to king as woman is to queen”
In an abstract notion we denote the entities by a (as man) , b (as king), c (as woman) and d (as queen), and the relations by r (as crown) and r′ (asmale 7→ female), respectively.",3.1. Analogical Structures,[0],[0]
"These give us the subject-relation-object triplets as follows:
a r→ b, c r→ d, a r
′",3.1. Analogical Structures,[0],[0]
"→ c, b r ′",3.1. Analogical Structures,[0],[0]
"→ d (3)
",3.1. Analogical Structures,[0],[0]
"For multi-relational embeddings, r and r′ are members of R and are modeled as linear maps in our case.
",3.1. Analogical Structures,[0],[0]
"The relational maps in (3) can be visualized using a commutative diagram (Adámek et al., 2004; Brown & Porter, 2006) from the Category Theory, as shown in Figure 2, where each node denotes an entity and each edge denotes a linear map that transforms one entity to the other.",3.1. Analogical Structures,[0],[0]
"We also refer to such a diagram as a “parallelogram” to highlight its particular algebraic structure1.
",3.1. Analogical Structures,[0],[0]
The parallelogram in Figure 2 represents a very basic analogical structure which could be informative for the inference about unknown facts (triplets).,3.1. Analogical Structures,[0],[0]
"To get a sense about why analogies would help in the inference about unobserved facts, we notice that for entities a, b, c, d which form an analogical structure in our example, the parallelogram structure is fully determined by symmetry.",3.1. Analogical Structures,[0],[0]
"This means that if we know a r→ b and a r ′
→ c, then we can induce the remaining triplets of c r→ d and b r ′
→ d.",3.1. Analogical Structures,[0],[0]
"In other words, understanding the relation betweenman and king helps us to fill up the unknown relation between woman and queen.
",3.1. Analogical Structures,[0],[0]
"1Notice that this is different from parallelograms in the geometric sense because each edge here is a linear map instead of the difference between two nodes in the vector space.
",3.1. Analogical Structures,[0],[0]
"Analogical structures are not limited to parallelograms, of course, though parallelograms often serve as the building blocks for more complex analogical structures.",3.1. Analogical Structures,[0],[0]
"As an example, in Figure 1 of §1 we show a compound analogical structure in the form of a triangular prism, for mirroring the correspondent entities/relations between the atom system and the solar system.",3.1. Analogical Structures,[0],[0]
"Formally define the desirable analogical structures in a computationally tractable objective for optimization is the key for solving our problem, which we will introduce next.",3.1. Analogical Structures,[0],[0]
"Although it is tempting to explore all potentially interesting parallelograms in the modeling of analogical structure, it is computationally intractable to examine the entire powerset of entities as the candidate space of analogical structures.",3.2. Commutative Constraint for Linear Maps,[0],[0]
"A more reasonable strategy is to identify some desirable properties of the analogical structures we want to model, and use those properties as constraints for reducing the candidate space.
",3.2. Commutative Constraint for Linear Maps,[0],[0]
An desirable property of the linear maps we want is that all the directed paths with the same starting node and end node form the compositional equivalence.,3.2. Commutative Constraint for Linear Maps,[0],[0]
"Denoting by “◦” the composition operator between two relations, the parallelogram in Figure 2 contains two equivalent compositions as:
r ◦ r′ = r′ ◦ r (4) which means that a is connected to d via either path.",3.2. Commutative Constraint for Linear Maps,[0],[0]
"We call this the commutativity property of the linear maps, which is a necessary condition for forming commutative parallelograms and therefore the corresponding analogical structures.",3.2. Commutative Constraint for Linear Maps,[0],[0]
"Yet another example is given by Figure 1, where sun can traverse to charge along multiple alternative paths of length three, implying the commutativity of relations surrounded by, made of , scale down.
",3.2. Commutative Constraint for Linear Maps,[0],[0]
"The composition of two relations (linear maps) is naturally implemented via matrix multiplication (Yang et al., 2014; Guu et al., 2015), hence equation (4) indicates
Wr◦r′ =WrWr′ =Wr′Wr (5)
",3.2. Commutative Constraint for Linear Maps,[0],[0]
One may further require the commutative constraint (5) to be satisfied for any pair of relations inR because they may be simultaneously present in the same commutative parallelogram for certain subsets of entities.,3.2. Commutative Constraint for Linear Maps,[0],[0]
"In this case, we say the relations inR form a commuting family.
",3.2. Commutative Constraint for Linear Maps,[0],[0]
It is worth mentioning thatNm(R) is not closed under matrix multiplication.,3.2. Commutative Constraint for Linear Maps,[0],[0]
"As the result, the composition rule in eq. (5) may not always yield a legal new relation—Wr◦r′ may no longer be a normal matrix.",3.2. Commutative Constraint for Linear Maps,[0],[0]
"However, any commuting family inNm(R) is indeed closed under multiplication.",3.2. Commutative Constraint for Linear Maps,[0],[0]
This explains the necessity of having a commuting family of relations from an alternative perspective.,3.2. Commutative Constraint for Linear Maps,[0],[0]
The generic goal for multi-relational embedding is to find entity and relation representations such that positive triples labeled as y = +1 receive higher score than the negative triples labeled as y = −1.,3.3. The Optimization Objective,[0],[0]
"This can be formulated as
min v,W
Es,r,o,y∼D ` (φv,W (s, r, o), y) (6)
where φv,W (s, r, o) = v>s Wrvo is our score function based on the embeddings, ` is our loss function, and D is the data distribution constructed based on the training set K.
To impose analogical structures among the representations, we in addition require the linear maps associated with relations to form a commuting family of normal matrices.",3.3. The Optimization Objective,[0],[0]
"This gives us the objective function for ANALOGY:
min v,W
Es,r,o,y∼D ` (φv,W (s, r, o), y) (7)
s.t.",3.3. The Optimization Objective,[0],[0]
WrW>r,3.3. The Optimization Objective,[0],[0]
"=W > r Wr ∀r ∈ R (8)
WrWr′ =Wr′Wr ∀r, r′ ∈ R (9)
where constraints (8) and (9) are corresponding to the normality and commutativity requirements, respectively.",3.3. The Optimization Objective,[0],[0]
Such a constrained optimization may appear to be computationally expensive at the first glance.,3.3. The Optimization Objective,[0],[0]
"In §4, however, we will recast it as a simple lightweight problem for which each SGD update can be carried out efficiently in O(m) time.",3.3. The Optimization Objective,[0],[0]
"The constrained optimization (7) is computationally challenging due to the large number of model parameters in tensor W , the matrix normality constraints, and the quadratic number of pairwise commutative constraints in (9).
",4. Efficient Inference Algorithm,[0],[0]
"Interestingly, by exploiting the special properties of commuting normal matrices, we will show in Corollary 4.2.1 that ANALOGY can be alternatively solved via an another formulation of substantially lower complexity.",4. Efficient Inference Algorithm,[0],[0]
"Our findings are based on the following lemma and theorem:
Lemma 4.1.",4. Efficient Inference Algorithm,[0],[0]
"(Wilkinson & Wilkinson, 1965)",4. Efficient Inference Algorithm,[0],[0]
"For any real normal matrix A, there exists a real orthogonal matrix Q and a block-diagonal matrix B such that A = QBQ>, where each diagonal block of B is either (1) A real scalar,
or (2) A 2-dimensional real matrix in the form of [ x −y y x ] , where both x, y are real scalars.
",4. Efficient Inference Algorithm,[0],[0]
"The lemma suggests any real normal matrix can be blockdiagonalized into an almost-diagonal canonical form.
",4. Efficient Inference Algorithm,[0],[0]
Theorem 4.2 (Proof given in the supplementary material).,4. Efficient Inference Algorithm,[0],[0]
"If a set of real normal matrices A1, A2, ... form a commuting family, namely AiAj = AjAi ∀i, j, then they can be block-diagonalized by the same real orthogonal basis",4. Efficient Inference Algorithm,[0],[0]
"Q.
",4. Efficient Inference Algorithm,[0],[0]
"The theorem above implies that the set of dense relational matrices {Wr}r∈R, if mutually commutative, can always be simultaneously block-diagonalized into another set of sparse almost-diagonal matrices {Br}r∈R. Corollary 4.2.1 (Alternative formulation for ANALOGY).",4. Efficient Inference Algorithm,[0],[0]
"For any given solution (v∗,W ∗) of optimization (7), there always exists an alternative set of embeddings (u∗, B∗) such that φv∗,W∗(s, r, o) ≡",4. Efficient Inference Algorithm,[0],[0]
"φu∗,B∗(s, r, o), ∀(s, r, o), and (u∗, B∗) is given by the solution of:
min u,B
Es,r,o,y∼D ` (φu,B(s, r, o), y) (10)
",4. Efficient Inference Algorithm,[0],[0]
"Br ∈ Bnm ∀r ∈ R (11)
where Bnm denotes all m×m almost-diagonal matrices in Lemma 4.1 with n <",4. Efficient Inference Algorithm,[0],[0]
"m real scalars on the diagonal.
proof sketch.",4. Efficient Inference Algorithm,[0],[0]
"With the commutative constraints, there must exist some orthogonal matrix Q, such that Wr = QBrQ>, Br ∈ Bnm, ∀r ∈ R. We can plug-in these expressions into optimization (7) and let u = vQ, obtaining
φv,W (s, r, o)",4. Efficient Inference Algorithm,[0],[0]
"=v > s Wrvo = v > s QBrQ >vo (12)
",4. Efficient Inference Algorithm,[0],[0]
"=u>s Bruo = φu,B(s, r, o) (13)
In addition, it is not hard to verify that constraints (8) and (9) are automatically satisfied by exploiting the facts thatQ is orthogonal and Bnm is a commutative normal family.
",4. Efficient Inference Algorithm,[0],[0]
Constraints (11) in the alternative optimization problem can be handled by simply binding together the coefficients within each of those 2× 2 blocks in Br.,4. Efficient Inference Algorithm,[0],[0]
"Note that each Br consists of only m free parameters, allowing the gradient w.r.t.",4. Efficient Inference Algorithm,[0],[0]
any given triple to be efficiently evaluated in O(m).,4. Efficient Inference Algorithm,[0],[0]
"In the following we provide a unified view of several embedding models (Yang et al., 2014; Trouillon et al., 2016; Nickel et al., 2016), by showing that they are restricted versions under our framework, hence are implicitly imposing analogical properties.",5. Unified View of Representative Methods,[0],[0]
This explains their strong empirical performance as compared to other baselines (§6).,5. Unified View of Representative Methods,[0],[0]
"DistMult (Yang et al., 2014) embeds both entities and relations as vectors, and defines the score function as
φ(s, r, o) = 〈vs, vr, vo〉 (14) where vs, vr, vo ∈ Rm,∀s, r, o (15)
where 〈·, ·, ·〉 denotes the generalized inner product.",5.1. DistMult,[0],[0]
Proposition 5.1.,5.1. DistMult,[0],[0]
"DistMult embeddings can be fully recovered by ANALOGY embeddings when n = m.
Proof.",5.1. DistMult,[0],[0]
"This is trivial to verify as the score function (15) can be rewritten as φ(s, r, o)",5.1. DistMult,[0],[0]
"= v>s Brvo whereBr is a diagonal matrix given by Br = diag(vr).
",5.1. DistMult,[0],[0]
Entity analogies are encouraged in DistMult as the diagonal matrices diag(vr)’s are both normal and mutually commutative.,5.1. DistMult,[0],[0]
"However, DistMult is restricted to model symmetric relations only, since φ(s, r, o) ≡ φ(o, r, s).",5.1. DistMult,[0],[0]
"ComplEx (Trouillon et al., 2016) extends the embeddings to the complex domain C, which defines
φ(s, r, o) = <",5.2. Complex Embeddings (ComplEx),[0],[0]
"(〈vs, vr, vo〉) (16) where vs, vr, vo ∈ Cm,∀s, r, o (17)
where x denotes the complex conjugate of x. Proposition 5.2.",5.2. Complex Embeddings (ComplEx),[0],[0]
"ComplEx embeddings of embedding size m can be fully recovered by ANALOGY embeddings of embedding size 2m when n = 0.
",5.2. Complex Embeddings (ComplEx),[0],[0]
Proof.,5.2. Complex Embeddings (ComplEx),[0],[0]
"Let <(x) and =(x) be the real and imaginary parts of any complex vector x. We recast φ in (16) as
φ(r, s, o) = + 〈 <(vr),<(vs),<(vo) 〉 (18)
+ 〈 <(vr),=(vs),=(vo) 〉 (19)
+ 〈 =(vr),<(vs),=(vo) 〉 (20)
",5.2. Complex Embeddings (ComplEx),[0],[0]
"− 〈 =(vr),=(vs),<(vo) 〉",5.2. Complex Embeddings (ComplEx),[0],[0]
= v′s > Brv ′ o,5.2. Complex Embeddings (ComplEx),[0],[0]
"(21)
",5.2. Complex Embeddings (ComplEx),[0],[0]
The last equality is obtained via a change of variables: For any complex entity embedding,5.2. Complex Embeddings (ComplEx),[0],[0]
"v ∈ Cm, we define a new real embedding v′ ∈",5.2. Complex Embeddings (ComplEx),[0],[0]
"R2m such that{
(v′)2k = <(v)k (v′)2k−1 = =(v)k ∀k = 1, 2, . .",5.2. Complex Embeddings (ComplEx),[0],[0]
.m,5.2. Complex Embeddings (ComplEx),[0],[0]
"(22)
The corresponding Br is a block-diagonal matrix in B02m with its k-th block given by [ <(vr)k −=(vr)k =(",5.2. Complex Embeddings (ComplEx),[0],[0]
vr)k <,5.2. Complex Embeddings (ComplEx),[0],[0]
(,5.2. Complex Embeddings (ComplEx),[0],[0]
vr)k ] .,5.2. Complex Embeddings (ComplEx),[0],[0]
"HolE (Nickel et al., 2016) defines the score function as
φ(s, r, o) = 〈vr, vs ∗ vo〉 (23) where vs, vr, vo ∈",5.3. Holographic Embeddings (HolE),[0],[0]
"Rm,∀s, r, o (24)
where the association of s and o is implemented via circular correlation denoted by ∗. This formulation is motivated by the holographic reduced representation (Plate, 2003).
",5.3. Holographic Embeddings (HolE),[0],[0]
"To relate HolE with ANALOGY, we rewrite (24) in a bilinear form with a circulant matrix C(vr) in the middle
φ(r, s, o) = v>s C(vr)vo (25)
where entries of a circulant matrix are defined as
C(x) =  x1 xm · · · x3 x2 x2",5.3. Holographic Embeddings (HolE),[0],[0]
x1 xm x3 ...,5.3. Holographic Embeddings (HolE),[0],[0]
x2 x1 . . .,5.3. Holographic Embeddings (HolE),[0],[0]
"...
",5.3. Holographic Embeddings (HolE),[0],[0]
xm−1 . . . . .,5.3. Holographic Embeddings (HolE),[0],[0]
.,5.3. Holographic Embeddings (HolE),[0],[0]
"xm
xm xm−1 · · · x2",5.3. Holographic Embeddings (HolE),[0],[0]
"x1
 (26)
It is not hard to verify that circulant matrices are normal and commute (Gray et al., 2006), hence entity analogies are encouraged in HolE, for which optimization (7) reduces to an unconstrained problem as equalities (8) and (9) are automatically satisfied when all Wr’s are circulant.
",5.3. Holographic Embeddings (HolE),[0],[0]
"The next proposition further reveals that HolE is equivalent to ComplEx with minor relaxation.
",5.3. Holographic Embeddings (HolE),[0],[0]
Proposition 5.3.,5.3. Holographic Embeddings (HolE),[0],[0]
"HolE embeddings can be equivalently obtained using the following score function
φ(s, r, o) = <",5.3. Holographic Embeddings (HolE),[0],[0]
"(〈vs, vr, vo〉) (27) where vs, vr, vo ∈ F(Rm),∀s, r, o (28)
where F(Rm) denotes the image of Rm in Cm through the Discrete Fourier Transform (DFT).",5.3. Holographic Embeddings (HolE),[0],[0]
"In particular, the above reduces to ComplEx by relaxing F(Rm) to Cm.
",5.3. Holographic Embeddings (HolE),[0],[0]
Proof.,5.3. Holographic Embeddings (HolE),[0],[0]
Let F be the DFT operator defined by F(x) = Fx where F ∈ Cm×m is called the Fourier basis of DFT.,5.3. Holographic Embeddings (HolE),[0],[0]
"A well-known property for circulant matrices is that anyC(x) can always be diagonalized by F , and its eigenvalues are given by Fx (Gray et al., 2006).
",5.3. Holographic Embeddings (HolE),[0],[0]
"Hence the score function can be further recast as
φ(r, s, o) = v>s F −1 diag(Fvr)Fvo (29)
",5.3. Holographic Embeddings (HolE),[0],[0]
"= 1
m (Fvs)
> diag(Fvr)(Fvo) (30)
= 1
m 〈F(vs),F(vr),F(vo)〉 (31) = <",5.3. Holographic Embeddings (HolE),[0],[0]
"[ 1
m 〈F(vs),F(vr),F(vo)〉
] (32)
",5.3. Holographic Embeddings (HolE),[0],[0]
"Let v′s = F(vs), v ′",5.3. Holographic Embeddings (HolE),[0],[0]
o = F(vo) and v ′,5.3. Holographic Embeddings (HolE),[0],[0]
"r = 1 mF(vr), we obtain exactly the same score function as used in ComplEx
φ(s, r, o) = <",5.3. Holographic Embeddings (HolE),[0],[0]
"( 〈v′s, v′r, v′o〉 ) (33)
(33) is equivalent to (16) apart from an additional constraint that v′s, v ′",5.3. Holographic Embeddings (HolE),[0],[0]
"r, v ′",5.3. Holographic Embeddings (HolE),[0],[0]
o are the image of R in the Fourier domain.,5.3. Holographic Embeddings (HolE),[0],[0]
"We evaluate ANALOGY and the baselines over two benchmark datasets for multi-relational embedding released by
previous work (Bordes et al., 2013), namely a subset of Freebase (FB15K) for generic facts and WordNet (WN18) for lexical relationships between words.
",6.1. Datasets,[0],[0]
The dataset statistics are summarized in Table 1.,6.1. Datasets,[0],[0]
We compare the performance of ANALOGY against a variety types of multi-relational embedding models developed in recent years.,6.2. Baselines,[0],[0]
"Those models can be categorized as:
• Translation-based models where relations are modeled as translation operators in the embedding space, including TransE (Bordes et al., 2013) and its variants TransH (Wang et al., 2014), TransR",6.2. Baselines,[0],[0]
"(Lin et al., 2015b), TransD (Ji et al., 2015), STransE (Nguyen et al., 2016) and RTransE (Garcia-Duran et al., 2015).
",6.2. Baselines,[0],[0]
"• Multi-relational latent factor models including LFM (Jenatton et al., 2012) and RESCAL (Nickel et al., 2011) based collective matrix factorization.
",6.2. Baselines,[0],[0]
"• Models involving neural network components such as neural tensor networks (Socher et al., 2013) and PTransE-RNN (Lin et al., 2015b), where RNN stands for recurrent neural networks.
",6.2. Baselines,[0],[0]
"• Pathwise models including three different variants of PTransE (Lin et al., 2015a) which extend TransE by explicitly taking into account indirect connections (relational paths) between entities.
",6.2. Baselines,[0],[0]
"• Models subsumed under our proposed framework (§5), including DistMult (Yang et al., 2014) based simple multiplicative interactions, ComplEx (Trouillon et al., 2016) using complex coefficients and HolE (Nickel et al., 2016) based on holographic representations.",6.2. Baselines,[0],[0]
"Those models are implicitly leveraging analogical structures per our previous analysis.
",6.2. Baselines,[0],[0]
• Models enhanced by external side information.,6.2. Baselines,[0],[0]
"We use Node+LinkFeat (NLF) (Toutanova & Chen, 2015) as a representative example, which leverages textual mentions derived from the ClueWeb corpus.",6.2. Baselines,[0],[0]
"Following the literature of multi-relational embedding, we use the conventional metrics of Hits@k and Mean Reciprocal Rank (MRR) which evaluate each system-produced
ranked list for each test instance and average the scores over all ranked lists for the entire test set of instances.
",6.3. Evaluation Metrics,[0],[0]
"The two metrics would be flawed for the negative instances created in the test phase as a ranked list may contain some positive instances in the training and validation sets (Bordes et al., 2013).",6.3. Evaluation Metrics,[0],[0]
"A recommended remedy, which we followed, is to remove all training- and validation-set triples from all ranked lists during testing.",6.3. Evaluation Metrics,[0],[0]
"We use “filt.” and “raw” to indicate the evaluation metrics with or without filtering, respectively.
",6.3. Evaluation Metrics,[0],[0]
"In the first set of our experiments, we used on Hits@k with k=10, which has been reported for most methods in the literature.",6.3. Evaluation Metrics,[0],[0]
"We also provide additional results of ANALOGY and a subset of representative baseline methods using MRR, Hits@1 and Hits@3, to enable the comparison with the methods whose published results are in those metrics.",6.3. Evaluation Metrics,[0],[0]
"We use the logistic loss for ANALOGY throughout all experiments, namely `(φ(s, r, o), y) =",6.4.1. LOSS FUNCTION,[0],[0]
"− log σ(yφ(s, r, o)), where σ is the sigmoid activation function.",6.4.1. LOSS FUNCTION,[0],[0]
We empirically found this simple loss function to perform reasonably well as compared to more sophisticated ranking loss functions.,6.4.1. LOSS FUNCTION,[0],[0]
"Our C++ implementation2 runs over a CPU, as ANALOGY only requires lightweight linear algebra routines.",6.4.2. ASYNCHRONOUS ADAGRAD,[0],[0]
"We use asynchronous stochastic gradient descent (SGD) for optimization, where the gradients with respect to different mini-batches are simultaneously evaluated in multiple threads, and the gradient updates for the shared model parameters are carried out without synchronization.",6.4.2. ASYNCHRONOUS ADAGRAD,[0],[0]
"Asynchronous SGD is highly efficient, and causes little performance drop when parameters associated with different mini-batches are mutually disjoint with a high probability (Recht et al., 2011).",6.4.2. ASYNCHRONOUS ADAGRAD,[0],[0]
"We adapt the learning rate based on historical gradients using AdaGrad (Duchi et al., 2011).",6.4.2. ASYNCHRONOUS ADAGRAD,[0],[0]
"Since only valid triples (positive instances) are explicitly given in the training set, invalid triples (negative instances) need to be artificially created.",6.4.3. CREATION OF NEGATIVE SAMPLES,[0],[0]
"Specifically, for every positive example (s, r, o), we generate three negative instances (s′, r, o), (s, r′, o), (s, r, o′) by corrupting s, r, o with random entities/relations s′ ∈ E , r′ ∈ R, o′ ∈ E .",6.4.3. CREATION OF NEGATIVE SAMPLES,[0],[0]
"The union of all positive and negative instances defines our data distribution D for SGD updates.
2Code available at https://github.com/quark0/ANALOGY.",6.4.3. CREATION OF NEGATIVE SAMPLES,[0],[0]
"We conducted a grid search to find the hyperparameters of ANALOGY which maximize the filtered MRR on the validation set, by enumerating all combinations of the embedding size m ∈ {100, 150, 200}, `2 weight decay factor λ ∈ {10−1, 10−2, 10−3} of model coefficients v and W , and the ratio of negative over positive samples α ∈ {3, 6}.",6.4.4. MODEL SELECTION,[0],[0]
"The resulting hyperparameters for the WN18 dataset are m = 200, λ = 10−2, α = 3, and those for the FB15K dataset are m = 200, λ = 10−3, α = 6.",6.4.4. MODEL SELECTION,[0],[0]
The number of scalars on the diagonal of each Br is always set to be m2 .,6.4.4. MODEL SELECTION,[0],[0]
We set the initial learning rate to be 0.1 for both datasets and adjust it using AdaGrad during optimization.,6.4.4. MODEL SELECTION,[0],[0]
All models are trained for 500 epochs.,6.4.4. MODEL SELECTION,[0],[0]
"Table 2 compares the Hits@10 score of ANALOGY with that of 23 competing methods using the published scores
Table 3.",6.5. Results,[0],[0]
"MRR and Hits@{1,3} of a subset of representative models on WN18 and FB15K. The performance scores of TransE and REACAL are cf.",6.5. Results,[0],[0]
"the results published in (Trouillon et al., 2016) and (Nickel et al., 2016), respectively.
",6.5. Results,[0],[0]
"WN18 FB15
Models MRR (filt.)",6.5. Results,[0],[0]
MRR (raw),6.5. Results,[0],[0]
Hits@1 (filt.),6.5. Results,[0],[0]
Hits@3 (filt.),6.5. Results,[0],[0]
MRR (filt.),6.5. Results,[0],[0]
MRR (raw),6.5. Results,[0],[0]
Hits@1 (filt.),6.5. Results,[0],[0]
"Hits@3 (filt.)
",6.5. Results,[0],[0]
"RESCAL (Nickel et al., 2011) 89.0 60.3 84.2 90.4 35.4 18.9 23.5 40.9",6.5. Results,[0],[0]
"TransE (Bordes et al., 2013) 45.4 33.5 8.9 82.3 38.0 22.1 23.1 47.2 DistMult (Yang et al., 2014) 82.2 53.2 72.8 91.4 65.4 24.2 54.6 73.3 HolE (Nickel et al., 2016) 93.8 61.6 93.0 94.5 52.4 23.2 40.2 61.3 ComplEx (Trouillon et al., 2016) 94.1 58.7 93.6 94.5 69.2 24.2 59.9 75.9
Our ANALOGY 94.2 65.7 93.9 94.4 72.5 25.3 64.6 78.5
for these methods in the literature on the WN18 and FB15K datasets.",6.5. Results,[0],[0]
"For the methods not having both scores, the missing slots are indicated by “–”.",6.5. Results,[0],[0]
"The best score on each dataset is marked in the bold face; if the differences among the top second or third scores are not statistically significant from the top one, then these scores are also bold faced.",6.5. Results,[0],[0]
"We used one-sample proportion test (Yang & Liu, 1999) at the 5% p-value level for testing the statistical significances3.
Table 3 compares the methods (including ours) whose results in additional metrics are available.",6.5. Results,[0],[0]
"The usage of the bold faces is the same as those in Table 2.
",6.5. Results,[0],[0]
"In both tables, ANALOGY performs either the best or the 2nd best which is in the equivalent class with the best score in each case according statistical significance test.",6.5. Results,[0],[0]
"Specifically, on the harder FB15K dataset in Table 2, which has a very large number of relations, our model outperforms all baseline methods.",6.5. Results,[0],[0]
These results provide a good evidence for the effective modeling of analogical structures in our approach.,6.5. Results,[0],[0]
"We are pleased to see in Table 3 that ANALOGY outperforms DistMult, ComplEx and HolE in all the metrics, as the latter three can be viewed as more constrained versions of our method (as discussed in (§5)).",6.5. Results,[0],[0]
"Furthermore, our assertion on HolE for being a special case of ComplEx (§5) is justified in the same table by the fact that the performance of HolE is dominated by ComplEx.
",6.5. Results,[0],[0]
"In Figure 3 we show the empirical scalability of ANALOGY, which not only completes one epoch in a few seconds on both datasets, but also scales linearly in the size of the embedding problem.",6.5. Results,[0],[0]
"As compared to single-threaded AdaGrad, our asynchronous AdaGrad over 16 CPU threads offers 11.4x and 8.3x speedup on FB15K and WN18, respectively, on a single commercial desktop.
3Notice that proportion tests only apply to performance scores as proportions, including Hits@k, but are not applicable to nonproportional scores such as MRR.",6.5. Results,[0],[0]
Hence we only conducted the proportion tests on the Hits@k scores.,6.5. Results,[0],[0]
"We presented a novel framework for explicitly modeling analogical structures in multi-relational embedding, along with a differentiable objective function and a linear-time inference algorithm for large-scale embedding of knowledge graphs.",7. Conclusion,[0],[0]
"The proposed approach obtains the state-of-the-art results on two popular benchmark datasets, outperforming a large number of strong baselines in most cases.
",7. Conclusion,[0],[0]
"Although we only focused on the multi-relational inference for knowledge-base embedding, we believe that analogical structures exist in many other machine learning problems beyond the scope of this paper.",7. Conclusion,[0],[0]
"We hope this work shed light on a broad range of important problems where scalable inference for analogical analysis would make an impact, such as machine translation and image captioning (both problems require modeling cross-domain analogies).",7. Conclusion,[0],[0]
We leave these interesting topics as our future work.,7. Conclusion,[0],[0]
We thank the reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work is supported in part by the National Science Founda-
tion (NSF) under grant IIS-1546329.",Acknowledgments,[0],[0]
Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs.,abstractText,[0],[0]
An effective and scalable solution for this problem is crucial for the true success of knowledgebased inference in a broad range of applications.,abstractText,[0],[0]
This paper proposes a novel framework for optimizing the latent representations with respect to the analogical properties of the embedded entities and relations.,abstractText,[0],[0]
"By formulating the learning objective in a differentiable fashion, our model enjoys both theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets.",abstractText,[0],[0]
"Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.",abstractText,[0],[0]
Analogical Inference for Multi-relational Embeddings,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2893–2897 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The past year has seen a renewal of interest in endto-end learning of communication strategies between pairs of agents represented with deep networks (Wagner et al., 2003).",1 Introduction,[0],[0]
"Approaches of this kind make it possible to learn decentralized policies from scratch (Foerster et al., 2016; Sukhbaatar et al., 2016), with multiple agents coordinating via learned communication protocol.",1 Introduction,[0],[0]
"More generally, any encoder–decoder model (Sutskever et al., 2014) can be viewed as implementing an analogous communication protocol, with the input encoding playing the role of a message in an artificial “language” shared by the encoder and decoder (Yu et al., 2016).",1 Introduction,[0],[0]
"Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical (Dircks and Stoness, 1999; Lazaridou et al., 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.
",1 Introduction,[0],[0]
1 Code and data are available at http://github.,1 Introduction,[0],[0]
"com/jacobandreas/rnn-syn.
",1 Introduction,[0],[0]
One of the distinguishing features of natural language is compositionality: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning.,1 Introduction,[0],[0]
"RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al., 2014).",1 Introduction,[0],[0]
It is thus natural to ask whether these “language-like” structures also arise spontaneously in models trained directly from an environment signal.,1 Introduction,[0],[0]
"Rather than using language as a form of supervision, we propose to use it as a probe—exploiting post-hoc statistical correspondences between natural language descriptions and neural encodings to discover regular structure in representation space.
",1 Introduction,[0],[0]
"To do this, we need to find (vector, string) pairs with matching semantics, which requires first aligning unpaired examples of human–human
2893
communication with network hidden states.",1 Introduction,[0],[0]
This is similar to the problem of “translating” RNN representations recently investigated in Andreas et al. (2017).,1 Introduction,[0],[0]
Here we build on that approach in order to perform a detailed analysis of compositional structure in learned “languages”.,1 Introduction,[0],[0]
"We investigate a communication game previously studied by FitzGerald et al. (2013), and make two discoveries: in a model trained without any access to language data,
1.",1 Introduction,[0],[0]
"The strategies employed by human speakers in a given communicative context are surprisingly good predictors of RNN behavior in the same context: humans and RNNs send messages whose interpretations agree on nearly 90% of object-level decisions, even outside the contexts in which they were produced.
2.",1 Introduction,[0],[0]
Interpretable language-like structure naturally arises in the space of representations.,1 Introduction,[0],[0]
"We identify geometric regularities corresponding to negation, conjunction, and disjunction, and show that it is possible to linearly transform representations in ways that approximately correspond to these logical operations.",1 Introduction,[0],[0]
"We focus our evaluation on a communication game due to FitzGerald et al. (2013) (Figure 1, top).",2 Task,[0],[0]
"In this game, the speaker observes (1) a world W of 1–20 objects labeled with with attributes and (2) a designated target subset X of objects in the world.",2 Task,[0],[0]
"The listener observes only W , and the speaker’s goal is to communicate a representation of X that enables the listener to accurately reconstruct it.",2 Task,[0],[0]
The GENX dataset collected for this purpose contains 4170 human-generated natural-language referring expressions and corresponding logical forms for 273 instances of this game.,2 Task,[0],[0]
"Because these human-generated expressions have all been pre-annotated, we treat language and logic interchangeably and refer to both with the symbol",2 Task,[0],[0]
"e. We write e(W ) for the expression generated by a human for a particular world W , and JeKW for the result of evaluating the logical form e against W .
",2 Task,[0],[0]
We are interested in using language data of this kind to analyze the behavior of a deep model trained to play the same game.,2 Task,[0],[0]
"We focus our analysis on a standard RNN encoder–decoder, with the encoder playing the role of the speaker and the
decoder playing the role of the listener.",2 Task,[0],[0]
"The encoder is a single-layer RNN with GRU cells (Cho et al., 2014) that consumes both the input world and target labeling and outputs a 64-dimensional hidden representation.",2 Task,[0],[0]
We write f(W ) for the output of this encoder model on a world W .,2 Task,[0],[0]
"To make predictions, this representation is passed to a decoder implemented as a multilayer perceptron.",2 Task,[0],[0]
The decoder makes an independent labeling decision about every object in W (taking as input both f and a feature representation of a particular object Wi).,2 Task,[0],[0]
We write JfKW for the full vector of decoder outputs on W .,2 Task,[0],[0]
We train the model maximize classification accuracy on randomly-generated scenes and target sets of the same form as in the GENX dataset.,2 Task,[0],[0]
We are not concerned with the RNN model’s raw performance on this task (it achieves nearly perfect accuracy).,3 Approach,[0],[0]
"Instead, our goal is to explore what kinds of messages the model computes in order to achieve this accuracy—and specifically whether these messages contain high-level semantics and low-level structure similar to the referring expressions produced by humans.",3 Approach,[0],[0]
But how do we judge semantic equivalence between natural language and vector representations?,3 Approach,[0],[0]
"Here, as in Andreas et al. (2017), we adopt an approach inspired by formal semantics, and represent the meaning of messages via their truth conditions (Figure 1).
",3 Approach,[0],[0]
"For every problem instance W in the dataset, we have access to one or more human messages e(W ) as well as the RNN encoding f(W ).",3 Approach,[0],[0]
"The truth-conditional account of meaning suggests that we should judge e and f to be equivalent if they designate the same set of of objects in the world (Davidson, 1967).",3 Approach,[0],[0]
"But it is not enough to compare their predictions solely in the context where they were generated—testing if JeKW = JfKW — because any pair of models that achieve perfect accuracy on the referring expression task will make the same predictions in this initial context, regardless of the meaning conveyed.
",3 Approach,[0],[0]
"Instead, we sample a collection of alternative worlds {Wi} observed elsewhere in the dataset, and compute a tabular meaning representation rep(e) =",3 Approach,[0],[0]
{JeKWi} by evaluating e in each world Wi.,3 Approach,[0],[0]
We similarly compute rep(f) =,3 Approach,[0],[0]
"{JfKWi}, allowing the learned decoder model to play the role of logical evaluation for message vectors.",3 Approach,[0],[0]
"For
logically equivalent messages, these tabular representations are guaranteed to be identical, so the sampling procedure can be viewed as an approximate test of equivalence.",3 Approach,[0],[0]
It additionally allows us to compute softer notions of equivalence by measuring agreement on individual worlds or objects.,3 Approach,[0],[0]
We begin with the simplest question we can answer with this tool: how often do the messages generated by the encoder model have the same meaning as messages generated by humans for the same context?,4 Interpreting the meaning of messages,[0],[0]
"Again, our goal is not to evaluate the performance of the RNN model, but instead our ability to understand its behavior.",4 Interpreting the meaning of messages,[0],[0]
Does it send messages with human-like semantics?,4 Interpreting the meaning of messages,[0],[0]
Is it more explicit?,4 Interpreting the meaning of messages,[0],[0]
"Or does it behave in a way indistinguishable from a random classifier?
",4 Interpreting the meaning of messages,[0],[0]
"For each scene in the GENX test set, we compute the model-generated message f and its tabular representation rep(f), and measure the extent to which this agrees with representations produced by three “theories” of model behavior (Figure 2): (1) a random theory that accepts or rejects objects with uniform probability, (2) a literal theory that predicts membership only for objects that exactly match some object in the original target set, and (3) a human theory that predicts according to the most frequent logical form associated with natural language descriptions of the target set (as described in the preceding section).",4 Interpreting the meaning of messages,[0],[0]
"We evaluate agreement at the level of individual objects, worlds, and full tabular meaning representations.
",4 Interpreting the meaning of messages,[0],[0]
Results are shown in Table 1.,4 Interpreting the meaning of messages,[0],[0]
"Model behavior is well explained by human decisions in the same context: object-level decisions can be predicted with close to 90% accuracy based on human judgments alone, and a third of message pairs agree exactly in every sampled scene, providing strong evidence that they carry the same semantics.
",4 Interpreting the meaning of messages,[0],[0]
"These results suggest that the model has learned a communication strategy that is at least superficially language-like: it admits representations of the same kinds of communicative abstractions that humans use, and makes use of these abstractions with some frequency.",4 Interpreting the meaning of messages,[0],[0]
"But this is purely a statement about the high-level behavior of the model, and not about the structure of the space of representations.",4 Interpreting the meaning of messages,[0],[0]
Our primary goal is to determine whether this behavior is achieved using lowlevel structural regularities in vector space that can themselves be associated with aspects of natural language communication.,4 Interpreting the meaning of messages,[0],[0]
For this we turn to a focused investigation of three specific logical constructions used in natural language: a unary operation (negation) and two binary operations (conjunction and disjunction).,5 Interpreting the structure of messages,[0],[0]
"All are used in the training data, with a variety of scopes (e.g. all green objects that are not a triangle, all the pieces that are not tan arches).
",5 Interpreting the structure of messages,[0],[0]
"Because humans often find it useful to specify the target set by exclusion rather than inclusion, we first hypothesize that the RNN language might find it useful to incorporate some mechanism cor-
responding to negation, and that messages can be predictably “negated” in vector space.",5 Interpreting the structure of messages,[0],[0]
"To test this hypothesis, we first collect examples of the form (e, f, e′, f ′), where e′ = ¬e, rep(e) = rep(f), and rep(e′) = rep(f ′).",5 Interpreting the structure of messages,[0],[0]
"In other words, we find pairs of pairs of RNN representations f and f ′ for which the natural language messages (e, e′) serve as a denotational certificate that f ′ behaves as a negation of f .",5 Interpreting the structure of messages,[0],[0]
"If the learned model does not have any kind of primitive notion of negation, we expect that it will not be possible to find any kind of predictable relationship between pairs (f, f ′).",5 Interpreting the structure of messages,[0],[0]
"(As an extreme example, we could imagine every possible prediction rule being associated with a different point in the representation space, with the correspondence between position and behavior essentially random.)",5 Interpreting the structure of messages,[0],[0]
"Conversely, if there is a first-class notion of negation, we should be able to select an arbitrary representation vector f with an associated referring expression e, apply some transformation N to f , and be able to predict a priori how the decoder model will interpret the representation Nf—i.e. in correspondence with ¬e.
",5 Interpreting the structure of messages,[0],[0]
Here we make the strong assumption that the negation operation is not only predictable but linear.,5 Interpreting the structure of messages,[0],[0]
"Previous work has found that linear operators are powerful enough to capture many hierarchical and relational structures (Paccanaro and Hinton, 2002; Bordes et al., 2014).",5 Interpreting the structure of messages,[0],[0]
"Using examples (f, f ′) collected from the training set as described above, we compute the least-squares estimate N̂ = arg minN
∑ ||Nf − f ′||22 .",5 Interpreting the structure of messages,[0],[0]
"To evaluate, we collect example representations from the test set that are equivalent to known logical forms, and measure how frequently model behaviors rep(Nf) agree with the logical predictions
rep(¬e)—in other words, how often the linear operator N actually corresponds to logical negation.",5 Interpreting the structure of messages,[0],[0]
Results are shown in the top portion of Table 2.,5 Interpreting the structure of messages,[0],[0]
"Correspondence with the logical form is quite high, resulting in 97% agreement at the level of individual objects and 45% agreement on full representations.",5 Interpreting the structure of messages,[0],[0]
We conclude that the estimated linear operator N̂ is analogous to negation in natural language.,5 Interpreting the structure of messages,[0],[0]
"Indeed, the behavior of this operator is readily visible in Figure 3: predicted negated forms (in red) lie close in vector space to their true values, and negation corresponds roughly to mirroring across a central point.
",5 Interpreting the structure of messages,[0],[0]
"In our final experiment, we explore whether the same kinds of linear maps can be learned for the binary operations of conjunction and disjunction.",5 Interpreting the structure of messages,[0],[0]
"As in the previous section, we collect examples from the training data of representations whose denotations are known to correspond to groups of logical forms in the desired relationship—in this case tuples (e, f, e′, f ′, e′′, f ′′), where rep(e) = rep(f), rep(e′) = rep(f ′), rep(e′′) = rep(f ′′) and either e′′ = e ∧ e′ (conjunction) or e′′ = e ∨ e′",5 Interpreting the structure of messages,[0],[0]
(disjunction).,5 Interpreting the structure of messages,[0],[0]
"Since we expect that our operator will be symmetric in its arguments, we solve for M̂ = arg minM ∑ ||Mf +",5 Interpreting the structure of messages,[0],[0]
Mf ′,5 Interpreting the structure of messages,[0],[0]
"− f ′′||22.
Results are shown in the bottom portions of Table 2.",5 Interpreting the structure of messages,[0],[0]
Correspondence between the behavior predicted by the contextual logical form and the model’s actual behavior is less tight than for negation.,5 Interpreting the structure of messages,[0],[0]
"At the same time, the estimated operators are clearly capturing some structure: in the case of disjunction, for example, model interpretations are correctly modeled by the logical form 92% of the time at the object level and 19% of the time at the denotation level.",5 Interpreting the structure of messages,[0],[0]
"This suggests that the operations of conjunction and disjunction do have some functional counterparts in the RNN language, but that these functions are not everywhere well approximated as linear.",5 Interpreting the structure of messages,[0],[0]
"Building on earlier tools for identifying neural codes with natural language strings, we have presented a technique for exploring compositional structure in a space of vector-valued representations.",6 Conclusions,[0],[0]
"Our analysis of an encoder–decoder model trained on a reference game identified a number of language-like properties in the model’s representation space, including transformations corresponding to negation, disjunction, and conjunction.",6 Conclusions,[0],[0]
"One major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure.",6 Conclusions,[0],[0]
We believe our experiments so far highlight the usefulness of a denotational perspective from formal semantics when interpreting the behavior of deep models.,6 Conclusions,[0],[0]
We investigate the compositional structure of message vectors computed by a deep network trained on a communication game.,abstractText,[0],[0]
"By comparing truth-conditional representations of encoder-produced message vectors to human-produced referring expressions, we are able to identify aligned (vector, utterance) pairs with the same meaning.",abstractText,[0],[0]
"We then search for structured relationships among these aligned pairs to discover simple vector space transformations corresponding to negation, conjunction, and disjunction.",abstractText,[0],[0]
Our results suggest that neural representations are capable of spontaneously developing a “syntax” with functional analogues to qualitative properties of natural language.1,abstractText,[0],[0]
Analogs of Linguistic Structure in Deep Representations,title,[0],[0]
This article is about the set of all decompositions (clusterings) of a graph.,1. Introduction,[0],[0]
"A decomposition of a graph G = (V,E) is a partition Π of the node set V such that, for every subset U ∈ Π of nodes, the subgraph of G induced by U is connected.",1. Introduction,[0],[0]
An example is depicted in Fig. 1.,1. Introduction,[0],[0]
"Decompositions of a graph arise in practice, as feasible solutions of clustering problems, and in theory, as a generalization of partitions of a set, to which they specialize for complete graphs.
",1. Introduction,[0],[0]
We study the set of all decompositions of a graph through its characterization as a set of multicuts.,1. Introduction,[0],[0]
"A multicut of G is a subset M ⊆ E of edges such that, for every (chordless) cycle C ⊆ E of G, we have |M ∩ C| 6=",1. Introduction,[0],[0]
1.,1. Introduction,[0],[0]
An example is depicted in Fig. 1.,1. Introduction,[0],[0]
"For any graph G, a one-to-one relation exists between the decompositions and the multicuts of G. The multicut induced by a decomposition is the set of edges that straddle distinct components.",1. Introduction,[0],[0]
"Multicuts are useful in the study of decompositions as the characteristic function x ∈ {0, 1}E of a multicut x−1(1) of G makes explicit, for every pair {v, w} ∈ E of neighboring nodes, whether v and w are in distinct components.",1. Introduction,[0],[0]
"To make explicit also for nonneighboring nodes, specifically, for all {v, w} ∈ E′ with
*Equal contribution 1Max Planck Institute for Informatics, Saarbrücken, Germany.",1. Introduction,[0],[0]
"Correspondence to: Bjoern Andres <andres@mpi-inf.mpg.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
E ⊆ E′ ⊆,1. Introduction,[0],[0]
"( V 2 ) , whether v andw are in distinct components, we define a lifting of the multicuts of G to multicuts of G′ = (V,E′).",1. Introduction,[0],[0]
The multicuts of G′ lifted from G are still in one-to-one relation with the decompositions of G.,1. Introduction,[0],[0]
"Yet, they are a more expressive model of these decompositions than the multicuts of G. We apply lifted multicuts in three ways:
",1. Introduction,[0],[0]
"Firstly, we study problems related to the definition of a class of decompositions by must-cut or must-join constraints (Section 4).",1. Introduction,[0],[0]
"Such constraints have applications where defining a decomposition totally is an ambiguous and tedious task, e.g., in the field of image segmentation.",1. Introduction,[0],[0]
"The first problem is to decide whether a set of such constraints is consistent, i.e., whether a decomposition of the given graph exists that satisfies the constraints.",1. Introduction,[0],[0]
We show that this decision problem is NP-complete in general and can be solved efficienty for a subclass of constraints.,1. Introduction,[0],[0]
"The second problem is to decide whether a consistent set of must-join and must-cut constraints is maximally specific, i.e., whether no such constraint can be added without changing the set of decompositions that satisfy the constraints.",1. Introduction,[0],[0]
We show that this decision problem is NP-hard in general and can be solved efficienty for a subclass of constraints.,1. Introduction,[0],[0]
"This finding is relevant for comparing the classes of decompositions definable by mustjoin and must-cut constraints by certain metrics, which is the next topic.
",1. Introduction,[0],[0]
"As a second application of lifted multicuts, we study the comparison of decompositions and classes of decompositions by metrics (Section 5).",1. Introduction,[0],[0]
"To obtain a metric on the set of all decompositions of a given graph, we define a metric on a set of lifted multicuts that characterize these decompositions.
",1. Introduction,[0],[0]
"By lifting to different graphs, we obtain different metrics, two of which are well-known and here generalized.",1. Introduction,[0],[0]
"To extend this metric to the classes of decompositions definable by must-join and must-cut constraints, we define a metric on partial lifted multicuts that characterize these classes, connecting results of Sections 4 and 5.",1. Introduction,[0],[0]
We show that computing this metric is NP-hard in general and efficient for a subclass of must-join and must-cut constraints.,1. Introduction,[0],[0]
"These findings have implications on the applicability of must-join and must-cut constraints as a form of supervision, more specifically, on the practicality of certain error metrics and loss functions.
",1. Introduction,[0],[0]
"As a third application of lifted multicuts, we study the optimization of graph decompositions by minimum cost lifted multicuts.",1. Introduction,[0],[0]
The minimum cost lifted multicut problem is a generalization of the correlation clustering problem.,1. Introduction,[0],[0]
Its applications in the field of computer vision are mentioned below.,1. Introduction,[0],[0]
"To tackle this problem, we establish some properties of some facets of lifted multicut polytopes (Fig. 2 and 3), define efficient separation procedures and apply these in a branch-and-cut algorithm.",1. Introduction,[0],[0]
Initial motivation to study decompositions of a graph by multicuts came from the field of polyhedral optimization.,1.1. Related Work,[0],[0]
"Multicut polytopes are studied by Grötschel & Wakabayashi (1989); Deza et al. (1991; 1992); Chopra & Rao (1993; 1995) and Deza & Laurent (1997) who characterize several classes of their facets.
",1.1. Related Work,[0],[0]
The binary linear program whose feasible solutions are all multicuts of a graph is known as the correlation clustering problem from the work of Bansal et al. (2004) and Demaine et al. (2006) who establish its APX-hardness and a logarithmic approximation.,1.1. Related Work,[0],[0]
The stability of its solutions is analyzed by Nowozin & Jegelka (2009).,1.1. Related Work,[0],[0]
Generalizations to multilinear objective functions are studied by Kim et al. (2014) and Kappes et al. (2016).,1.1. Related Work,[0],[0]
"The problem remains NP-hard for planar graphs (Voice et al., 2012; Bachrach et al., 2013) where it admits a PTAS (Klein et al., 2015) and relaxations that are often tight in practice (Yarkony et al., 2012).
",1.1. Related Work,[0],[0]
The lifting of multicuts we define makes path connectedness explicit.,1.1. Related Work,[0],[0]
"For a single component, this is studied by Nowozin & Lampert (2010) who introduce the connected subgraph polytope and outer relaxations.",1.1. Related Work,[0],[0]
"Applications of the minimum cost lifted multicut problem and experimental comparisons to the correlation clustering problem in the field of computer vision are by Keuper et al. (2015) and Tang et al. (2017) who find feasible solutions by local search (Keuper et al., 2015; Levinkov et al., 2017), and by Beier et al. (2017) who find feasible solutions by consensus optimization (Beier et al., 2016).",1.1. Related Work,[0],[0]
The complexity of several decision problems related to clustering with must-join and must-cut constraints is established by Davidson & Ravi (2007).,1.1. Related Work,[0],[0]
"Well-known metrics on the set of all decompositions of a graph are the metric of Rand (1971) and the variation of information (Meilă, 2007).",1.1. Related Work,[0],[0]
"Definition 1 Let G = (V,E) be any graph.",2. Multicuts,[0],[0]
"A subgraph G′ = (V ′, E′) of G is called a component of G iff G′ is non-empty, node-induced1, and connected2.",2. Multicuts,[0],[0]
"A partition Π of V is called a decomposition of G iff, for every U ∈ Π, the subgraph (U,E ∩ ( U 2 ) ) of G induced by U is connected (and hence a component of G).
",2. Multicuts,[0],[0]
"For any graph G, we denote by DG ⊂ 22 V
the set of all decompositions ofG. Useful in the study of decompositions are the multicuts of a graph: Definition 2 For any graph G = (V,E), a subset M ⊆ E of edges is called a multicut of G iff, for every cycle C ⊆ E of G, we have |C ∩M | 6= 1.",2. Multicuts,[0],[0]
"Lemma 1 (Chopra & Rao, 1993)",2. Multicuts,[0],[0]
"It is sufficient in Def. 2 to consider only the chordless cycles.
",2. Multicuts,[0],[0]
"For any graph G, we denote by MG ⊆ 2E the set of all multicuts of G. One reason why multicuts are useful in
1That is:",2. Multicuts,[0],[0]
"E′ = E ∩ ( V ′
2 ) 2We do not require a component to be maximal w.r.t.",2. Multicuts,[0],[0]
"the sub-
graph relation.
",2. Multicuts,[0],[0]
"the study of decompositions is that, for every graph G, a one-to-one relation exists between the decompositions and the multicuts of G. An example is depicted in Fig. 1: Lemma 2 For any graph G = (V,E), the map φG : DG → 2E defined by (1) is a bijection from DG to MG.
∀Π ∈ DG ∀{v, w} ∈ E : {v, w} ∈ φG(Π) ⇔ ∀U ∈",2. Multicuts,[0],[0]
Π(v /∈,2. Multicuts,[0],[0]
U ∨ w /∈,2. Multicuts,[0],[0]
"U) (1)
Another reason why multicuts are useful in the study of decompositions is that, for any graph G = (V,E) and any decomposition Π of G, the characteristic function of the multicut induced by Π is a 01-encoding of Π of fixed length |E|.",2. Multicuts,[0],[0]
"Lemma 3 (Chopra & Rao, 1993)",2. Multicuts,[0],[0]
"For any graph G = (V,E) and any x ∈ {0, 1}E , the set x−1(1) of those edges that are labeled 1 is a multicut of G iff (2) holds.",2. Multicuts,[0],[0]
"It is sufficient in (2) to consider only chordless cycles.
∀C ∈ cycles(G)",2. Multicuts,[0],[0]
∀e ∈ C :,2. Multicuts,[0],[0]
xe ≤,2. Multicuts,[0],[0]
"∑
e′∈C\{e}
xe′ (2)
",2. Multicuts,[0],[0]
"For any graph G = (V,E), we denote by XG the set of all x ∈ {0, 1}E that satisfy (2).",2. Multicuts,[0],[0]
"The decompositions of a complete graph KV := (V, ( V 2 ) ) are precisely the partitions of the node set V (by Def. 1).",2.1. Complete Graphs,[0],[0]
"The multicuts of a complete graph KV relate one-to-one to the equivalence relations on V : Lemma 4 For any set V and the complete graph KV , the map ψ :",2.1. Complete Graphs,[0],[0]
"MKV → 2V×V defined by (3) is a bijection between MKV and the set of all equivalence relations on V .
∀M ∈MKV ∀v, w ∈ V : (v, w) ∈ ψ(M) ⇔ {v, w} /∈M",2.1. Complete Graphs,[0],[0]
"(3)
The bijection between the decompositions of a graph and the multicuts of a graph (Lemma 2) specializes, for complete graphs, to the well-known bijection between the partitions of a set and the equivalence relations on the set (by Lemma 4).",2.1. Complete Graphs,[0],[0]
"In this sense, decompositions and multicuts of graphs generalize partitions of sets and equivalence relations on sets.",2.1. Complete Graphs,[0],[0]
"For any graph G = (V,E), the characteristic function x ∈ XG of a multicut x−1(1) of G makes explicit, for every pair {v, w} ∈ E of neighboring nodes, whether v and w are in distinct components.",3. Lifted Multicuts,[0],[0]
"To make explicit also for nonneighboring nodes, specifically, for all {v, w} ∈ E′ with E ⊆ E′ ⊆ ( V 2 ) , whether v andw are in distinct components, we define a lifting of the multicuts of G to multicuts of G′ = (V,E′):
Definition 3 For any graphs G = (V,E) and G′ = (V,E′) with E ⊆ E′, the composed map",3. Lifted Multicuts,[0],[0]
"λGG′ := φG′ ◦ φ−1G is called the lifting of multicuts from G to G′.
For any graphsG = (V,E) andG′ = (V,E′) withE ⊆ E′, we introduce the notation FGG′ := E′ \",3. Lifted Multicuts,[0],[0]
"E, for brevity.",3. Lifted Multicuts,[0],[0]
Lemma 5,3. Lifted Multicuts,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any x ∈ {0, 1}E′ , the set x−1(1) is a multicut of G′ lifted from G iff
∀C ∈",3. Lifted Multicuts,[0],[0]
cycles(G)∀e ∈ C :,3. Lifted Multicuts,[0],[0]
xe ≤,3. Lifted Multicuts,[0],[0]
"∑
e′∈C\{e}
xe′ (4)
∀vw",3. Lifted Multicuts,[0],[0]
∈,3. Lifted Multicuts,[0],[0]
FGG′ ∀P ∈ vw-paths(G),3. Lifted Multicuts,[0],[0]
": xvw ≤ ∑ e∈P xe (5)
",3. Lifted Multicuts,[0],[0]
∀vw ∈,3. Lifted Multicuts,[0],[0]
FGG′ ∀C ∈ vw-cuts(G) :,3. Lifted Multicuts,[0],[0]
"1− xvw ≤ ∑ e∈C (1− xe)
(6)
",3. Lifted Multicuts,[0],[0]
"For any graphs G = (V,E) and G′ = (V,E′) with E ⊆ E′",3. Lifted Multicuts,[0],[0]
"we denote by XGG′ the set of all x ∈ {0, 1}E ′",3. Lifted Multicuts,[0],[0]
that satisfy (4)–(6).,3. Lifted Multicuts,[0],[0]
"As a first application of lifted multicuts, we study the class of decompositions of a graph definable by must-join and must-cut constraints.",4. Partial Lifted Multicuts,[0],[0]
"For this, we consider partial functions.",4. Partial Lifted Multicuts,[0],[0]
"For any set E, a partial characteristic function of subsets of E is a function from any subset F ⊆ E to {0, 1}.",4. Partial Lifted Multicuts,[0],[0]
"With some abuse of notation, we denote the set of all partial characteristic functions of subsets of E by {0, 1, ∗}E :=⋃ F⊆E{0, 1}F .",4. Partial Lifted Multicuts,[0],[0]
"For any x ∈ {0, 1, ∗}E , we denote the domain of x by domx := x−1({0, 1}).
",4. Partial Lifted Multicuts,[0],[0]
"For any connected graph G = (V,E) whose decompositions we care about and any graph G′ = (V,E′) with E ⊆ E′, we consider a partial function x̃ ∈ {0, 1, ∗}E′ .",4. Partial Lifted Multicuts,[0],[0]
"For any {v, w} ∈ dom x̃, we constrain the nodes v and w to the same component if x̃vw = 0 and to distinct components if x̃vw = 1.",4. Partial Lifted Multicuts,[0],[0]
A natural question to ask is whether a decomposition of the graph G exists that satisfies these constraints.,4.1. Consistency,[0],[0]
"We show that this decision problem is NP-complete.
",4.1. Consistency,[0],[0]
Definition 4,4.1. Consistency,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′, and any x̃ ∈ {0, 1, ∗}E′ , the elements of
XGG′",4.1. Consistency,[0],[0]
"[x̃] := {x ∈ XGG′ | ∀e ∈ dom x̃ : xe = x̃e} (7)
are called the completions of x̃ in XGG′ .",4.1. Consistency,[0],[0]
"In addition, x̃ is called consistent and a partial characterization of multicuts
of G′ lifted from G iff
XGG′",4.1. Consistency,[0],[0]
[x̃] 6= ∅ .,4.1. Consistency,[0],[0]
"(8)
We denote the set of all partial characterizations of multicuts of G′ lifted from G by
X̃GG′",4.1. Consistency,[0],[0]
":= { x̃ ∈ {0, 1, ∗}E ′ ∣∣∣XGG′",4.1. Consistency,[0],[0]
[x̃] 6= ∅} .,4.1. Consistency,[0],[0]
"(9)
Theorem 1 Deciding consistency is NP-complete.
",4.1. Consistency,[0],[0]
"Lemma 6 Consistency can be decided efficiently if E ⊆ dom x̃ or
∀vw ∈",4.1. Consistency,[0],[0]
dom x̃,4.1. Consistency,[0],[0]
\ E : x̃vw = 1 ∨ ∃P ∈ vw-path(G)∀e ∈ P : x̃e = 0 (10),4.1. Consistency,[0],[0]
A less obvious question to ask for any partial characterization x̃ of multicuts of G′ lifted from G is whether x̃ is maximally specific for its completions in XGG′ .,4.2. Specificity,[0],[0]
"In other words, is there no edge e ∈ E′",4.2. Specificity,[0],[0]
\,4.2. Specificity,[0],[0]
"dom x̃ such that, for any completions x, x′ of x̃ in XGG′ , we have xe = x′e, i.e., an edge that could be included in dom x̃ without changing the set of completions of x̃ in XGG′?",4.2. Specificity,[0],[0]
"We show that deciding maximal specificity is NP-hard.
",4.2. Specificity,[0],[0]
"Definition 5 Let G = (V,E) a connected graph and G′ = (V,E′)",4.2. Specificity,[0],[0]
a graph with E ⊆ E′. For any x̃ ∈,4.2. Specificity,[0],[0]
"X̃GG′ , the edges
E′[x̃]",4.2. Specificity,[0],[0]
:= {e ∈ E′,4.2. Specificity,[0],[0]
"| ∀x,",4.2. Specificity,[0],[0]
x′ ∈ XGG′,4.2. Specificity,[0],[0]
"[x̃] : xe = x′e} (11)
are called decided.",4.2. Specificity,[0],[0]
The edges E′ \ E′[x̃] are called undecided.,4.2. Specificity,[0],[0]
"Moreover, x̃ is called maximally specific iff3
E′[x̃] ⊆ dom x̃ .",4.2. Specificity,[0],[0]
"(12)
Theorem 2",4.2. Specificity,[0],[0]
"Deciding maximal specificity is NP-hard.
",4.2. Specificity,[0],[0]
"Lemma 7 Maximal specificity can be decided efficiently if E′ = E or E ⊆ dom x̃.
",4.2. Specificity,[0],[0]
"Below, we justify the term maximal specificity and define an operation that maps any partial characterization of lifted multicuts to one that is maximally specific.
",4.2. Specificity,[0],[0]
Definition 6,4.2. Specificity,[0],[0]
"For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, the relation ≤ on X̃GG′ defined by (13) is called the specificity of partial characterizations of multicuts of G′ lifted from G.
∀x̃, x̃′ ∈",4.2. Specificity,[0],[0]
X̃GG′ :,4.2. Specificity,[0],[0]
(13) x̃ ≤,4.2. Specificity,[0],[0]
x̃′ ⇔ dom x̃ ⊆ dom x̃′ ∧ ∀e ∈ dom x̃ : x̃e =,4.2. Specificity,[0],[0]
x̃′e 3Note that (12) is equivalent to E′[x̃],4.2. Specificity,[0],[0]
"= dom x̃, as E′[x̃] ⊇
dom x̃ holds by definition of E′[x̃].
",4.2. Specificity,[0],[0]
Lemma 8,4.2. Specificity,[0],[0]
"For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, specificity is a partial order on X̃GG′ .
",4.2. Specificity,[0],[0]
"Note that two partial characterizations x̃, x̃′ ∈",4.2. Specificity,[0],[0]
X̃GG′ with the same completions,4.2. Specificity,[0],[0]
XGG′,4.2. Specificity,[0],[0]
[x̃] = XGG′,4.2. Specificity,[0],[0]
[x̃′] need not be comparable w.r.t.≤.,4.2. Specificity,[0],[0]
"For example, consider the graphsG,G′ from Fig. 3, consider x̃ :",4.2. Specificity,[0],[0]
"e1 7→ 0, e2 7→ 0",4.2. Specificity,[0],[0]
and x̃′ : f 7→ 0.,4.2. Specificity,[0],[0]
"Nevertheless, we have the following lemma.",4.2. Specificity,[0],[0]
Lemma 9,4.2. Specificity,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′, any x̃ ∈",4.2. Specificity,[0],[0]
"X̃GG′ and
X̃GG′",4.2. Specificity,[0],[0]
[x̃] := { x̃′ ∈,4.2. Specificity,[0],[0]
X̃GG′ ∣∣∣XGG′,4.2. Specificity,[0],[0]
[x̃′] = XGG′,4.2. Specificity,[0],[0]
[x̃]} (14) a maximum of X̃GG′,4.2. Specificity,[0],[0]
[x̃] w.r.t.,4.2. Specificity,[0],[0]
≤ exists and is unique.,4.2. Specificity,[0],[0]
"Moreover, x̃ is maximally specific in the sense of Def. 5 iff x̃ is maximal w.r.t.",4.2. Specificity,[0],[0]
≤ in X̃GG′,4.2. Specificity,[0],[0]
[x̃].,4.2. Specificity,[0],[0]
"Definition 7 Let G = (V,E) be a connected graph and let G′ = (V,E′) be a graph with E ⊆ E′.",4.2. Specificity,[0],[0]
For any x̃ ∈,4.2. Specificity,[0],[0]
"X̃GG′ , we call the unique maximum of X̃GG′",4.2. Specificity,[0],[0]
[x̃] w.r.t.≤ the closure of x̃ w.r.t.,4.2. Specificity,[0],[0]
G and G′,4.2. Specificity,[0],[0]
"and denote it by clGG′ x̃.
We denote by X̂GG′ the set of all maximally specific partial characterizations of multicuts of G′ lifted from G, i.e.:
X̂GG′ := { x̃ ∈",4.2. Specificity,[0],[0]
X̃GG′ ∣∣∣,4.2. Specificity,[0],[0]
x̃ =,4.2. Specificity,[0],[0]
clGG′ x̃} .,4.2. Specificity,[0],[0]
"(15) Theorem 3 For any x̃, x̃′ ∈",4.2. Specificity,[0],[0]
"X̃GG′ , we have XGG′",4.2. Specificity,[0],[0]
[x̃] = XGG′,4.2. Specificity,[0],[0]
"[x̃
′]⇔ X̃GG′",4.2. Specificity,[0],[0]
[x̃] =,4.2. Specificity,[0],[0]
X̃GG′,4.2. Specificity,[0],[0]
[x̃′]⇔ clGG′ x̃ =,4.2. Specificity,[0],[0]
clGG′ x̃′.,4.2. Specificity,[0],[0]
Lemma 10,4.2. Specificity,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any x ∈ XG, the closure y",4.2. Specificity,[0],[0]
:= clGG′ x of x w.r.t.,4.2. Specificity,[0],[0]
"G and G′ coincides with the lifting of the multicut x−1(1) of G to the multicut y−1(1) of G′, i.e.
(clGG′ x) −1(1) = λGG′(x −1(1)) .",4.2. Specificity,[0],[0]
"(16)
Theorem 4 Computing closures is NP-hard.",4.2. Specificity,[0],[0]
Lemma 11,4.2. Specificity,[0],[0]
"In the special case thatE′ = E orE ⊆ dom x̃, the closure can be computed efficiently.",4.2. Specificity,[0],[0]
"As a second application of lifted multicuts, we compare decompositions of a given graph by comparing lifted multicuts that characterize these decompositions.",5.1. Metrics on Decompositions,[0],[0]
We compare these lifted multicuts by comparing their characteristic functions by Hamming metrics:,5.1. Metrics on Decompositions,[0],[0]
"For any E 6= ∅ and any e ∈ E, we define d1e, d 1 E : {0, 1}E × {0, 1}E → N + 0 by the forms
d1e(x, x ′) = { 0 ifxe = x′e 1 otherwise
(17)
d1E(x, x ′) = ∑ e′∈E d1e′(x, x ′) .",5.1. Metrics on Decompositions,[0],[0]
"(18)
Theorem 5 For any connected graph G = (V,E), any graph G′ = (V,E′), any µ : E′ → R+, the set E′′ := E ∪ E′ and the graph G′′ := (V,E′′), the function dµE′ : XGG′′ ×XGG′′ → R+0 of the form (19) is a pseudo-metric on XGG′′ .",5.1. Metrics on Decompositions,[0],[0]
"Iff G′ is a supergraph of G, i.e., iff E ⊆ E′, dµE′ is a metric on XGG′′ .
",5.1. Metrics on Decompositions,[0],[0]
"dµE′(x, x ′) := ∑ e∈E′ µe d 1 e(x, x ′) (19)
",5.1. Metrics on Decompositions,[0],[0]
"By the one-to-one relation between decompositions and multicuts (Lemma 2), dµE′ induces a pseudo-metric on the set DG of all decompositions of G. Two special cases are well-known: For E′ = E and µ = 1, we have dµE′ = d 1 E , which is the Hamming metric (18) on the multicuts that characterize the decompositions, also known as the boundary metric on decompositions.",5.1. Metrics on Decompositions,[0],[0]
For E′ =,5.1. Metrics on Decompositions,[0],[0]
"( V 2 ) and µ = 1, d1E′ specializes to the metric of Rand (1971).",5.1. Metrics on Decompositions,[0],[0]
"Between these extremes, i.e., for E ⊆ E′ ⊆ ( V 2 ) , the metric dµE′ can be used to analyze more specifically how two decompositions of the same graph differ.",5.1. Metrics on Decompositions,[0],[0]
We propose an analysis w.r.t.,5.1. Metrics on Decompositions,[0],[0]
"the distance δvw of nodes v and w in G, i.e., w.r.t.",5.1. Metrics on Decompositions,[0],[0]
"the length of a shortest vw-path in G. For this, we denote by δG := max{δvw :",5.1. Metrics on Decompositions,[0],[0]
vw ∈,5.1. Metrics on Decompositions,[0],[0]
"( V 2 ) } the diameter of G.
Definition 8",5.1. Metrics on Decompositions,[0],[0]
"For any connected graph G = (V,E) and any n ∈ N, let E[n] := {vw ∈",5.1. Metrics on Decompositions,[0],[0]
( V 2 ),5.1. Metrics on Decompositions,[0],[0]
"| δvw = n} the set of pairs of nodes of distance n in G. Moreover, let µn : E[n]→ Q+ the constant function that maps any vw ∈ E[n] to 1/|E[n]|.",5.1. Metrics on Decompositions,[0],[0]
"For any connected graph G = (V,E), we call the sequence(
dµ n
E[n] )",5.1. Metrics on Decompositions,[0],[0]
"n∈{1,...,δG}
(20)
the spectrum of pseudo-metrics on decompositions of G. For E′ := ( V 2 ) and µ : E′",5.1. Metrics on Decompositions,[0],[0]
"→ Q+ : vw 7→ 1/(δG|E[δvw]|), we call the metric dµE′ the δ-metric on decompositions of G.
An example of a spectrum of pseudo-metrics is depicted in Fig. 4.",5.1. Metrics on Decompositions,[0],[0]
"For any two decompositions Π,Π′ of a connected graph G and suitable lifted multicuts x, x′ characterizing these decompositions, dµ n
E[n](x, x ′) equals the fraction of
pairs of nodes at distance n in G that are either cut by Π and joined by Π′, or cut by Π′ and joined by Π. I.e., the pseudometric dµ n
E[n] compares decompositions of G specifically w.r.t.",5.1. Metrics on Decompositions,[0],[0]
the distance n in G.,5.1. Metrics on Decompositions,[0],[0]
The δ-metric compares decompositions w.r.t.,5.1. Metrics on Decompositions,[0],[0]
"all distances, and each distance is weighted equally.",5.1. Metrics on Decompositions,[0],[0]
This is in contrast to Rand’s metric which is also a comparison w.r.t.,5.1. Metrics on Decompositions,[0],[0]
all distances but each distance is weighted by the number of pairs of nodes that have this distance.,5.1. Metrics on Decompositions,[0],[0]
"We compare classes of decompositions definable by mustjoin and must-cut constraints by comparing partial lifted
multicuts that characterize these decompositions.",5.2. Metrics on Classes of Decompositions,[0],[0]
"To compare partial lifted multicuts, we compare their partial characteristic functions by an extension of the Hamming metric: For any E 6=",5.2. Metrics on Classes of Decompositions,[0],[0]
"∅, any e ∈ E and any θ ∈ R+0 , we define dθe, d θ E : {0, 1, ∗}E × {0, 1, ∗}E → R + 0 such that for all x̃, x̃′ ∈ {0, 1, ∗}E :
dθe(x̃, x̃ ′) =  1 if e ∈",5.2. Metrics on Classes of Decompositions,[0],[0]
dom x̃ ∧ e ∈,5.2. Metrics on Classes of Decompositions,[0],[0]
dom x̃′ ∧,5.2. Metrics on Classes of Decompositions,[0],[0]
x̃e 6=,5.2. Metrics on Classes of Decompositions,[0],[0]
x̃′e 0,5.2. Metrics on Classes of Decompositions,[0],[0]
if e ∈ dom x̃ ∧ e ∈,5.2. Metrics on Classes of Decompositions,[0],[0]
dom x̃′ ∧ x̃e =,5.2. Metrics on Classes of Decompositions,[0],[0]
x̃′e 0,5.2. Metrics on Classes of Decompositions,[0],[0]
if e /∈,5.2. Metrics on Classes of Decompositions,[0],[0]
dom x̃,5.2. Metrics on Classes of Decompositions,[0],[0]
∧ e /∈,5.2. Metrics on Classes of Decompositions,[0],[0]
"dom x̃′
θ otherwise (21)
",5.2. Metrics on Classes of Decompositions,[0],[0]
"dθE(x̃, x̃ ′) = ∑ e′∈E dθe′(x̃, x̃ ′) .",5.2. Metrics on Classes of Decompositions,[0],[0]
"(22)
Theorem 6 For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any θ ∈",5.2. Metrics on Classes of Decompositions,[0],[0]
"[ 12 , 1], the function d̃θE′",5.2. Metrics on Classes of Decompositions,[0],[0]
: X̃GG′ ×,5.2. Metrics on Classes of Decompositions,[0],[0]
"X̃GG′ → R + 0 of the form
d̃θE′(x̃, x̃ ′)",5.2. Metrics on Classes of Decompositions,[0],[0]
":= dθE′(clGG′ x̃, clGG′ x̃ ′) (23)
is a pseudo-metric on X̃GG′ and a metric on X̂GG′ .",5.2. Metrics on Classes of Decompositions,[0],[0]
Moreover,5.2. Metrics on Classes of Decompositions,[0],[0]
", for any x̃, x̃′ ∈ X̃GG′ :
",5.2. Metrics on Classes of Decompositions,[0],[0]
X̃GG′,5.2. Metrics on Classes of Decompositions,[0],[0]
[x̃] =,5.2. Metrics on Classes of Decompositions,[0],[0]
X̃GG′,5.2. Metrics on Classes of Decompositions,[0],[0]
"[x̃ ′] ⇔ d̃θE′(x̃, x̃′) = 0 .",5.2. Metrics on Classes of Decompositions,[0],[0]
"(24)
",5.2. Metrics on Classes of Decompositions,[0],[0]
"By the one-to-one relation between decompositions and multicuts (Lemma 2), every partial characterization of a lifted multicut x̃ ∈",5.2. Metrics on Classes of Decompositions,[0],[0]
"X̃GG′ defines a class of decompositions of the graph G, namely those defined by the lifted multicuts characterized by XGG′",5.2. Metrics on Classes of Decompositions,[0],[0]
[x̃].,5.2. Metrics on Classes of Decompositions,[0],[0]
"By Theorem 6, d̃θE′ with θ ∈",5.2. Metrics on Classes of Decompositions,[0],[0]
"[ 12 , 1] well-defines a metric on these classes of decompositions and hence a means of comparing the classes of decompositions definable by must-join and must-cut constraints.",5.2. Metrics on Classes of Decompositions,[0],[0]
"Computing d̃θE′(x, x
′) involves computing the closures of x and x′ and is therefore NP-hard (by Theorem 4).",5.2. Metrics on Classes of Decompositions,[0],[0]
"As a third and final application of lifted multicuts, we turn to the optimization of graph decompositions by lifted multicuts of minimum cost.
",6. Polyhedral Optimization,[0],[0]
Definition 9,6. Polyhedral Optimization,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any c : E′",6. Polyhedral Optimization,[0],[0]
"→ R, the instance of the minimum cost lifted multicut problem w.r.t.",6. Polyhedral Optimization,[0],[0]
"G, G′ and c is the optimization problem
min {∑ e∈E′ cexe",6. Polyhedral Optimization,[0],[0]
∣∣∣∣∣ x ∈ XGG′ } .,6. Polyhedral Optimization,[0],[0]
"(25)
",6. Polyhedral Optimization,[0],[0]
"If E′ = E, (25) specializes to the minimum cost multicut problem w.r.t.",6. Polyhedral Optimization,[0],[0]
G′ and c that is also known as graph partition or correlation clustering.,6. Polyhedral Optimization,[0],[0]
If E′ ⊃,6. Polyhedral Optimization,[0],[0]
"E, the minimum cost lifted multicut problem w.r.t.",6. Polyhedral Optimization,[0],[0]
"G, G′ and c differs from the minimum cost multicut problem w.r.t.",6. Polyhedral Optimization,[0],[0]
G′ and c.,6. Polyhedral Optimization,[0],[0]
It has a smaller feasible set XGG′ ⊂,6. Polyhedral Optimization,[0],[0]
"XG′ , as we have shown in Section 3 and depicted for the smallest example in Fig. 2 and 3.",6. Polyhedral Optimization,[0],[0]
Unlike the minimum cost multicut problem w.r.t.,6. Polyhedral Optimization,[0],[0]
"G′ and c, the minimum cost lifted multicut problem w.r.t.",6. Polyhedral Optimization,[0],[0]
"G,G′ and c is such that any feasible solution x ∈ XGG′ indicates by xvw = 0 that the nodes v and w are connected in G by a path of edges labeled 0.",6. Polyhedral Optimization,[0],[0]
See also Fig. 5.,6. Polyhedral Optimization,[0],[0]
This property can be used to penalize by cvw > 0,6. Polyhedral Optimization,[0],[0]
precisely those decompositions of G for which v and w are in distinct components.,6. Polyhedral Optimization,[0],[0]
"For nodes v and w that are not neighbors in G, such costs are sometimes called non-local attractive.
",6. Polyhedral Optimization,[0],[0]
"To solve instances of the APX-hard minimum cost lifted multicut problem by means of a branch-and-cut algorithm, we study the geometry of lifted multicut polytopes.
",6. Polyhedral Optimization,[0],[0]
"Definition 10 (Deza & Laurent, 1997)",6. Polyhedral Optimization,[0],[0]
"For any graph G = (V,E), the convex hull ΞG := convXG of XG in RE is called the multicut polytope of G.
Definition 11",6. Polyhedral Optimization,[0],[0]
"For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, ΞGG′ := convXGG′ is called the lifted multicut polytope w.r.t.",6. Polyhedral Optimization,[0],[0]
"G and G′.
Examples are shown in Fig. 2 and 3, respectively.",6. Polyhedral Optimization,[0],[0]
"In general, the lifted multicut polytope ΞGG′ w.r.t.",6. Polyhedral Optimization,[0],[0]
graphs G and G′ (Fig. 3) is a subset of the multicut polytope ΞG′ of the graph G′ (Fig. 2).,6. Polyhedral Optimization,[0],[0]
"By Lemma 5, the system of cycle inequalities
(2) for G′ and cut inequalities (6) for G and G′ is redundant as a description ofXGG′ and thus of ΞGG′ .",6. Polyhedral Optimization,[0],[0]
"Below, we study the geometry of ΞGG′ .",6. Polyhedral Optimization,[0],[0]
"Theorem 7 For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, dim ΞGG′ = |E′|.
",6.1. Dimension,[0],[0]
We prove Theorem 7 by constructing |E′|+ 1 multicuts of G′ lifted from G whose characteristic functions are affine independent points.,6.1. Dimension,[0],[0]
"The strategy is to construct, for any e ∈ E′, an x ∈ XGG′ with xe = 0 and “as many ones as possible”.",6.1. Dimension,[0],[0]
The challenge is that edges cannot be labeled independently.,6.1. Dimension,[0],[0]
"In particular, for f ∈",6.1. Dimension,[0],[0]
"FGG′ , xf = 0 can imply, for certain f ′ ∈",6.1. Dimension,[0],[0]
FGG′,6.1. Dimension,[0],[0]
"\ {f}, that xf ′ = 0, as illustrated in Fig. 6.",6.1. Dimension,[0],[0]
"This structure is made explicit below, in Def. 12 and 13 and Lemmata 12 and 13.
",6.1. Dimension,[0],[0]
Definition 12,6.1. Dimension,[0],[0]
"For any connected graph G = (V,E) and any graph G′ = (V,E′) such that E ⊆ E′, the sequence (Fn)n∈N of subsets of FGG′ defined below is called the hierarchy of FGG′ with respect to G:
(a) F0 = ∅",6.1. Dimension,[0],[0]
"(b) For any n ∈ N and any {v, w} = f ∈",6.1. Dimension,[0],[0]
"FGG′ : {v,",6.1. Dimension,[0],[0]
"w} ∈ Fn iff there exists a vw-path in G such that, for any distinct nodes v′ and w′ in the path such that {v′, w′} 6= {v, w}, either {v′, w′} 6∈ FGG′ or there exists a natural number j < n",6.1. Dimension,[0],[0]
"such that {v′, w′} ∈ Fj .
",6.1. Dimension,[0],[0]
Lemma 12,6.1. Dimension,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any f ∈",6.1. Dimension,[0],[0]
"FGG′ , there exists an n ∈ N",6.1. Dimension,[0],[0]
such that f ∈ Fn.,6.1. Dimension,[0],[0]
Definition 13,6.1. Dimension,[0],[0]
"For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, the map ` : FGG′ → N such that ∀f ∈ FGG′∀n ∈ N : `(f) = n ⇔ f ∈ Fn",6.1. Dimension,[0],[0]
∧ f 6∈,6.1. Dimension,[0],[0]
Fn−1 is called the level function of FGG′ .,6.1. Dimension,[0],[0]
Lemma 13,6.1. Dimension,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and for any f ∈",6.1. Dimension,[0],[0]
"FGG′ , there exists an x ∈ XGG′ , called f -feasible, such that
(a) xf = 0 (b) xf ′",6.1. Dimension,[0],[0]
= 1 for all f ′ ∈,6.1. Dimension,[0],[0]
FGG′,6.1. Dimension,[0],[0]
\ {f} with `(f ′) ≥ `(f).,6.1. Dimension,[0],[0]
We characterize those edges e ∈ E′ for which the inequality xe ≤ 1 defines a facet of the lifted multicut polytope ΞGG′ .,6.2. Facets,[0],[0]
Theorem 8,6.2. Facets,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any e ∈ E′, the inequality xe ≤ 1 defines a facet of ΞGG′ iff there is no {v, w} = f ∈",6.2. Facets,[0],[0]
"FGG′ such that e connects a pair of v-w-cutvertices4.
4For any graph G = (V,E) and any v, w ∈ V , a v-w-cutvertex is a node u ∈ V that lies on every vw-path of G.
Next, we give conditions that contribute to identifying those edges e ∈ E′ for which the inequality 0 ≤ xe defines a facet of the lifted multicut polytope ΞGG′ .
",6.2. Facets,[0],[0]
Theorem 9,6.2. Facets,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′ and any e ∈ E′, the following assertions hold:",6.2. Facets,[0],[0]
"In case e ∈ E, the inequality 0 ≤ xe defines a facet of ΞGG′ iff there is no triangle in G′ containing e.",6.2. Facets,[0],[0]
In case uv = e ∈,6.2. Facets,[0],[0]
"FGG′ , the inequality 0 ≤ xe defines a facet of ΞGG′ only if the following necessary conditions hold:
(a) There is no triangle in G′ containing e. (b) The distance of any pair of u-v-cut-vertices except {u, v} is at least 3 in G′. (c)",6.2. Facets,[0],[0]
"There is no triangle of nodes s, s′, t in G′ such that {s, s′} is a u-v-separating node set and t is a u-v-cutvertex.
",6.2. Facets,[0],[0]
"Next, we characterize those inequalities of (4) and (5) that are facet-defining for ΞGG′ .",6.2. Facets,[0],[0]
Chopra & Rao (1993) have shown that an inequality of (2) defines a facet of the multicut polytope ΞG iff the cycle C is chordless.,6.2. Facets,[0],[0]
We establish a similar characterization of those inequalities of (4) and (5) that define a facet of the lifted multicut polytope ΞGG′ .,6.2. Facets,[0],[0]
"For clarity, we introduce some notation:",6.2. Facets,[0],[0]
"For any cycle C of G and any e ∈ C, let
SGG′(e, C) := x ∈ XGG′ ∣∣∣∣∣∣xe = ∑ e′∈C\{e} xe′  (26) ΣGG′(e, C) := convSGG′(e, C) .",6.2. Facets,[0],[0]
"(27)
",6.2. Facets,[0],[0]
For any vw = f ∈,6.2. Facets,[0],[0]
"FGG′ and any vw-path P in G, let
SGG′(f, P )",6.2. Facets,[0],[0]
":= { x ∈ XGG′ ∣∣∣∣∣xvw = ∑ e∈P xe } (28)
ΣGG′(f, P ) := convSGG′(f, P ) .",6.2. Facets,[0],[0]
"(29)
Theorem 10 For any connected graph G = (V,E) and any graph G′ = (V,E′) with E ⊆ E′, the following assertions hold:
(a) For any cycle C in G and any e ∈ C, the polytope ΣGG′(e, C) is a facet of ΞGG′ iff C is chordless in G′. (b) For any edge vw = f ∈",6.2. Facets,[0],[0]
"FGG′ and any vw-path P in G, the polytope ΣGG′(f, P ) is a facet of ΞGG′ iff P ∪ {f} is chordless in G′.
Inequalities defined by cycles in G′ that contain more than one edge from the set FGG′ do not occur in (4) or (5).",6.2. Facets,[0],[0]
They are valid for ΞGG′ as they are valid for ΞG′ ⊇ ΞGG′ .,6.2. Facets,[0],[0]
They define a (non-trivial) facet of ΞGG′ only if the cycle is chordless (as chordal cycles are not even facet-defining for ΞG′).,6.2. Facets,[0],[0]
"At the same time, chordlessness is not a sufficient condition for facet-definingness of non-trivial cycles.",6.2. Facets,[0],[0]
"For example, in Fig. 6a, the cycle inequality",6.2. Facets,[0],[0]
xf2 ≤ xf3,6.2. Facets,[0],[0]
+xv1v2 is dominated by the (non-trivial) valid inequality,6.2. Facets,[0],[0]
"xf2 ≤ xf3 .
",6.2. Facets,[0],[0]
"Next, we consider the cut inequalities (6).",6.2. Facets,[0],[0]
Examples of cuts that are not facet-defining for ΞGG′ are shown in Fig. 4 in the appendix.,6.2. Facets,[0],[0]
"To constrain the class of cuts that are facet-defining, we introduce additional notation: For any connected graph G = (V,E), any distinct nodes v, w ∈ V and any C ∈ vw-cuts(G), we denote by
G(v, C) = (V (v, C), E(v, C))",6.2. Facets,[0],[0]
"(30) G(w,C) =",6.2. Facets,[0],[0]
"(V (w,C), E(w,C))",6.2. Facets,[0],[0]
"(31)
the largest components of the graph (V,E \ C) that contain v and w, respectively.",6.2. Facets,[0],[0]
"By definition of a vw-cut5, we have
V (v, C) ∩ V (w,C) = ∅",6.2. Facets,[0],[0]
"(32) ∧ V (v, C) ∪ V (w,C) = V .",6.2. Facets,[0],[0]
"(33)
We denote by FGG′(vw,C) the set of those edges in FGG′ , except vw, that cross the vw-cut C of G, i.e.
FGG′(vw,C) := {f ∈",6.2. Facets,[0],[0]
"FGG′ \ {vw} | f 6⊆ V (v, C)∧ f 6⊆ V (w,C)} .",6.2. Facets,[0],[0]
"(34)
We denote by G′(vw,C) := (V, FGG′(vw,C) ∪ C) the subgraph of G′ that comprises all edges from FGG′(vw,C) and C. Finally, we define
SGG′(vw,C) := { x ∈ XGG′ ∣∣∣∣∣1− xvw = ∑ e∈C (1− xe) } (35)
ΣGG′(vw,C) := convSGG′(vw,C) .",6.2. Facets,[0],[0]
"(36)
Definition 14 For any connected graph G = (V,E), any distinct v, w ∈ V and any C ∈ vw-cuts(G), a component (V ∗, E∗) of G is called properly (vw,C)-connected iff
v ∈ V ∗ ∧ w ∈ V ∗ ∧ |E∗ ∩ C| = 1 .",6.2. Facets,[0],[0]
"(37) 5For any graph G = (V,E) and any distinct nodes v, w ∈ V , a vw-cut of G is a minimal (with respect to inclusion) set C ⊆ E such that v and w are not connected in (V,E \ C).
",6.2. Facets,[0],[0]
"It is called improperly (vw,C)-connected iff
V ∗ ⊆ V (v, C) ∨ V ∗ ⊆ V (w,C) .",6.2. Facets,[0],[0]
"(38)
It is called (vw,C)-connected iff it is properly or improperly (vw,C)-connected.
",6.2. Facets,[0],[0]
"For any (vw,C)-connected component (V ∗, E∗) of G, we denote by FV ∗ := {v′w′ = f ′ ∈ FGG′(vw,C)",6.2. Facets,[0],[0]
"| v′ ∈ V ∗∧ w′ ∈ V ∗} the set of those edges v′w′ = f ′ ∈ FGG′(vw,C) such that (V ∗, E∗) is also (v′w′, C)-connected.
Theorem 11",6.2. Facets,[0],[0]
"For any connected graph G = (V,E), any graph G′ = (V,E′) with E ⊆ E′, any vw = f ∈",6.2. Facets,[0],[0]
"FGG′ and any C ∈ vw-cuts(G), ΣGG′(vw,C) is a facet of ΞGG′ only if the following necessary conditions hold:
C1 For any e ∈ C, there exists a (vw,C)-connected component (V ∗, E∗) of G such that e ∈ E∗.
C2 For any ∅ 6=",6.2. Facets,[0],[0]
"F ⊆ FGG′(vw,C), there exists an edge e ∈ C and (vw,C)-connected components (V ∗, E∗) and (V ∗∗, E∗∗) of G such that e ∈ E∗",6.2. Facets,[0],[0]
"and e ∈ E∗∗ and |F ∩ FV ∗ | 6= |F ∩ FV ∗∗ |.
",6.2. Facets,[0],[0]
"C3 For any f ′ ∈ FGG′(vw,C), any ∅ 6=",6.2. Facets,[0],[0]
"F ⊆ FGG′(vw,C) \ {f ′} and any k ∈ N, there exist (vw,C)-connected components (V ∗, E∗) and (V ∗∗, E∗∗) with f ′ ∈ FV ∗ and f ′ /∈",6.2. Facets,[0],[0]
"FV ∗∗ such that
|F ∩ FV ∗ |",6.2. Facets,[0],[0]
6= k or |F ∩ FV ∗∗ | 6= 0 .,6.2. Facets,[0],[0]
"(39)
C4 For any v′ ∈ V (v, C), any w′ ∈ V (w,C) and any v′w′-path P = (VP , EP ) in G′(vw,C), there exists a properly (vw,C)-connected component (V ∗, E∗) of G such that
(v′ /∈",6.2. Facets,[0],[0]
"V ∗ ∨ ∃w′′ ∈ VP ∩ V (w,C) :",6.2. Facets,[0],[0]
w′′ /∈ V ∗) ∧ (w′ /∈,6.2. Facets,[0],[0]
V ∗ ∨ ∃v′′,6.2. Facets,[0],[0]
∈,6.2. Facets,[0],[0]
"VP ∩ V (v, C) : v′′ /∈ V ∗) .
",6.2. Facets,[0],[0]
"(40)
C5 For any cycle Y = (VY , EY ) in G′(vw,C), there exists a properly (vw,C)-connected component (V ∗, E∗) of G such that
(∃v′ ∈ VY ∩ V (v, C) : v′ /∈",6.2. Facets,[0],[0]
"V ∗) ∧ (∃w′ ∈ VY ∩ V (w,C) : w′ /∈",6.2. Facets,[0],[0]
V ∗) .,6.2. Facets,[0],[0]
(41),6.2. Facets,[0],[0]
"To study the relevance of geometric properties established above, we compare two separation procedures, α and β, for lifted multicut polytopes.",6.3. Algorithms,[0],[0]
We implement these for the branch-and-cut algorithm in the software Gurobi.,6.3. Algorithms,[0],[0]
Our code is available at https://github.com/bjoern-andres/graph.,6.3. Algorithms,[0],[0]
"The
procedure α is canonical and serves as a reference.",6.3. Algorithms,[0],[0]
It separates infeasible points by any of the inequalities (4)–(6).,6.3. Algorithms,[0],[0]
Violated inequalities of (4) and (5) are found by searching for shortest chordless paths.,6.3. Algorithms,[0],[0]
Violated inequalities of (5) are found by searching for minimum vw-cuts.,6.3. Algorithms,[0],[0]
The procedure β is less canonical:,6.3. Algorithms,[0],[0]
It separates infeasible points by some cycle inequalities w.r.t.,6.3. Algorithms,[0],[0]
G′ (cf. Theorem 10) and by cut inequalities (6).,6.3. Algorithms,[0],[0]
"Violated cycle inequalities of G′ are found by first seaching for paths and cycles as before but then replacing sub-paths by chords in G′. Violated cut-inequalities are found as before but added to the problem only conditionally: For each violated inequality of (6) that we find and the corresponding {v, w} ∈ FGG′ and C ∈ vw-cuts(G), we search for a vw-path P in G′ such that one of the cycle inequalities for the cycle formed by P and {v, w} is violated.",6.3. Algorithms,[0],[0]
"If it exists, only the cycle inequality is added.",6.3. Algorithms,[0],[0]
"Otherwise, the cut-inequality is added.",6.3. Algorithms,[0],[0]
"The advantage of β over α can be seen in Fig. 7 for an instance of the min cost lifted multicut problem by Keuper et al. (2015) with |V | = 126, |E| = 229 and |E′| = 1860.",6.3. Algorithms,[0],[0]
"By studying the set of all decompositions (clusterings) of a graph through its characterization as a set of lifted multicuts, we have gained three insights: 1.",7. Conclusion,[0],[0]
"Toward the definition of classes of decompositions by must-join and must-cut constraints, we have seen that consistency and maximal specificity are NP-hard to decide.",7. Conclusion,[0],[0]
2.,7. Conclusion,[0],[0]
"Toward the comparison of decompositions by metrics, we have defined a generalization of Rand’s metric and the boundary metric that enables more detailed analyses of how two decompositions of the same graph differ.",7. Conclusion,[0],[0]
This metric extends to classes of decompositions definable by must-join and must-cut constraints for which it is NP-hard to compute.,7. Conclusion,[0],[0]
3.,7. Conclusion,[0],[0]
"Toward the optimization of graph decompositions by minimum cost lifted multicuts, we have established some properties of some facets of lifted multicut polytopes.",7. Conclusion,[0],[0]
These properties have led us to efficient separation procedures and a branch-and-cut algorithm for the minimum cost lifted multicut problem.,7. Conclusion,[0],[0]
We study the set of all decompositions (clusterings) of a graph through its characterization as a set of lifted multicuts.,abstractText,[0],[0]
This leads us to practically relevant insights related to the definition of classes of decompositions by must-join and must-cut constraints and related to the comparison of clusterings by metrics.,abstractText,[0],[0]
"To find optimal decompositions defined by minimum cost lifted multicuts, we establish some properties of some facets of lifted multicut polytopes, define efficient separation procedures and apply these in a branchand-cut algorithm.",abstractText,[0],[0]
Analysis and Optimization of Graph Decompositions by Lifted Multicuts,title,[0],[0]
Neural networks have achieved state-of-the-art accuracy on many machine learning tasks.,1. Introduction,[0],[0]
"AlexNet (Krizhevsky et al., 2012) had a deep impact a few years ago in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and triggered intensive research efforts on deep neural networks.",1. Introduction,[0],[0]
"Recently, ResNet (He et al., 2016) has outperformed humans in recognition tasks.
",1. Introduction,[0],[0]
These networks have very high computational complexity.,1. Introduction,[0],[0]
"For instance, AlexNet has 60 million parameters and 650,000 neurons (Krizhevsky et al., 2012).",1. Introduction,[0],[0]
"Its convolutional layers alone require 666 million multiply-accumulates (MACs) per 227 × 227 image (13k MACs/pixel) and 2.3 million weights (Chen et al., 2016).",1. Introduction,[0],[0]
"Deepface’s network involves more than 120 million parameters (Taigman et al., 2014).",1. Introduction,[0],[0]
"ResNet is a 152-layer
The authors are with the University of Illinois at UrbanaChampaign, 1308 W Main St., Urabna, IL 61801 USA.",1. Introduction,[0],[0]
Correspondence to: Charbel Sakr,1. Introduction,[0],[0]
"<sakr2@illinois.edu>, Yongjune Kim <yongjune@illinois.edu>, Naresh Shanbhag <shanbhag@illinois.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
deep residual network.,1. Introduction,[0],[0]
This high complexity of deep neural networks prevents its deployment on energy and resource-constrained platforms such as mobile devices and autonomous platforms.,1. Introduction,[0],[0]
One of the most effective approaches for reducing resource utilization is to implement fixed-point neural networks.,1.1. Related Work,[0],[0]
"As mentioned in (Lin et al., 2016a), there are two approaches for designing fixed-point neural networks: (1) directly train a fixed-point neural network, and (2) quantize a pre-trained floating-point neural network to obtain a fixed-point network.
",1.1. Related Work,[0],[0]
"As an example of fixed-point training, Gupta et al. (2015) showed that 16-bit fixed-point representation incurs little accuracy degradation by using stochastic rounding.",1.1. Related Work,[0],[0]
A more aggressive approach is to design binary networks such as Kim & Smaragdis (2016) which used bitwise operations to replace the arithmetic operations and Rastegari et al. (2016) which explored optimal binarization schemes.,1.1. Related Work,[0],[0]
"BinaryConnect (Courbariaux et al., 2015) trained networks using binary weights while BinaryNet (Hubara et al., 2016b) trained networks with binary weights and activations.
",1.1. Related Work,[0],[0]
"Although these fixed-point training approaches make it possible to design fixed-point neural networks achieving excellent accuracy, training based on fixed-point arithmetic is generally harder than floating-point training since the optimization is done in a discrete space.
",1.1. Related Work,[0],[0]
"Hence, in this paper, we focus on the second approach that quantizes a pre-trained floating-pointing network to a fixed-point network.",1.1. Related Work,[0],[0]
"This approach leverages the extensive work in training state-of-the-art floating-point neural networks such as dropout (Srivastava et al., 2014), maxout (Goodfellow et al., 2013), network-in-network (Lin et al., 2013), and residual learning (He et al., 2016) to name a few.",1.1. Related Work,[0],[0]
"In this approach, proper precision needs to be determined after training to reduce complexity while minimizing the accuracy loss.",1.1. Related Work,[0],[0]
"In (Hwang & Sung, 2014), exhaustive search is performed to determine a suitable precision allocation.",1.1. Related Work,[0],[0]
"Recently, Lin et al. (2016a) offered an analytical solution for non-uniform bit precision based on the signalto-quantization-noise ratio (SQNR).",1.1. Related Work,[0],[0]
"However, the use of non-uniform quantization step sizes at each layer is diffi-
cult to implement as it requires multiple variable precision arithmetic units.
",1.1. Related Work,[0],[0]
"In addition to fixed-point implementation, many approaches have been proposed to lower the complexity of deep neural networks in terms of the number of arithmetic operations.",1.1. Related Work,[0],[0]
"Han et al. (2015) employs a three-step training method to identify important connections, prune the unimportant ones, and retrain on the pruned network.",1.1. Related Work,[0],[0]
Zhang et al. (2015) replaces the original convolutional layers by smaller sequential layers to reduce computations.,1.1. Related Work,[0],[0]
"These approaches are complementary to our technique of quantizing a pre-trained floating-point neural network into a fixedpoint one.
",1.1. Related Work,[0],[0]
"In this paper, we obtain analytical bounds on the accuracy of fixed-point networks that are obtained by quantizing a conventionally trained floating-point network.",1.1. Related Work,[0],[0]
"Furthermore, by defining meaningful measures of a fixed-point network’s hardware complexity viz. computational and representational costs, we develop a principled approach to precision assignment using these bounds in order to minimize these complexity measures.",1.1. Related Work,[0],[0]
Our contributions are both theoretical and practical.,1.2. Contributions,[0],[0]
"We summarize our main contributions:
• We derive theoretical bounds on the misclassification rate in presence of limited precision and thus determine analytically how accuracy and precision tradeoff with each other.",1.2. Contributions,[0],[0]
•,1.2. Contributions,[0],[0]
"Employing the theoretical bounds and the backpropagation algorithm, we show that proper precision assignments can be readily determined while maintaining accuracy close to floating-point networks.",1.2. Contributions,[0],[0]
"• We analytically determine which of weights or activations need more precision, and we show that typically the precision requirements of weights are greater than those of activations for fully-connected networks and are similar to each other for networks with shared weights such as convolutional neural networks.",1.2. Contributions,[0],[0]
• We introduce computational and representational costs as meaningful metrics to evaluate the complexity of neural networks under fixed precision assignment.,1.2. Contributions,[0],[0]
"• We validate our findings on the MNIST and CIFAR10 datasets demonstrating the ease with which fixedpoint networks with complexity smaller than stateof-the-art binary networks can be derived from pretrained floating-point networks with minimal loss in accuracy.
",1.2. Contributions,[0],[0]
It is worth mentioning that our proposed method is general and can be applied to every class of neural networks such as multilayer perceptrons and convolutional neural networks.,1.2. Contributions,[0],[0]
"For a given floating-point neural network and its fixedpoint counterpart we define: 1) the floating-point error probability pe,fl = Pr{Ŷfl 6= Y } where Ŷfl is the output of the floating-point network and Y is the true label; 2) the fixed-point error probability pe,fx = Pr{Ŷfx 6=",2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
Y } where Ŷfx is the output of the fixed-point network; 3) the mismatch probability between fixed-point and floating-point pm =,2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
Pr{Ŷfx 6= Ŷfl}.,2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
"Observe that:
pe,fx ≤ pe,fl + pm (1)
",2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
The right-hand-side represents the worst case of having no overlap between misclassified samples and samples whose predicted labels are in error due to quantization.,2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
We provide a formal proof of (1) in the supplementary section.,2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
"Note that pe,fx is a quantity of interest as it characterizes the accuracy of the fixed-point system.",2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
"We employ pm as a proxy to pe,fx because it brings in the effects of quantization into the picture as opposed to pe,fl which solely depends on the algorithm.",2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
"This observation was made in (Sakr et al., 2017) and allowed for an analytical characterization of linear classifiers as a function of precision.",2.1. Accuracy of Fixed-Point and Floating-Point Networks,[0],[0]
"The study of fixed-point systems and algorithms is well established in the context of signal processing and communication systems (Shanbhag, 2016).",2.2. Fixed-Point Quantization,[0],[0]
"A popular example is the least mean square (LMS) algorithm for which bounds on precision requirements for input, weights, and updates have been derived (Goel & Shanbhag, 1998).",2.2. Fixed-Point Quantization,[0],[0]
"In such analyses, it is standard practice (Caraiscos & Liu, 1984) to assume all signed quantities lie in [−1, 1] and all unsigned quantities lie in [0, 2].",2.2. Fixed-Point Quantization,[0],[0]
"Of course, activations and weights can be designed to satisfy this assumption during training.",2.2. Fixed-Point Quantization,[0],[0]
"A Bbit fixed-point number afx would be related to its floatingpoint value a as follows:
afx = a + qa (2)
where qa is the quantization noise which is modeled as an independent uniform random variable distributed over[ −∆2 , ∆ 2 ] , where ∆ = 2−(B−1) is the quantization step (Caraiscos & Liu, 1984).",2.2. Fixed-Point Quantization,[0],[0]
We argue that the complexity of a fixed-point system has two aspects: computational and representational costs.,2.3. Complexity in Fixed-Point,[0],[0]
"In what follows, we consider activations and weights to be quantized to BA and BW bits, respectively.
",2.3. Complexity in Fixed-Point,[0],[0]
"The computational cost is a measure of the computational resources utilized for generating a single decision, and is defined as the number of 1 bit full adders (FAs).",2.3. Complexity in Fixed-Point,[0],[0]
A full adder is a canonical building block of arithmetic units.,2.3. Complexity in Fixed-Point,[0],[0]
"We assume arithmetic operations are executed using the commonly used ripple carry adder (Knauer, 1989) and BaughWooley multiplier (",2.3. Complexity in Fixed-Point,[0],[0]
"Baugh & Wooley, 1973) architectures designed using FAs.",2.3. Complexity in Fixed-Point,[0],[0]
"Consequently, the number of FAs used to compute a D-dimensional dot product of activations and weights is (Lin et al., 2016b):
DBABW + (D − 1)(BA + BW + dlog2(D)e",2.3. Complexity in Fixed-Point,[0],[0]
"− 1) (3)
Hence, an important aspect of the computational cost of a dot product is that it is an increasing function of the product of activation precision (BA), weight precision (BW ), and dimension (D).
",2.3. Complexity in Fixed-Point,[0],[0]
"We define the representational cost as the total number of bits needed to represent all network parameters, i.e., both activations and weights.",2.3. Complexity in Fixed-Point,[0],[0]
This cost is a measure of the storage complexity and communications costs associated with data movement.,2.3. Complexity in Fixed-Point,[0],[0]
"The total representational cost of a fixedpoint network is:
|A|BA + |W|BW (4)
bits, where A and W are the index sets of all activations and weights in the network, respectively.",2.3. Complexity in Fixed-Point,[0],[0]
"Observe that the representational cost is linear in activation and weight precisions as compared to the computational cost.
",2.3. Complexity in Fixed-Point,[0],[0]
"Equations (3) - (4) illustrate that, though computational and representational costs are not independent, they are different.",2.3. Complexity in Fixed-Point,[0],[0]
"Together, they describe the implementation costs associated with a network.",2.3. Complexity in Fixed-Point,[0],[0]
We shall employ both when evaluating the complexity of fixed-point networks.,2.3. Complexity in Fixed-Point,[0],[0]
"Here, we establish notation.",2.4. Setup,[0],[0]
Let us consider neural networks deployed on a M -class classification task.,2.4. Setup,[0],[0]
"For a given input, the network would typically have M numerical outputs {zi}Mi=1 and the decision would be ŷ = arg max
i=1,...,M zi.",2.4. Setup,[0],[0]
"Each numerical output is a function of
weights and activations in the network:
zi = f ({ah}h∈A, {wh}h∈W) (5)
for i = 1, . . .",2.4. Setup,[0],[0]
",M , where ah denotes the activation indexed by h and wh denotes the weight indexed by h.",2.4. Setup,[0],[0]
"When activations and weights are quantized to BA and BW bits, respectively, the output zi is corrupted by quantization noise qzi so that:
zi + qzi = f",2.4. Setup,[0],[0]
"({ah + qah}h∈A, {wh + qwh}h∈W) (6)
where qah and qwh are the quantization noise terms of the activation ah and weight wh, respectively.",2.4. Setup,[0],[0]
"Here, {qah}h∈A are independent uniformly distributed random variables on [ −∆A2 , ∆A 2 ] and {qwh}h∈W are independent uniformly
distributed random variables on [ −∆W2 , ∆W 2 ] , with ∆A = 2−(BA−1) and ∆W = 2−(BW−1).
",2.4. Setup,[0],[0]
"In quantization noise analysis, it is standard to ignore crossproducts of quantization noise terms as their contribution is negligible.",2.4. Setup,[0],[0]
"Therefore, using Taylor’s theorem, we express the total quantization noise at the output of the fixed-point network as:
qzi = ∑ h∈A qah ∂zi ∂ah + ∑ h∈W qwh ∂zi ∂wh .",2.4. Setup,[0],[0]
"(7)
Note that the derivatives in (7) are obtained as part of the back-propagation algorithm.",2.4. Setup,[0],[0]
"Thus, using our results, it is possible to estimate the precision requirements of deep neural networks during training itself.",2.4. Setup,[0],[0]
"As will be shown later, this requires one additional back-propagation iteration to be executed after the weights have converged.",2.4. Setup,[0],[0]
We present our first result.,3.1. Second Order Bound,[0],[0]
"It is an analytical upper bound on the mismatch probability pm between a fixed-point neural network and its floating-point counterpart.
",3.1. Second Order Bound,[0],[0]
Theorem 1.,3.1. Second Order Bound,[0],[0]
"Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point counterpart is upper bounded as follows:
pm ≤",3.1. Second Order Bound,[0],[0]
"∆2A 24 E  M∑ i=1
",3.1. Second Order Bound,[0],[0]
"i 6=Ŷfl
∑ h∈A ∣∣∣∣∂(Zi−ZŶfl )∂Ah ∣∣∣∣2
|Zi − ZŶfl | 2

+ ∆2W 24 E  M∑ i=1
",3.1. Second Order Bound,[0],[0]
"i 6=Ŷfl
∑ h∈W ∣∣∣∣∂(Zi−ZŶfl )∂wh ∣∣∣∣2
|Zi − ZŶfl | 2  (8) where expectations are taken over a random input and {Ah}h∈A, {Zi}Mi=1, and Ŷfl are thus random variables.
",3.1. Second Order Bound,[0],[0]
Proof.,3.1. Second Order Bound,[0],[0]
The detailed proof can be found in the supplementary section.,3.1. Second Order Bound,[0],[0]
"Here, we provide the main idea and the intuition behind the proof.",3.1. Second Order Bound,[0],[0]
The heart of the proof lies in evaluating Pr ( zi + qzi > zj + qzj ) for any pair of outputs zi and zj where zj > zi.,3.1. Second Order Bound,[0],[0]
"Equivalently, we need to evaluate
Pr ( qzi − qzj > zj − zi ) .",3.1. Second Order Bound,[0],[0]
"But from (7), we have:
qzi − qzj = ∑ h∈A qah ∂(zi",3.1. Second Order Bound,[0],[0]
"− zj) ∂ah + ∑ h∈W qwh ∂(zi − zj) ∂wh .
",3.1. Second Order Bound,[0],[0]
"(9)
In (9), we have a linear combination of quantization noise terms, qzi",3.1. Second Order Bound,[0],[0]
− qzj is a zero mean random variable having a symmetric distribution.,3.1. Second Order Bound,[0],[0]
This means that Pr ( qzi − qzj > zj − zi ),3.1. Second Order Bound,[0],[0]
= 1 2 Pr ( |qzi − qzj | > |zj,3.1. Second Order Bound,[0],[0]
"− zi| ) , which allows us to use Chebyshev’s inequality.",3.1. Second Order Bound,[0],[0]
"Indeed, from (9), the variance of qzi − qzj is given by:
∆2A 12 ∑ h∈A",3.1. Second Order Bound,[0],[0]
∣∣∣∣∂(zi − zj)∂ah ∣∣∣∣2 + ∆2W12 ∑ h∈W,3.1. Second Order Bound,[0],[0]
"∣∣∣∣∂(zi − zj)∂wh ∣∣∣∣2 ,
so that Pr ( zi + qzi > zj + qzj ) ≤",3.1. Second Order Bound,[0],[0]
∆2A ∑ h∈A ∣∣∣∂(zi−zj)∂ah ∣∣∣2 + ∆2W ∑h∈W ∣∣∣∂(zi−zj)∂wh,3.1. Second Order Bound,[0],[0]
"∣∣∣2 24 |zi − zj |2 .
(10)
",3.1. Second Order Bound,[0],[0]
"As explained in the supplementary section, it is possible to obtain to (8) from (10) using standard probabilistic arguments.
",3.1. Second Order Bound,[0],[0]
"Before proceeding, we point out that the two expectations in (8) are taken over a random input but the weights {wh}h∈W are frozen after training and are hence deterministic.
",3.1. Second Order Bound,[0],[0]
Several observations are to be made.,3.1. Second Order Bound,[0],[0]
First notice that the mismatch probability pm increases with ∆2A and ∆ 2 W .,3.1. Second Order Bound,[0],[0]
This is to be expected as smaller precision leads to more mismatch.,3.1. Second Order Bound,[0],[0]
"Theorem 1 says a little bit more: the mismatch probability decreases exponentially with precision, because ∆A = 2 −(BA−1) and ∆W = 2−(BW−1).
",3.1. Second Order Bound,[0],[0]
Note that the quantities in the expectations in (8) can be obtained as part of a standard back-propagation procedure.,3.1. Second Order Bound,[0],[0]
"Indeed, once the weights are frozen, it is enough to perform one forward pass on an estimation set (which should have statistically significant cardinality), record the numerical outputs, perform one backward pass and probe all relevant derivatives.",3.1. Second Order Bound,[0],[0]
"Thus, (8) can be readily computed.
",3.1. Second Order Bound,[0],[0]
Another practical aspect of Theorem 1 is that this operation needs to be done only once as these quantities do not depend on precision.,3.1. Second Order Bound,[0],[0]
"Once they are determined, for any given precision assignment, we simply evaluate (8) and combine it with (1) to obtain an estimate (upper bound) on the accuracy of the fixed-point instance.",3.1. Second Order Bound,[0],[0]
This way the precision necessary to achieve a specific mismatch probability is obtained from a trained floating-point network.,3.1. Second Order Bound,[0],[0]
"This
clearly highlights the gains in practicality of our analytical approach over a trial-and-error based search.
",3.1. Second Order Bound,[0],[0]
"Finally, (8) reveals a very interesting aspect of the trade-off between activation precision BA and weight precision BW .",3.1. Second Order Bound,[0],[0]
"We rewrite (8) as:
pm ≤ ∆2AEA + ∆2WEW",3.1. Second Order Bound,[0],[0]
"(11)
where
EA = E  M∑ i=1
",3.1. Second Order Bound,[0],[0]
"i 6=Ŷfl
∑ h∈A ∣∣∣∣∂(Zi−ZŶfl )",3.1. Second Order Bound,[0],[0]
"∂Ah ∣∣∣∣2
24|Zi − ZŶfl | 2  and
EW = E  M∑ i=1
",3.1. Second Order Bound,[0],[0]
"i6=Ŷfl
∑ h∈W ∣∣∣∣∂(Zi−ZŶfl )∂wh ∣∣∣∣2
24|Zi − ZŶfl | 2  .",3.1. Second Order Bound,[0],[0]
The first term in (11) characterizes the impact of quantizing activations on the overall accuracy while the second characterizes that of weight quantization.,3.1. Second Order Bound,[0],[0]
It might be the case that one of the two terms dominates the sum depending on the values of EA and EW .,3.1. Second Order Bound,[0],[0]
This means that either the activations or the weights are assigned more precision than necessary.,3.1. Second Order Bound,[0],[0]
An intuitive first step to efficiently get a smaller upper bound is to make the two terms of comparable order.,3.1. Second Order Bound,[0],[0]
"That can be made by setting ∆2AEA = ∆ 2 WEW which is equivalent to
BA −BW = round ( log2 √ EA EW ) (12)
where round() denotes the rounding operation.",3.1. Second Order Bound,[0],[0]
"This is an effective way of taking care of one of the two degrees of freedom introduced by (8).
",3.1. Second Order Bound,[0],[0]
A natural question to ask would be which of EA and EW is typically larger.,3.1. Second Order Bound,[0],[0]
"That is to say, to whom, activations or weights, should one assign more precision.",3.1. Second Order Bound,[0],[0]
"In deep neural networks, there are more weights than activations, a trend particularly observed in deep networks with most layers fully connected.",3.1. Second Order Bound,[0],[0]
"This trend, though not as pronounced, is also observed in networks with shared weights, such as convolutional neural networks.",3.1. Second Order Bound,[0],[0]
"However, there exist a few counterexamples such as the networks in (Hubara et al., 2016b) and (Hubara et al., 2016a).",3.1. Second Order Bound,[0],[0]
"It is thus reasonable to expect EW ≥ EA, and consequently the precision requirements of weights will, in general, be more than those of activations.
",3.1. Second Order Bound,[0],[0]
"One way to interpret (11) is to consider minimizing the upper bound in (8) subject to BA+BW = c for some constant
c.",3.1. Second Order Bound,[0],[0]
"Indeed, it can be shown that (12) would be a necessary condition of the corresponding solution.",3.1. Second Order Bound,[0],[0]
This is an application of the arithmetic-geometric mean inequality.,3.1. Second Order Bound,[0],[0]
"Effectively, (11) is of particular interest when considering computational cost which increases as a function of the product of both precisions (see Section 2.3).",3.1. Second Order Bound,[0],[0]
"We present a tighter upper bound on pm based on the Chernoff bound.
",3.2. Tighter Bound,[0],[0]
Theorem 2.,3.2. Tighter Bound,[0],[0]
"Given BA and BW , the mismatch probability pm between a fixed-point network and its floating-point counterpart is upper bounded as follows:
pm ≤",3.2. Tighter Bound,[0],[0]
"E  M∑ i=1
",3.2. Tighter Bound,[0],[0]
"i6=Ŷfl
e−S (i,Ŷfl) P (i,Ŷfl) 1 P (i,Ŷfl) 2  (13)",3.2. Tighter Bound,[0],[0]
"where, for i 6= j,
S(i,j) = 3(Zi − Zj)2∑
h∈A
( D
(i,j)",3.2. Tighter Bound,[0],[0]
"Ah
)2 + ∑
h∈W
( D
(i,j) wh )2 , D
(i,j)",3.2. Tighter Bound,[0],[0]
Ah =,3.2. Tighter Bound,[0],[0]
∆A 2 ∂(Zi,3.2. Tighter Bound,[0],[0]
"− Zj) ∂Ah , D(i,j)wh = ∆W 2 ∂(Zi",3.2. Tighter Bound,[0],[0]
"− Zj) ∂wh ,
P (i,j) 1 = ∏ h∈A
sinh ( T (i,j)D
(i,j) Ah ) T (i,j)D
",3.2. Tighter Bound,[0],[0]
"(i,j)",3.2. Tighter Bound,[0],[0]
"Ah
,
P (i,j) 2 = ∏ h∈W
sinh ( T (i,j)D
(i,j) wh )",3.2. Tighter Bound,[0],[0]
"T (i,j)D
",3.2. Tighter Bound,[0],[0]
"(i,j) wh
,
and
T (i,j) = S(i,j)
",3.2. Tighter Bound,[0],[0]
"Zj − Zi .
",3.2. Tighter Bound,[0],[0]
Proof.,3.2. Tighter Bound,[0],[0]
"Again, we leave the technical details for the supplementary section.",3.2. Tighter Bound,[0],[0]
Here we also provide the main idea and intuition.,3.2. Tighter Bound,[0],[0]
"As in Theorem 1, we shall focus on evaluating Pr ( zi + qzi > zj + qzj )",3.2. Tighter Bound,[0],[0]
= Pr ( qzi − qzj >,3.2. Tighter Bound,[0],[0]
zj − zi ) for any pair of outputs zi and zj where zj > zi.,3.2. Tighter Bound,[0],[0]
The key difference here is that we will use the Chernoff bound in order to account for the complete quantization noise statistics.,3.2. Tighter Bound,[0],[0]
"Indeed, letting v = zj − zi, we have:
Pr ( qzi − qzj > v ) ≤",3.2. Tighter Bound,[0],[0]
e−tvE [ et(qzi−qzj ) ],3.2. Tighter Bound,[0],[0]
for any t > 0.,3.2. Tighter Bound,[0],[0]
"We show that:
E [ et(qzi−qzj ) ]",3.2. Tighter Bound,[0],[0]
"= ∏ h∈A sinh (tda,h) tda,h ∏",3.2. Tighter Bound,[0],[0]
h∈W,3.2. Tighter Bound,[0],[0]
"sinh (tdw,h) tdw,h
where da,h = ∆A2 ∂(zi−zj)",3.2. Tighter Bound,[0],[0]
"∂ah and dw,h = ∆W2 ∂(zi−zj) ∂wh
.",3.2. Tighter Bound,[0],[0]
"This yields:
Pr ( qzi − qzj > v ) ≤",3.2. Tighter Bound,[0],[0]
"e−tv
∏ h∈A sinh (tda,h) tda,h ∏",3.2. Tighter Bound,[0],[0]
"h∈W sinh (tdw,h) tdw,h .",3.2. Tighter Bound,[0],[0]
"(14)
We show that the right-hand-side is minimized over positive values of t when:
t = 3v∑
h∈A (da,h) 2 + ∑ h∈W (dw,h) 2 .
",3.2. Tighter Bound,[0],[0]
"Substituting this value of t into (14) and using standard probabilistic arguments, we obtain (13).
",3.2. Tighter Bound,[0],[0]
"The first observation to be made is that Theorem 2 indicates that, on average, pm is upper bounded by an exponentially decaying function of the quantity S(i,Ŷfl) for all i 6= Ŷfl up to a correction factor P (i,Ŷfl)1 P (i,Ŷfl) 2 .",3.2. Tighter Bound,[0],[0]
This correction factor is a product of terms typically centered around 1 (each term is of the form sinh(x)x,3.2. Tighter Bound,[0],[0]
≈ 1 for small x).,3.2. Tighter Bound,[0],[0]
"On the other hand, S(i,Ŷfl), by definition, is the ratio of the excess confidence the floating-point network has in the label Ŷfl over the total quantization noise variance reflected at the output, i.e., S(i,Ŷfl) is the SQNR.",3.2. Tighter Bound,[0],[0]
"Hence, Theorem 2 states that the tolerance of a neural network to quantization is, on average, exponentially decaying with the SQNR at its output.",3.2. Tighter Bound,[0],[0]
"In terms of precision, Theorem 2 states that pm is bounded by a double exponentially decaying function of precision (that is an exponential function of an exponential function).",3.2. Tighter Bound,[0],[0]
"Note how this bound is tighter than that of Theorem 1.
",3.2. Tighter Bound,[0],[0]
"This double exponential relationship between accuracy and precision is not too surprising when one considers the problem of binary hypothesis testing under additive Gaussian noise (Blahut, 2010) scenario.",3.2. Tighter Bound,[0],[0]
"In this scenario, it is wellknown that the probability of error is an exponentially decaying function of the signal-to-noise ratio (SNR) in the high-SNR regime.",3.2. Tighter Bound,[0],[0]
"Theorem 2 points out a similar relationship between accuracy and precision but it does so using rudimentary probability principles without relying on highSNR approximations.
",3.2. Tighter Bound,[0],[0]
"While Theorem 2 is much tighter than Theorem 1 theoretically, it is not as convenient to use.",3.2. Tighter Bound,[0],[0]
"In order to use Theorem 2, one has to perform a forward-backward pass and select relevant quantities and apply (13) for each choice of BA and BW .",3.2. Tighter Bound,[0],[0]
"However, a lot of information, e.g. the derivatives, can be reused at each run, and so the runs may be lumped into one forward-backward pass.",3.2. Tighter Bound,[0],[0]
"In a sense, the complexity of computing the bound in Theorem 2 lies between the evaluation of (11) and the complicated conventional trial-and-error based search.
",3.2. Tighter Bound,[0],[0]
We now illustrate the applications of these bounds.,3.2. Tighter Bound,[0],[0]
We conduct numerical simulations to illustrate both the validity and usefulness of the analysis developed in the previous section.,4. Simulation Results,[0],[0]
We show how it is possible to reduce precision in an aggressive yet principled manner.,4. Simulation Results,[0],[0]
We present results on two popular datasets: MNIST and CIFAR-10.,4. Simulation Results,[0],[0]
"The metrics we address are threefold:
• Accuracy measured in terms of test error.",4. Simulation Results,[0],[0]
"• Computational cost measured in #FAs (see Section
2.3, (3) was used to compute #FAs per MAC).",4. Simulation Results,[0],[0]
"• Representational cost measured in bits (see Section
2.3, (4) was used).
",4. Simulation Results,[0],[0]
"We compare our results to similar works conducting similar experiments: 1) the work on fixed-point training with stochastic quantization (SQ) (Gupta et al., 2015) and 2) BinaryNet (BN) (Hubara et al., 2016b).",4. Simulation Results,[0],[0]
"First, we conduct simulations on the MNIST dataset for handwritten character recognition (LeCun et al., 1998).",4.1. DNN on MNIST,[0],[0]
The dataset consists of 60K training samples and 10K test samples.,4.1. DNN on MNIST,[0],[0]
Each sample consists of an image and a label.,4.1. DNN on MNIST,[0],[0]
Images are of size 28 × 28 pixels representing a handwritten digit between 0 and 9.,4.1. DNN on MNIST,[0],[0]
"Labels take the value of the corresponding digit.
",4.1. DNN on MNIST,[0],[0]
"In this first experiment, we chose an architecture of 784 − 512 − 512 − 512 − 10, i.e., 3 hidden layers, each of 512 units.",4.1. DNN on MNIST,[0],[0]
We first trained the network in floating-point using the back-propagation algorithm.,4.1. DNN on MNIST,[0],[0]
We used a batch size of 200 and a learning rate of 0.1 with a decay rate of 0.978 per epoch.,4.1. DNN on MNIST,[0],[0]
"We restore the learning rate every 100 epochs, the decay rate makes the learning rate vary between 0.1 and 0.01.",4.1. DNN on MNIST,[0],[0]
"We train the first 300 epochs using 15% dropout, the second 300 epochs using 20% dropout, and the third
300 epochs using 25% dropout (900 epochs overall).",4.1. DNN on MNIST,[0],[0]
"It appears from the original dropout work (Srivastava et al., 2014) that the typical 50% dropout fraction works best for very wide multi-layer perceptrons (MLPs) (4096 to 8912 hidden units).",4.1. DNN on MNIST,[0],[0]
"For this reason, we chose to experiment with smaller dropout fractions.
",4.1. DNN on MNIST,[0],[0]
The only pre-processing done is to scale the inputs between −1 and 1.,4.1. DNN on MNIST,[0],[0]
We used ReLU activations with the subtle addition of a right rectifier for values larger than 2 (as discussed in Section 2).,4.1. DNN on MNIST,[0],[0]
The resulting activation is also called a hard sigmoid.,4.1. DNN on MNIST,[0],[0]
"We also clipped the weights to lie in [−1, 1] at each iteration.",4.1. DNN on MNIST,[0],[0]
"The resulting test error we obtained in floating-point is 1.36%.
",4.1. DNN on MNIST,[0],[0]
Figure 1 illustrates the validity of our analysis.,4.1. DNN on MNIST,[0],[0]
"Indeed, both bounds (based on Theorems 1 & 2) successfully upper bound the test error obtained through fixed-point simulations.",4.1. DNN on MNIST,[0],[0]
Figure 1 (b) demonstrates the utility of (12).,4.1. DNN on MNIST,[0],[0]
"Indeed, setting BW = BA allows us to reduce the precision to about 6 or 7 bits before the accuracy start degrading.",4.1. DNN on MNIST,[0],[0]
"In addition, under these conditions we found EA = 41 and EW = 3803 so that log2 √ EW EA",4.1. DNN on MNIST,[0],[0]
≈ 3.2.,4.1. DNN on MNIST,[0],[0]
"Thus, setting BW = BA + 3 as dictated by (12) allows for more aggressive precision reduction.",4.1. DNN on MNIST,[0],[0]
Activation precision BA can now be reduced to about 3 or 4 bits before the accuracy degrades.,4.1. DNN on MNIST,[0],[0]
"To compute the bounds, we used an estimation set of 1000 random samples from the dataset.
",4.1. DNN on MNIST,[0],[0]
We compare our results with SQ which used a 784−1000− 1000−10 architecture on 16-bit fixed-point activations and weights.,4.1. DNN on MNIST,[0],[0]
A stochastic rounding scheme was used to compensate for quantization.,4.1. DNN on MNIST,[0],[0]
We also compare our results with BN with a 784− 2048− 2048− 2048− 10 architecture on binary quantities.,4.1. DNN on MNIST,[0],[0]
"A stochastic rounding scheme was also used during training.
",4.1. DNN on MNIST,[0],[0]
"Table 1 shows some comparisons with related works in terms of accuracy, computational cost, and representational
cost.",4.1. DNN on MNIST,[0],[0]
"For comparison, we selected four notable design options from Figures 1 (a,b):
A. Smallest (BA, BW ) such that BW = BA and pm ≤ 1% as bounded by Theorem 1.",4.1. DNN on MNIST,[0],[0]
"In this case (BA, BW ) = (8, 8).",4.1. DNN on MNIST,[0],[0]
"B. Smallest (BA, BW ) such that BW = BA and pm ≤ 1% as bounded by Theorem 2.",4.1. DNN on MNIST,[0],[0]
"In this case (BA, BW ) = (6, 6).",4.1. DNN on MNIST,[0],[0]
"C. Smallest (BA, BW ) such that BW = BA + 3 as dictated by (12) and pm ≤ 1% as bounded by Theorem 1.",4.1. DNN on MNIST,[0],[0]
"In this case (BA, BW ) = (6, 9).",4.1. DNN on MNIST,[0],[0]
"D. Smallest (BA, BW ) such that BW = BA + 3 as dictated by (12) and pm ≤ 1% as bounded by Theorem 2.",4.1. DNN on MNIST,[0],[0]
"In this case (BA, BW ) = (4, 7).
",4.1. DNN on MNIST,[0],[0]
"As can be seen in Table 1, the accuracy is similar across all design options including the results reported by SQ and BN.",4.1. DNN on MNIST,[0],[0]
"Interestingly, for all four design options, our network has a smaller computational cost than BN.",4.1. DNN on MNIST,[0],[0]
"In addition, SQ’s computational cost is about 4.6× that of BN (533M/117M).",4.1. DNN on MNIST,[0],[0]
"The greatest reduction in computational cost is obtained for a precision assignment of (4, 7) corresponding to a 2.6× and 11.9× reduction compared to BN (117M/44.7M) and SQ (533M/44.7M), respectively.",4.1. DNN on MNIST,[0],[0]
The corresponding test error rate is of 1.43%.,4.1. DNN on MNIST,[0],[0]
Similar trends are observed for representational costs.,4.1. DNN on MNIST,[0],[0]
"Again, our four designs have smaller representational cost than even BN. BN itself has 2.8× smaller representational cost than SQ (28M/10M).",4.1. DNN on MNIST,[0],[0]
"Note that a precision assignment of (6, 6) yields 1.8× and 5.0× smaller representational costs than BN (10M/5.63M) and SQ (28M/5.63M), respectively.",4.1. DNN on MNIST,[0],[0]
"The corresponding test error rate is 1.54%.
",4.1. DNN on MNIST,[0],[0]
The fact that we are able to achieve lesser computational and representational costs than BN while maintaining similar accuracy highlights two important points.,4.1. DNN on MNIST,[0],[0]
"First, the width of a network severely impacts its complexity.",4.1. DNN on MNIST,[0],[0]
We made our network four times as narrow as BN’s and still managed to use eight times as many bits per parameter without exceeding BN’s complexity.,4.1. DNN on MNIST,[0],[0]
"Second, our results illustrate the strength of numbering systems, specifically, the
strength of fixed-point representations.",4.1. DNN on MNIST,[0],[0]
Our results indicate that a correct and meaningful multi-bit representation of parameters is better in both complexity and accuracy than a 1-bit unstructured allocation.,4.1. DNN on MNIST,[0],[0]
"We conduct a similar experiment on the CIFAR10 dataset (Krizhevsky & Hinton, 2009).",4.2. CNN on CIFAR 10,[0],[0]
"The dataset consists of 60K color images each representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships, and trucks.",4.2. CNN on CIFAR 10,[0],[0]
"50K of these images constitute the training set, and the 10K remaining are for testing.",4.2. CNN on CIFAR 10,[0],[0]
"SQ’s architecture on this dataset is a simple one: three convolutional layers, interleaved by max pooling layers.",4.2. CNN on CIFAR 10,[0],[0]
The output of the final pooling layer is fed to a 10-way softmax output layer.,4.2. CNN on CIFAR 10,[0],[0]
The reported accuracy using 16-bit fixed-point arithmetic is a 25.4% test error.,4.2. CNN on CIFAR 10,[0],[0]
"BN’s architecture is a much wider and deeper architecture based on VGG (Simonyan & Zisserman, 2014).",4.2. CNN on CIFAR 10,[0],[0]
"The reported accuracy of the binary network is an impressive 10.15% which is of benchmarking quality even for full precision networks.
",4.2. CNN on CIFAR 10,[0],[0]
"We adopt a similar architecture as SQ, but leverage re-
cent advances in convolutional neural networks (CNNs) research.",4.2. CNN on CIFAR 10,[0],[0]
"It has been shown that adding networks within convolutional layers (in the ‘Network in Network’ sense) as described in (Lin et al., 2013) significantly enhances accuracy, while not incurring much complexity overhead.",4.2. CNN on CIFAR 10,[0],[0]
"Hence, we replace SQ’s architecture by a deep one which we describe as 64C5− 64C1−",4.2. CNN on CIFAR 10,[0],[0]
"64C1−MP2− 64C5− 64C1−64C1−MP2−64C5−64FC−64FC−64FC− 10, where C5 denotes 5 × 5 kernels, C1 denotes 1 × 1 kernels (they emulate the networks in networks), MP2 denotes 2 × 2 max pooling, and FC denotes fully connected components.",4.2. CNN on CIFAR 10,[0],[0]
"As is customary for this dataset, we apply zero-phase component analysis (ZCA) whitening to the data before training.",4.2. CNN on CIFAR 10,[0],[0]
"Because this dataset is a challenging one, we first fine-tune the hyperparameters (learning rate, weight decay rate, and momentum), then train for 300 epochs.",4.2. CNN on CIFAR 10,[0],[0]
"The best accuracy we reach in floating point using this 12-layer deep network is 17.02%.
",4.2. CNN on CIFAR 10,[0],[0]
Figure 2 shows the results of our fixed-point simulation and analysis.,4.2. CNN on CIFAR 10,[0],[0]
"Note that, while both bounds from Theorems 1 and 2 still successfully upper bound the test error, these are not as tight as in our MNIST experiment.",4.2. CNN on CIFAR 10,[0],[0]
"Furthermore, in this case, (12) dictates keeping BW = BA as EA = 21033
and EW = 31641 so that log2 √ EW EA",4.2. CNN on CIFAR 10,[0],[0]
≈ 0.29.,4.2. CNN on CIFAR 10,[0],[0]
The fact that EW ≥ EA is expected as there are typically more weights than activations in a neural network.,4.2. CNN on CIFAR 10,[0],[0]
"However, note that in this case the contrast between EW and EA is not as sharp as in our MNIST experiment.",4.2. CNN on CIFAR 10,[0],[0]
"This is mainly due to the higher weight to activation ratio in fully connected DNNs than in CNNs.
",4.2. CNN on CIFAR 10,[0],[0]
"We again select two design options:
A. Smallest (BA, BW ) such that BW = BA and pm ≤ 1% as bounded by Theorem 1.",4.2. CNN on CIFAR 10,[0],[0]
"In this case (BA, BW ) = (12, 12).",4.2. CNN on CIFAR 10,[0],[0]
"B. Smallest (BA, BW ) such that BW = BA and pm ≤ 1% as bounded by Theorem 2.",4.2. CNN on CIFAR 10,[0],[0]
"In this case (BA, BW ) = (10, 10).
",4.2. CNN on CIFAR 10,[0],[0]
Table 2 indicates that BN is the most accurate with 10.15% test error.,4.2. CNN on CIFAR 10,[0],[0]
"Interestingly, it has lesser computational cost but more representational cost than SQ.",4.2. CNN on CIFAR 10,[0],[0]
"This is due to the dependence of the computational cost on the product of BA
and BW .",4.2. CNN on CIFAR 10,[0],[0]
"The least complex network is ours when setting (BA, BW ) =",4.2. CNN on CIFAR 10,[0],[0]
"(10, 10) and its test error is 17.23% which is already a large improvement on SQ in spite of having smaller computational and representational costs.",4.2. CNN on CIFAR 10,[0],[0]
"This network is also less complex than that of BN.
",4.2. CNN on CIFAR 10,[0],[0]
The main take away here is that CNNs are quite different from fully connected DNNs when it comes to precision requirements.,4.2. CNN on CIFAR 10,[0],[0]
"Furthermore, from Table 2 we observe that BN achieves the least test error.",4.2. CNN on CIFAR 10,[0],[0]
It seems that this better accuracy is due to its greater representational power rather than its computational power (BN’s representational cost is much higher than the others as opposed to its computational cost).,4.2. CNN on CIFAR 10,[0],[0]
In this paper we analyzed the quantization tolerance of neural networks.,5. Conclusion,[0],[0]
We used our analysis to efficiently reduce weight and activation precisions while maintaining similar fidelity as the floating-point initiation.,5. Conclusion,[0],[0]
"Specifically, we obtained bounds on the mismatch probability between a fixedpoint network and its floating-point counterpart in terms of precision.",5. Conclusion,[0],[0]
We showed that a neural network’s accuracy degradation due to quantization decreases double exponentially as a function of precision.,5. Conclusion,[0],[0]
Our analysis provides a straightforward method to obtain an upper bound on the network’s error probability as a function of precision.,5. Conclusion,[0],[0]
"We used these results on real datasets to minimize the computational and representational costs of a fixed-point network while maintaining accuracy.
",5. Conclusion,[0],[0]
Our work addresses the general problem of resource constrained machine learning.,5. Conclusion,[0],[0]
One take away is that it is imperative to understand the trade-offs between accuracy and complexity.,5. Conclusion,[0],[0]
"In our work, we used precision as a parameter to analytically characterize this trade-off.",5. Conclusion,[0],[0]
"Nevertheless, additional aspects of complexity in neural networks such as their structure and their sparsity can also be accounted for.",5. Conclusion,[0],[0]
"In fact, more work can be done in that regard.",5. Conclusion,[0],[0]
Our work may be viewed as a first step in developing a unified and principled framework to understand complexity vs. accuracy trade-offs in deep neural networks and other machine learning algorithms.,5. Conclusion,[0],[0]
"This work was supported in part by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA.",Acknowledgment,[0],[0]
The acclaimed successes of neural networks often overshadow their tremendous complexity.,abstractText,[0],[0]
We focus on numerical precision a key parameter defining the complexity of neural networks.,abstractText,[0],[0]
"First, we present theoretical bounds on the accuracy in presence of limited precision.",abstractText,[0],[0]
"Interestingly, these bounds can be computed via the back-propagation algorithm.",abstractText,[0],[0]
"Hence, by combining our theoretical analysis and the backpropagation algorithm, we are able to readily determine the minimum precision needed to preserve accuracy without having to resort to timeconsuming fixed-point simulations.",abstractText,[0],[0]
We provide numerical evidence showing how our approach allows us to maintain high accuracy but with lower complexity than state-of-the-art binary networks.,abstractText,[0],[0]
Analytical Guarantees on Numerical Precision of Deep Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4371–4382 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4371",text,[0],[0]
"Research on structural properties (typological features) of language, such as the order of subject, object and verb (examples are SOV and SVO) and the presence or absence of tone, is largely synchronic in nature.",1 Introduction,[0],[0]
"Since languages of the world exhibit an astonishing diversity, the sample of languages used in a typical typological study is selected from a diverse set of language families and from various geographical regions.",1 Introduction,[0],[0]
"Not surprisingly, most of them lack historical documentation that allows us to directly trace their evolutionary history.
",1 Introduction,[0],[0]
"At the same time, however, typologists have long struggled to dynamicize synchronic typology, or to infer diachronic universals of change from current cross-linguistic variation (Greenberg, 1978; Nichols, 1992; Maslova, 2000; Bickel, 2013).",1 Introduction,[0],[0]
"They have also tried to uncover deep historical relations between languages (Nichols, 1992).
",1 Introduction,[0],[0]
"One of the main developments in diachronic typology in the last decade has been the application of powerful statistical tools borrowed from the
field of evolutionary biology (Dediu, 2010; Greenhill et al., 2010; Dunn et al., 2011; Maurits and Griffiths, 2014; Greenhill et al., 2017).",1 Introduction,[0],[0]
"As illustrated in Figure 1, the key idea is that if a phylogenetic tree is given, we can infer the ancestral states with varying degrees of confidence, and by extension, can induce diachronic universals of change.",1 Introduction,[0],[0]
"To perform statistical inference, we assume that each feature evolves along the branches of the tree according to a continuous-time Markov chain (CTMC) model, which is controlled by a transition rate matrix (TRM).",1 Introduction,[0],[0]
"Once TRMs are estimated, we can gain insights from them, for example, by simulating language evolution (Maurits and Griffiths, 2014).
",1 Introduction,[0],[0]
"One problem in previous studies is that they do not adequately model a characteristic of typological features that has been central to linguistic typology, that is, the fact that these features are not independent but depend on each other (Greenberg, 1963; Daumé III and Campbell, 2007).",1 Introduction,[0],[0]
"For example, if a language takes a verb before an object (VO), then it takes postnominal relative clauses
1 1 0 0… 2 1 … 3 features x𝑙𝑙,∗parameters",1 Introduction,[0],[0]
"z𝑙𝑙,∗
",1 Introduction,[0],[0]
Step 1.,1 Introduction,[0],[0]
"Map each language into the latent representation
Step 2.",1 Introduction,[0],[0]
Infer a set of transition rate matrices using phylo.,1 Introduction,[0],[0]
"trees (also infer the states and dates of the internal nodes)
infer
(NRel) (VO → NRel, in shorthand), and a related universal, RelN → OV, also holds (Dryer, 2011).",1 Introduction,[0],[0]
"Despite the long-standing interest in interfeature dependencies, most statistical models assume independence between features (Daumé III, 2009; Dediu, 2010; Greenhill et al., 2010, 2017; Murawaki, 2016; Murawaki and Yamauchi, 2018).",1 Introduction,[0],[0]
"A rare exception is Dunn et al. (2011), who extended Greenberg’s idea by applying a phylogenetic model of correlated evolution (Pagel and Meade, 2006).",1 Introduction,[0],[0]
"However, the model adopted by Dunn et al. (2011) can only handle the dependency between a pair of binary features.",1 Introduction,[0],[0]
"Typological features have two or more possible values in general, and more importantly, the dependencies between features are not limited to a pair (Itoh and Ueda, 2004).",1 Introduction,[0],[0]
"For example, the order of relative clauses has connections to the order of adjective and noun (AdjN or NAdj), in addition to the order of object and verb, as two universals, RelN → AdjN and NAdj → NRel, are known to hold well (Dryer, 2011).
",1 Introduction,[0],[0]
"In this paper, we propose latent representationbased analysis of diachronic typology.",1 Introduction,[0],[0]
Figure 2 shows an overview of our framework.,1 Introduction,[0],[0]
"Follow-
ing Murawaki (2017), we assume that a sequence of discrete surface features that represents a language is generated from a sequence of binary latent variables called parameters (Step 1).",1 Introduction,[0],[0]
"Parameters are, by assumption, independent of each other and switching one parameter entails multiple changes of surface features in general.",1 Introduction,[0],[0]
"Thus, by performing phylogenetic inference on the latent space, we can handle the dependencies of all available features in an implicit manner (Step 2).",1 Introduction,[0],[0]
The latent parameter representation can be projected back to the surface feature representation when needed for analysis.,1 Introduction,[0],[0]
"Like Maurits and Griffiths (2014), we run simulation experiments to interpret the estimated model parameters (Step 3).
",1 Introduction,[0],[0]
"What we propose is a general framework with which we can analyze any discrete feature, but as a proof-of-concept demonstration, we follow Maurits and Griffiths (2014) in focusing on the order of subject, object and verb (hereafter simply referred to as basic word order or BWO).1",1 Introduction,[0],[0]
"In the dataset we use, the BWO feature has 7 possible values, 6 logically possible orders plus the special value No dominant order (Dryer, 2013b), meaning that it cannot be analyzed directly with Dunn et al.’s model.",1 Introduction,[0],[0]
We show that languages sharing the same word order are not a coherent group but exhibit varying degrees of diachronic stability depending on other features.,1 Introduction,[0],[0]
"The building block of statistical phylogenetic models2 is a time-tree, which places nodes on an axis of time.",2.1 Statistical Diachronic Typology,[0],[0]
"In their standard applications to language (Gray and Atkinson, 2003; Bouckaert et al., 2012), time-trees are inferred from cognate
1 We chose the BWO feature because it is appealing to a wider audience.",2.1 Statistical Diachronic Typology,[0],[0]
"We are aware that Matthew S. Dryer, who provided language data for the BWO feature, favors binary classifications (OV vs. VO and SV vs. VS) over the six-way classification (Dryer, 1997, 2013a).",2.1 Statistical Diachronic Typology,[0],[0]
"He argues that the binary classifications are more fundamental than the six-way classification, but our latent representation-based analysis does not require feature values to be primitive in nature because it reorganizes feature values into various latent parameters.
",2.1 Statistical Diachronic Typology,[0],[0]
2 Statistical phylogenetic models can be either distancebased and character-based.,2.1 Statistical Diachronic Typology,[0],[0]
Character-based models are classified into parsimony-based and likelihood-based.,2.1 Statistical Diachronic Typology,[0],[0]
"In this paper, we focus on likelihood-based Bayesian models for their ability to date internal nodes.",2.1 Statistical Diachronic Typology,[0],[0]
"However, it is worth noting that attempts to overcome the limitations of the tree model mostly rely on non-likelihood-based models (Nakhleh et al., 2005; Nelson-Sathi et al., 2010).
",2.1 Statistical Diachronic Typology,[0],[0]
"data (Dyen et al., 1992; Greenhill et al.,",2.1 Statistical Diachronic Typology,[0],[0]
"2008).3 However, if a tree is given a priori, phylogenetic models can also be used to estimate the parameters of a TRM, which controls how languages change their feature values over time.",2.1 Statistical Diachronic Typology,[0],[0]
"This is how typological features are analyzed in previous studies.
",2.1 Statistical Diachronic Typology,[0],[0]
Dediu (2010) aggregated TRMs taken from various families to measure the stability of features.,2.1 Statistical Diachronic Typology,[0],[0]
Greenhill et al. (2010) compared typological data with cognate data in terms of stability.,2.1 Statistical Diachronic Typology,[0],[0]
Maurits and Griffiths (2014) focused on the BWO feature and analyzed how it had changed in the past and was likely to change in the future.,2.1 Statistical Diachronic Typology,[0],[0]
"Dunn et al. (2011) estimated TRMs for pairs of binary features and found that perceived correlated evolution was mostly lineage-specific rather than universal.
",2.1 Statistical Diachronic Typology,[0],[0]
"Taking a closer look at these studies, we can see that they vary as to how to prepare trees, as summarized in Table 1.",2.1 Statistical Diachronic Typology,[0],[0]
"Leaf nodes are assumed to be at the present date t = 0, but how can we assign backward dates t to internal nodes?",2.1 Statistical Diachronic Typology,[0],[0]
"A popular approach (Greenhill et al., 2010; Dunn et al., 2011) is to construct a time-tree with absolute (calendar) dates, using binary-coded lexical cognate data, and then to fit each trait of interest independently on the time-tree.4
However, cognate data are available only for a handful of language families such as IndoEuropean, Austronesian and Niger-Congo (or its mammoth Bantu branch).",2.1 Statistical Diachronic Typology,[0],[0]
"Moreover, phylogenetic
3 See Pereltsvaig and Lewis (2015) for a criticism of computational approaches to historical linguistics and Chang et al. (2015) for an elegant solution to a set of problems commonly found in inferred time-trees.
",2.1 Statistical Diachronic Typology,[0],[0]
"4To be precise, a set of tree samples given by MCMC sampling is usually employed to account for uncertainty.
",2.1 Statistical Diachronic Typology,[0],[0]
inference was performed separately one after the other.,2.1 Statistical Diachronic Typology,[0],[0]
This marks a sharp contrast with the long tradition of testing against a worldwide sample.,2.1 Statistical Diachronic Typology,[0],[0]
"In fact, it is suggested that sample diversity and aggregate time depth are not large enough to draw meaningful conclusions (Croft et al., 2011; Levy and Daumé III, 2011).
",2.1 Statistical Diachronic Typology,[0],[0]
"For this reason, we take another approach, which was employed by Dediu (2010).",2.1 Statistical Diachronic Typology,[0],[0]
He used language families established by historical linguists.,2.1 Statistical Diachronic Typology,[0],[0]
"Because such tree topologies are not associated with dates, he inferred the dates of internal nodes together with the states of internal nodes and TRMs.",2.1 Statistical Diachronic Typology,[0],[0]
"This was possible because he jointly fitted a sequence of traits, instead of fitting each trait independently.",2.1 Statistical Diachronic Typology,[0],[0]
"If multiple traits are combined, they provide considerable information on a branch length, or the time elapsing from a parent to a child, because the elapsed time is roughly inversely proportional to the similarity between the two nodes.5
Our approach differs from Dediu’s mainly in two points.",2.1 Statistical Diachronic Typology,[0],[0]
"First, whereas Dediu (2010) performed posterior inference separately for each language family, we tie a single set of TRMs to all available language families.",2.1 Statistical Diachronic Typology,[0],[0]
"Second, Dediu (2010) only inferred relative dates because he did not perform calibration (Drummond and Bouckaert, 2015).",2.1 Statistical Diachronic Typology,[0],[0]
"In order to assign calendar dates to nodes, we use multiple calibration points (the clock in Figure 2 indicates a calibration point).",2.1 Statistical Diachronic Typology,[0],[0]
"As is com-
5 Although some previous studies adopted relaxed clock models, in which different branches have different rates of evolution (Drummond and Bouckaert, 2015), we use the simple strict clock model because our calibration points are not large enough in number to harness the very flexible models.
monly done in the cognate-based reconstruction of a time-tree (Bouckaert et al., 2012), we set the Gaussian, Gaussian mixture, log-normal and uniform distributions as priors on the dates of the corresponding internal nodes.",2.1 Statistical Diachronic Typology,[0],[0]
"While previous studies analyzed the evolution of a single categorical feature (Dediu, 2010; Greenhill et al., 2010; Maurits and Griffiths, 2014) or a pair of binary features (Dunn et al., 2011), we capture the dependencies of all available features by mapping each language to a sequence of independent latent variables.",2.2 Latent Representations of Languages,[0],[0]
"To our knowledge, Murawaki (2015) was the first to introduce latent representations to typological features.",2.2 Latent Representations of Languages,[0],[0]
"Pointing out several critical problems, however, Murawaki (2017) superseded the earlier model.",2.2 Latent Representations of Languages,[0],[0]
"The present study is built on top of a slightly modified version of the Bayesian model presented by Murawaki (2017).
",2.2 Latent Representations of Languages,[0],[0]
"Like the present study, Murawaki (2015) performed phylogenetic inference on the latent space.",2.2 Latent Representations of Languages,[0],[0]
"However, since this model lacks the notion of time, it does not have descriptive power beyond clustering.",2.2 Latent Representations of Languages,[0],[0]
"Borrowing statistical models from the field of evolutionary biology, we perform timeaware inference.",2.2 Latent Representations of Languages,[0],[0]
"Central to our framework of diachronic analysis are the latent representations of languages (Murawaki, 2017).",3.1 Latent Representations of Languages,[0],[0]
"Each language l is represented as a sequence of N discrete features xl,∗ =",3.1 Latent Representations of Languages,[0],[0]
"(xl,1, · · · , xl,N ) ∈",3.1 Latent Representations of Languages,[0],[0]
NN0 .,3.1 Latent Representations of Languages,[0],[0]
"xl,n can take a binary value (xl,n ∈ {0, 1}) or categorical value (xl,n ∈ {1, 2, · · · , Fn}, where Fn is the number of distinct values).",3.1 Latent Representations of Languages,[0],[0]
"We assume that xl,∗ is stochastically generated from its latent representation, zl,∗ = (zl,1, · · · , zl,K) ∈ {0, 1}K , where K is the number of binary parameters, which is given a priori.
Dependencies between surface features are captured by weight matrix W ∈ RK×M .",3.1 Latent Representations of Languages,[0],[0]
M will be described below.,3.1 Latent Representations of Languages,[0],[0]
"In the generative story, we first calculate feature score vector θ̃l,∗ =",3.1 Latent Representations of Languages,[0],[0]
"(zTl,∗W )
T ∈ RM .",3.1 Latent Representations of Languages,[0],[0]
"We then obtain model parameter vector θl,∗ ∈ (0, 1)M by normalizing θ̃l,∗ for each feature type n.",3.1 Latent Representations of Languages,[0],[0]
"We use the sigmoid function for binary features,
θl,f(n,1) = 1
1 + exp(−θ̃l,f(n,1)) , (1)
and the softmax function for categorical features,
θl,f(n,i) = exp(θ̃l,f(n,i))∑Fn i′=1 exp(θ̃l,f(n,i′)) .",3.1 Latent Representations of Languages,[0],[0]
"(2)
Note that while a binary feature corresponds to one model parameter, categorical feature n is tied to Fn model parameters.",3.1 Latent Representations of Languages,[0],[0]
"We use function f(n, i) ∈ {1, · · · ,m, · · · ,M} to map feature n to the corresponding model parameter index.",3.1 Latent Representations of Languages,[0],[0]
"Finally, we draw a binary feature from Bernoulli(θl,f(n,1)), and a categorical feature from Categorical(θl,f(n,1), · · · , θl,f(n,Fn)).
",3.1 Latent Representations of Languages,[0],[0]
"To gain an insight into how W captures interfeature dependencies, suppose that for parameter k, a certain group of languages take zl,k = 1.",3.1 Latent Representations of Languages,[0],[0]
"If two categorical feature values (n1, i1) and (n2, i2) have large positive weights (i.e., wk,f(n1,i1) > 0 and wk,f(n2,i2) > 0), then the pair must often cooccur in these languages because W raises both θl,f(n1,i1) and θl,f(n2,i2).",3.1 Latent Representations of Languages,[0],[0]
"Likewise, the fact that two feature values do not co-occur can be encoded as a positive weight for one value and a negative weight for the other.
",3.1 Latent Representations of Languages,[0],[0]
"The remaining question is how zl,k is generated.",3.1 Latent Representations of Languages,[0],[0]
"We draw z∗,k = (z1,k, · · · , zL,k) from an autologistic model (Besag, 1974) that incorporates the observation that phylogenetically or areally close languages tend to take the same value.
",3.1 Latent Representations of Languages,[0],[0]
"To complete the generative story, letX andZ be the matrices of languages in the surface and latent representations, respectively, and let A be a set of latent variables controllingK autologistic models.",3.1 Latent Representations of Languages,[0],[0]
"The joint distribution is defined as
P (A,Z,W,X)=P (A)P (Z|A)P (W )P (X|Z,W ),
where hyperparameters are omitted for brevity.",3.1 Latent Representations of Languages,[0],[0]
"For prior probabilities P (A) and P (W ), please refer to Murawaki (2017).
",3.1 Latent Representations of Languages,[0],[0]
"Even if less than 30% of the items of X are present, this model has been demonstrated to recover missing values reasonably well.",3.1 Latent Representations of Languages,[0],[0]
"Also, when plotted on a world map, some parameters appear to retain phylogenetic and areal signals observed for surface features, indicating that they are not mere statistical artifacts (Murawaki, 2017).",3.1 Latent Representations of Languages,[0],[0]
"We assume that each parameter k independently evolves along the branches of trees according to a continuous-time Markov chain (CTMC)
model (Drummond and Bouckaert, 2015).",3.2 Transition Rate Matrices (TRMs),[0],[0]
The CTMC is a continuous extension to the more familiar discrete-time Markov chain.,3.2 Transition Rate Matrices (TRMs),[0],[0]
It is is controlled by a TRMQk.,3.2 Transition Rate Matrices (TRMs),[0],[0]
"If the number of states (possible values) is 2, then Qk is a 2× 2 matrix:
",3.2 Transition Rate Matrices (TRMs),[0],[0]
"Qk = ( −αk αk βk −βk ) .
",3.2 Transition Rate Matrices (TRMs),[0],[0]
"We set Gamma priors on αk, βk > 0.",3.2 Transition Rate Matrices (TRMs),[0],[0]
"Qk can be used to calculate the transition probability, or the probability of language",3.2 Transition Rate Matrices (TRMs),[0],[0]
"l taking value b for parameter k conditioned on l’s parent π(l) and t, the time span between the two:
P (zl,k = b|zπ(l),k = a, t) =",3.2 Transition Rate Matrices (TRMs),[0],[0]
"exp(tQk)a,b. (3)
",3.2 Transition Rate Matrices (TRMs),[0],[0]
"The matrix exponential exp(tQk) can be solved analytically if Qk is a 2× 2 matrix:
exp(tQk)=
( βk+αke −(αk+βk)t
αk+βk αk−αke−(αk+βk)t
αk+βk βk−βke−(αk+βk)t
αk+βk
αk+βke −(αk+βk)t
αk+βk
) .
",3.2 Transition Rate Matrices (TRMs),[0],[0]
"As t approaches to infinity, we obtain the stationary probability ( βkαk+βk , αk αk+βk
)",3.2 Transition Rate Matrices (TRMs),[0],[0]
"T. We can see that αk and βk control both the speed of change (the larger the higher) and the stationary distribution.
",3.2 Transition Rate Matrices (TRMs),[0],[0]
A root node has no parent by definition.,3.2 Transition Rate Matrices (TRMs),[0],[0]
We draw the state of a root node from the stationary distribution.,3.2 Transition Rate Matrices (TRMs),[0],[0]
"Thus, language isolates do have impact on posterior inference of TRMs.",3.2 Transition Rate Matrices (TRMs),[0],[0]
"To estimate TRMs, we need to specify the generative model of time-trees and an inference algorithm.",3.3 Posterior Inference of Time-trees,[0],[0]
"In the generative story, each tree topology is drawn from some uniform distribution.",3.3 Posterior Inference of Time-trees,[0],[0]
The dates of its nodes are determined next.,3.3 Posterior Inference of Time-trees,[0],[0]
"If the node in question is not a calibration point, its date is drawn from some uniform distribution, subject to the ancestral ordering constraint: a node must be older than its descendants.",3.3 Posterior Inference of Time-trees,[0],[0]
"If the node is a calibration point, its date is drawn from the corresponding prior distribution.6 TRM parameters, αk and βk, are generated from Gamma priors.",3.3 Posterior Inference of Time-trees,[0],[0]
"For the root node, the value of parameter k is drawn from the corresponding stationary distribution.",3.3 Posterior Inference of Time-trees,[0],[0]
The states of the non-root nodes are generated using Eq.,3.3 Posterior Inference of Time-trees,[0],[0]
"(3).
",3.3 Posterior Inference of Time-trees,[0],[0]
"Given tree topologies, the states of the leaf nodes and calibration points, we need to infer
6This model is slightly leaky because some priors (e.g., Gaussian) assign non-zero probabilities to illogical time-trees that violate the ancestral ordering constraint.
",3.3 Posterior Inference of Time-trees,[0],[0]
"(1) the dates of the internal nodes, (2)",3.3 Posterior Inference of Time-trees,[0],[0]
"the states of the internal nodes, and (3) TRM parameters, αk and βk, for each latent parameter k. Gibbs sampling updates of these variables are as follows:
Update dates We update the dates of the internal nodes one by one.",3.3 Posterior Inference of Time-trees,[0],[0]
The time span in which the target node can move is bound by its parent (if there is) and its eldest child.,3.3 Posterior Inference of Time-trees,[0],[0]
"We use slice sampling (Neal, 2003) to update the date.",3.3 Posterior Inference of Time-trees,[0],[0]
"In addition, we use a Metropolis-Hastings operator that multiplies the dates of all the internal nodes of a tree by a rate drawn from a log-normal distribution.
",3.3 Posterior Inference of Time-trees,[0],[0]
"Update states For each parameter k, we blocksample a whole given tree.",3.3 Posterior Inference of Time-trees,[0],[0]
"Specifically, we implement a Bayesian version of Felsenstein’s tree-pruning algorithm, which is akin to the forward filtering-backward sampling algorithm for Bayesian hidden Markov models.
",3.3 Posterior Inference of Time-trees,[0],[0]
"Update αk and βk We jointly sample αk and βk for each k. Since both the transition and stationary probabilities can be obtained analytically for binary traits, we use Hamiltonian Monte Carlo to exploit gradient information (Neal, 2011).",3.3 Posterior Inference of Time-trees,[0],[0]
"Now we are ready to elaborate on the proposed framework of diachronic analysis (Figure 2).
",3.4 Three-Step Analysis,[0],[0]
Step 1,3.4 Three-Step Analysis,[0],[0]
"We map each language, represented as a sequence of N discrete surface features, to a sequence of K binary latent parameters.",3.4 Three-Step Analysis,[0],[0]
"Let feature matrix X be decomposed into observed and missing portions, Xobs and Xmis, respectively.",3.4 Three-Step Analysis,[0],[0]
"Given Xobs, we use Gibbs sampling to infer A, parameter matrix Z, weight matrix W , and Xmis (Murawaki, 2017).7
In the present study, we set K = 100.",3.4 Three-Step Analysis,[0],[0]
We run 5 independent MCMC chains.,3.4 Three-Step Analysis,[0],[0]
"For each chain, we start with 1,000 burn-in iterations.",3.4 Three-Step Analysis,[0],[0]
We then obtain 10 samples with an interval of 10 iterations.,3.4 Three-Step Analysis,[0],[0]
"Note that after burn-in iterations, we fix W and only sample A, Z, and Xmis to avoid the identifiability problems (e.g., label-switching).",3.4 Three-Step Analysis,[0],[0]
"For each item of Z and Xmis, we output the most frequent value among the 10 samples.",3.4 Three-Step Analysis,[0],[0]
"We do this to reduce uncertainty.
7",3.4 Three-Step Analysis,[0],[0]
"We employ a slightly modified Metropolis-Hastings operator to improve the mobility of Z.
Step 2 We fit a set of K TRMs on family trees around the world.",3.4 Three-Step Analysis,[0],[0]
"Formally, what are observed are tree topologies, the states of the leaf nodes (i.e., sequences of latent parameters), and multiple calibration points.",3.4 Three-Step Analysis,[0],[0]
"Given these, we infer TRM parameters, αk and βk, for each latent parameter k, as well as the dates and states of the internal nodes.",3.4 Three-Step Analysis,[0],[0]
"We, again, use Gibbs sampling as explained in Section 3.3.
",3.4 Three-Step Analysis,[0],[0]
"We collect 10 samples with an interval of 10 iterations after 1,000 burn-in iterations.",3.4 Three-Step Analysis,[0],[0]
We do this for each of the 5 samples obtained in Step 1.,3.4 Three-Step Analysis,[0],[0]
"As a result, we obtain 50 samples in total.
",3.4 Three-Step Analysis,[0],[0]
Step 3,3.4 Three-Step Analysis,[0],[0]
We analyze the TRMs by simulating language evolution.,3.4 Three-Step Analysis,[0],[0]
"Given the latent representation of language l, we stochastically generate its descendant l′ after some time span t. Specifically, we draw zl′,k according to the transition probability of Eq.",3.4 Three-Step Analysis,[0],[0]
"(3) for each parameter k. Using weight matrix W , we then project the latent representation zl′,∗ back to the surface representation xl′,∗. To be precise, we use model parameter vector θl′,∗, instead of xl′,∗, for further analysis.",3.4 Three-Step Analysis,[0],[0]
"For each of the 50 samples obtained in Step 2, we simulate the evolution of a given language 100 times (5,000 samples in total).",3.4 Three-Step Analysis,[0],[0]
"The database of typological features we used is the online edition8 of the World Atlas of Language Structures (WALS) (Haspelmath et al., 2005).",4.1 Data and Preprocessing,[0],[0]
"We preprocessed the database as was done in Murawaki (2017), with different thresholds.",4.1 Data and Preprocessing,[0],[0]
"As a result, we obtained a language–feature matrix consisting of L = 2,607 languages and N = 152 features.",4.1 Data and Preprocessing,[0],[0]
Only 19.98% of items in the matrix were present.,4.1 Data and Preprocessing,[0],[0]
We manually classified features into binary and categorical ones.,4.1 Data and Preprocessing,[0],[0]
"The number of model parameters, M , was 760.
",4.1 Data and Preprocessing,[0],[0]
"We used Glottolog 3.2 (Hammarström et al., 2018) as the source of family trees.",4.1 Data and Preprocessing,[0],[0]
"Glottolog has three advantages over Ethnologue (Lewis et al., 2014), another commonly-used catalog of the world’s languages.",4.1 Data and Preprocessing,[0],[0]
"(1) Glottolog makes explicit that it adopts a genealogical classification, rather than hierarchical clusterings of modern languages.",4.1 Data and Preprocessing,[0],[0]
(2) It reflects more recent research.,4.1 Data and Preprocessing,[0],[0]
"(3) Mapping between Glottolog and WALS is easy be-
8http://wals.info/
cause WALS provides Glottolog’s language codes (glottocodes) when available.
",4.1 Data and Preprocessing,[0],[0]
"After Step 1 of Section 3.4, we dropped languages from WALS that could not be mapped to Glottolog.",4.1 Data and Preprocessing,[0],[0]
"As a result, 2,557 languages remained.",4.1 Data and Preprocessing,[0],[0]
We subdivided a Glottolog node if multiple languages from WALS shared the same glottocode.,4.1 Data and Preprocessing,[0],[0]
We removed leaf nodes that were not present in WALS and repeatedly dropped internal nodes that had only one child.,4.1 Data and Preprocessing,[0],[0]
"We obtained 309 language families among which 154 had only one node (i.e., language isolates).
",4.1 Data and Preprocessing,[0],[0]
"We collected 50 calibration points from secondary literature (Holman et al., 2011; Bouckaert et al., 2012; Gray et al., 2009; Maurits and Griffiths, 2014; Grollemund et al., 2015).",4.1 Data and Preprocessing,[0],[0]
"For example, we set a Gaussian prior with mean 2,500 BP (before present) and standard deviation 500 on the date of (Proto-)Hmong-Mien.",4.1 Data and Preprocessing,[0],[0]
See Table S.1 of the supplementary materials for details.,4.1 Data and Preprocessing,[0],[0]
Our calibration points are by no means definitive or exhaustive but should be seen as a first step toward worldscale dating.,4.1 Data and Preprocessing,[0],[0]
"As a proof-of-concept demonstration of the proposed framework, we investigate the BWO feature, or WALS’s Feature 81A (Dryer, 2013b).",4.2 Case Study: Basic Word Order (BWO),[0],[0]
The cross-linguistic variation of BWO attracts attention not only from typologists but from psycholinguists (see Maurits and Griffiths (2014) for a brief review).,4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Some claim that the fact that SOV is the most frequent order indicates its optimality, presumably in terms of functionality.",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Some others point to an apparent historical trend of SOV changing to SVO, but not vice versa (Gell-Mann and Ruhlen, 2011), which might imply (1) the functional superiority of SVO over SOV and (2) an even higher prevalence of SOV in the past.",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"SOV preferences in emerging sign languages (Sandler et al., 2005) and in elicited pantomime (Goldin-Meadow et al., 2008) are also reported.
",4.2 Case Study: Basic Word Order (BWO),[0],[0]
Maurits and Griffiths (2014) fitted a 6×6 TRM9 on large language families.,4.2 Case Study: Basic Word Order (BWO),[0],[0]
"However, we suspect that singling out BWO is oversimplification.",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Given its profound effect on the whole grammatical system, a BWO change can hardly occur independently of other features.",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"In fact, Mithun (1995) lists a variety of morphological factors that have
9The special value No dominant order was removed in their experiments.
diachronically reduced the rigidity of SOV order in Native American languages.",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Her analysis suggests that languages sharing the same word order might not be a monolithic group.
",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Here, we use latent representation-based analysis to answer questions: how variable language sharing the same BWO are with respect to diachronic stability, and what kind of features are correlated with BWO stability?",4.2 Case Study: Basic Word Order (BWO),[0],[0]
"Among the 2,557 modern languages, we chose 1,357 languages for which the BWO feature was present.",4.3 Variability of Diachronic Stability,[0],[0]
"We simulated evolution with t = 2,000, as described in Section 3.4.",4.3 Variability of Diachronic Stability,[0],[0]
Let n be the index of the BWO feature.,4.3 Variability of Diachronic Stability,[0],[0]
"For the 5,000 samples of each simulated language l′, we averaged the BWO probability vectors, θl′,f(n,1), · · · , θl′,f(n,Fn).
",4.3 Variability of Diachronic Stability,[0],[0]
"Before going into inter-language variability, let us take a look at the overall trend.",4.3 Variability of Diachronic Stability,[0],[0]
We took the average of the BWO probability vectors for each word order.,4.3 Variability of Diachronic Stability,[0],[0]
The result is shown in Figure 3.,4.3 Variability of Diachronic Stability,[0],[0]
"Our findings largely agree with those of Maurits and Griffiths (2014): (1) SOV is the most diachronically stable word order, which is followed by SVO, (2) SOV prefers changing to SVO over VSO (although hardly visually recognizable), and (3) VSO is more likely to change to SVO than to SOV, just to name a few.
",4.3 Variability of Diachronic Stability,[0],[0]
"Next, the variability is visualized in Figure 4.",4.3 Variability of Diachronic Stability,[0],[0]
"We can see that languages sharing the same word order differ considerably in terms of diachronic
stability.",4.3 Variability of Diachronic Stability,[0],[0]
"For comparison, we fitted the 7×7 TRM of the BWO feature on the samples of time-trees obtained in Step 2 of Section 3.4.",4.3 Variability of Diachronic Stability,[0],[0]
"For SOV and SVO, the probabilities based on the surface feature pointed to the modal probabilities based on the latent representations.",4.3 Variability of Diachronic Stability,[0],[0]
"This is somewhat surprising because we anticipated that the combination of the stochastic surface-to-latent and latentto-surface mappings would amplify uncertainty of estimation.
",4.3 Variability of Diachronic Stability,[0],[0]
"The two least common word orders, OVS (0.8%) and OSV (0.3%) exhibited huge gaps between the two types of probabilities.",4.3 Variability of Diachronic Stability,[0],[0]
"The probabilities based on the surface feature were consistently larger (i.e., more stable).",4.3 Variability of Diachronic Stability,[0],[0]
Surface featurebased estimation had no other way to explain the presence of these uncommon word orders than slowing down the convergence to the stationary distribution (otherwise they go extinct).,4.3 Variability of Diachronic Stability,[0],[0]
Maurits and Griffiths (2014) also reported some counterintuitive results regarding OVS and OSV.,4.3 Variability of Diachronic Stability,[0],[0]
"By contrast, latent parameter-based estimation appears to have explained the low frequencies partly with the stochasticity of observation associated with the latent-to-surface mapping.
",4.3 Variability of Diachronic Stability,[0],[0]
Now we attempt to explain the variability of diachronic stability.,4.3 Variability of Diachronic Stability,[0],[0]
"Although we have all model parameters in hand, it is not easy to manually ana-
lyze their complex dependencies.",4.3 Variability of Diachronic Stability,[0],[0]
The approach we adopt in the present study is to let a simpler model explain the model’s complex behavior.,4.3 Variability of Diachronic Stability,[0],[0]
"Specifically, we used linear regression with L1 regularization (i.e., lasso).",4.3 Variability of Diachronic Stability,[0],[0]
The hyperparameter was tuned using 3-fold cross-validation.,4.3 Variability of Diachronic Stability,[0],[0]
"For each word order i, the target variable was the average of θl′,f(n,i) while explanatory variables were the current surface features, xl,1, · · · , xl,N .",4.3 Variability of Diachronic Stability,[0],[0]
"For better interpretability, we excluded from explanatory variables surface features that trivially depended on the BWO feature (Takamura et al., 2016).",4.3 Variability of Diachronic Stability,[0],[0]
"Note that missing values were imputed in Step 1 of Section 3.4.
",4.3 Variability of Diachronic Stability,[0],[0]
"Tables 2 and 3 show the results of regression analysis for SOV and SVO languages, respectively.",4.3 Variability of Diachronic Stability,[0],[0]
"As expected, feature values typically associated with the specified word order had positive weights while negative weights indicate inconsistency.",4.3 Variability of Diachronic Stability,[0],[0]
"A stable SOV language may use prenominal relative clauses, postpositions and/or case suffixes.",4.3 Variability of Diachronic Stability,[0],[0]
"The trend was less clear for SVO languages, but those characterized by heavy use of prefixes were stable too.",4.3 Variability of Diachronic Stability,[0],[0]
"Interestingly, Feature 85A (Order of Adposition and Noun Phrase) had two positively weighted values: Prepositions and No adpositions.",4.3 Variability of Diachronic Stability,[0],[0]
"We speculate that SVO order is suitable for analytic languages that rely heavily on word ordering to encode syntactic structure (e.g., English and languages of Mainland Southeast Asia) but is not necessarily so for languages with rich morphological devices for marking syn-
tactic structure so that word ordering can relatively freely convey information structure.",4.3 Variability of Diachronic Stability,[0],[0]
"In Section 4.3, we suggested that stable SVO languages do not form a coherent group but can be grouped into at least two clusters.",4.4 Language-Specific Analysis,[0],[0]
"This can be confirmed in Table 4, where most of the most stable SVO languages exhibit either (1) little affixation or (2) strong prefixation.",4.4 Language-Specific Analysis,[0],[0]
"To analyze these languages in detail, we performed language-specific simulations.",4.4 Language-Specific Analysis,[0],[0]
"We chose Tetum and South-Central Kikongo as the examples of analytic and strongly prefixing languages, respectively.
",4.4 Language-Specific Analysis,[0],[0]
"Figure 5 shows the word order probabilities of
Tetum as a function of time.",4.4 Language-Specific Analysis,[0],[0]
"For each time t, we performed simulation 500 times for each of the 50 samples and took the average of the BWO probability vectors, θl′,f(n,1), · · · , θl′,f(n,Fn).",4.4 Language-Specific Analysis,[0],[0]
"According to our analysis, Tetum will remain SVO with a probability of 81.1% at t = 2,000.",4.4 Language-Specific Analysis,[0],[0]
"SVO was followed by SOV (8.4%) and No dominant order (5.3%).
",4.4 Language-Specific Analysis,[0],[0]
"What will the Austronesian language of East Timor look like in the future, if it switches to SOV?",4.4 Language-Specific Analysis,[0],[0]
"To answer this question, we performed regression analysis again with t = 2,000.",4.4 Language-Specific Analysis,[0],[0]
"For each word order i, the target variable was θl′,f(n,i) of each sample of simulated language l′ whereas explanatory variables were the items of the probability vector θl′,∗. In other words, we aimed at finding out features that were characteristic of the specified word order.",4.4 Language-Specific Analysis,[0],[0]
"As before, we removed surface features with trivial dependencies on the BWO feature (Takamura et al., 2016) as well as the BWO feature itself.
",4.4 Language-Specific Analysis,[0],[0]
Table 5 shows the result of regression analysis.,4.4 Language-Specific Analysis,[0],[0]
"If the relatively analytic language switches to SOV, Tetum will be characterized by a holistic reconfiguration.",4.4 Language-Specific Analysis,[0],[0]
"It is likely to develop suffixes and to replace prepositions with postposi-
tions.",4.4 Language-Specific Analysis,[0],[0]
"South-Central Kikongo is analyzed in the same manner, as shown in Figure 6 and Table 6.",4.4 Language-Specific Analysis,[0],[0]
The Bantu language of Africa is markedly different from Tetum as it is characterized by a higher tendency to switch to No dominant order.,4.4 Language-Specific Analysis,[0],[0]
"In this paper, we presented a new framework of latent representation-based analysis of diachronic typology, which enables us to investigate correlated evolution of multiple surface features in an exploratory manner.",5 Conclusion,[0],[0]
"We focused on the order of subject, object and verb as a proof-of-concept demonstration, but investigating other features would be fruitful too.",5 Conclusion,[0],[0]
We analyzed the estimated model parameters with simulation experiments.,5 Conclusion,[0],[0]
"In the future, we would like to investigate the inferred trees in detail.10",5 Conclusion,[0],[0]
The source code is publicly available at https://github.com/murawaki/ lattyp.,5 Conclusion,[0],[0]
"This work was partly supported by JSPS KAKENHI Grant Number 18K18104.
10",Acknowledgments,[0],[0]
A preliminary analysis is presented in Section S.2 of the supplementary materials.,Acknowledgments,[0],[0]
"Statistical phylogenetic models have allowed the quantitative analysis of the evolution of a single categorical feature and a pair of binary features, but correlated evolution involving multiple discrete features is yet to be explored.",abstractText,[0],[0]
Here we propose latent representation-based analysis in which (1) a sequence of discrete surface features is projected to a sequence of independent binary variables and (2) phylogenetic inference is performed on the latent space.,abstractText,[0],[0]
"In the experiments, we analyze the features of linguistic typology, with a special focus on the order of subject, object and verb.",abstractText,[0],[0]
Our analysis suggests that languages sharing the same word order are not necessarily a coherent group but exhibit varying degrees of diachronic stability depending on other features.,abstractText,[0],[0]
Analyzing Correlated Evolution of Multiple Features Using Latent Representations,title,[0],[0]
"Machine learning is increasingly applied in security-critical domains such as automotive systems, healthcare, finance and robotics.",1. Introduction,[0],[0]
"To ensure safe deployment in these applications, there is an increasing need to design machine-learning algorithms that are robust in the presence of adversarial attacks.
",1. Introduction,[0],[0]
"A realistic attack paradigm that has received a lot of recent attention (Goodfellow et al., 2014; Papernot et al., 2016a; Szegedy et al., 2013; Papernot et al., 2017b) is test-time
*Equal contribution 1University of California, San Diego 2University of Wisconsin-Madison.",1. Introduction,[0],[0]
"Correspondence to: Kamalika Chaudhuri <kamalika@cs.ucsd.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"1Code available at: https://github.com/ EricYizhenWang/robust_nn_icml
attacks via adversarial examples.",1. Introduction,[0],[0]
"Here, an adversary has the ability to provide modified test inputs to an alreadytrained classifier, but cannot modify the training process in any way.",1. Introduction,[0],[0]
Their goal is to perturb legitimate test inputs by a “small amount” in order to force the classifier to report an incorrect label.,1. Introduction,[0],[0]
An example is an adversary that replaces a stop sign by a slightly defaced version in order to force an autonomous vehicle to recognize it as an yield sign.,1. Introduction,[0],[0]
"This attack is undetectable to the human eye if the perturbation is small enough.
",1. Introduction,[0],[0]
"Prior work has considered adversarial examples in the context of linear classifiers (Lowd and Meek, 2005), kernel SVMs (Biggio et al., 2013) and neural networks (Szegedy et al., 2013; Goodfellow et al., 2014; Papernot et al., 2017b; 2016a; Moosavi-Dezfooli et al., 2016).",1. Introduction,[0],[0]
"However, most of this work has either been empirical, or has focussed on developing theoretically motivated attacks and defenses.",1. Introduction,[0],[0]
"Consequently, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood.
",1. Introduction,[0],[0]
This work develops a theoretical framework for robust learning in order to understand the effects of distributional properties and finite samples on robustness.,1. Introduction,[0],[0]
"Building on traditional bias-variance theory (Friedman et al., 2000), we posit that a classification algorithm may be robust to adversarial examples due to three reasons.",1. Introduction,[0],[0]
"First, it may be distributionally robust, in the sense that the output classifier is robust as the number of training samples grow to infinity.",1. Introduction,[0],[0]
"Second, even the output of a distributionally robust classification algorithm may be vulnerable due to too few training samples – this is characterized by finite sample robustness.",1. Introduction,[0],[0]
"Finally, different training algorithms might result in classifiers with different degrees of robustness, which we call algorithmic robustness.",1. Introduction,[0],[0]
"These quantities are analogous to bias, variance and algorithmic effects respectively.
",1. Introduction,[0],[0]
"Next, we analyze a simple non-parametric classification algorithm: k-nearest neighbors in our framework.",1. Introduction,[0],[0]
"Our analysis demonstrates that large sample robustness properties of this algorithm depend very much on k.
Specifically, we identify two distinct regimes for k with vastly different robustness properties.",1. Introduction,[0],[0]
"When k is constant, we show that k-nearest neighbors has zero robustness in the
large sample limit in regions where p(y = 1|x) lies in (0, 1).",1. Introduction,[0],[0]
"This is in contrast with accuracy, which may be quite high in these regions.",1. Introduction,[0],[0]
"For k = Ω( √ dn log n), where d is the data dimension and n is the sample size, we show that the robustness region of k-nearest neighbors approaches that of the Bayes Optimal classifier in the large sample limit.",1. Introduction,[0],[0]
"This is again in contrast with accuracy, where convergence to the Bayes Optimal accuracy is known for a much slower growing k (Devroye et al., 1994; Chaudhuri and Dasgupta, 2014).",1. Introduction,[0],[0]
"Since k = Ω( √ dn log n) is too high to use in practice with nearest neighbors, we next propose a novel robust version of the 1-nearest neighbor classifier that operates on a modified training set.",1. Introduction,[0],[0]
"We provably show that in the large sample limit, this algorithm has superior robustness to standard 1-nearest neighbors for data distributions with certain properties.
",1. Introduction,[0],[0]
"Finally, we validate our theoretical results by empirically evaluating our algorithm on three datasets against several popular attacks.",1. Introduction,[0],[0]
Our experiments demonstrate that our algorithm performs better than or about as well as both standard 1-nearest neighbors and nearest neighbors with adversarial training – a popular and effective defense mechanism.,1. Introduction,[0],[0]
"This suggests that although our performance guarantees hold in the large sample limit, our algorithm may have good robustness properties even for realistic training data sizes.",1. Introduction,[0],[0]
"Adversarial examples have recently received a great deal of attention (Goodfellow et al., 2014; Biggio et al., 2013; Papernot et al., 2016a; Szegedy et al., 2013; Papernot et al., 2017b).",1.1. Related Work,[0],[0]
"Most of the work, however, has been empirical, and has focussed on developing increasingly sophisticated attacks and defenses.",1.1. Related Work,[0],[0]
Prior theoretical work on adversarial examples falls into two categories – analysis and theory-inspired defenses.,1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"Work on analysis includes (Fawzi et al., 2016), which analyzes the robustness of linear and quadratic classifiers under random and semi-random perturbations.",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"(Hein and Andriushchenko, 2017) provides robustness guarantees on linear and kernel classifiers trained on a given data set.",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"(Gilmer et al., 2018) shows that linear classifiers for high dimensional datasets may have inherent robustness-accuracy trade-offs.
",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"Work on theory-inspired defenses include (Mądry et al., 2017; Kolter and Wong, 2017; Aman Sinha, 2018), who provide defense mechanisms for adversarial examples in neural networks that are relaxations of certain principled optimization objectives.",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"(Katz et al., 2017) shows how to use program verification to certify robustness of neural networks around given inputs for small neural networks.
",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
Our work differs from these in two important ways.,1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"First, unlike most prior work which looks at a given training dataset, we consider effects of the data distribution and number of samples, and analyze robustness properties in the large sample limit.",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"Second, unlike prior work which largely focuses on parametric methods such as neural networks, our focus is on a canonical non-parametric method – the nearest neighbors classifier.",1.1.1. RELATED WORK ON ADVERSARIAL EXAMPLES,[0],[0]
"There has been a body of work on the convergence and consistency of nearest-neighbor classifiers and their many variants (Cover and Hart, 1967; Stone, 1977; Kulkarni and Posner, 1995; Devroye and Wagner, 1977; Chaudhuri and Dasgupta, 2014; Kontorovich and Weiss, 2015); all these works however consider accuracy and not robustness.
",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"In the asymptotic regime, (Cover and Hart, 1967) shows that the accuracy of 1-nearest neighbors converges in the large sample limit to 1 − 2R∗(1 − R∗) where R∗ is the expected error rate of the Bayes Optimal classifier.",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
This implies that even 1-nearest neighbor may achieve relatively high accuracy even when p(y = 1|x) is not 0 or 1.,1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"In contrast, we show that 1-nearest neighbor is inherently nonrobust when p(y = 1|x) ∈ (0, 1) under some continuity conditions.
",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"For larger k, the accuracy of k-nearest neighbors is known to converge to that of the Bayes Optimal classifier if kn → ∞ and kn/n → 0",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
as the sample size n → ∞.,1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
We show that the robustness also converges to that of the Bayes Optimal classifier when kn grows at a much higher rate – fast enough to ensure uniform convergence.,1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"Whether this high rate is necessary remains an intriguing open question.
",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"Finite sample rates on the accuracy of nearest neighbors are known to depend heavily on properties of the data distribution, and there is no distribution free rate as in parametric methods (Devroye and Wagner, 1977).",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"(Chaudhuri and Dasgupta, 2014) provides a clean characterization of the finite sample rates of nearest neighbors as a function of natural interiors of the classes.",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"Here we build on their results by defining a stricter, more robust version of interiors and providing bounds as functions of these new robust quantities.",1.1.2. RELATED WORK ON NEAREST NEIGHBORS,[0],[0]
"(Amsaleg et al., 2016) provides a method for generating adversarial examples for nearest neighbors, and shows that the effectiveness of attacks grow with intrinsic dimensionality.",1.1.3. OTHER RELATED WORK,[0],[0]
"Finally, (Papernot et al., 2016b; 2017b) provides black-box attacks on substitute classifiers; their experiments show that attacks from other types of substitute classifiers are not successful on nearest neighbors; our experiments corroborate these results.
2.",1.1.3. OTHER RELATED WORK,[0],[0]
The Setting and Definitions,1.1.3. OTHER RELATED WORK,[0],[0]
"We consider test-time attacks in a white box setting, where the adversary has full knowledge of the training process – namely, the type of classifier used, the training data and any parameters – but cannot modify training in any way.
",2.1. The Basic Setup,[0],[0]
"Given an input x, the adversary’s goal is to perturb it so as to force the trained classifier f to report a different label than f(x).",2.1. The Basic Setup,[0],[0]
"The amount of perturbation is measured by an application-specific metric d, and is constrained to be within a radius r.",2.1. The Basic Setup,[0],[0]
"Our analysis can be extended to any metric, but for this paper we assume that d is the Euclidean distance for mathematical simplicity; we also focus on binary classification, and leave extensions to multiclass for future work.
",2.1. The Basic Setup,[0],[0]
"Finally, we assume that unlabeled instances are drawn from an instance space X , and their labels are drawn from the label space {0, 1}.",2.1. The Basic Setup,[0],[0]
There is an underlying data distribution D that generates labeled examples; the marginal over X of D is µ and the conditional distribution of labels given x is denoted by η.,2.1. The Basic Setup,[0],[0]
"We begin by defining robustness, which for a classifier f at input x is measured by the robustness radius.",2.2. Robustness and astuteness,[0],[0]
Definition 2.1 (Robustness Radius).,2.2. Robustness and astuteness,[0],[0]
"The robustness radius of a classifier f at an instance x ∈ X , denoted by ρ(f, x), is the shortest distance between x and an input x� to which f assigns a label different from f(x):
ρ(f, x) = inf r {∃x� ∈ X ∩B(x, r) s.t f(x) �= f(x�)}
Observe that the robustness radius measures a classifier’s local robustness.",2.2. Robustness and astuteness,[0],[0]
A classifier f with robustness radius r at x guarantees that no adversarial example of x with norm of perturbation less than r can be created using any attack method.,2.2. Robustness and astuteness,[0],[0]
"A plausible way to extend this into a global notion is to require a lower bound on the robustness radius everywhere; however, only the constant classifier will satisfy this condition.",2.2. Robustness and astuteness,[0],[0]
"Instead, we consider robustness around meaningful instances, that we model as examples drawn from the underlying data distribution.",2.2. Robustness and astuteness,[0],[0]
Definition 2.2 (Robustness with respect to a Distribution).,2.2. Robustness and astuteness,[0],[0]
"The robustness of a classifier f at radius r with respect to a distribution µ over the instance space X , denoted by R(f, r, µ), is the fraction of instances drawn from µ for which the robustness radius is greater than or equal to r.
R(f, r, µ) =",2.2. Robustness and astuteness,[0],[0]
"Pr x∼µ
(ρ(f, x) ≥ r)
Finally, observe that we are interested in classifiers that are both robust and accurate.",2.2. Robustness and astuteness,[0],[0]
"This leads to the notion of
astuteness, which measures the fraction of instances on which a classifier is both accurate and robust.
",2.2. Robustness and astuteness,[0],[0]
Definition 2.3 (astuteness).,2.2. Robustness and astuteness,[0],[0]
"The astuteness of a classifier f with respect to a data distribution D and a radius r is the fraction of examples on which it is accurate and has robustness radius at least r; formally,
AstD(f, r) =",2.2. Robustness and astuteness,[0],[0]
"Pr (x,y)∼D
(ρ(f, x) ≥ r, f(x) = y),
Observe that astuteness is analogous to classification accuracy, and we argue that it is a more appropriate metric if we are concerned with both robustness and accuracy.",2.2. Robustness and astuteness,[0],[0]
"Unlike accuracy, astuteness cannot be directly empirically measured unless we have a way to certify a lower bound on the robustness radius.",2.2. Robustness and astuteness,[0],[0]
"In this work, we will prove bounds on the astuteness of classifiers, and in our experiments, we will approximate it by measuring resistance to standard attacks.",2.2. Robustness and astuteness,[0],[0]
"There are three plausible reasons why classifiers lack robustness – distributional, finite sample and algorithmic.",2.3. Sources of Robustness,[0],[0]
"These sources are analogous to bias, variance, and algorithmic effects respectively in standard bias-variance theory.
",2.3. Sources of Robustness,[0],[0]
Distributional robustness measures the effect of the data distribution on robustness when an infinitely large number of samples are used to train the classifier.,2.3. Sources of Robustness,[0],[0]
"Formally, if Sn is a training sample of size n drawn from D and A(Sn, ·) is a classifier obtained by applying the training procedure A on Sn, then the distributional robustness at radius r is limn→∞ ESn∼D[R(A(Sn, ·), r, µ)].",2.3. Sources of Robustness,[0],[0]
"In contrast, for finite sample robustness, we characterize the behaviour of R(A(Sn, ·), r, µ) for finite n – usually by putting high probability bounds over the training set.",2.3. Sources of Robustness,[0],[0]
"Thus, finite sample robustness depends on the training set size n, and quantifies how it changes with sample size.",2.3. Sources of Robustness,[0],[0]
"Finally, robustness also depends on the training algorithm itself; for example, some variants of nearest neighbors may have higher robustness than nearest neighbors itself.
2.4.",2.3. Sources of Robustness,[0],[0]
"Nearest Neighbor and Bayes Optimal Classifiers
",2.3. Sources of Robustness,[0],[0]
"Given a training set Sn = {(X1, Y1), . . .",2.3. Sources of Robustness,[0],[0]
", (Xn, Yn)} and a test example x, we use the notation X(i)(x) to denote the i-th nearest neighbor of x in Sn, and Y (i)(x) to denote the label of X(i)(x).
",2.3. Sources of Robustness,[0],[0]
"Given a test example x, the k-nearest neighbor classifier Ak(Sn, x) outputs:
= 1, if Y (1)(x) + . .",2.3. Sources of Robustness,[0],[0]
.+,2.3. Sources of Robustness,[0],[0]
"Y (k)(x) ≥ k/2 = 0, otherwise.
",2.3. Sources of Robustness,[0],[0]
"The Bayes optimal classifier g over a data distribution D
has the following classification rule:
g(x) = � 1 if η(x) =",2.3. Sources of Robustness,[0],[0]
Pr(y = 1|x) ≥ 1/2; 0 otherwise.,2.3. Sources of Robustness,[0],[0]
(1),2.3. Sources of Robustness,[0],[0]
How robust is the k-nearest neighbor classifier?,3. Robustness of Nearest Neighbors,[0],[0]
"We show that it depends on the value of k. Specifically, we identify two distinct regimes – constant k and k = Ω( √ dn log n) where d is the data dimension – and show that nearest neighbors has different robustness properties in the two.",3. Robustness of Nearest Neighbors,[0],[0]
"In this region, k is a constant that does not depend on the training set size n. Provided certain regularity conditions hold, we show that k-nearest neighbors is inherently nonrobust in this regime unless η(x) ∈ {0, 1} – in the sense that the distributional robustness becomes 0 in the large sample limit.",3.1. Low k Regime,[0],[0]
Theorem 3.1.,3.1. Low k Regime,[0],[0]
"Let x ∈ X ∩ supp(µ) such that (a) µ is absolutely continuous with respect to the Lebesgue measure (b) η(x) ∈ (0, 1) (c) η is continuous with respect to the Euclidean metric in a neighborhood of x. Then, for fixed k, ρ(Ak(Sn, ·), x) converges in probability to 0.
",3.1. Low k Regime,[0],[0]
Remarks.,3.1. Low k Regime,[0],[0]
"Observe that Theorem 3.1 implies that the distributional robustness (and hence astuteness) in a region where η(x) ∈ (0, 1) is 0.",3.1. Low k Regime,[0],[0]
"This is in contrast with accuracy; for 1-NN, the accuracy converges to 1 − 2R∗(1 − R∗) as n → ∞, where R∗ is the error rate of the Bayes Optimal classifier, and thus may be quite high.
",3.1. Low k Regime,[0],[0]
"The proof of Theorem 3.1 in the Appendix shows that the absolute continuity of µ with respect to the Lebesgue measure is not strictly necessary; absolute continuity with respect to an embedded manifold will give the same result, but will result in a more complex proof.
",3.1. Low k Regime,[0],[0]
"In the Appendix A (Theorem A.2), we show that k-nearest neighbor is astute in the interior of the region where η ∈ {0, 1}, and provide finite sample rates for this case.",3.1. Low k Regime,[0],[0]
"Prior work has shown that in the large sample limit, the accuracy of the nearest neighbor classifiers converge to the Bayes Optimal, provided k is set properly.",3.2. High k Regime,[0],[0]
"We next show that if k is Ω( √ dn log n), the regions of robustness and the astuteness of the k nearest neighbor classifiers also approach the corresponding quantities for the Bayes Optimal classifier as n → ∞. Thus, if the Bayes Optimal classifier is robust, then so is k-nearest neighbors in the large sample limit.",3.2. High k Regime,[0],[0]
"The main intuition is that k = Ω( √ dn log n) is large enough for uniform convergence – where, with high probability, all
Euclidean balls with k examples have the property that the empirical averages of their labels are close to their expectations.",3.2. High k Regime,[0],[0]
"This guarantees that for any x, the k-nearest neighbor reports the same label as the Bayes Optimal classifier for all x� close to x.",3.2. High k Regime,[0],[0]
"Thus, if the Bayes Optimal classifier is robust, so is nearest neighbors.",3.2. High k Regime,[0],[0]
We begin with some definitions that we can use to characterize the robustness of the Bayes Optimal classifier.,3.2.1. DEFINITIONS,[0],[0]
"Following (Chaudhuri and Dasgupta, 2014), we use the notation Bo(x, r) to denote an open ball and B(x, r) to denote a closed ball of radius r around x.",3.2.1. DEFINITIONS,[0],[0]
"We define the probability radius of a ball around x as:
rp(x) = inf{r | µ(B(x, r))",3.2.1. DEFINITIONS,[0],[0]
"≥ p}
We next define the r-robust (p,Δ)-strict interiors as follows:
X+r,Δ,p = {x ∈ supp(µ) | ∀x� ∈ Bo(x, r), ∀x�� ∈ B(x�, rp(x�)), η(x��) > 1/2",3.2.1. DEFINITIONS,[0],[0]
"+Δ} X−r,Δ,p = {x ∈ supp(µ)",3.2.1. DEFINITIONS,[0],[0]
"| ∀x� ∈ Bo(x, r), ∀x�� ∈ B(x�, rp(x�)), η(x��) < 1/2−Δ}
What is the significance of these interiors?",3.2.1. DEFINITIONS,[0],[0]
"Let x� be an instance such that all x�� ∈ B(x�, rp(x�)) have η(x��) > 1/2",3.2.1. DEFINITIONS,[0],[0]
+Δ.,3.2.1. DEFINITIONS,[0],[0]
"If p ≈ kn , then the k points x�� closest to x� have η(x��) > 1/2 + Δ. Provided the average of the labels of these points is close to expectation, which happens when k is large relative to 1/Δ, k-nearest neighbor outputs label 1 on x�.",3.2.1. DEFINITIONS,[0],[0]
"When x is in the r-robust (p,Δ)-strict interior region X+r,Δ,p, this is true for all x� within distance r of x, which means that k-nearest neighbors will be robust at x.",3.2.1. DEFINITIONS,[0],[0]
"Thus, the r-robust (p,Δ)-strict interior is the region where we natually expect k-nearest neighbor to have robustness radius r, when k is large relative to 1Δ and",3.2.1. DEFINITIONS,[0],[0]
p ≈ kn .,3.2.1. DEFINITIONS,[0],[0]
"Readers familiar with (Chaudhuri and Dasgupta, 2014) will observe that the set of all x� for which ∀x�� ∈ B(x�, rp(x�)), η(x��) > 1/2",3.2.1. DEFINITIONS,[0],[0]
"+Δ forms a stricter version of the (p,Δ)-interiors of the 1 region that was defined in this work; these x� also represent the region where k-nearest neighbors are accurate when k ≈ max(np, 1/Δ2).",3.2.1. DEFINITIONS,[0],[0]
"The rrobust (p,Δ)-strict interior is thus a somewhat stricter and more robust version of this definition.",3.2.1. DEFINITIONS,[0],[0]
"We begin by characterizing where the Bayes Optimal classifier is robust.
",3.2.2. MAIN RESULTS,[0],[0]
Theorem 3.2.,3.2.2. MAIN RESULTS,[0],[0]
"The Bayes Optimal classifier has robustness radius r at x ∈ X+r,0,0 ∪ X−r,0,0.",3.2.2. MAIN RESULTS,[0],[0]
"Moreover, its astuteness is E[η(x)1(x ∈ X+r,0,0)]",3.2.2. MAIN RESULTS,[0],[0]
"+ E[(1− η(x))1(x ∈ X−r,0,0)].
",3.2.2. MAIN RESULTS,[0],[0]
"The proof is in the Appendix, along with analogous results for astuteness.",3.2.2. MAIN RESULTS,[0],[0]
"The following theorem, along with a similar result for astuteness, proved in the Appendix, characterizes robustness in the large k regime.
",3.2.2. MAIN RESULTS,[0],[0]
Theorem 3.3.,3.2.2. MAIN RESULTS,[0],[0]
"For any n, pick a δ and a Δn → 0.",3.2.2. MAIN RESULTS,[0],[0]
"There exist constant C1 and C2 such that if
kn ≥ C1 √ dn log n+n log(1/δn)
",3.2.2. MAIN RESULTS,[0],[0]
"Δn , and pn ≥ knn (1 +
C2
� d log n+log(1/δ)
kn ), then, with probability ≥ 1− 3δ, kn-
NN has robustness radius r in x ∈ X+r,Δn,pn ∪ X",3.2.2. MAIN RESULTS,[0],[0]
"− r,Δn,pn .
Remarks.",3.2.2. MAIN RESULTS,[0],[0]
Some remarks are in order.,3.2.2. MAIN RESULTS,[0],[0]
"First, observe that as n → ∞, Δn and pn tend to 0; thus, provided certain continuity conditions hold, X+r,Δn,pn ∪X − r,Δn,pn
approaches X+r,0,0 ∪ X−r,0,0, the robustness region of the Bayes Optimal classifier.
",3.2.2. MAIN RESULTS,[0],[0]
"Second, observe that as r-robust strict interiors extend the definition of interiors in (Chaudhuri and Dasgupta, 2014), Theorem 3.3 is a robustness analogue of Theorem 5 in this work.",3.2.2. MAIN RESULTS,[0],[0]
"Unlike the latter, Theorem 3.3 has a more stringent requirement on k. Whether this is necessary is left as an open question for future work.",3.2.2. MAIN RESULTS,[0],[0]
Section 3 shows that nearest neighbors is robust for k as large as Ω( √ dn log n).,4. A Robust 1-NN Algorithm,[0],[0]
"However, this k is too high to use in practice – high values of k require even higher sample sizes (Chaudhuri and Dasgupta, 2014), and lead to higher running times.",4. A Robust 1-NN Algorithm,[0],[0]
"Thus a natural question is whether we can find a more robust version of the algorithm for smaller k. In this section, we provide a more robust version of 1-nearest neighbors, and analytically demonstrate its robustness.
",4. A Robust 1-NN Algorithm,[0],[0]
"Our algorithm is motivated by the observation that 1-nearest neighbor is robust when oppositely labeled points are far apart, and when test points lie close to training data.",4. A Robust 1-NN Algorithm,[0],[0]
"Most training datasets however contain nearby points that are oppositely labeled; thus, we propose to remove a subset of training points to enforce this property.
",4. A Robust 1-NN Algorithm,[0],[0]
Which points should we remove?,4. A Robust 1-NN Algorithm,[0],[0]
"A plausible approach is to keep the largest subset where oppositely labeled points are far apart; however, this subset has poor stability properties even for large n. Therefore, we propose to keep all points x such that: (a) we are highly confident about the label of x and its nearby points and (b) all points close to x have the same label.",4. A Robust 1-NN Algorithm,[0],[0]
"Given that all such x are kept, we remove as few points as possible, and execute nearest neighbors on the remaining dataset.
",4. A Robust 1-NN Algorithm,[0],[0]
"The following definition characterizes data where oppositely labeled points are far apart.
",4. A Robust 1-NN Algorithm,[0],[0]
Definition 4.1 (r-separated set).,4. A Robust 1-NN Algorithm,[0],[0]
"A set A = {(x1, y1), . . .",4. A Robust 1-NN Algorithm,[0],[0]
", (xm, ym)} of labeled examples is said to be r-separated if for all pairs (xi, yi), (xj , yj) ∈",4. A Robust 1-NN Algorithm,[0],[0]
"A, �xi − xj� ≤ r implies yi = yj .
",4. A Robust 1-NN Algorithm,[0],[0]
The full algorithm is described in Algorithm 1 and Algorithm 2.,4. A Robust 1-NN Algorithm,[0],[0]
"Given confidence parameters Δ and δ, Algorithm 2 returns a 0/1 label when this label agrees with the average of kn points closest to x; otherwise, it returns ⊥. kn is chosen such that with probability ≥ 1− δ, the empirical majority of kn labels agrees with the majority in expectation, provided the latter is at least Δ away from 12 .
",4. A Robust 1-NN Algorithm,[0],[0]
Algorithm 2 is used to determine whether an xi should be kept.,4. A Robust 1-NN Algorithm,[0],[0]
Let f(xi) be the output of Algorithm 2 on xi.,4. A Robust 1-NN Algorithm,[0],[0]
"If yi = f(xi) and if for all xj ∈ B(xi, r), f(xi) = f(xj) = yi, then we mark xi as red.",4. A Robust 1-NN Algorithm,[0],[0]
"Finally, we compute the largest r-separated subset of the training data that includes all the red points; this reduces to a constrained matching problem as in (Kontorovich and Weiss, 2015).",4. A Robust 1-NN Algorithm,[0],[0]
"The resulting set, returned by Algorithm 1, is our new training set.",4. A Robust 1-NN Algorithm,[0],[0]
"We observe that this set is r-separated from Lemma B.2 in the Appendix, and thus oppositely labeled points are far apart.",4. A Robust 1-NN Algorithm,[0],[0]
"Moreover, we keep all (xi, yi) when we are confident about the label of xi and its nearby points.",4. A Robust 1-NN Algorithm,[0],[0]
"Observe that our final procedure is a 1-NN algorithm, even though kn neighbors are used to determine if a point should be retained in the training set.",4. A Robust 1-NN Algorithm,[0],[0]
The following theorem establishes performance guarantees for Algorithm 1.,4.1. Performance Guarantees,[0],[0]
Theorem 4.2.,4.1. Performance Guarantees,[0],[0]
"Pick a Δn and δ, and set kn = 3 log(2n/δ)/Δ2n.",4.1. Performance Guarantees,[0],[0]
Pick a margin parameter τ .,4.1. Performance Guarantees,[0],[0]
"Then, there exist constants C and C0 such that the following hold.",4.1. Performance Guarantees,[0],[0]
"If we
set pn = knn (1 + C � d log n+log(1/δ) kn ), and define the set:
XR = � x ���x ∈ X+r+τ,Δn,pn ∪ X − r+τ,Δn,pn ,
µ(B(x, τ))",4.1. Performance Guarantees,[0],[0]
≥ 2C0 n,4.1. Performance Guarantees,[0],[0]
"(d log n+ log(1/δ))
",4.1. Performance Guarantees,[0],[0]
"�
Then, with probability ≥ 1 − 2δ over the training set, Algorithm 1 run with parameters r, Δn and δ has robustness radius at least r − 2τ on XR.
",4.1. Performance Guarantees,[0],[0]
Remarks.,4.1. Performance Guarantees,[0],[0]
"The proof is in the Appendix, along with an analogous result for astuteness.",4.1. Performance Guarantees,[0],[0]
"Observe that XR is roughly the high density subset of the r + τ -robust strict interior X+r+τ,Δn,pn ∪ X − r+τ,Δn,pn
.",4.1. Performance Guarantees,[0],[0]
Since η(x) is constrained to be greater than 12 + Δn or less than 1 2,4.1. Performance Guarantees,[0],[0]
"− Δn in this region, as opposed to 0 or 1, this is an improvement over standard nearest neighbors when the data distribution has a large high density region that intersects with the interiors.
",4.1. Performance Guarantees,[0],[0]
"A second observation is that as τ is an arbitrary constant, we can set to it be quite small and still satisfy the condition on µ(B(x, τ)) for a large fraction of x’s when n is very large.",4.1. Performance Guarantees,[0],[0]
"This means that in the large sample limit, r",4.1. Performance Guarantees,[0],[0]
"− 2τ may be close to r and XR may be close to the high density subset of X+r,Δn,pn ∪ X",4.1. Performance Guarantees,[0],[0]
"− r,Δn,pn for a lot of smooth distributions.
",4.1. Performance Guarantees,[0],[0]
"Algorithm 1 Robust_1NN(Sn, r, Δ, δ, x) for (xi, yi) ∈",4.1. Performance Guarantees,[0],[0]
"Sn do
f(xi) =",4.1. Performance Guarantees,[0],[0]
"Confident-Label(Sn,Δ, δ, xi) end for SRED = ∅ for (xi, yi) ∈",4.1. Performance Guarantees,[0],[0]
"Sn do
if f(xi) = yi and f(xi) = f(xj) for all xj such that �xi − xj� ≤ r and (xj , yj) ∈",4.1. Performance Guarantees,[0],[0]
"Sn then SRED = SRED
�{(xi, yi)} end if
end for Let S� be the largest r-separated subset of Sn that contains all points in SRED.",4.1. Performance Guarantees,[0],[0]
"return new training set S�
Algorithm 2 Confident-Label(Sn, Δ, δ, x) kn = 3 log(2n/δ)/Δ 2
ȳ =",4.1. Performance Guarantees,[0],[0]
(1/kn) �,4.1. Performance Guarantees,[0],[0]
kn i=1,4.1. Performance Guarantees,[0],[0]
Y (i)(x),4.1. Performance Guarantees,[0],[0]
"if ȳ ∈ [ 12 −Δ, 12 +Δ] then return ⊥ else return 12sgn(ȳ − 12 )",4.1. Performance Guarantees,[0],[0]
+ 12 end if,4.1. Performance Guarantees,[0],[0]
The results in Section 4 assume large sample limits.,5. Experiments,[0],[0]
"Thus, a natural question is how well Algorithm 1 performs with more reasonable amounts of training data.",5. Experiments,[0],[0]
"We now empirically investigate this question.
",5. Experiments,[0],[0]
"Since there are no general methods that certify robustness at an input, we assess robustness by measuring how our algorithm performs against a suite of standard attack methods.",5. Experiments,[0],[0]
"Specifically, we consider the following questions:
1.",5. Experiments,[0],[0]
"How does our algorithm perform against popular white box and black box attacks compared with standard baselines?
2.",5. Experiments,[0],[0]
"How is performance affected when we change the training set size relative to the data dimension?
",5. Experiments,[0],[0]
"These questions are considered in the context of three datasets with varying training set sizes relative to the dimension, as well as two standard white box attacks and black box attacks with two kinds of substitute classifiers.",5. Experiments,[0],[0]
Data.,5.1. Methodology,[0],[0]
"We use three datasets – Halfmoon, MNIST 1v7 and Abalone – with differing data sizes relative to dimension.",5.1. Methodology,[0],[0]
Halfmoon is a popular 2-dimensional synthetic data set for non-linear classification.,5.1. Methodology,[0],[0]
We use a training set of size 2000 and a test set of size 1000 generated with standard deviation σ = 0.2.,5.1. Methodology,[0],[0]
The MNIST 1v7 data set is a subset of the 784- dimensional MNIST data.,5.1. Methodology,[0],[0]
"For training, we use 1000 images each of Digit 1 and 7, and for test, 500 images of each digit.",5.1. Methodology,[0],[0]
"Finally, for the Abalone dataset (Lichman, 2013), our classification task is to distinguish whether an abalone is older than 12.5 years based on 7 physical measurements.",5.1. Methodology,[0],[0]
"For training, we use 500 and for test, 100 samples.",5.1. Methodology,[0],[0]
"In addition, a validation set with the same size as the test set is generated for each experiment for parameter tuning.
Baselines.",5.1. Methodology,[0],[0]
"We compare Algorithm 1, denoted by RobustNN, against three baselines.",5.1. Methodology,[0],[0]
"The first is the standard 1-nearest neighbor algorithm, denoted by StandardNN.",5.1. Methodology,[0],[0]
We use two forms of adversarially-trained nearest neighbors - ATNN and ATNN-all.,5.1. Methodology,[0],[0]
Let S be the training set used by standard nearest neighbors.,5.1. Methodology,[0],[0]
"In ATNN, we augment S by creating, for each (x, y) ∈ S, an adversarial example xadv using the attack method in the experiment, and adding (xadv, y).",5.1. Methodology,[0],[0]
The ATNN classifier is 1-nearest neighbor on this augmented data.,5.1. Methodology,[0],[0]
"In ATNN-all, for each (x, y) ∈ S, we create adversarial examples using all the attack methods in the experiment, and add them all to S. ATNN-all is the nearest neighbor classifier on this augmented data.",5.1. Methodology,[0],[0]
"For example, for white box Direct Attacks in Section 5.2, ATNN includes adversarial examples generated by the Direct Attack, and ATNN-all includes adversarial examples generated by both Direct and Kernel Substitute Attacks.
",5.1. Methodology,[0],[0]
Observe that all algorithms except StandardNN have parameters to tune.,5.1. Methodology,[0],[0]
"RobustNN has three input parameters – Δ, δ and a defense radius r which is an approximation to the robustness radius.",5.1. Methodology,[0],[0]
"For simplicity, we set Δ = 0.45, δ = 0.1 and tune r on the validation set; this can be viewed as tuning the parameter τ in Theorem 4.2.",5.1. Methodology,[0],[0]
"For ATNN and ATNN-all, the methods that generate the augmenting adversarial examples need a perturbation magnitude r; we call this the defense radius.",5.1. Methodology,[0],[0]
"To be fair to all algorithms, we tune the defense radius for each.",5.1. Methodology,[0],[0]
"We consider the adversary with the highest attack perturbation magnitude in the experiment, and select the defense radius that yields the highest validation accuracy against this adversary.",5.1. Methodology,[0],[0]
"To evaluate the robustness of Algorithm 1, we use two standard classes of attacks – white box and black box.",5.2. White-box Attacks and Results,[0],[0]
"For white-box attacks, the adversary knows all details about the classifier under attack, including its training data, the
training algorithm and any hyperparameters.",5.2. White-box Attacks and Results,[0],[0]
"We consider two white-box attacks – direct attack (Amsaleg et al., 2016) and Kernel Substitute Attack (Papernot et al., 2016b).
",5.2.1. ATTACK METHODS,[0],[0]
Direct Attack.,5.2.1. ATTACK METHODS,[0],[0]
"This attack takes as input a test example x, an attack radius r, and a training dataset S (which may be an augmented or reduced dataset).",5.2.1. ATTACK METHODS,[0],[0]
"It finds an x� ∈ S that is closest to x but has a different label, and returns the adversarial example xadv = x+ r x−x �
||x−x�||2 .
",5.2.1. ATTACK METHODS,[0],[0]
Kernel Substitute Attack.,5.2.1. ATTACK METHODS,[0],[0]
This method attacks a substitute kernel classifier trained on the same training set.,5.2.1. ATTACK METHODS,[0],[0]
"For a test input �x, a set of training points Z with one-hot labels Y , a
kernel classifier f predicts the class probability as:
f : �x →
� e−||�x−�z|| 2 2/c � �z∈X�
�z∈X e −||�x−�z||22/c
· Y
The adversary trains a kernel classifier on the training set of the corresponding nearest neighbors, and then generates adversarial examples against this kernel classifier.",5.2.1. ATTACK METHODS,[0],[0]
"The advantage is that the prediction of the kernel classifier is differentiable, which allows the use of standard gradientbased attack methods.",5.2.1. ATTACK METHODS,[0],[0]
"For our experiments, we use the popular fast-gradient-sign method (FSGM).",5.2.1. ATTACK METHODS,[0],[0]
"The parameter c is tuned to yield the most effective attack, and is set to 0.1 for Halfmoon and MNIST, and 0.01 for Abalone.",5.2.1. ATTACK METHODS,[0],[0]
Figure 1 shows the results.,5.2.2. RESULTS,[0],[0]
We see that RobustNN outperforms all baselines for Halfmoon and Abalone for all attack radii.,5.2.2. RESULTS,[0],[0]
"For MNIST, for low attack radii, RobustNN’s classification accuracy is slightly lower than the others, while it outperforms the others for large attack radii.",5.2.2. RESULTS,[0],[0]
"Additionally, as is to be expected, the Direct Attack results in lower general accuracy than the Kernel Substitute Attack.
",5.2.2. RESULTS,[0],[0]
"These results suggest that our algorithm mostly outperforms the baselines StandardNN, ATNN and ATNN-all.",5.2.2. RESULTS,[0],[0]
"As predicted by theory, the performance gain is higher when the training set size is large relative to the dimension – which is the setting where nearest neighbors work well in general.",5.2.2. RESULTS,[0],[0]
"It has superior performance for Halfmoon and Abalone, where the training set size is large to medium relative to dimension.",5.2.2. RESULTS,[0],[0]
"In contrast, in the sparse dataset MNIST, our algorithm has slightly lower classification accuracy for small attack radii, and higher otherwise.",5.2.2. RESULTS,[0],[0]
"(Papernot et al., 2017b) has observed that some defense methods that work by masking gradients remain highly amenable to black box attacks.",5.3. Black-box Attacks and Results,[0],[0]
"In this attack, the adversary is unaware of the target classifier’s nature, parameters or training data, but has access to a seed dataset drawn from the same distribution which they use to train and attack a substitute classifier.",5.3. Black-box Attacks and Results,[0],[0]
"To establish robustness properties of Algorithm 1, we therefore validate it against black box attacks based on two types of substitute classifiers.",5.3. Black-box Attacks and Results,[0],[0]
We use two types of substitute classifiers – kernel classifiers and neural networks.,5.3.1. ATTACK METHODS,[0],[0]
"The adversary trains the substitute classifier using the method of (Papernot et al., 2017b) and uses the adversarial examples against the substitute to attack the target classifier.
Kernel Classifier.",5.3.1. ATTACK METHODS,[0],[0]
"The kernel classifier substitute is the same as the one in Section 5.2, but trained using the seed data and the method of (Papernot et al., 2017b).
",5.3.1. ATTACK METHODS,[0],[0]
Neural Networks.,5.3.1. ATTACK METHODS,[0],[0]
"The neural network for MNIST is the ConvNet in (Papernot et al., 2017a)’s tutorial.",5.3.1. ATTACK METHODS,[0],[0]
"For Halfmoon and Abalone, the network is a multi-layer perceptron with 2 hidden layers.
",5.3.1. ATTACK METHODS,[0],[0]
Procedure.,5.3.1. ATTACK METHODS,[0],[0]
"To train the substitute classifier, the adversary uses the method of (Papernot et al., 2016b) to augment the seed data for two rounds; labels are obtained by querying the target classifier.",5.3.1. ATTACK METHODS,[0],[0]
"Adversarial examples against the substitutes are created by FGSM, following (Papernot et al., 2016b).",5.3.1. ATTACK METHODS,[0],[0]
"As a sanity check, we verify the performance of the substitute classifiers on the original training and test sets.",5.3.1. ATTACK METHODS,[0],[0]
"Details are in
Table 1 in the Appendix.",5.3.1. ATTACK METHODS,[0],[0]
Sanity checks on the performance of the substitute classifiers are presented in Table 1 in the Appendix.,5.3.1. ATTACK METHODS,[0],[0]
Figure 2 shows the results.,5.3.2. RESULTS,[0],[0]
"For all algorithms, black box attacks are less effective than white box, which corroborates the results of (Papernot et al., 2016b), who observed that black-box attacks are less successful against nearest neighbors.",5.3.2. RESULTS,[0],[0]
"We also find that the kernel substitute attack is more effective than the neural network substitute, which is expected as kernel classifiers have similar structure to nearest neighbors.",5.3.2. RESULTS,[0],[0]
"Finally, for Halfmoon and Abalone, our algorithm outperforms the baselines for both attacks; however, for MNIST neural network substitute, our algorithm does not perform as well for small attack radii.",5.3.2. RESULTS,[0],[0]
This again confirms the theoretical predictions that our algorithm’s performance is better when the training set is large relative to the data dimension – the setting in which nearest neighbors work well in general.,5.3.2. RESULTS,[0],[0]
The results show that our algorithm performs either better than or about the same as standard baselines against popular white box and black box attacks.,5.4. Discussion,[0],[0]
"As expected from our theoretical results, it performs better for denser datasets which have high or medium amounts of training data relative to the dimension, and either slightly worse or better for sparser datasets, depending on the attack radius.",5.4. Discussion,[0],[0]
"Since nonparametric methods such as nearest neighbors are mostly used for dense data, this suggests that our algorithm has good robustness properties even with reasonable amounts of training data.",5.4. Discussion,[0],[0]
"We introduce a novel theoretical framework for learning robust to adversarial examples, and introduce notions of distributional and finite-sample robustness.",6. Conclusion,[0],[0]
"We use these notions to analyze a non-parametric classifier, k-nearest neighbors, and introduce a novel modified 1-nearest neighbor algorithm that has good robustness properties in the large sample limit.",6. Conclusion,[0],[0]
"Experiments show that these properties are still retained for reasonable data sizes.
",6. Conclusion,[0],[0]
Many open questions remain.,6. Conclusion,[0],[0]
The first is to close the gap in analysis of k-nearest neighbors for k in between our two regimes.,6. Conclusion,[0],[0]
The second is to develop nearest neighbor algorithms with better robustness guarantees.,6. Conclusion,[0],[0]
"Finally, we believe that our work is a first step towards a comprehensive analysis of how the size of training data affects robustness; we believe that an important line of future work is to carry out similar analyses for other supervised learning methods.",6. Conclusion,[0],[0]
We thank NSF under IIS 1253942 for support.,Acknowledgement,[0],[0]
This work was also partially supported by ARO under contract number W911NF-1-0405.,Acknowledgement,[0],[0]
We thank all anonymous reviewers for their constructive comments.,Acknowledgement,[0],[0]
"Motivated by safety-critical applications, testtime attacks on classifiers via adversarial examples has recently received a great deal of attention.",abstractText,[0],[0]
"However, there is a general lack of understanding on why adversarial examples arise; whether they originate due to inherent properties of data or due to lack of training samples remains ill-understood.",abstractText,[0],[0]
"In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects.",abstractText,[0],[0]
We use our framework to analyze the robustness of a canonical non-parametric classifier – the k-nearest neighbors.,abstractText,[0],[0]
"Our analysis shows that its robustness properties depend critically on the value of k – the classifier may be inherently non-robust for small k, but its robustness approaches that of the Bayes Optimal classifier for fast-growing k. We propose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large sample limit.",abstractText,[0],[0]
Our experiments 1 suggest that this classifier may have good robustness properties even for reasonable data set sizes.,abstractText,[0],[0]
Analyzing the Robustness of Nearest Neighbors to Adversarial Examples,title,[0],[0]
"While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters. Such model complexity issues only compound when trying to generalize generative models to incorporate human input. We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an informationtheoretic framework. This framework naturally generalizes to hierarchical and semisupervised extensions with no additional modeling assumptions. In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention. Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semisupervised variants of LDA.",text,[0],[0]
"The majority of topic modeling approaches utilize probabilistic generative models, models which specify mechanisms for how documents are written in order to infer latent topics.",1 Introduction,[0],[0]
"These mechanisms may be explicitly stated, as in Latent Dirichlet Allocation (LDA) (Blei et al., 2003), or implicitly stated, as with matrix factorization techniques (Hofmann,
1999; Ding et al., 2008; Buntine and Jakulin, 2006).",1 Introduction,[0],[0]
"The core generative mechanisms of LDA, in particular, have inspired numerous generalizations that account for additional information, such as the authorship (Rosen-Zvi et al., 2004), document labels (McAuliffe and Blei, 2008), or hierarchical structure (Griffiths et al., 2004).
",1 Introduction,[0],[0]
"However, these generalizations come at the cost of increasingly elaborate and unwieldy generative assumptions.",1 Introduction,[0],[0]
"While these assumptions allow topic inference to be tractable in the face of additional metadata, they progressively constrain topics to a narrower view of what a topic can be.",1 Introduction,[0],[0]
"Such assumptions are undesirable in contexts where one wishes to minimize model complexity and learn topics without preexisting notions of how those topics originated.
",1 Introduction,[0],[0]
"For these reasons, we propose topic modeling by way of Correlation Explanation (CorEx),1 an information-theoretic approach to learning latent topics over documents.",1 Introduction,[0],[0]
"Unlike LDA, CorEx does not assume a particular data generating model, and instead searches for topics that are “maximally informative” about a set of documents.",1 Introduction,[0],[0]
"By learning informative topics rather than generated topics, we avoid specifying the structure and nature of topics ahead of time.
",1 Introduction,[0],[0]
"In addition, the lightweight framework underlying CorEx is versatile and naturally extends to hierarchical and semi-supervised variants with no additional modeling assumptions.",1 Introduction,[0],[0]
"More specifically, we
1Open source, documented code for the CorEx topic model available at https://github.com/gregversteeg/ corex_topic.
",1 Introduction,[0],[0]
"ar X
iv :1
61 1.
10 27
7v 4
[ cs
.C",1 Introduction,[0],[0]
"L
] 3
S ep
may flexibly incorporate word-level domain knowledge within the CorEx topic model.",1 Introduction,[0],[0]
Topic models are often susceptible to portraying only dominant themes of documents.,1 Introduction,[0],[0]
"Injecting a topic model, such as CorEx, with domain knowledge can help guide it towards otherwise underrepresented topics that are of importance to the user.",1 Introduction,[0],[0]
"By incorporating relevant domain words, we might encourage our topic model to recognize a rare disease that would otherwise be missed in clinical health notes, focus more attention to topics from news articles that can guide relief workers in distributing aid more effectively, or disambiguate aspects of a complex social issue.
",1 Introduction,[0],[0]
"Our contributions are as follows: first, we frame CorEx as a topic model and derive an efficient alteration to the CorEx algorithm to exploit sparse data, such as word counts in documents, for dramatic speedups.",1 Introduction,[0],[0]
"Second, we show how domain knowledge can be naturally integrated into CorEx through “anchor words” and the information bottleneck.",1 Introduction,[0],[0]
"Third, we demonstrate that CorEx and anchored CorEx produce topics of comparable quality to unsupervised and semi-supervised variants of LDA over several datasets and metrics.",1 Introduction,[0],[0]
"Finally, we carefully detail several anchoring strategies that highlight the versatility of anchored CorEx on a variety of tasks.",1 Introduction,[0],[0]
"Here we review the fundamentals of Correlation Explanation (CorEx), and adopt the notation used by Ver Steeg and Galstyan in their original presentation of the model (2014).",2.1 CorEx: Correlation Explanation,[0],[0]
"Let X be a discrete random variable that takes on a finite number of values, indicated with lowercase, x.",2.1 CorEx: Correlation Explanation,[0],[0]
"Furthermore, if we have n such random variables, let XG denote a sub-collection of them, where G ⊆ {1, . . .",2.1 CorEx: Correlation Explanation,[0],[0]
", n}.",2.1 CorEx: Correlation Explanation,[0],[0]
"The probability of observing XG = xG is written as p(XG = xG), which is typically abbreviated to p(xG).",2.1 CorEx: Correlation Explanation,[0],[0]
The entropy ofX is written asH(X) and the mutual information of two random variables X1 and X2 is given by I(X1 : X2) = H(X1) + H(X2),2.1 CorEx: Correlation Explanation,[0],[0]
"− H(X1, X2).
",2.1 CorEx: Correlation Explanation,[0],[0]
"The total correlation, or multivariate mutual information, of a group of random variables XG is ex-
pressed as TC(XG) =",2.1 CorEx: Correlation Explanation,[0],[0]
"∑ i∈G H(Xi)−H(XG) (1)
= DKL ( p(xG)||
∏ i∈G p(xi)
) .",2.1 CorEx: Correlation Explanation,[0],[0]
"(2)
We see that Eq. 1 does not quantify “correlation” in the modern sense of the word, and so it can be helpful to conceptualize total correlation as a measure of total dependence.",2.1 CorEx: Correlation Explanation,[0],[0]
"Indeed, Eq. 2 shows that total correlation can be expressed using the Kullback-Leibler Divergence and, therefore, it is zero if and only if the joint distribution of XG factorizes, or, in other words, there is no dependence between the random variables.
",2.1 CorEx: Correlation Explanation,[0],[0]
"The total correlation can be written when conditioning on another random variable Y , TC(XG | Y ) = ∑ i∈GH(Xi | Y )−H(XG | Y ).",2.1 CorEx: Correlation Explanation,[0],[0]
"So, we can consider the reduction in the total correlation when conditioning on Y .
",2.1 CorEx: Correlation Explanation,[0],[0]
TC(XG;Y ),2.1 CorEx: Correlation Explanation,[0],[0]
= TC(XG)− TC(XG | Y ) (3) = ∑ i∈G I(Xi : Y ),2.1 CorEx: Correlation Explanation,[0],[0]
"− I(XG : Y ) (4)
",2.1 CorEx: Correlation Explanation,[0],[0]
"The quantity expressed in Eq. 3 acts as a lower bound of TC(XG) (Ver Steeg and Galstyan, 2015), as readily verified by noting that TC(XG) and TC(XG|Y ) are always non-negative.",2.1 CorEx: Correlation Explanation,[0],[0]
"Also note, the joint distribution of XG factorizes conditional on Y if and only if TC(XG | Y ) = 0.",2.1 CorEx: Correlation Explanation,[0],[0]
"If this is the case, then TC(XG;Y ) is maximized, and Y explains all of the dependencies in XG.
",2.1 CorEx: Correlation Explanation,[0],[0]
"In the context of topic modeling, XG represents a group of word types and Y represents a topic to be learned.",2.1 CorEx: Correlation Explanation,[0],[0]
"Since we are always interested in grouping multiple sets of words into multiple topics, we will denote the binary latent topics as Y1, . . .",2.1 CorEx: Correlation Explanation,[0],[0]
"Ym and their corresponding groups of word types as XGj for j = 1, . . .",2.1 CorEx: Correlation Explanation,[0],[0]
",m respectively.",2.1 CorEx: Correlation Explanation,[0],[0]
"The CorEx topic model seeks to maximally explain the dependencies of words in documents through latent topics by maximizing TC(X;Y1, . . .",2.1 CorEx: Correlation Explanation,[0],[0]
", Ym).",2.1 CorEx: Correlation Explanation,[0],[0]
"To do this, we maximize the following lower bound on this expression:
max Gj ,p(yj |xGj ) m∑ j=1 TC(XGj ;Yj).",2.1 CorEx: Correlation Explanation,[0],[0]
"(5)
As we describe in the following section, this objective can be efficiently approximated, despite the search occurring over an exponentially large probability space (Ver Steeg and Galstyan, 2014).
",2.1 CorEx: Correlation Explanation,[0],[0]
"Since each topic explains a certain portion of the overall total correlation, we may choose the number of topics by observing diminishing returns to the objective.",2.1 CorEx: Correlation Explanation,[0],[0]
"Furthermore, since the CorEx implementation depends on a random initialization (as described shortly), one may restart the CorEx topic model several times and choose the one that explains the most total correlation.
",2.1 CorEx: Correlation Explanation,[0],[0]
"The latent factors, Yj , are optimized to be informative about dependencies in the data and do not require generative modeling assumptions.",2.1 CorEx: Correlation Explanation,[0],[0]
"Note that the discovered factors, Y , can be used as inputs to construct new latent factors, Z, and so on leading to a hierarchy of topics.",2.1 CorEx: Correlation Explanation,[0],[0]
"Although this extension is quite natural, we focus our analysis on the first level of topic representations for easier interpretation and evaluation.",2.1 CorEx: Correlation Explanation,[0],[0]
We summarize the implementation of CorEx as presented by Ver Steeg and Galstyan (2014) in preparation for innovations introduced in the subsequent sections.,2.2 CorEx Implementation,[0],[0]
The numerical optimization for CorEx begins with a random initialization of parameters and then proceeds via an iterative update scheme similar to EM.,2.2 CorEx Implementation,[0],[0]
"For computational tractability, we subject the optimization in Eq. 5 to the constraint that the groups, Gj , do not overlap, i.e. we enforce singlemembership of words within topics.",2.2 CorEx Implementation,[0],[0]
"The optimization entails a combinatorial search over groups, so instead we look for a form that is more amenable to smooth optimization.",2.2 CorEx Implementation,[0],[0]
"We rewrite the objective using the alternate form in Eq. 4 while introducing indicator variables αi,j which are equal to 1 if and only if word Xi appears in topic Yj (i.e. i ∈ Gj).
",2.2 CorEx Implementation,[0],[0]
"max αi,j ,p(yj |x)",2.2 CorEx Implementation,[0],[0]
m∑ j=1 ( n∑ i=1,2.2 CorEx Implementation,[0],[0]
"αi,jI(Xi :",2.2 CorEx Implementation,[0],[0]
Yj)− I(X :,2.2 CorEx Implementation,[0],[0]
Yj) ),2.2 CorEx Implementation,[0],[0]
"s.t. αi,j = I[j = arg max
j̄ I(Xi : Yj̄)].
(6)
Note that the constraint on non-overlapping groups now becomes a constraint on α.",2.2 CorEx Implementation,[0],[0]
"To make the optimization smooth we should relax the constraint so
that αi,j ∈",2.2 CorEx Implementation,[0],[0]
"[0, 1].",2.2 CorEx Implementation,[0],[0]
"To do so, we replace the second line with a softmax function.",2.2 CorEx Implementation,[0],[0]
"The update for α at iteration t becomes,
αti,j = exp ( λt(I(Xi : Yj)−max
j̄ I(Xi : Yj̄))
) .
",2.2 CorEx Implementation,[0],[0]
Now α ∈,2.2 CorEx Implementation,[0],[0]
"[0, 1] and the parameter λ controls the sharpness of the softmax function.",2.2 CorEx Implementation,[0],[0]
"Early in the optimization we use a small value of λ, then increase it later in the optimization to enforce a hard constraint.",2.2 CorEx Implementation,[0],[0]
The objective in Eq. 6 only lower bounds total correlation in the hard max limit.,2.2 CorEx Implementation,[0],[0]
"The constraint on α forces competition among latent factors to explain certain words, while setting λ = 0 results in all factors learning the same thing.",2.2 CorEx Implementation,[0],[0]
"Holding α fixed, taking the derivative of the objective (with respect to the variables p(yj |x), and setting it equal to zero leads to a fixed point equation.",2.2 CorEx Implementation,[0],[0]
"We use this fixed point to define update equations at iteration t.
pt(yj) = ∑ x̄ pt(yj |x̄)p(x̄) (7)
pt(xi|yj) = ∑ x̄ pt(yj |x̄)p(x̄)I[x̄i = xi]/pt(yj)
log pt+1(yj |x`) =",2.2 CorEx Implementation,[0],[0]
"(8)
log pt(yj)+ n∑ i=1",2.2 CorEx Implementation,[0],[0]
"αti,j log pt(x ` i | yj) p(x`i)",2.2 CorEx Implementation,[0],[0]
"− logZj(x`)
The first two lines just define the marginals in terms of the optimization parameter, pt(yj |x).",2.2 CorEx Implementation,[0],[0]
"We take p(x) to be the empirical distribution defined by some observed samples, x`, ` = 1, . . .",2.2 CorEx Implementation,[0],[0]
", N .",2.2 CorEx Implementation,[0],[0]
"The third line updates pt(yj |x`), the probabilistic labels for each latent factor, Yj , for a given sample, x`.",2.2 CorEx Implementation,[0],[0]
"Note that an easily calculated constant, Zj(x`), appears to ensure the normalization of pt(yj |x`) for each sample.",2.2 CorEx Implementation,[0],[0]
"We iterate through these updates until convergence.
",2.2 CorEx Implementation,[0],[0]
"After convergence, we use the mutual information terms I(Xi : Yj) to rank which words are most informative for each factor.",2.2 CorEx Implementation,[0],[0]
The objective is a sum of terms for each latent factor and this allows us to rank the contribution of each factor toward our lower bound on the total correlation.,2.2 CorEx Implementation,[0],[0]
"The expected log of the normalization constant, often called the free energy, E[logZj(x)], plays an important role since its expectation provides a free estimate of the i-th term in the objective (Ver Steeg and Galstyan, 2015), as
can be seen by taking the expectation of Eq. 8 at convergence and comparing it to Eq. 6.",2.2 CorEx Implementation,[0],[0]
"Because our sample estimate of the objective is just the mean of contributions from individual sample points, x`, we refer to logZj(x`) as the pointwise total correlation explained by factor j for sample `.",2.2 CorEx Implementation,[0],[0]
Pointwise TC can be used to localize which samples are particularly informative about specific latent factors.,2.2 CorEx Implementation,[0],[0]
"To alter the CorEx optimization procedure to exploit sparsity in the data, we now assume that all variables, xi, yj , are binary and x is a binary vector where X`i = 1 if word i occurs in document ` and X`i = 0 otherwise.",2.3.1 Derivation,[0],[0]
"Since all variables are binary, the marginal distribution, p(xi|yj), is just a two by two table of probabilities and can be estimated efficiently.",2.3.1 Derivation,[0],[0]
The time-consuming part of training is the subsequent update of the document labels in Eq. 8 for each document `.,2.3.1 Derivation,[0],[0]
"The computation of the log likelihood ratio for all n words over all documents is not efficient, as most words do not appear in a given document.",2.3.1 Derivation,[0],[0]
"We rewrite the logarithm in the interior of the sum.
log pt(x
` i | yj)
p(x`i) = log pt(Xi = 0",2.3.1 Derivation,[0],[0]
| yj) p(Xi = 0),2.3.1 Derivation,[0],[0]
"+ (9)
xli log
( pt(X ` i = 1 | yj)p(Xi = 0)
pt(Xi = 0",2.3.1 Derivation,[0],[0]
| yj)p(X`i = 1) ),2.3.1 Derivation,[0],[0]
"Note, when the word does not appear in the document, only the leading term of Eq. 9 will be nonzero.",2.3.1 Derivation,[0],[0]
"However, when the word does appear, everything but logP (X`i = 1 | yj)/p(X`i = 1) cancels out.",2.3.1 Derivation,[0],[0]
"So, we have taken advantage of the fact that the CorEx topic model binarizes documents to assume by default that a word does not appear in the document, and then correct the contribution to the update if the word does appear.
",2.3.1 Derivation,[0],[0]
"Thus, when substituting back into Eq. 8, the sum becomes a matrix multiplication between a matrix with dimensions of the number of variables by the number of documents and entries x`i",2.3.1 Derivation,[0],[0]
that is assumed to be sparse and a dense matrix with dimensions of the number of variables by the number of latent factors.,2.3.1 Derivation,[0],[0]
"Given n variables, N samples, and ρ nonzero entries in the data matrix, the
asymptotic scaling for CorEx goes from O(Nn) to O(n)+O(N)+O(ρ) exploiting sparsity.",2.3.1 Derivation,[0],[0]
"Latent tree modeling approaches are quadratic in n or worse, so we expect CorEx’s computational advantage to increase for larger datasets.",2.3.1 Derivation,[0],[0]
We perform experiments comparing the running time of CorEx before and after implementing the improvements which exploit sparsity.,2.3.2 Optimization Evaluation,[0],[0]
"We also compare with Scikit-Learn’s simple batch implementation of LDA using the variational Bayes algorithm (Hoffman et al., 2013).",2.3.2 Optimization Evaluation,[0],[0]
"Experiments were performed on a four core, Intel i5 chip running at 4 GHz with 32 GB RAM.",2.3.2 Optimization Evaluation,[0],[0]
We show run time when varying the data size in terms of the number of word types and the number of documents.,2.3.2 Optimization Evaluation,[0],[0]
We used 50 topics for all runs and set the number of iterations for each run to 10 iterations for LDA and 50 iterations for CorEx. Results are shown in Figure 1.,2.3.2 Optimization Evaluation,[0],[0]
"We see that CorEx exploiting sparsity is orders of magnitude faster than the
naive version and is generally comparable to LDA as the number of documents scales.",2.3.2 Optimization Evaluation,[0],[0]
"The slope on the log-log plot suggests a linear dependence of running time on the dataset size, as expected.",2.3.2 Optimization Evaluation,[0],[0]
"The information bottleneck formulates a trade-off between compressing data X into a representation Y , and preserving the information in X that is relevant to Z (typically labels in a supervised learning task) (Tishby et al., 1999; Friedman et al., 2001).",2.4 Anchor Words via the Bottleneck,[0],[0]
"More formally, the information bottleneck is expressed as
max p(y|x)
βI(Z",2.4 Anchor Words via the Bottleneck,[0],[0]
: Y ),2.4 Anchor Words via the Bottleneck,[0],[0]
"− I(X : Y ), (10)
where β is a parameter controlling the trade-off between compressing X and preserving information about the relevance variable, Z.
To see the connection with CorEx, we compare the CorEx objective as written in Eq. 6 with the bottleneck in Eq. 10.",2.4 Anchor Words via the Bottleneck,[0],[0]
"We see that we have exactly the same compression term for each latent factor, I(X : Yj), but the relevance variables now correspond to Z ≡ Xi.",2.4 Anchor Words via the Bottleneck,[0],[0]
"If we want to learn representations that are more relevant to specific keywords, we can simply anchor a word type Xi to topic Yj , by constraining our optimization so that αi,j = βi,j , where βi,j ≥ 1 controls the anchor strength.",2.4 Anchor Words via the Bottleneck,[0],[0]
"Otherwise, the updates on α remain the same.",2.4 Anchor Words via the Bottleneck,[0],[0]
"This schema is a natural extension of the CorEx optimization and it is flexible, allowing for multiple word types to be anchored to one topic, for one word type to be anchored to multiple topics, or for any combination of these semi-supervised anchoring strategies.",2.4 Anchor Words via the Bottleneck,[0],[0]
"With respect to integrating domain knowledge into topic models, we draw inspiration from Arora et al. (2012), who used anchor words in the context of non-negative matrix factorization.",3 Related Work,[0],[0]
"Using an assumption of separability, these anchor words act as high precision markers of particular topics and, thus, help discern the topics from one another.",3 Related Work,[0],[0]
"Although the original algorithm proposed by Arora et al. (2012), and subsequent improvements to their approach, find these anchor words automatically
(Arora et al., 2013; Lee and Mimno, 2014), recent adaptations allow manual insertion of anchor words and other metadata (Nguyen et al., 2014; Nguyen et al., 2015).",3 Related Work,[0],[0]
"Our work is similar to the latter, where we treat anchor words as fuzzy logic markers and embed them into the topic model in a semi-supervised fashion.",3 Related Work,[0],[0]
"In this sense, our work is closest to Halpern et al. (2014; 2015), who have also made use of domain expertise and semi-supervised anchored words in devising topic models.
",3 Related Work,[0],[0]
There is an adjacent line of work that has focused on incorporating word-level information into LDAbased models.,3 Related Work,[0],[0]
"Jagarlamudi et al. (2012) proposed SeededLDA, a model that seeds words into given topics and guides, but does not force, these topics towards these integrated words.",3 Related Work,[0],[0]
"Andrzejewski and Zhu (2009) presented a model that makes use of “zlabels,” words that are known to pertain to specific topics and that are restricted to appearing in some subset of all the possible topics.",3 Related Work,[0],[0]
"Although the zlabels can be leveraged to place different senses of a word into different topics, it requires additional effort to determine when these different senses occur.",3 Related Work,[0],[0]
"Our anchoring approach allows a user to more easily anchor one word to multiple topics, allowing CorEx to naturally find topics that revolve around different senses of a word.
",3 Related Work,[0],[0]
Andrzejewski et al. (2009) presented a second model which allows specification of Must-Link and Cannot-Link relationships between words that help partition otherwise muddled topics.,3 Related Work,[0],[0]
"These logical constraints help enforce topic separability, though these mechanisms less directly address how to anchor a single word or set of words to help a topic emerge.",3 Related Work,[0],[0]
"More generally, the Must/Cannot link and z-label topic models have been expressed in a powerful first-order-logic framework that allows the specification of arbitrary domain knowledge through logical rules (Andrzejewski et al., 2011).",3 Related Work,[0],[0]
"Others have built off this first-order-logic approach to automatically learn rule weights (Mei et al., 2014) and incorporate additional latent variable information (Foulds et al., 2015).
",3 Related Work,[0],[0]
"Mathematically, CorEx topic models most closely resemble topic models based on latent tree reconstruction (Chen et al., 2016).",3 Related Work,[0],[0]
"In Chen et al.’s (2016) analysis, their own latent tree approach and CorEx both report significantly better perplexity than hi-
erarchical topic models based on the hierarchical Dirichlet process and the Chinese restaurant process.",3 Related Work,[0],[0]
"CorEx has also been investigated as a way to find “surprising” documents (Hodas et al., 2015).",3 Related Work,[0],[0]
We use two challenging datasets with corresponding domain knowledge lexicons to evaluate anchored CorEx.,4.1 Data,[0],[0]
"Our first dataset consists of 504,000 humanitarian assistance and disaster relief (HA/DR) articles covering 21 disaster types collected from ReliefWeb, an HA/DR news article aggregator sponsored by the United Nations (Littell et al., 2018).",4.1 Data,[0],[0]
"To mitigate overwhelming label imbalances during anchoring, we both restrict ourselves to documents in English with one label, and randomly subsample 2,000 articles from each of the largest disaster type labels.",4.1 Data,[0],[0]
"This leaves us with a corpus of 18,943 articles.2
We accompany these articles with an HA/DR lexicon of approximately 34,000 words and phrases (Littell et al., 2018).",4.1 Data,[0],[0]
The lexicon was curated by first gathering 40–60 seed terms per disaster type from HA/DR domain experts and CrisisLex.,4.1 Data,[0],[0]
"This term list was then expanded by creating word embeddings for each disaster type, and taking terms within a specified cosine similarity of the seed words.",4.1 Data,[0],[0]
"These lists were then filtered by removing names, places, non-ASCII characters, and terms with fewer than three characters.",4.1 Data,[0],[0]
"Finally, the extracted terms were audited using CrowdFlower, where users rated the relevance of the terms on a Likert scale.",4.1 Data,[0],[0]
Low relevance terms were dropped from the lexicon.,4.1 Data,[0],[0]
"Of these terms 11,891 types appear in the HA/DR articles.
",4.1 Data,[0],[0]
"Our second dataset consists of 1,237 deidentified clinical discharge summaries from the Informatics for Integrating Biology and the Bedside (i2b2) 2008 Obesity Challenge.3 These summaries are labeled by clinical experts with 15 conditions frequently associated with obesity.",4.1 Data,[0],[0]
"For these documents, we leverage a text pipeline that extracts common med-
2HA/DR articles and accompanying lexicon available at http://dx.doi.org/10.7910/DVN/TGOPRU
3Data available upon data use agreement at https:// www.i2b2.org/NLP/Obesity/
ical terms and phrases (Dai et al., 2008; Chapman et al., 2001), which yields 3,231 such term types.",4.1 Data,[0],[0]
"For both sets of documents, we use their respective lexicons to break the documents down into bags of words and phrases.
",4.1 Data,[0],[0]
"We also make use of the 20 Newsgroups dataset, as provided and preprocessed in the Scikit-Learn library (Pedregosa et al., 2011).",4.1 Data,[0],[0]
"CorEx does not explicitly attempt to learn a generative model and, thus, traditional measures such as perplexity are not appropriate for model comparison against LDA.",4.2 Evaluation,[0],[0]
"Furthermore, it is well-known that perplexity and held-out log-likelihood do not necessarily correlate with human evaluation of semantic topic quality (Chang et al., 2009).",4.2 Evaluation,[0],[0]
"Therefore, we measure the semantic topic quality using Mimno et al.’s (2011) UMass automatic topic coherence score, which correlates with human judgments.
",4.2 Evaluation,[0],[0]
"We also evaluate the models in terms of multiclass logistic regression document classification (Pedregosa et al., 2011), where the feature set of each document is its topic distribution.",4.2 Evaluation,[0],[0]
"We perform all document classification tasks using a 60/40 trainingtest split.
",4.2 Evaluation,[0],[0]
"Finally, we measure how well each topic model does at clustering documents.",4.2 Evaluation,[0],[0]
We obtain a clustering by assigning each document to the topic that occurs with the highest probability.,4.2 Evaluation,[0],[0]
We then measure the quality within clusters (homogeneity) and across clusters (adjusted mutual information).,4.2 Evaluation,[0],[0]
The highest possible value for both measures is one.,4.2 Evaluation,[0],[0]
"We do not report clustering metrics on the clinical health notes because the documents are multi-label and, in that case, the metrics are not well-defined.",4.2 Evaluation,[0],[0]
"We follow the approach used by Jagarlamudi et al. (2012) to automatically generate anchor words: for each label in a data set, we find the words that have the highest mutual information with the label.",4.3 Choosing Anchor Words,[0],[0]
"For word w and label L, this is computed as
I(L : w) = H(L)−H(L | w), (11)
where for each document of label L we consider if the word w appears or not.",4.3 Choosing Anchor Words,[0],[0]
"We compare CorEx to LDA in terms of topic coherence, document classification, and document clustering across three datasets.",5.1 LDA Baseline Comparison,[0],[0]
"CorEx is trained on binary data, while LDA is trained on count data.",5.1 LDA Baseline Comparison,[0],[0]
"While not reported here, CorEx consistently outperformed LDA trained on binary data.",5.1 LDA Baseline Comparison,[0],[0]
"In doing these comparisons, we use the Gensim implementation of LDA (Řehůřek and Sojka, 2010).",5.1 LDA Baseline Comparison,[0],[0]
"The results of comparing CorEx to LDA as a function of the number of topics are presented in Figure 2.
",5.1 LDA Baseline Comparison,[0],[0]
"Across all three datasets, we find that the topics produced by CorEx yield document classification results that are on par with or better than those produced by LDA topics.",5.1 LDA Baseline Comparison,[0],[0]
"In terms of clustering, CorEx consistently produces document clusters of higher
homogeneity than LDA.",5.1 LDA Baseline Comparison,[0],[0]
"On the disaster relief articles, the CorEx clusters are nearly twice as homogeneous as the LDA clusters.
",5.1 LDA Baseline Comparison,[0],[0]
CorEx outperforms LDA in terms of topic coherence on two out of three of the datasets.,5.1 LDA Baseline Comparison,[0],[0]
"While LDA
produces more coherent topics for the clinical health notes, it is particularly striking that CorEx is able to produce high quality topics while only leveraging binary count data.",5.1 LDA Baseline Comparison,[0],[0]
Examples of these topics are shown in Table 1.,5.1 LDA Baseline Comparison,[0],[0]
"Despite the binary counts limitation, CorEx still finds meaningfully coherent and competitive structure in the data.",5.1 LDA Baseline Comparison,[0],[0]
We now examine the effects and benefits of guiding CorEx through anchor words.,5.2 Anchored CorEx Analysis,[0],[0]
"In doing so, we also compare anchored CorEx to other semi-supervised topic models.",5.2 Anchored CorEx Analysis,[0],[0]
We are first interested in how anchoring can be used to encourage topic separability so that documents cluster well.,5.2.1 Anchoring for Topic Separability,[0],[0]
"We focus on the HA/DR articles and 20 newsgroups datasets, since traditional clustering metrics are not well-defined on the multi-label clinical health notes.",5.2.1 Anchoring for Topic Separability,[0],[0]
"For both datasets, we fix the
number of topics to be equal to the number of document labels.",5.2.1 Anchoring for Topic Separability,[0],[0]
"It is in this context that we compare anchored CorEx to two other semi-supervised topic models: z-labels LDA and must/cannot link LDA.
",5.2.1 Anchoring for Topic Separability,[0],[0]
"Using the method described in Section 4.3, we automatically retrieve the top five anchors for each disaster type and newsgroup.",5.2.1 Anchoring for Topic Separability,[0],[0]
"We then filter these lists of any words that are ambiguous, i.e. words that are anchor words for more than one document label.",5.2.1 Anchoring for Topic Separability,[0],[0]
For anchored CorEx and z-labels LDA we simultaneously assign each set of anchor words to exactly one topic each.,5.2.1 Anchoring for Topic Separability,[0],[0]
"For must/cannot link LDA, we create must-links within the words of the same anchor
group, and create cannot-links between words of different anchor groups.
",5.2.1 Anchoring for Topic Separability,[0],[0]
"Since we are simultaneously anchoring to many topics, we use a weak anchoring parameter β = 2 for anchored CorEx.",5.2.1 Anchoring for Topic Separability,[0],[0]
"Using the notation from their original papers, we use η = 1 for z-labels LDA, and η = 1000 for must/cannot link LDA.",5.2.1 Anchoring for Topic Separability,[0],[0]
"For both LDA variants, we use α = 0.5, β = 0.1 and take 2,000 samples, and estimate the models using code implemented by the original authors.
",5.2.1 Anchoring for Topic Separability,[0],[0]
"The results of this comparison are shown in Figure 3, and examples of anchored CorEx topics are shown in Table 2.",5.2.1 Anchoring for Topic Separability,[0],[0]
Across all measures CorEx and anchored CorEx outperform LDA.,5.2.1 Anchoring for Topic Separability,[0],[0]
We find that anchored CorEx always improves cluster quality versus CorEx in terms of homogeneity and adjusted mutual information.,5.2.1 Anchoring for Topic Separability,[0],[0]
"Compared to CorEx, multiple simultaneous anchoring neither harms nor benefits the topic coherence of anchored CorEx.",5.2.1 Anchoring for Topic Separability,[0],[0]
"Together these metrics suggest that anchored CorEx is finding topics that are of equivalent coherence to CorEx, but more relevant to the document labels since gains are seen in terms of document clustering.
",5.2.1 Anchoring for Topic Separability,[0],[0]
"Against the other semi-supervised topic models, anchored CorEx compares favorably.",5.2.1 Anchoring for Topic Separability,[0],[0]
"The document clustering of anchored CorEx is similar to, or better than, that of z-labels LDA and must/cannot link LDA.",5.2.1 Anchoring for Topic Separability,[0],[0]
"Across the disaster relief articles, anchored CorEx finds less coherent topics than the two LDA variants, while it finds similarly coherent topics as must/cannot link LDA on the 20 newsgroup dataset.",5.2.1 Anchoring for Topic Separability,[0],[0]
"We now turn to studying how domain knowledge can be anchored to a single topic to help an otherwise dominated topic emerge, and how the anchoring parameter β affects that emergence.",5.2.2 Anchoring for Topic Representation,[0],[0]
"To discern this effect, we focus just on anchored CorEx along with the HA/DR articles and clinical health notes, datasets for which we have a domain expert lexicon.
",5.2.2 Anchoring for Topic Representation,[0],[0]
"We devise the following experiment: first, we determine the top five anchor words for each document label using the methodology described in Section 4.3.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Unlike in the previous section, we do not filter these lists of ambiguous anchor words.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Second, for each document label, we run an anchored CorEx topic model with that label’s anchor words anchored to exactly one topic.",5.2.2 Anchoring for Topic Representation,[0],[0]
"We compare this an-
chored topic model to an unsupervised CorEx topic model using the same random seeds, thus creating a matched pair where the only difference is the treatment of anchor words.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Finally, this matched pairs process is repeated 30 times, yielding a distribution for each metric over each label.
",5.2.2 Anchoring for Topic Representation,[0],[0]
We use 50 topics when modeling the ReliefWeb articles and 30 topics when modeling the i2b2 clinical health notes.,5.2.2 Anchoring for Topic Representation,[0],[0]
"These values were chosen by observing diminishing returns to the total correlation explained by additional topics.
",5.2.2 Anchoring for Topic Representation,[0],[0]
In Figure 4 we show how the results of this experiment vary as a function of the anchoring parameter β for each disaster and disease type in the two data sets.,5.2.2 Anchoring for Topic Representation,[0],[0]
"Since there is heavy variance across document labels for each metric, we also examine a more detailed cross section of these results in Figure 5, where we set β = 5 for the clinical health notes and set β = 10 for the disaster relief articles.",5.2.2 Anchoring for Topic Representation,[0],[0]
"As we show momentarily, disaster and disease types that benefit the most from anchoring were un-
0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6
Tropical Cyclone Flood
Epidemic Earthquake
Drought Volcano Flash Flood Insect Infestation
Cold Wave Technological Disaster
Tsunami Land Slide
Wild Fire Severe Local Storm
",5.2.2 Anchoring for Topic Representation,[0],[0]
"Other Snow Avalanche Extratropical Cyclone Mud Slide
Heat Wave Storm Surge
Fire
Anchoring Parameter β= 10
50 0 50 100
Anchoring Parameter β= 10
0.10 0.05 0.00 0.05 0.10 0.15 0.20
",5.2.2 Anchoring for Topic Representation,[0],[0]
"Anchoring Parameter β= 10
Asthma Coronary Heart Disease
Congestive Heart Failure Depression
Diabetes GERD Gallstones Gout
Hypercholesterolemia Hypertension
Hypertriglyceridemia Osteoarthritis Obstructive Sleep Apnea Obesity Peripheral Vascular Disease
Anchoring Parameter β= 5
Anchoring Parameter β= 5
Anchoring Parameter β= 5
0.0
0.5
1.0 P ro
p o rtio",5.2.2 Anchoring for Topic Representation,[0],[0]
n o f R u n s,5.2.2 Anchoring for Topic Representation,[0],[0]
A n ch,5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
re d T o,5.2.2 Anchoring for Topic Representation,[0],[0]
"p ic is th e M o st P re d ictiv e
0.0
0.5
1.0
A v e ra g e F 1 S co re P re -A",5.2.2 Anchoring for Topic Representation,[0],[0]
n ch,5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
"rin g
0.0
0.5
1.0",5.2.2 Anchoring for Topic Representation,[0],[0]
"A v e ra
g e T o p",5.2.2 Anchoring for Topic Representation,[0],[0]
ic O v e rla p P re,5.2.2 Anchoring for Topic Representation,[0],[0]
-A n ch,5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
"rin g
0.0
0.5
1.0 P ro
p",5.2.2 Anchoring for Topic Representation,[0],[0]
o rtio n,5.2.2 Anchoring for Topic Representation,[0],[0]
o f R u n s,5.2.2 Anchoring for Topic Representation,[0],[0]
A n ch,5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
re d T o,5.2.2 Anchoring for Topic Representation,[0],[0]
"p ic is th e M o st P re d ictiv e
0.0
0.5
1.0 A
v e ra g e F 1 S co re P re -A n ch",5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
"rin g
0.0
0.5
1.0",5.2.2 Anchoring for Topic Representation,[0],[0]
"A v e ra
g e T o p",5.2.2 Anchoring for Topic Representation,[0],[0]
ic O v e rla p P re,5.2.2 Anchoring for Topic Representation,[0],[0]
-A n ch,5.2.2 Anchoring for Topic Representation,[0],[0]
o,5.2.2 Anchoring for Topic Representation,[0],[0]
"rin g
derrepresented pre-anchoring.",5.2.2 Anchoring for Topic Representation,[0],[0]
Document labels that were well-represented prior to anchoring achieve only marginal gain.,5.2.2 Anchoring for Topic Representation,[0],[0]
"This results in the variance seen in Figure 4.
",5.2.2 Anchoring for Topic Representation,[0],[0]
A priori we do not know that anchoring will cause the anchor words to appear at the top of topics.,5.2.2 Anchoring for Topic Representation,[0],[0]
"So, we first measure how the topic overlap, the proportion of the top ten mutual information words that appear within the top ten words of the topics, changes before and after anchoring.",5.2.2 Anchoring for Topic Representation,[0],[0]
"From Figure 4 (row 1) we see that as β increases, more of these relevant words consistently appear within the topics.",5.2.2 Anchoring for Topic Representation,[0],[0]
"For the disaster relief articles, many disaster types see about two more words introduced, while in the clinical health notes the overlap increases by up to four words.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Analyzing the cross section in Figure 5 (column 1), we see many of these gains come from disaster and disease types that appeared less in the topics pre-anchoring.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Thus, we can sway the topic model towards less dominant themes through anchoring.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Document labels that occur the most frequently are those for which the topic overlap changes the least.
",5.2.2 Anchoring for Topic Representation,[0],[0]
"Next, we examine whether these anchored topics
are more coherent topics.",5.2.2 Anchoring for Topic Representation,[0],[0]
"To do so, we compare the coherence of the anchored topic with that of the most predictive topic pre-anchoring, i.e. the topic with the largest corresponding coefficient in magnitude of the logistic regression, when the anchored topic itself is most predictive.",5.2.2 Anchoring for Topic Representation,[0],[0]
"From Figure 4 (row 2), we see these results have more variance, but largely the anchored topics are more coherent.",5.2.2 Anchoring for Topic Representation,[0],[0]
"In some cases, the coherence is 1.5 to 2 times that of pre-anchoring.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Furthermore, by colors of the central panel of Figure 5, we find that the anchored topics are, indeed, often the most predictive topics for each document label.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Similar to topic overlap, the labels that see the least improvement are those that appear the most and are already well-represented in the topic model.
",5.2.2 Anchoring for Topic Representation,[0],[0]
"Finally, we find that the anchored, more coherent topics can lead to modest gains in document classification.",5.2.2 Anchoring for Topic Representation,[0],[0]
"For the disaster relief articles, Figure 4 (row 3) shows that there are mixed results in terms of F1 score improvement, with some disaster types performing consistently better, and others performing consistently worse.",5.2.2 Anchoring for Topic Representation,[0],[0]
"The results are more consistent for the clinical health notes, where there is an average increase of about 0.1 in the F1 score, and
some disease types see an increase of up to 0.3 in F1.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Given that we are only anchoring 5 words to the topic model, these are significant gains in predictive power.
",5.2.2 Anchoring for Topic Representation,[0],[0]
"Unlike the gains in topic overlap and coherence, the F1 score increases do not simply correlate with which document labels appeared most frequently.",5.2.2 Anchoring for Topic Representation,[0],[0]
"For example, we see in Figure 5 (column 3) that Tropical Cyclone exhibits the largest increase in predictive performance, even though it is also one of the most frequently appearing document labels.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Similarly, some of the major gains in F1 for the disease types, and major losses in F1 for the disaster types, do not come from the most or least frequent document labels.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Thus, if using anchoring single topics within CorEx for document classification, it is important to examine how the anchoring affects prediction for individual document labels.",5.2.2 Anchoring for Topic Representation,[0],[0]
"Finding topics that revolve around a word, such as a name or location, or a group of words can aid in understanding how a particular subject or event has been framed.",5.2.3 Anchoring for Topic Aspects,[0],[0]
We finish with a qualitative experiment where we disambiguate aspects of a topic by anchoring a set of words to multiple topics within the CorEx topic model.,5.2.3 Anchoring for Topic Aspects,[0],[0]
"Note, must/cannot link LDA cannot be used in this manner, and z-labels LDA would require us to know these aspects beforehand.
",5.2.3 Anchoring for Topic Aspects,[0],[0]
"We consider tweets containing #Ferguson (caseinsensitive), which detail reactions to the shooting of Black teenager Michael Brown by White police officer Darren Wilson on August 9th, 2014 in Ferguson, Missouri.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"These tweets were collected from the Twitter Gardenhose, a 10% random sample of all tweets, over the period August 9th, 2014 to November 30th, 2014.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"Since CorEx will seek maximally informative topics by exploiting redundancies, we remove duplicates of retweets, leaving us with 869,091 tweets.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"We filter these tweets of punctuation, stop words, hyperlinks, usernames, and the ‘RT’ retweet symbol, and use the top 20,000 word types.
",5.2.3 Anchoring for Topic Aspects,[0],[0]
"In the wake of both the shooting and the eventual non-indictment of Darren Wilson, several protests occurred.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"Some onlookers supported and encouraged such protests, while others characterized the protests as violent “riots.”",5.2.3 Anchoring for Topic Aspects,[0],[0]
"To disambiguate these
different depictions, we train a CorEx topic model with 55 topics, anchoring “protest” and “protests” together to five topics, and “riot” and “riots” together to five topics with β",5.2.3 Anchoring for Topic Aspects,[0],[0]
= 2.,5.2.3 Anchoring for Topic Aspects,[0],[0]
"These anchored topics are presented in Table 3.
",5.2.3 Anchoring for Topic Aspects,[0],[0]
"The anchored topics reflect different aspects of the framing of the “protests” and “riots,” and are generally interpretable, despite the typical difficulty of extracting coherent topics from short documents using LDA (Tang et al., 2014).",5.2.3 Anchoring for Topic Aspects,[0],[0]
"The “protest” topic aspects describe protests in St. Louis, Oakland, Beverly Hills, and parts of New York City (topics 1, 3, 4, 5), resistance by law enforcement (topics 3 and 4), and discussion of whether the protests were peaceful (topic 1).",5.2.3 Anchoring for Topic Aspects,[0],[0]
"Topic 2 revolves around hip-hop artists who marched in solidarity with protesters.
",5.2.3 Anchoring for Topic Aspects,[0],[0]
The “riot” topic aspects discuss racial dynamics of the protests (topic 7) and suggest the demonstrations are dangerous (topics 8 and 9).,5.2.3 Anchoring for Topic Aspects,[0],[0]
"Topic 10 describes the “riot” gear used in the militarized response to the Ferguson protesters, and Topic 7 also hints at aspects of conservatism through the hashtags #tcot (Top Conservatives on Twitter) and #pjnet (Patriot Journalist Network).
",5.2.3 Anchoring for Topic Aspects,[0],[0]
"As we see, anchored CorEx finds several interesting, non-trivial aspects around “protest” and “riot” that could spark additional qualitative investigation.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"Retrieving topic aspects through anchor words in this manner allows the user to explore different frames of complex issues, events, or discussions within documents.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"As with the other anchoring strategies, this has the potential to supplement qualitative research done by researchers within the social sciences and digital humanities.",5.2.3 Anchoring for Topic Aspects,[0],[0]
"We have introduced an information-theoretic topic model, CorEx, that does not rely on any of the generative assumptions of LDA-based topic models.",6 Discussion,[0],[0]
This topic model seeks maximally informative topics as encoded by their total correlation.,6 Discussion,[0],[0]
We also derived a flexible method for anchoring word-level domain knowledge in the CorEx topic model through the information bottleneck.,6 Discussion,[0],[0]
"Anchored CorEx guides the topic model towards themes that do not naturally emerge, and often produces more coherent and predictive topics.",6 Discussion,[0],[0]
"Both CorEx and anchored CorEx consistently produce topics that are of comparable quality to LDA-based methods, despite only making use of binarized word counts.
",6 Discussion,[0],[0]
Anchored CorEx is more flexible than previous attempts at integrating word-level information into topic models.,6 Discussion,[0],[0]
"Topic separability can be enforced by lightly anchoring disjoint groups of words to separate topics, topic representation can be promoted by assertively anchoring a group of words to a single topic, and topic aspects can be unveiled by anchoring a single group of words to multiple topics.",6 Discussion,[0],[0]
The flexibility of anchoring through the information bottleneck lends itself to many other possible creative anchoring strategies that could guide the topic model in different ways.,6 Discussion,[0],[0]
"Different goals may call for different anchoring strategies, and domain experts can
shape these strategies to their needs.",6 Discussion,[0],[0]
"While we have demonstrated several advantages of the CorEx topic model to LDA, it does have some technical shortcomings.",6 Discussion,[0],[0]
"Most notably, CorEx relies on binary count data in its sparsity optimization, rather than the standard count data that is used as input into LDA and other topic models.",6 Discussion,[0],[0]
"While we have demonstrated CorEx performs at the level of LDA despite this limitation, its effect would be more noticeable on longer documents.",6 Discussion,[0],[0]
This can be partly overcome if one chunks such longer documents into shorter subdocuments prior to running the topic model.,6 Discussion,[0],[0]
Our implementation also requires that each word appears in only one topic.,6 Discussion,[0],[0]
"These limitations are not fundamental limitations of the theory, but a matter of computational efficiency.",6 Discussion,[0],[0]
"In future work, we hope to remove these restrictions while preserving the speed of the sparse CorEx topic modeling algorithm.
",6 Discussion,[0],[0]
"As we have demonstrated, the informationtheoretic approach provided via CorEx has rich potential for finding meaningful structure in documents, particularly in a way that can help domain experts guide topic models with minimal intervention to capture otherwise eclipsed themes.",6 Discussion,[0],[0]
The lightweight and versatile framework of anchored CorEx leaves open possibilities for theoretical extensions and novel applications within the realm of topic modeling.,6 Discussion,[0],[0]
We would like to thank the Machine Intelligence and Data Science (MINDS) research group at the Information Sciences Institute for their help and insight during the course of this research.,Acknowledgments,[0],[0]
We also thank the Vermont Advanced Computing Core (VACC) for its computational resources.,Acknowledgments,[0],[0]
"We acknowledge the construction of the HA/DR corpus and lexicon by Leidos Corp. under funding from the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O), program: Low Resource Languages for Emergent Incidents (LORELEI), Contract No. HR0011-15-C0114.",Acknowledgments,[0],[0]
"Finally, we thank the anonymous reviewers and the TACL action editors Diane McCarthy and Kristina Toutanova for their time and effort in helping us improve our work.",Acknowledgments,[0],[0]
"Ryan J. Gallagher was
a visiting research assistant at the Information Sciences Institute while performing this research.",Acknowledgments,[0],[0]
Ryan J. Gallagher and Greg Ver Steeg were supported by DARPA award HR0011-15-C-0115 and David Kale was supported by the Alfred E. Mann Innovation in Engineering Doctoral Fellowship.,Acknowledgments,[0],[0]
"Disease Type Anchor Words Asthma asthma, albuterol, wheeze,advair, fluticasone
Coronary Artery Disease
coronary artery disease, aspirin, myocardial inarction, plavix
Congestive Heart Failure
congestive heart failure, lasix, diuresis, heart failure, cardiomyopathy
Depression depression, prozac, celexa,seroquel, remeron Diabetes diabetes mellitus, diabetes, nph insulin, insulin, metformin
Gastroesophageal Reflux Disease
gastroesophageal refulx, no known drug allergy, protonix, not:, reflux
Gallstones gallstone, cholecystitis, cholelithiasis, abdominal pain,vomiting Gout gout, allopurinol, colchicine, renal insufficiency, torsemide Hypercholesterolemia hypercholesterolemia, hyperlipidemia, aspirin, lipitor, dyslipidemia Hypertension hypertension, lisinopril, aspirin, diabetes mellitus, atorvastatin
Hypertriglyceridemia hypertriglyceridemia, gemfibrozil, citrate, orphenadrine, hydroxymethylglutaryl coa reductase inhibitors Osteoarthritis osteoarthritis, degenerative joint disease, arthritis, naproxen, fibromyalgia
Obstructive Sleep Apnea
sleep apnea, obstructive sleep apnea, morbid obese, obesity, ipratropium
Obesity obesity, morbid obesity, obese, sleep apnea, coronary artery disease
Peripheral Vascular Disease
cellulitis, erythema, ulcer, swelling, word finding difficulty
Table A1: Words that have the highest mutual information with each disease type.
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Disaster Type Anchor Words Cold Wave winter, snow, cold, temperatures, heavy snow Drought drought, taliban, wheat, refugees, severe drought Earthquake earthquake, quake, richter scale, tents, injured Epidemic virus, ebola outbreak, transmission, ebola virus, disaster Extratropical Cyclone typhoon, storm, farmland, houses, storm coincided Fire fire, hospitals, blaze, water crisis, firefighters Flash Flood flood, floods, flash floods, monitoring stations, muhuri Flood floods, flood, flooding, flood victims, rains Heat Waves heat, temperatures, heat wave, heatstroke, sunstroke Insect Infestation locust, food crisis, infestations, millet, harvest Land Slide landslides, houses, hunza river, search, village Mud Slide mudslides, rains, mudslide, torrential rains, houses Other climate, ocean, drought, impacts, warming Severe Local Storm tornado, storm, tornadoes, houses, storms Snow Avalanche avalanches, avalanche, snow, snowfall, an avalanche Storm Surge king tides, tropical storm, ocean, cyclone season, flooded Technological Disaster environmental, toxic waste, pollution, tanker, sludge Tropical Cyclone hurricane, cyclone, storm, tropical storm, national hurricane Tsunami earthquake, disaster, tsunamis, wave, rains Volcano eruption, lava, volcanic, crater, eruptions Wild Fire fires, fire, forest fires, firefighters, burning
Table A2: Words that have the highest mutual information with each disaster type.
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"re lie
f,r el
ie f s
up pl
ie s,
re lie
f e ffo
rts
co ld
,c ol
d w
ea th
er ,w
av e
fu el
,s up
pl y,
di es
el e
ne rg
y
la tri
ne s,
w at
er ta
nk s,
w at
er c
on ta
in er
s
lo cu
st ,a
tta ck
s, fig
ht in
g
cr op
s, ce
re al
,c",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"er
ea ls
er up
tio n,
vo lc
an ic
,la va
tra in
in g,
pa rtn
er s,
pr ot
ec tio
n
fir es
,fi re
,fo re
st fi
re s
su rv
iv or
s, re
lie f e
ffo rt,
re lie
f w or
ke rs
ca na
l,d is
ru pt
io n,
re ha
bi lit
at in
g
un ite
d na
tio ns
,h um
an ita
ria n
af fa
irs ,a
ge nc
ie",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"s
flo ur
,w he
at ,s
ug ar
ho us
eh ol
d, pr
oc ur
em en
t,v ul
ne ra
bl",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"e
gr ou
ps
ho us
in g,
re co
ns tru
ct io
n, co
ns tru
ct io
n
re sc
ue ,s
ea rc
h, in
ju re
d
m ilit
ar y,
ar m
ed ,c
iv ilia
ns
en vi
ro nm
en ta
l,p ol
lu tio
n, co
nt am
in at
io nh
ou se
s,",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ki
lle d,
vi lla
ge
su pp
or t,a
ss is
ta nc
e, ap
pe al
ch ol
er a
ou tb
re ak
,c ho
le ra
e pi
de m
ic ,p
oo r s
an ita
tio n
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"vo lu
nt ar
y, ba
si c
ne ed
s, re
ha bi
lit at
io n
ph as
e
bl an
ke ts
,te nt
s, fa
m ilie
s
pu",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"bl
ic h
ea lth
,o",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"rg
an iz
at io
n, m
in is
try",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"o
f
w in
te r,s
no w
,s no
w fa
ll
cr",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"im
in al
,d et
ai ne
d, pa
rli am
en t
fa m
in e,
se ve
re d
ro",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ug
ht ,c
ris es
ea rth
qu ak
e, qu
ak e,
ric",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ht
er s
ca le
m al
nu tri
tio n,
re fu
ge es
,fo od
a",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"id
m al
ar ia
,d ia
rrh oe
a, di
se as
es
ta lib
an ,re
pa tri
at io
n, el
ec tio
ns
sa ni
ta tio
n, pr
ov is
io n,
sa fe
d rin
ki ng
w at
er
go ve
rn m
en t,g
ov er
nm en
ts ,p
rim e
m in
is te
r
ve ge
ta tio
n, ec
ol og
ic al
,th re
at
flo",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"od
s, flo
od in
g, flo
od
st af
f,s up
pl ie
s, pe
rs on
ne l
ba si
n, m
on ito
rin g
st at
io ns
,b as
in s
ca m
ps ,li
vi ng
,a rm
y
pe ris
he d,
w at
er s
to ra
ge ,c
au se
d ex
te ns
iv e
da m
ag e
vi ru
s, eb
ol a
ou tb
re ak
,tr an
sm is
si on
dr ou
gh t,f
ar m
er s,
ha rv
es t
m ed
ic al
,p at
ie nt
s, ho
sp ita
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"l
ng os
,d on
or s,
hu m
an ita
ria n
in te
rn at
io na
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"l f ed
er at
io n,
re d
cr os
s, re
d cr
es ce
nt
di sa
st er
,d is
as te
rs ,d
is as
te r r
el ie
f
st or
m ,w
in ds
,c oa
st
w at
er ,w
at er
s up
pl y
tra",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ns
po rt,
fli gh
ts ,tr
uc ks
fa ci
lit ie
s, so
ap ,m
ed ic
al s
up pl
ie se
m er
ge nc
y, em
er ge
nc ie
s, oc
ha
Fi gu
re A
1: H
ie ra
rc hi
ca lC
or E
x to
pi c
m od
el of
th e
di sa
st er
re lie
fa rt
ic le
s. E
dg",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"e
w i",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"d
th s
ar e
pr op
or tio
na lt
o th
e m
ut ua
li nf
or m
at io
n w
ith th
e la
te nt
re pr
es en
ta tio
n.
Rank",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Topic 1 drought, farmers, harvest, crop, livestock, planting, grain, maize, rainfall, irrigation 2 floods, flooding, flood, rains, flooded, landslides, inundated, rivers, submerged, flash floods
3 eruption, volcanic, lava, crater, eruptions, volcanos, slopes, volcanic activity, evacuated, lava flows 4 storm, winds, coast, hurricane, weather, tropical storm, national hurricane, coastal, storms, meteorological 5 virus, ebola outbreak, transmission, health workers, vaccination, ebola virus, suspected cases, fluids, ebola virus disease, ebola patients 6 malnutrition, refugees, food aid, nutrition, feeding, refugees in, hunger, nutritional, refugee, food crisis 7 international federation, red cross, red crescent, societies, volunteers, disaster relief emergency, national societies, disaster preparedness, information bulletin, relief operation 8 winter, snow, snowfall, temperatures, heavy snow, heating, freezing, warm clothing, severe winter, avalanches
9 support, assistance, appeal, funds, assist, contributions, fund, cash, contribution, organizations 10 taliban, repatriation, elections, militia, convoy, ruling, talibans, islamic, convoys, vote 11 ngos, donors, humanitarian, un agencies, mission, funding, unicef, conduct, humanitarian assistance, inter-agency 12 fires, fire, forest fires, burning, firefighters, wildfires, blaze, flames, fire fighting, forests 13 earthquake, quake, richter scale, aftershocks, earthquakes, magnitude earthquake, magnitude, devastating earthquake, an earthquake, earthquake struck 14 blankets, tents, families, clothing, utensils, plastic sheeting, clothes, tarpaulins, schools, shelters 15 rescue, search, injured, helicopters, death toll, rescue operations, rescue teams, police, rescuers, stranded 16 crops, cereal, cereals, millet, food shortages, sorghum, harvests, shortage, ration, rainy 17 medical, patients, hospital, hospitals, nurses, clinics, clinic, doctor, medical team, beds 18 water, water supply, drinking water, pumps, drinking, water supplies, potable water, water distribution, installed, constructed 19 locust, attacks, fighting, infestations, pesticides, opposition, attack, reform, dialogue, governance 20 environmental, pollution, contamination, fish, impacts, water quality, polluted, pollutants, chemicals, tanker 21 malaria, diarrhoea, diseases, oral, rehydration, salts, contaminated, epidemics, borne diseases, respiratory infections, clean 22 emergency, emergencies, ocha, disaster response, coordinating, emergency response, coordinated, coordinators, transportation, rapid assessment 23 military, armed, civilians, soldiers, aircraft, weapons, rebel, planes, bombs, military personnel 24 united nations, humanitarian affairs, agencies, agency, governmental, united nations childrens fund, relief coordinator, general assembly, international cooperation, donor community 25 transport, flights, trucks, airport, transported, flight, truck, airlift, cargo, route
Table A3: Topics 1–25 resulting from the best of 10 CorEx topic models run on the disaster relief articles.",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Topics are ranked by total correlation explained.
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Rank Topic
26 basin, monitoring stations, basins, muhuri, flood forecasting, significant rainfall, moderate rainfall, upstream, light, sludge 27 criminal, detained, parliament, protest, crime, protests, protesters, suspects, firing, incident 28 public health, organization, ministry of, efforts, outbreaks, building, leaders, civil society, minister of, facility 29 housing, reconstruction, construction, repair, rebuilding, repairs, temporary housing, corrugated, permanent housing, debris removal 30 houses, killed, village, were killed, buildings, swept, debris, roofs, roof, collapse 31 training, partners, protection, interventions, delivery establishment, violence, benefit, unfpa, pilt 32 sanitation, provision, safe, drinking water, latrine, hygiene education, implementing partners, diarrhoeal diseases, rehabilitated, dispaced persons, sanitation services 33 flour, wheat, sugar, vegetable, beans, rations, food rations, bread, lentils, needy 34 camps, living, army, troops, resettlement, relocated, relocation, relocate, flee, settlement 35 disaster, disasters, disaster relief, cyclone, coordinating council, cyclones, aftermath, devastation, devastated, natural disaster 36 relief, relief supplies, relief efforts, relief operations, relief assistance, relief goods, relief materials, relief agencies, donate, providing relief 37 household, procurement, vulnerable groups, beneficiary, pipeline, rehabilitate, local ngos, iodised salt, rainfed areas, water harvesting 38 staff, supplies, personnel, deployed, staff members, airlifted specialists, flown, logistical support, airlifting 39 facilities, soap, medical supplies, clean water, sanitation facilities, emergency medical, international organization, psychosocial, tent, migration iom 40 fuel, supply, diesel energy, nitrate, diesel fuel, orphanages, grid, hydroelectric, storage, facilities 41 cold, cold weather, wave, warm clothes, extreme temperatures, firewood, severe cold weather, severe cold wave, average temperature 42 cholera outbreak, cholera epidemic, poor sanitation, cholera outbreaks, wash, poor hygiene, dirty water, disinfect, hygiene awareness, good hygiene practices 43 government, governments, prime minister, administration, national disaster management, corporation, dollars, bilateral donors, disburse, telecom 44 famine, severe drought, crises, prolonged drought, devastating, mortality rate, degradation, catastrophic, famine relief, agricultural practices 45 vegetation, ecological, threat, mosquitoes, insect, insecticides, lakes, prolonged, habitation, adverse weather 46 latrines, water tanks, water containers, affected communities, chlorine tablets, household kits, solid waste, reception centre, local organisations, piped water 47 survivors, relief effort, relief workers, survivor, clean drinking water, outlying areas, devastating cyclone, cyclone struck, cyclone survivors, medic 48 perished, water storage, caused extensive damage, soil erosion, total loss, sewage systems, salt water, soup, water purifying tablets, electric power 49 canal, disruption, rehabilitating, infrastructures, vulnerable areas, uninterrupted, power plants, stagnant, inaccessible areas, distress 50 voluntary, basic needs, rehabilitation phase, blankets mattresses, raised, freight, humanitarian organizations, government agency, delta region, persons displaced
Table A4:",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
Topics 26–50 resulting from the best of 10 CorEx topic models run on the disaster relief articles.,A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Topics are ranked by total correlation explained.
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"no t:,
pu lm
on ar
y ed
em a,
ca pt
op ril
cl in
da m
yc in
,im od
iu m
,p ul
m on
ar y
di se
as e
co la
ce ,c
on st
ip at
io n,
se nn
a
an tib
io tic
,m ic
on az
ol e,
w ou
nd
us e,
dr ug
,c om
pl ic
at io
n
re sp
ira to
ry fa
ilu re
,p re
dn is
on e,
im ur
an
pa in
,o xy
co do
ne ,ty
le no
l
di go
xi n,
ca rd
io m
yo pa
th y
al da
ct on
e, sp
iro no
la ct
on e
m yo
ca rd
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
ia,A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"l i
nf",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ar
ct io
n, an
gi na
,c he
st p
re ss
ur e
al bu
te ro
l,w he
ez e,
at ro
ve nt
vo m
iti ng
,n au
se a,
ab do
m in
al p
ai n
np h
in su
lin ,in
su lin
,in su
lin d
ep en
de nt
d ia
be te
s m
el lit
us an
xi et
y st
at e,
in so
m ni
a, at
iv an
le ft
ve nt
ric ul
ar ,h
yp",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"er
tro ph
y, dy
sp ne
a
no",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"k
no w
n dr
ug a
lle rg
y, ax
id ,p
ro ca
rd ia
x l
as pi
rin ,p
la vi
x, lip
ito r
er yt
he m
a, ce
llu lit
is ,li
ne zo
lid
tri cu
sp",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"id
v al
ve re
gu rg
ita tio
n, m
itr al
v al
ve re
gu rg
ita tio
n, m
itr al
re gu
rg ita
tio n
el ix
ir, ro
xi ce
t,s ch
iz op
hr en
ia pr
ilo se
c, om
ep ra
zo le
,lo ve
no x
de cr
ea se
d br
ea th
s ou
nd ,s
tro ke
,",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ta ch
yc ar
di a
ni tro
gl",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"yc
er in
,c he
st p
ai n,
co ro
na ry
a rte
ry",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"d
is ea
se
va nc
om yc
in ,c
om m
un ic
ab le
d is
ea se
,fl ag
yl
lo pr
es so
r,s te
no si
s, hy
pe rte
ns io
n
di ur
es is
,c on
ge st
iv e
he ar
t f",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"ai
lu re
,la si
x
co um
ad in
,a tri
al fi
br illa
tio n,
an tic
oa gu
la nt
hy po
th yr
oi di
sm ,s
yn th
ro id
,le vo
th yr
ox in
e
m ul
tiv ita
m in
,fo la
te ,m
ag ne
si um
le uk
oc yt
e es
te ra
se ,y
ea st
,fl uc
on az
ol e
en d
st ag
e re
na l d
is ea
se ,n
ep hr
oc ap
s, ph
os lo
Fi gu
re A
2: H
ie ra
rc hi
ca lC
or E
x to
pi c
m od
el of
th e
cl in
ic al
he al
th no
te s.
E dg
e",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"w
id th
s ar
e pr
op or
tio na
lt o
th e
m ut
ua li
nf or
m at
io n
w ith
th e
la te
nt re
pr es
en ta
tio n.
Rank Topic
1 use, drug, complication, allergy, sodium, infection, furosemide, docusate, shortness of breath, potassium chloride 2 vancomycin, communicable disease, flagyl, levofloxacin, diabetes, renal failure, sepsis, ceftazidime, nutrition, gentamicin 3 aspirin, plavix, lipitor, toprol xl, lantus, hydroxymethylglutaryl coa reductase inhibitors, atorvastatin, nexium, novolog, disease 4 diuresis, congestive heart failure, lasix, edema, orthopnea, crackle, heart failure, dyspnea on exertion, oxygen, torsemide 5 albuterol, wheeze, atrovent, chronic obstructive pulmonary disease, asthma, flovent, ipratropium, fluticasone, advair, combivent 6 end stage renal disease, nephrocaps, phoslo, calcitriol, cellcept, kidney transplant, arteriovenous fistula, acetate, cyclosporine, neoral 7 nitroglycerin, chest pain, coronary artery disease, hypokinesia, st depression, lesion, unstable angina, akinesia, st elevation, diaphoresis 8 respiratory failure, prednisone, imuran, immunosuppression, necrosis, cyclosporin, sick, magnesium oxide, tachypnea, arteriovenous malformation 9 elixir, roxicet, schizophrenia, risperdal, zofran, crushed, valproic acid, promethazine, phenergan, prochlorperazine
10 leukocyte esterase, yeast, fluconazole, urosepsis, dysphagia, oxycontin, lidoderm, chemotherapy, adriamycin, medical problems 11 colace, constipation, senna, lactulose, dulcolax, milk of magnesia, sennoside, dilaudid, protonix, reglan 12 vomiting, nausea, abdominal pain, diarrhea, fever, dehydration, chill, clostridium difficile, intravenous fluid, compazine 13 coumadin, atrial fibrillation, anticoagulant, warfarin, k vitamin, amiodarone, atrial flutter, flutter, deep venous thrombosis, allopurinol 14 digoxin, cardiomyopathy, aldactone, spironolactone, carvedilol, dobutamine, alcohol, idiopathic cardiomyopathy, ventricular rate, addiction 15 clindamycin, imodium, pulmonary disease, erythromycin, defervesced, sweating, carafate, quinidine, cytomegalovirus, cepacol 16 lopressor, stenosis, hypertension, heparin, hypercholesterolemia, aortic valve insufficiency, mitral valve insufficiency, aortic valve stenosis, sinus rhythm, peripheral vascular disease 17 antibiotic, miconazole, wound, nitrate, morbid obese, fentanyl, sleep apnea, obesity, abscess, ampicillin 18 erythema, cellulitis, linezolid, swelling, erythematous, osteomyelitis, ancef, keflex, dicloxacillin, bacitracin 19 anxiety state, insomnia, ativan, neurontin, depression, lorazepam, gabapentin, trazodone, fluoxetine, headache 20 multivitamin, folate, magnesium, folic acid, mvi, maalox, thiamine, vitamin c, gluconate, dyspepsia
Table A5:",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
Topics 1–20 resulting from the best of 10 CorEx topic models run on the clinical health notes.,A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Topics are ranked by total correlation explained.
",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"Rank Topic
21 decreased breath sound, stroke, tachycardia, seizure disorder, lymphocyte, atelectasis, polymorphonuclear leukocytes, ecchymosis, seizure, cefotaxime 22",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"not: , pulmonary edema, captopril, pleural effusion, rales, beta blocker, fatigue, dead, q wave, dysfunction 23 hypothyroidism, synthroid, levothyroxine, levoxyl, diovan, valsartan, angioedema, bestrophinopathy, atherosclerosis, ursodiol 24 nph insulin, insulin, insulin dependent diabetes mellitus, anemia, humulin insulin, retinopathy, hyperglycemia, humulin, gastrointestinal bleeding, nephropathy 25 tricuspid valve regurgitation, mitral valve regurgitation, mitral regurgitation, left atrial enlargement, zaroxolyn, ectopy, right atrial enlargement, metolazone, deficit, regurgitant 26 prilosec, omeprazole, lovenox, pulmonary embolism, enoxaparin, xalatan, oxybutynin, helicopter pylori, flonase, ramipril 27 pain, oxycodone, tylenol, percocet, ibuprofen, morphine, osteoarthritis, hernia, motrin, bleeding 28 left ventricular hypertrophy, dyspnea, living alone, smokes, syndrome, hives, palpitation, elderly, left axis deviation, usual state of health 29 myocardial infarction, angina, chest pressure, patent ductus arteriosus, atenolol, micronase, adenosine, non-insulin dependent diabetes mellitus, ecotrin, caltrate 30 no known drug allergy, axid, procardia xl, vasotec, obese, mevacor, tissue plasminogen activator, middle-aged, nifedipine, procardia
Table A6:",A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
Topics 21–30 resulting from the best of 10 CorEx topic models run on the clinical health notes.,A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
Topics are ranked by total correlation explained.,A Supplemental Material: Anchor Words and Topic Examples,[0],[0]
"While generative models such as Latent Dirichlet Allocation (LDA) have proven fruitful in topic modeling, they often require detailed assumptions and careful specification of hyperparameters.",abstractText,[0],[0]
Such model complexity issues only compound when trying to generalize generative models to incorporate human input.,abstractText,[0],[0]
"We introduce Correlation Explanation (CorEx), an alternative approach to topic modeling that does not assume an underlying generative model, and instead learns maximally informative topics through an informationtheoretic framework.",abstractText,[0],[0]
This framework naturally generalizes to hierarchical and semisupervised extensions with no additional modeling assumptions.,abstractText,[0],[0]
"In particular, word-level domain knowledge can be flexibly incorporated within CorEx through anchor words, allowing topic separability and representation to be promoted with minimal human intervention.",abstractText,[0],[0]
"Across a variety of datasets, metrics, and experiments, we demonstrate that CorEx produces topics that are comparable in quality to those produced by unsupervised and semisupervised variants of LDA.",abstractText,[0],[0]
Anchored Correlation Explanation: Topic Modeling with Minimal Domain Knowledge,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2215–2224, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Research in NLP relies heavily on the availability of human annotations for various linguistic prediction tasks.,1 Introduction,[0],[0]
"Such resources are commonly treated as de facto “gold standards” and are used for both training
1The experimental data in this study will be made publicly available.
and evaluation of algorithms for automatic annotation.",1 Introduction,[0],[0]
"At the same time, human agreement on these annotations provides an indicator for the difficulty of the task, and can be instrumental for estimating upper limits for the performance obtainable by computational methods.
",1 Introduction,[0],[0]
"Linguistic gold standards are often constructed using pre-existing annotations, generated by automatic tools.",1 Introduction,[0],[0]
The output of such tools is then manually corrected by human annotators to produce the gold standard.,1 Introduction,[0],[0]
"The justification for this annotation methodology was first introduced in a set of experiments on POS tag annotation conducted as part of the Penn Treebank project (Marcus et al., 1993).",1 Introduction,[0],[0]
"In this study, the authors concluded that tagger-based annotations are not only much faster to obtain, but also more consistent and of higher quality compared to annotations from scratch.",1 Introduction,[0],[0]
"Following the Penn Treebank, syntactic annotation projects for various languages, including German (Brants et al., 2002), French (Abeillé et al., 2003), Arabic (Maamouri et al., 2004) and many others, were annotated using automatic tools as a starting point.",1 Introduction,[0],[0]
"Despite the widespread use of this annotation pipeline, there is, to our knowledge, little prior work on syntactic annotation quality and on the reliability of system evaluations on such data.
",1 Introduction,[0],[0]
"In this work, we present a systematic study of the influence of automatic tool output on characteristics of annotations created for NLP purposes.",1 Introduction,[0],[0]
"Our investigation is motivated by the hypothesis that annotations obtained using such methodologies may be
2215
subject to the problem of anchoring, a well established and robust cognitive bias in which human decisions are affected by pre-existing values (Tversky and Kahneman, 1974).",1 Introduction,[0],[0]
"In the presence of anchors, participants reason relative to the existing values, and as a result may provide different solutions from those they would have reported otherwise.",1 Introduction,[0],[0]
"Most commonly, anchoring is manifested as an alignment towards the given values.
",1 Introduction,[0],[0]
"Focusing on the key NLP tasks of POS tagging and dependency parsing, we demonstrate that the standard approach of obtaining annotations via human correction of automatically generated POS tags and dependencies exhibits a clear anchoring effect – a phenomenon we refer to as parser bias.",1 Introduction,[0],[0]
"Given this evidence, we examine two potential adverse implications of this effect on parser-based gold standards.
",1 Introduction,[0],[0]
"First, we show that parser bias entails substantial overestimation of parser performance.",1 Introduction,[0],[0]
"In particular, we demonstrate that bias towards the output of a specific tagger-parser pair leads to over-estimation of the performance of these tools relative to other tools.",1 Introduction,[0],[0]
"Moreover, we observe general performance gains for automatic tools relative to their performance on human-based gold standards.",1 Introduction,[0],[0]
"Second, we study whether parser bias affects the quality of the resulting gold standards.",1 Introduction,[0],[0]
"Extending the experimental setup of Marcus et al. (1993), we demonstrate that parser bias may lead to lower annotation quality for parser-based annotations compared to humanbased annotations.
",1 Introduction,[0],[0]
"Furthermore, we conduct an experiment on interannotator agreement for POS tagging and dependency parsing which controls for parser bias.",1 Introduction,[0],[0]
Our experiment on a subset of section 23 of the WSJ Penn Treebank yields agreement rates of 95.65 for POS tagging and 94.17 for dependency parsing.,1 Introduction,[0],[0]
This result is significant in light of the state of the art tagging and parsing performance for English newswire.,1 Introduction,[0],[0]
"With parsing reaching the level of human agreement, and tagging surpassing it, a more thorough examination of evaluation resources and evaluation methodologies for these tasks is called for.
",1 Introduction,[0],[0]
"To summarize, we present the first study to measure and analyze anchoring in the standard parserbased approach to creation of gold standards for POS tagging and dependency parsing in NLP.",1 Introduction,[0],[0]
"We conclude that gold standard annotations that are
based on editing output of automatic tools can lead to inaccurate figures in system evaluations and lower annotation quality.",1 Introduction,[0],[0]
"Our human agreement experiment, which controls for parser bias, yields agreement rates that are comparable to state of the art automatic tagging and dependency parsing performance, highlighting the need for a more extensive investigation of tagger and parser evaluation in NLP.",1 Introduction,[0],[0]
"We examine two standard annotation tasks in NLP, POS tagging and dependency parsing.",2.1 Annotation Tasks,[0],[0]
"In the POS tagging task, each word in a sentence has to be categorized with a Penn Treebank POS tag (Santorini, 1990) (henceforth POS).",2.1 Annotation Tasks,[0],[0]
"The dependency parsing task consists of providing a sentence with a labeled dependency tree using the Universal Dependencies (UD) formalism (De Marneffe et al., 2014), according to version 1 of the UD English guidelines2.",2.1 Annotation Tasks,[0],[0]
"To perform this task, the annotator is required to specify the head word index (henceforth HIND) and relation label (henceforth REL) of each word in the sentence.
",2.1 Annotation Tasks,[0],[0]
"We distinguish between three variants of these tasks, annotation, reviewing and ranking.",2.1 Annotation Tasks,[0],[0]
"In the annotation variant, participants are asked to conduct annotation from scratch.",2.1 Annotation Tasks,[0],[0]
"In the reviewing variant, they are asked to provide alternative annotations for all annotation tokens with which they disagree.",2.1 Annotation Tasks,[0],[0]
"The participants are not informed about the source of the given annotation, which, depending on the experimental condition can be either parser output or human annotation.",2.1 Annotation Tasks,[0],[0]
"In the ranking task, the participants rank several annotation options with respect to their quality.",2.1 Annotation Tasks,[0],[0]
"Similarly to the review task, the participants are not given the sources of the different annotation options.",2.1 Annotation Tasks,[0],[0]
"Participants performing the annotation, reviewing and ranking tasks are referred to as annotators, reviewers and judges, respectively.",2.1 Annotation Tasks,[0],[0]
"All annotation tasks are performed using a CoNLL style text-based template, in which each word appears in a separate line.",2.2 Annotation Format,[0],[0]
"The first two columns of each line contain the word index and the word, re-
2http://universaldependencies.org/#en
spectively.",2.2 Annotation Format,[0],[0]
"The next three columns are designated for annotation of POS, HIND and REL.
",2.2 Annotation Format,[0],[0]
"In the annotation task, these values have to be specified by the annotator from scratch.",2.2 Annotation Format,[0],[0]
"In the review task, participants are required to edit preannotated values for a given sentence.",2.2 Annotation Format,[0],[0]
"The sixth column in the review template contains an additional # sign, whose goal is to prevent reviewers from overlooking and passively approving existing annotations.",2.2 Annotation Format,[0],[0]
"Corrections are specified following this sign in a space separated format, where each of the existing three annotation tokens is either corrected with an alternative annotation value or approved using a * sign.",2.2 Annotation Format,[0],[0]
Approval of all three annotation tokens is marked by removing the # sign.,2.2 Annotation Format,[0],[0]
"The example below presents a fragment from a sentence used for the reviewing task, in which the reviewer approves the annotations of all the words, with the exception of “help”, where the POS is corrected from VB to NN and the relation label xcomp is replaced with dobj.
",2.2 Annotation Format,[0],[0]
"... 5 you PRP 6 nsubj 6 need VBP 3 ccomp 7 help VB 6 xcomp # NN * dobj ...
",2.2 Annotation Format,[0],[0]
The format of the ranking task is exemplified below.,2.2 Annotation Format,[0],[0]
The annotation options are presented to the participants in a random order.,2.2 Annotation Format,[0],[0]
Participants specify the rank of each annotation token following the vertical bar.,2.2 Annotation Format,[0],[0]
"In this sentence, the label cop is preferred over aux for the word “be” and xcomp is preferred over advcl for the word “Common”.
...",2.2 Annotation Format,[0],[0]
"8 it PRP 10 nsubjpass 9 is VBZ 10 auxpass 10 planed VBN 0 root 11 to TO 15 mark 12 be VB 15 aux-cop | 2-1 13 in IN 15 case 14 Wimbledon NNP 15 compound 15 Common NNP 10 advcl-xcomp | 2-1 ...
",2.2 Annotation Format,[0],[0]
"The participants used basic validation scripts which checked for typos and proper formatting of the annotations, reviews and rankings.",2.2 Annotation Format,[0],[0]
We measure both parsing performance and interannotator agreement using tagging and parsing evaluation metrics.,2.3 Evaluation Metrics,[0],[0]
This choice allows for a direct comparison between parsing and agreement results.,2.3 Evaluation Metrics,[0],[0]
"In this context, POS refers to tagging accuracy.",2.3 Evaluation Metrics,[0],[0]
We utilize the standard metrics Unlabeled Attachment Score (UAS) and Label Accuracy (LA) to measure accuracy of head attachment and dependency labels.,2.3 Evaluation Metrics,[0],[0]
"We also utilize the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels.",2.3 Evaluation Metrics,[0],[0]
"In all our parsing and agreement experiments, we exclude punctuation tokens from the evaluation.",2.3 Evaluation Metrics,[0],[0]
"We use sentences from two publicly available datasets, covering two different genres.",2.4 Corpora,[0],[0]
"The first corpus, used in the experiments in sections 3 and 4, is the First Certificate in English (FCE) Cambridge Learner Corpus (Yannakoudakis et al., 2011).",2.4 Corpora,[0],[0]
"This dataset contains essays authored by upperintermediate level English learners3.
",2.4 Corpora,[0],[0]
"The second corpus is the WSJ part of the Penn Treebank (WSJ PTB) (Marcus et al., 1993).",2.4 Corpora,[0],[0]
"Since its release, this dataset has been the most commonly used resource for training and evaluation of English parsers.",2.4 Corpora,[0],[0]
"Our experiment on inter-annotator agreement in section 5 uses a random subset of the sentences in section 23 of the WSJ PTB, which is traditionally reserved for tagging and parsing evaluation.",2.4 Corpora,[0],[0]
We recruited five students at MIT as annotators.,2.5 Annotators,[0],[0]
Three of the students are linguistics majors and two are engineering majors with linguistics minors.,2.5 Annotators,[0],[0]
"Prior to participating in this study, the annotators completed two months of training.",2.5 Annotators,[0],[0]
"During training, the students attended tutorials, and learned the annotation guidelines for PTB POS tags, UD guidelines, as well as guidelines for annotating challenging syntactic structures arising from grammatical errors.",2.5 Annotators,[0],[0]
"The students also annotated individually six
3The annotation bias and quality results reported in sections 3 and 4 use the original learner sentences, which contain grammatical errors.",2.5 Annotators,[0],[0]
"These results were replicated on the error corrected versions of the sentences.
practice batches of 20-30 sentences from the English Web Treebank (EWT) (Silveira et al., 2014) and FCE corpora, and resolved annotation disagreements during group meetings.
",2.5 Annotators,[0],[0]
"Following the training period, the students annotated a treebank of learner English (Berzak et al., 2016) over a period of five months, three of which as a full time job.",2.5 Annotators,[0],[0]
"During this time, the students continued attending weekly meetings in which further annotation challenges were discussed and resolved.",2.5 Annotators,[0],[0]
"The annotation was carried out for sentences from the FCE dataset,",2.5 Annotators,[0],[0]
where both the original and error corrected versions of each sentence were annotated and reviewed.,2.5 Annotators,[0],[0]
"In the course of the annotation project, each annotator completed approximately 800 sentence annotations, and a similar number of sentence reviews.",2.5 Annotators,[0],[0]
The annotations and reviews were done in the same format used in this study.,2.5 Annotators,[0],[0]
"With respect to our experiments, the extensive experience of our participants and their prior work as a group strengthen our results, as these characteristics reduce the effect of anchoring biases and increase inter-annotator agreement.",2.5 Annotators,[0],[0]
Our first experiment is designed to test whether expert human annotators are biased towards POS tags and dependencies generated by automatic tools.,3 Parser Bias,[0],[0]
"We examine the common out-of-domain annotation scenario, where automatic tools are often trained on an existing treebank in one domain, and used to generate initial annotations to speed-up the creation of a gold standard for a new domain.",3 Parser Bias,[0],[0]
"We use the EWT UD corpus as the existing gold standard, and a sample of the FCE dataset as the new corpus.
",3 Parser Bias,[0],[0]
"Procedure Our experimental procedure, illustrated in figure 1(a) contains a set of 360 sentences (6,979 tokens) from the FCE, for which we generate three gold standards: one based on human annotations and two based on parser outputs.",3 Parser Bias,[0],[0]
"To this end, for each sentence, we assign at random four of the participants to the following annotation and review tasks.",3 Parser Bias,[0],[0]
"The fifth participant is left out to perform the quality ranking task described in section 4.
",3 Parser Bias,[0],[0]
"The first participant annotates the sentence from scratch, and a second participant reviews this an-
notation.",3 Parser Bias,[0],[0]
"The overall agreement of the reviewers with the annotators is 98.24 POS, 97.16 UAS, 96.3 LA and 94.81 LAS.",3 Parser Bias,[0],[0]
The next two participants review parser outputs.,3 Parser Bias,[0],[0]
"One participant reviews an annotation generated by the Turbo tagger and parser (Martins et al., 2013).",3 Parser Bias,[0],[0]
"The other participant reviews the output of the Stanford tagger (Toutanova et al., 2003) and RBG parser (Lei et al., 2014).",3 Parser Bias,[0],[0]
"The taggers and parsers were trained on the gold annotations of the EWT UD treebank, version 1.1.",3 Parser Bias,[0],[0]
"Both parsers use predicted POS tags for the FCE sentences.
",3 Parser Bias,[0],[0]
Assigning the reviews to the human annotations yields a human based gold standard for each sentence called “Human Gold”.,3 Parser Bias,[0],[0]
"Assigning the reviews to the tagger and parser outputs yields two parserbased gold standards, “Turbo Gold” and “RBG Gold”.",3 Parser Bias,[0],[0]
"We chose the Turbo-Turbo and StanfordRBG tagger-parser pairs as these tools obtain comparable performance on standard evaluation bench-
marks, while yielding substantially different annotations due to different training algorithms and feature sets.",3 Parser Bias,[0],[0]
"For our sentences, the agreement between the Turbo tagger and Stanford tagger is 96.97 POS.",3 Parser Bias,[0],[0]
"The agreement between the Turbo parser and RBG parser based on the respective tagger outputs is 90.76 UAS, 91.6 LA and 87.34 LAS.
",3 Parser Bias,[0],[0]
Parser Specific and Parser Shared Bias,3 Parser Bias,[0],[0]
"In order to test for parser bias, in table 1 we compare the performance of the Turbo-Turbo and Stanford-RBG tagger-parser pairs on our three gold standards.",3 Parser Bias,[0],[0]
"First, we observe that while these tools perform equally well on Human Gold, each taggerparser pair performs better than the other on its own reviews.",3 Parser Bias,[0],[0]
"These parser specific performance gaps are substantial, with an average of 1.15 POS, 2.63 UAS, 2.34 LA and 3.88 LAS between the two conditions.",3 Parser Bias,[0],[0]
This result suggests the presence of a bias towards the output of specific tagger-parser combinations.,3 Parser Bias,[0],[0]
"The practical implication of this outcome is that a gold standard created by editing an output of a parser is likely to boost the performance of that parser in evaluations and over-estimate its performance relative to other parsers.
",3 Parser Bias,[0],[0]
"Second, we note that the performance of each of the parsers on the gold standard of the other parser is still higher than its performance on the human gold standard.",3 Parser Bias,[0],[0]
"The average performance gap between these conditions is 1.08 POS, 1.66 UAS, 1.66 LA and 2.47 LAS.",3 Parser Bias,[0],[0]
"This difference suggests an annotation bias towards shared aspects in the predictions
of taggers and parsers, which differ from the human based annotations.",3 Parser Bias,[0],[0]
"The consequence of this observation is that irrespective of the specific tool that was used to pre-annotate the data, parser-based gold standards are likely to result in higher parsing performance relative to human-based gold standards.
",3 Parser Bias,[0],[0]
"Taken together, the parser specific and parser shared effects lead to a dramatic overall average error reduction of 49.18% POS, 33.71% UAS, 34.9% LA and 35.61% LAS on the parser-based gold standards compared to the human-based gold standard.",3 Parser Bias,[0],[0]
"To the best of our knowledge, these results are the first systematic demonstration of the tendency of the common approach of parser-based creation of gold standards to yield biased annotations and lead to overestimation of tagging and parsing performance.",3 Parser Bias,[0],[0]
In this section we extend our investigation to examine the impact of parser bias on the quality of parser-based gold standards.,4 Annotation Quality,[0],[0]
"To this end, we perform a manual comparison between human-based and parser-based gold standards.
",4 Annotation Quality,[0],[0]
"Our quality assessment experiment, depicted schematically in figure 1(b), is a ranking task.",4 Annotation Quality,[0],[0]
"For each sentence, a randomly chosen judge, who did not annotate or review the given sentence, ranks disagreements between the three gold standards Human Gold, Turbo Gold and RBG Gold, generated in the parser bias experiment in section 3.
",4 Annotation Quality,[0],[0]
"Table 2 presents the preference rates of judges
for the human-based gold standard over each of the two parser-based gold standards.",4 Annotation Quality,[0],[0]
"In all three evaluation categories, human judges tend to prefer the human-based gold standard over both parser-based gold standards.",4 Annotation Quality,[0],[0]
"This result demonstrates that the initial reduced quality of the parser outputs compared to human annotations indeed percolates via anchoring to the resulting gold standards.
",4 Annotation Quality,[0],[0]
The analysis of the quality assessment experiment thus far did not distinguish between cases where the two parsers agree and where they disagree.,4 Annotation Quality,[0],[0]
"In order to gain further insight into the relation between parser bias and annotation quality, we break down the results reported in table 2 into two cases which relate directly to the parser specific and parser shared components of the tagging and parsing performance gaps observed in the parser bias results reported in section 3.",4 Annotation Quality,[0],[0]
"In the first case, called “parser specific approval”, a reviewer approves a parser annotation which disagrees both with the output of the other parser and the Human Gold annotation.",4 Annotation Quality,[0],[0]
"In the second case, called “parser shared approval”, a reviewer approves a parser output which is shared by both parsers but differs with respect to Human Gold.
",4 Annotation Quality,[0],[0]
Table 3 presents the judge preference rates for the Human-Gold annotations in these two scenarios.,4 Annotation Quality,[0],[0]
We observe that cases in which the parsers disagree are of substantially worse quality compared to humanbased annotations.,4 Annotation Quality,[0],[0]
"However, in cases of agreement between the parsers, the resulting gold standards do not exhibit a clear disadvantage relative to the Human Gold annotations.
",4 Annotation Quality,[0],[0]
"This result highlights the crucial role of parser
specific approval in the overall preference of judges towards human-based annotations in table 2.",4 Annotation Quality,[0],[0]
"Furthermore, it suggests that annotations on which multiple state of the art parsers agree are of sufficiently high accuracy to be used to save annotation time without substantial impact on the quality of the resulting resource.",4 Annotation Quality,[0],[0]
In section 7 we propose an annotation scheme which leverages this insight.,4 Annotation Quality,[0],[0]
Agreement estimates in NLP are often obtained in annotation setups where both annotators edit the same automatically generated input.,5 Inter-annotator Agreement,[0],[0]
"However, in such experimental conditions, anchoring can introduce cases of spurious disagreement as well as spurious agreement between annotators due to alignment of one or both participants towards the given input.",5 Inter-annotator Agreement,[0],[0]
The initial quality of the provided annotations in combination with the parser bias effect observed in section 3 may influence the resulting agreement estimates.,5 Inter-annotator Agreement,[0],[0]
"For example, in Marcus et al. (1993) annotators were shown to produce POS tagging agreement of 92.8 on annotation from scratch, compared to 96.5 on reviews of tagger output.
",5 Inter-annotator Agreement,[0],[0]
"Our goal in this section is to obtain estimates for inter-annotator agreement on POS tagging and dependency parsing that control for parser bias, and
as a result, reflect more accurately human agreement on these tasks.",5 Inter-annotator Agreement,[0],[0]
"We thus introduce a novel pipeline based on human annotation only, which eliminates parser bias from the agreement measurements.",5 Inter-annotator Agreement,[0],[0]
Our experiment extends the human-based annotation study of Marcus et al. (1993) to include also syntactic trees.,5 Inter-annotator Agreement,[0],[0]
"Importantly, we include an additional review step for the initial annotations, designed to increase the precision of the agreement measurements by reducing the number of errors in the original annotations.
",5 Inter-annotator Agreement,[0],[0]
"Sentence
Scratch
Scratch reviewed
Figure 2: Experimental setup for the inter-annotator agreement experiment.",5 Inter-annotator Agreement,[0],[0]
"300 sentences (7,227 tokens) from section 23 of the PTB-WSJ are annotated and reviewed by four participants.",5 Inter-annotator Agreement,[0],[0]
The participants are assigned to the following tasks at random for each sentence.,5 Inter-annotator Agreement,[0],[0]
"Two participants annotate the sentence from scratch, and the remaining two participants review one of these annotations each.",5 Inter-annotator Agreement,[0],[0]
"Agreement is measured on the annotations (“scratch”) as well after assigning the review edits (“scratch reviewed”).
",5 Inter-annotator Agreement,[0],[0]
"For this experiment, we use 300 sentences (7,227 tokens) from section 23 of the PTB-WSJ, the standard test set for English parsing in NLP.",5 Inter-annotator Agreement,[0],[0]
"The experimental setup, depicted graphically in figure 2, includes four participants randomly assigned for each sentence to annotation and review tasks.",5 Inter-annotator Agreement,[0],[0]
"Two of the participants provide the sentence with annotations from scratch, while the remaining two participants provide reviews.",5 Inter-annotator Agreement,[0],[0]
"Each reviewer edits one of the annotations independently, allowing for correction of annotation errors while maintaining the independence of the annotation sources.",5 Inter-annotator Agreement,[0],[0]
"We measure agreement between the initial annotations (“scratch”), as well as the agreement between the reviewed versions of our sentences (“scratch reviewed”).
",5 Inter-annotator Agreement,[0],[0]
The agreement results for the annotations and the reviews are presented in table 4.,5 Inter-annotator Agreement,[0],[0]
"The initial agree-
ment rate on POS annotation from scratch is higher than in (Marcus et al., 1993).",5 Inter-annotator Agreement,[0],[0]
"This difference is likely to arise, at least in part, due to the fact that their experiment was conducted at the beginning of the annotation project, when the annotators had a more limited annotation experience compared to our participants.",5 Inter-annotator Agreement,[0],[0]
"Overall, we note that the agreement rates from scratch are relatively low.",5 Inter-annotator Agreement,[0],[0]
"The review round raises the agreement on all the evaluation categories due to elimination of annotation errors present the original annotations.
",5 Inter-annotator Agreement,[0],[0]
Our post-review agreement results are consequential in light of the current state of the art performance on tagging and parsing in NLP.,5 Inter-annotator Agreement,[0],[0]
"For more than a decade, POS taggers have been achieving over 97% accuracy with the PTB POS tag set on the PTB-WSJ test set.",5 Inter-annotator Agreement,[0],[0]
"For example, the best model of the Stanford tagger reported in Toutanova et al. (2003) produces an accuracy of 97.24 POS on sections 22-24 of the PTB-WSJ.",5 Inter-annotator Agreement,[0],[0]
"These accuracies are above the human agreement in our experiment.
",5 Inter-annotator Agreement,[0],[0]
"With respect to dependency parsing, recent parsers obtain results which are on par or higher than our inter-annotator agreement estimates.",5 Inter-annotator Agreement,[0],[0]
"For example, Weiss et al. (2015) report 94.26 UAS and Andor et al. (2016) report 94.61 UAS on section 23 of the PTB-WSJ using an automatic conversion of the PTB phrase structure trees to Stanford dependencies (De Marneffe et al., 2006).",5 Inter-annotator Agreement,[0],[0]
These results are not fully comparable to ours due to differences in the utilized dependency formalism and the automatic conversion of the annotations.,5 Inter-annotator Agreement,[0],[0]
"Nonetheless, we believe that the similarities in the tasks and evaluation data are sufficiently strong to indicate that dependency parsing for standard English newswire may be reaching human agreement levels.",5 Inter-annotator Agreement,[0],[0]
"The term “anchoring” was coined in a seminal paper by Tversky and Kahneman (1974), which demonstrated that numerical estimation can be biased by uninformative prior information.",6 Related Work,[0],[0]
"Subsequent work across various domains of decision making confirmed the robustness of anchoring using both informative and uninformative anchors (Furnham and Boo, 2011).",6 Related Work,[0],[0]
"Pertinent to our study, anchoring biases were also demonstrated when the participants were domain experts, although to a lesser degree than in the early anchoring experiments (Wilson et al., 1996; Mussweiler and Strack, 2000).
",6 Related Work,[0],[0]
"Prior work in NLP examined the influence of pre-tagging (Fort and Sagot, 2010) and pre-parsing (Skjærholt, 2013) on human annotations.",6 Related Work,[0],[0]
Our work introduces a systematic study of this topic using a novel experimental framework as well as substantially more sentences and annotators.,6 Related Work,[0],[0]
"Differently from these studies, our methodology enables characterizing annotation bias as anchoring and measuring its effect on tagger and parser evaluations.
",6 Related Work,[0],[0]
"Our study also extends the POS tagging experiments of Marcus et al. (1993), which compared inter-annotator agreement and annotation quality on manual POS tagging in annotation from scratch and tagger-based review conditions.",6 Related Work,[0],[0]
The first result reported in that study was that tagger-based editing increases inter-annotator agreement compared to annotation from scratch.,6 Related Work,[0],[0]
"Our work provides a novel agreement benchmark for POS tagging which reduces annotation errors through a review process while controlling for tagger bias, and obtains agreement measurements for dependency parsing.",6 Related Work,[0],[0]
The second result reported in Marcus et al. (1993) was that tagger-based edits are of higher quality compared to annotations from scratch when evaluated against an additional independent annotation.,6 Related Work,[0],[0]
"We modify this experiment by introducing ranking as an alternative mechanism for quality assessment, and adding a review round for human annotations from scratch.",6 Related Work,[0],[0]
"Our experiment demonstrates that in this configuration, parser-based annotations are of lower quality compared to human-based annotations.
",6 Related Work,[0],[0]
Several estimates of expert inter-annotator agreement for English parsing were previously reported.,6 Related Work,[0],[0]
"However, most such evaluations were conducted us-
ing annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014).",6 Related Work,[0],[0]
"A notable exception is the study of Sampson and Babarczy (2008) who measure agreement on annotation from scratch for English parsing in the SUSANNE framework (Sampson, 1995).",6 Related Work,[0],[0]
"The reported results, however, are not directly comparable to ours, due to the use of a substantially different syntactic representation, as well as a different agreement metric.",6 Related Work,[0],[0]
"Their study further suggests that despite the high expertise of the annotators, the main source of annotation disagreements was annotation errors.",6 Related Work,[0],[0]
"Our work alleviates this issue by using annotation reviews, which reduce the number of erroneous annotations while maintaining the independence of the annotation sources.",6 Related Work,[0],[0]
"Experiments on non-expert dependency annotation from scratch were previously reported for French, suggesting low agreement rates (79%) with an expert annotation benchmark (Gerdes, 2013).",6 Related Work,[0],[0]
"We present a systematic study of the impact of anchoring on POS and dependency annotations used in NLP, demonstrating that annotators exhibit an anchoring bias effect towards the output of automatic annotation tools.",7 Discussion,[0],[0]
"This bias leads to an artificial boost of performance figures for the parsers in question and results in lower annotation quality as compared with human-based annotations.
",7 Discussion,[0],[0]
"Our analysis demonstrates that despite the adverse effects of parser bias, predictions that are shared across different parsers do not significantly lower the quality of the annotations.",7 Discussion,[0],[0]
This finding gives rise to the following hybrid annotation strategy as a potential future alternative to human-based as well as parser-based annotation pipelines.,7 Discussion,[0],[0]
"In a hybrid annotation setup, human annotators review annotations on which several parsers agree, and complete the remaining annotations from scratch.",7 Discussion,[0],[0]
Such a strategy would largely maintain the annotation speed-ups of parser-based annotation schemes.,7 Discussion,[0],[0]
"At the same time, it is expected to achieve annotation quality comparable to human-based annotation by avoiding parser specific bias, which plays a pivotal role in the reduced quality of single-parser reviewing pipelines.
",7 Discussion,[0],[0]
"Further on, we obtain, to the best of our knowl-
edge for the first time, syntactic inter-annotator agreement measurements on WSJ-PTB sentences.",7 Discussion,[0],[0]
Our experimental procedure reduces annotation errors and controls for parser bias.,7 Discussion,[0],[0]
"Despite the detailed annotation guidelines, the extensive experience of our annotators, and their prior work as a group, our experiment indicates rather low agreement rates, which are below state of the art tagging performance and on par with state of the art parsing results on this dataset.",7 Discussion,[0],[0]
We note that our results do not necessarily reflect an upper bound on the achievable syntactic inter-annotator agreement for English newswire.,7 Discussion,[0],[0]
"Higher agreement rates could in principle be obtained through further annotator training, refinement and revision of annotation guidelines, as well as additional automatic validation tests for the annotations.",7 Discussion,[0],[0]
"Nonetheless, we believe that our estimates reliably reflect a realistic scenario of expert syntactic annotation.
",7 Discussion,[0],[0]
The obtained agreement rates call for a more extensive examination of annotator disagreements on parsing and tagging.,7 Discussion,[0],[0]
"Recent work in this area has already proposed an analysis of expert annotator disagreements for POS tagging in the absence of annotation guidelines (Plank et al., 2014).",7 Discussion,[0],[0]
"Our annotations will enable conducting such studies for annotation with guidelines, and support extending this line of investigation to annotations of syntactic dependencies.",7 Discussion,[0],[0]
"As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy.",7 Discussion,[0],[0]
"We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015).
",7 Discussion,[0],[0]
"Finally, since the release of the Penn Treebank in 1992, it has been serving as the standard benchmark for English parsing evaluation.",7 Discussion,[0],[0]
"Over the past few years, improvements in parsing performance on this dataset were obtained in small increments, and are commonly reported without a linguistic analysis of the improved predictions.",7 Discussion,[0],[0]
"As dependency parsing performance on English newswire may be reaching
human expert agreement, not only new evaluation practices, but also more attention to noisier domains and other languages may be in place.",7 Discussion,[0],[0]
"We thank our terrific annotators Sebastian Garza, Jessica Kenney, Lucia Lam, Keiko Sophie Mori and Jing Xian Wang.",Acknowledgments,[0],[0]
We are also grateful to Karthik Narasimhan and the anonymous reviewers for valuable feedback on this work.,Acknowledgments,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM) funded by NSF STC award CCF-1231216.",Acknowledgments,[0],[0]
This work was also supported by AFRL contract,Acknowledgments,[0],[0]
No. FA8750-15-C-0010 and by ERC Consolidator Grant LEXICAL (648909).,Acknowledgments,[0],[0]
We present a study on two key characteristics of human syntactic annotations: anchoring and agreement.,abstractText,[0],[0]
"Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards preexisting values.",abstractText,[0],[0]
We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output.,abstractText,[0],[0]
"Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with humanbased annotations.",abstractText,[0],[0]
"Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing.",abstractText,[0],[0]
"Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire.",abstractText,[0],[0]
We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.1,abstractText,[0],[0]
Anchoring and Agreement in Syntactic Annotations,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2215–2224, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Research in NLP relies heavily on the availability of human annotations for various linguistic prediction tasks.,1 Introduction,[0],[0]
"Such resources are commonly treated as de facto “gold standards” and are used for both training
1The experimental data in this study will be made publicly available.
and evaluation of algorithms for automatic annotation.",1 Introduction,[0],[0]
"At the same time, human agreement on these annotations provides an indicator for the difficulty of the task, and can be instrumental for estimating upper limits for the performance obtainable by computational methods.
",1 Introduction,[0],[0]
"Linguistic gold standards are often constructed using pre-existing annotations, generated by automatic tools.",1 Introduction,[0],[0]
The output of such tools is then manually corrected by human annotators to produce the gold standard.,1 Introduction,[0],[0]
"The justification for this annotation methodology was first introduced in a set of experiments on POS tag annotation conducted as part of the Penn Treebank project (Marcus et al., 1993).",1 Introduction,[0],[0]
"In this study, the authors concluded that tagger-based annotations are not only much faster to obtain, but also more consistent and of higher quality compared to annotations from scratch.",1 Introduction,[0],[0]
"Following the Penn Treebank, syntactic annotation projects for various languages, including German (Brants et al., 2002), French (Abeillé et al., 2003), Arabic (Maamouri et al., 2004) and many others, were annotated using automatic tools as a starting point.",1 Introduction,[0],[0]
"Despite the widespread use of this annotation pipeline, there is, to our knowledge, little prior work on syntactic annotation quality and on the reliability of system evaluations on such data.
",1 Introduction,[0],[0]
"In this work, we present a systematic study of the influence of automatic tool output on characteristics of annotations created for NLP purposes.",1 Introduction,[0],[0]
"Our investigation is motivated by the hypothesis that annotations obtained using such methodologies may be
2215
subject to the problem of anchoring, a well established and robust cognitive bias in which human decisions are affected by pre-existing values (Tversky and Kahneman, 1974).",1 Introduction,[0],[0]
"In the presence of anchors, participants reason relative to the existing values, and as a result may provide different solutions from those they would have reported otherwise.",1 Introduction,[0],[0]
"Most commonly, anchoring is manifested as an alignment towards the given values.
",1 Introduction,[0],[0]
"Focusing on the key NLP tasks of POS tagging and dependency parsing, we demonstrate that the standard approach of obtaining annotations via human correction of automatically generated POS tags and dependencies exhibits a clear anchoring effect – a phenomenon we refer to as parser bias.",1 Introduction,[0],[0]
"Given this evidence, we examine two potential adverse implications of this effect on parser-based gold standards.
",1 Introduction,[0],[0]
"First, we show that parser bias entails substantial overestimation of parser performance.",1 Introduction,[0],[0]
"In particular, we demonstrate that bias towards the output of a specific tagger-parser pair leads to over-estimation of the performance of these tools relative to other tools.",1 Introduction,[0],[0]
"Moreover, we observe general performance gains for automatic tools relative to their performance on human-based gold standards.",1 Introduction,[0],[0]
"Second, we study whether parser bias affects the quality of the resulting gold standards.",1 Introduction,[0],[0]
"Extending the experimental setup of Marcus et al. (1993), we demonstrate that parser bias may lead to lower annotation quality for parser-based annotations compared to humanbased annotations.
",1 Introduction,[0],[0]
"Furthermore, we conduct an experiment on interannotator agreement for POS tagging and dependency parsing which controls for parser bias.",1 Introduction,[0],[0]
Our experiment on a subset of section 23 of the WSJ Penn Treebank yields agreement rates of 95.65 for POS tagging and 94.17 for dependency parsing.,1 Introduction,[0],[0]
This result is significant in light of the state of the art tagging and parsing performance for English newswire.,1 Introduction,[0],[0]
"With parsing reaching the level of human agreement, and tagging surpassing it, a more thorough examination of evaluation resources and evaluation methodologies for these tasks is called for.
",1 Introduction,[0],[0]
"To summarize, we present the first study to measure and analyze anchoring in the standard parserbased approach to creation of gold standards for POS tagging and dependency parsing in NLP.",1 Introduction,[0],[0]
"We conclude that gold standard annotations that are
based on editing output of automatic tools can lead to inaccurate figures in system evaluations and lower annotation quality.",1 Introduction,[0],[0]
"Our human agreement experiment, which controls for parser bias, yields agreement rates that are comparable to state of the art automatic tagging and dependency parsing performance, highlighting the need for a more extensive investigation of tagger and parser evaluation in NLP.",1 Introduction,[0],[0]
"We examine two standard annotation tasks in NLP, POS tagging and dependency parsing.",2.1 Annotation Tasks,[0],[0]
"In the POS tagging task, each word in a sentence has to be categorized with a Penn Treebank POS tag (Santorini, 1990) (henceforth POS).",2.1 Annotation Tasks,[0],[0]
"The dependency parsing task consists of providing a sentence with a labeled dependency tree using the Universal Dependencies (UD) formalism (De Marneffe et al., 2014), according to version 1 of the UD English guidelines2.",2.1 Annotation Tasks,[0],[0]
"To perform this task, the annotator is required to specify the head word index (henceforth HIND) and relation label (henceforth REL) of each word in the sentence.
",2.1 Annotation Tasks,[0],[0]
"We distinguish between three variants of these tasks, annotation, reviewing and ranking.",2.1 Annotation Tasks,[0],[0]
"In the annotation variant, participants are asked to conduct annotation from scratch.",2.1 Annotation Tasks,[0],[0]
"In the reviewing variant, they are asked to provide alternative annotations for all annotation tokens with which they disagree.",2.1 Annotation Tasks,[0],[0]
"The participants are not informed about the source of the given annotation, which, depending on the experimental condition can be either parser output or human annotation.",2.1 Annotation Tasks,[0],[0]
"In the ranking task, the participants rank several annotation options with respect to their quality.",2.1 Annotation Tasks,[0],[0]
"Similarly to the review task, the participants are not given the sources of the different annotation options.",2.1 Annotation Tasks,[0],[0]
"Participants performing the annotation, reviewing and ranking tasks are referred to as annotators, reviewers and judges, respectively.",2.1 Annotation Tasks,[0],[0]
"All annotation tasks are performed using a CoNLL style text-based template, in which each word appears in a separate line.",2.2 Annotation Format,[0],[0]
"The first two columns of each line contain the word index and the word, re-
2http://universaldependencies.org/#en
spectively.",2.2 Annotation Format,[0],[0]
"The next three columns are designated for annotation of POS, HIND and REL.
",2.2 Annotation Format,[0],[0]
"In the annotation task, these values have to be specified by the annotator from scratch.",2.2 Annotation Format,[0],[0]
"In the review task, participants are required to edit preannotated values for a given sentence.",2.2 Annotation Format,[0],[0]
"The sixth column in the review template contains an additional # sign, whose goal is to prevent reviewers from overlooking and passively approving existing annotations.",2.2 Annotation Format,[0],[0]
"Corrections are specified following this sign in a space separated format, where each of the existing three annotation tokens is either corrected with an alternative annotation value or approved using a * sign.",2.2 Annotation Format,[0],[0]
Approval of all three annotation tokens is marked by removing the # sign.,2.2 Annotation Format,[0],[0]
"The example below presents a fragment from a sentence used for the reviewing task, in which the reviewer approves the annotations of all the words, with the exception of “help”, where the POS is corrected from VB to NN and the relation label xcomp is replaced with dobj.
",2.2 Annotation Format,[0],[0]
"... 5 you PRP 6 nsubj 6 need VBP 3 ccomp 7 help VB 6 xcomp # NN * dobj ...
",2.2 Annotation Format,[0],[0]
The format of the ranking task is exemplified below.,2.2 Annotation Format,[0],[0]
The annotation options are presented to the participants in a random order.,2.2 Annotation Format,[0],[0]
Participants specify the rank of each annotation token following the vertical bar.,2.2 Annotation Format,[0],[0]
"In this sentence, the label cop is preferred over aux for the word “be” and xcomp is preferred over advcl for the word “Common”.
...",2.2 Annotation Format,[0],[0]
"8 it PRP 10 nsubjpass 9 is VBZ 10 auxpass 10 planed VBN 0 root 11 to TO 15 mark 12 be VB 15 aux-cop | 2-1 13 in IN 15 case 14 Wimbledon NNP 15 compound 15 Common NNP 10 advcl-xcomp | 2-1 ...
",2.2 Annotation Format,[0],[0]
"The participants used basic validation scripts which checked for typos and proper formatting of the annotations, reviews and rankings.",2.2 Annotation Format,[0],[0]
We measure both parsing performance and interannotator agreement using tagging and parsing evaluation metrics.,2.3 Evaluation Metrics,[0],[0]
This choice allows for a direct comparison between parsing and agreement results.,2.3 Evaluation Metrics,[0],[0]
"In this context, POS refers to tagging accuracy.",2.3 Evaluation Metrics,[0],[0]
We utilize the standard metrics Unlabeled Attachment Score (UAS) and Label Accuracy (LA) to measure accuracy of head attachment and dependency labels.,2.3 Evaluation Metrics,[0],[0]
"We also utilize the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels.",2.3 Evaluation Metrics,[0],[0]
"In all our parsing and agreement experiments, we exclude punctuation tokens from the evaluation.",2.3 Evaluation Metrics,[0],[0]
"We use sentences from two publicly available datasets, covering two different genres.",2.4 Corpora,[0],[0]
"The first corpus, used in the experiments in sections 3 and 4, is the First Certificate in English (FCE) Cambridge Learner Corpus (Yannakoudakis et al., 2011).",2.4 Corpora,[0],[0]
"This dataset contains essays authored by upperintermediate level English learners3.
",2.4 Corpora,[0],[0]
"The second corpus is the WSJ part of the Penn Treebank (WSJ PTB) (Marcus et al., 1993).",2.4 Corpora,[0],[0]
"Since its release, this dataset has been the most commonly used resource for training and evaluation of English parsers.",2.4 Corpora,[0],[0]
"Our experiment on inter-annotator agreement in section 5 uses a random subset of the sentences in section 23 of the WSJ PTB, which is traditionally reserved for tagging and parsing evaluation.",2.4 Corpora,[0],[0]
We recruited five students at MIT as annotators.,2.5 Annotators,[0],[0]
Three of the students are linguistics majors and two are engineering majors with linguistics minors.,2.5 Annotators,[0],[0]
"Prior to participating in this study, the annotators completed two months of training.",2.5 Annotators,[0],[0]
"During training, the students attended tutorials, and learned the annotation guidelines for PTB POS tags, UD guidelines, as well as guidelines for annotating challenging syntactic structures arising from grammatical errors.",2.5 Annotators,[0],[0]
"The students also annotated individually six
3The annotation bias and quality results reported in sections 3 and 4 use the original learner sentences, which contain grammatical errors.",2.5 Annotators,[0],[0]
"These results were replicated on the error corrected versions of the sentences.
practice batches of 20-30 sentences from the English Web Treebank (EWT) (Silveira et al., 2014) and FCE corpora, and resolved annotation disagreements during group meetings.
",2.5 Annotators,[0],[0]
"Following the training period, the students annotated a treebank of learner English (Berzak et al., 2016) over a period of five months, three of which as a full time job.",2.5 Annotators,[0],[0]
"During this time, the students continued attending weekly meetings in which further annotation challenges were discussed and resolved.",2.5 Annotators,[0],[0]
"The annotation was carried out for sentences from the FCE dataset,",2.5 Annotators,[0],[0]
where both the original and error corrected versions of each sentence were annotated and reviewed.,2.5 Annotators,[0],[0]
"In the course of the annotation project, each annotator completed approximately 800 sentence annotations, and a similar number of sentence reviews.",2.5 Annotators,[0],[0]
The annotations and reviews were done in the same format used in this study.,2.5 Annotators,[0],[0]
"With respect to our experiments, the extensive experience of our participants and their prior work as a group strengthen our results, as these characteristics reduce the effect of anchoring biases and increase inter-annotator agreement.",2.5 Annotators,[0],[0]
Our first experiment is designed to test whether expert human annotators are biased towards POS tags and dependencies generated by automatic tools.,3 Parser Bias,[0],[0]
"We examine the common out-of-domain annotation scenario, where automatic tools are often trained on an existing treebank in one domain, and used to generate initial annotations to speed-up the creation of a gold standard for a new domain.",3 Parser Bias,[0],[0]
"We use the EWT UD corpus as the existing gold standard, and a sample of the FCE dataset as the new corpus.
",3 Parser Bias,[0],[0]
"Procedure Our experimental procedure, illustrated in figure 1(a) contains a set of 360 sentences (6,979 tokens) from the FCE, for which we generate three gold standards: one based on human annotations and two based on parser outputs.",3 Parser Bias,[0],[0]
"To this end, for each sentence, we assign at random four of the participants to the following annotation and review tasks.",3 Parser Bias,[0],[0]
"The fifth participant is left out to perform the quality ranking task described in section 4.
",3 Parser Bias,[0],[0]
"The first participant annotates the sentence from scratch, and a second participant reviews this an-
notation.",3 Parser Bias,[0],[0]
"The overall agreement of the reviewers with the annotators is 98.24 POS, 97.16 UAS, 96.3 LA and 94.81 LAS.",3 Parser Bias,[0],[0]
The next two participants review parser outputs.,3 Parser Bias,[0],[0]
"One participant reviews an annotation generated by the Turbo tagger and parser (Martins et al., 2013).",3 Parser Bias,[0],[0]
"The other participant reviews the output of the Stanford tagger (Toutanova et al., 2003) and RBG parser (Lei et al., 2014).",3 Parser Bias,[0],[0]
"The taggers and parsers were trained on the gold annotations of the EWT UD treebank, version 1.1.",3 Parser Bias,[0],[0]
"Both parsers use predicted POS tags for the FCE sentences.
",3 Parser Bias,[0],[0]
Assigning the reviews to the human annotations yields a human based gold standard for each sentence called “Human Gold”.,3 Parser Bias,[0],[0]
"Assigning the reviews to the tagger and parser outputs yields two parserbased gold standards, “Turbo Gold” and “RBG Gold”.",3 Parser Bias,[0],[0]
"We chose the Turbo-Turbo and StanfordRBG tagger-parser pairs as these tools obtain comparable performance on standard evaluation bench-
marks, while yielding substantially different annotations due to different training algorithms and feature sets.",3 Parser Bias,[0],[0]
"For our sentences, the agreement between the Turbo tagger and Stanford tagger is 96.97 POS.",3 Parser Bias,[0],[0]
"The agreement between the Turbo parser and RBG parser based on the respective tagger outputs is 90.76 UAS, 91.6 LA and 87.34 LAS.
",3 Parser Bias,[0],[0]
Parser Specific and Parser Shared Bias,3 Parser Bias,[0],[0]
"In order to test for parser bias, in table 1 we compare the performance of the Turbo-Turbo and Stanford-RBG tagger-parser pairs on our three gold standards.",3 Parser Bias,[0],[0]
"First, we observe that while these tools perform equally well on Human Gold, each taggerparser pair performs better than the other on its own reviews.",3 Parser Bias,[0],[0]
"These parser specific performance gaps are substantial, with an average of 1.15 POS, 2.63 UAS, 2.34 LA and 3.88 LAS between the two conditions.",3 Parser Bias,[0],[0]
This result suggests the presence of a bias towards the output of specific tagger-parser combinations.,3 Parser Bias,[0],[0]
"The practical implication of this outcome is that a gold standard created by editing an output of a parser is likely to boost the performance of that parser in evaluations and over-estimate its performance relative to other parsers.
",3 Parser Bias,[0],[0]
"Second, we note that the performance of each of the parsers on the gold standard of the other parser is still higher than its performance on the human gold standard.",3 Parser Bias,[0],[0]
"The average performance gap between these conditions is 1.08 POS, 1.66 UAS, 1.66 LA and 2.47 LAS.",3 Parser Bias,[0],[0]
"This difference suggests an annotation bias towards shared aspects in the predictions
of taggers and parsers, which differ from the human based annotations.",3 Parser Bias,[0],[0]
"The consequence of this observation is that irrespective of the specific tool that was used to pre-annotate the data, parser-based gold standards are likely to result in higher parsing performance relative to human-based gold standards.
",3 Parser Bias,[0],[0]
"Taken together, the parser specific and parser shared effects lead to a dramatic overall average error reduction of 49.18% POS, 33.71% UAS, 34.9% LA and 35.61% LAS on the parser-based gold standards compared to the human-based gold standard.",3 Parser Bias,[0],[0]
"To the best of our knowledge, these results are the first systematic demonstration of the tendency of the common approach of parser-based creation of gold standards to yield biased annotations and lead to overestimation of tagging and parsing performance.",3 Parser Bias,[0],[0]
In this section we extend our investigation to examine the impact of parser bias on the quality of parser-based gold standards.,4 Annotation Quality,[0],[0]
"To this end, we perform a manual comparison between human-based and parser-based gold standards.
",4 Annotation Quality,[0],[0]
"Our quality assessment experiment, depicted schematically in figure 1(b), is a ranking task.",4 Annotation Quality,[0],[0]
"For each sentence, a randomly chosen judge, who did not annotate or review the given sentence, ranks disagreements between the three gold standards Human Gold, Turbo Gold and RBG Gold, generated in the parser bias experiment in section 3.
",4 Annotation Quality,[0],[0]
"Table 2 presents the preference rates of judges
for the human-based gold standard over each of the two parser-based gold standards.",4 Annotation Quality,[0],[0]
"In all three evaluation categories, human judges tend to prefer the human-based gold standard over both parser-based gold standards.",4 Annotation Quality,[0],[0]
"This result demonstrates that the initial reduced quality of the parser outputs compared to human annotations indeed percolates via anchoring to the resulting gold standards.
",4 Annotation Quality,[0],[0]
The analysis of the quality assessment experiment thus far did not distinguish between cases where the two parsers agree and where they disagree.,4 Annotation Quality,[0],[0]
"In order to gain further insight into the relation between parser bias and annotation quality, we break down the results reported in table 2 into two cases which relate directly to the parser specific and parser shared components of the tagging and parsing performance gaps observed in the parser bias results reported in section 3.",4 Annotation Quality,[0],[0]
"In the first case, called “parser specific approval”, a reviewer approves a parser annotation which disagrees both with the output of the other parser and the Human Gold annotation.",4 Annotation Quality,[0],[0]
"In the second case, called “parser shared approval”, a reviewer approves a parser output which is shared by both parsers but differs with respect to Human Gold.
",4 Annotation Quality,[0],[0]
Table 3 presents the judge preference rates for the Human-Gold annotations in these two scenarios.,4 Annotation Quality,[0],[0]
We observe that cases in which the parsers disagree are of substantially worse quality compared to humanbased annotations.,4 Annotation Quality,[0],[0]
"However, in cases of agreement between the parsers, the resulting gold standards do not exhibit a clear disadvantage relative to the Human Gold annotations.
",4 Annotation Quality,[0],[0]
"This result highlights the crucial role of parser
specific approval in the overall preference of judges towards human-based annotations in table 2.",4 Annotation Quality,[0],[0]
"Furthermore, it suggests that annotations on which multiple state of the art parsers agree are of sufficiently high accuracy to be used to save annotation time without substantial impact on the quality of the resulting resource.",4 Annotation Quality,[0],[0]
In section 7 we propose an annotation scheme which leverages this insight.,4 Annotation Quality,[0],[0]
Agreement estimates in NLP are often obtained in annotation setups where both annotators edit the same automatically generated input.,5 Inter-annotator Agreement,[0],[0]
"However, in such experimental conditions, anchoring can introduce cases of spurious disagreement as well as spurious agreement between annotators due to alignment of one or both participants towards the given input.",5 Inter-annotator Agreement,[0],[0]
The initial quality of the provided annotations in combination with the parser bias effect observed in section 3 may influence the resulting agreement estimates.,5 Inter-annotator Agreement,[0],[0]
"For example, in Marcus et al. (1993) annotators were shown to produce POS tagging agreement of 92.8 on annotation from scratch, compared to 96.5 on reviews of tagger output.
",5 Inter-annotator Agreement,[0],[0]
"Our goal in this section is to obtain estimates for inter-annotator agreement on POS tagging and dependency parsing that control for parser bias, and
as a result, reflect more accurately human agreement on these tasks.",5 Inter-annotator Agreement,[0],[0]
"We thus introduce a novel pipeline based on human annotation only, which eliminates parser bias from the agreement measurements.",5 Inter-annotator Agreement,[0],[0]
Our experiment extends the human-based annotation study of Marcus et al. (1993) to include also syntactic trees.,5 Inter-annotator Agreement,[0],[0]
"Importantly, we include an additional review step for the initial annotations, designed to increase the precision of the agreement measurements by reducing the number of errors in the original annotations.
",5 Inter-annotator Agreement,[0],[0]
"Sentence
Scratch
Scratch reviewed
Figure 2: Experimental setup for the inter-annotator agreement experiment.",5 Inter-annotator Agreement,[0],[0]
"300 sentences (7,227 tokens) from section 23 of the PTB-WSJ are annotated and reviewed by four participants.",5 Inter-annotator Agreement,[0],[0]
The participants are assigned to the following tasks at random for each sentence.,5 Inter-annotator Agreement,[0],[0]
"Two participants annotate the sentence from scratch, and the remaining two participants review one of these annotations each.",5 Inter-annotator Agreement,[0],[0]
"Agreement is measured on the annotations (“scratch”) as well after assigning the review edits (“scratch reviewed”).
",5 Inter-annotator Agreement,[0],[0]
"For this experiment, we use 300 sentences (7,227 tokens) from section 23 of the PTB-WSJ, the standard test set for English parsing in NLP.",5 Inter-annotator Agreement,[0],[0]
"The experimental setup, depicted graphically in figure 2, includes four participants randomly assigned for each sentence to annotation and review tasks.",5 Inter-annotator Agreement,[0],[0]
"Two of the participants provide the sentence with annotations from scratch, while the remaining two participants provide reviews.",5 Inter-annotator Agreement,[0],[0]
"Each reviewer edits one of the annotations independently, allowing for correction of annotation errors while maintaining the independence of the annotation sources.",5 Inter-annotator Agreement,[0],[0]
"We measure agreement between the initial annotations (“scratch”), as well as the agreement between the reviewed versions of our sentences (“scratch reviewed”).
",5 Inter-annotator Agreement,[0],[0]
The agreement results for the annotations and the reviews are presented in table 4.,5 Inter-annotator Agreement,[0],[0]
"The initial agree-
ment rate on POS annotation from scratch is higher than in (Marcus et al., 1993).",5 Inter-annotator Agreement,[0],[0]
"This difference is likely to arise, at least in part, due to the fact that their experiment was conducted at the beginning of the annotation project, when the annotators had a more limited annotation experience compared to our participants.",5 Inter-annotator Agreement,[0],[0]
"Overall, we note that the agreement rates from scratch are relatively low.",5 Inter-annotator Agreement,[0],[0]
"The review round raises the agreement on all the evaluation categories due to elimination of annotation errors present the original annotations.
",5 Inter-annotator Agreement,[0],[0]
Our post-review agreement results are consequential in light of the current state of the art performance on tagging and parsing in NLP.,5 Inter-annotator Agreement,[0],[0]
"For more than a decade, POS taggers have been achieving over 97% accuracy with the PTB POS tag set on the PTB-WSJ test set.",5 Inter-annotator Agreement,[0],[0]
"For example, the best model of the Stanford tagger reported in Toutanova et al. (2003) produces an accuracy of 97.24 POS on sections 22-24 of the PTB-WSJ.",5 Inter-annotator Agreement,[0],[0]
"These accuracies are above the human agreement in our experiment.
",5 Inter-annotator Agreement,[0],[0]
"With respect to dependency parsing, recent parsers obtain results which are on par or higher than our inter-annotator agreement estimates.",5 Inter-annotator Agreement,[0],[0]
"For example, Weiss et al. (2015) report 94.26 UAS and Andor et al. (2016) report 94.61 UAS on section 23 of the PTB-WSJ using an automatic conversion of the PTB phrase structure trees to Stanford dependencies (De Marneffe et al., 2006).",5 Inter-annotator Agreement,[0],[0]
These results are not fully comparable to ours due to differences in the utilized dependency formalism and the automatic conversion of the annotations.,5 Inter-annotator Agreement,[0],[0]
"Nonetheless, we believe that the similarities in the tasks and evaluation data are sufficiently strong to indicate that dependency parsing for standard English newswire may be reaching human agreement levels.",5 Inter-annotator Agreement,[0],[0]
"The term “anchoring” was coined in a seminal paper by Tversky and Kahneman (1974), which demonstrated that numerical estimation can be biased by uninformative prior information.",6 Related Work,[0],[0]
"Subsequent work across various domains of decision making confirmed the robustness of anchoring using both informative and uninformative anchors (Furnham and Boo, 2011).",6 Related Work,[0],[0]
"Pertinent to our study, anchoring biases were also demonstrated when the participants were domain experts, although to a lesser degree than in the early anchoring experiments (Wilson et al., 1996; Mussweiler and Strack, 2000).
",6 Related Work,[0],[0]
"Prior work in NLP examined the influence of pre-tagging (Fort and Sagot, 2010) and pre-parsing (Skjærholt, 2013) on human annotations.",6 Related Work,[0],[0]
Our work introduces a systematic study of this topic using a novel experimental framework as well as substantially more sentences and annotators.,6 Related Work,[0],[0]
"Differently from these studies, our methodology enables characterizing annotation bias as anchoring and measuring its effect on tagger and parser evaluations.
",6 Related Work,[0],[0]
"Our study also extends the POS tagging experiments of Marcus et al. (1993), which compared inter-annotator agreement and annotation quality on manual POS tagging in annotation from scratch and tagger-based review conditions.",6 Related Work,[0],[0]
The first result reported in that study was that tagger-based editing increases inter-annotator agreement compared to annotation from scratch.,6 Related Work,[0],[0]
"Our work provides a novel agreement benchmark for POS tagging which reduces annotation errors through a review process while controlling for tagger bias, and obtains agreement measurements for dependency parsing.",6 Related Work,[0],[0]
The second result reported in Marcus et al. (1993) was that tagger-based edits are of higher quality compared to annotations from scratch when evaluated against an additional independent annotation.,6 Related Work,[0],[0]
"We modify this experiment by introducing ranking as an alternative mechanism for quality assessment, and adding a review round for human annotations from scratch.",6 Related Work,[0],[0]
"Our experiment demonstrates that in this configuration, parser-based annotations are of lower quality compared to human-based annotations.
",6 Related Work,[0],[0]
Several estimates of expert inter-annotator agreement for English parsing were previously reported.,6 Related Work,[0],[0]
"However, most such evaluations were conducted us-
ing annotation setups that can be affected by an anchoring bias (Carroll et al., 1999; Rambow et al., 2002; Silveira et al., 2014).",6 Related Work,[0],[0]
"A notable exception is the study of Sampson and Babarczy (2008) who measure agreement on annotation from scratch for English parsing in the SUSANNE framework (Sampson, 1995).",6 Related Work,[0],[0]
"The reported results, however, are not directly comparable to ours, due to the use of a substantially different syntactic representation, as well as a different agreement metric.",6 Related Work,[0],[0]
"Their study further suggests that despite the high expertise of the annotators, the main source of annotation disagreements was annotation errors.",6 Related Work,[0],[0]
"Our work alleviates this issue by using annotation reviews, which reduce the number of erroneous annotations while maintaining the independence of the annotation sources.",6 Related Work,[0],[0]
"Experiments on non-expert dependency annotation from scratch were previously reported for French, suggesting low agreement rates (79%) with an expert annotation benchmark (Gerdes, 2013).",6 Related Work,[0],[0]
"We present a systematic study of the impact of anchoring on POS and dependency annotations used in NLP, demonstrating that annotators exhibit an anchoring bias effect towards the output of automatic annotation tools.",7 Discussion,[0],[0]
"This bias leads to an artificial boost of performance figures for the parsers in question and results in lower annotation quality as compared with human-based annotations.
",7 Discussion,[0],[0]
"Our analysis demonstrates that despite the adverse effects of parser bias, predictions that are shared across different parsers do not significantly lower the quality of the annotations.",7 Discussion,[0],[0]
This finding gives rise to the following hybrid annotation strategy as a potential future alternative to human-based as well as parser-based annotation pipelines.,7 Discussion,[0],[0]
"In a hybrid annotation setup, human annotators review annotations on which several parsers agree, and complete the remaining annotations from scratch.",7 Discussion,[0],[0]
Such a strategy would largely maintain the annotation speed-ups of parser-based annotation schemes.,7 Discussion,[0],[0]
"At the same time, it is expected to achieve annotation quality comparable to human-based annotation by avoiding parser specific bias, which plays a pivotal role in the reduced quality of single-parser reviewing pipelines.
",7 Discussion,[0],[0]
"Further on, we obtain, to the best of our knowl-
edge for the first time, syntactic inter-annotator agreement measurements on WSJ-PTB sentences.",7 Discussion,[0],[0]
Our experimental procedure reduces annotation errors and controls for parser bias.,7 Discussion,[0],[0]
"Despite the detailed annotation guidelines, the extensive experience of our annotators, and their prior work as a group, our experiment indicates rather low agreement rates, which are below state of the art tagging performance and on par with state of the art parsing results on this dataset.",7 Discussion,[0],[0]
We note that our results do not necessarily reflect an upper bound on the achievable syntactic inter-annotator agreement for English newswire.,7 Discussion,[0],[0]
"Higher agreement rates could in principle be obtained through further annotator training, refinement and revision of annotation guidelines, as well as additional automatic validation tests for the annotations.",7 Discussion,[0],[0]
"Nonetheless, we believe that our estimates reliably reflect a realistic scenario of expert syntactic annotation.
",7 Discussion,[0],[0]
The obtained agreement rates call for a more extensive examination of annotator disagreements on parsing and tagging.,7 Discussion,[0],[0]
"Recent work in this area has already proposed an analysis of expert annotator disagreements for POS tagging in the absence of annotation guidelines (Plank et al., 2014).",7 Discussion,[0],[0]
"Our annotations will enable conducting such studies for annotation with guidelines, and support extending this line of investigation to annotations of syntactic dependencies.",7 Discussion,[0],[0]
"As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formulate further strategies for improving annotation accuracy.",7 Discussion,[0],[0]
"We believe that better understanding of human disagreements and their relation to disagreements between humans and parsers will also contribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an important topic that has received only limited attention thus far (Schwartz et al., 2011; Plank et al., 2015).
",7 Discussion,[0],[0]
"Finally, since the release of the Penn Treebank in 1992, it has been serving as the standard benchmark for English parsing evaluation.",7 Discussion,[0],[0]
"Over the past few years, improvements in parsing performance on this dataset were obtained in small increments, and are commonly reported without a linguistic analysis of the improved predictions.",7 Discussion,[0],[0]
"As dependency parsing performance on English newswire may be reaching
human expert agreement, not only new evaluation practices, but also more attention to noisier domains and other languages may be in place.",7 Discussion,[0],[0]
"We thank our terrific annotators Sebastian Garza, Jessica Kenney, Lucia Lam, Keiko Sophie Mori and Jing Xian Wang.",Acknowledgments,[0],[0]
We are also grateful to Karthik Narasimhan and the anonymous reviewers for valuable feedback on this work.,Acknowledgments,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM) funded by NSF STC award CCF-1231216.",Acknowledgments,[0],[0]
This work was also supported by AFRL contract,Acknowledgments,[0],[0]
No. FA8750-15-C-0010 and by ERC Consolidator Grant LEXICAL (648909).,Acknowledgments,[0],[0]
We present a study on two key characteristics of human syntactic annotations: anchoring and agreement.,abstractText,[0],[0]
"Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards preexisting values.",abstractText,[0],[0]
We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output.,abstractText,[0],[0]
"Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with humanbased annotations.",abstractText,[0],[0]
"Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing.",abstractText,[0],[0]
"Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for English newswire.",abstractText,[0],[0]
We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.1,abstractText,[0],[0]
Anchoring and Agreement in Syntactic Annotations,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1234–1243, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Ellipsis involves sentences with missing subparts, where those subparts must be interpretatively filled in by the hearer.",1 Introduction,[0],[0]
"How this is possible has been a major topic in linguistic theory for decades (Sag, 1976; Chung et al., 1995; Merchant, 2001).",1 Introduction,[0],[0]
"One widely studied example is verb phrase ellipsis (VPE), exemplified by (1).
",1 Introduction,[0],[0]
(1) Harry traveled to southern Denmark to study botany .,1 Introduction,[0],[0]
"Tom did too .
",1 Introduction,[0],[0]
In the second sentence (Tom did too),1 Introduction,[0],[0]
"the verb phrase is entirely missing, yet the hearer effortlessly ‘resolves’ (understands) its content to be traveled to southern Denmark to study botany.
",1 Introduction,[0],[0]
"Another widely studied case of ellipsis is sluicing, in which the majority of a question is unpronounced, as in (2).
",1 Introduction,[0],[0]
(2) Harry traveled to southern Denmark to study botany .,1 Introduction,[0],[0]
"I want to know why .
",1 Introduction,[0],[0]
"Here the content of the question, introduced by the WH-phrase why, is missing, yet it is understood by the hearer to be why did Harry travel to southern Denmark to study botany?.",1 Introduction,[0],[0]
"In both of these cases, ellipsis resolution is made possible by the presence of an antecedent, material in prior discourse that, informally speaking, is equivalent to what is missing.
",1 Introduction,[0],[0]
"Ellipsis poses an important challenge for many applications in language technology, as various forms of ellipsis are known to be frequent in a variety of languages and text types.",1 Introduction,[0],[0]
"This is perhaps most evident in the case of question-answering systems, since elliptical questions and elliptical answers are both very common in discourse.",1 Introduction,[0],[0]
"A computational system that can effectively deal with ellipsis involves three subtasks (Nielsen, 2005): ellipsis detection, in which a case of ellipsis is identified, antecedent selection, in which the antecedent for a case of ellipsis is found, and ellipsis resolution, where the content of the ellipsis is filled in with reference to the antecedent and the context of the ellipsis.",1 Introduction,[1.0],"['A computational system that can effectively deal with ellipsis involves three subtasks (Nielsen, 2005): ellipsis detection, in which a case of ellipsis is identified, antecedent selection, in which the antecedent for a case of ellipsis is found, and ellipsis resolution, where the content of the ellipsis is filled in with reference to the antecedent and the context of the ellipsis.']"
"Here, we focus on antecedent selection for sluicing.",1 Introduction,[1.0],"['Here, we focus on antecedent selection for sluicing.']"
"In addressing this problem of antecedent selection, we make use of a newly available annotated corpus of sluice occurrences (Anand and McCloskey, 2015).",1 Introduction,[1.0],"['In addressing this problem of antecedent selection, we make use of a newly available annotated corpus of sluice occurrences (Anand and McCloskey, 2015).']"
"This corpus consists of 4100 automatically parsed and annotated examples from the New York Times subset of the Gigaword Corpus, of which 2185 are
1234
publicly available.",1 Introduction,[1.0000000255557155],"['This corpus consists of 4100 automatically parsed and annotated examples from the New York Times subset of the Gigaword Corpus, of which 2185 are 1234 publicly available.']"
"Sluicing antecedent selection might appear simple – after all, it typically involves a sentential expression in the nearby context.",1 Introduction,[0],[0]
"However, analysis of the annotated corpus data reveals surprising ambiguity in the identification of the antecedent for sluicing.
",1 Introduction,[0],[0]
"In what follows, we describe a series of algorithms and models for antecedent selection in sluicing.",1 Introduction,[0],[0]
"Following section 2 on background, we describe our dataset in section 3.",1 Introduction,[0],[0]
"Then in section 4, we describe the structural factors that we have identified as relevant for antecedent selection.",1 Introduction,[0],[0]
"In section 5, we look at ways in which the content of the sluice and the content of the antecedent tend to be related to each other: we address lexical overlap, as well as the probabilistic relation of head verbs to WH-phrase types, and the relation of correlate expressions to sluice types.",1 Introduction,[0],[0]
"In section 6 we present two manually constructed baseline classifiers, and then we describe an approach to automatically tuning weights for the complete set of features.",1 Introduction,[0],[0]
"In section 7 we present the results of these algorithms and models, including results involving various subsets of features, to better understand their contributions to the overall results.",1 Introduction,[0],[0]
Finally in section 8 we discuss the results in light of plans for future work.,1 Introduction,[0],[0]
"Sluicing is formally defined in theoretical linguistics as ellipsis of a question, leaving only a WHphrase remnant.",2.1 Sluicing and ellipsis,[1.0],"['Sluicing is formally defined in theoretical linguistics as ellipsis of a question, leaving only a WHphrase remnant.']"
"While VPE is licensed only by a small series of auxiliaries (e.g., modals, do, see Lobeck (1995)), sluicing can occur wherever questions can, both in unembedded ‘root’ environments (e.g., Why?) or governed by the range of expressions that embed questions, like know in (2).",2.1 Sluicing and ellipsis,[0],[0]
"Sluicing is argued to be possible principally in contexts where there is uncertainty or vagueness about an issue (Ginzburg and Sag, 2000).",2.1 Sluicing and ellipsis,[0],[0]
"In some cases, this manifests as a correlate, an overt indefinite expression whose value is not further specified, like one of the candidates in (3).",2.1 Sluicing and ellipsis,[1.0],"['In some cases, this manifests as a correlate, an overt indefinite expression whose value is not further specified, like one of the candidates in (3).']"
"But in many others, like that in (2) or (4), there is no correlate, and the uncertainty is implicit.
",2.1 Sluicing and ellipsis,[0],[0]
"(3) They ’ve made an offer to [cor one of the can-
didates ] , but I ’m not sure which one
(4) They were firing , but at what was unclear
The existence of correlate-sluices suggests an obvious potential feature type for antecedent detection.",2.1 Sluicing and ellipsis,[0],[0]
"However, the annotated sluices in (Anand and McCloskey, 2015) have correlates only 22% of the time, making this process considerably harder.",2.1 Sluicing and ellipsis,[1.0],"['However, the annotated sluices in (Anand and McCloskey, 2015) have correlates only 22% of the time, making this process considerably harder.']"
We return to the question of correlates in section 5.1.,2.1 Sluicing and ellipsis,[0],[0]
"The first large-scale study of ellipsis is due to Hardt (1997), which addresses VPE.",2.2 Related Work,[0],[0]
"Examining 644 cases of VPE in the Penn Treebank, Hardt presents a manually constructed algorithm for locating the antecedent for VPE, and reports accuracy of 75% to 94.8%, depending on whether the metric used requires exact match or more liberal overlap or containment.",2.2 Related Work,[0],[0]
"Several preference factors for choosing VPE antecedents are identified (Recency, Clausal Relations, Parallelism, and Quotation).",2.2 Related Work,[0],[0]
"One of the central components of the analysis is the identification of structural constraints which rule out antecedents that improperly contain the ellipsis site, an issue we also address here for sluicing.",2.2 Related Work,[0],[0]
"Drawing on 1510 instances of VPE in both the British National Corpus (BNC) and the Penn Treebank, Nielsen (2005) shows that a maxent classifier using refinements of Hardt’s features can achieve roughly similar results to Hardt’s, but that additional lexical features do not help appreciably.
",2.2 Related Work,[0],[0]
"Nielsen chooses to optimize for Hardt’s Head Overlap metric, which assigns success to any candidate containing/contained in the correct antecedent.",2.2 Related Work,[0],[0]
"There are thus many “correct” antecedents for a given instance of VPE, which mitigates the class imbalance problem.",2.2 Related Work,[0],[0]
"However, the approach does not provide a way to discriminate between these containing candidates, an important step in the eventual goal of resolving the ellipsis.
",2.2 Related Work,[0],[0]
"There is no similar work on antecedent selection for sluicing, though there have been smallscale corpora gathered for sluices (Nykiel, 2010; Beecher, 2008).",2.2 Related Work,[0],[0]
"In addition, Fernandez et al. (2005) build rule-based and memory-based classifiers for the pragmatic import of root (unembedded) sluices in the BNC, based on the typology of Ginzburg and Sag (2000).",2.2 Related Work,[0],[0]
"Using features for the type of WH-
phrase, markers of mood (declarative/interrogative) and polarity (positive/negative) as well as the presence of correlate-like material (e.g., quantifiers, definites, etc.), they can diagnose the purpose of a sluice in a dataset of 300 root sluices with 79% average F-score, a 5% improvement over the MLE.",2.2 Related Work,[0],[0]
Fernandez et al. (2007) address the problem of identifying sluices and other non-sentential utterances.,2.2 Related Work,[0],[0]
We don’t address that problem in the current work.,2.2 Related Work,[0],[0]
"Furthermore, Fernandez et al. (2007) and Fernandez et al. (2008) address the general problem of nonsentential utterances or fragments in dialogue, including sluices.",2.2 Related Work,[0],[0]
"Sluicing in dialogue differs from sluicing in written text in various ways: there is a high proportion of root sluices, and antecedent selection is likely mitigated by the length of utterances and the order of conversation.",2.2 Related Work,[0],[0]
"As we discuss, many of our newswire sluices evince difficult patterns of containment inside the antecedent (particularly what we call interpolated and cataphoric sluices), and it does not appear from inspection that root sluices ever participate in such processes.
",2.2 Related Work,[0],[0]
"Looking more generally, there is an obvious potential connection between antecedent selection for ellipsis and the problem of coreference resolution (see Hardt (1999) for an explicit theoretical link between the two).",2.2 Related Work,[0],[0]
"However, entity coreference resolution is a problem with two major differences from ellipsis antecedent detection: a) the antecedent and anaphor often share a variety of syntactic, semantic, and morphological characteristics that can be featurally exploited; b) entity expressions in a text are often densely coreferent, which can help provide proxies for discourse salience of an entity.
",2.2 Related Work,[0],[0]
"In contrast, abstract anaphora, particularly discourse anaphora (this/that anaphora to something sentential), may offer a more parallel case to ours.",2.2 Related Work,[0],[0]
"Here, Kolhatkar et al. (2013) use a combination of syntactic type, syntactic/word context, length, and lexical features to identify the antecedents of anaphoric shell nouns (this fact) with precision from 0.35-0.72.",2.2 Related Work,[0],[0]
"Because of the sparsity of these cases, Kolhatkar et al. use Denis and Baldridge’s (2008) candidate ranking model (versus a standard mention-pair model (Soon et al., 2001)), in which all potential candidates for an anaphor receive a relative rank in the overall candidate pool.",2.2 Related Work,[0],[0]
"In this paper, we will pursue a hillclimbing approach to antecedent
selection, inspired by the candidate ranking scheme.",2.2 Related Work,[0],[0]
"Our dataset, described in Anand and McCloskey (2015), consists of 4100 sluicing examples from the New York Times subset of the Gigaword Corpus, 2nd edition.",3.1 The Annotated Dataset,[0],[0]
"This dataset is the first systematic, exhaustive corpus of sluicing.1 Each example is annotated with four main tags, given in terms of token sequence offsets: the sluice remnant, the antecedent, and then inside the antecedent the main predicate and the correlate, if any.",3.1 The Annotated Dataset,[0],[0]
The annotations also provide a free-text resolution.,3.1 The Annotated Dataset,[0],[0]
"Of the 4100 annotated, 2185 sluices have been made publicly available; we use that smaller dataset here.",3.1 The Annotated Dataset,[0],[0]
We make use of the annotation of the antecedent and remnant tags.,3.1 The Annotated Dataset,[1.0],['We make use of the annotation of the antecedent and remnant tags.']
See Anand and McCloskey (2015) for additional information on the dataset and the annotation scheme.,3.1 The Annotated Dataset,[0],[0]
"For the feature extraction in section 4, we rely on the the token, parsetree, and dependency parse information in Annotated Gigaword (extracted from Stanford CoreNLP).",3.1 The Annotated Dataset,[0],[0]
"Because of disagreements with the automatic parses of their data, Anand and McCloskey (2015) had annotators tag token sequences, not parsetree constituents.",3.2 Defining the Correct Antecedent,[0],[0]
"As a result, 10% of the annotations are not sentence-level (i.e., S, SBAR, SBARQ) constituents, such as the VP antecedent in (5), and 15% are not constituents at all, such as the case of (6), where the parse lacks an S node excluding the initial temporal clause.",3.2 Defining the Correct Antecedent,[0],[0]
We describe two different ways to define what will count as the correct antecedent in building and assessing our models.,3.2 Defining the Correct Antecedent,[0],[0]
Linguists generally agree that the antecedent for sluicing is a sentential constituent (see Merchant (2001) and references therein).,3.2.1 Constituent-Based Accuracy,[0],[0]
"Thus, it is straightforward to define the antecedent as the minimal
14100 sluices works out to roughly 0.14% of WH-phrases in the NYT portion of Gigaword.",3.2.1 Constituent-Based Accuracy,[0],[0]
"However, note that this includes all uses of WH-phrases (e.g., clefts and relative clauses), whereas sluicing is only possible for WH-questions.",3.2.1 Constituent-Based Accuracy,[0],[0]
"It’s not clear how many questions there are in the dataset (distinguishing questions and other WH-phrases is non-trivial).
",3.2.1 Constituent-Based Accuracy,[0],[0]
sentence-level constituent containing the token sequence marked as the antecedent.,3.2.1 Constituent-Based Accuracy,[0],[0]
"Then we define CONACCURACY as the percentage of cases in which the system selects the correct antecedent, as defined here.
",3.2.1 Constituent-Based Accuracy,[0],[0]
"While it is linguistically appealing to uniformly define candidates as sentential constituents, the annotator choices are sometimes not parsed that way, as in the following examples:
(5) “ I do n’t know how , ” said Mrs. Kitayeva , “",3.2.1 Constituent-Based Accuracy,[0],[0]
"but [S we want [V P to bring Lydia home ] , in any condition ] .",3.2.1 Constituent-Based Accuracy,[0],[0]
"”
(6) [S [SBAR When Brown , an all-America tight end , was selected in the first round in 1992 ]",3.2.1 Constituent-Based Accuracy,[0],[0]
"he was one of the highest rated players on the Giants ’ draft board ]
",3.2.1 Constituent-Based Accuracy,[0],[0]
"In such cases, there is a risk that we will not accurately assess the performance of our systems, since the system choice and annotator choice will only partially overlap.",3.2.1 Constituent-Based Accuracy,[0],[0]
"Here we define a metric which calculates the precision and recall of individual token occurrences, following Bos and Spenader (2011) (see also Kolhatkar and Hirst (2012)).",3.2.2 Token-Based Precision and Recall,[0],[0]
"This will accurately reflect the discrepancy in examples like (5) – according to ConAccuracy, a system choice of we want to bring Lydia home in any condition is simply considered correct, as it is the smallest sentential consituent containing the annotator choice.",3.2.2 Token-Based Precision and Recall,[0],[0]
"According to the Token-Based metric, we see that the system achieves recall of 1; however, since the system includes six extraneous tokens, precision is .4.",3.2.2 Token-Based Precision and Recall,[0],[0]
"We define TOKF as the harmonic mean of Token-Based Precision and Recall; for (5), TokF is .57.",3.2.2 Token-Based Precision and Recall,[1.0],"['We define TOKF as the harmonic mean of Token-Based Precision and Recall; for (5), TokF is .57.']"
The dataset consists of 2185 sluices extracted from the New York Times between July 1994 and December 2000.,3.3 Development and Test Data,[0],[0]
"For feature development, we segmented the data into a development set (DS) of the 453 sluices from July 1994 to December 1995.",3.3 Development and Test Data,[0],[0]
"The experiments in section 6 were carried out on a test set (TS) of the 1732 sluices in the remainder of the dataset, January 1996 to December 2000.",3.3 Development and Test Data,[0],[0]
"Under our assumptions, the candidate antecedent set for a given sluice is the set of all sentencelevel parsetree constituents within a n-sentence radius around the sluice sentence (based on DS, we set n = 2).",4 Structure,[0],[0]
"Because sentence-level constituents embed, in DS there are on average 6.4 candidate antecedents per sluice.",4 Structure,[0],[0]
"However, because ellipsis resolution involves identification of an antecedent, we assume that it, like anaphora resolution, should be sensitive to the overall salience of the antecedent.",4 Structure,[0],[0]
"This means that there should be, in principle, proxies for salience that we can exploit to diagnose the plausibility of a candidate for sluicing in general.",4 Structure,[0],[0]
"We consider four principle kinds of proxies: measures of candidate-sluice distance, measures of candidatesluice containment, measures of candidate ‘main point’, and candidate-sluice discourse relation markers.",4 Structure,[1.0],"['We consider four principle kinds of proxies: measures of candidate-sluice distance, measures of candidatesluice containment, measures of candidate ‘main point’, and candidate-sluice discourse relation markers.']"
"Within DS, 63% of antecedents are within the same sentence as the sluice site, and 33% are in the immediately preceding sentence.",4.1 Distance,[0],[0]
"In terms of candidates, the antecedent is on average the 5th candidate from the end of the n-sentence window.",4.1 Distance,[0],[0]
"The positive integer-valued feature DISTANCE tracks these notions of recency, where DISTANCE is 1 if the candidate is the candidate immediately preceding or following the sluice site (DISTANCE is defined to be 0 only for infinitival Ss like S0 in (7) below).",4.1 Distance,[1.0],"['The positive integer-valued feature DISTANCE tracks these notions of recency, where DISTANCE is 1 if the candidate is the candidate immediately preceding or following the sluice site (DISTANCE is defined to be 0 only for infinitival Ss like S0 in (7) below).']"
The feature FOLLOWS marks whether a candidate follows the sluice.,4.1 Distance,[0],[0]
"As two-thirds of the antecedents are in the same sentence as the sluice, we need measures to distinguish the candidates internal to the sentence containing the sluice.",4.2 Containment,[0],[0]
"In general, we want to exclude any candidate that ‘contains’ (i.e., dominates) the sluice, such as S0 and S-1 in (7).",4.2 Containment,[0],[0]
"One might have thought that we want to always exclude the entire sentence (here, S-4) as well, but there are several cases where the smallest sentence-level constituent containing the annotated antecedent dominates the sluice, including: parenthetical sluices inside the antecedent (8), sluices in subordinating clauses (9), or
sluice VPs coordinated with the antecedent VP (10).",4.2 Containment,[1.000000100518639],"['One might have thought that we want to always exclude the entire sentence (here, S-4) as well, but there are several cases where the smallest sentence-level constituent containing the annotated antecedent dominates the sluice, including: parenthetical sluices inside the antecedent (8), sluices in subordinating clauses (9), or sluice VPs coordinated with the antecedent VP (10).']"
"We thus need features to mark when such candidates are ‘non-containers’.
",4.2 Containment,[0],[0]
"(7) [S−4 [S−3 I have concluded that [S−2 I can not support the nomination ] , and [S−1 I need [S0 to explain why ]",4.2 Containment,[0.9931676495337617],"['(7) [S−4 [S−3 I have concluded that [S−2 I can not support the nomination ] , and [S−1 I need [S0 to explain why ] ]. ]']"
].,4.2 Containment,[0],[0]
"]
(8) [S−2 A major part of the increase in coverage , [S−1 though Mitchell ’s aides could not say just how much , ] would come from a provision providing insurance for children and pregnant women .",4.2 Containment,[0],[0]
"]
(9) [S−3 Weltlich still plans [S−2 to go , [S−1 although he does n’t know where ] ]",4.2 Containment,[0],[0]
"]
(10) [S−2 State regulators have ordered",4.2 Containment,[0],[0]
20th Century Industries Inc.,4.2 Containment,[0],[0]
[S−1 to begin paying $ 119 million in Proposition 103 rebates or explain why not by Nov. 14 .,4.2 Containment,[0],[0]
"]]
Conceptually, what renders S-3 in (9), S-2 in (8), and S-1 in (10) non-containers is that in all three cases the sluice is semantically dissociable from the rest of the sentence.",4.2 Containment,[0],[0]
We provide three features to mark this.,4.2 Containment,[0],[0]
"First, the boolean feature SLUICEINPARENTHETICAL marks when the sluice is dominated by a parenthetical (a PRN node in the parse or an (al)though SBAR delimited by punctuation).",4.2 Containment,[0],[0]
"Second, SLUICEINCOORDVP marks the configuration exemplified (10).
",4.2 Containment,[0],[0]
We also compute a less structure-specific measure of whether the candidate is meaningful once the sluice (and material dependent on it) is removed.,4.2 Containment,[0],[0]
"This means determining, for example, that S-4 in (7) is meaningful once to explain why .",4.2 Containment,[0],[0]
is removed but S-1 is not.,4.2 Containment,[0],[0]
"But the latter result follows from the fact that the main predicate of S-1, need takes the sluice govering verb explain as an argument, and hence removing that argument renders it semantically incomplete.",4.2 Containment,[0],[0]
We operationalize this in terms of complement dependency relations.,4.2 Containment,[0],[0]
We first locate the largest subgraph containing the sluice in a chain of ccomp and xcomp relations.,4.2 Containment,[0],[0]
"This gives us govmax, the highest such governor (i.e., explain) in Fig. 1.",4.2 Containment,[0],[0]
"The subgraph dependent on govmax is then removed, as indicated by the grayed boxes in Fig 1.",4.2 Containment,[0],[0]
"If the resulting subgraph contains a verbal governor, the candidate is meaningful and CONTAINSSLUICE
is false.",4.2 Containment,[0],[0]
"By this logic, S-4 in (7) is meaningful because it contains concluded, but S-1 is not, because there is no verbal material remaining.",4.2 Containment,[1.0],"['By this logic, S-4 in (7) is meaningful because it contains concluded, but S-1 is not, because there is no verbal material remaining.']"
"It has often been suggested (Asher, 1993; Hardt, 1997; Hardt and Romero, 2004) that the antecedent selection process is very closely tied to discourse relations, in the sense that there is a strong preference or even a requirement for a discourse relation between the antecedent and ellipsis.
",4.3 Discourse Structure,[1.0000000056511158],"['It has often been suggested (Asher, 1993; Hardt, 1997; Hardt and Romero, 2004) that the antecedent selection process is very closely tied to discourse relations, in the sense that there is a strong preference or even a requirement for a discourse relation between the antecedent and ellipsis.']"
"Here we define several features that indicate either that a discourse relation is present or is not present.
",4.3 Discourse Structure,[0],[0]
"We begin with features indicating that a discourse relation is not present: the theoretical linguistics literature on sluicing has noted that antecedents not in the ‘main point’ of an assertion (e.g., ones in appositives (AnderBois, 2014) or relative clauses (Cantor, 2013)) are very poor antecedents for sluices, presumably because their content is not very salient.",4.3 Discourse Structure,[1.0],"['We begin with features indicating that a discourse relation is not present: the theoretical linguistics literature on sluicing has noted that antecedents not in the ‘main point’ of an assertion (e.g., ones in appositives (AnderBois, 2014) or relative clauses (Cantor, 2013)) are very poor antecedents for sluices, presumably because their content is not very salient.']"
"The boolean features CANDINPARENTHETICAL (determined as for the sluice above) and CANDINRELCLAUSE mark these patterns.2
We also define features that would tend to indicate the presence of a discourse relation.",4.3 Discourse Structure,[1.0000000031411038],['The boolean features CANDINPARENTHETICAL (determined as for the sluice above) and CANDINRELCLAUSE mark these patterns.2 We also define features that would tend to indicate the presence of a discourse relation.']
These have to do with antecedents that occur after the sluice.,4.3 Discourse Structure,[1.0],['These have to do with antecedents that occur after the sluice.']
"Although antecedents overwhelmingly occur prior to sluices, we observe one prominent cataphoric pattern in DS, where the sentence containing the sluice is coordinated with a contrastive discourse relation; this is exemplified in (11).
",4.3 Discourse Structure,[0],[0]
"(11) “ I do n’t know why , but I like Jimmy Carter .",4.3 Discourse Structure,[1.0],"['(11) “ I do n’t know why , but I like Jimmy Carter .']"
"”
Three features are designed to capture this pattern: COORDWITHSLUICE indicates whether the sluice and candidate are connected by a coordination dependency, AFTERINITIALSLUICE marks the conjunctive condition where the candidate follows a sluice initial in its sentence, and IMMEDAFTERINITIALSLUICE marks a candidate that is the closest following candidate to an initial sluice.
",4.3 Discourse Structure,[0],[0]
"2This feature might be seen as an analog to the apposition features used in nominal coreference resolution (Bengtson and Roth, 2008), but there it is used to link appositives, whereas here it is to exclude candidates.",4.3 Discourse Structure,[0],[0]
"In addition to the structural features above, we also compute several features relating the content of the sluice site and the antecedent.",5 Content,[0],[0]
"The intuition behind these relational features is the following: each sluice type (why, who, how much, etc.) represents a certain type of question, and each candidate represents a particular type of predication.",5 Content,[0],[0]
"For a given a sluice type, some predications might fit more naturally than others.",5 Content,[1.0],"['For a given a sluice type, some predications might fit more naturally than others.']"
"More generally, it is a common view that an elliptical expression and its antecedent contain matching “parallel elements”.3
Below we describe three approaches to this: one simply looks for lexical overlap – words that occur both in the sluice expression and in the candidate.",5 Content,[0],[0]
The second involves a more general notion of how a predication fits with a sluice type.,5 Content,[0],[0]
"To capture this, we gather co-occurrence counts of main verb and sluice types.",5 Content,[0],[0]
The third approach compares potential correlates in candidates with the type of sluice.,5 Content,[0],[0]
One potential candidate for overlap information is the presence of a correlate in the antecedent.,5.1 Overlap,[0],[0]
"However, 75% of of sluices involve WH-phrases that typically involve no correlate (e.g., how, when, why).",5.1 Overlap,[0],[0]
"The pertinent exception to this are extent sluices ( ones where the remnant is how (much|many|JJ)), which have been argued to heavily favor a correlate (Merchant, 2001), such as (12) below (though see (13) for a counterexample).
",5.1 Overlap,[0],[0]
"3This term is from Dalrymple et al. (1991); a similar general view about parallelism in ellipsis arises in many different theories, such as Prüst et al. (1994) and Asher (1993).
",5.1 Overlap,[0],[0]
(12) The 49ers are [corr very good ] .,5.1 Overlap,[0],[0]
"It ’s hard to know how good because the Cowboys were the only team in the league who could test them .
",5.1 Overlap,[0],[0]
"We thus compute the number of tokens of OVERLAP between the content terms in the WH-phrase sluice (non-WH, non prepositional) and the entire antecedent.",5.1 Overlap,[0],[0]
"Even for correlate-less sluices, the WH-phrase must semantically cohere with the main predicate of the antecedent.",5.2 Wh-Predicate,[0],[0]
"Thus, in (13), S-3 is a more likely antecedent than S-2 because increase is more likely to take an implicit extent than predict.",5.2 Wh-Predicate,[0],[0]
"Although we could have consulted a lexically rich resource (e.g, VerbNet, FrameNet), our hope was that this general approach could carry over to less argument-specific combinations such as how with complete and raise in (14).
",5.2 Wh-Predicate,[0],[0]
"(13) [S−3 Deliveries would increase as a result of the acquisition ] ,",5.2 Wh-Predicate,[0],[0]
"[S−2 he predicted ] , but [S−1 he would not say by how much ]
(14) [S−4 [S−3 Once the city and team complete a contract ] , the Firebirds will begin to raise $ 9 million ] ,",5.2 Wh-Predicate,[0],[0]
"[S−2 team president Yount said ] , [S−1 but he would not say how ] .
",5.2 Wh-Predicate,[0],[0]
"Our assumption is that some main predicates are more likely than others for a given sluice type, and we wish to gather data that reveals these probabilities.",5.2 Wh-Predicate,[0],[0]
"This is somewhat similar to the approach of Hindle and Rooth (1993), who gather probabilities
that reflect the association of verbal and nominal heads with prepositions to disambiguiate prepositional phrase attachment.
",5.2 Wh-Predicate,[0],[0]
"One way to collect these would be to use our sluicing data, which consists of a total of 2185 annotated examples.",5.2 Wh-Predicate,[0],[0]
"However, the probabilities of interest are not about sluicing per se.",5.2 Wh-Predicate,[0],[0]
"Rather, they are about how well a given predication fits with a given type of question.",5.2 Wh-Predicate,[0],[0]
"Thus instead of using our comparatively small set of annotated sluicing examples, we used overt WH-constructions in Gigaword to observe cooccurrences between question types and main predicates.",5.2 Wh-Predicate,[0],[0]
"To find overt WH-constructions, we extracted all instances where a WH-phrase is: a) a dependent (to exclude cases like Who?) and b) not at the right edge of a VP (to exclude sluices like know who, per Anand and McCloskey (2015)).",5.2 Wh-Predicate,[0],[0]
"To further ensure that we were not overlapping with our dataset, we did this only for the non=NYT subsets of Gigaword (i.e., AFP, APW, CNA, and LTW).",5.2 Wh-Predicate,[0],[0]
"This procedure generated 687,000 WH-phrase instances, and 79,753 WH-phrase-governor bigram types.",5.2 Wh-Predicate,[0],[0]
"From these bigrams, we calculated WHPREDICATE, the normalized pmi of WH-phrase type and governor lemma in Annotated Gigaword.",5.2 Wh-Predicate,[0],[0]
"Twenty-two percent of our data has correlates, and these correlates should be discriminative for particular sluice types.",5.3 Correlate Overlap,[0],[0]
"For example, temporal (when) sluices have timespan correlates (e.g., tomorrow, later), while entity (who/what) sluices have individuals as correlates (e.g., someone, a book).",5.3 Correlate Overlap,[0],[0]
"We extracted four potential integer-valued correlate features from each candidate: LOCATIVECORR is the number of primarily locative prepositions (those with a locative MLE in The Preposition Project (Litowski and Hargraves, 2005)).",5.3 Correlate Overlap,[0],[0]
"ENTITYCORR is the number of nominals in the candidate that are indefinite (bare nominals or ones with a determiner relation to a, an and weak quantifiers (some, many, much, few, several).TEMPORALCORR is the number of lexical patterns in the candidate for TIMEX3 annotations in Timebank 1.2 (Pustejovesky et al., 2016).",5.3 Correlate Overlap,[0],[0]
WHICHCORR is the pattern for entities plus or.,5.3 Correlate Overlap,[0],[0]
"Mention-pair coreference models reduce coreference resolution to two steps: a local binary classification, and a global resolution of coreference chains.",6 Algorithms,[0],[0]
"We may see antecedent selection as a similar two-stage process: classification on the probability a given candidate is an antecedent, and then selection of the most likely candidate for a given sluice.",6 Algorithms,[0],[0]
"As Denis and Baldridge (2008) note, one limitation of this approach is that the overall rank of the candidates is never directly learned.",6 Algorithms,[0],[0]
"They instead propose to learn the rank of a candidate c for antecedent a, modeled as the log-linear score of a candidate across a set of coreference models m, (exp ∑ j wjmj(c, a)), normalized by the sum of candidate scores.",6 Algorithms,[0],[0]
"We apply the same approach to our problem, viewing each feature in Table 1 as a model, and estimating weights for the features by hill-climbing.",6 Algorithms,[0],[0]
We begin by defining constructed baselines which are implemented by manually assigning weights.,6 Algorithms,[0],[0]
We then consider the results of a maxent classifier over the features.,6 Algorithms,[0],[0]
"Finally, we determine the weights directly by hill-climbing with random restarts.",6 Algorithms,[0],[0]
Random simply selects candidates at random.,6.1 Manual Baselines,[0],[0]
Clst chooses the closest candidate that starts before the sluice.,6.1 Manual Baselines,[0],[0]
"This is done by assigning a weight of -1 to DISTANCE and -10 to FOLLOWING (to exclude
the following candidate), and 0 to all other features.",6.1 Manual Baselines,[0],[0]
"ClstBef chooses the closest candidate that entirely precedes the sluice (i.e., starts before and does not contain the sluice site).",6.1 Manual Baselines,[0],[0]
"To construct ClstBef, we change the weight of CONTAINSSLUICE to -10, which means that candidates containing the sluice will never be chosen.",6.1 Manual Baselines,[0],[0]
We trained a maxent classifier on the features in Table 1 for the binary antecedent-not antecedent task.,6.2 A maxent model,[1.0],['We trained a maxent classifier on the features in Table 1 for the binary antecedent-not antecedent task.']
"With 10-fold cross-validation on the test set, the maxent model achieved an average accuracy on the binary antecedent task of 87.1 and an F-score of 53.8 (P=63.9, R=46.5).",6.2 A maxent model,[0],[0]
We then constructed an antecedent selector that chose the candidate with the highest classifier score.,6.2 A maxent model,[1.0],['We then constructed an antecedent selector that chose the candidate with the highest classifier score.']
"We define a procedure to hill-climb over weights in order to maximize ConAccuracy over the entire training set (maximizing TokF yielded similar results, and is not reported here).",6.3 Hill-Climbing,[0],[0]
"Weights are initialized with random values in the interval [-10,10].",6.3 Hill-Climbing,[0],[0]
"At iteration i, the current weight vector is compared to alternatives differing from it by the current step size on one weight, and the best new vector is selected.",6.3 Hill-Climbing,[0],[0]
"For the results reported here, we performed 13 random restarts and exponential step size 10∗i.5 (values that maximized performance on the DS).",6.3 Hill-Climbing,[0],[0]
"We performed 10-fold cross-validation over TS on the hill-climbed and maxent models above, producing average ConAccuracy and TokF as shown in Table 2, which also gives results of the three baselines on the entire dataset.",7 Results,[0],[0]
"The hill-climbed approach with all features substantially outperformed the baselines, achieving a ConAccuracy of 72.4%.
",7 Results,[0],[0]
We investigated the performance of our hillclimbing procedure with ablation of several feature subsets.,7 Results,[0],[0]
"We ablated features by group, as in Table 1.",7 Results,[0],[0]
"Table 2 shows the results for using four groups and only one group, as well as the top two three group and two group combinations.
",7 Results,[0],[0]
Features fall in three tiers.,7 Results,[0],[0]
"Distance features are the most predictive: all the top systems use them, and they alone perform reasonably well (like Clst).
",7 Results,[0],[0]
Containment and then Discourse Structure features are the next most helpful.,7 Results,[0],[0]
"The full system has a ConAccuracy of 72.4 on the TS, not reliably different from several systems without Content and/or Correlate features.",7 Results,[0],[0]
"At the same time, the scores for these feature types on their own show that they are predictive of the antecedent: The Correlate feature R has a score of 22.2, which is a rather modest, but statistically significant, improvement over Random.",7 Results,[0],[0]
"The Content feature N improves quite substantially, up to 30.7.",7 Results,[0],[0]
"This suggests that there is some redundancy with the other features, so that the contributions of Content and Correlate are not observed in combination with them.",7 Results,[0],[0]
"(HC-N and HC-R’s lower than Random TokF is a result of precision: Random more often selects very small candidates inside the correct antecedent, leading to a higher precision.)
",7 Results,[0],[0]
"The Content and Correlate features concern relations between the type of sluice and the content of the antecedent; since other features do not capture this, it is puzzling that these provide no further improvement.",7 Results,[0],[0]
"To better understand why this is, we investigated the performance of our feature
sets by sluice type.",7 Results,[0],[0]
"For the top performing systems, we found that antecedent selection for sluices over extents (e.g, how much, how tall) performed 11% better than average and those over reasons (why) and manner (how) performed 13% worse than average; no other WH-phrase types differed significantly from average.",7 Results,[0],[0]
"Importantly, this finding was consistent even for the systems without Content or Correlate features, which we extracted in large part to help highlight possible correlate material for extent sluices as well as entity (who/what) and temporal (when) sluices.
",7 Results,[0],[0]
"We also examined systems knocking out our best performing features, Distance, Containment, and Discourse Structure.",7 Results,[0],[0]
"When Distance features were omitted, we saw a bimodal distribution: reason and manner sluice antecedent selection was 31% better than expected (based on the full system differences discussed above), and the other sluices performed 22% worse.",7 Results,[0],[0]
"When Containment features were omitted, reason sluices performed 10% better than expected, while extent ones were 10% worse.",7 Results,[1.0],"['When Containment features were omitted, reason sluices performed 10% better than expected, while extent ones were 10% worse.']"
"Finally, when Discourse Structure features were removed, entity and temporal sluices had half the error rate we would expect.",7 Results,[0],[0]
"While it is hard to provide a clear takeaway from these differences, they do point to the relative difficulty in locating sluice antecedents based on WH-phrase type, and they also suggest that different sluice types present quite different challenges.",7 Results,[0],[0]
This suggests that one promising line might be to learn different featural weights for each sluice type.,7 Results,[0],[0]
We have addressed the problem of sluicing antecedent selection by defining linguistically sophisticated features describing the structure and content of candidates.,8 Conclusion,[0],[0]
"We described a hill-climbed model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline.",8 Conclusion,[0],[0]
We have shown that both syntactic and discourse relationships are important in antecedent selection.,8 Conclusion,[0],[0]
"In future work, we hope to improve the performance of several of our features.",8 Conclusion,[0],[0]
Notable among these are the discourse structural proxies we found to make a contribution to the model.,8 Conclusion,[0],[0]
"These features constitute a quite limited view of discourse struc-
ture, and we suspect that a better representation of discourse structure might well lead to further improvements.",8 Conclusion,[0.9999999509543009],"['These features constitute a quite limited view of discourse struc- ture, and we suspect that a better representation of discourse structure might well lead to further improvements.']"
"One potential path would be to leverage data where discourse relations are explicitly annotated, such as that in the Penn Discourse Treebank",8 Conclusion,[0],[0]
"(Prasad et al., 2008).",8 Conclusion,[0],[0]
"In addition, although our Content and Correlate features were not useful alongside the others, we hope that more refined versions of those could provide some assistance.",8 Conclusion,[0],[0]
"We also noted that our performance was impacted by WH-types, and therefore it might be helpful to learn different featural weights per type.
",8 Conclusion,[0],[0]
"In closing, we would like to return to the larger question of effectively handling ellipsis.",8 Conclusion,[1.0],"['In closing, we would like to return to the larger question of effectively handling ellipsis.']"
"The solution to antecedent selection that we have presented here provides a starting point for addressing the problem of resolution, in which the content of the sluice is filled in.",8 Conclusion,[0],[0]
"However, even if the correct antecedent is selected, the missing content is not always an exact copy of the antecedent – often substantial modifications will be required – and an effective resolution system will have to negotiate such mismatches.",8 Conclusion,[0],[0]
"As it turns out, many incorrect antecedents differ from the correct antecedent in ways highly reminiscent of these mismatches.",8 Conclusion,[0],[0]
"Thus, some of the errors of our selection algorithm may be most naturally addressed by the resolution system, and it may be that the relative priority of the specific challenges we identified here will become clearer as we address the next step down in the overall pipeline.",8 Conclusion,[0],[0]
"We gratefully acknowledge the work of Jim McCloskey in helping to create the dataset investigated here, as well as the principal annotators on the project.",Acknowledgments,[0],[0]
"We thank Jordan Boyd-Graber, Ellen Riloff, and three incisive reviewers for helpful comments.",Acknowledgments,[0],[0]
This research has been sponsored by NSF grant number 1451819.,Acknowledgments,[0],[0]
Sluicing is an elliptical process where the majority of a question can go unpronounced as long as there is a salient antecedent in previous discourse.,abstractText,[0],[0]
This paper considers the task of antecedent selection: finding the correct antecedent for a given case of sluicing.,abstractText,[0],[0]
"We argue that both syntactic and discourse relationships are important in antecedent selection, and we construct linguistically sophisticated features that describe the relevant relationships.",abstractText,[0],[0]
We also define features that describe the relation of the content of the antecedent and the sluice type.,abstractText,[0],[0]
"We develop a linear model which achieves accuracy of 72.4%, a substantial improvement over a strong manually constructed baseline.",abstractText,[0],[0]
Feature analysis confirms that both syntactic and discourse features are important in antecedent selection.,abstractText,[0],[0]
Antecedent Selection for Sluicing: Structure and Content,title,[0],[0]
