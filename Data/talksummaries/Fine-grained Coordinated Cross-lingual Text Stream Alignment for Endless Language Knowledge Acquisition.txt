0	20	Coordinated text streams (Wang et al., 2007) refer to the text streams that are topically related and indexed by the same set of time points.
1	15	Previous studies (Wang et al., 2007; Hu et al., 2012) on coordinated text stream focus on discovering and aligning common topic patterns across languages.
2	26	Despite their contributions to applications like cross-lingual information retrieval and topic analysis, such a coarse-grained topic-level alignment framework inevitably overlooks many useful fine-grained alignment knowledge.
4	83	In addition to (a) bi-lingual word translations, we can also discover (b) polysemous and multi-referential words if one Chinese word is aligned to multiple English words, (c) synonymous and co-referential word pairs if two Chinese words are aligned to the same English word, and (d) entity phrases (e.g.,阿布扎比 in Figure 1) inject Dezhou Texas ASEAN Abu Dhabi Chinese English (a) (b) (c) (d) Figure 1: Knowledge derived from fine-grained crosslingual text stream alignments: (a) word translations; (b) polysemy/multi-references; (c) synonym/coreference; (d) entity phrases if adjacent Chinese words in text are aligned to the same English named entity.
5	52	In order to acquire language knowledge for Natural Language Processing (NLP) applications, we study fine-grained cross-lingual text stream alignment.
6	22	Instead of directly turning massive, unstructured data streams into structured knowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge.
7	43	Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams.
8	18	With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment.
9	55	For example, in Figure 2, each node in a BINet is a bursty word with one of its burst periods, representing an important information unit in a text stream.
10	93	To decipher the Chinese BINet, our approach first focuses on the nodes in the English BINet in Figure 2 as the candidates because they co-burst with the Chinese nodes.
11	56	Then, we decipher some nodes based on prior knowledge (the green node), the pronunciation similarity clue (the orange nodes) or literal translation similarity clue (the blue node).
12	21	These deciphered nodes will serve as neighbor clues to decipher their adjacent nodes (the red node) which will then be used for further decipherment (e.g., decipher the yellow node) through knowledge propagation across the network, as the dashed arrows in Figure 2 show.
15	22	The main contributions of this paper are: • We propose a promising framework to mine knowledge from inexhaustible coordinated cross-lingual text streams through finegrained alignment, exploring a paradigm for language knowledge acquisition.
26	16	We define Gc = 〈Vc, Ec,ωc,πc〉 and Ge = 〈Ve, Ee,ωe,πe〉 as the Chinese BINet and English BINet respectively.
44	24	Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: Sp ∈ [0, 1] ∝ 1LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string.
48	17	Even though “澳洲网球公开赛(Australian Open)” is not in the bi-lingual lexicon, “Australian” and “open” are in the lexicon and their Chinese translations are “澳洲的(Australian)” and “公开(open)” respectively.
49	29	If we literally translate “Australian Open” word by word, we will get “澳洲的公 开” which has long common subsequences with the Chinese node “澳洲网球公开赛(Australian Open)”, inferring that “Australian Open” is likely to be the translation of “澳洲网球公开赛”.
50	20	Motivated by this observation, for a candidate e ∈ Cand(c), we first extract its possible Chinese translations C(e) from the bilingual lexicon.
52	18	Then, for 〈c, e〉, we define St as its scaled translation similarity score whose range is [0, 1]: St ∈ [0, 1] ∝ maxc′∈C(e) LCS(c, c′) where maxc′∈C(e) LCS(c, c′) is maximum length of the longest common subsequence between c and c′ ∈ C(e).
54	31	By analyzing a node’s neighbors, we can learn useful topic-level knowledge to decipher the node.
55	22	For the example in Figure 2, “艾宁(Henin)” in the Chinese BINet has neighbors such as “威廉(Williams)”, “澳洲 网球公开赛(Australian Open)” and “郑洁(Zheng Jie)” while “Justine Henin” in the English BINet is connected with “Serena Williams”, “Australian Open” and “Zheng Jie”.
56	48	If we know “Serena Williams”, “Australian Open” and “Zheng Jie” are the counterpart of ‘威廉”, “澳洲网球公开赛” and “郑洁” respectively, we can infer “Justine Henin” is likely to be the counterpart of “艾宁”, which can be further used as a clue to decipher its neighbors such as “外卡(wildcard)” through knowledge propagation.
59	45	Correlation of burst If the word of e ∈ Cand(c) frequently co-bursts with the word of c, then e is likely to be the counterpart of c. For example, “Serena Williams” in the English stream usually co-bursts with “小威” in the Chinese stream, as shown in Figure 3, which is a useful clue to infer that “Serena Williams” is the counterpart of “小威”.
60	15	We define Sb as the burst correlation score: Sb = sw(c) · sw(e) ‖sw(c)‖1 + ‖sw(e)‖1 − sw(c) · sw(e) (2) where w(v) denotes the word of the node v and sw denotes the burst sequence of the word w in which each entry is a binary variable indicating if w bursts at a moment throughout the time frame.
63	24	We define the overall (credibility) score as the linear combination of the clues introduced above: Score(c, e) = ηSp + λSt + γSn + δSb (3) where Sp, St, Sn and Sb are the scores that measure the value/reliability of the pronunciation, translation, neighbor and burst correlation clues respectively, and η, λ, γ and δ are hyperparameters for adjusting their weights.
69	24	Algorithm 1 Graph-based Decipherment 1: For the determined pair 〈c, e〉 based on the prior knowl- edge, Score(c, e)← 1.0 2: For other undermined pairs 〈c, e〉, initialize Score(c, e) according to Eq (4); 3: while True (until ∆Conf(Gc, Ge) ≤ 0.0001) do 4: for each undetermined pair 〈c, e〉 do 5: Compute new score according to Eq (3); 6: update(c, e) = min(1.0, new score) 7: end for 8: for each undetermined pair 〈c, e〉 do 9: Score(c, e)← update(c, e) 10: end for 11: end while ∆Conf(Gc, Ge) in the 3rd line of Algorithm 1 is the difference between the network decipherment confidence score at the current iteration and that at its previous iteration.
79	15	Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation.
103	13	For top 100 mined pairs with the highest confidence scores (i.e., the score in Eq (3)), the accuracy is 98%.
123	17	Word/entity translations are the main knowledge that can be derived from our alignment results by extracting word pairs from the aligned cross-lingual node pairs.
129	17	Table 2 compares our approach to representative bilingual lexicon extraction approaches.
