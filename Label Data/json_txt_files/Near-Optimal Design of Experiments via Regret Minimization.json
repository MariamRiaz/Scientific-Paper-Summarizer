{"sections": [{"heading": "1. Introduction", "text": "Experimental design is an important problem in statistics and machine learning research (Pukelsheim, 2006). Consider a linear regression model\ny = X\u03b20 +w, (1)\nwhere X \u2208 Rn\u00d7p is a pool of n design points, y is the response vector, \u03b20 is a p-dimensional unknown regression model and w is a vector of i.i.d. noise variables satisfying Ewi = 0 and Ew2i < \u221e. The experimental design problem is to select a small subset of rows (i.e., design points) XS from the design pool X so that the statistical power of estimating \u03b20 is maximized from noisy response yS on the selected designs XS .\nAs an example, consider a material synthesis application where p is the number of variables (e.g., temperature, pressure, duration) that are hypothesized to affect the quality of the synthesized material and n is the total number of combinations of different parameters of experimental conditions. As experiments are expensive and time-consuming,\n*Author names listed in alphabetic order. 1Microsoft Research, Redmond, USA 2Princeton University, Princeton, USA 3Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Yining Wang <yiningwa@cs.cmu.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\none wishes to select k n experimental settings from X that are the most statistically efficient for establishing a model that connects experimental parameters with synthesized material quality, y. The experimental design problem is also related to many machine learning tasks, such as linear bandits (Deshpande & Montanari, 2012; Huang et al., 2016), diversity sampling (Kulesza & Taskar, 2012) and active learning (Ma et al., 2013; Chaudhuri et al., 2015; Hazan & Karnin, 2015; Balcan & Long, 2013; Wang & Singh, 2016).\nSince statistical efficiency can be measured in various ways, there exist a number of optimality criteria to guide the selection of experiments. We review some optimality criteria in Sec. 2 and interested readers are referred to Sec. 6 of (Pukelsheim, 2006) for a comprehensive review.\nTypically, an optimality criterion is a function f : S+p \u2192 R that maps from the p-dimensional positive definite cone to a real number. The experimental design problem can then be formulated as a combinatorial optimization problem:\nS\u2217(k) = arg min S\u2208S(n,k) f(X>SXS), (2)\nwhere S is either a set or a multi-set of size k, and XS \u2208 Rk\u00d7p is formed by stacking the rows of X that are in S. The constraint set S1/2(n, k) is defined as follows: 1. With replacement: S1(n, k) = {S multi-set : S \u2286\n[n], |S| \u2264 k}. Under this setting, XS may contain duplicate rows of the design pool X;\n2. Without replacement: S2(n, k) = {S standard set : S \u2286 [n], |S| \u2264 k}. Under this setting, XS only contains distinct rows of the design pool X.\nThe \u201cwith replacement\u201d setting is classical in statistics literature, where the multiple measurements in y with respect to the same design point lead to different values with statistically independent noise. The \u201cwithout replacement\u201d setting, on the other hand, is more relevant in machine learning applications, because labels are not likely to change if the same data point (e.g., the same image) is considered twice. Finally, it is worth pointing out that the \u201cwith replacement\u201d setting is easier, because it can be reduced (in polynomial time) to the \u201cwithout replacement\u201d setting by replicating each row of X for k times.\nFor many popular choices of f , the exact optimization\nproblem in Eq. (2) is NP-hard (C\u0327ivril & Magdon-Ismail, 2009; C\u030cerny\u0300 & Hlad\u0131\u0301k, 2012). In this paper, we propose a computationally tractable algorithm that approximately computes Eq. (2) for a wide range of optimality criteria, and under very weak conditions on n, k and p.\nBelow is our main theorem:\nTheorem 1.1. Suppose b \u2208 {1, 2}, n > k > p and let f : S+p \u2192 R be a regular optimality criterion (cf. Definition 2.1). There exists a polynomial-time algorithm that outputs S\u0302 \u2208 Sb(n, k) for any input matrix X \u2208 Rn\u00d7p with full column rank, and S\u0302 satisfies the following:\n1. For b = 1 (with replacement), there exists an absolute constant C0 \u2264 32 such that, for any \u03b5 \u2208 (0, 1), if k \u2265 C0p/\u03b5 2 then\nf(X> S\u0302 XS\u0302) \u2264 (1 + \u03b5) \u00b7 min S\u2208S1(n,k) f(X>SXS) . (3)\n2. For b = 2 (without replacement) and any \u03be > 2, there exists constant C1(\u03be) > 0 depending only on \u03be such that, if k \u2265 \u03bep then\nf(X> S\u0302 XS\u0302) \u2264 C1(\u03be) \u00b7 min S\u2208S2(n,k) f(X>SXS) . (4)\nMoreover, for \u03be \u2265 4 we have C1(\u03be) \u2264 32. 3. For b = 2 (without replacement) and any \u03b5 \u2208 (0, 1/2),\nif k, r satisfy k \u2265 4(1 + 7\u03b5)r and r \u2265 p/\u03b52, then\nf(X> S\u0302 XS\u0302) \u2264 (1 + \u03b5) \u00b7 min S\u2208S2(n,r) f(X>SXS). (5)\nWe interpret the significance of Theorem 1.1 as follows.\n\u2022 Under a very mild condition of k > 2p, our polynomialtime algorithm finds a set S\u0302 \u2282 [n] of size k, with objective value f(X>\nS\u0302 XS\u0302) being at most O(1) a constant\ntimes the optimum. See Eq. (4).\n\u2022 If replacement (b = 1) or over-sampling (k > r) is allowed, the approximation ratio can be tightened to 1+ \u03b5 for arbitrarily small \u03b5 > 0. See Eq. (3) and (5).\n\u2022 In all of the three cases, we only require k to grow linearly in p. Recall that k \u2265 p is necessary to ensure the singularity of X>\nS\u0302 XS\u0302 . In contrast, no polynomial-\ntime algorithm has achieved O(1) approximation in the regime k = O(p) for non-submodular optimality criteria (e.g., A- and V-optimality) under the without replacement setting.\n\u2022 Our algorithm works for any regular optimality criterion. To the best of our knowledge, no known polynomial-time algorithm can achieve a (1 + \u03b5) approximation for the D- and T-optimality criteria, or even an O(1) approximation for the E- and G-optimality criteria. See Table 1 for a comparison.\nThe key idea behind our proof of Theorem 1.1 is a regret minimization characterization of the least eigenvalue of positive semidefinite (PSD) matrices. Similar ideas were developed in (Allen-Zhu et al., 2015; Silva et al., 2016) to construct efficient algorithms for linear-sized graph sparsifiers. In this paper we adopt the regret minimization framework and present novel potential function analysis for the specific application of experimental design."}, {"heading": "1.1. Notations", "text": "S+p is the positive definite cone of p \u00d7 p matrices: a p \u00d7 p symmetric matrix A belongs to S+p if and only if v>Av > 0 for all v \u2208 Rp\\{0}. For symmetric matrices A and B, we write A B if v>(A \u2212 B)v \u2265 0 for all v \u2208 Rp. The inner product \u3008A,B\u3009 is defined as \u3008A,B\u3009 = tr(B>A) = \u2211pi,j=1 AijBij . We use \u2016A\u20162 = supv\u2208Rp\\{0} \u2016Av\u20162/\u2016v\u20162 to denote the spectral norm, and \u2016A\u2016F = \u221a\u2211p i,j=1 A 2 ij = \u221a \u3008A,A\u3009 to denote\nthe Frobenius norm of A. For A 0, we write B = A1/2 as the unique B 0 that satisfies B2 = A. For a design pool X \u2208 Rn\u00d7p, we use xi \u2208 Rp to denote the i-th row of X. We use \u03c3min(A) for the least (smallest) singular value of a PSD matrix A."}, {"heading": "1.2. Related work", "text": "Experimental design is an old topic in statistics research (Pukelsheim, 2006; Fedorov, 1972). Computationally efficient experimental design algorithms (with provable guarantees) are, however, a less studied field. In the case of submodular optimality criteria (e.g., D- and T-optimality), the classical pipage rounding method (Ageev & Sviridenko, 2004; Horel et al., 2014; Ravi et al., 2016) combined with semi-definite programming results in computationally efficient algorithms that enjoy a constant approximation ratio. Bouhtou et al. (2010) improves the approximation ratio when k is very close to n. Deshpande & Rademacher (2010); Li et al. (2017) considered polynomial-time algorithms for sampling from a D-optimality criterion. These algorithms are not applicable to non-submodular criteria, such as A-, V-, E- or G-optimality.\nFor the particular A-optimality criterion, (Avron & Boutsidis, 2013) proposed a greedy algorithm with an approximation ratio of O(n/k) with respect to f(X>X). It was shown that in the worst case min|S|\u2264k f(X>SXS) \u2248 O(n/k) \u00b7 f(X>X) and hence the bound is tight. However, for general design pool min|S|\u2264k f(X>SXS) could be far smaller than O(n/k) \u00b7 f(X>X), making the theoretical results powerless in such scenarios. Wang et al. (2016) considered a variant of the greedy method and showed an approximation ratio quadratic in design dimension p and independent of pool size n.\nWang et al. (2016) derived algorithms based on effective resistance sampling (Spielman & Srivastava, 2011) that attain (1 + \u03b5) approximation ratio if k = \u2126(p log p/\u03b52) and repetitions of design points are allowed. The algorithm fundamentally relies on the capability of \u201cre-weighting\u201d (repeating) design points and cannot be adapted to the more general \u201cwithout replacement\u201d setting. Naive sampling based methods were considered in (Wang et al., 2016; Chaudhuri et al., 2015; Dhillon et al., 2013), which also achieve (1+\u03b5) approximation but requires the subset size k to be much larger than the condition number of X.\nA related however different topic is low-rank matrix column subset selection and CUR approximation, which seeks column subset C and row subset R such that \u2016X \u2212 CC\u2020X\u2016F and/or \u2016X \u2212 CUR\u2016F are minimized (Drineas et al., 2008; Boutsidis & Woodruff, 2014; Wang & Singh, 2015b; Drineas & Mahoney, 2005; Wang & Zhang, 2013; Wang & Singh, 2015a). These problems are unsupervised in nature and do not in general correspond to statistical\nproperties under supervised regression settings. Pilanci & Wainwright (2016); Raskutti & Mahoney (2014); Woodruff (2014) considered fast methods for solving ordinary least squares (OLS) problems. They are computationally oriented and typically require knowledge of the full response vector y, which is different from the experimental design problem."}, {"heading": "2. Regular criteria and continuous relaxation", "text": "We start with the definition of regular optimality criteria:\nDefinition 2.1 (Regular criteria). An optimality criterion f : S+p \u2192 R is regular if it satisfies the followig properties: 1. Convexity: 1 f(\u03bbA + (1 \u2212 \u03bb)B) \u2264 \u03bbf(A) + (1 \u2212 \u03bb)f(B) for all \u03bb \u2208 [0, 1] and A,B \u2208 S+p ;\n2. Monotonicity: If A B then f(A) \u2265 f(B); 3. Reciprocal multiplicity: f(tA) = t\u22121f(A) for all t >\n0 and A \u2208 S+p .\nAlmost all optimality criteria used in the experimental design literature are regular. Below we list a few popular examples; their statistical implications can be found in (Pukelsheim, 2006):\n- A-optimality (Average): fA(\u03a3) = 1p tr(\u03a3 \u22121);\n- D-optimality (Determinant): fD(\u03a3) = (det |\u03a3|)\u2212 1 p ;\n- T-optimality (Trace): fT (\u03a3) = p/tr(\u03a3);\n- E-optimality (Eigenvalue): fE(\u03a3) = \u2016\u03a3\u22121\u20162; - V-optimality (Variance): fV (\u03a3) = 1n tr(X\u03a3 \u22121X>); - G-optimality: fG(\u03a3) = max diag(X\u03a3\u22121X>).\nThe (A-, D-, T-, E-) criteria concern estimates of regression coefficients and the (V-, G-) criteria are about insample predictions. All criteria listed above are regular. Note that for D-optimality the proxy function gD(\u03a3) = \u2212 log det(\u03a3) is considered to satisfy the convexity property. In addition, by the standard arithmetic inequality we have that fT \u2264 fD \u2264 fA \u2264 fE and that fV \u2264 fG. Although exact optimization of the combinatorial problem Eq. (2) is intractable, it is nevertheless easy to solve a continuous relaxation of Eq. (2) given the convexity property in Definition 2.1. We consider the following continuous optimization problem:\n\u03c0\u2217(b) = arg min \u03c0=(\u03c01,\u00b7\u00b7\u00b7 ,\u03c0n) f\n( n\u2211\ni=1\n\u03c0ixix > i\n) , (6)\ns.t. \u03c0 \u2265 0, \u2016\u03c0\u20161 \u2264 r, I[b = 2] \u00b7 \u2016\u03c0\u2016\u221e \u2264 1. 1This property could be relaxed to allow a proxy function g :\nS+p \u2192 R being convex, where g(A) \u2264 g(B)\u21d4 f(A) \u2264 f(B).\nThe \u2016\u03c0\u20161 \u2264 r constraint makes sure only r rows of X are \u201cselected\u201d, where r \u2264 k is a parameter that controls the degree of oversampling. The 0 \u2264 \u03c0i \u2264 1 constraint enforces that each row of X is \u201cselected\u201d at most once and is only applicable to the without replacement setting (b = 2). Eq. (6) is a relaxation of the original combinatorial problem Eq. (2), which we formalize below: Fact 2.1. For b \u2208 {1, 2} we have f(\u2211ni=1 \u03c0\u2217i (b)xix>i ) \u2264 minS\u2208Sb(n,r) f(X > SXS)\nIn addition, because of the monotonicity property of f the sum constraint must bind: Fact 2.2. For b \u2208 {1, 2} it holds that\u2211ni=1 \u03c0\u2217i (b) = r.\nProofs of Facts 2.1 and 2.2 are straightforward and are placed in the supplementary material.\nBoth the objective function and the constraint set in Eq. (6) are convex, and hence it can be efficiently solved to global optimality by conventional convex optimization algorithms. In particular, for differentiable f we suggest the following projected gradient descent (PGD) procedure:\n\u03c0(t+1) = PC ( \u03c0(t) \u2212 \u03b3t\u2207f(\u03c0(t)) ) , (7)\nwhere PC(x) = arg miny\u2208C \u2016x \u2212 y\u20162 is the projection operator onto the feasible set C = {\u03c0 \u2208 Rp : \u03c0 \u2265 0, \u2016\u03c0\u20161 \u2264 r, I[b = 2] \u00b7 \u2016\u03c0\u2016\u221e \u2264 1} and {\u03b3t}t\u22651 > 0 is a sequence of step sizes typically chosen by backtracking line search. When f is not differentiable everywhere, projected subgradient descent could be used with either constant or diminishing step sizes. We defer detailed gradient computations to the supplementary material. It was shown in (Wang et al., 2016; Su et al., 2012) that the projection operator PC(x) could be efficiently computed up to precision \u03b4 in O(n log(\u2016x\u2016\u221e/\u03b4)) operations."}, {"heading": "3. Sparsification via regret minimization", "text": "The optimal solution \u03c0\u2217 of Eq. (6) does not naturally lead to a valid approximation of the combinatorial problem in Eq. (2), because the number of non-zero components in \u03c0\u2217 may far exceed k. The primary focus of this section is to design efficient algorithms that sparsify the optimal solution \u03c0\u2217 into s \u2208 [k]n (with replacement) or s \u2208 {0, 1}n (without replacement), while at the same time bounding the increase in the objective.\nDue to the monotonicity and reciprocal multiplicity properties of f , it suffices to find a sparsifier s that satisfies\n( n\u2211\ni=1\nsixix > i\n) \u03c4 \u00b7 ( n\u2211\ni=1\n\u03c0\u2217i xix > i\n) (8)\nfor some constant \u03c4 \u2208 (0, 1). By Definition 2.1, Eq. (8) immediately implies f( \u2211n i=1 sixix > i ) \u2264\n\u03c4\u22121f( \u2211n i=1 \u03c0 \u2217 i xix > i ). The key idea behind our algorithm is a regret-minimization interpretation of the least eigenvalue of a positive definite matrix, which arises from recent progress in the spectral graph sparsification literature (Silva et al., 2016; Allen-Zhu et al., 2015).\nIn the rest of this section, we adopt the notation that \u03a0 = diag(\u03c0\u2217) and S = diag(s), both being n\u00d7n non-negative diagonal matrices. We also use I to denote the identity matrix, whose dimension should be clear from the context."}, {"heading": "3.1. The whitening trick", "text": "Consider the linear transform xi 7\u2192 (X\u03a0X>)\u22121/2xi =: x\u0303i. It is easy to verify that \u2211n i=1 \u03c0 \u2217 i x\u0303ix\u0303 > i = I. Such a transform is usually referred to as whitening, because the sample covariance of the transformed data is the identity matrix. Define W = \u2211n i=1 six\u0303ix\u0303 > i . We then have the following:\nProposition 3.1. For \u03c4 > 0, W \u03c4I if and only if ( \u2211n i=1 sixix > i ) \u03c4( \u2211n i=1 \u03c0 \u2217 i xix > i ).\nProof. The proposition holds because W \u03c4I if and only if (X\u03a0X>)1/2W(X\u03a0X>)1/2 \u03c4X\u03a0X>, and that (X\u03a0X>)1/2W(X\u03a0X>)1/2 = XSX>.\nProposition 3.1 shows that, without loss of generality, we may assume \u2211n i=1 \u03c0 \u2217 i xix > i = X\u03a0X\n> = I. The question of proving W = XSX> \u03c4I is then reduced to lower bounding the smallest eigenvalue of W.\nRecall that W can be written as a sum of rank-1 PSD matrices W = \u2211k t=1 Ft, where Ft = xix > i for some i \u2208 [n]. In the next section we give a novel characterization of the least eigenvalue of W from a regret minimization perspective. The problem of lower bounding the least eigenvalue of W can then be reduced to bounding the regret of a particular Follow-The-Regularized-Leader (FTRL) algorithm, which is a much easier task as FTRL admits closed-form solutions."}, {"heading": "3.2. Smallest eigenvalue as regret minimization", "text": "We first review the concept of regret minimization in a classical linear bandit setting. Let \u2206p = {A \u2208 Rp\u00d7p : A 0, tr(A) = 1} be an action space that consists of positive semi-definite matrices of dimension p and unit trace norm. Consider the linear bandit problem, which operates in k iterations. At iteration t, the player chooses an action At \u2208 \u2206p; afterwards, a \u201creference\u201d action Ft 0 is observed and the loss \u3008Ft,At\u3009 is incurred. The objective of the player is to minimize his/her regret:\nR({At}kt=1) := k\u2211\nt=1\n\u3008Ft,At\u3009 \u2212 inf U\u2208\u2206p\nk\u2211\nt=1\n\u3008Ft,U\u3009,\nwhich is the \u201cexcess loss\u201d of {At}kt=1 compared to the single optimal action U \u2208 \u2206p in hindsight, knowing all the reference actions {Ft}kt=1. A popular algorithm for regret minimization is Follow-The-Regularized-Leader (FTRL), also known to be equivalent to Mirror Descent (MD) (McMahan, 2011), which solves for\nAt = arg min A\u2208\u2206p\n{ w(A) + \u03b1 \u00b7 t\u22121\u2211\n`=1\n\u3008F`,A\u3009 } . (9)\nHere w(A) is a regularization term and \u03b1 > 0 is a parameter that balances model fitting and regularization. For the proof of our purpose we adopt the `1/2-regularizerw(A) = \u22122tr(A1/2) introduced in (Allen-Zhu et al., 2015), which leads to the closed-form solution\nAt = ( ctI + \u03b1 t\u22121\u2211\n`=1\nF`\n)\u22122 , (10)\nwhere ct \u2208 R is the unique constant that ensures At \u2208 \u2206p. The following lemma from (Allen-Zhu et al., 2015) bounds the regret of FTRL using the particular `1/2-regularizer:\nLemma 3.1 (Theorem 3.2 of (Allen-Zhu et al., 2015), specialized to `1/2-regularization). Suppose \u03b1 > 0, rank(Ft) = 1 and let {At}kt=1 be FTRL solutions defined in Eq. (10). If \u03b1\u3008Ft,A1/2t \u3009 > \u22121 for all t, then\nR({At}kt=1) := k\u2211\nt=1\n\u3008Ft,At\u3009 \u2212 inf U\u2208\u2206p\nk\u2211\nt=1\n\u3008Ft,U\u3009\n\u2264 \u03b1 k\u2211\nt=1 \u3008Ft,At\u3009\u3008Ft,A1/2t \u3009 1 + \u03b1\u3008Ft,A1/2t \u3009\n+ 2 \u221a p\n\u03b1 .\nNow consider each Ft = xitx > it\nto be the outer product of a design point selected from the design pool X. One remarkable consequence of Lemma 3.1 is that, in order to lower bound the smallest eigenvalue of \u2211k t=1 Ft, which by defi-\nnition is infU\u2208\u2206p\u3008 \u2211k t=1 Ft,U\u3009, it suffices to lower bound\u2211k\nt=1 \u3008Ft,At\u3009. Because At admits closed-form expression in Eq. (10), choosing a sequence of {Ft}kt=1 with large\u2211k t=1 \u3008Ft,At\u3009 becomes a much more manageable analytical task, which we shall formalize in the next section."}, {"heading": "3.3. Proof of Theorem 1.1", "text": "Re-organizing terms in Lemma 3.1 we obtain\ninf U\u2208\u2206p\nk\u2211\nt=1\n\u3008Ft,U\u3009 \u2265 k\u2211\nt=1 \u3008Ft,At\u3009 1 + \u03b1\u3008Ft,A1/2t \u3009\n\u2212 2 \u221a p\n\u03b1 . (11)\nThe k near-optimal design points are selected in a sequential manner. Let \u039bt \u2208 Sb(n, t) be the set of selected design points at or prior to iteration t (\u039b0 = \u2205), and define\nFt = xitx > it\n, where it is the design point selected at iteration t. Define also \u039bt = \u2211t `=1 F` = \u2211 i\u2208\u039bt xix > i .\nWe first consider the with replacement setting b = 1. Lemma 3.2. Suppose \u2211n i=1 \u03c0 \u2217 i xix > i = I where \u03c0 \u2217 i \u2265 0\nand \u2211n i=1 \u03c0 \u2217 i = r. Then for 1 \u2264 t \u2264 k we have that maxi\u2208[n] \u3008xix>i ,At\u3009\n1+\u03b1\u3008xix>i ,A 1/2 t \u3009 \u2265 1r+\u03b1\u221ap .\nProof. Recall that tr(At) = 1 and \u2211n i=1 \u03c0 \u2217 i xix > i =\nI. Subsequently, \u2211n i=1 \u03c0 \u2217 i \u3008xix>i ,At\u3009 = 1. On the other hand, we have that \u2211n i=1 \u03c0 \u2217 i (1 + \u03b1\u3008xix>i ,A 1/2 t \u3009) = \u2211n i=1 \u03c0 \u2217 i + \u03b1 \u00b7 tr(A 1/2 t ) (a) \u2264 r + \u03b1 \u00b7 tr(A1/2t ) (b)\n\u2264 r + \u03b1 \u221a p. Here (a) is due to the optimization constraint that \u2016\u03c0\u2217\u20161 \u2264 r, and (b) is because tr(A1/2t ) = \u2016\u03c3(A1/2t )\u20161 \u2264\u221a p\u2016\u03c3(A1/2t )\u20162 = \u221a p \u221a \u2016\u03c3(At)\u20161 = \u221ap \u221a tr(At) =\u221a\np, where \u03c3(\u00b7) is the vector of all eigenvalues of a PSD matrix. Combining both inequalities we have that maxi\u2208[n]\n\u3008xix>i ,At\u3009 1+\u03b1\u3008xix>i ,A 1/2 t \u3009\n\u2265 \u2211n i=1 \u03c0 \u2217 i \u3008xix>i ,At\u3009\u2211n\ni=1 \u03c0 \u2217 i (1+\u03b1\u3008xix>i ,A 1/2 t \u3009)\n,\nwhere the right-hand side is lower bounded by 1/(r + \u03b1 \u221a p).\nLet it = arg maxi\u2208[n] \u3008xix>i ,At\u3009\n1+\u03b1\u3008xix>i ,A 1/2 t \u3009\nbe the design point\nselected at iteration t. Combining Eq. (11) and Lemma 3.2,\n\u039bk = \u2211\ni\u2208\u039bk xix\n> i\n( k r + \u03b1 \u221a p \u2212 2 \u221a p \u03b1 ) I. (12)\nTo prove Eq. (3), set \u03b1 = 8 \u221a p/\u03b5. Because k = r \u2265\nC0p/\u03b5 2, we have that kr+\u03b1\u221ap \u2212\n2 \u221a p\n\u03b1 \u2265 11+8\u03b5/C0 \u2212 \u03b5 4 . With\nC0 = 32 the right-hand side is lower bounded by 1\u2212 \u03b5/2. Eq. (3) is thus proved because (1\u2212 \u03b5/2)\u22121 \u2264 1 + \u03b5. We next consider the without replacement setting b = 2.\nLemma 3.3. Fix arbitrary \u03b2 \u2208 (0, 1] and suppose\u2211n i=1 \u03c0 \u2217 i xix > i = I where \u03c0 \u2217 i \u2208 [0, \u03b2] and \u2211n i=1 \u03c0 \u2217 i = r. Then for all 1 \u2264 t \u2264 k,\nmax i/\u2208\u039bt\u22121 \u3008xix>i ,At\u3009 1 + \u03b1\u3008xix>i ,A 1/2 t \u3009\n\u2265 1\u2212 \u03b2\u03c3min(\u039bt\u22121)\u2212 \u221a p/\u03b1\nr + \u03b1 \u221a p\n.\nProof. On one hand, we have \u2211 i/\u2208\u039bt\u22121 \u03c0 \u2217 i \u3008xix>i ,At\u3009 (a)\n\u2265 \u3008At, I\u2212 \u03b2\u039bt\u22121\u3009 (b) = 1 \u2212 tr [ (\u03b1\u039bt\u22121 + ctI) \u22122 \u03b2\u039bt\u22121 ] =\n1+ \u03b2ct\u03b1 \u2212 \u03b2 \u03b1 tr [ (\u03b1\u039bt\u22121 + ctI) \u22121 ] = 1+ \u03b2ct\u03b1 \u2212 tr(A 1/2 t ) \u03b1 (c)\n\u2265 1 + \u03b2ct\u03b1 \u2212 \u221a p \u03b1 . Here (a) is due to \u2211n i=1 \u03c0 \u2217 i xix > i = I and \u03c0\u2217i \u2208 [0, \u03b2]; (b) is due to \u3008At, I\u3009 = tr(At) = 1 and (c) is proved in the proof of Lemma 3.2. Because \u03b1\u039bt\u22121 +ctI 0, we conclude that ct \u2265 \u2212\u03b1\u03c3min(\u039bt\u22121) and therefore\u2211 i/\u2208\u039bt\u22121 \u03c0 \u2217 i \u3008xix>i ,At\u3009 \u2265 1 \u2212 \u03b2\u03c3min(\u039bt\u22121) \u2212 \u221a p/\u03b1. On the other hand, \u2211 i/\u2208\u039bt\u22121 \u03c0 \u2217 i (1 + \u03b1\u3008xix>i ,A 1/2 t \u3009) \u2264\nr + \u03b1 \u221a p by the same argument as in the proof of Lemma 3.2. Subsequently, maxi/\u2208\u039bt\u22121 \u3008xix>i ,At\u3009\n1+\u03b1\u3008xix>i ,A 1/2 t \u3009 \u2265\n\u2211 i/\u2208\u039bt\u22121 \u03c0 \u2217 i \u3008xix>i ,At\u3009\n\u2211 i/\u2208\u039bt\u22121 \u03c0 \u2217 i (1+\u03b1\u3008xix>i ,A 1/2 t \u3009)\n\u2265 1\u2212\u03b2\u03c3min(\u039bt\u22121)\u2212 \u221a p/\u03b1\nr+\u03b1 \u221a p .\nLet it = arg maxi/\u2208\u039bt\u22121 \u3008xix>i ,At\u3009\n1+\u03b1\u3008xix>i ,A 1/2 t \u3009\n. Combining\nEq. (11) and Lemma 3.3 with \u03b2 = 1, we have that\n\u039bk ( k\u2211\nt=1\n1\u2212 \u03bat \u2212\u221ap/\u03b1 r + \u03b1 \u221a p \u2212 2 \u221a p \u03b1 ) I, (13)\nwhere \u03bat := \u03c3min(\u039bt). We are now ready to prove Eqs. (4,5) in Theorem 1.1.\nProof of Eq. (4). Note that\n\u039bk sup u>0 min\n{ u, 1\u2212 u\u2212\u221ap/\u03b1 r + \u03b1 \u221a p \u00b7 k \u2212 2 \u221a p \u03b1 } I. (14)\nEq. (14) can be proved by a case analysis: if u \u2264 \u03bat for some 1 \u2264 t \u2264 k then \u03c3min(\u039bk) \u2265 \u03c3min(\u039bt\u22121) \u2265 u; otherwise 1\u2212\u03bat\u2212\u221ap/\u03b1 \u2265 1\u2212u\u2212\u221ap/\u03b1 for all 1 \u2264 t \u2264 k. Suppose k = r \u2265 \u03bep for some \u03be > 2. and let \u03b1 = \u03bd\u221ap, u = (1\u22122/\u03be)\u03bd\u22123\u03bd(2+\u03bd/\u03be) , where \u03bd > 1 is some parameter to be specified later. Eq. (14) then yields \u039bk (1\u22122/\u03be)\u03bd\u22123\u03bd(2+\u03bd/\u03be) I. Because \u03be > 2, it is possible to select \u03bd > 0 such that C1(\u03be)\n\u22121 = (1\u22122/\u03be)\u03bd\u22123\u03bd(2+\u03bd/\u03be) > 0. Finally, for \u03be \u2265 4 and \u03bd = 8 we have C1(\u03be)\u22121 \u2265 1/32. Eq. (4) is thus proved.\nProof of Eq. (5). Let \u03b2 \u2208 (0, 1) be a parameter to be specified later, and define \u03a3\u2217\u03b2 := \u2211 \u03c0\u2217i\u2265\u03b2 \u03c0 \u2217 i xix > i and \u03a3\u0304 \u2217 \u03b2 :=\nI \u2212 \u03a3\u2217\u03b2 = \u2211 \u03c0\u2217i<\u03b2 \u03c0\u2217i xix > i . Let S\u0302 be constructed such that it includes all points in S\u2217\u03b2 := {i : \u03c0\u2217i \u2265 \u03b2}, plus the resulting set by running Algorithm 1 on the remaining weights smaller than \u03b2, with subset size k\u2212k\u2032 = k\u2212|S\u2217\u03b2 |. Define \u03b1 = 2 \u221a p/\u03b5, r\u2032 :=\n\u2211 \u03c0\u2217i\u2265\u03b2 \u03c0 \u2217 i , k\u0303 := k \u2212 k\u2032 and\nr\u0303 := r\u2212 r\u2032+\u03b1\u221ap = r\u2212 r\u2032+ 2p/\u03b5. Let \u039b = \u2211i\u2208S\u0302 xix>i be the sample covariance of the selected subset. By the definition of S\u0302 and Lemma 3.3, together with the whitening trick (Sec. 3.1) on \u03a3\u0304\u2217\u03b2 , we have\n\u039b \u03a3\u2217\u03b2 + sup u>0\nmin { u, (1\u2212 \u03b2u\u2212 \u03b5/2)k\u0303/r\u0303 \u2212 \u03b5 } \u03a3\u0304\u2217\u03b2\nsup u>0\nmin { u, (1\u2212 \u03b2u\u2212 \u03b5/2)k\u0303/r\u0303 \u2212 \u03b5 } I,\nwhere the second line holds because \u03a3\u2217\u03b2 + \u03a3\u0304 \u2217 \u03b2 = I and u \u2264 1. Now set \u03b2 = 0.5 and note that k\u2032 \u2264 r\u2032/\u03b2 \u2264 2r\u2032 by definition of S\u2217\u03b2 . Subsequently, r \u2265 p/\u03b52 and k \u2265 4(1 + 7\u03b5)r for \u03b5 \u2208 (0, 1/2) implies that k\u0303r\u0303 \u2265 1+2\u03b5(1\u2212\u03b5/2)(1\u2212\u03b2) , which yields u \u2265 1 \u2212 \u03b5/2 and hence f(X>\nS\u0302 XS\u0302) \u2264 (1 +\n\u03b5)f(X>S\u2217XS\u2217). Eq. (5) is thus proved.\nAlgorithm 1 Near-optimal experimental design 1: Input: design pool X \u2208 Rn\u00d7p, budget parameters k \u2265 r \u2265 p, algorithmic parameter \u03b1 > 0.\n2: Solve the convex optimization problem Eq. (6) with parameter s; Let \u03c0\u2217 be the optimal solution; 3: Whitening: X\u2190 X(X>diag(\u03c0\u2217)X)\u22121/2; 4: Initialization: \u039b0 = \u2205; 5: for t = 1 to k do 6: ct \u2190 FINDCONSTANT( \u2211 i\u2208\u039bt\u22121 xix > i , \u03b1);\n7: At \u2190 (ctI + \u2211 i\u2208\u039bt\u22121 xix > i ) \u22122; 8: If b = 1 then \u0393t = [n]; else \u0393t = [n]\\\u039bt\u22121; 9: it \u2190 arg maxi\u2208\u0393t\n\u3008xix>i ,At\u3009 1+\u03b1\u3008xix>i ,A 1/2 t \u3009 ;\n10: \u039bt = \u039bt\u22121 \u222a {it}; 11: end for 12: Output: S\u0302 = \u039bk.\nAlgorithm 2 FINDCONSTANT(Z, \u03b1) 1: Initialization: c` = \u2212\u03c3min(Z), cu = \u221ap; = 10\u22129; 2: while |c` \u2212 cu| > do 3: c\u0304\u2190 (c` + cu)/2; 4: If tr[(c\u0304I + Z)\u22122] > 1 then c` \u2190 c\u0304; else cu \u2190 c\u0304; 5: end while 6: Output: c = (c` + cu)/2.\nOur proof of Theorem 1.1 is constructive and yields a computationally efficient iterative algorithm which finds subset S\u0302 \u2208 Sb(n, k) that satisfies the approximation results in Theorem 1.1. In Algorithm 1 we give a pseducode description of the algorithm, which makes use of a binary search routine (Algorithm 2) that finds the unique constant ct for which tr(At) = tr[(ctI + \u2211 i\u2208\u039bt\u22121 xix > i ) \u22122] = 1. Note that for Eq. (5) to be valid, it is necessary to run Algorithm 2 on the remaining set of \u03c0\u2217 after including all points xi with \u03c0\u2217i \u2265 1/2 in S\u0302."}, {"heading": "4. Extension to generalized linear models", "text": "The experimental algorithm presented in this paper could be easily extended beyond the linear regression model. For this purpose we consider the Generalized Linear Model (GLM), which assumes that\ny|x i.i.d.\u223c p(y|x>\u03b20), where p(\u00b7|\u00b7) is a known distribution and \u03b20 is an unknown p-dimensional regression model. Examples include the logistic regression model p(y = 1|x) = exp(x\n>\u03b20) 1+exp(x>\u03b20) , the\nPossion count model p(yi = y|x) = exp(yx >\u03b20\u2212e\u2212x >\u03b20 ) y! , and many others.\nLet S \u2208 Sb(n, k) be the set of selected design points from X. Under the classical statistics regime,\nthe maximum likelihood (ML) estimator \u03b2\u0302 ML\n= arg min\u03b2 \u2211 i\u2208S log p(yi|x>i \u03b2) is asymptotically efficient, and its asymptotic variance equals the Fisher\u2019s information\nI(XS ;\u03b20) := \u2211\ni\u2208S Ey|x>i \u03b20\n[ \u2212\u2202\n2 log p(y|xi;\u03b20) \u2202\u03b2\u2202\u03b2>\n]\n\u03b7i=x > i \u03b20=\n\u2211 i\u2208S Ey|\u03b7i\n[ \u2212\u2202\n2 log p(y|\u03b7i) \u2202\u03b72i\n] \u00b7 xix>i .\nHere the second equality is due to the sufficiency of x>i \u03b20 in a GLM. Note that for the linear regression model y = X\u03b20 + w, the ML estimator is the ordinary least squares (OLS) \u03b2\u0302 = (X>SXS)\n\u22121XSyS and its Fisher\u2019s information equals the sample covariance X>SXS . The experimental design problem can then be formalized as follows: 2\nmin S\u2208Sb(n,k) f(I(XS ;\u03b20)) = min S\u2208Sb(n,k) f\n(\u2211\ni\u2208S ziz > i\n) ;\n(15)\nzi = \u221a \u2212Ey|\u03b7i [ \u2212\u2202\n2 log p(yi|\u03b7i) \u2202\u03b72i\n] , \u03b7i = x > i \u03b20.\nSuppose \u03b2\u030c is a \u201cpilot\u201d estimate of \u03b20, obtained from a uniformly sampled design subset S1. A near-optimal design set S2 can then be constructed by minimizing Eq. (15) using \u03b7\u030ci = x>i \u03b2\u030c. Such an approach was adopted in sequential design and active learning for ML estimators (Chaudhuri et al., 2015; Khuri et al., 2006); however, with our algorithm the quality of S2 is greatly improved.\n2Under very mild conditions E[\u2212 \u22022 log p \u2202\u03b72 ] = E[( \u2202 log p \u2202\u03b7 )2] is non-negative (Van der Vaart, 2000)."}, {"heading": "5. Numerical results", "text": "We compare the proposed method with several baseline methods on both synthetic and real-world data sets. We only consider the harder \u201cwithout replacement\u201d setting, where each row of X can be selected at most once."}, {"heading": "5.1. Methods and their implementation", "text": "We compare our algorithm with three simple heuristic methods that apply to all optimality criteria:\n1. Uniform sampling: S\u0302 is sampled uniformly at random without replacement from the design pool X;\n2. Weighted sampling: first the optimal solution \u03c0\u2217 of Eq. (6) is computed with r = k; afterwards, S\u0302 is sampled without replacement according to the distribution specified by \u03c0\u2217/k. Recall that (Wang et al., 2016) proved that weighted sampling works when k is sufficiently large compared to p (cf. Table 1). 3\n3. Fedorov\u2019s exchange (Miller & Nguyen, 1994): the algorithm starts with a random subset S0 \u2208 Sb(n, k) and iteratively exchanges two coordinates i \u2208 S0, j /\u2208 S0 such that the objective is minimized after the exchange. The algorithm terminates if no such exchange can reduce the objective, or T iterations are reached.\nAll algorithms are implemented in MATLAB, except for the Fedorov\u2019s exchange algorithm, which is implemented in C due to efficiency concerns. We also apply the ShermanMorrison formula (A+\u03bbuu>)\u22121 = A\u22121 + \u03bbA \u22121uu>A\u22121\n1+\u03bbu>A\u22121u\nand the matrix determinant lemma det(A + \u03bbuu>) =\n3Fact 2.2 ensures that \u03c0\u2217/k is a valid probability distribution.\nTable 3. Results on the Minnesota wind speed dataset (n =\n2642, p = 15, k = 30). MSE is defined as\n\u221a\n1 n \u2016y \u2212V\u03b2\u0302\u201622.\nfV MSE fG MSE\nUNIFORM SAMPLING 94.1 1.10 3093 1.34 WEIGHTED SAMPLING 21.4 0.89 2451 1.13\nFEDOROV\u2019S EXCHANGE 10.0 0.86 29.2 0.78 (running time /secs) 15 - 1857 -\nALGORITHM 1 10.8 0.72 29.2 0.76 (running time /secs) < 1 - < 1 -\nFULL-SAMPLE OLS - 0.55 - 0.55\n(1 + \u03bbu>A\u22121u>) det(A) to accelerate computations of rank-1 updates of matrix inverse and determinant. For uniform sampling and weighted sampling, we report the median objective of 50 indpendent trials. We only report the objective for one trial of Fedorov\u2019s exchange method due to time constraints. The maximum number of iterations T for Fedorov\u2019s exchange is set at T = 100. We always set k = r in the optimization problem Eq. (6), and details of solving Eq. (6) are placed in the appendix. In Algorithm 1 we set \u03b1 = 10; our similuations suggest that the algorithm is not sensitive to \u03b1."}, {"heading": "5.2. Synthetic data", "text": "We synthesize a 1000\u00d7 50 design pool X as follows:\nX =\n[ XA 0500\u00d725\n0500\u00d725 XB\n] .\nXA is a 500 \u00d7 25 random Gaussian matrix, re-scaled so that the eigenvalues of X>AXA satisfy a quadratic decay: \u03c3j(X > AXA) \u221d j\u22122; XB is a 500 \u00d7 25 Gaussian matrix with i.i.d. standard Normal variables. Both XA and XB have comparable Frobenius norm.\nIn Table 2 we report results on all 6 optimality criteria (fA, fD, fT , fE , fV , fG) for k \u2208 {2p, 3p, 5p, 10p}. We also report the running time (measured in seconds) of Algorithm 1 and the Fedorov\u2019s exchange algorithm. The other two sampling based algorithms are very efficient and always terminate within one second. We observe that our algorithm has the best performance for fE and fG, while still achieving comparable results for the other optimality criteria. It is also robust when k is small compared to p, while sampling based methods occasionally produce designs that are not even full rank. Finally, Algorithm 1 is computationally efficient and terminates within seconds for all settings."}, {"heading": "5.3. The Minnesota wind speed dataset", "text": "The Minnesota wind dataset collects wind speed information across n = 2642 locations in Minnesota, USA for a\nperiod of 24 months (for the purpose of this experiment, we only use wind speed data for one month). The 2642 locations are connected with 3304 bi-directional roads, which form an n\u00d7 n sparse unweighted undirected graph G. Let L = diag(d)?G be the n\u00d7n Laplacian of G, where d is a vector of node degrees, and let V \u2208 Rn\u00d7p be an orthonormal eigenbasis corresponding to the smallest p eigenvalues of L. (Chen et al., 2015) shows that the relatively smooth wind speed signal y \u2208 Rn can be well approximated by using only p = 15 graph Laplacian basis.\nIn Table 3 we compare the mean-square error (MSE) for prediction on the full design pool V: MSE =\u221a\n1 n\u2016y \u2212V\u03b2\u0302\u201622. Because the objective is prediction based, we only consider the two prediction related criteria: fV (\u03a3) = tr(V\u03a3\u22121V>) and fG(\u03a3) = max diag(V\u03a3\u22121V>). The subset size k is set as k = 2p = 30, which is much smaller than n = 2642. We observe that Algorithm 1 consistently outperforms the other heuristic methods, and is so efficient that its running time is negligible. It is also interesting that by using k = 30 samples Algorithm 1 already achieves an MSE that is comparable to the OLS on the entire n = 2642 design pool."}, {"heading": "6. Concluding remarks and open questions", "text": "We proposed a computationally efficient algorithm that approximately computes optimal solutions for the experimental design problem, with near-optimal requirement on k (i.e., the number of experiments to choose). In particular, we obtained a constant approximation under the very weak condition k > 2p, and a (1 + \u03b5) approximation if replacement or over-sampling is allowed. Our algorithm works for all regular optimality criteria.\nAn important open question is to achieve (1 + \u03b5) relative approximation ratio under the \u201cproper sampling\u201d regime k = r, or the \u201cslight over-sampling\u201d regime k = (1 + \u03b4)r, for the without replacement model. It was shown in (Wang et al., 2016) that a simple greedy method achieves (1 + \u03b5) approximation ratio for A- and V-optimality provided that k = \u2126(p2/\u03b5). Whether such analysis can be extended to other optimality criteria and whether the p2 term can be further reduced to a near linear function of p remain open.\nAnother practical question is to develop fast-converging optimization methods for the continuous problem in Eq. (6), especially for criteria that are not differentiable such as the E- and G-optimality, where subgradient methods have very slow convergence rate.\nAcknowledgement This work is supported by NSF grants CAREER IIS-1252412 and CCF-1563918. We thank Adams Wei Yu for providing an efficient implementation of the projection step, and other useful discussions."}], "year": 2017, "references": [{"title": "Pipage rounding: A new method of constructing algorithms with proven performance guarantee", "authors": ["Ageev", "Alexander A", "Sviridenko", "Maxim I"], "venue": "Journal of Combinatorial Optimization,", "year": 2004}, {"title": "Spectral sparsification and regret minimization beyond matrix multiplicative updates", "authors": ["Allen-Zhu", "Zeyuan", "Liao", "Zhenyu", "Orecchia", "Lorenzo"], "venue": "In Proceedings of Annual Symposium on the Theory of Computing (STOC),", "year": 2015}, {"title": "Faster subset selection for matrices and applications", "authors": ["Avron", "Haim", "Boutsidis", "Christos"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2013}, {"title": "Active and passive learning of linear separators under log-concave distributions", "authors": ["Balcan", "Maria-Florina", "Long", "Philip M"], "venue": "In Proceedings of Annual Conference on Learning Theory (COLT),", "year": 2013}, {"title": "Submodularity and randomized rounding techniques for optimal experimental design", "authors": ["Bouhtou", "Mustapha", "Gaubert", "Stephane", "Sagnol", "Guillaume"], "venue": "Electronic Notes in Discrete Mathematics,", "year": 2010}, {"title": "Optimal CUR matrix decompositions", "authors": ["Boutsidis", "Christos", "Woodruff", "David P"], "venue": "In Proceedings of Annual Symposium on the Theory of Computing (STOC),", "year": 2014}, {"title": "Two complexity results on C-optimality in experimental design", "authors": ["\u010cern\u1ef3", "Michal", "Hlad\u0131\u0301k", "Milan"], "venue": "Computational Optimization and Applications,", "year": 2012}, {"title": "Convergence rates of active learning for maximum likelihood estimation", "authors": ["Chaudhuri", "Kamalika", "Kakade", "Sham", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Signal representations on graphs: Tools and applications", "authors": ["Chen", "Siheng", "Varma", "Rohan", "Singh", "Aarti", "Kova\u010devi\u0107", "Jelena"], "venue": "arXiv preprint arXiv:1512.05406,", "year": 2015}, {"title": "On selecting a maximum volume sub-matrix of a matrix and related problems", "authors": ["\u00c7ivril", "Ali", "Magdon-Ismail", "Malik"], "venue": "Theoretical Computer Science,", "year": 2009}, {"title": "Efficient volume sampling for row/column subset selection", "authors": ["Deshpande", "Amit", "Rademacher", "Luis"], "venue": "In Proceedings of Annual Conference on Foundations of Computer Science (FOCS),", "year": 2010}, {"title": "Linear bandits in high dimension and recommendation systems", "authors": ["Deshpande", "Yash", "Montanari", "Andrea"], "venue": "In Proceedings of Annual Allerton Conference on Communication, Control, and Computing (Allerton),", "year": 2012}, {"title": "New subsampling algorithms for fast least squares regression", "authors": ["Dhillon", "Paramveer", "Lu", "Yichao", "Foster", "Dean P", "Ungar", "Lyle"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "year": 2013}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "authors": ["Drineas", "Petros", "Mahoney", "Michael W"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Relative-error CUR matrix decompositions", "authors": ["Drineas", "Petros", "Mahoney", "Michael W", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2008}, {"title": "Theory of optimal experiments", "authors": ["Fedorov", "Valerii Vadimovich"], "year": 1972}, {"title": "Hard-margin active linear regression", "authors": ["Hazan", "Elad", "Karnin", "Zohar"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Budget feasible mechanisms for experimental design", "authors": ["Horel", "Thibaut", "Ioannidis", "Stratis", "S. Muthukrishnan"], "venue": "In Proceedings of Latin American Symposium on Theoretical Informatics (LATIN),", "year": 2014}, {"title": "Following the leader and fast rates in linear prediction: Curved constraint sets and other regularities", "authors": ["Huang", "Ruitong", "Lattimore", "Tor", "Gy\u00f6rgy", "Andr\u00e1s", "Szepesv\u00e1ri", "Csaba"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Design issues for generalized linear models: a review", "authors": ["Khuri", "Andre", "Mukherjee", "Bhramar", "Sinha", "Bikas", "Ghosh", "Malay"], "venue": "Statistical Science,", "year": 2006}, {"title": "Polynomial time algorithms for dual volume sampling", "authors": ["Li", "Chengtao", "Jegelka", "Stefanie", "Sra", "Suvrit"], "venue": "arXiv preprint arXiv:1703.02674,", "year": 2017}, {"title": "\u03c3optimality for active learning on Gaussian random fields", "authors": ["Ma", "Yifei", "Garnett", "Roman", "Schneider", "Jeff"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "year": 2013}, {"title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1 regularization", "authors": ["McMahan", "H Brendan"], "venue": "In Procedings of International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2011}, {"title": "A Fedorov exchange algorithm for d-optimal design", "authors": ["Miller", "Alan", "Nguyen", "Nam-Ky"], "venue": "Journal of the Royal Statistical Society, Series C (Applied Statistics),", "year": 1994}, {"title": "Iterative Hessian sketch: Fast and accurate solution approximation for constrained least-squares", "authors": ["Pilanci", "Mert", "Wainwright", "Martin J"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Optimal design of experiments", "authors": ["Pukelsheim", "Friedrich"], "year": 2006}, {"title": "A statistical perspective on randomized sketching for ordinary leastsquares", "authors": ["Raskutti", "Garvesh", "Mahoney", "Michael"], "venue": "arXiv preprint arXiv:1406.5986,", "year": 2014}, {"title": "Experimental design on a budget for sparse linear models and applications", "authors": ["Ravi", "Sathya N", "Ithapu", "Vamsi K", "Johnson", "Sterling C", "Singh", "Vikas"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Sparse sums of positive semidefinite matrices", "authors": ["Silva", "Marcel K", "Harvey", "Nicholas JA", "Sato", "Cristiane M"], "venue": "ACM Transactions on Algorithms,", "year": 2016}, {"title": "Graph sparsification by effective resistances", "authors": ["Spielman", "Daniel A", "Srivastava", "Nikhil"], "venue": "SIAM Journal on Computing,", "year": 1913}, {"title": "Efficient euclidean projections onto the intersection of norm balls", "authors": ["Su", "Hao", "Yu", "Adams Wei", "Li", "Fei-Fei"], "venue": "In Proceedings of International Conference on Machine Learning (ICML),", "year": 2012}, {"title": "Improving CUR matrix decomposition and the Nystr\u00f6m approximation via adaptive sampling", "authors": ["Wang", "Shusen", "Zhang", "Zhihua"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "An empirical comparison of sampling techniques for matrix column subset selection", "authors": ["Wang", "Yining", "Singh", "Aarti"], "venue": "In Proceedings of Annual Allerton Conference on Communication,", "year": 2015}, {"title": "Provably correct active sampling algorithms for matrix column subset selection with missing data", "authors": ["Wang", "Yining", "Singh", "Aarti"], "venue": "arXiv preprint arXiv:1505.04343,", "year": 2015}, {"title": "Noise-adaptive marginbased active learning and lower bounds under tsybakov noise condition", "authors": ["Wang", "Yining", "Singh", "Aarti"], "venue": "In Proceedings of AAAI Conference on Artificial Intelligence (AAAI),", "year": 2016}, {"title": "On computationally tractable selection of experiments in regression models", "authors": ["Wang", "Yining", "Yu", "Wei Adams", "Singh", "Aarti"], "venue": "arXiv preprints:", "year": 2016}], "id": "SP:75ffdce867d30585bff0111ae7c861433cb474f0", "authors": [{"name": "Zeyuan Allen-Zhu", "affiliations": []}, {"name": "Yuanzhi Li", "affiliations": []}, {"name": "Aarti Singh", "affiliations": []}, {"name": "Yining Wang", "affiliations": []}], "abstractText": "We consider computationally tractable methods for the experimental design problem, where k out of n design points of dimension p are selected so that certain optimality criteria are approximately satisfied. Our algorithm finds a (1 + \u03b5)approximate optimal design when k is a linear function of p; in contrast, existing results require k to be super-linear in p. Our algorithm also handles all popular optimality criteria, while existing ones only handle one or two such criteria. Numerical results on synthetic and real-world design problems verify the practical effectiveness of the proposed algorithm.", "title": "Near-Optimal Design of Experiments via Regret Minimization"}