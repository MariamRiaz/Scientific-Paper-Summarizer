0	11	Natural language inference (NLI) refers to the problem of determining entailment and contradiction relationships between a premise and a hypothesis.
1	40	NLI is a central problem in language understanding (Katz, 1972; Bos and Markert, 2005; van Benthem, 2008; MacCartney and Manning, 2009) and recently the large SNLI corpus of 570K sentence pairs was created for this task (Bowman et al., 2015).
2	30	We present a new model for NLI and leverage this corpus for comparison with prior work.
3	37	A large body of work based on neural networks for text similarity tasks including NLI has been published in recent years (Hu et al., 2014; Rocktäschel et al., 2016; Wang and Jiang, 2016; Yin et al., 2016, inter alia).
4	25	The dominating trend in these models is to build complex, deep text representation models, for example, with convolutional networks (LeCun et al., 1990, CNNs henceforth) or long short-term memory networks (Hochreiter and Schmidhuber, 1997, LSTMs henceforth) with the goal of deeper sentence comprehension.
5	12	While these approaches have yielded impressive results, they are often computationally very expensive, and result in models having millions of parameters (excluding embeddings).
6	15	Here, we take a different approach, arguing that for natural language inference it can often suffice to simply align bits of local text substructure and then aggregate this information.
8	26	The first sentence is complex in structure and it is challenging to construct a compact representation that expresses its entire meaning.
9	53	However, it is fairly easy to conclude that the second sentence follows from the first one, by simply aligning Bob with Bob and cannot sleep with awake and recognizing that these are synonyms.
10	19	Similarly, one can conclude that It is sunny outside contradicts the first sentence, by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible.
14	57	Given two sentences, where each word is repre- 2249 sented by an embedding vector, we first create a soft alignment matrix using neural attention (Bahdanau et al., 2015).
16	29	Finally, the results of these subproblems are merged to produce the final classification.
17	27	In addition, we optionally apply intra-sentence attention (Cheng et al., 2016) to endow the model with a richer encoding of substructures prior to the alignment step.
24	68	We assume that each ai, bj ∈ Rd is a word embedding vector of dimension d and that each sentence is prepended with a “NULL” token.
26	9	, y (n) C ) is an indicator vector encoding the label and C is the number of output classes.
27	36	At test H ( )+ +…+=ŷ in the park alice plays so m eo ne pl ay in g m us ic ou ts id e flute a solo G ( , ) G ( , ) park outside alice someone flute+ solo music … G ( , )= = = flute music F ( , ) Figure 1: Pictoral overview of the approach, showing the Attend (left), Compare (center) and Aggregate (right) steps.
48	14	We first aggregate over each set by summation: v1 = `a∑ i=1 v1,i , v2 = `b∑ j=1 v2,j .
49	9	(4) and feed the result through a final classifier H , that is a feed forward network followed by a linear layer: ŷ = H([v1,v2]) , (5) where ŷ ∈ RC represents the predicted (unnormalized) scores for each class and consequently the predicted class is given by ŷ = argmaxiŷi.
50	26	For training, we use multi-class cross-entropy loss with dropout regularization (Srivastava et al., 2014): L(θF , θG, θH) = 1 N N∑ n=1 C∑ c=1 y(n)c log exp(ŷc)∑C c′=1 exp(ŷc′) .
53	57	However, we can augment this input representation with intra-sentence attention to encode compositional relationships between words within each sentence, as proposed by Cheng et al. (2016).
57	12	These terms are bucketed such that all distances greater than 10 words share the same bias.
76	20	Data preprocessing: Following Bowman et al. (2015), we remove examples labeled “–” (no gold label) from the dataset, which leaves 549,367 pairs for training, 9,842 for development, and 9,824 for testing.
82	14	Each embedding vector was normalized to have `2 norm of 1 and projected down to 200 dimensions, a number determined via hyperparameter tuning.
90	22	All settings were run for 50 million steps (each step indicates one batch) but model parameters were saved frequently as training progressed and we chose the model that did best on the development set.
92	39	Our vanilla approach achieves state-of-theart results with almost an order of magnitude fewer parameters than the LSTMN of Cheng et al. (2016).
93	20	Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art.
94	39	Table 2 gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral, while most losses come from contradiction pairs.
100	46	our vanilla approach is incorrect but mLSTM and SPINN-PI are correct.
104	69	Finally, Examples G-I are cases that all methods get wrong.
109	13	Our results suggest that, at least for this task, pairwise comparisons are relatively more important than global sentence-level representations.
