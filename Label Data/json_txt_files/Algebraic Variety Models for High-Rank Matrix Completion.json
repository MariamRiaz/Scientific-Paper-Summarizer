{"sections": [{"heading": "1. Introduction", "text": "Work in the last decade on matrix completion has shown that it is possible to leverage linear structure in order to interpolate missing values in a low-rank matrix (Candes & Recht, 2012). The high-level idea of this work is that if the data defining the matrix belongs to a structure having fewer degrees of freedom than the entire dataset, that structure provides redundancy that can be leveraged to complete\n1Department of EECS, University of Michigan, Ann Arbor, Michigan, USA 2Department of ECE, University of Wisconsin, Madison, Wisconsin, USA. Correspondence to: Greg Ongie <gongie@umich.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nthe matrix. The assumption that the matrix is low-rank is equivalent to assuming the data lies on (or near) a lowdimensional linear subspace.\nIt is of great interest to generalize matrix completion to exploit low-complexity nonlinear structures in the data. Several avenues have been explored in the literature, from generic manifold learning (Lee et al., 2013), to unions of subspaces (Eriksson et al., 2012; Elhamifar & Vidal, 2013), to low-rank matrices perturbed by a nonlinear monotonic function (Ganti et al., 2015; Song et al., 2016). In each case missing data has been considered, but there lacks a clear, unifying framework for these ideas.\nIn this work we study the problem of completing a matrix whose columns belong to an algebraic variety, i.e., the set of solutions to a system of polynomial equations (Cox et al., 2015). This is a strict generalization of the linear (or affine) subspace model, which can be written as the set of points satisfying a system of linear equations. Unions of subspaces and unions of affine spaces are also algebraic varieties. Plus, a much richer class of non-linear curves, surfaces, and their unions, are captured by a variety model.\nThe matrix completion problem using a variety model can be formalized as follows. Let X = [ x1, . . . ,xs ] \u2208 Rn\u00d7s be a matrix of s data points where each column xi \u2208 Rn. Define \u03c6d : Rn \u2192 RN as the mapping that sends the vector x = (x1, ..., xn) to the vector of all monomials in x1, ..., xn of degree at most d, and let \u03c6d(X) denote the matrix that results after applying \u03c6d to each column of X , which we call the lifted matrix. We will show the lifted matrix is rank deficient if and only if the columns ofX belong to an algebraic variety. This motivates the following matrix completion approach:\nmin X\u0302\nrank\u03c6d(X\u0302) such that P\u2126(X\u0302) = P\u2126(X) (1)\nwhere P\u2126(\u00b7) represents a projection that restricts to some observation set \u2126 \u2282 {1, . . . , n} \u00d7 {1, . . . , s}. The rank of \u03c6d(X\u0302) depends on the choice of the polynomial degree d and the underlying \u201ccomplexity\u201d of the variety, in a sense we will make precise. Figure 1 shows three examples of datasets that have low-rank in the lifted space for different polynomial degrees d.\nIn this work we investigate the factors that influence the sampling complexity of varieties as well as algorithms for\ncompletion. The challenges are (a) to characterize varieties having low-rank (and therefore few degrees of freedom) in the lifted space, i.e., determine when \u03c6d(X) is low-rank, (b) devise efficient algorithms for solving (1) that can exploit these few degrees of freedom in a matrix completion setting, and (c) determine the trade-offs relative to existing matrix completion approaches. This work contributes considerable progress towards these goals.\nOur main contributions are as follows. We identify bounds on the rank of a matrix \u03c6d(X) when the columns of the data matrix X belong to an algebraic variety. We study how many entries of such a matrix should be observed in order to recover the full matrix from an incomplete sample. We show as a case study that monomial representations produce low-rank representations of unions of subspaces, and we characterize the rank. The standard union of subspace representation as a discrete collection of individual subspaces is inherently non-smooth in nature, whereas the algebraic variety allows for a purely continuous parameterization. This leads to a general algorithm for completion of a data matrix whose columns belong to a variety. The algorithm\u2019s performance is showcased on data simulated as a union of subspaces, a union of low-dimensional parametric surfaces, and real data from a motion segmentation dataset and a motion capture dataset. The simulations show that the performance of our algorithm matches our predictions and outperforms other methods. In addition, the analysis of the degrees of freedom associated with the proposed representations introduces several new research avenues at the intersection of nonlinear algebraic geometry and random matrix theory."}, {"heading": "1.1. Related work", "text": "There has been a great deal of research activity on matrix completion problems since (Candes & Recht, 2012), where the authors showed that one can recover an incomplete matrix from few entries using a convex relaxation of the rank minimization optimization problem. For example, it is now known that only O(rn) entries are necessary and sufficient (Pimentel-Alarco\u0301n et al., 2016b) for almost every n \u00d7 n rank r matrix as long as the measurement pattern satisfies\ncertain deterministic conditions. However, these methods and theory are restricted to low-rank linear models. A great deal of real data exhibit nonlinear structure, and so it is of interest to generalize this approach. Work in that direction has dealt with union of subspaces models (Eriksson et al., 2012; Yang et al., 2015; Elhamifar, 2016; PimentelAlarco\u0301n et al., 2016a; Pimentel-Alarcon & Nowak, 2016), locally linear approximations (Lee et al., 2013), as well as low-rank models perturbed by an arbitrary nonlinear link function (Ganti et al., 2015; Song et al., 2016; Rao et al., 2017). In this paper we instead seek a more general model that captures both linear and nonlinear structure. The variety model has as instances low-rank subspaces and their union as well as quadratic and higher degree curves and surfaces.\nWork on kernel PCA (cf., (Sanguinetti & Lawrence, 2006; Nguyen & Torre, 2009)) leverage similar geometry to ours. In Kernel Spectral Curvature Clustering (Chen et al., 2009), the authors similarly consider clustering of data points via subspace clustering in a lifted space using kernels. These works are algorithmic in nature, with promising numerical experiments, but do not systematically consider missing data or analyze relative degrees of freedom.\nThis paper also has close ties to algebraic subspace clustering (ASC) (Vidal et al., 2003; 2005; 2016; Tsakiris & Vidal, 2015), also known as generalized PCA. Similar to our approach, the ASC framework models unions of subspaces as an algebraic variety, and makes use of monomial liftings of the data to identify the subspaces. Characterizations of the rank of data belonging to union of subspaces under the monomial lifting are used in the ASC framework (Vidal et al., 2016) based on results in (Derksen, 2007). The difference of the results in (Derksen, 2007) and those in Prop. 1 is that ours hold for monomial liftings of all degrees d, not just d \u2265 k, where k is the number of subspaces. Also, the main focus of ASC is to recover unions of subspaces or unions of affine spaces, whereas we consider data belonging to a more general class of algebraic varieties. Finally, the ASC framework has not been adapted to the case of missing data, which is the main focus of this work."}, {"heading": "2. Variety Models", "text": "As a toy example to illustrate our approach, consider a matrix\nX = ( x1,1 x1,2 \u00b7 \u00b7 \u00b7 x1,6 x2,1 x2,2 \u00b7 \u00b7 \u00b7 x2,6 ) \u2208 R2\u00d76\nwhose six columns satisfy the quadratic equation\nc0+c1 x1,i+c2 x2,i+c3 x 2 1,i+c4 x1,ix2,i+c5 x 2 2,i = 0 (2)\nfor i = 1, . . . , 6 and some unknown constants c0, ..., c5 that are not all zero. Generically,X will be full rank. However,\nsuppose we vertically expand each column of the matrix to make a 6\u00d7 6 matrix\nY =  1 1 \u00b7\u00b7\u00b7 1 x1,1 x1,2 \u00b7\u00b7\u00b7 x1,6 x2,1 x2,2 \u00b7\u00b7\u00b7 x2,6 x21,1 x 2 1,2 \u00b7\u00b7\u00b7 x 2 1,6\nx1,1x2,1 x1,2x2,2 \u00b7\u00b7\u00b7 x1,6x2,6 x22,1 x 2 2,2 \u00b7\u00b7\u00b7 x 2 2,6  \u2208 R6\u00d76, i.e., we augment each column of X with a 1 and with the quadratic monomials x21,i, x1,ix2,i, x 2 2,i. This allows us to re-express the polynomial equation (2) as the matrixvector product Y T c = 0 where c = (c0, c1, .., c5)T . In other words, Y is rank deficient. Suppose, for example, that we are missing entry x1,1 of X . Since X is full rank, there is no way to uniquely complete the missing entry by leveraging linear structure alone. Instead, we ask: Can we complete x1,1 using the linear structure present in Y ? Due to the missing entry x1,1, the first column of Y will having the following pattern of missing entries: (1,\u2212, x2,1,\u2212,\u2212, x22,1)T . However, assuming the five complete columns in Y are linearly independent, we can uniquely determine the nullspace vector c up to a scalar multiple. Then from (2) we have\nc3 x 2 1,1 + (c1 + c4 x2,1)x1,1 = \u2212c0 \u2212 c2 x2,1 \u2212 c5 x22,1.\nIn general, this equation will yield at most two possibilities for x1,1. Moreover, there are conditions where we can uniquely recover x1,1, namely when c3 = 0 and c1 + c4 x2,1 6= 0.\nThis example shows that even without a priori knowledge of the particular polynomial equation satisfied by the data, it is possible to uniquely recover missing entries in the original matrix by leveraging induced linear structure in the matrix of expanded monomials. We now show how to considerably generalize this example to the case of data belonging to an arbitrary algebraic variety."}, {"heading": "2.1. Formulation", "text": "LetX = [ x1, . . . ,xs ] \u2208 Rn\u00d7s be a matrix of s data points where each column xi \u2208 Rn. Define \u03c6d : Rn \u2192 RN as the mapping that sends the vector x = (x1, ..., xn) to the vector of all monomials in x1, ..., xn of degree at most d:\n\u03c6d(x) = (x \u03b1)|\u03b1|\u2264d \u2208 RN\nwhere \u03b1 = (\u03b11, ..., \u03b1n) is a multi-index of non-negative integers, withx\u03b1 := x\u03b111 \u00b7 \u00b7 \u00b7x\u03b1nn , and |\u03b1| := \u03b11+\u00b7 \u00b7 \u00b7+\u03b1n. In the context of kernel methods in machine learning, the map \u03c6d is often called a polynomial feature map (Muller et al., 2001). Borrowing this terminology, we call \u03c6d(x) a feature vector, the entries of \u03c6d(x) features, and the range of \u03c6d feature space. Note that the number of features is given by N = N(n, d) = ( n+d n ) = ( n+d d ) , the number of unique monomials in n variables of degree at most d.\nWhen X = [x1, ...,xs] is an n\u00d7 s matrix, we use \u03c6d(X) to denote the N \u00d7 s matrix [\u03c6d(x1), ..., \u03c6d(xs)].\nThe problem we consider is this: can we complete a partially observed matrixX under the assumption that \u03c6d(X) is low-rank? This can be posed as the optimization problem given above in (1). We give a practical algorithm for solving a relaxation of (1) in Section 4. Similar to previous work cited above on using polynomial feature maps, our method leverages the kernel trick for efficient computations. The success of this optimization and its relaxations will depend on many factors, but clearly the rank of \u03c6d(X) and the number of sampled entries will play an important role. The number of samples, rank, and dimensions all grow in the mapping to feature space, but they grow at different rates depending on the underlying geometry; it is not immediately obvious what conditions on the geometry and sampling rates impact our ability to determine the missing entries. In the remainder of this section, we show how to relate the rank of \u03c6d(X) to the underlying variety, and we study the sampling requirements necessary for the completion of the matrix in feature space."}, {"heading": "2.2. Rank properties", "text": "To better understand what determines the rank of the matrix \u03c6d(X), we introduce some additional notation and concepts from algebraic geometry. Let R[x] denote the space of all polynomials with real coefficients in n variables x = (x1, ..., xn). We model a collection of data as belonging to a real (affine) algebraic variety (Cox et al., 2015), which is defined as the common zero set of a system of polynomials P \u2282 R[x]:\nV (P ) = {x \u2208 Rn : f(x) = 0 for all f \u2208 P}.\nSuppose the variety V (P ) is defined by the finite set of polynomials P = {f1, ..., fq}, where each fi has degree at most d. Let C \u2208 RN\u00d7q be the matrix whose columns are given by the vectorized coefficients (c\u03b1,i)|\u03b1|\u2264d of the polynomials fi(x), i = 1, ..., q in P . Then the columns of X belong to the variety V (P ) if and only if \u03c6d(X)TC = 0. In particular, assuming the columns ofC are linearly independent, this shows that \u03c6d(X) has rank\u2264 min(N \u2212 q, s). In particular, when the number of data points s > N \u2212 q, then \u03c6d(X) is rank deficient.\nHowever, the exact rank of \u03c6d(X) could be much smaller than min(N \u2212 q, s), especially when the degree d is large. This is because the coefficients c of any polynomial that vanishes at every column of X satisfies \u03c6d(X)T c = 0. We will find it useful to identify this space of coefficients with a finite dimensional vector space of polynomials. Let Rd[x] be the space of all polynomials in n real variables of degree at most d. We define the vanishing ideal of degree d corresponding to a set X \u2282 Rn, denoted by Id(X ), to be\nsubspace of polynomials belonging to Rd[x] that vanish at all points in X :\nId(X ) := {f \u2208 Rd[x] : f(x) = 0 for all x \u2208 X}.\nWe also define the non-vanishing ideal of degree d corresponding to X , denoted by Sd(X ), to be the orthogonal complement of Id(X ) in Rd[x]:\nSd(X ) :={g \u2208 Rd[x] :\u3008f, g\u3009 = 0 for all f \u2208 Id(X )},\nwhere the inner product \u3008f, g\u3009 of polynomials f, g \u2208 Rd[x] is defined as the inner product of their coefficient vectors. Hence, the rank R of \u03c6d(X) can expressed in terms of the dimension of non-vanishing ideal of degree d corresponding to X = {x1, ....,xs}, the set of all columns of X . Specifically, we have rank \u03c6d(X) = min(R, s) where\nR = dim Sd(X ) = N \u2212 dim Id(X ) .\nIn general the dimension of the space Id(X ) or Sd(X ) is difficult to determine when X is an arbitrary set of points. However, if we assume X is a subset of a variety V , then Id(V ) \u2286 Id(X ) and hence\nrank \u03c6d(X) \u2264 dim Sd(V ).\nIn certain cases dim Sd(V ) can be computed exactly or bounded using properties of the polynomials defining V . For example, it is possible to compute the dimension of Sd(V ) directly from a Gro\u0308bner basis for the vanishing ideal associated with V (Cox et al., 2015). In Section 3 we show how to bound the dimension of Sd(V ) in the case where V is a union of subspaces."}, {"heading": "2.3. Sampling rate", "text": "Informally, the degrees of freedom of a class of objects is the minimum number of free variables needed to describe an element in that class uniquely. For example, a n\u00d7s rank r matrix has r(n+ s\u2212 r) degrees of freedom: nr parameters to describe r linearly independent columns making up a basis of the column space, and r(s\u2212 r) parameters to describe the remaining s \u2212 r columns in terms of this basis. It is impossible to uniquely complete a matrix in this class if we sample fewer than this many entries.\nWe can make a similar argument to specify the minimum number of samples needed to uniquely complete a matrix that is low-rank when mapped to feature space. First, we characterize how missing entries of the data matrix translate to missing entries in feature space. For simplicity, we will assume a sampling model where we sample a fixed number of entries m from each column of the original data matrix. Let x \u2208 Rn represent a single column of the data matrix, and \u2126 \u2282 {1, ..., n} with m = |\u2126| denote the indices of the sampled entries of x. The pattern of revealed\nentries in \u03c6d(x) corresponds to the set of multi-indices:\n{\u03b1 = (\u03b11, ..., \u03b1n) : |\u03b1| \u2264 d, \u03b1i = 0 for all i \u2208 \u2126c},\nwhich has the same cardinality as the set of all monomials of degree at most d in m variables, i.e., ( m+d d ) . If we call this quantity M , then the ratio of revealed entries in \u03c6d(x) to the feature space dimension is\nM N =\n( m+d d )( n+d d ) = (m+ d)(m+ d\u2212 1) \u00b7 \u00b7 \u00b7 (m+ 1) (n+ d)(n+ d\u2212 1) \u00b7 \u00b7 \u00b7 (n+ 1) ,\nwhich is on the order of (mn ) d for small d. More precisely, we have the bounds(m n )d \u2264 M N \u2264 ( m+ d n )d . (3)\nIn total, observing m entries per column of the data matrix translates to M entries per column in feature space. Suppose the N \u00d7 s lifted matrix \u03c6d(X) is rank R. By the preceding discussion, we need least R(N + s\u2212R) entries of the feature space matrix \u03c6d(X) to complete it uniquely among the class of all N \u00d7 s matrices of rank R. Hence, at minimum we need to satisfy\nMs \u2265 R(N + s\u2212R). (4)\nLet m0 denote the minimal value of m such that M =( m+d d ) achieves the bound (4), and set M0 = ( m0+d d ) . Dividing (4) through by the feature space dimension N and s gives\nM0 N \u2265 ( R N )( N + s\u2212R s ) = ( R s + R N ( 1\u2212 R s )) ,\n(5) and so from (3) we see we can guarantee this bound with\n\u03c10 := m0 n \u2265 ( R s + R N ( 1\u2212 R s )) 1 d , (6)\nand this in fact will result in tight satisfaction of (5) because (M0/N) 1 d \u2248 m0/n for small d and large n.\nAt one extreme where the matrix \u03c6d(X) is full rank, then R/s = 1 or R/N = 1 and according to (6) we need \u03c10 \u2248 1, i.e., full sampling of every data column. At the other extreme where instead we have many more data points than the feature space rank, R/s 1, then (6) gives the asymptotic bound \u03c10 \u2248 (R/N) 1 d .\nThe above discussion bounds the degrees of freedom of a matrix that is rank-R in feature space. Of course, the proposed variety model has potentially fewer degrees of freedom than this, because additionally the columns of the lifted matrix are constrained to lie in the image of the feature map. We use the above bound only as a rule of thumb\nfor sampling requirements on our matrix. Furthermore, we note that sample complexities for standard matrix completion often require that locations are observed uniformly at random, whereas in our problem the locations of observations in the lifted space will necessarily be structured. However, there is recent work that shows matrix completion can suceed without these assumptions (PimentelAlarco\u0301n et al., 2016b; Chen et al., 2014) that gives reason to believe random samples in the original space may allow completion in the lifted space, and our empirical results in Section 5 support this rationale."}, {"heading": "3. Case Study: Union of Affine Subspaces", "text": "A union of affine subspaces can be modeled as an algebraic variety. For example, with (x, y, z) \u2208 R3, the union of the plane z = 1 and the line x = y is the zero-set of the quadratic polynomial (z \u2212 1)(x\u2212 y). In general, if A1,A2 \u2282 Rn are affine spaces of dimension r1 and r2, respectively, then we can write A1 = {x : fi(x) = 0 for i = 1, ..., n\u2212 r1} and A2 = {x : gi(x) = 0 for i = 1, ..., n\u2212 r2} where the fi and gi are affine functions. The union A \u222a B can be expressed as the common zero set of all possible products of the fi and gi, i.e., A1 \u222a A2 is the common zero set of a system of (n\u2212 r1)(n\u2212 r2) quadratic equations. Similarly, a union of k affine subspaces of dimensions r1, ..., rk is a variety described by a system of \u220fk i=1(n\u2212 ri) polynomial equations of degree k.\nIn this section we establish bounds on the feature space rank for data belonging to a union of affine subspaces. We will make use of the following lemma that shows the dimension of a vanishing ideal is fixed under an affine change of variables:\nLemma 1. Let T : Rn \u2192 Rn be an affine change of variables, i.e., T (x) = Ax+b, where b \u2208 Rn andA \u2208 Rn\u00d7n is invertible. Then for any S \u2282 Rn,\ndim Id(S) = dim Id(T (S)). (7)\nWe omit the proof for brevity, but the result is elementary and relies on the fact the degree of a polynomial is unchanged under an affine change of variables. Our next result establishes a bound on the feature space rank for a single affine subspace:\nProposition 1. If the columns of a matrixXn\u00d7s belong to an affine subspace of dimension at most r, then\nrank\u03c6d(X) \u2264 ( r + d\nd\n) , for all d \u2265 1. (8)\nProof. By Lemma 1, dim Id(A) is preserved under an affine transformation of A. Note that we can always find an affine change of variables y = Ax + c with invertible A \u2208 Rn\u00d7n and c \u2208 Rn such that in the coordinates\ny = (y1, ..., yn) the variety A becomes\nA = {(y1, . . . , yr, 0, . . . , 0) : y1, ..., yr \u2208 R}. (9)\nFor any polynomial f(y) = \u2211 |\u03b1|\u2264d c\u03b1y\n\u03b1, the only monomial terms in f(y) that do not vanish on A are those of the form y\u03b111 \u00b7 \u00b7 \u00b7 y\u03b1rr . Furthermore, any polynomial in just these monomials that vanishes on all ofA must be the zero polynomial, since the y1, ..., yr are free variables. Hence,\nSd(A) = span{y\u03b111 \u00b7 \u00b7 \u00b7 y\u03b1rr : \u03b11 + \u00b7 \u00b7 \u00b7+ \u03b1r \u2264 d} (10)\ni.e., the non-vanishing ideal coincides with the space of polynomials in r variables of degree at most d, which has dimension ( r+d d ) , proving the claim. We note that for s sufficiently large, the bound in (8) becomes an equality, provided the data points are distributed generically within the affine subspace, meaning they are not the solution to additional non-trivial polynomial equations of degree at most d.\nProposition 1 shows that points belonging to a single affine subspace of dimension r are mapped to a linear subspace of dimension ( r+d d ) under \u03c6d. Therefore, if the columns of a data matrix are drawn from a union of k affine subspaces of dimension r, their image under \u03c6d will belong to a union of k linear subspaces each of dimension at most ( r+d d ) . The\nlinear span of this union has dimension at most k ( r+d d ) , which yields the following result:\nProposition 2. If the columns of a matrixXn\u00d7s belong to a union of k affine subspaces each of dimension at most r, then\nrank\u03c6d(X) \u2264 k ( r + d\nd\n) , for all d \u2265 1. (11)\nIn some cases the bound (11) is (nearly) tight. For example, if the data lies on the union of two r-dimensional affine subspaces A and B that are mutually orthogonal, one can show1 rank\u03c6d(X) = 2 ( r+d d ) \u2212 1. Empirically, we observe that the bound in (11) is order-optimal with respect to k, r, and d. In this case, the feature space rank to dimension ratio is R/N = O(k ( r n )d ). Recall that the minimum sampling rate is approximately (R/N) 1 d for s R. Hence the mininum number of samples per column m should be\nm \u2248 O(k 1d r). (12)\nThis rate is favorable to low-rank matrix completion approaches, which need m = O(kr) for a union of k subspaces having dimension r. At first glance, this bound suggests it is always better to take the degree d as large as possible. However, this is only true for sufficiently large s.\n1The rank is one less than the bound in (11) because Sd(A) \u2229 Sd(B) has dimension one, coinciding with the space of constant polynomials.\nTo take advantage of the improved sampling rate implied by (12), according to (6) we need the number of data vectors per subspace to be O(rd). In other words, our model is able to accommodate more subspaces with larger d but at the expense of requiring exponentially more data points per subspace. Note that if the number of data points is sufficiently large, we could take d = log k and require only m \u2248 O(r) observed entries per column. In this case, for moderately sized k (e.g., k \u2264 20) we should choose d = 2 or 3. In fact, we find that for these values of d we get excellent empirical results for the recovery of union of subspaces data, as shown in Section 5."}, {"heading": "4. Algorithm", "text": "There are several existing matrix completion algorithms that could potentially be adapted to solve a relaxation of the rank minimization problem (1), such as singular value thresholding (Cai et al., 2010), or alternating minimization (Jain et al., 2013). However, these approaches do not easily lend themselves to \u201ckernelized\u201d implementations, i.e., ones that do not require forming the high-dimensional lifted matrix \u03c6d(X) explicitly, but instead make use of the efficiently computable kernel function for polynomial feature maps 2\nkd(x,y) := \u03c6d(x) T\u03c6d(y) = (x Ty + 1)d. (13)\nFor matrices X = [x1, ...,xs],Y = [y1, ...,ys] \u2208 Rn\u00d7s, we use kd(X,Y ) to denote the matrix whose (i, j)-th entry is kd(xi,yj), or equivalently,\nkd(X,Y ) := \u03c6d(X) T\u03c6d(Y ) = (X TY + 1) d, (14)\nwhere 1 \u2208 Rs\u00d7s is the matrix of all ones, and (\u00b7) d denotes the entrywise d-th power of a matrix. A kernelized implentation is critical for even modest sizes of d, since the number of rows of the lifted matrix scales exponentially with d.\nOne class of algorithm that kernelizes very naturally is the iterative reweighted least squares (IRLS) approach of (Fornasier et al., 2011; Mohan & Fazel, 2012) for low-rank matrix completion. The algorithm also has the advantage of being able to accommodate the non-convex Schatten-p relaxation of the rank penalty, in addition to the convex nuclear norm relaxation. Specifically, we use an IRLS approach to solve the following variety-based matrix completion (VMC) optimization problem:\nmin X \u2016\u03c6d(X)\u2016pSp s.t. P\u2126(X) = P\u2126(X0), (VMC)\n2Strictly speaking, kd is not kernel associated with the polynomial feature map \u03c6d as defined in (2.1). Instead, it is the kernel of the related map \u03c6\u0303d(x) := { \u221a c\u03b1x\n\u03b1 : |\u03b1| \u2264 d} where c\u03b1 are appropriately chosen multinomial coefficients.\nAlgorithm 1 Kernelized IRLS to solve (VMC). Require: InitializeX = X0, \u03b3 = \u03b30. Choose \u03b7,\u03b3min.\nwhile not converged do Step 1: Inverse power of kernel matrix K \u2190 kd(X,X) (V ,S) = eig(K). W \u2190 V (S + \u03b3I) p 2\u22121V T\nStep 2: Projected gradient descent step \u03c4 \u2190 \u03b31\u2212 p 2 X \u2190X \u2212 \u03c4X(W kd\u22121(X,X)) X \u2190 P\u2126(X0) + P\u2126c(X) \u03b3 \u2190 max{\u03b3/\u03b7, \u03b3min}\nend while\nwhere \u2016Y \u2016Sp is the Schatten-p quasi-norm defined as\n\u2016Y \u2016Sp := (\u2211 i \u03c3i(Y ) p ) 1 p , 0 < p \u2264 1 (15)\nwith \u03c3i(Y ) denoting the ith singular value of Y . Algorithm 1 gives the pseudo-code of the proposed IRLS algorithm for solving (VMC), which we derive below.\nFirst, consider the simpler problem of minimizing the Schatten-p norm of a matrix variable Y belonging to a constraint set C. The main idea behind the IRLS approach is re-express the Schatten-p quasi-norm as\n\u2016Y \u2016pSp = tr[(Y TY ) p 2 ] = tr[(Y TY )W ], (16)\nwhere W := (Y TY ) p 2\u22121. Note if W is treated as constant, then (16) is a smooth, quadratic function of Y . This motivates the following iterative approach:\nWn = (Y T n Yn + \u03b3n)\np 2\u22121\nYn+1 = arg min Y \u2208C\ntr[(Y TY )Wn].\nHere \u03b3n is a sequence of smoothing parameters satisfying \u03b3n \u2192 \u03b3min as n \u2192 \u221e, where \u03b3min is close to zero, which is included to improve numerical stability and avoid local minima; this is equivalent to minimizing a smooth approximation of the Schatten-p cost (Mohan & Fazel, 2012).\nMaking the substitution Y = \u03c6d(X) in the above derivation, gives the following approach for solving (VMC):\nWn = (k(Xn,Xn) + \u03b3nI) p 2\u22121\nXn+1 = arg min X\ntr[k(X,X)Wn] s.t. P\u2126(X) = P\u2126(X0)\nRather than finding the exact minimum in the X update, which could be costly, following the approach in (Mohan & Fazel, 2012), we instead take a single projected gradient descent step to update X . A straightforward calculation shows that the gradient of\nthe objective F (X) = tr[k(X,X)W ] is given by \u2207F (X) = X(W kd\u22121(X,X)), where denotes an entry-wise product. Hence a projected gradient step with step-size \u03c4n > 0 is given by\nX\u0303n = Xn \u2212 \u03c4nXn(Wn kd\u22121(Xn,Xn)) Xn = P\u2126(X0) + P\u2126c(X\u0303n).\nSimilar to (Mohan & Fazel, 2012), one can show that every limit point of the above iterates converges to a stationary point of a smoothed Schatten-p cost for appropriate choices of step-sizes \u03c4n. Heuristics are given in (Mohan & Fazel, 2012) for updating the smoothing parameter \u03b3n, which we adopt as well. Specifically, we set \u03b3n = \u03b30/\u03b7n, where \u03b30 and \u03b7 are user-defined parameters, and update \u03c4n = \u03b3 1\u2212p/2 n . The appropriate choice of \u03b30 and \u03b7 will depend on the scaling and spectral properties of the data. Empirically, we find that setting \u03b30 = (0.1)d\u03bbmax, where \u03bbmax is the largest eigenvalue of the kernel matrix obtained from the initialization, and \u03b7 = 1.01 work well in a variety of settings. For all our experiments in Section 5 we fix p = 1/2, which was found to give the best matrix recovery results for synthetic data. We also use a zero-filled initializationX0 in all cases."}, {"heading": "5. Numerical Experiments", "text": ""}, {"heading": "5.1. Empirical validation of sampling bounds", "text": "In Figure 2 we report the results of two experiments to validate the predicted minimum sampling rate \u03c10 in (4) on synthetic variety data. In the first experiment we generated n\u00d7 s data matrices whose columns belong to a union of k subspaces each of dimension r (with n = 15, s = 100k,\nr = 3). In the second experiment we generated data matrices of size 20 \u00d7 300 whose columns belong to a union of randomly generated parametric surfaces of low dimension, where we sorted each dataset by its empirically determined feature space rank R. For both experiments, we undersampled each column of the matrix taking m entries uniformly at random at various values of k and R, and then attempted to recover the missing entries using our proposed IRLS algorithm for VMC (Algorithm 1 with p = 1/2) for d = 2, 3. For the union of subspaces data, we also compare with low-rank matrix completion in the original matrix domain via nuclear norm minimization (LRMC) and non-convex Schatten-1/2 minimization (LRMC-NCVX), implemented using Algorithm 1 with a linear kernel (d = 1 in (13)). We said a column was successfully recovered if \u2016x\u2212x0\u2016/\u2016x0\u2016 \u2264 10\u22125, where x is the recovered column and x0 is the original column. For each pair of parameters (m, k) or (m,R) we perform 10 random trials to determine the probability of successful recovery.\nConsistent with our theory, VMC is successful at recovering most of the data columns above the predicted minimum sampling rate, substantially extending the range of recovery over LRMC. While VMC often fails to recover 100% of the columns near the predicted rate, in fact a large proportion of the columns (%99\u2013%90) are still successfully completed. Sometimes the recovery dips below the predicted rate (e.g., VMC, d = 2 in Fig. 2(a) and VMC, d = 3 in Fig. 2(b)). However, since the predicted rate relies on what is likely an over-estimate of the true degrees of freedom, it is not surprising that the VMC algorithm occasionally succeeds below this rate, too."}, {"heading": "5.2. Motion segmentation of real data", "text": "In Figure 3 we apply VMC to the problem of motion segmentation (Kanatani, 2001) with missing data using the Hopkins 155 dataset (Tron & Vidal, 2007). This data consists of several feature points tracked across frames of the video. We reproduce the experimental setting in (Yang et al., 2015), and simulate high-rank data by undersampling frames of the dataset. We simulate missing trajectories by sampling uniformly at random from the feature points across all frames. To obtain a clustering we first completed the missing entries using VMC and then ran the sparse subspace clustering (SSC) algorithm (Elhamifar & Vidal, 2009) on the result, calling this VMC+SSC. A similar approach of standard LRMC followed by SSC (LRMC+SSC) provides a consistent baseline for subspace clustering with missing data (Yang et al., 2015; Elhamifar, 2016). We also compare against SSC with entrywise zerofill (SSC-EWZF) (Yang et al., 2015). We find the VMC+SSC approach gives similar or lower clustering error than LRMC+SCC for low missing rates. Likewise, VMC+SSC also substantially outperforms SSC-EWZF for high missing rates. Unlike SSC-EWZF and the other algorithms in (Yang et al., 2015), VMC+SSC also succeeds in setting where the data is low-rank (i.e., when all frames are retained). This is because the performance of VMC is similar to standard LRMC in the low-rank setting."}, {"heading": "5.3. Completion of motion capture data", "text": "In Figure 4 we demonstrate VMC for completing timeseries trajectories from motion capture sensors using a dataset from the CMU Mocap database3 (subject 56, trial 6). Empirically, this dataset has been shown to be locally low-rank over the time frames corresponding to each separate activity, and can be modeled as a union of subspaces (Elhamifar, 2016). The data had measurements from n = 62 sensors at s = 6784 time instants. We randomly undersampled the columns of this matrix and attempt to complete the data using VMC, LRMC, and LRMC-NCVX and measure the resulting completion error: \u2016X \u2212X0\u2016F /\u2016X0\u2016F , where X is the recovered matrix and X0 is the original matrix. Similar to results on synthetic data, we find the\n3http://mocap.cs.cmu.edu\nVMC approach outperforms LRMC-NCVX for appropriately chosen degree d. In particular, VMC with d = 2, 3 perform similar for small missing rates, but VMC d = 2 gives lower completion error over d = 3 for large missing rates, consistent with the results in Figure 2."}, {"heading": "6. Conclusion", "text": "We introduce a matrix completion approach that generalizes low-rank matrix completion to a much wider class of variety models, including data belonging to a union of subspaces. We present a hypothesized sampling complexity bound for the completion of a matrix whose columns belong to an algebraic variety. A surprising result of our analysis that that a union of k affine subspaces of dimension r should be recoverable fromO(rk1/d) measurements per column, provided we haveO(rd) data points (columns) per subspace, where d is the degree of the feature space map. In particular, if we choose d = log k, then we need only O(r) measurements per column as long as we have O(rlog k) columns per subspace. We additionally introduce an efficient algorithm based on an iterative reweighted least squares approach that realizes these hypothesized bounds on synthetic data, and reaches state-of-the-art performance on for matrix completion on several real high-rank datasets.\nOur algorithm can easily accommodate other smooth kernels, including the popular Gaussian RBF kernel (Muller et al., 2001). A similar optimization formulation to ours was presented in the recent pre-print (Garg et al., 2016) using Gaussian RBF kernels in place of polynomial kernels, showing good empirical results in a matrix completion context. However, analysis of the sample complexity in this case is complicated by the fact that a feature space representation for Gaussian RBF kernel is necessarily infinitedimensional. Understanding the sample requirements in this case would be an interesting avenue for future work."}, {"heading": "Acknowledgements", "text": "For this work, Balzano and Ongie were supported in part by ARO grant W911NF-14-1-0634. Willett and Nowak were supported in part by NSF IIS-1447449, NSF CCF0353079, and NIH 1 U54 AI117924-01, and Nowak also by AFOSR FA9550-13-1-0138."}], "year": 2017, "references": [{"title": "A singular value thresholding algorithm for matrix completion", "authors": ["Cai", "Jian-Feng", "Cand\u00e8s", "Emmanuel J", "Shen", "Zuowei"], "venue": "SIAM Journal on Optimization,", "year": 1956}, {"title": "Exact matrix completion via convex optimization", "authors": ["Candes", "Emmanuel", "Recht", "Benjamin"], "venue": "Communications of the ACM,", "year": 2012}, {"title": "Kernel spectral curvature clustering (kscc)", "authors": ["Chen", "Guangliang", "Atev", "Stefan", "Lerman", "Gilad"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "year": 2009}, {"title": "Coherent matrix completion", "authors": ["Chen", "Yudong", "Bhojanapalli", "Srinadh", "Sanghavi", "Sujay", "Ward", "Rachel"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "year": 2014}, {"title": "Ideals, Varieties, and Algorithms", "authors": ["Cox", "David A", "Little", "John", "O\u2019Shea", "Donal"], "year": 2015}, {"title": "Hilbert series of subspace arrangements", "authors": ["Derksen", "Harm"], "venue": "Journal of pure and applied algebra,", "year": 2007}, {"title": "High-rank matrix completion and clustering under self-expressive models", "authors": ["Elhamifar", "Ehsan"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Sparse subspace clustering", "authors": ["Elhamifar", "Ehsan", "Vidal", "Ren\u00e9"], "venue": "In Computer Vision and Pattern Recognition,", "year": 2009}, {"title": "Sparse subspace clustering: Algorithm, theory, and applications", "authors": ["Elhamifar", "Ehsan", "Vidal", "Ren\u00e9"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "year": 2013}, {"title": "High-rank matrix completion", "authors": ["Eriksson", "Brian", "Balzano", "Laura", "Nowak", "Robert D"], "venue": "In AISTATS, pp", "year": 2012}, {"title": "Low-rank matrix recovery via iteratively reweighted least squares minimization", "authors": ["Fornasier", "Massimo", "Rauhut", "Holger", "Ward", "Rachel"], "venue": "SIAM Journal on Optimization,", "year": 2011}, {"title": "Matrix completion under monotonic single index models", "authors": ["Ganti", "Ravi Sastry", "Balzano", "Laura", "Willett", "Rebecca"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Non-linear dimensionality regularizer for solving inverse problems", "authors": ["Garg", "Ravi", "Eriksson", "Anders", "Reid", "Ian"], "venue": "arXiv preprint arXiv:1603.05015,", "year": 2016}, {"title": "Low-rank matrix completion using alternating minimization", "authors": ["Jain", "Prateek", "Netrapalli", "Praneeth", "Sanghavi", "Sujay"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Motion segmentation by subspace separation and model selection", "authors": ["Kanatani", "Ken-ichi"], "venue": "In Computer Vision,", "year": 2001}, {"title": "Local low-rank matrix approximation", "authors": ["Lee", "Joonseok", "Kim", "Seungyeon", "Lebanon", "Guy", "Singer", "Yoram"], "venue": "ICML (2),", "year": 2013}, {"title": "Iterative reweighted algorithms for matrix rank minimization", "authors": ["Mohan", "Karthik", "Fazel", "Maryam"], "venue": "The Journal of Machine Learning Research,", "year": 2012}, {"title": "An introduction to kernel-based learning algorithms", "authors": ["Muller", "K-R", "Mika", "Sebastian", "Ratsch", "Gunnar", "Tsuda", "Koji", "Scholkopf", "Bernhard"], "venue": "IEEE Transactions on Neural Networks,", "year": 2001}, {"title": "Robust kernel principal component analysis", "authors": ["Nguyen", "Minh H", "Torre", "Fernando"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Group-sparse subspace clustering with missing data", "authors": ["D Pimentel-Alarc\u00f3n", "L Balzano", "R Marcia", "R Nowak", "R. Willett"], "venue": "In Statistical Signal Processing Workshop (SSP),", "year": 2016}, {"title": "The information-theoretic requirements of subspace clustering with missing data", "authors": ["Pimentel-Alarcon", "Daniel", "Nowak", "Robert"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "A characterization of deterministic sampling patterns for low-rank matrix completion", "authors": ["Pimentel-Alarc\u00f3n", "Daniel L", "Boston", "Nigel", "Nowak", "Robert D"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "year": 2016}, {"title": "On learning high dimensional structured single index models", "authors": ["Rao", "Nikhil", "Ganti", "Ravi", "Balzano", "Laura", "Willett", "Rebecca", "Nowak", "Robert"], "venue": "In Proceedings of the 31st AAAI conference on artificial intelligence,", "year": 2017}, {"title": "Missing data in kernel PCA", "authors": ["Sanguinetti", "Guido", "Lawrence", "Neil D"], "venue": "In European Conference on Machine Learning,", "year": 2006}, {"title": "Blind regression: Nonparametric regression for latent variable models via collaborative filtering", "authors": ["Song", "Dogyoon", "Lee", "Christina E", "Li", "Yihua", "Shah", "Devavrat"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "A benchmark for the comparison of 3-d motion segmentation algorithms", "authors": ["Tron", "Roberto", "Vidal", "Ren\u00e9"], "venue": "In Computer Vision and Pattern Recognition,", "year": 2007}, {"title": "Algebraic clustering of affine subspaces", "authors": ["Tsakiris", "Manolis C", "Vidal", "Ren\u00e9"], "venue": "arXiv preprint arXiv:1509.06729,", "year": 2015}, {"title": "An algebraic geometric approach to the identification of a class of linear hybrid systems", "authors": ["Vidal", "Ren\u00e9", "Soatto", "Stefano", "Ma", "Yi", "Sastry", "Shankar"], "venue": "In Decision and Control,", "year": 2003}, {"title": "Generalized principal component analysis (GPCA)", "authors": ["Vidal", "Ren\u00e9", "Ma", "Yi", "Sastry", "Shankar"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 1945}, {"title": "Generalized Principal Component Analysis", "authors": ["Vidal", "Ren\u00e9", "Ma", "Yi", "Sastry", "Shankar"], "year": 2016}, {"title": "Sparse subspace clustering with missing entries", "authors": ["Yang", "Congyuan", "Robinson", "Daniel", "Vidal", "Ren\u00e9"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "year": 2015}], "id": "SP:0ff29dc8b03957ae9d61b856a38a097e7177b27d", "authors": [{"name": "Greg Ongie", "affiliations": []}, {"name": "Rebecca Willett", "affiliations": []}, {"name": "Robert D. Nowak", "affiliations": []}, {"name": "Laura Balzano", "affiliations": []}], "abstractText": "We consider a generalization of low-rank matrix completion to the case where the data belongs to an algebraic variety, i.e., each data point is a solution to a system of polynomial equations. In this case the original matrix is possibly high-rank, but it becomes low-rank after mapping each column to a higher dimensional space of monomial features. Many well-studied extensions of linear models, including affine subspaces and their union, can be described by a variety model, as well as a rich class of nonlinear quadratic and higher degree curves and surfaces. We study the sampling requirements for matrix completion under a variety model with a focus on a union of affine subspaces. We also propose an efficient matrix completion algorithm that minimizes a convex or non-convex surrogate of the rank of the matrix of monomial features, using the wellknown \u201ckernel trick\u201d to avoid working directly with the high-dimensional monomial matrix. We show the proposed algorithm is able to recover synthetically generated data up to the predicted sampling complexity bounds, and outperforms standard low rank matrix completion and subspace clustering algorithms in experiments with real data.", "title": "Algebraic Variety Models for High-Rank Matrix Completion"}