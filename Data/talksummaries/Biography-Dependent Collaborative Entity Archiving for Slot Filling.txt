1	115	Towards a target entity of a specific type, such as the ones discussed in this paper, a person or an organization, the goal of entity archiving is to search and collect all relevant documents from large-scale data sets under limited prior knowledge of the entity.
2	43	We limit our study to the regular English entity archiving, in which the prior knowledge contains the com- monly used full name (formatted by English entity naming criteria) along with a gold-standard reference document, such as a news story on President “George W. Bush”.
3	62	Entity archiving plays a fundamental role in KBP tasks.
5	50	Such documents, on one hand, contain informative content on a target entity, which is extremely favorable for background knowledge extraction.
7	15	As for KBP slot filling and verification tasks (Surdeanu and Ji, 2014), the archived relevant documents to an entity provide sufficient contexts (provenances) of the concrete instances (fillers) of the entity attributes (slots).
9	13	The main challenges of entity archiving are as 665 follows: 1) it is difficult to retrieve all relevant documents through exact matching at the level of entity name, because an entity can be mentioned in various forms, such as alternate names and abbreviations; 2) in contrast, fuzzy matching introduces a large amount of noise into retrieval results (see the examples in Figure 1), although it is capable of recalling an overwhelming majority of relevant documents; 3) inadequate prior knowledge makes it difficult to generate a full profile of an entity; 4) although pseudo-feedback is helpful to enrich the prior knowledge, traditional entity profiling (e.g., bag-of-words) methods establish vague boundaries among different life slices of an entity.
15	27	Experiments show that CEA has substantial advantages over traditional retrieval methods (section 5.1).
38	15	It is worth considering that the acquired documents are not straightforwardly defined as the final entity archiving results.
41	80	In the retrieval phase, a query Q is formulated as the full name of the target entity, while a document D is represented as a string of words.
43	22	Accordingly we name such words as entity mentions.
45	18	Other commonly used preprocessing steps (stemming and lemmatization) are disabled because they may cause confusion between entity mentions and common words.
50	13	Accordingly it only acquires the documents which contain the entity mentions in the form of completely-preserved full name.
62	10	EM yields precise archiving results because the constraint conditions are helpful to reduce uncertainty in string matching.
69	32	In view of the abovementioned investigation, we partition the string matching results into two parts, exact and fuzzy ones, which are used as reliable prior knowledge (named reference source) and unrefined prior knowledge (candidate source) respectively.
70	10	Most documents in the reference are truly related to the target entity but the scale is not big (see Recall of EM in Table 3), while the candidate is full of both true answers and noise (see Precision and Recall of FM in Table 3), respectively.
81	15	Finally CEA selects all the preserved documents in the reference source as the final output.
82	12	We design a generative approach to estimate the biography-document relevance r, which calculates the conditional probability that a candidate document D generates the biography B:  ( | )r P B D (1) In total we leverage three probabilistic models for modeling B and D, including relevance model, topic model and context-level topic model.
84	57	Relevance Model (RM) Generally, Relevance Model (RM) (Huang and Croft, 2009) refers to the probability distribution over all words conditioned on their occurrences in a set of previously-known relevant documents (or high-quality pseudo-relevant documents), i.e., , ( | )w V P w R  , where V is the vocabulary, R is the document set, and P(w|R) can be estimated by TF-IDF.
88	73	The agreement can be estimated with Hellinger Distance between the models: 2 ( | ) ( ( | ) ( | )) w V H RM DM P w R P w D   (2) RM is a widely-used probabilistic model for information retrieval.
89	12	It determines the relevance of a document to an object in accordance with homogeneousness in content between the document and the relevant documents of the object.
90	68	For an entity, in our case, we generate RM on the reference source, and regard it as the probabilistic model of a macro-level all-embracing biography B over the prior knowledge R. For a candidate document D, the biography-document relevance r is measured with Hellinger Distance between RM and DM: P(B|D)=H(RM|DM).
92	37	Topic Model (TM) Empirically, RM is coarse-grained.
93	60	It mixes up different, separate and incoherent life slices of an entity.
94	14	A more serious problem is that RM assigns uneven weights to life slices, giving excessive weights to the words about the popular slices, but low or even zeroth weights to the unpopular ones.
95	49	A popular slice is defined as the slice of greater concern, which is normally frequently mentioned in the reference source, such as the slice of “the career of George W. Bush as the President” (high weight) versus “his childhood” (low weight).
96	11	As a result, the RM based biography-document relevance is only helpful to identify and recall the documents relevant to the popular slices but not to the unpopular ones.
97	33	As a modification, we employ Topic Model (TM) for biography modeling.
100	15	We leverage Latent Dirichlet Allocation (LDA) (Blei et al, 2003; Wei and Croft, 2006) for topic discovery and modeling in the reference source.
101	40	A topic is modeled as a probability distribution over all words in lexicon conditioned on the association of the words with the topic, denoted as w V  , P(w|tR), in which tR refers to a topic in the reference source, representing a life slice s. Table 5 shows partial topic models (slices) in the reference source of Mark Fisher, where the highlighted probability values by a box reveal the words that well characterize a topic (slice).
103	241	It is worth noting that those topics (tC) may represent anything, namely the slices of the target entity or namesakes, related or unrelated events, etc.
