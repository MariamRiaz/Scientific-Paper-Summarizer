0	33	Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding.
1	25	Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014).
12	52	Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)).
13	41	In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (STACKPTR).
15	45	Our STACKPTR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures.
16	48	The STACKPTR parser performs parsing in an incremental, topdown, depth-first fashion; at each step, it generates an arc by assigning a child for the head word at the top of the internal stack.
32	23	Pointer Networks (PTR-NET) (Vinyals et al., 2015) are a variety of neural network capable of learning the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence.
35	49	Formally, the words of the sentence x are fed one-by-one into the encoder (a multiple-layer bidirectional RNN), producing a sequence of encoder hidden states si.
36	43	At each time step t, the decoder (a uni-directional RNN) receives the input from last step and outputs decoder hidden state ht.
37	37	The attention vector at is calculated as follows: eti = score(ht, si) at = softmax (et) (1) where score(·, ·) is the attention scoring function, which has several variations such as dot-product, concatenation, and biaffine (Luong et al., 2015).
39	32	Similarly to PTR-NET, STACKPTR first reads the whole sentence and encodes each word into the encoder hidden state si.
40	26	The internal stack σ is always initialized with the root symbol $.
41	105	At each time step t, the decoder receives the input vector corresponding to the top element of the stack σ (the head wordwp where p is the word index), generates the hidden state ht, and computes the attention vector at using Eq.
44	72	At one step if the parser points wh to itself, i.e. c = h, it indicates that all children of the head word wh have already been selected.
45	55	Then the parser goes to the next step by popping wh out of σ.
46	41	At test time, in order to guarantee a valid dependency tree containing all the words in the input sentences exactly once, the decoder maintains a list of “available” words.
51	19	In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details).
76	50	( Z+ &$ , µ To utilize higher-order information, the decoder’s input at each step is the sum of the encoder hidden states of three ords: βt = sh + sg + ss where βt is the input vector of decoder at time t and h, g, s are the indices of the head word and its grandparent and sibling, respectively.
92	24	Following Dozat and Manning (2017), the classifier takes the information of the head word and its child as features.
122	35	For each experiment, we report the mean values with corresponding standard deviations over 5 repetitions.
123	34	Baseline For fair comparison of the parsing performance, we re-implemented the graph-based Deep Biaffine (BIAF) parser (Dozat and Manning, 2017), which achieved state-of-the-art results on a wide range of languages.
126	31	We compare the performance of four variations of our model with different decoder inputs — Org, +gpar, +sib and Full — where the Org model utilizes only the encoder hidden states of head words, while the +gpar and +sib models augments the original one with grandparent and sibling information, respectively.
128	19	Figure 2 illustrates the performance (five metrics) of different variations of our STACKPTR parser together with the results of baseline BIAF re-implemented by us, on the test sets of the three languages.
133	82	The results of our parser on RA are slightly worse than BIAF.
147	28	While the graph-based BIAF parser still performs better for longer dependency arcs and transition-based STACKPTR parser does better for shorter ones, the gap between the two systems is marginal, much smaller than that shown in McDonald and Nivre (2011).
155	43	We run experiments on PTB using our STACKPTR parser with gold-standard and predicted POS tags, and without tags, respectively.
174	63	On Italian and Romanian, BIAF obtains marginally better parsing performance than STACKPTR.
175	27	In this paper, we proposed STACKPTR, a transition-based neural network architecture, for dependency parsing.
179	168	First, we intend to consider how to conduct experiments to improve the analysis of parsing errors qualitatively and quantitatively.
180	89	Another interesting direction is to further improve our model by exploring reinforcement learning approaches to learn an optimal order for the children of head words, instead of using a predefined fixed order.
