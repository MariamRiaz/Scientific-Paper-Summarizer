0,1,label2,summary_sentences
"ar X
iv :1
80 2.
06 09
3v 4
[ cs
.L G",text,[0],[0]
"Residual networks (He et al., 2016) are deep neural networks in which, roughly, subnetworks determine how a feature transformation should differ from the identity, rather than how it should differ from zero.",1 Introduction,[0],[0]
"After enabling the winning entry in the ILSVRC 2015 classification task, they have become established as a central idea in deep networks.
",1 Introduction,[0],[0]
Hardt & Ma (2017) provided a theoretical analysis that shed light on residual networks.,1 Introduction,[0],[0]
"They showed that (a) any linear transformation with a positive determinant and a bounded condition number can be approximated by a “deep linear network” of the form f(x) = ΘLΘL−1...Θ1x, where,
for large L, each layer Θi is close to the identity, and (b) for networks that compose near-identity transformations this way, if the excess loss is large, then the gradient is steep.",1 Introduction,[0],[0]
"Bartlett et al. (2018) extended both results to the nonlinear case, showing that any smooth, bi-Lipschitz map can be represented as a composition of near-identity functions, and that a suboptimal loss in a composition of near-identity functions implies that the functional gradient of the loss with respect to a function in the composition cannot be small.",1 Introduction,[0],[0]
"These results are interesting because they suggest that, in many cases, this non-convex objective may be efficiently optimized through gradient descent if the layers stay close to the identity, possibly with the help of a regularizer.
",1 Introduction,[0],[0]
"This paper describes and analyzes such algorithms for linear regression with d input variables and d response variables with respect to the quadratic loss, the same setting analyzed by Hardt and Ma.",1 Introduction,[0],[0]
We abstract away sampling issues by analyzing an algorithm that performs gradient descent with respect to the population loss.,1 Introduction,[0],[0]
We focus on the case that the distribution on the input patterns is isotropic.,1 Introduction,[0],[0]
(The data may be transformed through a preprocessing step to satisfy this constraint.),1 Introduction,[0],[0]
"
",1 Introduction,[0],[0]
"The traditional analysis of convex optimization algorithms (see Boyd & Vandenberghe, 2004) provides a bound in terms of the quality of the initial solution, together with bounds on the eigenvalues of the Hessian of the loss.",1 Introduction,[0],[0]
"For the non-convex problem of this paper, we show that if gradient descent starts at the identity in each layer, and if the excess loss of that initial solution is bounded by a constant, then the Hessian remains well-conditioned enough throughout training for successful learning.",1 Introduction,[0],[0]
"Specifically, there is a constant c0 such that, if the excess loss of the identity (over the least squares linear map) is at most c0, then back-propagation initialized at the identity in each layer achieves loss within at most ǫ of optimal in time polynomial in log(1/ǫ), d, and L (Section 3).",1 Introduction,[0],[0]
"On the other hand, we show that there is a constant c1 and a least squares matrix Φ such that the identity has excess loss c1 with respect to Φ, but backpropagation with identity initialization fails to learn Φ (Section 6).
",1 Introduction,[0],[0]
"We also show that if the least squares matrix Φ is symmetric positive definite then gradient descent with identity initialization achieves excess loss at most ǫ in a number of steps bounded by a polynomial in log(d/ǫ), L and the condition number of Φ (Section 4).
",1 Introduction,[0],[0]
"In contrast, for any least squares matrix Φ that is symmetric but has a negative eigenvalue, we show that no such guarantee is possible for a wide variety of algorithms of this type: the excess loss is forever bounded below by the square of this negative eigenvalue.",1 Introduction,[0],[0]
"This holds for step-and-project algorithms, and also algorithms that initialize to the identity and regularize by early stopping or penalizing ∑
i ||Θi − I|| 2 F (Section 6).",1 Introduction,[0],[0]
"Both this and the previous impossibility result can be
proved using a least squares matrix Φ with a positive determinant and a good condition number.",1 Introduction,[0],[0]
"Recall that such Φ were proved by Hardt and Ma to have a good approximation as a product of near-identity matrices – we prove that gradient descent cannot learn them, even with the help of regularizers that reward near-identity representations.
",1 Introduction,[0],[0]
"In Section 5 we provide a convergence guarantee for a least squares matrix Φ that may not be symmetric, but satisfies the positivity condition u⊤Φu",1 Introduction,[0],[0]
> γ for some γ > 0,1 Introduction,[0],[0]
that appears in the bounds.,1 Introduction,[0],[0]
We call such matrices γ-positive.,1 Introduction,[0],[0]
Such Φ include rotations by acute angles.,1 Introduction,[0],[0]
"In this case, we consider an algorithm that regularizes in addition to a near-identity initialization.",1 Introduction,[0],[0]
"After the gradient update, the algorithm performs what we call power projection, projecting its hypothesis ΘLΘL−1...Θ1 onto the set of γ-positive matrices.",1 Introduction,[0],[0]
"Second, it “balances” Θ1, ...,ΘL so that, informally, they contribute equally to ΘLΘL−1...Θ1.",1 Introduction,[0],[0]
(See Section 5 for the details.),1 Introduction,[0],[0]
"We view this
regularizer as a theoretically tractable proxy for regularizers that promote positivity and balance between layers by adding penalties.
",1 Introduction,[0],[0]
"While, in practice, deep networks are non-linear, analysis of the linear case can provide a tractable way to gain insight through rigorous theoretical analysis (Saxe et al., 2013; Kawaguchi, 2016; Hardt & Ma, 2017).",1 Introduction,[0],[0]
We might view back-propagation in the non-linear case as an approximation to a procedure that locally modifies the function computed by each layer in a manner that reduces the loss as fast as possible.,1 Introduction,[0],[0]
"If a non-linear network is obtained by composing transformations, each of which is chosen from a Hilbert space of functions (as in Daniely et al. (2016)), then a step in “function space” corresponds to a step in an (infinite-dimensional) linear space of functions.
",1 Introduction,[0],[0]
Related work.,1 Introduction,[0],[0]
The motivation for this work comes from the papers of Hardt & Ma (2017) and Bartlett et al. (2018).,1 Introduction,[0],[0]
Saxe et al. (2013) studied the dynamics of a continuous-time process obtained by taking the step size of backpropagation applied to deep linear neural networks to zero.,1 Introduction,[0],[0]
Kawaguchi (2016) showed that deep linear neural networks have no suboptimal local minima.,1 Introduction,[0],[0]
"In the case that L = 2, the problem studied here has a similar structure as problems arising from low-rank approximation of matrices, especially as regards algorithms that approximate a matrix A by iteratively improving an approximation of the form UV .",1 Introduction,[0],[0]
"For an interesting survey on the rich literature on these algorithms, please see Ge et al. (2017a); successful algorithms have included a regularizer that promotes balance in the sizes of U and V .",1 Introduction,[0],[0]
"Taghvaei et al. (2017) studied the properties of critical points on the loss when learning deep linear neural networks in the presence of a weight decay regularizer; they studied networks that transform the input to the output through a process indexed by a continuous variable, instead of through discrete layers.",1 Introduction,[0],[0]
"Lee et al. (2016) showed that, given regularity conditions, for a random initialization, gradient descent converges to a local minimizer almost surely; while their paper yields useful insights, their regularity condition does not hold for our problem.",1 Introduction,[0],[0]
Many papers have analyzed learning of neural networks with non-linearities.,1 Introduction,[0],[0]
The papers most closely related to this work analyze algorithms based on gradient descent.,1 Introduction,[0],[0]
"Some of these (Andoni et al., 2014; Brutzkus & Globerson, 2017; Ge et al., 2017b; Li & Yuan, 2017; Zhong et al., 2017; Zhang et al., 2018; Brutzkus et al., 2018; Ge et al., 2018) analyze constant-depth networks.",1 Introduction,[0],[0]
Daniely (2017) showed that stochastic gradient descent learns a subclass of functions computed by log-depth networks in polynomial time; this class includes constant-degree polynomials with polynomially bounded coefficients.,1 Introduction,[0],[0]
"Other theoretical treatments of neural network learning algorithms include Lee et al. (1996); Arora et al. (2014); Livni et al. (2014); Janzamin et al. (2015); Safran & Shamir (2016); Zhang et al. (2016); Nguyen & Hein (2017); Zhang et al. (2017); Orhan & Pitkow (2018), although these are less closely related.
",1 Introduction,[0],[0]
Our three upper bound analyses combine a new upper bound on the operator norm of the Hessian of a deep linear network with the result of Hardt and Ma that gradients are lower bounded in terms of the loss for near-identity matrices.,1 Introduction,[0],[0]
They otherwise have different outlines.,1 Introduction,[0],[0]
The bound in terms of the loss of the initial solution proceeds by showing that the distance from each layer to the identity grows slowly enough that the loss is reduced before the layers stray far enough to harm the conditioning of the Hessian.,1 Introduction,[0],[0]
"The bound for symmetric positive definite matrices proceeds by showing that, in this case, all of the layers are the same, and each of their eigenvalues converges to the Lth root of a corresponding eigenvalue of Φ. As mentioned above, the bound for γ-positive matrices Φ is for an algorithm that achieves favorable conditioning through regularization.
",1 Introduction,[0],[0]
"We expect that the theoretical analysis reported here will inform the design of practical algorithms
for learning non-linear deep networks.",1 Introduction,[0],[0]
One potential avenue for this arises from the fact that the leverage provided by regularizing toward the identity appears to already be provided by a weaker policy of promoting the property that the composition of layers is (potentially asymmetric) positive definite.,1 Introduction,[0],[0]
"Also, balancing singular values of the layers of the network aided our analysis; an analogous balancing of Jacobians associated with various layers may improve conditioning in practice in the non-linear case.",1 Introduction,[0],[0]
"For a joint distribution P with support contained in ℜd × ℜd and g : ℜd → ℜd, define ℓP (g) = E(X,Y )∼P (||g(X)",2.1 Setting,[0],[0]
− Y || 2/2).,2.1 Setting,[0],[0]
"We focus on the case that, for (X,Y ) drawn from P , the marginal on X is isotropic, with",2.1 Setting,[0],[0]
EXX⊤ = Id.,2.1 Setting,[0],[0]
"For convenience, we assume that Y = ΦX for Φ ∈ ℜd×d.",2.1 Setting,[0],[0]
This assumption is without loss of generality: if Φ is the least squares matrix (so that f defined by f(X) = ΦX minimizes ℓP,2.1 Setting,[0],[0]
"(f) among linear functions), for any linear g we have
ℓP (g) =",2.1 Setting,[0],[0]
E‖g(X),2.1 Setting,[0],[0]
− f(X)‖ 2/2 + E‖f(X)−,2.1 Setting,[0],[0]
"Y ‖2/2
+ E",2.1 Setting,[0],[0]
((g(X),2.1 Setting,[0],[0]
− f(X))(f(X),2.1 Setting,[0],[0]
"− Y ))
= E‖g(X)",2.1 Setting,[0],[0]
− f(X)‖2/2 +,2.1 Setting,[0],[0]
"E‖f(X)− Y ‖2/2
= E‖g(X)",2.1 Setting,[0],[0]
− ΦX)‖2/2 + E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2,
since f is the projection of Y onto the set of linear functions ofX.",2.1 Setting,[0],[0]
"So assuming Y = ΦX corresponds to setting Φ as the least squares matrix and replacing the loss ℓP (g) by the excess loss
E‖g(X)",2.1 Setting,[0],[0]
− ΦX‖2/2 = E‖g(X),2.1 Setting,[0],[0]
− Y ‖2/2−,2.1 Setting,[0],[0]
E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2.
",2.1 Setting,[0],[0]
We study algorithms that learn linear mappings parameterized by deep networks.,2.1 Setting,[0],[0]
"The network with L layers and parameters Θ = (Θ1, . . .",2.1 Setting,[0],[0]
",ΘL) computes the parameterized function fΘ(x) = ΘLΘL−1 · · ·Θ1x, where",2.1 Setting,[0],[0]
"x ∈ ℜd and Θi ∈ ℜd×d.
",2.1 Setting,[0],[0]
"We use the notation Θi:j = ΘjΘj−1 · · ·Θi for i ≤ j, so that we can write fΘ(x) = Θ1:Lx = Θi+1:LΘiΘ1:i−1x.
",2.1 Setting,[0],[0]
"When there is no possibility of confusion, we will sometimes refer to loss ℓ(fΘ) simply as ℓ(Θ).",2.1 Setting,[0],[0]
"Because the distribution of X is isotropic, ℓ(Θ) = 12 ||Θ1:L − Φ|| 2 F with respect to least squares matrix Φ. When Θ is produced by an iterative algorithm, will we also refer to loss of the tth iterate by ℓ(t).
",2.1 Setting,[0],[0]
Definition 1.,2.1 Setting,[0],[0]
"For γ > 0, a matrix A ∈ ℜd×d is γ-positive if, for all unit length u, we have u⊤Au > γ.",2.1 Setting,[0],[0]
"We use ||A||F for the Frobenius norm of matrix A, ||A||2 for its operator norm, and σmin(A) for its least singular value.",2.2 Tools and background,[0],[0]
"For vector v, we use ||v|| for its Euclidian norm.
",2.2 Tools and background,[0],[0]
"For a matrix A and a matrix-valued function B, define DAB(A) to be the matrix with
(DAB(A))i,j = ∂vec(B(A))i ∂vec(A)j ,
where vec(A) is the column vector constructed by stacking the columns of A. We use Td,d to denote the d2 × d2 permutation matrix mapping vec(A) to vec(A⊤) for A ∈ ℜd×d.",2.2 Tools and background,[0],[0]
"For A ∈ ℜn×m and B ∈ ℜp×q, A⊗B denotes the Kronecker product, that is, the np×mq matrix of n×m blocks, with the i, jth block given by AijB.
We will need the gradient and Hessian of ℓ. (The gradient, which can be computed using backprop, is of course well known.)",2.2 Tools and background,[0],[0]
"The proof is in Appendix A.
Lemma 1.
DΘiℓ (fΘ)=(vec(Id))",2.2 Tools and background,[0],[0]
"⊤ (( Θ⊤1:i−1 ⊗ (Θ1:L−Φ) ⊤Θi+1:L ))
",2.2 Tools and background,[0],[0]
"= vec(G)⊤,
where G is the d× d matrix given by
G def = Θ⊤i+1:",2.2 Tools and background,[0],[0]
"L (Θ1:L − Φ)Θ ⊤ 1:i−1. (1)
",2.2 Tools and background,[0],[0]
"For i < j,
DΘjDΘiℓ (fΘ) =",2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
",2.2 Tools and background,[0],[0]
"((Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1)Td,d + (Θ ⊤ i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L)).
DΘiDΘiℓ (fΘ)",2.2 Tools and background,[0],[0]
=,2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",2.2 Tools and background,[0],[0]
"Td,d.",2.2 Tools and background,[0],[0]
"In this section, we prove an upper bound for gradient descent in terms of the loss of the initial solution.",3 Targets near the identity,[0],[0]
"First, set Θ(0) =",3.1 Procedure and upper bound,[0],[0]
"(I, I, ..., I), and then iteratively update
Θ (t+1)",3.1 Procedure and upper bound,[0],[0]
i = Θ,3.1 Procedure and upper bound,[0],[0]
(t),3.1 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",3.1 Procedure and upper bound,[0],[0]
"L − Φ
)
",3.1 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
Theorem 1.",3.1 Procedure and upper bound,[0],[0]
"There are positive constants c1 and c2 and polynomials p1 and p2 such that, if ℓ(Θ (0) 1:L) ≤ c1, L ≥ c2, and η ≤ 1 p1(L,d,||Φ||2) , then the above gradient descent procedure achieves
ℓ(fΘ(t)) ≤ ǫ within t = p2
(
1 η
)",3.1 Procedure and upper bound,[0],[0]
"ln (
ℓ(0) ǫ
)
iterations.",3.1 Procedure and upper bound,[0],[0]
"The following lemma, which is implicit in the proof of Theorem 2.2 in Hardt & Ma (2017), shows that the gradient is steep if the loss is large and the singular values of the layers are not too small.
",3.2 Proof of Theorem 1,[0],[0]
Lemma 2 (Hardt & Ma 2017).,3.2 Proof of Theorem 1,[0],[0]
Let ∇Θℓ(Θ) be the gradient of ℓ(Θ) with respect to any flattening of Θ.,3.2 Proof of Theorem 1,[0],[0]
"If, for all layers i, σmin(Θi) ≥ 1− a, then ||∇Θℓ(Θ)|| 2 ≥ 4ℓ(Θ)L(1− a)2L.
Next, we show that, if Θ(t) and Θ(t+1) are both close to the identity, then the gradient is not changing very fast between them, so that rapid progress continues to be made.",3.2 Proof of Theorem 1,[0],[0]
"We prove this through an upper bound on the operator norm of the Hessian that holds uniformly over members of a ball around the identity, which in turn can be obtained through a bound on the Frobenius norm.",3.2 Proof of Theorem 1,[0],[0]
"The proof is in Appendix B.
Lemma 3.",3.2 Proof of Theorem 1,[0],[0]
"Choose an arbitrary Θ with ||Θi||2 ≤ 1 + z for all i, and least squares matrix Φ with ||Φ||2 ≤ (1 + z)
L.",3.2 Proof of Theorem 1,[0],[0]
"Let ∇2 be the Hessian of ℓ(fΘ) with respect to an arbitrary flattening of the parameters of Θ. We have
||∇2||F ≤ 3Ld 5(1 + z)2L.
Armed with Lemmas 2 and 3, let us now analyze gradient descent.",3.2 Proof of Theorem 1,[0],[0]
"Very roughly, our strategy will be to show that the distance from the identity to the various layers grows slowly enough for the leverage from Lemmas 2 and 3 to enable successful learning.",3.2 Proof of Theorem 1,[0],[0]
Let R(Θ) = maxi ||Θi − I||2.,3.2 Proof of Theorem 1,[0],[0]
"From the update, we have
||Θ (t+1)",3.2 Proof of Theorem 1,[0],[0]
i − I||2 ≤ ||Θ (t) i,3.2 Proof of Theorem 1,[0],[0]
"− I||2 + η||(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤||2
≤ ||Θ (t) i",3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(Θ (t)))L||Θ (t) 1:L − Φ||2 ≤ ||Θ,3.2 Proof of Theorem 1,[0],[0]
(t),3.2 Proof of Theorem 1,[0],[0]
i,3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
"+R(Θ (t)))L||Θ (t) 1:L − Φ||F .
",3.2 Proof of Theorem 1,[0],[0]
If R(t) = maxs≤tR(Θ(s)),3.2 Proof of Theorem 1,[0],[0]
(so R(0) = 0) and ℓ(t) = 12 ||Θ (t) 1:,3.2 Proof of Theorem 1,[0],[0]
"L − Φ|| 2 F , this implies
R(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(t))L √ 2ℓ(t).,3.2 Proof of Theorem 1,[0],[0]
"(2)
By Lemma 3, for all Θ on the line segment from Θ(t) to Θ(t+1), we have
||∇2Θ||2 ≤ ||∇ 2 Θ||F ≤",3.2 Proof of Theorem 1,[0],[0]
"3Ld 5 max{(1 +R(t+ 1))2L, ||Φ||22},
so that
ℓ(t+ 1) ≤ ℓ(t)− η||∇Θ(t)",3.2 Proof of Theorem 1,[0],[0]
||,3.2 Proof of Theorem 1,[0],[0]
"2 +
3 2",3.2 Proof of Theorem 1,[0],[0]
η2Ld5 max{(1,3.2 Proof of Theorem 1,[0],[0]
"+R(t+ 1))2L, ||Φ||22}||∇Θ(t) || 2.
",3.2 Proof of Theorem 1,[0],[0]
"Thus, if we ensure
η ≤ 1
3Ld5 max{(1 +R(t+ 1))2L, ||Φ||22} , (3)
we have ℓ(t+ 1) ≤ ℓ(t)− (η/2)||∇Θ(t) || 2, which, using Lemma 2, gives
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).",3.2 Proof of Theorem 1,[0],[0]
"(4)
Pick any c ≥ 1.",3.2 Proof of Theorem 1,[0],[0]
"Assume that L ≥ (4/3) ln c = c2, ℓ(Θ (0) 1:L) ≤
ln(c)2
8c10 = c1 and η ≤ 1
3Ld5 max{c4,||Φ||22} .
",3.2 Proof of Theorem 1,[0],[0]
"We claim that, for all t ≥ 0,
1. R(t) ≤ ηc √ 2ℓ(0) ∑ 0≤s<t exp ( − sηLc4 )
2.",3.2 Proof of Theorem 1,[0],[0]
"ℓ(t) ≤ ( exp (
−2tηL c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"The base case holds as R(0) = 0 and ℓ(0) = ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Before starting the inductive step, notice that for any t ≥ 0,
ηc √ 2ℓ(0) ∑
0≤s<t exp
(
− sηL
c4
)
≤ ηc √ 2ℓ(0)× 1
1− exp (
−ηL c4
)
≤ ηc √ 2ℓ(0)× 2c4
ηL (since ηLc4 ≤ 1)
= 2c5 √ 2ℓ(0)
L ≤
ln c
L ≤ 3/4
where the last two inequalities follow from the constraints on ℓ(0) and L.
Using (2),
R(t+ 1) ≤ R(t) + η(1",3.2 Proof of Theorem 1,[0],[0]
"+R(t))L √ 2ℓ(t)
≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
"+ η
(
1 + ln c
L
)L √
2ℓ(t)
≤ R(t) + ηc √ 2ℓ(t)
≤ R(t) + ηc √ 2ℓ(0) exp
(
− tηL
c4
)
≤ ηc √ 2ℓ(0)",3.2 Proof of Theorem 1,[0],[0]
"∑
0≤s<t+1 exp
(
− sηL
c4
)
.
",3.2 Proof of Theorem 1,[0],[0]
"Since R(t+ 1) ≤ ln cL , the choice of η satisfies (3), so
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).
",3.2 Proof of Theorem 1,[0],[0]
"Now consider (1−R(t))2L:
ln ( (1−R(t))2L )",3.2 Proof of Theorem 1,[0],[0]
"= 2L ln(1−R(t))
",3.2 Proof of Theorem 1,[0],[0]
≥ 2L(−2R(t)),3.2 Proof of Theorem 1,[0],[0]
since R(t) ∈,3.2 Proof of Theorem 1,[0],[0]
"[0, 3/4]
≥ 2L
(
−2 ln c
L
)
since R(t) ≤",3.2 Proof of Theorem 1,[0],[0]
ln,3.2 Proof of Theorem 1,[0],[0]
"c
L
(1−R(t))2L ≥ 1/c4.
",3.2 Proof of Theorem 1,[0],[0]
"Using this in the bound on ℓ(t+ 1):
ℓ(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
"( 1− 2ηL(1 −R(t))2L ) ℓ(t)
≤
(
1− 2ηL
c4
)
ℓ(t)
≤
(
exp
(
− 2ηL
c4
))",3.2 Proof of Theorem 1,[0],[0]
"(
exp
(
− 2tηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0)
",3.2 Proof of Theorem 1,[0],[0]
"=
(
exp
(
− 2(t+ 1)ηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Solving ℓ(0) exp (
−2tηL c4
)
≤ ǫ for t and recalling that η < 1/c4 completes the proof of the theorem.",3.2 Proof of Theorem 1,[0],[0]
"In this section, we analyze the procedure of Section 3.1 when the least squares matrix Φ is symmetric and positive definite.
Theorem 2.",4 Symmetric positive definite targets,[0],[0]
"There is an absolute positive constant c3 such that, if Φ is symmetric and γ-positive with 0 <",4 Symmetric positive definite targets,[0],[0]
"γ < 1, and L ≥ c3 ln (||Φ||2/γ), then for all η ≤
1 L(1+||Φ||22) , gradient descent achieves
ℓ(fΘ(t))",4 Symmetric positive definite targets,[0],[0]
"≤ ǫ in poly(L, ||Φ||2/γ, 1/η) log(d/ǫ) iterations.
",4 Symmetric positive definite targets,[0],[0]
Note that a symmetric matrix is γ-positive when its minimum eigenvalue is at least γ.,4 Symmetric positive definite targets,[0],[0]
"Let Φ be a symmetric, real, γ-positive matrix with γ > 0, and let Θ(0),Θ(1), ... be the iterates of gradient descent with a step size 0 < η ≤ 1
L(1+||Φ||22) .
",4.1 Proof of Theorem 2,[0],[0]
Definition 2.,4.1 Proof of Theorem 2,[0],[0]
"Symmetric matrices A ⊆ ℜd×d are commuting normal matrices if there is a single unitary matrix U such that for all A ∈ A, U⊤AU is diagonal.
",4.1 Proof of Theorem 2,[0],[0]
"We will use the following well-known facts about commuting normal matrices.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 4 (Horn & Johnson 2013),4.1 Proof of Theorem 2,[0],[0]
.,4.1 Proof of Theorem 2,[0],[0]
"If A ⊆ ℜd×d is a set of symmetric commuting normal matrices and A,B ∈ A, the following hold:
• AB = BA;
",4.1 Proof of Theorem 2,[0],[0]
"• for all scalars α and β, A ∪ {αA+ βB,AB} are commuting normal;
• there is a unitary matrix U such that U⊤AU and U⊤BU are real and diagonal;
• the multiset of singular values of A is the same as the multiset of magnitudes of its eigenvalues;
• ||A− I||2 is the largest value of |z",4.1 Proof of Theorem 2,[0],[0]
"− 1| for an eigenvalue z of A.
Lemma 5.",4.1 Proof of Theorem 2,[0],[0]
"The matrices {Φ} ∪ {Θ (t) i : i ∈ {1, ..., L}, t ∈ Z +} are commuting normal.",4.1 Proof of Theorem 2,[0],[0]
"For all t, Θ (t) 1 = ... = Θ (t) L .
Proof.",4.1 Proof of Theorem 2,[0],[0]
The proof is by induction.,4.1 Proof of Theorem 2,[0],[0]
"The base case follows from the fact that Φ and I are commuting normal.
",4.1 Proof of Theorem 2,[0],[0]
"For the induction step, the fact that
{Φ} ∪ {
Θ (s)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
} ∪ {
Θ (s+1)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
}
are commuting normal follows from Lemma 4.",4.1 Proof of Theorem 2,[0],[0]
"The update formula now reveals that Θ (t+1) 1 = ... = Θ (t+1) L .
",4.1 Proof of Theorem 2,[0],[0]
Now we are ready to analyze the dynamics of the learning process.,4.1 Proof of Theorem 2,[0],[0]
"Let Φ = U⊤DLU be a diagonalization of Φ. Let Γ = max{1, ||Φ||2}.",4.1 Proof of Theorem 2,[0],[0]
"We next describe a sense in which gradient descent learns each eigenvalue independently.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 6.,4.1 Proof of Theorem 2,[0],[0]
"For each t, there is a real diagonal matrix D̂(t) such that, for all i, Θ (t)",4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U and
D̂(t+1) = D̂(t)",4.1 Proof of Theorem 2,[0],[0]
− η(D̂(t))L−1((D̂(t))L −DL).,4.1 Proof of Theorem 2,[0],[0]
"(5)
Proof.",4.1 Proof of Theorem 2,[0],[0]
Lemma 5 implies that there is a single real U such that Θ (t),4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U for all i. Applying Lemma 1, recalling that Θ (t) 1 = ...",4.1 Proof of Theorem 2,[0],[0]
"= Θ (t) L , and applying the fact that Θ (t) i and Φ commute, we get
Θ (t+1)",4.1 Proof of Theorem 2,[0],[0]
i = Θ,4.1 Proof of Theorem 2,[0],[0]
(t),4.1 Proof of Theorem 2,[0],[0]
"i − η(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L−1 (
(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L − Φ ) .
",4.1 Proof of Theorem 2,[0],[0]
"Replacing each matrix by its diagonalization, we get
U⊤D̂(t+1)U = U⊤D̂(t)U",4.1 Proof of Theorem 2,[0],[0]
"− η(U⊤(D̂(t))L−1U) ( U⊤(D̂(t))LU − U⊤DLU )
",4.1 Proof of Theorem 2,[0],[0]
"= U⊤D̂(t)U − ηU⊤(D̂(t))L−1 ( (D̂(t))L −DL ) U,
and left-multiplying by U and right-multiplying by U⊤ gives (5).
",4.1 Proof of Theorem 2,[0],[0]
We will now analyze the convergence of each D̂ (t) kk to Dkk separately.,4.1 Proof of Theorem 2,[0],[0]
"Let us focus for now on an arbitrary single index k, let λ = Dkk and λ̂",4.1 Proof of Theorem 2,[0],[0]
(t) =,4.1 Proof of Theorem 2,[0],[0]
"D̂ (t) kk .
",4.1 Proof of Theorem 2,[0],[0]
Recalling that ||Φ||2 ≤,4.1 Proof of Theorem 2,[0],[0]
"Γ, we have γ 1/L ≤ λ ≤",4.1 Proof of Theorem 2,[0],[0]
"Γ1/L. Also, Γ1/L = e 1 L ln Γ ≤ e1/a ≤ 1+2/a whenever a ≥ 1 and L ≥ a ln Γ. Similarly, γ1/L ≥ 1 − a whenever L ≥ a ln(1/γ).",4.1 Proof of Theorem 2,[0],[0]
"Thus, there are absolute constants c3 and c4 such that",4.1 Proof of Theorem 2,[0],[0]
|1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ)
L < 1 for all L ≥ c3 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"We claim that, for all t, λ̂(t) lies between 1 and λ inclusive, so that |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
The base case holds because λ̂(t) = 1 and |1,4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
Now let us work on the induction step.,4.1 Proof of Theorem 2,[0],[0]
"Applying (5) together with Lemma 1, we get
λ̂(t+1) = λ̂(t) +",4.1 Proof of Theorem 2,[0],[0]
η(λ̂(t))L−1(λL,4.1 Proof of Theorem 2,[0],[0]
− (λ̂(t))L).,4.1 Proof of Theorem 2,[0],[0]
"(6)
By the induction hypothesis, we just need to show that sign(λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)),4.1 Proof of Theorem 2,[0],[0]
= sign(λ − λ̂(t)) and |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)| (i.e., the step is in the correct direction, and does not “overshoot”).",4.1 Proof of Theorem 2,[0],[0]
"First, to see that the step is in the right direction, note that λL ≥ (λ̂(t))L if and only if λ ≥ (λ̂(t)), and the inductive hypothesis implies that λ̂(t), and therefore (λ̂(t))L−1, is non-negative.",4.1 Proof of Theorem 2,[0],[0]
To show that |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|, it suffices to show that η(λ̂(t))L−1 ∣ ∣ ∣ λL − (λ̂(t))L) ∣ ∣ ∣ ≤ |λ",4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|,
which, in turn would be implied by η ≤
∣ ∣ ∣ ∣ 1 (λ̂(t))L−1( ∑L−1 i=0 (λ̂ (t))iλL−1−i) ∣ ∣ ∣ ∣ (since λL − (λ̂(t))L = (λ −
λ̂(t))",4.1 Proof of Theorem 2,[0],[0]
"∑L−1 i=0 (λ̂ (t))iλL−1−i), which follows from the inductive hypothesis and η ≤ 1 LΓ2 .",4.1 Proof of Theorem 2,[0],[0]
"We have proved that each λ̂(t) lies between λ and 1, so that |1−",4.1 Proof of Theorem 2,[0],[0]
λ̂(t)| ≤ |1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"Now, since the step is in the right direction, and does not overshoot,
|λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ |λ̂(t) − λ| − η(λ̂(t))L−1|λL,4.1 Proof of Theorem 2,[0],[0]
"− (λ̂(t))L|
≤ |λ̂(t) − λ|
( 1− η(λ̂(t))L−1 ( L−1 ∑
i=0
(λ̂(t))iλL−1−i ))
≤ |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
"− λ| ( 1− ηLγ2 ) ,
since the fact that λ̂(t) lies between 1 and λ implies that λ̂(t) ≥ γ1/L. Thus, |λ̂(t) − λ| ≤ (
1− ηLγ2 )t c4",4.1 Proof of Theorem 2,[0],[0]
ln(Γ/γ).,4.1 Proof of Theorem 2,[0],[0]
"This implies that, for any ǫ ∈ (0, 1), for any absolute constant c5, there is a
constant c6 such that, after c6 1 ηLγ2 ln ( dL lnΓ γǫ )
steps, we have |λ̂(t)−λ| ≤ c5γ √ ǫ
LΓ √ d .Writing",4.1 Proof of Theorem 2,[0],[0]
"r = λ̂(t)−λ,
this implies, if c5 is small enough, that
((λ̂(t))L − λL)2 = ((λ+r)L−λL)2
≤ Γ2",4.1 Proof of Theorem 2,[0],[0]
"( ( 1+ r
λ
)L −1
)2
≤ Γ2 ( 2c5rL
λ
)2
≤ Γ2 ( 2c5rL
γ
)2
≤ ǫ
d .
",4.1 Proof of Theorem 2,[0],[0]
"Thus, after O (
1 ηLγ2
ln (
dL lnΓ γǫ
))
steps, (Dkk − D̂ (t) kk ) 2 ≤ ǫ/d for all k, and therefore ℓ(Θ(t))",4.1 Proof of Theorem 2,[0],[0]
"≤ ǫ,
completing the proof.",4.1 Proof of Theorem 2,[0],[0]
"We have seen that if the least squares matrix is symmetric, γ-positivity is sufficient for convergence of gradient descent.",5 Asymmetric positive definite matrices,[0],[0]
We shall see in Section 6 that positivity is also necessary for a broad family of gradient-based algorithms to converge to the optimal solution when the least squares matrix is symmetric.,5 Asymmetric positive definite matrices,[0],[0]
"Thus, in the symmetric case, positivity characterizes the success of gradient methods.
",5 Asymmetric positive definite matrices,[0],[0]
"In this section, we show that positivity suffices for the convergence of a gradient method even without the assumption that the least squares matrix is symmetric.
",5 Asymmetric positive definite matrices,[0],[0]
Note that the set of γ-positive (but not necessarily symmetric) matrices includes both rotations by an acute angle and “partial reflections” of the form ax + b refl(x) where refl(·) is a lengthpreserving reflection and 0 ≤,5 Asymmetric positive definite matrices,[0],[0]
|b| < a.,5 Asymmetric positive definite matrices,[0],[0]
"Since ( u⊤Au )⊤
= u⊤A⊤u, a matrix A is γ-positive if",5 Asymmetric positive definite matrices,[0],[0]
"and only if u⊤(A+A⊤)u ≥ 2γ for all unit length u, i.e. A+A⊤ is positive definite with eigenvalues at least 2γ.",5 Asymmetric positive definite matrices,[0],[0]
"The algorithm analyzed in this section uses a construction that is new, as far as we know, that we call a balanced factorization.",5.1 Balanced factorizations,[0],[0]
"This factorization may be of independent interest.
",5.1 Balanced factorizations,[0],[0]
Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .,5.1 Balanced factorizations,[0],[0]
The principal Lth root of a complex number whose expression in polar coordinates is reθi is r1/Leθi/L.,5.1 Balanced factorizations,[0],[0]
"The principal Lth root of a matrix A is the matrix B such that BL = A, and each eigenvalue of B is the principal Lth root of the corresponding eigenvalue of A.
Definition 3.",5.1 Balanced factorizations,[0],[0]
"If A be a matrix with polar decomposition RP , then A has the balanced factorization A = A1, ..., AL where for each i,
Ai = R 1/LPi, with Pi = R (L−i)/LP 1/LR−(L−i)/L,
and each of the Lth roots is the principal Lth root.
",5.1 Balanced factorizations,[0],[0]
The motivation for balanced factorization is as follows.,5.1 Balanced factorizations,[0],[0]
"We want each factor to do a 1/L fraction of the total amount of rotation, and a 1/L fraction of the total amount of scaling.",5.1 Balanced factorizations,[0],[0]
"However, the scaling done by the ith factor should be done in directions that take account of the partial rotations done by the other factors.",5.1 Balanced factorizations,[0],[0]
"The following is the key property of the balanced factorization; its proof is in Appendix C.
Lemma 7.",5.1 Balanced factorizations,[0],[0]
"If σ1, ..., σd are the singular values of A, and A1, ..., AL is a balanced factorization of A, then the following hold: (a) A = ∏L
i=1Ai; (b) for each i ∈ {1, ..., L}, σ 1/L 1 , ..., σ 1/L d are the singular
values of Ai.",5.1 Balanced factorizations,[0],[0]
The following is the power projection algorithm.,5.2 Procedure and upper bound,[0],[0]
"It has a positivity parameter γ > 0, and uses H = {A : ∀u s.t. ||u||",5.2 Procedure and upper bound,[0],[0]
"= 1, u⊤Au ≥ γ} as its “hypothesis space”.",5.2 Procedure and upper bound,[0],[0]
"First, it initializes Θ(0)i = γ 1/LI for all i ∈ {1, ..., L}.",5.2 Procedure and upper bound,[0],[0]
"Then, for each t, it does the following.
",5.2 Procedure and upper bound,[0],[0]
• Gradient Step.,5.2 Procedure and upper bound,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",5.2 Procedure and upper bound,[0],[0]
i = Θ,5.2 Procedure and upper bound,[0],[0]
(t),5.2 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",5.2 Procedure and upper bound,[0],[0]
"L − Φ
)
",5.2 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
• Power Project.",5.2 Procedure and upper bound,[0],[0]
Compute the projection Ψ(t+1/2) (w.r.t.,5.2 Procedure and upper bound,[0],[0]
"the Frobenius norm) of Θ (t+1/2) 1:L
onto H.
• Factor.",5.2 Procedure and upper bound,[0],[0]
"Let Θ (t+1) 1 , ...,Θ (t+1) L be the balanced factorization of Ψ (t+1/2), so that Ψ(t+1/2) =
Θ (t+1) 1:L .
",5.2 Procedure and upper bound,[0],[0]
Theorem 3.,5.2 Procedure and upper bound,[0],[0]
For any Φ such that u⊤Φu >,5.2 Procedure and upper bound,[0],[0]
"γ for all unit-length u, the power projection algorithm produces Θ(t) with ℓ(Θ(t))",5.2 Procedure and upper bound,[0],[0]
"≤ ǫ in poly(d, ||Φ||F , 1 γ ) log(1/ǫ) iterations.",5.2 Procedure and upper bound,[0],[0]
Lemma 8.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L ∈ H.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"Θ (0) 1:L = γI ∈ H, and, for all t, Ψ (t+1/2) is obtained by projection onto H, and Θ (t+1) 1:L = Ψ(t+1/2).
",5.3 Proof of Theorem 3,[0],[0]
Definition 4.,5.3 Proof of Theorem 3,[0],[0]
The exponential of a matrix A is exp(A),5.3 Proof of Theorem 3,[0],[0]
"def = ∑∞
k=0 1 k!A k, and B is a logarithm of A if A = exp(B).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 9 (Culver 1966).,5.3 Proof of Theorem 3,[0],[0]
"A real matrix has a real logarithm if and only if it is invertible and each Jordan block belonging to a negative eigenvalue occurs an even number of times.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 10.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L has a real Lth root.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Since Θ (t) 1:L ∈ H implies u,5.3 Proof of Theorem 3,[0],[0]
⊤Θ(t)1,5.3 Proof of Theorem 3,[0],[0]
:Lu > 0,5.3 Proof of Theorem 3,[0],[0]
"for all u, Θ (t) 1:L does not have a negative eigenvalue and is invertible.",5.3 Proof of Theorem 3,[0],[0]
"By Lemma 9, Θ (t) 1:L has a real logarithm.",5.3 Proof of Theorem 3,[0],[0]
"Thus, its real Lth root can be constructed via exp(log(Θ (t) 1:L)/L).
",5.3 Proof of Theorem 3,[0],[0]
"The preceding lemma implies that the algorithm is well-defined, since all of the required roots can be calculated.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 11.,5.3 Proof of Theorem 3,[0],[0]
"H is convex.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Suppose A and B are in H and λ ∈ (0, 1).",5.3 Proof of Theorem 3,[0],[0]
"We have
u⊤(λA+ (1− λ)B)u = λu⊤Au+ (1− λ)u⊤Bu ≥ γ.
Lemma 12.",5.3 Proof of Theorem 3,[0],[0]
"For all A ∈ H, σmin(A) ≥ γ.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Let u and v be singular vectors such that u⊤Av = σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
"γ ≤ v⊤Av = σmin(A)v ⊤u ≤ σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 13.,5.3 Proof of Theorem 3,[0],[0]
"For all t, σmin(Θ (t) i )",5.3 Proof of Theorem 3,[0],[0]
"≥ γ 1/L.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"First, σmin(Θ (0) i ) =",5.3 Proof of Theorem 3,[0],[0]
γ 1/L ≥ γ1/L. Now consider t > 0.,5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t−1/2) was projected into H, we have σmin(Ψ(t−1/2))",5.3 Proof of Theorem 3,[0],[0]
≥ γ.,5.3 Proof of Theorem 3,[0],[0]
"Lemma 7 then completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
"Define U(t) = max {
maxs≤tmaxi ||Θ (s) i ||2, ||Φ|| 1/L 2
}
, B(t) = mins≤tmini σmin(Θ (s) i ), and recall that
ℓ(t) = ||Θ (t) 1:L − Φ|| 2 F .
",5.3 Proof of Theorem 3,[0],[0]
"Arguing as in the initial portion of Section 3.2, as long as
η ≤ 1
3Ld5U(t)2L (7)
we have ℓ(t + 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLB(t)2L ) ℓ(t) (see Equation 4).,5.3 Proof of Theorem 3,[0],[0]
"Lemma 13 gives B(t) ≥ γ1/L, so ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
"( 1− ηLγ2 )
ℓ(t).",5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t+1/2) is the projection of Θ (t+1/2) 1:L onto a convex set H that
contains Φ, and Θ (t+1) 1:L = Ψ (t+1/2), (7) implies
ℓ(t+ 1) ≤ ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLγ2 ) ℓ(t).,5.3 Proof of Theorem 3,[0],[0]
"(8)
Next, we prove an upper bound on U .
",5.3 Proof of Theorem 3,[0],[0]
Lemma 14.,5.3 Proof of Theorem 3,[0],[0]
"For all t, U(t) ≤ ( √
ℓ(t) + ||Φ||F
)1/L .
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Recall that ℓ(t) = ||Θ (t) 1:L−Φ|| 2 F .,5.3 Proof of Theorem 3,[0],[0]
"By the triangle inequality, ||Θ (t) 1:L||F ≤",5.3 Proof of Theorem 3,[0],[0]
√ ℓ(t)+ ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
Thus ||Θ (t) 1:L||2 ≤ √ ℓ(t) + ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
"By Lemma 7, for all i, we have ||Θ (t)",5.3 Proof of Theorem 3,[0],[0]
i ||2 ≤ ( √ ℓ(t) + ||Φ||F )1/L .,5.3 Proof of Theorem 3,[0],[0]
"Since ||Φ||2 ≤ ||Φ||F , this completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
Note that the triangle inequality implies that ℓ(0) ≤ ||Θ (0) 1:L|| 2 F + ||Φ|| 2 F ≤,5.3 Proof of Theorem 3,[0],[0]
γ 2d,5.3 Proof of Theorem 3,[0],[0]
+,5.3 Proof of Theorem 3,[0],[0]
||Φ||2F .,5.3 Proof of Theorem 3,[0],[0]
Since σmin(Φ) ≥,5.3 Proof of Theorem 3,[0],[0]
"γ, we have ||Φ|| 2 F ≥ γ 2d, so ℓ(t) ≤ 2||Φ||2F and U(t) ≤ (3||Φ||2) 1/L.",5.3 Proof of Theorem 3,[0],[0]
"Now, if we set η = 1 cLd5||Φ||2
F , for a large enough absolute constant c, then (7) is satisfied, so that (8) gives ℓ(t+1) ≤ (
1− γ 2
cd5||Φ||2 F
)
ℓ(t) and the power projection algorithm achieves ℓ(t+ 1) ≤ ǫ after
O
(
d5||Φ||2F γ2 log
(
ℓ(0)
ǫ
))
",5.3 Proof of Theorem 3,[0],[0]
"=O
(
d5||Φ||2F γ2 log
(
||Φ||2F ǫ
))
updates.",5.3 Proof of Theorem 3,[0],[0]
"In this section, we show that positive definite Φ are necessary for several gradient descent algorithms with different kinds of regularization to minimize the loss.",6 Failure,[0],[0]
"One family of algorithms that we will
analyze is parameterized by a function ψ mapping the number of inputs d and the number of layers L to a radius ψ(d, L), step sizes ηt and initialization parameter γ ≥ 0.",6 Failure,[0],[0]
"In particular, a ψ-step-and-project algorithm is any instantiation of the following algorithmic template.
",6 Failure,[0],[0]
Initialize each Θ (0),6 Failure,[0],[0]
"i = γ 1/LI for some γ ≥ 0 and iterate:
• Gradient Step.",6 Failure,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",6 Failure,[0],[0]
i = Θ,6 Failure,[0],[0]
(t),6 Failure,[0],[0]
"i − ηt(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤.
• Project.",6 Failure,[0],[0]
Set each Θt+1i,6 Failure,[0],[0]
"to the projection of Θ t+1/2 i onto {A : ||A− I||2 ≤ ψ(d, L)}.
",6 Failure,[0],[0]
We will also show that Penalty Regularized Gradient Descent which uses gradient descent with any step sizes ηt on the regularized objective ℓ(Θ) + κ 2 ∑ i ||I,6 Failure,[0],[0]
"−Θ|| 2 F also fails to minimize the loss.
",6 Failure,[0],[0]
"Both results use the simple observation that when Θ1:L and Φ are mutually diagonalizable then
||Θ1:L − Φ|| 2 F = ||U ⊤D̂U",6 Failure,[0],[0]
"− U⊤DU ||2F = d ∑
j=1
(D̂jj −Djj) 2,
where the Dii are the eigenvalues of Φ.
Theorem 4.",6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then Penalty Regularized Gradient Descent produces hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and L is even, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"For all t, Penalty Regularized Gradient Descent produces Θ (t+1) i = (1 − κ)Θ (t) i + κI",6 Failure,[0],[0]
− ηt(Θ (t) i+1:L) ⊤ ( Θ (t) 1:L −Φ ) (Θ (t) 1:i−1) ⊤.,6 Failure,[0],[0]
"Thus, by induction, the Θ(t)i are matrix polynomials of Φ, and therefore they are all commuting normal.",6 Failure,[0],[0]
As in Lemmas 5 and 6 each Θ (t) i is the same U ⊤D̃(t)U and Θ (t) 1,6 Failure,[0],[0]
:L = U ⊤(D̃(t))LU .,6 Failure,[0],[0]
"Since L is even, each (D̃(t))Ljj ≥ 0, so ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L−Φ|| 2 F ≥ λ 2/2.
To analyze step-and-project algorithms, it is helpful to first characterize the project step (see also (Lefkimmiatis et al., 2013)).
",6 Failure,[0],[0]
Lemma 15.,6 Failure,[0],[0]
"Let X be a symmetric matrix and let U⊤DU be its diagonalization.
",6 Failure,[0],[0]
"For a > 0, let Y be the Frobenius norm projection of X onto Ba = {A : A is symmetric psd and ||A−I||2 ≤ a}.",6 Failure,[0],[0]
"Then Y = U
⊤D̃U where D̃ is obtained from D by projecting all of its diagonal elements onto [1− a, 1 + a].
",6 Failure,[0],[0]
"Thus {X,Y } are symmetric commuting normal matrices.
",6 Failure,[0],[0]
Proof.,6 Failure,[0],[0]
"First, if X ∈ Ba, then Y = X and we are done.",6 Failure,[0],[0]
Assume X 6∈ Ba.,6 Failure,[0],[0]
"Clearly U ⊤D̃U ∈ Ba, so we just need to show that any member of Ba is at least as far from X as U⊤D̃U is.",6 Failure,[0],[0]
"Let Λ be the multiset of eigenvalues of X (with repetitions) that are not in [1 − a, 1 + a], and for each λ ∈ Λ, let eλ be the adjustment to λ necessary to bring it to [1− a, 1 + a]; i.e., so that λ+ eλ is the projection of λ onto [1− a, 1 + a].
",6 Failure,[0],[0]
"If uλ is the eigenvector associated with λ, we have U ⊤D̃U −X = ∑ λ∈Λ eλuλu ⊤ λ , so that ||U ⊤D̃U",6 Failure,[0],[0]
− X||2F = ∑,6 Failure,[0],[0]
λ∈Λ,6 Failure,[0],[0]
e 2 λ.,6 Failure,[0],[0]
Let Z be an arbitrary member of Ba.,6 Failure,[0],[0]
We would like to show that ||Z,6 Failure,[0],[0]
− X|| 2 F ≥ ∑ λ∈Λ e 2 λ.,6 Failure,[0],[0]
"Since Z ∈ Ba, we have ||Z − I||2 ≤ a. ||Z",6 Failure,[0],[0]
− I||2 is the largest singular value of Z,6 Failure,[0],[0]
"− I so, for any unit length vector, in particular some uλ for λ ∈ Λ, |u ⊤ λ (Z",6 Failure,[0],[0]
− I)uλ| = |u ⊤ λ,6 Failure,[0],[0]
"Zuλ − 1| ≤ a, which implies u⊤λZuλ ∈",6 Failure,[0],[0]
"[1 − a, 1 + a].",6 Failure,[0],[0]
Since U is unitary U ⊤(X,6 Failure,[0],[0]
"− Z)U has the same eigenvalues as X − Z, and, since the Frobenius norm is a function of the eigenvalues, ||U⊤(X − Z)U ||F = ||X − Z||F .",6 Failure,[0],[0]
But since u⊤λZuλ ∈,6 Failure,[0],[0]
"[1 − a, 1 + a] for all λ ∈ Λ, just summing over the diagonal elements, we get ||U⊤(X − Z)U",6 Failure,[0],[0]
"||2F ≥ ∑ λ∈Λ e 2 λ, completing the proof.
",6 Failure,[0],[0]
Theorem 5.,6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then ψ-step-and-project algorithms produce hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and either L is even or ψ(L, d) ≤ 1, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"As in Lemmas 5 and 6, the Θ (t+1/2)",6 Failure,[0],[0]
i are identical and mutually diagonalizable with Φ. Lemma 15 shows that this is preserved by the projection step.,6 Failure,[0],[0]
Thus there is a real diagonal D̃(t) such that each Θ (t) i = U ⊤D(t)i,6 Failure,[0],[0]
"U , so Θ (t) 1:L = U ⊤(D̃(t))LU .",6 Failure,[0],[0]
"When L is even, each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"When ψ(d, L) ≤ 1",6 Failure,[0],[0]
"then the projection ensures that the elements of D̃(t) are non-negative, and thus each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"In either case, ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L− Φ||2F ≥ λ 2/2.
",6 Failure,[0],[0]
"One choice of Φ that satisfies the requirements of Theorems 4 and 5 is Φ = diag(−λ, 1, 1, ..., 1).",6 Failure,[0],[0]
"For constant λ, the loss of Θ(0) = (I, I, ..., I) is a constant for this target.",6 Failure,[0],[0]
"Another choice is Φ = diag(−λ,−λ, 1, 1, ..., 1), which has a positive determinant.
",6 Failure,[0],[0]
Our proof of failure to minimize the loss exploits the fact that the layers are initialized to multiples of the identity.,6 Failure,[0],[0]
"Since the training process is a continuous function of the initial solution, this implies that any convergence to a good solution will be very slow if the initializations are sufficiently close to the identity.",6 Failure,[0],[0]
"We thank Yair Carmon, Nigel Duffy, Matt Feiszli, Roy Frostig, Vineet Gupta, Moritz Hardt, Tomer Koren, Antoine Saliou, Hanie Sedghi, Yoram Singer and Kunal Talwar for valuable conversations.
",Acknowledgements,[0],[0]
Peter Bartlett gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS).,Acknowledgements,[0],[0]
"We rely on the following facts (Horn, 1986; Harville, 1997).
",A Proof of Lemma 1,[0],[0]
Lemma 16.,A Proof of Lemma 1,[0],[0]
"For compatible matrices (and, where m,n, p, q, r, s are mentioned, A ∈ ℜm×n, B ∈ ℜp×q, X ∈ ℜr×s):
A⊗ (B ⊗ E) =",A Proof of Lemma 1,[0],[0]
"(A⊗B)⊗E,
AC ⊗BD = (A⊗B)(C ⊗D),
(A⊗B)⊤ = A⊤ ⊗B⊤,
vec(AXB) = (B⊤ ⊗A)vec(X),
Tm,nvec(A) def = vec(A⊤),
Tn,mTm,n = Imn,
Tm,n = T ⊤ n,m,
T1,n = Tn,1 =",A Proof of Lemma 1,[0],[0]
"In,
DX(A(B(X)))",A Proof of Lemma 1,[0],[0]
"= DB(A(B(X)))DX (B(X)),
DX(A(X)B(X))",A Proof of Lemma 1,[0],[0]
= (B(X) ⊤ ⊗ Im)DXA(X),A Proof of Lemma 1,[0],[0]
+,A Proof of Lemma 1,[0],[0]
"(Iq ⊗A(X))DXB(X),
DX(A(X) T ) = Tn,mDX(A(X)),
DX(AXB)",A Proof of Lemma 1,[0],[0]
"= B ⊤ ⊗A,
DA(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(Imn ⊗ vec(B))
=",A Proof of Lemma 1,[0],[0]
"(Inq ⊗ Tm,p)(In ⊗ vec(B)⊗",A Proof of Lemma 1,[0],[0]
"Im),
DB(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(vec(A)⊗ Ipq)
= (Tp,q ⊗ Imn)(Iq ⊗ vec(A)⊗",A Proof of Lemma 1,[0],[0]
"Ip).
",A Proof of Lemma 1,[0],[0]
"Armed with Lemma 16, we now prove Lemma 1.",A Proof of Lemma 1,[0],[0]
"We have
DΘifΘ(x) = DΘi (Θi+1:LΘiΘ1:i−1x) =",A Proof of Lemma 1,[0],[0]
"(Θ1:i−1x) ⊤ ⊗Θi+1:L.
Again, from Lemma 16
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= DΘi
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
= DΘ1:j−1x
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
DΘi (Θ1:j−1x)
(by the chain rule, since i < j)
= DΘ1:j−1x
(
(
(Θ1:j−1x)⊗Θ ⊤ j+1:L
)⊤ ) (
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.",A Proof of Lemma 1,[0],[0]
"(9)
Define P = Θ1:j−1x and Q = Θj+1:L, so that P ∈ ℜd×1 and Q ∈ ℜd×d.",A Proof of Lemma 1,[0],[0]
"We have
DP
(
( P ⊗Q⊤ )⊤ )
= Td2,dDP
( P ⊗Q⊤ )
",A Proof of Lemma 1,[0],[0]
"= Td2,d(I1 ⊗ Td,d ⊗ Id)(Id ⊗ vec(Q T ))",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Q ⊤)).
",A Proof of Lemma 1,[0],[0]
"Substituting back into (9), we get
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Θ ⊤ j+1:L))
",A Proof of Lemma 1,[0],[0]
"(
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.
",A Proof of Lemma 1,[0],[0]
"The product rule in Lemma 16 gives, for each i,
DΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= E(DΘi(ℓ(fΘ(X)))
= E(DΘi( 1
2 (fΘ(X)− ΦX)
⊤(fΘ(X)",A Proof of Lemma 1,[0],[0]
"−ΦX)))
",A Proof of Lemma 1,[0],[0]
= E(((Θ1:L − Φ)X) ⊤DΘifΘ(X)),A Proof of Lemma 1,[0],[0]
"= E ( ((Θ1:L − Φ)X) ⊤ ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
( (I1 ⊗,A Proof of Lemma 1,[0],[0]
"((Θ1:L − Φ)X) ⊤) ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
"= E (( (Θ1:i−1X) ⊤ ⊗ ((Θ1:L −Φ)X) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
(( X⊤Θ⊤1:i−1 ) ⊗,A Proof of Lemma 1,[0],[0]
( X⊤(Θ1:L −Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
L )),A Proof of Lemma 1,[0],[0]
"= E ( (X⊤ ⊗X⊤) (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
))
= E ((X ⊗X)vec(1))⊤ ( Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L )
= E ( vec(XX⊤) )",A Proof of Lemma 1,[0],[0]
"⊤ (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
)
= (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Hence,
(DΘiℓ (fΘ))",A Proof of Lemma 1,[0],[0]
"⊤ =
(
Θ1:i−1 ⊗Θ ⊤ i+1:L(Θ1:L − Φ)
)
(vec(Id))
",A Proof of Lemma 1,[0],[0]
"= vec ( Θ⊤i+1:L(Θ1:L − Φ)IdΘ ⊤ 1:i−1 ) .
",A Proof of Lemma 1,[0],[0]
"Also, recalling that i < j, we have
DΘjDΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= DΘj
( (vec(Id))",A Proof of Lemma 1,[0],[0]
T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘj
(
Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L
)
= (Id2 ⊗ (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ) (Id ⊗ Td,d ⊗",A Proof of Lemma 1,[0],[0]
"Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘj",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ) .
",A Proof of Lemma 1,[0],[0]
"Continuing with the subproblem,
DΘj
(
(Θ1:L −Φ) ⊤Θi+1:",A Proof of Lemma 1,[0],[0]
"L
)
",A Proof of Lemma 1,[0],[0]
"= (Θ⊤i+1:L ⊗ Id)DΘj
( (Θ1:L − Φ) ⊤ )
",A Proof of Lemma 1,[0],[0]
+ (Id ⊗ (Θ1:,A Proof of Lemma 1,[0],[0]
L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id)DΘj
( Θ⊤1:L )
+",A Proof of Lemma 1,[0],[0]
(Id ⊗ (Θ1:L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 ) DΘj (Θ ⊤ j )
+",A Proof of Lemma 1,[0],[0]
"(Id ⊗ (Θ1:L − Φ) ⊤) ( Θ⊤i+1:j−1 ⊗Θj+1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 )",A Proof of Lemma 1,[0],[0]
"Td,d
+ (Id ⊗ (Θ1:L − Φ) ⊤)",A Proof of Lemma 1,[0],[0]
"( Θ⊤i+1:j−1 ⊗Θj+1:L )
=",A Proof of Lemma 1,[0],[0]
( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 ),A Proof of Lemma 1,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Finally,
DΘiDΘiℓ (fΘ) = DΘi
( (vec(Id)) T ( Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
"L − Φ) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘi
(
Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
L − Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
"L
)
= (Id2 ⊗ (vec(Id)) T )",A Proof of Lemma 1,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘi",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L )
and
DΘi
(
(Θ1:L − Φ) ⊤Θi+1:L
)
= (Θ⊤i+1:L ⊗ Id)DΘi
( (Θ1:L − Φ) ⊤ )
= (Θ⊤i+1:L ⊗ Id)DΘi
( Θ⊤1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ) DΘi(Θ ⊤ i ) =",A Proof of Lemma 1,[0],[0]
(Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ),A Proof of Lemma 1,[0],[0]
"Td,d = (
Θ⊤i+1:LΘi+1:",A Proof of Lemma 1,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",A Proof of Lemma 1,[0],[0]
"Td,d.",A Proof of Lemma 1,[0],[0]
"We have ||∇2||2F = 2 ∑
i<j
||DΘjDΘiℓ(fΘ)|| 2 F +
∑
i
||DΘiDΘiℓ(fΘ)|| 2 F .",B Proof of Lemma 3,[0],[0]
"(10)
Let’s start with the easier term.",B Proof of Lemma 3,[0],[0]
Choose Θ such that ||Θi − I||2 ≤ z,B Proof of Lemma 3,[0],[0]
"for all i. We have
||DΘiDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(Id)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣
F
≤ ∣ ∣
∣
∣ ∣ ∣ (Id2 ⊗ (vec(Id)) ⊤) (Id ⊗ Td,d ⊗ Id) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗Id2 )( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
∣ ∣ ∣ ∣ ∣ ∣
F
≤ d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 ) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ∣ ∣
∣
∣ ∣
∣ vec(Θ⊤1:i−1)
∣ ∣ ∣ ∣ ∣ ∣
F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ||Θ1:i−1||F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4 ||Θ1:i−1||2
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 ) ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ F × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ F
≤",B Proof of Lemma 3,[0],[0]
"d5(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ 2 × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ 2
≤ d5(1 + z)2(L−1).
",B Proof of Lemma 3,[0],[0]
"Similarly,
||DΘjDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(I)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )
)
∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣ ∣ ∣
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ) ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"(∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
= d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ||Θi+1:j−1||F × ∣ ∣ ∣ ∣ ∣ ∣ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
( d(1 + z)2L−1−i + 2d(1 + z)2L−1−i ),B Proof of Lemma 3,[0],[0]
"= 3d5(1 + z)2L−2.
",B Proof of Lemma 3,[0],[0]
"Putting these together with (10), we get ||∇2||2F ≤",B Proof of Lemma 3,[0],[0]
"L 29d10(1 + z)4L, so that
||∇2||F ≤ 3Ld 5(1 + z)2L.",B Proof of Lemma 3,[0],[0]
"Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .
",C Proof of Lemma 7,[0],[0]
"Lemma 17 ((Horn & Johnson, 2013)).",C Proof of Lemma 7,[0],[0]
"A is a unitary matrix if and only if all of the (complex) eigenvalues z of A have magnitude 1.
",C Proof of Lemma 7,[0],[0]
"Lemma 18 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is unitary then A is normal.
",C Proof of Lemma 7,[0],[0]
"Lemma 19 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is normal with eigenvalues λ1, ..., λd, the singular values of A are |λ1|, ..., |λd|.
Lemma 20.",C Proof of Lemma 7,[0],[0]
"If A is unitary, then A1/L is unitary, and thus Ai/L is unitary for any non-negative integer i.
Lemma 21.",C Proof of Lemma 7,[0],[0]
"If A is invertible and normal with singular values σ1, ..., σd, then, for any positive integer L, the singular values of A1/L are σ 1/L 1 , ..., σ 1/L d .
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"Follows from Lemma 19 together with the fact that raising a non-singular matrix to a power results in raising its eigenvalues to the same power.
",C Proof of Lemma 7,[0],[0]
"Lemma 22 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A = RP is the polar decomposition of A, then the singular values of A are the same as the singular values of P .
",C Proof of Lemma 7,[0],[0]
Lemma 23.,C Proof of Lemma 7,[0],[0]
"If σ1, ..., σd are the principal components of A, and A = ∏L i=1Ai is a balanced factorization of A, then then σ 1/L 1 , ..., σ 1/L d are the principal components of Ai, for each i ∈ {1, ..., L}.
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"The singular values of Ai = RiPi are the same as the singular values of Pi, which is similar to P 1/L, whose singular values are the Lth roots of the singular values of P , which are the same as the singular values of A.
Lemma 24.",C Proof of Lemma 7,[0],[0]
"If A1, ..., AL is a balanced factorization of A, then
A = L ∏
i=1
Ai.
Proof.",C Proof of Lemma 7,[0],[0]
"We have
A = RP
= R1/LR1−1/LP 1/LP 1−1/L = R1/LR1−1/LP 1/LR−(1−1/L)R1−1/LP 1−1/L = R1P1R 1−1/LP 1−1/L = A1R 1−1/LP 1−1/L = A1R 1/LR1−2/LP 1/LP 1−2/L
and so on.",C Proof of Lemma 7,[0],[0]
"We analyze algorithms for approximating a function f(x) = Φxmapping R to R using deep linear neural networks, i.e. that learn a function h parameterized by matrices Θ1, ...,ΘL and defined by h(x)",abstractText,[0],[0]
= ΘLΘL−1...Θ1x.,abstractText,[0],[0]
We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.,abstractText,[0],[0]
"We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix Φ, in the case where the initial hypothesis Θ1 = ...",abstractText,[0],[0]
= ΘL = I has excess loss bounded by a small enough constant.,abstractText,[0],[0]
"On the other hand, we show that gradient descent fails to converge for Φ whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help.",abstractText,[0],[0]
"If Φ is symmetric positive definite, we show that an algorithm that initializes Θi = I learns an ǫ-approximation of f using a number of updates polynomial in L, the condition number of Φ, and log(d/ǫ).",abstractText,[0],[0]
"In contrast, we show that if the least squares matrix Φ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.",abstractText,[0],[0]
"We analyze an algorithm for the case that Φ satisfies uΦu > 0 for all u, but may not be symmetric.",abstractText,[0],[0]
This algorithm uses two regularizers: one that maintains the invariant u⊤ΘLΘL−1...,abstractText,[0],[0]
Θ1u > 0,abstractText,[0],[0]
"for all u, and another that “balances” Θ1, ...,ΘL so that they have the same singular values.",abstractText,[0],[0]
"Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks.",1. Introduction,[0],[0]
"However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems
1Magic Leap, Inc. Correspondence to: Zhao Chen <zchen@magicleap.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
such as smartphones, wearable devices, and robots/drones.",1. Introduction,[0],[0]
"Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass.",1. Introduction,[0],[0]
"Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result.",1. Introduction,[0],[0]
"In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance.
",1. Introduction,[0],[0]
"In general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks.",1. Introduction,[0],[0]
"Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features (Long & Wang, 2015) or optimizing multitask network architectures (Misra et al., 2016), etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients.",1. Introduction,[0],[0]
"A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes.",1. Introduction,[0],[0]
"We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function.
",1. Introduction,[0],[0]
"In practice, the multitask loss function is often assumed to be linear in the single task losses Li, L = ∑ i wiLi, where the sum runs over all T tasks.",1. Introduction,[0],[0]
"In our case, we propose an adaptive method, and so wi can vary at each training step t: wi = wi(t).",1. Introduction,[0],[0]
"This linear form of the loss function is convenient for implementing gradient balancing, as wi very directly and linearly couples to the backpropagated gradient magnitudes from each task.",1. Introduction,[0],[0]
The challenge is then to find the best value for each wi at each training step t that balances the contribution of each task for optimal model training.,1. Introduction,[0],[0]
"To optimize the weights wi(t) for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small.",1. Introduction,[0],[0]
"The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight wi(t) should decrease relative to other task weights wj(t)|j 6=i to allow other tasks more influence on
training.",1. Introduction,[0],[0]
"Our algorithm is similar to batch normalization (Ioffe & Szegedy, 2015) with two main differences: (1) we normalize across tasks instead of across data batches, and (2) we use rate balancing as a desired objective to inform our normalization.",1. Introduction,[0],[0]
"We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting.
",1. Introduction,[0],[0]
"Our main contributions to multitask learning are as follows:
1.",1. Introduction,[0],[0]
"An efficient algorithm for multitask loss balancing which directly tunes gradient magnitudes.
2.",1. Introduction,[0],[0]
"A method which matches or surpasses the performance of very expensive exhaustive grid search procedures, but which only requires tuning a single hyperparameter.
3.",1. Introduction,[0],[0]
A demonstration that direct gradient interaction provides a powerful way of controlling multitask learning.,1. Introduction,[0],[0]
"Multitask learning was introduced well before the advent of deep learning (Caruana, 1998; Bakker & Heskes, 2003), but the robust learned features within deep networks and their excellent single-task performance have spurned renewed interest.",2. Related Work,[0],[0]
"Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Collobert & Weston, 2008; Hashimoto et al., 2016; Søgaard & Goldberg, 2016) to speech synthesis (Seltzer & Droppo, 2013; Wu et al., 2015), from very domain-specific applications such as traffic prediction (Huang et al., 2014) to very general cross-domain work (Bilen & Vedaldi, 2017).",2. Related Work,[0],[0]
"Multitask learning has also been explored in the context of curriculum learning (Graves et al., 2017), where subsets of tasks are subsequently trained based on local rewards; we here explore the opposite approach, where tasks are jointly trained based on global rewards such as total loss decrease.
",2. Related Work,[0],[0]
"Multitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding.",2. Related Work,[0],[0]
"Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus, 2015; Teichmann et al., 2016) to much larger subsets as in UberNet (Kokkinos, 2016).",2. Related Work,[0],[0]
"Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al., 2017) or YOLO-9000 for object detection (Redmon & Farhadi, 2016).",2. Related Work,[0],[0]
Particularly of note is the rich and significant body of work on finding explicit ways to exploit task relationships within a multitask model.,2. Related Work,[0],[0]
"Clustering methods have shown success beyond deep models (Jacob et al., 2009; Kang et al., 2011), while constructs such as deep relationship networks (Long & Wang, 2015) and cross-stich networks (Misra et al., 2016)
give deep networks the capacity to search for meaningful relationships between tasks and to learn which features to share between them.",2. Related Work,[0],[0]
"Work in (Warde-Farley et al., 2014) and (Lu et al., 2016) use groupings amongst labels to search through possible architectures for learning.",2. Related Work,[0],[0]
"Perhaps the most relevant to the current work, (Kendall et al., 2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task.",2. Related Work,[0],[0]
"For a multitask loss function L(t) = ∑ wi(t)Li(t), we aim to learn the functions wi(t) with the following goals: (1) to place gradient norms for different tasks on a common scale through which we can reason about their relative magnitudes, and (2) to dynamically adjust gradient norms so different tasks train at similar rates.",3.1. Definitions and Preliminaries,[0],[0]
"To this end, we first define the relevant quantities, first with respect to the gradients we will be manipulating.
",3.1. Definitions and Preliminaries,[0],[0]
•,3.1. Definitions and Preliminaries,[0],[0]
W : The subset of the full network weights W ⊂ W where we actually apply GradNorm.,3.1. Definitions and Preliminaries,[0],[0]
"W is generally chosen as the last shared layer of weights to save on compute costs1.
",3.1. Definitions and Preliminaries,[0],[0]
• G(i)W (t) = ||∇Wwi(t)Li(t)||2: the L2 norm of the gradient of the weighted single-task loss wi(t)Li(t),3.1. Definitions and Preliminaries,[0],[0]
"with respect to the chosen weights W .
• GW (t) = Etask[G(i)W (t)]: the average gradient norm across all tasks at training time t.
We also define various training rates for each task i:
• L̃i(t) = Li(t)/Li(0): the loss ratio for task",3.1. Definitions and Preliminaries,[0],[0]
"i at time t. L̃i(t) is a measure of the inverse training rate of task i (i.e. lower values of L̃i(t) correspond to a faster training rate for task i)2.
• ri(t) = L̃i(t)/Etask[L̃i(t)]",3.1. Definitions and Preliminaries,[0],[0]
": the relative inverse training rate of task i.
With the above definitions in place, we now complete our description of the GradNorm algorithm.",3.1. Definitions and Preliminaries,[0],[0]
"As stated in Section 3.1, GradNorm should establish a common scale for gradient magnitudes, and also should balance
1In our experiments this choice of W causes GradNorm to increase training time by only ∼ 5% on NYUv2.
2Networks in this paper all had stable initializations and Li(0) could be used directly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When Li(0) is sharply dependent on initialization, we can use a theoretical initial loss instead.",3.2. Balancing Gradients with GradNorm,[0],[0]
"E.g. for Li the CE loss across C classes, we can use Li(0) = log(C).
training rates of different tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The common scale for gradients is most naturally the average gradient norm, GW (t), which establishes a baseline at each timestep t by which we can determine relative gradient sizes.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The relative inverse training rate of task i, ri(t), can be used to rate balance our gradients.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Concretely, the higher the value of ri(t), the higher the gradient magnitudes should be for task i in order to encourage the task to train more quickly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Therefore, our desired gradient norm for each task i is simply:
G (i) W (t) 7→ GW (t)× [ri(t)]",3.2. Balancing Gradients with GradNorm,[0],[0]
"α, (1)
where α is an additional hyperparameter.",3.2. Balancing Gradients with GradNorm,[0],[0]
α sets the strength of the restoring force which pulls tasks back to a common training rate.,3.2. Balancing Gradients with GradNorm,[0],[0]
"In cases where tasks are very different in their complexity, leading to dramatically different learning dynamics between tasks, a higher value of α should be used to enforce stronger training rate balancing.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of α is appropriate.",3.2. Balancing Gradients with GradNorm,[0],[0]
Note that α = 0 will always try to pin the norms of backpropagated gradients from each task to be equal at W .,3.2. Balancing Gradients with GradNorm,[0],[0]
"See Section 5.4 for more details on the effects of tuning α.
",3.2. Balancing Gradients with GradNorm,[0],[0]
"Equation 1 gives a target for each task i’s gradient norms, and we update our loss weights wi(t) to move gradient
norms towards this target for each task.",3.2. Balancing Gradients with GradNorm,[0],[0]
"GradNorm is then implemented as an L1 loss function Lgrad between the actual and target gradient norms at each timestep for each task, summed over all tasks:
Lgrad(t;wi(t))",3.2. Balancing Gradients with GradNorm,[0],[0]
"= ∑ i ∣∣∣∣G(i)W (t)−GW (t)× [ri(t)]α∣∣∣∣ 1 (2)
where the summation runs through all T tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When differentiating this loss Lgrad, we treat the target gradient norm GW (t)× [ri(t)]α as a fixed constant to prevent loss weights wi(t) from spuriously drifting towards zero.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Lgrad is then differentiated only with respect to the wi, as the wi(t) directly control gradient magnitudes per task.",3.2. Balancing Gradients with GradNorm,[0],[0]
The computed gradients ∇wiLgrad are then applied via standard update rules to update each wi (as shown in Figure 1).,3.2. Balancing Gradients with GradNorm,[0],[0]
The full GradNorm algorithm is summarized in Algorithm 1.,3.2. Balancing Gradients with GradNorm,[0],[0]
"Note that after every update step, we also renormalize the weights wi(t) so that ∑ i wi(t) = T in order to decouple gradient normalization from the global learning rate.",3.2. Balancing Gradients with GradNorm,[0],[0]
"To illustrate GradNorm on a simple, interpretable system, we construct a common scenario for multitask networks: training tasks which have similar loss functions but different loss scales.",4. A Toy Example,[0],[0]
"In such situations, if we naı̈vely pick wi(t) = 1
Algorithm 1 Training with GradNorm Initialize wi(0) = 1 ∀i Initialize network weightsW Pick value for α > 0 and pick the weightsW (usually the
final layer of weights which are shared between tasks) for t = 0",4. A Toy Example,[0],[0]
"to max train steps do
Input batch xi to compute Li(t) ∀i and L(t) = ∑ i wi(t)Li(t)",4. A Toy Example,[0],[0]
"[standard forward pass] Compute G(i)W (t) and ri(t) ∀i Compute GW (t) by averaging the G (i) W (t)
Compute Lgrad = ∑ i|G (i) W (t)−GW (t)× [ri(t)]α|1 Compute GradNorm gradients∇wiLgrad, keeping targets GW (t)× [ri(t)]α constant Compute standard gradients∇WL(t) Update wi(t) 7→ wi(t+ 1) using ∇wiLgrad UpdateW(t) 7→ W(t+ 1) using∇WL(t)",4. A Toy Example,[0],[0]
"[standard
backward pass] Renormalize wi(t+ 1) so that ∑ i wi(t+ 1) = T
end for
for all loss weights wi(t), the network training will be dominated by tasks with larger loss scales that backpropagate larger gradients.",4. A Toy Example,[0],[0]
"We will demonstrate that GradNorm overcomes this issue.
",4. A Toy Example,[0],[0]
"Consider T regression tasks trained using standard squared loss onto the functions
fi(x) =",4. A Toy Example,[0],[0]
"σi tanh((B + i)x), (3)
where tanh(·) acts element-wise.",4. A Toy Example,[0],[0]
"Inputs are dimension 250 and outputs dimension 100, while B and i are constant matrices with their elements generated IID from N (0, 10) and N (0, 3.5), respectively.",4. A Toy Example,[0],[0]
Each task therefore shares information in B but also contains task-specific information i.,4. A Toy Example,[0],[0]
The σi are the key parameters within this problem; they are fixed scalars which set the scales of the outputs fi.,4. A Toy Example,[0],[0]
A higher scale for fi induces a higher expected value of squared loss for that task.,4. A Toy Example,[0],[0]
"Such tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients.",4. A Toy Example,[0],[0]
"This scenario generally leads to suboptimal training dynamics when the higher σi tasks dominate the training across all tasks.
",4. A Toy Example,[0],[0]
"To train our toy models, we use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk.",4. A Toy Example,[0],[0]
A final affine transformation layer gives T final predictions (corresponding to T different tasks).,4. A Toy Example,[0],[0]
"To ensure valid analysis, we only compare models initialized to the same random values and fed data generated from the same fixed random seed.",4. A Toy Example,[0],[0]
"The asymmetry α is set low to 0.12 for these experiments, as the output functions fi are all of the same functional form and thus we expect the asymmetry between tasks to be minimal.
",4. A Toy Example,[0],[0]
"In these toy problems, we measure the task-normalized testtime loss to judge test-time performance, which is the sum of the test loss ratios for each task, ∑ i Li(t)/Li(0).",4. A Toy Example,[0],[0]
We do this because a simple sum of losses is an inadequate performance metric for multitask networks when different loss scales exist; higher loss scale tasks will factor disproportionately highly in the loss.,4. A Toy Example,[0],[0]
"There unfortunately exists no general single scalar which gives a meaningful measure of multitask performance in all scenarios, but our toy problem was specifically designed with tasks which are statistically identical except for their loss scales σi.",4. A Toy Example,[0],[0]
"There is therefore a clear measure of overall network performance, which is the sum of losses normalized by each task’s variance σ2i - equivalent (up to a scaling factor) to the sum of loss ratios.
",4. A Toy Example,[0],[0]
"For T = 2, we choose the values (σ0, σ1)",4. A Toy Example,[0],[0]
"= (1.0, 100.0) and show the results of training in the top panels of Figure 2.",4. A Toy Example,[0],[0]
"If we train with equal weightswi = 1, task 1 suppresses task 0 from learning due to task 1’s higher loss scale.",4. A Toy Example,[0],[0]
"However, gradient normalization increases w0(t) to counteract the larger gradients coming from T1, and the improved task balance results in better test-time performance.
",4. A Toy Example,[0],[0]
The possible benefits of gradient normalization become even clearer when the number of tasks increases.,4. A Toy Example,[0],[0]
"For T = 10, we sample the σi from a wide normal distribution and plot the results in the bottom panels of Figure 2.",4. A Toy Example,[0],[0]
GradNorm significantly improves test time performance over naı̈vely weighting each task the same.,4. A Toy Example,[0],[0]
"Similarly to the T = 2 case, for T = 10 the wi(t) grow larger for smaller σi tasks.
",4. A Toy Example,[0],[0]
"For both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by (Kendall et al., 2017).",4. A Toy Example,[0],[0]
"Uncertainty weighting, which enforces that wi(t) ∼ 1/Li(t), tends to grow the weights wi(t) too large and too quickly as the loss for each task drops.",4. A Toy Example,[0],[0]
"Although such networks train quickly at the onset, the training soon deteriorates.",4. A Toy Example,[0],[0]
"This issue is largely caused by the fact that uncertainty weighting allows wi(t) to change without constraint (compared to GradNorm which ensures∑ wi(t) = T always), which pushes the global learning rate up rapidly as the network trains.
",4. A Toy Example,[0],[0]
The traces for each wi(t) during a single GradNorm run are observed to be stable and convergent.,4. A Toy Example,[0],[0]
"In Section 5.3 we will see how the time-averaged weightsEt[wi(t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure.",4. A Toy Example,[0],[0]
"We use two variants of NYUv2 (Nathan Silberman & Fergus, 2012) as our main datasets.",5. Application to a Large Real-World Dataset,[0],[0]
"Please refer to the Supplementary Materials for additional results on a 9-task facial landmark dataset found in (Zhang et al., 2014).",5. Application to a Large Real-World Dataset,[0],[0]
"The standard NYUv2 dataset carries depth, surface normals, and semantic
segmentation labels (clustered into 13 distinct classes) for a variety of indoor scenes in different room types (bathrooms, living rooms, studies, etc.).",5. Application to a Large Real-World Dataset,[0],[0]
"NYUv2 is relatively small (795 training, 654 test images), but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm across various tasks.
",5. Application to a Large Real-World Dataset,[0],[0]
"We augment the standard NYUv2 depth dataset with flips and additional frames from each video, resulting in 90,000 images complete with pixel-wise depth, surface normals, and room keypoint labels (segmentation labels are, unfortunately, not available for these additional frames).",5. Application to a Large Real-World Dataset,[0],[0]
"Keypoint labels are professionally annotated by humans, while surface normals are generated algorithmically.",5. Application to a Large Real-World Dataset,[0],[0]
The full dataset is then split by scene for a 90/10 train/test split.,5. Application to a Large Real-World Dataset,[0],[0]
See Figure 6 for examples.,5. Application to a Large Real-World Dataset,[0],[0]
"We will generally refer to these two datasets as NYUv2+seg and NYUv2+kpts, respectively.
",5. Application to a Large Real-World Dataset,[0],[0]
All inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels.,5. Application to a Large Real-World Dataset,[0],[0]
"We use these resolutions following (Lee et al., 2017), which represents the state-of-the-art in room keypoint prediction and from which we also derive our VGG-style model architecture.",5. Application to a Large Real-World Dataset,[0],[0]
These resolutions also allow us to keep models relatively slim while not compromising semantic complexity in the ground truth output maps.,5. Application to a Large Real-World Dataset,[0],[0]
"We try two different models: (1) a SegNet (Badrinarayanan et al., 2015; Lee et al., 2017) network with a symmetric VGG16 (Simonyan & Zisserman, 2014) encoder/decoder,
and (2) an FCN (Long et al., 2015) network with a modified ResNet-50",5.1. Model and General Training Characteristics,[0],[0]
"(He et al., 2016) encoder and shallow ResNet decoder.",5.1. Model and General Training Characteristics,[0],[0]
"The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCN learns all upsampling filters.",5.1. Model and General Training Characteristics,[0],[0]
"The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet:",5.1. Model and General Training Characteristics,[0],[0]
stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers.,5.1. Model and General Training Characteristics,[0],[0]
"Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet.",5.1. Model and General Training Characteristics,[0],[0]
All model parameters are shared amongst all tasks until the final layer.,5.1. Model and General Training Characteristics,[0],[0]
"Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate GradNorm’s robustness to the choice of base architecture.
",5.1. Model and General Training Characteristics,[0],[0]
"We use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals.",5.1. Model and General Training Characteristics,[0],[0]
"As in (Lee et al., 2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss.",5.1. Model and General Training Characteristics,[0],[0]
"Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use ri(t) for each task i as a direct proxy for each task’s relative inverse training rate.
",5.1. Model and General Training Characteristics,[0],[0]
All runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference.,5.1. Model and General Training Characteristics,[0],[0]
All NYUv2 runs begin with a learning rate of 2e5.,5.1. Model and General Training Characteristics,[0],[0]
"NYUv2+kpts runs last 80000 steps with a learning rate
decay of 0.2 every 25000 steps.",5.1. Model and General Training Characteristics,[0],[0]
NYUv2+seg runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps.,5.1. Model and General Training Characteristics,[0],[0]
"Updating wi(t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting ((Kendall et al., 2017)) baseline.",5.1. Model and General Training Characteristics,[0],[0]
"All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen.",5.1. Model and General Training Characteristics,[0],[0]
We implement GradNorm using TensorFlow v1.2.1.,5.1. Model and General Training Characteristics,[0],[0]
In Table 1 we display the performance of GradNorm on the NYUv2+seg dataset.,5.2. Main Results on NYUv2,[0],[0]
"We see that GradNorm α = 1.5 improves the performance of all three tasks with respect to the equal-weights baseline (where wi(t) = 1 for all t,i), and either surpasses or matches (within statistical noise) the best performance of single networks for each task.",5.2. Main Results on NYUv2,[0],[0]
"The GradNorm Static network uses static weights derived from a GradNorm network by calculating the time-averaged weights Et[wi(t)] for each task during a GradNorm training run, and retraining a network with weights fixed to those values.",5.2. Main Results on NYUv2,[0],[0]
GradNorm thus can also be used to extract good values for static weights.,5.2. Main Results on NYUv2,[0],[0]
"We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.
",5.2. Main Results on NYUv2,[0],[0]
"To show how GradNorm can perform in the presence of a larger dataset, we also perform extensive experiments on the NYUv2+kpts dataset, which is augmented to a factor of 50x more data.",5.2. Main Results on NYUv2,[0],[0]
The results are shown in Table 2.,5.2. Main Results on NYUv2,[0],[0]
"As with the NYUv2+seg runs, GradNorm networks outperform other multitask methods, and either matches (within noise) or surpasses the performance of single-task networks.
",5.2. Main Results on NYUv2,[0],[0]
Figure 3 shows test and training loss curves for GradNorm (α = 1.5) and baselines on the larger NYUv2+kpts dataset for our VGG SegNet models.,5.2. Main Results on NYUv2,[0],[0]
"GradNorm improves test-time depth error by ∼ 5%, despite converging to a much higher training loss.",5.2. Main Results on NYUv2,[0],[0]
"GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry α = 1.5), and ultimately suppresses the depth weight wdepth(t) to lower than 0.10 (see Section 5.4 for more details).",5.2. Main Results on NYUv2,[0],[0]
"The same
trend exists for keypoint regression, and is a clear signal of network regularization.",5.2. Main Results on NYUv2,[0],[0]
"In contrast, uncertainty weighting (Kendall et al., 2017) always moves test and training error in the same direction, and thus is not a good regularizer.",5.2. Main Results on NYUv2,[0],[0]
"Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results.",5.2. Main Results on NYUv2,[0],[0]
"For our VGG SegNet, we train 100 networks from scratch with random task weights on NYUv2+kpts.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
Weights are sampled from a uniform distribution and renormalized to sum to T = 3.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm
α = 1.5 VGG SegNet network at the same 15000 steps.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"The results are shown in Figure 4.
",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even after 100 networks trained, grid search still falls short of our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights Et[wi(t)].",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"At an L2 distance of ∼ 3, grid search networks on average have almost double the errors per task compared to our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
GradNorm has therefore found the optimal grid search weights in one single training run.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
The only hyperparameter in our algorithm is the asymmetry α.,5.4. Effects of tuning the asymmetry α,[0],[0]
"The optimal value of α for NYUv2 lies near α = 1.5, while in the highly symmetric toy example in Section 4 we used α = 0.12.",5.4. Effects of tuning the asymmetry α,[0],[0]
"This observation reinforces our characterization of α as an asymmetry parameter.
",5.4. Effects of tuning the asymmetry α,[0],[0]
"Tuning α leads to performance gains, but we found that for NYUv2, almost any value of 0",5.4. Effects of tuning the asymmetry α,[0],[0]
< α < 3 will improve network performance over an equal weights baseline (see Supplementary for details).,5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 5 shows that higher values of α tend to push the weights wi(t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth).",5.4. Effects of tuning the asymmetry α,[0],[0]
"Remarkably, at α = 1.75 (not shown) wdepth(t) is suppressed to below 0.02 at no detriment to network performance on the depth task.",5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 6 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the NYUv2+seg and NYUv2+kpts datasets.",5.5. Qualitative Results,[0],[0]
"Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network.",5.5. Qualitative Results,[0],[0]
"Some
improvements are incremental, but GradNorm produces superior visual results in tasks for which there are significant quantitative improvements in Tables 1 and 2.",5.5. Qualitative Results,[0],[0]
"We introduced GradNorm, an efficient algorithm for tuning loss weights in a multi-task learning setting based on balancing the training rates of different tasks.",6. Conclusions,[0],[0]
"We demonstrated on both synthetic and real datasets that GradNorm improves multitask test-time performance in a variety of scenarios, and can accommodate various levels of asymmetry amongst the different tasks through the hyperparameter α.",6. Conclusions,[0],[0]
"Our empirical results indicate that GradNorm offers su-
perior performance over state-of-the-art multitask adaptive weighting methods and can match or surpass the performance of exhaustive grid search while being significantly less time-intensive.
",6. Conclusions,[0],[0]
"Looking ahead, algorithms such as GradNorm may have applications beyond multitask learning.",6. Conclusions,[0],[0]
"We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance.",6. Conclusions,[0],[0]
"We thus believe that our work not only provides a robust new algorithm for multitask learning, but also reinforces the powerful idea that gradient tuning is fundamental for training large, effective models on complex tasks.",6. Conclusions,[0],[0]
"Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly.",abstractText,[0],[0]
We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes.,abstractText,[0],[0]
"We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques.",abstractText,[0],[0]
"GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α.",abstractText,[0],[0]
"Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks.",abstractText,[0],[0]
"Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",abstractText,[0],[0]
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,title,[0],[0]
"Deep neural networks have become the state-of-the-art systems for image recognition (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Qiao et al., 2018; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Wang et al., 2017; Zeiler & Fergus, 2013) as well as other vision tasks (Chen et al., 2015; Girshick et al., 2014; Long et al., 2015; Qiao et al., 2017; Ren et al., 2015; Shen et al., 2015; Xie & Tu, 2015).",1. Introduction,[0],[0]
"The architectures keep going deeper, e.g., from five convolutional layers (Krizhevsky et al., 2012) to 1001 layers (He et al., 2016b).",1. Introduction,[0],[0]
"The benefit of deep architectures is their strong learning capacities because each new layer can potentially introduce more non-linearities and typically uses larger receptive fields (Simonyan & Zisserman, 2014).",1. Introduction,[0],[0]
"In addition, adding certain types of layers (e.g. (He et al., 2016b)) will not harm the performance theoretically since they can just learn identity mapping.",1. Introduction,[0],[0]
"This makes stacking up layers more appealing in the network designs.
",1. Introduction,[0],[0]
1Johns Hopkins University 2Shanghai University 3Hikvision Research.,1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siyuan Qiao <siyuan.qiao@jhu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Although deeper architectures usually lead to stronger learning capacities, cascading convolutional layers (e.g. VGG (Simonyan & Zisserman, 2014)) or blocks (e.g. ResNet (He et al., 2016a)) is not necessarily the only method to achieve this goal.",1. Introduction,[0],[0]
"In this paper, we present a new way to increase the depth of the networks as an alternative to stacking up convolutional layers or blocks.",1. Introduction,[0],[0]
Figure 2 provides an illustration that compares our proposed convolutional network that gradually updates the feature representations against the traditional convolutional network that computes its output simultaneously.,1. Introduction,[0],[0]
"By only adding an ordering to the channels without any additional computation, the later computed channels become deeper than the corresponding ones in the traditional convolutional network.",1. Introduction,[0],[0]
We refer to the neural networks with the proposed computation orderings on the channels as Gradually Updated Neural Networks (GUNN).,1. Introduction,[0],[0]
Figure 1 provides two examples of architecture designs based on cascading building blocks and GUNN.,1. Introduction,[0],[0]
"Without repeating the building blocks, GUNN increases the depths of the networks as well as their learning capacities.
",1. Introduction,[0],[0]
It is clear that converting plain networks to GUNN increases the depths of the networks without any additional computations.,1. Introduction,[0],[0]
"What is less obvious is that GUNN in fact eliminates the overlap singularities inherent in the loss landscapes of the cascading-based convolutional networks, which have been shown to adversely affect the training of deep neural networks as well as their performances (Wei et al., 2008;
Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"Overlap singularity is when internal neurons collapse into each other, i.e. they are unidentifiable by their activations.",1. Introduction,[0],[0]
"It happens in the networks, increases the training difficulties and degrades the performances (Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"However, if a plain network is converted to GUNN, the added computation orderings will break the symmetry between the neurons.",1. Introduction,[0],[0]
We prove that the internal neurons in GUNN are impossible to collapse into each other.,1. Introduction,[0],[0]
"As a result, the effective dimensionality can be kept during training and the model will be free from the degeneracy caused by collapsed neurons.",1. Introduction,[0],[0]
"Reflected in the training dynamics and the performances, this means that converting to GUNN will make the plain networks easier to train and perform better.",1. Introduction,[0],[0]
"Figure 3 compares the training dynamics of a 15-layer plain network on CIFAR-10 dataset (Krizhevsky & Hinton, 2009) before and after converted to GUNN.
",1. Introduction,[0],[0]
"In this paper, we test our proposed GUNN on highly competitive benchmark datasets, i.e. CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015).",1. Introduction,[0],[0]
Experimental results demonstrate that our proposed GUNNbased networks achieve the state-of-the-art performances compared with the previous cascading-based architectures.,1. Introduction,[0],[0]
"The research focuses of image recognition have moved from feature designs (Dalal & Triggs, 2005; Lowe, 2004) to architecture designs (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Sermanet et al., 2014; Simonyan
& Zisserman, 2014; Szegedy et al., 2015; Xie et al., 2017; Zeiler & Fergus, 2013) due to the recent success of the deep neural networks.",2. Related Work,[0],[0]
"Highway Networks (Srivastava et al., 2015) proposed architectures that can be trained end-to-end with more than 100 layers.",2. Related Work,[0],[0]
The main idea of Highway Networks is to use bypassing paths.,2. Related Work,[0],[0]
"This idea was further investigated in ResNet (He et al., 2016a), which simplifies the bypassing paths by using only identity mappings.",2. Related Work,[0],[0]
"As learning ultra-deep networks became possible, the depths of the models have increased tremendously.",2. Related Work,[0],[0]
"ResNet with pre-activation (He et al., 2016b) and ResNet with stochastic depth (Huang et al., 2016) even managed to train neural networks with more than 1000 layers.",2. Related Work,[0],[0]
"FractalNet (Larsson et al., 2016) argued that in addition to summation, concatenation also helps train a deep architecture.",2. Related Work,[0],[0]
"More recently, ResNeXt (Xie et al., 2017) used group convolutions in ResNet and outperformed the original ResNet.",2. Related Work,[0],[0]
"DenseNet (Huang et al., 2017b) proposed an architecture with dense connections by feature concatenation.",2. Related Work,[0],[0]
"Dual Path Net (Chen et al., 2017) finds a middle point between ResNet and DenseNet by concatenating them in two paths.",2. Related Work,[0],[0]
"Unlike the above cascading-based methods, GUNN eliminates the overlap singularities caused by the architecture symmetry.",2. Related Work,[0],[0]
"The detailed analyses can be found in Section 4.3.
",2. Related Work,[0],[0]
"Alternative to increasing the depth of the neural networks, another trend is to increase the widths of the networks.",2. Related Work,[0],[0]
"GoogleNet (Szegedy et al., 2015; 2016) proposed an Inception module to concatenate feature maps produced by different filters.",2. Related Work,[0],[0]
"Following ResNet (He et al., 2016a), the WideResNet (Zagoruyko & Komodakis, 2016) argued that compared with increasing the depth, increasing the width of the networks can be more effective in improving the performances.",2. Related Work,[0],[0]
"Besides varying the width and the depth, there are also other design strategies for deep neural networks (Hariharan et al., 2015; Kontschieder et al., 2015; Pezeshki et al., 2016; Rasmus et al., 2015; Yang & Ramanan, 2015).",2. Related Work,[0],[0]
"Deeply-Supervised Nets (Lee et al., 2014) used auxiliary classifiers to provide direct supervisions for the internal layers.",2. Related Work,[0],[0]
"Network in Network (Lin et al., 2013) adds micro perceptrons to the convolutional layers.",2. Related Work,[0],[0]
"We consider a feature transformation F : Rm×n → Rm×n, where n denotes the channel of the features and m denotes the feature location on the 2-D feature map.",3.1. Feature Update,[0],[0]
"For example, F can be a convolutional layer with n channels for both the input and the output.",3.1. Feature Update,[0],[0]
Let x ∈,3.1. Feature Update,[0],[0]
Rm×n be the input and y ∈,3.1. Feature Update,[0],[0]
"Rm×n be the output, we have
y = F(x) (1)
Suppose that F can be decomposed into channel-wise transformation Fc(·) that are independent with eath other, then for any location k and channel c we have
ykc = Fc(xr(k))",3.1. Feature Update,[0],[0]
"(2)
where xr(k) denotes the receptive field of the location k",3.1. Feature Update,[0],[0]
"and Fc denotes the transformation on channel c.
Let UC denote a feature update on channel set C, i.e.,
UC(x) :",3.1. Feature Update,[0],[0]
"y k c = Fc(xr(k)),∀c ∈ C, k ykc",3.1. Feature Update,[0],[0]
"= x k c ,∀c",3.1. Feature Update,[0],[0]
"∈ C, k
(3)
",3.1. Feature Update,[0],[0]
"Then, UC = F when C = {1, ..., n}.",3.1. Feature Update,[0],[0]
"By defining the feature update UC on channel set C, the commonly used one-layer CNN is a special case of feature updates where every channel is updated simultaneously.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, we can also update the channels gradually.",3.2. Gradually Updated Neural Networks,[0],[0]
"For example, the proposed GUNN can be formulated by
GUNN(x) =",3.2. Gradually Updated Neural Networks,[0],[0]
(Ucl ◦ Uc(l−1),3.2. Gradually Updated Neural Networks,[0],[0]
"◦ ... ◦ Uc2 ◦ Uc1)(x)
",3.2. Gradually Updated Neural Networks,[0],[0]
"where l⋃
i=1
ci = {1, 2, ..., n} and ci ∩ cj = Φ, ∀i 6= j
(4) When l = 1, GUNN is equivalent to F .
",3.2. Gradually Updated Neural Networks,[0],[0]
"Note that the number of parameters and computation of GUNN are the same as those of the corresponding F for any partitions c1, ..., cl of {1, ..., n}.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, by decomposing F into channel-wise transformations and sequentially applying them, the later computed channels are deeper than the previous ones.",3.2. Gradually Updated Neural Networks,[0],[0]
"As a result, the depth of the network can be increased, as well as the network’s learning capacity.",3.2. Gradually Updated Neural Networks,[0],[0]
"We consider the residual learning proposed by ResNet (He et al., 2016a) in our model.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Specifically, we consider the channel-wise transformation Fc : Rm×n → Rm×1 to be
Fc(x) = Gc(x) + xc (5)
",3.3. Channel-wise Update by Residual Learning,[0],[0]
Algorithm 1 Back-propagation for GUNN Input :U(·) =,3.3. Channel-wise Update by Residual Learning,[0],[0]
(Ucl ◦ Uc(l−1),3.3. Channel-wise Update by Residual Learning,[0],[0]
"◦ ... ◦ Uc1)(·), input x,
output y = U(x), gradients ∂L/∂y, and parameters Θ for U .
",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Output :∂L/∂Θ, ∂L/∂x ∂L/∂x← ∂L/∂y for i←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"l to 1 do
yc ←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"xc, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"∈ ci ∂L/∂y, ∂L/∂Θci ← BP(y, ∂L/∂x, Uci ,Θci) (∂L/∂x)c ← (∂L/∂y)c, ∀c ∈ ci (∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"6∈ ci
end
where Gc is a convolutional neural network Gc : Rm×n → Rm×1.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"The motivation of expressing F in a residual learning manner is to reduce overlap singularities (Orhan & Pitkow, 2018), which will be discussed in Section 4.",3.3. Channel-wise Update by Residual Learning,[0],[0]
Here we show the backpropagation algorithm for learning the parameters in GUNN that uses the same amount of computations and memory as in F .,3.4. Learning GUNN by Backpropagation,[0],[0]
"In Eq. 4, let the feature update Uci be parameterized by Θci .",3.4. Learning GUNN by Backpropagation,[0],[0]
"Let BP(x, ∂L/∂y, f,Θ) be the back-propagation algorithm for differentiable function y = f(x; Θ) with the loss L and the parameters Θ. Algorithm 1 presents the back-propagation algorithm for GUNN.",3.4. Learning GUNN by Backpropagation,[0],[0]
"Since Uci has the residual structures (He et al., 2016a), the last two steps can be merged into
(∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c (6)
which further simplifies the implementation.",3.4. Learning GUNN by Backpropagation,[0],[0]
It is easy to see that converting networks to GUNN-based does not increase the memory usage in feed-forwarding.,3.4. Learning GUNN by Backpropagation,[0],[0]
"Given Algorithm 1, converting networks to GUNN will not affect the memory in both the training and the evaluation.",3.4. Learning GUNN by Backpropagation,[0],[0]
Overlap singularities are inherent in the loss landscapes of some network architectures which are caused by the nonidentifiability of subsets of the neurons.,4. GUNN Eliminates Overlap Singularities,[0],[0]
"They are identified and discussed in previous work (Wei et al., 2008; Anandkumar & Ge, 2016; Orhan & Pitkow, 2018), and are shown to be harmful for the performances of deep networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Intuitively, overlap singularities exist in architectures where the internal neurons collapse into each other.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"As a result, the models are degenerate and the effective dimensionality is reduced.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"(Orhan & Pitkow, 2018) demonstrated through experiments that residual learning (see Eq. 5) helps to reduce the overlap singularities in deep networks, which partly explains the exceptional performances of ResNet (He et al., 2016a) compared with plain networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"In the following, we first use linear transformation as an example to demonstrate
how GUNN-based networks break the overlap singularities.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Then, we generalize the results to ReLU DNN.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Finally, we compare GUNN with the previous state-of-the-art network architectures from the perspective of singularity elimination.",4. GUNN Eliminates Overlap Singularities,[0],[0]
Consider a linear function y = f(x) :,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Rn → Rn such that
yi = n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (7)
Suppose that there exists a pair of collapsed neurons yp and yq (p < q).",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, for ∀x, yp = yq, and the equality holds after any number of gradient descents, i.e. ∆yp = ∆yq .
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
Eq. 7 describes a plain network.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"The solution for the existence of yp and yq is that ωp,j = ωq,j ,∀j.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This is the case that is mostly discussed previously, which happens in the networks and degrades the performances.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When we add the residual learning, Eq. 7 becomes
yi = xi + n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (8)
Collapsed neurons require that ωp,p + 1 = ωq,p, ωq,q + 1 = ωp,q.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This will make the collapse of yp and yq very hard when ω is initialized from a normal distribution N (0, √ 2/n) as in ResNet, but still possible.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Next, we convert Eq. 8 to GUNN, i.e.,
yi = xi + i−1∑ j=1 ωi,jyj + n∑ j=i ωi,jxj , ∀i ∈ {1, .., n} (9)
Suppose that yp and yq (p < q) collapse.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Consider ∆y, the value difference at x after one step of gradient descent on ω with input x, ∂L/∂y and learning rate .",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When → 0,
∆yi = ∂L
∂yi ( i−1∑ j=1 y2j + n∑ j=i x2j )",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"+ i−1∑ j=1 ωi,j∆yj (10)
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"As ∆yp = ∆yq,∀x, we have ωq,j = 0, ∀j : p",4.1. Overlap Singularities in Linear Transformations,[0],[0]
< j < q.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"But this condition will be broken in the next update; thus, q = p + 1.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, we derive that yp = yq = 0.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
But these will also be broken in the next step of gradient descent optimization.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Hence, yp and yq cannot collapse into each other.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
The complete proof can be found in the appendix.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"In practice, architectures are usually composed of several linear layers and non-linearity layers.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Analyzing all the possible architectures is beyond our scope.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Here, we discuss the commonly used ReLU DNN, in which only linear transformations and ReLUs are used by simple layer cascading.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Following the notations in §3, we use y = G(x) + x, in which G(x) is a ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Note that G is continuous piecewise linear (PWL) function (Arora et al., 2018), which means that there exists a finite set of polyhedra whose union is Rn, and G is affine linear over each polyhedron.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Suppose that we convert G(x)+x to GUNN and there exists a pair of collapsed neurons yp and yq (p < q).,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, the set of polyhedra for yp is the same as for yq.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Let P be a polyhedron for yp and yq defined above.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, ∀x,P, i,
yi = xi + i−1∑ j=1 ωi,j(P)yj + n∑ j=i ωi,j(P)xj (11)
where ω(P) denotes the parameters for polyhedron P. Note that on each P, y is a function of x in the form of Eq. 9; hence, yp and yq cannot collapse into each other.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Since the union of all polyhedra is Rn, we conclude that GUNN eliminates the overlap singularities in ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"The previous two subsections consider the GUNN conversion where |ci| = 1,∀i (see Eq. 4).",4.3. Discussions and Comparisons,[0],[0]
But this will slow down the computation on GPU due to the data dependency.,4.3. Discussions and Comparisons,[0],[0]
"Without specialized hardware or library support, we decide to increase |ci| to > 10.",4.3. Discussions and Comparisons,[0],[0]
"The resulted models run at the speed between ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b).",4.3. Discussions and Comparisons,[0],[0]
But this change introduces singularities into the channels from the same set ci.,4.3. Discussions and Comparisons,[0],[0]
"Then, the residual learning helps GUNN to reduce the singularities within the same set ci since we initialize the parameters from a normal distributionN (0, √ 2/n).",4.3. Discussions and Comparisons,[0],[0]
"We will compare the results of GUNN with and without residual learning in the experiments.
",4.3. Discussions and Comparisons,[0],[0]
We compare GUNN with the state-of-the-art architectures from the perspective of overlap singularities.,4.3. Discussions and Comparisons,[0],[0]
"ResNet (He et al., 2016a) and its variants use residual learning, which reduces but cannot eliminate the singularities.",4.3. Discussions and Comparisons,[0],[0]
"ResNeXt (Xie et al., 2017) uses group convolutions to break the symmetry between groups, which further helps to avoid neuron collapses.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet (Huang et al., 2017b) concatenates the outputs of layers as the input to the next layer.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet and GUNN both create dense connections, while DenseNet reuses the outputs by concatenating and GUNN by adding them back to the inputs.",4.3. Discussions and Comparisons,[0],[0]
But the channels within the same layer of DenseNet are still possible to collapse into each other since they are symmetric.,4.3. Discussions and Comparisons,[0],[0]
"In contrast, adding back makes residual learning possible in GUNN.",4.3. Discussions and Comparisons,[0],[0]
This makes residual learning indispensable in GUNN-based networks.,4.3. Discussions and Comparisons,[0],[0]
"In this section, we will present the details of our architectures for the CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015) datasets.",5. Network Architectures,[0],[0]
"Since the proposed GUNN is a method for increasing the depths of the convolutional networks, specifying the architectures to be converted is equivalent to specifying the GUNN-based architectures.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The architectures before conversion, the Simultaneously Updated Neural Networks (SUNN), become natural baselines for our proposed GUNN networks.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We first study what baseline architectures can be converted.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There are two assumptions about the feature transformation F (see Eq. 1): (1) the input and the output sizes are the same, and (2) F is channel-wise decomposable.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the first assumption, we will first use a convolutional layer with Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to transform the feature space to a new space where the number of the channels is wanted.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the second assumption, instead of directly specifying the transform F , we focus on designing Fci , where ci is a subset of the channels (see Eq. 4).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To be consistent with the term update used in GUNN and SUNN, we refer to Fci as the update units for channels ci.
Bottleneck Update Units In the architectures proposed in this paper, we adopt bottleneck neural networks as shown in Figure 4 for the update units for both the SUNN and GUNN.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Suppose that the update unit maps the input features of channel size nin to the output features of size nout.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Each unit contains three convolutional layers.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The first convolutional layer transforms the input features to K × nout using a 1× 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The second convolutional layer is of kernel size 3× 3, stride 1, and padding 1, outputting the features of size K × nout.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The third layer computes the features of size nout using a 1 × 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The output is then added back to the input, following the residual architecture proposed in ResNet (He et al., 2016a).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We add batch normalization layer (Ioffe & Szegedy, 2015) and ReLU layer (Nair & Hinton, 2010) after the first and the second convolutional layers, while only adding batch
normalization layer after the third layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Stacking up M update units also generates a new one.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In total, we have two hyperparameters for designing an update unit: the expansion rate K and the number of the 3-layer update units M .
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"One Resolution, One Representation Our architectures will have only one representation at one resolution besides the pooling layers and the convolutional layers that initialize the needed numbers of channels.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Take the architecture in Table 1 as an example.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
There are two processes for each resolution.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The first one is the transition process, which computes the initial features with the dimensions of the next resolution, then down samples it to 1/4 using a 2×2 average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
A convolutional operation is needed here because F is assumed to have the same input and output sizes.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The next process is using GUNN to update this feature space gradually.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Each channel will only be updated once, and all channels will be updated after this process.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Unlike most of the previous networks, after this two processes, the feature transformations at this resolution are complete.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There will be no more convolutional layers or blocks following this feature representation, i.e., one resolution, one representation.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, the network will compute the initial features for the next resolution, or compute the final vector representation of the entire image by a global average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"By designing networks in this way, SUNN networks usually have about 20 layers before converting to GUNN-based networks.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Channel Partitions With the clearly defined update units, we can easily build SUNN and GUNN layers by using the units to update the representations following Eq. 4.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The hyperparameters for the SUNN/GUNN layer are the number of the channels N and the partition over those channels.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In our proposed architectures, we evenly partition the channels into P segments.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, we can useN and P to represent the configuration of a layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Together with the hyperparameters in the update units, we have four hyperparameters to tune for one SUNN/GUNN layer, i.e. {N,P,K,M}.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We have implemented two neural networks based on GUNN to compete with the previous state-of-the-art methods on CIFAR datasets, i.e., GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
Table 1 shows the big picture of GUNN-15.,5.2. Architectures for CIFAR,[0],[0]
"Here, we present the details of the hyperparameter settings for GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-15, we have three GUNN layers, Conv2, Conv3 and Conv4.",5.2. Architectures for CIFAR,[0],[0]
"The configuration for Conv2 is {N = 240, P = 20,K = 2,M = 1}, the configuration for Conv3 is {N = 300, P = 25,K = 2,M = 1}, and the configuration for Conv4 is {N = 360, P = 30,K = 2,M = 1}.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-24, based on GUNN-15, we change the number of output channels of Conv1 to 720, Trans1 to 900, Trans2 to 1080, and Trans3 to 1080.",5.2. Architectures for CIFAR,[0],[0]
"The hyperparameters are {N = 720, P = 20,K = 3,M = 2} for
Conv2, {N = 900, P = 25,K = 3,M = 2} for Conv3, and {N = 1080, P = 30,K = 3,M = 2} for Conv3.",5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-15 is 1585746 for CIFAR10 and 1618236 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-24 is 29534106 for CIFAR-10 and 29631396 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
"The GUNN-15 is aimed to compete with the methods published in an early stage by using a much smaller model, while GUNN-24 is targeted at comparing with ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b) to get the state-of-the-art performance.",5.2. Architectures for CIFAR,[0],[0]
We implement a neural network GUNN-18 to compete with the state-of-the-art neural networks on ImageNet with a similar number of parameters.,5.3. Architectures for ImageNet,[0],[0]
Table 2 shows the big picture of the neural network architecture of GUNN-18.,5.3. Architectures for ImageNet,[0],[0]
"Here, we present the detailed hyperparameters for the GUNN layers in GUNN-18.",5.3. Architectures for ImageNet,[0],[0]
"The GUNN layers include Conv2, Conv3, Conv4 and Conv5.",5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 400, P = 10,K = 2,M = 1} for Conv2,
{N = 800, P = 20,K = 2,M = 1} for Conv3, {N = 1600, P = 40,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 28909736.,5.3. Architectures for ImageNet,[0],[0]
"The GUNN-18 is targeted at competing with the previous state-of-the-art methods that have similar numbers of parameters, e.g., ResNet50 (Xie et al., 2017), ResNeXt-50 (Xie et al., 2017) and DenseNet-264 (Huang et al., 2017b).
",5.3. Architectures for ImageNet,[0],[0]
We also implement a wider GUNN-based neural networks Wide-GUNN-18 for better capacities.,5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 1200, P = 30,K = 2,M = 1} for Conv2, {N = 1600, P = 40,K = 2,M = 1} for Conv3, {N = 2000, P = 50,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 45624936.,5.3. Architectures for ImageNet,[0],[0]
"The Wide-GUNN-18 is targeted at competing with ResNet-101, ResNext-101, DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",5.3. Architectures for ImageNet,[0],[0]
"In this section, we demonstrate the effectiveness of the proposed GUNN on several benchmark datasets.",6. Experiments,[0],[0]
"CIFAR CIFAR (Krizhevsky & Hinton, 2009) has two color image datasets: CIFAR-10 (C10) and CIFAR-100 (C100).",6.1. Benchmark Datasets,[0],[0]
Both datasets consist of natural images with the size of 32× 32 pixels.,6.1. Benchmark Datasets,[0],[0]
"The CIFAR-10 dataset has 10 categories, while the CIFAR-100 dataset has 100 categories.",6.1. Benchmark Datasets,[0],[0]
"For both of the datasets, the training and test set contain 50, 000 and 10, 000 images, respectively.",6.1. Benchmark Datasets,[0],[0]
"To fairly compare our method with the state-of-the-arts (He et al., 2016a; Huang et al., 2017b; 2016; Larsson et al., 2016; Lee et al., 2014; Lin et al., 2013; Romero et al., 2014; Springenberg et al., 2014; Srivastava et al., 2015; Xie et al., 2017), we use the same training and testing strategies, as well as the data processing methods.",6.1. Benchmark Datasets,[0],[0]
"Specifically, we adopt a commonly used data augmentation scheme, i.e., mirroring and shifting, for these two datasets.",6.1. Benchmark Datasets,[0],[0]
"We use channel means and standard derivations to normalize the images for data pre-processing.
",6.1. Benchmark Datasets,[0],[0]
"ImageNet The ImageNet dataset (Russakovsky et al., 2015) contains about 1.28 million color images for training and 50, 000 for validation.",6.1. Benchmark Datasets,[0],[0]
The dataset has 1000 categories.,6.1. Benchmark Datasets,[0],[0]
"We adopt the same data augmentation methods as in the state-of-the-art architectures (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017) for training.",6.1. Benchmark Datasets,[0],[0]
"For testing, we use single-crop at the size of 224× 224.",6.1. Benchmark Datasets,[0],[0]
"Following the
state-of-the-arts (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017), we report the validation error rates.",6.1. Benchmark Datasets,[0],[0]
We train all of our networks using stochastic gradient descents.,6.2. Training Details,[0],[0]
"On CIFAR-10/100 (Krizhevsky & Hinton, 2009), the initial learning rate is set to 0.1, the weight decay is set to 1e−4, and the momentum is set to 0.9 without dampening.",6.2. Training Details,[0],[0]
We train the models for 300 epochs.,6.2. Training Details,[0],[0]
The learning rate is divided by 10 at 150th epoch and 225th epoch.,6.2. Training Details,[0],[0]
"We set the batch size to 64, following (Huang et al., 2017b).",6.2. Training Details,[0],[0]
"All the results reported for CIFAR, regardless of the detailed configurations, were trained using 4 NVIDIA Titan X GPUs with the data parallelism.",6.2. Training Details,[0],[0]
"On ImageNet (Russakovsky et al., 2015), the learning rate is also set to 0.1 initially, and decreases following the schedule in DenseNet (Huang et al., 2017b).",6.2. Training Details,[0],[0]
The batch size is set to 256.,6.2. Training Details,[0],[0]
"The network parameters are also initialized following (He et al., 2016a).",6.2. Training Details,[0],[0]
We use 8 Tesla V100 GPUs with the data parallelism to get the reported results.,6.2. Training Details,[0],[0]
"Our results are directly comparable with ResNet, WideResNet, ResNeXt and DenseNet.",6.2. Training Details,[0],[0]
We train two models GUNN-15 and GUNN-24 for the CIFAR-10/100 dataset.,6.3. Results on CIFAR,[0],[0]
Table 3 shows the comparisons between our method and the previous state-of-the-art methods.,6.3. Results on CIFAR,[0],[0]
Our method GUNN achieves the best results in the test of both the single model and the ensemble test.,6.3. Results on CIFAR,[0],[0]
"Here, we use Snapshot Ensemble (Huang et al., 2017a).
",6.3. Results on CIFAR,[0],[0]
Baseline Methods,6.3. Results on CIFAR,[0],[0]
Here we present the details of baseline methods in Table 3.,6.3. Results on CIFAR,[0],[0]
"The performances of ResNet (He et al., 2016a) are reported in Stochastic Depth (Huang et al., 2016) for both C10 and C100.",6.3. Results on CIFAR,[0],[0]
"The WideResNet (Zagoruyko & Komodakis, 2016) WRN-40-10 is reported in their official code repository on GitHub.",6.3. Results on CIFAR,[0],[0]
"The ResNeXt in the third group is of configuration 16 × 64d, which has the best result reported in the paper (Xie et al., 2017).",6.3. Results on CIFAR,[0],[0]
"The DenseNet is of configuration DenseNet-BC (k = 40), which achieves the best performances on CIFAR-10/100.",6.3. Results on CIFAR,[0],[0]
"The Snapshot Ensemble (Huang et al., 2017a) uses 6 DenseNet-100 to ensemble during inference.",6.3. Results on CIFAR,[0],[0]
"We do not compare with methods that use more data augmentation (e.g. (Zhang et al., 2017)) or stronger regularizations (e.g. (Gastaldi, 2017)) for the fairness of comparison.
",6.3. Results on CIFAR,[0],[0]
"Ablation Study For ablation study, we compare GUNN with SUNN, i.e., the networks before the conversion.",6.3. Results on CIFAR,[0],[0]
"Table 5 shows the comparison results, which demonstrate the effectiveness of GUNN.",6.3. Results on CIFAR,[0],[0]
We also compare the performances of GUNN with and without residual learning.,6.3. Results on CIFAR,[0],[0]
"We evaluate the GUNN on the ImageNet classification task, and compare our performances with the state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"These methods include VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016a), ResNeXt (Xie
et al., 2017), DenseNet (Huang et al., 2017b), DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",6.4. Results on ImageNet,[0],[0]
The comparisons are shown in Table 4.,6.4. Results on ImageNet,[0],[0]
"The results of ours, ResNeXt, and DenseNet are directly comparable as these methods use the same framework for training and testing networks.",6.4. Results on ImageNet,[0],[0]
"Table 4 groups the methods by their numbers of parameters, except VGGNet which has 1.38× 108 parameters.
",6.4. Results on ImageNet,[0],[0]
"The results presented in Table 4 demonstrate that with the similar number of parameters, GUNN can achieve comparable performances with the previous state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"For GUNN-18, we also conduct an ablation experiment by comparing the corresponding SUNN with GUNN of the same configuration.",6.4. Results on ImageNet,[0],[0]
"Consistent with the experimental results on the CIFAR-10/100 dataset, the proposed GUNN improves the accuracy on ImageNet dataset.",6.4. Results on ImageNet,[0],[0]
"In this paper, we propose Gradually Updated Neural Network (GUNN), a novel, simple yet effective method to increase the depths of neural networks as an alternative to cascading layers.",7. Conclusions,[0],[0]
"GUNN is based on Convolutional Neural Networks (CNNs), but differs from CNNs in the way of computing outputs.",7. Conclusions,[0],[0]
The outputs of GUNN are computed gradually rather than simultaneously as in CNNs in order to increase the depth.,7. Conclusions,[0],[0]
"Essentially, GUNN assumes the input and the output are of the same size and adds a computation ordering to the channels.",7. Conclusions,[0],[0]
The added ordering increases the receptive fields and non-linearities of the later computed channels.,7. Conclusions,[0],[0]
"Moreover, it eliminates the overlap singularities inherent in the traditional convolutional networks.",7. Conclusions,[0],[0]
We test GUNN on the task of image recognition.,7. Conclusions,[0],[0]
"The evaluations are done in three highly competitive benchmarks, CIFAR10, CIFAR-100 and ImageNet.",7. Conclusions,[0],[0]
The experimental results demonstrate the effectiveness of the proposed GUNN on image recognition.,7. Conclusions,[0],[0]
"In the future, since the proposed GUNN can be used to replace CNNs in other neural networks, we will study the applications of GUNN in other visual tasks, such as object detection and semantic segmentation.",7. Conclusions,[0],[0]
"We thank Wanyu Huang, Huiyu Wang and Chenxi Liu for their insightful comments and suggestions.",Acknowledgments,[0],[0]
We gratefully acknowledge funding supports from NSF award CCF-1317376 and ONR N00014-15-1-2356.,Acknowledgments,[0],[0]
This work was also supported in part by the National Natural Science Foundation of China under Grant 61672336.,Acknowledgments,[0],[0]
Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition.,abstractText,[0],[0]
The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks.,abstractText,[0],[0]
"In this paper, we present an alternative method to increase the depth.",abstractText,[0],[0]
"Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner.",abstractText,[0],[0]
"The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better.",abstractText,[0],[0]
Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.,abstractText,[0],[0]
Gradually Updated Neural Networks for Large-Scale Image Recognition,title,[0],[0]
"In recent years, there has been an explosion of interest in sequence labelling tasks.",1. Introduction,[0],[0]
"Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR) (Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al.,
*Equal contribution 1Baidu Silicon Valley AI Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089, USA.",1. Introduction,[0],[0]
"Correspondence to: Hairong Liu <liuhairong@baidu.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016), machine translation (Sébastien et al., 2015), and parsing (Vinyals et al., 2015).",1. Introduction,[0],[0]
"These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al., 2016a), and 2) a fixed and pre-determined decomposition of target sequences into these basic units.",1. Introduction,[0],[0]
"While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions.",1. Introduction,[0],[0]
"CTC models are especially harmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space - an assumption which is often violated in practice.
",1. Introduction,[0],[0]
"The problem with fixed set of basic units is obvious: it is really hard, if not impossible, to determine the optimal set of basic units beforehand.",1. Introduction,[0],[0]
"For example, in English ASR, if we use words as basic units, we will need to deal with the large vocabulary-sized softmax, as well as rare words and data sparsity problem.",1. Introduction,[0],[0]
"On the other hand, if we use characters as basic units, the model is forced to learn the complex rules of English spelling and pronunciation.",1. Introduction,[0],[0]
"For example, the ""oh"" sound can be spelled in any of following ways, depending on the word it occurs in - { o, oa, oe, ow, ough, eau, oo, ew }.",1. Introduction,[0],[0]
"While CTC can easily model commonly co-occuring grams together, it is impossible to give roughly equal probability to many possible spellings when transcribing unseen words.",1. Introduction,[0],[0]
"Most speech recognition systems model phonemes, sub-phoneme units and senones e.g, (Xiong et al., 2016a) to get around these problems.",1. Introduction,[0],[0]
"Similarly, state-of-the-art neural machine translation systems use pre-segmented word pieces e.g, (Wu et al., 2016a) aiming to find the best of both worlds.
",1. Introduction,[0],[0]
"In reality, groups of characters are typically cohesive units for many tasks.",1. Introduction,[0],[0]
"For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as ‘tion’ and ‘eaux’).",1. Introduction,[0],[0]
"For the machine translation task, there may be values in decomposing words as root words and extensions (so that meaning may be shared explicitly between ‘paternal’ and ‘paternity’).",1. Introduction,[0],[0]
"Since this information is already available in the training data, it is perhaps, better to let the model figure it out by itself.",1. Introduction,[0],[0]
"At the same time, it raises another import question: how to de-
compose a target sequence into basic units?",1. Introduction,[0],[0]
"This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine.",1. Introduction,[0],[0]
"Recently, there are some interesting attempts in these directions in the seq2seq framework.",1. Introduction,[0],[0]
"For example, Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as a function of both input sequence and the output sequence.
",1. Introduction,[0],[0]
"In this work, we propose Gram-CTC - a strictly more general version of CTC - to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target sequences into sequences of grams.",1. Introduction,[0],[0]
"Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.",1. Introduction,[0],[0]
"Since it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks.",1. Introduction,[0],[0]
"Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.",1. Introduction,[0],[0]
"The basic text units that previous works utilized for text prediction tasks (e.g,, automatic speech recognition, handwriting recognition, machine translation, and image captioning) can be generally divided into two categories: handcrafted ones and learning-based ones.
",2. Related Work,[0],[0]
Hand-crafted Basic Units.,2. Related Work,[0],[0]
"Fixed sets of characters (graphemes) (Graves et al., 2006; Amodei et al., 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al., 2016; Sébastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.",2. Related Work,[0],[0]
"Using these fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.
",2. Related Work,[0],[0]
• Word-segmented models remove the component of learning to spell and thus enable direct optimization towards reducing Word Error Rate (WER).,2. Related Work,[0],[0]
"However, these models suffer from having to handle a large vocabulary (1.7 million in (Soltau et al., 2016)), out-of-vocabulary words (Soltau et al., 2016; Sébastien et al., 2015) and data sparsity problems (Soltau et al., 2016).
",2. Related Work,[0],[0]
"• Using characters results in much smaller vocabularies (e.g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015),
which is very noisy for languages like English.
",2. Related Work,[0],[0]
"• Word-pieces lie at the middle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.
",2. Related Work,[0],[0]
"• For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).",2. Related Work,[0],[0]
"However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.
",2. Related Work,[0],[0]
Learning-based Basic Units.,2. Related Work,[0],[0]
"More recently, attempts have been made to learn basic unit sets automatically.",2. Related Work,[0],[0]
"(Luong and Manning, 2016) proposed a hybrid WordCharacter model which translates mostly at the word level and consults the character components for rare words.",2. Related Work,[0],[0]
"Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decompositions framework to decomposes target sequences with variable length-ed basic units as a function of both input sequence and the output sequence.
",2. Related Work,[0],[0]
"There exist some earlier works on the “unit discovery” task (Cartwright and Brent, 1994; Goldwater et al., 2006).",2. Related Work,[0],[0]
"A standard problem with MLE solutions to this task is that there are degenerate solutions, i.e., predicting the full corpus with probability 1 at the start.",2. Related Work,[0],[0]
Often Bayesian priors or “minimum description length” constraints are used to remedy this.,2. Related Work,[0],[0]
"CTC (Graves et al., 2006) is a very popular method in seq2seq learning since it does not require the alignment information between inputs and outputs, which is usually expensive, if not impossible, to obtain.
",3.1. CTC,[0],[0]
"Since there is no alignment information, CTC marginalizes over all possible alignments.",3.1. CTC,[0],[0]
"That is, it tries to maximize p(l|x) =",3.1. CTC,[0],[0]
"∑ π p(π|x), where x is input, and π represent a valid alignment.",3.1. CTC,[0],[0]
"For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.",3.1. CTC,[0],[0]
"For the details, please refer to the original paper (Graves et al., 2006).",3.1. CTC,[0],[0]
"In CTC, the basic units are fixed, which is not desirable in some applications.",3.2. From CTC to Gram-CTC,[0],[0]
"Here we generalize CTC by considering a sequence of basic units, called gram, as a whole, which is usually more reasonable in many applications.
",3.2. From CTC to Gram-CTC,[0],[0]
"Let G be a set of n-grams of the set of basic units C of the target sequence, and τ be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G|+1 units, that is, the probability over all grams inG and one additional symbol, blank.",3.2. From CTC to Gram-CTC,[0],[0]
"To simplify the problem, we also assume C ⊆ G. 1
For an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G′ = G ∪ {blank}, then we have
p(π|x)",3.2. From CTC to Gram-CTC,[0],[0]
"= T∏ t=1 ytπt ,∀π ∈ G ′T (1)
Just as in the case of CTC, here we refer to the elements of G′T as paths, and denote them by π, which represents a possible alignment between input and output.",3.2. From CTC to Gram-CTC,[0],[0]
"The difference is that for each word in the target sequence, it may be decomposed into different sequences of grams.",3.2. From CTC to Gram-CTC,[0],[0]
"For example, the word ‘hello’ can only be decomposed into the sequence [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [‘he’, ‘ll’, ‘o’] if ‘he’ and ‘ll’ are in G.
For each π, we map it into a target sequence in the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks.",3.2. From CTC to Gram-CTC,[0],[0]
"Note that essentially it is these rules which de-
1This is because there may be no valid decompositions for some target sequences if C 6⊆ G. Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training, this condition guarantees that there is at least one valid decomposition for every target sequence.
termine the transitions between the states of adjacent time steps in Figure 1.",3.2. From CTC to Gram-CTC,[0],[0]
This is a many-to-one mapping and we denote it by B. Note that other rules can be adopted here and the general idea presented in this paper does not depend on these specific rules.,3.2. From CTC to Gram-CTC,[0],[0]
"For a target sequence l, B−1(l) represents all paths mapped to l. Then, we have
p(l|x) = ∑
π∈B−1(l)
p(π|x) (2)
",3.2. From CTC to Gram-CTC,[0],[0]
"This equation allows for training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.",3.2. From CTC to Gram-CTC,[0],[0]
"Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.
",3.2. From CTC to Gram-CTC,[0],[0]
"Note that for each target sequence l, the set B−1(l) has O(τ2) more paths than it does in CTC.",3.2. From CTC to Gram-CTC,[0],[0]
"This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.",3.2. From CTC to Gram-CTC,[0],[0]
"The original CTC method is thus, a special case of Gram-CTC when G = C and τ = 1.",3.2. From CTC to Gram-CTC,[0],[0]
"While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.",3.2. From CTC to Gram-CTC,[0],[0]
"Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .",3.2. From CTC to Gram-CTC,[0],[0]
"To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.",3.3. The Forward-Backward Algorithm,[0],[0]
"The essence here is identifying
the states of the problem, so that we may solve future states by reusing solutions to earlier states.",3.3. The Forward-Backward Algorithm,[0],[0]
"In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.",3.3. The Forward-Backward Algorithm,[0],[0]
"Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} is the length of the last gram (li−j+1:i) used for making the prefix.",3.3. The Forward-Backward Algorithm,[0],[0]
j = 0 is valid and means that blank was used.,3.3. The Forward-Backward Algorithm,[0],[0]
"We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).",3.3. The Forward-Backward Algorithm,[0],[0]
"For readability, we will further shorten sji (l) to s j i and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i (l) to g j i .,3.3. The Forward-Backward Algorithm,[0],[0]
"For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.",3.3. The Forward-Backward Algorithm,[0],[0]
"During dynamic programming, we are dealing with sequence of states, for a state sequence ζ, its corresponding gram sequences is unique, denoted by ζg .
",3.3. The Forward-Backward Algorithm,[0],[0]
Figure 1 illustrates partially the dynamic programming process for the target sequence ‘CAT’.,3.3. The Forward-Backward Algorithm,[0],[0]
Here we suppose G contains all possible uni-grams and bi-grams.,3.3. The Forward-Backward Algorithm,[0],[0]
"Thus, for each character in ‘CAT’, there are three possible states associated with it: 1) the current character, 2) the bi-gram ending in current character, and 3) the blank after current character.",3.3. The Forward-Backward Algorithm,[0],[0]
There is also one blank at beginning.,3.3. The Forward-Backward Algorithm,[0],[0]
"In total we have 10 states.
",3.3. The Forward-Backward Algorithm,[0],[0]
"Supposing the maximum length of grams inG is τ , we first scan l to get the set S of all possible states, such that for all sji ∈ S, its corresponding g j i ∈",3.3. The Forward-Backward Algorithm,[0],[0]
"G′. i ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", |l|}",3.3. The Forward-Backward Algorithm,[0],[0]
"and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ}.",3.3. The Forward-Backward Algorithm,[0],[0]
"For a target sequence l, define the forward variable αt(s) for any s ∈ S to the total probability of all valid paths prefixes that end at state s at time t.
αt(s) def = ∑ ζ|B(ζg)=l1:e(s),ζt=s t∏ t′=1 yt ′ ζt′g (3)
",3.3. The Forward-Backward Algorithm,[0],[0]
"Following this definition, we have the following rules for initialization
α1(s) =  y1b s = s 0 0 y1 gii
s = sii ∀i ∈ {1, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(4)
and recursion
αt(s) =  α̂it−1 ∗ ytb when s = s0i , [α̂i−jt−1 + αt−1(s)] ∗ ytgji when s = sji and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i 6=,3.3. The Forward-Backward Algorithm,[0],[0]
"g j i−j , [α̂i−jt−1 + αt−1(s)− αt−1(s j i−j)] ∗ ytgji
when s = sji and g j i = g j i−j
(5)
where α̂it = ∑τ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
αt(s j i ) and y t b is the probability of blank at time,3.3. The Forward-Backward Algorithm,[0],[0]
"t.
The total probability of the target sequence l is then expressed in the following way:
p(l|x) = τ∑ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
"αT (s j |l|) (6)
similarly, we can define the backward variable βt(s) as:
βt(s) def = ∑ ζ|B(ζg)=lb(s):l,ζt=s T∏ t′=t yt ′ ζt′g (7)
",3.3. The Forward-Backward Algorithm,[0],[0]
"For the initialization and recursion of βt(s), we have
βT",3.3. The Forward-Backward Algorithm,[0],[0]
"(s) =  yTb s = s 0 T yT giT
s = siT ∀i ∈ {1, . .",3.3. The Forward-Backward Algorithm,[0],[0]
.,3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(8)
and
βt(s) =  β̂it+1 ∗ ytb when s = s0i , [β̂i+jt+1 + βt+1(s)] ∗ ytgji when s = sji and g j i 6=",3.3. The Forward-Backward Algorithm,[0],[0]
"g j i+j , [β̂i+jt+1 + βt+1(s)− βt+1(s j i+j)] ∗ ytgji
when s = sji and g j i = g j i+j
(9) where β̂it = ∑τ j=0 βt(s j i+j)",3.3. The Forward-Backward Algorithm,[0],[0]
"Similar to CTC, we have the following expression:
p(l|x) = ∑ s∈S αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg ∀t ∈ {1, . . .",3.4. BackPropagation,[0],[0]
",T} (10)
",3.4. BackPropagation,[0],[0]
"The derivative with regards to ytk is:
∂p(l|x) ∂ytk",3.4. BackPropagation,[0],[0]
"= 1 ytk 2 ∑ s∈lab(l,k) αt(s)βt(s) (11)
where lab(l, k) is the set of states in S whose corresponding gram is",3.4. BackPropagation,[0],[0]
k.,3.4. BackPropagation,[0],[0]
"This is because there may be multiple states corresponding to the same gram.
",3.4. BackPropagation,[0],[0]
"For the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.
∂ ln p(l|x) ∂utk = ytk",3.4. BackPropagation,[0],[0]
"− 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12)
where Zt def = ∑ s∈S
αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg .
",3.4. BackPropagation,[0],[0]
(a) Training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"(b) Training curves without (blue) and with (orange) joint-training
Gram-CTC C _",3.4. BackPropagation,[0],[0]
"AT
CTC _",3.4. BackPropagation,[0],[0]
C _,3.4. BackPropagation,[0],[0]
"A T _
Gram-CTC C - AT
CTC - C - A T -
(c) Joint-training Architecture
Figure 2.",3.4. BackPropagation,[0],[0]
(Figure 2a) compares the training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.",3.4. BackPropagation,[0],[0]
Figure (2b) Training curve of model with and without joint-training.,3.4. BackPropagation,[0],[0]
"The model corresponding to the orange training curve is jointly trained together with vanilla CTC, such models are often more stable during training.",3.4. BackPropagation,[0],[0]
Figure (2c) Typical joint-training model architecture - vanilla CTC loss is best applied a few levels lower than the Gram-CTC loss.,3.4. BackPropagation,[0],[0]
Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.,4. Methodology,[0],[0]
"Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G.",4.1. Iterative Gram Selection,[0],[0]
The total number of possible grams is usually huge.,4.1. Iterative Gram Selection,[0],[0]
"For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, . . .",4.1. Iterative Gram Selection,[0],[0]
", which grows exponentially and quickly becomes intractable.",4.1. Iterative Gram Selection,[0],[0]
"However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.
",4.1. Iterative Gram Selection,[0],[0]
"In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.",4.1. Iterative Gram Selection,[0],[0]
"Then, we train a model with Gram-CTC on all the remaining grams.",4.1. Iterative Gram Selection,[0],[0]
"By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.",4.1. Iterative Gram Selection,[0],[0]
"Ultimately, we choose high frequency grams together with all uni-grams as our final gram set G. Table 1 shows the impact of iterative gram selection on WSJ (without LM).",4.1. Iterative Gram Selection,[0],[0]
Figure 2a shows its corresponding training curve.,4.1. Iterative Gram Selection,[0],[0]
"For details, please refer to Section 5.2.",4.1. Iterative Gram Selection,[0],[0]
"Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.",4.2. Joint Training with Vanilla CTC,[0],[0]
"This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results
in models converging to a worse optima.",4.2. Joint Training with Vanilla CTC,[0],[0]
"To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).",4.2. Joint Training with Vanilla CTC,[0],[0]
"Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).
",4.2. Joint Training with Vanilla CTC,[0],[0]
"A typical joint-training model looks like Figure 2c, and the corresponding training curve is shown in Figure 2b.",4.2. Joint Training with Vanilla CTC,[0],[0]
The effect of joint-training are shown in Table 4 and Table 5 in the experiments.,4.2. Joint Training with Vanilla CTC,[0],[0]
"We test the Gram-CTC loss on the ASR task, while both CTC and the introduced Gram-CTC are generic techniques for other sequence labelling tasks.",5. Experiments,[0],[0]
"For all of the experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) -",5. Experiments,[0],[0]
"The model is a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers, followed by K forward (Fwd) or bidirectional (Bidi)",5. Experiments,[0],[0]
"Gated Recurrent layers, N cells each, and one fully connected layer before a softmax layer.",5. Experiments,[0],[0]
"In short hand, such a model is written as ‘2x2D Conv - KxN GRU’.",5. Experiments,[0],[0]
"The network is trained end-to-end with the CTC, GramCTC or a weighted combination of both.",5. Experiments,[0],[0]
"This combination is described in the earlier section.
",5. Experiments,[0],[0]
"In all experiments, audio data is is sampled at 16kHz.",5. Experiments,[0],[0]
"Linear FFT features are extracted with a hop size of 10ms and window size of 20ms, and are normalized so that each input feature has zero mean and unit variance.",5. Experiments,[0],[0]
The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.,5. Experiments,[0],[0]
"At each epoch, 40% of the utterances are randomly selected to add
background noise to.",5. Experiments,[0],[0]
The optimization method we use is stochastic gradient descent with Nesterov momentum.,5. Experiments,[0],[0]
"Learning hyperparameters (batch-size, learning-rate, momentum, and etc.) vary across different datasets and are tuned for each model by optimizing a hold-out set.",5. Experiments,[0],[0]
Typical values are a learning rate of 10−3 and momentum of 0.99.,5. Experiments,[0],[0]
Wall Street Journal (WSJ).,5.1. Data and Setup,[0],[0]
"This corpora consists primarily of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news text, and contains about 80 hours speech data.",5.1. Data and Setup,[0],[0]
"We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for testing.",5.1. Data and Setup,[0],[0]
"This is a relatively ‘clean’ task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).
",5.1. Data and Setup,[0],[0]
Fisher-Switchboard.,5.1. Data and Setup,[0],[0]
"This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data.",5.1. Data and Setup,[0],[0]
"Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.
10K Speech Dataset.",5.1. Data and Setup,[0],[0]
"We conduct large scale ASR experiments on a noisy internal dataset of 10,000 hours.",5.1. Data and Setup,[0],[0]
"This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on.",5.1. Data and Setup,[0],[0]
"Due to its inherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application.",5.1. Data and Setup,[0],[0]
"We employ the WSJ dataset for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea verification.",5.2. Gram Selection,[0],[0]
"However, because it is small, we cannot use large grams here due to data sparsity problem.",5.2. Gram Selection,[0],[0]
"Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but the procedure of refinement is the same for them.
",5.2. Gram Selection,[0],[0]
"We first train a model using all uni-grams and bi-grams (29
uni-grams and 262 = 676 bi-grams, in total 705 grams), and then do decoding with the obtained model on another speech dataset to get the statistics of the usage of grams.",5.2. Gram Selection,[0],[0]
Top 100 bi-grams together with all 29 uni-grams (autorefined grams) are used for the second round of training.,5.2. Gram Selection,[0],[0]
"For comparison, we also present the result of the best handpicked grams, as well as the results on uni-grams.",5.2. Gram Selection,[0],[0]
"All the results are shown in Table 1.
",5.2. Gram Selection,[0],[0]
Some interesting observations can be found in Table 1.,5.2. Gram Selection,[0],[0]
"First, the performance of auto-refined grams is only slightly better than the combination of all uni-grams and all bigrams.",5.2. Gram Selection,[0],[0]
This is probably because WSJ is so small that gram learning suffers from the data sparsity problem here (similar to word-segmented models).,5.2. Gram Selection,[0],[0]
"The auto-refined gram set contains only a small subset of bi-grams, thus more robust.",5.2. Gram Selection,[0],[0]
"This is also why we only try bi-grams, not including higher-order grams.",5.2. Gram Selection,[0],[0]
"Second, the performance of best handpicked grams is worse than auto-refined grams.",5.2. Gram Selection,[0],[0]
This is desirable.,5.2. Gram Selection,[0],[0]
"It is time-consuming to handpick grams, especially when you consider high-order grams.",5.2. Gram Selection,[0],[0]
"The method of iterative gram selection is not only fast, but usually better.",5.2. Gram Selection,[0],[0]
"Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams.",5.2. Gram Selection,[0],[0]
"This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment.",5.2. Gram Selection,[0],[0]
WSJ is too small to provide enough data to train Gram-CTC.,5.2. Gram Selection,[0],[0]
"Using a large time stride for sequence labelling with RNNs can greatly boost the overall computation efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward propagation.",5.3. Sequence Labelling in Large Stride,[0],[0]
"However, the largest stride that can be used is limited by the gram set we use.",5.3. Sequence Labelling in Large Stride,[0],[0]
The (unigram) CTC has to work in a high time resolution (small stride) in order to have enough number of frames to output every character.,5.3. Sequence Labelling in Large Stride,[0],[0]
"This is very inefficient as we know the same acoustic feature could correspond to several grams of different lengths (e.g., {‘i’, ‘igh’, ‘eye’}) .",5.3. Sequence Labelling in Large Stride,[0],[0]
"The larger the grams are, the larger stride we are potentially able to use.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"DS2 (Amodei et al., 2015) employed non-overlapping bigram outputs to allow for a larger stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
"This imposes an artificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For example, part is split as [pa, rt] but the word
apart is forced to be decomposed as [ap, ar, t].",5.3. Sequence Labelling in Large Stride,[0],[0]
GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition.,5.3. Sequence Labelling in Large Stride,[0],[0]
"Comparison results show this change enables Gram-CTC to work much better than bigram CTC, as in Table 2.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"In Table 2, we compare the performance of trained model and training efficiency on two strides, 2 and 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For GramCTC, we use the auto-refined gram set from previous section.",5.3. Sequence Labelling in Large Stride,[0],[0]
"As expected, using stride 4 almost cuts the training time per epoch into half, compared to stride 2.",5.3. Sequence Labelling in Large Stride,[0],[0]
"From stride 2 to stride 4, the performance of uni-gram CTC drops quickly.",5.3. Sequence Labelling in Large Stride,[0],[0]
This is because small grams inherently need higher time resolutions.,5.3. Sequence Labelling in Large Stride,[0],[0]
"As for Gram-CTC, from stride 2 to stride 4, its performance decreases a little bit, while in experiments on the other datasets, Gram-CTC constantly works better in stride 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well.,5.3. Sequence Labelling in Large Stride,[0],[0]
"In contrast, the performance of bi-gram CTC is not as good as that of Gram-CTC in either stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
Figure 3 illustrates the max-decoding results of both CTC and Gram-CTC on nine utterances.,5.4. Decoding Examples,[0],[0]
"Here the label set for CTC is the set of all characters, and the label set for GramCTC is an auto-refined gram set containing all uni-grams and some high-frequency high-order grams.",5.4. Decoding Examples,[0],[0]
"Here Gram-
CTC uses stride 4 while CTC uses stride 2.
",5.4. Decoding Examples,[0],[0]
"From Figure 3, we can find that: 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as ‘the’, ‘ng’, and ‘are’.",5.4. Decoding Examples,[0],[0]
2) It also decomposes the sentences into segments which are meaningful in term of pronunciation.,5.4. Decoding Examples,[0],[0]
"This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural.",5.4. Decoding Examples,[0],[0]
"3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more robust.",5.4. Decoding Examples,[0],[0]
"One example is the word ‘will’ in the last sentence in Figure 3. 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-toend.",5.4. Decoding Examples,[0],[0]
This makes such decomposition superior to phonetic decomposition.,5.4. Decoding Examples,[0],[0]
"In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes.",5.4. Decoding Examples,[0],[0]
"The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or Gram-CTC loss.",5.5.1. WSJ DATASET,[0],[0]
The results are shown in Table 3.,5.5.1. WSJ DATASET,[0],[0]
"For all models we trained, language model can greatly improve their performances, in term of WER.",5.5.1. WSJ DATASET,[0],[0]
"Though this dataset contains very limited amount of text data for learning gram selection and decomposition, Gram-
CTC can still improve the vanilla CTC notably.",5.5.1. WSJ DATASET,[0],[0]
The acoustic model trained here is composed of two 2D convolutions and six bi-directional GRU layer in 2048 dimension.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"The corresponding labels are used for training N-gram language models.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
• Switchboard,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"English speech 97S62 • Fisher English speech Part 1 - 2004S13, 2004T19 • Fisher English speech Part 2 - 2005S13, 2005T19
We use a sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) for tuning language model hyper-parameters.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
The evaluation is done on the NIST 2000 set.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
This configuration forms a standard benchmark for evaluating ASR models.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Results are in Table 4.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
We compare our model against best published results on in-domain data.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Together these techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Finally, we experiment on a large noisy dataset collected by ourself for building large-vocabulary Continuous Speech Recognition (LVCSR) systems.",5.5.3. 10K SPEECH DATASET,[0],[0]
"This dataset contains about 10000 hours speech in a diversity of scenarios, such as farfield, background noises, accents.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In all cases, the model is [2x2D Conv, 3x2560 Fwd GRU, LA Conv] with only a change in the loss function.",5.5.3. 10K SPEECH DATASET,[0],[0]
"‘LA Conv’ refers to a look ahead convolution layer as seen in (Amodei et al., 2015) which works together with forward-only RNNs for deployment purpose.
",5.5.3. 10K SPEECH DATASET,[0],[0]
"As with the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset.",5.5.3. 10K SPEECH DATASET,[0],[0]
"Thus, in both experiments, both Gram-CTC and vanilla
CTC + Gram-CTC are trained mush faster than vanilla CTC itself.",5.5.3. 10K SPEECH DATASET,[0],[0]
The result is shown in Table 5.,5.5.3. 10K SPEECH DATASET,[0],[0]
Gram-CTC performs better than CTC.,5.5.3. 10K SPEECH DATASET,[0],[0]
"After joint-training with vanilla CTC and alignment information through a CE loss, its performance is further boosted, which verifies joint-training helps training.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In fact, with only a small additional cost of time, it effectively reduces the WER from 27.56% to 25.59% (without language model).",5.5.3. 10K SPEECH DATASET,[0],[0]
"In this paper, we have proposed the Gram-CTC loss to enable automatic decomposition of target sequences into learned grams.",6. Conclusions and Future Work,[0],[0]
We also present techniques to train the Gram-CTC in a clean and stable way.,6. Conclusions and Future Work,[0],[0]
"Our extensive experiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling.",6. Conclusions and Future Work,[0],[0]
"Comparison experiments on multiplescale datasets show the proposed Gram-CTC obtains stateof-the-art results on various ASR tasks.
",6. Conclusions and Future Work,[0],[0]
"An interesting observation is that the learning of GramCTC implicitly avoids the “degenerated solution” that occurring in the traditional “unit discovery” task, without involving any Bayesian priors or the “minimum description length” constraint.",6. Conclusions and Future Work,[0],[0]
"Using a small gram set that contains only short (up to 5 in our experiments) as well as highfrequency grams may explain the success here.
",6. Conclusions and Future Work,[0],[0]
"We will continue investigating techniques of improving the optimization of Gram-CTC loss, as well as the applications of Gram-CTC for other sequence labelling tasks.",6. Conclusions and Future Work,[0],[0]
Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units.,abstractText,[0],[0]
"These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed.",abstractText,[0],[0]
These drawbacks usually result in sub-optimal performance of modeling sequences.,abstractText,[0],[0]
"In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC.",abstractText,[0],[0]
"While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences.",abstractText,[0],[0]
"Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency.",abstractText,[0],[0]
"We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",abstractText,[0],[0]
Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,title,[0],[0]
"Generative machine learning models have been used recently to produce extraordinary results, from realistic musical improvisation (Jaques et al., 2016), to changing facial expressions in images (Radford et al., 2015; Upchurch et al., 2016), to creating realistic looking artwork (Gatys et al., 2015).",1. Introduction,[0],[0]
"In large part, these generative models have been successful at representing data in continuous domains.",1. Introduction,[0],[0]
"Recently there is increased interest in training generative models to construct more complex, discrete data types such as arithmetic expressions (Kusner & Hernández-Lobato, 2016), source code (Gaunt et al., 2016; Riedel et al., 2016)
",1. Introduction,[0],[0]
*Equal contribution 1Alan Turing Institute 2University of Warwick 3University of Cambridge.,1. Introduction,[0],[0]
"Correspondence to: <mkusner@turing.ac.uk>, <bpaige@turing.ac.uk>, <jmh233@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
and molecules (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"To train generative models for these tasks, these objects are often first represented as strings.",1. Introduction,[0],[0]
"This is in large part due to the fact that there exist powerful models for text sequence modeling such as Long Short Term Memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Units (GRUs) (Cho et al., 2014), and Dynamic Convolutional Neural Networks (DCNNs) (Kalchbrenner et al., 2014).",1. Introduction,[0],[0]
"For instance, molecules can be represented by so-called SMILES strings (Weininger, 1988) and GómezBombarelli et al. (2016b) has recently developed a generative model for molecules based on SMILES strings that uses GRUs and DCNNs.",1. Introduction,[0],[0]
"This model is able to encode and decode molecules to and from a continuous latent space, allowing one to search this space for new molecules with desirable properties (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"However, one immediate difficulty in using strings to represent discrete objects is that the representation is very brittle: small changes in the string can lead to completely different objects, or often do not correspond to valid objects at all.",1. Introduction,[0],[0]
"Specifically, Gómez-Bombarelli et al. (2016b) described that while searching for new molecules, the probabilistic decoder — the distribution which maps from the continuous latent space into the space of molecular structures — would sometimes accidentally put high probability on strings which are not valid SMILES strings or do not encode plausible molecules.
",1. Introduction,[0],[0]
"To address this issue, we propose to directly incorporate knowledge about the structure of discrete data using a grammar.",1. Introduction,[0],[0]
"Grammars exist for a wide variety of discrete domains such as symbolic expressions (Allamanis et al., 2016), standard programming languages such as C (Kernighan et al., 1988), and chemical structures (James et al., 2015).",1. Introduction,[0],[0]
"For instance the set of syntactically valid SMILES strings is described using a context free grammar, which can be used for parsing and validation1.
",1. Introduction,[0],[0]
"Given a grammar, every valid discrete object can be described as a parse tree from the grammar.",1. Introduction,[0],[0]
"Thus, we propose the grammar variational autoencoder (GVAE) which encodes and decodes directly from and to these parse trees.",1. Introduction,[0],[0]
"Generating parse trees as opposed to strings ensures that
1http://opensmiles.org/spec/open-smiles-2-grammar.html
all outputs are valid based on the grammar.",1. Introduction,[0],[0]
"This frees the GVAE from learning syntactic rules and allows it to wholly focus on learning other ‘semantic’ properties.
",1. Introduction,[0],[0]
We demonstrate the GVAE on two tasks for generating discrete data: 1) generating simple arithmetic expressions and 2) generating valid molecules.,1. Introduction,[0],[0]
"We show not only does our model produce a higher proportion of valid outputs than a character based autoencoder, it also produces smoother latent representations.",1. Introduction,[0],[0]
"We also show that this learned latent space is effective for searching for arithmetic expressions that fit data, for finding better drug-like molecules, and for making accurate predictions about target properties.",1. Introduction,[0],[0]
We wish to learn both an encoder and a decoder for mapping data x to and from values z in a continuous space.,2.1. Variational autoencoder,[0],[0]
"The variational autoencoder (Kingma & Welling, 2014; Rezende et al., 2014) provides a formulation in which the encoding z is interpreted as a latent variable in a probabilistic generative model; a probabilistic decoder is defined by a likelihood function p
✓ (x|z) and parameterized by ✓.",2.1. Variational autoencoder,[0],[0]
"Alongside a prior distribution p(z) over the latent variables, the posterior distribution p
✓ (z|x) / p(z)p ✓ (x|z) can then be interpreted as a probabilistic encoder.
",2.1. Variational autoencoder,[0],[0]
"To admit efficient inference, the variational Bayes approach simultaneously learns both the parameters of p
✓ (x|z) as well as those of a posterior approximation q
(z|x).",2.1. Variational autoencoder,[0],[0]
"This is achieved by maximizing the evidence lower bound (ELBO)
",2.1. Variational autoencoder,[0],[0]
"L( , ✓;x) = E q (z|x)",2.1. Variational autoencoder,[0],[0]
"[log p✓(x, z) log q (z|x)] , (1)
with L( , ✓;x)  log p ✓ (x).",2.1. Variational autoencoder,[0],[0]
"So long as p ✓ (x|z) and q
(z|x) can be computed pointwise, and are differentiable with respect to their parameters, the ELBO can be maximized via gradient descent; this allows wide flexibility in choice of encoder and decoder models.",2.1. Variational autoencoder,[0],[0]
Typically these will take the form of exponential family distributions whose parameters are the weights of a multi-layer neural network.,2.1. Variational autoencoder,[0],[0]
"A context-free grammar (CFG) is traditionally defined as a 4-tuple G = (V,⌃, R, S): V is a finite set of non-terminal symbols; the alphabet ⌃ is a finite set of terminal symbols, disjoint from V ; R is a finite set of production rules; and S is a distinct non-terminal known as the start symbol.",2.2. Context-free grammars,[0],[0]
The rules R are formally described as ↵ !,2.2. Context-free grammars,[0],[0]
"for ↵ 2 V and 2 (V [ ⌃)⇤, with ⇤ denoting the Kleene closure.",2.2. Context-free grammars,[0],[0]
"In practice, these rules are defined as a set of mappings from a single left-hand side non-terminal in V to a sequence of terminal and/or non-terminal symbols, and can be interpreted as ‘replacement’ instructions.
",2.2. Context-free grammars,[0],[0]
"Repeatedly applying production rules beginning with a non-terminal symbol defines a tree, with symbols on the right-hand side of the production rule becoming child nodes for the left-hand side parent.",2.2. Context-free grammars,[0],[0]
"The grammar G thus defines a set of possible trees extending from each nonterminal symbol in V , produced by recursively applying rules in R to leaf nodes until all leaf nodes are terminal symbols in ⌃.",2.2. Context-free grammars,[0],[0]
The language of G is the set of all terminal symbol sequences that can be generated as leaf nodes in a tree.,2.2. Context-free grammars,[0],[0]
"Given a string in the language (i.e., a sequence of terminals), a parse tree is a tree rooted at S which has this sequence of terminal symbols as its leaf nodes.",2.2. Context-free grammars,[0],[0]
The ubiquity of context-free languages in computer science is due in part to the presence of efficient parsing algorithms to generate parse trees.,2.2. Context-free grammars,[0],[0]
"For more background on CFGs and automata theory, see e.g. Hopcroft et al. (2006).
",2.2. Context-free grammars,[0],[0]
Our work builds off the work of probabilistic context-free grammars (PCFGs).,2.2. Context-free grammars,[0],[0]
"A PCFG assigns probabilities to each production rule in the grammar, and thus defines a probability distribution over parse trees (Baker, 1979; Booth & Thompson, 1973).",2.2. Context-free grammars,[0],[0]
"A string can be generated by repeatedly sampling and applying production rules, beginning at the start symbol, until no non-terminals remain.",2.2. Context-free grammars,[0],[0]
"Modern approaches allow the probabilities used at each stage to depend on the state of the parse tree (Johnson et al., 2007).",2.2. Context-free grammars,[0],[0]
In this section we describe how a grammar can improve variational autoencoders (VAE) for discrete data.,3. Methods,[0],[0]
It will do so by drastically reducing the number of invalid outputs generated from the VAE.,3. Methods,[0],[0]
"We illustrate our approach on molecular data, however it will extend to any descrete data that can be described by a grammar.
",3. Methods,[0],[0]
"One glaring issue with a character-based VAE is that it may frequently map latent points to sequences that are not valid, hoping the VAE will infer from training data what constitutes a valid sequence.",3. Methods,[0],[0]
"Instead of implicitly encouraging the VAE to produce valid sequences, we propose to give the VAE explicit knowledge about how to produce valid sequences.",3. Methods,[0],[0]
We do this by using a grammar for the sequences: given a grammar we can take any valid sequence and parse it into a parse tree.,3. Methods,[0],[0]
A pre-order traversal on this parse tree yields a sequence of production rules.,3. Methods,[0],[0]
Applying these rules in order will yield the original sequence.,3. Methods,[0],[0]
Our approach then will be to learn a VAE that produces sequences of grammar production rules.,3. Methods,[0],[0]
"The benefit is that it is trivial to generate valid sequences of production rules, as the grammar describes the valid set of rules that can be selected at any point during the generation process.",3. Methods,[0],[0]
"Thus, our model is able to focus on learning semantic properties of sequence data without also having to learn syntactic constraints.",3. Methods,[0],[0]
We propose a grammar variational autoencoder (GVAE) that encodes/decodes in the space of grammar production rules.,3.1. An illustrative example,[0],[0]
"We describe how it works with a simple example.
Encoding.",3.1. An illustrative example,[0],[0]
"Consider a subset of the SMILES grammar as shown in Figure 1, box 1 .",3.1. An illustrative example,[0],[0]
These are the possible production rules that can be used for constructing a molecule.,3.1. An illustrative example,[0],[0]
Imagine we are given as input the SMILES string for benzene: ‘c1ccccc1’.,3.1. An illustrative example,[0],[0]
"Figure 1, box 3 shows this molecule.",3.1. An illustrative example,[0],[0]
To encode this molecule into a continuous latent representation we begin by using the SMILES grammar to parse this string into a parse tree (partially shown in box 2 ).,3.1. An illustrative example,[0],[0]
This tree describes how ‘c1ccccc1’ is generated by the grammar.,3.1. An illustrative example,[0],[0]
"We decompose this tree into a sequence of production rules by performing a pre-order traversal on the branches of the parse tree from left-to-right, shown in box 4 .",3.1. An illustrative example,[0],[0]
"We convert these rules into 1-hot indicator vectors, where each dimension corresponds to a rule in the SMILES grammar, box 5 .",3.1. An illustrative example,[0],[0]
"These 1-hot vectors are concatenated into the rows of a matrix X of dimension T (X)⇥K, where K is the number of production rules in the SMILES grammar, and T (X) is the number of production rules used to generate X.
We use a deep convolutional neural network to map the collection of 1-hot vectors X to a continuous latent vector z.",3.1. An illustrative example,[0],[0]
"The architecture of the encoding network is described in the supplementary material.
",3.1. An illustrative example,[0],[0]
Decoding.,3.1. An illustrative example,[0],[0]
We now describe how we map continuous vectors back to a sequence of production rules (and thus SMILES strings).,3.1. An illustrative example,[0],[0]
"Crucially we construct the decoder so that, at any time while we are decoding a sequence, the decoder will only be allowed to select a subset of production rules that are ‘valid’.",3.1. An illustrative example,[0],[0]
"This will cause the decoder to only produce valid parse sequences from the grammar.
",3.1. An illustrative example,[0],[0]
"We begin by passing the continuous vector z through a recurrent neural network which produces a set of unnormalized log probability vectors (or ‘logits’), shown in Figure 2, box 1 and 2 .",3.1. An illustrative example,[0],[0]
"Exactly like the 1-hot vectors produced by the encoder, each dimension of the logit vectors cor-
responds to a production rule in the grammar.",3.1. An illustrative example,[0],[0]
"We can again write these collection of logit vectors as a matrix F 2 RTmax⇥K , where T
max is the maximum number of timesteps (production rules) allowed by the decoder.",3.1. An illustrative example,[0],[0]
"During the rest of the decoding operations, we will use the rows of F to select a sequence of valid production rules.
",3.1. An illustrative example,[0],[0]
"To ensure that any sequence of production rules generated from the decoder is valid, we keep track of the state of the parsing using a last-in first-out (LIFO) stack.",3.1. An illustrative example,[0],[0]
"This is shown in Figure 2, box 3 .",3.1. An illustrative example,[0],[0]
"At the beginning, every valid parse from the grammar must start with the start symbol: smiles, which is placed on the stack.",3.1. An illustrative example,[0],[0]
"Next we pop off whatever non-terminal symbol that was placed last on the stack (in this case smiles), and we use it to mask out the invalid dimensions of the current logit vector.",3.1. An illustrative example,[0],[0]
"Formally, for every non-terminal ↵ we define a fixed binary mask vector m
↵ 2 [0, 1]K .",3.1. An illustrative example,[0],[0]
"This takes the value ‘1’ for all indices in 1, . . .",3.1. An illustrative example,[0],[0]
",K corresponding to production rules that have ↵ on their left-hand-side.
",3.1. An illustrative example,[0],[0]
"In the previous example, the only production rule in the grammar beginning with smiles is the first so we maskout every dimension except the first, shown in Figure 2, box 4 .",3.1. An illustrative example,[0],[0]
"We then sample from the remaining unmasked rules, using their values in the logit vector.",3.1. An illustrative example,[0],[0]
"To sample from this masked logit at any timestep t we form the following masked distribution:
p(x t = k|↵, z) = m↵,k exp(ftk)P K
j=1 m↵,k exp(ftj) , (2)
where f tk is the (t, k)-element of the logit matrix F. As only the first rule is unmasked we will select this rule smiles !",3.1. An illustrative example,[0],[0]
"chain as the first rule in our sequence, box 5 .",3.1. An illustrative example,[0],[0]
"Now the next rule must begin with chain, so we push it onto the stack (Figure 2, box 3 ).",3.1. An illustrative example,[0],[0]
"We sample this nonterminal and, as before, use it to mask out all of the rules that cannot be applied in the current logit vector.",3.1. An illustrative example,[0],[0]
We then sample a valid rule from this logit vector: chain!,3.1. An illustrative example,[0],[0]
"chain, branched atom.",3.1. An illustrative example,[0],[0]
"Just as before we push the non-terminals on the right-hand side of this rule onto the stack, adding the individual non-terminals in from right to left, such that the leftmost non-terminal is on the top of the stack.",3.1. An illustrative example,[0],[0]
"For the
Algorithm 1 Sampling from the decoder Input: Deterministic decoder output F 2 RTmax⇥K ,
masks m ↵ for each production rule ↵ Output: Sampled productions X from p(X|z)
1: Initialize empty stack S , and push the start symbol S onto the top; set t = 0 2: while S is nonempty do 3:",3.1. An illustrative example,[0],[0]
"Pop the last-pushed non-terminal ↵ from the stack S 4: Use Eq. (2) to sample a production rule R 5: Let x
t be the 1-hot vector corresponding to R 6: Let RHS(R) denote all non-terminals on the righthand side of rule R, ordered from right to left 7: for non-terminal in RHS(R) do 8: Push on to the stack S 9: end for
10: Set X [X>,x t ]",3.1. An illustrative example,[0],[0]
"> 11: Set t t+ 1 12: end while
next state we again pop the last rule placed on the stack and mask the current logit, etc.",3.1. An illustrative example,[0],[0]
"This process continues until the stack is empty or we reach the maximum number of logit vectors T
max .",3.1. An illustrative example,[0],[0]
We describe this decoding procedure formally in Algorithm 1.,3.1. An illustrative example,[0],[0]
"In practice, because sampling from the decoder often finishes before t reaches T
max , we introduce an additional ‘no-op’ rule to the grammar that we use to pad X until the number of rows equals T
max
.
",3.1. An illustrative example,[0],[0]
We note the explicit connection between the process in Algorithm 1 and parsing algorithms for pushdown automata.,3.1. An illustrative example,[0],[0]
"A pushdown automaton is a finite state machine which has access to a single stack for long-term storage, and are equivalent to context-free grammars in the sense that every CFG can be converted into a pushdown automaton, and vice-versa (Hopcroft et al., 2006).",3.1. An illustrative example,[0],[0]
"The decoding algorithm performs the sequence of actions taken by a nondeterministic pushdown automaton at each stage of a parsing algorithm; the nondeterminism is resolved by sampling according to the probabilities in the emitted logit vector.
Contrasting the character VAE.",3.1. An illustrative example,[0],[0]
"Notice that the key difference between this grammar VAE decoder and a
character-based VAE decoder is that at every point in the generated sequence, the character VAE can sample any possible character.",3.1. An illustrative example,[0],[0]
There is no stack or masking operation.,3.1. An illustrative example,[0],[0]
"The grammar VAE however is constrained to select syntactically-valid sequences.
",3.1. An illustrative example,[0],[0]
Syntactic vs. semantic validity.,3.1. An illustrative example,[0],[0]
It is important to note that the grammar encodes syntactically valid molecules but not necessarily semantically valid molecules.,3.1. An illustrative example,[0],[0]
This is mainly because of three reasons.,3.1. An illustrative example,[0],[0]
"First, certain molecules produced by the grammar may be very unstable molecules or not chemically-valid (for instance an oxygen atom cannot bond to 3 other atoms as it only has 2 free electrons for bonding, although it would be possible to generate this in a molecule from the grammar).",3.1. An illustrative example,[0],[0]
"Second, the SMILES language has non-context free aspects, e.g. a ringbond must be opened and closed by the same digit, starting with ‘1’ (as is the case for benzene ‘c1ccccc1’).",3.1. An illustrative example,[0],[0]
"The particular challenge for matching digits, in contrast to matching grouping symbols such as parentheses, is that they do not compose in a nested manner; for example, ‘C12(CCCCC1)CCCCC2’ is a valid molecule.",3.1. An illustrative example,[0],[0]
Keeping track of which digit to use for each ringbond is not context-free.,3.1. An illustrative example,[0],[0]
"Third, we note that the GVAE can output an undetermined sequence if there are still non-terminal symbols on the stack after processing all T max
logit vectors.",3.1. An illustrative example,[0],[0]
"While this could be fixed by a procedure that converts these non-terminals to terminals, for simplicity we mark these sequences as invalid.",3.1. An illustrative example,[0],[0]
"During training, each input SMILES encoded as a sequence of 1-hot vectors X 2 {0, 1}Tmax⇥K , also defines a sequence of T
max mask vectors.",3.2. Training,[0],[0]
"Each mask at timestep t = 1, . . .",3.2. Training,[0],[0]
", T
max is selected by the left-hand side of the production rule indicated in the 1-hot vector x
t .",3.2. Training,[0],[0]
"Given these masks we can compute the decoder’s mapping
p(X|z)",3.2. Training,[0],[0]
"= T (X)Y
t=1
p(x t |z,x1:(t 1)), (3)
with the individual probabilities at each timestep defined as in Eq. (2).",3.2. Training,[0],[0]
"We pad any remaining timesteps after T (X) up
Algorithm 2 Training the Grammar VAE Input: Dataset {X(i)}N
i=1
Output: Trained VAE model p ✓ (X|z), q (z|X) 1: while VAE not converged do 2: Select element: X 2 {X(i)}N
i=1 (or minibatch) 3:",3.2. Training,[0],[0]
"Encode: z ⇠ q
(z|X) 4: Decode: given z, compute logits F 2 RTmax⇥K 5: for t in [1, . . .",3.2. Training,[0],[0]
", T
max ] do 6: Compute p
✓
(x
t |z) via Eq.",3.2. Training,[0],[0]
"(2), with mask m x
t
and logits f t
7: end for 8: Update ✓, using estimates p
✓
(X|z), q (z|X), via gradient descent on the ELBO in Eq.",3.2. Training,[0],[0]
"(4)
9: end while
to T max with a ‘no-op’ rule, a one-hot vector indicating the parse tree is complete and no actions are to be taken.
",3.2. Training,[0],[0]
"In all our experiments, q(z|X) is a Gaussian distribution whose mean and variance parameters are the output of the encoder network, with an isotropic Gaussian prior p(z)",3.2. Training,[0],[0]
"= N (0, I)",3.2. Training,[0],[0]
.,3.2. Training,[0],[0]
"At training time, we sample a value of z from q(z|X) to compute the ELBO
L( , ✓;X) = E q (z|X)",3.2. Training,[0],[0]
"[log p✓(X, z) log q (z|X)] .",3.2. Training,[0],[0]
"(4)
Following Kingma & Welling (2014), we apply a noncentered parameterization on the encoding Gaussian distribution and optimize Eq.",3.2. Training,[0],[0]
"(4) using gradient descent, learning encoder and decoder neural network parameters and ✓.",3.2. Training,[0],[0]
Algorithm 2 summarizes the training procedure.,3.2. Training,[0],[0]
We show the usefulness of our proposed grammar variational autoencoder (GVAE)2 on two sequence optimization problems: 1) searching for an arithmetic expression that best fits a dataset and 2) finding new drug molecules.,4. Experiments,[0],[0]
"We begin by showing the latent space of the GVAE and a character variational autoencoder (CVAE), similar to that of Gómez-Bombarelli et al. (2016b)3, on each of the problems.",4. Experiments,[0],[0]
"We demonstrate that the GVAE learns a smooth, meaningful latent space for arithmetic equations and molecules.",4. Experiments,[0],[0]
"Given this we perform optimization in this latent space using Bayesian optimization, inspired by the technique of Gómez-Bombarelli et al. (2016b).",4. Experiments,[0],[0]
"We demonstrate that the GVAE improves upon a previous character variational autoencoder, by selecting an arithmetic expression that matches the data nearly perfectly, and by finding novel molecules with better drug properties.
2Code available at: https://github.com/mkusner/grammarVAE 3https://github.com/maxhodak/keras-molecules",4. Experiments,[0],[0]
We describe in detail the two sequence optimization problems we seek to solve.,4.1. Problems,[0],[0]
The first consists in optimizing the fit of an arithmetic expression.,4.1. Problems,[0],[0]
"We are given a set of 100,000 randomly generated univariate arithmetic expressions from the following grammar:
S !",4.1. Problems,[0],[0]
S ‘+ ’ T | S ‘⇤ ’ T | S ‘ / ’ T | T T !,4.1. Problems,[0],[0]
‘ ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ s i n ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ exp ( ’ S ‘ ) ’ T !,4.1. Problems,[0],[0]
"‘x ’ | ‘1 ’ | ‘2 ’ | ‘3 ’
where S and T are non-terminals and the symbol | separates the possible production rules generated from each non-terminal.",4.1. Problems,[0],[0]
"By parsing this grammar we can randomly generate strings of univariate arithmetic equations (functions of x) such as the following: sin(2), x/(3+ 1), 2+ x+ sin(1/2), and x/2 ⇤ exp(x)/exp(2 ⇤ x).",4.1. Problems,[0],[0]
We limit the length of every selected string to have at most 15 production rules.,4.1. Problems,[0],[0]
Given this dataset we train both the CVAE and GVAE to learn a latent space of arithmetic expressions.,4.1. Problems,[0],[0]
We propose to perform optimization in this latent space of expressions to find an expression that best fits a fixed dataset.,4.1. Problems,[0],[0]
A common measure of best fit is the test MSE between the predictions made by a selected expression and the true data.,4.1. Problems,[0],[0]
"In the generated expressions, the presence of exponential functions can result in very large MSE values.",4.1. Problems,[0],[0]
"For this reason, we use as target variable log(1 + MSE) instead of MSE.
",4.1. Problems,[0],[0]
"For the second optimization problem, we follow (GómezBombarelli et al., 2016b) and optimize the drug properties of molecules.",4.1. Problems,[0],[0]
"Our goal is to maximize the water-octanol partition coefficient (logP), an important metric in drug design that characterizes the drug-likeness of a molecule.",4.1. Problems,[0],[0]
"As in Gómez-Bombarelli et al. (2016b) we consider a penalized logP score that takes into account other molecular properties such as ring size and synthetic accessibility (Ertl & Schuffenhauer, 2009).",4.1. Problems,[0],[0]
"The training data for the CVAE and GVAE models are 250,000 SMILES strings (Weininger, 1988) extracted at random from the ZINC database by Gómez-Bombarelli et al. (2016b).",4.1. Problems,[0],[0]
We describe the context-free grammar for SMILES strings that we use to train our GVAE in the supplementary material.,4.1. Problems,[0],[0]
Arithmetic expressions.,4.2. Visualizing the latent space,[0],[0]
"To qualitatively evaluate the smoothness of the VAE embeddings for arithmetic expressions, we attempt interpolating between two arithmetic expressions, as in Bowman et al. (2016).",4.2. Visualizing the latent space,[0],[0]
This is done by encoding two equations and then performing linear interpolation in the latent space.,4.2. Visualizing the latent space,[0],[0]
Results comparing the character and grammar VAEs are shown in Table 1.,4.2. Visualizing the latent space,[0],[0]
"Although the character VAE smoothly interpolates between the text representation of equations, it passes through intermediate points which do not decode to valid equations.",4.2. Visualizing the latent space,[0],[0]
"In contrast, the grammar VAE also provides smooth interpolation and produces valid equations for any location in the latent space.",4.2. Visualizing the latent space,[0],[0]
"A further exploration of a 2-dimensional latent space is shown in the appendix.
Molecules.",4.2. Visualizing the latent space,[0],[0]
We are interested if the GVAE produces a coherent latent space of molecules.,4.2. Visualizing the latent space,[0],[0]
To assess this we begin by encoding a molecule.,4.2. Visualizing the latent space,[0],[0]
We then generate 2 random orthogonal unit vectors in latent space (scaled down to only search the neighborhood of the molecules).,4.2. Visualizing the latent space,[0],[0]
Moving in combinations of these directions defines a grid and at each point in the grid we decode the latent vector 1000 times.,4.2. Visualizing the latent space,[0],[0]
We select the molecule that appears most often as the representative molecule.,4.2. Visualizing the latent space,[0],[0]
Figure 3 shows this latent space search surrounding two different molecules.,4.2. Visualizing the latent space,[0],[0]
Compare this to Figures 13-15 in Gómez-Bombarelli et al. (2016b).,4.2. Visualizing the latent space,[0],[0]
"We note that in each plot of the GVAE the latent space is very smooth, in many cases moving from one grid point to another will only change a single atom in a molecule.",4.2. Visualizing the latent space,[0],[0]
"In the CVAE (Gómez-Bombarelli et al., 2016b) we do not observe such fine-grained smoothness.",4.2. Visualizing the latent space,[0],[0]
We now perform a series of experiments using the autoencoders to produce novel sequences with improved properties.,4.3. Bayesian optimization,[0],[0]
"For this, we follow the approach proposed by GómezBombarelli et al. (2016b) and after training the GVAE, we
train an additional model to predict properties of sequences from their latent representation.",4.3. Bayesian optimization,[0],[0]
"To propose promising new sequences, we can start from the latent vector of an encoded sequence and then use the output of this predictor (including its gradient) to move in the latent space direction most likely to improve the property.",4.3. Bayesian optimization,[0],[0]
"The resulting new latent points can then be decoded into corresponding sequences.
",4.3. Bayesian optimization,[0],[0]
"In practice, measuring the property of each new sequence could be an expensive process.",4.3. Bayesian optimization,[0],[0]
"For example, the sequence could represent an organic photovoltaic molecule and the property could be the result of an expensive quantum mechanical simulation used to estimate the molecule’s powerconversion efficiency (Hachmann et al., 2011).",4.3. Bayesian optimization,[0],[0]
The sequence could also represent a program or expression which may be computationally expensive to evaluate.,4.3. Bayesian optimization,[0],[0]
"Therefore, ideally, we would like the optimization process to perform only a reduced number of property evaluations.",4.3. Bayesian optimization,[0],[0]
"For this, we use Bayesian optimization methods, which choose the next point to evaluate by maximizing an acquisition function that quantifies the benefit of evaluating the property at a particular location (Shahriari et al., 2016).
",4.3. Bayesian optimization,[0],[0]
"After training the GVAE, we obtain a latent feature vector for each sequence in the training data, given by the mean of the variational encoding distributions.",4.3. Bayesian optimization,[0],[0]
"We use these vectors and their corresponding property estimates to train a sparse Gaussian process (SGP) model with 500 inducing points (Snelson & Ghahramani, 2005), which is used to make predictions for the properties of new points in latent space.",4.3. Bayesian optimization,[0],[0]
"After training the SGP, we then perform 5 iterations of batch Bayesian optimization using the expected improvement (EI) heuristic (Jones et al., 1998).",4.3. Bayesian optimization,[0],[0]
"On each iteration, we select a batch of 50 latent vectors by sequentially maximizing the EI acquisition function.",4.3. Bayesian optimization,[0],[0]
"We use the Kriging Believer Algorithm to account for pending evaluations in the batch selection process (Cressie, 1990).",4.3. Bayesian optimization,[0],[0]
"That is, after selecting each new data point in the batch, we add that data point as a new inducing point in the sparse GP model with associated target variable equal to the mean of the GP predictive distribution at that point.",4.3. Bayesian optimization,[0],[0]
"Once a new batch of 50 latent vectors is selected, each point in the batch is transformed into its corresponding sequence using the decoder network in the GVAE.",4.3. Bayesian optimization,[0],[0]
The properties of the newly generated sequences are then computed and the resulting data is added to the training set before retraining the SGP and starting the next BO iteration.,4.3. Bayesian optimization,[0],[0]
"Note that some of the new sequences will be invalid and consequently, it will not be possible to obtain their corresponding property estimate.",4.3. Bayesian optimization,[0],[0]
"In this case we fix the property to be equal to the worst value observed in the original training data.
",4.3. Bayesian optimization,[0],[0]
Arithmetic expressions.,4.3. Bayesian optimization,[0],[0]
Our goal is to see if we can find an arithmetic expression that best fits a fixed dataset.,4.3. Bayesian optimization,[0],[0]
"Specifically, we generate this dataset by selecting 1000
input values, x, that are linearly-spaced between 10 and 10.",4.3. Bayesian optimization,[0],[0]
We then pass these through our true function 1/3+ x+,4.3. Bayesian optimization,[0],[0]
sin(x ⇤ x),4.3. Bayesian optimization,[0],[0]
to generate the true target observations.,4.3. Bayesian optimization,[0],[0]
We use Bayesian optimization (BO) as described above search for this equation.,4.3. Bayesian optimization,[0],[0]
We run BO for 5 iterations and average across 10 repetitions of the process.,4.3. Bayesian optimization,[0],[0]
Table 2 (rows 1 & 2) shows the results obtained.,4.3. Bayesian optimization,[0],[0]
The third column in the table reports the fraction of arithmetic sequences found by BO that are valid.,4.3. Bayesian optimization,[0],[0]
The GVAE nearly always finds valid sequences.,4.3. Bayesian optimization,[0],[0]
"The only cases in which it does not is when there are still non-terminals on the stack of
the decoder upon reaching the maximum number of timesteps T
max , however this is rare.",4.3. Bayesian optimization,[0],[0]
"Additionally, the GVAE finds squences with better scores on average when compared with the CVAE.
",4.3. Bayesian optimization,[0],[0]
"Table 3 shows the top 3 expressions found by GVAE and CVAE during the BO search, together with their associated score values.",4.3. Bayesian optimization,[0],[0]
Figure 4 shows how the best expression found by GVAE and CVAE compare to the true function.,4.3. Bayesian optimization,[0],[0]
"We note that the CVAE has failed to find the sinusoidal portion of the true expression, while the difference between the GVAE expression and the true function is negligible.
Molecules.",4.3. Bayesian optimization,[0],[0]
We now consider the problem of finding new drug-like molecules.,4.3. Bayesian optimization,[0],[0]
"We perform 5 iterations of BO, and average results across 10 trials.",4.3. Bayesian optimization,[0],[0]
Table 2 (rows 3 & 4) shows the overall BO results.,4.3. Bayesian optimization,[0],[0]
"In this problem, the GVAE produces about twice more valid sequences than the CVAE.",4.3. Bayesian optimization,[0],[0]
The valid sequences produced by the GVAE also result in higher scores on average.,4.3. Bayesian optimization,[0],[0]
The best found SMILES strings by each method and their scores are shown in Table 4; the molecules themselves are plotted in Figure 5.,4.3. Bayesian optimization,[0],[0]
We now perform a series of experiments to evaluate the predictive performance of the latent representations found by each autoencoder.,4.4. Predictive performance of latent representation,[0],[0]
"For this, we use the sparse GP model used in the previous Bayesian optimization experiments and look at its predictive performance on a left-out test set with 10% of the data, where the data is formed by the latent representation of the available sequences (these are the inputs to the sparse GP model) and the associated properties of those sequences (these are the outputs in the sparse GP model).",4.4. Predictive performance of latent representation,[0],[0]
Table 5 show the average test RMSE and test loglikelihood for the GVAE and the CVAE across 10 different splits of the data for the expressions and for the molecules.,4.4. Predictive performance of latent representation,[0],[0]
This table shows that the GVAE produces latent features that yield much better predictive performance than those produced by the CVAE.,4.4. Predictive performance of latent representation,[0],[0]
"Parse trees have been used to learn continuous representations of text in recursive neural network models (Socher et al., 2013; Irsoy & Cardie, 2014; Paulus et al., 2014).",5. Related Work,[0],[0]
These models learn a vector at every non-terminal in the parse tree by recursively combining the vectors of child nodes.,5. Related Work,[0],[0]
"Recursive autoencoders learn these representations by minimizing the reconstruction error between true child vectors and those predicted by the parent (Socher et al., 2011a;b).",5. Related Work,[0],[0]
"Recently, Allamanis et al. (2016) learn representations for symbolic expressions from their parse trees.",5. Related Work,[0],[0]
"Importantly, all of these methods are discriminative and do not learn a generative latent space.",5. Related Work,[0],[0]
"Like our decoder, re-
current neural network grammars (Dyer et al., 2016) produce sequences through a linear traversal of the parse tree, but focus on the case where the underlying grammar is unknown and not context-free.",5. Related Work,[0],[0]
Maddison & Tarlow (2014) describe generative models of natural source code based on probabilistic context free grammars and neuro-probabilistic language models.,5. Related Work,[0],[0]
"However, these works are not geared towards learning a latent representation of the data.
",5. Related Work,[0],[0]
"Learning arithmetic expressions to fit data, often called symbolic regression, are generally based on genetic programming (Willis et al., 1997) or other computationally demanding evolutionary algorithms to propose candidate expressions (Schmidt & Lipson, 2009).",5. Related Work,[0],[0]
"Alternatives include running particle MCMC inference to estimate a Bayesian posterior over parse trees (Perov & Wood, 2016).
",5. Related Work,[0],[0]
"In molecular design, searching for new molecules is traditionally done by sifting through large databases of potential molecules and then subjecting them to a virtual screening process (Pyzer-Knapp et al., 2015; Gómez-Bombarelli et al., 2016a).",5. Related Work,[0],[0]
"These databases are too large to search via exhaustive enumeration, and require novel stochastic search algorithms tailored to the domain (Virshup et al., 2013; Rupakheti et al., 2015).",5. Related Work,[0],[0]
"Segler et al. (2017) fit a recurrent neural network to chemicals represented by SMILES strings, however their goal is more akin to density estimation; they learn a simulator which can sample proposals for novel molecules, but it is not otherwise used as part of an optimization or inference process itself.",5. Related Work,[0],[0]
"Our work most closely resembles Gómez-Bombarelli et al. (2016b) for novel molecule synthesis, in that we also learn a latent variable model which admits a continuous representation of the domain.",5. Related Work,[0],[0]
"However, both Segler et al. (2017) and Gómez-Bombarelli et al. (2016b) use character-level models for molecules.",5. Related Work,[0],[0]
"Empirically, it is clear that representing molecules and equations by way of their parse tree generated from a grammar outperforms text-based representations.",6. Discussion,[0],[0]
"We believe this approach will be broadly useful for representation learning, inference, and optimization in any domain which can be represented as text in a context-free language.",6. Discussion,[0],[0]
This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.,Acknowledgements,[0],[0]
"Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio.",abstractText,[0],[0]
"However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges.",abstractText,[0],[0]
"Crucially, state-of-the-art methods often produce outputs that are not valid.",abstractText,[0],[0]
"We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar.",abstractText,[0],[0]
"We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid.",abstractText,[0],[0]
"Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs.",abstractText,[0],[0]
We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.,abstractText,[0],[0]
Grammar Variational Autoencoder,title,[0],[0]
"Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a).",1 Introduction,[0],[0]
"State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.",1 Introduction,[0],[0]
"One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders,
including RNNs.",1 Introduction,[0],[0]
"Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)).",1 Introduction,[0],[0]
"Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task.",1 Introduction,[0],[0]
"This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.
",1 Introduction,[0],[0]
"Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.",1 Introduction,[0],[0]
"Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output.",1 Introduction,[0],[0]
"Since vectors correspond to words, it is natural for us to use dependency syntax.",1 Introduction,[0],[0]
"Dependency trees (see Figure 1) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.
",1 Introduction,[0],[0]
"In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).",1 Introduction,[0],[0]
"GCNs can be regarded as computing a latent-feature representation of a node (i.e. a real-valued vector) based on its k-
1957 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1957–1967
Copenhagen, Denmark, September 7–11, 2017.",1 Introduction,[0],[0]
"c©2017 Association for Computational Linguistics
th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017).",1 Introduction,[0],[0]
They are generally simple and computationally inexpensive.,1 Introduction,[0],[0]
"We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017).
",1 Introduction,[0],[0]
"Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework.",1 Introduction,[0],[0]
"As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task.",1 Introduction,[0],[0]
"Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information.",1 Introduction,[0],[0]
"A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016).",1 Introduction,[0],[0]
"Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively.
",1 Introduction,[0],[0]
"In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains).",1 Introduction,[0],[0]
"For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees.",1 Introduction,[0],[0]
"However, in this work we solely focus on dependency syntax and leave more general investigation for future work.
",1 Introduction,[0],[0]
"Our main contributions can be summarized as follows:
• we introduce a method for incorporating structure into NMT using syntactic GCNs;
• we show that GCNs can be used along with RNN and CNN encoders;
• we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German.",1 Introduction,[0],[0]
Notation.,2 Background,[0],[0]
"We use x for vectors, x1:t for a sequence of t vectors, and X for matrices.",2 Background,[0],[0]
The i-th value of vector x is denoted by xi.,2 Background,[0],[0]
We use ◦ for vector concatenation.,2 Background,[0],[0]
"In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx) of translating a source sentence",2.1 Neural Machine Translation,[0],[0]
x1:,2.1 Neural Machine Translation,[0],[0]
Tx (a sequence of Tx words) into a target sentence y1:Ty .,2.1 Neural Machine Translation,[0],[0]
"NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism.",2.1 Neural Machine Translation,[0],[0]
We will now briefly describe the components that we use in this paper.,2.1 Neural Machine Translation,[0],[0]
An encoder is a function that takes as input the source sentence and produces a representation encoding its semantic content.,2.1.1 Encoders,[0],[0]
"We describe recurrent, convolutional and bag-of-words encoders.
",2.1.1 Encoders,[0],[0]
Recurrent.,2.1.1 Encoders,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990) model sequential data.",2.1.1 Encoders,[0],[0]
They receive one input vector at each time step and update their hidden state to summarize all inputs up to that point.,2.1.1 Encoders,[0],[0]
"Given an input sequence x1:Tx = x1,x2, . . .",2.1.1 Encoders,[0],[0]
",xTx of word embeddings an RNN is defined recursively as follows:
",2.1.1 Encoders,[0],[0]
"RNN(x1:t) = f(xt,RNN(x1:t−1))
where f is a nonlinear function such as an LSTM (Hochreiter and Schmidhuber, 1997) or a GRU (Cho et al., 2014b).",2.1.1 Encoders,[0],[0]
"We will use the function RNN as an abstract mapping from an input sequence x1:T to final hidden state RNN(x1:Tx), regardless of the used nonlinearity.",2.1.1 Encoders,[0],[0]
"To not only summarize the past of a word, but also its future, a bidirectional RNN (Schuster and Paliwal, 1997; Irsoy and
Cardie, 2014) is often used.",2.1.1 Encoders,[0],[0]
"A bidirectional RNN reads the input sentence in two directions and then concatenates the states for each time step:
BIRNN(x1:Tx , t) = RNNF (x1:t)◦RNNB(xTx:t)
where RNNF and RNNB are the forward and backward RNNs, respectively.",2.1.1 Encoders,[0],[0]
"For further details we refer to the encoder of Bahdanau et al. (2015).
Convolutional.",2.1.1 Encoders,[0],[0]
"Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word (Gehring et al., 2016).",2.1.1 Encoders,[0],[0]
"One advantage of this approach over RNNs is that it allows for fast parallel computation, while sacrificing non-local context.",2.1.1 Encoders,[0],[0]
"To remedy the loss of context, multiple CNN layers can be stacked.",2.1.1 Encoders,[0],[0]
"Formally, given an input sequence x1:Tx , we define a CNN as follows:
",2.1.1 Encoders,[0],[0]
"CNN(x1:Tx , t) = f(xt−bw/2c, ..,xt, ..,xt+bw/2c)
where f is a nonlinear function, typically a linear transformation followed by ReLU, andw is the size of the window.
",2.1.1 Encoders,[0],[0]
Bag-of-Words.,2.1.1 Encoders,[0],[0]
In a bag-of-words (BoW) encoder every word is simply represented by its word embedding.,2.1.1 Encoders,[0],[0]
"To give the decoder some sense of word position, position embeddings (PE) may be added.",2.1.1 Encoders,[0],[0]
"There are different strategies for defining position embeddings, and in this paper we choose to learn a vector for each absolute word position up to a certain maximum length.",2.1.1 Encoders,[0],[0]
"We then represent the t-th word in a sequence as follows:
BOW(x1:Tx , t) = xt + pt
where xt is the word embedding and pt is the t-th position embedding.",2.1.1 Encoders,[0],[0]
A decoder produces the target sentence conditioned on the representation of the source sentence induced by the encoder.,2.1.2 Decoder,[0],[0]
"In Bahdanau et al. (2015) the decoder is implemented as an RNN conditioned on an additional input ci, the context vector, which is dynamically computed at each time step using an attention mechanism.
",2.1.2 Decoder,[0],[0]
"The probability of a target word yi is now a function of the decoder RNN state, the previous target word embedding, and the context vector.",2.1.2 Decoder,[0],[0]
The model is trained end-to-end for maximum log likelihood of the next target word given its context.,2.1.2 Decoder,[0],[0]
We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016).,2.2 Graph Convolutional Networks,[0],[0]
"For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).
",2.2 Graph Convolutional Networks,[0],[0]
"A GCN is a multilayer neural network that operates directly on a graph, encoding information about the neighborhood of a node as a realvalued vector.",2.2 Graph Convolutional Networks,[0],[0]
"In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its immediate neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"When multiple GCN layers are stacked, information about larger neighborhoods gets integrated.",2.2 Graph Convolutional Networks,[0],[0]
"For example, in the second layer, a node will receive information from its immediate neighbors, but this information already includes information from their respective neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"By choosing the number of GCN layers, we regulate the distance the information travels: with k layers a node receives information from neighbors at most k hops away.
",2.2 Graph Convolutional Networks,[0],[0]
"Formally, consider an undirected graph G = (V, E), where V is a set of n nodes, and E is a set of edges.",2.2 Graph Convolutional Networks,[0],[0]
"Every node is assumed to be connected to itself, i.e. ∀v ∈ V : (v, v) ∈ E .",2.2 Graph Convolutional Networks,[0],[0]
"Now, let X ∈ Rd×n be a matrix containing all n nodes with their features, where d is the dimensionality of the feature vectors.",2.2 Graph Convolutional Networks,[0],[0]
"In our case, X will contain word embeddings, but in general it can contain any kind of features.",2.2 Graph Convolutional Networks,[0],[0]
"For a 1-layer GCN, the new node representations are computed as follows:
hv = ρ ( ∑ u∈N (v) Wxu + b )
where W ∈ Rd×d is a weight matrix and b ∈ Rd a bias vector.1 ρ is an activation function, e.g. a ReLU.N (v) is the set of neighbors of v, which we assume here to always include v itself.",2.2 Graph Convolutional Networks,[0],[0]
"As stated before, to allow information to flow over multiple hops, we need to stack GCN layers.",2.2 Graph Convolutional Networks,[0],[0]
"The recursive computation is as follows:
h(j+1)v = ρ",2.2 Graph Convolutional Networks,[0],[0]
"( ∑ u∈N (v) W (j)h(j)u + b (j) )
where j indexes the layer, and h(0)v = xv.
1We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).",2.2 Graph Convolutional Networks,[0],[0]
Marcheggiani and Titov (2017) generalize GCNs to operate on directed and labeled graphs.2,2.3 Syntactic GCNs,[0],[0]
"This makes it possible to use linguistic structures such as dependency trees, where directionality and edge labels play an important role.",2.3 Syntactic GCNs,[0],[0]
They also integrate edge-wise gates which let the model regulate contributions of individual dependency edges.,2.3 Syntactic GCNs,[0],[0]
"We will briefly describe these modifications.
Directionality.",2.3 Syntactic GCNs,[0],[0]
"In order to deal with directionality of edges, separate weight matrices are used for incoming and outgoing edges.",2.3 Syntactic GCNs,[0],[0]
"We follow the convention that in dependency trees heads point to their dependents, and thus outgoing edges are used for head-to-dependent connections, and incoming edges are used for dependent-to-head connections.",2.3 Syntactic GCNs,[0],[0]
"Modifying the recursive computation for directionality, we arrive at:
h(j+1)v = ρ",2.3 Syntactic GCNs,[0],[0]
"( ∑ u∈N (v) W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) dir(u,v) )
",2.3 Syntactic GCNs,[0],[0]
"where dir(u, v) selects the weight matrix associated with the directionality of the edge connecting u and v (i.e. WIN for u-to-v, WOUT for v-to-u, and WLOOP for v-to-v).",2.3 Syntactic GCNs,[0],[0]
"Note that self loops are modeled separately,
so there are now three times as many parameters as in a non-directional GCN.
2For",2.3 Syntactic GCNs,[0],[0]
"an alternative approach to integrating labels and directions, see applications of GCNs to statistical relation learning (Schlichtkrull et al., 2017).
",2.3 Syntactic GCNs,[0],[0]
Labels.,2.3 Syntactic GCNs,[0],[0]
Making the GCN sensitive to labels is straightforward given the above modifications for directionality.,2.3 Syntactic GCNs,[0],[0]
"Instead of using separate matrices for each direction, separate matrices are now defined for each direction and label combination:
h(j+1)v = ρ ( ∑ u∈N (v) W (j) lab(u,v) h (j) u + b (j) lab(u,v) )
where we incorporate the directionality of an edge directly in its label.
",2.3 Syntactic GCNs,[0],[0]
"Importantly, to prevent over-parametrization, only bias terms are made label-specific, in other words: Wlab(u,v) = Wdir(u,v).",2.3 Syntactic GCNs,[0],[0]
"The resulting syntactic GCN is illustrated in Figure 2 (shown on top of a CNN, as we will explain in the subsequent section).
",2.3 Syntactic GCNs,[0],[0]
Edge-wise gating.,2.3 Syntactic GCNs,[0],[0]
"Syntactic GCNs also include gates, which can down-weight the contribution of individual edges.",2.3 Syntactic GCNs,[0],[0]
"They also allow the model to deal with noisy predicted structure, i.e. to ignore potentially erroneous syntactic edges.",2.3 Syntactic GCNs,[0],[0]
"For each edge, a scalar gate is calculated as follows:
g(j)u,v = σ",2.3 Syntactic GCNs,[0],[0]
"( h(j)u · ŵ (j) dir(u,v) + b̂ (j) lab(u,v) )",2.3 Syntactic GCNs,[0],[0]
"where σ is the logistic sigmoid function, and ŵ
(j) dir(u,v) ∈ R d and b̂(j)lab(u,v) ∈",2.3 Syntactic GCNs,[0],[0]
R are learned parameters for the gate.,2.3 Syntactic GCNs,[0],[0]
"The computation becomes:
h(j+1)v =ρ (∑ u∈N (v) g(j)u,v ( W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) lab(u,v) ))",2.3 Syntactic GCNs,[0],[0]
"In this work we focus on exploiting structural information on the source side, i.e. in the encoder.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that using an encoder that incorporates syntax will lead to more informative representations of words, and that these representations, when used as context vectors by the decoder, will lead to an improvement in translation quality.",3 Graph Convolutional Encoders,[0],[0]
"Consequently, in all our models, we use the decoder of Bahdanau et al. (2015) and keep this part of the model constant.",3 Graph Convolutional Encoders,[0],[0]
"As is now common practice, we do not use a maxout layer in the decoder, but apart from this we do not deviate from the original definition.",3 Graph Convolutional Encoders,[0],[0]
"In all models we make use of GRUs (Cho et al., 2014b) as our RNN units.
",3 Graph Convolutional Encoders,[0],[0]
"Our models vary in the encoder part, where we exploit the power of GCNs to induce syntacticallyaware representations.",3 Graph Convolutional Encoders,[0],[0]
"We now define a series of encoders of increasing complexity.
BoW + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our first and simplest model, we propose a bag-of-words encoder (with position embeddings, see §2.1.1), with a GCN on top.",3 Graph Convolutional Encoders,[0],[0]
"In other words, inputs h(0) are a sum of embeddings of a word and its position in a sentence.",3 Graph Convolutional Encoders,[0],[0]
"Since the original BoW encoder captures the linear ordering information only in a very crude way (through the position embeddings), the structural information provided by GCN should be highly beneficial.
",3 Graph Convolutional Encoders,[0],[0]
Convolutional + GCN.,3 Graph Convolutional Encoders,[0],[0]
"In our second model, we use convolutional neural networks to learn word representations.",3 Graph Convolutional Encoders,[0],[0]
"CNNs are fast, but by definition only use a limited window of context.",3 Graph Convolutional Encoders,[0],[0]
"Instead of the approach used by Gehring et al. (2016) (i.e. stacking multiple CNN layers on top of each other), we use a GCN to enrich the one-layer CNN representations.",3 Graph Convolutional Encoders,[0],[0]
Figure 2 shows this model.,3 Graph Convolutional Encoders,[0],[0]
"Note that, while the figure shows a CNN with a window size of 3, we will use a larger window size of 5 in our experiments.",3 Graph Convolutional Encoders,[0],[0]
"We expect this model to perform better than BoW + GCN, because of the additional local context captured by the CNN.
BiRNN + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our third and most powerful model, we employ bidirectional recurrent neural networks.",3 Graph Convolutional Encoders,[0],[0]
"In this model, we start by encoding the source sentence using a BiRNN (i.e. BiGRU), and use the resulting hidden states as input to a GCN.",3 Graph Convolutional Encoders,[0],[0]
"Instead of relying on linear order only, the GCN will allow the encoder to ‘teleport’ over parts of the input sentence, along dependency edges, con-
necting words that otherwise might be far apart.",3 Graph Convolutional Encoders,[0],[0]
"The model might not only benefit from this teleporting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see §2.3 for details).
",3 Graph Convolutional Encoders,[0],[0]
"This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information without explicit supervision (Linzen et al., 2016), and hence they should be hard to improve on by incorporating treebank syntax.
",3 Graph Convolutional Encoders,[0],[0]
Marcheggiani and Titov (2017) did not observe improvements from using multiple GCN layers in semantic role labeling.,3 Graph Convolutional Encoders,[0],[0]
"However, we do expect that propagating information from further in the tree should be beneficial in principle.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that the first layer is the most influential one, capturing most of the syntactic context, and that additional layers only modestly modify the representations.",3 Graph Convolutional Encoders,[0],[0]
"To ease optimization, we add a residual connection (He et al., 2016) between the GCN layers, when using more than one layer.",3 Graph Convolutional Encoders,[0],[0]
"Experiments are performed using the Neural Monkey toolkit3 (Helcl and Libovický, 2017), which implements the model of Bahdanau et al. (2015) in TensorFlow.",4 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 (0.0002 for CNN models).4",4 Experiments,[0],[0]
The batch size is set to 80.,4 Experiments,[0],[0]
"Between layers we apply dropout with a probability of 0.2, and in experiments with GCNs5 we use the same value for edge dropout.",4 Experiments,[0],[0]
"We train for 45 epochs, evaluating the BLEU performance of the model every epoch on the validation set.",4 Experiments,[0],[0]
"For testing, we select the model with the highest validation BLEU.",4 Experiments,[0],[0]
L2 regularization is used with a value of 10−8.,4 Experiments,[0],[0]
All the model selection (incl. hyperparameter selections) was performed on the validation set.,4 Experiments,[0],[0]
"In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.
",4 Experiments,[0],[0]
"We will describe an artificial experiment in §4.1 and MT experiments in §4.2.
",4 Experiments,[0],[0]
"3https://github.com/ufal/neuralmonkey 4Like Gehring et al. (2016) we note that Adam is too aggressive for CNN models, hence we use a lower learning rate.",4 Experiments,[0],[0]
5GCN code at https://github.com/bastings/neuralmonkey,4 Experiments,[0],[0]
Our goal here is to provide an intuition for the capabilities of GCNs.,4.1 Reordering artificial sequences,[0],[0]
We define a reordering task where randomly permuted sequences need to be put back into the original order.,4.1 Reordering artificial sequences,[0],[0]
"We encode the original order using edges, and test if GCNs can successfully exploit them.",4.1 Reordering artificial sequences,[0],[0]
Note that this task is not meant to provide a fair comparison to RNNs.,4.1 Reordering artificial sequences,[0],[0]
"The input (besides the edges) simply does not carry any information about the original ordering, so RNNs cannot possibly solve this task.
Data.",4.1 Reordering artificial sequences,[0],[0]
"From a vocabulary of 26 types, we generate random sequences of 3-10 tokens.",4.1 Reordering artificial sequences,[0],[0]
"We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels.",4.1 Reordering artificial sequences,[0],[0]
"Additionally, we point every token to an arbitrary position in the sequence with a label from a distinct set of 5 ‘fake’ labels.",4.1 Reordering artificial sequences,[0],[0]
"We sample 25000 training and 1000 validation sequences.
",4.1 Reordering artificial sequences,[0],[0]
Model.,4.1 Reordering artificial sequences,[0],[0]
"We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN on top.",4.1 Reordering artificial sequences,[0],[0]
"We use 32, 64 and 128 units for embeddings, GRU units and GCN layers, respectively.
Results.",4.1 Reordering artificial sequences,[0],[0]
"After 6 epochs of training, the model learns to put permuted sequences back into order, reaching a validation BLEU of 99.2.",4.1 Reordering artificial sequences,[0],[0]
"Figure 3 shows that the mean values of the bias terms of gates (i.e. b̂) for real and fake edges are far apart, suggesting that the GCN learns to distinguish them.",4.1 Reordering artificial sequences,[0],[0]
"Interestingly, this illustrates why edge-wise gating is beneficial.",4.1 Reordering artificial sequences,[0],[0]
"A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases b would then be label-dependent.",4.1 Reordering artificial sequences,[0],[0]
"Consequently, it would only do a mediocre job in reordering.",4.1 Reordering artificial sequences,[0],[0]
"Although using label-specific matrices W would also help, this would not scale to the real scenario (see §2.3).",4.1 Reordering artificial sequences,[0],[0]
Data.,4.2 Machine Translation,[0],[0]
For our experiments we use the En-De and En-Cs News Commentary v11 data from the WMT16 translation task.6,4.2 Machine Translation,[0],[0]
For En-De we also train on the full WMT16 data set.,4.2 Machine Translation,[0],[0]
"As our validation set and test set we use newstest2015 and newstest2016, respectively.
",4.2 Machine Translation,[0],[0]
Pre-processing.,4.2 Machine Translation,[0],[0]
"The English sides of the corpora are tokenized and parsed into dependency
6http://www.statmt.org/wmt16/translation-task.html
trees by SyntaxNet,7 using the pre-trained Parsey McParseface model.8 The Czech and German sides are tokenized using the Moses tokenizer.9 Sentence pairs where either side is longer than 50 words are filtered out after tokenization.
",4.2 Machine Translation,[0],[0]
Vocabularies.,4.2 Machine Translation,[0],[0]
"For the English sides, we construct vocabularies from all words except those with a training set frequency smaller than three.",4.2 Machine Translation,[0],[0]
"For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b).",4.2 Machine Translation,[0],[0]
"Given the size of our data set, and following Wu et al. (2016), we use 8000 BPE merges to obtain robust frequencies for our subword units (16000 merges for full data experiment).",4.2 Machine Translation,[0],[0]
"Data set statistics are summarized in Table 1 and vocabulary sizes in Table 2.
Hyperparameters.",4.2 Machine Translation,[0],[0]
"We use 256 units for word embeddings, 512 units for GRUs (800 for En-De full data set experiment), and 512 units for convolutional layers (or equivalently, 512 ‘channels’).",4.2 Machine Translation,[0],[0]
"The dimensionality of the GCN layers is equiva-
7https://github.com/tensorflow/models/tree/master/syntaxnet 8The used dependency parses can be reproduced by using
the syntaxnet/demo.sh shell script.",4.2 Machine Translation,[0],[0]
"9https://github.com/moses-smt/mosesdecoder
lent to the dimensionality of their input.",4.2 Machine Translation,[0],[0]
"We report results for 2-layer GCNs, as we find them most effective (see ablation studies below).
",4.2 Machine Translation,[0],[0]
Baselines.,4.2 Machine Translation,[0],[0]
"We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN.",4.2 Machine Translation,[0],[0]
"See §2.1.1 for details.
",4.2 Machine Translation,[0],[0]
Evaluation.,4.2 Machine Translation,[0],[0]
"We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10",4.2 Machine Translation,[0],[0]
English-German.,4.2.1 Results,[0],[0]
Table 3 shows test results on English-German.,4.2.1 Results,[0],[0]
"Unsurprisingly, the bag-ofwords baseline performs the worst.",4.2.1 Results,[0],[0]
"We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens.",4.2.1 Results,[0],[0]
"The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model.",4.2.1 Results,[0],[0]
"The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9.",4.2.1 Results,[0],[0]
"Interestingly, GCNs still manage to improve the result by +2.3 BLEU1 and +1.2 BLEU4 points.",4.2.1 Results,[0],[0]
"Finally, we observe a big jump in BLEU4 by using the full data set and beam search (beam 12).",4.2.1 Results,[0],[0]
"The BiRNN now reaches 23.3, while adding a GCN achieves a score of 23.9.
",4.2.1 Results,[0],[0]
English-Czech.,4.2.1 Results,[0],[0]
Table 4 shows test results on English-Czech.,4.2.1 Results,[0],[0]
"While it is difficult to obtain high absolute BLEU scores on this dataset, we can still see similar relative improvements.",4.2.1 Results,[0],[0]
"Again the BoW baseline scores worst, with the BoW+GCN easily beating that result.",4.2.1 Results,[0],[0]
"The CNN baseline scores BLEU4 of 8.1, but the CNN+GCN improves on that, this time by +1.0 and +0.6 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"Interestingly, BLEU1 scores for the BoW+GCN and CNN+GCN models are
10See Stanojević and Simaan (2015).",4.2.1 Results,[0],[0]
"TER (Snover et al., 2006) and BEER (Stanojević and Sima’an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.
higher than both baselines so far.",4.2.1 Results,[0],[0]
"Finally, the BiRNN baseline scores a BLEU4 of 8.9, but it is again beaten by the BiRNN+GCN model with +1.9 BLEU1 and +0.7 BLEU4.
",4.2.1 Results,[0],[0]
Effect of GCN layers.,4.2.1 Results,[0],[0]
How many GCN layers do we need?,4.2.1 Results,[0],[0]
Every layer gives us an extra hop in the graph and expands the syntactic neighborhood of a word.,4.2.1 Results,[0],[0]
Table 5 shows validation BLEU performance as a function of the number of GCN layers.,4.2.1 Results,[0],[0]
"For English-German, using a 1-layer GCN improves BLEU-1, but surprisingly has little effect on BLEU4.",4.2.1 Results,[0],[0]
"Adding an additional layer gives improvements on both BLEU1 and BLEU4 of +1.3 and +0.73, respectively.",4.2.1 Results,[0],[0]
"For English-Czech, performance increases with each added GCN layer.
",4.2.1 Results,[0],[0]
Effect of sentence length.,4.2.1 Results,[0],[0]
We hypothesize that GCNs should be more beneficial for longer sentences: these are likely to contain long-distance syntactic dependencies which may not be adequately captured by RNNs but directly encoded in GCNs.,4.2.1 Results,[0],[0]
"To test this, we partition the validation data into five buckets and calculate BLEU for each of them.",4.2.1 Results,[0],[0]
Figure 4 shows that GCN-based models outperform their respective baselines rather uniformly across all buckets.,4.2.1 Results,[0],[0]
This is a surprising result.,4.2.1 Results,[0],[0]
"One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs.
",4.2.1 Results,[0],[0]
Discussion.,4.2.1 Results,[0],[0]
Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER).,4.2.1 Results,[0],[0]
"Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax.",4.2.1 Results,[0],[0]
"We review various accounts to syntax in NMT as well as other convolutional encoders.
",5 Related Work,[0],[0]
Syntactic features and/or constraints.,5 Related Work,[0],[0]
"Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings.",5 Related Work,[0],[0]
Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree.,5 Related Work,[0],[0]
"In the decoder, word and node representations compete under the same attention mechanism.",5 Related Work,[0],[0]
"Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT.
Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments.",5 Related Work,[0],[0]
"Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees.
",5 Related Work,[0],[0]
Multi-task Learning.,5 Related Work,[0],[0]
Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations.,5 Related Work,[0],[0]
Luong et al. (2015a) predict linearized constituency parses as an additional task.,5 Related Work,[0],[0]
"Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side.",5 Related Work,[0],[0]
Nadejde et al. (2017) multi,5 Related Work,[0],[0]
"-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags.
",5 Related Work,[0],[0]
Latent structure.,5 Related Work,[0],[0]
Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer.,5 Related Work,[0],[0]
They encode source words as a learned average of potential parents emulating a relaxed dependency tree.,5 Related Work,[0],[0]
"While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improvements on English-Japanese.",5 Related Work,[0],[0]
"Yogatama et al. (2016) introduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees.
",5 Related Work,[0],[0]
Convolutional encoders.,5 Related Work,[0],[0]
Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders.,5 Related Work,[0],[0]
To increase the receptive field of a word’s context they stack multiple CNN layers.,5 Related Work,[0],[0]
Kalchbrenner et al. (2016) use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field.,5 Related Work,[0],[0]
"In contrast to both approaches, we use a GCN informed by dependency structure to increase it.",5 Related Work,[0],[0]
"Finally, Cho et al. (2014a) propose a recursive convolutional neural network which builds a tree out of the word leaf nodes, but which ends up compressing the source sentence in a single vector.",5 Related Work,[0],[0]
We have presented a simple and effective approach to integrating syntax into neural machine translation models and have shown consistent BLEU4 improvements for two challenging language pairs: English-German and English-Czech.,6 Conclusions,[0],[0]
"Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-
yond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.",6 Conclusions,[0],[0]
We would like to thank Michael Schlichtkrull and Thomas Kipf for their suggestions and comments.,Acknowledgments,[0],[0]
"This work was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).",Acknowledgments,[0],[0]
We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation.,abstractText,[0],[0]
"We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data.",abstractText,[0],[0]
Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods.,abstractText,[0],[0]
"GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks).",abstractText,[0],[0]
We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.,abstractText,[0],[0]
Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 772–776, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
The goal of relation extraction is to extract tuples of a particular relation from a corpus of natural language text.,1 Introduction,[0],[0]
"A widely employed approach to relation extraction is based on iterative bootstrapping (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006; Pantel and Pennacchiotti, 2006), which can be applied with only small amounts of supervision and which scales well to very large datasets.
",1 Introduction,[0],[0]
"A well-known problem with iterative bootstrapping is a phenomenon known as semantic drift (Curran et al., 2007): as bootstrapping proceeds it is likely that unreliable patterns will lead to false extractions.",1 Introduction,[0],[0]
"These extraction errors are amplified in the following iterations and the extracted relation will drift away
from the intended target.",1 Introduction,[0],[0]
Semantic drift often results in low precision extractions and therefore poses a major limitation of iterative bootstrapping algorithms.,1 Introduction,[0],[0]
"Previous work on iterative bootstrapping has addressed the issue of reducing semantic drift for example by bagging the results of various runs employing differing seed tuples, constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process (T. McIntosh, 2010; McIntosh and Curran, 2009; Curran et al., 2007).
",1 Introduction,[0],[0]
"However, the analysis of Komachi et al. (2008) has shown that semantic drift is an inherent property of iterative bootstrapping algorithms and therefore poses a fundamental problem.",1 Introduction,[0],[0]
"They have shown that iterative bootstrapping without pruning corresponds to an eigenvector computation and thus as the number of iterations increases the resulting ranking will always converge towards the same static ranking of tuples, regardless of the particular choice of seed instances.
",1 Introduction,[0],[0]
"In this paper, we describe an alternative method, that is not susceptible to semantic drift.",1 Introduction,[0],[0]
"We represent our data as a bipartite graph, whose vertices correspond to patterns and tuples respectively and whose edges capture cooccurrences and then measure the distance of a tuple to the seed set in terms of random walk hitting times.",1 Introduction,[0],[0]
"Experimental results confirm that semantic drift is avoided by our method and show that substantial improvements over iterative forms of bootstrapping are possible.
772",1 Introduction,[0],[0]
"From a given corpus, we extract a dataset consisting of tuples and patterns.",2 Scoring with Hitting Times,[0],[0]
"Tuples are pairs of co-occurring strings in the corpus, such as (Bill Gates, Microsoft), which potentially belong to a particular relation of interest.",2 Scoring with Hitting Times,[0],[0]
"In our case, patterns are simply the sequence of tokens occurring between tuple elements, e.g. “is the founder of”.",2 Scoring with Hitting Times,[0],[0]
"We represent all the tuple types1 X and all the extraction pattern types Y contained in a given corpus through an undirected, weighted, bipartite graph G = (V,E) with vertices V = X ∪ Y and edges E ⊂",2 Scoring with Hitting Times,[0],[0]
X,2 Scoring with Hitting Times,[0],[0]
"× Y , where an edge (x, y) ∈ E indicates that tuple x occurrs with pattern y somewhere in the corpus.",2 Scoring with Hitting Times,[0],[0]
"Edge weights are defined through a weight matrix W which holds the weight Wi,j = w(vi, vj) for edges (vi, vj) ∈ E. Specifically, we use the count of how many times a tuple occurs with a pattern in the corpus and weights for unconnected vertices are zero.
",2 Scoring with Hitting Times,[0],[0]
"Our goal is to compute a score vector σ holding a score σi = σ(xi) for each tuple xi ∈ X, which quantifies how well the tuple matches the seed tuples.",2 Scoring with Hitting Times,[0],[0]
"Higher scores indicate that the tuple is more likely to belong to the relation defined through the seeds and thus the score vector effectively provides a ranking of the tuples.
",2 Scoring with Hitting Times,[0],[0]
We define scores of tuples based on their distance2 to the seed tuples in the graph.,2 Scoring with Hitting Times,[0],[0]
"The distance of some tuple x to the seed set S can be naturally formalized in terms of the average time it takes until a random walk starting in S reaches x, the hitting time.",2 Scoring with Hitting Times,[0],[0]
The random walk is defined through the probability distribution over start vertices and through a matrix of transition probabilities.,2 Scoring with Hitting Times,[0],[0]
"Edge weights are constrained to be non-negative, which allows us to define the transition matrix P with Pi,j = p(vj |vi) = 1dviw(vi, vj), where dv =∑
vk∈V w(v, vk) is the degree of a vertex v ∈ V .",2 Scoring with Hitting Times,[0],[0]
The distance of two vertices is measured in terms of the average time of a random walk be1Note that we are using tuple and pattern types rather than particular mentions in the corpus. 2The term is used informally.,2 Scoring with Hitting Times,[0],[0]
"In particular, hitting times are not a distance metric, since they can be asymmetric.
",2 Scoring with Hitting Times,[0],[0]
tween the two.,2 Scoring with Hitting Times,[0],[0]
"Specifically, we adopt the notion of T-truncated hitting time (Sarkar and Moore, 2007) defined as the expected number of steps it takes until a random walk of at most T steps starting at vi reaches vj for the first time:
hT",2 Scoring with Hitting Times,[0],[0]
(vj |vi) = { 0 iff.,2 Scoring with Hitting Times,[0],[0]
"vj = vi or T=0 1 + ∑ vk∈V p(vk|vi)h T−1(vj |vk)
",2 Scoring with Hitting Times,[0],[0]
The truncated hitting time hT,2 Scoring with Hitting Times,[0],[0]
"(vj |vi) can be approximately computed by sampling M independent random walks starting at vi of length T and computing
ĥT",2 Scoring with Hitting Times,[0],[0]
(vj |vi),2 Scoring with Hitting Times,[0],[0]
"= 1
M m∑ k=1 tk",2 Scoring with Hitting Times,[0],[0]
"+ (1− m M )T (1)
",2 Scoring with Hitting Times,[0],[0]
where {t1 . . .,2 Scoring with Hitting Times,[0],[0]
"tm} are the sampled first-hit times of random walks which reach vj within T steps (Sarkar et al., 2008).
",2 Scoring with Hitting Times,[0],[0]
The score σHT (v) of a vertex v /∈,2 Scoring with Hitting Times,[0],[0]
"S to the seed set S is then defined as the inverse of the average T -truncated hitting time of random walks starting at a randomly chosen vertex s ∈ S:
1 σHT (v) = hT",2 Scoring with Hitting Times,[0],[0]
(v|S) = 1 |S| ∑ s∈S hT (v|s) (2),2 Scoring with Hitting Times,[0],[0]
"We extracted tuples and patterns from the fifth edition of the Gigaword corpus (Parker et al., 2011), by running a named entity tagger and extracting all pairs of named entities and extracting occurring within the same sentence which do not have another named entity standing between them.",3 Experiments,[0],[0]
"Gold standard seed and test tuples for a set of relations were obtained from YAGO (Suchanek et al., 2007).",3 Experiments,[0],[0]
"Specifically, we took all relations for which there are at least 300 tuples, each of which occurs at least once in the corpus.",3 Experiments,[0],[0]
"This resulted in the set of relations shown in Table 1, plus the development relation hasWonPrize.
",3 Experiments,[0],[0]
"For evaluation, we use the percentile rank of the median test set element (PRM, see Francois et al. 2007), which reflects the quality of the
full produced ranking, not just the top N elements and is furthermore computable with only a small set of labeled test tuples 3.
",3 Experiments,[0],[0]
We compare our proposed method based on hitting times (HT) with two variants of iterative bootstrapping.,3 Experiments,[0],[0]
The first one (IB1) does not employ pruning and corresponds to the algorithm described in Komachi et al. (2008).,3 Experiments,[0],[0]
The second one (IB2) corresponds to a standard bootstrapping algorithm which employs pruning after each step in order to reduce semantic drift.,3 Experiments,[0],[0]
"Specifically, scores are pruned after projecting from X onto Y and from Y onto X, retaining only the top N (t) =",3 Experiments,[0],[0]
N0t scores at iteration t and setting all other scores to zero.,3 Experiments,[0],[0]
The experiments in this section were conducted on the held out development relation hasWonPrize.,3.1 Parametrizations,[0],[0]
"The ranking produced by both forms of iterative bootstrapping IB1 and IB2 depend on the number of iterations, as shown in Figure 1.",3.1 Parametrizations,[0],[0]
IB1 achieves an optimal ranking after just one iteration and thereafter scores get worse due to semantic drift.,3.1 Parametrizations,[0],[0]
"In contrast, pruning helps avoid semantic drift for IB2, which attains an optimal score after 2 iterations and achieves relatively constant scores for several iterations.",3.1 Parametrizations,[0],[0]
"However, during iteration 9 an incorrect pattern is kept and this at once leads to a drastic loss in accuracy, showing that semantic drift is only deferred and not completely eliminated.
",3.1 Parametrizations,[0],[0]
"Our method HT has parameter T , corresponding to the truncation time, i.e., maximal number of steps of a random walk.",3.1 Parametrizations,[0],[0]
Figure 2 shows the PRM of our method for different values of T .,3.1 Parametrizations,[0],[0]
"Performance gets better as T increases and is optimal for T = 12, whereas for larger values, the performance gets slightly worse again.",3.1 Parametrizations,[0],[0]
"The figure shows that, if T is large enough (> 5), the PRM is relatively constant and there is no phenomenon comparable to semantic drift, which causes instability in the produced rankings.
",3.1 Parametrizations,[0],[0]
3other common metrics do not satisfy these conditions.,3.1 Parametrizations,[0],[0]
"To evaluate the methods, firstly the parameters for each method were set to the optimal values as determined in the previous section.",3.2 Method Comparison,[0],[0]
"For the experiments here, we again use 200 randomly chosen tuples as the seeds for each relation.",3.2 Method Comparison,[0],[0]
"All the remaining gold standard tuples are used for testing.
",3.2 Method Comparison,[0],[0]
Table 1 shows the PRM for the three methods.,3.2 Method Comparison,[0],[0]
"For a majority of the relations (12/16) HT attains the best, i.e. lowest, PRM, which confirms that hitting times constitute an accurate way of measuring the distance of tuples to the seed set.",3.2 Method Comparison,[0],[0]
IB1 and IB2 each perform best on 2/16 of the relations.,3.2 Method Comparison,[0],[0]
"A sign test on these results yields that
HT is better than both IB1 and IB2 at significance level α < 0.01.
",3.2 Method Comparison,[0],[0]
"Moreover, the ranking produced by HT is stable and not affected by semantic drift, given that even where results are worse than for IB1 or IB2, they are still close to the best performing method.",3.2 Method Comparison,[0],[0]
"In contrast, when semantic drift occurs, the performance of IB1 and IB2 can deteriorate drastically, e.g. for the worksAt relation, where both IB1 and IB2 produce rankings that are a lot worse than the one produced by HT.",3.2 Method Comparison,[0],[0]
Figure 3 shows the PRM for each of the three methods as a function of the size of the seed set for the relation created.,3.3 Sensitivity to Seed Set Size,[0],[0]
"For small seed sets, the performance of the iterative methods can be increased by adding more seeds.",3.3 Sensitivity to Seed Set Size,[0],[0]
"However, from a seed set size of 50 onwards, performance remains relatively constant.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In other words, iterative bootstrapping is not benefitting from the information provided by the additional labeled data, and thus has a poor learning performance.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In contrast, for our method based on hitting times, the performance continually improves as the seed set size is increased.",3.3 Sensitivity to Seed Set Size,[0],[0]
"Thus, also in terms of learning performance, our method is more sound than iterative bootstrapping.",3.3 Sensitivity to Seed Set Size,[0],[0]
The paper has presented a graph-based method for seed set expansion which is not susceptible to semantic drift and on most relations outperforms iterative bootstrapping.,4 Conclusions,[0],[0]
The method measures distance between vertices through random walk hitting times.,4 Conclusions,[0],[0]
"One property which makes hitting times an appropriate distance measure is their ability to reflect the overall connectivity structure of the graph, in contrast to measures such as the shortest path between two vertices.",4 Conclusions,[0],[0]
"The hitting time will decrease when the number of paths from the start vertex to the target vertex increases, when the length of paths decreases or when the likelihood (weights) of paths increases.",4 Conclusions,[0],[0]
"These properties are particularly important when the observed graph edges must be assumed to be merely a sample of all plausible edges, possibly perturbated by noise.",4 Conclusions,[0],[0]
"This has also been asserted by previous work, which has shown that hitting times successfully capture the notion of similarity for other natural language processing problems such as learning paraphrases (Kok and Brockett, 2010) and related problems such as query suggestion (Mei et al., 2008).",4 Conclusions,[0],[0]
Future work will be aimed towards employing our hitting time based method in combination with a richer feature set.,4 Conclusions,[0],[0]
"Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision.",abstractText,[0],[0]
"Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions.",abstractText,[0],[0]
"This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph.",abstractText,[0],[0]
"In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results in better extractions than iterative methods.",abstractText,[0],[0]
Graph-Based Seed Set Expansion for Relation Extraction Using Random Walk Hitting Times,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1537–1546 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980).",1 Introduction,[0],[0]
"In the most influential account of metaphor to date, Lakoff and Johnson explain the phenomenon through the presence of systematic metaphorical associations between two distinct concepts or domains.",1 Introduction,[0],[0]
"For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept).",1 Introduction,[0],[0]
"Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process.
",1 Introduction,[0],[0]
"Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).",1 Introduction,[0],[0]
"A number of approaches to metaphor processing have thus been proposed, focusing pre-
dominantly on classifying linguistic expressions as literal or metaphorical.",1 Introduction,[0],[0]
"They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014).",1 Introduction,[0],[0]
"While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them.",1 Introduction,[0],[0]
"In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016).",1 Introduction,[0],[0]
"Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data.
",1 Introduction,[0],[0]
We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition.,1 Introduction,[0],[0]
"Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.",1 Introduction,[0],[0]
"In this paper, we present a novel architecture which (1) models the interaction between the source and target domains in the metaphor via a gating function; (2) specialises word representations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity.",1 Introduction,[0],[0]
"We experimented with two types of word representations
1537
as inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.
",1 Introduction,[0],[0]
"We evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically.",1 Introduction,[0],[0]
Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification.,1 Introduction,[0],[0]
"We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existing systems based on hand-coded lexical knowledge.",1 Introduction,[0],[0]
The majority of approaches to metaphor processing cast the problem as classification of linguistic expressions as metaphorical or literal.,2 Related Work,[0],[0]
Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial discourse.,2 Related Work,[0],[0]
"They used the maximum entropy classifier and the verbs’ nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results.",2 Related Work,[0],[0]
"Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from SUMO ontology, including domain types (ABSTRACT, PHYSICAL, SOCIAL, MENTAL) and event status (PROCESS, STATE, OBJECT).",2 Related Work,[0],[0]
"Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses.",2 Related Work,[0],[0]
They have shown that the model learned with such coarse semantic features is portable across languages.,2 Related Work,[0],[0]
The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features.,2 Related Work,[0],[0]
"They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees.",2 Related Work,[0],[0]
Mohler et al. (2013) aimed at modelling conceptual information.,2 Related Work,[0],[0]
They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets.,2 Related Work,[0],[0]
"The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures
of the known ones.
",2 Related Work,[0],[0]
"With the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone.",2 Related Work,[0],[0]
Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora.,2 Related Work,[0],[0]
"For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features.",2 Related Work,[0],[0]
"As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain.",2 Related Work,[0],[0]
Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples.,2 Related Work,[0],[0]
Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way.,2 Related Work,[0],[0]
"Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context.",2 Related Work,[0],[0]
Gutiérrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework.,2 Related Work,[0],[0]
"Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model.",2 Related Work,[0],[0]
"They then used these representations in the metaphor identification task, achieving promising results.
",2 Related Work,[0],[0]
"The more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features.",2 Related Work,[0],[0]
Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features.,2 Related Work,[0],[0]
"They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively.",2 Related Work,[0],[0]
"They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity.",2 Related Work,[0],[0]
"They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar-
ity and a threshold was tuned on the development set to distinguish between literal and metaphorical phrases.",2 Related Work,[0],[0]
"This approach was their best performing linguistic model, outperformed only by a multimodal system which included both linguistic and visual features.
",2 Related Work,[0],[0]
"Bulat et al. (2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005).",2 Related Work,[0],[0]
"They first learn a mapping from the skip-gram embedding vector space to the property norm space using linear regression, which allows them to generate property norm representations for unseen words.",2 Related Work,[0],[0]
The authors then train an SVM classifier to detect metaphors using these representations as input.,2 Related Work,[0],[0]
Bulat et al. (2017) have shown that the cognitively-driven property norms outperform standard skip-gram representations in this task.,2 Related Work,[0],[0]
"Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity.",3 Supervised Similarity Network,[0],[0]
"For example, the phrase ‘colourful personality’ receives a score:
s = cos(xc, xp) (1)
where xc is the embedding for colourful and xp is the embedding for personality.",3 Supervised Similarity Network,[0],[0]
"The combined phrase is classified as being metaphorical based on a threshold, which is optimised on a development dataset.",3 Supervised Similarity Network,[0],[0]
"In this paper, we propose several extensions to this general idea, creating a supervised version of the cosine similarity metric which can be optimised on training data to be more suitable for metaphor detection.",3 Supervised Similarity Network,[0],[0]
Directly comparing the vector representations of both words treats each of the embeddings as an independent unit.,3.1 Word Representation Gating,[0],[0]
"In reality, however, word meanings vary and adapt based on the context.",3.1 Word Representation Gating,[0],[0]
"In case of metaphorical language (e.g. “cure crime”), the source domain properties of the verb (e.g. cure) are projected onto the target domain noun (e.g. crime), resulting in the interaction of the two domains in the interpretation of the metaphor.
",3.1 Word Representation Gating,[0],[0]
"In order to integrate this idea into the metaphor detection method, we can construct a gating function that modulates the representation of one word based on the other.",3.1 Word Representation Gating,[0],[0]
"Given embeddings x1 and x2, the gating values are predicted as a non-linear transformation of x1 and applied to x2 through element-wise multiplication:
g = σ(Wgx1) (2)
x̃2",3.1 Word Representation Gating,[0],[0]
"= x2 g (3)
",3.1 Word Representation Gating,[0],[0]
"whereWg is a weight matrix that is optimised during training, σ is the sigmoid activation function, and represents element-wise multiplication.",3.1 Word Representation Gating,[0],[0]
"In an adjective-noun phrase, this architecture allows the network to first look at the adjective, then use its meaning to change the representation of the noun.",3.1 Word Representation Gating,[0],[0]
"The sigmoid activation function makes it act as a filter, choosing which information from the original embedding gets through to the rest of the network.",3.1 Word Representation Gating,[0],[0]
"While learning a more complex gating function could be beneficial for very large training resources, the filtering approach is more suitable for the annotated metaphor datasets which are relatively small in size.",3.1 Word Representation Gating,[0],[0]
"As the next step, we implement position-specific mappings for the word embeddings.",3.2 Vector Space Mapping,[0],[0]
"The original method uses word embeddings that have been pretrained using the distributional skip-gram objective (Mikolov et al., 2013a).",3.2 Vector Space Mapping,[0],[0]
"While this tunes the vectors for predicting context words, there is no reason to believe that the same space is also optimal for the task of metaphor detection.",3.2 Vector Space Mapping,[0],[0]
"In order to address this shortcoming, we allow the model to learn a mapping from the skip-gram vector space to a new metaphor-specific vector space:
z1 = tanh(Wz1x1) (4)
z2 = tanh(Wz2 x̃2) (5)
where Wz1 and Wz2 are weight matrices, z1 and z2 are the new position-specific word representations.",3.2 Vector Space Mapping,[0],[0]
"While the original embeddings x1 and x2 are pre-trained on a large unannotated corpus, the transformation process is optimised using annotated metaphor examples, resulting in word representations that are more suitable for this task.",3.2 Vector Space Mapping,[0],[0]
"Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words.",3.2 Vector Space Mapping,[0],[0]
"In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order.",3.2 Vector Space Mapping,[0],[0]
"If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multiplication followed by a sum over all elements:
cos(x1, x2) ∝",3.3 Weighted Cosine,[0],[0]
"∑
i
x1,ix2,i (6)
",3.3 Weighted Cosine,[0],[0]
This calculation of cosine similarity can be formulated as a small neural network where the two unit-normalised input vectors are directly multiplied together.,3.3 Weighted Cosine,[0],[0]
"This is followed by a single output neuron, with all the intermediate weights set to value 1.",3.3 Weighted Cosine,[0],[0]
"Such a network would calculate the same sum over the element-wise multiplication, outputting the value of cosine similarity.
",3.3 Weighted Cosine,[0],[0]
"Since there is no reason to assume that all the embedding dimensions are equally important when detecting metaphors, we can explore other strategies for weighting the similarity calculation.
",3.3 Weighted Cosine,[0],[0]
Rei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations.,3.3 Weighted Cosine,[0],[0]
We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training.,3.3 Weighted Cosine,[0],[0]
"This is done by first creating a vector m, which is an element-wise multiplication of the two word representations:
mi = z1,iz2,i (7)
where mi is the i-th element of vector m and z1,i is the i-th element of vector z1.",3.3 Weighted Cosine,[0],[0]
"After that, the resulting vector is used as input for a hidden neural layer:
d = γ(Wdm) (8)
whereWd is a weight matrix and γ is an activation function.",3.3 Weighted Cosine,[0],[0]
"If the length of d is 1, all the weights in Wd have value 1, and γ is a linear activation, then this formula is equivalent to a regular cosine similarity.",3.3 Weighted Cosine,[0],[0]
"However, we use a larger length for d to capture more features, use tanh as the activation function, and optimise the weights of Wd during training, giving the framework more flexibility to customise the model for the task of metaphor detection.",3.3 Weighted Cosine,[0],[0]
"Based on vector d we can output a prediction for the word pair, showing whether it is literal or metaphorical:
y = σ(Wyd) (9)
where Wy is a weight matrix, σ is the logistic activation function, and y is a real-valued prediction with values between 0 and 1.
",3.4 Prediction and Optimisation,[0],[0]
"We optimise the model based on an annotated training dataset, while minimising the following hinge loss function:
E = ∑
k
qk (10)
qk = { (ỹ − y)2 if |ỹ − y| > 0.4 0, otherwise
(11)
where y is the predicted value, ỹ is the true label, and k iterates over all training examples.",3.4 Prediction and Optimisation,[0],[0]
Equation 11 optimises the model to minimise the squared error between the predicted and true labels.,3.4 Prediction and Optimisation,[0],[0]
"However, this is only done for training examples where the predicted error is not already close enough to the desired result.",3.4 Prediction and Optimisation,[0],[0]
The condition |ỹ,3.4 Prediction and Optimisation,[0],[0]
− y| > 0.4 only updates training examples where the difference from the true label is greater than 0.4.,3.4 Prediction and Optimisation,[0],[0]
The true labels ỹ can only take values 0,3.4 Prediction and Optimisation,[0],[0]
"(literal) or 1 (metaphorical), and the threshold 0.4 is chosen so that datapoints that are on the correct side of the decision boundary by more than 0.1 would be ignored, which helps reduce overfitting and allows the model to focus on the misclassified examples.
",3.4 Prediction and Optimisation,[0],[0]
The diagram of the complete network can be seen in Figure 1.,3.4 Prediction and Optimisation,[0],[0]
"Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.
",4 Word Representations,[0],[0]
"The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.
",4 Word Representations,[0],[0]
"We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015).",4 Word Representations,[0],[0]
These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space.,4 Word Representations,[0],[0]
"We evaluate our method using two datasets of phrases manually annotated for metaphoricity.
",5 Datasets,[0],[0]
"Since these datasets include examples for different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the extent to which our model is able to discriminate between different word senses, as opposed to merely selecting the most frequent class for a given word.
",5 Datasets,[0],[0]
"Mohammad et al. dataset (MOH) Mohammad et al. (2016) used WordNet to find verbs that had between three and ten senses and extracted the sentences exemplifying them in the corresponding glosses, yielding a total of 1639 verb uses in sentences.",5 Datasets,[0],[0]
Each of these was annotated for metaphoricity by 10 annotators via the crowdsourcing platform CrowdFlower1.,5 Datasets,[0],[0]
Mohammad et al. selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset.,5 Datasets,[0],[0]
"We extracted verb–direct object and verb–subject relations of the annotated verbs from this dataset, discarding the instances with pronominal or clausal subject or object.",5 Datasets,[0],[0]
This resulted in a dataset of 647 verb–noun pairs (316 metaphorical and 331 literal).,5 Datasets,[0],[0]
"Some examples of annotated verb phrases from MOH are presented in Table 1.
",5 Datasets,[0],[0]
Tsvetkov et al. dataset (TSV) Tsvetkov et al. (2014) construct a dataset of adjective–noun pairs annotated for metaphoricity.,5 Datasets,[0],[0]
This is divided into a training set consisting of 884 literal and 884 metaphorical pairs (TSV-TRAIN) and a test set containing 100 literal and 100 metaphorical pairs (TSV-TEST).,5 Datasets,[0],[0]
Table 2 shows a portion of annotated adjective-noun phrases from TSV-TEST.,5 Datasets,[0],[0]
"TSV-TRAIN was collected from publicly available metaphor collections on the web and manually
1www.crowdflower.com
curated by removing duplicates and metaphorical phrases that depend on wider context for their interpretation (e.g. drowning students).",5 Datasets,[0],[0]
TSVTEST was constructed by extracting nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus2 using SketchEngine.,5 Datasets,[0],[0]
The selected adjective-noun pairs were annotated for metaphoricity by 5 annotators with an interannotator agreement of κ = 0.76.,5 Datasets,[0],[0]
"Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",5 Datasets,[0],[0]
We randomly separated 200 (out of the 1536) examples from the training set to use for development experiments.,5 Datasets,[0],[0]
"The word representations in our model were initialised with either the 100-dimensional skip-gram embeddings or the 2,526-dimensional attribute vectors (Section 4).",6 Experiments and Results,[0],[0]
"These were kept fixed and not updated, which reduces overfitting on the available training examples.",6 Experiments and Results,[0],[0]
"For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and shows that the improvements are coming from the novel architecture and are not due to a different embedding initialisation.
",6 Experiments and Results,[0],[0]
"The network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning rates.",6 Experiments and Results,[0],[0]
The models were evaluated after each full pass over the training data and training was stopped if the F-score on the development set had not improved for 5 epochs.,6 Experiments and Results,[0],[0]
"The transformed embeddings z1 and z2 were set to size 300, layer d was set to size 50.",6 Experiments and Results,[0],[0]
The values for these hyperparameters were chosen experimentally using the development dataset.,6 Experiments and Results,[0],[0]
"In order to avoid drawing conclusions based on outlier results due to random initialisations, we ran each experiment 25 times with random seeds and present the averaged results in this paper.",6 Experiments and Results,[0],[0]
"We implemented the framework using Theano (Al-Rfou et al., 2016) and are making the source code publicly available.3
Table 3 contains results of different system configurations on the TSV dataset.",6 Experiments and Results,[0],[0]
"The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as
2https://www.sketchengine.co.uk/ententen-corpus/ 3http://www.marekrei.com/projects/ssn
the lexical abstractness, imageability scores and the relative number of supersenses for each word in the dataset.",6 Experiments and Results,[0],[0]
"Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings.",6 Experiments and Results,[0],[0]
They also proposed combining the linguistic model with a system using visual word representations and achieved performance improvements.,6 Experiments and Results,[0],[0]
"Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations can outperform regular skip-gram embeddings.
",6 Experiments and Results,[0],[0]
"As an additional baseline, we report the performance on metaphor detection using a basic feedforward network (FFN).",6 Experiments and Results,[0],[0]
"In this configuration, the word embeddings x1 and x2 are directly connected to the hidden layer d, skipping all the intermediate network structure.",6 Experiments and Results,[0],[0]
"The FFN achieves 74.4% F-score on TSV-TEST, showing that even such a simple model can perform relatively well in a supervised setting.",6 Experiments and Results,[0],[0]
"Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).
",6 Experiments and Results,[0],[0]
"The architecture described in Section 3, which we refer to as a supervised similarity network (SSN), outperforms the baseline and achieves 80.1% F-score using skip-gram embeddings and 80.6% with attribute-based representations.",6 Experiments and Results,[0],[0]
We also created a fusion of these two models where the predictions from both are combined as a weighted average.,6 Experiments and Results,[0],[0]
"In this setting, the two networks are trained in tandem and a real-valued weight, which is also optimised during training, is
used to combine them together.",6 Experiments and Results,[0],[0]
"This configuration achieves 81.1% F-score, indicating that the the skip-gram embeddings and attribute vectors capture somewhat complementary information.",6 Experiments and Results,[0],[0]
"Excluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any visual information.",6 Experiments and Results,[0],[0]
"The attribute-based SSN also improves over Bulat et al. (2017) by 5.6% absolute, using the same word representations as input.
",6 Experiments and Results,[0],[0]
Table 4 contains results of different system architectures on the MOH dataset.,6 Experiments and Results,[0],[0]
"Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing.",6 Experiments and Results,[0],[0]
"Since this corpus contains only 647 annotated examples, we instead evaluated the systems using 10-fold cross-validation.",6 Experiments and Results,[0],[0]
"The feedforward baseline with skip-gram embeddings returns an F-score that is close to the linguistic configuration of Shutova et al, whereas the best results are achieved by the similarity network with skip-gram embeddings.",6 Experiments and Results,[0],[0]
"In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.
",6 Experiments and Results,[0],[0]
"Table 5 contains examples from the TSV development set, together with gold annotations and predicted scores.",6 Experiments and Results,[0],[0]
"The system confidently detects literal phrases such as sunny country and meaningless discussion, along with metaphorical phrases such as unforgiving heights and blind hope.",6 Experiments and Results,[0],[0]
"The predicted output disagrees with the annotation on
cases such as humane treatment and rich programmer – some of these examples could also be argued as being metaphorical, depending on the specific sense of the words.",6 Experiments and Results,[0],[0]
"While the system was relatively unsure about the false positives (the scores were close to 0.5), it tended to assign more decisive scores to the false negatives.",6 Experiments and Results,[0],[0]
"Results in Section 6 show that performance on the TSV dataset is higher than the MOH dataset, likely due to the former having more examples available for training.",7 The Effects of Training Data,[0],[0]
"Therefore, we ran an additional experiment to investigate the effect of dataset size on the performance of metaphor detection.",7 The Effects of Training Data,[0],[0]
"Gutiérrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource.",7 The Effects of Training Data,[0],[0]
"While it contains only 23 unique adjectives, the total number of phrases reaches 8,592.",7 The Effects of Training Data,[0],[0]
"We remove any phrases that occur in the development or test data of TSV, then incrementally add the remaining examples to the TSV training data and evaluate on the TSV-TEST.
",7 The Effects of Training Data,[0],[0]
"Figure 2 shows a graph of the system performance, when increasing the training data at intervals of 500.",7 The Effects of Training Data,[0],[0]
"There is a very rapid increase in performance until around 2,000 training points, whereas the existing TSV-TRAIN is limited to 1,336 examples.",7 The Effects of Training Data,[0],[0]
Providing even more data to the system gives an additional increase that is more gradual.,7 The Effects of Training Data,[0],[0]
"The final performance of the system us-
ing both datasets is 88.3 F-score, which is the highest result reported on the TSV dataset and translates to 36% relative error reduction with respect to the same system trained only on the original dataset.
",7 The Effects of Training Data,[0],[0]
We report the exact values in Table 6 for the different training sets.,7 The Effects of Training Data,[0],[0]
"The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset.",7 The Effects of Training Data,[0],[0]
"In order to include the data from Gutiérrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance.",7 The Effects of Training Data,[0],[0]
"The architecture in Section 3 also acts as a semantic composition model, extracting the meaning of the phrase by combining the meanings of its component words.",8 Qualitative analysis,[0],[0]
"Therefore, we performed a qualitative experiment to investigate: (1) how well do traditional compositional methods capture metaphors, without any fine-tuning; and (2) whether the supervised representations still retain their domain-specific semantic information.",8 Qualitative analysis,[0],[0]
"For this purpose, we construct three vector spaces and visualise some examples from the TSV training set,
using t-SNE (Van Der Maaten and Hinton, 2008).",8 Qualitative analysis,[0],[0]
"Figure 3 contains examples for three different composition methods: the additive method simply sums the skip-gram embeddings for both words (top); the multiplicative method multiplies the skip-gram embeddings (middle); the final system uses layer m from the SSN model to represent the
phrases (bottom).",8 Qualitative analysis,[0],[0]
"The visualisation shows that the additive and multiplicative models are both comparable when it comes to semantic clustering of the phrases, but metaphorical examples are mixed together with literal clusters.",8 Qualitative analysis,[0],[0]
The SSN is optimised for metaphor classification and therefore it produces representations with a very clear boundary for metaphoricity.,8 Qualitative analysis,[0],[0]
"Interestingly, the graph also reveals a misannotated example in the dataset, since ‘fiery temper’ should be labeled as a metaphor.",8 Qualitative analysis,[0],[0]
"At the same time, this space also retains the general semantic information, as similar phrases with the same label are still positioned close together.",8 Qualitative analysis,[0],[0]
"Future work could investigate models of multi-task training where metaphor detection is trained together with an unsupervised objective, allowing the system to take better advantage of unlabeled data while still learning to separate metaphors.",8 Qualitative analysis,[0],[0]
"In this paper, we introduced the first deep learning architecture designed to capture metaphorical composition and evaluated it on a metaphor identification task.
",9 Conclusion,[0],[0]
"Firstly, we demonstrated that the proposed framework outperforms both a metaphor-agnostic baseline (a feed-forward neural network) as well as previous corpus-driven approaches to metaphor identification.",9 Conclusion,[0],[0]
"The results showed that it is beneficial to construct a specialised network architecture for metaphor detection, which includes a gating function for capturing the interaction between the source and target domains, word embeddings mapped to a metaphor-specific space, and optimisation using a hinge loss function.
",9 Conclusion,[0],[0]
"Secondly, our qualitative analysis indicates that our supervised similarity network learns phrase representations with a very clear boundary for metaphoricity, in contrast to traditional compositional methods.
",9 Conclusion,[0],[0]
"Finally, we show that with a sufficiently large training set our model can also outperform the state-of-the art metaphor identification systems based on hand-coded lexical knowledge.",9 Conclusion,[0],[0]
Ekaterina Shutova’s research is supported by the Leverhulme Trust Early Career Fellowship.,Acknowledgments,[0],[0]
The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding.,abstractText,[0],[0]
"Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task.",abstractText,[0],[0]
"In this paper, we present the first deep learning architecture designed to capture metaphorical composition.",abstractText,[0],[0]
Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.,abstractText,[0],[0]
Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4778",text,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) has now achieved impressive performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention.",1 Introduction,[0],[0]
"NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word.
",1 Introduction,[0],[0]
"Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser,
*Corresponding Author
1989), which forces the model to generate translation strictly matching the ground-truth at the word level.",1 Introduction,[0],[0]
"However, in practice it is impossible to generate translation totally the same as ground truth.",1 Introduction,[0],[0]
"Once different target words are generated, the word-level loss cannot evaluate the translation properly, usually under-estimating the translation.",1 Introduction,[0],[0]
"In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.",1 Introduction,[0],[0]
"Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words.",1 Introduction,[0],[0]
"Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training.",1 Introduction,[0],[0]
"However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss.
",1 Introduction,[0],[0]
"In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss.",1 Introduction,[0],[0]
"However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update.",1 Introduction,[0],[0]
"Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable
training and limited improvements.",1 Introduction,[0],[0]
"Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update.",1 Introduction,[0],[0]
"However, these methods do not allow for direct optimization with respect to evaluation metrics.
",1 Introduction,[0],[0]
"In this paper, we propose a method to combine the strengths of the word-level and sequencelevel training, that is the direct gradient update without gradient estimation from word-level training and the greater flexibility from sequence-level training.",1 Introduction,[0],[0]
"Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable.",1 Introduction,[0],[0]
"During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words.",1 Introduction,[0],[0]
Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework.,1 Introduction,[0],[0]
The experiments also indicate that greedy search strategy indeed has superiority over teacher forcing.,1 Introduction,[0],[0]
"NMT is based on an end-to-end framework which directly models the translation probability from the source sentence x to the target sentence ŷ:
",2 Background,[0],[0]
"P (ŷ|x) = T∏ j=1 p(ŷj |ŷ<j ,x, θ), (1)
where T is the target length and θ is the model parameters.",2 Background,[0],[0]
"Given the training set D = {XM,YM} withM sentences pairs, the training objective is to maximize the log-likelihood of the training data as
θ = argmax θ {L(θ)}
L(θ) = M∑ m=1 lm∑ j=1 log(p(ŷmj |ŷm<j ,xm, θ)), (2)
where the superior m indicates the m-th sentence in the dataset and lm is the length of m-th target sentence.
",2 Background,[0],[0]
"In the above model, the probability of each target word p(ŷmj |ŷm<j ,xm, θ) is conditioned on the previous target words.",2 Background,[0],[0]
"The scenario is that in the
training time, the teacher forcing algorithm is employed and the ground truth words from the target sentence are fed as context, while during inference, the ground truth words are not available and the previous predicted words are instead fed as context.",2 Background,[0],[0]
This discrepancy is called exposure bias.,2 Background,[0],[0]
"Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching.",3.1 Sequence-Level Objectives,[0],[0]
Assuming that y and ŷ are the output sentence and the ground truth sentence with length T and T ′,3.1 Sequence-Level Objectives,[0],[0]
"respectively, the count of an n-gram g = (g1, . . .",3.1 Sequence-Level Objectives,[0],[0]
", gn) in sentence y is calculated as
Cy(g) =",3.1 Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.1 Sequence-Level Objectives,[0],[0]
"1{gi = yt+i}, (3)
where 1{·} is the indicator function.",3.1 Sequence-Level Objectives,[0],[0]
"The matching count of the n-gram g between ŷ and y is given by
Cŷy(g) = min (Cy(g),Cŷ(g)).",3.1 Sequence-Level Objectives,[0],[0]
"(4)
Then the precision pn and the recall rn of the predicted n-grams are calculated as follows
pn =
∑ g∈y C
ŷ",3.1 Sequence-Level Objectives,[0],[0]
"y(g)∑
g∈y Cy(g) , (5)
",3.1 Sequence-Level Objectives,[0],[0]
"rn =
∑ g∈y C
ŷ y(g)∑
g∈ŷ Cŷ(g) .",3.1 Sequence-Level Objectives,[0],[0]
"(6)
BLEU, the most widely used metric for machine translation evaluation, is defined based on the n-gram precision as follows
BLEU = BP · exp( N∑ n=1",3.1 Sequence-Level Objectives,[0],[0]
"wn log pn), (7)
where BP stands for the brevity penalty",3.1 Sequence-Level Objectives,[0],[0]
and wn is the weight for the n-gram.,3.1 Sequence-Level Objectives,[0],[0]
"In contrast, GLEU is the minimum of recall and precision of 1-4 grams where 1-4 grams are counted together:
GLEU =",3.1 Sequence-Level Objectives,[0],[0]
"min(p1-4, r1-4).",3.1 Sequence-Level Objectives,[0],[0]
(8),3.1 Sequence-Level Objectives,[0],[0]
"In the output sentence y, the prediction probability varies among words.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Some words are translated by the model with high confidence while some words are translated with high uncertainty.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"However, when calculating the count of n-grams in Eq.(3), all the words in the output sentence are treated equally, regardless of their respective prediction probabilities.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(yj |y<j ,x, θ) as the count of word yj , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Then the probabilistic count of g = (g1, . . .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
", gn) is calculated by summing over the output sentence y as
C̃y(g) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"1{gi = yt+i} · p(yt+i|y<t+i,x, θ).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(9)
Now the probabilistic sequence-level objective can be got by replacing Cy(g) with C̃y(g) (the tilde over the head indicates the probabilistic version) and keeping the rest unchanged.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Here, we take BLEU as an example and show how the probabilistic BLEU (denoted as P-BLEU) is defined.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"From this purpose, the matching count of n-gram g in Eq.(4) is modified as follows
C̃ ŷ
y(g) = min(C̃y(g),Cŷ(g)).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(10)
and the predict precision of n-grams changes into
p̃n =
∑ g∈y C̃ ŷ
y(g)∑ g∈y C̃y(g) .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(11)
Finally, the probabilistic BLEU (P-BLEU) is defined as
P-BLEU = BP · exp( N∑ n=1 wn log p̃n), (12)
Probabilistic GLEU (P-GLEU) can be defined in a similar way.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Specifically, we denote the probabilistic precision of n-grams as P-Pn.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"The probabilistic precision is more reasonable than recall since the denominator in Eq.(11) plays a normalization role, so we modify the definition in Eq.(8) and define P-GLEU as simply the probabilistic precision of 1-4 grams.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"The general probabilistic loss function is:
L(θ) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
− M∑ m=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"P(ym, ŷm), (13)
where P represents the probabilistic sequencelevel objectives, and ym and ŷm are the predicted translation and the ground truth for the m-th sentence respectively.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
The calculation of the probabilistic objective is illustrated in Figure 1.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
This probabilistic loss can work with decoding strategies such as greedy search and teacher forcing.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
We carry out experiments on Chinese-to-English translation.1 The training data consists of 1.25M pairs of sentences extracted from LDC corpora2.,4.1 Settings,[0],[0]
Sentence pairs with either side longer than 50 were dropped.,4.1 Settings,[0],[0]
We use NIST 2002 (MT 02) as the validation set and NIST 2003-2006 (MT 03-08) as the test sets.,4.1 Settings,[0],[0]
"We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.
",4.1 Settings,[0],[0]
"We apply our method to an attention-based NMT system (Bahdanau et al., 2014) implemented by Pytorch.",4.1 Settings,[0],[0]
"Both source and target vocabularies are limited to 30K. All word embedding sizes are set to 512, and the sizes of hidden units in both encoder and decoder RNNs are also set to 512.",4.1 Settings,[0],[0]
"All parameters are initialized by uniform distribution over [−0.1, 0.1].",4.1 Settings,[0],[0]
The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model with batch size of 40.,4.1 Settings,[0],[0]
"In addition, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ = 0.95 and = 1e-6.",4.1 Settings,[0],[0]
Dropout is applied on the output layer with dropout rate of 0.5.,4.1 Settings,[0],[0]
The beam size is set to 10.,4.1 Settings,[0],[0]
"Systems We first pretrain the baseline model by maximum likelihood estimation (MLE) and then refine the model using probabilistic sequencelevel objectives, including P-BLEU, P-GLEU and P-P2 (probabilistic 2-gram precision).",4.2 Performance,[0],[0]
"In addition, we reproduce previous works which train the NMT model through minimum risk training (MRT) (Shen et al., 2016) and REINFORCE algo-
1Experiment code: https://github.com/ictnlp/GS4NMT 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
",4.2 Performance,[0],[0]
"rithm (RF) (Ranzato et al., 2015).",4.2 Performance,[0],[0]
"When reproducing their works, we set BLEU, GLEU and 2-gram precision as training objectives respectively and find out that GLEU yields the best performance.",4.2 Performance,[0],[0]
"In the following, we only report the results with training objective GLEU.",4.2 Performance,[0],[0]
Performance Table 1 shows the translation performance on test sets measured in BLEU score.,4.2 Performance,[0],[0]
"Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms.",4.2 Performance,[0],[0]
"We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2.",4.2 Performance,[0],[0]
"Notice that our method only changes the loss function, without any modification on model structure and training data.",4.2 Performance,[0],[0]
We use the probabilistic loss to finetune the baseline model rather than training from scratch.,4.3 Why Pretraining,[0],[0]
This is in line with our motivation: to alleviate the exposure bias and make the model exposed to its own output during training.,4.3 Why Pretraining,[0],[0]
"In the very beginning of the training, the model’s translation capability is nearly zero and the generated sentences are often meaningless and do not contain useful information for the training, so it is unreasonable to directly apply the greedy search strategy.",4.3 Why Pretraining,[0],[0]
"Therefore, we first apply the teacher forcing algorithm to pretrain the model, and then we let the model generate the sentences itself and learn from its own outputs.
",4.3 Why Pretraining,[0],[0]
Another reason favoring pretraining is that pretraining can lower the training cost.,4.3 Why Pretraining,[0],[0]
The training cost of the introduced probabilistic loss is about three times higher than the cost of cross entropy.,4.3 Why Pretraining,[0],[0]
"Without pretraining, the training time will be much higher than usual.",4.3 Why Pretraining,[0],[0]
"Otherwise, the training cost is acceptable if the probabilistic loss is only for finetuning.",4.3 Why Pretraining,[0],[0]
"The probabilistic loss, defined in Eq.(13), is computed from the model output y and reference ŷ.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we apply two different decoding strategies to generate y: 1.",4.4 Effect of Decoding Strategy,[0],[0]
"teacher forcing, which uses the ground truth as decoder input.",4.4 Effect of Decoding Strategy,[0],[0]
"2. greedy search, which feeds the word with maximum probability.",4.4 Effect of Decoding Strategy,[0],[0]
"By conducting this experiment, we attempt to figure out where the improvements come from: the modification of loss or the mitigation of exposure bias?
",4.4 Effect of Decoding Strategy,[0],[0]
Figure 2 shows the learning curves of the two decoding strategies with training objective P-P2.,4.4 Effect of Decoding Strategy,[0],[0]
Teacher forcing raises about 0.5 BLEU improvements and greedy search outperform the teacher forcing algorithm by nearly 1 BLEU point.,4.4 Effect of Decoding Strategy,[0],[0]
"We conclude that the probabilistic loss has its own advantage even when trained by the teacher forcing algorithm, and greedy search is effective in alleviating the exposure bias.
",4.4 Effect of Decoding Strategy,[0],[0]
Notice that the greedy search strategy highly relys on the probabilistic loss and can not be conducted independently.,4.4 Effect of Decoding Strategy,[0],[0]
Greedy search together with the word-level loss is very similar with the scheduled sampling(SS).,4.4 Effect of Decoding Strategy,[0],[0]
"However, SS is inconsistent with the word-level loss since the word-level loss requires strict alignment between hypothesis and reference, which can only be accomplished by the teacher forcing algorithm.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we explore how the probabilistic objective correlates with the real evaluation metric.",4.5 Correlation with Evaluation Metrics,[0],[0]
"We randomly sample 100 pairs of sentences
from the training set and compute their P-GLEU and GLEU scores (Wu et al. (2016) indicates that GLEU have better performance in the sentencelevel evaluation than BLEU).
",4.5 Correlation with Evaluation Metrics,[0],[0]
"Directly computing the correlation between GLEU and P-GLEU gives the correlation coefficient 0.86, which indicates strong correlation.",4.5 Correlation with Evaluation Metrics,[0],[0]
"In addition, we draw the scatter diagram of the 100 pairs of sentences in Figure 3 with GLEU as x-axis and P-GLEU as y-axix.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Figure 3 shows that PGLEU correlates well with GLEU, suggesting that it is reasonable to directly train the NMT model with P-GLEU.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Word-level loss cannot evaluate the translation properly and suffers from the exposure bias, and sequence-level objectives are usually indifferentiable and require gradient estimation.",5 Conclusion,[0],[0]
"We propose probabilistic sequence-level objectives based on ngram matching, which relieve the dependence on gradient estimation and can directly train the NMT model.",5 Conclusion,[0],[0]
Experiment results show that our method significantly outperforms previous sequence-level training works and successfully alleviates the exposure bias through performing greedy search.,5 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,6 Acknowledgments,[0],[0]
This work was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61472428 and the project NO. 61662077.,6 Acknowledgments,[0],[0]
"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias.",abstractText,[0],[0]
"Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation.",abstractText,[0],[0]
"On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework.",abstractText,[0],[0]
"In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias.",abstractText,[0],[0]
Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.,abstractText,[0],[0]
Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881–1890 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1881",text,[0],[0]
"Many key linguistic tasks, within and across languages or domains, including machine translation, rely on learning cross-lingual correspondences between words or other semantic units.",1 Introduction,[0],[0]
"While the associated alignment problem could be solved with access to large amounts of parallel data, broader applicability relies on the ability to do so with largely mono-lingual data, from Part-of-Speech (POS) tagging (Zhang et al., 2016), dependency parsing (Guo et al., 2015), to machine translation (Lample et al., 2018).",1 Introduction,[0],[0]
"The key subtask of bilingual lexical induction, for example, while long standing as a problem (Fung, 1995; Rapp, 1995, 1999), has been actively pursued recently (Artetxe et al., 2016; Zhang et al., 2017a; Conneau et al., 2018).
",1 Introduction,[0],[0]
"Current methods for learning cross-domain correspondences at the word level rely on distributed representations of words, building on the observation that mono-lingual word embeddings exhibit
similar geometric properties across languages Mikolov et al. (2013).",1 Introduction,[0],[0]
"While most early work assumed some, albeit minimal, amount of parallel data (Mikolov et al., 2013; Dinu et al., 2014; Zhang et al., 2016), recently fully-unsupervised methods have been shown to perform on par with their supervised counterparts (Conneau et al., 2018; Artetxe et al., 2018).",1 Introduction,[0],[0]
"While successful, the mappings arise from multiple steps of processing, requiring either careful initial guesses or postmapping refinements, including mitigating the effect of frequent words on neighborhoods.",1 Introduction,[0],[0]
"The associated adversarial training schemes can also be challenging to tune properly (Artetxe et al., 2018).
",1 Introduction,[0],[0]
"In this paper, we propose a direct optimization approach to solving correspondences based on recent generalizations of optimal transport (OT).",1 Introduction,[0],[0]
"OT is a general mathematical toolbox used to evaluate correspondence-based distances and establish mappings between probability distributions, including discrete distributions such as point-sets.",1 Introduction,[0],[0]
"However, the nature of mono-lingual word embeddings renders the classic formulation of OT inapplicable to our setting.",1 Introduction,[0],[0]
"Indeed, word embeddings are estimated primarily in a relational manner to the extent that the algorithms are naturally interpreted as metric recovery methods (Hashimoto et al., 2016).",1 Introduction,[0],[0]
"In such settings, previous work has sought to bypass this lack of registration by jointly optimizing over a matching and an orthogonal mapping (Rangarajan et al., 1997; Zhang et al., 2017b).",1 Introduction,[0],[0]
"Due to the focus on distances rather than points, we instead adopt a relational OT formulation based on the Gromov-Wasserstein distance that measures how distances between pairs of words are mapped across languages.",1 Introduction,[0],[0]
"We show that the resulting mapping admits an efficient solution and requires little or no tuning.
",1 Introduction,[0],[0]
"In summary, we make the following contributions:
• We propose the use of the GromovWasserstein distance to learn correspondences between word embedding spaces in a fully-unsupervised manner, leading to a theoretically-motivated optimization problem that can be solved efficiently, robustly, in a single step, and requires no post-processing or heuristic adjustments.
",1 Introduction,[0],[0]
"• To scale up to large vocabularies we realize an extended mapping to words not part of the original optimization problem.
",1 Introduction,[0],[0]
"• We show that the proposed approach performs on par with state-of-the-art neural network based methods on benchmark word translation tasks, while requiring a fraction of the computational cost and/or hyperparameter tuning.",1 Introduction,[0],[0]
"In the unsupervised bilingual lexical induction problem we consider two languages with vocabularies Vx and Vy, represented by word embeddings X = {x(i)}ni=1 and Y = {y(j)}mj=1, respectively, where x(i) ∈ X ⊂",2 Problem Formulation,[0],[0]
Rdx corresponds to wxi ∈,2 Problem Formulation,[0],[0]
Vx and y(j) ∈,2 Problem Formulation,[0],[0]
Y ⊂,2 Problem Formulation,[0],[0]
Rdy to wyj ∈ Vy.,2 Problem Formulation,[0],[0]
"For simplicity, we let m = n and dx = dy, although our methods carry over to the general case with little or no modifications.",2 Problem Formulation,[0],[0]
"Our goal is to learn an alignment between these two sets of words without any parallel data, i.e., we learn to relate x(i) ↔ y(j) with the implication that wxi translates to w y j .
",2 Problem Formulation,[0],[0]
"As background, we begin by discussing the problem of learning an explicit map between embeddings in the supervised scenario.",2 Problem Formulation,[0],[0]
The associated training procedure will later be used for extending unsupervised alignments (Section 3.2).,2 Problem Formulation,[0],[0]
"In the supervised setting, we learn a map T : X → Y such that T (x(i))",2.1 Supervised Maps: Procrustes,[0],[0]
≈ y(j) whenever wyj is a translation of wxi .,2.1 Supervised Maps: Procrustes,[0],[0]
"Let X and Y be the matrices whose columns are vectors x(i) and y(j), respectively.",2.1 Supervised Maps: Procrustes,[0],[0]
"Then we can find T by solving
min T∈F ‖X− T (Y)‖2F (1)
",2.1 Supervised Maps: Procrustes,[0],[0]
"where ‖ · ‖F is the Frobenius norm ‖A‖F =√∑ i,j |aij |2.",2.1 Supervised Maps: Procrustes,[0],[0]
"Naturally, both the difficulty of finding T and the quality of the resulting alignment depend on the choice of space F .",2.1 Supervised Maps: Procrustes,[0],[0]
"A classic
approach constrains T to be orthonormal matrices, i.e., rotations and reflections, resulting in the orthogonal Procrustes problem
min P∈O(n)
‖X−PY‖2F (2)
where O(n) = {P ∈ Rn×n | P>P = I}.",2.1 Supervised Maps: Procrustes,[0],[0]
"One key advantage of this formulation is that it has a closed-form solution in terms of a singular value decomposition (SVD), whereas for most other choices of constraint set F it does not.",2.1 Supervised Maps: Procrustes,[0],[0]
"Given an SVD decomposition UΣV> of XY>, the solution to problem (2) is P∗",2.1 Supervised Maps: Procrustes,[0],[0]
"= UV> (Schönemann, 1966).",2.1 Supervised Maps: Procrustes,[0],[0]
"Besides obvious computational advantage, constraining the mapping between spaces to be orthonormal is justified in the context of word embedding alignment because orthogonal maps preserve angles (and thus distances), which is often the only information used by downstream tasks (e.g., for nearest neighbor search) that rely on word embeddings.",2.1 Supervised Maps: Procrustes,[0],[0]
"(Smith et al., 2017) further show that orthogonality is required for self-consistency of linear transformations between vector spaces.
",2.1 Supervised Maps: Procrustes,[0],[0]
"Clearly, the Procrustes approach only solves the supervised version of the problem as it requires a known correspondence between the columns of X and Y. Steps beyond this constraint include using small amounts of parallel data (Zhang et al., 2016) or an unsupervised technique as the initial step to generate pseudo-parallel data (Conneau et al., 2018) before solving for P.",2.1 Supervised Maps: Procrustes,[0],[0]
"Optimal transport formalizes the problem of finding a minimum cost mapping between two point sets, viewed as discrete distributions.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Specifically, we assume two empirical distributions over embeddings, e.g.,
µ = n∑
i=1 piδx(i) , ν = m∑ j=1 qjδy(i) (3)
where p and q are vectors of probability weights associated with each point set.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In our case, we usually consider uniform weights, e.g., pi = 1/n and qj = 1/m, although if additional information were provided (such as in the form of word frequencies), those could be naturally incorporated via p and q (see discussion at the end of Section 3).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"We find a transportation map T realizing
inf T {∫ X c(x, T (x))dµ(x)",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"| T#µ = ν } , (4)
where the cost c(x, T (x)) is typically just ‖x − T (x)‖ and T#µ = ν implies that the source points must exactly map to the targets.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"However, such a map need not exist in general and we instead follow a relaxed Kantorovich’s formulation.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In this case, the set of transportation plans is a polytope:
Π(p,q) =",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"{Γ ∈ Rn×m+ | Γ1n = p, Γ>1n = q}.
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The cost function is given as a matrix C ∈ Rn×m, e.g., Cij = ‖x(i)",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
− y(j)‖.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The total cost incurred by Γ is 〈Γ, C〉 := ∑ ij ΓijCij .",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Thus, the discrete optimal transport (DOT) problem consists of finding a plan Γ that solves
min Γ∈Π(p,q)
〈Γ,C〉.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(5)
Problem (5) is a linear program, and thus can be solved exactly in O(n3 log n) with interior point methods.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"However, regularizing the objective leads to more efficient optimization and often better empirical results.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The most common such regularization, popularized by Cuturi (2013), involves adding an entropy penalization:
min Γ∈Π(p,q)
〈Γ,C〉 − λH(Γ).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(6)
The solution of this strictly convex optimization problem has the form Γ∗ = diag (a)Kdiag (b), with K = e− C λ (element-wise), and can be obtained efficiently via the Sinkhorn-Knopp algorithm, a matrix-scaling procedure which iteratively computes:
a← p Kb and b← q",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"K>a, (7)
where denotes entry-wise division.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The derivation of these updates is immediate from the form of Γ∗ above, combined with the marginal constraints Γ1n = p, Γ>1n",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
= q,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(Peyré and Cuturi, 2018).
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Although simple, efficient and theoreticallymotivated, a direct application of discrete OT for unsupervised word translation is not appropriate.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"One reason is that the mono-lingual embeddings are estimated in a relative manner, leaving, e.g., an overall rotation unspecified.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
Such degrees of freedom can dramatically change the entries of the cost matrix Cij = ‖x(i) − y(j)‖ and the resulting transport map.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"One possible solution is to simultaneously learn an optimal coupling and an orthogonal transformation (Zhang et al., 2017b).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The transport problem is then solved iteratively, using
Cij = ‖x(i) − Py(j)‖, where P is in turn chosen to minimize the transport cost (via Procrustes).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"While promising, the resulting iterative approach is sensitive to initialization, perhaps explaining why Zhang et al. (2017b) used an adversarially learned mapping as the initial step.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The computational cost can also be prohibitive (Artetxe et al., 2018) though could be remedied with additional development.
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"We adopt a theoretically well-founded generalization of optimal transport for pairs of points (their distances), thus in line with how the embeddings are estimated in the first place.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
We explain the approach in detail in the next Section.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In this section we introduce the GromovWasserstein distance, describe an optimization algorithm for it, and discuss how to extend the approach to out-of-sample vectors.",3 Transporting across unaligned spaces,[0],[0]
The classic optimal transport requires a distance between vectors across the two domains.,3.1 The Gromov Wasserstein Distance,[0],[0]
"Such a metric may not be available, for example, when the sample sets to be matched do not belong to the same metric space (e.g., different dimension).",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein distance (Mémoli, 2011) generalizes optimal transport by comparing the metric spaces directly instead of samples across the spaces.",3.1 The Gromov Wasserstein Distance,[0],[0]
"In other words, this framework operates on distances between pairs of points calculated within each domain and measures how these distances compare to those in the other domain.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Thus, it requires a weaker but easy to define notion of distance between distances, and operates on pairs of points, turning the problem from a linear to a quadratic one.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Formally, in its discrete version, this framework considers two measure spaces expressed in terms of within-domain similarity matrices (C,p) and (C′,q) and a loss function defined between similarity pairs: L : R × R → R, where L(Cik, C ′jl) measures the discrepancy between the distances d(x(i),x(k)) and d′(y(j),y(l)).",3.1 The Gromov Wasserstein Distance,[0],[0]
"Typical choices for L are L(a, b) = 12(a",3.1 The Gromov Wasserstein Distance,[0],[0]
"− b)
2 or L(a, b) = KL(a|b).",3.1 The Gromov Wasserstein Distance,[0],[0]
"In this framework, L(Cik, C ′jl) can also be understood as the cost of “matching” i to j and k to l.
All the relevant values of L(·, ·) can be put in a 4-th order tensor L ∈ RN1×N1×N2×N2 , where Lijkl = L(Cik, C ′ jl).",3.1 The Gromov Wasserstein Distance,[0],[0]
"As before, we seek a cou-
pling Γ specifying how much mass to transfer between each pair of points from the two spaces.",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein problem is then defined as solving
GW(C,C′,p,q) = min Γ∈Π(p,q) ∑",3.1 The Gromov Wasserstein Distance,[0],[0]
"i,j,k,l LijklΓijΓkl (8)
Compared to problem (5), this version is substantially harder since the objective is now not only non-linear, but non-convex too.1 In addition, it requires operating on a fourth-order tensor, which would be prohibitive in most settings.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Surprisingly, this problem can be optimized efficiently with first-order methods, whereby each iteration involves solving a traditional optimal transport problem (Peyré et al., 2016).",3.1 The Gromov Wasserstein Distance,[0],[0]
"Furthermore, for suitable choices of loss function L, Peyré et al. (2016) show that instead of the O(N21N 2 2 ) complexity implied by naive fourthorder tensor product, this computation reduces to O(N21N2 + N1N 2 2 ) cost.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Their approach consists of solving (5) by projected gradient descent, which yields iterations that involve projecting onto Π(p,q) a pseudo-cost matrix of the form
ĈΓ(C,C ′,Γ) =",3.1 The Gromov Wasserstein Distance,[0],[0]
Cxy,3.1 The Gromov Wasserstein Distance,[0],[0]
− h1(C)Γh2(C′)>,3.1 The Gromov Wasserstein Distance,[0],[0]
"(9)
where
Cxy = f1(C)p1 > m + 1nq >f2(C ′)>
and f1, f2, h2, h2 are functions that depend on the loss L. We provide an explicit algorithm for the case L = L2 at the end of this section.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"1In fact, the discrete (Monge-type) formulation of the problem is essentially an instance of the well-known (and NP-hard) quadratic assignment problem (QAP).
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Once we have solved (8), the optimal transport coupling Γ∗ provides an explicit (soft) matching between source and target samples, which for the problem of interest can be interpreted as a probabilistic translation: for every pair of words (w (i) src, w (j) trg), Γ ∗ ij provides a likelihood that these two words are translations of each other.",3.1 The Gromov Wasserstein Distance,[0],[0]
"This itself is enough to translate, and we show in the experiments section that Γ∗ by itself, without any further post-processing, provides highquality translations.",3.1 The Gromov Wasserstein Distance,[0],[0]
"This stands in sharp contrast to mapping-based methods, which rely on nearest-neighbor computation to infer translations, and thus become prone to hub-word effects which have to be mitigated with heuristic postprocessing techniques such as Inverted Softmax (Smith et al., 2017) and Cross-Domain Similarity Scaling (CSLS) (Conneau et al., 2018).",3.1 The Gromov Wasserstein Distance,[0],[0]
"The transportation coupling Γ, being normalized by construction, requires no such artifacts.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein problem (8) possesses various desirable theoretical properties, including the fact that for a suitable choice of the loss function it is indeed a distance:
Theorem 3.1 (Mémoli 2011).",3.1 The Gromov Wasserstein Distance,[0],[0]
"With the choice L = L2, GW 1 2 is a distance on the space of metric measure spaces.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Solving problem (8) therefore yields a fascinating accompanying notion: the GromovWasserstein distance between languages, a measure of semantic discrepancy purely based on the relational characterization of their word embeddings.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Owing to Theorem 3.1, such values can be
interpreted as distances, so that, e.g., the triangle inequality holds among them.",3.1 The Gromov Wasserstein Distance,[0],[0]
"In Section 4.4 we compare various languages in terms of their GWdistance.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Finally, we note that whenever word frequency counts are available, those would be used for p and q.",3.1 The Gromov Wasserstein Distance,[0],[0]
"If they are not, but words are sorted according to occurrence (as they often are in popular off-the-shelf embedding formats), one can estimate rank-probabilities such as Zipf power laws, which are known to accurately model multiple languages (Piantadosi, 2014).",3.1 The Gromov Wasserstein Distance,[0],[0]
"In order to provide a fair comparison to previous work, throughout our experiments we use uniform distributions so as to avoid providing our method with additional information not available to others.",3.1 The Gromov Wasserstein Distance,[0],[0]
"While the pure Gromov-Wasserstein approach leads to high quality solutions, it is best suited to small-to-moderate vocabulary sizes,2 since its optimization becomes prohibitive for very large problems.",3.2 Scaling Up,[0],[0]
"For such settings, we propose a twostep approach in which we first match a subset of the vocabulary via the optimal coupling, after which we learn an orthogonal mapping through a modified Procrustes problem.",3.2 Scaling Up,[0],[0]
"Formally, suppose we solve problem (8) for a reduced matrices X1:k and Yi:k consisting of the first columns k of X and Y, respectively, and let Γ∗ be the optimal coupling.",3.2 Scaling Up,[0],[0]
"We seek an orthogonal matrix that best recovers the barycentric mapping implied by Γ∗. Namely, we seek to find P which solves:
min P∈O(n)
‖XΓ∗ −PY‖22 (10)
Just as problem (2), it is easy to show that this Procrustes-type problem has a closed form solution in terms of a singular value decomposition.",3.2 Scaling Up,[0],[0]
"Namely, the solution to (10) is P∗ = UV>, where UΣV∗ = X1:mΓ
∗Y>1:m. After obtaining this projection, we can immediately map the rest of the embeddings via ŷ(j)",3.2 Scaling Up,[0],[0]
"= P∗y(j).
",3.2 Scaling Up,[0],[0]
We point out that this two-step procedure resembles that of Conneau et al. (2018).,3.2 Scaling Up,[0],[0]
"Both ultimately produce an orthogonal mapping obtained by solving a Procrustes problems, but they differ in the way they produce pseudo-matches to allow for such second-step: while their approach relies
2As shown in the experimental section, we are able to run problems of size in the order of |Vs| ≈ 105",3.2 Scaling Up,[0],[0]
"≈ |Vt| on a single machine without relying on GPU computation.
",3.2 Scaling Up,[0],[0]
"Algorithm 1 Gromov-Wasserstein Computation for Word Embedding Alignment
Input: Source and target embeddings X, Y. Regularization λ.",3.2 Scaling Up,[0],[0]
"Probability vectors p,q. //",3.2 Scaling Up,[0],[0]
Compute intra-language similarities,3.2 Scaling Up,[0],[0]
"Cs ← cos(X,X), Ct ← cos(Y,Y) Cst ← C2sp1>m + 1nq(C2t )> while not converged",3.2 Scaling Up,[0],[0]
"do
//",3.2 Scaling Up,[0],[0]
Compute pseudo-cost matrix (Eq. (9)),3.2 Scaling Up,[0],[0]
ĈΓ ← Cst − 2CsΓC>t //,3.2 Scaling Up,[0],[0]
Sinkhorn iterations (Eq. (7)),3.2 Scaling Up,[0],[0]
"a← 1, K← exp{−ĈΓ/λ} while not converged do a← p Kb, b← q",3.2 Scaling Up,[0],[0]
"K>a end while Γ← diag (a)Kdiag (b)
end while //",3.2 Scaling Up,[0],[0]
"Optional step: Learn explicit projection U,Σ,V> ← SVD(XΓY>)",3.2 Scaling Up,[0],[0]
"P = UV> return Γ,P
on an adversarially-learned transformation, we use an explicit optimization problem.
",3.2 Scaling Up,[0],[0]
We end this section by discussing parameter and configuration choices.,3.2 Scaling Up,[0],[0]
"To leverage the fast algorithm of Peyré et al. (2016), we always use the L2 distance as the loss function L between cost matrices.",3.2 Scaling Up,[0],[0]
"On the other hand, we observed throughout our experiments that the choice of cosine distance as the metric in both spaces consistently leads to better results, which agrees with common wisdom on computing distances between word embeddings.",3.2 Scaling Up,[0],[0]
This leaves us with a single hyperparameter to control: the entropy regularization term λ.,3.2 Scaling Up,[0],[0]
"By applying any sensible normalization on the cost matrices (e.g., dividing by the mean or median value), we are able to almost entirely eliminate sensitivity to that parameter.",3.2 Scaling Up,[0],[0]
"In practice, we use a simple scheme in all experiments: we first try the same fixed value (λ = 5× 10−5), and if the regularization proves too small (by leading to floating point errors), we instead use λ = 1× 10−4.",3.2 Scaling Up,[0],[0]
We never had to go beyond these two values in all our experiments.,3.2 Scaling Up,[0],[0]
We emphasize that at no point we use train (let alone test) supervision available with many datasets—model selection is done solely in terms of the unsupervised objective.,3.2 Scaling Up,[0],[0]
Pseudocode for the full method (with L = L2 and cosine similarity) is shown here as Algorithm 1.,3.2 Scaling Up,[0],[0]
"Through this experimental evaluation we seek to: (i) understand the optimization dynamics of the proposed approach (§4.2), evaluate its performance on benchmark cross-lingual word embedding tasks (§4.3), and (iii) qualitatively investigate the notion of distance-between-languages it computes (§4.4).",4 Experiments,[0],[0]
"Rather than focusing solely on prediction accuracy, we seek to demonstrate that the proposed approach offers a fast, principled, and robust alternative to state-of-the-art multi-step methods, delivering comparable performance.",4 Experiments,[0],[0]
Datasets We evaluate our method on two standard benchmark tasks for cross-lingual embeddings.,4.1 Evaluation Tasks and Methods,[0],[0]
"First, we consider the dataset of Conneau et al. (2018), which consists of word embeddings trained with FASTTEXT (Bojanowski et al., 2017) on Wikipedia and parallel dictionaries for 110 language pairs.",4.1 Evaluation Tasks and Methods,[0],[0]
"Here, we focus on the language pairs for which they report results: English (EN) from/to Spanish (ES), French (FR), German (DE), Russian (RU) and simplified Chinese (ZH).",4.1 Evaluation Tasks and Methods,[0],[0]
"We do not report results on Esperanto (EO) as dictionaries for that language were not provided with the original dataset release.
",4.1 Evaluation Tasks and Methods,[0],[0]
"For our second set of experiments, we consider the—substantially harder3—dataset of (Dinu et al., 2014), which has been extensively compared against in previous work.",4.1 Evaluation Tasks and Methods,[0],[0]
"It consists of embeddings and dictionaries in four pairs of languages; EN from/to ES, IT, DE, and FI (Finnish).
",4.1 Evaluation Tasks and Methods,[0],[0]
"3We discuss the difference in hardness of these two benchmark datasets in Section 4.3.
",4.1 Evaluation Tasks and Methods,[0],[0]
"Methods To see how our fully-unsupervised method compares with methods that require (some) cross-lingual supervision, we follow (Conneau et al., 2018) and consider a simple but strong baseline consisting of solving a procrustes problem directly using the available cross-lingual embedding pairs.",4.1 Evaluation Tasks and Methods,[0],[0]
We refer to this method simply as PROCRUSTES.,4.1 Evaluation Tasks and Methods,[0],[0]
"In addition, we compare against the fully-unsupervised methods of Zhang et al. (2017a), Artetxe et al. (2018) and Conneau et al. (2018).4 As proposed by the latter, we use CSLS whenever nearest neighbor search is required, which has been shown to improve upon naive nearest-neighbor retrieval in multiple work.",4.1 Evaluation Tasks and Methods,[0],[0]
"As previously mentioned, our approach involves only two optimization choices, one of which is required only for very large settings.",4.2 Training Dynamics of G-W,[0],[0]
"When running Algorithm 1 for the full set of embeddings is infeasible (due to memory limitations), one must decide what fraction of the embeddings to use during optimization.",4.2 Training Dynamics of G-W,[0],[0]
"In our experiments, we use the largest possible size allowed by memory constraints, which was found to be K = 20, 000 for the personal computer we used.
",4.2 Training Dynamics of G-W,[0],[0]
The other—more interesting—optimization choice involves the entropy regularization parameter λ used within the Sinkhorn iterations.,4.2 Training Dynamics of G-W,[0],[0]
"Large regularization values lead to denser optimal coupling Γ∗, while less regularization leads to sparser solutions,5 at the cost of a harder (more
4Despite its relevance, we do not include the OT-based method of Zhang et al. (2017b) in the comparison because their implementation required use of proprietary software.
",4.2 Training Dynamics of G-W,[0],[0]
"5In the limit λ→ 0, when n = m, the solution converges
non-convex) optimization problem.",4.2 Training Dynamics of G-W,[0],[0]
In Figure 2 we show the training dynamics of our method when learning correspondences between word embeddings from the dataset of Conneau et al. (2018).,4.2 Training Dynamics of G-W,[0],[0]
"As expected, larger values of λ lead to smoother improvements with faster runtime-per-iteration, at a price of some drop in performance.",4.2 Training Dynamics of G-W,[0],[0]
"In addition, we found that computing GW distances between closer languages (such as EN and FR) leads to faster convergence than for more distant ones (such as EN and RU, in Fig. 2c).
",4.2 Training Dynamics of G-W,[0],[0]
"Worth emphasizing are three desirable optimization properties that set apart the GromovWasserstein distance from other unsupervised alignment approaches, particularly adversarialtraining ones: (i) the objective decreases monotonically (ii) its value closely follows the true metric of interest (translation, which naturally is not available during training) and (iii) there is no risk of degradation due to overtraining, as is the case for adversarial-based methods trained with stochastic gradient descent (Conneau et al., 2018).",4.2 Training Dynamics of G-W,[0],[0]
We report the results on the dataset of Conneau et al. (2018) in Table 1.,4.3 Benchmark Results,[0],[0]
The strikingly high performance of all methods on this task belies the hardness of the general problem of unsupervised cross-lingual alignment.,4.3 Benchmark Results,[0],[0]
"Indeed, as pointed out by Artetxe et al. (2018), the FASTTEXT embeddings provided in this task are trained on very large and highly comparable—across languages— corpora (Wikipedia), and focuses on closely related pairs of languages.",4.3 Benchmark Results,[0],[0]
"Nevertheless, we carry out experiments here to have a broad evaluation of our approach in both easier and harder settings.
",4.3 Benchmark Results,[0],[0]
"Next, we present results on the more challeng-
to a permutation matrix, which gives a hard-matching solution to the transportation problem (Peyré and Cuturi, 2018).
",4.3 Benchmark Results,[0],[0]
"ing dataset of (Dinu et al., 2014) in Table 2.",4.3 Benchmark Results,[0],[0]
"Here, we rely on the results reported by (Artetxe et al., 2018) since by the time of writing the present work their implementation was not available yet.
",4.3 Benchmark Results,[0],[0]
"Part of what makes this dataset hard is the wide discrepancy between word distance across languages, which translates into uneven distance matrices (Figure 3), and in turn leads to poor results for G-W. To account for this, previous work has relied on an initial whitening step on the embeddings.",4.3 Benchmark Results,[0],[0]
"In our case, it suffices to normalize the pairwise similarity matrices to the same range to obtain substantially better results.",4.3 Benchmark Results,[0],[0]
"While we have observed that careful choice of the regularization parameter λ can obviate the need for this step, we opt for the normalization approach since it allows us to optimize without having to tune λ.
",4.3 Benchmark Results,[0],[0]
"We compare our method (with and without nor-
malization) against alternative approaches in Table 2.",4.3 Benchmark Results,[0],[0]
"Note that we report the runtimes of Artetxe et al. (2018) as-is, which are obtained by running on a Titan XP GPU, while our runtimes are, as before, obtained purely by CPU computation.",4.3 Benchmark Results,[0],[0]
"As mentioned earlier, Theorem 3.1 implies that the optimal value of the Gromov-Wasserstein problem can be legitimately interpreted as a distance between languages, or more explicitly, between their word embedding spaces.",4.4 Qualitative Results,[0],[0]
This distributional notion of distance is completely determined by pairwise geometric relations between these vectors.,4.4 Qualitative Results,[0],[0]
"In Figure 4 we show the values GW(Cs,Ct,p,q) computed on the FASTTEXT word embeddings of Conneau et al. (2018) corresponding to the most frequent 2000 words in each language.
",4.4 Qualitative Results,[0],[0]
"Overall, these distances conform to our intuitions: the cluster of romance languages exhibits some of the shortest distances, while classical Chinese (ZH) has the overall largest discrepancy with all other languages.",4.4 Qualitative Results,[0],[0]
"But somewhat surprisingly, Russian is relatively close to the romance languages in this metric.",4.4 Qualitative Results,[0],[0]
We conjecture that this could be due to Russian’s rich morphology (a trait shared by romance languages but not English).,4.4 Qualitative Results,[0],[0]
"Furthermore, both Russian and Spanish are prodrop languages (Haspelmath, 2001) and share syntactic phenomena, such as dative subjects (Moore and Perlmutter, 2000; Melis et al., 2013) and differential object marking (Bossong, 1991), which might explain why ES is closest to RU overall.
",4.4 Qualitative Results,[0],[0]
"On the other hand, English appears remarkably isolated from all languages, equally distant from its germanic (DE) and romance (FR) cousins.",4.4 Qualitative Results,[0],[0]
"Indeed, other aspects of the data (such as corpus size) might be underlying these observations.",4.4 Qualitative Results,[0],[0]
Study of the problem of bilingual lexical induction goes back to Rapp (1995) and Fung (1995).,5 Related Work,[0],[0]
"While the literature on this topic is extensive, we focus here on recent fully-unsupervised and minimallysupervised approaches, and refer the reader to one of various existing surveys for a broader panorama (Upadhyay et al., 2016; Ruder et al., 2017).
",5 Related Work,[0],[0]
Methods with coarse or limited parallel data.,5 Related Work,[0],[0]
"Most of these fall in one of two categories: methods that learn a mapping from one space to the other, e.g., as a least-squares objective (e.g., (Mikolov et al., 2013)) or via orthogonal transformations Zhang et al. (2016); Smith et al. (2017); Artetxe et al. (2016), and methods that find a com-
mon space on which to project both sets of embeddings (Faruqui and Dyer, 2014; Lu et al., 2015).
",5 Related Work,[0],[0]
Fully Unsupervised methods.,5 Related Work,[0],[0]
Conneau et al. (2018) and Zhang et al. (2017a) rely on adversarial training to produce an initial alignment between the spaces.,5 Related Work,[0],[0]
The former use pseudo-matches derived from this initial alignment to solve a Procrustes (2) alignment problem.,5 Related Work,[0],[0]
"Our GromovWasserstein framework can be thought of as providing an alternative to these adversarial training steps, albeit with a concise optimization formulation and producing explicit matches (via the optimal coupling) instead of depending on nearest neighbor search, as the adversarially-learnt mappings do.
",5 Related Work,[0],[0]
Zhang et al. (2017b) also leverage optimal transport distances for the cross-lingual embedding task.,5 Related Work,[0],[0]
"However, to address the issue of nonalignment of embedding spaces, their approach follows the joint optimization of the transportation and procrustes problem as outlined in Section 2.2.",5 Related Work,[0],[0]
"This formulation makes an explicit modeling assumption (invariance to unitary transformations), and requires repeated solution of Procrustes problems during alternating minimization.",5 Related Work,[0],[0]
"GromovWasserstein, on the other hand, is more flexible and makes no such assumption, since it directly deals with similarities rather than vectors.",5 Related Work,[0],[0]
"In the case where it is required, such an orthogonal mapping can be obtained by solving a single procrustes problem, as discussed in Section 3.2.",5 Related Work,[0],[0]
In this work we provided a direct optimization approach to cross-lingual word alignment.,6 Discussion and future work,[0],[0]
The Gromov-Wasserstein distance is well-suited for this task as it performs a relational comparison of word-vectors across languages rather than wordvectors directly.,6 Discussion and future work,[0],[0]
"The resulting objective is concise, and can be optimized efficiently.",6 Discussion and future work,[0],[0]
"The experimental results show that the resulting alignment framework is fast, stable and robust, yielding near stateof-the-art performance at a computational cost orders of magnitude lower than that of alternative fully unsupervised methods.
",6 Discussion and future work,[0],[0]
"While directly solving Gromov-Wasserstein problems of reasonable size is feasible, scaling up to large vocabularies made it necessary to learn an explicit mapping via Procrustes.",6 Discussion and future work,[0],[0]
GPU computations or stochastic optimization could help avoid this secondary step.,6 Discussion and future work,[0],[0]
The authors would like to thank the anonymous reviewers for helpful feedback.,Acknowledgments,[0],[0]
"The work was partially supported by MIT-IBM grant “Adversarial learning of multimodal and structured data”, and Graduate Fellowships from Hewlett Packard and CONACYT.",Acknowledgments,[0],[0]
Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning.,abstractText,[0],[0]
"Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools.",abstractText,[0],[0]
"Current state-of-theart methods, however, involve multiple steps, including heuristic post-hoc refinement strategies.",abstractText,[0],[0]
"In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms.",abstractText,[0],[0]
"Indeed, we exploit the GromovWasserstein distance that measures how similarities between pairs of words relate across languages.",abstractText,[0],[0]
"We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",abstractText,[0],[0]
Gromov-Wasserstein Alignment of Word Embedding Spaces,title,[0],[0]
"In this paper, we focus on the multi-term nonsmooth convex composite optimization
min x∈X f(x) + n∑ i=1",1. Introduction,[0],[0]
gi(x),1. Introduction,[0],[0]
", (1)
where X is a linear space, gi : X → (−∞,+∞] is a proper, lower semicontinuous convex function for all i = 1, · · · , n, and f : X → (−∞,+∞) is a continuous
1Tencent AI Lab, China 2Sun Yat-sen University, China 3The Chinese University of Hong Kong, China.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
Li Shen,1. Introduction,[0],[0]
"<mathshenli@gmail.com>, Wei Liu <wliu@ee.columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"differentiable convex function with its gradient satisfying the inequality that
1
L ∥∥∇f(x)−∇f(y)∥∥2 ≤ 〈∇f(x)−∇f(y), x− y〉. (2)",1. Introduction,[0],[0]
"The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010), graph-guided fused Lasso (Chen et al., 2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; Dupé et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007).",1. Introduction,[0],[0]
"By introducing the multi-term nonsmooth regularization term∑n i=1 gi(x) such as structured sparsity (Huang et al., 2011; Bach et al., 2012; Bach, 2010) and nonnegativity (Chen & Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models.",1. Introduction,[0],[0]
"However, due to the multi-term nonsmooth regularization term ∑n i=1 gi(x), the optimization problem (1) is too complicated to be solved even for small n. For n ≤ 2, some existing popular first-order optimization methods are accelerated proximal gradient method (Beck & Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three operator splitting method (Davis & Yin, 2015), and some primal-dual operator splitting methods such as majorized alternating direction method of multiplier (ADMM) (Cui et al., 2016; Lin et al., 2011), fast proximity method (Li & Zhang, 2016), and so on.
",1. Introduction,[0],[0]
"On the other hand, when n ≥ 3, there also exist some algorithms for solving problem (1).",1. Introduction,[0],[0]
"A directly method for (1) is smoothing accelerated proximal gradient (S-APG) proposed by Nesterov (Nesterov, 2005a;b).",1. Introduction,[0],[0]
"Then, Yu (Yu, 2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average approximation technique and Nesterov’s acceleration technique, which has been enhanced very recently by Shen et al. (Shen et al., 2017).",1. Introduction,[0],[0]
Their proposed method called APA-APG adopts an adaptive stepsize strategy.,1. Introduction,[0],[0]
"However,
the above mentioned methods S-APG, PA-APG and its enhanced version APA-APG all need a strict restriction on the nonsmooth functions {gi(x)} that each gi(x) must be Lipschitz continuous.",1. Introduction,[0],[0]
"In addition, some primal-dual parallel splitting methods (Briceno-Arias et al., 2011; Combettes & Pesquet, 2007; 2008; Condat, 2013; Vũ, 2013) generalized from traditional operator splitting, such as forward backward splitting method (Chen & Rockafellar, 1997) and Douglas Rachford splitting method (Eckstein & Bertsekas, 1992), can also solve the multi-term nonsmooth convex composite optimization problem (1).",1. Introduction,[0],[0]
"Different from prior work, Raguet et al. (Raguet et al., 2013) proposed an efficient primal operator splitting method called generalized forward backward splitting method using the classic forward backward splitting technique, which has shown the superiority over numerous existing primal-dual splitting methods (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) in dealing with variational image restoration problems.",1. Introduction,[0],[0]
All the above mentioned methods for problem (1) with n ≥ 3 share a common feature that they all split the nonsmooth composite term∑n i=1,1. Introduction,[0],[0]
gi(x),1. Introduction,[0],[0]
"in the Jacobi iteration manner, i.e., parallelly.",1. Introduction,[0],[0]
"This is one of the main differences between existing splitting methods and our proposed method in this paper.
",1. Introduction,[0],[0]
"To split the nonsmooth composite term ∑n i=1 gi(x) more efficiently, we propose a novel operator splitting algorithm to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal mapping of the current function gi(x) uses the proximal mappings of gj(x) for all j < i which have already been computed ahead.",1. Introduction,[0],[0]
"In addition, to further improve the algorithm’s efficiency, we leverage the over-relaxation acceleration technique.",1. Introduction,[0],[0]
"What’s more, we provide a new strategy that the over-relaxation stepsize can be determined adaptively, ensuring a larger value to accelerate the algorithm.",1. Introduction,[0],[0]
The most important is that the convergence of our proposed GSOS algorithm is established by a newly developed analysis technique.,1. Introduction,[0],[0]
"In detail, given an invertible linear operator R, we first argue that the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of problem (1) can be recovered
by the zero point set [ (R∗)−1SR, ∂g+A◦∇f◦A,NV ]−1 (0).",1. Introduction,[0],[0]
"This is fulfilled through adopting the tool of operator optimization theory, in which the composite operator SR, ∂g+A◦∇f◦A,NV is generalized from the definition of the composite monotone operator Sλ,A,B in (Eckstein & Bertsekas, 1992).",1. Introduction,[0],[0]
"Next, by unitizing the definition of the -enlargement of maximal monotone (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), we establish a key property for SR, ∂g+A◦∇f◦A,NV , that is, gph ( SR, (∂g+A∗◦∇f◦A)[",1. Introduction,[0],[0]
"],NV )",1. Introduction,[0],[0]
"⊆
gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]",1. Introduction,[0],[0]
[ ] ) .,1. Introduction,[0],[0]
"Based on this observation, we equivalently reformulate the GSOS algorithm as a two-step iterations algorithm.",1. Introduction,[0],[0]
"Then, the
global convergence of the proposed GSOS algorithm is easily established based on this reformulation.
",1. Introduction,[0],[0]
"The closest algorithm to our proposed GSOS algorithm is the generalized forward backward splitting method proposed by Raguet et al. (Raguet et al., 2013).",1. Introduction,[0],[0]
"By carefully selecting the scaling matrix H in the forthcoming GSOS algorithm, it is easy to check that GSOS covers the generalized forward backward splitting method as a special case.",1. Introduction,[0],[0]
"Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method (Luo & Tseng, 1991; Yuan et al., 2016).",1. Introduction,[0],[0]
"Choosing the scaling matrixH suitably, the proposed GSOS algorithm can inherit the advantage of the matrix splitting technique which has shown the efficiency in (Yuan et al., 2016) for coping with a special class of coordinate separable composite optimization problems.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
"In Section 2, we first give the definitions of some useful notations which can make the paper much more readable.",1. Introduction,[0],[0]
"We also establish some lemmas and propositions based on monotone operator theory (Bauschke & Combettes, 2011), which are the key to the convergence of the GSOS algorithm.",1. Introduction,[0],[0]
"In Section 3, we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity.",1. Introduction,[0],[0]
"In Section 4, we conduct numerical experiments on overlapping group Lasso and graph-guided fused Lasso problems to evaluate the efficacy of the GSOS algorithm.",1. Introduction,[0],[0]
"Finally, we draw conclusions in Section 5.",1. Introduction,[0],[0]
Let Y = ∏n i=1,2. Preliminaries and Notations,[0],[0]
"Xi be the product space of Xi with Xi = X for all i ∈ {1, 2, · · · , n}.",2. Preliminaries and Notations,[0],[0]
Let V be a linear space and V⊥ be its complementary space with the following definitions V=,2. Preliminaries and Notations,[0],[0]
{ y ∈ Y | y1 = · · ·,2. Preliminaries and Notations,[0],[0]
"= yn } , V⊥= { y ∈ Y |
n∑ i yi = 0 } .
",2. Preliminaries and Notations,[0],[0]
Let IX : X → X be the identity map and EY : X → Y be a block linear operator defined as EY =( IX · · · IX )∗ .,2. Preliminaries and Notations,[0],[0]
Let A : Y → X be a linear operator defined as Ay = 1nE ∗,2. Preliminaries and Notations,[0],[0]
Yy,2. Preliminaries and Notations,[0],[0]
= 1 n ∑n i=1 yi.,2. Preliminaries and Notations,[0],[0]
"Hence, its adjoint operator A∗ : X → Y is defined as A∗x = 1nEYx.",2. Preliminaries and Notations,[0],[0]
"Let H,R : Y → Y be block lower triangular linear invertible operators satisfying (R∗)−1 = H and H + H∗ 0.",2. Preliminaries and Notations,[0],[0]
"Moreover,H is defined as H1,1 0 · · · 0 ... . . .",2. Preliminaries and Notations,[0],[0]
"...
...",2. Preliminaries and Notations,[0],[0]
"Hn−1,1 · · · Hn−1,n−1 0",2. Preliminaries and Notations,[0],[0]
"Hn,1 · · ·",2. Preliminaries and Notations,[0],[0]
"Hn−1,n Hn,n  , (3) where Hi,j : X → X is a linear operator for all (i, j) ∈ {1, · · · , n}.",2. Preliminaries and Notations,[0],[0]
It is worthwhile to emphasize that,2. Preliminaries and Notations,[0],[0]
"Hi,i is also possible to be a lower triangular linear operator satisfying
Hi,i +H∗i,i 0.",2. Preliminaries and Notations,[0],[0]
"Next, we abuse the notation ‖ · ‖H which is induced by the inner product 〈·,H·〉 satisfying
‖ · ‖H : = √ 〈·,H·〉 = √ 〈·,H∗·〉
= √ 〈·, H+H ∗
2 ·〉 = ‖ · ‖H+H∗ 2 .",2. Preliminaries and Notations,[0],[0]
"(4)
In addition, we define the generalized proximal mapping of a proper, lower semicontinuous convex function gi(x) with respect to the invertible linear operatorHi,i.
Definition 1 For a given x, the proximal mapping denoted by ProxH−1i,i gi(x) of a proper, lower semicontinuous convex function gi with respect to an invertible linear operator",2. Preliminaries and Notations,[0],[0]
"Hi,i satisfying Hi,i + H∗i,i 0 is defined to be the zero point of the following inclusion equation
0 ∈ ∂gi(·)",2. Preliminaries and Notations,[0],[0]
"+Hi,i(· − x).",2. Preliminaries and Notations,[0],[0]
"(5)
Moreover, if Hi,i is symmetric, it can be reformulated as the following convex minimization
ProxHi,igi(x) := arg min y∈X
gi(y) + 1
2 ‖y",2. Preliminaries and Notations,[0],[0]
"− x‖2Hi,i .
",2. Preliminaries and Notations,[0],[0]
"Next, we recall the definition of -enlargement of monotone operators (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), which is an effective tool for establishing the convergence of the proposed GSOS algorithm.
",2. Preliminaries and Notations,[0],[0]
Definition 2,2. Preliminaries and Notations,[0],[0]
"Given a maximal monotone operator T : X ⇒ X, the (≥ 0)-enlargement of T is defined as the set T",2. Preliminaries and Notations,[0],[0]
"[ ](x) := { v ∈ Y | 〈w − v, z",2. Preliminaries and Notations,[0],[0]
− x〉 ≥,2. Preliminaries and Notations,[0],[0]
"− for all z ∈
X, w ∈ T",2. Preliminaries and Notations,[0],[0]
"(z) } .
",2. Preliminaries and Notations,[0],[0]
Recall that f(x) is a gradient Lipschitz convex function satisfying inequality (2).,2. Preliminaries and Notations,[0],[0]
"There exits 0 Σ Σ̂ LI such that the following two inequalities hold for any x, x′ ∈ X
f(x) ≤ f(x′) + 〈∇f(x′), x− x′〉+ 1 2",2. Preliminaries and Notations,[0],[0]
‖x− x′‖2,2. Preliminaries and Notations,[0],[0]
"Σ̂ , (6) f(x) ≥ f(x′) + 〈∇f(x′), x− x′〉+ 1 2 ‖x− x′‖2Σ. (7)
Actually, when f(x) is a quadratic function, it holds Σ = Σ̂ directly in inequalities (6) and (7).",2. Preliminaries and Notations,[0],[0]
"The following lemma establishes the property of the enlargement of the composite operatorA∗ ◦∇f ◦A with f satisfying inequalities (6)- (7) or (2), which is an essential ingredient for reformulating the GSOS algorithm as a two-step iterations algorithm.
",2. Preliminaries and Notations,[0],[0]
Proposition 1 Assume that f is a gradient Lipschitz continuous convex function satisfying inequality (2).,2. Preliminaries and Notations,[0],[0]
"For any x1, x2 ∈ Y , it holds that
(A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
A)(x2) ∈ (A∗ ◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A)[ ](x1) (8)
with = L4 ‖Ax1−Ax2‖ 2.",2. Preliminaries and Notations,[0],[0]
"In addition, if f further satisfies inequalities (6)-(7), it holds that
(A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
A)(x2) ∈ (A∗ ◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A)[ ](x1) (9)
with = 14‖Ax1 −Ax2‖ 2 2Σ̂−Σ .
",2. Preliminaries and Notations,[0],[0]
"Remark 1 Two comments are made for Proposition 1:
(1)",2. Preliminaries and Notations,[0],[0]
This proposition gives two types of estimations for in( A∗ ◦∇f ◦A ),2. Preliminaries and Notations,[0],[0]
[,2. Preliminaries and Notations,[0],[0]
] in (8) and (9).,2. Preliminaries and Notations,[0],[0]
"When f is a quadratic
function, it is easy to check that
1 4 ‖Ax1 −Ax2‖22Σ̂−Σ ≤",2. Preliminaries and Notations,[0],[0]
L 4,2. Preliminaries and Notations,[0],[0]
‖Ax1,2. Preliminaries and Notations,[0],[0]
"−Ax2‖2
due to Σ̂ = Σ LI.",2. Preliminaries and Notations,[0],[0]
"When f is a general gradient Lipschitz continuous function, we do not know which estimation for is tighter in (8) and (9).
",2. Preliminaries and Notations,[0],[0]
"(2) The second part of this proposition can be regarded as an intensified version of Lemma 2.2 in (Svaiter, 2014) for a specified composite operator A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A.,2. Preliminaries and Notations,[0],[0]
"The first part of the proposition coincides with the results by applying Lemma 2.2 in (Svaiter, 2014) for A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
"A.
",2. Preliminaries and Notations,[0],[0]
"Next, we generalize the notation Sλ,T1,T2 in (Eckstein & Bertsekas, 1992) for a given λ > 0 and two maximal monotone operators T1, T2 as SR,T1,T2 for a given invertible linear operatorR defined as
gphSR, T1, T2 (10) := { (x1 +Ry2, x2",2. Preliminaries and Notations,[0],[0]
− x1),2. Preliminaries and Notations,[0],[0]
"| y1 ∈ T1(x1),
y2 ∈ T2(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"By (Eckstein & Bertsekas, 1992), we know that Sλ,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone.",2. Preliminaries and Notations,[0],[0]
"However, its generalized operator SR,T1,T2 is not monotone unless the invertible linear operator R reduces to be a constant.",2. Preliminaries and Notations,[0],[0]
"Very interesting, it can be shown that its composition with (R∗)−1, i.e., (R∗)−1SR,T1,T2 , is maximal monotone for any invertible linear operatorR.
Lemma 1 For any given invertible linear operator R, operator (R∗)−1SR,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone operators.
Setting T1 = ∂g + A∗",2. Preliminaries and Notations,[0],[0]
◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A, T2 = NV , we obtain SR, ∂g+A∗◦∇f◦A,NV , which is defined as
gph ( SR,∂g+A∗◦∇f◦A,NV ) (11)
:= { (x1+Ry2, x2−x1)",2. Preliminaries and Notations,[0],[0]
"| y1∈(∂g +A∗ ◦ ∇f ◦ A)(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"By Lemma 1, we know that (R∗)−1SR, ∂g+A∗◦∇f◦A,NV is maximal monotone due to the maximal monotonicity of ∂g + A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A and NV .,2. Preliminaries and Notations,[0],[0]
"Hence, given a constant ≥ 0, the enlargement [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV",2. Preliminaries and Notations,[0],[0]
][ ] is well defined.,2. Preliminaries and Notations,[0],[0]
"In addition, based on the definition of SR,T1,T2 again, we set T1 = ∂g + (A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
"A)[ ], or T1 = (∂g + A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A)[ ] and T2 = NV in (10).,2. Preliminaries and Notations,[0],[0]
"Then we have the definition of SR,∂g+(A∗◦∇f◦A)[ ],NV or SR,(∂g+A∗◦∇f◦A)[ ],NV for any given invertible linear operatorR and constant ≥ 0",2. Preliminaries and Notations,[0],[0]
"as follows
gph ( SR,∂g+(A∗◦∇f◦A)",2. Preliminaries and Notations,[0],[0]
"[ ],NV ) (12)
:= { (x1+Ry2, x2−x1)|y1∈(∂g+(A∗◦∇f ◦A)[ ])(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } ,
gph ( SR,(∂g+A∗◦∇f◦A)[ ],NV ) (13)
:= { (x1+Ry2, x2−x1)|y1∈(∂g +A∗◦∇f ◦A)[ ])(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"In the proposition below, we will establish the relationships among the above mentioned three operators SR,∂g+(A∗◦∇f◦A)",2. Preliminaries and Notations,[0],[0]
"[ ],NV , SR,(∂g+A∗◦∇f◦A)[ ],NV and [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]",2. Preliminaries and Notations,[0],[0]
"[ ].
Proposition 2",2. Preliminaries and Notations,[0],[0]
"Given a constant ≥ 0 and an invertible linear operatorR, it holds that
gph ( SR, ∂g+(A∗◦∇f◦A)[ ],NV ) ⊆ gph ( SR, (∂g+A∗◦∇f◦A)[ ],NV
) ⊆ gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV",2. Preliminaries and Notations,[0],[0]
],2. Preliminaries and Notations,[0],[0]
"[ ] ) .
In the following, we establish the relationship between the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of prob-
lem (1) and [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0), which means that we can recover the solution of problem (1) through [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0).
",2. Preliminaries and Notations,[0],[0]
Lemma 2 Let linear operators H and R satisfy (R∗)−1 = H and H satisfy (3).,2. Preliminaries and Notations,[0],[0]
Denote Ω =,2. Preliminaries and Notations,[0],[0]
"[ (R∗)−1SR, (∂g+A∗◦∇f◦A),NV ]−1 (0).",2. Preliminaries and Notations,[0],[0]
"It holds that[ ∇f +
n∑ i=1",2. Preliminaries and Notations,[0],[0]
"∂gi
]−1 (0) = ( ETYH∗EY
)−1ETYH∗(Ω).",2. Preliminaries and Notations,[0],[0]
"In this section, we first propose the Gauss-Seidel operator splitting algorithm for solving the multi-term nonsmooth convex composite problem (1).",3. GSOS Algorithm,[0],[0]
"Then, based on the preliminaries in Section 2, we establish the convergence and iteration complexity of the GSOS algorithm.
",3. GSOS Algorithm,[0],[0]
"Algorithm 1 GSOS Algorithm Parameters: Choose σ ∈ (0, 1), a linear operator H satisfying (3) and a starting point z0 ∈ Z .",3. GSOS Algorithm,[0],[0]
Set θfix1 ∈,3. GSOS Algorithm,[0],[0]
"( − 1, θ1 ] and θfix2 ∈",3. GSOS Algorithm,[0],[0]
"( − 1, θ2 ] , where θ1 and θ2 are
defined via equations (14a) and (14b), respectively.",3. GSOS Algorithm,[0],[0]
"for k = 0, 1, 2, · · · ,K do xk := EY ( ETYHEY )−1ETYHzk; for i = 1, 2 · · · , n do yki := ProxH−1i,i gi ( H−1i,i",3. GSOS Algorithm,[0],[0]
"[ ∑i j=1Hi,j(2xkj − zkj )",3. GSOS Algorithm,[0],[0]
"−
1 n∇f( 1 n ∑n i=1",3. GSOS Algorithm,[0],[0]
x,3. GSOS Algorithm,[0],[0]
k i ),3. GSOS Algorithm,[0],[0]
"− ∑i−1 j=1Hi,jykj ] ) ;
end for set θadap1k as (14c) and θ adap2 k as (14d); set θk ∈",3. GSOS Algorithm,[0],[0]
"[θfix1, θadap1k ]",3. GSOS Algorithm,[0],[0]
"∪ [θfix2, θ adap2 k ]; zk+1 := zk + (1 + θk)(y k − xk);
end for return ωK := ( ETYH∗EY )−1ETYH∗zK .",3. GSOS Algorithm,[0],[0]
"In Algorithm 1, parameters θ1, θ1, θ adap1 k , θ adap1 k are defined as θ1 = max { θ|(θ − σ)(H+H∗) + LA∗A 0 } ; (14a) θ2 = max { θ | (θ − σ)(H+H∗) (14b) +",3. GSOS Algorithm,[0],[0]
A∗(2Σ̂− Σ)A 0 } ; θadap1k = σ,3. GSOS Algorithm,[0],[0]
− L‖A(xk − yk)‖2 ‖xk,3. GSOS Algorithm,[0],[0]
− yk‖2H+H∗ ; (14c) θadap2k,3. GSOS Algorithm,[0],[0]
= σ,3. GSOS Algorithm,[0],[0]
− ‖A(xk,3. GSOS Algorithm,[0],[0]
"− yk)‖2
2Σ̂−Σ ‖xk",3. GSOS Algorithm,[0],[0]
− yk‖2H+H∗ .,3. GSOS Algorithm,[0],[0]
"(14d)
",3. GSOS Algorithm,[0],[0]
Remark 2,3. GSOS Algorithm,[0],[0]
"We make some comments on GSOS below.
",3. GSOS Algorithm,[0],[0]
"(1) For the updating step of xk, we obtain xk = EY (∑K i,j=1Hij )",3. GSOS Algorithm,[0],[0]
"−1∑K j=1 ∑K i=j Hijzkj by using the
notations H and EY .",3. GSOS Algorithm,[0],[0]
"Similarly, we have ωk =(∑K i,j=1Hij )−1∑K j=1 ∑j i=iH∗jizkj .",3. GSOS Algorithm,[0],[0]
"Hence, we need
to compute the inverse of ∑n i,j=1Hi,j .",3. GSOS Algorithm,[0],[0]
"However, if Hi,j is a lower triangular matrix operator, xk and ωk can be obtained easily.
",3. GSOS Algorithm,[0],[0]
"(2) By the definitions of ProxH−1i,i gi and y k, we need to
solve the following inclusion equation
Gki ∈",3. GSOS Algorithm,[0],[0]
"Hi,iyki + ∂gi(yki ),
where Gki = H −1",3. GSOS Algorithm,[0],[0]
"i,i",3. GSOS Algorithm,[0],[0]
"[∑i j=1Hi,j(2xkj − zkj )",3. GSOS Algorithm,[0],[0]
− 1 n∇f( 1 n ∑n i=1 x,3. GSOS Algorithm,[0],[0]
k i ),3. GSOS Algorithm,[0],[0]
"− ∑i−1 j=1Hi,jykj ] .",3. GSOS Algorithm,[0],[0]
"Usually, it is easy to choose a suitable Hi,i such that the solution of the above inclusion equation has a closed form.
",3. GSOS Algorithm,[0],[0]
(3) θk is the over-relaxation stepsize for accelerating the GSOS algorithm.,3. GSOS Algorithm,[0],[0]
"If the computations of θadap1k and θadap2k are time consuming, we can set θk = max{θfix1, θfix2}.
(4) WhenH is a diagonal matrix, i.e.,Hi,j = 0",3. GSOS Algorithm,[0],[0]
"andHi,i = aiI with some nonnegative constant ai, and the over relaxation stepsize θk is fixed to a smaller region, the GSOS algorithm reduces to the generalized forward backward splitting method in (Raguet et al., 2013).
",3. GSOS Algorithm,[0],[0]
"In the following, we reformulate the GSOS algorithm as a two-step iterations algorithm by utilizing monotone optimization theory established in Section 2, which is the key to the convergence of the GSOS algorithm.
",3. GSOS Algorithm,[0],[0]
"Proposition 3 Let g : Y → (−∞,+∞] be the function defined as g(x) = ∑n i=1 gi(xi).",3. GSOS Algorithm,[0],[0]
"Assume that the sequences (xk, yk) and zk are generated by Algorithm 1 with σ ∈ (0, 1).",3. GSOS Algorithm,[0],[0]
Let vk = (R∗)−1(xk − yk) and zk = yk + R(R∗)−1(zk,3. GSOS Algorithm,[0],[0]
− xk).,3. GSOS Algorithm,[0],[0]
"Then, for all k ∈ N, there exists k ≥ 0 such that the iterations in Algorithm 1 can be reformulated as the following two-step iterations algorithm:",3. GSOS Algorithm,[0],[0]
vk ∈,3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR, ∂g+(A∗◦∇f◦A),NV ]",3. GSOS Algorithm,[0],[0]
"[ k](zk), (15a) θk‖R∗vk‖2R−1 + ‖R ∗vk + zk",3. GSOS Algorithm,[0],[0]
"− zk‖2R−1
+2 k ≤",3. GSOS Algorithm,[0],[0]
σ‖zk,3. GSOS Algorithm,[0],[0]
"− zk‖2R−1 , (15b)
",3. GSOS Algorithm,[0],[0]
and zk+1 =,3. GSOS Algorithm,[0],[0]
"zk − (1 + θk)R∗vk.
",3. GSOS Algorithm,[0],[0]
"Remark 3 Based on Proposition 3, the GSOS algorithm can be regarded as an inexact over-relaxed metric proximal point algorithm for the composite inclusion
0 ∈",3. GSOS Algorithm,[0],[0]
"(R∗)−1SR,∂g+A∗◦∇f◦A,NV (z).
",3. GSOS Algorithm,[0],[0]
"By Proposition 3 and Lemma 2, we can establish the convergence of the GSOS algorithm based on the relationship
between the two zero point sets [∇f+ n∑ i=1 ∂gi] −1(0) and Ω.
Theorem 1 Let {(xk, yk, zk)} be the sequence generated by Algorithm 1.",3. GSOS Algorithm,[0],[0]
"We have:
(i) for any z∗ ∈",3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0), it holds that
‖zk+1",3. GSOS Algorithm,[0],[0]
− z∗‖2R−1 ≤,3. GSOS Algorithm,[0],[0]
‖z,3. GSOS Algorithm,[0],[0]
k,3. GSOS Algorithm,[0],[0]
"− z∗‖2R−1 (16)
− (1− σ)(1 + θk)‖xk − yk‖2R−1 ;
(ii) zk converges to a point belonging to zero point set",3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0)",3. GSOS Algorithm,[0],[0]
"and ωk converges to a point belonging to [∇f + ∑n i=1 ∂gi] −1 (0), i.e., the optimal solution set
of problem (1).
",3. GSOS Algorithm,[0],[0]
Theorem 1 indicates that ‖xk − yk‖ approaching to zero implies the convergence of the GSOS algorithm.,3. GSOS Algorithm,[0],[0]
"In the theorem below, we measure the convergence rates of two sequences ‖xk − yk‖ and ‖ωk − ωk+1‖.
Theorem 2 Let zk be the sequence generated by the GSOS algorithm.",3. GSOS Algorithm,[0],[0]
"Then, there exists i ∈ {1, 2, · · · , k} such that
‖xi − yi‖2 ≤",3. GSOS Algorithm,[0],[0]
"O (1 k ) ,",3. GSOS Algorithm,[0],[0]
∥∥ωi+1,3. GSOS Algorithm,[0],[0]
"− ωi∥∥2 ≤ O(1 k ) .
",3. GSOS Algorithm,[0],[0]
"Due to the space limit, all proofs of the propositions, lemmas and theorems are placed into the supplementary material.",3. GSOS Algorithm,[0],[0]
"In this section, we apply the proposed algorithm to the overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010) and graph-guided fused Lasso problems (Chen et al., 2012; Kim & Xing, 2009), which can be formulated as
min 1
2 ‖Sx− b‖2 + K∑ i=1",4. Experiments,[0],[0]
gi(x).,4. Experiments,[0],[0]
"(17)
For overlapping group Lasso problem (21), gi(x) = ναi‖xGi‖ andK denotes the number of groups.",4. Experiments,[0],[0]
"For graphguided Lasso problem (25), gi(x) = ναij‖xi−xj‖ and K denotes the number of edges in the graph edge set E.
We describe the detailed techniques in the experimental implementation for (17).",4. Experiments,[0],[0]
"Given a > 12 and a positive definite operator D satisfying D STS, we set
Hi,j = {
1 K2D, i ≥ j ∈ {1, 2, · · · ,K}; a K2D, i = j ∈ {1, 2, · · · ,K}.
(18)
",4. Experiments,[0],[0]
"Hence, it easy to check that H + H∗ = A∗DA + 2a−1 K2 Diag ( EYD ) 0.",4. Experiments,[0],[0]
"Due to the smooth term in overlapping group Lasso (21) is quadratic, the two estimations θ2 and θadap2k in (14b) and (14d) are preferred to be used.",4. Experiments,[0],[0]
"By specific H, we obtain ∑K i,j=1Hi,j =
K(K−1)+2αK 2K2 D and∑K
j=1 ∑K i=j Hi,jzkj = D K2 ∑K j=1(a+K−j)zkj ,which fur-
ther imply xk =",4. Experiments,[0],[0]
"(∑K i,j=1Hi,j )−1∑K j=1 ∑K i=j Hi,jzkj =
2 ∑K j=1(a+K−j)z k j
K(K−1)+2aK .",4. Experiments,[0],[0]
"Moreover, by the positive definiteness of Hi,i and D, it holds that ∑n j=1 ∑j i=iH∗j,izkj =
D K2 ∑K j=1(a + j − 1)zkj .",4. Experiments,[0],[0]
"Hence, we attain ωk =
2 ∑K j=1(a+j−1)z k j
K(K−1)+2aK .",4. Experiments,[0],[0]
"In addition, by the definition of H, we reformulate the estimation (14b) for θk as the following form:
θ = max { θ | EY [ (σ − θ)D − STS ]",4. Experiments,[0],[0]
"E∗Y
+ (2a− 1) ( σ − θ ) Diag(EYD) 0 } .
",4. Experiments,[0],[0]
"Due to a ≥ 12 and the positive definiteness of D, a sufficient condition satisfying the constraint in the above set is{
(σ − θ)D − STS 0, θ ≤ σ }
.",4. Experiments,[0],[0]
"Hence, we have an alternative estimation for θ as
θ = max { θ | (σ − θ)D − STS 0, θ ≤ σ } .",4. Experiments,[0],[0]
"(19)
Similarly, the adaptive stepsize estimation (14d) is reformulated as
θadapk = σ",4. Experiments,[0],[0]
"−
1 2K2 ∥∥∥∥ K∑ i=1",4. Experiments,[0],[0]
(xk − yki ),4. Experiments,[0],[0]
"∥∥∥∥2 STS
K∑ j=1 K∑",4. Experiments,[0],[0]
i=j (xk − yki )THij(xk,4. Experiments,[0],[0]
− ykj ) .,4. Experiments,[0],[0]
"(20)
Therefore, the GSOS algorithm can be specified as the following form for solving problem (17).
",4. Experiments,[0],[0]
Algorithm 2 GSOS Algorithm for Solving Problem (,4. Experiments,[0],[0]
"17) Parameters: Choose σ ∈ (0, 1), positive definite operators D and Hi,j satisfying (18), and a starting point z0 ∈ Z .",4. Experiments,[0],[0]
"Set θ as (19) and θfix ∈ ( − 1, θ1 ] .
for k = 0, 1, 2, · · · , do xk := 2 ∑K j=1(α+K−j)z k j
K(K−1)+2αK ; for i = 1, 2 · · · ,K do yki := ProxH−1i,i gi ( H−1i,i",4. Experiments,[0],[0]
"[ ∑i j=1Hi,j(2xk − zkj )",4. Experiments,[0],[0]
"−
1 KS T (Sxk − b)− ∑i−1 j=1Hi,jykj ] ) ;
end for set θk ∈",4. Experiments,[0],[0]
"[θfixk , θ adap k",4. Experiments,[0],[0]
"], where θ adap k is defined via (20); for j = 1, 2 · · · ,K do zk+1j := z k j + (1 + θk)(y k j − xk);
end for end for return ωN = 2 ∑K j=1(α+j−1)z N j
K(K−1)+2αK .
",4. Experiments,[0],[0]
"In this paper, we compare the proposed GSOS algorithm with four state-of-the-art algorithms below.
",4. Experiments,[0],[0]
"• GFB (Raguet et al., 2013): Generalized Forward Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.
",4. Experiments,[0],[0]
"• PDM (Condat, 2013):",4. Experiments,[0],[0]
"A first-order Primal-Dual splitting Method (PDM) (Condat, 2013) for solving jointly the primal and dual formulations of large-scale convex minimization problems involving Lipschitz, proximal and linear composite terms.
",4. Experiments,[0],[0]
"• PA-APG (Yu, 2013): Proximal Average approximated Accelerated Proximal Gradient (PA-APG) algorithm (Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al., 2008) to separate the multi-term nonsmooth function in (1).",4. Experiments,[0],[0]
"It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov, 2005b;a).
",4. Experiments,[0],[0]
"• APA-APG (Shen et al., 2017):",4. Experiments,[0],[0]
"An enhanced version of PA-APG, which incorporates the Adaptive Proximal Average approximation technique with the Accelerated Proximal Gradient (APA-APG) method to improve the efficiency of the optimization procedure.
",4. Experiments,[0],[0]
"It is worthwhile to emphasize that PA-APG and APA-APG algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization is Lipschitz continuous.",4. Experiments,[0],[0]
"Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided fused Lasso are all exactly Lipschitz continuous, the two efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with the GSOS algorithm to illustrate the efficacy of GSOS.",4. Experiments,[0],[0]
"In the implementation, the approximation parameter for PAAPG is set as 1.0e− 5.",4. Experiments,[0],[0]
"In this subsection, we apply the proposed GSOS algorithm to the overlapping group Lasso problem, which takes the following formal definition:
min 1
2 ‖Sx− b‖2 + ν K∑ i=1 αi‖xGi‖, (21)
where S ∈ Rn×d is the sampling matrix, b is the noisy observation vector, G = {G1, · · · ,GK} denotes the set of overlapping groups (Gi ⊂ {1, · · · , d} satisfying ⋃K i=1",4.1. Overlapping Group Lasso,[0],[0]
"Gi =
{1, · · · , d} and Gi ⋂ Gj 6= ∅",4.1. Overlapping Group Lasso,[0],[0]
"for some i, j), xGi ∈ Rd is a duplication of x with x{1,··· ,d}\Gi = 0, αi is the weight for the i-th group, and ν is the regularization parameter controlling group sparsity.
",4.1. Overlapping Group Lasso,[0],[0]
"During the implementation of Algorithm 2, we need to calculate the generalized proximal mapping of ‖xGi‖ in the updating step of yki .",4.1. Overlapping Group Lasso,[0],[0]
"By the positive definiteness of Hi,i, the calculation of yki in Algorithm 2 is equivalent to solving the following problem:
yki := arg min x
1 2 ‖x− bk‖2Hi,i + ναi‖xGi‖,
where bk = H−1i,i",4.1. Overlapping Group Lasso,[0],[0]
"[∑i j=1Hi,j(2xk − zkj )",4.1. Overlapping Group Lasso,[0],[0]
"− 1 KS
T (Sxk − b)− ∑i−1 j=1Hi,jykj ] .",4.1. Overlapping Group Lasso,[0],[0]
"In the proposition below, given c, diag-
onal positive definite operatprHi,i and group G, we solve
x∗",4.1. Overlapping Group Lasso,[0],[0]
":= arg min x
1 2 ‖x− c‖2Hi,i + ν‖xG‖. (22)
",4.1. Overlapping Group Lasso,[0],[0]
"When Hi,i is identity matrix I, (22) has the closed-form solution
x∗ = { x∗G , i ∈ G, ci, else,
where x∗G",4.1. Overlapping Group Lasso,[0],[0]
"= { (1− ν/‖cG‖)cG , ‖cG‖ ≥ t; 0, else.
",4.1. Overlapping Group Lasso,[0],[0]
"Proposition 4 Let (Hi,i)G be the subdiagonal matrix of Hi,i with the index set G, and t∗ be the optimal solution of the one-dimensional optimization problem
min t≥0
{ 1
2
〈 cG , [ (Hi,i)−1G + 2tI",4.1. Overlapping Group Lasso,[0],[0]
]−1 cG 〉 + tν2 } .,4.1. Overlapping Group Lasso,[0],[0]
"(23)
Hence, the optimal solution of (22) has the following form
x∗ =
{ cG",4.1. Overlapping Group Lasso,[0],[0]
"− [ I + 2t∗(Hi,i)G ]−1 cG , i ∈ G;
ci, else.",4.1. Overlapping Group Lasso,[0],[0]
"(24)
Like (Chen et al., 2012; Yu, 2013), the entries of sampling matrix S ∈ Rn×d are sampled from an i.i.d. normal distribution, and x ∈ Rd with xj = (−1)j exp−(j−1)/100 and d = 90K + 10.",4.1. Overlapping Group Lasso,[0],[0]
"Let ξ be the noise sampled from the standard normal distribution, and the noisy observation satisfies b = Sx + ξ.",4.1. Overlapping Group Lasso,[0],[0]
"In addition, we set ν = 1 and αi = 1K2 for each group Gi and the groups {Gi} are overlapped by 10 elements, that is{
G1 = {1, · · · , 100} G2 = {91, · · · , 190} · · · GK = {d− 99, · · · , d}
} .
",4.1. Overlapping Group Lasso,[0],[0]
"The sampling size and the number of groups (n,K) are chosen from the following set
(n,K) ∈ {
(1000, 20), (2000, 40), (4000, 60), (4000, 80), (5000, 80), (5000, 100)
} .
",4.1. Overlapping Group Lasso,[0],[0]
"To further reduce the computations, in Algorithm 2 we set Hi,i = ‖STS‖I and the over-relaxation stepsize θk as θ in (19).",4.1. Overlapping Group Lasso,[0],[0]
"Hence, the compared five solvers GSOS, GFB, PDM, PA-APG and APA-APG have the same computational cost in each iteration.",4.1. Overlapping Group Lasso,[0],[0]
"To be fair, all the compared algorithms start with the same initial point.",4.1. Overlapping Group Lasso,[0],[0]
"The following six pictures in Figures 1 and 2 display the comparisons of the five solvers for a variety of (n,K).",4.1. Overlapping Group Lasso,[0],[0]
It is apparent that our proposed GSOS algorithm shows great superiorities over the other four solvers.,4.1. Overlapping Group Lasso,[0],[0]
The primal-dual solver PDM is slightly faster than the primal solver GFB.,4.1. Overlapping Group Lasso,[0],[0]
"PA-APG is the slowest algorithm, because the prespecified proximal average approximation precision is 1.0e − 5 which leads to a very small stepsize.",4.1. Overlapping Group Lasso,[0],[0]
"Also, APA-APG is much faster than the other four solvers at the first 50 iterations.",4.1. Overlapping Group Lasso,[0],[0]
"However, it is slowed down since the stepsize used in AP-APG becomes smaller and smaller as the iterations go on.",4.1. Overlapping Group Lasso,[0],[0]
"In this subsection, we perform experiments on graphguided fused Lasso which is formulated as
min 1
2 ‖Sx− b‖2 + ν ∑",4.2. Graph-Guided Fused Lasso,[0],[0]
"(i,j)∈E αij |xi",4.2. Graph-Guided Fused Lasso,[0],[0]
"− xj |, (25)
where αij ≥ 0 is the weight for the fused term ‖xi − xj‖ for all (i, j) ∈ E (E is the given graph edge set), and ν is
the regularization parameter.
",4.2. Graph-Guided Fused Lasso,[0],[0]
"In the implementation of Algorithm 2 for tackling graphguided fused Lasso (25), we need to solve the following optimization in the updating step of yk:
x∗ := arg min x
1 2 ‖x− b‖2Hi,i + ν|xi",4.2. Graph-Guided Fused Lasso,[0],[0]
"− xj |, (26)
whereHi,i is a diagonal positive definite matrix, and b and ν are given constants.",4.2. Graph-Guided Fused Lasso,[0],[0]
"Let hii and hjj be the i-th and j-th diagonal elements ofHi,i, respectively.
",4.2. Graph-Guided Fused Lasso,[0],[0]
"Proposition 5 The optimal solution of (26) takes the following closed-form:
x∗ =  ",4.2. Graph-Guided Fused Lasso,[0],[0]
bl,4.2. Graph-Guided Fused Lasso,[0],[0]
− h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"ll λ ∗, l = i, bl + h −1",4.2. Graph-Guided Fused Lasso,[0],[0]
"ll λ
∗, l = j, bl, l 6=",4.2. Graph-Guided Fused Lasso,[0],[0]
"i, j,
(27)
where λ∗ is defined as
λ∗ =  bi−bj",4.2. Graph-Guided Fused Lasso,[0],[0]
h−1ii,4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj , ∣∣∣ bi−bj h−1ii",4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj ∣∣∣ ≤ ν; sign (bi − bj) ν, ∣∣∣ bi−bj h−1ii",4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj
∣∣∣ >",4.2. Graph-Guided Fused Lasso,[0],[0]
ν.,4.2. Graph-Guided Fused Lasso,[0],[0]
"In the implementation, we use the similar parameter settings of S, ν as above.",4.2. Graph-Guided Fused Lasso,[0],[0]
"The dimension parameter pair (n, d) is chosen from the following set
(n, d) ∈ {
(2000, 500), (2000, 1000), (5000, 1000), (5000, 2000), (10000, 2000), (10000, 4000)
} ,
and the parameter αi = 100/|E|2.",4.2. Graph-Guided Fused Lasso,[0],[0]
"Similarly, all the compared algorithms start with the same initial point.",4.2. Graph-Guided Fused Lasso,[0],[0]
"The following six pictures in Figures 3 and 4 display the comparisons of the five solvers for six kinds of choices of (n, d).",4.2. Graph-Guided Fused Lasso,[0],[0]
"It
is obvious that the other four solvers GFB, PDM, AP-APG and APA-APG are not as efficient as the proposed GSOS algorithm, which demonstrates that the Gauss-Seidel technique is very useful for addressing nonsmooth optimization.",4.2. Graph-Guided Fused Lasso,[0],[0]
It is worthwhile to point out that the primal solver GFB is faster than the primal-dual solver PDM on graphguided fused Lasso.,4.2. Graph-Guided Fused Lasso,[0],[0]
"One possible reason is that the number of nonsmooth terms is too large, which will lead to a large quantity of dual variables introduced in PDM and hence slow down the updating of primal variables.",4.2. Graph-Guided Fused Lasso,[0],[0]
"In this paper, we proposed a novel first-order algorithm called GSOS for addressing multi-term nonsmooth convex composite optimization.",5. Conclusions,[0],[0]
"This algorithm inherits the advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated.",5. Conclusions,[0],[0]
"We found that the GSOS algorithm includes the generalized forward backward splitting method (Raguet et al., 2013) as a special case.",5. Conclusions,[0],[0]
"In addition, we developed a new technique to establish the global convergence and iteration complexity of the GSOS algorithm.",5. Conclusions,[0],[0]
"Last, we applied the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared it against several state-of-the-art algorithms.",5. Conclusions,[0],[0]
The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness.,5. Conclusions,[0],[0]
Yuan is supported by NSF-China (61402182).,Acknowledgements,[0],[0]
"In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics.",abstractText,[0],[0]
"The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity.",abstractText,[0],[0]
"In addition, we develop a new technique to establish the global convergence of the GSOS algorithm.",abstractText,[0],[0]
"To be specific, we first reformulate the iterations of GSOS as a twostep iterations algorithm by employing the tool of operator optimization theory.",abstractText,[0],[0]
"Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation.",abstractText,[0],[0]
"At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems.",abstractText,[0],[0]
Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.,abstractText,[0],[0]
GSOS: Gauss-Seidel Operator Splitting Algorithm for  Multi-Term Nonsmooth Convex Composite Optimization,title,[0],[0]
"↵ (1 e ↵) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications.",text,[0],[0]
"Many important problems, such as experimental design and sparse modeling, are naturally formulated as a subset selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e.,
max S✓V,|S|K F (S), (P)
1Department of Computer Science, ETH Zurich, Zurich, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Joachim M. Buhmann <jbuhmann@inf.ethz.ch>, Andreas Krause <krausea@ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"where V = {v1, . . .",1. Introduction,[0],[0]
", vn} is the ground set.",1. Introduction,[0],[0]
"Specifically, in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized.",1. Introduction,[0],[0]
This problem arises naturally in domains where performing experiments is costly.,1. Introduction,[0],[0]
"In sparse modeling, the task is to identify sparse representations of signals, enabling interpretability and robustness in high-dimensional statistical problems—properties that are crucial in modern data analysis.
",1. Introduction,[0],[0]
"Frequently, the standard GREEDY algorithm (Alg. 1) is used to (approximately) solve (P).",1. Introduction,[0],[0]
"For the case that F (S)
Algorithm 1: The GREEDY Algorithm Input: Ground set V , set function F : 2V!R+, budget K S
0 ; for t = 1, . . .",1. Introduction,[0],[0]
",K do
v ⇤ argmax v2V\St 1 F (S t 1",1. Introduction,[0],[0]
"[ {v}) F (St 1) S
t St 1 [ {v⇤} Output: SK
is a monotone nondecreasing submodular set function1, the GREEDY algorithm enjoys the multiplicative approximation guarantee of (1 1/e) (Nemhauser et al., 1978; Vondrák, 2008; Krause & Golovin, 2014).",1. Introduction,[0],[0]
"This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.e., F (S) and F (S) are submodular).
",1. Introduction,[0],[0]
"However, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold.",1. Introduction,[0],[0]
"In practice, however, the standard GREEDY algorithm often achieves very good performance on these applications, e.g., in subset selection with the R2 (squared multiple correlation) ob-
1 F (·) is monotone nondecreasing if 8A ✓ V, v 2 V , F (A [ {v})",1. Introduction,[0],[0]
F (A).,1. Introduction,[0],[0]
F (·) is submodular iff it satisfies the diminishing returns property F (A [ {v}),1. Introduction,[0],[0]
F (A) F (B [ {v}) F (B) for all A ✓ B ✓ V \{v}.,1. Introduction,[0],[0]
Assume wlog.,1. Introduction,[0],[0]
"that F (·) is normalized, i.e., F (;) = 0.
jective (Das & Kempe, 2011).",1. Introduction,[0],[0]
"To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set function is to being submodular.
",1. Introduction,[0],[0]
"Another important class of non-submodular set functions comes as the auxiliary function when optimizing a continuous function f(x) s.t. combinatorial constraints, i.e., min
x2C,supp(x)2I f(x), where supp(x) := {i | xi 6= 0} is the support set of x, C is a convex set, and I is the independent sets of the combinatorial structure.",1. Introduction,[0],[0]
"One of the most popular ways to solve this problem is to use the GREEDY algorithm to maximize the auxiliary function F (S) : = max
x2C,supp(x)✓S f(x).",1. Introduction,[0],[0]
"This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016).",1. Introduction,[0],[0]
"Recently, Elenberg et al. (2016) proved that if f(x) has L-restricted smoothness and m-restricted strong convexity, then the submodularity ratio of F (S) is lower bounded by m/L.",1. Introduction,[0],[0]
"This result significantly enlarges the domain where the GREEDY algorithm can be applied.
",1. Introduction,[0],[0]
"In this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant factor approximation guarantees of the GREEDY algorithm.",1. Introduction,[0],[0]
Our guarantees allow us to better characterize the empirical success of applying GREEDY on a significantly larger class of non-submodular functions.,1. Introduction,[0],[0]
"Furthermore, we bound these characteristics for important applications, rendering the usage of GREEDY a principled choice rather than a mere heuristic.",1. Introduction,[0],[0]
"Our main contributions are:
- We prove the first tight constant-factor approximation guarantees for GREEDY on maximizing nonsubmodular nondecreasing set functions s.t.",1. Introduction,[0],[0]
"a cardinality constraint, characterized by a novel combination of the (generalized) notions of submodularity ratio and curvature ↵.
- By theoretically bounding parameters ( ,↵) for several important objectives, including Bayesian A-optimality in experimental design, the determinantal function of a square submatrix and maximization of LPs with combinatorial constraints, our theory implies the first guarantees for them.
- Lastly, we experimentally validate our theory on several real-world applications.",1. Introduction,[0],[0]
"It is worth noting that for the Bayesian A-optimality objective, GREEDY generates comparable solutions as the classically used semidefinite programming (SDP) based method, but is usually two orders of magnitude faster.
Notation.",1. Introduction,[0],[0]
"We use boldface letters, e.g., x, to represent vectors, and capital boldface letters, e.g., A, to denote matrices.",1. Introduction,[0],[0]
"x
i is the ith entry of the vector x. We refer to V = {v1, ..., vn} as the ground set.",1. Introduction,[0],[0]
"We use f(·) to denote a continuous function, and F (·) to represent a set function.",1. Introduction,[0],[0]
"supp(x) := {i 2 V | x
i 6= 0} is the support set of the vector x, and [n] := {1, ..., n} for an integer n 1.",1. Introduction,[0],[0]
We denote the marginal gain of a set ⌦ ✓ V in context of a set S ✓ V as ⇢⌦(S) := F (⌦ [ S) F (S).,1. Introduction,[0],[0]
"For v 2 V , we use the shorthand ⇢
v (S) for ⇢{v}(S).",1. Introduction,[0],[0]
"In this section we provide the submodularity ratio and curvature for general, not necessarily submodular functions2, they are natural extensions of the classical ones.",2. Submodularity Ratio and Curvature,[0],[0]
"Let S 0 = ;, St = {j1, ..., jt}, t = 1, ...,K be the successive sets chosen by GREEDY.",2. Submodularity Ratio and Curvature,[0],[0]
"For brevity, let ⇢ t : = ⇢ jt(S t 1 ) be the marginal gain of GREEDY in step t.
Definition 1 (Submodularity ratio (Das & Kempe, 2011)).",2. Submodularity Ratio and Curvature,[0],[0]
"The submodularity ratio of a non-negative set function F (·) is the largest scalar s.t.
X !2⌦\S ⇢ !",2. Submodularity Ratio and Curvature,[0],[0]
"(S) ⇢⌦(S), 8 ⌦, S ✓ V.
The greedy submodularity ratio is the largest scalar G s.t. X
!2⌦\St ⇢ !",2. Submodularity Ratio and Curvature,[0],[0]
"(S
t )",2. Submodularity Ratio and Curvature,[0],[0]
"G⇢⌦(St), 8|⌦|=K, t = 0, . .",2. Submodularity Ratio and Curvature,[0],[0]
.,2. Submodularity Ratio and Curvature,[0],[0]
",K 1.
",2. Submodularity Ratio and Curvature,[0],[0]
It is easy to see that G .,2. Submodularity Ratio and Curvature,[0],[0]
The submodularity ratio measures to what extent F (·) has submodular properties.,2. Submodularity Ratio and Curvature,[0],[0]
"We make the following observations:
Remark 1.",2. Submodularity Ratio and Curvature,[0],[0]
"For a nondecreasing function F (·), it holds a) ,
G 2",2. Submodularity Ratio and Curvature,[0],[0]
"[0, 1]; b) F (·) is submodular iff = 1.",2. Submodularity Ratio and Curvature,[0],[0]
Definition 2 (Generalized curvature).,2. Submodularity Ratio and Curvature,[0],[0]
"The curvature of a non-negative function F (·) is the smallest scalar ↵ s.t.
⇢
i (S \ {i} [ ⌦) (1 ↵)⇢",2. Submodularity Ratio and Curvature,[0],[0]
"i (S \ {i}), 8 ⌦, S ✓ V, i 2 S\⌦.
",2. Submodularity Ratio and Curvature,[0],[0]
"The greedy curvature is the smallest scalar ↵G 0 s.t.
⇢ ji(S i 1 [ ⌦) (1 ↵G)⇢",2. Submodularity Ratio and Curvature,[0],[0]
"ji(S i 1 ),
8 ⌦ : |⌦| = K, i : j i 2 SK 1\⌦.",2. Submodularity Ratio and Curvature,[0],[0]
2Curvature is commonly defined for submodular functions.,2. Submodularity Ratio and Curvature,[0],[0]
Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions.,2. Submodularity Ratio and Curvature,[0],[0]
We show in Appendix C the details of these notions and the relations to ours.,2. Submodularity Ratio and Curvature,[0],[0]
"Additionally, we prove in Remark 3 of Appendix C.2 that our combination of curvature and submodularity ratio is more expressive than that of Sviridenko et al. (2013) in characterizing the maximization of problem (P) using standard GREEDY.
",2. Submodularity Ratio and Curvature,[0],[0]
"When K = n or 1, SK 1\⌦ = ;, it is natural to define ↵ G
= 0.",2. Submodularity Ratio and Curvature,[0],[0]
It is easy to observe that ↵G  ↵.,2. Submodularity Ratio and Curvature,[0],[0]
"Note that the classical total curvature is ↵total := 1 min i2V ⇢i(V\{i})
⇢i(;) .
",2. Submodularity Ratio and Curvature,[0],[0]
Remark 2.,2. Submodularity Ratio and Curvature,[0],[0]
"For a nondecreasing function F (·), it holds: a) ↵,↵G 2",2. Submodularity Ratio and Curvature,[0],[0]
"[0, 1]; b) F (·) is supermodular iff ↵ = 0; c)",2. Submodularity Ratio and Curvature,[0],[0]
"If F (·) is submodular, then ↵G  ↵ = ↵total.
",2. Submodularity Ratio and Curvature,[0],[0]
"So for a submodular function, our notion of curvature is consistent with ↵total.",2. Submodularity Ratio and Curvature,[0],[0]
"Notably, ↵G usually characterizes the problem better than ↵total, as will be validated in Section 5.",2. Submodularity Ratio and Curvature,[0],[0]
We present approximation guarantee of GREEDY in Theorem 1.,3. Approximation Guarantee,[0],[0]
Note that both versions of the submodularity ratio and curvature apply in the proof.,3. Approximation Guarantee,[0],[0]
"For brevity, we use and ↵ to refer to any of these versions in the sequel.",3. Approximation Guarantee,[0],[0]
In Section 3.3 we prove tightness of the approximation guarantees.,3. Approximation Guarantee,[0],[0]
All omitted proofs are given in Appendix B. Theorem 1.,3. Approximation Guarantee,[0],[0]
Let F (·) be a non-negative nondecreasing set function with submodularity ratio 2,3. Approximation Guarantee,[0],[0]
"[0, 1] and curvature ↵ 2 [0, 1].",3. Approximation Guarantee,[0],[0]
"The GREEDY algorithm enjoys the following approximation guarantee for solving problem (P):
F (S
K ) 1 ↵
"" 1 ✓ K ↵
K
◆ K # F (⌦ ⇤ )
1 ↵
(1 e ↵ )",3. Approximation Guarantee,[0],[0]
"F (⌦⇤), (1)
where ⌦⇤ is the optimal solution of (P) and SK the output of the GREEDY algorithm.3",3. Approximation Guarantee,[0],[0]
"Before proving the theorem, we want to give the reader an intuition of the results and show how our results recover and extend several classical guarantees for the GREEDY algorithm.",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 0 (i.e., F (·) is supermodular), the approximation guarantee is lim
↵!0 1 ↵ (1 e ↵ ) = , which gives the first guarantee of greedily maximizing a nondecreasing supermodular function with bounded .",3.1. Interpreting Theorem 1,[0],[0]
"When = 1, (i.e., F (·) is submodular), we recover the guarantee of ↵ 1(1 e ↵) (Conforti & Cornuéjols, 1984).",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 1, we have a guarantee of (1 e ) (Das & Kempe, 2011).",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 1, = 1, we recover the classical guarantee of (1 1/e) (Nemhauser et al., 1978).",3.1. Interpreting Theorem 1,[0],[0]
We plot the constant-factor approximation guarantees for different values of and ↵ in Fig. 1.,3.1. Interpreting Theorem 1,[0],[0]
"One interesting phenomenon is that and ↵ play different roles: Looking at = 0, the approximation factor is always 0, independent of the value ↵ takes.",3.1. Interpreting Theorem 1,[0],[0]
"In contrast, for ↵ = 0, the
3For the setting that GREEDY is allowed to pick more than K elements, e.g., pick K0 > K elements, our theory can be easily extended to show that",3.1. Interpreting Theorem 1,[0],[0]
F (SK 0 ) ↵ 1(1 e ↵ K 0/,3.1. Interpreting Theorem 1,[0],[0]
"K)F (⌦⇤).
",3.1. Interpreting Theorem 1,[0],[0]
approximation guarantee is (1 e ).,3.1. Interpreting Theorem 1,[0],[0]
This can be interpreted as the curvature boosting the guarantees.,3.1. Interpreting Theorem 1,[0],[0]
The high-level proof framework is based on Conforti & Cornuéjols (1984) (where they derive the approximation guarantee for maximizing a nondecreasing submodular function with bounded curvature).,3.2. Proof of Theorem 1,[0],[0]
"However, adapting the proof to non-submodular functions requires several changes detailed in Section 6.
",3.2. Proof of Theorem 1,[0],[0]
Proof overview.,3.2. Proof of Theorem 1,[0],[0]
"Let us denote all problem instances of maximizing a non-negative nondecreasing function F (·) s.t. K-cardinality constraint (max|S|K F (S)) to be P K,↵,
, where F (·) is parametrized by submodularity ratio and curvature ↵.",3.2. Proof of Theorem 1,[0],[0]
"Let P⌦⇤,SK 2 PK,↵, denote those problem instances with optimal solution ⌦⇤ and greedy solution SK .",3.2. Proof of Theorem 1,[0],[0]
"We group all problem instances P
K,↵, according to the set ⌦",3.2. Proof of Theorem 1,[0],[0]
"⇤ \ SK := {l1 = jm1 , l2 = jm2 , . . .",3.2. Proof of Theorem 1,[0],[0]
", ls = j
ms}, where jm1 , . . .",3.2. Proof of Theorem 1,[0],[0]
", jms are consistent with the order of greedy selection.",3.2. Proof of Theorem 1,[0],[0]
"Let us denote the problem instances with ⌦ ⇤\SK = {l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} as the group PK,↵, ({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}).
",3.2. Proof of Theorem 1,[0],[0]
"The main idea of the proof is to investigate the worst-case approximation ratio of each group of the problem instances P K,↵,
({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}), 8{l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} ✓ SK .",3.2. Proof of Theorem 1,[0],[0]
We do this by constructing LPs based on the properties of the problem instances.,3.2. Proof of Theorem 1,[0],[0]
"By studying the structures of these LPs, we will prove that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \SK = ;.",3.2. Proof of Theorem 1,[0],[0]
"Thus the desired approximation guarantee corresponds to the worst-case approximation ratio of P
K,↵,
(;).
",3.2. Proof of Theorem 1,[0],[0]
The proof.,3.2. Proof of Theorem 1,[0],[0]
"When = 0 or F (⌦⇤) = 0, (1) holds naturally.",3.2. Proof of Theorem 1,[0],[0]
"In the following, let 2 (0, 1] and F (⌦⇤) > 0.",3.2. Proof of Theorem 1,[0],[0]
"First, we present Lemma 1, which will be used to construct the LPs.
",3.2. Proof of Theorem 1,[0],[0]
Lemma 1.,3.2. Proof of Theorem 1,[0],[0]
"For any ⌦ ✓ V with |⌦| = K and any t 2 {0, . . .",3.2. Proof of Theorem 1,[0],[0]
",K 1}, let wt := |St \ ⌦|.",3.2. Proof of Theorem 1,[0],[0]
"It holds that
↵
X
i:ji2St\⌦
⇢
i
+
X
i:ji2St\⌦
⇢
i
+ 1 (K wt)⇢
t+1 F (⌦).
",3.2. Proof of Theorem 1,[0],[0]
"We now specify the constructing of the LPs: For any problem instance P⌦⇤,SK 2 PK,↵, ({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}), we know that F (SK) =",3.2. Proof of Theorem 1,[0],[0]
"P K
i=1 ⇢",3.2. Proof of Theorem 1,[0],[0]
i (telescoping sum).,3.2. Proof of Theorem 1,[0],[0]
"Hence, the approximation ratio is F (S
K) F (⌦⇤) =",3.2. Proof of Theorem 1,[0],[0]
P i ⇢,3.2. Proof of Theorem 1,[0],[0]
"i
F (⌦⇤) , which we denote as R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) =",3.2. Proof of Theorem 1,[0],[0]
P i ⇢,3.2. Proof of Theorem 1,[0],[0]
i F (⌦⇤) .,3.2. Proof of Theorem 1,[0],[0]
Define xi,3.2. Proof of Theorem 1,[0],[0]
":= ⇢i
F (⌦⇤) , i 2",3.2. Proof of Theorem 1,[0],[0]
[K].,3.2. Proof of Theorem 1,[0],[0]
"Since F is nondecreasing, x
i 0.",3.2. Proof of Theorem 1,[0],[0]
"Plugging ⌦ = ⌦⇤ into Lemma 1, and considering t = 0, . . .",3.2. Proof of Theorem 1,[0],[0]
",K 1, we have in total K constraints over the variables x
i , which constitute the constraints of the LP.",3.2. Proof of Theorem 1,[0],[0]
"So the worst-case approximation ratio of the group P
K,↵, ({l1, . .",3.2. Proof of Theorem 1,[0],[0]
.,3.2. Proof of Theorem 1,[0],[0]
", ls}) is:
R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) =",3.2. Proof of Theorem 1,[0],[0]
"min X K
i=1",3.2. Proof of Theorem 1,[0],[0]
x,3.2. Proof of Theorem 1,[0],[0]
"i
, s.t. x",3.2. Proof of Theorem 1,[0],[0]
"i 0 and,
row (0) row (1)
... row (l1 1) row (l2 1) row (q = lr) ...",3.2. Proof of Theorem 1,[0],[0]
"row (ls 1) ... row (K 1)
2
66666666666666666664
K
↵
K
... ... . . .",3.2. Proof of Theorem 1,[0],[0]
↵ ↵ · · · K 0 ↵ ↵ · · · 1 K 1 ↵ ↵ · · · 1 1 K r ... ... ... ...,3.2. Proof of Theorem 1,[0],[0]
"...
. . .",3.2. Proof of Theorem 1,[0],[0]
↵ ↵ · · · 1 1 ↵ · · · K s+1 ... ... ... ... ... ... . . .,3.2. Proof of Theorem 1,[0],[0]
"↵ ↵ · · · 1 1 ↵ · · · 1 · · · K s
3
77777777777777777775 ·
2
66666666666666664
x1
x2
...",3.2. Proof of Theorem 1,[0],[0]
"xl1 xl2 xq+1
...",3.2. Proof of Theorem 1,[0],[0]
"xls
... xK
3
77777777777777775
2
66666666666666664 1 1 ... 1 1 1 ... 1 ... 1
3
77777777777777775
(2)
",3.2. Proof of Theorem 1,[0],[0]
"The following Lemma presents the key structure of the constructed LPs, which will be used to deduce the relation between the LPs of different problem instance groups.",3.2. Proof of Theorem 1,[0],[0]
Lemma 2.,3.2. Proof of Theorem 1,[0],[0]
Assume that the optimal solution of the constructed LP is x⇤ 2 RK+,3.2. Proof of Theorem 1,[0],[0]
and that s = |⌦⇤ \ SK | 1.,3.2. Proof of Theorem 1,[0],[0]
"For all 1  r  s it holds that x⇤
q  x⇤ q+1, where q = lr.
",3.2. Proof of Theorem 1,[0],[0]
Proof sketch of Lemma 2.,3.2. Proof of Theorem 1,[0],[0]
"Assume by virture of creating a contradiction that x⇤
q
> x ⇤ q+1.",3.2. Proof of Theorem 1,[0],[0]
"We can always create a
new feasible solution y⇤ 2 RK+ by decreasing x⇤q by some ✏ > 0, while increasing all the x⇤
q+1 to x⇤ K by some proper values, s.t. y⇤ has smaller LP objective value.",3.2. Proof of Theorem 1,[0],[0]
"Specifically, we define y⇤ as: for k = 1, . . .",3.2. Proof of Theorem 1,[0],[0]
", q 1, y⇤
k
:= x ⇤ k ; y
⇤ q",3.2. Proof of Theorem 1,[0],[0]
":= x ⇤ q ✏; for k = q + 1, . . .",3.2. Proof of Theorem 1,[0],[0]
",K, y⇤ k",3.2. Proof of Theorem 1,[0],[0]
":= x ⇤ k + ✏ k where ✏
k s are defined recursively as: ✏ q+1 = ✏ K r , and
✏ q+1+u =",3.2. Proof of Theorem 1,[0],[0]
"✏q+u K r u+ 1 K r u , 1  u  K q 1.
",3.2. Proof of Theorem 1,[0],[0]
Claim 1.,3.2. Proof of Theorem 1,[0],[0]
"a) The new solution y⇤ 0; b) All of the constraints in (2) are still feasible for y⇤.
",3.2. Proof of Theorem 1,[0],[0]
"After that the change of the LP objective is,
LP = ✏+ ✏ q+1 + ✏q+2 + . .",3.2. Proof of Theorem 1,[0],[0]
.+,3.2. Proof of Theorem 1,[0],[0]
"✏K .
",3.2. Proof of Theorem 1,[0],[0]
"One can prove that the LP objective decreases:
Claim 2.",3.2. Proof of Theorem 1,[0],[0]
"For all K 1, 1  r  q < K, it holds that
LP  0, 8 2 (0, 1].",3.2. Proof of Theorem 1,[0],[0]
Equality is achieved when r = q and = 1.,3.2. Proof of Theorem 1,[0],[0]
"Therefore we reach the contradiction that x⇤ is an optimal solution of the constructed LP.
",3.2. Proof of Theorem 1,[0],[0]
"Given Lemma 2, we prove in the following Lemma, which states that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \ SK = ;.",3.2. Proof of Theorem 1,[0],[0]
Lemma 3.,3.2. Proof of Theorem 1,[0],[0]
"For all {l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} ✓ SK , it holds that
R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) R(;) = 1 ↵
 1 ⇣ K ↵
K
⌘ K
.
",3.2. Proof of Theorem 1,[0],[0]
"So the greedy solution has objective F (SK) 1 ↵  1 ⇣ K ↵ K ⌘ K F (⌦ ⇤ ) 1 ↵ (1 e ↵ )F (⌦⇤).
3.3.",3.2. Proof of Theorem 1,[0],[0]
"Tightness Result
We demonstrate that the approximation guarantee in Theorem 1 is tight, i.e., for every submodularity ratio and every curvature ↵, there exist set functions that achieve the bound exactly.
",3.2. Proof of Theorem 1,[0],[0]
"Assume the ground set V contains the elements in S :=
{j1, . . .",3.2. Proof of Theorem 1,[0],[0]
", jK}",3.2. Proof of Theorem 1,[0],[0]
"and the elements in ⌦ := {!1, . . .",3.2. Proof of Theorem 1,[0],[0]
",!K} (S \ ⌦ = ;) and n 2K dummy elements.",3.2. Proof of Theorem 1,[0],[0]
"The objective function we are going to construct will not depend on these dummy elements, i.e., the objective value of a set does not change if dummy elements are removed from or added to that set.",3.2. Proof of Theorem 1,[0],[0]
"Consequently, the dummy elements will not affect the submodularity ratio and the curvature.",3.2. Proof of Theorem 1,[0],[0]
"For the constants ↵ 2 [0, 1], 2 (0, 1], we define the objective function as,
F (T ) := f(|⌦ \ T |)
K
1 ↵
X
i:ji2S\T
⇠i
+
X
i:ji2S\T
⇠i, (3)
where ⇠ i : = 1 K
⇣ K ↵
K
⌘ i 1
, i 2",3.2. Proof of Theorem 1,[0],[0]
"[K]; f(x) = 1 1 K 1 x 2 +
K 1 K 1 x. Note that f(x) is convex nondecreasing over [0,K], and that f(0)",3.2. Proof of Theorem 1,[0],[0]
"= 0, f(1) = 1, f(K) = K/ .",3.2. Proof of Theorem 1,[0],[0]
It is clear that F (;) = 0 and F (·) is monotone nondecreasing.,3.2. Proof of Theorem 1,[0],[0]
"The following lemma shows that it is generally nonsubmodular and non-supermodular.
",3.2. Proof of Theorem 1,[0],[0]
Lemma 4.,3.2. Proof of Theorem 1,[0],[0]
"For the objective in (3): a) When ↵ = 0, it is supermodular; b) When = 1, it is submodular; c) F (T ) has submodularity ratio and curvature ↵.
",3.2. Proof of Theorem 1,[0],[0]
"Considering the problem of max|T |K F (T ), we claim that the GREEDY algorithm may output S. This can be proved by induction.",3.2. Proof of Theorem 1,[0],[0]
"One can see that ⇢
j1(;) =",3.2. Proof of Theorem 1,[0],[0]
"⇠1 = ⇢
!1(;), so GREEDY can choose j1 in the first step.",3.2. Proof of Theorem 1,[0],[0]
"Assume in step t 1 GREEDY has chosen St 1 = {j1, . . .",3.2. Proof of Theorem 1,[0],[0]
", jt 1}, one can verify that the marginal gains coincide, i.e., ⇢
jt(S t 1 )",3.2. Proof of Theorem 1,[0],[0]
= ⇠ t = ⇢ !,3.2. Proof of Theorem 1,[0],[0]
t(S t 1 ).,3.2. Proof of Theorem 1,[0],[0]
"However, the optimal solution is actually ⌦ with function value as F (⌦) = 1 .",3.2. Proof of Theorem 1,[0],[0]
"So the
approximation ratio is F (S) F (⌦) = 1 ↵
 1 ⇣ K ↵
K
⌘ K
, which
matches our approximation guarantee in Theorem 1.",3.2. Proof of Theorem 1,[0],[0]
We consider several important real-world applications and their corresponding objective functions.,4. Applications,[0],[0]
"We show that the submodularity ratio and the curvature of these functions can be bounded and, hence, the approximation guarantees from our theoretical results are applicable.",4. Applications,[0],[0]
All the omitted proofs are provided in Appendix D.,4. Applications,[0],[0]
"In Bayesian experimental design (Chaloner & Verdinelli, 1995), the goal is to select a set of experiments to perform s.t.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"some statistical criterion is optimized, e.g., the variance of certain parameter estimates is minimized.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Krause et al. (2008) investigated several criteria for this purpose, amongst others the Bayesian A-optimality criterion.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
This criterion is used to maximally reduce the variance in the posterior distribution over the parameters.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"In general, the criterion is not submodular as shown in Krause et al. (2008, Section 8.4).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Formally, assume there are n experimental stimuli {x1, . . .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
",xn}, each xi 2 Rd, which constitute the data matrix X 2 Rd⇥n. Let us arrange a set S ✓ V of stimuli as a matrix X
S
:
=",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"[x v1 , . . .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
",xvs ] 2 Rd⇥|S|.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Let ✓ 2 Rd be the parameter vector in the linear model y
S = X> S ✓ +w, where w is the Gaussian noise with zero mean and variance
2, i.e., w ⇠ N (0, 2I), and y S is the vector of dependent variables.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Suppose the prior takes the form of an isotropic Gaussian, i.e., ✓ ⇠ N (0,⇤ 1),⇤ = 2I. Then,  y
S
✓
⇠ N (0,⌃),⌃ =

2I+X> S ⇤ 1X S X> S ⇤ 1
⇤ 1X S
⇤ 1
.
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
This implies that ⌃,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
✓|yS =,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
(⇤ + 2X S X> S ) 1.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The Aoptimality objective is defined as,
F
A
(S)
: = tr(⌃ ✓ ) tr(⌃ ✓|yS ) (4) = tr(⇤ 1) tr((⇤+ 2X S X> S ) 1 ).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The following Proposition gives bounds on the submodularity ratio and curvature of (4).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
Proposition 1.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Assume normalized stimuli, i.e., kx i k = 1, 8i 2 V .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
Let the spectral norm of X be kXk.4,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Then, a) The objective in (4) is monotone nondecreasing.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
b),4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Its submodularity ratio can be lower bounded by 2
kXk2( 2+ 2kXk2) , and its curvature ↵ can be upper
bounded by 1 2
kXk2( 2+ 2kXk2) .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The determinantal function of a square submatrix is widely used in many areas, e.g., in determinantal point processes (Kulesza & Taskar, 2012) and active set selection for sparse Gaussian processes.",4.2. The Determinantal Function,[0],[0]
Monotone nondecreasing determinantal functions appear in the second problem.,4.2. The Determinantal Function,[0],[0]
Assume ⌃ is the covariance matrix parameterized by a positive definite kernel.,4.2. The Determinantal Function,[0],[0]
"In the Informative Vector Machine (Lawrence et al., 2003), the information gain of a subset of points S ✓ V is 1 2 logF (S), where
F (S)
:",4.2. The Determinantal Function,[0],[0]
"= det(I+ 2⌃ S ), (5)
where is the noise variance in the Gaussian process model, ⌃
S is the square submatrix with both its rows and columns indexed by S. Although logF (S) is submodular, F (S) is in general not submodular.",4.2. The Determinantal Function,[0],[0]
The approximation guarantee of GREEDY for maximizing logF (S) does not translate to a guarantee for maximizing F (S).,4.2. The Determinantal Function,[0],[0]
"The following Proposition characterizes (5).
",4.2. The Determinantal Function,[0],[0]
Proposition 2.,4.2. The Determinantal Function,[0],[0]
"a) F (S) in (5) is supermodular, its curvature is 0; b) Let the eigenvalues of A := I + 2⌃ be 1 · · · n >",4.2. The Determinantal Function,[0],[0]
1.,4.2. The Determinantal Function,[0],[0]
"The greedy submodularity ratio of F (S) can be lower bounded by K( n 1)
",4.2. The Determinantal Function,[0],[0]
( QK j=1 j) 1 .,4.2. The Determinantal Function,[0],[0]
LPs with combinatorial constraints appear frequently in practice.,4.3. LPs with Combinatorial Constraints,[0],[0]
Consider the following example: Suppose that V is the set of all products a company can produce.,4.3. LPs with Combinatorial Constraints,[0],[0]
"Given budget constraints on the raw materials needed, companies consider the LP max
x2Phd,xi, where d is the vector of profits for the individual products and where P is a polytope representing the continuous constraints.",4.3. LPs with Combinatorial Constraints,[0],[0]
The above LP can be used to assess the profit maximizing production plan.,4.3. LPs with Combinatorial Constraints,[0],[0]
Usually the company needs to consider combinatorial constraints as well.,4.3. LPs with Combinatorial Constraints,[0],[0]
"For instance, the company has at most K production lines, thus they have to select a subset of K products to produce.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Often this kind of problems can be formalized as max
x2P,supp(x)2Ihd,xi, where I is the independent set of the combinatorial structure.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Hence, a natural auxiliary set function is,
F (S) := maxsupp(x)✓S, x2Phd,xi, 8S ✓ V. (6) 4By",4.3. LPs with Combinatorial Constraints,[0],[0]
"Weyl’s inequality, a naive upper bound is kXk  p n.
Let P = {x 2",4.3. LPs with Combinatorial Constraints,[0],[0]
"Rn | 0  x  ¯u,Ax  b, ¯u 2 Rn+,A 2 Rm⇥n+ , b 2 Rm+}.",4.3. LPs with Combinatorial Constraints,[0],[0]
In general F (S) in (6) is non-submodular as illustrated by two examples in Appendix D.3.,4.3. LPs with Combinatorial Constraints,[0],[0]
"Upper bounding the curvature is equivalent to lower bounding F (S[⌦) F (S\{i}[⌦)
F (S) F (S\{i}) , which can be 0 in the worst case.",4.3. LPs with Combinatorial Constraints,[0],[0]
"However, the submodularity ratio can be lower bounded by a non-zero scalar.
",4.3. LPs with Combinatorial Constraints,[0],[0]
Proposition 3.,4.3. LPs with Combinatorial Constraints,[0],[0]
a) F (S) in (6) is a normalized nondecreasing set function.,4.3. LPs with Combinatorial Constraints,[0],[0]
b),4.3. LPs with Combinatorial Constraints,[0],[0]
"With regular non-degenerancy assumptions (details in Appendix D.3.2), its submodularity ratio can be lower bounded by 0 > 0.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Many real-world applications can benefit from the theory in this work, for instance: subset selection using the R2 objective, sparse modeling and the budget allocation problem with combinatorial constraints.",4.4. More Applications,[0],[0]
Details on these applications are deferred to Appendix G.,4.4. More Applications,[0],[0]
We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for several applications.,5. Experimental Results,[0],[0]
"Since it is too time consuming to calculate the full versions of ↵ and using exhaustive search, we only calculated the greedy versions (↵G, G).",5. Experimental Results,[0],[0]
All averaged results are from 20 repeated experiments.,5. Experimental Results,[0],[0]
Source code is available at https://github.com/bianan/ non-submodular-max.5 More results are put in Appendix H.,5. Experimental Results,[0],[0]
We considered the Bayesian A-optimality objective for both synthetic and real-world data.,5.1. Bayesian Experimental Design,[0],[0]
"In all experiments, we normalized the data points to have unit `2-norm.
",5.1. Bayesian Experimental Design,[0],[0]
Real-world results: We used the Boston Housing Data.,5.1. Bayesian Experimental Design,[0],[0]
5All experiments were implemented using Matlab.,5.1. Bayesian Experimental Design,[0],[0]
"We used the SDP solver provided by CVX (Version 2.1).
",5.1. Bayesian Experimental Design,[0],[0]
"The dataset6 has 14 features (e.g., crime rate, property tax rates, etc.) and 516 samples.",5.1. Bayesian Experimental Design,[0],[0]
"To be able to quickly calculate the parameters and optimal solution by exhaustive search, the first n = 14 samples were used.",5.1. Bayesian Experimental Design,[0],[0]
"As a baseline, we used an SDP-based algorithm (abbreviated as SDP, details are available in Appendix E).",5.1. Bayesian Experimental Design,[0],[0]
Results are shown in Fig. 2 for varying values of K. In Fig.,5.1. Bayesian Experimental Design,[0],[0]
2a we can observe that both GREEDY and SDP compute near-optimal solutions.,5.1. Bayesian Experimental Design,[0],[0]
From Fig.,5.1. Bayesian Experimental Design,[0],[0]
"2b we can see that the greedy submodularity ratio G is close to 1, and that the greedy curvature ↵G is less than 1, while the classical curvature ↵total is always 1 (the worstcase value).",5.1. Bayesian Experimental Design,[0],[0]
"This implies that the classical total curvature ↵
total characterizes the considered maximization problems less accurate than the greedy curvature.
",5.1. Bayesian Experimental Design,[0],[0]
Synthetic results: We generated random observations from a multivariate Gaussian distribution with different correlations.,5.1. Bayesian Experimental Design,[0],[0]
"To be able to assess the ground truth, we used n = 12 samples with d = 6 features.",5.1. Bayesian Experimental Design,[0],[0]
"Fig. 3 shows the results with correlation 0.2 (first column) and 0.6 (second column), respectively: The first row shows the average objective values over the optimal value with error bars, and the second row shows the parameters.",5.1. Bayesian Experimental Design,[0],[0]
One can observe that GREEDY always obtains near-optimal solutions and that these solutions are roughly comparable with those obtained by the SDP.,5.1. Bayesian Experimental Design,[0],[0]
"The classical curvature ↵total is always close to 1, while ↵G take smaller values, and G takes values close to 1, thus characterize the performance of GREEDY better.
",5.1. Bayesian Experimental Design,[0],[0]
"Medium-scale synthetic experiments: To compare the runtime of SDP and GREEDY, we considered mediumscale datasets (we cannot report results on larger datasets because of the huge computational demands of the SDP).
",5.1. Bayesian Experimental Design,[0],[0]
"6https://archive.ics.uci.edu/ml/datasets/ Housing
Fig. 4 shows the objective value achieved by GREEDY and SDP for different numbers of features d and numbers of samples n, as well as the correlations.",5.1. Bayesian Experimental Design,[0],[0]
We can observe that GREEDY computes solutions that are on par or superior to those of SDP.,5.1. Bayesian Experimental Design,[0],[0]
"In Table 1 we summarize the runtime of GREEDY and SDP for different values of d and n, for correlation 0.5.",5.1. Bayesian Experimental Design,[0],[0]
"Furthermore, we show the ratio of runtimes of the two algorithms.",5.1. Bayesian Experimental Design,[0],[0]
We can observe that GREEDY is usually two orders of magnitude faster than SDP.,5.1. Bayesian Experimental Design,[0],[0]
We generated synthetic LPs as follows:,5.2. LPs with Combinatorial Constraints,[0],[0]
"Firstly, we generated the matrix A 2 Rm⇥n+ , Aij 2 [0, 1] by drawing all entries independently from a uniform distribution on
[0, 1].",5.2. LPs with Combinatorial Constraints,[0],[0]
"We set b = d = 1, and set ¯u as 1.",5.2. LPs with Combinatorial Constraints,[0],[0]
The first row of Fig. 5 plots the optimal LP objective (calculated using exhaustive search) and the LP objective returned by GREEDY.,5.2. LPs with Combinatorial Constraints,[0],[0]
The second row shows the curvature and submodularity ratio.,5.2. LPs with Combinatorial Constraints,[0],[0]
"The first column (Fig. 5a) presents the results for n = 6,m = 20, while the second column (Fig. 5b) presents that for n = 8,m = 30.",5.2. LPs with Combinatorial Constraints,[0],[0]
"Note the greedy submodularity ratio takes values between ⇠ 0.15 and 1, and that the curvature is close to the worst-case value of 1.",5.2. LPs with Combinatorial Constraints,[0],[0]
These observations are consistent with the theory in Section 4.3.,5.2. LPs with Combinatorial Constraints,[0],[0]
"We experimented with synthetic and real-world data: For synthetic data, we generated random covariance matrices ⌃ 2 Rn⇥n with uniformly distributed eigenvalues in [0, 1].",5.3. Determinantal Functions Maximization,[0],[0]
"We set n = 10, = 2.",5.3. Determinantal Functions Maximization,[0],[0]
In Fig. 6 (left) we plot the optimal determinantal objective value and the value achieved by GREEDY.,5.3. Determinantal Functions Maximization,[0],[0]
Fig. 6,5.3. Determinantal Functions Maximization,[0],[0]
"(right) traces the greedy submodularity ratio G. Since the determinantal objective is supermodular, so the approximation guarantee equals to G. We can see that G can reasonably predict the performance of GREEDY.
",5.3. Determinantal Functions Maximization,[0],[0]
"For real-world data, we considered an active set selection task on the CIFAR-107 dataset.",5.3. Determinantal Functions Maximization,[0],[0]
"The first n = 12 images in the test set were used to calculate the covariance matrix with an squared exponential kernel (k(x
i
,x
j
) =
exp( kx i x j k2/h2), h was set to be 1).",5.3. Determinantal Functions Maximization,[0],[0]
"The results in Fig. 7 shows similar results as with the synthetic data.
7https://www.cs.toronto.edu/˜kriz/cifar.",5.3. Determinantal Functions Maximization,[0],[0]
html,5.3. Determinantal Functions Maximization,[0],[0]
"In this section we briefly discuss related work on various notions of non-submodularity and the optimization of nonsubmodular functions (Further details in Appendix F).
",6. Related Work,[0],[0]
Relation to Conforti & Cornuéjols (1984) in deriving approximation guarantees.,6. Related Work,[0],[0]
"In proving Theorem 1 we use the similar proof framework (i.e., utilizing LP formulations to analyze the worst-case approximation ratios of different groups of problem instances) as that in Conforti & Cornuéjols (1984), where they derive guarantees for maximizing submodular functions.",6. Related Work,[0],[0]
"However, since we are proving guarantees for non-submodular functions, the specific techniques on how to manipulate these LPs are different.",6. Related Work,[0],[0]
"Specifically, 1)",6. Related Work,[0],[0]
The building block to construct LPs (Lemma 1) is different;,6. Related Work,[0],[0]
"2) The technique to prove the structure of the LPs (which corresponds to Lemma 2) is significantly different for a submodular function and a nonsubmodular function, and Lemma 2 is the key to investigate the worst-case approximation ratios of different groups of problem instances.",6. Related Work,[0],[0]
3),6. Related Work,[0],[0]
"The specific way to prove Lemma 3 is also different since the constraints of the LPs are different for submodular and non-submodular functions.
",6. Related Work,[0],[0]
Submodularity ratio and curvature.,6. Related Work,[0],[0]
Curvature is typically defined for submodular functions.,6. Related Work,[0],[0]
Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions.,6. Related Work,[0],[0]
Appendix C provides details of that notion and relates it to our definition.,6. Related Work,[0],[0]
Yoshida (2016) prove an improved approximation ratio for knapsack-constrained maximization of submodular functions with bounded curvature.,6. Related Work,[0],[0]
"Submodularity ratio (Das & Kempe, 2011) is a quantity characterizing how close a function is to being submodular.
",6. Related Work,[0],[0]
Approximate submodularity.,6. Related Work,[0],[0]
"Krause et al. (2008) define approximately submodular functions with parameter ✏ 0 as those functions F that satisfy an approximate diminishing returns property, i.e., 8A ✓ B ✓ V \ v it holds that ⇢
v (A) ⇢",6. Related Work,[0],[0]
v (B) ✏.,6. Related Work,[0],[0]
GREEDY yields a solution with objective F (SK) (1 e 1)F,6. Related Work,[0],[0]
"(⌦⇤) K✏, for maximizing a monotone F s.t.",6. Related Work,[0],[0]
a K-cardinality constraint.,6. Related Work,[0],[0]
Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity.,6. Related Work,[0],[0]
"Restricted submodularity refers to functions which are submodular only over some collection of subsets of V , and shifted submodularity can be viewed as a special case of the approximate diminishing returns as defined above.",6. Related Work,[0],[0]
"Recently, Horel & Singer (2016) study ✏-approximately submodular functions, which arised from their research on “noisy” submodular functions.",6. Related Work,[0],[0]
A function F (·) is ✏-approximately submodular if there exists a submodular function G s.t. (1 ✏)G(S)  ,6. Related Work,[0],[0]
"F (S)  (1 + ✏)G(S), 8S ✓ V .
",6. Related Work,[0],[0]
Weak submodularity.,6. Related Work,[0],[0]
"Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) |S \T |F (S [T )+ |S",6. Related Work,[0],[0]
[T |F (S \T ).,6. Related Work,[0],[0]
"For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i)",6. Related Work,[0],[0]
F (·) is weakly submodular; ii),6. Related Work,[0],[0]
"The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1.
",6. Related Work,[0],[0]
Other notions of non-submodularity.,6. Related Work,[0],[0]
Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions.,6. Related Work,[0],[0]
They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree.,6. Related Work,[0],[0]
"Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.
",6. Related Work,[0],[0]
Optimization of non-submodular functions.,6. Related Work,[0],[0]
"The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012).",6. Related Work,[0],[0]
"Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in general, and propose efficient algorithms for optimization.",6. Related Work,[0],[0]
Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function.,6. Related Work,[0],[0]
"Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation factor.",6. Related Work,[0],[0]
We analyzed the guarantees for greedy maximization of non-submodular nondecreasing set functions.,7. Conclusion,[0],[0]
"By combining the (generalized) curvature ↵ and submodularity ratio for generic set functions, we prove the first tight approximation bounds in terms of these definitions for greedily maximizing nondecreasing set functions.",7. Conclusion,[0],[0]
These approximation bounds significantly enlarge the domain where GREEDY has guarantees.,7. Conclusion,[0],[0]
"Furthermore, we theoretically bounded the parameters ↵ and for several non-trivial applications, and validate our theory in various experiments.",7. Conclusion,[0],[0]
"The authors would like to thank Adish Singla, Kfir Y. Levy and Aurelien Lucchi for valuable discussions.",ACKNOWLEDGEMENTS,[0],[0]
This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems.,ACKNOWLEDGEMENTS,[0],[0]
This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.,ACKNOWLEDGEMENTS,[0],[0]
We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions.,abstractText,[0],[0]
"While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones.",abstractText,[0],[0]
"However, GREEDY enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design.",abstractText,[0],[0]
We prove theoretical guarantees supporting the empirical performance.,abstractText,[0],[0]
Our guarantees are characterized by a combination of the (generalized) curvature ↵ and the submodularity ratio .,abstractText,[0],[0]
"In particular, we prove that GREEDY enjoys a tight approximation guarantee of 1 ↵ (1 e ↵) for cardinality constrained maximization.",abstractText,[0],[0]
"In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints.",abstractText,[0],[0]
We experimentally validate our theoretical findings for both synthetic and real-world applications.,abstractText,[0],[0]
Guarantees for Greedy Maximization of Non-submodular Functions with Applications,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1308–1317 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1308",text,[0],[0]
"Acronyms are abbreviations formed from the initial components of words or phrases (e.g., “AI” from “Artificial Intelligence”).",1 Introduction,[0],[0]
"As acronyms can shorten long names and make communications
∗Work done while authors were at Microsoft Research.
more efficient, they are widely used at almost everywhere in enterprises, including notifications, emails, reports and social network posts.",1 Introduction,[0],[0]
Figure 1 shows a sample enterprise social network post.,1 Introduction,[0],[0]
"As we can see, acronyms are frequently used there.
",1 Introduction,[0],[0]
"The enterprise acronym disambiguation task is challenging due to the high ambiguity of acronyms, e.g., “SP” could stand for “Service Pack”, “SharePoint” or “Surface Pro” in Microsoft.",1 Introduction,[0],[0]
"And there is one additional challenge compared with previous disambiguation tasks: in an enterprise document, an acronym could refer
to either an internal meaning (concepts created by the enterprise that may or may not be found outside) or an external meaning (all concepts that are not internal).",1 Introduction,[0],[0]
"For example, regarding the acronym “AI”, “Asset Intelligence” is an internal meaning mainly used only in Microsoft, while “Artificial Intelligence” is an external meaning widely used in public.",1 Introduction,[0],[0]
A good acronym disambiguation system should be able to handle both internal and external meanings.,1 Introduction,[0],[0]
"As we will explain in details, it is important to make such distinction and different strategies are needed for such two cases.
",1 Introduction,[0],[0]
"For internal meanings, there are some previous work on word sense disambiguation (Navigli, 2009) and acronym disambiguation (Feng et al., 2009; Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) on a closed-domain corpus.",1 Introduction,[0],[0]
"The main challenge here is that there are rarely any domain-specific knowledge bases available in enterprises, therefore all the signals for disambiguation (including potential meanings, and their popularity scores, context representations, etc.) need to be mined from plain text.",1 Introduction,[0],[0]
Training data should also be automatically generated to make the system easily scale out to all enterprises.,1 Introduction,[0],[0]
"Compared with previous work, we developed a more comprehensive and advanced set of features in the disambiguation model, and also used a much less restrictive way to discover meaning candidates and training data, so that both precision and recall can be improved.",1 Introduction,[0],[0]
"Moreover, one main limitation of all previous work is that they do not distinguish internal and external meanings.",1 Introduction,[0],[0]
"They merely rely on the enterprise corpus to discover information about external meanings, which we observe is quite ineffective.",1 Introduction,[0],[0]
"The reason is that for popular external meaning like “Artificial Intelligence”, people often directly use its acronym in enterprises without explanation, therefore there is limited information about the connection between the acronym and the external meaning in the enterprise corpus.",1 Introduction,[0],[0]
"On the other hand, there are much more such information available in the public domain, which should be leveraged by the system.
",1 Introduction,[0],[0]
"If we consider utilizing a public knowledge base such as Wikipedia to better handle external meanings of acronyms, the problem becomes very related to the well studied Entity Linking (Ji and Grishman, 2011; Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., 2011; Li et al., 2013, 2016; Ratinov et al., 2011; Shen et al., 2012) prob-
lem, which is to map entity mentions in texts to their corresponding entities in a reference knowledge base (e.g. Wikipedia).",1 Introduction,[0],[0]
"But our disambiguation task is different from the entity linking task, because the system also needs to handle internal meanings which are not covered by any knowledge bases, and ultimately needs to decide whether an acronym refers to an internal meaning or an external meaning.",1 Introduction,[0],[0]
It is nontrivial to combine the information mined from the enterprise corpus and the public knowledge base so that the system can get the best of both worlds.,1 Introduction,[0],[0]
"For instance, we have tried to run an internal disambiguator (leveraging information mined from enterprise corpus) and then resort to a public entity linking system if the internal one’s confidence is low, but the performance is very poor.",1 Introduction,[0],[0]
"Even for external meanings, it is important to leverage signals from the enterprise corpus since the context surrounding them could be quite different from that in the external world, and context is one of the most important factor for disambiguation.",1 Introduction,[0],[0]
"For example, in public world, when people mention “Operating System” they mainly talk about how to install or use it; while within Microsoft, when people mention “Operating System” most of the time they focus on how to design or implement it.
",1 Introduction,[0],[0]
"In this work, we design a novel, end-to-end framework to address all the above challenges.",1 Introduction,[0],[0]
Our framework takes the enterprise corpus and certain public knowledge base as input and produces a high-quality acronym disambiguation system as output.,1 Introduction,[0],[0]
"The models are all trained via distant supervised learning, therefore our system requires no manually labeled training examples and can be easily deployed to any enterprises.",1 Introduction,[0],[0]
The Enterprise Acronym Disambiguation problem is comprised of two sub-problems.,2 Problem Statement,[0],[0]
"The first one is Acronym Meaning Mining (Adar, 2004; Ao and Takagi, 2005; Park and Byrd, 2001; Schwartz and Hearst, 2002; Jain et al., 2007; Larkey et al., 2000; Nadeau and Turney, 2005; Taneva et al., 2013), which aims at mining acronym/meaning pairs from the enterprise corpus.",2 Problem Statement,[0],[0]
"Each meaning m should contain the full name expansion e, popularity score p (indicating how often m is used as the genuine meaning of acronym a) and context words W (i.e. words frequently used in context of the meaning).",2 Problem Statement,[0],[0]
"The popularity score and
context words can provide critical information for making disambiguation decisions.",2 Problem Statement,[0],[0]
"The second one is Meaning Candidate Ranking, whose goal is to rank the candidate meanings associated with the target acronym a and select the genuine meaning m based on the given context.
",2 Problem Statement,[0],[0]
"In this paper we assume the acronyms for disambiguation are provided as input to the system, either by the user or by an existing acronym detection module.",2 Problem Statement,[0],[0]
"We do not try to optimize the performance of acronym detection (e.g. identifying acronyms beyond the simple capitalized rule, or distinguishing cases where a capitalized term is not an acronym but a regular English word, such as “OK”).",2 Problem Statement,[0],[0]
The task of acronym detection is also interesting and important.,2 Problem Statement,[0],[0]
"But due to the space limit, it is beyond the scope of this paper.",2 Problem Statement,[0],[0]
We propose a novel end-to-end framework to solve the Enterprise Acronym Disambiguation problem.,3 Framework,[0],[0]
Our framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,3 Framework,[0],[0]
Figure 2 shows the details of our proposed framework.,3 Framework,[0],[0]
"In the mining module, we will sequentially perform Candidates Generation, Popularity Calculation, Candidates Deduplication and Context Harvesting on the input enterprise corpus.",3 Framework,[0],[0]
The details of these steps will be discussed in Section 4.,3 Framework,[0],[0]
"After mining steps, we will get an acronym/meaning repository storing all the mined acronym/meaning pairs.",3 Framework,[0],[0]
"Feed this repository together with the training data (automatically generated via distant supervision from the enterprise corpus) to the training module, we will get a candidate ranking model, a confidence estimation model and a final selection model.",3 Framework,[0],[0]
These models form the final acronym disambiguator and will be used in the testing module for actual acronym disambiguation.,3 Framework,[0],[0]
"In the testing module, given the target acronym along with some context as input, the system will output the predicted meaning.",3 Framework,[0],[0]
"Note that the mining and training module run offline once for the entire corpus or periodically when the corpus update, while the testing can be run online repeatedly for processing new documents.",3 Framework,[0],[0]
"As there is no reference dictionary or knowledge base available in enterprise telling us the potential
meanings of acronyms, we have to extract them from plain text.",4.1 Candidates Generation,[0],[0]
We propose a strategy called Hybrid Generation to balance extraction accuracy and coverage.,4.1 Candidates Generation,[0],[0]
"Namely, we treat a phrase as a meaning candidate for an acronym if: (1) the initial letters of the phrase match the acronym and the phrase and the acronym co-occur in at least one document in the enterprise corpus; or (2) it is a valid candidate for the acronym in public knowledge bases (e.g. Wikipedia).",4.1 Candidates Generation,[0],[0]
The insight of this strategy is that the valid candidates missed by condition (1) are mainly public meanings which can be found in public knowledge bases.,4.1 Candidates Generation,[0],[0]
With this strategy we can make our system understand both the internal world and the external world.,4.1 Candidates Generation,[0],[0]
"As mentioned in Section 2, for each candidate meaning, we need to calculate its popularity score, which reveals how often the candidate meaning is used as the genuine meaning of the acronym.",4.2 Popularity Calculation,[0],[0]
"In previous research on Entity Linking, popularity is calculated as the fraction of times a candidate being the target page for an anchor text in a reference knowledge base (e.g. Wikipedia).",4.2 Popularity Calculation,[0],[0]
"However, in enterprises, we do not have a knowledge base with anchor links.",4.2 Popularity Calculation,[0],[0]
Thus we cannot calculate popularity in the same way.,4.2 Popularity Calculation,[0],[0]
"Here we propose to calculate two types of popularity to mimic the effect.
1.",4.2 Popularity Calculation,[0],[0]
Marginal Popularity.,4.2 Popularity Calculation,[0],[0]
"MP (mi) = Count(mi)∑n j=1Count(mj) , (1)
where m1, m2, . . .",4.2 Popularity Calculation,[0],[0]
", mn are the meaning candidates of acronym a and Count(mi) is the number of occurrences for mi in the corpus.
2.",4.2 Popularity Calculation,[0],[0]
"Conditional Popularity.
",4.2 Popularity Calculation,[0],[0]
"CP (mi) = Count(mi, a)∑n j=1Count(mj , a) , (2) where m1, m2, . .",4.2 Popularity Calculation,[0],[0]
"., mn are the meaning candidates of acronym a and Count(mi, a) is the number of document-level cooccurrences for mi and a in the corpus.
",4.2 Popularity Calculation,[0],[0]
Conditional Popularity can more reasonably reveal how often the acronym is used to represent each meaning candidate.,4.2 Popularity Calculation,[0],[0]
"However, due to the data sparsity issue in enterprises, many valid candidates may get zero value for conditional popularity since they may never co-occur with the acronyms in the enterprise corpus.",4.2 Popularity Calculation,[0],[0]
The Marginal Popularity does not have this problem since it is calculated from the raw counts of the candidates.,4.2 Popularity Calculation,[0],[0]
"Yet on the other hand, high marginal popularity score does not necessarily indicate high correlation between the candidate and the acronym.",4.2 Popularity Calculation,[0],[0]
"It is unclear how to combine the two scores into one popularity score, so we use both of them as features in the disambiguation model.",4.2 Popularity Calculation,[0],[0]
"In enterprises, people often create many variants (including abbreviations, plurals or even misspellings) for the same meaning, therefore many mined meaning candidates are actually equivalent.",4.3 Candidates Deduplication,[0],[0]
"For example, for the meaning “Certificate Authority” of the acronym “CA”, the variants include “Cert Auth”, “Certificate Authorities” and many others.",4.3 Candidates Deduplication,[0],[0]
It is important to deduplicate these variants before sending them to the disambiguation module.,4.3 Candidates Deduplication,[0],[0]
The deduplication helps aggregate disambiguation evidences and reduce noises.,4.3 Candidates Deduplication,[0],[0]
We design several heuristic rules1 to perform the deduplication.,4.3 Candidates Deduplication,[0],[0]
Experiments show that the rules can accurately group the variants together.,4.3 Candidates Deduplication,[0],[0]
"After grouping, we sort the variants within the same group based on their marginal popularity.",4.3 Candidates Deduplication,[0],[0]
The candidate with the largest marginal popularity is selected as the canonical candidate for the group.,4.3 Candidates Deduplication,[0],[0]
Other variants in the group will be deleted from the candidate list and their popularity scores will be aggregated to the canonical candidate.,4.3 Candidates Deduplication,[0],[0]
We maintain a table to record the variants for each canonical candidate.,4.3 Candidates Deduplication,[0],[0]
"In this step, we aim to harvest context words for each meaning candidate.",4.4 Context Harvesting,[0],[0]
These context words could be used to calculate context similarity with the query context.,4.4 Context Harvesting,[0],[0]
"For each meaning candidate m, we put its canonical form and all its variants (from the variants table in Section 4.3) into set S. Then we scan the enterprise corpus, each time we find a match of any e ∈ S, we harvest the words in a
1Due to space limitations, the detailed rules are omitted.",4.4 Context Harvesting,[0],[0]
"Example rules are “word overlap percentage after stemming > 0.8”, “corresponding component words share same prefix”.
width-W word window surrounding e as the context words of m. In our experiments we set window size as 30 after trying to vary the window size from 10 to 50 and finding 30 gives the best result.
",4.4 Context Harvesting,[0],[0]
"As mentioned before, some popular public meanings might be mentioned very rarely by their full names in the enterprise corpus since people directly use their acronyms most of the time.",4.4 Context Harvesting,[0],[0]
"Therefore, the above context harvesting process can only get very few context words for those public meanings.",4.4 Context Harvesting,[0],[0]
"To alleviate this, for each public meaning we add its Wikipedia page’s content as complementary context.",4.4 Context Harvesting,[0],[0]
"By doing so, we ensure almost all valid candidates get a reasonable amount of context words.",4.4 Context Harvesting,[0],[0]
We first train a candidate ranking model to rank candidates with respect to the likelihood of being the genuine meaning for the target acronym.,5.1 Candidate Ranking,[0],[0]
"In order to train a robust ranking model, we need to get adequate amount of labeled training data.",5.1.1 Training Data Generation,[0],[0]
"Manually labeling is obviously too expensive and it requires a lot of domain knowledge, which severely limits our framework’s generalization capability.",5.1.1 Training Data Generation,[0],[0]
"To tackle this problem, we propose to automatically generate training data via distant supervision.",5.1.1 Training Data Generation,[0],[0]
"The intuition is that since acronyms and the corresponding meanings are semantically equivalent, people use them interchangeably in enterprises.",5.1.1 Training Data Generation,[0],[0]
"Therefore we can fetch documents containing the meaning, replace the meaning with the corresponding acronym and treat the meaning as ground truth.",5.1.1 Training Data Generation,[0],[0]
Figure 3 shows an example of this automatic training data generation process.,5.1.1 Training Data Generation,[0],[0]
Any learning-to-rank algorithms can be used here.,5.1.2 Training Algorithm,[0],[0]
"In our system we utilize the LambdaMART algorithm (Burges, 2010) to train the model.",5.1.2 Training Algorithm,[0],[0]
Now we explain the features we developed for the candidate ranking model.,5.1.3 Features,[0],[0]
"First, we have the Marginal Popularity score and Conditional Popularity score as two context-independent features, which could compensate for each other.",5.1.3 Features,[0],[0]
"However, as discussed in the previous section, some popular public meanings (e.g., “Artificial Intelligence”) can be rarely mentioned in enterprise corpus by their full names, therefore both their marginal popularity score and conditional popularity score can be very low.",5.1.3 Features,[0],[0]
"To address this, we add a third feature called Wiki Popularity, which is calculated from Wikipedia anchor texts to capture how often an acronym refers to a public meaning in Wikipedia.",5.1.3 Features,[0],[0]
The fourth feature we adopt is Context Similarity.,5.1.3 Features,[0],[0]
We convert the harvested context for the meaning and the query context of the target acronym into TFIDF vectors and then compute their cosine similarity2.,5.1.3 Features,[0],[0]
"We also include two features (i.e. LeftNeighborScore and RightNeighborScore) to capture the effect of the immediate neighboring words, which are more important than further context words since immediate words could form phrases with the acronym.",5.1.3 Features,[0],[0]
"For example, if we see an acronym “SP” followed by the word “2”, then likely it stands for “Service Pack”.",5.1.3 Features,[0],[0]
"However, if we see “SP” followed by “2003”, then probably its genuine meaning is “SharePoint”.",5.1.3 Features,[0],[0]
The last feature we use is FullNamePercentage.,5.1.3 Features,[0],[0]
This feature is defined as the percentage of the meaning candidate’s component words appearing in the context of the target acronym.,5.1.3 Features,[0],[0]
Table 1 summarizes the features used to train the candidate ranking model.,5.1.3 Features,[0],[0]
"After getting the ranking results, we propose to apply a confidence estimation step to decide whether to trust the top ranked answer.",5.2 Confidence Estimation,[0],[0]
There are two motivations behind.,5.2 Confidence Estimation,[0],[0]
"First, our candidate generation approach is not perfect, therefore we could encounter cases in which the genuine meaning is not in our candidates.",5.2 Confidence Estimation,[0],[0]
"For such cases, the top ranked answer is obviously incorrect.",5.2 Confidence Estimation,[0],[0]
"Second, our training data is biased towards the internal meanings since external meanings may rarely appear with full names.
",5.2 Confidence Estimation,[0],[0]
"2One popular alternative to measure context similarity is using word embeddings (Mikolov et al., 2013; Li et al., 2015).",5.2 Confidence Estimation,[0],[0]
"In our system we experimented replacing TFIDF cosine similarity with word embedding similarity, or adding word embedding similarity as an additional feature, but both hurt the disambiguation accuracy.",5.2 Confidence Estimation,[0],[0]
"So we only included the TFIDF cosine similarity as the context similarity feature in our system.
",5.2 Confidence Estimation,[0],[0]
"As a result, the learned ranking model may lack the capability to properly rank the external meanings.",5.2 Confidence Estimation,[0],[0]
"In such cases, we would better have the system return nothing rather than directly provide a wrong answer to mislead the user.",5.2 Confidence Estimation,[0],[0]
"In this step, we train a confidence estimation model, which will estimate the top result’s confidence.",5.2 Confidence Estimation,[0],[0]
"Similar to the ranker training, here the training data is also automatically generated.",5.2.1 Training Data Generation,[0],[0]
"We run the learned ranker on some distant labeled data (generated from a different corpus), and then check if the top ranked answer is correct or not.",5.2.1 Training Data Generation,[0],[0]
"If it is correct, we generate a positive training example; otherwise we make a negative training example.",5.2.1 Training Data Generation,[0],[0]
Any classification algorithms can be used here.,5.2.2 Training Algorithm,[0],[0]
"In our system we utilize the MART boosted tree algorithm (Friedman, 2000) to train the model.",5.2.2 Training Algorithm,[0],[0]
We design 7 features (summarized in Table 2) to train the confidence estimation model.,5.2.3 Features,[0],[0]
"There are two intuitions behind: (1) If the top-ranked answer’s ranking score is very small, or the topranked answer’s score is close to the secondranked answer’s score, then the ranking is not very confident; (2) If the acronym has a dominating candidate in the public domain (e.g., “Personal Computer” is the dominating candidate for “PC”), and the candidates’ Wiki popularity distribution is significantly different from their marginal/conditional popularity distributions, then the ranker’s output is not very confident.",5.2.3 Features,[0],[0]
"The first intuition covers the first 3 features, while the second intuition covers the last 4 features.",5.2.3 Features,[0],[0]
We have discussed that one particular motivation for confidence estimation is that the candidate ranking stage has some bias so it does not always rank public meanings at top when they are correct.,5.3 Final Selection,[0],[0]
"Therefore, assuming the confidence estimation model can remove incorrect top-ranked result, we still need one additional step to decide if any public meaning is correct, which we call a final selection model.",5.3 Final Selection,[0],[0]
"In this step, we determine whether to return the most popular public meaning (based on Wiki Popularity) as the final answer, and this step is only triggered when the confidence estimator judges that the ranking result is unconfident.
",5.3 Final Selection,[0],[0]
The goal of the final selection model is similar to that of the confidence estimation model.,5.3 Final Selection,[0],[0]
"In confidence estimation, we judge whether the topranked answer is correct; while in final selection, we check whether the most popular external meaning is correct.",5.3 Final Selection,[0],[0]
"Thanks to this similarity, we can reuse the data, features and training algorithm in confidence estimation model.",5.3 Final Selection,[0],[0]
"We take the same training data in Section 5.2.1 and update the labels correspondingly: if the genuine answer is the most popular external meaning, we generate a positive example; otherwise we make a negative one.",5.3 Final Selection,[0],[0]
We use both the Microsoft Answer Corpus (MAC) and the Microsoft Yammer Corpus (MYC) as the mining corpus.,6.1.1 Mining and Training Corpus,[0],[0]
These corpus are kindly shared to us by Microsoft for research purpose.,6.1.1 Mining and Training Corpus,[0],[0]
MAC contains 0.3 million web pages from a Microsoft internal question answering forum.,6.1.1 Mining and Training Corpus,[0],[0]
MYC is consisted of 6.8 million posts from Microsoft’s Yammer social network.,6.1.1 Mining and Training Corpus,[0],[0]
"In total, our mining module harvested 5287 acronyms and 17258 meaning candidates from this joint corpus.
",6.1.1 Mining and Training Corpus,[0],[0]
"For model training, the confidence estimation model and final selection model need to be trained on a different corpus than the candidate ranking model.",6.1.1 Mining and Training Corpus,[0],[0]
"So we train the candidate ranking model
on MAC, with 12500 training examples being automatically generated; and train the confidence estimation and final selection model on MYC, with 40000 training instances being generated.",6.1.1 Mining and Training Corpus,[0],[0]
We prepared four datasets3 for evaluation purposes.,6.1.2 Evaluation Datasets,[0],[0]
The first one Manual is obtained from the recent pages of Microsoft answer forum.,6.1.2 Evaluation Datasets,[0],[0]
Note these pages are disjoint from those used as mining/training corpus.,6.1.2 Evaluation Datasets,[0],[0]
We randomly sampled 300 pages and filtered out pages which do not contain ambiguous acronyms.,6.1.2 Evaluation Datasets,[0],[0]
"After filtering, 240 test cases were left and we manually labeled them.
",6.1.2 Evaluation Datasets,[0],[0]
The second one Distant is generated via distant labeling on Microsoft Office365 documents.,6.1.2 Evaluation Datasets,[0],[0]
We sampled 2000 documents which contain at least one occurrence of a meaning candidate.,6.1.2 Evaluation Datasets,[0],[0]
Then we replaced the meanings with the corresponding acronyms and treat the meanings as ground truths.,6.1.2 Evaluation Datasets,[0],[0]
"We manually checked through this dataset to remove some bad cases (e.g., “AS” for “App Store”).",6.1.2 Evaluation Datasets,[0],[0]
"This resulted in a test set of 1949 test cases.
",6.1.2 Evaluation Datasets,[0],[0]
"Comparing the Manual dataset with the Distant dataset, the Manual one, though in smaller size, can more accurately evaluate the system performance, since the target acronyms in it are sampled from the real distribution, while in the Distant dataset acronyms are artificially generated from
3Due to data confidentiality issue, we were unable to directly release these datasets.
",6.1.2 Evaluation Datasets,[0],[0]
randomly sampled meanings.,6.1.2 Evaluation Datasets,[0],[0]
We also want to compare our method with the state-of-the-art Entity Linking (EL) systems based on public knowledge bases such as Wikipedia.,6.1.2 Evaluation Datasets,[0],[0]
"However, it is unfair to directly compare as most enterprise specific meanings are unknown to them.",6.1.2 Evaluation Datasets,[0],[0]
"Therefore, we need to only consider cases where the true meaning is a public meaning covered by both our system and the compared system.",6.1.2 Evaluation Datasets,[0],[0]
"By filtering the distant dataset from Office365, we get the third dataset JoinW (1659 test cases) for comparing with the Wikifier (Ratinov et al., 2011), and the fourth dataset JoinA (237 test cases) for comparing with AIDA (Hoffart et al., 2011).",6.1.2 Evaluation Datasets,[0],[0]
"We compare the following ablations of our system, to illustrate the effectiveness of the features and components.
",6.2.1 Ablations of Our System,[0],[0]
"• Internal Popularity (IP): Only the internal popularity features (i.e., marginal popularity and conditional popularity).
",6.2.1 Ablations of Our System,[0],[0]
"• Popularity (P): The internal popularity features plus Wiki popularity features.
",6.2.1 Ablations of Our System,[0],[0]
• Popularity+Context (P+C):,6.2.1 Ablations of Our System,[0],[0]
"The popularity features plus context similarity feature.
",6.2.1 Ablations of Our System,[0],[0]
"• Popularity+Context+Neighbbor (P+C+N): The popularity features, context similarity feature and immediate neighbor features.
",6.2.1 Ablations of Our System,[0],[0]
• Popularity+ Context+,6.2.1 Ablations of Our System,[0],[0]
"Neighbbor+ Fullname (a.k.a. Candidate Ranker, or CR): Using all the features in candidate ranking module.
",6.2.1 Ablations of Our System,[0],[0]
"• Candidate Ranker + Confidence Estimator (CR+ CE): Using the candidate ranking model plus the confidence estimation model.
",6.2.1 Ablations of Our System,[0],[0]
"• Candidate Ranker + Confidence Estimator + Final Selector (a.k.a. Acronym Disambiguator, or AD):",6.2.1 Ablations of Our System,[0],[0]
"Using the candidate ranking model, the confidence estimation model and the final selection model.",6.2.1 Ablations of Our System,[0],[0]
Full version of our system.,6.2.1 Ablations of Our System,[0],[0]
"We also compare our method with two state-ofthe-art Entity Linking (EL) systems.
",6.2.2 State-of-the-art EL Systems,[0],[0]
"• Wikifier: a popular EL system using machine learning to combine various features together.
• AIDA: a robust EL system using mention-entity graph to find the best mention-entity mapping.",6.2.2 State-of-the-art EL Systems,[0],[0]
We first conduct experiments to evaluate the quality of the acronym/meaning pairs harvested through our offline mining module.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"Out of the 17258 mined pairs, we randomly sampled 2000 of them and asked 5 domain experts to manually check their validness.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
An acronym/meaning pair is considered as valid if the majority of the experts think the acronym is indeed used to abbreviate the meaning.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"For example, (AS, Analysis Service) is a valid pair, but (AS, App Store) is considered as invalid because people will not actually use AS to represent App Store.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"Among the sampled 2000 pairs, 94.5% are labeled as valid, indicating our offline mining module can accurately extract acronym/meaning pairs from enterprise corpus.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"It is hard to precisely evaluate the coverage/recall of our mining method, since it is very difficult to obtain the complete meaning list for a given acronym.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"To get a rough idea, we randomly picked up 100 acronyms and asked the 5 domain experts to enumerate the valid meanings for these acronyms.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
In total we got 230 valid meanings and all of them are covered by the mined pairs.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"We first conduct experiments to evaluate the disambiguation performance of our ranking model, and compare the helpfulness of the features used in the model.",6.4 Disambiguation Performance,[0],[0]
"Figure 4 shows the precision (i.e., percentage of correctly disambiguated cases among all predicted cases), recall (i.e., percentage of correctly disambiguated cases among all test cases) and F1 (i.e., harmonic mean of precision and recall) of the compared methods on the Manual dataset and the Distant dataset.",6.4 Disambiguation Performance,[0],[0]
"In terms of the helpfulness of the features, the context similarity feature and the immediate neighbor features contribute most to the performance gain.",6.4 Disambiguation Performance,[0],[0]
"Other features are less helpful, yet still bring improvements to the overall performance.
",6.4 Disambiguation Performance,[0],[0]
Next we conduct experiments to illustrate the effectiveness of the confidence estimation module and the final selection module in our system.,6.4 Disambiguation Performance,[0],[0]
"Figure 5 shows the precision, recall and F1 of the compared system configurations on the Manual and Distant dataset.",6.4 Disambiguation Performance,[0],[0]
"As can be seen, the confidence estimation module can improve precision at the cost of hurting recall.",6.4 Disambiguation Performance,[0],[0]
"Fortunately, the final selection module can recover some recall losses without sacrificing too much on precision.",6.4 Disambiguation Performance,[0],[0]
"In
terms of the F1 measure, the final system achieves the best performance.
",6.4 Disambiguation Performance,[0],[0]
"Note that the ablation P+C naturally corresponds to the existing acronym disambiguation approaches (Feng et al., 2009; Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) mainly relying on context words and domain specific resources.",6.4 Disambiguation Performance,[0],[0]
These approaches do not specifically distinguish internal and external meanings.,6.4 Disambiguation Performance,[0],[0]
"They merely rely on the internal corpus to discover information about external meanings, which is quite ineffective in the scenario of enterprise acronym disambiguation (as discussed in Section 1).",6.4 Disambiguation Performance,[0],[0]
"In comparison, our system (AD) is able to leverage public resources together with the internal corpus to better handle the problem and therefore significantly outperforms them.",6.4 Disambiguation Performance,[0],[0]
We also compare our system (AD) with two stateof-the-art Entity Linking (EL) systems: Wikifier and AIDA.,6.5 Comparison with EL Systems,[0],[0]
"As explained in Section 6.1.2, we
made two datasets (i.e., JoinW and JoinA) for fair comparisons.",6.5 Comparison with EL Systems,[0],[0]
"Figure 6(a) and Figure 6(b) present the comparison of our AD system against Wikifer and AIDA, respectively.",6.5 Comparison with EL Systems,[0],[0]
"As we can see from the figures, AD significantly outperforms both Wikifier and AIDA on all three measures.",6.5 Comparison with EL Systems,[0],[0]
"The reason is that even for public meanings (e.g., Operating System) indexed by Wikifier and AIDA, the usage of them could be quite different in enterprises (e.g., inside Microsoft people talk more about designing Operating System rather than how to install it).",6.5 Comparison with EL Systems,[0],[0]
"Wikifier and AIDA utilize information from public knowledge bases (e.g., Wikipedia) to generate features, therefore can hardly capture such enterprisespecific signals.",6.5 Comparison with EL Systems,[0],[0]
"In contrast, our AD system mines disambiguation features directly from the enterprise corpus and utilizes them together with the public signals.",6.5 Comparison with EL Systems,[0],[0]
"As a result, it can more accurately represent the characteristics of the enterprise and lead to much better disambiguation performances.",6.5 Comparison with EL Systems,[0],[0]
Acronym meaning discovery has received a lot of attentions in vertical domains (mainly in biomedical).,7 Related Work,[0],[0]
"Most of the proposed approaches (Adar, 2004; Ao and Takagi, 2005; Park and Byrd, 2001; Schwartz and Hearst, 2002; Wren et al., 2002) utilized generic rules or text patterns (e.g. brackets, colons) to discover acronym meanings.",7 Related Work,[0],[0]
"These methods are usually based on the assumption that
acronyms are co-mentioned with the corresponding meanings in the same document.",7 Related Work,[0],[0]
"However, in enterprises, this assumption rarely holds.",7 Related Work,[0],[0]
"Enterprises themselves are closed ecosystems, so it is very common for people to define the acronyms somewhere and use them elsewhere.",7 Related Work,[0],[0]
"As a result, such methods cannot be used for acronym meaning discovery in enterprises.
",7 Related Work,[0],[0]
"Recently, there have been a few works (Jain et al., 2007; Larkey et al., 2000; Nadeau and Turney, 2005; Taneva et al., 2013) on automatically mining acronym meanings by leveraging Web data (e.g., query sessions, click logs).",7 Related Work,[0],[0]
"However, it is hard to apply them directly to enterprises, since most data in enterprises are raw text and therefore the query sessions/click logs are rarely available.
",7 Related Work,[0],[0]
"Acronym disambiguation can be seen as a special case for the Entity Linking (EL) (Ji and Grishman, 2011; Dredze et al., 2010) problem.",7 Related Work,[0],[0]
Approaches that link entity mentions to Wikipedia date back to Bunescu et.,7 Related Work,[0],[0]
"al’s work (Bunescu and Paşca, 2006).",7 Related Work,[0],[0]
They computed the cosine similarity between the text around the mention and the entity candidate’s Wikipedia page.,7 Related Work,[0],[0]
The referent entity with the maximum similarity score is selected as the disambiguation result.,7 Related Work,[0],[0]
"Cucerzan’s work (Cucerzan, 2007) is the first one to realize the effectiveness of using topical coherence to globally perform EL.",7 Related Work,[0],[0]
"In that work, the topical coherence between the referent entity candidate and other entities within the same context is calculated based on their overlaps in categories and incoming links in Wikipedia.",7 Related Work,[0],[0]
"Recently, several methods (Hoffart et al., 2011; Li et al., 2013, 2016; Ratinov et al., 2011; Shen et al., 2012; Cheng and Roth, 2013) also tried to enrich “context similarity” and “topical coherence” using hybrid strategies.",7 Related Work,[0],[0]
Shen et.,7 Related Work,[0],[0]
"al (Shen et al., 2015) provided a comprehensive survey for the techniques used in EL.",7 Related Work,[0],[0]
"However, these EL techniques cannot be used for acronym disambiguation in enterprises, since most enterprise meanings are not covered by public knowledge bases, and there are rarely any domain-specific knowledge bases available in enterprises.",7 Related Work,[0],[0]
"Automatic knowledge base construction (Suchanek et al., 2013) is promising, but the quality is far from applicable.",7 Related Work,[0],[0]
"Moreover, the structural information (e.g. entity taxonomy, crossdocument hyperlinks) within Wikipedia, is rarely available in enterprises.
",7 Related Work,[0],[0]
"Most of the previous work (Feng et al., 2009;
Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) on acronym disambiguation heavily rely on context words and domain specific resources.",7 Related Work,[0],[0]
"In comparison, our method explored a more comprehensive set of domain-independent features.",7 Related Work,[0],[0]
"Moreover, our method used a much less restrictive way to discover meaning candidates and training data, which is far more general than the methods relying on strict definition patterns (Schwartz and Hearst, 2002).",7 Related Work,[0],[0]
Another particular limitation of all these previous work is that they do not distinguish internal and external meanings.,7 Related Work,[0],[0]
"They merely rely on the internal corpus to discover information about external meanings, which is quite ineffective.",7 Related Work,[0],[0]
"In this paper, we studied the Acronym Disambiguation for Enterprises problem.",8 Conclusions,[0],[0]
"We proposed a novel, end-to-end framework to solve this problem.",8 Conclusions,[0],[0]
Our framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,8 Conclusions,[0],[0]
"The disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples.",8 Conclusions,[0],[0]
"Different from all the previous acronym disambiguation approaches, our system is capable of accurately resolving acronyms to both enterprise-specific meanings and public meanings.",8 Conclusions,[0],[0]
Experimental results on Microsoft enterprise data demonstrated that our system can effectively construct acronym/meaning repositories from scratch and accurately disambiguate acronyms to their meanings with over 90% precision.,8 Conclusions,[0],[0]
"Furthermore, our proposed framework can be easily deployed to any enterprises without requiring any domain knowledge.",8 Conclusions,[0],[0]
Acronyms are abbreviations formed from the initial components of words or phrases.,abstractText,[0],[0]
"In enterprises, people often use acronyms to make communications more efficient.",abstractText,[0],[0]
"However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity.",abstractText,[0],[0]
"To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context.",abstractText,[0],[0]
Acronym disambiguation for enterprises is challenging for several reasons.,abstractText,[0],[0]
"First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings.",abstractText,[0],[0]
"Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises.",abstractText,[0],[0]
"Finally, the system should be generic to work for any enterprise.",abstractText,[0],[0]
In this work we propose an end-to-end framework to tackle all these challenges.,abstractText,[0],[0]
The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,abstractText,[0],[0]
"Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples.",abstractText,[0],[0]
"Therefore, our proposed framework can be deployed to any enterprise to support highquality acronym disambiguation.",abstractText,[0],[0]
Experimental results on real world data justified the effectiveness of our system.,abstractText,[0],[0]
Guess Me if You Can: Acronym Disambiguation for Enterprises,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 366–376 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1034",text,[0],[0]
"With the rapid growth of products reviews at the web, it has become common for people to read reviews before making a purchase decision.",1 Introduction,[0],[0]
The reviews usually contain abundant consumers’ personal experiences.,1 Introduction,[0],[0]
It has led to a significant influence on financial gains and fame for businesses.,1 Introduction,[0],[0]
"Existing studies have shown that an extra halfstar rating on Yelp causes restaurants to sell out 19% points more frequently (Anderson and Magruder, 2012), and a one-star increase in Yelp rating leads to a 5-9 % increase in revenue (Luca, 2011).",1 Introduction,[0],[0]
"This, unfortunately, gives strong incentives for imposters (called spammers) to game the system.",1 Introduction,[0],[0]
They post fake reviews or opinions (called review spam) to promote or to discredit some targeted products and services.,1 Introduction,[0],[0]
"The news from BBC has shown that around 25% of Yelp reviews could be fake.1 Therefore, it is urgent to detect review s-
1http://www.bbc.com/news/technology-24299742
pam, to ensure that the online review continues to be trusted.
",1 Introduction,[0],[0]
Jindal and Liu (2008) make the first step to detect review spam.,1 Introduction,[0],[0]
Most efforts are devoted to exploring effective linguistic and behavioral features by subsequent work to distinguish such spam from the real reviews.,1 Introduction,[0],[0]
"However, to notice such patterns or form behavioral features, developers should take a long time to observe the data, because the features are based on statistics.",1 Introduction,[0],[0]
"For instance, the feature activity window proposed by Mukherjee et al. (2013c) is to measure the activity freshness of reviewers.",1 Introduction,[0],[0]
It usually takes several months to count the difference of timestamps between the last and first reviews for reviewers.,1 Introduction,[0],[0]
"When the features show themselves finally, some major damages might have already been done.",1 Introduction,[0],[0]
"Thus, it is important to design algorithms that can detect review spam as soon as possible, ideally, right after they are posted by the new reviewers.",1 Introduction,[0],[0]
"It is a coldstart problem which is the focus of this paper.
",1 Introduction,[0],[0]
"In this paper, we assume that we must identify fake reviews immediately when a new reviewer posts just one review.",1 Introduction,[0],[0]
"Unfortunately, it is very difficult because the available information for detecting fake reviews is very poor.",1 Introduction,[0],[0]
Traditional behavioral features based on the statistics can only work well on users’ abundant behaviors.,1 Introduction,[0],[0]
"The more behavioral information obtained, the more effective the traditional behavioral features are (see experiments in Section 3 ).",1 Introduction,[0],[0]
"In the scenario of cold-start, a new reviewer only has a behavior: post a review.",1 Introduction,[0],[0]
"As a result, we can not get effective behavioral features from the data.",1 Introduction,[0],[0]
"Although, the linguistic features of reviews do not need to take much time to form, Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, where we also obtain the same observation (the details are shown in Section 3).
",1 Introduction,[0],[0]
"366
Therefore, the main difficulty of the cold-start spam problem is that there are no sufficient behaviors of the new reviewers for constructing effective behavioral features.",1 Introduction,[0],[0]
"Nevertheless, there is ample textual and behavioral information contained in the abundant reviews posted by the existing reviewers (Figure 1).",1 Introduction,[0],[0]
We could employ behavioral information of existing similar reviewers to a new reviewer to approximate his behavioral features.,1 Introduction,[0],[0]
"We argue that a reviewer’s individual characteristics such as background information, motivation, and interactive behavior style have a great influence on a reviewer’s textual and behavioral information.",1 Introduction,[0],[0]
So the textual information and the behavioral information of a reviewer are correlated with each other (similar argument in Li et al. (2016)).,1 Introduction,[0],[0]
"For example, the students of the college are likely to choose the youth hostel during summer vacation and tend to comment the room price in their reviews.",1 Introduction,[0],[0]
"But the financial analysts on a business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews.
",1 Introduction,[0],[0]
"To augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews.",1 Introduction,[0],[0]
"There are several ways to model the textual information of the review spam, such as Unigram (Mukherjee et al., 2013c), POS (Ott et al., 2011) and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003).",1 Introduction,[0],[0]
"We employ the CNN (Convolutional Neural Network) to model the review text, which has been proved that it can capture complex global semantic information that is difficult to express using traditional discrete manual features (Ren and Zhang, 2016).",1 Introduction,[0],[0]
Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer.,1 Introduction,[0],[0]
"An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s behavioral features as the new reviewers’ features (detailed in Section 5.3).",1 Introduction,[0],[0]
"However, there is abundant behavioral information in the review graph (Figure 1), it is difficult for the traditional discrete manual behavioral features to record the global behavioral information (Wang et al., 2016).",1 Introduction,[0],[0]
"Moreover, the traditional features can not capture the reviewer’s individual characteristics, because there is no explicit characteristic tag available in the review system (experi-
ments in Section 5.3).",1 Introduction,[0],[0]
"So, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem.",1 Introduction,[0],[0]
"By encoding the review graph structure (Figure 1), the proposed model can record the global footprints of the existing reviewers in an unsupervised way, and further record the reviewers’ latent characteristic information in the footprints.",1 Introduction,[0],[0]
The jointly learnt review embeddings can model the correlation of the reviewers’ textual and behavioral information.,1 Introduction,[0],[0]
"When a new reviewer posts a review, the proposed model can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings.",1 Introduction,[0],[0]
"Finally, the embeddings of the new review are fed into a classifier to identify whether it is spam or not.
",1 Introduction,[0],[0]
"In summary, our major contributions include: • To our best knowledge, this is the first work
that explores the cold-start problem in review spam detection.",1 Introduction,[0],[0]
We qualitatively and quantitatively prove that the traditional linguistic and behavioral features are not effective enough in detecting review spam for the coldstart task.,1 Introduction,[0],[0]
• We propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for the cold-start spam detection task.,1 Introduction,[0],[0]
It is an unsupervised distributional representation model which can learn from large scale unlabeled review data.,1 Introduction,[0],[0]
• Experimental results on two domains (hotel and restaurant) give good confidence that the proposed model performs effectively in the cold-start spam detection task.,1 Introduction,[0],[0]
Jindal and Liu (2008) make the first step to detect review spam.,2 Related Work,[0],[0]
"Subsequent work devoted most
efforts to explore effective features and spammerlike clues.
",2 Related Work,[0],[0]
Linguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features.,2 Related Work,[0],[0]
Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage.,2 Related Work,[0],[0]
Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews.,2 Related Work,[0],[0]
The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015).,2 Related Work,[0],[0]
Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis.,2 Related Work,[0],[0]
Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features.,2 Related Work,[0],[0]
"Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective than the RNN and the traditional discrete manual linguistic features.",2 Related Work,[0],[0]
"Hovy (2016) used N-gram generative models to produce reviews and evaluated their effectiveness.
",2 Related Work,[0],[0]
Behavioral features: Lim et al. (2010) analyzed reviewers’ rating behavioral features; Jindal et al. (2010) identified unusual review patterns which can represent suspicious behaviors of reviews; Li et al. (2011) proposed a two-view semisupervised co-training method base on behavioral features.,2 Related Work,[0],[0]
Feng et al. (2012b) study the distributions of individual spammers’ behaviors.,2 Related Work,[0],[0]
The group spammers’ behavioral features were studied in Mukherjee et al. (2012).,2 Related Work,[0],[0]
"Temporal patterns of spammers were investigated by Xie et al. (2012), Fei et al. (2013); Li et al. (2015) explored the temporal and spatial patterns.",2 Related Work,[0],[0]
"The review graph was analyzed by Wang et al. (2011), Akoglu et al. (2013); Mukherjee et al. (2013a) studied the spamicity of reviewers.",2 Related Work,[0],[0]
"Mukherjee et al. (2013c), Mukherjee et al. (2013b) proved that reviewers’ behavioral features are more effective than reviews’ linguistic features for detecting review spam.",2 Related Work,[0],[0]
"Based on this conclusion, recently, researchers (Rayana and Akoglu, 2015; KC and Mukherjee, 2016) have put more efforts in employing reviewers’ behavioral features for de-
tecting review spam, the intuition behind which is to capture the reviewers’ actions and supposes that those reviews written with spammer-like behaviors would be spam.",2 Related Work,[0],[0]
Wang et al. (2016) explored a method to learn the review representation with global behavioral information.,2 Related Work,[0],[0]
Viviani and Pasi (2017) concentrated on the aggregation process with respect to each single veracity feature.,2 Related Work,[0],[0]
"As a new reviewer posted just one review and we have to identify it immediately, the major challenge of the cold-start task is that the available information about the new reviewer is very poor.",3 Whether Traditional Features are Effective,[0],[0]
The new reviewer only provides us with one review record.,3 Whether Traditional Features are Effective,[0],[0]
"For most traditional features based on the statistics, they can not form themselves or make no sense, such as the percentage of reviews written at weekends (Li et al., 2015), the entropy of rating distribution of user’s review (Rayana and Akoglu, 2015).",3 Whether Traditional Features are Effective,[0],[0]
"To investigate whether traditional features are effective in the cold-start task, we conducted experiments on the Yelp dataset in Mukherjee et al. (2013c).",3 Whether Traditional Features are Effective,[0],[0]
"We trained SVM models with different features on the existing reviews posted before January 1, 2012, and tested on the new reviews which just posted by the new reviewers after January 1, 2012.",3 Whether Traditional Features are Effective,[0],[0]
Results are shown in Table 1.,3 Whether Traditional Features are Effective,[0],[0]
The linguistic features need not take much time to form.,3.1 Linguistic Features’ Poor Performance,[0],[0]
"But Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, compared with the performances on the crowd source datasets (Ott et al., 2011).",3.1 Linguistic Features’ Poor Performance,[0],[0]
"They showed that the word bigrams perform better than the other linguistic features, such as LIWC (Newman et al., 2003; Pennebaker et al., 2007), part-of-speech sequence patterns (Mukherjee and Liu, 2010), deep syntax (Feng et al., 2012a), information gain (Mukherjee et al., 2013c) and so on.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"So, we conduct experiments with the word bigrams feature.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"As shown in Table 1 (a, b) row 1, the word bigrams result in only around 55% in accuracy in both the hotel and restaurant domains.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"It indicates that the most effective traditional linguistic feature (i.e., the word bigrams) can’t detect the review spam effectively in the cold start task.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"Abundant Information
Because there is not enough available information about the new reviewer, for most traditional behavioral features based on the statistical mechanism, they couldn’t form themselves or make no sense.",3.2 Behavioral Features only Work Well with,[0],[0]
We investigated the previous work and found that there are three behavioral features can be applied to the cold-start task.,3.2 Behavioral Features only Work Well with,[0],[0]
"They are proposed by Mukherjee et al. (2013b), i.e., 1.Review length (RL) : the length of the new review posted by the new reviewer; 2.Reviewer deviation (RD): the absolute rating deviation of the new reviewer’s review from other reviews on the same business; 3.Maximum content similarity (MCS) : the maximum content similarity (using cosine similarity) between the new reviewer’s review with other reviews on the same business.
",3.2 Behavioral Features only Work Well with,[0],[0]
"Table 1 (a, b) row 2 shows the experiment results by the combinations of the bigrams feature and the three behavioral features described above.",3.2 Behavioral Features only Work Well with,[0],[0]
The behavioral features make around 5% improvement in accuracy in the hotel domain (2.7% in the restaurant domain) as compared with only using bigrams.,3.2 Behavioral Features only Work Well with,[0],[0]
The accuracy is improved but it is just near 60% in average.,3.2 Behavioral Features only Work Well with,[0],[0]
It indicates that the traditional features are not effective enough with poor behavioral information.,3.2 Behavioral Features only Work Well with,[0],[0]
"What’s more, the behavioral features cause around 4.6% decrease in F1-
score and around 19% decrease in Recall in both hotel and restaurant domains.",3.2 Behavioral Features only Work Well with,[0],[0]
It is obvious that there is more false-positive review spam caused by the behavioral features as compared to only using bigrams.,3.2 Behavioral Features only Work Well with,[0],[0]
"It further indicates that the traditional behavioral features’ discrimination for review spam gets to be weakened by the poor behavioral information.
",3.2 Behavioral Features only Work Well with,[0],[0]
"To go a step further, we carried experiments with the three behavioral features which are formed on abundant behavioral information.",3.2 Behavioral Features only Work Well with,[0],[0]
"When the new reviewers continue to post more reviews in after weeks, their behavioral information gets to be more.",3.2 Behavioral Features only Work Well with,[0],[0]
Then the review system could obtain sufficient data to extract behavior features as compared to the poor information in the cold-start period.,3.2 Behavioral Features only Work Well with,[0],[0]
So the behavioral features with abundant information make an obvious improvement in accuracy (6.4%) in the hotel domain (Table 1 (a) row 3) as compared with the results in Table 1 (a) row 2.,3.2 Behavioral Features only Work Well with,[0],[0]
But it is only 0.6% in the restaurant domain.,3.2 Behavioral Features only Work Well with,[0],[0]
"By statistics on the datasets, we found that the new reviewers posted about 54.4 reviews in average after their first post in the hotel domain, but it is only 10 reviews in average for the new reviewers in the restaurant domain.",3.2 Behavioral Features only Work Well with,[0],[0]
The added behavioral information in the hotel domain is richer than that in the restaurant domain.,3.2 Behavioral Features only Work Well with,[0],[0]
"It indicates that:
• the traditional behavioral features can only work well with abundant behavioral information; • the more behavioral information can be obtained, the more effective the traditional behavioral features are.",3.2 Behavioral Features only Work Well with,[0],[0]
The difficulty of detecting review spam in the cold-start task is that the available behavioral information of new reviewers is very poor.,4 The Proposed Model,[0],[0]
"The new reviewer just posted one review and we have to filter it out immediately, there is not any historical review provided to us.",4 The Proposed Model,[0],[0]
"As we argued, the textual information and the behavioral information of a reviewer are correlated with each other.",4 The Proposed Model,[0],[0]
"So, to augment the behavioral information of new reviewers, we try to find the textual information which is similar with that of the new reviewer, from existing reviews.",4 The Proposed Model,[0],[0]
Then we take the behavioral information which is correlated with the found textual information as the most possible behavioral information of the new reviewer.,4 The Proposed Model,[0],[0]
"For this purpose, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem (shown in Figure 2).",4 The Proposed Model,[0],[0]
"When a new reviewer posts a review, the neural network can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings.",4 The Proposed Model,[0],[0]
"Finally, embeddings of the new review are fed into a classifier to identify whether it is spam or not.",4 The Proposed Model,[0],[0]
"In Figure 1, there is a part of review graph which is simplified from the Yelp website.",4.1 Behavioral Information Encoding,[0],[0]
"As it shows, the review graph contains the global behavioral information (footprints) of the existing reviewers.",4.1 Behavioral Information Encoding,[0],[0]
"Because the motivations of the spammers and the real reviewers are totally different, the distributions of the behavioral information of them are different (Mukherjee et al., 2013a).",4.1 Behavioral Information Encoding,[0],[0]
"There are businesses (even highly reputable ones) paying people to write fake reviews for them to promote their products/services and/or to discredit their competitors (Liu, 2015).",4.1 Behavioral Information Encoding,[0],[0]
So the behavioral footprints of the spammers are decided by the demands of the businesses.,4.1 Behavioral Information Encoding,[0],[0]
But the real reviewers only post reviews to the product or services they have actually experienced.,4.1 Behavioral Information Encoding,[0],[0]
Their behavioral footprints are influenced by their own characteristics.,4.1 Behavioral Information Encoding,[0],[0]
Previous work extracts behavioral features for reviewers from these behavioral information.,4.1 Behavioral Information Encoding,[0],[0]
But it is impractical to the new reviewers in the cold-start task.,4.1 Behavioral Information Encoding,[0],[0]
"Moreover, the traditional discrete features can not effectively record the global behavioral information (Wang et al., 2016).",4.1 Behavioral Information Encoding,[0],[0]
"Besides, there is no explicit charac-
teristic tag available in the review system, and we need to find a way to record the reviewers’ latent characters information in footprints.
",4.1 Behavioral Information Encoding,[0],[0]
"Therefore we encode these behavioral information into our model by utilizing an embedding learning model which is similar with TransE (Bordes et al., 2013).",4.1 Behavioral Information Encoding,[0],[0]
"TransE is a model which can encode the graph structure, and represent the nodes and edges (head, translation/relation, tail) in low dimension vector space.",4.1 Behavioral Information Encoding,[0],[0]
"TransE has been proved that it is good at describing the global information of the graph structure by the work about distributional representation for knowledge base (Guu et al., 2015).",4.1 Behavioral Information Encoding,[0],[0]
We consider that each reviewer in review graph describes the product in his/her own view and writes the review.,4.1 Behavioral Information Encoding,[0],[0]
"When we represent the product, reviewer, and review in low dimension vector space, the reviewer embeddings can be taken as a translation vector, which has translated the product embeddings to the review embeddings.",4.1 Behavioral Information Encoding,[0],[0]
"So, as shown in Figure 2, we take the products (hotels/restaurants) as the head part of the TransE network in our model, take the reviewers as the translation (relation) part and take the review as the tail part.",4.1 Behavioral Information Encoding,[0],[0]
"By learning from the existing large scale unlabeled reviews of the review graph, we can encode the global behavioral information into our model without extracting any traditional behavioral feature, and record reviewers’ latent characteristics information.
",4.1 Behavioral Information Encoding,[0],[0]
"More formally, we minimize a margin-based criterion over the training set:
L = ∑
(β,α,τ )",4.1 Behavioral Information Encoding,[0],[0]
"∈S
∑
(β′,α,τ ′)∈S′ max
{0, 1 + d(β + α, τ )",4.1 Behavioral Information Encoding,[0],[0]
"− d(β′ + α, τ ′)} (1)
S denotes the training set of triples (β, α, τ ) composed product β (β ∈ B, products set (head part)), reviewer α (α ∈ A, reviewers set (translation part)) and review text embeddings learnt by the CNN τ",4.1 Behavioral Information Encoding,[0],[0]
"(τ ∈ T , review texts set (tail part)).
",4.1 Behavioral Information Encoding,[0],[0]
"S′ = {(β′, α, τ )",4.1 Behavioral Information Encoding,[0],[0]
"|β′ ∈ B} ∪ {(β, α, τ ′)|τ ′",4.1 Behavioral Information Encoding,[0],[0]
"∈ T} (2)
",4.1 Behavioral Information Encoding,[0],[0]
"The set of corrupted triplets S′ (Equation (2)), is composed of training triplets with either the product or review text replaced by a random chosen one (but not both at the same time).
",4.1 Behavioral Information Encoding,[0],[0]
"d(β + α, τ )",4.1 Behavioral Information Encoding,[0],[0]
= ∥β + α,4.1 Behavioral Information Encoding,[0],[0]
"− τ∥22 , s.t.",4.1 Behavioral Information Encoding,[0],[0]
"∥β∥22 = ∥α∥22 = ∥τ∥22 = 1
(3)
d(β",4.1 Behavioral Information Encoding,[0],[0]
"+ α, τ ) is the dissimilarity function with the squared euclidean distance.",4.1 Behavioral Information Encoding,[0],[0]
"To encode the textual information into our model, we adopt a convolutional neural network (CNN) to learn to represent the existing reviews.",4.2 Textual Information Encoding,[0],[0]
"By statistics, we find that a review usually refers to several aspects of the products or services.",4.2 Textual Information Encoding,[0],[0]
"For example, a hotel review may comment the room price, the free WiFi, and the bathroom at the same time.",4.2 Textual Information Encoding,[0],[0]
"Compared with the recurrent neural network (RNN), the CNN can do a better job of modeling the different aspects of a review.",4.2 Textual Information Encoding,[0],[0]
"Ren and Zhang (2016) have proved that the CNN can capture complex global semantic information and detect review spam more effectively, compared with traditional discrete manual features and the RNN model.",4.2 Textual Information Encoding,[0],[0]
"As shown in Figure 2, we take the learnt embeddings τ of reviews by the CNN as the tail part.
",4.2 Textual Information Encoding,[0],[0]
"Specifically, we denote the review text consisting of n words as {w1, w2, ..., wn}, the word embeddings e(wi) ∈ RD, D is the word vector dimension.",4.2 Textual Information Encoding,[0],[0]
"We take the concatenation of the word embeddings in a fixed length window size Z as the input of the linear layer, which is denoted as Ii ∈ RD×Z .",4.2 Textual Information Encoding,[0],[0]
"So the output of the linear layer Hi is calculated by Hk,i = Wk · Ii + bi, where Wk ∈ RD×Z is the weight matrix of filter k.",4.2 Textual Information Encoding,[0],[0]
We utilize a max pooling layer to get the output of each filter.,4.2 Textual Information Encoding,[0],[0]
"Then we take tanh as the activation function and concatenate the outputs as the final review embeddings, which is denoted as τi.",4.2 Textual Information Encoding,[0],[0]
"To model the correlation of the textual and behavioral information, we employ the jointly information encoding.",4.3 Jointly Information Encoding,[0],[0]
"By jointly learning from the global review graph, the textual and behavioral information of existing spammers and real reviewers are embedded into the word embeddings.
",4.3 Jointly Information Encoding,[0],[0]
"In addition, the rating usually represents the sentiment polarity of a review, e.g., five star means ‘like’ and one star means ‘dislike’.",4.3 Jointly Information Encoding,[0],[0]
"The spammers often review their target products with a low rating for discredited purpose, and with a high rating for promoted purpose.",4.3 Jointly Information Encoding,[0],[0]
"To encode the semantics of the sentiment polarity into the review embeddings, we learn the embeddings of 1-5 stars rating in our model at the same time.",4.3 Jointly Information Encoding,[0],[0]
They are taken as the constraints of the review embeddings during the joint learning.,4.3 Jointly Information Encoding,[0],[0]
"They are calculated as:
C = ∑
(τ ,γ)∈Γ
∑
(τ ,γ′)∈Γ′ max{0, 1+ g(τ , γ)− g(τ , γ′)} (4)
",4.3 Jointly Information Encoding,[0],[0]
"The set of corrupted tuples Γ′ is composed of training tuples Γ with the rating of review replaced by its opposite rating (i.e., 1 by 5, 2 by 4, 3 by 1 or 5).",4.3 Jointly Information Encoding,[0],[0]
"g(τ , γ) =",4.3 Jointly Information Encoding,[0],[0]
"∥τ − γ∥22, norm constraints: ∥γ∥22 = 1.
",4.3 Jointly Information Encoding,[0],[0]
"The final joint loss function is as follows:
LJ = (1 − θ)L + θC (5)
where θ is a hyper-parameter.",4.3 Jointly Information Encoding,[0],[0]
"Datasets: To evaluate the proposed method, we conducted experiments on Yelp dataset that was used in (Mukherjee et al., 2013b,c; Rayana and Akoglu, 2015).",5.1 Datasets and Evaluation Metrics,[0],[0]
The statistics of the Yelp dataset are listed in Table 2 and Table 3.,5.1 Datasets and Evaluation Metrics,[0],[0]
The reviewed product here refers to a hotel or restaurant.,5.1 Datasets and Evaluation Metrics,[0],[0]
"We take the existing reviews posted before January 1, 2012 as the datasets for training our embedding learning model, and take the first new reviews which just posted by the new reviewers after January 1, 2012 as the test datasets.",5.1 Datasets and Evaluation Metrics,[0],[0]
Table 4 displays the statistics of the balanced datasets for training and testing the classifier.,5.1 Datasets and Evaluation Metrics,[0],[0]
Evaluation Metrics:,5.1 Datasets and Evaluation Metrics,[0],[0]
"We select precision (P), recall (R), F1-Score (F1), accuracy (A) as metrics.",5.1 Datasets and Evaluation Metrics,[0],[0]
"To illustrate the effectiveness of our model, we conduct experiments on the public datasets, and make comparison with the most effective traditional linguistic features, e.g., bigrams, and the three practicable traditional behavioral features (RL, RD, MCS (Mukherjee et al., 2013b)) referred in Section 3.2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The results are shown in Table 5.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"For our model, we set the dimension of embeddings to 100, the number of CNN filters to 100, θ to 0.1, Z to 2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The hyper-parameters are tuned by grid search on the development dataset.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"The product and reviewer embeddings are randomly ini-
tialized from a uniform distribution (Socher et al., 2013).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"The word embeddings are initialized with 100-dimensions vectors pre-trained by the CBOW model (Word2Vec) (Mikolov et al., 2013).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"As Table 5 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
Review Embeddings,5.2 Our Model v.s. the Traditional Features,[0],[0]
"Compared with the traditional linguistic features, e.g., bigrams, using the review embeddings learnt by our model, results in around 3.4% improvement in F1 and around 7.4% improvement in A in the hotel domain (1.1% in F1 and 5.0% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 1, 5).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Compared with the combination of the bigrams and the traditional behavioral features, using the review embeddings learnt by our model, results in around 7.6% improvement in F1 and around 2.2% improvement in A in the hotel domain (6.1% in F1 and 2.3% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 2, 5).",5.2 Our Model v.s. the Traditional Features,[0],[0]
The F1-Score (F1) of the classification under the balance distribution reflects the ability to detect the review spam.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The accuracy (A) of the classification under the balance distribution reflects the ability to identify both the review spam and the real review.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The experiment results indicate that our model performs significantly better than the traditional methods in F1 and A at the same time.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"The learnt review embeddings with encoded linguistic and behavioral information are more effective in detecting review
spam for the cold-start task.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Rating Embeddings As we referred in Section 4.3, the rating of a review usually means the sentiment polarity of a real reviewer or the motivation of a spammer.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"As shown in Table 5 (a,b) rows 6, adding the rating embeddings of the products (hotel/restaurant) and reviews renders even higher F1 and A.",5.2 Our Model v.s. the Traditional Features,[0],[0]
We suppose that different rating embeddings are encoded with different semantic meanings.,5.2 Our Model v.s. the Traditional Features,[0],[0]
They reflect the semantic divergences between the average rating of the product and the review rating.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"In results, using RE+RRE+PRE which makes the best performance of our model, results in around 5.5% improvement in F1 and around 9.4% improvement in A in the hotel domain (2.9% in F1 and 6.2% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 1, 6), compared with the LF.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Using RE+RRE+PRE results in around 9.7% improvement in F1 and around 4.2% improvement in A in the hotel domain (7.9% in F1 and 3.5% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 2, 6), compared with the LF+BF.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
The experiment results prove that our model is effective.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The improvements in both the F1 and A prove that our model performs well in both detecting the review spam and identifying the real review.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"Furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability 2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"It can learn to represent the reviews with global linguistic and behavioral information from largescale unlabeled existing reviews.
2The improvements in hotel domain are greater than that in restaurant domain.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The possible reason is the proportion of the available training data in hotel domain is higher than that in restaurant domain (99.01% vs. 97.40% in Table 2).,5.2 Our Model v.s. the Traditional Features,[0],[0]
"As mentioned in Section 1, to approximate the behavioral information of the new reviewers, there are other intuitive methods.",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
So we conduct experiments with two intuitive methods as a comparison.,5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"One is finding the most similar existing review by edit distance ratio and taking the found reviewers’ behavioral features as an approximation, and then training the classifier on the behavioral features and bigrams (BF EditSim+LF).",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"The other is finding the most similar existing review by cosine similarity of review embeddings which is the average of the pre-trained word embeddings (using Word2Vec), and then training the classifier on the behavioral features and review embeddings (BF W2Vsim+W2V).",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"As shown in Table 5, our joint embeddings (Ours RE and Ours RE+RRE+PRE) obviously perform better than the intuitive methods, such as the Ours RE is 3.8% (Accuracy) and 3.2% (F1) better than BF W2Vsim+W2V in the hotel domain.",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
The experiments indicate that our joint embeddings do a better job in capturing the reviewer’s characteristics and modeling the correlation of textual and behavioral information.,5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"Behavioral Information
To further evaluate the effectiveness of encoding the global behavioral information in our model, we build an independent supervised convolutional neural network which has the same structure and parameter settings with the CNN part of our model.",5.4 The Effectiveness of Encoding the Global,[0],[0]
"There is not any review graphic or behavioral information in this independent supervised CNN (Tabel 6 (a,b) row 2).",5.4 The Effectiveness of Encoding the Global,[0],[0]
"As shown in Tabel 6 (a,b) rows 2, 3, compared with the review embeddings learnt by the independent supervised CNN, using
the review embeddings learnt by our model results in around 9.0% improvement in F1 and around 3.8% improvement in A in the hotel domain (7.9% in F1 and 3.7% in A for the restaurant domain).",5.4 The Effectiveness of Encoding the Global,[0],[0]
The results show that our model can represent the new reviews posted by the new reviewers with the correlated behavioral information encoded in the word embeddings.,5.4 The Effectiveness of Encoding the Global,[0],[0]
The transE part of our model has effectively recorded the behavioral information of the review graph.,5.4 The Effectiveness of Encoding the Global,[0],[0]
"Thus, our model is more effective by jointly embedding the textual and behavioral informations, it helps to augment the possible behavioral information of the new reviewer.",5.4 The Effectiveness of Encoding the Global,[0],[0]
"Compared with the the most effective linguistic features, e.g., bigrams, our independent supervised convolutional neural network performs better in A than F1 (shown in Tabel 5 (a,b) rows 1, 2).",5.5 The Effectiveness of CNN,[0],[0]
It indicates that the CNN do a better job in identifying the real review than the review spam.,5.5 The Effectiveness of CNN,[0],[0]
We suppose that the possible reason is that the CNN is good at modeling the different semantic aspects of a review.,5.5 The Effectiveness of CNN,[0],[0]
"And the real reviewers usually tend to describe different aspects of a hotel or restaurant according to their real personal experiences, but the spammers can only forge fake reviews with their own infinite imagination.",5.5 The Effectiveness of CNN,[0],[0]
"Mukherjee et al. (2013b) also proved that different psychological states of the minds of the spammers and non-spammers, lead to significant linguistic differences between review spam and non-spam.",5.5 The Effectiveness of CNN,[0],[0]
This paper analyzes the importance and difficulty of the cold-start challenge in review spam combat.,6 Conclusion and Future Work,[0],[0]
We propose a neural network model that jointly embeds the existing textual and behavioral information for detecting review spam in the coldstart task.,6 Conclusion and Future Work,[0],[0]
It can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way.,6 Conclusion and Future Work,[0],[0]
"Then, a classifier is applied to detect the review spam.",6 Conclusion and Future Work,[0],[0]
Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability.,6 Conclusion and Future Work,[0],[0]
It is also applicable to a large-scale dataset in an unsupervised way.,6 Conclusion and Future Work,[0],[0]
"To our best knowledge, this is the first work to handle the cold-start problem in review spam detection.",6 Conclusion and Future Work,[0],[0]
"We are going to explore more effective models in fu-
ture.",6 Conclusion and Future Work,[0],[0]
This work was supported by the Natural Science Foundation of China (No. 61533018) and the National Basic Research Program of China (No. 2014CB340503).,Acknowledgments,[0],[0]
And this research work was also supported by Google through focused research awards program.,Acknowledgments,[0],[0]
"We would like to thank Prof. Bing Liu for useful advice, and the anonymous reviewers for their detailed comments and suggestions.",Acknowledgments,[0],[0]
Solving the cold-start problem in review spam detection is an urgent and significant task.,abstractText,[0],[0]
"It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work.",abstractText,[0],[0]
"This paper proposes a novel neural network model to detect review spam for the cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information.",abstractText,[0],[0]
Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability.,abstractText,[0],[0]
It is also applicable to a large-scale dataset in an unsupervised way.,abstractText,[0],[0]
Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1907–1917 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1907",text,[0],[0]
"Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) with the goal of developing systems that can answer questions about the content of a given passage or document.",1 Introduction,[0],[0]
Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field.,1 Introduction,[0],[0]
And there have been a number of efforts in this direction.,1 Introduction,[0],[0]
"Miller et al. (2016), for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehension (RC); and Hill et al. (2015) and Hermann
et al. (2015), for the related task of answering cloze questions (Winograd, 1972; Levesque et al., 2011).",1 Introduction,[0],[0]
"To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used.",1 Introduction,[0],[0]
"The (semi-)synthetic datasets (e.g., Hermann et al. (2015)) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"In comparison, high-quality crowdsourced datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design (Chen et al., 2016).
",1 Introduction,[0],[0]
"Thus, there is a need for methods that can automatically generate high-quality question-answer pairs.",1 Introduction,[0],[0]
Serban et al. (2016) propose the use of recurrent neural networks to generate QA pairs from structured knowledge resources such as Freebase.,1 Introduction,[0],[0]
"Their work relies on the existence of automatically acquired KBs, which are known to have errors and suffer from incompleteness.",1 Introduction,[0],[0]
They are also nontrivial to obtain.,1 Introduction,[0],[0]
"In addition, the questions in the resulting dataset are limited to queries regarding a single fact (i.e., tuple) in the KB.
",1 Introduction,[0],[0]
"Motivated by the need for large scale QA pairs and the limitations of recent work, we investigate methods that can automatically “harvest” (generate) question-answer pairs from raw text/unstructured documents, such as Wikipediatype articles.
",1 Introduction,[0],[0]
"Recent work along these lines (Du et al., 2017; Zhou et al., 2017) (see Section 2) has proposed the use of attention-based recurrent neural models trained on the crowdsourced SQuAD dataset (Rajpurkar et al., 2016) for question generation.",1 Introduction,[0],[0]
"While successful, the resulting QA pairs are based on information from a single sentence.",1 Introduction,[0],[0]
"As described in Du et al. (2017), however, nearly 30% of the questions in the human-generated questions of SQuAD rely on information beyond a single sentence.",1 Introduction,[0],[0]
"For example, in Figure 1, the second and third questions require coreference information (i.e., recognizing that “His” in sentence 2 and “He” in sentence 3 both corefer with “Tesla” in sentence 1) to answer them.
",1 Introduction,[0],[0]
"Thus, our research studies methods for incorporating coreference information into the training of a question generation system.",1 Introduction,[0],[0]
"In particular, we propose gated Coreference knowledge for Neural Question Generation (CorefNQG), a neural sequence model with a novel gating mechanism that leverages continuous representations of coreference clusters — the set of mentions used to refer to each entity — to better encode linguistic knowledge introduced by coreference, for paragraph-level question generation.
",1 Introduction,[0],[0]
"In an evaluation using the SQuAD dataset, we find that CorefNQG enables better question generation.",1 Introduction,[0],[0]
"It outperforms significantly the baseline neural sequence models that encode information from a single sentence, and a model that encodes all preceding context and the input sentence itself.",1 Introduction,[0],[0]
"When evaluated on only the portion of SQuAD that requires coreference resolution, the gap be-
tween our system and the baseline systems is even larger.
",1 Introduction,[0],[0]
"By applying our approach to the 10,000 topranking Wikipedia articles, we obtain a question answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6.",1 Introduction,[0],[0]
The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA.,1 Introduction,[0],[0]
"Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities.",2.1 Question Generation,[0],[0]
"Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015).",2.1 Question Generation,[0],[0]
Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates.,2.1 Question Generation,[0],[0]
"Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012).
",2.1 Question Generation,[0],[0]
"With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation.",2.1 Question Generation,[0],[0]
Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs.,2.1 Question Generation,[0],[0]
paragraph-level information.,2.1 Question Generation,[0],[0]
Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage.,2.1 Question Generation,[0],[0]
"Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask.
",2.1 Question Generation,[0],[0]
"In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge.",2.1 Question Generation,[0],[0]
Recently there has been an increasing interest in question answering with the creation of many datasets.,2.2 Question Answering Datasets and Creation,[0],[0]
"Most are built using crowdsourcing; they are generally comprised of fewer than 100,000 QA pairs and are time-consuming to create.",2.2 Question Answering Datasets and Creation,[0],[0]
"WebQuestions (Berant et al., 2013), for example, contains 5,810 questions crawled via the Google Suggest API and is designed for knowledge base QA with answers restricted to Freebase entities.",2.2 Question Answering Datasets and Creation,[0],[0]
"To tackle the size issues associated with WebQuestions, Bordes et al. (2015) introduce SimpleQuestions, a dataset of 108,442 questions authored by English speakers.",2.2 Question Answering Datasets and Creation,[0],[0]
"SQuAD (Rajpurkar et al., 2016) is a dataset for machine comprehension; it is created by showing a Wikipedia paragraph to human annotators and asking them to write questions based on the paragraph.",2.2 Question Answering Datasets and Creation,[0],[0]
"TriviaQA (Joshi et al., 2017) includes 95k question-answer authored by trivia enthusiasts and corresponding evidence documents.
",2.2 Question Answering Datasets and Creation,[0],[0]
"(Semi-)synthetic generated datasets are easier to build to large-scale (Hill et al., 2015; Hermann et al., 2015).",2.2 Question Answering Datasets and Creation,[0],[0]
They usually come in the form of cloze-style questions.,2.2 Question Answering Datasets and Creation,[0],[0]
"For example, Hermann et al. (2015) created over a million examples by pairing CNN and Daily Mail news articles with their summarized bullet points.",2.2 Question Answering Datasets and Creation,[0],[0]
"Chen et al. (2016) showed that this dataset is quite noisy due to the method of data creation and concluded that performance of QA systems on the dataset is almost saturated.
",2.2 Question Answering Datasets and Creation,[0],[0]
Closest to our work is that of Serban et al. (2016).,2.2 Question Answering Datasets and Creation,[0],[0]
"They train a neural triple-to-sequence model on SimpleQuestions, and apply their system to Freebase to produce a large collection of human-like question-answer pairs.",2.2 Question Answering Datasets and Creation,[0],[0]
Our goal is to harvest high quality questionanswer pairs from the paragraphs of an article of interest.,3 Task Definition,[0],[0]
"In our task formulation, this consists of two steps: candidate answer extraction and answer-specific question generation.",3 Task Definition,[0],[0]
"Given an input paragraph, we first identify a set of question-worthy candidate answers ans = (ans1, ans2, ..., ansl), each a span of text as denoted in color in Figure 1.",3 Task Definition,[0],[0]
"For each candidate answer ansi, we then aim to generate a question Q — a sequence of tokens y1, ..., yN — based on the
sentence S that contains candidate ansi such that:
• Q asks about an aspect of ansi that is of potential interest to a human;
• Q might rely on information from sentences that precede S in the paragraph.
",3 Task Definition,[0],[0]
"Mathematically then,
Q = argmax Q
P (Q|S,C) (1)
where P (Q|S,C) = ∏N
n=1 P (yn|y<n, S, C) where C is the set of sentences that precede S in the paragraph.",3 Task Definition,[0],[0]
"In this section, we introduce our framework for harvesting the question-answer pairs.",4 Methodology,[0],[0]
"As described above, it consists of the question generator CorefNQG (Figure 2) and a candidate answer extraction module.",4 Methodology,[0],[0]
"During test/generation time, we (1) run the answer extraction module on the input text to obtain answers, and then (2) run the question generation module to obtain the corresponding questions.",4 Methodology,[0],[0]
"As shown in Figure 2, our generator prepares the feature-rich input embedding — a concatenation of (a) a refined coreference position feature embedding, (b) an answer feature embedding, and (c) a word embedding, each of which is described below.",4.1 Question Generation,[0],[0]
"It then encodes the textual input using an LSTM unit (Hochreiter and Schmidhuber, 1997).",4.1 Question Generation,[0],[0]
"Finally, an attention-copy equipped decoder is used to decode the question.
",4.1 Question Generation,[0],[0]
"More specifically, given the input sentence S (containing an answer span) and the preceding context C, we first run a coreference resolution system to get the coref-clusters for S and C and use them to create a coreference transformed input sentence: for each pronoun, we append its most representative non-pronominal coreferent mention.",4.1 Question Generation,[0],[0]
"Specifically, we apply the simple feedforward network based mention-ranking model of Clark and Manning (2016) to the concatenation of C and S to get the coref-clusters for all entities in C and S. The C&M model produces a score/representation s for each mention pair (m1,m2),
s(m1,m2) = Wmhm(m1,m2) + bm",4.1 Question Generation,[0],[0]
"(2)
…
Sentence encoder
...
where Wm is a 1 × d weight matrix and b is the bias.",4.1 Question Generation,[0],[0]
"hm(m1,m2) is representation of the last hidden layer of the three layer feedforward neural network.
",4.1 Question Generation,[0],[0]
"For each pronoun in S, we then heuristically identify the most “representative” antecedent from its coref-cluster.",4.1 Question Generation,[0],[0]
(Proper nouns are preferred.),4.1 Question Generation,[0],[0]
We append the new mention after the pronoun.,4.1 Question Generation,[0],[0]
"For example, in Table 1, “the panthers” is the most representative mention in the coref-cluster for “they”.",4.1 Question Generation,[0],[0]
The new sentence with the appended coreferent mention is our coreference transformed input sentence S ′,4.1 Question Generation,[0],[0]
"(see Figure 2).
",4.1 Question Generation,[0],[0]
Coreference Position Feature Embedding For each token in S ′,4.1 Question Generation,[0],[0]
", we also maintain one position feature fc = (c1, ..., cn), to denote pronouns (e.g., “they”) and antecedents (e.g., “the panthers”).",4.1 Question Generation,[0],[0]
We use the BIO tagging scheme to label the associated spans in S ′ .,4.1 Question Generation,[0],[0]
"“B_ANT” denotes the start of an antecedent span, tag “I_ANT” continues the antecedent span and tag “O” marks tokens that do not form part of a mention span.",4.1 Question Generation,[0],[0]
"Similarly, tags “B_PRO” and “I_PRO” denote the pronoun span.",4.1 Question Generation,[0],[0]
"(See Table 1, “coref.",4.1 Question Generation,[0],[0]
"feature”.)
",4.1 Question Generation,[0],[0]
Refined Coref.,4.1 Question Generation,[0],[0]
"Position Feature Embedding Inspired by the success of gating mecha-
nisms for controlling information flow in neural networks (Hochreiter and Schmidhuber, 1997; Dauphin et al., 2017), we propose to use a gating network here to obtain a refined representation of the coreference position feature vectors fc = (c1, ..., cn).",4.1 Question Generation,[0],[0]
The main idea is to utilize the mention-pair score (see Equation 2) to help the neural network learn the importance of the coreferent phrases.,4.1 Question Generation,[0],[0]
We compute the refined (gated) coreference position feature vector fd =,4.1 Question Generation,[0],[0]
"(d1, ..., dn) as follows,
gi = ReLU(Waci +Wbscorei + b)
",4.1 Question Generation,[0],[0]
"di = gi ci (3)
where denotes an element-wise product between two vectors and ReLU is the rectified linear activation function.",4.1 Question Generation,[0],[0]
"scorei denotes the mentionpair score for each antecedent token (e.g., “the” and “panthers”) with the pronoun (e.g., “they”); scorei is obtained from the trained model (Equation 2) of the C&M.",4.1 Question Generation,[0],[0]
"If token i is not added later as an antecedent token, scorei is set to zero.",4.1 Question Generation,[0],[0]
"Wa, Wb are weight matrices and b is the bias vector.
",4.1 Question Generation,[0],[0]
"Answer Feature Embedding We also include an answer position feature embedding to generate answer-specific questions; we denote the answer span with the usual BIO tagging scheme (see,
e.g., “the arizona cardinals” in Table 1).",4.1 Question Generation,[0],[0]
"During training and testing, the answer span feature (i.e., “B_ANS”, “I_ANS” or “O”) is mapped to its feature embedding space: fa = (a1, ..., an).
",4.1 Question Generation,[0],[0]
"Word Embedding To obtain the word embedding for the tokens themselves, we just map the tokens to the word embedding space: x = (x1, ..., xn).
",4.1 Question Generation,[0],[0]
"Final Encoder Input As noted above, the final input to the LSTM-based encoder is a concatenation of (1) the refined coreference position feature embedding (light blue units in Figure 2), (2) the answer position feature embedding (red units), and (3) the word embedding for the token (green units),
ei = concat(di, ai, xi) (4)
Encoder As for the encoder itself, we use bidirectional LSTMs to read the input e = (e1, ..., en) in both the forward and backward directions.",4.1 Question Generation,[0],[0]
"After encoding, we obtain two sequences of hidden vectors, namely, −→ h = ( −→ h1, ..., −→ hn) and ←− h = ( ←− h1, ..., ←− hn).",4.1 Question Generation,[0],[0]
"The final output state of the encoder is the concatenation of −→ h and ←− h where
hi = concat( −→ hi , ←− hi) (5)
",4.1 Question Generation,[0],[0]
"Question Decoder with Attention & Copy On top of the feature-rich encoder, we use LSTMs with attention (Bahdanau et al., 2015) as the decoder for generating the question y1, ..., ym one token at a time.",4.1 Question Generation,[0],[0]
"To deal with rare/unknown words, the decoder also allows directly copying words from the source sentence via pointing (Vinyals et al., 2015).
",4.1 Question Generation,[0],[0]
"At each time step t, the decoder LSTM reads the previous word embedding wt−1 and previous hidden state st−1 to compute the new hidden state,
st = LSTM(wt−1, st−1) (6) Then we calculate the attention distribution αt as in Bahdanau et al. (2015),
et,i = h T",4.1 Question Generation,[0],[0]
i,4.1 Question Generation,[0],[0]
"Wcst−1 αt = softmax(et) (7)
where Wc is a weight matrix and attention distribution αt is a probability distribution over the source sentence words.",4.1 Question Generation,[0],[0]
"With αt, we can obtain the context vector h∗t ,
h∗t = n∑
i=1
αithi (8)
Then, using the context vector h∗t and hidden state st, the probability distribution over the target (question) side vocabulary is calculated as,
Pvocab = softmax(Wdconcat(h∗t , st))",4.1 Question Generation,[0],[0]
"(9)
Instead of directly using Pvocab for training/generating with the fixed target side vocabulary, we also consider copying from the source sentence.",4.1 Question Generation,[0],[0]
"The copy probability is based on the context vector h∗t and hidden state st,
λcopyt = σ",4.1 Question Generation,[0],[0]
"(Weh ∗ t +Wfst) (10)
and the probability distribution over the source sentence words is the sum of the attention scores of the corresponding words,
Pcopy(w) = n∑ i=1 αit ∗ 1{w == wi} (11)
",4.1 Question Generation,[0],[0]
"Finally, we obtain the probability distribution over the dynamic vocabulary (i.e., union of original target side and source sentence vocabulary) by summing over Pcopy and Pvocab,
P (w) = λcopyt Pcopy(w) + (1− λ copy t )Pvocab(w)
(12) where σ is the sigmoid function, and Wd, We,",4.1 Question Generation,[0],[0]
Wf are weight matrices.,4.1 Question Generation,[0],[0]
"We frame the problem of identifying candidate answer spans from a paragraph as a sequence labeling task and base our model on the BiLSTM-CRF approach for named entity recognition (Huang et al., 2015).",4.2 Answer Span Identification,[0],[0]
"Given a paragraph of n tokens, instead of directly feeding the sequence of word vectors x = (x1, ..., xn) to the LSTM units, we first construct the feature-rich embedding x ′ for each token, which is the concatenation of the word embedding, an NER feature embedding, and a character-level representation of the word (Lample et al., 2016).",4.2 Answer Span Identification,[0],[0]
"We use the concatenated vector as the “final” embedding x ′ for the token,
x ′",4.2 Answer Span Identification,[0],[0]
"i = concat(xi,CharRepi,NERi) (13)
where CharRepi is the concatenation of the last hidden states of a character-based biLSTM.",4.2 Answer Span Identification,[0],[0]
"The intuition behind the use of NER features is that SQuAD answer spans contain a large number of named entities, numeric phrases, etc.
",4.2 Answer Span Identification,[0],[0]
"Then a multi-layer Bi-directional LSTM is applied to (x
′ 1, ..., x ′ n) and we obtain the output state
zt for time step t by concatenation of the hidden states (forward and backward) at time step t from the last layer of the BiLSTM.",4.2 Answer Span Identification,[0],[0]
"We apply the softmax to (z1, ..., zn) to get the normalized score representation for each token, which is of size n× k, where k is the number of tags.
",4.2 Answer Span Identification,[0],[0]
"Instead of using a softmax training objective that minimizes the cross-entropy loss for each individual word, the model is trained with a CRF (Lafferty et al., 2001) objective, which minimizes the negative log-likelihood for the entire correct sequence: − log(py),
py = exp(q(x ′ ,y))∑
y′∈Y′",4.2 Answer Span Identification,[0],[0]
"exp(q(x ′ ,y′))
",4.2 Answer Span Identification,[0],[0]
"(14)
where q(x ′ ,y) = ∑n t=1 Pt,yt + ∑n−1 t=0 Ayt,yt+1 , Pt,yt is the score of assigning tag yt to the t th token, and Ayt,yt+1 is the transition score from tag yt to yt+1, the scoring matrix A is to be learned.",4.2 Answer Span Identification,[0],[0]
Y ′ represents all the possible tagging sequences.,4.2 Answer Span Identification,[0],[0]
"We use the SQuAD dataset (Rajpurkar et al., 2016) to train our models.",5.1 Dataset,[0],[0]
It is one of the largest general purpose QA datasets derived from Wikipedia with over 100k questions posed by crowdworkers on a set of Wikipedia articles.,5.1 Dataset,[0],[0]
The answer to each question is a segment of text from the corresponding Wiki passage.,5.1 Dataset,[0],[0]
The crowdworkers were users of Amazon’s Mechanical Turk located in the US or Canada.,5.1 Dataset,[0],[0]
"To obtain high-quality articles, the authors sampled 500 articles from the top 10,000 articles obtained by Nayuki’s Wikipedia’s internal PageRanks.",5.1 Dataset,[0],[0]
"The question-answer pairs were generated by annotators from a paragraph; and although the dataset is typically used to evaluate reading comprehension, it has also been used in an open domain QA setting (Chen et al., 2017; Wang et al., 2018).",5.1 Dataset,[0],[0]
"For training/testing answer extraction systems, we pair each paragraph in the dataset with the gold answer spans that it contains.",5.1 Dataset,[0],[0]
"For the question generation system, we pair each sentence that contains an answer span with the corresponding gold question as in Du et al. (2017).
",5.1 Dataset,[0],[0]
"To quantify the effect of using predicted (rather than gold standard) answer spans on question generation (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented “Training set w/ noisy examples”
(see Table 2).",5.1 Dataset,[0],[0]
"This training set contains all of the original training examples plus new examples for predicted answer spans (from the top-performing answer extraction model, bottom row of Table 3) that overlap with a gold answer span.",5.1 Dataset,[0],[0]
We pair the new training sentence (w/ predicted answer span) with the gold question.,5.1 Dataset,[0],[0]
"The added examples comprise 42.21% of the noisy example training set.
",5.1 Dataset,[0],[0]
"For generation of our one million QA pair corpus, we apply our systems to the 10,000 topranking articles of Wikipedia.",5.1 Dataset,[0],[0]
"For question generation evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences.",5.2 Evaluation Metrics,[0],[0]
"METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases.
",5.2 Evaluation Metrics,[0],[0]
"For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.",5.2 Evaluation Metrics,[0],[0]
"Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match.",5.2 Evaluation Metrics,[0],[0]
"Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap (Johansson and Moschitti, 2010; Irsoy and Cardie, 2014).",5.2 Evaluation Metrics,[0],[0]
"For question generation, we compare to the stateof-the-art baselines and conduct ablation tests as follows: Du et al. (2017)’s model is an attention-based RNN sequence-to-sequence neural network (without using the answer location information feature).",5.3 Baselines and Ablation Tests,[0],[0]
"Seq2seq + copyw/ answer is the attention-based sequence-to-sequence model augmented with a copy mechanism, with answer features concatenated with the word embeddings during encoding.",5.3 Baselines and Ablation Tests,[0],[0]
"Seq2seq + copyw/ full context + answer is the same model as the previous one, but we allow access to the full context (i.e., all the preceding sentences and the input sentence itself).",5.3 Baselines and Ablation Tests,[0],[0]
We denote it as ContextNQG henceforth for simplicity.,5.3 Baselines and Ablation Tests,[0],[0]
CorefNQG is the coreference-based model proposed in this paper.,5.3 Baselines and Ablation Tests,[0],[0]
"CorefNQG–gating is an
1We use the evaluation scripts of Du et al. (2017).
ablation test, the gating network is removed and the coreference position embedding is not refined.",5.3 Baselines and Ablation Tests,[0],[0]
"CorefNQG–mention-pair score is also an ablation test where all mention-pair scorei are set to zero.
",5.3 Baselines and Ablation Tests,[0],[0]
"For answer span extraction, we conduct experiments to compare the performance of an off-theshelf NER system and BiLSTM based systems.
",5.3 Baselines and Ablation Tests,[0],[0]
"For training and implementation details, please see the Supplementary Material.",5.3 Baselines and Ablation Tests,[0],[0]
"Table 2 shows the BLEU-{3, 4} and METEOR scores of different models.",6.1 Automatic Evaluation,[0],[0]
Our CorefNQG outperforms the seq2seq baseline of Du et al. (2017) by a large margin.,6.1 Automatic Evaluation,[0],[0]
"This shows that the copy mechanism, answer features and coreference resolution all aid question generation.",6.1 Automatic Evaluation,[0],[0]
"In addition, CorefNQG outperforms both Seq2seq+Copy models significantly, whether or not they have access to the full context.",6.1 Automatic Evaluation,[0],[0]
This demonstrates that the coreference knowledge encoded with the gating network explicitly helps with the training and generation: it is more difficult for the neural sequence model to learn the coreference knowledge in a latent way.,6.1 Automatic Evaluation,[0],[0]
(See input 1 in Figure 3 for an example.),6.1 Automatic Evaluation,[0],[0]
Building end-to-end models that take into account coreference knowledge in a latent way is an interesting direction to explore.,6.1 Automatic Evaluation,[0],[0]
"In the ablation tests, the performance drop of CorefNQG–gating
shows that the gating network is playing an important role for getting refined coreference position feature embedding, which helps the model learn the importance of an antecedent.",6.1 Automatic Evaluation,[0],[0]
"The performance drop of CorefNQG–mention-pair score shows the mention-pair score introduced from the external system (Clark and Manning, 2016) helps the neural network better encode coreference knowledge.
",6.1 Automatic Evaluation,[0],[0]
"To better understand the effect of coreference resolution, we also evaluate our model and the baseline models on just that portion of the test set that requires pronoun resolution (36.42% of the examples) and show the results in Table 4.",6.1 Automatic Evaluation,[0],[0]
The gaps of performance between our model and the baseline models are still significant.,6.1 Automatic Evaluation,[0],[0]
"Besides, we see that all three systems’ performance drop on this partial test set, which demonstrates the hardness of generating questions for the cases that require pronoun resolution (passage context).
",6.1 Automatic Evaluation,[0],[0]
"We also show in Table 2 the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans.
",6.1 Automatic Evaluation,[0],[0]
"There is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans.",6.1 Automatic Evaluation,[0],[0]
"We see that CorefNQG still outperforms the baseline models across all metrics.
",6.1 Automatic Evaluation,[0],[0]
Figure 3 provides sample output for input sentences that require contextual coreference knowledge.,6.1 Automatic Evaluation,[0],[0]
We see that ContextNQG fails in all cases; our model misses only the third example due to an error introduced by coreference resolution — the “city” and “it” are considered coreferent.,6.1 Automatic Evaluation,[0],[0]
"We can also see that human-generated questions are more natural and varied in form with better paraphrasing.
",6.1 Automatic Evaluation,[0],[0]
"In Table 3, we show the evaluation results for different answer extraction models.",6.1 Automatic Evaluation,[0],[0]
"First we see that all variants of BiLSTM models outperform the off-the-shelf NER system (that proposes all NEs as answer spans), though the NER system has a higher recall.",6.1 Automatic Evaluation,[0],[0]
The BiLSTM-CRF that encodes the character-level and NER features for each token performs best in terms of F-measure.,6.1 Automatic Evaluation,[0],[0]
We hired four native speakers of English to rate the systems’ outputs.,6.2 Human Study,[0],[0]
"Detailed guidelines for the raters are listed in the supplementary materials.
",6.2 Human Study,[0],[0]
The evaluation can also be seen as a measure of the quality of the generated dataset (Section 6.3).,6.2 Human Study,[0],[0]
"We randomly sampled 11 passages/paragraphs from the test set; there are in total around 70 questionanswer pairs for evaluation.
",6.2 Human Study,[0],[0]
"We consider three metrics — “grammaticality”, “making sense” and “answerability”.",6.2 Human Study,[0],[0]
The evaluators are asked to first rate the grammatical correctness of the generated question (before being shown the associated input sentence or any other textual context).,6.2 Human Study,[0],[0]
"Next, we ask them to rate the degree to which the question “makes sense” given the input sentence (i.e., without considering the correctness of the answer span).",6.2 Human Study,[0],[0]
"Finally, evaluators rate the “answerability” of the question given the full context.
",6.2 Human Study,[0],[0]
Table 5 shows the results of the human evaluation.,6.2 Human Study,[0],[0]
Bold indicates top scores.,6.2 Human Study,[0],[0]
"We see that the original human questions are preferred over the two NQG systems’ outputs, which is understandable given the examples in Figure 3.",6.2 Human Study,[0],[0]
"The humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context.",6.2 Human Study,[0],[0]
How exactly to capture the preceding context so as to ask better and more diverse questions is an interesting future direction for research.,6.2 Human Study,[0],[0]
"In terms of grammaticality, however, the neural models do quite well, achieving very close to human performance.",6.2 Human Study,[0],[0]
"In addition, we see that our method (CorefNQG) performs statistically significantly better across all metrics in comparison to the baseline model (ContextNQG), which has access to the entire preceding context in the passage.",6.2 Human Study,[0],[0]
"Our system generates in total 1,259,691 questionanswer pairs, nearly 126 questions per article.",6.3 The Generated Corpus,[0],[0]
"Figure 5 shows the distribution of different types of
questions in our dataset vs. the SQuAD training set.",6.3 The Generated Corpus,[0],[0]
"We see that the distribution for “In what”, “When”, “How long”, “Who”, “Where”, “What does” and “What do” questions in the two datasets is similar.",6.3 The Generated Corpus,[0],[0]
"Our system generates more “What is”, “What was” and “What percentage” questions, while the proportions of “What did”, “Why” and “Which” questions in SQuAD are larger than ours.",6.3 The Generated Corpus,[0],[0]
"One possible reason is that the “Why”, “What did” questions are more complicated to ask (sometimes involving world knowledge) and the answer spans are longer phrases of various types that are harder to identify.",6.3 The Generated Corpus,[0],[0]
"“What is” and “What was” questions, on the other hand, are often safer for the neural networks systems to ask.
",6.3 The Generated Corpus,[0],[0]
"In Figure 4, we show some examples of the generated question-answer pairs.",6.3 The Generated Corpus,[0],[0]
The answer extractor identifies the answer span boundary well and all three questions correspond to their answers.,6.3 The Generated Corpus,[0],[0]
Q2 is valid but not entirely accurate.,6.3 The Generated Corpus,[0],[0]
"For more examples, please refer to our supplementary materials.
",6.3 The Generated Corpus,[0],[0]
"Table 6 shows the performance of a topperforming system for the SQuAD dataset (Document Reader (Chen et al., 2017)) when applied to the development and test set portions of our generated dataset.",6.3 The Generated Corpus,[0],[0]
The system was trained on the training set portion of our dataset.,6.3 The Generated Corpus,[0],[0]
"We use the SQuAD evaluation scripts, which calculate exact match (EM) and F-1 scores.2 Performance of the
2F-1 measures the average overlap between the predicted answer span and ground truth answer (Rajpurkar et al., 2016).
",6.3 The Generated Corpus,[0],[0]
neural machine reading model is reasonable.,6.3 The Generated Corpus,[0],[0]
"We also train the DocReader on our training set and test the models’ performance on the original dev set of SQuAD; for this, the performance is around 45.2% on EM and 56.7% on F-1 metric.",6.3 The Generated Corpus,[0],[0]
"DocReader trained on the original SQuAD training set achieves 69.5% EM, 78.8% F-1 indicating that our dataset is more difficult and/or less natural than the crowd-sourced QA pairs of SQuAD.",6.3 The Generated Corpus,[0],[0]
We propose a new neural network model for better encoding coreference knowledge for paragraphlevel question generation.,7 Conclusion,[0],[0]
Evaluations with different metrics on the SQuAD machine reading dataset show that our model outperforms state-ofthe-art baselines.,7 Conclusion,[0],[0]
The ablation study shows the effectiveness of different components in our model.,7 Conclusion,[0],[0]
"Finally, we apply our question generation framework to produce a corpus of 1.26 million questionanswer pairs, which we hope will benefit the QA research community.",7 Conclusion,[0],[0]
It would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.,7 Conclusion,[0],[0]
We thank the anonymous reviewers and members of Cornell NLP group for helpful comments.,Acknowledgments,[0],[0]
We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence.,abstractText,[0],[0]
We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism.,abstractText,[0],[0]
"Compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-theart.",abstractText,[0],[0]
"We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top-ranking Wikipedia articles and create a corpus of over one million questionanswer pairs.",abstractText,[0],[0]
We also provide a qualitative analysis for this large-scale generated corpus from Wikipedia.,abstractText,[0],[0]
Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4791",text,[0],[0]
"Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016).",1 Introduction,[0],[0]
"Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018).",1 Introduction,[0],[0]
"Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1",1 Introduction,[0],[0]
"Laudably, Hassan et al. (2018) have
1The term “parity” may raise the expectation that there is evidence for equivalence, but the term is used in the definition of “there [being] no statistical significance between [two outputs] for a test set of candidate translations” by Hassan et al. (2018).",1 Introduction,[0],[0]
"Still, we consider this finding noteworthy given the strong evaluation setup.
released their data publicly to allow external validation of their claims.",1 Introduction,[0],[0]
"Their claims are further strengthened by the fact that they follow best practices in human machine translation evaluation, using evaluation protocols and tools that are also used at the yearly Conference on Machine Translation (WMT) (Bojar et al., 2017), and take great care in guarding against some confounds such as test set selection and rater inconsistency.
",1 Introduction,[0],[0]
"However, the implications of a statistical tie between two machine translation systems in a shared translation task are less severe than that of a statistical tie between a machine translation system and a professional human translator, so we consider the results worthy of further scrutiny.",1 Introduction,[0],[0]
We perform an independent evaluation of the professional translation and best machine translation system that were found to be of equal quality by Hassan et al. (2018).,1 Introduction,[0],[0]
"Our main interest lies in the evaluation protocol, and we empirically investigate if the lack of document-level context could explain the inability of human raters to find a quality difference between human and machine translations.",1 Introduction,[0],[0]
"We test the following hypothesis:
A professional translator who is asked to rank the quality of two candidate translations on the document level will prefer a professional human translation over a machine translation.
",1 Introduction,[0],[0]
"Note that our hypothesis is slightly different from that tested by Hassan et al. (2018), which could be phrased as follows:
A bilingual crowd worker who is asked to directly assess the quality of candidate translations on the sentence level will prefer a professional human translation over a machine translation.
",1 Introduction,[0],[0]
"As such, our evaluation is not a direct replication of that by Hassan et al. (2018), and a failure to reproduce their findings does not imply an error on either our or their part.",1 Introduction,[0],[0]
"Rather, we hope to indirectly assess the accuracy of different evaluation protocols.",1 Introduction,[0],[0]
"Our underlying assumption is that professional human translation is still superior to neural machine translation, but that the sensitivity of human raters to these quality differences depends on the evaluation protocol.",1 Introduction,[0],[0]
"Machine translation is typically evaluated by comparing system outputs to source texts, reference translations, other system outputs, or a combination thereof (for examples, see Bojar et al., 2016a).",2 Human Evaluation of Machine Translation,[0],[0]
"The scientific community concentrates on two aspects: adequacy, typically assessed by bilinguals; and target language fluency, typically assessed by monolinguals.",2 Human Evaluation of Machine Translation,[0],[0]
"Evaluation protocols have been subject to controversy for decades (e. g., Van Slype, 1979), and we identify three aspects with particular relevance to assessing human parity: granularity of measurement (ordinal vs. interval scales), raters (experts vs. crowd workers), and experimental unit (sentence vs. document).",2 Human Evaluation of Machine Translation,[0],[0]
Granularity of Measurement Callison-Burch et al. (2007) show that ranking (Which of these translations is better?) leads to better inter-rater agreement than absolute judgement on 5-point Likert scales (How good is this translation?) but gives no insight about how much a candidate translation differs from a (presumably perfect) reference.,2.1 Related Work,[0],[0]
"To this end, Graham et al. (2013) suggest the use of continuous scales for direct assessment of translation quality.",2.1 Related Work,[0],[0]
"Implemented as a slider between 0 (Not at all) and 100 (Perfectly), their method yields scores on a 100-point interval scale in practice (Bojar et al., 2016b, 2017), with each raters’ rating being standardised to increase homogeneity.",2.1 Related Work,[0],[0]
Hassan et al. (2018) use source-based direct assessment to avoid bias towards reference translations.,2.1 Related Work,[0],[0]
"In the shared task evaluation by Cettolo et al. (2017), raters are shown the source and a candidate text, and asked: How accurately does the above candidate text convey the semantics of the source text?",2.1 Related Work,[0],[0]
"In doing so, they have translations produced by humans and machines rated indepen-
dently, and parity is assumed if the mean score of the former does not significantly differ from the mean score of the latter.
",2.1 Related Work,[0],[0]
"Raters To optimise cost, machine translation quality is typically assessed by means of crowdsourcing.",2.1 Related Work,[0],[0]
"Combined ratings of bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009).",2.1 Related Work,[0],[0]
"Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.”",2.1 Related Work,[0],[0]
"However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation.
",2.1 Related Work,[0],[0]
"Experimental Unit Machine translation evaluation is predominantly performed on single sentences, presented to raters in random order (e. g., Bojar et al., 2017; Cettolo et al., 2017).",2.1 Related Work,[0],[0]
There are two main reasons for this.,2.1 Related Work,[0],[0]
"The first is cost: if raters assess entire documents, obtaining the same number of data points in an evaluation campaign multiplies the cost by the average number of sentences per document.",2.1 Related Work,[0],[0]
The second is experimental validity.,2.1 Related Work,[0],[0]
"When comparing systems that produce sentences without considering documentlevel context, the perceived suprasentential cohesion of a system output is likely due to randomness and thus a confounding factor.",2.1 Related Work,[0],[0]
"While incorporating document-level context into machine translation systems is an active field of research (Webber et al., 2017), state-of-the-art systems still operate at the level of single sentences (Sennrich et al., 2017; Vaswani et al., 2017; Hassan et al., 2018).",2.1 Related Work,[0],[0]
"In contrast, human translators can and do take document-level context into account (Krings, 1986).",2.1 Related Work,[0],[0]
The same holds for raters in evaluation campaigns.,2.1 Related Work,[0],[0]
"In the discussion of their results, Wu et al. (2016) note that their raters “[did] not necessarily fully understand each randomly sampled sentence sufficiently” because it was provided with no context.",2.1 Related Work,[0],[0]
"In such setups, raters cannot reward textual cohesion and coherence.
2“Experts” here are computational linguists who develop MT systems, who may not be expert translators.",2.1 Related Work,[0],[0]
"We conduct a quality evaluation experiment with a 2× 2 mixed factorial design, testing the effect of source text availability (adequacy, fluency) and experimental unit (sentence, document) on ratings by professional translators.
",2.2 Our Evaluation Protocol,[0],[0]
Granularity of Measurement We elicit judgements by means of pairwise ranking.,2.2 Our Evaluation Protocol,[0],[0]
"Raters choose the better (with ties allowed) of two translations for each item: one produced by a professional translator (HUMAN), the other by machine translation (MT).",2.2 Our Evaluation Protocol,[0],[0]
"Since our evaluation includes that of human translation, it is reference-free.",2.2 Our Evaluation Protocol,[0],[0]
"We evaluate in two conditions: adequacy, where raters see source texts and translations (Which translation expresses the meaning of the source text more adequately?); and fluency, where raters only see translations (Which text is better English?).
",2.2 Our Evaluation Protocol,[0],[0]
"Raters We recruit professional translators, only considering individuals with at least three years of professional experience and positive client reviews.
",2.2 Our Evaluation Protocol,[0],[0]
"Experimental Unit To test the effect of context on perceived translation quality, raters evaluate entire documents as well as single sentences in random order (i. e., context is a within-subjects factor).",2.2 Our Evaluation Protocol,[0],[0]
"They are shown both translations (HUMAN and MT) for each unit; the source text is only shown in the adequacy condition.
",2.2 Our Evaluation Protocol,[0],[0]
"Quality Control To hedge against random ratings, we convert 5 documents and 16 sentences per set into spam items (Kittur et al., 2008): we render one of the two options nonsensical by shuffling its words randomly, except for 10 % at the beginning and end.
",2.2 Our Evaluation Protocol,[0],[0]
Statistical Analysis We test for statistically significant preference of HUMAN over MT or vice versa by means of two-sided Sign Tests.,2.2 Our Evaluation Protocol,[0],[0]
"Let a be the number of ratings in favour of MT, b the number of ratings in favour of HUMAN, and t the number of ties.",2.2 Our Evaluation Protocol,[0],[0]
"We report the number of successes x and the number of trials n for each test, such that x = b and n = a+ b.3
3Emerson and Simon (1979) suggest the inclusion of ties such that x = b+0.5t and n = a+ b+ t. This modification has no effect on the significance levels reported in this paper.",2.2 Our Evaluation Protocol,[0],[0]
We use the experimental protocol described in the previous section for a quality assessment of Chinese to English translations of news articles.,2.3 Data Collection,[0],[0]
"To this end, we randomly sampled 55 documents and 2×120 sentences from the WMT 2017 test set.",2.3 Data Collection,[0],[0]
"We only considered the 123 articles (documents) which are native Chinese,4 containing 8.13 sentences on average.",2.3 Data Collection,[0],[0]
"Human and machine translations (REFERENCE-HT as HUMAN, and COMBO6 as MT) were obtained from data released by Hassan et al. (2018).5
The sampled documents and sentences were rated by professional translators we recruited from ProZ:6 4 native in Chinese (2), English (1), or both (1) to rate adequacy, and 4 native in English to rate fluency.",2.3 Data Collection,[0],[0]
"On average, translators had 13.7 years of experience and 8.8 positive client reviews on ProZ, and received US$ 188.75 for rating 55 documents and 120 sentences.
",2.3 Data Collection,[0],[0]
"The averages reported above include an additional translator we recruited when one rater showed poor performance on document-level spam items in the fluency condition, whose judgements we exclude from analysis.",2.3 Data Collection,[0],[0]
"We also exclude sentence-level results from 4 raters because there was overlap with the documents they annotated, which means that we cannot rule out that the sentence-level decisions were informed by access to the full document.",2.3 Data Collection,[0],[0]
"To allow for external validation and further experimentation, we make all experimental data publicly available.7",2.3 Data Collection,[0],[0]
"In the adequacy condition, MT and HUMAN are not statistically significantly different on the sentence level (x=86, n=189, p= .244).",3 Results,[0],[0]
This is consistent with the results Hassan et al. (2018) obtained with an alternative evaluation protocol (crowdsourcing and direct assessment; see Section 2.1).,3 Results,[0],[0]
"However, when evaluating entire doc-
4While it is common practice in machine translation to use the same test set in both translation directions, we consider a direct comparison between human “translation” and machine translation hard to interpret if one is in fact the original English text, and the other an automatic translation into English of a human translation into Chinese.",3 Results,[0],[0]
"In concurrent work, Toral et al. (2018) expand on the confounding effect of evaluating text where the target side is actually the original document.
5 http://aka.ms/Translator-HumanParityData 6 https://www.proz.com 7 https://github.com/laeubli/parity
uments, raters show a statistically significant preference for HUMAN (x=104, n=178, p<.05).",3 Results,[0],[0]
"While the number of ties is similar in sentenceand document-level evaluation, preference for MT drops from 50 to 37 % in the latter (Figure 1a).
",3 Results,[0],[0]
"In the fluency condition, raters prefer HUMAN on both the sentence (x= 106, n=172, p<.01) and document level (x=99, n=143, p< .001).",3 Results,[0],[0]
"In contrast to adequacy, fluency ratings in favour of HUMAN are similar in sentence- and document-level evaluation, but raters find more ties with document-level context as preference for MT drops from 32 to 22 % (Figure 1b).
",3 Results,[0],[0]
We note that these large effect sizes lead to statistical significance despite modest sample size.,3 Results,[0],[0]
Inter-annotator agreement (Cohen’s κ) ranges from 0.13 to 0.32 (see Appendix for full results and discussion).,3 Results,[0],[0]
Our results emphasise the need for suprasentential context in human evaluation of machine translation.,4 Discussion,[0],[0]
"Starting with Hassan et al.’s (2018) finding of no statistically significant difference in translation quality between HUMAN and MT for their Chinese–English test set, we set out to test this result with an alternative evaluation protocol which we expected to strengthen the ability of raters to judge translation quality.",4 Discussion,[0],[0]
"We employed professional translators instead of crowd workers, and pairwise ranking instead of direct assessment, but in a sentence-level evaluation of adequacy, raters still found it hard to discriminate between HUMAN and MT: they did not show a statistically significant preference for either of them.
",4 Discussion,[0],[0]
"Conversely, we observe a tendency to rate HUMAN more favourably on the document level than on the sentence level, even within single raters.",4 Discussion,[0],[0]
Adequacy raters show a statistically significant preference for HUMAN when evaluating entire documents.,4 Discussion,[0],[0]
"We hypothesise that document-level evaluation unveils errors such as mistranslation of an ambiguous word, or errors related to textual cohesion and coherence, which remain hard or impossible to spot in a sentence-level evaluation.",4 Discussion,[0],[0]
"For a subset of articles, we elicited both sentence-level and document-level judgements, and inspected articles for which sentence-level judgements were mixed, but where HUMAN was strongly preferred in document-level evaluation.",4 Discussion,[0],[0]
"In these articles, we do indeed observe the hypothesised phenomena.",4 Discussion,[0],[0]
"We find an example of lexical coherence in a 6-sentence article about a new app “微信挪 车”, which HUMAN consistently translates into “WeChat Move the Car”.",4 Discussion,[0],[0]
"In MT, we find three different translations in the same article: “Twitter Move Car”, “WeChat mobile”, and “WeChat Move”.",4 Discussion,[0],[0]
"Other observations include the use of more appropriate discourse connectives in HUMAN, a more detailed investigation of which we leave to future work.
",4 Discussion,[0],[0]
"To our surprise, fluency raters show a stronger preference for HUMAN than adequacy raters (Figure 1).",4 Discussion,[0],[0]
"The main strength of neural machine translation in comparison to previous statistical approaches was found to be increased fluency, while adequacy improvements were less clear (Bojar et al., 2016b; Castilho et al., 2017b), and we expected a similar pattern in our evaluation.",4 Discussion,[0],[0]
"Does this indicate that adequacy is in fact a strength of
MT, not fluency?",4 Discussion,[0],[0]
We are wary to jump to this conclusion.,4 Discussion,[0],[0]
"An alternative interpretation is that MT, which tends to be more literal than HUMAN, is judged more favourably by raters in the bilingual condition, where the majority of raters are native speakers of the source language, because of L1 interference.",4 Discussion,[0],[0]
We note that the availability of document-level context still has a strong impact in the fluency condition (Section 3).,4 Discussion,[0],[0]
"In response to recent claims of parity between human and machine translation, we have empirically tested the impact of sentence and document level context on human assessment of machine translation.",5 Conclusions,[0],[0]
"Raters showed a markedly stronger preference for human translations when evaluating at the level of documents, as compared to an evaluation of single, isolated sentences.
",5 Conclusions,[0],[0]
We believe that our findings have several implications for machine translation research.,5 Conclusions,[0],[0]
"Most importantly, if we accept our interpretation that human translation is indeed of higher quality in the dataset we tested, this points to a failure of current best practices in machine translation evaluation.",5 Conclusions,[0],[0]
"As machine translation quality improves, translations will become harder to discriminate in terms of quality, and it may be time to shift towards document-level evaluation, which gives raters more context to understand the original text and its translation, and also exposes translation errors related to discourse phenomena which remain invisible in a sentence-level evaluation.
",5 Conclusions,[0],[0]
"Our evaluation protocol was designed with the aim of providing maximal validity, which is why we chose to use professional translators and pairwise ranking.",5 Conclusions,[0],[0]
"For future work, it would be of high practical relevance to test whether we can also elicit accurate quality judgements on the document-level via crowdsourcing and direct assessment, or via alternative evaluation protocols.",5 Conclusions,[0],[0]
"The data released by Hassan et al. (2018) could serve as a test bed to this end.
",5 Conclusions,[0],[0]
"One reason why document-level evaluation widens the quality gap between machine translation and human translation is that the machine translation system we tested still operates on the sentence level, ignoring wider context.",5 Conclusions,[0],[0]
It will be interesting to explore to what extent existing and future techniques for document-level machine translation can narrow this gap.,5 Conclusions,[0],[0]
"We ex-
pect that this will require further efforts in creating document-level training data, designing appropriate models, and supporting research with discourse-aware automatic metrics.",5 Conclusions,[0],[0]
We thank Xin Sennrich for her help with the analysis of translation errors.,Acknowledgements,[0],[0]
We also thank Antonio Toral and the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese– English news translation task.,abstractText,[0],[0]
"We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents.",abstractText,[0],[0]
"In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences.",abstractText,[0],[0]
Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.,abstractText,[0],[0]
Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,title,[0],[0]
"Sequential LSTMs have been extended to model tree structures, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain references to input words for each node. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure. Experiments show that both extensions give better representations of tree structures. Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.",text,[0],[0]
Both sequence structured and tree structured neural models have been applied to NLP problems.,1 Introduction,[0],[0]
"Seminal work uses convolutional neural networks (Collobert and Weston, 2008), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and recursive neural networks (Socher et al., 2011) for sequence and tree modeling.",1 Introduction,[0],[0]
"Long short-term memory (LSTM) networks have significantly improved accuracies in a variety of sequence tasks (Sutskever et al., 2014; Bahdanau et al., 2015) compared to
vanilla recurrent neural networks.",1 Introduction,[0],[0]
"Addressing diminishing gradients effectively, they have been extended to tree structures, achieving promising results for tasks such as syntactic language modeling (Zhang et al., 2016), sentiment analysis (Li et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015; Tai et al., 2015; Teng et al., 2016) and relation extraction (Miwa and Bansal, 2016).
",1 Introduction,[0],[0]
"Depending on the node type, typical tree structures in NLP can be categorized to constituent trees and dependency trees.",1 Introduction,[0],[0]
A salient difference between the two types of tree structures is in the node.,1 Introduction,[0],[0]
"While dependency tree nodes are input words themselves, constituent tree nodes represent syntactic constituents.",1 Introduction,[0],[0]
Only leaf nodes in constituent trees correspond to words.,1 Introduction,[0],[0]
"Though LSTM structures have been developed for both types of trees above, we investigate constituent trees in this paper.",1 Introduction,[0],[0]
"There are three existing methods for constituent tree LSTMs (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structured LSTMs.",1 Introduction,[0],[0]
"We take the method of Zhu et al. (2015) as our baseline.
",1 Introduction,[0],[0]
"Figure 1 shows the sequence structured LSTM of Hochreiter and Schmidhuber (1997) and the treestructured LSTM of Zhu et al. (2015), illustrating the input (x), cell (c) and hidden (h) nodes at a certain time step t. The most important difference between Figure 1(a) and Figure 1(b) is the branching factor.",1 Introduction,[0],[0]
"While a cell in the sequence structure LSTM depends on the single previous hidden node, a cell in the tree-structured LSTM depends on a left hidden node and a right hidden node.",1 Introduction,[0],[0]
"Such tree-structured extensions of the sequence structured LSTM assume
163
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"163–177, 2017.",1 Introduction,[0],[0]
Action Editor: Scott Yih.,1 Introduction,[0],[0]
"Submission batch: 5/2016; Revision batch: 12/2016; Published 6/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
that the constituent tree is binarized, building hidden nodes from the input words in the bottom-up direction.",1 Introduction,[0],[0]
"The leaf node structure is shown in Figure 1(c).
",1 Introduction,[0],[0]
A second salient difference between the two types of LSTMs is the modeling of input words.,1 Introduction,[0],[0]
"While each cell in the sequence structure LSTM directly depends on its corresponding input word (Figure 1(a)), only leaf cells in the tree structure LSTM directly depend on corresponding input words (Figure 1(c)).",1 Introduction,[0],[0]
"This corresponds well to the constituent tree structure, where there is no direct association between non-leaf constituent nodes and input words.",1 Introduction,[0],[0]
"However, it leaves the tree structure a degraded version of a perfect binary-branching variation of the sequence-structure LSTM, with one important source of information (i.e. words) missing in forming a cell (Figure 1(b)).
",1 Introduction,[0],[0]
"We fill this gap by proposing an extension to the tree LSTM model, injecting lexical information into every node in the tree.",1 Introduction,[0],[0]
"Our method takes inspiration from work on head-lexicalization, which shows that each node in a constituent tree structure is governed by a head word.",1 Introduction,[0],[0]
"As shown in Figure 2, the head word for the verb phrase “visited Mary” is “visited”, and the head word of the adverb phrase “this afternoon” is “afternoon”.",1 Introduction,[0],[0]
"Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004).",1 Introduction,[0],[0]
"Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in Figure 1(b).
",1 Introduction,[0],[0]
"Traditional head-lexicalization relies on specific rules (Collins, 2003; Zhang and Clark, 2009), typically extracting heads from constituent treebanks according to certain grammar formalisms.",1 Introduction,[0],[0]
"For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent, which is language- and formalism-dependent.
",1 Introduction,[0],[0]
"Based on such head lexicalization, we further make a bidirectional extension of the tree structured LSTM, propagating information in the top-down direction as well as the bottom-up direction.",1 Introduction,[0],[0]
"This is analogous to the bidirectional extension of sequence structured LSTMs, which are commonly used for NLP tasks such as speech recognition (Graves et al., 2013), sentiment analysis (Tai et al., 2015; Li et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) tasks.
",1 Introduction,[0],[0]
Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of Zhu et al. (2015).,1 Introduction,[0],[0]
We achieve the best reported results for sentiment classification.,1 Introduction,[0],[0]
"Interestingly, the head lexical information that is learned automatically from the sentiment treebank consists of both syntactic head information and key sentiment word information.",1 Introduction,[0],[0]
This shows the advantage of automatic head-finding as compared with rule-based head lexicalization.,1 Introduction,[0],[0]
We make our code available under GPL at https://github.com/ zeeeyang/lexicalized_bitreelstm.,1 Introduction,[0],[0]
"LSTM Recurrent neural network (RNN) (Elman, 1990; Mikolov et al., 2010) achieves success on
modeling linear structures due to its ability to preserve history over arbitrary length sequences.",2 Related Work,[0],[0]
"At each step, RNN decides its hidden state based on both the current input and the previous hidden state.",2 Related Work,[0],[0]
"In theory, it can carry over unbounded history.",2 Related Work,[0],[0]
"Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a special type of RNN that leverages multiple gate vectors and a memory cell vector to solve the vanishing and exploding gradient problems of training RNNs.",2 Related Work,[0],[0]
"It has been successfully applied to parsing (Vinyals et al., 2015a), sentiment classification (Tai et al., 2015; Li et al., 2015), speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) and image captioning (Vinyals et al., 2015b).",2 Related Work,[0],[0]
"There are many variants of sequential LSTMs, such as simple Gated Recurrent Neural Networks (Cho et al., 2014).",2 Related Work,[0],[0]
Greff et al. (2017) compared various architectures of LSTM.,2 Related Work,[0],[0]
"In this paper, we take the standard LSTM with peephole connections (Gers and Schmidhuber, 2000) as a baseline.
",2 Related Work,[0],[0]
Structured LSTM There has been a line of research that extends the standard sequential LSTM in order to model more complex structures.,2 Related Work,[0],[0]
Kalchbrenner et al. (2016) proposed Grid LSTMs to process multi-dimensional data.,2 Related Work,[0],[0]
Theis and Bethge (2015) proposed Spatial LSTMs to handle image data.,2 Related Work,[0],[0]
Dyer et al. (2015) designed Stack LSTMs by adding a top pointer to sequential LSTMs to deal with push and pop sequences of a stack.,2 Related Work,[0],[0]
"Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015) extended sequential LSTMs to Tree-Structured LSTMs (Tree LSTMs) by adding branching factors.",2 Related Work,[0],[0]
"Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification.",2 Related Work,[0],[0]
Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary.,2 Related Work,[0],[0]
"In addition, Li et al. (2016) employed graph gated units to model graph-based structures.
",2 Related Work,[0],[0]
Tree LSTM,2 Related Work,[0],[0]
"The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees.",2 Related Work,[0],[0]
"In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who
explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks.",2 Related Work,[0],[0]
"These can also be regarded as variants of Tree LSTMs.
",2 Related Work,[0],[0]
"Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree.",2 Related Work,[0],[0]
"The difference between Zhu et al. (2015) and Le and Zuidema (2015) is that besides using two forget gates, Le and Zuidema (2015) also make use of two input gates to let a node know its sibling.",2 Related Work,[0],[0]
Tai et al. (2015) introduced Child-Sum Tree LSTM and Nary Tree LSTM.,2 Related Work,[0],[0]
"Child-Sum Tree LSTMs can support multiple children, while N-ary Tree LSTMs work for trees with a branching factor of at most N .",2 Related Work,[0],[0]
"In this perspective, Binary Tree LSTM is a special case of N-ary Tree LSTM with N = 2.
",2 Related Work,[0],[0]
"When a Child-Sum Tree LSTM is applied to a dependency tree, it is referred to as a Dependency Tree LSTM.",2 Related Work,[0],[0]
A Binary Tree LSTM is also referred to as a Constituent Tree LSTM.,2 Related Work,[0],[0]
"Based on Tai et al. (2015), Miwa and Bansal (2016) introduced a Tree LSTM model that can handle different types of children.",2 Related Work,[0],[0]
"A dependency tree naturally contains lexical information at every node, while only leaf nodes contain lexical information in a constituent tree.",2 Related Work,[0],[0]
"None of these methods (Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015) make direct use of lexical input for internal nodes when using constituent Tree LSTMs.
",2 Related Work,[0],[0]
"Bi-LSTM Another common extension to sequential LSTM is to include bidirectional information (Graves et al., 2013), which can model history both left-to-right and right-to-left.",2 Related Work,[0],[0]
"The aforementioned Tree LSTM models (Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015) propagate the history of children to their parent in the bottom-up direction only, while ignoring the top-down information flow from parents to children.",2 Related Work,[0],[0]
Zhang et al. (2016) proposed a top-down Tree LSTM to estimate the generation probability of a dependency tree.,2 Related Work,[0],[0]
"However, no corresponding bottom-up Tree LSTM is incorporated into their model.
",2 Related Work,[0],[0]
Paulus et al. (2014) leveraged bidirectional information over recursive binary trees by propagating global belief down from the tree root to leaf nodes.,2 Related Work,[0],[0]
"However, their model is based on recursive neural
network rather than LSTM.",2 Related Work,[0],[0]
Miwa and Bansal (2016) adopted a bidirectional Tree LSTM model to jointly extract named entities and relations under a dependency tree structure.,2 Related Work,[0],[0]
"For constituent tree structures, however, their model does not work due to lack of word inputs on non-leaf constituent nodes, and in particular the root node.",2 Related Work,[0],[0]
Our head lexicalization allows us to investigate the top-down constituent Tree LSTM.,2 Related Work,[0],[0]
"To our knowledge, we are the first to report a bidirectional constituent Tree LSTM.",2 Related Work,[0],[0]
"A sequence-structure LSTM estimates a sequence of hidden state vectors given a sequence of input vectors, through the calculation of a sequence of hidden cell vectors using a gate mechanism.",3 Baselines,[0],[0]
"For NLP, the input vectors are typically word embeddings (Mikolov et al., 2013), but can also include partof-speech (POS) embeddings, character embeddings or other types of information.",3 Baselines,[0],[0]
"For notational convenience, we refer to the input vectors as lexical vectors.
",3 Baselines,[0],[0]
"Formally, given an input vector sequence x1, x2, . . .",3 Baselines,[0],[0]
", xn, each state vector ht is estimated from the Hadamard product of a cell vector ct and a corresponding output gate vector ot
ht = ot ⊗ tanh(ct) (1)
Here the cell vector depends on both the previous cell vector ct, and a combination of the previous state vector ht−1; the current input vector xt:
ct = ft ⊗ ct−1 + it ⊗ gt gt = tanh(Wxgxt +Whght−1 + bg)
(2)
",3 Baselines,[0],[0]
"The combination of ct−1 and gt is controlled by the Hadamard product between a forget gate vector ft and an input gate vector it, respectively.",3 Baselines,[0],[0]
"The gates ot, ft and it are defined as follows
it = σ(Wxixt +Whiht−1 +Wcict−1 + bi)
ft = σ(Wxfxt",3 Baselines,[0],[0]
"+Whfht−1 +Wcfct−1 + bf )
",3 Baselines,[0],[0]
"ot = σ(Wxoxt +Whoht−1 +Wcoct + bo),
(3)
where σ is the sigmoid function.",3 Baselines,[0],[0]
"Wxg, Whg, bg, Wxi, Whi, Wci, bi, Wxf , Whf , Wcf , bf , Wxo, Who, Wco and bo are model parameters.
",3 Baselines,[0],[0]
"The bottom-up Tree LSTM of Zhu et al. (2015) extends the left-to-right sequence LSTM by splitting
the previous state vector ht−1 into a left child state vector hLt−1 and a right child state vector h R t−1, and the previous cell vector ct−1 into a left child cell vector cLt−1 and a right child cell vector c R t−1, calculating ct as
ct = f L t ⊗ cLt−1 + fRt ⊗ cRt−1 +",3 Baselines,[0],[0]
"it ⊗ gt, (4)
and the input/output gates it/ot as
it = σ",3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhih N",3 Baselines,[0],[0]
t−1 +W N ci c N t−1) +,3 Baselines,[0],[0]
"bi
)
",3 Baselines,[0],[0]
ot = σ,3 Baselines,[0],[0]
"( ∑
N∈{L,R} WNhoh N t−1 +Wcoct + bo
) (5)
",3 Baselines,[0],[0]
"The forget gate ft is split into fLt and f R t for regulating cLt−1 and c R t−1, respectively:
fLt = σ",3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhflh N t−1 +W N cfl cNt−1)",3 Baselines,[0],[0]
"+ bfl
)
",3 Baselines,[0],[0]
fRt = σ,3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhfrh N t−1 +W N cfrc N t−1) + bfr
)
(6)
",3 Baselines,[0],[0]
"gt depends on both hLt−1 and h R t−1, but as shown in Figure 1 (b), it does not depend on xt
gt = tanh ( ∑
N∈{L,R} WNhgh N t−1 + bg
) (7)
Finally, the hidden state vector ht is calculated in the same way as in the sequential LSTM model shown in Equation 1.",3 Baselines,[0],[0]
"WLhi, W R hi, W L ci , W R ci , bi, W L ho, WRho, Wco, bo, W L hfl , WRhfl , W L cfl , WRcfl , bfl , W L hfr
, WRhfr , W L cfr , WRcfr , bfr , W L hg, W R hg",3 Baselines,[0],[0]
and bg are model parameters.,3 Baselines,[0],[0]
We introduce an input lexical vector xt to the calculation of each cell vector ct via a bottom-up head propagation mechanism.,4.1 Head Lexicalization,[0],[0]
"As shown in the shaded nodes in Figure 3 (b), the head propagation mechanism is parallel to the cell propagation mechanism.",4.1 Head Lexicalization,[0],[0]
"In contrast, the method of Zhu et al. (2015) in Figure 3 (a) does not have the input vector xt for non-leaf constituents.
",4.1 Head Lexicalization,[0],[0]
There are multiple ways to choose a head lexicon for a given binary-branching constituent.,4.1 Head Lexicalization,[0],[0]
"One
simple method is to choose the head lexicon of the left child as the head (left-headedness).",4.1 Head Lexicalization,[0],[0]
"Correspondingly, an alternative is to use the right child for head lexicon.",4.1 Head Lexicalization,[0],[0]
There is less consistency in the governing head lexicons across variations of the same type of constituents with slightly different typologies.,4.1 Head Lexicalization,[0],[0]
"Hence, simple baselines can be less effective compared to linguistically motivated head findings.
",4.1 Head Lexicalization,[0],[0]
"Rather than selecting head lexicons using manually-defined head-finding rules, which are language- and formalism-dependent (Collins, 2003), we cast head finding as a part of the neural network model, learning the head lexicon of each constituent by a gated combination of the head lexicons of its two children1.",4.1 Head Lexicalization,[0],[0]
"Formally,
xt",4.1 Head Lexicalization,[0],[0]
= zt ⊗ xLt−1 +,4.1 Head Lexicalization,[0],[0]
"(1− zt)⊗ xRt−1, (8)
where xt represents the head lexicon vector of the current constituent, xLt−1 represents the head lexicon of its left child constituent, and xRt−1 represents the head lexicon of its right child constituent.",4.1 Head Lexicalization,[0],[0]
"The gate zt is calculated based on xLt−1 and x R t−1,
zt = σ(W L zxx L t−1 +W R zxx R t−1 + bz) (9)
Here WLzx, W R zx and bz are model parameters.",4.1 Head Lexicalization,[0],[0]
"Given head lexicon vectors for nodes, the Tree LSTM of Zhu et al. (2015) can be extended by leveraging xt in calculating the corresponding ct.",4.2 Lexicalized Tree LSTM,[0],[0]
"In particular, xt is used to estimate the input (it), output
1In this paper, we work on binary trees only, which is a common form for CKY and shift-reduce parsing.",4.2 Lexicalized Tree LSTM,[0],[0]
"Typical binarization methods, such as head binarization (Klein and Manning, 2003) , also rely on specific head-finding rules.
",4.2 Lexicalized Tree LSTM,[0],[0]
(ot) and forget (fRt and f L t ),4.2 Lexicalized Tree LSTM,[0],[0]
"gates:
it = σ",4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxixt+
∑
N∈{L,R} (WNhih N",4.2 Lexicalized Tree LSTM,[0],[0]
t−1 +W N ci c N t−1) +,4.2 Lexicalized Tree LSTM,[0],[0]
"bi
)
",4.2 Lexicalized Tree LSTM,[0],[0]
fLt = σ,4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxfxt+
∑
N∈{L,R} (WNhflh N t−1 +W N cfl cNt−1)",4.2 Lexicalized Tree LSTM,[0],[0]
"+ bfl
)
",4.2 Lexicalized Tree LSTM,[0],[0]
fRt = σ,4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxfxt+
∑
N∈{L,R} (WNhfrh N t−1 +W N cfrc N t−1) + bfr
)
",4.2 Lexicalized Tree LSTM,[0],[0]
"ot = σ ( Wxoxt+
∑
N∈{L,R} WNhoh N t−1 +Wcoct + bo
)
(10)
",4.2 Lexicalized Tree LSTM,[0],[0]
"In addition, xt is also used in computing gt,
gt = tanh ( Wxgxt + ∑
N∈{L,R} WNhgh N t−1 + bg
) (11)
",4.2 Lexicalized Tree LSTM,[0],[0]
"With the new definition of it, fRt , f L t and gt, the computing of ct remains the same as the baseline Tree LSTM model as shown in Equation 4.",4.2 Lexicalized Tree LSTM,[0],[0]
"Similarly, ht remains the Hadamard product of ct and the new ot as shown in Equation 1.
",4.2 Lexicalized Tree LSTM,[0],[0]
"In this model, Wxi, Wxf , Wxg and Wxo are newly-introduced model parameters.",4.2 Lexicalized Tree LSTM,[0],[0]
The use of xt in computing the gate and cell values are consistent with those in the baseline sequential LSTM.,4.2 Lexicalized Tree LSTM,[0],[0]
Given a sequence of input vectors,4.3 Bidirectional Extensions,[0],[0]
"[x1, x2, . . .",4.3 Bidirectional Extensions,[0],[0]
", xn], a bidirectional sequential LSTM (Graves et al., 2013) computes two sets of hidden state vectors, [h̃1, h̃2, . . .",4.3 Bidirectional Extensions,[0],[0]
", h̃n] and",4.3 Bidirectional Extensions,[0],[0]
"[h̃′n, h̃ ′",4.3 Bidirectional Extensions,[0],[0]
"n−1, . .",4.3 Bidirectional Extensions,[0],[0]
.,4.3 Bidirectional Extensions,[0],[0]
", h̃ ′",4.3 Bidirectional Extensions,[0],[0]
"1] in the left-to-right and the right-to-left directions, respectively.",4.3 Bidirectional Extensions,[0],[0]
"The final hidden state hi of the input xi is the concatenation of the corresponding state vectors in the two LSTMs,",4.3 Bidirectional Extensions,[0],[0]
hi = h̃i ⊕ h̃′n−i+1 (12),4.3 Bidirectional Extensions,[0],[0]
The two LSTMs can share the same model parameters or use different parameters.,4.3 Bidirectional Extensions,[0],[0]
"We choose the latter in our baseline experiments.
",4.3 Bidirectional Extensions,[0],[0]
"We make a bidirectional extension to the Lexicalized Tree LSTM in Section 4.2 by following the sequential LSTMs in Section 3, adding an additional
set of hidden state vectors in the top-down direction.",4.3 Bidirectional Extensions,[0],[0]
"Different from the bottom-up direction, each hidden state in the top-down LSTM has exactly one predecessor.",4.3 Bidirectional Extensions,[0],[0]
"In fact, the path from the root of a tree down to any node forms a sequential LSTM.
",4.3 Bidirectional Extensions,[0],[0]
"Note, however, that two different sets of model parameters are used when the current node is the left and the right child of its predecessor.",4.3 Bidirectional Extensions,[0],[0]
"Denoting the two sets of parameters as UL and UR, respectively, the hidden state vector h7 in Figure 4 is calculated from the hidden state vector h1 using the parameter set sequence [UL,UL,UR].",4.3 Bidirectional Extensions,[0],[0]
"Similarly, h8 is calculated from h1 using [UL,UR,UL].",4.3 Bidirectional Extensions,[0],[0]
"At each step t, the computing of ht follows the sequential LSTM model:
ht = ot ⊗ tanh(ct−1)",4.3 Bidirectional Extensions,[0],[0]
ct = ft ⊗ ct−1 + it ⊗ gt gt =,4.3 Bidirectional Extensions,[0],[0]
tanh(W N xg↓xt−1,4.3 Bidirectional Extensions,[0],[0]
"+W N hg↓ht−1 + b N g↓)
(13)
",4.3 Bidirectional Extensions,[0],[0]
"With the gate values being defined as:
it = σ(W N xi↓xt +W N hi↓ht−1 +W N ci↓ct−1 + b N i↓) ft = σ(W N xf↓xt",4.3 Bidirectional Extensions,[0],[0]
+W N,4.3 Bidirectional Extensions,[0],[0]
hf↓ht−1,4.3 Bidirectional Extensions,[0],[0]
+W N cf↓ct−1 + b N f↓),4.3 Bidirectional Extensions,[0],[0]
ot = σ(W N xo↓xt,4.3 Bidirectional Extensions,[0],[0]
"+W N ho↓ht−1 +W N co↓ct + b N o↓)
",4.3 Bidirectional Extensions,[0],[0]
"(14)
Here N ∈ {L,R} and UN = {WNxg↓,WNhg↓, bNg↓,W N xi↓,W N",4.3 Bidirectional Extensions,[0],[0]
"hi↓,W N ci↓, b N i↓ ,W N xf↓,W N hf↓,W N cf↓, b N f↓, WNxo↓,W N ho↓,W N",4.3 Bidirectional Extensions,[0],[0]
"co↓, b N o↓}.",4.3 Bidirectional Extensions,[0],[0]
UL and UR are model parameters in the top-down Tree LSTM.,4.3 Bidirectional Extensions,[0],[0]
"One final note is that the top-down Tree LSTM is enabled by the head propagation mechanism, which allows a head lexicon node to be made available for the root constituent node.",4.3 Bidirectional Extensions,[0],[0]
"Without such information, it would be difficult to build top-down LSTM for constituent trees.",4.3 Bidirectional Extensions,[0],[0]
"We apply the bidirectional Tree LSTM to classification tasks, where the input is a sentence with its binarized constituent tree, and the output is a discrete label.",5 Usage for Classification,[0],[0]
"We denote the bottom-up hidden state vector of the root as h̃ROOT↑, the top-down hidden state vector of the root as h̃ROOT↓ and the top-down hidden state vectors of the input words x1, x2, . . .",5 Usage for Classification,[0],[0]
", xn as h̃′1, h̃ ′ 2, . . .",5 Usage for Classification,[0],[0]
", h̃ ′",5 Usage for Classification,[0],[0]
n.,5 Usage for Classification,[0],[0]
"We take the concatenation of h̃ROOT↑, h̃ROOT↓ and the average of h̃′1, h̃ ′ 2, . . .",5 Usage for Classification,[0],[0]
", h̃ ′ n",5 Usage for Classification,[0],[0]
"as the final representation h of the sentence:
h = h̃ROOT↑ ⊕",5 Usage for Classification,[0],[0]
"h̃ROOT↓ ⊕ 1
n
n∑
i=1
h̃′i (15)
",5 Usage for Classification,[0],[0]
"A softmax classifier is used to predict the probability pj of sentiment label j from h by
hl = ReLU(Whlh+ bhl) P = softmax(Wlphl + blp)
pj = P",5 Usage for Classification,[0],[0]
"[j],
(16)
where Whl, bhl, Wlp and blp are model parameters, and ReLU is the rectifier function f(x) = max(0, x).",5 Usage for Classification,[0],[0]
"During prediction, the largest probability component of P will be taken as the answer.",5 Usage for Classification,[0],[0]
We train our classifier to maximize the conditional log-likelihood of gold labels of training samples.,6 Training,[0],[0]
"Formally, given a training set of size |D|, the training objective is defined by
L(Θ) =",6 Training,[0],[0]
"− |D|∑
i=1
log pyi + λ
2 ||Θ||2, (17)
where Θ is the set of model parameters, λ is a regularization parameter, yi is the gold label of the ith training sample and pyi is obtained according to Equation 16.",6 Training,[0],[0]
"For sequential LSTM models, we collect errors over each sequence.",6 Training,[0],[0]
"For Tree LSTMs, we sum up errors at every node.
",6 Training,[0],[0]
"The model parameters are optimized using ADAM (Kingma and Ba, 2015) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input
2https://github.com/clab/dynet
embeddings with a fixed probability pdrop to avoid overfitting.",6 Training,[0],[0]
"pdrop is set to 0.5 for all tasks.
",6 Training,[0],[0]
"Following Tai et al. (2015), Li et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), we use Glove-300d word embeddings3 to train our model.",6 Training,[0],[0]
The pretrained word embeddings are fine-tuned for all tasks.,6 Training,[0],[0]
Unknown words are handled in two steps.,6 Training,[0],[0]
"First, if a word is not contained in the pretrained word embeddings, but its lowercased form exists in the embedding table, we use the lowercase as a replacement.",6 Training,[0],[0]
"Second, if both the original word and its lowercased form cannot be found, we treat the word as unk.",6 Training,[0],[0]
"The embedding vector of the UNK token is initialized as the average of all embedding vectors.
",6 Training,[0],[0]
We use one hidden layer and the same dimensionality settings for both sequential and Tree LSTMs.,6 Training,[0],[0]
LSTM hidden states are of size 150.,6 Training,[0],[0]
"The output hidden size is 128 and 64 for the sentiment classification task and the question type classification task, respectively.",6 Training,[0],[0]
Each model is trained for 30 iterations.,6 Training,[0],[0]
"The same training procedure repeats five times using different random seeds, with parameters being evaluated at the end of every iteration on the development set.",6 Training,[0],[0]
The model that gives the best development result is used for final tests.,6 Training,[0],[0]
The effectiveness of our model is tested mainly on a sentiment classification task and a question type classification task.,7 Experiments,[0],[0]
Sentiment Classification.,7.1 Tasks,[0],[0]
"For sentiment classification, we use the same data settings as Zhu et al. (2015).",7.1 Tasks,[0],[0]
"Specifically, we use the Stanford Sentiment Treebank (Socher et al., 2013b).",7.1 Tasks,[0],[0]
Each sentence is annotated with a constituent tree.,7.1 Tasks,[0],[0]
Every internal node corresponds to a phrase.,7.1 Tasks,[0],[0]
"Each node is manually assigned an integer sentiment label from 0 to 4, that correspond to five sentiment classes: very negative, negative, neutral, positive and very positive, respectively.",7.1 Tasks,[0],[0]
"The root label represents the sentiment label of the whole sentence.
",7.1 Tasks,[0],[0]
We perform both binary classification and finegrained classification.,7.1 Tasks,[0],[0]
"Following previous work, we use labels of all phrases for training.",7.1 Tasks,[0],[0]
"Gold-standard
3http://nlp.stanford.edu/data/glove.840B.300d.zip
tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015; Tai et al., 2015).",7.1 Tasks,[0],[0]
"Accuracies are evaluated for both the sentence root labels and phrase labels.
",7.1 Tasks,[0],[0]
Question Type Classification.,7.1 Tasks,[0],[0]
"For the question type classification task, we use the TREC data (Li and Roth, 2002).",7.1 Tasks,[0],[0]
Each training sample in this dataset contains a question sentence and its corresponding question type.,7.1 Tasks,[0],[0]
"We work on the sixway coarse classification task, where the six question types are ENTY, HUM, LOC, DESC, NUM and ABBR, corresponding to ENTITY, HUMAN, LOCATION, DESCRIPTION, NUMERIC VALUE and ABBREVIATION, respectively.",7.1 Tasks,[0],[0]
"For example, the type for the sentence “What year did the Titanic sink?” is NUM.",7.1 Tasks,[0],[0]
"The training set consists of 5,452 examples and the test set contains 500 examples.",7.1 Tasks,[0],[0]
"Since there is no development set, we follow Zhou et al. (2015), randomly extracting 500 examples from the training set as a development set.",7.1 Tasks,[0],[0]
"Unlike the sentiment treebank, there is no annotated tree for each sentence.",7.1 Tasks,[0],[0]
"Instead, we obtain an automatically parsed tree for each sentence using ZPar4 off-the-shelf (Zhang and Clark, 2011).",7.1 Tasks,[0],[0]
"Another difference between the TREC data and the sentiment treebank is that there is only one label, at the root node, rather than a label for each phrase.",7.1 Tasks,[0],[0]
We consider two models for our baselines.,7.2 Baselines,[0],[0]
"The first is bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).",7.2 Baselines,[0],[0]
Our bidirectional constituency Tree LSTM (BiConTree) is compared against BiLSTM to investigate the effectiveness of tree structures.,7.2 Baselines,[0],[0]
"For the sentiment task, following Tai et al. (2015) and Li et al. (2015), we convert the treebank into sequences to allow the bidirectional LSTM model to make use of every phrase span as a training example.",7.2 Baselines,[0],[0]
The second baseline model is the bottom-up Tree LSTM model of Zhu et al. (2015).,7.2 Baselines,[0],[0]
"We compare this model with our lexicalized bidirectional models to show the effects of adding head lexicalization and top-down information flow.
4https://github.com/SUTDNLP/ZPar, version 7.5",7.2 Baselines,[0],[0]
"Table 1 shows the main results for the sentiment classification task, where RNTN is the recursive neural tensor model of Socher et al. (2013b), ConTree and DepTree denote constituency Tree LSTMs and dependency Tree LSTMs, respectively.",7.3 Main Results,[0],[0]
"Our reimplementations of sequential bidirectional LSTM and constituent Tree LSTM (Zhu et al., 2015) give comparable results to the original implementations.
",7.3 Main Results,[0],[0]
"After incorporating head lexicalization into our constituent Tree LSTM, the fine-grained sentiment classification accuracy increases from 51.2 to 52.8, and the binary sentiment classification accuracy increases from 88.5 to 89.2, which demonstrates the effectiveness of the head lexicalization mechanism.
",7.3 Main Results,[0],[0]
Table 1 also shows that a vanilla top-down ConTree LSTM by head-lexicalization (i.e. the topdown half of the final bidirectional model) alone obtains comparable accuracies to the bottom-up ConTree LSTM model.,7.3 Main Results,[0],[0]
"The BiConTree model can further improve the classification accuracies by 0.7 points (fine-grained) and 1.3 points (binary) compared to the unidirectional bottom-up lexicalized ConTree LSTM model, respectively.
",7.3 Main Results,[0],[0]
Table 1 includes 5 class accuracies for all nodes.,7.3 Main Results,[0],[0]
"There is no significant difference between different models, consistent with the observation of Li et al. (2015).",7.3 Main Results,[0],[0]
"To our knowledge, these are the best reported results for this sentiment classification task.
",7.3 Main Results,[0],[0]
Table 2 shows the question type classification results.,7.3 Main Results,[0],[0]
"Our final model gives better results compared
to the BiLSTM model and the bottom-up ConTree model, achieving comparable results to the state-ofthe-art SVM classifier with carefully designed features.",7.3 Main Results,[0],[0]
Introducing head lexicalization and bidirectional extension to the model increases the model complexity.,7.4 Training Time and Model Size,[0],[0]
"In this section, we analyze training time and model size with the fine-grained sentiment classification task.
",7.4 Training Time and Model Size,[0],[0]
We run all the models using an i7-4790 3.60GHz CPU with a single thread.,7.4 Training Time and Model Size,[0],[0]
Table 3 shows the average running time for different models over 30 iterations.,7.4 Training Time and Model Size,[0],[0]
The baseline ConTree model takes about 1.3 hours to finish the training procedure.,7.4 Training Time and Model Size,[0],[0]
"ConTree+Lex takes about 1.5 times longer than ConTree. BiConTree takes about 3.2 hours, which is about 2.5 times longer than that of ConTree.
Table 4 compares the model sizes.",7.4 Training Time and Model Size,[0],[0]
We did not count the number of parameters in the lookup table since these parameters are the same for all models.,7.4 Training Time and Model Size,[0],[0]
"Because the size of LSTM models mainly depends on the dimensionality of the state vector h, we change the size of h to study the effect of model size.",7.4 Training Time and Model Size,[0],[0]
"When |h| = 150, the model size of the baseline model ConTree is the smallest, which consists of about 538K parameters.",7.4 Training Time and Model Size,[0],[0]
The model size of ConTree+Lex is about 1.4 times as large as that of the baseline model.,7.4 Training Time and Model Size,[0],[0]
"The bidirectional model BiConTree is the largest, about 1.7 times as large as that of the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
"However, this parameter set is not very large compared to the modern memory capacity, even for a computer with 16GB RAM.",7.4 Training Time and Model Size,[0],[0]
"In conclusion, in terms of both time, number of parameters and accuracy, head lexicalization method is
a good choice.",7.4 Training Time and Model Size,[0],[0]
Table 4 also helps to clarify whether the gain of the BiConTree model over the ConTree+Lex model is from the top-down information flow or more parameters.,7.4 Training Time and Model Size,[0],[0]
"For the same model, increasing the model size can improve the performance to some extent.",7.4 Training Time and Model Size,[0],[0]
"For example, doubling the size of |h| (75 → 150) increases the performance from 51.5 to 52.8 for the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
"Similarly, we boost the performance of the BiConTree model when doubling the size of |h| from 75 to 150.",7.4 Training Time and Model Size,[0],[0]
"However, doubling the size of |h| from 150 to 300 empirically decreases the performance of the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
The size of the BiConTree model with |h| = 75 is much smaller than that of the ConTree+Lex model with |h| = 150.,7.4 Training Time and Model Size,[0],[0]
"However the performance of these two models is quite close, which indicates that top-down information is useful even for a small model.",7.4 Training Time and Model Size,[0],[0]
A ConTree+Lex model with |h| = 215 and a BiConTree model with |h| = 150 are of similar size.,7.4 Training Time and Model Size,[0],[0]
"The performance of the ConTree+Lex model is again worse than that of the BiConTree model (52.5 v.s. 53.5), which shows the effectiveness of top-down information.",7.4 Training Time and Model Size,[0],[0]
"In this experiment, we investigate the effect of our head lexicalization method over heuristic baselines.",7.5 Head Lexicalization Methods,[0],[0]
"We consider three baseline methods, namely left branching (L), right branching (R) and averaging (A).",7.5 Head Lexicalization Methods,[0],[0]
"For L, a parent node accepts lexical information of its left child while ignoring the right child.",7.5 Head Lexicalization Methods,[0],[0]
"Correspondingly, for R, a parent node accepts lexical information of its right child while ignoring the left child.",7.5 Head Lexicalization Methods,[0],[0]
"For A, a parent node takes the average of the lexical vectors of its children.
",7.5 Head Lexicalization Methods,[0],[0]
"Table 5 shows the accuracies on the test set, where G denotes our gated head lexicalization method de-
scribed in Section 4.1.",7.5 Head Lexicalization Methods,[0],[0]
R gives better results compared to L due to relatively more right-branching structures in this treebank.,7.5 Head Lexicalization Methods,[0],[0]
A simple average yields similar results compared with right branching.,7.5 Head Lexicalization Methods,[0],[0]
"In contrast, G outperforms A method by considering the relative weights of each branch according to treelevel contexts.
",7.5 Head Lexicalization Methods,[0],[0]
"We then investigate what lexical heads can be learned by G. Interestingly, the lexical heads contain both syntactic and sentiment information.",7.5 Head Lexicalization Methods,[0],[0]
"Some heads correspond well to syntactic rules (Collins, 2003), others are driven by subjective words.",7.5 Head Lexicalization Methods,[0],[0]
"Compared to Collins’ rules, our method found 30.68% and 25.72% overlapping heads on the development and test sets, respectively.
",7.5 Head Lexicalization Methods,[0],[0]
"Based on the cosine similarity between the head lexical vector and its children, we visualize the head of a node by choosing the head of the child that gives the largest similarity value.",7.5 Head Lexicalization Methods,[0],[0]
"Figure 5 shows some examples, where <> indicates head words, sentiment labels (e.g. 2, 3) are also included.",7.5 Head Lexicalization Methods,[0],[0]
"In Figure 5a, “Emerges” is the syntactic head word of the whole phrase, which is consistent with Collins-style head finding.",7.5 Head Lexicalization Methods,[0],[0]
"However, “rare” is the head word of the phrase “something rare”, which is different from the syntactic head.",7.5 Head Lexicalization Methods,[0],[0]
"Similar observations are found in Figure 5b, where “good” is the head word of the whole phrase, rather than the syntactic head “place”.",7.5 Head Lexicalization Methods,[0],[0]
The sentiment label of “good” and the sentiment label of the whole phrase are both 3.,7.5 Head Lexicalization Methods,[0],[0]
Figure 5c shows more complex interactions between syntax and sentiment for deciding the head word.,7.5 Head Lexicalization Methods,[0],[0]
"Table 6 shows some example sentences incorrectly predicted by the baseline bottom-up tree model, but correctly labeled by our final model.",7.6 Error Analysis,[0],[0]
"The head word of sentence #1 by our model is “Gloriously”, which is consistent with the sentiment of the whole sentence.",7.6 Error Analysis,[0],[0]
This shows how head lexicalization can affect sentiment classification results.,7.6 Error Analysis,[0],[0]
"Sentences #2 and #3 show the usefulness of top-down informa-
tion for complex semantic structures, where compositionality has subtle effects.",7.6 Error Analysis,[0],[0]
"Our final model improves the results for the ‘very negative’ and ‘very positive’ classes by 10% and 11%, respectively.",7.6 Error Analysis,[0],[0]
"It also boosts the accuracies for sentences with negation (e.g. “not”, “no”, and “none”) by 4.4%.
",7.6 Error Analysis,[0],[0]
"Figure 6 shows the accuracy distribution accord-
ing to the sentence length.",7.6 Error Analysis,[0],[0]
"We find that our model can improve the classification accuracy for longer sentences (>30 words) by 3.5 absolute points compared to the baseline ConTree LSTM of Zhu et al. (2015), which demonstrates the strength of our model for handling long range information.",7.6 Error Analysis,[0],[0]
"By considering bidirectional information over tree structures, our model is aware of more contexts for making better predictions.",7.6 Error Analysis,[0],[0]
"Our main results are obtained on semanticdriven sentence classification tasks, where the automatically-learned head words contain mixed syntactic and semantic information.",8 Applications,[0],[0]
"To further investigate the effectiveness of automatically learned head information on a pure syntactic task, we additionally conduct a simple parser reranking experiment.",8 Applications,[0],[0]
"Further, we discuss findings in language modeling by Kuncoro et al. (2017) on the model of recurrent neural network grammars (Dyer et al., 2016).",8 Applications,[0],[0]
"Finally, we show potential future work leveraging our idea for more tasks.",8 Applications,[0],[0]
We use our tree LSTM models to rerank the 10 best outputs of the Charniak (2000) parser.,8.1 Syntactic Parsing,[0],[0]
"Given a sentence x, suppose that Y (x) is a set of parse tree candidates generated by a baseline parser for x, the goal of a syntactic reranker is to choose the best parsing hypothesis ŷ",8.1 Syntactic Parsing,[0],[0]
"according to a score function f(x, y; Θ).",8.1 Syntactic Parsing,[0],[0]
"Formally,
ŷ = arg maxy∈Y (x){f(x, y; Θ)} (18)
",8.1 Syntactic Parsing,[0],[0]
"For each tree y of sentence x, we follow Socher et al. (2013a) and define the score f(x, y; Θ) as the sum of scores of each constituent node,
f(x, y; Θ) = ∑
r∈node(x,y) Score(r; Θ) (19)
",8.1 Syntactic Parsing,[0],[0]
"Without loss of generality, we take a binary node as an example.",8.1 Syntactic Parsing,[0],[0]
"Given a node A, suppose that its two children are B and C. Let the learned composition state vectors of A, B and C by our proposed TreeLSTM model be nA, nB and nC , respectively.",8.1 Syntactic Parsing,[0],[0]
"The head word vector of node A is hA. Score(A; Θ) is defined as:
oBCA = ReLU(W L s nB +W R s nC",8.1 Syntactic Parsing,[0],[0]
+W H s hA + b s) ScoreBCA,8.1 Syntactic Parsing,[0],[0]
= log(softmax(o BC A )),8.1 Syntactic Parsing,[0],[0]
"[A],
(20)
where WLs , W R s and b s are model parameters.",8.1 Syntactic Parsing,[0],[0]
Training.,8.1 Syntactic Parsing,[0],[0]
"Given a training instance 〈xi, Y (xi)〉 in the training set D, we use a max-margin loss function to train our reranking model.",8.1 Syntactic Parsing,[0],[0]
"Suppose that the oracle parse tree in Y (xi) is yi, the loss function L(Θ) is
L(Θ) = 1
|D|
|D|∑
i=1
ri(Θ) + λ
2 ||Θ||2 (21)
",8.1 Syntactic Parsing,[0],[0]
Here λ is a regularization parameter and ri(Θ) is the margin loss between yi and the highest score tree ŷi predicted by the reranking model.,8.1 Syntactic Parsing,[0],[0]
"ri(Θ) is given by
ri(Θ) = max",8.1 Syntactic Parsing,[0],[0]
"ŷi∈Y (xi) (0, f(xi, ŷi; Θ)+
∆(yi, ŷi)− f(xi, yi; Θ)), (22)
where ∆(yi, ŷi) is the structure loss between yi and ŷi by counting the number of incorrect nodes in the oracle tree:
∆(yi, ŷi) =",8.1 Syntactic Parsing,[0],[0]
"∑
node∈ŷi κ1{node /∈ yi}.",8.1 Syntactic Parsing,[0],[0]
"(23)
κ is a scalar.",8.1 Syntactic Parsing,[0],[0]
"With this loss function, we require the score of the oracle tree to be higher than the other candidates by a score margin.",8.1 Syntactic Parsing,[0],[0]
"Intuitively, the score of the yi will increase and the score of ŷi will decrease during training.
Results.",8.1 Syntactic Parsing,[0],[0]
"We experiment on the WSJ portion of the Penn Treebank, following the standard split (Collins, 2003).",8.1 Syntactic Parsing,[0],[0]
"Sections 2-21 are used for training, Section 24 and Section 23 are the development
set and test set, respectively.",8.1 Syntactic Parsing,[0],[0]
"The Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) is adopted for our baseline by following the settings of Choe and Charniak (2016).
",8.1 Syntactic Parsing,[0],[0]
"To obtain N-best lists on the development set and test set, we first train a baseline parser on the training set.",8.1 Syntactic Parsing,[0],[0]
"To obtain N-best lists on the training data, we split the training data into 20 folds and trained 20 parsers.",8.1 Syntactic Parsing,[0],[0]
Each parser was trained on 19 folds data and used to produce the n-best list of the remaining fold.,8.1 Syntactic Parsing,[0],[0]
"For the neural reranking model, we use the pretrained word vectors from Collobert et al. (2011).",8.1 Syntactic Parsing,[0],[0]
The input dimension is 50.,8.1 Syntactic Parsing,[0],[0]
The dimension of state vectors in Tree-LSTM model is 60.,8.1 Syntactic Parsing,[0],[0]
"These parameters are trained with ADAM (Kingma and Ba, 2015) with a batch size of 20.",8.1 Syntactic Parsing,[0],[0]
We set κ = 0.1 for all experiments.,8.1 Syntactic Parsing,[0],[0]
"For practical reasons, we use the ConTree+Lex model to learn the node representations and define Y (xi) to be the 10-best parsing trees of xi.
Table 7 shows the reranking results on WSJ test set.",8.1 Syntactic Parsing,[0],[0]
The baseline F1 score is 89.7.,8.1 Syntactic Parsing,[0],[0]
Our ConTree improves the baseline model to 90.6.,8.1 Syntactic Parsing,[0],[0]
Using ConTree+Lex model can further improve the performance (90.6 → 90.9).,8.1 Syntactic Parsing,[0],[0]
This suggests that automatic heads can also be useful for a syntactic task.,8.1 Syntactic Parsing,[0],[0]
"Among neural rerankers, our model outperforms Socher et al. (2013a), but underperforms current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015a; Choe and Charniak, 2016) and recurrent neural network grammars (Dyer et al., 2016).",8.1 Syntactic Parsing,[0],[0]
This is likely due to our simple reranking configurations and settings5.,8.1 Syntactic Parsing,[0],[0]
"Nevertheless, it serves our goal of contrasting the tree LSTM models.",8.1 Syntactic Parsing,[0],[0]
"Kuncoro et al. (2017) investigate composition functions in recurrent neural network grammars (RNNG) (Dyer et al., 2016), finding that syntactic head information can be automatically learned.",8.2 Language Modeling,[0],[0]
"Their observa-
5Dyer et al. (2016) employs 2-layerd LSTMs with input and hidden dimensions of size 256 and 128.",8.2 Language Modeling,[0],[0]
Choe and Charniak (2016) use 3-layered LSTMs with both the input and hidden dimensions of size 1500.,8.2 Language Modeling,[0],[0]
"In addition, we only use the tree LSTM for scoring candidate parses in order to isolate the effect of tree LSTMs.",8.2 Language Modeling,[0],[0]
"In contrast, the previous works use the complex feature combinations in order to achieve high accuracies, which is different from our goal.
tion is consistent with ours.",8.2 Language Modeling,[0],[0]
"Formally, an RNNG is a tuple 〈N,Σ, R, S,Θ〉, where N is the set of nonterminals, Σ is the set of terminals, R is a set of top-down transition-based rules, S is the start symbol and Θ is the set of model parameters.",8.2 Language Modeling,[0],[0]
"Given S, the derivation process resembles transition-based parsing, which is performed incrementally from left to right.",8.2 Language Modeling,[0],[0]
"Unlike surface language models, RNNGs model sentences with explicit grammar.",8.2 Language Modeling,[0],[0]
"Comparing naive sequence-to-sequence models of syntax (Vinyals et al., 2015a), RNNGs have the advantage of explicitly modeling syntactic composition between constituents, by combining the vector representation of child constituents into a single vector representation of their parent using a neural network.",8.2 Language Modeling,[0],[0]
"Kuncoro et al. (2017) show that such compositions are the key to the success, and further investigate several alternatives neural network structures.",8.2 Language Modeling,[0],[0]
"In particular, they compare vanilla LSTMs to attention networks when composing child constituents.",8.2 Language Modeling,[0],[0]
"Interestingly, the attention values represent syntactic heads among the child constituents to some extent.",8.2 Language Modeling,[0],[0]
"In addition, the vector constituent representation implicitly reflects constituent types.",8.2 Language Modeling,[0],[0]
Their finding is consistent with ours in that a neural network can learn pure syntactic head information from constituent vectors.,8.2 Language Modeling,[0],[0]
"Our head-lexicalized tree model can be used for all tasks that require representation learning for sentences, given their constituent syntax.",8.3 Relation Extraction,[0],[0]
One example of future work is relation extraction.,8.3 Relation Extraction,[0],[0]
"For example, given the sentence “John is from Google Inc.”, a relation ‘works in’ can be extracted between ‘John’ and ‘Google Inc.’.
",8.3 Relation Extraction,[0],[0]
"Miwa and Bansal (2016) solve this task by using the Child-Sum tree representation of Tai et al. (2015) to represent the input sentence, extracting features for the two entities according to their related nodes in the dependency tree, and then conducting rela-
tion classification based on these features.",8.3 Relation Extraction,[0],[0]
Headlexicalization and top-down information can potentially be useful for improving relation extraction in the framework of Miwa and Bansal (2016).,8.3 Relation Extraction,[0],[0]
We proposed lexicalized variants for constituent tree LSTMs.,9 Conclusion,[0],[0]
"Learning the heads of constituents automatically using a neural model, our lexicalized tree LSTM is applicable to arbitrary binary branching trees in CFG, and is formalism-independent.",9 Conclusion,[0],[0]
"In addition, lexical information on the root further allows a top-down extension to the model, resulting in a bidirectional constituent Tree LSTM.",9 Conclusion,[0],[0]
Experiments on two well-known datasets show that head-lexicalization improves the unidirectional Tree LSTM model.,9 Conclusion,[0],[0]
"In addition, the bidirectional Tree LSTM gives superior labeling results compared to both unidirectional Tree LSTMs and bidirectional sequential LSTMs.",9 Conclusion,[0],[0]
We thank the anonymous reviewers for their detailed and constructive comments.,Acknowledgments,[0],[0]
Yue Zhang is the corresponding author.,Acknowledgments,[0],[0]
"Sequential LSTMs have been extended to model tree structures, giving competitive results for a number of tasks.",abstractText,[0],[0]
"Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes.",abstractText,[0],[0]
"This is different from sequential LSTMs, which contain references to input words for each node.",abstractText,[0],[0]
"In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node.",abstractText,[0],[0]
"In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure.",abstractText,[0],[0]
Experiments show that both extensions give better representations of tree structures.,abstractText,[0],[0]
Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.,abstractText,[0],[0]
Head-Lexicalized Bidirectional Tree LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–363 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
358
tection may broadly be categorized according to two paradigms: pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.",text,[0],[0]
Hierarchical relationships play a central role in knowledge representation and reasoning.,1 Introduction,[0],[0]
"Hypernym detection, i.e., the modeling of word-level hierarchies, has long been an important task in natural language processing.",1 Introduction,[0],[0]
"Starting with Hearst (1992), pattern-based methods have been one of the most influential approaches to this problem.",1 Introduction,[0],[0]
Their key idea is to exploit certain lexico-syntactic patterns to detect is-a relations in text.,1 Introduction,[0],[0]
"For instance, patterns like “NPy such as NPx”, or “NPx and other NPy” often indicate hypernymy relations of the form x is-a y. Such patterns may be predefined, or they may be learned automatically (Snow et al., 2004; Shwartz et al., 2016).",1 Introduction,[0],[0]
"However, a well-known problem of Hearst-like patterns is their extreme sparsity: words must co-occur in exactly the right configuration, or else no relation can be detected.
",1 Introduction,[0],[0]
"To alleviate the sparsity issue, the focus in hypernymy detection has recently shifted to distributional representations, wherein words are represented as vectors based on their distribution across large corpora.",1 Introduction,[0],[0]
"Such methods offer rich representations of lexical meaning, alleviating the sparsity problem, but require specialized similarity mea-
sures to distinguish different lexical relationships.",1 Introduction,[0],[0]
"The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear.",1 Introduction,[0],[0]
"Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance.",1 Introduction,[0],[0]
"An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017).",1 Introduction,[0],[0]
"Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016).
",1 Introduction,[0],[0]
"While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora.
",1 Introduction,[0],[0]
"Here, we revisit the idea of using pattern-based methods for hypernym detection.",1 Introduction,[0],[0]
"We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH.",1 Introduction,[0],[0]
"We find that simple pattern-based methods consistently outperform specialized DIH methods on several difficult hypernymy tasks, including detection, direction prediction, and graded entailment ranking.",1 Introduction,[0],[0]
"Moreover, we find that taking low-rank embeddings of pattern-based models substantially improves performance by remedying the sparsity issue.",1 Introduction,[0],[0]
"Overall, our results show that Hearst patterns provide high-quality and robust predictions on large corpora by capturing important contextual constraints, which are not yet modeled in distributional methods.",1 Introduction,[0],[0]
"In the following, we discuss pattern-based and distributional methods to detect hypernymy relations.",2 Models,[0],[0]
We explicitly consider only relatively simple pattern-based approaches that allow us to directly compare their performance to DIH-based methods.,2 Models,[0],[0]
"First, let P = {(x, y)}ni=1 denote the set of hypernymy relations that have been extracted via Hearst patterns from a text corpus T .",2.1 Pattern-based Hypernym Detection,[0],[0]
"Furthermore let w(x, y) denote the count of how often (x, y) has been extracted and let W = ∑
(x,y)∈P w(x, y) denote the total number extractions.",2.1 Pattern-based Hypernym Detection,[0],[0]
"In the first, most direct application of Hearst patterns, we then simply use the counts w(x, y) or, equivalently, the extraction probability
p(x, y) = w(x, y)
W (1)
to predict hypernymy relations from T .",2.1 Pattern-based Hypernym Detection,[0],[0]
"However, simple extraction probabilities as in Equation (1) are skewed by the occurrence probabilities of their constituent words.",2.1 Pattern-based Hypernym Detection,[0],[0]
"For instance, it is more likely that we extract (France, country) over (France, republic), just because the word country is more likely to occur than republic.",2.1 Pattern-based Hypernym Detection,[0],[0]
This skew in word distributions is well-known for natural language and also translates to Hearst patterns (see also Figure 1).,2.1 Pattern-based Hypernym Detection,[0],[0]
"For this reason, we also consider predicting hypernymy relations based on the Pointwise Mutual Information of Hearst patterns: First, let p−(x) =",2.1 Pattern-based Hypernym Detection,[0],[0]
"∑
(x,y)∈P w(x, y)/W
and p+(x)",2.1 Pattern-based Hypernym Detection,[0],[0]
"= ∑
(y,x)∈P w(y, x)/W denote the probability that x occurs as a hyponym and hypernym, respectively.",2.1 Pattern-based Hypernym Detection,[0],[0]
"We then define the Positive Pointwise Mutual Information for (x, y) as
ppmi(x, y) = max
(
0, log p(x, y)
p−(x)p+(y)
)
.",2.1 Pattern-based Hypernym Detection,[0],[0]
"(2)
While Equation (2) can correct for different word occurrence probabilities, it cannot handle missing data.",2.1 Pattern-based Hypernym Detection,[0],[0]
"However, sparsity is one of the main issues when using Hearst patterns, as a necessarily incomplete set of extraction rules will lead inevitably to missing extractions.",2.1 Pattern-based Hypernym Detection,[0],[0]
"For this purpose, we also study low-rank embeddings of the PPMI matrix, which allow us to make predictions for unseen pairs.",2.1 Pattern-based Hypernym Detection,[0],[0]
"In particular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}| denote the number of unique terms in P .",2.1 Pattern-based Hypernym Detection,[0],[0]
"Furthermore, let X ∈ Rm×m be the PPMI matrix with
entries Mxy = ppmi(x, y) and let M = UΣV ⊤ be its Singular Value Decomposition (SVD).",2.1 Pattern-based Hypernym Detection,[0],[0]
"We can then predict hypernymy relations based on the truncated SVD of M via
spmi(x, y) = u⊤xΣrvy (3)
where ux, vy denote the x-th and y-th row of U and V , respectively, and where Σr is the diagonal matrix of truncated singular values (in which all but the r largest singular values are set to zero).
",2.1 Pattern-based Hypernym Detection,[0],[0]
Equation (3) can be interpreted as a smoothed version of the observed PPMI matrix.,2.1 Pattern-based Hypernym Detection,[0],[0]
"Due to the truncation of singular values, Equation (3) computes a low-rank embedding of M where similar words (in terms of their Hearst patterns) have similar representations.",2.1 Pattern-based Hypernym Detection,[0],[0]
"Since Equation (3) is defined for all pairs (x, y), it allows us to make hypernymy predictions based on the similarity of words.",2.1 Pattern-based Hypernym Detection,[0],[0]
"We also consider factorizing a matrix that is constructed from occurrence probabilities as in Equation (1), denoted by sp(x, y).",2.1 Pattern-based Hypernym Detection,[0],[0]
"This approach is then closely related to the method of Cederberg and Widdows (2003), which has been proposed to improve precision and recall for hypernymy detection from Hearst patterns.",2.1 Pattern-based Hypernym Detection,[0],[0]
"Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012; Shwartz et al., 2017).",2.2 Distributional Hypernym Detection,[0],[0]
"Here, we compare to two methods with strong empirical results.",2.2 Distributional Hypernym Detection,[0],[0]
"As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces.",2.2 Distributional Hypernym Detection,[0],[0]
"First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of
x which are included in the set of a broader term’s features, y:
WeedsPrec(x,y) = ∑n i=1",2.2 Distributional Hypernym Detection,[0],[0]
xi ∗,2.2 Distributional Hypernym Detection,[0],[0]
"✶yi>0 ∑n
i=1",2.2 Distributional Hypernym Detection,[0],[0]
"xi
Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term.",2.2 Distributional Hypernym Detection,[0],[0]
"In particular, let
CL(x,y) =",2.2 Distributional Hypernym Detection,[0],[0]
∑n i=1,2.2 Distributional Hypernym Detection,[0],[0]
"min(xi, yi)",2.2 Distributional Hypernym Detection,[0],[0]
"∑n
i=1",2.2 Distributional Hypernym Detection,[0],[0]
"xi
denote the degree of inclusion of x in y as proposed by Clarke (2009).",2.2 Distributional Hypernym Detection,[0],[0]
"To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as
invCL(x,y) = √ CL(x,y) ∗ (1− CL(y,x))
",2.2 Distributional Hypernym Detection,[0],[0]
"Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014; Shwartz et al., 2017).",2.2 Distributional Hypernym Detection,[0],[0]
"Intuitively, the SLQS model presupposes that general words appear mostly in uninformative contexts, as measured by entropy.",2.2 Distributional Hypernym Detection,[0],[0]
"Specifically, SLQS depends on the median entropy of a term’s top N contexts, defined as
Ex = median N i=1",2.2 Distributional Hypernym Detection,[0],[0]
"[H(ci)] ,
where H(ci) is the Shannon entropy of context ci across all terms, and N is chosen in hyperparameter selection.",2.2 Distributional Hypernym Detection,[0],[0]
"Finally, SLQS is defined using the ratio between the two terms:
SLQS(x, y) = 1− Ex Ey .
",2.2 Distributional Hypernym Detection,[0],[0]
"Since the SLQS model only compares the relative generality of two terms, but does not make judgment about the terms’ relatedness, we report SLQS-cos, which multiplies the SLQS measure by cosine similarity of x and y (Santus et al., 2014).
",2.2 Distributional Hypernym Detection,[0],[0]
"For completeness, we also include cosine simi-
larity as a baseline in our evaluation.",2.2 Distributional Hypernym Detection,[0],[0]
"To evaluate the relative performance of patternbased and distributional models, we apply them to several challenging hypernymy tasks.",3 Evaluation,[0],[0]
Detection:,3.1 Tasks,[0],[0]
"In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation.",3.1 Tasks,[0],[0]
"For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of BLESS, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns.",3.1 Tasks,[0],[0]
"Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs.",3.1 Tasks,[0],[0]
"This version contains 14,542 total pairs with 1,337 positive examples.",3.1 Tasks,[0],[0]
"Second, we evaluate on LEDS (Baroni et al., 2012), which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs.",3.1 Tasks,[0],[0]
"We also consider EVAL (Santus et al., 2015), containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations.",3.1 Tasks,[0],[0]
EVAL is notable for its absence of random pairs.,3.1 Tasks,[0],[0]
"The largest dataset is SHWARTZ (Shwartz et al., 2016), which was collected from a mixture of WordNet, DBPedia, and other resources.",3.1 Tasks,[0],[0]
"We limit ourselves to a 52,578 pair subset excluding multiword expressions.",3.1 Tasks,[0],[0]
"Finally, we evaluate on WBLESS (Weeds et al., 2014), a 1,668 pair subset of BLESS, with negative pairs being selected from co-hyponymy, random, and hyponymy relations.",3.1 Tasks,[0],[0]
"Previous work has used different metrics for evaluating on BLESS (Lenci and Benotto, 2012; Levy et al., 2015; Roller and Erk, 2016).",3.1 Tasks,[0],[0]
We chose to evaluate the global ranking using Average Precision.,3.1 Tasks,[0],[0]
"This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in Shwartz et al. (2017).
",3.1 Tasks,[0],[0]
Direction:,3.1 Tasks,[0],[0]
"In direction prediction, the task is to identify which term is broader in a given pair
of words.",3.1 Tasks,[0],[0]
"For this task, we evaluate all models on three datasets described by Kiela et al. (2015):",3.1 Tasks,[0],[0]
"On BLESS, the task is to predict the direction for all 1337 positive pairs in the dataset.",3.1 Tasks,[0],[0]
"Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. score(x, y) >",3.1 Tasks,[0],[0]
"score(y, x).",3.1 Tasks,[0],[0]
"We reserve 10% of the data for validation, and test on the remaining 90%.",3.1 Tasks,[0],[0]
"On WBLESS, we follow prior work (Nguyen et al., 2017; Vulić and Mrkšić, 2017) and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data.",3.1 Tasks,[0],[0]
We report average accuracy across all iterations.,3.1 Tasks,[0],[0]
"Finally, we evaluate on BIBLESS (Kiela et al., 2015), a variant of WBLESS with hypernymy and hyponymy pairs explicitly annotated for their direction.",3.1 Tasks,[0],[0]
"Since this task requires three-way classification (hypernymy, hyponymy, and other), we perform two-stage classification.",3.1 Tasks,[0],[0]
"First, a threshold is tuned using 2% of the data, identifying whether a pair exhibits hypernymy in either direction.",3.1 Tasks,[0],[0]
"Second, the relative comparison of scores determines which direction is predicted.",3.1 Tasks,[0],[0]
"As with WBLESS, we report the average accuracy over 1000 iterations.
",3.1 Tasks,[0],[0]
Graded Entailment:,3.1 Tasks,[0],[0]
"In graded entailment, the task is to quantify the degree to which a hypernymy relation holds.",3.1 Tasks,[0],[0]
"For this task, we follow prior work (Nickel and Kiela, 2017; Vulić and Mrkšić, 2017) and use the noun part of HYPERLEX (Vulić et al., 2017), consisting of 2,163 noun pairs which are annotated to what degree x is-a y holds on a scale of [0, 6].",3.1 Tasks,[0],[0]
"For all models, we report Spearman’s rank correlation ρ.",3.1 Tasks,[0],[0]
We handle out-ofvocabulary (OOV) words by assigning the median of the scores (computed across the training set) to pairs with OOV words.,3.1 Tasks,[0],[0]
"Pattern-based models: We extract Hearst patterns from the concatenation of Gigaword and Wikipedia, and prepare our corpus by tokenizing, lemmatizing, and POS tagging using CoreNLP 3.8.0.",3.2 Experimental Setup,[0],[0]
The full set of Hearst patterns is provided in Table 1.,3.2 Experimental Setup,[0],[0]
"Our selected patterns match prototypical Hearst patterns, like “animals such as cats,” but also include broader patterns like “New Year is the most important holiday.”",3.2 Experimental Setup,[0],[0]
"Leading and following noun phrases are allowed to match limited modifiers (compound nouns, adjectives, etc.), in which case we also generate a hit for the head of the noun phrase.",3.2 Experimental Setup,[0],[0]
"Dur-
ing postprocessing, we remove pairs which were not extracted by at least two distinct patterns.",3.2 Experimental Setup,[0],[0]
"We also remove any pair (y, x) if p(y, x) < p(x, y).",3.2 Experimental Setup,[0],[0]
"The final corpus contains roughly 4.5M matched pairs, 431K unique pairs, and 243K unique terms.",3.2 Experimental Setup,[0],[0]
"For SVD-based models, we select the rank from r ∈ {5, 10, 15, 20, 25, 50, 100, 150, 200, 250, 300, 500, 1000} on the validation set.",3.2 Experimental Setup,[0],[0]
"The other pattern-based models do not have any hyperparameters.
",3.2 Experimental Setup,[0],[0]
"Distributional models: For the distributional baselines, we employ the large, sparse distributional space of Shwartz et al. (2017), which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks.",3.2 Experimental Setup,[0],[0]
The corpus was POS tagged and dependency parsed.,3.2 Experimental Setup,[0],[0]
"Distributional contexts were constructed from adjacent words in dependency parses (Padó and Lapata, 2007; Levy and Goldberg, 2014).",3.2 Experimental Setup,[0],[0]
"Targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co-occurrence matrix was PPMI transformed.1",3.2 Experimental Setup,[0],[0]
The resulting space contains representations for 218K words over 732K context dimensions.,3.2 Experimental Setup,[0],[0]
"For the SLQS model, we selected the number of contexts N from the same set of options as the SVD rank in pattern-based models.",3.2 Experimental Setup,[0],[0]
Table 2 shows the results from all three experimental settings.,3.3 Results,[0],[0]
"In nearly all cases, we find that patternbased approaches substantially outperform all three distributional models.",3.3 Results,[0],[0]
Particularly strong improvements can be observed on BLESS (0.76 average precision vs 0.19) and WBLESS (0.96 vs. 0.69) for the detection tasks and on all directionality tasks.,3.3 Results,[0],[0]
"For directionality prediction on BLESS, the SVD models surpass even the state-of-the-art supervised model of Vulić and Mrkšić (2017).",3.3 Results,[0],[0]
"Moreover, both SVD models perform generally better than their sparse counterparts on all tasks and datasets except on HYPERLEX.",3.3 Results,[0],[0]
"We performed a posthoc analysis of the validation sets comparing the ppmi and spmi models, and found that the truncated SVD improved recall via its matrix completion properties.",3.3 Results,[0],[0]
"We also found that the spmi model downweighted
1In addition, we also experimented with further distributional spaces and weighting schemes from Shwartz et al. (2017).",3.3 Results,[0],[0]
"We also experimented with distributional spaces using the same corpora and preprocessing as the Hearst patterns (i.e., Wikipedia and Gigaword).",3.3 Results,[0],[0]
"We found that the reported setting generally performed best, and omit others for brevity.
",3.3 Results,[0],[0]
"many high-scoring outlier pairs composed of rare terms.
",3.3 Results,[0],[0]
"When comparing the p(x, y) and ppmi models to distributional models, we observe mixed results.",3.3 Results,[0],[0]
The SHWARTZ dataset is difficult for sparse models due to its very long tail of low frequency words that are hard to cover using Hearst patterns.,3.3 Results,[0],[0]
"On EVAL, Hearst-pattern based methods get penalized by OOV words, due to the large number of verbs and adjectives in the dataset, which are not captured by our patterns.",3.3 Results,[0],[0]
"However, in 7 of the 9 datasets, at least one of the sparse models outperforms all distributional measures, showing that Hearst patterns can provide strong performance on large corpora.",3.3 Results,[0],[0]
We studied the relative performance of Hearst pattern-based methods and DIH-based methods for hypernym detection.,4 Conclusion,[0],[0]
Our results show that the pattern-based methods substantially outperform DIH-based methods on several challenging benchmarks.,4 Conclusion,[0],[0]
We find that embedding methods alleviate sparsity concerns of pattern-based approaches and substantially improve coverage.,4 Conclusion,[0],[0]
We conclude that Hearst patterns provide important contexts for the detection of hypernymy relations that are not yet captured in DIH models.,4 Conclusion,[0],[0]
Our code is available at https://github.com/ facebookresearch/hypernymysuite.,4 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
"We also thank Vered Shwartz, Enrico Santus, and Dominik Schlechtweg for providing us with their distributional spaces and baseline implementations.",Acknowledgments,[0],[0]
Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods.,abstractText,[0],[0]
"In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets.",abstractText,[0],[0]
Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.,abstractText,[0],[0]
Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 46–56 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHESSION, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHESSION over the state-of-the-art.",text,[0],[0]
One of the most important tasks towards text understanding is to detect and categorize semantic relations between two entities in a given context.,1 Introduction,[0],[0]
"For example, in Fig. 1, with regard to the sentence of c1, relation between Jesse James and Missouri should be categorized as died in.",1 Introduction,[0],[0]
"With accurate identification, relation extraction systems can provide essential support for many applications.",1 Introduction,[0],[0]
"One
∗Equal contribution.
example is question answering, regarding a specific question, relation among entities can provide valuable information, which helps to seek better answers (Bao et al., 2014).",1 Introduction,[0],[0]
"Similarly, for medical science literature, relations like protein-protein interactions (Fundel et al., 2007) and gene disease associations (Chun et al., 2006) can be extracted and used in knowledge base population.",1 Introduction,[0],[0]
"Additionally, relation extractors can be used in ontology construction (Schutz and Buitelaar, 2005).
",1 Introduction,[0],[0]
"Typically, existing methods follow the supervised learning paradigm, and require extensive annotations from domain experts, which are costly and time-consuming.",1 Introduction,[0],[0]
"To alleviate such drawback, attempts have been made to build relation extractors with a small set of seed instances or humancrafted patterns (Nakashole et al., 2011; Carlson et al., 2010), based on which more patterns and instances will be iteratively generated by bootstrap learning.",1 Introduction,[0],[0]
"However, these methods often suffer from semantic drift (Mintz et al., 2009).",1 Introduction,[0],[0]
"Besides, knowledge bases like Freebase have been leveraged to automatically generate training data and provide distant supervision (Mintz et al., 2009).",1 Introduction,[0],[0]
"Nevertheless, for many domain-specific applications, distant supervision is either non-existent or insufficient (usually less than 25% of relation mentions are covered (Ren et al., 2015; Ling and Weld, 2012)).
",1 Introduction,[0],[0]
"Only recently have preliminary studies been developed to unite different supervisions, including knowledge bases and domain specific patterns, which are referred as heterogeneous supervision.",1 Introduction,[0],[0]
"As shown in Fig. 1, these supervisions often conflict with each other (Ratner et al., 2016).",1 Introduction,[0],[0]
"To address these conflicts, data programming (Ratner et al., 2016) employs a generative model, which encodes supervisions as labeling functions, and adopts the source consistency assumption: a source is likely to provide true information with
46
the same probability for all instances.",1 Introduction,[0],[0]
"This assumption is widely used in true label discovery literature (Li et al., 2016) to model reliabilities of information sources like crowdsourcing and infer the true label from noisy labels.",1 Introduction,[0],[0]
"Accordingly, most true label discovery methods would trust a human annotator on all instances to the same level.
",1 Introduction,[0],[0]
"However, labeling functions, unlike human annotators, do not make casual mistakes but follow certain “error routine”.",1 Introduction,[0],[0]
"Thus, the reliability of a labeling function is not consistent among different pieces of instances.",1 Introduction,[0],[0]
"In particular, a labeling function could be more reliable for a certain subset (Varma et al., 2016) (also known as its proficient subset) comparing to the rest.",1 Introduction,[0],[0]
"We identify these proficient subsets based on context information, only trust labeling functions on these subsets and avoid assuming global source consistency.
",1 Introduction,[0],[0]
"Meanwhile, embedding methods have demonstrated great potential in capturing semantic meanings, which also reduce the dimension of overwhelming text features.",1 Introduction,[0],[0]
"Here, we present REHESSION, a novel framework capturing context’s semantic meaning through representation learning, and conduct both relation extraction and true label discovery in a context-aware manner.",1 Introduction,[0],[0]
"Specifically, as depicted in Fig. 1, we embed relation mentions in a low-dimension vector space, where similar relation mentions tend to have similar relation types and annotations.",1 Introduction,[0],[0]
"‘True’ labels are further inferred based on reliabilities of labeling functions, which are calculated with their proficient subsets’ representations.",1 Introduction,[0],[0]
"Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction.",1 Introduction,[0],[0]
"Besides, the context representation bridges relation extraction with true label dis-
covery, and allows them to enhance each other.",1 Introduction,[0],[0]
"To the best of our knowledge, the framework proposed here is the first method that utilizes representation learning to provide heterogeneous supervision for relation extraction.",1 Introduction,[0],[0]
The high-quality context representations serve as the backbone of true label discovery and relation extraction.,1 Introduction,[0],[0]
"Extensive experiments on benchmark datasets demonstrate significant improvements over the state-ofthe-art.
",1 Introduction,[0],[0]
The remaining of this paper is organized as follows.,1 Introduction,[0],[0]
Section 2 gives the definition of relation extraction with heterogeneous supervision.,1 Introduction,[0],[0]
"We then present the REHESSION model and the learning algorithm in Section 3, and report our experimental evaluation in Section 4.",1 Introduction,[0],[0]
"Finally, we briefly survey related work in Section 5 and conclude this study in Section 6.",1 Introduction,[0],[0]
"In this section, we would formally define relation extraction and heterogeneous supervision, including the format of labeling functions.",2 Preliminaries,[0],[0]
"Here we conduct relation extraction in sentencelevel (Bao et al., 2014).",2.1 Relation Extraction,[0],[0]
"For a sentence d, an entity mention is a token span in d which represents an entity, and a relation mention is a triple (e1, e2, d) which consists of an ordered entity pair (e1, e2) and",2.1 Relation Extraction,[0],[0]
"d. And the relation extraction task is to categorize relation mentions into a given set of relation typesR, or Not-Target-Type (None) which means the type of the relation mention does not belong to R.",2.1 Relation Extraction,[0],[0]
"Similar to (Ratner et al., 2016), we employ labeling functions as basic units to encode supervision information and generate annotations.",2.2 Heterogeneous Supervision,[0],[0]
"Since different supervision information may have different proficient subsets, we require each labeling function to encode only one elementary supervision information.",2.2 Heterogeneous Supervision,[0],[0]
"Specifically, in the relation extraction scenario, we require each labeling function to only annotate one relation type based on one elementary piece of information, e.g., four examples are listed in Fig. 1.
Notice that knowledge-based labeling functions are also considered to be noisy because relation extraction is conducted in sentence-level, e.g. although president of (Obama, USA) exists in KB, it should not be assigned with “Obama was born in Honolulu, Hawaii, USA”, since president of is irrelevant to the context.",2.2 Heterogeneous Supervision,[0],[0]
"For a POS-tagged corpus D with detected entities, we refer its relation mentions as C = {ci = (ei,1, ei,2, d),∀d ∈ D}.",2.3 Problem Definition,[0],[0]
"Our goal is to annotate entity mentions with relation types of interest (R = {r1, . . .",2.3 Problem Definition,[0],[0]
", rK}) or None.",2.3 Problem Definition,[0],[0]
"We require users to provide heterogeneous supervision in the form of labeling function Λ = {λ1, . . .",2.3 Problem Definition,[0],[0]
", λM},",2.3 Problem Definition,[0],[0]
"and mark the annotations generated by Λ as O = {oc,i|λi generate annotation oc,i for c ∈ C}.",2.3 Problem Definition,[0],[0]
"We record relation mentions annotated by Λ as Cl, and refer relation mentions without annotation as Cu.",2.3 Problem Definition,[0],[0]
"Then, our task is to train a relation extractor based on Cl and categorize relation mentions in Cu.",2.3 Problem Definition,[0],[0]
"Here, we present REHESSION, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances
into a set of relation types.",3 The REHESSION Framework,[0],[0]
"Intuitively, errors of annotations (O) come from mismatch of contexts, e.g., in Fig. 1, λ1 annotates c1 and c2 with ’true’ labels but for mismatched contexts ‘killing’ and ’killed’.",3 The REHESSION Framework,[0],[0]
"Accordingly, we should only trust labeling functions on matched context, e.g., trust λ1 on c3 due to its context ‘was born in’, but not on c1 and c2.",3 The REHESSION Framework,[0],[0]
"On the other hand, relation extraction can be viewed as matching appropriate relation type to a certain context.",3 The REHESSION Framework,[0],[0]
"These two matching processes are closely related and can enhance each other, while context representation plays an important role in both of them.
",3 The REHESSION Framework,[0],[0]
Framework Overview.,3 The REHESSION Framework,[0],[0]
We propose a general framework to learn the relation extractor from automatically generated noisy labels.,3 The REHESSION Framework,[0],[0]
"As plotted in Fig. 1, distributed representation of context bridges relation extraction with true label discovery, and allows them to enhance each other.",3 The REHESSION Framework,[0],[0]
"Specifically, it follows the steps below:
1.",3 The REHESSION Framework,[0],[0]
"After being extracted from context, text features are embedded in a low dimension space by representation learning (see Fig. 2);
2.",3 The REHESSION Framework,[0],[0]
"Text feature embeddings are utilized to calculate relation mention embeddings (see Fig. 2);
3.",3 The REHESSION Framework,[0],[0]
"With relation mention embeddings, true labels are inferred by calculating labeling functions’ reliabilities in a context-aware manner (see Fig. 1);
4.",3 The REHESSION Framework,[0],[0]
"Inferred true labels would ‘supervise’ all components to learn model parameters (see Fig. 1).
",3 The REHESSION Framework,[0],[0]
We now proceed by introducing these components of the model in further details.,3 The REHESSION Framework,[0],[0]
"As shown in Table 2, we extract abundant lexical features (Ren et al., 2016; Mintz et al., 2009) to characterize relation mentions.",3.1 Modeling Relation Mention,[0],[0]
"However, this abundance also results in the gigantic dimension of original text features (∼ 107 in our case).",3.1 Modeling Relation Mention,[0],[0]
"In
order to achieve better generalization ability, we represent relation mentions with low dimensional (∼ 102) vectors.",3.1 Modeling Relation Mention,[0],[0]
"In Fig. 2, for example, relation mention c3 is first represented as bag-of-features.",3.1 Modeling Relation Mention,[0],[0]
"After learning text feature embeddings, we use the average of feature embedding vectors to derive the embedding vector for c3.
",3.1 Modeling Relation Mention,[0],[0]
Text Feature Representation.,3.1 Modeling Relation Mention,[0],[0]
"Similar to other principles of embedding learning, we assume text features occurring in the same contexts tend to have similar meanings (also known as distributional hypothesis(Harris, 1954)).",3.1 Modeling Relation Mention,[0],[0]
"Furthermore, we let each text feature’s embedding vector to predict other text features occurred in the same relation mentions or context.",3.1 Modeling Relation Mention,[0],[0]
"Thus, text features with similar meaning should have similar embedding vectors.",3.1 Modeling Relation Mention,[0],[0]
"Formally, we mark text features as F = {f1, · · · , f|F|}, record the feature set for ∀c ∈ C as fc, and represent the embedding vector for fi as vi ∈ Rnv , and we aim to maximize the following log likelihood: ∑ c∈Cl ∑ fi,fj∈fc log p(fi|fj), where
p(fi|fj) = exp(vTi v∗j )/",3.1 Modeling Relation Mention,[0],[0]
"∑
fk∈F exp(v T i v
∗ k).
",3.1 Modeling Relation Mention,[0],[0]
"However, the optimization of this likelihood is impractical because the calculation of ∇p(fi|fj) requires summation over all text features, whose size exceeds 107 in our case.",3.1 Modeling Relation Mention,[0],[0]
"In order to perform efficient optimization, we adopt the negative sampling technique (Mikolov et al., 2013) to avoid this summation.",3.1 Modeling Relation Mention,[0],[0]
"Accordingly, we replace the log likelihood with Eq. 1 as below:
JE = ∑ c∈Cl
fi,fj∈fc
(log σ(vTi v ∗ j )",3.1 Modeling Relation Mention,[0],[0]
− V∑,3.1 Modeling Relation Mention,[0],[0]
"k=1 Efk′∼P̂ [log σ(−v T i v ∗ k′)])
(1)
where P̂ is noise distribution used in (Mikolov et al., 2013), σ is the sigmoid function and V is number of negative samples.
",3.1 Modeling Relation Mention,[0],[0]
Relation Mention Representation.,3.1 Modeling Relation Mention,[0],[0]
"With text feature embeddings learned by Eq. 1, a naive method to
represent relation mentions is to concatenate or average its text feature embeddings.",3.1 Modeling Relation Mention,[0],[0]
"However, text features embedding may be in a different semantic space with relation types.",3.1 Modeling Relation Mention,[0],[0]
"Thus, we directly learn a mapping g from text feature representations to relation mention representations (Van Gysel et al., 2016a,b) instead of simple heuristic rules like concatenate or average (see Fig. 2):
zc = g(fc) =",3.1 Modeling Relation Mention,[0],[0]
"tanh(W · 1|fc| ∑
fi∈fc vi) (2)
where zc is the representation of c ∈ Cl, W is a nz × nv matrix, nz is the dimension of relation mention embeddings and tanh is the element-wise hyperbolic tangent function.
",3.1 Modeling Relation Mention,[0],[0]
"In other words, we represent bag of text features with their average embedding, then apply linear map and hyperbolic tangent to transform the embedding from text feature semantic space to relation mention semantic space.",3.1 Modeling Relation Mention,[0],[0]
"The non-linear tanh function allows non-linear class boundaries in other components, and also regularize relation mention representation to range [−1, 1] which avoids numerical instability issues.",3.1 Modeling Relation Mention,[0],[0]
"Because heterogeneous supervision generates labels in a discriminative way, we suppose its errors follow certain underlying principles, i.e., if a
labeling function annotates a instance correctly / wrongly, it would annotate other similar instances correctly / wrongly.",3.2 True Label Discovery,[0],[0]
"For example, λ1 in Fig. 1 generates wrong annotations for two similar instances c1, c2 and would make the same errors on other similar instances.",3.2 True Label Discovery,[0],[0]
"Since context representation captures the semantic meaning of relation mention and would be used to identify relation types, we also use it to identify the mismatch of context and labeling functions.",3.2 True Label Discovery,[0],[0]
"Thus, we suppose for each labeling function λi, there exists an proficient subset Si on Rnz , containing instances that λi can precisely annotate.",3.2 True Label Discovery,[0],[0]
"In Fig. 1, for instance, c3 is in the proficient subset of λ1, while c1 and c2 are not.",3.2 True Label Discovery,[0],[0]
"Moreover, the generation of annotations are not really random, and we propose a probabilistic model to describe the level of mismatch from labeling functions to real relation types instead of annotations’ generation.
",3.2 True Label Discovery,[0],[0]
"As shown in Fig. 3, we assume the indicator of whether c belongs to Si, sc,i = δ(c ∈ Si), would first be generated based on context representation
p(sc,i = 1|zc, li) =",3.2 True Label Discovery,[0],[0]
"p(c ∈ Si) = σ(zTc li) (3)
Then the correctness of annotation oc,i, ρc,i = δ(oc,i = o∗c), would be generated.",3.2 True Label Discovery,[0],[0]
"Furthermore, we assume p(ρc,i = 1|sc,i = 1) = ϕ1 and p(ρc,i = 1|sc,i = 0) = ϕ0 to be constant for all relation mentions and labeling functions.
",3.2 True Label Discovery,[0],[0]
"Because sc,i would not be used in other components of our framework, we integrate out sc,i and write the log likelihood as
JT = ∑
oc,i∈O",3.2 True Label Discovery,[0],[0]
"log(σ(zTc li)ϕ
δ(oc,i=o ∗ c ) 1",3.2 True Label Discovery,[0],[0]
"(1− ϕ1)δ(oc,i ̸=o ∗ c )
",3.2 True Label Discovery,[0],[0]
"+ (1− σ(zTc li))ϕδ(oc,i=o ∗ c ) 0",3.2 True Label Discovery,[0],[0]
"(1− ϕ0)δ(oc,i ̸=o ∗ c ))",3.2 True Label Discovery,[0],[0]
"(4)
Note that o∗c is a hidden variable but not a model parameter, and JT is the likelihood of ρc,i = δ(oc,i = o∗c).",3.2 True Label Discovery,[0],[0]
"Thus, we would first infer o∗c = argmaxo∗c JT , then train the true label discovery model by maximizing JT .",3.2 True Label Discovery,[0],[0]
We now discuss the model for identifying relation types based on context representation.,3.3 Modeling Relation Type,[0],[0]
"For each relation mention c, its representation zc implies its relation type, and the distribution of relation type can be described by the soft-max function:
p(ri|zc) =",3.3 Modeling Relation Type,[0],[0]
"exp(z T c ti)∑
rj∈R∪{None} exp(z T c tj)
(5)
where ti ∈ Rvz is the representation for relation type ri.",3.3 Modeling Relation Type,[0],[0]
"Moreover, with the inferred true label o∗c , the relation extraction model can be trained as a multi-class classifier.",3.3 Modeling Relation Type,[0],[0]
"Specifically, we use Eq. 5 to approach the distribution
p(ri|o∗c) =",3.3 Modeling Relation Type,[0],[0]
{ 1 ri = o ∗,3.3 Modeling Relation Type,[0],[0]
"c
0 ri ̸= o∗c (6)
",3.3 Modeling Relation Type,[0],[0]
"Moreover, we use KL-divergence to measure the dissimilarity between two distributions, and formulate model learning as maximizing JR:
JR = − ∑ c∈Cl KL(p(.|zc)||p(.|o∗c)) (7)
where KL(p(.|zc)||p(.|o∗c)) is the KL-divergence from p(ri|o∗c) to p(ri|zc), p(ri|zc) and p(ri|o∗c) has the form of Eq. 5 and Eq. 6.",3.3 Modeling Relation Type,[0],[0]
"Based on Eq. 1, Eq. 4 and Eq. 7, we form the joint optimization problem for model parameters as
min W,v,v∗,l,t,o∗ J = −JR − λ1JE − λ2JT s.t. ∀c",3.4 Model Learning,[0],[0]
"∈ Cl, o∗c = argmax
o∗c JT , zc = g(fc) (8)
Collectively optimizing Eq. 8 allows heterogeneous supervision guiding all three components, while these components would refine the context representation, and enhance each other.
",3.4 Model Learning,[0],[0]
"In order to solve the joint optimization problem in Eq. 8 efficiently, we adopt the stochastic gradient descent algorithm to update {W,v,v∗, l, t} iteratively, and oc∗ is estimated by maximizing JT after calculating zc.",3.4 Model Learning,[0],[0]
"Additionally, we apply the widely used dropout techniques (Srivastava et al., 2014) to prevent overfitting and improve generalization performance.
",3.4 Model Learning,[0],[0]
The learning process of REHESSION is summarized as below.,3.4 Model Learning,[0],[0]
"In each iteration, we would sample a relation mention c from Cl, then sample c’s text
features and conduct the text features’ representation learning.",3.4 Model Learning,[0],[0]
"After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c .",3.4 Model Learning,[0],[0]
We now discuss the strategy of performing type inference for Cu.,3.5 Relation Type Inference,[0],[0]
"As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl.",3.5 Relation Type Inference,[0],[0]
"Additionally, not like other relation types in R, None does not have a coherent semantic meaning.",3.5 Relation Type Inference,[0],[0]
"Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our relation extractor predict it as None, or (2) the entropy of p(.|zc) over R exceeds a pre-defined threshold η.",3.5 Relation Type Inference,[0],[0]
The entropy is calculated as H(p(.|zc)),3.5 Relation Type Inference,[0],[0]
= −∑ri∈R p(ri|zc)log(p(ri|zc)).,3.5 Relation Type Inference,[0],[0]
And the second situation means based on relation extractor this relation mention is not likely belonging to any relation types in R.,3.5 Relation Type Inference,[0],[0]
"In this section, we empirically validate our method by comparing to the state-of-the-art relation extraction methods on news and Wikipedia articles.",4 Experiments,[0],[0]
"In the experiments, we conduct investigations on two benchmark datasets from different domains:1 NYT (Riedel et al., 2010) is a news corpus sampled from∼ 294k 1989-2007 New York Times news articles.",4.1 Datasets and settings,[0],[0]
"It consists of 1.18M sentences, while 395 of them are annotated by authors of (Hoffmann et al., 2011) and used as test data; Wiki-KBP utilizes 1.5M sentences sampled from 780k Wikipedia articles (Ling and Weld, 2012) as training corpus, while test set consists of the 2k sentences manually annotated in 2013 KBP slot filling assessment results (Ellis et al., 2012).
",4.1 Datasets and settings,[0],[0]
"For both datasets, the training and test sets partitions are maintained in our experiments.",4.1 Datasets and settings,[0],[0]
"Furthermore, we create validation sets by randomly sampling 10% mentions from each test set and used the remaining part as evaluation sets.
",4.1 Datasets and settings,[0],[0]
Feature Generation.,4.1 Datasets and settings,[0],[0]
"As summarized in Table 2, we use a 6-word window to extract context features for each entity mention, apply the Stanford
1 Codes and datasets used in this paper can be downloaded at: https://github.com/LiyuanLucasLiu/ ReHession.
CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets.",4.1 Datasets and settings,[0],[0]
"Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2.",4.1 Datasets and settings,[0],[0]
"All these features are shared with all compared methods in our experiments.
",4.1 Datasets and settings,[0],[0]
Labeling Functions.,4.1 Datasets and settings,[0],[0]
"In our experiments, labeling functions are employed to encode two kinds of supervision information.",4.1 Datasets and settings,[0],[0]
"One is knowledge base, the other is handcrafted domain-specific patterns.",4.1 Datasets and settings,[0],[0]
"For domain-specific patterns, we manually design a number of labeling functions3; for knowledge base, annotations are generated following the procedure in (Ren et al., 2016; Riedel et al., 2010).
",4.1 Datasets and settings,[0],[0]
"Regarding two kinds of supervision information, the statistics of the labeling functions are summarized in Table 4.",4.1 Datasets and settings,[0],[0]
"We can observe that heuristic patterns can identify more relation types for KBP datasets, while for NYT datasets, knowledge base can provide supervision for more relation types.",4.1 Datasets and settings,[0],[0]
"This observation aligns with our intuition that single kind of information might be insufficient while different kinds of information can complement each other.
",4.1 Datasets and settings,[0],[0]
We further summarize the statistics of annotations in Table 6.,4.1 Datasets and settings,[0],[0]
"It can be observed that a large portion of instances is only annotated as None, while lots of conflicts exist among other instances.",4.1 Datasets and settings,[0],[0]
This phenomenon justifies the motivation to employ true label discovery model to resolve the conflicts among supervision.,4.1 Datasets and settings,[0],[0]
"Also, we can observe most conflicts involve None type, accordingly, our proposed method should have more advantages over traditional true label discovery methods on the relation extraction task comparing to the relation classification task that excludes None type.",4.1 Datasets and settings,[0],[0]
"We compare REHESSION with below methods: FIGER (Ling and Weld, 2012) adopts multi-label
2https://github.com/percyliang/ brown-cluster
3pattern-based labeling functions can be accessed at: https://github.com/LiyuanLucasLiu/ ReHession
learning with Perceptron algorithm.",4.2 Compared Methods,[0],[0]
"BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011)",4.2 Compared Methods,[0],[0]
"models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model.",4.2 Compared Methods,[0],[0]
"CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor.
",4.2 Compared Methods,[0],[0]
"Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods.",4.2 Compared Methods,[0],[0]
"The first is to keep all noisy labels, marked as ‘NL’.",4.2 Compared Methods,[0],[0]
"Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities.",4.2 Compared Methods,[0],[0]
"Then, the second strategy is to only feed the inferred true labels, referred as ‘TD’.
",4.2 Compared Methods,[0],[0]
4We use liblinear package from https//github.,4.2 Compared Methods,[0],[0]
"com/cjlin1/liblinear
Universal Schemas (Riedel et al., 2013) is proposed to unify different information by calculating a low-rank approximation of the annotations O. It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type.",4.2 Compared Methods,[0],[0]
But it doesnt explicitly model noise and not fit our scenario very well.,4.2 Compared Methods,[0],[0]
"Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4.",4.2 Compared Methods,[0],[0]
"Indeed, it performs similarly to the Investment method.
",4.2 Compared Methods,[0],[0]
Evaluation Metrics.,4.2 Compared Methods,[0],[0]
"For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score (Bunescu and Mooney, 2005; Bach and Badaskar, 2007) are employed.",4.2 Compared Methods,[0],[0]
"Notice that both relation extraction and relation classification are conducted and evaluated in sentence-level (Bao et al., 2014).
",4.2 Compared Methods,[0],[0]
Parameter Settings.,4.2 Compared Methods,[0],[0]
"Based on the semantic meaning of proficient subset, we set ϕ2 to 1/|R∪{None}|, i.e., the probability of generating right label with random guess.",4.2 Compared Methods,[0],[0]
"Then we set ϕ1 to 1 − ϕ2, λ1 = λ2 = 1, and the learning rate α = 0.025.",4.2 Compared Methods,[0],[0]
"As for other parameters, they are tuned on the validation sets for each dataset.",4.2 Compared Methods,[0],[0]
"Similarly, all parameters of compared methods are tuned on validation set, and the parameters achieving highest F1 score are chosen for relation extraction.",4.2 Compared Methods,[0],[0]
"Given the experimental setup described above, the averaged evaluation scores in 10 runs of relation classification and relation extraction on two datasets are summarized in Table 5.
",4.3 Performance Comparison,[0],[0]
"From the comparison, it shows that NL strategy yields better performance than TD strategy, since the true labels inferred by Investment are actually wrong for many instances.",4.3 Performance Comparison,[0],[0]
"On the other hand, as discussed in Sec. 4.4, our method introduces context-awareness to true label discovery, while the inferred true label guides the relation extractor achieving the best performance.",4.3 Performance Comparison,[0],[0]
"This observation justifies the motivation of avoiding the source consistency assumption and the effectiveness of proposed true label discovery model.
",4.3 Performance Comparison,[0],[0]
One could also observe the difference between REHESSION and the compared methods is more significant on the NYT dataset than on the WikiKBP dataset.,4.3 Performance Comparison,[0],[0]
"This observation accords with the fact that the NYT dataset contains more conflicts than KBP dataset (see Table 6), and the intuition is that our method would have more advantages on more conflicting labels.
",4.3 Performance Comparison,[0],[0]
"Among four tasks, the relation classification of Wiki-KBP dataset has highest label quality, i.e. conflicting label ratio, but with least number of training instances.",4.3 Performance Comparison,[0],[0]
And CoType-RM and DSL reach relatively better performance among all compared methods.,4.3 Performance Comparison,[0],[0]
"CoType-RM performs much better than DSL on Wiki-KBP relation classification task, while DSL gets better or similar performance with CoType-RM on other tasks.",4.3 Performance Comparison,[0],[0]
"This may be because the representation learning method is able to generalize better, thus performs better when the training set size is small.",4.3 Performance Comparison,[0],[0]
"However, it is rather vulnerable to the noisy labels compared to DSL.",4.3 Performance Comparison,[0],[0]
"Our method employs embedding techniques, and also integrates context-aware true label dis-
covery to de-noise labels, making the embedding method rather robust, thus achieves the best performance on all tasks.",4.3 Performance Comparison,[0],[0]
"Context Awareness of True Label Discovery.
",4.4 Case Study,[0],[0]
"Although Universal Schemas does not adopted the source consistency assumption, but it’s conducted in document-level, and is context-agnostic in our sentence-level setting.",4.4 Case Study,[0],[0]
"Similarly, most true label discovery methods adopt the source consistency assumption, which means if they trust a labeling function, they would trust it on all annotations.",4.4 Case Study,[0],[0]
"And our method infers true labels in a context-aware manner, which means we only trust labeling functions on matched contexts.
",4.4 Case Study,[0],[0]
"For example, Investment and Universal Schemas refer None as true type for all four instances in Table 7.",4.4 Case Study,[0],[0]
"And our method infers born-in as the true label for the first two relation mentions; after replacing the matched contexts (born) with other words (elected and examined), our method no longer trusts born-in since the modified contexts are no longer matched, then infers None as the true label.",4.4 Case Study,[0],[0]
"In other words, our proposed method infer the true label in a context aware manner.
",4.4 Case Study,[0],[0]
Effectiveness of True Label Discovery.,4.4 Case Study,[0],[0]
"We explore the effectiveness of the proposed contextaware true label discovery component by comparing REHESSION to its variants REHESSION-TD and REHESSION-US, which uses Investment or Universal Schemas to resolve conflicts.",4.4 Case Study,[0],[0]
The averaged evaluation scores are summarized in Table 8.,4.4 Case Study,[0],[0]
We can observe that REHESSION significantly outperforms its variants.,4.4 Case Study,[0],[0]
"Since the only difference between REHESSION and its variants is the model employed to resolve conflicts, this gap verifies the effectiveness of the proposed contextaware true label discovery method.",4.4 Case Study,[0],[0]
Relation extraction aims to detect and categorize semantic relations between a pair of entities.,5.1 Relation Extraction,[0],[0]
"To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances).",5.1 Relation Extraction,[0],[0]
"Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus.",5.1 Relation Extraction,[0],[0]
"Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type.",5.1 Relation Extraction,[0],[0]
"Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way.
",5.1 Relation Extraction,[0],[0]
"Word embedding has demonstrated great potential in capturing semantic meaning (Mikolov et al., 2013), and achieved great success in a wide range of NLP tasks like relation extraction (Zeng et al., 2014; Takase and Inui, 2016; Nguyen and Grishman, 2015).",5.1 Relation Extraction,[0],[0]
"In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better.",5.1 Relation Extraction,[0],[0]
"True label discovery methods have been developed to resolve conflicts among multi-source information under the assumption of source consistency (Li et al., 2016; Zhi et al., 2015).",5.2 Truth Label Discovery,[0],[0]
"Specifically, in the spammer-hammer model (Karger et al., 2011), each source could either be a spammer, which annotates instances randomly; or a hammer, which annotates instances precisely.",5.2 Truth Label Discovery,[0],[0]
"In this paper, we assume each labeling function would be a hammer on its proficient subset, and would be a spammer otherwise, while the proficient subsets are identified in the embedding space.
",5.2 Truth Label Discovery,[0],[0]
"Besides data programming, socratic learning (Varma et al., 2016) has been developed to conduct binary classification under heterogeneous supervi-
sion.",5.2 Truth Label Discovery,[0],[0]
"Its true label discovery module supervises the discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset.",5.2 Truth Label Discovery,[0],[0]
"Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier.",5.2 Truth Label Discovery,[0],[0]
"Thus, its discriminative module might suffer from the overwhelming size of text features.",5.2 Truth Label Discovery,[0],[0]
"In this paper, we propose REHESSION, an embedding framework to extract relation under heterogeneous supervision.",6 Conclusion and Future Work,[0],[0]
"When dealing with heterogeneous supervisions, one unique challenge is how to resolve conflicts generated by different labeling functions.",6 Conclusion and Future Work,[0],[0]
"Accordingly, we go beyond the “source consistency assumption” in prior works and leverage context-aware embeddings to induce proficient subsets.",6 Conclusion and Future Work,[0],[0]
"The resulting framework bridges true label discovery and relation extraction with context representation, and allows them to mutually enhance each other.",6 Conclusion and Future Work,[0],[0]
"Experimental evaluation justifies the necessity of involving contextawareness, the quality of inferred true label, and the effectiveness of the proposed framework on two real-world datasets.
",6 Conclusion and Future Work,[0],[0]
There exist several directions for future work.,6 Conclusion and Future Work,[0],[0]
One is to apply transfer learning techniques to handle label distributions’ difference between training set and test set.,6 Conclusion and Future Work,[0],[0]
Another is to incorporate OpenIE methods to automatically find domainspecific patterns and generate pattern-based labeling functions.,6 Conclusion and Future Work,[0],[0]
Research was sponsored in part by the U.S. Army Research Lab.,7 Acknowledgments,[0],[0]
under Cooperative Agreement No.,7 Acknowledgments,[0],[0]
"W911NF-09-2-0053 (NSCTA), National Science Foundation IIS-1320617, IIS 16-18481, and",7 Acknowledgments,[0],[0]
"NSF IIS 17-04532, and grant 1U54GM114838 awarded by NIGMS through funds provided by the transNIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).",7 Acknowledgments,[0],[0]
The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government.,7 Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.,7 Acknowledgments,[0],[0]
Relation extraction is a fundamental task in information extraction.,abstractText,[0],[0]
"Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming.",abstractText,[0],[0]
"To overcome this drawback, we propose a novel framework, REHESSION, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics.",abstractText,[0],[0]
"These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance.",abstractText,[0],[0]
"Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion.",abstractText,[0],[0]
Extensive experimental results demonstrate the superiority of REHESSION over the state-of-the-art.,abstractText,[0],[0]
Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1723–1731 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1723",text,[0],[0]
Many idiomatic expressions may be interpreted both figuratively or literally.,1 Introduction,[0],[0]
Their intended usages depend on how they fit with their contexts.,1 Introduction,[0],[0]
"For example, the idiom ”spill the beans” is used figuratively in the first instance below, and literally in the second:
(1) [fig.]",1 Introduction,[0],[0]
The beans have been spilled.,1 Introduction,[0],[0]
From what I’ve read on Twitter I could probably fill out the forms and submit it to the FISA court.,1 Introduction,[0],[0]
"I don’t know what the big secret is.1
(2) [lit.]",1 Introduction,[0],[0]
"Spill the beans, flip the fruit, bust open a box of hot pockets.",1 Introduction,[0],[0]
"Make a general mess of the kitchen.2
This type of ambiguity is commonplace – prior work suggests that about half out of a sample of
1https://twitter.com/BTeboe/status/ 958792419302100993
2https://twitter.com/DukeRaccoon/ status/477530732173471744
60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009).",1 Introduction,[0],[0]
"Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015).
",1 Introduction,[0],[0]
"While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017).",1 Introduction,[0],[0]
"A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013).",1 Introduction,[0],[0]
Some previous unsupervised models tried to exploit linguistic differences in usages.,1 Introduction,[0],[0]
"For example, Fazly et al.(2009) observed that an idiom appearing in its canonical form is usually used figuratively; Sporleder and Li(2009) relied on the break in lexical coherence between the idioms and the context to signal a figurative usage.",1 Introduction,[0],[0]
"These heuristics, however, are not always applicable because the distinctions they depend upon may not be present or obvious.",1 Introduction,[0],[0]
"To improve generalization across different idioms and usage contexts, we need a more reliable heuristic, and appropriately incorporate it into an unsupervised learning framework.
",1 Introduction,[0],[0]
"We propose a heuristic that differentiates usages based on distributional semantics (Harris, 1954; Turney and Pantel, 2010).",1 Introduction,[0],[0]
"Our key insight is that when an idiom is used literally, its relationship with its context is more predictable than when it is used figuratively.",1 Introduction,[0],[0]
"This is because the literal meaning of an idiom is compositional (Katz and Giesbrecht, 2006), and the constituent words that make up the idiom are also meant literally.",1 Introduction,[0],[0]
"For example, in instance (2), spill is meant literally and can take on objects other than beans; moreover, one of the context words, mess, can often be seen to co-
occur with spill in other text, even without beans.",1 Introduction,[0],[0]
Our strategy is to represent an idiom’s literal usage in terms of the word embeddings of the idiom’s constituent words and other words they frequently co-occur with.,1 Introduction,[0],[0]
"Then, for any instance in which the idiom’s usage is not known, we only need to determine the semantic similarity between that instance and the idiom’s literal representation.",1 Introduction,[0],[0]
"We define a literal usage metric that estimates the likelihood that an instance would be labeled ”literal”.
",1 Introduction,[0],[0]
"While the literal usage metric captures the distributional semantic information of the context, we find that some other linguistic cues are also significant for usage detection (such as whether the subject of the sentence is a person); therefore, we allow our model to further refine through unsupervised methods.",1 Introduction,[0],[0]
"Specifically, we treat the usage (figurative or literal) as a hidden variable in probabilistic latent variable models, and we define a set of features that are linguistically relevant for idiom usage detection as observables.",1 Introduction,[0],[0]
"We integrate our literal usage metric with the latent variable models by treating the metric outputs as soft labels to guide the latent variable models toward grouping by usages.
",1 Introduction,[0],[0]
"We hypothesize that unsupervised learning in a more linguistically motivated feature space, informed by soft labels from a semantically driven metric, will produce more robust classifiers.",1 Introduction,[0],[0]
We conduct experiments comparing our approach against other supervised and unsupervised baselines.,1 Introduction,[0],[0]
Results suggest that our approach achieves performances that are competitive to supervised models.,1 Introduction,[0],[0]
"Despite the common perception that idioms are mainly used figuratively, many can also be meant literally.",2 Related Work,[0],[0]
A number of models have been proposed in the literature to recognize an idiom’s usages under different context.,2 Related Work,[0],[0]
Many rely on specific linguistic property to draw a clear-cut decision boundary between literal and figurative usages.,2 Related Work,[0],[0]
"For example, Fazly et al. (2009) proposed a method that relies on the concept of canonical form.",2 Related Work,[0],[0]
"Based on the observation that while literal usages are less syntactically restricted, figurative usages tend to occur in a small number of canonical form(s).",2 Related Work,[0],[0]
"As shown in the examples above, however, this rule of thumb does not always hold.",2 Related Work,[0],[0]
"Sporleder and Li (2009) proposed a method by
building a cohesion graph to include all content words in the context; if removing the idiom improves cohesion, they assume the instance is figurative.",2 Related Work,[0],[0]
"Later, Li and Sporleder (2009) used their cohesion graph method to label a subset of the test data with high confidence.",2 Related Work,[0],[0]
"This subset is then passed on as training data to the supervised classifier, which then labels the remainder of the dataset.
",2 Related Work,[0],[0]
"When manually annotated examples are available, supervised classifiers are effective.",2 Related Work,[0],[0]
"Rajani et al. (2014) extracted all non-stop-words in the context and used them as ”bag of words” features to train a L2 regularized Logistic Regression (L2LR) classifier (Fan et al., 2008).",2 Related Work,[0],[0]
"As local context of an idiom holds clues for discriminating between its literal and figurative usages, Liu and Hwa (2017) find that context representation also plays a significant role in idiom usage recognition.",2 Related Work,[0],[0]
"They took an adaptive approach, applying supervised ensemble learning over three classifiers based on different context representations (Peng et al., 2014; Birke and Sarkar, 2006; Rajani et al., 2014).",2 Related Work,[0],[0]
"Given a target idiomatic expression and a collection of instances in which the idiom occurs, our proposed system (Figure 1) determines whether the idiom in each instance is meant figuratively or literally.",3 Our Approach,[0],[0]
We first build a Literal Usage Representation for each idiom by leveraging the distributional semantics of its constituents (Sec 3.1).,3 Our Approach,[0],[0]
"Given an instance of idiom, we can determine its usage by the semantic similarity between the context of the instance and the Literal Usage Representation.",3 Our Approach,[0],[0]
"We define a Literal Usage Metric to transform the semantic similarity score into soft label, i.e., an initial rough estimation of the instance’s usage (Sec 3.2).",3 Our Approach,[0],[0]
"Finally, we treat the soft labels as distant supervision for downstream probabilistic latent variable models, in which the usages are considered as the hidden variables and are represented over a set of features.",3 Our Approach,[0],[0]
An idiom co-occurs with different sets of words depending on whether it is meant literally or figuratively.,3.1 Literal Usage Representation,[0],[0]
"For example, when used literally, get wind is more likely to co-occur with words such as rain, storm or weather; in contrast, when used figuratively, it frequently co-occurs with rumor or
story, etc.",3.1 Literal Usage Representation,[0],[0]
"Comparing the two sets of words associated with the idiom, we see that the literal set of words also tend to co-occur with just wind, a constituent word within the idiom.",3.1 Literal Usage Representation,[0],[0]
"Therefore, even without annotated data or dictionary, we may still approximate a representation for the literal meaning of an idiom by the idiom’s constituent words and their semantic relationship to other words.",3.1 Literal Usage Representation,[0],[0]
"To do so, we begin by initializing a literal meaning set to just the idiom’s main constituent words3; we then grow the set by adding two types of semantically related words.",3.1 Literal Usage Representation,[0],[0]
"First, we look for co-occuring words in a large textual corpus (e.g., (David et al., 2005)): for each constituent word w, we randomly sample s sentences that containw from the corpus; we extract the top n most frequent words (excluding stop words) and add them to the literal meaning set.",3.1 Literal Usage Representation,[0],[0]
"Second, we look for words that are semantically close in a word embedding space: we train a continuous bag-of-words (CBOW) embedding model (Mikolov et al., 2013) and add additional t words that are the most related to w using cosine similarity.
",3.1 Literal Usage Representation,[0],[0]
"All together, the literal usage representation is a collection of vectors, i.e., the embeddings of the words in the final extended literal meaning set.",3.1 Literal Usage Representation,[0],[0]
"The size of the set depends on parameters s, n, and t; if the chosen values are too small, we do not end up with a word collection that is representative enough; if the numbers are too large, we would only be wasting computing resources chasing Zipfian tails.",3.1 Literal Usage Representation,[0],[0]
"Parameter setting choices are discussed further in the experiment section.
",3.1 Literal Usage Representation,[0],[0]
"3We observe that the nouns tend to be the most indicative of the idiom’s literal meaning, but if the idiom does not contain any noun, we back off to any constituent word that is not a stop word.",3.1 Literal Usage Representation,[0],[0]
"Among all the instances to be classified, we expect the context words of the literal cases to be more semantically close to the literal usage representation we just formed.",3.2 Literal Usage Metrics,[0],[0]
Let L denote the set of words in the literal usage representation for the target idiom.,3.2 Literal Usage Metrics,[0],[0]
"For each instance, letC be the set of non-stop context words in the instance.",3.2 Literal Usage Metrics,[0],[0]
"We calculate s, the semantic similarity score between the context of the instance and the literal usage representation as follows:
s = 1 |C| ∑ c∈C 1 |L| ∑ l∈L sim(c, l) (1)
where c denotes a word in C, l denotes a word in L and sim(c, l) refers to the cosine similarity between the word embeddings of c and l.
Let S = {s1, s2, ...sn} be the set of semantic similarity scores for all the instances we wish to classify.",3.2 Literal Usage Metrics,[0],[0]
Instances with higher scores are more likely to use the idiom literally.,3.2 Literal Usage Metrics,[0],[0]
A naive literal usage metrics is to choose a predefined threshold for all idioms and label all the instances with score above the threshold as literal usages.,3.2 Literal Usage Metrics,[0],[0]
This approach is unlikely to work well in practice.,3.2 Literal Usage Metrics,[0],[0]
"As noted by previous work, idioms have different levels of semantic analyzability (Gibbs et al., 1989; Cacciari and Levorato, 1998).",3.2 Literal Usage Metrics,[0],[0]
"When an idiom has a high degree of semantic analyzability, its contextual words will be more semantically close to the literal usage representation, thus a higher threshold is needed.
",3.2 Literal Usage Metrics,[0],[0]
"In this work, we select a different decision threshold for each idiom adaptively based on the similarity scores distribution.",3.2 Literal Usage Metrics,[0],[0]
"And most importantly, rather than generate a hard label, we transform these scores into a probabilistic metric,
where 0 means the usage in the instance is almost certainly figurative while 1.0 means it is literal.
",3.2 Literal Usage Metrics,[0],[0]
We propose a metric based on the principle of Minimum Variance (MinV).,3.2 Literal Usage Metrics,[0],[0]
"That is, we first sort the scores in S and choose the threshold (from these scores) that minimizes the sum of variances of the two resulting clusters.",3.2 Literal Usage Metrics,[0],[0]
"For each instance i, we then apply the following metric to estimate the probability that the idiom in instance i is meant literally based on its semantic similarity score si :
Pri = 1
1 + e−k∗(si−t) (2)
where k is a constant weighting factor and t indicates the learned threshold.",3.2 Literal Usage Metrics,[0],[0]
"The intuition is that the larger the difference between si and the threshold is, the more likely the instance i is literal; the probability of literal usage is not linearly correlated to the difference, we use the sigmoid function to account for this non-linearity.",3.2 Literal Usage Metrics,[0],[0]
We incorporate k to scale the value of the difference since it is generally very small (close to 0).,3.2 Literal Usage Metrics,[0],[0]
"Without k, all the Pr values gravitate toward 0.5, rendering the soft label being equivalent to random guess.",3.2 Literal Usage Metrics,[0],[0]
We set k to 5 for all the idioms based on a development set.,3.2 Literal Usage Metrics,[0],[0]
"The soft label, generated by MinV (the literal usage metric), captures the distributional semantic information of the context.",3.3 Heuristically Informed Usage Recognition,[0],[0]
"In practice, there are a variety of other linguistic features which are also informative of the intended usage of idiom.",3.3 Heuristically Informed Usage Recognition,[0],[0]
We explore probabilistic latent variable models over a collection of features that are linguistically relevant for idiom usage detection.,3.3 Heuristically Informed Usage Recognition,[0],[0]
The soft label is integrated into the unsupervised learning of hidden usages as a distant supervision.,3.3 Heuristically Informed Usage Recognition,[0],[0]
"In this section, we will describe the proposed features in the latent variable models and how we integrate the soft label into the learning process.",3.3 Heuristically Informed Usage Recognition,[0],[0]
"To predict an idiom’s usage in instances, we consider two representative probabilistic latent variable models:",3.3.1 Latent Variable Models,[0],[0]
"Latent Dirichlet Allocation (LDA) (Blei et al., 2003)4 and unsupervised Naive Bayes (NB).",3.3.1 Latent Variable Models,[0],[0]
"For both models, the latent variable is the idiom usage (figurative vs. literal); the observables
4Although originally conceived for modeling document content, LDA can be applied to any kind of discrete input
are linguistic features that can be extracted from the instances, described below:
",3.3.1 Latent Variable Models,[0],[0]
"Subordinate Clause We encode a binary feature indicating whether the target expression is followed by a subordinate clause (the Stanford Parser (Chen and Manning, 2014) is used).",3.3.1 Latent Variable Models,[0],[0]
This feature is useful for some idioms such as in the dark.,3.3.1 Latent Variable Models,[0],[0]
"It usually suggests a figurative usage as in You’ve kept us totally in the dark about what happened that night.
",3.3.1 Latent Variable Models,[0],[0]
"Selectional Preference Violation of selectional preference is normally a signal of figurative usage (e.g., having an abstract entity as the subject of play with fire).",3.3.1 Latent Variable Models,[0],[0]
We encode this feature if the head word of the idiom is a verb and focus on the subject of the verb.,3.3.1 Latent Variable Models,[0],[0]
"We apply Stanford Name Entity tagger (Finkel et al., 2005) with 3 classes (”Location”, ”Person”, ”Organization”) on the sentence containing the idiom.",3.3.1 Latent Variable Models,[0],[0]
"If the subject is labeled as an Entity, its class will be encoded in the feature vector.",3.3.1 Latent Variable Models,[0],[0]
Pronouns such as ”I” and ”he” also indicate the subject is a ”Person”.,3.3.1 Latent Variable Models,[0],[0]
"However, they are normally not tagged by Stanford Name Entity tagger.",3.3.1 Latent Variable Models,[0],[0]
"To overcome this issue, we add Part-of-Speech of the subject into the feature vector.
",3.3.1 Latent Variable Models,[0],[0]
Abstractness Abstract words refer to things which are hard to perceive directly with our senses.,3.3.1 Latent Variable Models,[0],[0]
"Abstractness has been shown to be useful in the detection of metaphor, another type of figurative language (Turney et al., 2011).",3.3.1 Latent Variable Models,[0],[0]
A figurative usage of an idiomatic phrase may have relatively more abstract contextual words.,3.3.1 Latent Variable Models,[0],[0]
"For example, in the sentence She has lived life in the fast lane, the word life is considered as an abstract word.",3.3.1 Latent Variable Models,[0],[0]
This is a useful indicator that in the fast lane is used figuratively.,3.3.1 Latent Variable Models,[0],[0]
"We use the MRC Psycholinguistic Database Machine Usable Dictionary (Coltheart, 1981) which contains a list of 4295 words with their abstractness measure between 100 and 700.",3.3.1 Latent Variable Models,[0],[0]
We calculate the average abstractness score for all the contextual words (with stop words being removed) in the sentence containing the idiom.,3.3.1 Latent Variable Models,[0],[0]
"The score is then transformed into categorical feature to overcome sparsity problem based on the following criteria: concrete (450 - 700), medium (350 - 450), abstract (100 - 350).
",3.3.1 Latent Variable Models,[0],[0]
Neighboring Words Words preceding and following the idiomatic expression can be very informative in terms of usage recognition.,3.3.1 Latent Variable Models,[0],[0]
"For example, words such as relax or shower before the idiom in hot water often signal a literal usage.
",3.3.1 Latent Variable Models,[0],[0]
Part-of-Speech of the Neighboring Words Class of neighboring words might be useful as well.,3.3.1 Latent Variable Models,[0],[0]
"For example, a pronoun preceding dog’s age generally indicates a literal usage, as in I think my dog’s age is starting to catch up.",3.3.1 Latent Variable Models,[0],[0]
"She sometimes needs help to jump on to my bed, while a determiner usually marks a figurative usage, as in It’s been a dog’s age since I’ve used Twitter.",3.3.1 Latent Variable Models,[0],[0]
"Given a collection of instances and their features, either LDA or NB can separate the instances into two groups (hopefully, by usages), but it does not associate the right label (i.e., ”figurative” or ”literal”) to the groups.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We do not want to rely on any manual annotations for this step.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Therefore, we integrate the automatically generated soft labels (based on MinV, our literal usage metric) into the unsupervised learning procedure as a weak form of supervision.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Formally, we want to estimate each instance’s posterior distribution over (literal/figurative) usages θdu and usage-feature distribution φuf .",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"For LDA, we derive a Gibbs sampling algorithm which incorporates the soft label into the learning procedure.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We refer it as informed Gibbs sampling (infGibbs).,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"For unsupervised naive Bayes model, we adapt the classical Expectation-Maximization algorithm to integrate the soft label.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"We refer it as informed ExpectationMaximization (infEM).
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Informed Gibbs Sampling The Gibbs sampling algorithm (Griffiths and Steyvers, 2004) used in traditional LDA initializes each word token a random hidden topic.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The system needs to interpret the learned topics post-hoc, e.g., by human annotation.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"In our case, for each feature f in each instance, an initial random usage biased by the instance’s soft label is assigned to f (i.e., a Bernoulli trial).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Since the soft label explicitly encodes an instance’s literal and figurative usage distribution, we do not need to interpret the learned usages at the end of the algorithm.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Based on these assignments, we build a feature-usage counting matrix CFU and instance-usage counting matrix CDU with dimensions |F | × 2 and |D|",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"× 2 respectively (|F | is the feature size and |D| is the number of instances): CFUi,j is the count of feature i assigned to usage j; CDUd,j is the count of features assigned to usage j in instance d.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Then for each feature f in each instance, we resample a new usage for f and matrices CFU and CDU will
be updated accordingly.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This step will be repeated for T times.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The resampling equation is:
p(ui = j|u−i, f) ∝",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"pj · C
fi −i,j+β
C (∗) −i,j+|F |β
· C di −i,j+α
C di −i,∗+|U |α
(3) where i indexes features in the instance d, j is an index into literal and figurative usages, ∗ indicates a summation over that dimension and − means excluding the corresponding instance.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The first factor pj is the soft label encoding prior usage distribution.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The second factor represents the probability of feature f under usage j (Cfi−i,j is the count of the feature f assigned to usage j, excluding the current usage assignment ui).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The third factor represents the probability of usage j in the current instance (Cdi−i,j is the count of linguistic features which are assigned to usage j in the current instance, excluding the current feature f ).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The value of |U | is 2, representing the number of usages (i.e., figurative and literal).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
α and β are the hyper-parameters from the Dirichlet priors (we set both of them to 1).,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The core idea of Equation 3 is to integrate both distribution semantic information (soft label, the first factor) and linguistically motivated features (the second and third factors) into the inference procedure.
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The matrices of CFU and CDU from the last 10% ∗ T iterations are averaged and then normalized to approximate the true usage-feature distribution φuf and instance-usage distribution θdu respectively.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The final result is determined by θdu, i.e., assigning each instance with the usage of probability higher than 0.5.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We do average to have a more stable result because an accidental bad sampling would affect our model negatively if we only use the CFU and CDU from the last iteration.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This procedure is important for some idioms if their feature space is sparse.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The iteration number T is set to 500 based on a development set.
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Informed Expectation Maximization Combining a Naive Bayes classifier with the EM algorithm has been widely used in text classification and word sense disambiguation (Hristea, 2013; Nigam et al., 2000).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"In our case, we want to construct a model to recover the missing literal and figurative labels of the instances of the target idiom.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This section describes two extensions to the basic EM algorithm for idiom usage recognition.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The extensions help improve parameter estimation by taking the automatically learned soft labels into
consideration.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Our informed EM method extends a basic version for NB (Hristea, 2013), where the initial parameter values θdu and φuf are chosen randomly.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"At each iteration, the E-step of the algorithm estimates the expectations of the missing values (i.e. the literal and figurative usage) given the latest iteration of the model parameters; the M-step maximizes the likelihood of the model parameters using the previously-computed expectations of the missing values.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"As we’ve done with extending Gibbs sampling for LDA, we also perform two similar adaptations on conventional EM for NB to incorporate soft labels.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"First, we assign each instance an initial usage distribution θdu directly using the soft label, and then initialize the usagefeature distribution φuf using these assignments.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We refer it as informed initialization.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Second, in the E-step, we multiply the expectation result of the basic EM with the soft label as the new expected usage for each instance (i.e., updating θdu).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The M-step is the same as basic EM to update the usage-feature distribution φuf .,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"We conduct experiments to address three questions:
1.",4 Evaluation,[0],[0]
How effective is our overall approach?,4 Evaluation,[0],[0]
"How does it compare against previous work?
2.",4 Evaluation,[0],[0]
"How effective is our literal usage metric (i.e., MinV) compared to other heuristics?
3.",4 Evaluation,[0],[0]
How effective is our literal usage metric at informing downstream learning processes?,4 Evaluation,[0],[0]
Models Our main experiments will evaluate the two variants of the proposed fully unsupervised model as described in section 3: MinV+infGibbs and MinV+infEM.,4.1 Experimental Setup,[0],[0]
We report the average performance of our models over 5 runs.,4.1 Experimental Setup,[0],[0]
Performing multiple runs is necessary because we have a sampling process.,4.1 Experimental Setup,[0],[0]
"They are compared with three baseline unsupervised models: Sporleder and Li (2009), Li and Sporleder (2009)5 and Fazly et al. (2009); and two baseline supervised models: Rajani et al. (2014) and Liu and Hwa (2017) (using 5-fold cross validation).
",4.1 Experimental Setup,[0],[0]
"5We replace Normalized Google Distance (NGD) with word embeddings to measure the semantic relatedness between words due to the query frequency restriction on the API of NGD.
",4.1 Experimental Setup,[0],[0]
"Parameter setting Recall that in order to build the literal usage representation of an idiom, we need to sample s sentences that contain each constituent word w from an external corpus; extract from them the top n most frequently co-occurring words with w; then separately find t words that are semantically similar to w using word embeddings.",4.1 Experimental Setup,[0],[0]
"To set parameters with values in reasonable ranges, we evaluated MinV on a small development set.",4.1 Experimental Setup,[0],[0]
"We picked 10 idioms that are different from the evaluation set, scraped 50 instances from the web for each idiom, and labeled them ourselves.",4.1 Experimental Setup,[0],[0]
"We find that s >= 100, n=10, and t=5 yield good results.
",4.1 Experimental Setup,[0],[0]
"We use the gensim toolkit (Řehůřek and Sojka, 2010) and train our word embedding model using the continuous bag of word model on Text8 Corpus6.",4.1 Experimental Setup,[0],[0]
Negative sampling is applied as the training method; the min count is set to 2.,4.1 Experimental Setup,[0],[0]
"For the other parameters, we use the default settings in gensim.",4.1 Experimental Setup,[0],[0]
Evaluative Data,4.1 Experimental Setup,[0],[0]
"Our goal is to compare all the methods under two public available corpora: SemEval 2013 Task 5B corpus (Korkontzelos et al., 2013), which is used by prior supervised methods (Liu and Hwa, 2017; Rajani et al., 2014) and verb–noun combination (VNC) dataset (Cook et al., 2008), which is used by a prior unsupervised method (Fazly et al., 2009).",4.1 Experimental Setup,[0],[0]
"However, there are some methods-datasets conflicts that have to be resolved.",4.1 Experimental Setup,[0],[0]
"Because the idioms in the SemEval dataset are all in their canonical forms, and because the idioms are not restricted to the verb-noun combination, we cannot evaluate the method by Fazly et al. on this dataset (as their method is tailored to verbnoun combination).",4.1 Experimental Setup,[0],[0]
"Some idioms from the VNC dataset are almost always used figuratively (or literally), which presents a problem for supervised methods.",4.1 Experimental Setup,[0],[0]
"To facilitate full comparisons, we select the subset of idioms from the VNC corpus whose number of literal and figurative instances are both higher than 10.",4.1 Experimental Setup,[0],[0]
A summary of the two corpora is shown in Table 1.,4.1 Experimental Setup,[0],[0]
"Note that each instance in SemEval corpus has about 3∼5 sentences; for consistency, we use 3 sentences as the context: the sentence with the target idiom and two neighboring sentences.",4.1 Experimental Setup,[0],[0]
Evaluation metric,4.1 Experimental Setup,[0],[0]
"Following the convention in prior works, we report the F-score for the recognition of figurative usages and the overall accuracy.
6From http://mattmahoney.net/dc/text8.",4.1 Experimental Setup,[0],[0]
zip,4.1 Experimental Setup,[0],[0]
Table 2 shows the result of our models and the other comparative methods.,4.2 The Performance of Our Full Models,[0],[0]
"Our proposed models show consistent performance across the two corpora, outperforming the unsupervised baselines from Sporleder and Li (2009), Li and Sporleder (2009) and the supervised model from Rajani et al. (2014).",4.2 The Performance of Our Full Models,[0],[0]
"Moreover, there is no statistical significance in the F-score difference between the supervised ensemble model from Liu and Hwa (2017) and our models.
",4.2 The Performance of Our Full Models,[0],[0]
"On the VNC corpus, our models have comparable average scores as that of Fazly et al. (2009); our scores are more stable across different idioms.",4.2 The Performance of Our Full Models,[0],[0]
"While the method of Fazly et al. is nearly perfect for some idioms (0.98 on ”take heart”), it performs poorly for others (e.g., 0.33 on ”pull leg”).",4.2 The Performance of Our Full Models,[0],[0]
Their algorithm has trouble with idioms whose canonical and non-canonical forms can appear frequently both in literal and figurative usages.,4.2 The Performance of Our Full Models,[0],[0]
"The core of our approach is MinV, the literal usage metric we developed to generate soft labels to guide the unsupervised learning.",4.3 Effectiveness of MinV,[0],[0]
"This experiment examines its effectiveness by creating usage classifications directly from it (i.e., if MinV predicts a probability of >0.5, predict ”literal”).",4.3 Effectiveness of MinV,[0],[0]
"We compare MinV against two alternative heuristics.
",4.3 Effectiveness of MinV,[0],[0]
MinV is based on two core ideas.,4.3 Effectiveness of MinV,[0],[0]
"First, if an idiom is used figuratively, we expect to see a big difference (low similarity scores) between its context and the semantic representation of idiom’s literal usage.",4.3 Effectiveness of MinV,[0],[0]
"The idea is similar to that of Sporleder and Li (2009), but they relied on lexical chain instead of distributional semantics.",4.3 Effectiveness of MinV,[0],[0]
"Second, instead of choosing a predefined threshold to separate the raw semantic similarity scores, we select a different decision threshold for each idiom adaptively based on the distribution of the scores.",4.3 Effectiveness of MinV,[0],[0]
"So as an alternative, we compare MinV against a Fixed-Threshold heuristic that labels an instance as ”literal” if its raw score is higher than some
global threshold (set to 0.346 based on development data).
",4.3 Effectiveness of MinV,[0],[0]
"In Table 3, we observe that Minv outperforms both Sporleder and Li’s model as well as FixedThreshold, but using MinV by itself is not sufficient.",4.3 Effectiveness of MinV,[0],[0]
"It has great fluctuations, e.g., the F-Score for individual idioms varies from 0.43 to 0.88.",4.3 Effectiveness of MinV,[0],[0]
Recall that MinV +infGibbs has a smaller fluctuation across different idioms in Table 2.,4.3 Effectiveness of MinV,[0],[0]
"These results suggest that the subsequent learning process is effective.
",4.3 Effectiveness of MinV,[0],[0]
"Through error analysis, we find two major factors contributing to the performance fluctuation.",4.3 Effectiveness of MinV,[0],[0]
"First, the context itself could be misleading.",4.3 Effectiveness of MinV,[0],[0]
"An error case of play ball by MinV is:
All 10-year-old Minnie Cruttwell wants to do is play with the boys , but the Football Association are not playing ball.",4.3 Effectiveness of MinV,[0],[0]
"She is a member of a mixed team called Balham Blazers , but the FA say she must join a girls’ team when she is 12.
",4.3 Effectiveness of MinV,[0],[0]
The context words in bold (which are related to the word ”ball”) mislead MinV to predict a ”literal” usage when it is actually a ”figurative” usage (since an organization such as the Football Association cannot literally play ball).,4.3 Effectiveness of MinV,[0],[0]
"Second, not all content words in the context are relevant for distinguishing the idiom’s usage.",4.3 Effectiveness of MinV,[0],[0]
A future direction is to prune contextual words more intelligently.,4.3 Effectiveness of MinV,[0],[0]
We have argued that an advantage of using a metric with a probabilistic interpretation instead of a binary class heuristic is that its scores can be incorporated into subsequent learning models as soft labels.,4.4 Integration of MinV into Learning,[0],[0]
"In this set of experiments, we evaluate the impact of the metric on the learning methods.",4.4 Integration of MinV into Learning,[0],[0]
"First, we consider unsupervised learning without input from the literal usage metric.",4.4 Integration of MinV into Learning,[0],[0]
We cluster the instances with the original Gibbs sampling and EM algorithms and then label the two clusters with the majority usage within the clusters.,4.4 Integration of MinV into Learning,[0],[0]
"Second, we explore using the information from the literal usage metric as ”noisy gold standard” to perform supervised training on a nearest neighbors (NN) classifier.",4.4 Integration of MinV into Learning,[0],[0]
"Specifically, the literal and figurative instances labeled by MinV with high confidence (top 30%) are used as example set.",4.4 Integration of MinV into Learning,[0],[0]
"Then for each test instance, we calculate its cosine similarity (in feature space) to the literal and figurative example sets and assign the label of the closest set.",4.4 Integration of MinV into Learning,[0],[0]
"We refer this model as MinV +NN.
Table 4 shows the performances of the new models, which are all worse than our full models MinV +infGibbs and MinV +infEM.",4.4 Integration of MinV into Learning,[0],[0]
This highlights the advantage of integrating distributional semantic information and local features into one single learning procedure.,4.4 Integration of MinV into Learning,[0],[0]
"Without the informed prior (encoded by the soft labels), the Gibbs sampling and EM algorithms only seek to maximize the probability of the observed data, and may fail to learn the underlying usage structure.
",4.4 Integration of MinV into Learning,[0],[0]
The model MinV +NN is not as competitive as our full models.,4.4 Integration of MinV into Learning,[0],[0]
It is too sensitive to the selected instances.,4.4 Integration of MinV into Learning,[0],[0]
"Even though the training examples are instances that MinV is the most confident about, there are still mislabelled instances.",4.4 Integration of MinV into Learning,[0],[0]
These ”noisy training examples” would lead the NN classifier to make unreliable predictions.,4.4 Integration of MinV into Learning,[0],[0]
"In contrast, our unsupervised learning is less sensitive to the performance of MinV; it can achieve a decent performance for an idiom even when the quality of the soft labels is poor.",4.4 Integration of MinV into Learning,[0],[0]
"For example, when using MinV as a stand-alone model for break a leg, its figura-
tive F-score is only 0.43, but through further training, the full model MinV+infGibbs achieves 0.64.",4.4 Integration of MinV into Learning,[0],[0]
Fig. 2 shows the training curve.,4.4 Integration of MinV into Learning,[0],[0]
A possible reason for this phenomenon is that the soft label is integrated into the learning process by biasing the sampling procedure (see Equation 3).,4.4 Integration of MinV into Learning,[0],[0]
We only encourage our model to follow the distributional semantic evidence captured by soft label and do not force it.,4.4 Integration of MinV into Learning,[0],[0]
So if there are strong evidences encoded by the linguistically motivated features in the instances to overcome the soft label it still has the freedom to do so.,4.4 Integration of MinV into Learning,[0],[0]
We have presented an unsupervised method for idiom usage recognition built upon the heuristic that instances that use the idiom literally are semantically closer to constituent words of the idiom.,5 Conclusion,[0],[0]
Experimental results on two different corpora suggest that our models are competitive against supervised methods and prior unsupervised methods.,5 Conclusion,[0],[0]
"Depending on the surrounding context, an idiomatic expression may be interpreted figuratively or literally.",abstractText,[0],[0]
This paper proposes an unsupervised learning method for recognizing the intended usages of idioms.,abstractText,[0],[0]
We treat the possible usages as a latent variable in probabilistic models and train them in a linguistically motivated feature space.,abstractText,[0],[0]
"Crucially, we show that distributional semantics serves as a helpful heuristic for formulating a literal usage metric to estimate the likelihood that the idiom is intended literally.",abstractText,[0],[0]
This information can then guide the unsupervised training process for the probabilistic models.,abstractText,[0],[0]
Experiments show that our overall model performs competitively against supervised methods.,abstractText,[0],[0]
Heuristically Informed Unsupervised Idiom Usage Recognition,title,[0],[0]
"Hierarchical clustering (HC) is a widely used data analysis tool, ubiquitous in information retrieval, data mining, and machine learning (see a survey by Berkhin [2006]).",1 Introduction,[0],[0]
This clustering technique represents a given dataset as a binary tree; each leaf represents an individual data point and each internal node represents a cluster on the leaves of its descendants.,1 Introduction,[0],[0]
"HC has become the most popular method for gene expression data analysis Eisen et al. [1998], and also has been used in the analysis of social networks Leskovec et al. [2014], Mann et al. [2008], bioinformatics Diez et al. [2015], image and text classification Steinbach et al. [2000], and even in analysis of financial markets Tumminello et al. [2010].",1 Introduction,[0],[0]
"It is attractive because it provides richer information at all levels of granularity simultaneously, compared to more traditional flat clustering approaches like k-means or k-median.
",1 Introduction,[0],[0]
"Recently, Dasgupta [2016] formulated HC as a combinatorial optimization problem, giving a principled way to compare the performance of different HC algorithms.",1 Introduction,[0],[0]
"This optimization viewpoint has since received a lot of attention Roy and Pokutta [2016], Charikar and Chatziafratis [2017], Cohen-Addad et al. [2017], Moseley and Wang [2017], Cohen-Addad et al. [2018] that has led not only to the development of new algorithms but also to theoretical justifications for the observed success of popular HC algorithms (e.g. average-linkage).
",1 Introduction,[0],[0]
"However, in real applications of clustering, the user often has background knowledge about the data that may not be captured by the input to the clustering algorithm.",1 Introduction,[0],[0]
"There is a rich body of work on constrained (flat) clustering formulations that take into account such user input in the form of “cannot link” and “must link” constraints Wagstaff and Cardie [2000], Wagstaff et al. [2001],
ar X
iv :1
80 5.
09 47
6v 2
[ cs
.D S]
1 4
Ju",1 Introduction,[0],[0]
"l 2
Bilenko et al. [2004], Rangapuram and Hein [2012].",1 Introduction,[0],[0]
"Very recently, “semi-supervised” versions of HC that incorporate additional constraints have been studied Vikram and Dasgupta [2016], where the natural form of such constraints is triplet (or “must link before”) constraints ab|c1: these require that valid solutions contain a sub-cluster with a, b together and c previously separated from them.2 Such triplet constraints, as we formally show later, can encode more general structural constraints in the form of rooted subtrees.",1 Introduction,[0],[0]
"Surprisingly, such simple triplet constraints already pose significant challenges for bottom-up linkage methods.",1 Introduction,[0],[0]
"(Figure 1).
",1 Introduction,[0],[0]
Our work is motivated by applying the optimization lens to study the interaction of hierarchical clustering algorithms with structural constraints.,1 Introduction,[0],[0]
Constraints can be fairly naturally incorporated into top-down (i.e. divisive) algorithms for hierarchical clustering; but can we establish guarantees on the quality of the solution they produce?,1 Introduction,[0],[0]
Another issue is that incorporating constraints from multiple experts may lead to a conflicting set of constraints; can the optimization viewpoint of hierarchical clustering still help us obtain good solutions even in the presence of infeasible constraints?,1 Introduction,[0],[0]
"Finally, different objective functions for HC have been studied in the literature; do algorithms designed for these objectives behave similarly in the presence of constraints?",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to propose a unified approach for constrained HC through the lens of optimization and to give provable approximation guarantees for a collection of fast and simple top-down algorithms that have been used for unconstrained HC in practice (e.g. community detection in social networks Mann et al. [2008]).
",1 Introduction,[0],[0]
Background on Optimization View of HC.,1 Introduction,[0],[0]
Dasgupta [2016] introduced a natural optimization framework for HC.,1 Introduction,[0],[0]
"Given a weighted graph G(V,E,w) and pairwise similarities wij ≥ 0 between the n data points i, j ∈ V , the goal is to find a hierarchical tree T ∗ such that
T ∗ = arg min all trees T ∑ (i,j)∈E wij · |Tij",1 Introduction,[0],[0]
"| (1)
where Tij is the subtree rooted at the lowest common ancestor of i, j in T and |Tij",1 Introduction,[0],[0]
"| is the number 1Hierarchies on data imply that all datapoints are linked at the highest level and all are separated at the lowest level, hence “cannot link” and “must link” constraints are not directly meaningful.",1 Introduction,[0],[0]
2For,1 Introduction,[0],[0]
"a concrete example from taxonomy of species, a triplet constraint may look like (Tuna,Salmon|Lion).
of leaves it contains.3",1 Introduction,[0],[0]
We denote (1) as similarity-HC.,1 Introduction,[0],[0]
"For applications where the geometry of the data is given by dissimilarities, again denoted by {wij}(i,j)∈E , Cohen-Addad et al. [2018] proposed an analogous approach, where the goal is to find a hierarchical tree T ∗ such that
T ∗ = arg max all trees T ∑ (i,j)∈E wij · |Tij",1 Introduction,[0],[0]
"| (2)
We denote (2) as dissimilarity-HC.",1 Introduction,[0],[0]
"A comprehensive list of desirable properties of the aformentioned objectives can be found in Dasgupta [2016], Cohen-Addad et al. [2018].",1 Introduction,[0],[0]
"In particular, if there is an underlying ground-truth hierarchical structure in the data, then T ∗ can recover the ground-truth.",1 Introduction,[0],[0]
"Also, both objectives are NP-hard to optimize, so the focus is on approximation algorithms.
",1 Introduction,[0],[0]
Our Results.,1 Introduction,[0],[0]
i),1 Introduction,[0],[0]
"We design algorithms that take into account both the geometry of the data, in the form of similarities, and the structural constraints imposed by the users.",1 Introduction,[0],[0]
Our algorithms emerge as the natural extensions of Dasgupta’s original recursive sparsest cut algorithm and the recursive balanced cut suggested in Charikar and Chatziafratis [2017].,1 Introduction,[0],[0]
"We generalize previous analyses to handle constraints and we prove an O(kαn)-approximation guarantee4, thus surprisingly matching the best approximation guarantee of the unconstrained HC problem for constantly many constraints.
ii)",1 Introduction,[0],[0]
"In the case of infeasible constraints, we extend the similarity-HC optimization framework, and we measure the quality of a possible tree T by a constraint-based regularized objective.",1 Introduction,[0],[0]
The regularization naturally favors solutions with as few constraint violations as possible and as far down the tree as possible (similar to the motivation behind similarity-HC objective).,1 Introduction,[0],[0]
"For this problem, we provide a top-down O(kαn)-approximation algorithm by drawing an interesting connection to an instance of the hypergraph sparsest cut problem.
",1 Introduction,[0],[0]
iii) We then change gears and study the dissimilarity-HC objective.,1 Introduction,[0],[0]
"Surprisingly, we show that known top-down techniques do not cope well with constraints, drawing a contrast with the situation for similarity-HC.",1 Introduction,[0],[0]
"Specifically, the (locally) densest cut heuristic performs poorly even if there is only one triplet constraint, blowing up its approximation factor to O(n).",1 Introduction,[0],[0]
"Moreover, we improve upon the state-of-the-art in Cohen-Addad et al. [2018], by showing a simple randomized partitioning is a 23 -approximation algorithm.",1 Introduction,[0],[0]
We also give a deterministic local-search algorithm with the same worst-case guarantee.,1 Introduction,[0],[0]
"Furthermore, we show that our randomized algorithm is robust under constraints, mainly because of its “exploration” behavior.",1 Introduction,[0],[0]
"In fact, besides the number of constraints, we propose an inherent notion of dependency measure among constraints to capture this behavior quantitatively.",1 Introduction,[0],[0]
"This helps us not only to explain why “non-exploring” algorithms may perform poorly, but also gives tight guarantees for our randomized algorithm.
",1 Introduction,[0],[0]
Experimental Results.,1 Introduction,[0],[0]
"We run experiments on the Zoo dataset [Lichman, 2013] to demonstrate our approach and the performance of our algorithms for a taxonomy application.",1 Introduction,[0],[0]
We consider a setup where there is a ground-truth tree and extra information regarding this tree is provided for the algorithm in the form of triplet constraints.,1 Introduction,[0],[0]
"The upshot is we believe specific variations of our algorithms can exploit this information; In this practical application, our algorithms have around %9 imrpvements in the objective compared to the naive recursive sparsest cut proposed in Dasgupta [2016] that does not use this information.",1 Introduction,[0],[0]
"See Appendix B for more details on the setup and precise conclusions of our experiments.
",1 Introduction,[0],[0]
"3Observe that in HC, all edges get cut eventually.",1 Introduction,[0],[0]
"Therefore it is better to postpone cutting “heavy” edges to when the clusters become small, i.e .as far down the tree as possible.
",1 Introduction,[0],[0]
"4For n data points, αn =",1 Introduction,[0],[0]
"O( √ logn) is the best approximation factor for the sparsest cut and k is the number of
constraints.
",1 Introduction,[0],[0]
Constrained HC work-flow in Practice.,1 Introduction,[0],[0]
"Throughout this paper, we develop different tools to handle user-defined structural constraints for hierarchical clustering.",1 Introduction,[0],[0]
"Here we describe a recipe on how to use our framework in practice.
",1 Introduction,[0],[0]
(1) Preprocessing constraints to form triplets.,1 Introduction,[0],[0]
User-defined structural constraints as rooted binary subtrees are convenient for the user and hence for the usability of our algorithm.,1 Introduction,[0],[0]
"The following proposition (whose proof is in the supplement) allows us to focus on studying HC with just triplet constraints.
",1 Introduction,[0],[0]
Proposition 1.,1 Introduction,[0],[0]
"Given constraints as a rooted binary subtree T on k data points (k ≥ 3), there is linear time algorithm that returns an equivalent set of at most k triplet constraints.
",1 Introduction,[0],[0]
(2) Detecting feasibility.,1 Introduction,[0],[0]
"The next step is to see if the set of triplet constraints is consistent, i.e. whether there exists a HC satisfying all the constraints.",1 Introduction,[0],[0]
"For this, we use a simple linear time algorithm called BUILD Aho et al. [1981].
(3) Hard constraints vs. regularization.",1 Introduction,[0],[0]
"BUILD can create a hierarchical decomposition that satisfies triplet constraints, but ignores the geometry of the data, whereas our goal here is to consider both simultaneously.",1 Introduction,[0],[0]
"Moreover, in the case that the constraints are infeasible, we aim to output a clustering that minimizes the cost of violating constraints combined with the cost of the clustering itself.",1 Introduction,[0],[0]
"• Feasible instance: to output a feasible HC, we propose using Constrained Recursive Sparsest Cut (CRSC) or Constrained Recursive Balanced Cut (CRBC): two simple top-down algorithms which are natural adaptations of recursive sparsest cut [Mann et al., 2008, Dasgupta, 2016] or recursive balanced cut Charikar and Chatziafratis [2017] to respect constraints (Section 2).",1 Introduction,[0],[0]
"• Infeasible instance: in this case, we turn our attention to a regularized version of HC, where the cost of violating constraints is added to the tree cost.",1 Introduction,[0],[0]
"We then propose an adaptation of CRSC, namely Hypergraph Recursive Sparsest Cut (HRSC) for the regularized problem (Section 3).
",1 Introduction,[0],[0]
Real-world application example.,1 Introduction,[0],[0]
"In phylogenetics, which is the study of the evolutionary history and relationships among species, an end-user usually has access to whole genomes data of a group of organisms.",1 Introduction,[0],[0]
"There are established methods in phylogeny to infer similarity scores between pairs of datapoints, which give the user the similarity weights wij .",1 Introduction,[0],[0]
"Often the user also has access to rare structural footprints of a common ancestry tree (e.g. through gene rearrangement data, gene inversions/transpositions etc., see Patané et al. [2018]).",1 Introduction,[0],[0]
"These rare, yet informative, footprints play the role of the structural constraints.",1 Introduction,[0],[0]
"The user can follow our pre-processing step to get triplet constraints from the given rare footprints, and then use Aho’s BUILD algorithm to choose between regularized or hard version of the HC problem.",1 Introduction,[0],[0]
"The above illustrates how to use our workflow and why using our algorithms facilitates HC when expert domain knowledge is available.
",1 Introduction,[0],[0]
Further related work.,1 Introduction,[0],[0]
"Similar to Vikram and Dasgupta [2016], constraints in the form of triplet queries have been used in an (adaptive) active learning framework by Tamuz et al. [2011], Emamjomeh-Zadeh and Kempe [2018], showing that approximately O(n log n) triplet queries are enough to learn an underlying HC.",1 Introduction,[0],[0]
"Other forms of user interaction in order to improve the quality of the produced clusterings have been used in Balcan and Blum [2008], Awasthi et al. [2014] where they prove that interactive feedback in the form of cluster split/merge requests can lead to significant improvements.",1 Introduction,[0],[0]
Robust algorithms for HC in the presence of noise were studied in Balcan et al. [2014] and a variety of sufficient conditions on the similarity function that would allow linkage-style methods to produce good clusters was explored in Balcan et al. [2008].,1 Introduction,[0],[0]
"On a different setting, the notion of triplets has been used as a measure of distance between hierarchical decomposition trees
on the same data points Brodal et al. [2013].",1 Introduction,[0],[0]
More technically distant analogs of how to use relations among triplets points have recently been proposed in Kleindessner and von Luxburg,1 Introduction,[0],[0]
[2017] for defining kernel functions corresponding to high-dimensional embeddings.,1 Introduction,[0],[0]
"Given an instance of the constrained hierarchical clustering, our proposed CRSC algorithm uses a blackbox αn-approximation algorithm for the sparsest cut problem (the best-known approximation factor for this problem is O( √ log n) due to Arora et al. [2009]).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Moreover, it also maintains the feasibility of the solution in a top-down approach by recursive partitioning of what we call the supergraph G′. Informally speaking, the supergraph is a simple data structure to track the progress of the algorithm and the resolved constraints.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"More formally, for every constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"we merge the nodes a and b into a supernode {a, b} while maintaining the edges in G (now connecting to their corresponding supernodes).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Note that G′ may have parallel edges, but this can easily be handled by grouping edges together and replacing them with the sum of their weights.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We repeatedly continue this merging procedure until there are no more constraints.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Observe that any feasible solution needs to start splitting the original graph G by using a cut that is also present in G′. When cutting the graph G′ = (G1, G2), if a constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"is resolved,5 then we can safely unpack the supernode {a, b} into two nodes again (unless there is another constraint ab|c′ in which case we should keep the supernode).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"By continuing and recursively finding approximate sparsest cuts on the supergraph G1 and G2, we can find a feasible hierarchical decomposition of G respecting all triplet constraints.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Next, we show the approximation guarantees for our algorithm.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Algorithm 1 CRSC 1:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given G and the triplet constraints ab|c, run BUILD to create the supergraph G′. 2: Use a blackbox access to an αn-approximation oracle for the sparsest cut problem, i.e.
arg minS⊆V wG′ (S,S̄) |S|·|S̄| .
3:",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given the output cut (S, S̄), separate the graph G′ into two pieces G1(S,E1) and G2(V \S,E2).
4: Recursively compute a HC T1 for G1 using only G1’s active constraints.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Similarly compute T2 for G2. 5: Output T = (T1, T2).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Analysis of CRSC Algorithm.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The main result of this section is the following theorem:
Theorem 1.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"for a, b, c ∈ V , the CRSC algorithm outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for the HC-similarity objective as in (1).
Notations and Definitions.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We slightly abuse notation by having OPT denote the optimum hierarchical decomposition or its optimum value as measured by (1).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Similarly for CRSC.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
For t ∈,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[n], OPT(t) denotes the maximal clusters in OPT of size at most t. Note that OPT(t) induces a partitioning of V .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We use OPT(t) to denote edges cut by OPT(t) (i.e. edges with endpoints in different clusters in OPT(t)) or their total weight; the meaning will be clear from context.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For convenience, we define
5A constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"is resolved, if c gets separated from a, b.
OPT(0) =",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"∑
(i,j)∈E wij .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For a cluster A created by CRSC, a constraint ab|c is active if a, b, c ∈",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"A, otherwise ab|c is resolved and can be discarded.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Overview of the Analysis.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
There are three main ingredients:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The first is to view a HC of n datapoints as a collection of partitions, one for each level t = n",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"− 1, . . .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
", 1, as in [Charikar and Chatziafratis, 2017].",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For a level t, the partition consists of maximal clusters of size at most t. The total cost incurred by OPT is then a combination of costs incurred at each level of this partition.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
This is useful for comparing our CRSC cost with OPT.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
The second idea is in handling constraints and it is the main obstacle where previous analyses Charikar and Chatziafratis,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[2017], Cohen-Addad et al. [2018] break down: constraints inevitably limit the possible cuts that are feasible at any level, and since the set of active constraints6 differ for CRSC and OPT, a direct comparison between them is impossible.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"If we have no constraints, we can charge the cost of partitioning a cluster A to lower levels of the OPT decomposition.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"However, when we have triplet constraints, the partition induced by the lower levels of OPT in a cluster A will not be feasible in general (Figure 2).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The natural way to overcome this obstacle is merging pieces of this partition so as to respect constraints and using higher levels of OPT, but it still may be impossible to compare CRSC with OPT if all pieces are merged.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We overcome this difficulty by an indirect comparison between the CRSC cost and lower levels r6kA of OPT, where kA is the number of active constraints in A. Finally, after a cluster-by-cluster analysis bounding the CRSC cost for each cluster, we exploit disjointness of clusters of the same level in the CRSC partition allowing us to combine their costs.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Proof of Theorem 1.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We start by borrowing the following facts from [Charikar and Chatziafratis, 2017], modified slightly for the purpose of our analysis (proofs are provided in the supplementary materials).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Fact 1 (Decomposition of OPT).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The total cost paid by OPT can be decomposed into costs of the different levels in the OPT partition, i.e. OPT = ∑n t=0w(OPT(t)).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Fact 2 (OPT at scaled levels).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let k ≤,2 Constrained Sparsest (Balanced) Cut,[0],[0]
n6 be the number of constraints.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Then, OPT ≥ 1 6k ·∑n
t=0w(OPT(b t 6kc)).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"6All constraints are active in the beginning of CRSC.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given the above facts, we look at any cluster A of size r produced by the algorithm.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Here is the main technical lemma that allows us to bound the cost of CRSC for partitioning A.
Lemma 1.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Suppose CRSC partitions a cluster A (|A| = r) in two clusters (B1, B2) (w.l.o.g. |B1| = s, |B2| = r− s, s ≤ b r2c ≤ r− s).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let the size r ≥ 6k,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"and let l = 6kA, where kA denotes the number of active constraints for A. Then: r · w(B1, B2) ≤ 4αn · s · w(OPT(b rl c) ∩A).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Proof.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The cost incurred by CRSC for partitioning A is r · w(B1, B2).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Now consider OPT(b rl c).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"This induces a partitioning of A into pieces {Ai}i∈[m], where by design |Ai| = γi|A|, γi ≤ 1l , ∀i ∈",2 Constrained Sparsest (Balanced) Cut,[0],[0]
[m].,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Now, consider the cuts {(Ai, A \ Ai)}.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Even though all m cuts are allowed for OPT, for CRSC some of them are forbidden: for example, in Figure 2, the constraints ab|c, de|f render 4 out of the 6 cuts infeasible.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
But how many of them can become infeasible with kA active constraints?,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Since every constraint is involved in at most 2 cuts, we may have at most 2kA infeasible cuts.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let F ⊆,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[m] denote the index set of feasible cuts, i.e. if i ∈ F , the cut (Ai, A \Ai) is feasible for CRSC.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"To cut A, we use an αn-approximation of sparsest cut, whose sparsity is upper bounded by any feasible cut:
w(B1, B2)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ≤ αn · SP.CUT(A),2 Constrained Sparsest (Balanced) Cut,[0],[0]
≤ αn,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"min i∈F w(Ai, A \Ai) |Ai||A",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"\Ai|
≤ αn ∑
i∈F w(Ai, A \Ai)∑ i∈F |Ai||A \Ai|
where for the last inequality we used the standard fact that mini µiνi ≤ ∑ i µi∑",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"i νi
for µi ≥ 0",2 Constrained Sparsest (Balanced) Cut,[0],[0]
and νi > 0.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We also have the following series of inequalities:
αn ∑ i∈F w(Ai, A \Ai)∑ i∈F |Ai||A \Ai| ≤ αn 2w(OPT(b rl c) ∩A) r2 ∑ i∈F γi(1− γi) ≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
4αn w(OPT(b rl c) ∩A),2 Constrained Sparsest (Balanced) Cut,[0],[0]
"r2
where the first inequality holds because we double count some (potentially all) edges of OPT(b rl c)∩A (these are the edges cut by OPT(b rl c) that are also present in cluster A, i.e. they have both endpoints in A) and the second inequality holds because γi ≤ 16k",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"=⇒ 1− γi ≥
6k−1 6k and∑",2 Constrained Sparsest (Balanced) Cut,[0],[0]
i∈F γi(1− γi) ≥ m∑ i=1,2 Constrained Sparsest (Balanced) Cut,[0],[0]
γi(1−,2 Constrained Sparsest (Balanced) Cut,[0],[0]
γi)− 2 ∑ i∈[m]\F 1 6k ≥ 6k,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"− 1 6k m∑ i=1 γi − 2k 6k = 4k − 1 6k ≥ 1/2
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Finally, we are ready to prove the lemma by combining the above inequalities ( r−sr ≤ 1):
r · w(B1, B2) = r ·",2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"w(B1, B2)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"s(r − s)
≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
r ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"4αn w(OPT(b rl c) ∩A)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"r2 ≤ 4αn · s · w(OPT(b rl c) ∩A).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"It is clear that we exploited the charging to lower levels of OPT, since otherwise if all pieces in A were merged, the denominator with the |Ai|’s would become 0.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The next lemma lets us combine the costs incurred by CRSC for different clusters A (proof is in the supplementary materials)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Lemma 2 (Combining the costs of clusters in CRSC).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The total CRSC cost for partitioning all clusters A into (B1, B2) (with |A| = rA, |B1| = sA) is bounded by:
(1) ∑
A:|A|≥6k rA · w(B1, B2) ≤ O(αn) ·",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"n∑ t=0 w(OPT(b t6kc))
(2) ∑
A:|A|<6k
rAw(B1, B2) ≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"6k · OPT
Combining Fact 2 and Lemma 2 finishes the proof.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Remark 1.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"In the supplementary material, we prove how one can use balanced cut, i.e. finding a cut S such that
arg min S⊆V :|S|≥n/3,|S̄|≥n/3 wG′(S, S̄) (3)
instead of sparsest cut, and using approximation algorithms for this problem achieves the same approximation factor as in Theorem 1, but with better running time.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Remark 2.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Optimality of the CRSC algorithm:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Note that complexity theoretic lower-bounds for the unconstrained version of HC from Charikar and Chatziafratis [2017] also apply to our setting; more specifically, they show that no constant factor approximation exists for HC assuming the Small-Set Expansion Hypothesis.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Theorem 2 (The divisive algorithm using balanced cut).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"for a, b, c ∈ V , the constrained recursive balanced cut algorithm CRBC (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for Dasgupta’s HC objective.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Moreover, the running time is almost linear time.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Previously, we assumed that constraints were feasible.",3 Constraints and Regularization,[0],[0]
"However, in many practical applications, users/experts may disagree, hence our algorithm may receive conflicting constraints as input.",3 Constraints and Regularization,[0],[0]
Here we want to explore how to still output a satisfying HC that is a good in terms of objective (1) (similarity-HC) and also respects the constraints as much as possible.,3 Constraints and Regularization,[0],[0]
"To this end, we propose a regularized version of Dasgupta’s objective, where the regularizer measures quantitatively the degree by which constraints get violated.
",3 Constraints and Regularization,[0],[0]
"Informally, the idea is to penalize a constraint more if it is violated at top levels of the decomposition compared to lower levels.",3 Constraints and Regularization,[0],[0]
We also allow having different violation weights for different constraints (potentially depending on the expertise of the users providing the constraints).,3 Constraints and Regularization,[0],[0]
"More concretely, inspired by the Dasgupta’s original objective function, we consider the following optimization problem:
min T∈T ( ∑ (i,j)∈E wij |Tij",3 Constraints and Regularization,[0],[0]
|+,3 Constraints and Regularization,[0],[0]
"λ · ∑ ab|c∈K cab|c|Tab| · 1{ab|c is violated} ) , (4)
where T is the set of all possible binary HC trees for the given data points, K is the set of the k triplet constraints, Tab is the size of the subtree rooted at the least common ancestor of a, b, and cab|c is defined as the base cost of violating triplet constraint ab|c.",3 Constraints and Regularization,[0],[0]
"Note that the regularization parameter λ ≥ 0 allows us to interpolate between satisfying the constraints or respecting the geometry of the data.
",3 Constraints and Regularization,[0],[0]
"Hypergraph Recursive Sparsest Cut In order to design approximation algorithms for the regularized objective, we draw an interesting connection to a different problem, which we call 3- Hypergraph Hierarchical Clustering (3HHC).",3 Constraints and Regularization,[0],[0]
"An instance of this problem consists of a hypergraph GH = (V,E,EH) with edges E, and hyperedges of size 3, EH, together with similarity weights for
edges, {wij}(i,j)∈E , and similarity weights for 3-hyperedges,7 {wij|k}(i,j,k)∈EH .",3 Constraints and Regularization,[0],[0]
"We now think of HC on the hypergraph GH, where for every binary tree T we define the cost to be the natural extension of Dasgupta’s objective: ∑
(i,j)∈E
wij |Tij",3 Constraints and Regularization,[0],[0]
|+,3 Constraints and Regularization,[0],[0]
"∑
(i,j,k)∈EH wTijk|Tijk| (5)
where wTijk is either equal to wij|k, wjk|i or wki|j , and Tijk is either the subtree rooted at LCA(i, j), 8 LCA(i, k) or LCA(k, j), all depending on how T cuts the 3-hyperedge {i, j, k}.",3 Constraints and Regularization,[0],[0]
"The goal is to find a hierarchical clustering of this hypergraph, so as to minimize the cost (5) of the tree.
",3 Constraints and Regularization,[0],[0]
Reduction from Regularization to 3HHC.,3 Constraints and Regularization,[0],[0]
"Given an instance of HC with constraints (with their costs of violations) and a parameter λ, we create a hypergraph GH so that the total cost of any binary clustering tree in the 3HHC problem (5) corresponds to the regularized objective of the same tree as in (4).",3 Constraints and Regularization,[0],[0]
"GH has exactly the same set of vertices, (normal) edges and (normal) edge weights as in the original instance of the HC problem.",3 Constraints and Regularization,[0],[0]
"Moreover, for every constraint ab|c",3 Constraints and Regularization,[0],[0]
"(with cost cab|c) it has a hyperedge {a, b, c}, to which we assign three weights wab|c = 0, wac|b = wbc|a = λ ·cab|c.",3 Constraints and Regularization,[0],[0]
"Therefore, we ensure that any divisive algorithm for the 3HHC problem avoids the cost |Tabc|·λ·cab|c only if it chops {a, b, c} into {a, b} and {c} at some level, which matches the regularized objective.
",3 Constraints and Regularization,[0],[0]
Reduction from 3HHC to Hypergraph Sparsest Cut.,3 Constraints and Regularization,[0],[0]
"A natural generalization of the sparsest cut problem for our hypergraphs, which we call Hyper Sparsest Cut (HSC), is the following problem:
arg min S⊆V
( w(S, S̄)",3 Constraints and Regularization,[0],[0]
"+ ∑ (i,j,k)∈EH w S ijk
|S||S̄|
) ,
where w(S, S̄) is the weight of the cut (S, S̄) and wSijk is either equal to wij|k, wjk|i or wki|j , depending on how (S, S̄) chops the hyperedge {i, j, k}.",3 Constraints and Regularization,[0],[0]
"Now, similar to Charikar and Chatziafratis [2017], Dasgupta [2016], we can recursively run a blackbox approximation algorithm for HSC to solve 3HHC.",3 Constraints and Regularization,[0],[0]
"The main result of this section is the following technical proposition, whose proof is analogous to that of Theorem 1 (provided in the supplementary materials).
",3 Constraints and Regularization,[0],[0]
Proposition 2.,3 Constraints and Regularization,[0],[0]
"Given the hypergraph GH with k hyperedges, and given access to an algorithm which is αn-approximation for HSC, the Recursive Hypergraph Sparsest Cut (R-HSC) algorithm achieves an O(kαn)-approximation.
Reduction from HSC back to Sparsest Cut.",3 Constraints and Regularization,[0],[0]
We now show how to get an αn-approximation oracle for our instance of the HSC problem by a general reduction to sparsest cut.,3 Constraints and Regularization,[0],[0]
"Our reduction is simple: given a hypergraph GH and all the weights, create an instance of sparsest cut with the same vertices, (normal) edges and (normal) edge weights.",3 Constraints and Regularization,[0],[0]
"Moreover, for every 3-hyperedge {a, b, c}
7We have 3 different weights corresponding to the 3 possible ways of partitioning {i, j, k} in two parts: wij|k, wjk|i and wki|j .
",3 Constraints and Regularization,[0],[0]
"8LCA(i, j) denotes the lowest common ancestor of i, j ∈ T .
consider adding a triangle to the graph, i.e. three weighted edges connecting {a, b, c}, where:
w′ab = wbc|a + wac|b",3 Constraints and Regularization,[0],[0]
"− wab|c
2 = λ · cab|c,
w′ac = wbc|a + wab|c",3 Constraints and Regularization,[0],[0]
"− wac|b
2 = 0,
w′bc = wac|b +",3 Constraints and Regularization,[0],[0]
wab|c,3 Constraints and Regularization,[0],[0]
"− wbc|a
2 = 0.
",3 Constraints and Regularization,[0],[0]
This construction can be seen in Figure 3.,3 Constraints and Regularization,[0],[0]
The important observation is that w′ab+w ′,3 Constraints and Regularization,[0],[0]
"ac = wbc|a, w′ab+ w′bc = wac|b and w ′",3 Constraints and Regularization,[0],[0]
bc +w ′,3 Constraints and Regularization,[0],[0]
"ac = wab|c, which are exactly the weights associated with the corresponding splits of the 3-hyperedge {a, b, c}.",3 Constraints and Regularization,[0],[0]
"So, correctness of the reduction9 follows as the weight of each cut is preserved between the hypergraph and the graph after adding the triangles.",3 Constraints and Regularization,[0],[0]
"For a discussion on extending this gadget more generally, see the supplement.
",3 Constraints and Regularization,[0],[0]
Remark 3.,3 Constraints and Regularization,[0],[0]
Reduction to hypergraphs: we would like to emphasize the necessity of the hypergraph version in order for the reduction to work.,3 Constraints and Regularization,[0],[0]
"One might think that just adding extra heavy edges would be sufficient, but there is a technical difficulty with this approach.",3 Constraints and Regularization,[0],[0]
"Consider a triplet constraint ab|c; once c is separated from a and b at some level, there is no extra tendency anymore to keep a and b together (i.e. only the similarity weight should play role after this point).",3 Constraints and Regularization,[0],[0]
This behavior cannot be captured by only adding heavy-weight edges.,3 Constraints and Regularization,[0],[0]
"Instead, one needs to add a heavy edge between a and b that disappears once c is separated, and this is exactly why we need the hyperedge gadget.",3 Constraints and Regularization,[0],[0]
"One can replace the reduction for a one-shot proof, but we believe it will be less modular and less transparent.",3 Constraints and Regularization,[0],[0]
"In this section we study dissimilarity-HC, and we look into the problem of designing approximation algorithms for both unconstrained and constrained hierarchical clustering.",4 Variations on a Theme,[0],[0]
"In Cohen-Addad et al. [2017], they show that average linkage is a 12 -approximation for this problem and they propose a top-
down approach based on locally densest cut achieving a (23− )-approximation",4 Variations on a Theme,[0],[0]
in time Õ ( n2(n+m) ),4 Variations on a Theme,[0],[0]
.,4 Variations on a Theme,[0],[0]
"Notably, when gets small the running time blows up.",4 Variations on a Theme,[0],[0]
"Here, we prove that the most natural randomized algorithm for this problem, i.e. recursive random cutting, is a 23 -approximation with expected running time O(n log n).",4 Variations on a Theme,[0],[0]
"We further derandomize this algorithm to get a simple deterministic local-search style 23 -approximation algorithm.
",4 Variations on a Theme,[0],[0]
"If we also have structural constraints for the dissimilarity-HC, we show that the existing approaches fail.",4 Variations on a Theme,[0],[0]
In fact we show that they lead to an Ω(n)-approximation factor due to the lack of “exploration” (e.g. recursive densest cut).,4 Variations on a Theme,[0],[0]
"We then show that recursive random cutting is robust to adding user constraints, and indeed it preserves a constant approximation factor when there are, roughly speaking, constantly many user constraints.
",4 Variations on a Theme,[0],[0]
"9Since all weights in the final graph are non-negative, standard techniques for Sparsest Cut can be used.
",4 Variations on a Theme,[0],[0]
Randomized 23-approximation.,4 Variations on a Theme,[0],[0]
"Consider the most natural randomized algorithm for hierarchical clustering, i.e. recursively partition each cluster into two, where each point in the current cluster independently flips an unbiased coin and based on the outcome, it is put in one of the two parts.
",4 Variations on a Theme,[0],[0]
Theorem 3.,4 Variations on a Theme,[0],[0]
"Recursive-Random-Cutting is a 23 -approximation for maximizing dissimilarity-HC objective.
",4 Variations on a Theme,[0],[0]
Proof sketch.,4 Variations on a Theme,[0],[0]
"An alternative view of Dasgupta’s objective is to divide the reward of the clustering tree between all possible triples {i, j, k}, where (i, j) ∈ E and k is another point (possibly equal to i or j).",4 Variations on a Theme,[0],[0]
"Now, in any hierarchical clustering tree, if at the moment right before i and j become separated the vertex k has still been in the same cluster as {i, j}, then this triple contributes wij to the objective function.",4 Variations on a Theme,[0],[0]
We claim this event happens with probability exactly 23 .,4 Variations on a Theme,[0],[0]
"To see this, consider an infinite independent sequence of coin flips for i, j, and k.",4 Variations on a Theme,[0],[0]
"Without loss of generality, condition on i’s sequence to be all heads.",4 Variations on a Theme,[0],[0]
The aforementioned event happens only if j’s first tales in its sequence happens no later than k’s first tales in its sequence.,4 Variations on a Theme,[0],[0]
"This happens with probability∑
i≥1 1 2( 1 4) i−1 = 23 .",4 Variations on a Theme,[0],[0]
"Therefore, the algorithm gets the total reward 2n 3 ∑ (i,j)∈E wij in expectation.
",4 Variations on a Theme,[0],[0]
"Moreover, the total reward of any hierarchical clustering is upper-bounded by n ∑
(i,j)∈E wij , which completes the proof of the 23 -approximation.
",4 Variations on a Theme,[0],[0]
Remark 4.,4 Variations on a Theme,[0],[0]
"This algorithm runs in time O(n log n) in expectation, due to the fact that the binary clustering tree has expected depth O(log n) (see for example Cormen et al. [2009]) and at each level we only perform n operations.
",4 Variations on a Theme,[0],[0]
We now derandomize the recursive random cutting algorithm using the method of conditional expectations.,4 Variations on a Theme,[0],[0]
"At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the “left” partition or “right” partition for the next recursion.",4 Variations on a Theme,[0],[0]
"Once we make a decision for a point, we fix that point and go to the next one.",4 Variations on a Theme,[0],[0]
"Roughly speaking, these local improvements can be done in polynomial time, which will result in a simple local-search style deterministic algorithm.
",4 Variations on a Theme,[0],[0]
Theorem 4.,4 Variations on a Theme,[0],[0]
"There is a deterministic local-search style 23 -approximation algorithm for maximizing dissimilarity-HC objective that runs in time O(n2(n+m)).
",4 Variations on a Theme,[0],[0]
"Maximizing the Objective with User Constraints From a practical point of view, one can think of many settings in which the output of the hierarchical clustering algorithm should satisfy user-defined hard constraints.",4 Variations on a Theme,[0],[0]
"Now, combining the new perspective of maximizing Dasgupta’s objective with this practical consideration raises a natural question: which algorithms are robust to adding user constraints, in the sense that a simple variation of these algorithms still achieve a decent approximation factor?
",4 Variations on a Theme,[0],[0]
• Failure of “Non-exploring” Approaches.,4 Variations on a Theme,[0],[0]
"Surprisingly enough, there are convincing reasons that adapting existing algorithms for maximizing Dasgupta’s objective (e.g. those proposed in Cohen-Addad et al. [2018]) to handle user constraints is either challenging or hopeless.",4 Variations on a Theme,[0],[0]
"First, bottom-up algorithms, e.g. average-linkage, fail to output a feasible outcome if they only consider each constraint separately and not all the constraints jointly (as we saw in Figure 1).",4 Variations on a Theme,[0],[0]
"Second, maybe more surprisingly, the natural extension of (locally) Recursive-Densest-Cut10 algorithm proposed in Cohen-Addad et al. [2018] to handle user constraints performs poorly in the worst-case, even
10While a locally densest cut can be found in poly-time, desnest cut is NP-hard, making our negative result stronger.
",4 Variations on a Theme,[0],[0]
when we have only one constraint.,4 Variations on a Theme,[0],[0]
"Recursive-Densest-Cut proceeds by repeatedly picking the cut that has maximum density, i.e. arg maxS⊆V w(S,S̄) |S|·|S̄| and making two clusters.",4 Variations on a Theme,[0],[0]
"To handle the user constraints, we run it recursively on the supergraph generated by the constraints, similar to the approach in Section 2.",4 Variations on a Theme,[0],[0]
"Note that once the algorithm resolves a triplet constraint, it also breaks its corresponding supernode.
",4 Variations on a Theme,[0],[0]
"Now consider the following example in Figure 4, in which there is just one triplet constraint ab|c.",4 Variations on a Theme,[0],[0]
The weight W should be thought of as large and as small.,4 Variations on a Theme,[0],[0]
"By choosing appropriate weights on the edges of the clique Kn, we can fool the algorithm into cutting the dense parts in the clique, without ever resolving the ab|c constraint until it is too late.",4 Variations on a Theme,[0],[0]
"The algorithm gets a gain of O(n3+W ) whereas OPT gets Ω(nW ) by starting with the removal of the edge (b, c) and then removing (a, b), thus enjoying a gain of ≈ nW .
",4 Variations on a Theme,[0],[0]
• Constrained Recursive Random Cutting.,4 Variations on a Theme,[0],[0]
"The example in Figure 4, although a bit pathological, suggests that a meaningful algorithm for this problem should explore cutting low-weight edges that might lead to resolving constraints, maybe randomly, with the hope of unlocking rewarding edges that were hidden before this exploration.
",4 Variations on a Theme,[0],[0]
"Formally, our approach is showing that the natural extension of recursive random cutting for the constrained problem, i.e. by running it on the supergraph generated by constraints and unpacking supernodes as we resolve the constraints (in a similar fashion to CSC), achieves a constant factor approximation when the constraints have bounded dependency.",4 Variations on a Theme,[0],[0]
"In the remaining of this section, we define an appropriate notion of dependency between the constraints, under the name of dependency measure and analyze the approximation factor of constrained recursive random cutting (Constrained-RRC ) based on this notion.
",4 Variations on a Theme,[0],[0]
"Suppose we are given an instance of hierarchical clustering with triplet constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}, where ci = xi|yizi,∀i ∈",4 Variations on a Theme,[0],[0]
[k].,4 Variations on a Theme,[0],[0]
"For any triplet constraint ci, lets call the pair {yi, zi} the base, and zi the key of the constraint.",4 Variations on a Theme,[0],[0]
"We first partition our constraints into equivalence classes C1, . . .",4 Variations on a Theme,[0],[0]
", CN , where Ci ⊆ {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}.",4 Variations on a Theme,[0],[0]
"For every i, j, the constraints ci and cj belong to the same class C if they share the same base (see Figure 5).
",4 Variations on a Theme,[0],[0]
Definition 1 (Dependency digraph).,4 Variations on a Theme,[0],[0]
"The Dependency digraph is a directed graph with vertex set {C1, . . .",4 Variations on a Theme,[0],[0]
", CL}.",4 Variations on a Theme,[0],[0]
"For every i, j, there is a directed edge Ci → Cj if ∃ c = x|yz, c′ = x′|y′z′, such that c ∈ Ci, c′ ∈ Cj , and either {x, z} = {y′, z′} or {x, y} = {y′, z′} (see Figure 6).
",4 Variations on a Theme,[0],[0]
The dependency digraph captures how groups of constraints impact each other.,4 Variations on a Theme,[0],[0]
"Formally, the existence of the edge Ci → Cj implies that all the constraints in Cj should be resolved before one can separate the two endpoints of the (common) base edge of the constraints in Ci.",4 Variations on a Theme,[0],[0]
Remark 5.,4 Variations on a Theme,[0],[0]
"If the constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck} are feasible, i.e. there exists a hierarchical clustering that can respect all the constraints, the dependency digraph is clearly acyclic.
",4 Variations on a Theme,[0],[0]
Definition 2 (Layered dependency subgraph).,4 Variations on a Theme,[0],[0]
"Given any class C, the layered dependency subgraph of C is the induced subgraph in the dependency digraph by all the classes that are reachable from C. Moreover, the vertex set of this subgraph can be partitioned into layers {I0, I1, . . .",4 Variations on a Theme,[0],[0]
", IL}, where L is the maximum length of any directed path leaving C and Il is a subset of classes where the length of the longest path from C to each of them is exactly equal to l (see Figure 7).
",4 Variations on a Theme,[0],[0]
We are now ready to define a crisp quantity for every dependency graph.,4 Variations on a Theme,[0],[0]
"This will later help us give a more meaningful and refined beyond-worst-case guarantee for the approximation factor of the Constrained-RRC algorithm.
",4 Variations on a Theme,[0],[0]
Definition 3 (Dependency measure).,4 Variations on a Theme,[0],[0]
"Given any class C, the dependency measure of C is defined as
DM(C) , L∏ l=0 (1 + ∑ C′∈Il |C′|),
where I0, . . .",4 Variations on a Theme,[0],[0]
", IL are the layers of the dependency subgraph of C, as in Definition 2.",4 Variations on a Theme,[0],[0]
"Moreover, the dependency measure of a set of constraints DMC({c1, . . .",4 Variations on a Theme,[0],[0]
", ck}) is defined as maxC DM(C), where the maximum is taken over all the classes generated by {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}.
Intuitively speaking, the notion of the dependency measure quantitatively expresses how “deeply” the base of a constraint is protected by the other constraints, i.e. how many constraints need to be resolved first before the base of a particular constraint is unpacked and the Constrained-RRC algorithm can enjoy its weight.",4 Variations on a Theme,[0],[0]
"This intuition is formalized through the following theorem, whose proof is deferred to the supplementary materials.
",4 Variations on a Theme,[0],[0]
Theorem 5.,4 Variations on a Theme,[0],[0]
"The constrained recursive random cutting (Constrained-RRC ) algorithm is an αapproximation algorithm for maximizing dissimilarity-HC objective objective given a set of feasible constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}, where
α = 2(1− k/n) 3 ·DMC({c1, . . .",4 Variations on a Theme,[0],[0]
", ck}) ≤ 2(1−",4 Variations on a Theme,[0],[0]
"k/n) 3 ·maxC DM(C)
Corollary 1.",4 Variations on a Theme,[0],[0]
"Constrained-RRC is an O(1)-approximation for maximizing dissimilarity-HC objective, given feasible constraints of constant dependency measure.",4 Variations on a Theme,[0],[0]
We studied the problem of hierarchical clustering when we have structural constraints on the feasible hierarchies.,5 Conclusion,[0],[0]
"We followed the optimization viewpoint that was recently developed in Dasgupta [2016], Cohen-Addad et al. [2018] and we analyzed two natural top-down algorithms giving provable approximation guarantees.",5 Conclusion,[0],[0]
"In the case where the constraints are infeasible, we proposed and analyzed a regularized version of the HC objective by using the hypergraph version of the sparsest cut problem.",5 Conclusion,[0],[0]
"Finally, we also explored a variation of Dasgupta’s objective and improved upon previous techniques, both in the unconstrained and in the constrained setting.",5 Conclusion,[0],[0]
Vaggos Chatziafratis was partially supported by ONR grant N00014-17-1-2562.,Acknowledgements,[0],[0]
Rad Niazadeh was supported by Stanford Motwani fellowship.,Acknowledgements,[0],[0]
Moses Charikar was supported by NSF grant CCF1617577 and a Simons Investigator Award.,Acknowledgements,[0],[0]
"We would also like to thank Leo Keselman, Aditi Raghunathan and Yang Yuan for providing comments on an earlier draft of the paper.",Acknowledgements,[0],[0]
We also thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgements,[0],[0]
"A.1 Missing proofs and discussion in Section 2
Proof of Proposition 1.",A Supplementary Materials,[0],[0]
"For nodes u, v ∈ T , let P (u) denote the parent of u in the tree and LCA(u, v) denote the lowest common ancestor of u, v.",A Supplementary Materials,[0],[0]
"For a leaf node li, i ∈",A Supplementary Materials,[0],[0]
"[k], we say that its label is li, whereas for an internal node of T , we say that its label is the label of any of its two children.",A Supplementary Materials,[0],[0]
"As long as there are any two nodes a, b that are siblings (i.e. P (a) ≡ P (b)), we create a constraint ab|c",A Supplementary Materials,[0],[0]
where c is the label of the second child of P (P (a)).,A Supplementary Materials,[0],[0]
"We delete leaves a, b from the tree and repeat until there are fewer than 3 leaves left.",A Supplementary Materials,[0],[0]
"To see why the above procedure will only create at most k constraints, notice that every time a new constraint is created, we delete two nodes of the given tree T .",A Supplementary Materials,[0],[0]
"Since T has k leaves and is binary, it can have at most 2k − 1 nodes in total.",A Supplementary Materials,[0],[0]
It follows that we create at most 2k−12 < k triplet constraints.,A Supplementary Materials,[0],[0]
"For the equivalence between the constraints imposed by T and the created triplet constraints, observe that all triplet constraints we create are explicitly imposed by the given tree (since we only create constraints for two leaves that are siblings) and that for any three datapoints a, b, c ∈ T with LCA(a, c)=LCA(b, c), our set of triplet constraints will indeed imply ab|c, because LCA(a, b) appears further down the tree than LCA(a, c) and hence a, b become siblings before a, c or b, c.
Proof of Fact 1 from Charikar and Chatziafratis [2017].",A Supplementary Materials,[0],[0]
"We will measure the contribution of an edge e = (u, v) ∈ E to the RHS and to the LHS.",A Supplementary Materials,[0],[0]
"Suppose that r denotes the size of the minimal cluster in OPT that contains both u and v. Then the contribution of the edge e = (u, v) to the LHS is by definition r · we.",A Supplementary Materials,[0],[0]
"On the other hand, (u, v) ∈",A Supplementary Materials,[0],[0]
"OPT(t),∀t ∈ {0, ..., r",A Supplementary Materials,[0],[0]
− 1}.,A Supplementary Materials,[0],[0]
"Hence the contribution to the RHS is also r · we.
",A Supplementary Materials,[0],[0]
Proof of Fact 2 from Charikar and Chatziafratis [2017].,A Supplementary Materials,[0],[0]
"We rewrite OPT using the fact that
w(OPT(t))",A Supplementary Materials,[0],[0]
"≥ 0
at every level t ∈",A Supplementary Materials,[0],[0]
"[n]:
6k · OPT = 6k n∑ t=0 w(OPT(t))
",A Supplementary Materials,[0],[0]
= 6k(w(OPT(0)),A Supplementary Materials,[0],[0]
+ · · ·+ w(OPT(n))),A Supplementary Materials,[0],[0]
≥ 6k(w(OPT(0)),A Supplementary Materials,[0],[0]
"+ · · ·+ w(OPT(b n6kc)))
",A Supplementary Materials,[0],[0]
"= n∑ t=0 w(OPT(b t6kc))
",A Supplementary Materials,[0],[0]
Proof of Lemma 2.,A Supplementary Materials,[0],[0]
"By using the previous lemma we have: CRSC = ∑ A rAw(B1, B2) ≤≤ O(αn) ∑ A sAw(OPT(b rA6kA c) ∩A)
",A Supplementary Materials,[0],[0]
"Observe that w(OPT(t)) is a decreasing function of t, since as t decreases, more and more edges are getting cut.",A Supplementary Materials,[0],[0]
"Hence we can write:
∑ A sA · w(OPT(b rA6k c) ∩A) ≤ ∑",A Supplementary Materials,[0],[0]
"A rA∑ t=rA−sA+1 w(OPT(b rA6kA c) ∩A)
",A Supplementary Materials,[0],[0]
"To conclude with the proof of the first part all that remains to be shown is that:
∑ A rA∑ t=rA−sA+1 w(OPT(b t6kA c) ∩A) ≤ n∑ t=0 w(OPT(b t6kc))
",A Supplementary Materials,[0],[0]
To see why this is true consider the clusters A with a contribution to the LHS.,A Supplementary Materials,[0],[0]
We have that rA−sA+1 ≤,A Supplementary Materials,[0],[0]
"t ≤ rA, hence |B2| < tmeaning thatA is aminimal cluster of size",A Supplementary Materials,[0],[0]
|A| ≥ t > |B2|,A Supplementary Materials,[0],[0]
"≥ |B1|, i.e. if both A’s children are of size less than t, then this cluster A contributes such a term.",A Supplementary Materials,[0],[0]
"The set of all such A form a disjoint partition of V because of the definition for minimality (in order for them to overlap in the hierarchical clustering, one of them needs to be ancestor of the other and this cannot happen because of minimality).",A Supplementary Materials,[0],[0]
"Since OPT(b t6kc)∩A for all such A forms a disjoint partition of OPT(b t6kc), the claim follows by summing up over all t.
Note that so far our analysis handles clusters A with size rA ≥ 6k.",A Supplementary Materials,[0],[0]
"However, for clusters with smaller size rA < 6k we can get away by using a crude bound for bounding the total cost and still not affecting the approximation guarantee that will be dominated by O(kαn):∑
|A|<6k rAw(B1, B2) < 6k · ∑ ij∈E wij = 6k · OPT(1) ≤ 6k · OPT
Theorem 6 (The divisive algorithm using balanced cut).",A Supplementary Materials,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",A Supplementary Materials,[0],[0]
"for a, b, c ∈ V , the constrained recursive balanced cut algorithm (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for the HC objective (1).
",A Supplementary Materials,[0],[0]
Proof.,A Supplementary Materials,[0],[0]
"It is not hard to show that one can use access to balanced cut rather than sparsest cut and achieve the same approximation factor by the recursive balanced cut algorithm.
",A Supplementary Materials,[0],[0]
We will follow the same notation as in the sparsest cut analysis and we will use some of the facts and inequalities we previously proved about OPT(t).,A Supplementary Materials,[0],[0]
"Again, for a cluster A of size r, the important observation is that the partition A1, . . .",A Supplementary Materials,[0],[0]
", Al (at the end, we will again choose l = 6kA) induced inside the cluster A by OPT( rl ) can be separated into two groups, let’s say (C1, C2) such that r/3 ≤ |C1|, |C2| ≤ 2r/3.",A Supplementary Materials,[0],[0]
"In other words we can demonstrate a Balanced Cut with ratio 13 : 2 3 for the cluster A. Since we cut fewer edges when creating C1, C2 compared to the partitioning of OPT( rl ):
w(C1, C2) ≤ w(OPT(b rl c) ∩A)",A Supplementary Materials,[0],[0]
"By the fact we used an αn-approximation to balanced cut we can get the following inequality
(similarly to Lemma 1):
r · w(C1, C2) ≤ O(αn) · s · w(OPT(b rl c) ∩A)
",A Supplementary Materials,[0],[0]
"Finally, we have to sum up over all the clusters A (now in the summation we should write rA, sA instead of just r, s, since there is dependence in A) produced by the constrained recursive balanced cut algorithm for Hierarchical Clustering and we get that we can approximate the HC objective function up to O(kαn).
",A Supplementary Materials,[0],[0]
Remark 6.,A Supplementary Materials,[0],[0]
Using balanced-cut can be useful for two reasons.,A Supplementary Materials,[0],[0]
"First, the runtime of sparsest and balanced cut on a graph with n nodes and m edges are Õ(m+n1+ ).",A Supplementary Materials,[0],[0]
"When run recursively however as in our case, taking recursive sparsest cuts might be worse off by a factor of n (in case of unbalanced splits at every step) in the worst case.",A Supplementary Materials,[0],[0]
"However, recursive balanced cut is still Õ(m+n1+ ).",A Supplementary Materials,[0],[0]
"Second, it is known that an α-approximation for the sparsest cut yields an O(α)-approximation for balanced cut, but not the other way.",A Supplementary Materials,[0],[0]
"This gives more flexibility to the balanced cut algorithm, and there is a chance it can achieve a better approximation factor (although we don’t study it further in this paper).",A Supplementary Materials,[0],[0]
Proof sketch of Proposition 2.,A.2 Missing proofs in Section 3,[0],[0]
"Here the main obstacle is similar to the one we handled when proving Theorem (1): for a given cluster A created by the R-HSC algorithm, different constraints are, in general, active compared to the OPT decomposition for this cluster A.",A.2 Missing proofs in Section 3,[0],[0]
"Note of course, that OPT itself will not respect all constraints, but because we don’t know which constraints are active for OPT, we still need to use a charging argument to low levels of OPT.",A.2 Missing proofs in Section 3,[0],[0]
"Observe that here we are allowed to cut an edge ab even if we had the ab|c constraint (incurring the corresponding cost cab|c), however we cannot possibly hope to charge this to the OPT solution, as OPT, for all we know, may have respected this constraint.",A.2 Missing proofs in Section 3,[0],[0]
"In the analysis, we crucially use a merging procedure between sub-clusters of A having active constraints between them and this allows us to compare the cost of our R-HSC with the cost of OPT .
3-hyperedges to triangles for general weights .",A.2 Missing proofs in Section 3,[0],[0]
"Even though the general reduction presented in Section 3 (Figure 3) to transform a 3-hyperedge to a triangle is valid even for general instances of HSC with 3-hyperedges and arbitrary weights, the reduced sparsest cut problem may have negative weights, e.g. when wbc|a + wac|b < wab|c.",A.2 Missing proofs in Section 3,[0],[0]
"To the best of our knowledge, sparsest cut with negative weights has not been studied.",A.2 Missing proofs in Section 3,[0],[0]
"Notice however that if the original weights wbc|a, wac|b, wab|c satisfy the triangle inequality (or as a special case, if two of them are zero which is usually the case when we have a triplet constraints), then we can actually solve (approximately) the HSC instance, as the sparsest cut instance will only have non-negative weights.",A.2 Missing proofs in Section 3,[0],[0]
Proof of Theorem 3.,A.3 Missing proofs in Section 4,[0],[0]
"We start by looking at the objective value of any algorithm as the summation of contributions of different triples i, j and k to the objective, where (i, j) ∈ E and k is some other point (possibly equal to i or j).
",A.3 Missing proofs in Section 4,[0],[0]
"OBJ = ∑
(i,j)∈E
wij |Tij",A.3 Missing proofs in Section 4,[0],[0]
"| = ∑
(i,j)∈E,k∈V
wij1{k ∈ leaves(Tij)} = ∑
(i,j)∈E ∑ k∈V Yi,j,k,
where random variable Yi,j,k denotes the contribution of the edge (i, j) and vertex k to the objective value.",A.3 Missing proofs in Section 4,[0],[0]
"The vertex k is a leaf of Tij if and only if right before the time that i and j gets separated k is still in the same cluster as i and j. Therefore,
Yi,j,k = wij1{i separates from k no earlier than j }
We now show",A.3 Missing proofs in Section 4,[0],[0]
that E,A.3 Missing proofs in Section 4,[0],[0]
"[Yi,j,k] = 23wij .",A.3 Missing proofs in Section 4,[0],[0]
"Given this, the expected objective value of recursive random cutting algorithm will be at least 2n3 ∑ (i,j)∈E wij .",A.3 Missing proofs in Section 4,[0],[0]
"Moreover, the objective value of the optimal
hierarchical clustering, i.e. maximizer of the Dasgupta’s objective, is no more than n ∑
(i,j)∈E wij , and we conclude that recursive random cutting is a 23 -approximation.",A.3 Missing proofs in Section 4,[0],[0]
"To see why E [Yi,j,k] = 2 3wij , think of randomized cutting as flipping an independent unbiased coin for each vertex, and then deciding on which side of the cut this vertex belongs to based on the outcome of its coin.",A.3 Missing proofs in Section 4,[0],[0]
"Look at the sequence of the coin flips of i, j and k.",A.3 Missing proofs in Section 4,[0],[0]
"Our goal is to find the probability of the event that for the first time that i and j sequences are not matched, still i’s sequence and k’s sequence are matched up to this point, or still j’s sequence and k’s sequence are matched up to this.",A.3 Missing proofs in Section 4,[0],[0]
The probability of each of these events is equal to 13 .,A.3 Missing proofs in Section 4,[0],[0]
"To see this for the first event, suppose i’s sequence is all heads (H).",A.3 Missing proofs in Section 4,[0],[0]
"We then need the pair of coin flips of (j, k) to be a sequence of (H,H)’s ending with a (T,H), and this happens with probability ∑ i≥1( 1 4) i = 13 .",A.3 Missing proofs in Section 4,[0],[0]
The probability of the second event is similarly calculated.,A.3 Missing proofs in Section 4,[0],[0]
"Now, these events are disjoint.",A.3 Missing proofs in Section 4,[0],[0]
"Hence, the probability that i is separated from k no earlier than j is exactly 23 , as desired.
",A.3 Missing proofs in Section 4,[0],[0]
Proof of Theorem 4.,A.3 Missing proofs in Section 4,[0],[0]
We derandomize the recursive random cutting algorithm using the method of conditional expectations.,A.3 Missing proofs in Section 4,[0],[0]
"At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the “left” partition or “right” partition for the next recursion.",A.3 Missing proofs in Section 4,[0],[0]
"Once we make a decision for a point, we fix that point and go to the next one.",A.3 Missing proofs in Section 4,[0],[0]
"Now suppose for a cluster C we have already fixed points S ⊆ C, and now we want to make a decision for i ∈ C",A.3 Missing proofs in Section 4,[0],[0]
\ S.,A.3 Missing proofs in Section 4,[0],[0]
"The reward of assigning to left(right) partition is now defined as the expected value of recursive random cutting restricted to C, when the points in S are fixed (i.e. it is already decided which points in S are going to the left partition and which ones are going to the right partition), i goes to the left(right) partition and j ∈",A.3 Missing proofs in Section 4,[0],[0]
C \,A.3 Missing proofs in Section 4,[0],[0]
({i} ∪ S) are randomly assigned to either the left or right.,A.3 Missing proofs in Section 4,[0],[0]
"Note that these two rewards (or the difference of the two rewards) can be calculated exactly in polynomial time by considering all triples consisting of an edge and another vertex, and then calculating the probability that this triple contributes to the objective function (this is similar to the proof of Theorem 3, and we omit the details for brevity here).",A.3 Missing proofs in Section 4,[0],[0]
"Because we know the randomized assignment of i gives a 23 -approximation (Theorem 3), we conclude that assigning to the better of left or right partition for every vertex will remain to be at least a 23 -approximation.",A.3 Missing proofs in Section 4,[0],[0]
"For running time, we have at most n clusters to investigate.",A.3 Missing proofs in Section 4,[0],[0]
"Moreover, a careful counting argument shows that the total number of operations required to calculate the differences of the rewards of assigning to left and right partitions for all vertices is at most n(n+ 2m).",A.3 Missing proofs in Section 4,[0],[0]
"Hence, the running time is bounded by O(n2(n+m)).
",A.3 Missing proofs in Section 4,[0],[0]
Proof sketch of Theorem 5.,A.3 Missing proofs in Section 4,[0],[0]
"Before starting to prove the theorem, we prove the following simple lemma.
",A.3 Missing proofs in Section 4,[0],[0]
Lemma 3.,A.3 Missing proofs in Section 4,[0],[0]
"There is no edge between any two classes in the same layer Il.
Proof of Lemma 3.",A.3 Missing proofs in Section 4,[0],[0]
"If such an edge exists, then there is a path of length l + 1 from C to a class in Il, a contradiction.
",A.3 Missing proofs in Section 4,[0],[0]
"Now, similar to the proof of Theorem 3, we consider every triple {x, y, z}, where (x, y) ∈ E and z is another point , but this time we only consider z’s that are not involved in any triplet constraint (there are at least n− k such points).",A.3 Missing proofs in Section 4,[0],[0]
"We claim with probability at least 23·DMC({c1,...,ck}) the supernode containing z is still in the same cluster as supernodes containing x and y right before x and y gets separated.",A.3 Missing proofs in Section 4,[0],[0]
"By summing over all such triples, we show that the algorithm gets a gain of at least 2(n−k)3·DMC({c1,...,ck}) ∑ (x,y)∈E wxy, which proves the α-approximation as the optimal clustering
has a reward bounded by n ∑
(x,y)∈E wxy.",A.3 Missing proofs in Section 4,[0],[0]
"To prove the claim, if (x, y) is not the base of any triplet constraint then a similar argument as in the proof of Theorem 3 shows the desired probability is exactly 23 (with a slight adaptation, i.e. by looking at the coin sequences of supernodes containing x and y, which are going to be disjoint in this case at all iterations, and the coin sequence of z).",A.3 Missing proofs in Section 4,[0],[0]
"Now suppose (x, y) is the base of any constraint c and suppose c belongs to a class C. Consider the layered dependency subgraph of C as in Definition 2 and let the layers to be I0, . . .",A.3 Missing proofs in Section 4,[0],[0]
", IL.",A.3 Missing proofs in Section 4,[0],[0]
"In order for z to be in the same cluster as x and y when they get separated, a chain of L + 1 independent events needs to happen.",A.3 Missing proofs in Section 4,[0],[0]
"These events are defined inductively; for the first event, consider the coin sequence of z, coin sequence of (the supernode containing all the bases of) constraints in ∪Ll=0Il and coin sequences of all the keys of constraints in IL (there are ∑ C′∈IL |C
′| of them).",A.3 Missing proofs in Section 4,[0],[0]
"Without loss of generality, suppose the coin sequence of (the supernode containing) ∪Ll=0Il is all heads.",A.3 Missing proofs in Section 4,[0],[0]
Now the event happens only if at the time z flips its first tales all keys of IL have already flipped at least one tales.,A.3 Missing proofs in Section 4,[0],[0]
"Conditioned on this event happening, all the constraints in IL will be resolved and z remains in the same cluster as x and y.",A.3 Missing proofs in Section 4,[0],[0]
"Now, remove IL from the dependency subgraph and repeat the same process to define the events 2, . . .",A.3 Missing proofs in Section 4,[0],[0]
", L in a similar fashion.",A.3 Missing proofs in Section 4,[0],[0]
"For the lth event to happen, we need to look at 1 + ∑ C′∈IL |C
′| number of i.i.d.",A.3 Missing proofs in Section 4,[0],[0]
"symmetric geometric random variable, and calculate the probability that first of
them is no smaller than the rest.",A.3 Missing proofs in Section 4,[0],[0]
"This event happens with a probability at least ( 1 + ∑ C′∈IL |C ′| )−1
.",A.3 Missing proofs in Section 4,[0],[0]
"Moreover the events are independent, as there is no edge between any two classes in Il for l ∈",A.3 Missing proofs in Section 4,[0],[0]
"[L], and different classes have different keys.",A.3 Missing proofs in Section 4,[0],[0]
"After these L events, the final event that needs to happen is when all the constraints are unlocked, and z needs to remain in the same cluster as x and y at the time they get separated.",A.3 Missing proofs in Section 4,[0],[0]
This event happens with probability 23 .,A.3 Missing proofs in Section 4,[0],[0]
Multiplying all of these probabilities due to independence implies the desired approximation factor.,A.3 Missing proofs in Section 4,[0],[0]
The purpose of this section is to present the benefits of incorporating triplet constraints when performing Hierarchical Clustering.,B Experiments,[0],[0]
We will focus on real data using the Zoo dataset (Lichman [2013]) for a taxonomy application.,B Experiments,[0],[0]
"We demonstrate that using our approach, the performance of simple recursive spectral clustering algorithms can be improved by approximately 9% as measured by the Dasgupta’s Hierarchical Clustering cost function (1).",B Experiments,[0],[0]
"More specifically:
• The Zoo dataset : It contains 100 animals forming 7 different categories (e.g. mammals, amphibians etc.).",B Experiments,[0],[0]
"The features of each animal are provided by a 16-dimensional vector containing
information such as if the animal has hair or feathers etc.
",B Experiments,[0],[0]
"• Evaluation method : Given the feature vectors, we can create a similarity matrixM(·, ·) indexed by the labels of the animals.",B Experiments,[0],[0]
"We choose the widely used cosine similarity to create M .
•",B Experiments,[0],[0]
Algorithms: We use a simple implementation of spectral clustering based on the second eigenvector of the normalized Laplacian of M .,B Experiments,[0],[0]
"By applying the spectral clustering algorithm once, we can create two clusters; by applying it recursively we can create a complete hierarchical decomposition, which is ultimately the output of the HC algorithm.
",B Experiments,[0],[0]
"• Baseline comparison: Since triplet constraints are especially useful when there is noisy information (i.e. noisy features), we simulate this situation by hiding some of the features of our Zoo dataset.",B Experiments,[0],[0]
"Specifically, when we want to find the target HC tree T ∗, we use the full 16-dimensional feature vectors, but for the comparison between the unconstrained and the constrained HC algorithms we will use a noisy version of the feature vectors which consists of only the first 10 coordinates from every vector.
",B Experiments,[0],[0]
"In more detail, the first step in our experiments is to evaluate the cost of the target clustering T ∗.",B Experiments,[0],[0]
"For this, we use the full feature vectors and perform repeated spectral clustering to get a hierarchical decomposition (without incorporating any constraints).",B Experiments,[0],[0]
"We call this cost OPT.
",B Experiments,[0],[0]
"The second step is to perform unconstrained HC but with noisy information, i.e. to run the spectral clustering algorithm repeatedly on the 10-dimensional feature vectors (again without taking into account any triplet constraints).",B Experiments,[0],[0]
"This will output a hierarchical tree that has cost in terms of the Dasgupta’s HC cost Unconstrained_Noisy_Cost.11
The final step is to choose some structural constraints (that are valid in T ∗)12 and perform again HC with noisy information.",B Experiments,[0],[0]
We again use the 10-dimensional feature vectors but the spectral clustering algorithm is allowed only cuts that do not violate any of the given structural constraints.,B Experiments,[0],[0]
"Repeating until we get a decomposition gives us the final output which will have cost in terms of the Dasgupta’s HC cost Constrained_Noisy_Cost.
",B Experiments,[0],[0]
"The first main result of our experimental evaluation is that the Constrained_Noisy_Cost is surprisingly close to OPT, even though to get the Constrained_Noisy_Cost the features used were noisy and the second main result is that incorporating the structural constraints yields ≈ 9% improvement over the noisy unconstrained version of HC with cost Unconstrained_Noisy_Cost.",B Experiments,[0],[0]
"Now that we have presented the experimental set-up, we can proceed by describing our results and final observations in greater depth.",B Experiments,[0],[0]
"We ran our experiments for 20, 50, 80 and 100 animals from the Zoo dataset and for the evaluation of the % improvement in terms of the Dasgupta’s HC cost (1), we used the following formula:",B.1 Experimental Results,[0],[0]
"The improvements obtained due to the constrained version are presented in Table 1.
11The cost of the trees are always evaluated using the actual similarities obtained from the full feature vectors.",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
12Here we chose triplet constraints that will induce the same first cut as T ∗ and required no constraints after that.,Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
"This corresponds to a high-level separation of the animals, for example to those that are “land” animals versus those that are “water” animals.
",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
"#animals OPT Unconstrained_Noisy_Cost Constrained_Noisy_Cost % Improvement
20 1137 1286 1142 12.63 50 23088 25216 23443 7.68 80 89256 99211 90419 9.85 100 171290 190205 173499 9.75",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
Hierarchical clustering is a popular unsupervised data analysis method.,abstractText,[0],[0]
"For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm.",abstractText,[0],[0]
This gives rise to the problem of hierarchical clustering with structural constraints.,abstractText,[0],[0]
"Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output.",abstractText,[0],[0]
"In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information [Dasgupta, 2016].",abstractText,[0],[0]
"We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective.",abstractText,[0],[0]
"We further explore a variation of this objective for dissimilarity information [Cohen-Addad et al., 2018] and improve upon current techniques.",abstractText,[0],[0]
"Finally, we demonstrate our approach on a real dataset for the taxonomy application.",abstractText,[0],[0]
Hierarchical Clustering with Structural Constraints,title,[0],[0]
"Multivariate time series (MTS) analysis (Hamilton, 1994; Reinsel, 2003) has attracted a lot of attention in machine learning, signal processing, and other related areas, due to its impact and usefulness in many real world applications such as healthcare, climate, and financial forecasting.",1. Introduction,[0],[0]
"Statespace models such as Kalman filters (Kalman et al., 1960) and hidden Markov models (Rabiner, 1989) have been developed to model MTS and have shown promising results on prediction tasks such as forecasting and interpolation.",1. Introduction,[0],[0]
"However, in many applications, the MTS observations usually come from multiple sources and are often characterized
*Equal contribution 1Department of Computer Science, University of Southern California, Los Angeles, California, United States.",1. Introduction,[0],[0]
"Correspondence to: Zhengping Che, Sanjay Purushotham, Guangyu Li, Bo Jiang, Yan Liu <{zche,spurusho,guangyul,boj,yanliu.cs}@usc.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
by various sampling rates.",1. Introduction,[0],[0]
"For example, in healthcare, vital signs such as heart rate are sampled frequently, while lab results such as pH are measured infrequently; in finance, the stock prices are sampled daily or even more frequently, while macro-economic data such as employment, GDP are sampled monthly or quarterly.",1. Introduction,[0],[0]
Such time series observations with either regular or irregular sampling rates are termed as Multi-Rate Multivariate Time Series (MR-MTS) data.,1. Introduction,[0],[0]
Modeling the MR-MTS using state-space models is challenging since MR-MTS naturally comes with multiple temporal dependencies and these dependencies may not have direct relationship to the sampling rates.,1. Introduction,[0],[0]
"That is, the long and short-term temporal dependencies may be associated with a few or all the time series data with different sampling rates.",1. Introduction,[0],[0]
"Capturing these temporal dependencies is important as they model the underlying data generation mechanism, and they impact the interpolation and forecasting tasks.",1. Introduction,[0],[0]
"Upsampling or downsampling MR-MTS to a single rate time series cannot address this challenge, since these simple techniques may artifically introduce or remove some naturally occurring dependencies present in MR-MTS.",1. Introduction,[0],[0]
"For example, forward/backward imputation will introduce long-term dependencies.",1. Introduction,[0],[0]
"Therefore, building models which can capture multiple temporal dependencies directly from the MR-MTS data is still an open problem in the time series analysis field.
",1. Introduction,[0],[0]
"Deep learning models such as recurrent neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) have emerged as successful models for time series analysis (Graves et al., 2013; Mikolov et al., 2010) and sequence modeling applications (Socher et al., 2011; Xu et al., 2015).",1. Introduction,[0],[0]
"While deep discriminative models (Hermans & Schrauwen, 2013; Martens & Sutskever, 2011; Pascanu et al., 2013; Chung et al., 2016) have been shown to model complex non-linear temporal dependencies present in MTS, deep generative models (Gan et al., 2015; Rezende et al., 2014) have become more popular since they are intuitive, interpretable and are more powerful than their discriminative counterparts (Durbin & Koopman, 2012) and they capture the data generation process.",1. Introduction,[0],[0]
"Despite their success with single-rate time series data, the existing deep generative models are not suitable for modeling MRMTS as they are not designed to capture multiple temporal dependencies from different sampling rates.
",1. Introduction,[0],[0]
"Recently, latent hierarchical structure learning based on deep learning models have led to remarkable ad-
vances in capturing temporal dependencies from sequential data (El Hihi & Bengio, 1995; Chung et al., 2016; Koutnik et al., 2014).",1. Introduction,[0],[0]
"Motivated by these models, we propose a novel deep generative model termed as Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), which learns multiple temporal dependencies directly from MR-MTS by jointly modeling time series with different sampling rates.",1. Introduction,[0],[0]
MRHDMM learns the latent hierarchical structures along with learnable switches and captures the data generation process of MR-MTS.,1. Introduction,[0],[0]
It simultaneously learns a inference network and a generative model by leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution.,1. Introduction,[0],[0]
"The data generation process of MR-HDMM can automatically infer the hierarchical structures directly from data, which is extremely helpful for downstream tasks such as interpolation and forecasting.
",1. Introduction,[0],[0]
"In summary, we develop a first-of-a-kind novel deep generative model called MR-HDMM to systematically capture the multiple temporal dependencies present in MR-MTS by using hierarchical latent structures and learnable switches.",1. Introduction,[0],[0]
"In addition, we also propose a new structured inference network for MR-HDMM.",1. Introduction,[0],[0]
A comprehensive and systematic evaluation of the MR-HDMM model is conducted on two real-world datasets to demonstrate the state-of-the-art performance in forecasting and interpolation tasks.,1. Introduction,[0],[0]
"Finally, we interpret the learnt latent hierarchies from MR-HDMM to study the captured temporal dependencies.",1. Introduction,[0],[0]
"State-space models such as Kalman filters (KF) (Kalman et al., 1960), and hidden Markov models (HMMs) (Rabiner, 1989) have been widely used in various time series applications such as speech recognition (Rabiner, 1989), atmospheric monitoring (Houtekamer & Mitchell, 2001), and robotic control (Negenborn, 2003).",2. Related Work,[0],[0]
"These approaches successfully model regularly sampled (i.e. sampled at the same frequency/rate) time series data, however, they cannot be directly used for MR-MTS as they cannot simultaneously capture the multiple temporal dependencies present in MR-MTS.",2. Related Work,[0],[0]
"To handle MR-MTS with state-space models, researchers have extended KF models and proposed multirate Kalman filters (MR-KF) (Armesto et al., 2008; Safari et al., 2014).",2. Related Work,[0],[0]
MR-KF approaches either fuse the data with different sampling rates or fuse the estimates for KFs trained on each sampling rate.,2. Related Work,[0],[0]
Many of these MR-KF approaches aim to improve the estimates for the highest sampled rate data and do not focus on capturing the multiple temporal dependencies present in MR-MTS.,2. Related Work,[0],[0]
"Moreover, the linear transition and emission functionality of the MR-KF models limits their usability on complex real-world data.
",2. Related Work,[0],[0]
"Recently, researchers have resorted to deep learning models (Chung et al., 2016; Krishnan et al., 2015; Gan et al.,
2015) to model the non-linear temporal dynamics of realworld and sequential data.",2. Related Work,[0],[0]
"Discriminative models such as hierarchical recurrent neural network (El Hihi & Bengio, 1995), hierarchical multiscale recurrent neural network (HM-RNN) (Chung et al., 2016), and phased long short-term memory (PLSTM) (Neil et al., 2016) have been proposed to capture temporal dependencies of sequential data.",2. Related Work,[0],[0]
"However, these discriminative models do not capture the underlying data generation process and therefore are not suited for forecasting and interpolation tasks.",2. Related Work,[0],[0]
"Deep generative models (Rezende et al., 2014; Krishnan et al., 2015; Gan et al., 2015) have been developed to model the data generation process of the complex time series data.",2. Related Work,[0],[0]
"Krishnan et al. (2015) proposed deep Kalman filter, a nonlinear state-space model, by marrying the ideas of deep neural networks with Kalman filters.",2. Related Work,[0],[0]
Fraccaro et al. (2016) introduced stochastic recurrent neural network (SRNN) which glued a RNN with a state space model together to form a stochastic and sequential neural generative model.,2. Related Work,[0],[0]
"Even though these deep generative models are the state-of-the-art approaches to obtain the underlying data generation process, they are not designed to capture all the temporal dependencies of MR-MTS.",2. Related Work,[0],[0]
None of the existing deep learning models or state-space models can be directly used for modeling MR-MTS.,2. Related Work,[0],[0]
"Thus, in this work, we develop a deep generative model which leverages the properties of the above discriminative and generative models, to model the data generation process of MR-MTS while also capturing the multiple temporal dependencies using a latent hierarchical structure.",2. Related Work,[0],[0]
"In this section, we present our proposed Multi RateHierarchical Deep Markov Model (MR-HDMM).",3. Our Model,[0],[0]
"We first clarify the notations and definitions used in this paper.
",3. Our Model,[0],[0]
"Notations Given a MR-MTS of L different sampling rates and length T , we use a vectorxlt ∈ RDl to represent the time series observations of lth rate at time t. Here l = 1, . . .",3. Our Model,[0],[0]
", L, t = 1, . . .",3. Our Model,[0],[0]
", T , and Dl is the dimension of time series with lth rate.",3. Our Model,[0],[0]
"The L sampling rates are in descending order, i.e., l = 1 and l = L refer to the highest and lowest sampling rates.",3. Our Model,[0],[0]
"To make the notations succinct, we use xl:l ′
t:t′ to denote all observed time series of lth to l′th rates and from time t to t′.",3. Our Model,[0],[0]
We use θ(.) and φ(.) to denote the parameter sets for generation model pθ and inference network qφ respectively.,3. Our Model,[0],[0]
we use L layers of RNNs in the inference network to model MR-MTS of L different sampling rates.,3. Our Model,[0],[0]
"We use LHS , the number of hidden layers in both generation model and inference network, to control the depth of the learnt hierarchical structures.",3. Our Model,[0],[0]
"In the rest of this paper we take LHS = L for model simplicity, but in practice they are not tied.",3. Our Model,[0],[0]
"The latent states or variables are denoted by z, s and h. Their superscript and subscript respectively indicate the corresponding layer(s) and the time step(s) (e.g., z1:L1:T , s 2:L t , h l t).
",3. Our Model,[0],[0]
Figure 1 illustrates our MR-HDMM model which consists of the generation model and inference network.,3. Our Model,[0],[0]
"MR-HDMM captures the underlying data generation process by using the variational inference methods (Rezende et al., 2014; Kingma & Welling, 2013) and learns the latent hierarchical structures using learnable switches and auxiliary connections to adaptively encode the dependencies across the hierarchies and the timestamps.",3. Our Model,[0],[0]
"In particular, the switches use an update-and-reuse mechanism to control the updates of the latent states of a layer based on their previous states (i.e., utilizing temporal information) and the lower latent layers (i.e., utilizing the hierarchy).",3. Our Model,[0],[0]
"The switch triggers an update of the current states if it gets enough information from lower-level states, otherwise it reuses the previous states.",3. Our Model,[0],[0]
"Thus, the higher-level states act as summarized representations over the lower-level states and the switches help to propagate the temporal dependencies.",3. Our Model,[0],[0]
The auxiliary connections (dashed lines in Figure 1(a)) between MR-MTS of different sampling rates and different latent layers help the model effectively capture the short-term and long-term temporal dependencies.,3. Our Model,[0],[0]
"Without the auxiliary connections, the higher-rate time series may mask the multi-scale dependencies present in the lower-rate time series data while propagating dependencies through bottom-up connections.",3. Our Model,[0],[0]
"Note that, the auxiliary connections are not related to the sampling rate of MR-MTS, and the sampling rate of higherrate variable need not be a multiple of sampling rate of the lower-rate variable.",3. Our Model,[0],[0]
"Due to the flexibility of auxiliary connections, our MR-HDMM can also handle irregularly sampled time series data or missing data.",3. Our Model,[0],[0]
"We can a) zero-out the missing data points in the inference network and remove the corresponding auxiliary connections in the generation model during training, and b) interpolate missing values by adding auxiliary connections in the well-trained model.",3. Our Model,[0],[0]
Figure 1(a) shows the generation model of our MR-HDMM.,3.1. Generation Model,[0],[0]
"The generation process of our MR-HDMM follows the transition and emission framework, which is obtained by applying deep recurrent neural networks to non-linear continuous state space models.",3.1. Generation Model,[0],[0]
"The generation model is carefully designed to incorporate the switching mechanism and auxiliary connections in order to capture the multiple temporal dependencies present in MR-MTS.
",3.1. Generation Model,[0],[0]
Transition We design the transition process of the latent state z to capture the hierarchical structure for multiple temporal dependencies with learnable binary switches s.,3.1. Generation Model,[0],[0]
"For each non-bottom layer l > 1 and time step t ≥ 1, we use a binary switch state slt to control the updates of the corresponding latent states zlt, as shown in Figure 2.",3.1. Generation Model,[0],[0]
"slt is obtained based on the values of the previous latent states zlt−1 and the lower layer latent states z l−1 t by a de-
terministic mapping slt =",3.1. Generation Model,[0],[0]
"I ( gθs(z l t−1, z l−1 t ) ≥ 0 ) .",3.1. Generation Model,[0],[0]
"When the switch is on (i.e., update operation, slt = 1), z l t is updated based on zlt−1 and z l−1 t through a learnt transition distribution.",3.1. Generation Model,[0],[0]
"We use a multivariate Gaussian distribution N ( µlt,Σ l t|zlt−1, zl−1t ; θz ) with mean and covariance given by (µlt,Σ l t) = gθz",3.1. Generation Model,[0],[0]
"(z l t−1, z l−1 t )",3.1. Generation Model,[0],[0]
as the transition distribution.,3.1. Generation Model,[0],[0]
"When the switch is off (i.e., reuse operation, slt = 0), z l t will
be drawn from the same distribution as its previous states zlt−1, which is N ( µlt−1,Σ l t−1 ) .",3.1. Generation Model,[0],[0]
"Note, unlike Chung et al. (2016), we do not copy the previous state since our latent states are stochastic.",3.1. Generation Model,[0],[0]
The latent states of the first layer (z11:T ) are always updated at each time step.,3.1. Generation Model,[0],[0]
"In our model, gθs is parameterized by a multilayer perceptron (MLP), and gθz is parameterized by gated recurrent units (GRU) (Chung et al., 2014) to capture the temporal dependencies.",3.1. Generation Model,[0],[0]
"With this update-or-reuse transition mechanism, higher latent layers tend to capture longer-term temporal dependencies through the bottom-up connections in the latent layers.
",3.1. Generation Model,[0],[0]
Emission Multi-rate multivariate observation x needs to be generated from z in the emission process.,3.1. Generation Model,[0],[0]
"In order to embed the multiple temporal dependencies in the generated MR-MTS, we introduce auxiliary connections (denoted by the dashed lines in Figure 1(a)) from the higher latent layers to the lower rate time series.",3.1. Generation Model,[0],[0]
"That is, time series of lth rate at time t (i.e., xlt) is generated from all latent states up to lth layer z1:lt through emission distribution Π ( xlt|z1:lt ; θx ) .",3.1. Generation Model,[0],[0]
The choice of emission distribution Π is flexible and depends on the data type.,3.1. Generation Model,[0],[0]
"Multinomial distribution is used for categorical data, and Gaussian distribution is used for continuous data.",3.1. Generation Model,[0],[0]
"Since all the data in our tasks are continuous, we use Gaussian distribution where the mean µ(x) l
t
and covariance Σ(x)",3.1. Generation Model,[0],[0]
"l
t are determined by gθx(z 1:l t ), which
is parameterized by an MLP.
",3.1. Generation Model,[0],[0]
"To summarize, the overall generation process is described in Algorithm 1.",3.1. Generation Model,[0],[0]
"The parameter set of generation model is θ = {θx, θz, θs}.",3.1. Generation Model,[0],[0]
"Given this, the joint probability of MRMTS and the latent states/switches can be factorized by the following Equation (1).
pθ ( x1:L1:T ,z 1:L 1:T , s 2:L 1:T |z1:L0 ) =pθ ( x1:L1:T |z1:L1:T ) pθ ( z1:L1:T , s 2:L 1:",3.1. Generation Model,[0],[0]
"T |z1:L0
) =
T∏ t=1 pθ ( x1:Lt |z1:Lt ) · T∏ t=1 pθ ( z1:Lt , s 2:L t |z1:Lt−1 ) =
T∏ t=1",3.1. Generation Model,[0],[0]
L∏ l=1 pθx ( xlt|z1:lt ) · T∏ t=1 pθz ( z1t |z1t−1 ) · T∏ t=1,3.1. Generation Model,[0],[0]
L∏ l=2 pθs (,3.1. Generation Model,[0],[0]
"slt|zlt−1,zl−1t ) pθz ( zlt|zlt−1,zl−1t , slt ) (1)
",3.1. Generation Model,[0],[0]
"In order to obtain the parameters of MR-HDMM, we need to maximize the log marginal likelihood of all MR-MTS data points, which is the summation of the log marginal likelihood L(θ) = log pθ",3.1. Generation Model,[0],[0]
( x1:L1:T |z1:L0 ) of each MR-MTS data point x1:L1:T .,3.1. Generation Model,[0],[0]
The log marginal likelihood of one data point can be achieved by integrating out all possible z and s in Equation (1).,3.1. Generation Model,[0],[0]
"Since s are deterministic binary variables, integrating them out can be done straightforwardly by taking their values in the likelihood.",3.1. Generation Model,[0],[0]
"However, stochastic variable
Algorithm 1 Generation model of MR-HDMM 1: Initialize z1:L0 ∼ N (0, I) 2: for t = 1, . . .",3.1. Generation Model,[0],[0]
", T do 3: ( µ1t ,Σ 1 t ) = gθz",3.1. Generation Model,[0],[0]
"(z 1 t−1)
4: z1t ∼ N ( µ1t ,Σ 1 t ) {Transition of the first layer.}",3.1. Generation Model,[0],[0]
"5: for l = 2, · · · , L do 6: slt =",3.1. Generation Model,[0],[0]
I,3.1. Generation Model,[0],[0]
"( gθs(z l t−1,z l−1 t )",3.1. Generation Model,[0],[0]
"≥ 0
) 7: ( µlt,Σ l t )",3.1. Generation Model,[0],[0]
"= { gθz (z l t−1,z l−1 t )",3.1. Generation Model,[0],[0]
"if s l t = 1(
µlt−1,Σ l t−1 )
otherwise.",3.1. Generation Model,[0],[0]
"8: zlt ∼ N ( µlt,Σ l t ) {Transition of other layers.}
9: end for 10: for l = 1, · · · , L do 11: ( µ(x) l
t,Σ (x)l t ) =",3.1. Generation Model,[0],[0]
"gθx(z 1:l t )
12: xlt ∼ N ( µ(x) l t,Σ (x)l t ) {Emission.} 13: end for 14: end for
z cannot be analytically integrated out.",3.1. Generation Model,[0],[0]
"Thus, we resort to the well-known variational principle (Jordan, 1998) and introduce our inference network below.",3.1. Generation Model,[0],[0]
We design our inference network to mimic the structure of the generative model.,3.2. Inference Network,[0],[0]
The goal is to obtain an objective which can be optimized easily and which can make the model parameter learning amenable.,3.2. Inference Network,[0],[0]
"Instead of directly maximizing L(θ) w.r.t θ, we build an inference network with a tractable distribution qφ , and maximize the variational evidence lower bound (ELBO) F(θ, φ) ≤ L(θ) with respect to both θ and φ.",3.2. Inference Network,[0],[0]
"Note, φ is the parameter set of the inference network which will is formally defined at the end of this section.",3.2. Inference Network,[0],[0]
"The lower bound can be written as (please refer to the supplementary materials for full derivation):
F(θ, φ) =",3.2. Inference Network,[0],[0]
Eqφ [ log pθ ( x1:L1:T |z1:L0:T )],3.2. Inference Network,[0],[0]
"−DKL ( qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0
)∥∥∥pθ (z1:L1:T , s2:L1:T |z1:L0 ))",3.2. Inference Network,[0],[0]
"(2)
where the expectation of the first term is under qφ ( z1:L1:T |x1:L1:T , z1:L0 ) .",3.2. Inference Network,[0],[0]
"To get a tight bound and an accurate estimate from our MR-HDMM, we need to properly design a new inference network as using the existing inference networks from SRNN (Fraccaro et al., 2016) or DMM (Krishnan et al., 2015) is not applicable for MR-MTS.",3.2. Inference Network,[0],[0]
"In the following, we show how we design the inference network (Figure 1(b)) to obtain a good structured approximation to the posterior.",3.2. Inference Network,[0],[0]
"First, we maintain the Markov properties of z in the inference network, which leads to the factorization:
qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0 ) =",3.2. Inference Network,[0],[0]
T∏ t=1,3.2. Inference Network,[0],[0]
"qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) (3)
We then leverage the hierarchical structure and inherit the switches from the generation model into the
Table 1.",3.2. Inference Network,[0],[0]
"Comparison of structured inference networks.
",3.2. Inference Network,[0],[0]
Inference network Implemented with RNN output Captured in hlt Variational approximation for zlt filtering forward RNN hforward xl1:,3.2. Inference Network,[0],[0]
"t qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) smoothing backward RNN hbackward xlt:T qφ ( zlt|zlt−1,zl−1t , slt,x1:Lt:T
) bi-direction bi-directional RNN [ hforward,hbackward ]",3.2. Inference Network,[0],[0]
"xl1:T qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T
) inference network.",3.2. Inference Network,[0],[0]
"That is, the same gθs from the generation model is used in the inference network, i.e., qφ ( slt|zlt−1, zl−1t ,x1:L1:T ) =",3.2. Inference Network,[0],[0]
"qφs ( slt|zlt−1, zl−1t ) =
pθs ( slt|zlt−1, zl−1t ) .",3.2. Inference Network,[0],[0]
"Then, for each term in the righthand side of Equation (3) and for all t = 1, · · · , T , we have:
qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) =qφ ( z1t |z1t−1,x1:L1:T
) ·",3.2. Inference Network,[0],[0]
"L∏ l=2 qφ ( slt|zlt−1,zl−1t ,x1:L1:T ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T
) =qφ ( z1t |z1t−1,x1:L1:T
) ·",3.2. Inference Network,[0],[0]
"L∏ l=2 pθs ( slt|zlt−1,zl−1t )",3.2. Inference Network,[0],[0]
"qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) (4) Thus, the inference network can be factorized by Equation (3) and (4).",3.2. Inference Network,[0],[0]
"Note, we also can factorize generative model based on Equation (1).",3.2. Inference Network,[0],[0]
"Given these, we further factorize the ELBO in Equation (2) as a summation of expectations of conditional log likelihood and KL divergence terms over time steps and hierarchical layers:
F(θ, φ) = T∑ t=1 L∑",3.2. Inference Network,[0],[0]
l=1,3.2. Inference Network,[0],[0]
EQ∗(z1,3.2. Inference Network,[0],[0]
":lt ) log pθx ( xlt|z1:lt ) +
T∑ t=1",3.2. Inference Network,[0],[0]
"EQ∗(z1t−1)DKL ( qφ ( z1t |x1:L1:T ,z1t−1 )∥∥∥pθ (z1t |z1t−1))",3.2. Inference Network,[0],[0]
"+
T∑ t=1 L∑ l=2 EQ∗(z1t−1,zl−1t )
DKL ( qφ ( zlt|x1:L1:T ,zlt−1,zl−1t )∥∥∥pθ (z1t |z1t−1,zl−1t )) (5) where Q∗ (·) denotes the marginal distribution of (·) from qφ .",3.2. Inference Network,[0],[0]
"The details about the factorization and the marginalized distribution are provided in the supplementary materials.
",3.2. Inference Network,[0],[0]
"Parameterization of inference network We parameterize the inference network and construct the variational approximation qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) used in Equation 5 by deep learning models.",3.2. Inference Network,[0],[0]
"First, we use L RNNs to capture MR-MTS with L different sampling rates such that each rate is modeled by one RNN model separately.",3.2. Inference Network,[0],[0]
"Second, to obtain lth latent states zlt of the inference network at time step t, we not only use the previous latent states zlt−1 and the lower layer latent states z l−1 t but also take the lth RNN output denoted by hlt as an input.",3.2. Inference Network,[0],[0]
"Third, we reuse
the same latent state distribution and switch mechanism from the generation model to generate z of the inference network.",3.2. Inference Network,[0],[0]
"To be more specific, zlt is drawn from a multivariate normal distribution, where the mean and covariance are reused from those of zlt−1 if s l t = 1 and l > 1, otherwise the mean and covariance are modeled by gated recurrent units (GRU) with input [ hlt, z l t−1, z l−1 t ] .",3.2. Inference Network,[0],[0]
"The choice of the RNN models for hlt affects what and how the information at other time steps is considered in the approximation at time t, i.e. the form of qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) .",3.2. Inference Network,[0],[0]
"Inspired by Krishnan et al. (2016), we construct the variational approximation in three settings (filtering, smoothing, bi-direction) for forecasting and interpolation tasks.",3.2. Inference Network,[0],[0]
"In filtering setting, we only consider the information up to time t (i.e., x1:L1:t ) using forward RNNs.",3.2. Inference Network,[0],[0]
"By doing this, we have hlt = hlt forward = RNN forward ( hlt−1 forward ,xlt ) , and thus
qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) =",3.2. Inference Network,[0],[0]
"qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) .",3.2. Inference Network,[0],[0]
"The filtering setting does not use future information, so it is suitable for forecasting task at future time step t′ > T .",3.2. Inference Network,[0],[0]
"For interpolation tasks, we can use backward RNNs to utilize the information after time t (i.e., x1:Lt:T ) with h l t = h",3.2. Inference Network,[0],[0]
"l t backward =
RNN backward ( hlt+1 backward ,xlt ) , or bi-directional RNNs to uti-
lize information across all time steps, which is x1:L1:T , at any time t with hlt =",3.2. Inference Network,[0],[0]
"[ hlt forward ,hlt backward ] .",3.2. Inference Network,[0],[0]
"These two models lead to smoothing and bidirection settings, respectively.",3.2. Inference Network,[0],[0]
We summarize the three inference networks in Table 1.,3.2. Inference Network,[0],[0]
"We use φh and φz to denote the parameter sets related to h and z respectively and use φ = {φh, φz, φs = θs} to represent the parameter set of the inference network.",3.2. Inference Network,[0],[0]
"We jointly learn the parameters (θ, φ) of the generative model pθ and the inference network qφ by maximizing the ELBO in Equation (5).",3.3. Learning the Parameters,[0],[0]
"The main challenge in the optimization is obtaining the gradients of all the terms under the correct expectation i.e, EQ∗ .",3.3. Learning the Parameters,[0],[0]
"We use stochastic backpropagation (Kingma & Welling, 2013) for estimating all these gradients and train the model by stochastic gradient descent (SGD) approaches.",3.3. Learning the Parameters,[0],[0]
We employ ancestral sampling techniques to obtain the samples z .,3.3. Learning the Parameters,[0],[0]
"That is, we draw all samples z in a sequential way from time 1 to T and from layer 1 to L. Given the samples from previous layer l − 1 or previous time t− 1, the new samples at time t and layer l will be distributed according to the marginal distribution Q∗. Notice
Algorithm 2 Learning MR-HDMM with stochastic backpropagation and SGD Require: X : a set of MR-MTS of L sampling rates; Initial (θ, φ)
1: while not converged do 2: Choose a random minibatch of MR-MTS X ′ ⊂ X 3: for each sample",3.3. Learning the Parameters,[0],[0]
x1:L1:T ∈ X ′,3.3. Learning the Parameters,[0],[0]
"do 4: Compute h1:L1:T by inference network φh on input x 1:L 1:T 5: Sample ẑ1:L0 ∼ N (0, I) 6: for t = 1, · · · , T do 7: Estimate µ1t (φ) ,Σ1t
(φ) by φz , and µ1t ,Σ1t by θz , given samples ẑ1t−1 and h 1 t
8:",3.3. Learning the Parameters,[0],[0]
"Based onµ1t (φ) ,Σ1t (φ) ,µ1t ,Σ 1 t , compute the gradient of DKL ( qφ ( z1t |· )∥∥∥pθ (z1t |·))",3.3. Learning the Parameters,[0],[0]
"9: Sample ẑ1t ∼ N ( µ1t (φ) ,Σ1t (φ) )
10: for l = 2, · · · , L do 11: Compute slt by θs from samples ẑlt−1 and ẑ l−1 t 12: Estimate µlt (φ) ,Σlt (φ) by φz , and µlt,Σlt by θz ,
given samples ẑlt−1, ẑ l−1 t , s l t, and hlt
13: Based on µlt (φ) ,Σlt (φ) ,µlt,Σ l t, compute the gradient of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·))
14: Sample ẑlt ∼ N ( µlt (φ) ,Σlt (φ) )",3.3. Learning the Parameters,[0],[0]
"15: end for 16: Compute the gradient of log pθx ( xlt|ẑ1:lt
) 17: end for 18: end for 19: Update (θ, φ) using all gradients 20: end while
that all terms of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·))",3.3. Learning the Parameters,[0],[0]
"in Equation (5) are KL divergences between two multivariate Gaussian distributions, and pθx ( xlt|z1:lt ) is also a multivariate Gaussian distribution.",3.3. Learning the Parameters,[0],[0]
"Thus, all the required gradients can be estimated analytically from the samples drawn in our proposed way.",3.3. Learning the Parameters,[0],[0]
Algorithm 2 shows the overall learning procedure.,3.3. Learning the Parameters,[0],[0]
We conducted experiments on two real-world datasets - the MIMIC-III healthcare dataset and the USHCN climate dataset - and answer the following questions: (a) How does our proposed model perform when compared to the existing state-of-the-art approaches?,4. Experiments,[0],[0]
"(b) To what extent, are the proposed learnable hierarchical latent structure and auxiliary connections useful to model the data generation process?",4. Experiments,[0],[0]
(c) How do we interpret the hierarchy learned by the proposed model?,4. Experiments,[0],[0]
"In the remainder of this section, we will describe the datasets, methods, empirical results and interpretations to answer the above questions.",4. Experiments,[0],[0]
"MIMIC-III dataset MIMIC-III is a public de-identified dataset collected at Beth Israel Deaconess Medical Cen-
ter from 2001 to 2012 (Johnson et al., 2016).",4.1. Datasets and Experimental Design,[0],[0]
"It contains over 58,000 hospital admission records of 38,645 adults and 7,875 neonates.",4.1. Datasets and Experimental Design,[0],[0]
"For our experiments, we chose 10,709 adult admission records and extracted 62 temporal features from the first 72 hours.",4.1. Datasets and Experimental Design,[0],[0]
"These features had one of the three sampling rates of 1 hour, 4 hours and 12 hours.",4.1. Datasets and Experimental Design,[0],[0]
To fill-in any missing entries in our dataset we used forward or linear imputation similar to Che et al. (2016).,4.1. Datasets and Experimental Design,[0],[0]
"To ensure fair comparison, we only evaluate and compare all the models on the original time-series (i.e. non-imputed data).",4.1. Datasets and Experimental Design,[0],[0]
"Our main tasks on the MIMIC-III dataset are forecasting on time series with all rates, and interpolation of the low-rate time series values.
",4.1. Datasets and Experimental Design,[0],[0]
"USHCN climate dataset The U.S. Historical Climatology Network Monthly (USHCN) dataset (Menne et al., 2010) is publicly available and consists of daily meteorological data of 54 stations in California spanning from 1887 to 2009.",4.1. Datasets and Experimental Design,[0],[0]
"It has five climate variables for each station: a) daily maximum temperature, b) daily minimum temperature, c) whether it was a snowy day or not, d) total daily precipitation, and e) daily snow precipitation.",4.1. Datasets and Experimental Design,[0],[0]
We preprocessed this dataset to extract daily climate data for 100 consecutive years starting from 1909.,4.1. Datasets and Experimental Design,[0],[0]
"To get multi-rate time series data, we extract 208 features and split all features into 3 groups with sampling rates of 1 day, 5 days, and 10 days respectively.",4.1. Datasets and Experimental Design,[0],[0]
This public dataset has been carefully processed by National Oceanic and Atmospheric Administration (NOAA) to ensure quality control and it has no missing entries.,4.1. Datasets and Experimental Design,[0],[0]
"Our tasks on this dataset are climate forecasting on all features and interpolation on 5-day and 10-day sampled data.
",4.1. Datasets and Experimental Design,[0],[0]
Tasks We use the proposed MR-HDMM on two prediction tasks: multi-rate time series forecasting and low-rate time series interpolation.,4.1. Datasets and Experimental Design,[0],[0]
"Since both datasets have 3 different sampling rates, we use HSR/MSR/LSR to denote high/medium/low sampling rate respectively.
",4.1. Datasets and Experimental Design,[0],[0]
•,4.1. Datasets and Experimental Design,[0],[0]
Forecasting:,4.1. Datasets and Experimental Design,[0],[0]
Predict the future multivariate time series based on its history.,4.1. Datasets and Experimental Design,[0],[0]
"For MIMIC-III dataset, we predict the last 24 hrs time series based on the first (previous) 48 hours time series data.",4.1. Datasets and Experimental Design,[0],[0]
"In USHCN dataset, we forecast the climate for the next 30 days based on the observations of the previous year.",4.1. Datasets and Experimental Design,[0],[0]
•,4.1. Datasets and Experimental Design,[0],[0]
Interpolation: Fill-in the low rate time series based on co-evolving higher rate time series data.,4.1. Datasets and Experimental Design,[0],[0]
"For MIMIC-III dataset, we down-sampled 8 features from MSR to LSR and then performed interpolation task by up-sampling these 8 features back to MSR.",4.1. Datasets and Experimental Design,[0],[0]
"For USHCN dataset, the interpolation task involved up-sampling the MSR and LSR features to HSR features, i.e. up-sample 5-day and 10-day data to 1-day.",4.1. Datasets and Experimental Design,[0],[0]
"We demonstrate in-sample interpolation (i.e. interpolation within training dataset) and out-sample interpolation (i.e. interpolation in the testing dataset) on the MIMIC-III dataset and in-sample interpolation on the USHCN dataset.
",4.1. Datasets and Experimental Design,[0],[0]
Baselines We compare MR-HDMM with several strong baselines in these two tasks.,4.1. Datasets and Experimental Design,[0],[0]
"Additionally, to show the advantage of learnable hierarchical latent structure and auxiliary connections, we simplify MR-HDMM into two other models for comparison: (a) Multi-Rate Deep Markov Models (MR-DMM) which removes the hierarchical structure in latent space; (b) Hierarchical Deep Markov Models (HDMM) which drops the auxiliary connections between the lowerrate time series and higher level latent layers.",4.1. Datasets and Experimental Design,[0],[0]
"MR-DMM and HDMM are discussed in the supplementary materials.
",4.1. Datasets and Experimental Design,[0],[0]
"For forecasting tasks, we compare MR-HDMM with the following baseline models:
• Single-rate: Kalman Filters (KF), Vector AutoRegression (VAR), Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), PhasedLSTM (PLSTM) (Neil et al., 2016), Deep Markov Models (DMM) (Krishnan et al., 2015) and Hierarchical Multiscale Recurrent Neural Networks (HM-RNN) (Chung et al., 2016).",4.1. Datasets and Experimental Design,[0],[0]
"• Multi-rate: Multiple Kalman Filters (MKF) (Drolet et al.,
2000), Multi-rate Kalman Filters (MR-KF) (Safari et al., 2014), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM).
",4.1. Datasets and Experimental Design,[0],[0]
"For interpolation task, we compare MR-HDMM with the following baseline models:
• Imputation methods: Mean imputation (Simple-Mean), Cubic Spline (CubicSpline) (De Boor et al., 1978), Multiple Imputations by Chained Equations (MICE) (White et al., 2011), MissForest (Stekhoven & Bühlmann, 2011), SoftImpute (Mazumder et al., 2010).",4.1. Datasets and Experimental Design,[0],[0]
"• Deep learning models: Deep Markov Models (DMM), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM).",4.1. Datasets and Experimental Design,[0],[0]
We show the evaluation results of our MR-HDMM on the following: (a) Forecasting: we generate the next latent state using the learned transition distribution and then generate observations from these new latent states; (b) Interpolation: we use the mode of the approximated posterior in the generation model to generate the unseen data in low-rate time series.,4.2. Evaluation and Implementation Details,[0],[0]
"(c) Inference: we take multi-rate time series as the input to obtain the approximate posterior of latent states.
",4.2. Evaluation and Implementation Details,[0],[0]
"For generation model in MR-HDMM, we use multivariate Gaussian with diagonal covariance for both emission distribution and transition distribution.",4.2. Evaluation and Implementation Details,[0],[0]
"We parameterized the emission mapping gθx by a 3-layer MLP with ReLU activations, the transition mapping gθz by gated recurrent unit (GRU), and mapping gθs by a 3-layer MLP with ReLU activations on the hidden layers and linear activations on the output layer.",4.2. Evaluation and Implementation Details,[0],[0]
"For inference networks, we adopt filter-
ing setting for forecasting and bidirection setting for interpolation from Table 1 with 3-layer GRUs.",4.2. Evaluation and Implementation Details,[0],[0]
"To update θs, we replace the sign function with a sharp sigmoid function during training, and use the indicator function during validation.",4.2. Evaluation and Implementation Details,[0],[0]
"The single-rate baseline models cannot handle multi-rate data directly, and we up-sample all the lower rate data into higher rate data using linear interpolation.",4.2. Evaluation and Implementation Details,[0],[0]
"We use the stats-toolbox (Seabold & Perktold, 2010) in python for the VAR model implementation.",4.2. Evaluation and Implementation Details,[0],[0]
"We use pykalman (Duckworth, 2013) to implement all the KFbased models.",4.2. Evaluation and Implementation Details,[0],[0]
The implementation details of the KF-based methods are discussed in the supplementary materials.,4.2. Evaluation and Implementation Details,[0],[0]
"For LSTM and PLSTM model, we use one layer with 100 neurons to model the time-series, and then apply a soft-max regressor on top of the last hidden state to do regression.
",4.2. Evaluation and Implementation Details,[0],[0]
"To ensure a fair comparison, we use roughly the same amount of parameters for all models.",4.2. Evaluation and Implementation Details,[0],[0]
"For experiments on USHCN dataset, train/valid/test sets were split as 70/10/20.",4.2. Evaluation and Implementation Details,[0],[0]
"For experiments on MIMIC-III, we used 5-fold cross validation (train on 3 folds, validate on another fold and test on the remaining fold) and report the average Mean Squared Error (MSE) of 5 runs for both forecasting and interpolation tasks.",4.2. Evaluation and Implementation Details,[0],[0]
"Note that, we train all the deep learning models with the Adam optimization method (Kingma & Ba, 2014) and use validation set to find the best weights, and report the results on the held-out test set.",4.2. Evaluation and Implementation Details,[0],[0]
All the input variables are normalized to be of 0 mean and 1 standard deviation.,4.2. Evaluation and Implementation Details,[0],[0]
Forecasting Table 2 and 3 respectively show the forecasting results on MIMIC-III and USHCN datasets in terms of MSE.,4.3. Quantitative Results,[0],[0]
Our proposed MR-HDMM outperforms all the competing multi-rate latent space models by at least 5% and beats the single-rate models by at least 15% on both datasets with all features.,4.3. Quantitative Results,[0],[0]
"Our model also performs the best on single-rate HSR and MSR forecasting tasks, and performs well on the LSR forecasting task on MIMIC-III and USHCN datasets.
",4.3. Quantitative Results,[0],[0]
Interpolation Table 4 shows the interpolation results on the two datasets.,4.3. Quantitative Results,[0],[0]
"Since VAR and LSTM cannot be directly
used for the interpolation task, we focus on evaluating generative models and imputation methods.",4.3. Quantitative Results,[0],[0]
"From Table 4, we observe that our proposed model outperforms the baselines and the competing multi-rate latent space models by a large margin on all the interpolation tasks on these two datasets.
",4.3. Quantitative Results,[0],[0]
Inference We also compare the lower bound of loglikelihood of all generative models in Table 5.,4.3. Quantitative Results,[0],[0]
"The higher
lower bound value indicates a better fitted model given the training data.",4.3. Quantitative Results,[0],[0]
Our MR-HDMM model achieves the best performance on both datasets.,4.3. Quantitative Results,[0],[0]
"In all our experiments, MR-HDMM outperforms other generative models by a significant margin.",4.4. Discussion,[0],[0]
"Considering that all the deep generative models have the same amount of parameters, this improvement empirically demonstrates the effectiveness of our proposed learnable latent hierarchical structure and auxiliary connections.",4.4. Discussion,[0],[0]
"In Figure 3(a) and 3(b), we visualize the latent hierarchical structure of MR-HDMM learned from the first 48 hours of an admission in MIMICIII dataset and one-year climate observations in USHCN dataset.",4.4. Discussion,[0],[0]
"A color block indicates that the latent state zlt is updated from zlt−1 and z l−1 t (update), while the white block indicates zlt is generated from the same distribution of z l t−1 (reuse).",4.4. Discussion,[0],[0]
"As expected, the higher latent layers tend to update less frequently and capture the long-term temporal dependencies.",4.4. Discussion,[0],[0]
"To understand learned hierarchical structure more intuitively, we also show precipitation time series from USCHN dataset along with learned switches in Figure 3(b).",4.4. Discussion,[0],[0]
"We observe that the higher latent layer tends to update along with the precipitation, which is reasonable since precipitation makes significant changes to the underlying weather condition which is captured by the higher latent layer.",4.4. Discussion,[0],[0]
We proposed the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM) - a novel deep generative model for forecasting and interpolation tasks on multi-rate multivariate time series (MR-MTS) data.,5. Summary,[0],[0]
MR-HDMM models the data generation process by learning a latent hierarchical structure using auxiliary connections and learnable switches to capture the temporal dependencies.,5. Summary,[0],[0]
Empirically we showed that our proposed model outperforms the existing single-rate and multi-rate models on healthcare and climate datasets.,5. Summary,[0],[0]
"This work is supported in part by NSF Research Grant IIS-1254206 and IIS-1539608, and MURI grant W911NF-11-1-0332.",Acknowledgments,[0],[0]
"The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government.",Acknowledgments,[0],[0]
Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies.,abstractText,[0],[0]
State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data.,abstractText,[0],[0]
"To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS.",abstractText,[0],[0]
Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks.,abstractText,[0],[0]
"In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.",abstractText,[0],[0]
Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 97–109 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
97",text,[0],[0]
"Identifying and understanding entities is a central component in knowledge base construction (Roth et al., 2015) and essential for enhancing downstream tasks such as relation extraction
*equal contribution Data and code for experiments: https://github.
com/MurtyShikhar/Hierarchical-Typing
(Yaghoobzadeh et al., 2017b), question answering (Das et al., 2017; Welbl et al., 2017) and search (Dalton et al., 2014).",1 Introduction,[0],[0]
"This has led to considerable research in automatically identifying entities in text, predicting their types, and linking them to existing structured knowledge sources.
",1 Introduction,[0],[0]
Current state-of-the-art models encode a textual mention with a neural network and classify the mention as being an instance of a fine grained type or entity in a knowledge base.,1 Introduction,[0],[0]
"Although in many cases the types and their entities are arranged in a hierarchical ontology, most approaches ignore this structure, and previous attempts to incorporate hierarchical information yielded little improvement in performance (Shimaoka et al., 2017).",1 Introduction,[0],[0]
"Additionally, existing benchmark entity typing datasets only consider small label sets arranged in very shallow hierarchies.",1 Introduction,[0],[0]
"For example, FIGER (Ling and Weld, 2012), the de facto standard fine grained entity type dataset, contains only 113 types in a hierarchy only two levels deep.
",1 Introduction,[0],[0]
"In this paper we investigate models that explicitly integrate hierarchical information into the embedding space of entities and types, using a hierarchy-aware loss on top of a deep neural network classifier over textual mentions.",1 Introduction,[0],[0]
"By using this additional information, we learn a richer, more robust representation, gaining statistical efficiency when predicting similar concepts and aiding the classification of rarer types.",1 Introduction,[0],[0]
"We first validate our methods on the narrow, shallow type system of FIGER, out-performing state-of-the-art methods not incorporating hand-crafted features and matching those that do.
To evaluate on richer datasets and stimulate further research into hierarchical entity/typing prediction with larger and deeper ontologies, we introduce two new human annotated datasets.",1 Introduction,[0],[0]
"The first is MedMentions, a collection of PubMed ab-
stracts in which 246k concept mentions have been annotated with links to the Unified Medical Language System (UMLS) ontology (Bodenreider, 2004), an order of magnitude more annotations than comparable datasets.",1 Introduction,[0],[0]
UMLS contains over 3.5 million concepts in a hierarchy having average depth 14.4.,1 Introduction,[0],[0]
"Interestingly, UMLS does not distinguish between types and entities (an approach we heartily endorse), and the technical details of linking to such a massive ontology lead us to refer to our MedMentions experiments as entity linking.",1 Introduction,[0],[0]
"Second, we present TypeNet, a curated mapping from the Freebase type system into the WordNet hierarchy.",1 Introduction,[0],[0]
"TypeNet contains over 1900 types with an average depth of 7.8.
",1 Introduction,[0],[0]
"In experimental results, we show improvements with a hierarchically-aware training loss on each of the three datasets.",1 Introduction,[0],[0]
"In entity-linking MedMentions to UMLS, we observe a 6% relative increase in accuracy over the base model.",1 Introduction,[0],[0]
"In experiments on entity-typing from Wikipedia into TypeNet, we show that incorporating the hierarchy of types and including a hierarchical loss provides a dramatic 29% relative increase in MAP.",1 Introduction,[0],[0]
"Our models even provide benefits for shallow hierarchies allowing us to match the state-of-art results of Shimaoka et al. (2017) on the FIGER (GOLD) dataset without requiring hand-crafted features.
",1 Introduction,[0],[0]
"We will publicly release the TypeNet and MedMentions datasets to the community to encourage further research in truly fine-grained, hierarchical entity-typing and linking.",1 Introduction,[0],[0]
"Over the years researchers have constructed many large knowledge bases in the biomedical domain (Apweiler et al., 2004; Davis et al., 2008; Chatraryamontri et al., 2017).",2.1 MedMentions,[0],[0]
"Many of these knowledge bases are specific to a particular sub-domain encompassing a few particular types such as genes and diseases (Piñero et al., 2017).
",2.1 MedMentions,[0],[0]
"UMLS (Bodenreider, 2004) is particularly comprehensive, containing over 3.5 million concepts (UMLS does not distinguish between entities and types) defining their relationships and a curated hierarchical ontology.",2.1 MedMentions,[0],[0]
For example LETM1 Protein IS-A Calcium Binding Protein IS-A Binding Protein IS-A Protein IS-A Genome Encoded Entity.,2.1 MedMentions,[0],[0]
"This fact makes UMLS particularly well suited for methods explicitly exploiting hierarchical struc-
ture.",2.1 MedMentions,[0],[0]
Accurately linking textual biological entity mentions to an existing knowledge base is extremely important but few richly annotated resources are available.,2.1 MedMentions,[0],[0]
"Even when resources do exist, they often contain no more than a few thousand annotated entity mentions which is insufficient for training state-of-the-art neural network entity linkers.",2.1 MedMentions,[0],[0]
"State-of-the-art methods must instead rely on string matching between entity mentions and canonical entity names (Leaman et al., 2013; Wei et al., 2015; Leaman and Lu, 2016).",2.1 MedMentions,[0],[0]
"To address this, we constructed MedMentions, a new, large dataset identifying and linking entity mentions in PubMed abstracts to specific UMLS concepts.",2.1 MedMentions,[0],[0]
"Professional annotators exhaustively annotated UMLS entity mentions from 3704 PubMed abstracts, resulting in 246,000 linked mention spans.",2.1 MedMentions,[0],[0]
"The average depth in the hierarchy of a concept from our annotated set is 14.4 and the maximum depth is 43.
",2.1 MedMentions,[0],[0]
"MedMentions contains an order of magnitude more annotations than similar biological entity linking PubMed datasets (Doğan et al., 2014; Wei et al., 2015; Li et al., 2016).",2.1 MedMentions,[0],[0]
"Additionally, these datasets contain annotations for only one or two entity types (genes or chemicals and disease etc.).",2.1 MedMentions,[0],[0]
MedMentions instead contains annotations for a wide diversity of entities linking to UMLS.,2.1 MedMentions,[0],[0]
Statistics for several other datasets are in Table 1 and further statistics are in 2.,2.1 MedMentions,[0],[0]
TypeNet is a new dataset of hierarchical entity types for extremely fine-grained entity typing.,2.2 TypeNet,[0],[0]
"TypeNet was created by manually aligning Freebase types (Bollacker et al., 2008) to noun synsets from the WordNet hierarchy (Fellbaum, 1998), naturally producing a hierarchical type set.
",2.2 TypeNet,[0],[0]
"To construct TypeNet, we first consider all Freebase types that were linked to more than 20 entities.",2.2 TypeNet,[0],[0]
This is done to eliminate types that are either very specific or very rare.,2.2 TypeNet,[0],[0]
"We also remove all Freebase API types, e.g. the [/freebase, /dataworld, /schema, /atom, /scheme, and /topics] domains.
",2.2 TypeNet,[0],[0]
"For each remaining Freebase type, we generate a list of candidate WordNet synsets through a substring match.",2.2 TypeNet,[0],[0]
"An expert annotator then attempted to map the Freebase type to one or more synsets in the candidate list with a parent-of, child-of or equivalence link by comparing the definitions of each synset with example entities of the Freebase type.",2.2 TypeNet,[0],[0]
"If no match was found, the annotator manually formulated queries for the online WordNet API until an appropriate synset was found.",2.2 TypeNet,[0],[0]
"See Table 9 for an example annotation.
",2.2 TypeNet,[0],[0]
Two expert annotators independently aligned each Freebase type before meeting to resolve any conflicts.,2.2 TypeNet,[0],[0]
The annotators were conservative with assigning equivalence links resulting in a greater number of child-of links.,2.2 TypeNet,[0],[0]
"The final dataset contained 13 parent-of, 727 child-of, and 380 equivalence links.",2.2 TypeNet,[0],[0]
"Note that some Freebase types have multiple child-of links to WordNet, making TypeNet, like WordNet, a directed acyclic graph.",2.2 TypeNet,[0],[0]
"We then took the union of each of our annotated Freebase types, the synset that they linked to, and any ancestors of that synset.
",2.2 TypeNet,[0],[0]
We also added an additional set of 614 FB → FB links 4.,2.2 TypeNet,[0],[0]
This was done by computing conditional probabilities of Freebase types given other Freebase types from a collection of 5 million randomly chosen Freebase entities.,2.2 TypeNet,[0],[0]
"The conditional probability P(t2 | t1) of a Freebase type t2 given another Freebase type t1 was calculated as #(t1,t2)#t1 .",2.2 TypeNet,[0],[0]
Links with a conditional probability less than or equal to 0.7 were discarded.,2.2 TypeNet,[0],[0]
"The remaining links were manually verified by an expert annotator and valid links were added to the final dataset, preserving acyclicity.",2.2 TypeNet,[0],[0]
We define a textual mention m as a sentence with an identified entity.,3.1 Background: Entity Typing and Linking,[0],[0]
The goal is then to classify m with one or more labels.,3.1 Background: Entity Typing and Linking,[0],[0]
"For example, we could take the sentence m = “Barack Obama is the President of the United States.”",3.1 Background: Entity Typing and Linking,[0],[0]
with the identified entity string Barack Obama.,3.1 Background: Entity Typing and Linking,[0],[0]
"In the task of entity linking, we want to map m to a specific entity in a knowledge base such as “m/02mjmr” in Freebase.",3.1 Background: Entity Typing and Linking,[0],[0]
"In mention-level typing, we label m with one or more types from our type system T such as tm = {president, leader, politician} (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017).",3.1 Background: Entity Typing and Linking,[0],[0]
"In entity-level typing, we instead consider a bag of mentions Be which are all linked to the same entity.",3.1 Background: Entity Typing and Linking,[0],[0]
"We label Be with te, the set of all types expressed in all m ∈ Be (Yao et al., 2013; Neelakantan and Chang, 2015; Verga et al., 2017; Yaghoobzadeh et al., 2017a).",3.1 Background: Entity Typing and Linking,[0],[0]
Our model converts each mention m to a d dimensional vector.,3.2 Mention Encoder,[0],[0]
This vector is used to classify the type or entity of the mention.,3.2 Mention Encoder,[0],[0]
The basic model depicted in Figure 1 concatenates the averaged word embeddings of the mention string with the output of a convolutional neural network (CNN).,3.2 Mention Encoder,[0],[0]
"The
word embeddings of the mention string capture global, context independent semantics while the CNN encodes a context dependent representation.",3.2 Mention Encoder,[0],[0]
Each sentence is made up of s tokens which are mapped to dw dimensional word embeddings.,3.2.1 Token Representation,[0],[0]
"Because sentences may contain mentions of more than one entity, we explicitly encode a distinguished mention in the text using position embeddings which have been shown to be useful in state of the art relation extraction models (dos Santos et al., 2015; Lin et al., 2016) and machine translation (Vaswani et al., 2017).",3.2.1 Token Representation,[0],[0]
Each word embedding is concatenated with a dp dimensional learned position embedding encoding the token’s relative distance to the target entity.,3.2.1 Token Representation,[0],[0]
"Each token within the distinguished mention span has position 0, tokens to the left have a negative distance from [−s, 0), and tokens to the right of the mention span have a positive distance from (0, s].",3.2.1 Token Representation,[0],[0]
We denote the final sequence of token representations as M .,3.2.1 Token Representation,[0],[0]
The embedded sequence M is then fed into our context encoder.,3.2.2 Sentence Representation,[0],[0]
"Our context encoder is a single layer CNN followed by a tanh non-linearity to produce C. The outputs are max pooled across
time to get a final context embedding, mCNN.
",3.2.2 Sentence Representation,[0],[0]
"ci = tanh(b+ w∑
j=0
W [j]M [i− bw 2 c+ j])
mCNN = max 0≤i≤n−w+1",3.2.2 Sentence Representation,[0],[0]
"ci
Each W [j] ∈",3.2.2 Sentence Representation,[0],[0]
"Rd×d is a CNN filter, the bias b ∈ Rd, M [i] ∈",3.2.2 Sentence Representation,[0],[0]
"Rd is a token representation, and the max is taken pointwise.",3.2.2 Sentence Representation,[0],[0]
"In all of our experiments we set w = 5.
",3.2.2 Sentence Representation,[0],[0]
"In addition to the contextually encoded mention, we create a global mention encoding, mG, by averaging the word embeddings of the tokens within the mention span.
",3.2.2 Sentence Representation,[0],[0]
"The final mention representation mF is constructed by concatenating mCNN and mG and applying a two layer feed-forward network with tanh non-linearity (see Figure 1):
mF = W2 tanh(W1",3.2.2 Sentence Representation,[0],[0]
[ mSFM mCNN ] + b1) + b2,3.2.2 Sentence Representation,[0],[0]
Mention level entity typing is treated as multilabel prediction.,4.1 Mention-Level Typing,[0],[0]
"Given the sentence vector mF, we compute a score for each type in typeset T as:
yj = tj >mF
where tj is the embedding for the jth type in T and yj is its corresponding score.",4.1 Mention-Level Typing,[0],[0]
"The mention is labeled with tm, a binary vector of all types where tmj = 1 if the j
th type is in the set of gold types for m and 0 otherwise.",4.1 Mention-Level Typing,[0],[0]
"We optimize a multi-label binary cross entropy objective:
Ltype(m) =",4.1 Mention-Level Typing,[0],[0]
− ∑ j tmj log yj + (1− tmj ) log(1− yj),4.1 Mention-Level Typing,[0],[0]
"In the absence of mention-level annotations, we instead must rely on distant supervision (Mintz et al., 2009) to noisily label all mentions of entity e with all types belonging to e. This procedure inevitably leads to noise as not all mentions of an entity express each of its known types.",4.2 Entity-Level Typing,[0],[0]
"To alleviate this noise, we use multi-instance multi-label learning (MIML) (Surdeanu et al., 2012) which operates over bags rather than mentions.",4.2 Entity-Level Typing,[0],[0]
"A bag of mentions Be = {m1,m2, . . .",4.2 Entity-Level Typing,[0],[0]
",mn} is the set of
all mentions belonging to entity e.",4.2 Entity-Level Typing,[0],[0]
"The bag is labeled with te, a binary vector of all types where tej = 1 if the j
th type is in the set of gold types for e and 0 otherwise.
",4.2 Entity-Level Typing,[0],[0]
"For every entity, we subsample k mentions from its bag of mentions.",4.2 Entity-Level Typing,[0],[0]
Each mention is then encoded independently using the model described in Section 3.2 resulting in a bag of vectors.,4.2 Entity-Level Typing,[0],[0]
"Each of the k sentence vectors miF is used to compute a score for each type in te:
yij = tj >miF
where tj is the embedding for the jth type in te and yi is a vector of logits corresponding to the ith mention.",4.2 Entity-Level Typing,[0],[0]
"The final bag predictions are obtained using element-wise LogSumExp pooling across the k logit vectors in the bag to produce entity level logits y:
y = log ∑ i exp(yi)
",4.2 Entity-Level Typing,[0],[0]
"We use these final bag level predictions to optimize a multi-label binary cross entropy objective:
Ltype(Be) =",4.2 Entity-Level Typing,[0],[0]
− ∑ j tej log yj + (1− tej) log(1− yj),4.2 Entity-Level Typing,[0],[0]
Entity linking is similar to mention-level entity typing with a single correct class per mention.,4.3 Entity Linking,[0],[0]
"Because the set of possible entities is in the millions, linking models typically integrate an alias table mapping entity mentions to a set of possible candidate entities.",4.3 Entity Linking,[0],[0]
"Given a large corpus of entity linked data, one can compute conditional probabilities from mention strings to entities (Spitkovsky and Chang, 2012).",4.3 Entity Linking,[0],[0]
In many scenarios this data is unavailable.,4.3 Entity Linking,[0],[0]
"However, knowledge bases such as UMLS contain a canonical string name for each of its curated entities.",4.3 Entity Linking,[0],[0]
"State-of-the-art biological entity linking systems tend to operate on various string edit metrics between the entity mention string and the set of canonical entity strings in the existing structured knowledge base (Leaman et al., 2013; Wei et al., 2015).
",4.3 Entity Linking,[0],[0]
"For each mention in our dataset, we generate 100 candidate entities ec = (e1, e2, . . .",4.3 Entity Linking,[0],[0]
", e100) each with an associated string similarity score csim.",4.3 Entity Linking,[0],[0]
See Appendix A.5.1 for more details on candidate generation.,4.3 Entity Linking,[0],[0]
"We generate the sentence representation mF using our encoder and compute a similarity score between mF and the learned embedding
e of each of the candidate entities.",4.3 Entity Linking,[0],[0]
This score and string cosine similarity csim are combined via a learned linear combination to generate our final score.,4.3 Entity Linking,[0],[0]
"The final prediction at test time ê is the maximally similar entity to the mention.
",4.3 Entity Linking,[0],[0]
"φ(m, e)",4.3 Entity Linking,[0],[0]
= α e>mF + β,4.3 Entity Linking,[0],[0]
"csim(m, e)
ê = argmax e∈ec φ(m, e)
We optimize this model by multinomial cross entropy over the set of candidate entities and correct entity e.
Llink(m, ec) =",4.3 Entity Linking,[0],[0]
"− φ(m, e) + log ∑ e′∈ec expφ(m, e′)",4.3 Entity Linking,[0],[0]
Both entity typing and entity linking treat the label space as prediction into a flat set.,5 Encoding Hierarchies,[0],[0]
"To explicitly incorporate the structure between types/entities into our training, we add an additional loss.",5 Encoding Hierarchies,[0],[0]
"We consider two methods for modeling the hierarchy of the embedding space: real and complex bilinear maps, which are two of the state-of-the-art knowledge graph embedding models.",5 Encoding Hierarchies,[0],[0]
Bilinear:,5.1 Hierarchical Structure Models,[0],[0]
"Our standard bilinear model scores a hypernym link between (c1, c2) as:
s(c1, c2) = c1 >Ac2
where A ∈ Rd×d is a learned real-valued nondiagonal matrix and c1 is the child of c2 in the hierarchy.",5.1 Hierarchical Structure Models,[0],[0]
"This model is equivalent to RESCAL (Nickel et al., 2011) with a single IS-A relation type.",5.1 Hierarchical Structure Models,[0],[0]
The type embeddings are the same whether used on the left or right side of the relation.,5.1 Hierarchical Structure Models,[0],[0]
We merge this with the base model by using the parameter A as an additional map before type/entity scoring.,5.1 Hierarchical Structure Models,[0],[0]
"Complex Bilinear: We also experiment with a complex bilinear map based on the ComplEx model (Trouillon et al., 2016), which was shown to have strong performance predicting the hypernym relation in WordNet, suggesting suitability for asymmetric, transitive relations such as those in our type hierarchy.",5.1 Hierarchical Structure Models,[0],[0]
"ComplEx uses complex valued vectors for types, and diagonal complex matrices for relations, using Hermitian inner products (taking the complex conjugate of the second argument, equivalent to treating the right-hand-side
type embedding to be the complex conjugate of the left hand side), and finally taking the real part of the score1.",5.1 Hierarchical Structure Models,[0],[0]
"The score of a hypernym link between (c1, c2) in the ComplEx model is defined as:
s(c1, c2) =",5.1 Hierarchical Structure Models,[0],[0]
"Re(< c1, rIS-A, c2 >) = Re( ∑ k c1krk c̄2k)
= 〈Re(c1),Re(rIS-A),Re(c2)〉 + 〈Re(c1), Im(rIS-A), Im(c2)〉 + 〈Im(c1),Re(rIS-A), Im(c2)〉 − 〈Im(c1), Im(rIS-A),Re(c2)〉
where c1, c2 and rIS-A are complex valued vectors representing c1, c2 and the IS-A relation respectively.",5.1 Hierarchical Structure Models,[0],[0]
Re(z) represents the real component of z and Im(z) is the imaginary component.,5.1 Hierarchical Structure Models,[0],[0]
"As noted in Trouillon et al. (2016), the above function is antisymmetric when rIS-A is purely imaginary.
",5.1 Hierarchical Structure Models,[0],[0]
"Since entity/type embeddings are complex vectors, in order to combine it with our base model, we also need to represent mentions with complex vectors for scoring.",5.1 Hierarchical Structure Models,[0],[0]
"To do this, we pass the output of the mention encoder through two different affine transformations to generate a real and imaginary component:
Re(mF) = WrealmF + breal Im(mF)",5.1 Hierarchical Structure Models,[0],[0]
"= WimgmF + bimg
where mF is the output of the mention encoder, and Wreal, Wimg ∈ Rd×d and breal, bimg ∈ Rd .",5.1 Hierarchical Structure Models,[0],[0]
Learning a hierarchy is analogous to learning embeddings for nodes of a knowledge graph with a single hypernym/IS-A relation.,5.2 Training with Hierarchies,[0],[0]
"To train these embeddings, we sample (c1, c2) pairs, where each pair is a positive link in our hierarchy.",5.2 Training with Hierarchies,[0],[0]
"For each positive link, we sample a set N of n negative links.",5.2 Training with Hierarchies,[0],[0]
"We encourage the model to output high scores for positive links, and low scores for negative links via a binary cross entropy (BCE) loss:
Lstruct = − log σ(s(c1i, c2i))",5.2 Training with Hierarchies,[0],[0]
+,5.2 Training with Hierarchies,[0],[0]
"∑ N log(1− σ(s(c1i, c′2i)))
",5.2 Training with Hierarchies,[0],[0]
L = Ltype/link + γLstruct,5.2 Training with Hierarchies,[0],[0]
"1This step makes the scoring function technically not bilinear, as it commutes with addition but not complex multiplication, but we term it bilinear for ease of exposition.
where s(c1, c2) is the score of a link (c1, c2), and σ(·) is the logistic sigmoid.",5.2 Training with Hierarchies,[0],[0]
"The weighting parameter γ is ∈ {0.1, 0.5, 0.8, 1, 2.0, 4.0}.",5.2 Training with Hierarchies,[0],[0]
The final loss function that we optimize is L.,5.2 Training with Hierarchies,[0],[0]
"We perform three sets of experiments: mentionlevel entity typing on the benchmark dataset FIGER, entity-level typing using Wikipedia and TypeNet, and entity linking using MedMentions.",6 Experiments,[0],[0]
CNN:,6.1 Models,[0],[0]
Each mention is encoded using the model described in Section 3.2.,6.1 Models,[0],[0]
The resulting embedding is used for classification into a flat set labels.,6.1 Models,[0],[0]
Specific implementation details can be found in Appendix A.2.,6.1 Models,[0],[0]
CNN+Complex:,6.1 Models,[0],[0]
The CNN+Complex model is equivalent to the CNN model but uses complex embeddings and Hermitian dot products.,6.1 Models,[0],[0]
Transitive:,6.1 Models,[0],[0]
This model does not add an additional hierarchical loss to the training objective (unless otherwise stated).,6.1 Models,[0],[0]
"We add additional labels to each entity corresponding to the transitive closure, or the union of all ancestors of its known types.",6.1 Models,[0],[0]
This provides a rich additional learning signal that greatly improves classification of specific types.,6.1 Models,[0],[0]
Hierarchy:,6.1 Models,[0],[0]
"These models add an explicit hierarchical loss to the training objective, as described in Section 5, using either complex or real-valued bilinear mappings, and the associated parameter sharing.",6.1 Models,[0],[0]
To evaluate the efficacy of our methods we first compare against the current state-of-art models of Shimaoka et al. (2017).,6.2 Mention-Level Typing in FIGER,[0],[0]
The most widely used type system for fine-grained entity typing is FIGER which consists of 113 types organized in a 2 level hierarchy.,6.2 Mention-Level Typing in FIGER,[0],[0]
"For training, we use the publicly available W2M data (Ren et al., 2016) and optimize the mention typing loss function defined in Section4.1 with the additional hierarchical loss where specified.",6.2 Mention-Level Typing in FIGER,[0],[0]
"For evaluation, we use the manually annotated FIGER (GOLD) data by Ling and Weld (2012).",6.2 Mention-Level Typing in FIGER,[0],[0]
See Appendix A.2 and A.3 for specific implementation details.,6.2 Mention-Level Typing in FIGER,[0],[0]
"In Table 5 we see that our base CNN models (CNN and CNN+Complex) match LSTM models of Shimaoka et al. (2017) and Gupta et al. (2017), the
previous state-of-the-art for models without handcrafted features.",6.2.1 Results,[0],[0]
"When incorporating structure into our models, we gain 2.5 points of accuracy in our CNN+Complex model, matching the overall state of the art attentive LSTM that relied on handcrafted features from syntactic parses, topic models, and character n-grams.",6.2.1 Results,[0],[0]
The structure can help our model predict lower frequency types which is a similar role played by hand-crafted features.,6.2.1 Results,[0],[0]
Next we evaluate our models on entity-level typing in TypeNet using Wikipedia.,6.3 Entity-Level Typing in TypeNet,[0],[0]
"For each entity, we follow the procedure outlined in Section 4.2.",6.3 Entity-Level Typing in TypeNet,[0],[0]
We predict labels for each instance in the entity’s bag and aggregate them into entity-level predictions using LogSumExp pooling.,6.3 Entity-Level Typing in TypeNet,[0],[0]
Each type is assigned a predicted score by the model.,6.3 Entity-Level Typing in TypeNet,[0],[0]
"We then rank these scores and calculate average precision for each of the types in the test set, and use these scores to calculate mean average precision (MAP).",6.3 Entity-Level Typing in TypeNet,[0],[0]
"We evaluate using MAP instead of accuracy which is standard in large knowledge base link prediction tasks (Verga et al., 2017; Trouillon et al., 2016).",6.3 Entity-Level Typing in TypeNet,[0],[0]
"These scores are calculated only over Freebase types, which tend to be lower in the hierarchy.",6.3 Entity-Level Typing in TypeNet,[0],[0]
This is to avoid artificial score inflation caused by trivial predictions such as ‘entity.’,6.3 Entity-Level Typing in TypeNet,[0],[0]
See Appendix A.4 for more implementation details.,6.3 Entity-Level Typing in TypeNet,[0],[0]
Table 6 shows the results for entity level typing on our Wikipedia TypeNet dataset.,6.3.1 Results,[0],[0]
We see that both the basic CNN and the CNN+Complex models perform similarly with the CNN+Complex model doing slightly better on the full data regime.,6.3.1 Results,[0],[0]
"We also see that both models get an improvement when adding an explicit hierarchy loss, even before adding in the transitive closure.",6.3.1 Results,[0],[0]
"The transitive closure itself gives an additional increase
in performance to both models.",6.3.1 Results,[0],[0]
"In both of these cases, the basic CNN model improves by a greater amount than CNN+Complex.",6.3.1 Results,[0],[0]
This could be a result of the complex embeddings being more difficult to optimize and therefore more susceptible to variations in hyperparameters.,6.3.1 Results,[0],[0]
"When adding in both the transitive closure and the explicit hierarchy loss, the performance improves further.",6.3.1 Results,[0],[0]
"We observe similar trends when training our models in a lower data regime with ~150,000 examples, or about 5% of the total data.
",6.3.1 Results,[0],[0]
"In all cases, we note that the baseline models that do not incorporate any hierarchical information (neither the transitive closure nor the hierarchy loss) perform ~9 MAP worse, demonstrating the benefits of incorporating structure information.",6.3.1 Results,[0],[0]
"In addition to entity typing, we evaluate our model’s performance on an entity linking task using MedMentions, our new PubMed / UMLS dataset described in Section 2.1.",6.4 MedMentions Entity Linking with UMLS,[0],[0]
Table 7 shows results for baselines and our proposed variant with additional hierarchical loss.,6.4.1 Results,[0],[0]
"None of these models incorporate transitive clo-
sure information, due to difficulty incorporating it in our candidate generation, which we leave to future work.",6.4.1 Results,[0],[0]
The Normalized metric considers performance only on mentions with an alias table hit; all models have 0 accuracy for mentions otherwise.,6.4.1 Results,[0],[0]
We also report the overall score for comparison in future work with improved candidate generation.,6.4.1 Results,[0],[0]
"We see that incorporating structure information results in a 1.1% reduction in absolute error, corresponding to a ~6% reduction in relative error on this large-scale dataset.
",6.4.1 Results,[0],[0]
Table 8 shows qualitative predictions for models with and without hierarchy information incorporated.,6.4.1 Results,[0],[0]
"Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity.",6.4.1 Results,[0],[0]
"In the first and second example, the baseline model becomes extremely dependent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences).",6.4.1 Results,[0],[0]
This shows that modeling the structure of the entity hierarchy helps the model disambiguate rare entities.,6.4.1 Results,[0],[0]
"In the third example, structure helps the model understand the hierarchical nature of the labels and prevents it from predicting an entity that is overly specific (e.g predicting Interleukin-27 rather than the correct and more general entity IL2 Gene).
",6.4.1 Results,[0],[0]
"Note that, in contrast with the previous tasks, the complex hierarchical loss provides a significant boost, while the real-valued bilinear model does not.",6.4.1 Results,[0],[0]
"A possible explanation is that UMLS is a far larger/deeper ontology than even TypeNet, and the additional ability of complex embeddings to model intricate graph structure is key to realizing gains from hierarchical modeling.",6.4.1 Results,[0],[0]
"By directly linking a large set of mentions and typing a large set of entities with respect to a new ontology and corpus, and our incorporation of structural learning between the many entities and types in our ontologies of interest, our work draws on many different but complementary threads of research in information extraction, knowledge base population, and completion.
",7 Related Work,[0],[0]
"Our structural, hierarchy-aware loss between types and entities draws on research in Knowledge Base Inference such as Jain et al. (2018), Trouillon et al. (2016) and Nickel et al. (2011).",7 Related Work,[0],[0]
"Combining KB completion with hierarchical structure in knowledge bases has been explored in (Dalvi et al., 2015; Xie et al., 2016).",7 Related Work,[0],[0]
"Recently, Wu et al. (2017) proposed a hierarchical loss for text classification.
",7 Related Work,[0],[0]
"Linking mentions to a flat set of entities, often in Freebase or Wikipedia, is a long-standing task in NLP (Bunescu and Pasca, 2006; Cucerzan, 2007; Durrett and Klein, 2014; Francis-Landau et al., 2016).",7 Related Work,[0],[0]
"Typing of mentions at varying levels of granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task.",7 Related Work,[0],[0]
"A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017).
",7 Related Work,[0],[0]
"The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012).",7 Related Work,[0],[0]
"Del Corro et al. (2015) use manually crafted rules and patterns (Hearst patterns (Hearst, 1992), appositives, etc) to automati-
cally match entity types to Wordnet synsets.",7 Related Work,[0],[0]
"Recent work has moved towards unifying these two highly related tasks by improving entity linking by simultaneously learning a fine grained entity type predictor (Gupta et al., 2017).",7 Related Work,[0],[0]
"Learning hierarchical structures or transitive relations between concepts has been the subject of much recent work (Vilnis and McCallum, 2015; Vendrov et al., 2016; Nickel and Kiela, 2017)
",7 Related Work,[0],[0]
"We draw inspiration from all of this prior work, and contribute datasets and models to address previous challenges in jointly modeling the structure of large-scale hierarchical ontologies and mapping textual mentions into an extremely fine-grained space of entities and types.",7 Related Work,[0],[0]
We demonstrate that explicitly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets.,8 Conclusion,[0],[0]
"Additionally, we introduce two new humanannotated datasets: MedMentions, a corpus of 246k mentions from PubMed abstracts linked to the UMLS knowledge base, and TypeNet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets.
",8 Conclusion,[0],[0]
"While this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as Box embeddings (Vilnis et al., 2018) and Poincaré embeddings (Nickel and Kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking.",8 Conclusion,[0],[0]
"Most of all, we are excited to see new techniques from the NLP community using the resources we have presented.",8 Conclusion,[0],[0]
"We thank Nicholas Monath, Haw-Shiuan Chang and Emma Strubell for helpful comments on early drafts of the paper.",9 Acknowledgements,[0],[0]
Creation of the MedMentions corpus is supported and managed by the Meta team at the Chan Zuckerberg Initiative.,9 Acknowledgements,[0],[0]
A pre-release of the dataset is available at http://github.com/chanzuckerberg/ MedMentions.,9 Acknowledgements,[0],[0]
"This work was supported in part by the Center for Intelligent Information Retrieval and the Center for Data Science, in part by the Chan Zuckerberg Initiative under the project
Scientific Knowledge Base Construction., and in part by the National Science Foundation under Grant No. IIS-1514053.",9 Acknowledgements,[0],[0]
"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",9 Acknowledgements,[0],[0]
"A.1 TypeNet Construction
A.2 Model Implementation Details For all of our experiments, we use pretrained 300 dimensional word vectors from Pennington et al. (2014).",A Supplementary Materials,[0],[0]
These embeddings are fixed during training.,A Supplementary Materials,[0],[0]
"The type vectors and entity vectors are all 300 dimensional vectors initialized using Glorot initialization (Glorot and Bengio, 2010).",A Supplementary Materials,[0],[0]
"The number of negative links for hierarchical training n ∈ {16, 32, 64, 128, 256}.
",A Supplementary Materials,[0],[0]
"For regularization, we use dropout (Srivastava et al., 2014) with p ∈ {0.5, 0.75, 0.8} on the sentence encoder output and L2 regularize all learned parameters with λ ∈ {1e-5, 5e-5, 1e-4}.",A Supplementary Materials,[0],[0]
"All our parameters are optimized using Adam (Kingma and Ba, 2014) with a learning rate of 0.001.",A Supplementary Materials,[0],[0]
"We tune our hyper-parameters via grid search and early stopping on the development set.
",A Supplementary Materials,[0],[0]
"A.3 FIGER Implementation Details To train our models, we use the mention typing loss function defined in Section-5.",A Supplementary Materials,[0],[0]
"For models with structure training, we additionally add in the hierarchical loss, along with a weight that is obtained by tuning on the dev set.",A Supplementary Materials,[0],[0]
We follow the same inference time procedure as Shimaoka et al. (2017),A Supplementary Materials,[0],[0]
"For each mention, we first assign the type with the largest probability according to the logits, and then assign additional types based on the condition that their corresponding probability be greater than 0.5.
A.4 Wikipedia Data and Implementation Details
At train time, each training example randomly samples an entity bag of 10 mentions.",A Supplementary Materials,[0],[0]
At test time we classify bags of 20 mentions of an entity.,A Supplementary Materials,[0],[0]
"The dataset contains a total of 344,246 entities mapped to the 1081 Freebase types from TypeNet.",A Supplementary Materials,[0],[0]
We consider all sentences in Wikipedia between 10 and 50 tokens long.,A Supplementary Materials,[0],[0]
"Tokenization and sentence splitting was performed using NLTK (Loper and Bird, 2002).",A Supplementary Materials,[0],[0]
"From these sentences, we considered all entities annotated with a cross-link in Wikipedia that we could link to Freebase and assign types in TypeNet.",A Supplementary Materials,[0],[0]
"We then split the data by entities into a 90-5-5 train, dev, test split.
",A Supplementary Materials,[0],[0]
A.5 UMLS,A Supplementary Materials,[0],[0]
Implementation details,A Supplementary Materials,[0],[0]
We pre,A Supplementary Materials,[0],[0]
-process each string by lowercasing and removing stop words.,A Supplementary Materials,[0],[0]
"We consider ngrams from size 1 to 5 and keep the top 100,000 features and the final vectors are L2 normalized.",A Supplementary Materials,[0],[0]
"For each mention, In our experiments we consider the top 100 most similar entities as the candidate set.
A.5.1 Candidate Generation Details Each mention and each canonical entity string in UMLS are mapped to TFIDF character ngram vectors.",A Supplementary Materials,[0],[0]
We pre-process each string by lowercasing and removing stop words.,A Supplementary Materials,[0],[0]
"We consider ngrams from size 1 to 5 and keep the top 100,000 features and the final vectors are L2 normalized.",A Supplementary Materials,[0],[0]
"For each mention, we calculate the cosine similarity, csim, between the mention string and each canonical entity string.",A Supplementary Materials,[0],[0]
In our experiments we consider the top 100 most similar entities as the candidate set.,A Supplementary Materials,[0],[0]
"Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies.",abstractText,[0],[0]
Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies.,abstractText,[0],[0]
"This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset.",abstractText,[0],[0]
"We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types.",abstractText,[0],[0]
In experiments on all three datasets we show substantial gains from hierarchy-aware training.,abstractText,[0],[0]
Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3100–3109 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3100",text,[0],[0]
"Since 1665, over 50 million scholarly research articles have been published (Jinha, 2010), with approximately 2.5 million new scientific papers coming out each year (Ware and Mabe, 2015).",1 Introduction,[0],[0]
"While this enormous corpus provides us with the ability to conclusively accept or reject hypotheses and yields insight into promising research directions, it is getting harder and harder to extract useful information from the literature in an efficient and timely manner due to its sheer amount.",1 Introduction,[0],[0]
"Therefore, an automatic and intelligent tool to help users locate the information of interest quickly and comprehensively is highly desired.
",1 Introduction,[0],[0]
"When searching for relevant literature for a certain field, investigators first check the abstracts of scientific papers to see whether they match the criterion of interest.",1 Introduction,[0],[0]
"This process can be expedited if the abstracts are structured; that is, if the rhetorical structural elements of scientific abstracts such as purpose, methods, results, and conclusions (American National Standards Institute,
1979) are explicitly stated.",1 Introduction,[0],[0]
"However, even today, a significant portion of scientific abstracts is still unstructured, which causes great difficulty in information retrieval.",1 Introduction,[0],[0]
"In this paper, we develop a machine-learning based approach to automatically categorize sentences in scientific abstracts into rhetorical sections so that the desired information can be efficiently retrieved.
",1 Introduction,[0],[0]
"In a scientific abstract, each sentence can be assigned to a rhetorical structural element sequentially.",1 Introduction,[0],[0]
"This rhetorical structure profiling process can be formulated as a sequential sentence classification task, as the element assignment of any single sentence is greatly associated with the assignments of the surrounding sentences.",1 Introduction,[0],[0]
"This is in contrast to the general sentence classification problem, where each sentence is classified individually and no contextual information can be used.",1 Introduction,[0],[0]
"Previous state-of-the-art methods relied on Conditional Random Fields (CRFs) to take into account the inter-dependence between subsequent labels, which improved joint sentence classification performance by considering the label sequence information.",1 Introduction,[0],[0]
"In this work, we add a bi-directional long short-term memory (bi-LSTM) layer over the representations of individual sentences so that it can encode the contextual content and semantics from preceding and succeeding sentences for better categorical inference of the current one.
",1 Introduction,[0],[0]
"In this work, we present a hierarchical neural network model for the sequential sentence classification task, which we call a hierarchical sequential labeling network (HSLN).",1 Introduction,[0],[0]
"Our model first uses a RNN or CNN layer to individually encode the sentence representation from the sequence of word embeddings, then uses another bi-LSTM layer to take as input the individual sentence representation and output the contextualized sentence representation, subsequently uses a single-hidden-layer feed-forward network to transform the sentence
representation to the probability vector, and finally optimizes the predicted label sequence jointly via a CRF layer.",1 Introduction,[0],[0]
"We evaluate our model on two benchmarking datasets, PubMed RCT (Dernoncourt and Lee, 2017) and NICTA-PIBOSO (Kim et al., 2011), which were both generated from the PubMed database1.",1 Introduction,[0],[0]
"Our key contributions are summarized as follows:
1.",1 Introduction,[0],[0]
"Based on the previous best performing architecture for sequential sentence classification (Dernoncourt et al., 2016), we add one more layer to extract contextual information from surrounding sentences for more accurate prediction of the current one.",1 Introduction,[0],[0]
"Together with the CRF algorithm, this allows us to make use of not only the preceding labels’ information but also the content and semantics of adjacent sentences to infer the label of the target sentence.
",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
We remove the need for a character-based word embedding component without sacrificing performance.,1 Introduction,[0],[0]
"For individual sentence encoding, we propose the use of a CNN module as an alternative to RNN for small datasets, suffering less from over-fitting as evidenced by our experiments.",1 Introduction,[0],[0]
"Moreover, we incorporate attention-based pooling in both RNN and CNN models to further improve the performance.
3.",1 Introduction,[0],[0]
"We adopt dropout with expectation-linear regularization instead of the standard one to reduce the performance gap between training and test phases.
4.",1 Introduction,[0],[0]
"We obtain state-of-the-art results on two datasets for sequential sentence classification in medical abstracts, outperforming the previous best models by at least 2% in terms of F1 scores.",1 Introduction,[0],[0]
Previous systems for sequential sentence classification concentrate on the rhetorical structure analysis of biomedical abstracts.,2 Related Work,[0],[0]
"They are mainly based on naive Bayes (Ruch et al., 2007), support vector machine (SVM) (McKnight and Srinivasan, 2003; Yamamoto and Takagi, 2005; Liu et al., 2013), Hidden Markov Model (HMM) (Lin
1https://www.ncbi.nlm.nih.gov/pubmed/
et al., 2006), and CRF (Kim et al., 2011; Hassanzadeh et al., 2014; Hirohata et al., 2008; Chung, 2009).",2 Related Work,[0],[0]
"All these methods heavily rely on numerous carefully hand-engineered features such as lexical (bag-of-words (BOW)), semantic (hypernyms, synonyms), structural (part of speech (POS) tags, lemmas, orthographic shapes, headings), statistical (statistical distributions of token types) and sequential (sentence position, surrounding features, predicted labels) features.
",2 Related Work,[0],[0]
"In contrast, current emerging artificial neural network (ANN) based models have removed the need for manually selected features; instead, features are self-learned from the token and/or character embeddings.",2 Related Work,[0],[0]
"These deep learning models have revolutionized the natural language processing (NLP) field with state-of-the-art results achieved in various tasks, including the most relevant text classification task (Kim, 2014; Zhang et al., 2016; Conneau et al., 2017; Lai et al., 2015; Joulin et al., 2016; Ma et al., 2015).",2 Related Work,[0],[0]
"Most of these models are built upon deep CNNs or RNNs as well as combinations of them, where CNN is good at extracting local n-gram features while RNN is suitable for sequence modeling.
",2 Related Work,[0],[0]
"The above-mentioned works for short-text classification do not consider any context of sentence semantics in the models, making them underperform in the sequential sentence classification scenario, where surrounding sentences can play a big role in inferring the label of the current sentence.",2 Related Work,[0],[0]
"Recent works that apply deep neural networks to the sequential sentence classification problem include the system proposed by Lee et al. (Lee and Dernoncourt, 2016), where the preceding utterances were used to help classify the current utterance in a dialog into the corresponding dialogue act.",2 Related Work,[0],[0]
"Most recent work from Dernoncourt et al. (Dernoncourt et al., 2016) used a CRF layer to optimize the predicted label sequence, where the preceding labels have influence on determining the current label.",2 Related Work,[0],[0]
This model outperformed the state-of-the-art results on two datasets PubMed RCT and NICTA-PIBOSO for sentence classification in medical abstracts.,2 Related Work,[0],[0]
"Notation We denote scalars in italic lowercase (e.g., k), vectors in bold italic lowercase (e.g., s) and matrices in italic uppercase (e.g., W ).",3 Proposed Model,[0],[0]
Colon notations,3 Proposed Model,[0],[0]
"xi:j and si:j are used to denote the se-
quence of scalars (xi, xi+1, ..., xj) and vectors (si, si+1, ..., sj).
",3 Proposed Model,[0],[0]
"Our model is composed of four components: the word embedding layer, the sentence encoding layer, the context enriching layer, and the label sequence optimization layer.",3 Proposed Model,[0],[0]
In the following sections they will be discussed in detail.,3 Proposed Model,[0],[0]
Given a sentence w =,3.1 Word Embedding Layer,[0],[0]
"[ w1 w2 · · · wN ] comprising N words, this layer maps each word to a real-valued vector as its lexical-semantic representation.",3.1 Word Embedding Layer,[0],[0]
"Word representations are encoded by the column vector in the embedding matrix Wword ∈ Rdw×|V |, where dw is the dimension of the word vector and V is the vocabulary of the dataset.",3.1 Word Embedding Layer,[0],[0]
Each column Wwordi ∈,3.1 Word Embedding Layer,[0],[0]
Rd w is the word embedding vector for the ith word in the vocabulary.,3.1 Word Embedding Layer,[0],[0]
"The word embeddings Wword can be pre-trained on large unlabeled datasets using unsupervised algorithms such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2016).",3.1 Word Embedding Layer,[0],[0]
This layer takes as input the embedding vector of each token in a sentence from the word embedding layer and produces a vector s to encode this sentence.,3.2 Sentence Encoding Layer,[0],[0]
"The sequence of embedding vectors is first processed by a bi-directional RNN (bi-RNN) or CNN layer, similar to the ones used in the text classification before (Kim, 2014; Lee and Dernoncourt, 2016; Liu et al., 2016).",3.2 Sentence Encoding Layer,[0],[0]
This layer outputs a sequence of hidden states h1:N (h ∈ Rd hs ) for a sentence of N words with each hidden state corresponding to a word.,3.2 Sentence Encoding Layer,[0],[0]
"To form the final representation vector s of this sentence, attention-based pooling is used, which can be described using the following equations:
A = softmax(Us tanh(WsH + bs)), (1)
S = AHT , (2) where H = [ h1 h2 · · · hN ] ∈ Rdhs×N , Ws ∈ Rd a×dhs is the transformation matrix for soft alignment,",3.2 Sentence Encoding Layer,[0],[0]
"bs ∈ Rd a
is the bias vector, Us ∈ Rr×da is the token level context matrix used to measure the relevance or importance of each token with respect to the whole sentence, softmax is
performed along the second dimension of its input matrix, and A ∈ Rr×N is the attention matrix.
",3.2 Sentence Encoding Layer,[0],[0]
Here each row of Us is a context vector us ∈,3.2 Sentence Encoding Layer,[0],[0]
Rda and it is expected to reflect an aspect or component of the semantics of a sentence.,3.2 Sentence Encoding Layer,[0],[0]
"To represent the overall semantics of the sentence, we use multiple context vectors to focus on different parts of this sentence.
",3.2 Sentence Encoding Layer,[0],[0]
"Finally, the sentence encoding vector s ∈ Rrdhs is obtained by reshaping the matrix S into a vector.",3.2 Sentence Encoding Layer,[0],[0]
"This layer takes as input the sequence of individual sentence encoding vectors in a given abstract of n sentences obtained from the last sentence encoding layer, with each vector corresponding to a sentence.",3.3 Context Enriching Layer,[0],[0]
"It outputs a new sequence of contextualized sentence encoding vectors, which are enriched with the contextual information from surrounding sentences.",3.3 Context Enriching Layer,[0],[0]
"Specifically, the sequence of individual sentence encoding vectors is input into
a bi-LSTM layer, which produces a sequence of hidden state vectors h′1:n (h′ ∈ Rd hd ) with each corresponding to a sentence.",3.3 Context Enriching Layer,[0],[0]
"Each of these vectors is subsequently input to a feed-forward neural network with only one hidden layer to get the corresponding probability vector r ∈ Rl, which represents the probability that this sentence belongs to each label, where l is the number of labels.",3.3 Context Enriching Layer,[0],[0]
"Within the abstract, the sequence of sentence categories implicitly follows some patterns.",3.4 Label Sequence Optimization Layer,[0],[0]
"For example, the category Results is always followed by Conclusion, and the category Methods is certainly after the Background.",3.4 Label Sequence Optimization Layer,[0],[0]
"Making use of such patterns can boost the classification performance via the CRF algorithm (Lample et al., 2016).",3.4 Label Sequence Optimization Layer,[0],[0]
"Given the sequence of probability vectors r1:n from the last context enriching layer for an abstract of n sentences, this layer outputs a sequence of labels y1:n, where yi represents the predicted label assigned to the ith sentence.
",3.4 Label Sequence Optimization Layer,[0],[0]
"In the CRF algorithm, in order to model dependencies between subsequent labels, we incorporate a matrix T that contains the transition probabilities between two subsequent labels; we define T",3.4 Label Sequence Optimization Layer,[0],[0]
"[i, j] as the probability that a token with label i is followed by a token with the label j.",3.4 Label Sequence Optimization Layer,[0],[0]
"The score of a label sequence y1:n is defined as the sum of the probabilities of individual labels and the transition probabilities:
s(y1:n) = n∑ i=1",3.4 Label Sequence Optimization Layer,[0],[0]
ri(yi),3.4 Label Sequence Optimization Layer,[0],[0]
+ n∑ i=2 T,3.4 Label Sequence Optimization Layer,[0],[0]
"[yi−1, yi].",3.4 Label Sequence Optimization Layer,[0],[0]
"(3)
The score in the above equation can be transformed into the probability of a certain label sequence by taking a softmax operation over all possible label sequences:
p(y1:n) =",3.4 Label Sequence Optimization Layer,[0],[0]
"es(y1:n)∑
ŷ1:n∈Y e s(ŷ1:n)
, (4)
where Y denotes the set of all possible label sequences.",3.4 Label Sequence Optimization Layer,[0],[0]
"During the training phase, the objective is to maximize the probability of the gold label sequence.",3.4 Label Sequence Optimization Layer,[0],[0]
"In the testing phase, given an input sequence, the corresponding sequence of predicted labels is chosen as the one that maximizes the score, computed via the Viterbi algorithm (Forney, 1973).",3.4 Label Sequence Optimization Layer,[0],[0]
"We evaluate our model on two sources of benchmarking datasets on medical scientific abstracts, where each sentence of the abstract is annotated with one label that is associated with the rhetorical structure.",4.1 Datasets,[0],[0]
"Table 1 summarizes the statistics of the two datasets.
",4.1 Datasets,[0],[0]
"NICTA-PIBOSO This dataset2 was shared from the ALTA 2012 Shared Task (Amini et al., 2012), the goal of which is to build automatic sentence classifiers that can map the sentences from biomedical abstracts into a set of pre-defined categories for Evidence-Based Medicine (EBM).
",4.1 Datasets,[0],[0]
PubMed RCT,4.1 Datasets,[0],[0]
"This new dataset was curated by (Dernoncourt and Lee, 2017)3 and is currently the largest dataset for sequential sentence classification.",4.1 Datasets,[0],[0]
"It is based on the PubMed database of biomedical literature and each sentence of each abstract is labeled with its role in the abstract using one of the following classes: background, objective, method, result, and conclusion.",4.1 Datasets,[0],[0]
Table 2 presents an example abstract comprising structured sentences with their annotated labels.,4.1 Datasets,[0],[0]
"For both datasets, test performance is assessed on the training epoch with best validation performance and F1 scores (weighted average by support (the number of true instances for each label)) are reported as the results.
",4.2 Training Settings,[0],[0]
"The token embeddings were pre-trained on a large corpus combining Wikipedia, PubMed, and PMC texts (Moen and Ananiadou, 2013) using the word2vec tool4 (denoted as “Word2vecwiki+P.M.”).",4.2 Training Settings,[0],[0]
They are fixed during the training phase to avoid over-fitting.,4.2 Training Settings,[0],[0]
"We also tried other types of word embeddings, such as the word2vec embeddings pre-trained on the Google News dataset5 (denoted as “Word2vecNews”), word2vec embeddings pre-trained on the Wikipedia corpus6 (denoted as “Word2vecwiki”), GloVe embeddings pre-trained on the cor-
2This dataset can be found online at https://www.kaggle.com/c/alta-nicta-challenge2
3This dataset can be downloaded from https://github.com/Franck-Dernoncourt/pubmed-rct
4The word vectors can be downloaded at http://bio.nlplab.org/
5https://code.google.com/archive/p/word2vec/ 6https://github.com/jind11/word2vec-on-wikipedia
pus of Wikipedia 2014 + Gigaword 57 (denoted as “Glove-wiki”), fastText embeddings pre-trained on Wikipedia8 (denoted as “FastText-wiki”), and fastText embeddings initialized with the standard GloVe Common Crawl embeddings and then finetuned on PubMed abstracts plus MIMIC-III notes (denoted as “FastText-P.M.+MIMIC”).",4.2 Training Settings,[0],[0]
"The comparison results are summarized in the next section.
",4.2 Training Settings,[0],[0]
"The model is trained using the Adam optimization method (Kingma and Ba, 2014).",4.2 Training Settings,[0],[0]
The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch.,4.2 Training Settings,[0],[0]
"For regularization, dropout (Srivastava et al., 2014) is applied to each layer.",4.2 Training Settings,[0],[0]
"For the version of dropout used in practice (e.g., the dropout function implemented in the TensorFlow and Pytorch libraries), the model ensemble generated by dropout in the training phase is approximated by a single model with scaled weights in the inference phase, resulting in a gap between training and inference.",4.2 Training Settings,[0],[0]
"To reduce this gap, we adopted the dropout with expectation-linear regularization introduced by Ma et al. (2016) to explicitly control the inference gap and thus improve the generaliza-
7http://nlp.stanford.edu/data/glove.6B.zip 8https://github.com/facebookresearch/fastText/blob/master/
pretrained-vectors.md
tion performance.",4.2 Training Settings,[0],[0]
Hyperparameters were optimized via grid search based on the validation set and the best configuration is shown in Table 3.,4.2 Training Settings,[0],[0]
"The window sizes of the CNN encoder in the sentence encoding layer are 2, 3, 4 and 5.",4.2 Training Settings,[0],[0]
The RNN encoder in the sentence encoding layer is set as LSTM for the PubMed datasets and gated recurrent unit (GRU) for the NICTA-PIBOSO dataset.,4.2 Training Settings,[0],[0]
Code for this work is available online9.,4.2 Training Settings,[0],[0]
"Table 4 compares our model against the best performing models in the literature (Dernoncourt et al., 2016; Liu et al., 2013).",5 Results and Discussion,[0],[0]
There are two variants of our model in terms of different implementations of the sentence encoding layer: the model that uses bi-RNN to encode the sentence is called HSLN-RNN; while the model that uses the CNN module is named HSLN-CNN.,5 Results and Discussion,[0],[0]
We have evaluated both model variants on all datasets.,5 Results and Discussion,[0],[0]
"And as evidenced by Table 4, our best model can improve the F1 scores by 2%-3% in absolute number compared with the previous best published results for all
9https://github.com/jind11/HSLN-Joint-SentenceClassification
datasets.",5 Results and Discussion,[0],[0]
"For the PubMed 20k and 200k datasets, our HSLN-RNN model achieves better results; however, for the NICTA dataset, the HSLN-CNN model performs better.",5 Results and Discussion,[0],[0]
"This makes sense because the CNN sentence encoder has fewer parameters to be optimized, thus the HSLN-CNN model is less likely to over-fit in a smaller dataset such as NICTA.",5 Results and Discussion,[0],[0]
"With sufficient data, however, the increased capacity of the HSLN-RNN model offers performance benefits.",5 Results and Discussion,[0],[0]
"To be noted, this performance gap between RNN and CNN sentence encoder gets larger as the dataset size increases from 20k to 200k for the PubMed dataset.
",5 Results and Discussion,[0],[0]
"Model PubMed
NICTA 20k 200k
Best Published Marco Lui (Lui, 2012)
- - 82.0 bi-ANN (Dernoncourt et al., 2016) 90.0 91.6 82.7",5 Results and Discussion,[0],[0]
"Our Models HSLN-CNN 92.2 92.8 84.7 HSLN-RNN 92.6 93.9 84.3
Table 4:",5 Results and Discussion,[0],[0]
Comparison of F1 scores (weighted average by support (the number of true instances for each label)) between our model and the best published methods.,5 Results and Discussion,[0],[0]
"The presented results of our model are evaluated on the test set of the run with the highest F1 score on the validation set.
",5 Results and Discussion,[0],[0]
"Table 5 presents the ablation analysis of our model (on the PubMed 20k dataset), where we remove one component at a time and quantify the performance drop (reported on F1 scores).",5 Results and Discussion,[0],[0]
"As can be seen from Table 5, our HSLN-CNN model uni-
formly suffers a little more from the component removal than the HSLN-RNN model, indicating that the HSLN-RNN model is more robust.",5 Results and Discussion,[0],[0]
"When the context enriching layer is removed, both models experience the most significant performance drop and can only be on par with the previous stateof-the-art results, strongly demonstrating that this proposed component is the key to the performance improvement of our model.",5 Results and Discussion,[0],[0]
"Furthermore, even without the label sequence optimization layer, our model still significantly outperforms the best published methods that are empowered by this layer, indicating that the context enriching layer we propose can help optimize the label sequence by considering the context information from the surrounding sentences.",5 Results and Discussion,[0],[0]
"Last but not the least, the dropout regularization and attention-based pooling components we add to our system can help further improve the model in a limited extent.
",5 Results and Discussion,[0],[0]
"Table 6 and 7 detail the results of classification for each label in terms of performance scores (precision, recall and F1) and confusion matrix, respectively (for our HSLN-RNN model trained on the PubMed 20k dataset).",5 Results and Discussion,[0],[0]
"These show that the classifier is very good at predicting the labels Methods, Results and Conclusions, whereas the greatest difficulty the classifier has is in distinguishing Background sections from Objectives sections.",5 Results and Discussion,[0],[0]
"One fifth of Background sentences are incorrectly classified as Objectives, while around one forth of Objectives sentences are wrongly assigned to the label of Background.",5 Results and Discussion,[0],[0]
"We conjecture this difficulty mainly comes from the fact that the difference between Background and Ob-
jectives sentences in terms of writing style is less obvious compared with the other sections of the abstract.",5 Results and Discussion,[0],[0]
"Moreover, our model has some difficulty in telling Methods sentences apart from Results sentences.
",5 Results and Discussion,[0],[0]
Table 8 presents a few examples of prediction errors that are produced by our HSLN-RNN model trained on the PubMed 20k dataset.,5 Results and Discussion,[0],[0]
This error analysis suggests that one of the biggest model error sources could be from the debatable gold standard labels of the dataset.,5 Results and Discussion,[0],[0]
"For example, the sentence “Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14% in the general population.”",5 Results and Discussion,[0],[0]
"is indeed introducing the background of the problem (depressive disorders) on which this article is going to focus; however, the gold label classifies it into the Objective category.",5 Results and Discussion,[0],[0]
"For another instance, the sentence “A post hoc analysis was conducted with the use of data from the evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness (escape).”",5 Results and Discussion,[0],[0]
"belongs to the Result label according to the gold standard, but it makes more sense that it should be classified as a Method label.
",5 Results and Discussion,[0],[0]
"Figure 2 presents an example of the transition matrix after the HSLN-RNN model has been trained on the PubMed 20k dataset, which encodes the transition probability between two subsequent labels.",5 Results and Discussion,[0],[0]
It effectively reflects what label is the most likely one that follows the current one.,5 Results and Discussion,[0],[0]
"For example, by comparing the transition scores in the Result row in Figure 2, we can conclude that a sentence pertaining to the Result is typically followed by a sentence pertaining to the Conclusion and is unlikely to be followed by a sentence in the Background category (transition scores of 2.48 vs -5.46), which makes sense.",5 Results and Discussion,[0],[0]
"From this transition matrix, we can figure out the most probable label sequence: Background → Objective → Method → Result → Conclusion, which is also consistent with our expectations.
",5 Results and Discussion,[0],[0]
"In order to test the importance of pretrained word embeddings, we performed experiments with different sets of publicly published word embeddings, as well as our locally curated word embeddings, to initialize our model.",5 Results and Discussion,[0],[0]
Table 9 gives the performance of six different word embeddings for our HSLN-RNN model trained on the PubMed 20k dataset.,5 Results and Discussion,[0],[0]
"According to Table 9, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does.",5 Results and Discussion,[0],[0]
"The combination of Wikipedia and PubMed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the Wikipedia corpus or the PubMed abstracts performs much worse.",5 Results and Discussion,[0],[0]
"Although the dataset we
are using for evaluation is also from PubMed abstracts, using only the PubMed abstracts together with MIMIC notes without the Wikipedia corpus does not guarantee better result (see the “FastTextP.M.+MIMIC” embeddings in Table 9), which may be because the corpus size of PubMed abstracts plus MIMIC notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the Wikipedia.",5 Results and Discussion,[0],[0]
"In this work, we have presented an ANN based hierarchical sequential labeling network to classify sentences that appear sequentially in text.",6 Conclusion,[0],[0]
"We demonstrate that incorporating the contextual information from surrounding sentences to help
classify the current one by using an LSTM layer to sequentially process the encoded sentence representations can improve the overall quality of predictions.",6 Conclusion,[0],[0]
Our model outperforms the state-of-theart results by 2%-3% on two datasets for sequential sentence classification in medical abstracts.,6 Conclusion,[0],[0]
"We expect that our proposed model can be generalized to any problem that is related to sequential sentence classification, such as the paragraph-level sequential sentence categorization in full-text articles for better text mining and document retrieval (Westergaard et al., 2018).",6 Conclusion,[0],[0]
"Although the whole PubMed database contains over 2 million abstracts with part of them accompanied by full-text articles, only a small fraction of them are structured and contain the label information utilized in this work.",7 Future Work,[0],[0]
"We plan to make use of the rest unannotated abstracts or full texts to pre-train our model and then fine tune it to the target annotated datasets inspired by the work from (Howard and Ruder, 2018) so that the performance can be further boosted.",7 Future Work,[0],[0]
This work was supported by funding grant U54HG007963 from National Human Genome Research Institute (NHGRI).,Acknowledgments,[0],[0]
Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear.,abstractText,[0],[0]
"This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance.",abstractText,[0],[0]
"In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence.",abstractText,[0],[0]
Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.,abstractText,[0],[0]
Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 889–898 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
889",text,[0],[0]
"Story-telling is on the frontier of current text generation technology: stories must remain thematically consistent across the complete document, requiring modeling very long range dependencies; stories require creativity; and stories need a high level plot, necessitating planning ahead rather than word-by-word generation (Wiseman et al., 2017).
",1 Introduction,[0],[0]
"We tackle the challenges of story-telling with a hierarchical model, which first generates a sentence called the prompt describing the topic for the story, and then conditions on this prompt when generating the story.",1 Introduction,[0],[0]
Conditioning on the prompt or premise makes it easier to generate consistent stories because they provide grounding for the overall plot.,1 Introduction,[0],[0]
"It also reduces the tendency of standard sequence models to drift off topic.
",1 Introduction,[0],[0]
"We find that standard sequence-to-sequence (seq2seq) models (Sutskever et al., 2014) applied to hierarchical story generation are prone to degenerating into language models that pay little attention to the writing prompt (a problem that has been noted in other domains, such as dialogue response generation (Li et al., 2015a)).",1 Introduction,[0],[0]
"This failure is due to the complex and underspecified dependencies between the prompt and the story, which are much harder to model than the closer dependencies required for language modeling (for example, consider the subtle relationship between the first sentence and prompt in Figure 1).
",1 Introduction,[0],[0]
"To improve the relevance of the generated story to its prompt, we introduce a fusion mechanism (Sriram et al., 2017) where our model is trained on top of an pre-trained seq2seq model.",1 Introduction,[0],[0]
"To improve over the pre-trained model, the second model must focus on the link between the prompt and the story.",1 Introduction,[0],[0]
"For the first time, we show that fusion mechanisms can help seq2seq models build dependencies between their input and output.
",1 Introduction,[0],[0]
Another major challenge in story generation is the inefficiency of modeling long documents with standard recurrent architectures—stories contain 734 words on average in our dataset.,1 Introduction,[0],[0]
"We improve efficiency using a convolutional architecture, al-
lowing whole stories to be encoded in parallel.",1 Introduction,[0],[0]
"Existing convolutional architectures only encode a bounded amount of context (Dauphin et al., 2017), so we introduce a novel gated self-attention mechanism that allows the model to condition on its previous outputs at different time-scales.
",1 Introduction,[0],[0]
"To train our models, we gathered a large dataset of 303,358 human generated stories paired with writing prompts from an online forum.",1 Introduction,[0],[0]
"Evaluating free form text is challenging, so we also introduce new evaluation metrics which isolate different aspects of story generation.
",1 Introduction,[0],[0]
Experiments show that our fusion and selfattention mechanisms improve over existing techniques on both automated and human evaluation measures.,1 Introduction,[0],[0]
"Our new dataset and neural architectures allow for models which can creatively generate longer, more consistent and more fluent passages of text.",1 Introduction,[0],[0]
Human judges prefer our hierarchical model’s stories twice as often as those of a nonhierarchical baseline.,1 Introduction,[0],[0]
"We collect a hierarchical story generation dataset1 from Reddit’s WRITINGPROMPTS forum.2 WRITINGPROMPTS is a community where online users inspire each other to write by submitting story premises, or prompts, and other users freely respond.",2 Writing Prompts Dataset,[0],[0]
Each prompt can have multiple story responses.,2 Writing Prompts Dataset,[0],[0]
"The prompts have a large diversity of topic, length, and detail.",2 Writing Prompts Dataset,[0],[0]
"The stories must be at least 30 words, avoid general profanity and inappropriate content, and should be inspired by the prompt (but do not necessarily have to fulfill every requirement).",2 Writing Prompts Dataset,[0],[0]
"Figure 1 shows an example.
",2 Writing Prompts Dataset,[0],[0]
We scraped three years of prompts and their associated stories using the official Reddit API.,2 Writing Prompts Dataset,[0],[0]
"We clean the dataset by removing automated bot posts, deleted posts, special announcements, com-
1 www.github.com/pytorch/fairseq 2www.reddit.com/r/WritingPrompts/
ments from moderators, and stories shorter than 30 words.",2 Writing Prompts Dataset,[0],[0]
We use NLTK for tokenization.,2 Writing Prompts Dataset,[0],[0]
The dataset models full text to generate immediately human-readable stories.,2 Writing Prompts Dataset,[0],[0]
"We reserve 5% of the prompts for a validation set and 5% for a test set, and present additional statistics about the dataset in Table 1.
",2 Writing Prompts Dataset,[0],[0]
"For our experiments, we limit the length of the stories to 1000 words maximum and limit the vocabulary size for the prompts and the stories to words appearing more than 10 times each.",2 Writing Prompts Dataset,[0],[0]
We model an unknown word token and an end of document token.,2 Writing Prompts Dataset,[0],[0]
"This leads to a vocabulary size of 19,025 for the prompts and 104,960 for the stories.",2 Writing Prompts Dataset,[0],[0]
"As the dataset is scraped from an online forum, the number of rare words and misspellings is quite large, so modeling the full vocabulary is challenging and computationally intensive.",2 Writing Prompts Dataset,[0],[0]
"The challenges of WRITINGPROMPTS are primarily in modeling long-range dependencies and conditioning on an abstract, high-level prompt.",3 Approach,[0],[0]
"Recurrent and convolutional networks have successfully modeled sentences (Jozefowicz et al., 2016; Dauphin et al., 2017), but accurately modeling several paragraphs is an open problem.",3 Approach,[0],[0]
"While seq2seq networks have strong performance on a variety of problems, we find that they are unable to build stories that accurately reflect the prompts.",3 Approach,[0],[0]
We will evaluate strategies to address these challenges in the following sections.,3 Approach,[0],[0]
"High-level structure is integral to good stories, but language models generate on a strictly-word-byword basis and so cannot explicitly make highlevel plans.",3.1 Hierarchical Story Generation,[0],[0]
We introduce the ability to plan by decomposing the generation process into two levels.,3.1 Hierarchical Story Generation,[0],[0]
"First, we generate the premise or prompt of the story using the convolutional language model from Dauphin et al. (2017).",3.1 Hierarchical Story Generation,[0],[0]
The prompt gives a sketch of the structure of the story.,3.1 Hierarchical Story Generation,[0],[0]
"Second, we use a seq2seq model to generate a story that follows the premise.",3.1 Hierarchical Story Generation,[0],[0]
Conditioning on the prompt makes it easier for the story to remain consistent and also have structure at a level beyond single phrases.,3.1 Hierarchical Story Generation,[0],[0]
"The length of stories in our dataset is a challenge for RNNs, which process tokens sequentially.",3.2 Efficient Learning with Convolutional Sequence-to-Sequence Model,[0],[0]
"To transform prompts into stories, we instead build on the convolutional seq2seq model of Gehring et al. (2017), which uses deep convolutional networks as the encoder and decoder.",3.2 Efficient Learning with Convolutional Sequence-to-Sequence Model,[0],[0]
"Convolutional models are ideally suited to modeling long sequences, because they allow parallelism of computation within the sequence.",3.2 Efficient Learning with Convolutional Sequence-to-Sequence Model,[0],[0]
"In the Conv seq2seq model, the encoder and decoder are connected with attention modules (Bahdanau et al., 2015) that perform a weighted sum of encoder outputs, using attention at each layer of the decoder.",3.2 Efficient Learning with Convolutional Sequence-to-Sequence Model,[0],[0]
"CNNs can only model a bounded context window, preventing the modeling of long-range dependencies within the output story.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"To enable modeling of unbounded context, we supplement the decoder with a self-attention mechanism (Sukhbaatar et al., 2015; Vaswani et al., 2017),
which allows the model to refer to any previously generated words.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"The self-attention mechanism improves the model’s ability to extract long-range context with limited computational impact due to parallelism.
",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Gated Attention: Similar to Vaswani et al. (2017), we use multi-head attention to allow each head to attend to information at different positions.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"However, the queries, keys and values are not given by linear projections but by more expressive gated deep neural nets with Gated Linear Unit (Dauphin et al., 2017) activations.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"We show that gating lends the self-attention mechanism crucial capacity to make fine-grained selections.
",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
Multi-Scale Attention:,3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Further, we propose to have each head operating at a different time scale, depicted in Figure 2.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Thus the input to each head is downsampled a different amount—the first head sees the full input, the second every other input timestep, the third every third input timestep, etc.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
The different scales encourage the heads to attend to different information.,3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"The downsampling operation limits the number of tokens in the attention maps, making them sharper.
",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"The output of a single attention head is given by hL+10:t = Linear ( v(hL0:t−1) (1)
",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"softmax(q(hL0:t)k(hL0:t)>) )
where hL0:t contains the hidden states up to time t
at layer L, and q, k, v are gated downsampling networks as shown in Figure 2.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Unlike Vaswani et al. (2017), we allow the model to optionally attend to a 0 vector at each timestep, if it chooses to ignore the information of past timesteps (see Figure 3).",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
This mechanism allows the model to recover the non-self-attention architecture and avoid attending to the past if it provides only noise.,3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Additionally, we do not allow the self-attention mechanism to attend to the current timestep, only the past.",3.3 Modeling Unbounded Context with Gated Multi-Scale Self-attention,[0],[0]
"Unlike tasks such as translation, where the semantics of the target are fully specified by the source, the generation of stories from prompts is far more open-ended.",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"We find that seq2seq models ignore the prompt and focus solely on modeling the stories, because the local dependencies required for language modeling are easier to model than the subtle dependencies between prompt and story.
",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
We propose a fusion-based approach to encourage conditioning on the prompt.,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
We train a seq2seq model that has access to the hidden states of a pretrained seq2seq model.,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
Doing so can be seen as a type of boosting or residual learning that allows the second model to focus on what the first model failed to learn—such as conditioning on the prompt.,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"To our knowledge, this paper is the first to show that fusion reduces the problem of seq2seq models degenerating into language models that capture primarily syntactic and grammatical information.
",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
The cold fusion mechanism of Sriram et al. (2017) pretrains a language model and subsequently trains a seq2seq model with a gating mechanism that learns to leverage the final hidden layer of the language model during seq2seq training.,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"We modify this approach by combining two seq2seq models as follows (see Figure 4):
gt = σ(W",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
[h Training t ;h Pretrained t ] + b) ht = gt ◦,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"[hTrainingt ;hPretrainedt ]
where the hidden state of the pretrained seq2seq model and training seq2seq model (represented by ht) are concatenated to learn gates gt.",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
The gates are computed using a linear projection with the weight matrix W .,3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"The gated hidden layers are combined by concatenation and followed by more fully connected layers with GLU activations (see
Appendix).",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"We use layer normalization (Ba et al., 2016) after each fully connected layer.",3.4 Improving Relevance to Input Prompt with Model Fusion,[0],[0]
"Sequence-to-sequence neural networks (Sutskever et al., 2014) have achieved state of the art performance on a variety of text generation tasks, such as machine translation (Sutskever et al., 2014) and summarization (Rush et al., 2015).",4.1 Story Generation,[0],[0]
"Recent work has applied these models to more open-ended generation tasks, including writing Wikipedia articles (Liu et al., 2018) and poetry (Zhang and Lapata, 2014).
",4.1 Story Generation,[0],[0]
"Previous work on story generation has explored seq2seq RNN architectures (Roemmele, 2016), but has focused largely on using various content to inspire the stories.",4.1 Story Generation,[0],[0]
"For instance, Kiros et al. (2015) uses photos to inspire short paragraphs trained on romance novels, and Jain et al. (2017) chain a series of independent descriptions together into a short story.",4.1 Story Generation,[0],[0]
"Martin et al. (2017) decompose story generation into two steps, first converting text into event representations, then modeling stories as sequences of events before translating back to natural language.",4.1 Story Generation,[0],[0]
"Similarly, Harrison et al. (2017) generate summaries of movies as sequences of events using an RNN, then sample event representations using MCMC.",4.1 Story Generation,[0],[0]
"They find this technique can generate text of the desired genre, but the movie plots
are not interpretable (as the model outputs events, not raw text).",4.1 Story Generation,[0],[0]
"However, we are not aware of previous work that has used hierarchical generation from a textual premise to improve the coherence and structure of stories.",4.1 Story Generation,[0],[0]
Previous work has proposed decomposing the challenge of generating long sequences of text into a hierarchical generation task.,4.2 Hierarchical Text Generation,[0],[0]
"For instance, Li et al. (2015b) use an LSTM to hierarchically learn word, then sentence, then paragraph embeddings, then transform the paragraph embeddings into text.",4.2 Hierarchical Text Generation,[0],[0]
"Yarats and Lewis (2017) generate a discrete latent variable based on the context, then generates text conditioned upon it.",4.2 Hierarchical Text Generation,[0],[0]
Previous work has investigated the integration of language models with seq2seq models.,4.3 Fusion Models,[0],[0]
"The two models can be leveraged together without architectural modifications: Ramachandran et al. (2016) use language models to initialize the encoder and decoder side of the seq2seq model independently, and Chorowski and Jaitly (2016) combine the predictions of the language model and seq2seq model solely at inference time.",4.3 Fusion Models,[0],[0]
Recent work has also proposed deeper integration.,4.3 Fusion Models,[0],[0]
Gulcehre et al. (2015) combined a trained language model with a trained seq2seq model to learn a gating function that joins them.,4.3 Fusion Models,[0],[0]
Sriram et al. (2017) propose training the seq2seq model given the fixed language model then learning a gate to filter the information from the language model.,4.3 Fusion Models,[0],[0]
"We evaluate a number of baselines: (1) Language Models: Non-hierarchical models for story generation, which do not condition on the prompt.",5.1 Baselines,[0],[0]
"We use both the gated convolutional language (GCNN) model of Dauphin et al. (2017) and our additional self-attention mechanism.
",5.1 Baselines,[0],[0]
"(2) seq2seq: using LSTMs and convolutional seq2seq architectures, and Conv seq2seq with decoder self-attention.
",5.1 Baselines,[0],[0]
"(3) Ensemble: an ensemble of two Conv seq2seq with self-attention models.
",5.1 Baselines,[0],[0]
(4) KNN: we also compare with a KNN model to find the closest prompt in the training set for each prompt in the test set.,5.1 Baselines,[0],[0]
"A TF-IDF vector for
each prompt was created using FASTTEXT (Bojanowski et al., 2016) and FAISS (Johnson et al., 2017) was used for KNN search.",5.1 Baselines,[0],[0]
The retrieved story from the training set is limited to 150 words to match the length of generated stories.,5.1 Baselines,[0],[0]
"To train the fusion model, we first pretrain a Conv seq2seq with self-attention model on the WRITINGPROMPTS dataset.",5.2 Fusion Training,[0],[0]
This pretrained model is fixed and provided to the second Conv seq2seq with self-attention model during training time.,5.2 Fusion Training,[0],[0]
The two models are integrated with the fusion mechanism described in Section 3.4.,5.2 Fusion Training,[0],[0]
We implement models with the fairseq-py library in PyTorch.,5.3 Training,[0],[0]
"Similar to Gehring et al. (2017), we train using the Nesterov accelerated gradient method (Sutskever et al., 2013) using gradient clipping (Pascanu et al., 2013).",5.3 Training,[0],[0]
We perform hyperparameter optimization on each of our models by cross-validating with random search on a validation set.,5.3 Training,[0],[0]
We provide model architectures in the appendix.,5.3 Training,[0],[0]
We generate stories from our models using a top-k random sampling scheme.,5.4 Generation,[0],[0]
"At each timestep, the model generates the probability of each word in the vocabulary being the likely next word.",5.4 Generation,[0],[0]
We randomly sample from the k = 10 most likely candidates from this distribution.,5.4 Generation,[0],[0]
"Then, subsequent timesteps generate words based on the previously selected words.",5.4 Generation,[0],[0]
"We find this sampling strategy substantially more effective than beam search, which tends to produce common phrases and repetitive text from the training set (Vijayakumar et al., 2016; Shao et al., 2017).",5.4 Generation,[0],[0]
"Sentences pro-
duced by beam search tend to be short and generic.",5.4 Generation,[0],[0]
"Completely random sampling can introduce very unlikely words, which can damage generation as the model has not seen such mistakes at training time.",5.4 Generation,[0],[0]
"The restriction of sampling from the 10 most likely candidates reduces the risk of these lowprobability samples.
",5.4 Generation,[0],[0]
"For each model, we tune a temperature parameter for the softmax at generation time.",5.4 Generation,[0],[0]
"To ease human evaluation, we generate stories of 150 words and do not generate unknown word tokens.
",5.4 Generation,[0],[0]
"For prompt generation, we use a selfattentive GCNN language model trained with the same prompt-side vocabulary as the sequence-tosequence story generation models.",5.4 Generation,[0],[0]
The language model to generate prompts has a validation perplexity of 63.06.,5.4 Generation,[0],[0]
"Prompt generation is conducted using the top-k random sampling from the 10 most likely candidates, and the prompt is completed when the language model generates the end of prompt token.",5.4 Generation,[0],[0]
We propose a number of evaluation metrics to quantify the performance of our models.,5.5 Evaluation,[0],[0]
"Many commonly used metrics, such as BLEU for ma-
chine translation or ROUGE for summarization, compute an n-gram overlap between the generated text and the human text—however, in our openended generation setting, these are not useful.",5.5 Evaluation,[0],[0]
We do not aim to generate a specific story; we want to generate viable and novel stories.,5.5 Evaluation,[0],[0]
"We focus on measuring both the fluency of our models and their ability to adhere to the prompt.
",5.5 Evaluation,[0],[0]
"For automatic evaluation, we measure model perplexity on the test set and prompt ranking accuracy.",5.5 Evaluation,[0],[0]
"Perplexity is commonly used to evaluate the quality of language models, and it reflects how fluently the model can produce the correct next word given the preceding words.",5.5 Evaluation,[0],[0]
We use prompt ranking to assess how strongly a model’s output depends on its input.,5.5 Evaluation,[0],[0]
Stories are decoded under 10 different prompts—9 randomly sampled prompts and 1 true corresponding prompt—and the likelihood of the story given the various prompts is recorded.,5.5 Evaluation,[0],[0]
We measure the percentage of cases where the true prompt is the most likely to generate the story.,5.5 Evaluation,[0],[0]
"In our evaluation, we examined 1000 stories from the test set for each model.
",5.5 Evaluation,[0],[0]
"For human evaluation, we use Amazon Mechanical Turk to conduct a triple pairing task.",5.5 Evaluation,[0],[0]
We use each model to generate stories based on heldout prompts from the test set.,5.5 Evaluation,[0],[0]
"Then, groups of three stories are presented to the human judges.",5.5 Evaluation,[0],[0]
"The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts.",5.5 Evaluation,[0],[0]
"105 stories per model are grouped into questions, and each question is evaluated by 15 judges.
",5.5 Evaluation,[0],[0]
"Lastly, we conduct human evaluation to evaluate the importance of hierarchical generation for story writing.",5.5 Evaluation,[0],[0]
We use Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt.,5.5 Evaluation,[0],[0]
400 pairs of stories were evaluated by 5 judges each in a blind test.,5.5 Evaluation,[0],[0]
"We analyze the effect of our modeling improvements on the WRITINGPROMPTS dataset.
",6 Results,[0],[0]
"Effect of Hierarchical Generation: We explore leveraging our dataset to perform hierarchical story generation by first using a self-attentive GCNN language model to generate a prompt, and then using a fusion model to write a story given the generated prompt.",6 Results,[0],[0]
"We evaluate the effect of hierarchical generation using a human study in Table 4. 400 stories were generated from a selfattentive GCNN language model, and another 400 were generated from our hierarchical fusion model given generated prompts from a language model.",6 Results,[0],[0]
"In a blind comparison where raters were asked to choose the story they preferred reading, human raters preferred the hierarchical model 67% of the time.
",6 Results,[0],[0]
Effect of new attention mechanism: Table 2 shows the effect of the proposed additions to the self-attention mechanism proposed by Vaswani et al. (2017).,6 Results,[0],[0]
Table 3 shows that deep multi-scale self-attention and fusion each significantly improve the perplexity compared to the baselines.,6 Results,[0],[0]
"In combination these additions to the Conv seq2seq baseline reduce the perplexity by 9 points.
",6 Results,[0],[0]
"Effect of model fusion: Results in Table 3 show that adding our fusion mechanism substantially improves the likelihood of human-generated stories, and even outperforms an ensemble despite having fewer parameters.",6 Results,[0],[0]
We observe in Figure 5 that fusion has a much more significant impact on the topicality of the stories.,6 Results,[0],[0]
"In comparison, ensembling has no effect on people’s ability to associate stories with a prompt, but adding model fusion leads improves the pairing accuracy of the human judges by 7%.",6 Results,[0],[0]
"These results suggest that by training a second model on top of the first, we have encouraged that model to learn the challeng-
ing additional dependencies to relate to the source sequence.",6 Results,[0],[0]
"To our knowledge, these are the first results to show that fusion has such capabilities.
",6 Results,[0],[0]
Comparison with Nearest Neighbours: Nearest Neighbour Search (KNN) provides a strong baseline for text generation.,6 Results,[0],[0]
Figure 5 shows that the fusion model can match the performance of nearest neighbour search in terms of the connection between the story and prompt.,6 Results,[0],[0]
"The real value in our generative approach is that it can produce an unlimited number of stories, whereas KNN can never generalize from its training data.",6 Results,[0],[0]
"To quantify this improvement, Figure 7 plots the relevance of the kth best story to a given prompt; the performance of KNN degrades much more rapidly.",6 Results,[0],[0]
Our proposed fusion model is capable of generating unique text without copying directly from the training set.,7.1 Generation Quality,[0],[0]
"When analyzing 500 150-word generated stories from test-set prompts, the average longest common subsequence is 8.9.",7.1 Generation Quality,[0],[0]
"In contrast, the baseline Conv seq2seq model copies 10.2 words on average and the KNN baseline copies all 150 words from a story in the training set.
",7.1 Generation Quality,[0],[0]
"Figure 8 shows the values of the fusion gates for an example story, averaged at each timestep.",7.1 Generation Quality,[0],[0]
The pretrained seq2seq model acts similarly to a language model producing common words and punctuation.,7.1 Generation Quality,[0],[0]
"The second seq2seq model learns to focus on rare words, such as horned and robe.
",7.1 Generation Quality,[0],[0]
"However, the fusion model has limitations.",7.1 Generation Quality,[0],[0]
Using random sampling to generate can produce errors.,7.1 Generation Quality,[0],[0]
"For example, can’t is tokenized to ca n’t, and the model occasionally produces the first token but misses the second.",7.1 Generation Quality,[0],[0]
"A similar error is after one line of dialogue, the model may move to another line of dialogue without generating a newline token.",7.1 Generation Quality,[0],[0]
A further obstacle is repetition.,7.1 Generation Quality,[0],[0]
"The model focuses frequently on what it has recently produced, which leads to the generation of similar text multiple times.
",7.1 Generation Quality,[0],[0]
"In the generation of prompts using the GCNN language model, we find that prompts are fairly generic compared to human prompts.",7.1 Generation Quality,[0],[0]
"Language models often struggle to model rare words accurately, as the probability distribution over the next word is dominated by more common words.",7.1 Generation Quality,[0],[0]
"This tends to produce similar prompts, particularly at
the start — we see many prompts that start with the man.",7.1 Generation Quality,[0],[0]
"In contrast, many of the human prompts are very unique (e.g. prompting stories in fantasy worlds such as Harry Potter and Game of Thrones) and the language model rarely produces the specific vocabulary required by these settings.",7.1 Generation Quality,[0],[0]
"We analyze the encoder-decoder attention in the fusion model and find that unlike attention maps in machine translation, where each decoder timestep tends to attend to a different word on the encoderside, the attention map for each decoder timestep looks similar and focuses mainly on salient words in the prompt.",7.2 Use of Attention,[0],[0]
We further look at the usage of the self-attention layers within the decoder.,7.2 Use of Attention,[0],[0]
"While they could be leveraged to look at words generated very far in the past, at many timesteps the selfattention focuses on the recent past.",7.2 Use of Attention,[0],[0]
We have collected the first dataset for creative text generation based on short writing prompts.,8 Conclusion,[0],[0]
This new dataset pushes the boundaries of text generation by requiring longer range dependencies and conditioning on an abstract premise.,8 Conclusion,[0],[0]
"Building on this dataset, we show through automatic and human evaluation that novel hierarchical models, self-attention mechanisms and model fusion significantly improves the fluency, topicality, and overall quality of the generated stories.",8 Conclusion,[0],[0]
We explore story generation: creative systems that can build coherent and fluent passages of text about a topic.,abstractText,[0],[0]
We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum.,abstractText,[0],[0]
"Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text.",abstractText,[0],[0]
"We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context.",abstractText,[0],[0]
Experiments show large improvements over strong baselines on both automated and human evaluations.,abstractText,[0],[0]
Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.,abstractText,[0],[0]
Hierarchical Neural Story Generation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3783–3792 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3783",text,[0],[0]
Scripts were originally proposed by Schank and Abelson (1977) as “structures that describe the appropriate sequence of events in a particular context”.,1 Introduction,[0],[0]
"These event sequences define expectations for how common scenarios (such as going to a restaurant) should unfold, thus enabling better language understanding.",1 Introduction,[0],[0]
"Although scripts represented many other factors (roles, entry conditions, outcomes) recent work in script induction (Rudinger et al., 2015; Pichotta and Mooney, 2016; Peng and Roth, 2016) has focused on language modeling (LM) approaches where the “appropriate sequence of events” is the textual order of events (tuples of event predicates and their arguments).",1 Introduction,[0],[0]
"Modeling a distribution of text se-
quences gives the intuitive interpretation of appropriate event sequences being roughly equivalent to probable textual sequences.",1 Introduction,[0],[0]
"We continue with an LM approach, but we tackle two very important LM problems that have not yet been addressed with regards to event sequence modeling.
",1 Introduction,[0],[0]
The first problem to address is that language models tend towards local coherency.,1 Introduction,[0],[0]
"Count based models are restricted by window size and sparse counts, while neural language models are known to rely on the local context for predictions.",1 Introduction,[0],[0]
"Since scripts are meant to describe longer coherent scenarios, this is a major issue.",1 Introduction,[0],[0]
"For example, contradictory events like (he denied charges) and (he pleads guilty) are given high probability in a typical language model.",1 Introduction,[0],[0]
"Our model instead captures these variations with learned latent variables.
",1 Introduction,[0],[0]
The second problem with recent work is that the hierarchical nature of scripts is not explicitly captured.,1 Introduction,[0],[0]
A high level script (like a suspect getting arrested) can branch off into many possible variations.,1 Introduction,[0],[0]
These variations are called the “tracks” of a script.,1 Introduction,[0],[0]
Figure 1 shows a script with two tracks learned by our model.,1 Introduction,[0],[0]
"LM-based approaches often fail to explicitly capture this structure, instead
throwing it all into one big distribution.",1 Introduction,[0],[0]
"This muddies the water for language understanding, making it difficult to tease apart differences like going to a fancy restaurant or a casual restaurant.
",1 Introduction,[0],[0]
"To remedy these problems, we propose a model that captures hierarchical structure via global latent variables.",1 Introduction,[0],[0]
"The latent variables are categorical (representing the various types of scripts and thier possible tracks and variations) and form a tree (or more generally, a DAG)1, thus capturing hierarchical structure with the top (or bottom) levels of the tree representing high (or low) level features of the script.",1 Introduction,[0],[0]
"The top might control for large differences like restaurant vs crime, while the bottom selects between fancy and casual dining.
",1 Introduction,[0],[0]
"The overall model, which we describe below, takes the form of an autoencoder, with an encoder network inferring values of the latents and a decoder conditioned on the latents generating scripts.",1 Introduction,[0],[0]
"We show the usefulness of these latent representations against a prior RNN language model based system (Pichotta and Mooney, 2016) on several tasks.",1 Introduction,[0],[0]
"We additionally evaluate the perplexity of the system against the RNN language model, a task that autoencoder models have typically struggled with (Bowman et al., 2016).",1 Introduction,[0],[0]
"We find that the latent tree reduces model perplexity by a significant amount, possibly indicating the usefulness of the model in a more general sense.",1 Introduction,[0],[0]
"Variational Autoencoders (VAEs, Kingma and Welling (2014)) are generative models which learn latent codes z for the data x by maximizing a lower bound on the data likelihood:
log(p(x))",2.1 Variational Autoencoders,[0],[0]
"≥ Eq(z|x)[p(x|z)]−KL[q(z|x)||p(z)]
VAEs consist of two components: an encoder which parameterizes the latent posterior q(z|x) and a decoder which parameterizes p(x|z).",2.1 Variational Autoencoders,[0],[0]
"The objective function can be made completely differentiable via the reparameterization trick, with the full model resembling an autoencoder and the KL term acting as a regularizer.
While VAEs have been useful in continuous domains, they have been less successful in generating discrete domains whose outputs have lo-
1In this work we only look at linear chains of categorical variables, which is enough to encode trees (such as the one in Figure 1)
cal syntactic regularities.",2.1 Variational Autoencoders,[0],[0]
"Part of this is due to the “posterior collapse” problem (Bowman et al., 2016); when VAEs are equipped with powerful autoregressive decoders, they tend to ignore the latent, collapsing the posterior q(z|x) to the (usually zero-mean Gaussian)",2.1 Variational Autoencoders,[0],[0]
prior p(z).,2.1 Variational Autoencoders,[0],[0]
"By doing this, the model takes no penalty from the KL term, but effectively ignores its encoder.",2.1 Variational Autoencoders,[0],[0]
"Vector Quantized VAEs (VQ-VAEs, van den Oord et al. (2017)) are a recently proposed class of models which both alleviates the posterior collapse problem and allows the model to use a latent space of discrete values.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
In VQ-VAEs the latent z is represented as a categorical variable that can take on K values.,2.2 Vector Quantized Variational Autoencoders,[0],[0]
Each of these values k ∈,2.2 Vector Quantized Variational Autoencoders,[0],[0]
"{1, ...,K} has associated with it a vector embedding ek.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"The posterior of VQ-VAEs are discrete, deterministic, and parameterized as follows:
q(z = k|x)",2.2 Vector Quantized Variational Autoencoders,[0],[0]
= { 1 k=argmini||f(x)− ei||2 0,2.2 Vector Quantized Variational Autoencoders,[0],[0]
"elsewise
where f(x) is a function defined by an encoder network.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"The decoding portion of the network is similar to VAEs, where a decoder parameterizes a distribution p(x|z = k) = g(ek), where g is the decoder network, and ek is the corresponding embedding, which is fed as input to the decoder.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"This process can be seen as a ”quantization” operation mapping the continuous encoder output to the latent embedding it falls closest to, and then feeding this latent embedding (in lieu of the encoder output) to the decoder.
",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"The quantization operator is not differentiable, thus during training, the gradient of the loss with respect to the decoder input is used as an estimation to the gradient of the loss with respect to the encoder output.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"If one assumes a uniform prior over the latents (we do so here), then the KL term in the VAE objective becomes constant and may be ignored.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
"In practice, multiple latent variables z may by used, each with their own (or shared) embeddings space.",2.2 Vector Quantized Variational Autoencoders,[0],[0]
Our goal is to build a model that can generate globally coherent multi-track scripts which allow us to account for the different ways in which a script can unfold.,3 Hierarchical Quantized Autoencoder,[0],[0]
"The main idea behind our approach
is to use a hierarchical latent space which conditions the generation of the scripts.",3 Hierarchical Quantized Autoencoder,[0],[0]
The VQ-VAE models (described earlier) provide a way to model discrete variables in an autoencoding framework while avoiding the posterior collapse problem.,3 Hierarchical Quantized Autoencoder,[0],[0]
"We build on this framework to propose a new HierarchicAl Quantized Autoencoder (HAQAE) model.
",3 Hierarchical Quantized Autoencoder,[0],[0]
"HAQAEs are autoencoders with M latent variables, z0, ..., zM .",3 Hierarchical Quantized Autoencoder,[0],[0]
Each latent variable is categorical taking on K different values.,3 Hierarchical Quantized Autoencoder,[0],[0]
"Like in VQVAEs, every categorical value k for variable z has an associated embeddings ezk.",3 Hierarchical Quantized Autoencoder,[0],[0]
"The latent variables are given a tree structure and the full posterior over all M latents z factorizes as:
q(z|x) = q0(z0|x) M−1∏ i=1",3 Hierarchical Quantized Autoencoder,[0],[0]
"qi(zi|pr(zi), x)
where pr(zi) denotes the parent of zi in the tree.",3 Hierarchical Quantized Autoencoder,[0],[0]
"Since the latent variables are meant to capture the hierarchical categorization of the script, we make the assumption that when a higher level script category (for example, z0) is observed with the actual sequence of events (x), determining the immediate lower level category (z1) is a deterministic operation.",3 Hierarchical Quantized Autoencoder,[0],[0]
"Thus, similar to VQ-VAEs, we parameterize the individual factors of the posterior, qi(zi = k|pr(zi), x), as:{
1 k=argminj ||fi(x, pr(zi))− eij ||2 0",3 Hierarchical Quantized Autoencoder,[0],[0]
"elsewise
where fi(x, pr(zi)) is an encoding function specific to latent zi and eij is the jth value em-
beddings for zi.",3 Hierarchical Quantized Autoencoder,[0],[0]
"The distribution p(x|z) is similarly parameterized by an decoder function g(ze), where ze is the set of corresponding value embedding for each latent zi.",3 Hierarchical Quantized Autoencoder,[0],[0]
We describe the forms of the encoder and decoder in the next section.,3 Hierarchical Quantized Autoencoder,[0],[0]
"During the encoding process, certain parts of the input may provide more evidence towards different parts of the hierarchy.",3.1 HAQAE Encoder and Decoder,[0],[0]
"For example, the event (he ate food) gives evidence to the high level category of a restaurant script, while the more specific event (he drank wine) gives more evidence to the lower level category fancy restaurant.",3.1 HAQAE Encoder and Decoder,[0],[0]
"Thus during encoding, it makes sense to allow each latent to decide which parts of the input to take into consideration, based on its parent latents.",3.1 HAQAE Encoder and Decoder,[0],[0]
"This is accomplished by parameterizing the encoding function for latent zi as an attention over the input x, with the parent of zi (more specifically, the embedding for the parent’s current value) acting as the ‘query’ vector.",3.1 HAQAE Encoder and Decoder,[0],[0]
"As is standard when using attention, the input sequence of events, x = (x1, ...xn), is first encoded into a sequence of hidden states hx = (h1, ..., hn) via a RNN encoder.",3.1 HAQAE Encoder and Decoder,[0],[0]
"The full encoding function for latent zi can thus be written as:
fi(x, pr(zi))",3.1 HAQAE Encoder and Decoder,[0],[0]
"= attn(hx, pr(zi))
",3.1 HAQAE Encoder and Decoder,[0],[0]
"Though any attention formulation is possible, we use the bi-linear attention proposed in Luong et al. (2015) in our implementations.",3.1 HAQAE Encoder and Decoder,[0],[0]
"For the root of the latent tree (z0), which has no parents, we use the averaged value of the encoder vectors hx as the query vector for its attention.
",3.1 HAQAE Encoder and Decoder,[0],[0]
We can define the decoder in a similar fashion.,3.1 HAQAE Encoder and Decoder,[0],[0]
"As is usually done, the distribution p(x|ze) can be defined in an autoregressive manner using a RNN decoder network.",3.1 HAQAE Encoder and Decoder,[0],[0]
"Like the encoding process, different parts of the hierarchy may affect the generation of different parts of the input.",3.1 HAQAE Encoder and Decoder,[0],[0]
"We thus also allow the decoder network g(ze) to be a RNN with a standard attention mechanism over the latent value embeddings, ze.",3.1 HAQAE Encoder and Decoder,[0],[0]
"Since the latent root z0 is supposed to capture the highest level information about the script, we use its embedding value, (passed through an affine transformation and tanh activation) to initialize the hidden state of the decoder.",3.1 HAQAE Encoder and Decoder,[0],[0]
Both encoder and decoder can be trained end to end using the same gradient estimation used for VQ-VAE.,3.1 HAQAE Encoder and Decoder,[0],[0]
The training objective for HAQAE is nearly the same as the VQ-VAE objective proposed in van den Oord et al. (2017).,3.2 Training Objective,[0],[0]
"For a single training example, xi the objective can be written as:
L = − log p(xi|z) + 1
M M∑ j LRj + 1 M M∑ j LCj
where LRj and L C j are the reconstruct and commit loss for the jth latent variable.",3.2 Training Objective,[0],[0]
"As in van den Oord et al. (2017), we let sg(·) stand for a stop gradient operator, such that any term passed to it is treated as a constant.",3.2 Training Objective,[0],[0]
"The reconstruction loss is defined as: LRj = ||sg(fj(x, pr(zj)))− e∗j ||22 where e∗j is the argmin value embedding for zj (for the given input).",3.2 Training Objective,[0],[0]
"The reconstruct loss is how the value embeddings for z are learned, and pushes the value embeddings to be closer to the output of the fi.",3.2 Training Objective,[0],[0]
"The commit loss is defined as:
LCj = β||fj(x, pr(zj))− sg(e∗j )||22
which forces the encoder to push its output closer to some embedding, preventing a situation in which the encoder maps inputs far away from all embeddings.",3.2 Training Objective,[0],[0]
β is a hyperparameter that weighs the commit loss2.,3.2 Training Objective,[0],[0]
Note that the commitment loss may be propogated all the way up through the hierarchy of latent nodes.,3.2 Training Objective,[0],[0]
We allow the latent embeddings to receive updates only from the reconstruct and commit loss (not from the NLL loss).,3.2 Training Objective,[0],[0]
Dataset We use the New York Times Gigaword Corpus as our dataset.,4.1 Dataset and Preprocessing,[0],[0]
The corpus contains a total of around 1.8 million articles.,4.1 Dataset and Preprocessing,[0],[0]
We hold out 4000 articles from the corpus to construct our development (dev) set for hyperparameter tuning and 6000 articles for the test set.,4.1 Dataset and Preprocessing,[0],[0]
The input and output of the model is in the form of an event sequence.,4.1 Dataset and Preprocessing,[0],[0]
"Each event is defined as a 4-tuple, (v, s, o, p), containing the verb, subject, object and preposition.",4.1 Dataset and Preprocessing,[0],[0]
Events without prepositions are given a null token in their preposition slot.,4.1 Dataset and Preprocessing,[0],[0]
"The components of the events (the verb, subject, etc.) are all taken to be individual tokens, and can thus be treated more or
2In our implementations we set β = 0.25
less like normal text.",4.1 Dataset and Preprocessing,[0],[0]
"For example, the events (he played harp), (he touched moon), would be tokenized and given to the model as: played he harp null tup touched he moon null, where null is the null preposition token and tup is a special separation token between events.
",4.1 Dataset and Preprocessing,[0],[0]
"We extract event tuples using Open Information Extraction system Ollie (Mausam et al., 2012).",4.1 Dataset and Preprocessing,[0],[0]
We then group together event tuples for 4 subsequent sentences to create a single event sequence.,4.1 Dataset and Preprocessing,[0],[0]
"We also ignore tuples with common (is, are, be, ...) and repeating predicates.",4.1 Dataset and Preprocessing,[0],[0]
"Finally we have 7123097, 19425, and 28667 event sequences for training, dev, and test dataset respectively.",4.1 Dataset and Preprocessing,[0],[0]
For all the experiments we fix the minimum and maximum sequence lengths to be 8 and 50 respectively.,4.1 Dataset and Preprocessing,[0],[0]
"The HAQAE model we use across all evaluations uses 5 discrete latent variables, structured in the form of a linear chain (thus no variable has more than one child or parent).",4.2 HAQAE Model Details,[0],[0]
"Each variable can initially take on K = 512 values, with all latents having an embeddings dimension of 256.",4.2 HAQAE Model Details,[0],[0]
"The encoder RNN that performs the initial encoding of the event sequence is a bidirectional, single layer RNN with GRU cell (Cho et al., 2014) with a hidden dimension of 512.",4.2 HAQAE Model Details,[0],[0]
The inputs to this encoder are word embeddings derived from the onehot encodings of the tokens in the event sequence.,4.2 HAQAE Model Details,[0],[0]
The embeddings size is 300.,4.2 HAQAE Model Details,[0],[0]
"We find initializing the embeddings with pretrained GloVe (Pennington et al., 2014) vectors to be useful.",4.2 HAQAE Model Details,[0],[0]
The decoder RNN is also a single layer RNN with GRU cells with a hidden dimension of 512 and 300 dimensional (initialized) word embeddings as inputs.,4.2 HAQAE Model Details,[0],[0]
For all experiments we use a vocabulary size of 50k.,4.2 HAQAE Model Details,[0],[0]
"We train the model using Adam (Kingma and Ba, 2014) with a learning rate of 0.0005, and gradient clipping at 5.0.",4.2 HAQAE Model Details,[0],[0]
We find that the training converges around 1.5 epochs on our dataset.,4.2 HAQAE Model Details,[0],[0]
Further details can be found in our implementation3,4.2 HAQAE Model Details,[0],[0]
"We compare the performance of our proposed model against three previous baselines and a modification of our HAQAE model that removes explicit dependencies between latents.
",4.3 Baselines,[0],[0]
RNN Language Model,4.3 Baselines,[0],[0]
For our first baseline system we train a RNN sequence model.,4.3 Baselines,[0],[0]
"This
3github.com/StonyBrookNLP/HAQAE
model is 2 layered GRU cells with hidden size 512 and embedding size 300.",4.3 Baselines,[0],[0]
We use Adam with a learning rate 0.001.,4.3 Baselines,[0],[0]
"To prevent the problem of exploding gradients, we clip the gradients at 10.",4.3 Baselines,[0],[0]
"We use uniform distribution [-0.1, 0.1] for random initialization and biases initialized to zero.",4.3 Baselines,[0],[0]
We also use a dropout of 0.15 on the input and output embedding layers but none on the recurrent layers.,4.3 Baselines,[0],[0]
We initialize the word embedding layer with pretrained Glove vectors as it improved the performance and makes the system directly comparable to HAQAE.,4.3 Baselines,[0],[0]
"We refer to this model as RNNLM in the following sections.
",4.3 Baselines,[0],[0]
RNNLM + Role Embeddings,4.3 Baselines,[0],[0]
We also reproduced the model from Pichotta and Mooney (2016) for comparison.,4.3 Baselines,[0],[0]
This model is similar to the one above except that at each time step the model has an additional role marker input going into it.,4.3 Baselines,[0],[0]
"The marker guides the language model further by indicating what type of input is being currently fed to it: a subject, object, or predicate.",4.3 Baselines,[0],[0]
These role embeddings are learned during training itself.,4.3 Baselines,[0],[0]
Hyperparameters are exactly the same as the RNNLN except that the role embeddings have a dimension of 300.,4.3 Baselines,[0],[0]
We will refer to this model as RNNLM+Role.,4.3 Baselines,[0],[0]
We perform hyperparameter tuning of both the models using the development set.,4.3 Baselines,[0],[0]
We use a vocabulary size of 50k.,4.3 Baselines,[0],[0]
"We trained both the baseline models for 2 epochs on the training set.
",4.3 Baselines,[0],[0]
VAE We report results using a vanilla VAE model similar to the one used in Bowman et al. (2016).,4.3 Baselines,[0],[0]
"The encoder/decoder for the VAE baseline has the same specs as the encoder/decoder for the HAQAE model, with a latent dimension of 300.",4.3 Baselines,[0],[0]
"We use linear KL annealing for the first 15000 steps and 0.5 as the word dropout rate.
",4.3 Baselines,[0],[0]
Hierarchyless HAQAE (NOHIER),4.3 Baselines,[0],[0]
"In order to test the effect of explicitly having a hierarchy in the latent variables, we additionally train another HAQAE model with no explicit hierarchical latent space.",4.3 Baselines,[0],[0]
"The model still has 5 discrete latent variables like our original model, however each of the variables are independent of each other (given the input).",4.3 Baselines,[0],[0]
All five variables are have an attention over the input and take the average of encoder vectors hx as the query vector (as done with the latent root z0 in the original model).,4.3 Baselines,[0],[0]
We additionally designate one of the variables to be used to initialize the hidden state of the decoder.,4.3 Baselines,[0],[0]
"We found the same
training hyperparmaters used in the training of the original model to work well here.",4.3 Baselines,[0],[0]
"As our proposed models are essentially language models, it is natural to evaluate their perplexity scores, which can be viewed as an indirect measure of how well the models can identify scripts.",5.1 Language Modeling: Perplexity,[0],[0]
We compute per-word perplexity and per-word negative log likelihood on the validation and test sets.,5.1 Language Modeling: Perplexity,[0],[0]
We compute these values without considering the end-of-sentence (EOS) token.,5.1 Language Modeling: Perplexity,[0],[0]
Table 1 gives these results.,5.1 Language Modeling: Perplexity,[0],[0]
A good language model should assign low perplexity (high probability) to the validation and test sets.,5.1 Language Modeling: Perplexity,[0],[0]
We observe that HAQAE achieves the minimum negative log likelihood and perplexity scores on both the validation and test sets as compared to the previous RNN-based models.,5.1 Language Modeling: Perplexity,[0],[0]
The result is particularly interesting as autoencoders usually perform worse or comparable to other RNN language models in terms of perplexity (negative log likelihood) as is in the case of the vanilla VAE here; similar observations have also been made in Bowman et al. (2016).,5.1 Language Modeling: Perplexity,[0],[0]
Narrative cloze evaluations of event based language models (LMs) start with a sequence of events as input and test whether the LMs correctly predict a single held-out event.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"The standard narrative cloze task has various issues (Chambers, 2017).",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"In our evaluations we opt instead for the multiple choice variant proposed in GranrothWilding and Clark (2016).
",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
One of our goals is to test if our generative model can produce globally coherent scripts by evaluating their ability to generate coherent event sequences.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"To evaluate this we create a new in-
verse narrative cloze task.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"Instead of being given an event sequence and predicting a single event to go with it, we instead are given only one event and the model must identify the rest of the event sequence.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"The model is identifying sequences of events, not single events.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"We chose this setup because sequences is what we ultimately want, but also because identifying single events resulted in very high scores (around 98% accuracy).",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"This task proved to be more challenging as an evaluation.
",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
We thus score a model based on the probability it assigns to event sequences that begin with a single input event.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
A legitimate event sequence should have high probability compared to an event sequence that is stitched together using two random event sequences.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
We create legitimate event sequences of a fixed length (six) by extracting actual event sequences observed in documents.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"For every legitimate event sequence, we use the first event in the sequence as a seed event.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"Then, we construct detractor event sequences for this seed by appending a different sequence of events (number of events being five) from a randomly chosen document.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
We create five such detractor sequences for every legitimate sequence.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
We rank the six sequences based on the probabilities assigned by the model and then evaluate the accuracy of the top ranked sequence.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
A random model will uniformly choose one among the six sequences and thus will score 16 = 16.60% on the task.,5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"We report results averaged over 2000 sets of legitimate and detractor sequences.
",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"Results in Table 2 show that the HAQAE is substantially better than both RNN LMs and vanilla VAE and similar to the NOHIER model, which shows the usefulness of the quantized embeddings overall as a global representation.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"The comparable results of the NOHIER model on this task might
also indicate that explicitly modeling the hierarchical structure may not be completely necessary if ones only aim is to capture global coherence.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
"The results on the perplexity task do indicate that overall, modeling the hierarchical structure is useful for better prediction.",5.2 Inverse Multiple Choice Narrative Cloze,[0],[0]
Both HAQAE and NOHIER models achieve the best results across all tasks.,5.3 Comparing HAQAE and NOHIER,[0],[0]
"The HAQAE model does better on the perplexity task, while results of the two models on the cloze task are nearly the same.",5.3 Comparing HAQAE and NOHIER,[0],[0]
One clear benefit of explicitly connecting the latent variables together appears to be in the efficiency of the learning.,5.3 Comparing HAQAE and NOHIER,[0],[0]
"The HAQAE model performs comparable or better than the NOHIER model despite (in this case at least) having fewer parameters4.
",5.3 Comparing HAQAE and NOHIER,[0],[0]
The HAQAE model also appears to learn much faster than the NOHIER model.,5.3 Comparing HAQAE and NOHIER,[0],[0]
"We show this in Figure 3, which shows the per-word cross entropy error on the validation set as training progresses.",5.3 Comparing HAQAE and NOHIER,[0],[0]
We observe that the cross entropy error drops much faster in the latter model than the former one.,5.3 Comparing HAQAE and NOHIER,[0],[0]
"Also, the error is always lower for the HAQAE model.
",5.3 Comparing HAQAE and NOHIER,[0],[0]
"One possibility is that the NOHIER model learns similar information as the HAQAE model, but due to its lack of explicit inductive bias, takes a longer time to learn this.",5.3 Comparing HAQAE and NOHIER,[0],[0]
"We leave it as future work to confirm whether this is the case through an in depth study into the properties of the learned discrete latents.
",5.3 Comparing HAQAE and NOHIER,[0],[0]
"4NOHIER has more parameters in our case due to each latent taking a bidirectional encoder state as a query vector, as opposed to taking the parent latent vector as the query",5.3 Comparing HAQAE and NOHIER,[0],[0]
"So far, we’ve evaluated how well the models recognize real textual events (perplexity) and how well the models predict events in scripts (narrative cloze).",5.4 Evaluating Event Schemas,[0],[0]
"This section evaluates the script generation ability of the model, and specifically its ability to capture hierarchical information with different tracks in scripts (e.g., pleading guilty causes different events to occur than does pleading innocent).",5.4 Evaluating Event Schemas,[0],[0]
"In many respects, this section illustrates best the power of HAQAE even though the results are partly subjective.
While we presented two automatic evaluations above, we shift to human judgment to evaluate the scripts themselves.",5.4 Evaluating Event Schemas,[0],[0]
We believe this complements the empirical gains already presented.,5.4 Evaluating Event Schemas,[0],[0]
"The scripts generated by the models were shown to human judges and scored on several metrics.
",5.4 Evaluating Event Schemas,[0],[0]
Most previous work on script induction starts with a seed event and then grows the script based on measures of event proximity or from sampling the distribution with the seed as context.,5.4 Evaluating Event Schemas,[0],[0]
"While effective in generating a bag of events, a major problem in all previous work is that conflicting events are included (sentenced and acquitted).",5.4 Evaluating Event Schemas,[0],[0]
"While the events are related and part of the same high-level script, they should never appear together in an actual instance of a script.
",5.4 Evaluating Event Schemas,[0],[0]
"In order to evaluate our model for this type of knowledge, we instead defined a seed as 2 events: the first event sets the general topic, and the second event starts a specific track in that topic.",5.4 Evaluating Event Schemas,[0],[0]
"For instance, below are two seeds that are intended to generate two tracks for the same script:
“people reported fire” “fire spread in neighborhood”
“people reported fire” “fire spread to forest”
For each seed (2 events in one seed) we select the first 3 events generated by a model conditioned on the seed as context.",5.4 Evaluating Event Schemas,[0],[0]
"The 2 events in a seed thus initialize the latent variable values, which then inform the decoder to generate more events (we choose the first 3).",5.4 Evaluating Event Schemas,[0],[0]
"The strength of our model is that the second event helps select the more specific script track, and to ignore conflicting events in other tracks.
",5.4 Evaluating Event Schemas,[0],[0]
"While generating for both RNNLM+Role and HAQAE models, we additionally enforce a constraint that restricts models from generating events
that have a predicate that has already been generated, as well as events whose subject and object are the same.
",5.4 Evaluating Event Schemas,[0],[0]
We evaluated the RNNLM+Role model from Pichotta and Mooney (2016) against our proposed HAQAE.,5.4 Evaluating Event Schemas,[0],[0]
Each model was given 40 seeds (20 first event each with 2 contrasting second seed) and thus generated 40 scripts.,5.4 Evaluating Event Schemas,[0],[0]
"The annotators were also shown the seeds (2 events), and then asked to rate each three-event sequence for various metrics described below.",5.4 Evaluating Event Schemas,[0],[0]
"Non-Sensical (Sense): Binary, is each event itself non-sensical or understandable?",5.4 Evaluating Event Schemas,[0],[0]
"Event Relevance (Rel): Binary, each event was scored for being relevant or not to the script topic.",5.4 Evaluating Event Schemas,[0],[0]
This ignored whether it was consistent with the seed’s branch.,5.4 Evaluating Event Schemas,[0],[0]
"Coherency with Branch (Coh): 0-2, each event was scored for being coherent with the seed’s specific branch (the second event).",5.4 Evaluating Event Schemas,[0],[0]
"0 means not at all, 1 means somewhat, and 2 means yes.",5.4 Evaluating Event Schemas,[0],[0]
"Branching Uniqueness (BranchU): 0-2, each pair of scripts (both branches of the same topic) were scored for overlap of events.",5.4 Evaluating Event Schemas,[0],[0]
"0 means similar events generated for both, 1 means some similar events, and 2 means distinct.",5.4 Evaluating Event Schemas,[0],[0]
This score is important because some RNN decoders might ignore the second event and focus on the general topic only.,5.4 Evaluating Event Schemas,[0],[0]
"Branching Quality (BranchQ): 0-2, each generated branch was scored for branch quality.",5.4 Evaluating Event Schemas,[0],[0]
"0 means the generated events are not specific to the branch, 1 means some are specific, and 2 means most/all events are specific.",5.4 Evaluating Event Schemas,[0],[0]
"This is the most important score in measuring how well a model captures hierarchical structure and script tracks.
",5.4 Evaluating Event Schemas,[0],[0]
Two expert annotators evaluated the generated event sequences.,5.4 Evaluating Event Schemas,[0],[0]
"In case of disagreements in scores, we also involved a third annotator to resolve these conflicts.",5.4 Evaluating Event Schemas,[0],[0]
"Results for this task are shown in Table 3.
",5.4 Evaluating Event Schemas,[0],[0]
"Both RNNLM and HAQAE produce sensical events, but the HAQAE model outperforms on all other metrics.",5.4 Evaluating Event Schemas,[0],[0]
It produces more relevant and coherent events for the topic at hand (relevance and coherency).,5.4 Evaluating Event Schemas,[0],[0]
"But most important to the goals of this paper, it doubles the RNNLM scores on branching uniqueness and quality.",5.4 Evaluating Event Schemas,[0],[0]
"This is because an RNNLM mostly generates from a bag of events after encoding the seed, but the HAQAE utilizes its latent space to produce branch-specific tracks of event sequences.",5.4 Evaluating Event Schemas,[0],[0]
"Tables 4 show a few such ex-
amples.",5.4 Evaluating Event Schemas,[0],[0]
"We also look at how changing the values of various latent variables change the resulting output, in order to get a small idea as to what properties the variables capture.",5.5 Observations about the Latent Variables,[0],[0]
"We find that the root level variable z0 has the largest effect on the output, and typically corresponds to the domain that the sequence of events belong to.",5.5 Observations about the Latent Variables,[0],[0]
"The non root variables generally change the output on a smaller scale, however we find no correspondence between the level of the variable and the amount of output that is affected upon changing its value.
",5.5 Observations about the Latent Variables,[0],[0]
"One reason for the difficulty of interpreting the variables is that the model conditions on them through attention, thus changing the value of one does not necessarily need to have any effect.
",5.5 Observations about the Latent Variables,[0],[0]
We do find that changing the lower level latents generally leads to the ending/beginning of the sequence changing or the entities of the sequence changing (but still remaining in the same topical domain).,5.5 Observations about the Latent Variables,[0],[0]
"We additionally find that changing
the top level latent may often preserve the overall form of the event sequence, and only transform the topic.",5.5 Observations about the Latent Variables,[0],[0]
We provide examples of these output by our system in Table 5.,5.5 Observations about the Latent Variables,[0],[0]
Scripts were originally proposed by Schank and Abelson (1975) and further expanded upon in Schank and Abelson (1977).,6 Related Work,[0],[0]
The notion of hierarchies in scripts has been studied in the works of Abbott et al. (1985) and Bower et al. (1979).,6 Related Work,[0],[0]
Mooney and DeJong (1985) present an early non probabilistic system for extracting scripts from text.,6 Related Work,[0],[0]
"A highly related work by Miikkulainen (1990) provides an early example of a system explicitly designed to take advantage of the hierarchical nature of scripts, creating a model of scripts based on self organizing maps (Kohonen, 1982).",6 Related Work,[0],[0]
"Interestingly, self organizing maps also utilize vector quantization during learning (albeit in a different way than done here).
",6 Related Work,[0],[0]
"Recent work starting from Chambers and Jurafsky (2008) has focused on learning scripts as
prototypical sequences of events using event cooccurrence.",6 Related Work,[0],[0]
"Further work has framed this task as a language modeling problem (Pichotta and Mooney, 2016; Rudinger et al., 2015; Peng and Roth, 2016).",6 Related Work,[0],[0]
"Other work has looked at learning more structured forms of script knowledge called schemas (Chambers, 2013; Balasubramanian et al., 2013; Nguyen et al., 2015) which focuses on additionally inducing script specific roles to be filled by entities.",6 Related Work,[0],[0]
"In this work we treat event components as separate tokens, though work has also looked into methods for composing this components into a single distributed event representation (Modi and Titov, 2014; Modi, 2016; Weber et al., 2018).",6 Related Work,[0],[0]
"We leave this as possible future work.
",6 Related Work,[0],[0]
"The hierarchical structure of our proposed model is similar to structure of the latent space in other VAE variants (Sonderby et al., 2016; Zhao et al., 2017), with the discrete variables and attentions in our model being the major differences.",6 Related Work,[0],[0]
"Hu et al. (2017) present a VAE based model for controllable text generation, with different latents controlling different aspects of the generated text, but requiring labels for semi-supervision.",6 Related Work,[0],[0]
"Other methods using discrete variables for VAEs have also been proposed (Rolfe, 2017), as have variations in the VQ-VAE learning process (Sonderby et al., 2017)",6 Related Work,[0],[0]
"We proposed a new model, HAQAE, for script learning and generation that is one of the first to model the hierarchy that is inherent in this type of real-world knowledge.",7 Conclusion,[0],[0]
"Previous work has focused on modeling event sequences with language models, while ignoring the problem of contradictory events and different tracks being jumbled together.",7 Conclusion,[0],[0]
"The hierarchical latent space of HAQAE instead attends to the choice points in event sequences, and is able to provide some discrimination between tracks of events.
",7 Conclusion,[0],[0]
"While HAQAE is motivated by the specific need for hierarchies in scripts, it can also be seen as a general event language model.",7 Conclusion,[0],[0]
"As a language model HAQAE has a substantially lower perplexity on our test set than previous RNN models despite HAQAE’s decoder having fewer parameters.
",7 Conclusion,[0],[0]
We also presented a new inverse narrative cloze task that is a multiple-choice selection of event sequences.,7 Conclusion,[0],[0]
"It proved to be a very difficult task with systems producing accuracies in the mid 20%
range.",7 Conclusion,[0],[0]
HAQAE and NOHEIR were the only systems to break 30 with a top accuracy of 34.0%.,7 Conclusion,[0],[0]
"This further illustrates that using a latent space to capture script differences helps identify relevant sequences.
",7 Conclusion,[0],[0]
"To our knowledge, all previous work on script induction has focused on learning single event sequences or bags of events.",7 Conclusion,[0],[0]
"We view our proposed model as a new step toward learning different details about scripts, such as tracks and hierarchies.",7 Conclusion,[0],[0]
"Though the proposed model works well empirically, understanding exactly what is learned in the latent variables is non trivial, and is a possible direction for future work.",7 Conclusion,[0],[0]
This work is supported in part by the National Science Foundation under Grant IIS-1617969.,7.1 Acknowledgements,[0],[0]
Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold.,abstractText,[0],[0]
One of the challenges to learning scripts is the hierarchical nature of the knowledge.,abstractText,[0],[0]
"For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen.",abstractText,[0],[0]
"To capture this type of information, we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables.",abstractText,[0],[0]
"We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value.",abstractText,[0],[0]
This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting.,abstractText,[0],[0]
"Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modelingbased method.",abstractText,[0],[0]
Hierarchical Quantized Representations for Script Generation,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 899–907, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Deep Neural Network (DNN), a neural network with multiple layers, has been proven powerful in many different domains, such as visual recognition (Kavukcuoglu et al., 2010) and speech recognition (Dahl et al., 2012), ever since Hinton et al. (2006) formulated an efficient training method for it.
",1 Introduction,[0],[0]
"In addition to the applications mentioned above, many neural network based methods have also been applied to natural language processing (NLP) tasks with great success.",1 Introduction,[0],[0]
"For example, Collobert et al. (2011) propose a generalized DNN framework for a variety of fundamental NLP tasks, including part-of-speech tagging (postag), chunking, named
∗Contribution during internship at Microsoft Research.
entity recognition (NER), and semantic role labeling.
",1 Introduction,[0],[0]
"DNN is successfully introduced to do wordlevel language modeling, aka., to predict the next word given the history words.",1 Introduction,[0],[0]
Bengio et al. (2003) propose a feedforward neural network to train a word-level language model with a limited n-gram history.,1 Introduction,[0],[0]
"To leverage as much history as possible, Mikolov et al. (2010) apply recurrent neural network to word-level language modeling.",1 Introduction,[0],[0]
"The model absorbs one word each time, keeps the information in a history vector, and predicts the next word with all the word history in the vector.
",1 Introduction,[0],[0]
Word-level language model can only learn the relationship between words in one sentence.,1 Introduction,[0],[0]
"For sentences in one document which talks about one or several specific topics, the words in the next sentence are chosen partially in accordance with the previous sentences.",1 Introduction,[0],[0]
"To model this kind of coherence of sentences, Le and Mikolov (2014) extend word embedding learning network (Mikolov et al., 2013) to learn the paragraph embedding as a fixed-length vector representation for paragraph or sentence.",1 Introduction,[0],[0]
"Li and Hovy (2014) propose a neural network coherence model which employs distributed sentence representation and then predict the probability of whether a sequence of sentences is coherent or not.
",1 Introduction,[0],[0]
"In contrast to the methods mentioned above which learn the word relationship in or between the sentences separately, we propose a hierarchical recurrent neural network language model (HRNNLM) to capture the word sequence across the sentence boundaries at the document level.",1 Introduction,[0],[0]
"HRNNLM is essentially a combination of a wordlevel language model and a sentence-level language model, both of which are recurrent neural networks.",1 Introduction,[0],[0]
"The word-level recurrent neural network follows (Mikolov et al., 2010).",1 Introduction,[0],[0]
"The sentence-level language model is another recurrent neural network that takes sentence represen-
899
tation as input, and predicts the words in the next sentence.",1 Introduction,[0],[0]
"Similar to (Mikolov et al., 2010), the hidden layer in the sentence-level recurrent neural network contains the sentence history information.",1 Introduction,[0],[0]
The hidden layer containing the history information of previous sentences is then linked as an input to the word-level recurrent neural network to predict the next word together with the word-level history vector.,1 Introduction,[0],[0]
"This allows the language model to predict the next word probability distribution beyond the words in the current sentence.
",1 Introduction,[0],[0]
We propose a two-step training approach to optimize the parameters of HRNNLM.,1 Introduction,[0],[0]
"In the first step, we train the sentence-level language models independently .",1 Introduction,[0],[0]
"And then, we connect the hidden layer of the sentence-level language model to the input of word-level RNNLM and train the two models jointly until converged.",1 Introduction,[0],[0]
"At sentence level, we evaluate our model with a sentence ordering task and the result shows our method can outperform a maximum entropy based and another stateof-the-art solution.",1 Introduction,[0],[0]
"At word level, we compare our method with the conventional recurrent neural network based language model, finding the perplexity is reduced significantly.",1 Introduction,[0],[0]
"We also apply our method to rank machine translation output and conduct experiments on a Chinese-English document translation task, yielding a better translation results compared with a state-of-the-art baseline system.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows: Section 2 introduces work related to applying neural network to document modeling and SMT.,1 Introduction,[0],[0]
Section 3 introduces the general framework for document modeling.,1 Introduction,[0],[0]
"Our sentence-level language model and its training is described in Section 4, and the overall HRNNLM and its training is presented in Section 5.",1 Introduction,[0],[0]
Section 6 presents our experiments and their results.,1 Introduction,[0],[0]
"Finally, we conclude in Section 7.",1 Introduction,[0],[0]
"In this section, we introduce previous efforts on applying neural network to model words coherence across sentence boundaries as well as works on improving machine translation performance at discourse level.
",2 Related work,[0],[0]
Mikolov and Zweig (2012) propose a RNNLDA model to implement a context dependent language model.,2 Related work,[0],[0]
"They augment the contextual information into the conventional RNNLM via a realvalued input vector, which is the probability distri-
bution computed by LDA topics for using a block of preceding text.",2 Related work,[0],[0]
They train a Latent Dirichlet Allocation (LDA) model using documents consisting of about 10 sentences long text from Penn Treebank (PTB) training data.,2 Related work,[0],[0]
"Their approach outperforms RNNLM in perplexity on PTB data with a limited context history over topics instead of complete information of preceding sentences.
Le and Mikolov (2014) extend the Continuous Bag-of-Words Model (CBOW) and Continuous Skip-gram Model (Skip-gram) (Mikolov et al., 2013) by introducing a paragraph vector.",2 Related work,[0],[0]
"In their method, the paragraph vector is learnt in a similar way of word vector model, and there will be N × P parameters, if there are N paragraphs and each paragraph is mapped to P dimensions.",2 Related work,[0],[0]
"Different from them, the sentence vectors of our model are learnt with nearly unlimited sentence history based on a RNN framework, in which, bag of words in the sentence are used as input.",2 Related work,[0],[0]
"The sentence vector is no longer related with the sentence id, but only based on the words in the sentence.",2 Related work,[0],[0]
"And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot.
Li and Hovy (2014) implement a neural network model to predict discourse coherence quality in essays.",2 Related work,[0],[0]
"In their work, recurrent (Sutskever et al., 2011) and recursive (Socher et al., 2013) neural networks are both examined to learn distributed sentence representation given pre-trained word embedding.",2 Related work,[0],[0]
The distributed sentence representation is assigned to capture both syntactic and semantic information.,2 Related work,[0],[0]
"With a slide window of the distributed sentence representation, a neural network classifier is trained to evaluate the coherence of the text.",2 Related work,[0],[0]
"Successful as it is in scoring the coherence for a given sequence of sentences, this method is attempted to discriminate the different word order within a sentence.
",2 Related work,[0],[0]
"An attempt of introducing RNN into convolutional neural network (CNN) is investigated by (Xu and Sarikaya, 2014) for spoken language understanding (SLU).",2 Related work,[0],[0]
"To alleviate more contextual information, they apply a CNN with Jordan-type (Jordan, 1997) recurrent connections.",2 Related work,[0],[0]
The recurrent connections send the distribution of the last softmax layer’s output to the current input layer as additional features.,2 Related work,[0],[0]
"Aimed to improve SLU domain classification, their model is essentially a kind of document representation with certain text
information, neglecting the coherence information between sentences.
",2 Related work,[0],[0]
"Following the thread modeling the word sequence relationship within and across sentences, we propose a hierarchical recurrent neural network language model consist of a sentence-level language model and a word-level language model.",2 Related work,[0],[0]
"This overall network is trained to capture the coherence between sentences and predict words sequence with preceding sentence contexts.
",2 Related work,[0],[0]
"For statistical machine translation (SMT) in which we checked out model as a scenario, DNN has also been revealed for certain good results in several components.",2 Related work,[0],[0]
"Yang et al. (2013) adapt and extend the CD-DNN-HMM (Dahl et al., 2012) model to the HMM-based word alignment model.",2 Related work,[0],[0]
"In their method, they use bilingual word embedding to capture the lexical translation information and modeling the context with surrounding words.",2 Related work,[0],[0]
Liu et al. (2014) propose a recursive recurrent neural network (R2NN) for end-to-end decoding to help improve translation quality.,2 Related work,[0],[0]
And Cho et al. (2014) propose a RNN Encoder-Decoder which is a joint recurrent neural network model at the sentence level as conventional SMT decoder does.,2 Related work,[0],[0]
"However, at the discourse level, there is little report on applying DNN to boost the translation result of a document.",2 Related work,[0],[0]
Statistical language model assigns a probability to a natural language sequence.,3 Document Language Modeling,[0],[0]
Conventional language models only focus on the word sequence within a sentence.,3 Document Language Modeling,[0],[0]
"For sentences in one document talking about one or several specific topics, the adjacent sentences should be in a coherent order.",3 Document Language Modeling,[0],[0]
"Therefore, the words in the next sentence are also dependent on the preceding sentences.",3 Document Language Modeling,[0],[0]
"To model the coherence of sentences in the document D, which contains N sentences S1, S2, S3, ..., SN , we need to maximize the objective as follow:
p(D) =p(S1, S2, ..., SN ) =p(S1) ·",3 Document Language Modeling,[0],[0]
"p(S2|S1) · p(S3|S1, S2)
...p(SN |S1, S2, ..., SN−1) (1)
",3 Document Language Modeling,[0],[0]
"For the sentence Sk containing words w1, w2, w3, ..., wT , p(Sk|S1, S2, ..., Sk−1) is defined as:
p(Sk|S1, S2, ..., Sk−1) = p(w1, w2, ..., wT |S1, ..., Sk−1) =",3 Document Language Modeling,[0],[0]
"p(w1|S1, ..., Sk−1) ·",3 Document Language Modeling,[0],[0]
"p(w2|w1, S1, ..., Sk−1) ...",3 Document Language Modeling,[0],[0]
"p(wT |w1, w2, ..., wT−1, S1, ..., Sk−1)
(2) As a special case of approximation to this, classical n-gram language model keep only several words as history, discarding any information across the sentence boundaries.",3 Document Language Modeling,[0],[0]
"Recurrent neural network language model (Mikolov et al., 2010) uses a hidden layer which employs a real-valued vector recurrently as network’s input to keep as many history as possible.",3 Document Language Modeling,[0],[0]
"This makes RNNLM be able to extend for capturing history beyond a sentence.
",3 Document Language Modeling,[0],[0]
"To prevent the potential exponential decay of the history, the history length in RNN can not be too long.",3 Document Language Modeling,[0],[0]
"Here we approximate the history information of previous sentences, p(Sk|S1, S2, ..., Sk−1), by the following:
p(Sk|S1, S2, ..., Sk−1) = p(BoWSk |BoWS1 , ..., BoWSk−1) ·",3 Document Language Modeling,[0],[0]
p(Sk|BoWSk) (3) where BoWSk denotes the bag of words for the sentence Sk.,3 Document Language Modeling,[0],[0]
"The document is thus generated in two steps.
",3 Document Language Modeling,[0],[0]
"• Given the previous sentences BoWS1 , ..., BoWSk−1 (treating them as bag of words here), first generate the words which will show in the next sentence without considering their order with p(BoWSk |BoWS1 , ..., BoWSk−1)
• Generate the words one by one with p(Sk|BoWSk).
",3 Document Language Modeling,[0],[0]
"The first phase actually completes sentence-level language modeling, and the second addresses the word-level language modeling.",3 Document Language Modeling,[0],[0]
"Because recurrent neural network has a natural advantage in processing sequential data, we investigate how to model the whole process under a unified framework of recurrent neural network.",3 Document Language Modeling,[0],[0]
"In this section, we describe how to leverage recurrent neural network for sentence-level language modeling.",4 Sentence-level Language Model,[0],[0]
"Mikolov et al. (2010) demonstrate a recurrent neural network language model (RNNLM)
for word ordering.",4 Sentence-level Language Model,[0],[0]
"It overcomes the limitations of classical language model in capturing only a fixedlength history, yielding a significant performance improvements in terms of perplexity reduction and speech recognition accuracy.",4 Sentence-level Language Model,[0],[0]
"Here we adept this framework for a RNN based sentence-level language modeling, i.e. RNNSLM.",4 Sentence-level Language Model,[0],[0]
"A conventional language model reads a word each time, keeps several words as history and then predict the probability distribution of the next word.",4.1 Model,[0],[0]
"Similar to this, our sentence-level language model reads a sentence which is a bag of words representation.",4.1 Model,[0],[0]
And then it stores the sentence history which captures coherence of sentences in a realvalued history vector.,4.1 Model,[0],[0]
"With the history vector, our model can predict which words are most likely to appear in the next sentence.",4.1 Model,[0],[0]
"All these will be modeled by a recurrent neural network.
",4.1 Model,[0],[0]
"As shown in Figure 1, similar to the conventional recurrent neural network, for the sentence j, our network has two input layers xsj and hsj−1.",4.1 Model,[0],[0]
"xsj is the current sentence representation, and hsj−1 is the history information vector before sentence",4.1 Model,[0],[0]
"j. The model has a hidden layer hsj , which will combine the history information of hsj−1 and the current sentence input xsj , and an output layer ysj+1, which generates the probabilities of the words in the sentence j + 1.",4.1 Model,[0],[0]
"The layers are computed as follows:
hsj = f(Us · hsj−1 + Ws · xsj) (4) ysj+1 = g(Vs · hsj) (5)
where Ws, Us and Vs denote the weight matrix.
f(z) is a HTanh function:
f(zj) =  −1 zj < −1",4.1 Model,[0],[0]
z −1,4.1 Model,[0],[0]
"< zj < 1 1 zj > 1
(6)
and g(z) is a softmax function:
g(zj) = ezj∑",4.1 Model,[0],[0]
"k e zk (7)
",4.1 Model,[0],[0]
"The output layer ysj+1 is a 1×V vector that represents probability distribution of words in the next sentence given the current sentence xsj and previous history hsj−1, where V denotes vocabulary size.
",4.1 Model,[0],[0]
"To emphasize coherence between the adjacent sentences, we further add some bigram-like bag of words feature to the output layer.",4.1 Model,[0],[0]
"As mentioned in (Mikolov, 2012), this is kind of maximum entropy feature which can be derived by a two-layer neural network.",4.1 Model,[0],[0]
Some experiments show that perplexity significantly decreases after adding these features.,4.1 Model,[0],[0]
"Following (Mikolov, 2012), where, the maximum entropy bigram features are added to our RNNSLM by a direct connection between the feature input array and output layer ysj+1.",4.1 Model,[0],[0]
"Following (Mahoney, 2000), we map bigram maximum entropy features to a fixed-length array to reduce the memory complexity of direct connections with feature hashing.",4.1 Model,[0],[0]
"Then the output layer can be computed as follow:
ysj+1(t)",4.1 Model,[0],[0]
"= g(Vs(t) · hsj + ∑
w∈xsj Dhash(w,t))
(8)
where (t) denotes the t-th row of a vector or a matrix.",4.1 Model,[0],[0]
"D denotes that the hash array contains feature weights and hash(wi, wj) denotes the hash function for mapping bigram features to a fixed-length array.",4.1 Model,[0],[0]
"For a output ysj+1, multiple connections may be activated according to the words in sentence xsj .",4.1 Model,[0],[0]
The training objective of our RNNSLM is to find the best parameters for predicting the words of next sentence.,4.2 Training,[0],[0]
"Formally, given the next sentence Sk containing words w1, w2, w3, ..., wT .",4.2 Training,[0],[0]
"The training objective according to (Mikolov et al.,
2013) can be denoted by:
log(p(BoWSk |BoWS1 , ..., BoWSk−1))
",4.2 Training,[0],[0]
= 1 T T∑ t=1,4.2 Training,[0],[0]
"logp(wt|BoWS1 , ..., BoWSk−1) (9)
",4.2 Training,[0],[0]
"For weight matrix Ws, Us, Vs and hash feature weight D, the parameter are trained similar to the conventional recurrent neural network.",4.2 Training,[0],[0]
"The learning rate α is set to 0.1 at the start of the training as suggested in (Mikolov et al., 2010).",4.2 Training,[0],[0]
"After each epoch, it can be determined by the training loss of network.",4.2 Training,[0],[0]
"If the loss decreases significantly, training continues with the same learning rate.",4.2 Training,[0],[0]
"Otherwise, if the loss increases, the training will be executed with a new learning rate α/2.",4.2 Training,[0],[0]
The training process will be terminated after about 30 epochs.,4.2 Training,[0],[0]
All elements in weight matrix,4.3 Initialization,[0],[0]
Ws and Us are initialized by randomly sampling from a uniform distribution,4.3 Initialization,[0],[0]
"[− 1K1 , 1K1 ], where K1 is the size of the input layer.",4.3 Initialization,[0],[0]
Elements in weight matrix Vs are initialized by randomly sampling from a uniform distribution,4.3 Initialization,[0],[0]
"[− 1K2 , 1K2 ], where K2 denotes the size of the hidden layer.",4.3 Initialization,[0],[0]
"The hash feature weight array D is initialized as 0.
",4.3 Initialization,[0],[0]
"For the initialization of hs0, it can be set to a vector of the same values, which is 0.1.",4.3 Initialization,[0],[0]
"In the previous section, we propose a RNNSLM which models the coherence between sentences but ignores the word sequence within a sentence.",5 Hierarchical Recurrent Neural Network,[0],[0]
"Ideally, a perfect document model should not only capture the information between sentences but also the information with sentence.",5 Hierarchical Recurrent Neural Network,[0],[0]
So we propose a hierarchical recurrent neural network language model (HRNNLM) to fulfill this issue.,5 Hierarchical Recurrent Neural Network,[0],[0]
A hierarchical recurrent neural network consists of two independent recurrent neural network.,5.1 Model,[0],[0]
"For a conventional word-level language model, it predict the next word only using the word history within the sentence.",5.1 Model,[0],[0]
"To capture the longer history, we integrate the sentence history into the word-level language model from sentence-level language model, which forms a hierarchical recurrent neural network.
",5.1 Model,[0],[0]
"As illustrated Figure 2, the upper part is the unfolded illustration of conventional recurrent neural network based language model.",5.1 Model,[0],[0]
It takes one word wi each time with the previous history information hwi−1 together and predicts the probability of the next word p(wi+1) with the information kept in the history vector hwi.,5.1 Model,[0],[0]
"The lower part is our RNNSLM, which takes the bag of words representation of a sentence xsj each time with the history information of previous sentences hsj−1 together and predicts the bag of words in the next sentence p(sj+1) with the information kept in hsj .
",5.1 Model,[0],[0]
We integrate these two recurrent neural networks together by adding connections between the sentence-level history vector hsj−1 and word level history vector hwi.,5.1 Model,[0],[0]
"So while predicting the next word wi+1 of the current sentence, our model will consider the current word wi, history of previous sentences hsj−1 and history of previous words hwi−1.",5.1 Model,[0],[0]
"The new word-level history vector hwi is computed as:
hwi = f(Uw · hwi−1 + Ww · xwi + Usw · hsj−1) (10)
where f(z) is a HTanh function.",5.1 Model,[0],[0]
"For HRNNLM, we also add a bigram hash feature, similar as we do for RNNSLM.",5.1 Model,[0],[0]
The HRNNLM can be trained from scratch following Mikolov et al. (2010) with a dual objective.,5.2 Training,[0],[0]
But this is not without problem.,5.2 Training,[0],[0]
"Beginning
of training phase, the sentence history is unstable since the parameters of sentence-level language model are kept updating.",5.2 Training,[0],[0]
"Consequently, the training of HRNNLM will be also unstable and hard to converge with unstable sentence history.
",5.2 Training,[0],[0]
"In this paper, we approximate the whole training of HRNNLM by a two-step training method.",5.2 Training,[0],[0]
We first train a RNNSLM until it converges.,5.2 Training,[0],[0]
Then we connect the hidden layer of RNNSLM to the hidden layer of RNNWLM.,5.2 Training,[0],[0]
"To increase the training speed, all the parameters of RNNSLM are fixed while training HRNNLM.",5.2 Training,[0],[0]
"We only update the random initialized parameters in HRNNLM, though ideally the gradient of the sentence history vector could change and the RNNSLM could be updated again.",5.2 Training,[0],[0]
The learning rate α is set to 0.1 and the updating of learning rate is the same as suggested in Section 4.2.,5.2 Training,[0],[0]
All the parameters can be initialize as suggested in Section 4.3.,5.2 Training,[0],[0]
"We evaluate the sentence-level performance of HRNNLM by the common coherence evaluation of sentence ordering task, its word-level performance by perplexity measure.",6 Experiments,[0],[0]
We also apply our HRNNLM to SMT reranking task in an open Chinese-English translation dataset.,6 Experiments,[0],[0]
"The translation performance index is the IBM version of BLEU-4 (Papineni et al., 2002).",6 Experiments,[0],[0]
"We follow (Barzilay and Lapata, 2008) to evaluate our sentence-level language model via a sentence ordering task with test set 2010 (tst2010), 2011 (tst2011) and 2012 (tst2012) from IWSLT 2014, totaling 37 English documents.",6.1 Sentence Ordering,[0],[0]
20 random permutations of sentences for each document are generated.,6.1 Sentence Ordering,[0],[0]
Each permutation and its original document are combined as an article pair.,6.1 Sentence Ordering,[0],[0]
"Our goal is to find the original one among all the article pairs.
",6.1 Sentence Ordering,[0],[0]
"The training data for sentence-level language model is the 1,414 English documents from the parallel corpus also provided by the IWSLT 2014 spoken language translation task.",6.1 Sentence Ordering,[0],[0]
90% of the documents are for training and the rest are reserved for validation.,6.1 Sentence Ordering,[0],[0]
"The size of the hidden layer is set to 30 and hash array size is 107.
",6.1 Sentence Ordering,[0],[0]
We define the log probability of a given document as its coherence score.,6.1 Sentence Ordering,[0],[0]
"The document with the higher score is regarded as the original document.
",6.1 Sentence Ordering,[0],[0]
We provide two baselines for sentence ordering.,6.1 Sentence Ordering,[0],[0]
"One is the state-of-the-art recursive neural network based method proposed by (Li and Hovy, 2014).",6.1 Sentence Ordering,[0],[0]
We implement their model trained and tested with our data.,6.1 Sentence Ordering,[0],[0]
The other is a maximum entropy classifier trained with bag of words features of adjacent sentences which can generate a coherent probability of adjacent sentences.,6.1 Sentence Ordering,[0],[0]
The document with the higher sum of log probability for each adjacency sentences is regarded as the original document.,6.1 Sentence Ordering,[0],[0]
"Table 1 shows the accuracy of our system and baseline.
",6.1 Sentence Ordering,[0],[0]
From Table 1 we can see that the maximum entropy model and the recursive neural network model has almost the same performance.,6.1 Sentence Ordering,[0],[0]
"Compared with the baseline systems, the proposed HRNNLM achieves significant improvement with nearly 4.3% improvement in term of accuracy.",6.1 Sentence Ordering,[0],[0]
The experimental result shows that the HRNNLM can model document coherence and capture crosssentence information.,6.1 Sentence Ordering,[0],[0]
We compare the word level performance of HRNNLM with the most popular RNNLM in terms of model perplexity.,6.2 Word-level Model Perplexity,[0],[0]
"For a fair comparison, we follow (Mikolov et al., 2010) and train the model also on 90% of the 1.414 English documents form IWSLT 2014, totaling about 3M words.",6.2 Word-level Model Perplexity,[0],[0]
Then we train our model with the same hidden layer size and hash array size as the baseline system.,6.2 Word-level Model Perplexity,[0],[0]
"The perplexity of these two models is evaluated on held-out documents, about 370K words.",6.2 Word-level Model Perplexity,[0],[0]
"The results are shown in Table 2.
",6.2 Word-level Model Perplexity,[0],[0]
"According to Table 2, it is reasonable to claim that, by integrating history information of previous
sentences, the model perplexity decreased significantly.",6.2 Word-level Model Perplexity,[0],[0]
"Empirically, this confirms the hypothesis that the words selection for the next sentence is dependent on its preceding sentences in the same document.",6.2 Word-level Model Perplexity,[0],[0]
"The conventional SMT systems translate sentences independently, without considering the coherence of the sentences in the same document.",6.3 Spoken Language Translation,[0],[0]
"In order to learn translation coherence between sentences, we apply the HRNNLM to machine translation reranking task.",6.3 Spoken Language Translation,[0],[0]
The data comes from the IWSLT 2014 spoken language translation task.,6.3.1 Data Setting and Baselines,[0],[0]
"The training data consists of 1,414 documents on TED talks, and contains 179k sentence pairs, about 3M Chinese words, and 3.3M English words.",6.3.1 Data Setting and Baselines,[0],[0]
The language model for SMT is a 4-gram language model trained with the English documents in the training data.,6.3.1 Data Setting and Baselines,[0],[0]
"The development set is specified by IWSLT as dev2010, and the test set contains 37 documents from tst2010, tst2011 and tst2012.
",6.3.1 Data Setting and Baselines,[0],[0]
"The IWSLT 2014 baseline system is built upon the open-source machine translation toolkit Moses at the default configuration, proposed by (Cettolo et al., 2012).",6.3.1 Data Setting and Baselines,[0],[0]
"We also train a decoder, which is an in-house Bracketing Transduction Grammar (BTG) (Wu, 1997) in a CKY-style decoder with a lexical reordering model trained with maximum entropy (Xiong et al., 2006).",6.3.1 Data Setting and Baselines,[0],[0]
"The decoder uses commonly used features, such as translation probabilities, lexical weights, a language model, word penalty, and distortion probabilities.",6.3.1 Data Setting and Baselines,[0],[0]
"Our reranking system is a linear model with several features, including the SMT system final scores, sentence-level language model scores, and HRNNLM scores.",6.3.2 Rerank System,[0],[0]
It should be noted all these features are actually employed by the SMT model except for the HRNNLM score.,6.3.2 Rerank System,[0],[0]
"Since Minimum Error Rate Training (MERT) (Och, 2003) is the most general method adopted in SMT systems for tuning, the feature weights are fixed by MERT.
",6.3.2 Rerank System,[0],[0]
"For our reranking system, to score the translation of one sentence we need the translation results of all the previous sentences in the document.",6.3.2 Rerank System,[0],[0]
"Our SMT decoder generates 10-best results of all the sentences of the documents and the rerank-
ing system select the best translation result for the first sentence at first.",6.3.2 Rerank System,[0],[0]
"With the translation of first sentence, we score all the translation candidates of the second sentence and select the best one as the result.",6.3.2 Rerank System,[0],[0]
"Following this procedure, we can get the translation results for all the sentences in the document.",6.3.2 Rerank System,[0],[0]
"The HRNNLM focus on exploiting longer context, esp.",6.3.3 Results,[0],[0]
cross-sentence word dependencies.,6.3.3 Results,[0],[0]
Therefor the translation data for IWSLT 2014 is organized as documents instead of sentences for our rerank system.,6.3.3 Results,[0],[0]
"We hope HRNNLM will enable a contextsensitive reranking process, capturing the syntactic and logic relationships between the sentences in the same document.
",6.3.3 Results,[0],[0]
The translation performance comparison is shown in Table 3.,6.3.3 Results,[0],[0]
"From Table 3, we can find that the rerank system improves SMT performance consistently.",6.3.3 Results,[0],[0]
"For a single sentence without the context information, there are several appropriate translations and it is hard to tell which one is better.",6.3.3 Results,[0],[0]
"When considering the context of a document (previous sentences for our model), some translation candidates may not be coherent with the others which should not be selected.",6.3.3 Results,[0],[0]
"Our model can generate the most coherent translation results by considering previous sentence history.
",6.3.3 Results,[0],[0]
"For example, we have the following two Chinese sentence in one document together with their correct translation: 我拍摄过的冰山,有些冰是非常年 轻",6.3.3 Results,[0],[0]
-,6.3.3 Results,[0],[0]
-几千年年龄,6.3.3 Results,[0],[0]
Some of the ice in the icebergs that I photograph is very young - - a couple thousand years old.,6.3.3 Results,[0],[0]
有些冰超过十万年,6.3.3 Results,[0],[0]
"And some of the ice is over 100,000 years old.
",6.3.3 Results,[0],[0]
Chinese word “ 有些” means “some” in English.,6.3.3 Results,[0],[0]
"But when it is used in parallelism sentences, it means “some of” instead of “some”.",6.3.3 Results,[0],[0]
The traditional SMT system translates the italics part without considering the context.,6.3.3 Results,[0],[0]
"The translation result for this kind of system is:
Some ice more than 100,000 years.",6.3.3 Results,[0],[0]
"For our system, the HRNNLM can take previous sentence as context and learn the parallelism between the two sentences.",6.3.3 Results,[0],[0]
"It can select the best translation “some of” for 有些, and the output of our system is:
Some of the ice more than 100,000 years.",6.3.3 Results,[0],[0]
We also calculate the BLEU increase ratio of our system on document level.,6.3.3 Results,[0],[0]
"The ratio is defined as 1N #(BleuDrerank > BleuDbaseline), where N denotes the number of documents, and #(BleuDrerank > BleuDbaseline) denotes the number of documents for which document level BLEU score of reranking system is higher than the baselines.",6.3.3 Results,[0],[0]
"The results are shown in Table 4.
",6.3.3 Results,[0],[0]
"From Table 4, we can find that, for all the three test data sets, our reranking system can achieve better performance for more than 70% documents.",6.3.3 Results,[0],[0]
"In this paper, we propose a hierarchical recurrent neural network language model for document modeling.",7 Conclusion and Future Work,[0],[0]
We first built a RNNSLM to capture the information between sentences.,7 Conclusion and Future Work,[0],[0]
Then we integrate the hidden layer of RNNSLM into the input layer of word-level language model to form a hierarchical recurrent neural network.,7 Conclusion and Future Work,[0],[0]
This enables the model be able to capture both in-sentence and cross-sentence information in a unified RNN.,7 Conclusion and Future Work,[0],[0]
"Compared with conventional language models, our model can perceive a longer history than other language models and captures the context patterns in the previous sentences.",7 Conclusion and Future Work,[0],[0]
"At sentence level, we examine our model with sentence ordering task.",7 Conclusion and Future Work,[0],[0]
"At word level, we test the model perplexity.",7 Conclusion and Future Work,[0],[0]
We also conduct a SMT rerank experiment on IWSLT 2014 data set.,7 Conclusion and Future Work,[0],[0]
"All these experimental results show that our hierarchical recurrent neural network has a satisfying performance.
",7 Conclusion and Future Work,[0],[0]
"In the future, we will explore better sentence representation such as distributed sentence representation as input for our sentence-level language model to better model document coherence.",7 Conclusion and Future Work,[0],[0]
We can even update the gradient from different RNN to get a better performance.,7 Conclusion and Future Work,[0],[0]
We are grateful to the three anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
"We also thank Dongdong Zhang, Lei Cui for useful discussions.",Acknowledgments,[0],[0]
"This paper is supported by the project of National Natural Science Foundation of China (Grant No. 61272384, 61370170 &61402134).",Acknowledgments,[0],[0]
This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling.,abstractText,[0],[0]
"After establishing a RNN to capture the coherence between sentences in a document, HRNNLM integrates it as the sentence history information into the word level RNN to predict the word sequence with cross-sentence contextual information.",abstractText,[0],[0]
"A two-step training approach is designed, in which sentence-level and word-level language models are approximated for the convergence in a pipeline style.",abstractText,[0],[0]
"Examined by the standard sentence reordering scenario, HRNNLM is proved for its better accuracy in modeling the sentence coherence.",abstractText,[0],[0]
"And at the word level, experimental results also indicate a significant lower model perplexity, followed by a practical better translation result when applied to a Chinese-English document translation reranking task.",abstractText,[0],[0]
Hierarchical Recurrent Neural Network for Document Modeling,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1964–1974 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"The adoption of NLP methods has led to significant advances in the field of computational social science (Lazer et al., 2009), including political science (Grimmer and Stewart, 2013).",1 Introduction,[0],[0]
"Among a myriad of data sources, election manifestos are a core artifact in political analysis.",1 Introduction,[0],[0]
"One of the most widely used datasets by political scientists is the Comparative Manifesto Project (CMP) dataset (Volkens et al., 2017), which contains manifestos in various languages, covering over 1000 parties across 50 countries, from elections dating back to 1945.
",1 Introduction,[0],[0]
"In CMP, a subset of the manifestos has been manually annotated at the sentence-level with one of 57 political themes, divided into 7 major categories.1 Such categories capture party positions (FAVORABLE, UNFAVORABLE or NEITHER)
1https://manifesto-project.wzb.eu/ coding_schemes/mp_v5
on fine-grained policy themes, and are also useful for downstream tasks including calculating manifesto-level (policy-based) left–right position scores (Budge et al., 2001; Lowe et al., 2011; Däubler and Benoit, 2017).",1 Introduction,[0],[0]
"An example sentence from the Green Party of England and Wales 2015 election manifesto where they take an UNFAVORABLE position on MILITARY is:
We would: Ensure that ... less is spent on military research.
",1 Introduction,[0],[0]
"Elsewhere, they take a FAVORABLE position on WELFARE STATE:
Double Child Benefit.
",1 Introduction,[0],[0]
"Such manual annotations are labor-intensive and prone to annotation inconsistencies (Mikhaylov et al., 2012).",1 Introduction,[0],[0]
"In order to overcome these challenges, supervised sentence classification approaches have been proposed (Verberne et al., 2014; Subramanian et al., 2017).
",1 Introduction,[0],[0]
"Other than the sentence-level labels, the manifesto text also has a document-level score that quantifies its position on the left–right spectrum.",1 Introduction,[0],[0]
"Different approaches have been proposed to derive this score, based on alternate definitions of “left–right” (Slapin and Proksch, 2008; Benoit and Laver, 2007; Lo et al., 2013; Däubler and Benoit, 2017).",1 Introduction,[0],[0]
"Among these, the RILE index is the most widely adopted (Merz et al., 2016; Jou and Dalton, 2017), and has been shown to correlate highly with other popular scores (Lowe et al., 2011).",1 Introduction,[0],[0]
"RILE is defined as the difference between RIGHT and LEFT positions on (pre-determined) policy themes across sentences in a manifesto (Volkens et al., 2013); for instance, UNFAVORABLE position on MILITARY is categorized as LEFT.",1 Introduction,[0],[0]
"RILE is popular in CMP in particular, as mapping individual sentences to LEFT/RIGHT/NEUTRAL categories has
1964
been shown to be less sensitive to systematic errors than other sentence-level class sets (Klingemann et al., 2006; Volkens et al., 2013).
",1 Introduction,[0],[0]
"Finally, expert survey scores are gaining popularity as a means of capturing manifesto-level political positions, and are considered to be contextand time-specific, unlike RILE (Volkens et al., 2013; Däubler and Benoit, 2017).",1 Introduction,[0],[0]
"We use the Chapel Hill Expert Survey (CHES) (Bakker et al., 2015), which comprises aggregated expert surveys on the ideological position of various political parties.",1 Introduction,[0],[0]
"Although CHES is more subjective than RILE, the CHES scores are considered to be the gold-standard in the political science domain.
",1 Introduction,[0],[0]
"In this work, we address both fine- and coarsegrained multilingual manifesto text policy position analysis, through joint modeling of sentence-level classification and document-level positioning (or ranking) tasks.",1 Introduction,[0],[0]
"We employ a two-level structured model, in which the first level captures the structure within a manifesto, and the second level captures context and temporal dependencies across manifestos.",1 Introduction,[0],[0]
"Our contributions are as follows: • we employ a hierarchical sequential deep model
that encodes the structure in manifesto text for the sentence classification task; • we capture the dependency between the
sentence- and document-level tasks, and also utilize additional label structure (categorization into LEFT/RIGHT/NEUTRAL: Volkens et al. (2013)) using a joint-structured model; • we incorporate contextual information (such as
political coalitions) and encode temporal dependencies to calibrate the coarse-level manifesto position using probabilistic soft logic (Bach et al., 2015), which we evaluate on the prediction of the RILE index or expert survey party position score.",1 Introduction,[0],[0]
Analysing manifesto text is a relatively new application at the intersection of political science and NLP.,2 Related Work,[0],[0]
"One line of work in this space has been on sentence-level classification, including classifying each sentence according to its major political theme (1-of-7 categories) (Zirn et al., 2016; Glavaš et al., 2017a), its position on various policy themes (Verberne et al., 2014; Biessmann, 2016; Subramanian et al., 2017), or its relative disagreement with other parties (Menini et al., 2017).",2 Related Work,[0],[0]
"Recent approaches (Glavaš et al., 2017a; Subrama-
nian et al., 2017) have also handled multilingual manifesto text (given that manifestos span multiple countries and languages; see Section 5.1) using multilingual word embeddings.
",2 Related Work,[0],[0]
"At the document level, there has been work on using label count aggregation of (manuallyannotated) fine-grained policy positions, as features for inductive analysis (Lowe et al., 2011; Däubler and Benoit, 2017).",2 Related Work,[0],[0]
"Text-based approaches has used dictionary-based supervised methods, unsupervised factor analysis based techniques and graph propagation based approaches (Hjorth et al., 2015; Bruinsma and Gemenis, 2017; Glavaš et al., 2017b).",2 Related Work,[0],[0]
"A recent paper closely aligned with our work is Subramanian et al. (2017), who address both sentence- and document-level tasks jointly in a multilingual setting, showing that a joint approach outperforms previous approaches.",2 Related Work,[0],[0]
"But they do not exploit the structure of the text and use a much simpler model architecture: averages of word embeddings, versus our bi-LSTM encodings; and they do not leverage domain information and temporal regularities that can influence policy positions (Greene, 2016).",2 Related Work,[0],[0]
"This work will act as a baseline in our experiments in Section 5.
Policy-specific position classification can be seen as related to target-specific stance classification (Mohammad et al., 2017), except that the target is not explicitly mentioned in most cases.",2 Related Work,[0],[0]
"Secondly, manifestos have both fine- and coarsegrained positions, similar to sentiment analysis (McDonald et al., 2007).",2 Related Work,[0],[0]
"Finally, manifesto text is well structured within and across documents (based on coalition), has temporal dependencies, and is multilingual in nature.",2 Related Work,[0],[0]
"In this section, we detail the first step of our two-stage approach.",3 Proposed Approach,[0],[0]
"We use a hierarchical bidirectional long short-term memory (“biLSTM”) model (Hochreiter and Schmidhuber, 1997; Graves et al., 2013; Li et al., 2015) with a multi-task objective for the sentence classification and document-level regression tasks.",3 Proposed Approach,[0],[0]
"A post-hoc calibration of coarse-grained manifesto position is given in Section 4.
",3 Proposed Approach,[0],[0]
"Let D be the set of manifestos, where a manifesto d ∈ D is made up of L sentences, and a sentence si has T words: wi1, wi2, ...wiT .",3 Proposed Approach,[0],[0]
"The set Ds ⊂ D is annotated at the sentence-level
with positions on fine-grained policy issues (57 classes).",3 Proposed Approach,[0],[0]
"The task here is to learn a model that can: (a) classify sentences according to policy issue classes; and (b) score the overall document on the policy-based left–right spectrum (RILE), in an inter-dependent fashion.
",3 Proposed Approach,[0],[0]
"Word encoder: We initialize word vector representations using a multilingual word embedding matrix, We.",3 Proposed Approach,[0],[0]
"We construct We by aligning the embedding matrices of all the languages to English, in a pair-wise fashion.",3 Proposed Approach,[0],[0]
"Bilingual projection matrices are built using pre-trained FastText monolingual embeddings (Bojanowski et al., 2017) and a dictionary D constructed by translating 5000 frequent English words using Google Translate.",3 Proposed Approach,[0],[0]
"Given a pair of embedding matrices E (English) andO (Other), we use singular value decomposition of OTDE (which is UΣV T ) to get the projection matrix (W ∗=UV T ), since it also enforces monolingual invariance (Artetxe et al., 2016; Smith et al., 2017).",3 Proposed Approach,[0],[0]
"Finally, we obtain the aligned embedding matrix, We, as OW ∗.
We use a bi-LSTM to derive a vector representation of each word in context.",3 Proposed Approach,[0],[0]
"The bi-LSTM traverses the sentence si in both the forward and backward directions, and the encoded representation for a given word wit ∈ si, is defined by concatenating its forward ( −→ h it) and backward hidden states ( ←− h it), t ∈",3 Proposed Approach,[0],[0]
"[ 1, T ] .
",3 Proposed Approach,[0],[0]
Sentence model:,3 Proposed Approach,[0],[0]
"Similarly, we use a bi-LSTM to generate a sentence embedding from the wordlevel bi-LSTM, where each input sentence si is represented using the last hidden state of both the forward and backward LSTMs.",3 Proposed Approach,[0],[0]
"The sentence embedding is obtained by concatenating the hidden representations of the sentence-level bi-LSTM, in both the directions, hi =",3 Proposed Approach,[0],[0]
"[−→ h i, ←−",3 Proposed Approach,[0],[0]
h,3 Proposed Approach,[0],[0]
"i ] , i ∈",3 Proposed Approach,[0],[0]
"[ 1, L ] .",3 Proposed Approach,[0],[0]
"With this representation, we perform fine-grained classification (to one-of-57 classes), using a softmax output layer for each sentence.",3 Proposed Approach,[0],[0]
"We minimize the cross-entropy loss for this task, over the sentence-level labeled set Ds ⊂ D.",3 Proposed Approach,[0],[0]
"This loss is denoted LS .
",3 Proposed Approach,[0],[0]
"Document model: To represent a document d we use average-pooling over the sentence representations hi and predicted output distributions (yi) of individual sentences,2 i.e.,
2Preliminary experiments suggested that this representation performs better than using either hidden representations or just the output distribution.
",3 Proposed Approach,[0],[0]
Vd = 1 L ∑ i∈d [ yi hi ] .,3 Proposed Approach,[0],[0]
"The range of RILE is [−100, 100], which we scale to the range [−1, 1], and model using a final tanh layer.",3 Proposed Approach,[0],[0]
"We minimize the mean-squared error loss function between the predicted r̂d and actual RILE score rd, which is denoted as LD:
LD = 1
|D|
|D|∑
d=1
‖r̂d",3 Proposed Approach,[0],[0]
"− rd‖22 (1)
Overall, the loss function for the joint model (Figure 1), combining LS and LD, is:
LJ = αLS + (1− α)LD (2)
where 0 ≤ α ≤ 1 is a hyper-parameter which is tuned on a development set.",3 Proposed Approach,[0],[0]
"The RILE score is calculated directly from the sentence labels, based on mapping each label according to its positioning on policy themes, as LEFT, RIGHT and NEUTRAL (Volkens et al., 2013).",3.1 Joint-Structured Model,[0],[0]
"Specifically, 13 out of 57 classes are categorized as LEFT, another 13 as RIGHT, and the rest as NEUTRAL.",3.1 Joint-Structured Model,[0],[0]
We employ an explicit structured loss which minimizes the deviation between sentencelevel LEFT/RIGHT/NEUTRAL polarity predictions p and the document-level RILE score.,3.1 Joint-Structured Model,[0],[0]
"The motivation to do this is two-fold: (a) enabling interaction between the sentence- and document-level tasks with homogeneous target space (polarity and RILE); and (b) since we have more documents with just RILE and no sentence-level labels,3 augmenting an explicit semi-supervised learning objective could propagate down the RILE label to generate sentence labels that concord with the document score.
",3.1 Joint-Structured Model,[0],[0]
"For the sentence-level polarity prediction (shown in Figure 1), we use cross-entropy loss over the sentence-level labeled set Ds ⊂ D, which is denoted as LSP .",3.1 Joint-Structured Model,[0],[0]
"The explicit structured sentence-document loss is given as:
Lstruc = 1
|D|
|D|∑
d=1
( 1
Ld
∑ i∈d (piright − pileft )",3.1 Joint-Structured Model,[0],[0]
"− rd
)2
(3)
3Strictly speaking, for these documents even, sentence annotation was used to derive the RILE score, but the sentencelevel labels were never made available.
where piright and pileft are the predicted RIGHT and LEFT class probabilities for a sentence si (∈ d), rd is the actual RILE score for the document d, and Ld is the length of each document, d ∈ D. We augment the joint model’s loss function (Equation (2)) with LSP and Lstruc to generate a regularized multi-task loss:
LT = LJ + βLSP",3.1 Joint-Structured Model,[0],[0]
"+ γLstruc (4)
where β, γ ≥ 0 are hyper-parameters which are, once again, tuned on the development set.",3.1 Joint-Structured Model,[0],[0]
"We refer to the model trained with Equation (2) as “Joint”, and that trained with Equation (4) as “Jointstruc”.",3.1 Joint-Structured Model,[0],[0]
"We leverage party-level information to enforce smoothness and regularity in manifesto positioning on the left–right spectrum (Greene, 2016).",4 Manifesto Position Re-ranking,[0],[0]
"For example, manifestos released by parties in a coalition are more likely to be closer in RILE score, and a party’s position in an election is often a relative shift from its position in earlier election, so temporal information can provide smoother estimations.",4 Manifesto Position Re-ranking,[0],[0]
"To address this, we propose an approach using hinge-loss Markov random fields (“HL-MRFs”), a scalable class of continuous, conditional graphical models (Bach et al., 2013).",4.1 Probabilistic Soft Logic,[0],[0]
"HL-MRFs have
been used for many tasks including political framing analysis on Twitter (Johnson et al., 2017) and user stance classification on socio-political issues (Sridhar et al., 2014).",4.1 Probabilistic Soft Logic,[0],[0]
"These models can be specified using Probabilistic Soft Logic (“PSL”) (Bach et al., 2015), a weighted first order logical template language.",4.1 Probabilistic Soft Logic,[0],[0]
"An example of a PSL rule is
λ : P(a) ∧ Q(a, b)→ R(b)
where P, Q, and R are predicates, a and b are variables, and λ is the weight associated with the rule.",4.1 Probabilistic Soft Logic,[0],[0]
"PSL uses soft truth values for predicates in the interval [ 0, 1 ] .",4.1 Probabilistic Soft Logic,[0],[0]
"The degree of ground rule satisfaction is determined using the Lukasiewicz t-norm and its corresponding co-norm as the relaxation of the logical AND and OR, respectively.",4.1 Probabilistic Soft Logic,[0],[0]
"The weight of the rule indicates its importance in the HL-MRF probabilistic model, which defines a probability density function of the form:
P (Y|X) ∝",4.1 Probabilistic Soft Logic,[0],[0]
"exp ( − M∑
r=1
λrφr(Y,X)
) ,
φr(Y,X) = max {lr(Y,X), 0}ρr , (5)
where φr(Y,X) is a hinge-loss potential corresponding to an instantiation of a rule, and is specified by a linear function lr and optional exponent ρr ∈ {1, 2}.",4.1 Probabilistic Soft Logic,[0],[0]
Note that the hinge-loss potential captures the distance to satisfaction.4,4.1 Probabilistic Soft Logic,[0],[0]
"Here we elaborate our PSL model (given in Table 1) based on coalition information, manifesto content-based features (manifesto similarity and right–left ratio), and temporal dependency.",4.2 PSL Model,[0],[0]
"Our target pos (calibrated RILE) is a continuous variable [ 0, 1 ] , where 1 indicates that a manifesto occupies an extreme right position, 0 denotes an extreme left position, and 0.5 indicates center.",4.2 PSL Model,[0],[0]
"Each instance of a manifesto and its party affiliation are denoted by the predicates Manifesto and Party.
",4.2 PSL Model,[0],[0]
"Coalition: We model multi-relational networks based on regional coalitions within a given country (RegCoalition),5 and also crosscountry coalitions in the European parliament
4Degree of satisfaction for the example PSL rule r, ¬P ∨ ¬Q ∨ R, using the Lukasiewicz co-norm is given as min{2− P− Q+ R, 1}.",4.2 PSL Model,[0],[0]
"From this, the distance to satisfaction is given as max{P+ Q− R− 1, 0}, where P+",4.2 PSL Model,[0],[0]
"Q− R− 1 indicates the linear function lr .
",4.2 PSL Model,[0],[0]
"5http://www.parlgov.org/
(EUCoalition).6 We set the scope of interaction between manifestos (x and y) from a country to the same election (SameElec).",4.2 PSL Model,[0],[0]
"For manifestos across countries, we consider only the most recent manifesto (Recent) from each party (y), released within 4 years relative to x.",4.2 PSL Model,[0],[0]
"We use a logistic transformation of the number of times two parties have been in a coalition in the past (to get a value between 0 and 1), for both RegCoalition and EUCoalition.",4.2 PSL Model,[0],[0]
"We also construct rules based on transitivity for both the relational features, i.e., parties which have had common coalition partners, even if they were not allies themselves, are likely to have similar policy positions.
",4.2 PSL Model,[0],[0]
"Manifesto similarity: Manifestos that are similar in content are expected to have similar RILE scores (and associated sentence-
6http://www.europarl.europa.eu
level label distributions), similar to the modeling intuition captured by Burford et al. (2015) in the context of congressional debate vote prediction.",4.2 PSL Model,[0],[0]
"For a pair of recent manifestos (Recent) we use the cosine similarity (Similarity) between their respective document vectors Vd (Figure 1).
",4.2 PSL Model,[0],[0]
Right–left ratio:,4.2 PSL Model,[0],[0]
"For a given manifesto, we compute the ratio of sentences categorized under RIGHT to OTHERS ( # RIGHT# RIGHT+# LEFT+# NEUTRAL ), where the categorization for sentences is obtained using the joint-structured model (Equation (4)).",4.2 PSL Model,[0],[0]
"We also encode the location of sentence ls in a document, by weighing the count of sentences for each class C by its location value∑
s∈C log(ls + 1) (referred to as loc lr).",4.2 PSL Model,[0],[0]
"The intuition here is that the beginning parts of a manifesto tends to contain generic information such as preamble, compared to
later parts which are more policy-dense.",4.2 PSL Model,[0],[0]
"We perform a logistic transformation of loc lr to derive the LwRightLeftRatio.
",4.2 PSL Model,[0],[0]
"Temporal dependency: We capture the temporal dependency between a party’s current manifesto position and its previous manifesto position (PreviousManifesto).
",4.2 PSL Model,[0],[0]
"Other than for the look-up based random variables, the network is instantiated with predictions (for Similarity, LwRightLeftRatio and pos) from the joint-structured model (Figure 1).",4.2 PSL Model,[0],[0]
"All the random variables, except pos (which is the target variable), are fixed in the network.",4.2 PSL Model,[0],[0]
"These values are then used inside a PSL model for collective probabilistic reasoning, where the first-order logic given in Table 1 is used to define the graphical model (HL-MRF) over the random variables detailed above.",4.2 PSL Model,[0],[0]
"Inference on the HL-MRF is used to obtain the most probable interpretation such that it satisfies most ground rule instances, i.e., considering the relational and temporal dependencies.",4.2 PSL Model,[0],[0]
"As our dataset, we use manifestos from CMP for European countries only, as in Section 5.5 we will validate the manifesto’s overall position on the left-right spectrum, using the Chapel Hill Expert Survey (CHES), which is only available for European countries (Bakker et al., 2015).",5.1 Experimental Setup,[0],[0]
"In this, we sample 1004 manifestos from 12 European countries, written in 10 different languages — Danish (Denmark), Dutch (Netherlands), English (Ireland, United Kingdom), Finnish (Finland), French (France), German (Austria, Germany), Italian (Italy), Portuguese (Portugal), Spanish (Spain), and Swedish (Sweden).",5.1 Experimental Setup,[0],[0]
"Out of the 1004 manifestos, 272 are annotated with both sentence-level labels and RILE scores, and the remainder only have RILE scores (see Table 2 for further statistics).
",5.1 Experimental Setup,[0],[0]
"There are (less) scenarios where a natural sentence is segmented into sub-sentences and annotated with different classes (Däubler et al., 2012).",5.1 Experimental Setup,[0],[0]
Hence we use NLTK sentence tokenizer followed by heuristics from Däubler et al. (2012) to obtain sub-sentences.,5.1 Experimental Setup,[0],[0]
"Consistent with previous work (Subramanian et al., 2017), we present results with manually segmented and annotated test documents.",5.1 Experimental Setup,[0],[0]
"Sentence-level baseline approaches include:
• BoW-NN : TF-IDF-weighted unigram bagof-words representation of sentences (Biessmann, 2016), and monolingual training using a multi-layer perceptron (“MLP”) model.
",5.2 Baseline Approaches,[0],[0]
"• BoT-NN : Similar to above, but trigram bagof-words.
",5.2 Baseline Approaches,[0],[0]
"• AE-NN : MLP model with average multilingual word embeddings as the sentence representation (Subramanian et al., 2017).
• CNN :",5.2 Baseline Approaches,[0],[0]
"Convolutional neural network (“CNN”: Glavaš et al. (2017a)) with multilingual word embeddings.
",5.2 Baseline Approaches,[0],[0]
• Bi-LSTM :,5.2 Baseline Approaches,[0],[0]
"Simple bi-LSTM over multilingual word embeddings, last hidden units are concatenated to form the sentence representation, and fed directly into a softmax sentencelevel layer.",5.2 Baseline Approaches,[0],[0]
"We evaluate two scenarios: (1) with a trainable embedding matrix We (BiLSTM(+up) ); and (2) without a trainable We.
",5.2 Baseline Approaches,[0],[0]
"Document-level baseline approaches include:
• BoC : Bag-of-centroids (BoC) document representation based on clustering the word embeddings (Lebret and Collobert, 2014), fed into a neural network regression model.
",5.2 Baseline Approaches,[0],[0]
"• HCNN : Hierarchical CNN, where we encode both the sentence and document using stacked CNN layers.
",5.2 Baseline Approaches,[0],[0]
"• HNN : State-of-the-art hierarchical neural network model of Subramanian et al. (2017), based on average embedding representations for sentences and the document.
",5.2 Baseline Approaches,[0],[0]
"We present results evaluated under two different settings: (a) 80–20% random split averaged across 10 runs to validate the hierarchical model (Section 5.3 and Section 5.4); and (b) temporal setting, where train- and test-set are split chronologically, to validate both the hierarchical deep model and the PSL approach especially, since we encode temporal dependencies (Section 5.5).",5.2 Baseline Approaches,[0],[0]
"We present sentence-level results with a 80–20% random split in Table 3, stratified by country, averaged across 10 runs.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"For Bi-LSTM , we found the setting with a trainable embedding matrix (Bi-LSTM(+up) ) to perform better than the nontrainable case (Bi-LSTM ).",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
Hence we use a similar setting for Joint and Jointstruc.,5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"We show the effect of α (from Equation (2)) in Figure 2a, based on which we set α = 0.3 hereafter.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"With the chosen model, we study the effect of the structured loss (Equation (4)), by varying γ with fixed β = 0.1, as shown in Figure 2b.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"We observe that γ = 0.7 gives the best performance, and varying β with γ at 0.7 does not result in any further improvement (see Figure 2c).",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"Sentence-level results measured using F-measure, for baseline approaches and the proposed models selected from Figure 2a (Joint), Figures 2b and 2c (Jointstruc) are given in Table 3.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"We also evaluate the special case of α = 1, in the form of sentence-only model Jointsent.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"For the document-level task, results for overall manifesto positioning measured using Pearson’s correlation (r) and Spearman’s rank correlation (ρ) are given in Table 4.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"We also evaluate the hierarchical biLSTM model with document-level objective only, Jointdoc.
",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"We observe that hierarchical modeling (Jointsent, Joint and Jointstruc) gives the best performance for sentence-level classification for all the languages except Portuguese, on which it performs slightly worse than Bi-LSTM(+up) .",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"Also, Jointstruc, does not improve over Jointsent.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
We perform further analysis to see the effect of joint-structured model on the sentence-level task under sparsely-labeled conditions in Section 5.4.,5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"On the other hand, for the document-level task,
the joint model (Joint) performs better than Jointdoc and all the baseline approaches.",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"Lastly, the joint-structured model (Jointstruc) provides further improvement over Joint .",5.3 Hierarchical Sentence- and Document-level Model,[0],[0]
"To understand the utility of joint modeling, especially given that there are more manifestos with document-level labels only than both sentenceand document-level labels, we compare the following two settings: (1) Jointstruc, which uses additional manifestos with document-level supervision (RILE); and (2) Jointsent, which uses manifestos with sentence-level supervision only.",5.4 Analysis of Joint-Structured Model for Sentence-level task,[0],[0]
"We vary the proportion of labeled documents at the sentence-level, from 10% to 80%, to study the effect under sparsely-labeled conditions.",5.4 Analysis of Joint-Structured Model for Sentence-level task,[0],[0]
Note that 80% is the maximum labeled training data under the cross-validation setting.,5.4 Analysis of Joint-Structured Model for Sentence-level task,[0],[0]
"In other cases, a subset (say 10%) is randomly sampled for train-
ing.",5.4 Analysis of Joint-Structured Model for Sentence-level task,[0],[0]
"From Figure 3, having more manifestos with document-level supervision demonstrates the advantage of semi-supervised learning, especially when the sentence-level supervision is sparse (≤ 40%)— Jointstruc performs better than Jointsent.",5.4 Analysis of Joint-Structured Model for Sentence-level task,[0],[0]
"Finally, we present the results using PSL, which calibrates the overall manifesto position on the left–right spectrum, obtained using the jointstructured model (Jointstruc).",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"As we evaluate the effect of temporal dependency, we use manifestos before 2008-09 for training (868 in total) and the later ones (until 2015, 136 in total) for testing.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"This test set covers one recent set of election manifestos for most countries, and two for the Nether-
lands, Spain and United Kingdom.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"To avoid variance in right-to-left ratio and the target variable (pos, initialized using Jointstruc) between the training and test sets, we build a stacked network (Fast and Jensen, 2008), whereby we estimate values for the training set using cross-validation across the training partition, and estimate values for the test-set with a model trained over the entire training data.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"Note that we build the Jointstruc model afresh using the chronologically split training set, and the parameters are tuned again using an 80-20 random split of the training set.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"For a consistent view of results for both the tasks (and stages), we provide micro-averaged results for sentence-classification with the competing approaches (from Table 3): AE-NN (Subramanian et al., 2017), Bi-LSTM(+up) , and Jointstruc.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"Results are presented in Table 5, noting that the results for a given method will differ from earlier due to the different data split.
",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"For the document-level regression task, we also evaluate other approaches based on manifesto similarity and automated scaling with sentence-level policy positions:
• Cross-lingual scaling (CLS ): A recent unsupervised approach for crosslingual political speech text scoring (Glavaš et al., 2017b), based on TF-IDF weighed average wordembeddings to represent documents, and a graph constructed using pair-wise document
similarity.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"Given two pivot texts (for left and right), label propagation approach is used to position other documents.
",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"• PCA: Apply principal component analysis (Gabel and Huber, 2000) on the distribution of sentence-level policy positions (56 classes, without 000), and use the projection on its principal component to explain maximum variance in its sentence-level positions, as a latent manifesto-level position score.
",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"• Jointstruc: We evaluate the scores obtained using Jointstruc, which we calibrate using PSL.
",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
We validate the calibrated position scores using both RILE and CHES7 scores.,5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"We use CHES 2010-14, and map the manifestos to the closest survey year (wrt its election date).",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
CHES scores are used only for evaluation and not during training.,5.5 Manifesto Position Re-ranking using PSL,[0],[0]
We provide results in Table 6 by augmenting features for the PSL model (Table 1) incrementally.,5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"We observed that the coalition-based feature, and polarity of sentences with its position information improves the overall ranking (r, ρ).",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"Document similarity based relational feature provides only mild improvement (similarly to Burford et al. (2015)), and temporal dependency provides further improvement against CHES.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
"That is, combining content, network and temporal features provides the best results.",5.5 Manifesto Position Re-ranking using PSL,[0],[0]
This work has been targeted at both fine- and coarse-grained manifesto text position analysis.,6 Conclusion and Future Work,[0],[0]
"We have proposed a two-stage approach, where in the first step we use a hierarchical multi-task
7https://www.chesdata.eu/
deep model to handle the sentence- and documentlevel tasks together.",6 Conclusion and Future Work,[0],[0]
"We also utilize additional information on label structure, to augment an auxiliary structured loss.",6 Conclusion and Future Work,[0],[0]
"Since the first step places the manifesto on the left–right spectrum using text only, we leverage context information, such as coalition and temporal dependencies to calibrate the position further using PSL.",6 Conclusion and Future Work,[0],[0]
"We observed that: (a) a hierarchical bi-LSTM model performs best for the sentence-level classification task, offering a 10% improvement over the state-of-art approach (Subramanian et al., 2017); (b) modeling the document-level task jointly, and also augmenting the structured loss, gives the best performance for the document-level task and also helps the sentence-level task under sparse supervision scenarios; and (c) the inclusion of a calibration step with PSL provides significant gains in performance against both RILE and CHES, in the form of an increase from ρ = 0.42 to 0.61 wrt CHES survey scores.
",6 Conclusion and Future Work,[0],[0]
"There are many possible extensions to this work, including: (a) learning multilingual word embeddings with domain information; and (b) modeling other policy related scores from text, such as “support for EU integration”.",6 Conclusion and Future Work,[0],[0]
We thank the anonymous reviewers for their insightful comments and valuable suggestions.,Acknowledgements,[0],[0]
"This work was funded in part by the Australian Government Research Training Program Scholarship, and the Australian Research Council.",Acknowledgements,[0],[0]
"Election manifestos document the intentions, motives, and views of political parties.",abstractText,[0],[0]
"They are often used for analysing a party’s finegrained position on a particular issue, as well as for coarse-grained positioning of a party on the left–right spectrum.",abstractText,[0],[0]
In this paper we propose a two-stage model for automatically performing both levels of analysis over manifestos.,abstractText,[0],[0]
"In the first step we employ a hierarchical multi-task structured deep model to predict fineand coarse-grained positions, and in the second step we perform post-hoc calibration of coarse-grained positions using probabilistic soft logic.",abstractText,[0],[0]
"We empirically show that the proposed model outperforms state-of-art approaches at both granularities using manifestos from twelve countries, written in ten different languages.",abstractText,[0],[0]
Hierarchical Structured Model for Fine-to-coarse Manifesto Text Analysis,title,[0],[0]
"Bayesian optimization method has established itself as an efficient way to optimize black-box functions (Jones et al.,
*Equal contribution 1Centre for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia.",1. Introduction,[0],[0]
"Correspondence to: Santu Rana <santu.rana@deakin.edu.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
1998) which are also expensive to evaluate.",1. Introduction,[0],[0]
"Examples include experimental design to optimize the quality of a physical product (Brochu et al., 2010) or hyperparameter tuning of machine learning algorithms (Bardenet et al., 2013).",1. Introduction,[0],[0]
In both cases the response functions are unknown and each evaluation of either making the product to test the quality or training a model from large data can be time-consuming.,1. Introduction,[0],[0]
"Likewise, it has found applications in a variety of domains including computer vision (Denil et al., 2012) and sensor set selection (Garnett et al., 2010).
",1. Introduction,[0],[0]
Bayesian optimization is a sequential procedure where a probabilistic form of the unknown function is maintained using a Gaussian process (GP).,1. Introduction,[0],[0]
A GP is specified by a mean function and a covariance function.,1. Introduction,[0],[0]
"A popular choice of covariance function is the squared exponential kernel (Rasmussen and Williams, 2005).",1. Introduction,[0],[0]
A crucial parameter of the kernel is the length-scale which dictates prior belief about the smoothness of the objective function.,1. Introduction,[0],[0]
The posterior of a Gaussian process is analytically tractable and is used to estimate both the mean and the variance of the estimation at unobserved locations.,1. Introduction,[0],[0]
"Next, a cheap surrogate function is built that seeks the location where lies the highest possibility of obtaining a higher response.",1. Introduction,[0],[0]
The possibility is expressed through a variety of acquisition functions which trade-off exploitation of the predicted best mean and exploration around high predicted variance.,1. Introduction,[0],[0]
"Typical acquisition functions include Expected Improvement (EI) (Mockus, 1994) and GP-UCB (Srinivas et al., 2010).
",1. Introduction,[0],[0]
"Acquisition functions are continuous functions, yet they may be extremely sharp functions at higher dimensions, especially when the size of observed data is small.",1. Introduction,[0],[0]
"Generally, they have some peaks and a large area of mostly flat surface.",1. Introduction,[0],[0]
"For this reason, the global optimization of high-dimensional acquisition functions is hard and can be prohibitively expensive.",1. Introduction,[0],[0]
This makes it difficult to scale Bayesian optimization to high dimensions.,1. Introduction,[0],[0]
"Generic global optimization algorithms such as DIRECT (Jones et al., 1993) or simplex-based methods such as Nelder-Mead (Olsson and Nelson, 1975) or genetic algorithm based methods (Runarsson and Yao, 2005; Beyer and Schwefel, 2002) perform reasonably when the dimension is low, but at higher dimensions they can become extremely inefficient and actually become infeasible within the practical limitation of resource and time.",1. Introduction,[0],[0]
"Multi-start based method start
from multiple initializations to achieve local maxima and then choose the best one.",1. Introduction,[0],[0]
"However, the multi-start method may not be able to find the non-flat portion of the acquisition function by random search.",1. Introduction,[0],[0]
A related discussion for high dimensional Bayesian optimization concerns with the usefulness of Gaussian process for high dimensional modeling.,1. Introduction,[0],[0]
"Fortunately, Srinivas et al. (2010) showed that Gaussian process (GP) can handle “curse of dimensionality” to a good extent.
",1. Introduction,[0],[0]
Limited work has addressed the issue of highdimensionality in Bayesian optimization.,1. Introduction,[0],[0]
"Nearly all the existing work assumes that the objective function only depends on a limited number of “active” features (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).",1. Introduction,[0],[0]
"For example, Wang et al. (2013) projected the high-dimensional space into a low-dimensional subspace by random embedding and then optimized the acquisition function in a low-dimensional subspace assuming that many dimensions are correlated.",1. Introduction,[0],[0]
"This assumption seems too restrictive in real applications (Kandasamy et al., 2015; Li et al., 2017).",1. Introduction,[0],[0]
"The Add-GP-UCB model (Kandasamy et al., 2015) allows the objective function to vary along the entire feature domain.",1. Introduction,[0],[0]
The objective function is assumed to be the sum of a set of low-dimensional functions with disjoint feature dimensions.,1. Introduction,[0],[0]
Thus the optimization of acquisition function is performed in the low-dimensional space.,1. Introduction,[0],[0]
Li et al. (2016a) further generalized the AddGP-UCB by eliminating an axis-aligned representation.,1. Introduction,[0],[0]
"However, none of them are not applicable if the underlying function does not have assumed structure, that is, if the dimensions are not correlated or if the function is not decomposable in some predefined forms.",1. Introduction,[0],[0]
"Thus efficient Bayesian optimization for high dimensional functions is still an open problem.
",1. Introduction,[0],[0]
To address that we propose an efficient algorithm to optimize the acquisition function in high dimension without requiring any assumption on the structure of the underlying function.,1. Introduction,[0],[0]
We recall a key characteristic of the acquisition function that they are mostly flat functions with only a few peaks.,1. Introduction,[0],[0]
Gradients on the large mostly flat surfaces of the high-dimensional acquisition functions would be close to zero.,1. Introduction,[0],[0]
Thus gradient-dependent methods would fail to work since a random initialization would most likely fall in the large flat region.,1. Introduction,[0],[0]
"However, we theoretically prove that for a location where the gradient is currently insignificant it is possible to find a large enough kernel length-scale which when used to build a new GP can make the derivative of the new acquisition function becomes significant.",1. Introduction,[0],[0]
Different locations may need different length-scales above which the derivative at that location becomes significant.,1. Introduction,[0],[0]
We prove it for both the Expected Improvement (EI) and Upper Confidence Bound (UCB) acquisition functions.,1. Introduction,[0],[0]
"Next, we theoretically prove that the difference in the acquisition func-
tions is smooth with respect to the change in length-scales, which implies that the extremums of the consecutive acquisition functions are close if the difference in the lengthscales is small.",1. Introduction,[0],[0]
Based on these two observations we build a novel optimization algorithm for acquisition functions.,1. Introduction,[0],[0]
In the first part of our algorithm we search for a large enough length-scale for which a randomly selected location in the domain starts to have significant gradients.,1. Introduction,[0],[0]
"Next, we gradually reduce the length scale to move from a gross to a finer function approximation, controlling this transition by slowly reducing the length-scale of the Gaussian process kernel.",1. Introduction,[0],[0]
We solve a sequence of local optimization problems wherein we begin with a very gross approximation of the function and then the extrema of this approximation is used as the initial point for the optimization of the next approximation which is a little bit finer.,1. Introduction,[0],[0]
Following this sequence we reach to the extrema of the acquisition function for the Gaussian process with the target length-scale.,1. Introduction,[0],[0]
The target length-scale is either pre-specified by the user or estimated for some covariance functions.,1. Introduction,[0],[0]
"It may seem to the reader that the problem can be avoided if a large length scale is chosen for the original GP model itself, however, as it shows in Figure 1 the right length-scale actually lies at a smaller value, especially for high dimensional functions.",1. Introduction,[0],[0]
"Since in our algorithm we use Gaussian processes with large to small length-scales akin to fitting an elastic function, we denote our method as Elastic Gaussian Process (EGP) method.",1. Introduction,[0],[0]
We note that EGP is a meta-algorithm that enables a gradient-dependent local optimization algorithm to perform by removing the problem associated with flat surface.,1. Introduction,[0],[0]
Newton’s gradient based method is used as a local optimization tool for our algorithm.,1. Introduction,[0],[0]
"It is to be noted that our algorithm EGP can easily be converted to pursue a global solution by employing multiple starts with different random initializations.
",1. Introduction,[0],[0]
"We demonstrate our algorithm on two synthetic examples and two real-world applications involving one of training
cascaded classifier and the other involving optimization of an alloy based on thermodynamic simulation.",1. Introduction,[0],[0]
"We compare with the the state-of-the-art additive model (Kandasamy et al., 2015), high dimensional optimization using random embedding (Wang et al., 2013), a vanilla multi-start method and 2x random search.",1. Introduction,[0],[0]
All the methods are given equal computational budget to have a fair comparison.,1. Introduction,[0],[0]
In all experiments our proposed method outperforms the baselines.,1. Introduction,[0],[0]
"In summary, our main contributions are:
• Proposal of a new method to handle high dimensional Bayesian optimization without any assumptions about the underlying structure in the objective function;
• Derivation of theoretical guarantees that underpins our proposed algorithm;
• Validation on both synthetic and real-world applications to demonstrate the usefulness of our method.",1. Introduction,[0],[0]
"We briefly review Gaussian process (Rasmussen and Williams, 2005) here.",2.1. Gaussian Process,[0],[0]
"Gaussian process (GP) is a distribution over functions specified by its mean m(.) and covariance kernel function k(., .).",2.1. Gaussian Process,[0],[0]
"Give a set of observations x1:t, the probability of any finite set of f is Gaussian
f(x1:t) ∼ N (m(x1:t),K(x1:t,x1:t))",2.1. Gaussian Process,[0],[0]
"(2.1)
where f(x1:t) is a vector of response values of x1:t and K(x1:t,x1:t) is a covariance matrix presented by
K(x1:t,x1:t) =  k(x1,x1) · · · k(x1,xt)... . . .",2.1. Gaussian Process,[0],[0]
...,2.1. Gaussian Process,[0],[0]
"k(xt,x1) · · · k(xt,xt)  (2.2) where k is a kernel function.",2.1. Gaussian Process,[0],[0]
"If the observations are contaminated with noise, K should include the noise variance.",2.1. Gaussian Process,[0],[0]
The choice of the kernel depends on prior beliefs about smoothness properties of the objective function.,2.1. Gaussian Process,[0],[0]
"A popular kernel function is the squared exponential (SE) function, which is defined as
k(xi,xj) = exp
( − 1
2l2 ||xi − xj ||2 ) where the kernel length-scale l reflects the smoothness of the objective function.
",2.1. Gaussian Process,[0],[0]
The predictive distribution of GP is tractable analytically.,2.1. Gaussian Process,[0],[0]
"For a new point xt+1, the joint probability distribution of the known values f1:t = f(x1:t) and the predicted function value ft+1 is given by(
f1:t ft+1
) ∼ N ([ m(x1:t) m(xt+1) ] , [ K(x,x) k kT k(xt+1,xt+1) ])
",2.1. Gaussian Process,[0],[0]
where k =,2.1. Gaussian Process,[0],[0]
"[k(xt+1,x1) · · · k(xt+1,xt)]T and K(x,x) = K(x1:t,x1:t).",2.1. Gaussian Process,[0],[0]
We simplify the problem by using m(x1:t) = 0.,2.1. Gaussian Process,[0],[0]
"The predictive distribution of ft+1 can be represented by
ft+1 | f1:",2.1. Gaussian Process,[0],[0]
"t ∼ N (µt+1(xt+1 | x1:t, f1:t), σ2t+1(xt+1",2.1. Gaussian Process,[0],[0]
"| x1:t, f1:t))",2.1. Gaussian Process,[0],[0]
(2.3) with µt+1(.),2.1. Gaussian Process,[0],[0]
= kTK−1f1:t and σ2t+1(.),2.1. Gaussian Process,[0],[0]
"= k(xt+1,xt+1)− kTK−1k.",2.1. Gaussian Process,[0],[0]
A traditional optimization problem is to find the maximum or minimum of a function f(x) over a compact domain X .,2.2. Bayesian Optimization,[0],[0]
"In real applications such as hyperparameter tuning for a machine learning model or experiments involving making of physical products, f(x) is unknown in advance and expensive to evaluate.",2.2. Bayesian Optimization,[0],[0]
Bayesian optimization (BO) is a powerful tool to optimize such expensive black-box functions.,2.2. Bayesian Optimization,[0],[0]
A common method to model the unknown function is using a Gaussian process as a prior.,2.2. Bayesian Optimization,[0],[0]
The posterior is maintained based on observations and allows prediction for expected function values at unseen locations (Eq.2.3).,2.2. Bayesian Optimization,[0],[0]
"A acquisition function a(x | x1:t, f1:t) is constructed to guide the search for the optimum.",2.2. Bayesian Optimization,[0],[0]
"Some examples about acquisition functions include Expected Improvement (EI) and UCB (Srinivas et al., 2010).
",2.2. Bayesian Optimization,[0],[0]
The EI-based acquisition function is to compute the expected improvement with respect to the current maximum f(x+).,2.2. Bayesian Optimization,[0],[0]
"The improvement function is written as
I(x) = max{0, ft+1(x)− f(x+)}
ft+1(x) is Gaussian distributed with the mean µ(x) and variance σ2(x), as per the predictive distribution of GP (2.3).",2.2. Bayesian Optimization,[0],[0]
Thus the expected improvement is the expectation over these Gaussian variables.,2.2. Bayesian Optimization,[0],[0]
"In closed form (Mockus et al., 1978; Jones et al., 1998),
EI(x) = { (µ(x)− f(x+))Φ(Z) + σ(x)φ(Z) σ(x) > 0 0 σ(x) = 0
(2.4) where Z = µ(x)−f(x
+) σ(x) .",2.2. Bayesian Optimization,[0],[0]
"Φ(Z) and φ(Z) are the CDF and
PDF of standard normal distribution.
",2.2. Bayesian Optimization,[0],[0]
"The UCB (Srinivas et al., 2010) acquisition function is defined as
UCB(x) = µ(x) + νσ(x) (2.5)
where ν is a sequence of increasing positive numbers.
",2.2. Bayesian Optimization,[0],[0]
"In each iteration of Bayesian optimization, the most promising xt+1 is found by maximizing the acquisition function and then yt+1 is evaluated.",2.2. Bayesian Optimization,[0],[0]
The new observation is augmented to update the GP and in turn is used to construct a new acquisition function.,2.2. Bayesian Optimization,[0],[0]
These steps are repeated till a satisfactory outcome is reached or the iteration budget is exhausted.,2.2. Bayesian Optimization,[0],[0]
"In Bayesian optimization, the objective function is expensive to evaluate while the acquisition function is tractable analytically.",2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
Our task is to maximize the acquisition function a(x | D1:t) over a compact region or with constraints.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
Global optimization heuristics are often used to find the extremum of a function.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
The gradient-based and derivativefree approaches are two main types.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
"Gradients for the EI acquisition function can be computed as in (Frean and Boyle, 2008).",2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
"DIRECT (Jones et al., 1993) is a popular choice to globally optimize the acquisition function.",2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
It is a deterministic and derivative-free algorithm which divides the search space into smaller and smaller hyperrectangles and leverages the Lipschitizian-continuity of the acquisition function.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
However.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
it takes time exponential to dimension and becomes practically infeasible beyond 10 dimensions.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
The multi-start gradient based approach is potentially attractive in high dimension but it mostly fails in high-dimensional scenario as a random initialization may not be able to escape from the large flat region that acquisition functions generally have.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
Our proposed method utilizes properties of the acquisition function to derive a meta-algorithm that enables the gradient-based optimizer to move even when initialized in a seemingly flat region.,2.2.1. OPTIMIZATION OF ACQUISITION FUNCTIONS,[0],[0]
"We would like to employ Bayesian optimization to solve a high-dimensional maximization problem maxx∈X f(x) in a compact subset X ⊆ R. To model f(x), we use a Gaussian process with zero mean as a prior and the SE kernel as the covariance function with a target length-scale lτ .",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"The target length-scale can be set by the user or can be separately inferred by using the maximum likelihood-based estimation method (Snoek et al., 2012).",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"The SE kernel although a simple kernel, is versatile and popular.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
Hence we choose to use it in our framework.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"Acquisition functions, such as EI (Mockus, 1994) and UCB (Srinivas et al., 2010), depend on the predictive mean µ(x) and variance σ2(x) of GP
µ(x) = kTK−1y
σ2(x) = 1− kTK−1k
Hence, the acquisition function is also associated with the GP kernel length-scale l and we denote it as a(x | D1:t, l).",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"The core task of Bayesian optimization is to find the most promising point xt+1 for the next function evaluation by globally maximizing acquisition function.
",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
Figure 2 serves as an inspired example for our approach.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
We plot the acquisition function for different length-scales.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"As can be seen when the length-scale is low, some portions of the parameter space are flat.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"This is especially remark-
able in high-dimensional problems.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"For example, the acquisition function with length-scale 0.1 is extremely flat.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"However when the length-scale is above 0.2, the acquisition functions starts to have significant gradients.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"Additionally, we also show that the optimal solutions for different length-scales are close.
",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
We construct our method based on the above observations.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
Specifically in Lemma 1 we theoretically guarantee that it is possible to find a large enough length-scale for which the derivative of the acquisition function becomes non-insignificant at any location in the domain.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
Proof for both the Expected Improvement (EI) and Upper Confidence Bound (UCB) based acquisition functions are derived.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"Relying on this guarantee, we search for a large enough length-scale for which a randomly selected location in the domain starts to have significant gradients.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"Next in Lemma 2, we theoretically guarantee that the difference in the acquisition function is smooth with respect to the change in length-scale.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
This implies that the extrema of the consecutive acquisition functions are close but different only due to a small difference in the length-scales.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
The details of these lemmas are provided later.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"However, we can now conceive that an algorithm to overcome flat region can be constructed by first finding a large enough length-scale to solve for the optima at that length-scale and then gradually reduce the length-scale and solve a sequence of local optimization problems wherein the optimum of a larger length-scale is used as the initialization for the optimization of the acquisition function based on the Gaussian process with a smaller length-scale.",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
This is continued till the optimum at the target length-scale lτ is reached.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
We denote our method as Elastic GP (EGP) method.,3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"The whole proposed algorithms are presented in Alg.1 and Alg. 2.
",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"Algorithm 1 High Dimensional Bayesian Optimization with Elastic Gaussian Process
1: for t = 1, 2 · · · do 2: Sample the next point xt+1←argmaxxt+1∈X a(x | D1:t, l) using Alg. 2 3: Evaluate the value yt+1 4: Augment the data D1:t+1 = {D1:t, {xt+1, yt+1}} 5: Update the kernel matrix K 6: end for",3. High-dimensional Bayesian Optimization with Elastic Gaussian Process,[0],[0]
"In the first step of our algorithm (seen in Step 1 of Alg.2) , we want to show that gradient of the acquisition functions becomes significant beyond a certain l so that our algorithm can find an optimal solution compared to any start point.
",3.1. Theoretical Analysis,[0],[0]
Lemma 1. ∃l :,3.1. Theoretical Analysis,[0],[0]
"∥∥∥∥∂a(x)∂x ∥∥∥∥
2
≥ ε for lτ ≤",3.1. Theoretical Analysis,[0],[0]
"l ≤ lmax.
",3.1. Theoretical Analysis,[0],[0]
Proof.,3.1. Theoretical Analysis,[0],[0]
"The Lemma can be proved if we prove that∣∣∣∂a(x)∂xi ∣∣∣ ≥ ε, ∀i.",3.1. Theoretical Analysis,[0],[0]
We consider both forms: UCB and EI.,3.1. Theoretical Analysis,[0],[0]
•,3.1. Theoretical Analysis,[0],[0]
"For the UCB acquisition function (Eq. 2.5),
the partial derivative of UCB can be written as
∂a(x)
∂xi =
∂µ(x)
∂xi + ν
∂σ(x)
∂xi
= ∂kT
∂xi K−1y +
ν σ(x) (−∂k
T
∂xi K−1k)
",3.1. Theoretical Analysis,[0],[0]
"The ∂k T
∂xi is dependent on the form of the covariance func-
tion: it is 1× t matrix whose (1, j)th element is ∂cov(x,xj)∂xi .",3.1. Theoretical Analysis,[0],[0]
"For the SE kernel
∂cov(x,xj)
∂xi = exp
( −||x− xj || 2
2l2
)( − (xi − xji)
l2 ) = −dji
l2 cov(x,xj) (3.1)
where dji = xi − xji.
",3.1. Theoretical Analysis,[0],[0]
"To simplify the proof, we assume that we have the worst case that only one observation x0 exists and thus
∂a(x)
",3.1. Theoretical Analysis,[0],[0]
"∂xi
= y0∂cov(x,x0) ∂xi − vcov(x,x0)√ 1− cov2(x,",3.1. Theoretical Analysis,[0],[0]
"x0) ∂cov(x,x0) ∂xi
= −d0i l2",3.1. Theoretical Analysis,[0],[0]
"cov(x,x0)y0 +",3.1. Theoretical Analysis,[0],[0]
"vcov(x,x0)√ 1− cov2(x,x0) d0i l2 cov(x,x0)
= d0i l2 ( vcov2(x,x0)√ 1− cov2(x,x0)",3.1. Theoretical Analysis,[0],[0]
"− cov(x,x0)y0 ) (3.2)
",3.1. Theoretical Analysis,[0],[0]
Algorithm 2,3.1. Theoretical Analysis,[0],[0]
"Optimizing acquistion function using EGP Input: a random start point xinit ∈ χ, the length-scale
interval4l, l = lτ .
1: Step 1: 2: while l ≤ lmax do 3: Sample x∗",3.1. Theoretical Analysis,[0],[0]
← argmaxx∗∈Xa(x,3.1. Theoretical Analysis,[0],[0]
"| D1:t, l) starting with xinit; 4: if ||xinit − x∗|| = 0",3.1. Theoretical Analysis,[0],[0]
"then 5: l = l +4l 6: else 7: xinit = x∗, break; 8: end if 9: end while
10: Step 2: 11: while l ≥ lτ do 12:",3.1. Theoretical Analysis,[0],[0]
l = l −4l 13: Sample x∗ ← argmaxx∗∈Xa(x,3.1. Theoretical Analysis,[0],[0]
"| D1:t, l) starting with xinit; 14: if ||xinit − x∗|| = 0",3.1. Theoretical Analysis,[0],[0]
"then 15: 4l=4l/2 16: else 17: xinit = x∗ 18: end if 19: end while
Output: the optimal point xt+1",3.1. Theoretical Analysis,[0],[0]
"= x∗ to be used in Alg.1
The cov(x,x0) is a small value due to ||x − x0|| 0",3.1. Theoretical Analysis,[0],[0]
and then the first term in the fourth line of the Eq.(3.2) can be ignored compared to its second term.,3.1. Theoretical Analysis,[0],[0]
"Therefore we have
∂a(x) ∂xi = −d0i l2 cov(x,x0)y0
= −d0iy0 l2 exp
( −||x− x0|| 2
2l2
) (3.3)
",3.1. Theoretical Analysis,[0],[0]
"We rewrite it as∣∣∣∣∂a(x)∂xi ∣∣∣∣ = α1l2 exp(−α2l2 )
where α1 = |d0iy0| and α2 = ||x− x0||2/2.",3.1. Theoretical Analysis,[0],[0]
"To have ∣∣∣∂a(x)∂xi ∣∣∣ ≥ ε, the equation exp (−α2l2 ) ≥ εl2α1 must hold for a l between lτ ≤",3.1. Theoretical Analysis,[0],[0]
l ≤ lmax.,3.1. Theoretical Analysis,[0],[0]
"In fact, we can find a l to hold the inequality since exp ( −α2l2 ) is a decreasing function with the range (0, 1] whilst εl 2
α1 is an increasing
function with the range (0,+∞) by considering lτ can approach to 0 and lmax can approach to infinity in theory.",3.1. Theoretical Analysis,[0],[0]
"Therefore Lemma 1 has been proved for the UCB acquisition function.
",3.1. Theoretical Analysis,[0],[0]
"• For the EI acquisition function (Eq. 2.4),
the partial derivative can be written as
∂a(x)
∂xi =",3.1. Theoretical Analysis,[0],[0]
"[ZΦ(Z) + φ(Z)]
∂σ(x)
∂xi + σ(x)Φ(Z)
∂Z
∂xi
where Z = µ(x)−f(x +)
σ(x) and
∂σ(x)
∂xi = −
( ∂kT
∂xi K−1k
) /σ(x)
∂Z",3.1. Theoretical Analysis,[0],[0]
"∂xi =
( ∂kT
∂xi K−1y − Z ∂σ(x) ∂xi
) /σ(x)
therefore,
∂a(x)
∂xi = −φ(Z)
( ∂kT
∂xi K−1k
) /σ(x)+Φ(Z)",3.1. Theoretical Analysis,[0],[0]
"∂kT
∂xi K−1y
(3.4)",3.1. Theoretical Analysis,[0],[0]
We substitute Eq.(3.1) into Eq.,3.1. Theoretical Analysis,[0],[0]
(3.4) and make the similar assumption within the proof at the UCB acquisition function.,3.1. Theoretical Analysis,[0],[0]
"The Eq. (3.4) then becomes as
∂a(x)
∂xi
= y0Φ(Z)∂cov(x,x0) ∂xi − φ(Z)cov(x,x0)√ 1− cov2(x",3.1. Theoretical Analysis,[0],[0]
",x0) ∂cov(x,x0) ∂xi
= d0i l2
( φ(Z)cov2(x,x0)√
1− cov2(x,x0)",3.1. Theoretical Analysis,[0],[0]
"− cov(x,xj)y0Φ(Z)
)
Since φ(Z) lies 0-1, we can ignore the first term.",3.1. Theoretical Analysis,[0],[0]
"The equation above is further written as
∂a(x) ∂xi = −d0i l2 exp
( −||x− x0|| 2
2l2
) y0Φ(
µ(x)− f(x+) σ(x) )
",3.1. Theoretical Analysis,[0],[0]
"As l increasing, µ(x) becomes smaller and σ(x) becomes larger and then Φ(·) → 1.",3.1. Theoretical Analysis,[0],[0]
The equation above becomes similar with Eq.(3.3).,3.1. Theoretical Analysis,[0],[0]
"Therefore Lemma 1 is proved for EI.
",3.1. Theoretical Analysis,[0],[0]
"In the second step of our algorithm (seen in Step 2 of Alg.2), our purpose is to find 4l which makes the start point of the local optimizer move to a finer region.",3.1. Theoretical Analysis,[0],[0]
We need to show that∣∣∣∣∣∂a(x)∂xi,3.1. Theoretical Analysis,[0],[0]
∣∣∣∣,3.1. Theoretical Analysis,[0],[0]
"l=l∗ − ∂a(x) ∂xi ∣∣∣∣ l=l∗+4l
∣∣∣∣∣ ≤ ε, for4l",3.1. Theoretical Analysis,[0],[0]
< δ,3.1. Theoretical Analysis,[0],[0]
"It is directly related to∂a(x|D1:t,l)∂x being smooth.",3.1. Theoretical Analysis,[0],[0]
"The following lemma guarantees that.
",3.1. Theoretical Analysis,[0],[0]
Lemma 2.,3.1. Theoretical Analysis,[0],[0]
"g(x, l) is a smooth function with respect to l, where g(x, l) = ∂a(x|D1:t,l)∂x .
",3.1. Theoretical Analysis,[0],[0]
"For the UCB, we compute the derivative of g(x, l) with respect to l based on Eq.(3.3)
∂g(xi, l)
∂l = 2d0iy0 l3 exp
( −||x− x0|| 2
2l2 ) + d0iy0 l2 exp ( −||x− x0|| 2 2l2 ) ||x−",3.1. Theoretical Analysis,[0],[0]
"x0||2 l3
Apparently, ∂g(xi,l)∂l is continuous in the domain of l. Therefore, g(x, l) is a smooth function with respect to l. The similar proof can be done for EI.",3.1. Theoretical Analysis,[0],[0]
We evaluate our method on three different benchmark test functions and two real-world applications including training cascaded classifiers and for alloy composition optimization.,4. Experiments,[0],[0]
"We use low memory BFGS implementation in NLopt (Johnson, 2014) as the local optimization algorithm, and a variant of DIRECT (GN DIRECT L) for global optimization1.",4. Experiments,[0],[0]
"Our comparators are:
• Global optimization using DIRECT (Global)
• Multi-start local optimization (Multi-start)
• High-dimensional Bayesian optimization via additive models (Kandasamy et al., 2015) (Add-d′, where d′ is the dimensionality in each group)
",4. Experiments,[0],[0]
•,4. Experiments,[0],[0]
"Bayesian optimization using random embedding (Wang et al., 2013) (REMBO-d′, where d′ is the projected dimensionality)
",4. Experiments,[0],[0]
"• Best of 2 x Random search, which has shown competitiveness over many algorithms(Li et al., 2016b).
",4. Experiments,[0],[0]
Global optimization with DIRECT is used only at dimension d ≤ 10 as it consistently returns erroneous results at higher dimension.,4. Experiments,[0],[0]
"For the additive model variables are divided into a set of additive clusters by maximizing the marginal likelihood (Kandasamy et al., 2015).",4. Experiments,[0],[0]
"In all experiments, we use EI as the acquisition function and the SE kernel as the covariance function.",4. Experiments,[0],[0]
"The search bounds are rescaled to [0, 1].",4. Experiments,[0],[0]
"We use the target length-scale lτ = 0.1, lmax = √ d and 4lmin = 10−5.",4. Experiments,[0],[0]
"In Figure 1 we plot the simple regret vs iteration for three different choices of scale l = 0.1, 0.3 and 0.5.",4. Experiments,[0],[0]
"Out of them l = 0.1 provides the fastest convergence, justifying our choice for the lengthscale.",4. Experiments,[0],[0]
"In our experience any smaller length-scale slows down the convergence and surprisingly, in most of the cases lτ = 0.1 turns out to be a good choice.",4. Experiments,[0],[0]
The number of initial observations are set at d + 1.,4. Experiments,[0],[0]
"All the algorithms are
1The code is available on request.
",4. Experiments,[0],[0]
given the same fixed time duration per iteration (Topt).,4. Experiments,[0],[0]
"The computer used is a Xeon Quad-core PC running at 2.6 GHz, with 16 GB of RAM.",4. Experiments,[0],[0]
Bayesian optimization has been implemented in Matlab with mex interface to a C-based acquisition function optimizer that uses NLOPT library.,4. Experiments,[0],[0]
We run each algorithm 20 trials with different initializations and report the average results and standard errors .,4. Experiments,[0],[0]
"In this study we demonstrate the application of Bayesian optimization on three different benchmark test functions
1.",4.1. Benchmark Test Functions,[0],[0]
"Hertmann6d in [0, 1] for all dimensions.
",4.1. Benchmark Test Functions,[0],[0]
2.,4.1. Benchmark Test Functions,[0],[0]
"Unnormalized Gaussian PDF with a maximum of 1 in [−1, 1]d for d=20 and [−0.5, 0.5]d for d=50.
",4.1. Benchmark Test Functions,[0],[0]
3.,4.1. Benchmark Test Functions,[0],[0]
"Generalized Rosenbrock function (Picheny et al., 2013) in [−5, 10]d.
",4.1. Benchmark Test Functions,[0],[0]
"We set the covariance matrix of the Gaussian PDF to be
a block diagonal matrix Σ =  A · · · 0... . . .",4.1. Benchmark Test Functions,[0],[0]
"... 0 · · · A , where A = [ 1 0.9 0.9 1 ] .",4.1. Benchmark Test Functions,[0],[0]
"In this case, variables are partially correlated and, therefore, the function does not admit additive decomposition with high probability.",4.1. Benchmark Test Functions,[0],[0]
"The function is further scaled such that the maximum value of the function
remains at 1 irrespective of the number of variables (dimensions).",4.1. Benchmark Test Functions,[0],[0]
"Since, for all these test functions neither the assumptions of additive decomposition based method or the assumptions of REMBO (many dimensions are correlated) are true, they perform poorly on these.",4.1. Benchmark Test Functions,[0],[0]
"Hence, we do not include them in our comparison for benchmark functions.
",4.1. Benchmark Test Functions,[0],[0]
We first demonstrate the efficiency of our EGP based optimization given limited amount of time.,4.1. Benchmark Test Functions,[0],[0]
In Figure 3 we show how the three algorithms perform when given two different amounts of optimization time per iteration (Topt = 0.001×d and Topt = 0.01×d) on both Hertmann6 and Gaussian PDF functions.,4.1. Benchmark Test Functions,[0],[0]
"The plot shows that when Topt is small then Multi-start performs the worst, even performing lower than the 2x Random search.",4.1. Benchmark Test Functions,[0],[0]
"However, both EGP and the DIRECT perform much better and almost perform similarly.",4.1. Benchmark Test Functions,[0],[0]
When Topt is increased then all the methods start to perform almost similarly with EGP providing slightly better performance.,4.1. Benchmark Test Functions,[0],[0]
"This demonstrates two things: a) EGP is more efficient in using time than the Multi-start, and",4.1. Benchmark Test Functions,[0],[0]
b) EGP being gradient-based is more numerically precise than the grid-based DIRECT algorithm.,4.1. Benchmark Test Functions,[0],[0]
In Figure 4 we demonstrate our method on both Rosenbrock and Gaussian PDF functions at high dimensions.,4.1. Benchmark Test Functions,[0],[0]
The optimization time for all these high-dimensional optimization problem is set as Topt = 0.1×d sec.,4.1. Benchmark Test Functions,[0],[0]
EGP clearly beats all the comparators for these benchmark test functions.,4.1. Benchmark Test Functions,[0],[0]
Then UCB acquisition function has the similar behaviour with EI for our model although no result shown here.,4.1. Benchmark Test Functions,[0],[0]
"Here we evaluate our method by training a cascade classifier on three real datasets from UCI repository (Blake and Merz, 1998): Ionosphere, German and IJCNN1.",4.2. Training cascade classifier,[0],[0]
"A K-cascade classifier consists of K stages and each stage has a weak classifier, a one-level decision stump.",4.2. Training cascade classifier,[0],[0]
Instances are re-weighted after each stage.,4.2. Training cascade classifier,[0],[0]
"Generally, independently computing the thresholds are not an optimal strategy and thus we seek to find an optimal set of thresholds by maximizing the training AUC.",4.2. Training cascade classifier,[0],[0]
"Features in all datesets are scaled between [0, 1].",4.2. Training cascade classifier,[0],[0]
The number of stages is set same with the number of features in the dataset.,4.2. Training cascade classifier,[0],[0]
"Therefore, simultaneously optimizing thresholds in multiple stages is a difficult task and thus being used as a challenging test case for highdimensional Bayesian optimization.",4.2. Training cascade classifier,[0],[0]
We create the additive model with 10 dimensions per group and 10 dimensions for random embedding in REMBO.,4.2. Training cascade classifier,[0],[0]
The results are plotted in Figure 5 (a)-(c).,4.2. Training cascade classifier,[0],[0]
In all three cases EGP provides the best performance.,4.2. Training cascade classifier,[0],[0]
REMBO performs the worst of all.,4.2. Training cascade classifier,[0],[0]
"Surprisingly, for IJCNN1 the Random Search turned out to be competitive to EGP.",4.2. Training cascade classifier,[0],[0]
"However, in the other two datasets it performs much worse than EGP.",4.2. Training cascade classifier,[0],[0]
AA-2050 is a low density high corrosion resistant alloy and is used for aerospace applications.,4.3. Optimizing alloy for aeronautic applications,[0],[0]
The current alloy has been designed decades ago and is considered by our metallurgist collaborator as a prime candidate for further improvement.,4.3. Optimizing alloy for aeronautic applications,[0],[0]
"ThermoCalc, a software based thermodynamic simulator (Andersson et al., 2002), is used to compute the utility of a composition by looking at the occurrences of different phases at different temperatures.",4.3. Optimizing alloy for aeronautic applications,[0],[0]
Some phases are beneficial for mechanical properties whereas some are not.,4.3. Optimizing alloy for aeronautic applications,[0],[0]
"Guided by our metallurgist partners a weighted combination of phase fractions is used as the utility function The alloy consists of 9 elements (Al, Cu, Mg, Zn, Cr, Mn, Ti, Zr, and Sc) and along with that we have 4 operational pa-
rameters that have to be optimized together.",4.3. Optimizing alloy for aeronautic applications,[0],[0]
In all we have a 13-dimensional optimization problem.,4.3. Optimizing alloy for aeronautic applications,[0],[0]
The result is given in Figure 5(d).,4.3. Optimizing alloy for aeronautic applications,[0],[0]
"Since, the simulations are expensive we run only upto 30 iterations and compare with only the additive model and the REMBO.",4.3. Optimizing alloy for aeronautic applications,[0],[0]
We ran only once after starting from expert-specified starting points.,4.3. Optimizing alloy for aeronautic applications,[0],[0]
"Clearly, EGP is quicker in reaching better value and always remained better than both the baselines.",4.3. Optimizing alloy for aeronautic applications,[0],[0]
In this paper we propose a novel algorithm for Bayesian optimization in high dimension.,5. Conclusion,[0],[0]
At high dimension the acquisition function becomes very flat on a large region of the space rendering gradient-dependent methods to fail at high dimension.,5. Conclusion,[0],[0]
We prove a) gradient can be induced by increasing the length-scales of the GP prior and b) acquisition functions which differ only due to small difference in length-scales are close.,5. Conclusion,[0],[0]
"Based on these we formulate our algorithm that first finds a large enough length-scale to enable the gradient-dependent optimizer to perform, and then the gradually reduces the length-scale while also sequentially using the optimum of the larger length-scale as the initialization for the smaller.",5. Conclusion,[0],[0]
In experiments the proposed algorithm clearly performs better than the baselines on a set of test functions and two real applications of training cascade classifiers and alloy composition optimization.,5. Conclusion,[0],[0]
This work is partially funded by Australian Government through ARC and the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning.,Acknowledgement,[0],[0]
Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).,Acknowledgement,[0],[0]
We thank our metallurgist collaborators Dr.Thomas Dorin from Institute of Frontier Materials Deakin and his team for the alloy case study and anonymous reviewers for their valuable comments.,Acknowledgement,[0],[0]
Bayesian optimization is an efficient way to optimize expensive black-box functions such as designing a new product with highest quality or tuning hyperparameter of a machine learning algorithm.,abstractText,[0],[0]
"However, it has a serious limitation when the parameter space is high-dimensional as Bayesian optimization crucially depends on solving a global optimization of a surrogate utility function in the same sized dimensions.",abstractText,[0],[0]
"The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely sharp at high dimension having only a few peaks marooned in a large terrain of almost flat surface.",abstractText,[0],[0]
Global optimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-dependent methods cannot move if initialized in the flat terrain.,abstractText,[0],[0]
"We propose an algorithm that enables local gradient-dependent algorithms to move through the flat terrain by using a sequence of gross-tofiner Gaussian process priors on the objective function as we leverage two underlying facts a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the parameter space, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales.",abstractText,[0],[0]
Theoretical guarantees are provided and experiments clearly demonstrate the utility of the proposed method on both benchmark test functions and real-world case studies.,abstractText,[0],[0]
High Dimensional Bayesian Optimization with Elastic Gaussian Process,title,[0],[0]
"Estimators for high-dimensional parametric (linear) models have been developed and analyzed extensively in the last two decades (see for example (Bühlmann & van de Geer, 2011; Vershynin, 2015) for comprehensive overviews).",1. Introduction,[0],[0]
"While being a useful testbed for illustrating conceptual phenomenon, they often suffer from a lack of flexibility in modeling real-world situations.",1. Introduction,[0],[0]
"On the other hand, completely nonparametric models, although flexible, suffer from the curse of dimensionality unless restrictive additive sparsity or smoothness assumptions are imposed (Ravikumar et al., 2009; Yuan et al., 2016).",1. Introduction,[0],[0]
"An interesting compromise between the parametric and nonparametric models is provided by the so-called semiparametric index models (Horowitz, 2009).",1. Introduction,[0],[0]
"Here, the response and the covariate are linked through a low-dimensional nonparametric function that takes in as input a linear transformation of the covariate.",1. Introduction,[0],[0]
"The nonparametric component is also called as the link function and the linear components are
1 Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ 08544, USA.",1. Introduction,[0],[0]
"Correspondence to: Han Liu <hanliu@princeton.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"called as the indices.
",1. Introduction,[0],[0]
"In this work, we focus on the simplest family of such models, the single index models (SIMs), which assume that the response Y and the covariate X satisfy Y = f(〈X,β∗〉) + , where β∗ is the true signal, is the mean-zero random noise, and f is a univariate link function.",1. Introduction,[0],[0]
(see §2 for the precise definition).,1. Introduction,[0],[0]
"They form the basis of more complicated models such as Multiple Index Models (MIMs) (Diaconis & Shahshahani, 1984) and Deep Neural Networks (DNNs) (LeCun et al., 2015), which are cascades of MIMs.",1. Introduction,[0],[0]
"Moreover, we focus on the task of estimating the parametric (linear) component β∗ without the knowledge of the nonparametric part f in the high-dimensional setting, where the number of samples is much smaller than the dimensionality of β∗.
Estimating the parametric component without depending on the specific form of the nonparametric part appears naturally in several situations.",1. Introduction,[0],[0]
"For example, in one-bit compressed sensing (Boufounos & Baraniuk, 2008) and sparse generalized linear models (Loh & Wainwright, 2015), we are interested in recovering the true signal vector based on nonlinear measurements.",1. Introduction,[0],[0]
"Furthermore, in a DNN, the activation function is pre-specified and the task is to estimate the linear components, which are used for prediction in the test stage.",1. Introduction,[0],[0]
"Performing nonlinear least-squares in this setting, leads to nonconvex optimization problems that are invariably sub-optimal without further assumptions.",1. Introduction,[0],[0]
"Hence, developing estimators for the linear component that are both statistically accurate and computationally efficient for a class of activation functions provide a compelling alternative.",1. Introduction,[0],[0]
"Understanding such estimators for SIMs is hence crucial for understanding the more complicated DNNs.
",1. Introduction,[0],[0]
"Although SIMs appear to be a simple extension of the standard linear models, most existing work in the highdimensional setting assume X follows a Gaussian distribution for estimating β∗ without the knowledge of the nonparametric part.",1. Introduction,[0],[0]
It is not clear whether those estimation methods are still valid and optimal when X is drawn from a more general class of distributions.,1. Introduction,[0],[0]
"To relax the Gaussian assumption, we study the setting where the distribution of X is non-Gaussian but known a priori.",1. Introduction,[0],[0]
There are significant challenges that appear when we are dealing with estimators for SIMs.,1.1. Challenges of the Single Index Models,[0],[0]
"They can be summarized as assumptions on either the link function or the data distribution (for example, non-Gaussian assumption).
1.",1.1. Challenges of the Single Index Models,[0],[0]
"Knowledge of link function: Suppose the link function is known, for example, f(u) = u2 which corresponds to the phase retrieval model (see (Jaganathan et al., 2015) for a survey and history of this model).",1.1. Challenges of the Single Index Models,[0],[0]
"Then using an M-estimator to estimate β∗ is a natural procedure (Jaganathan et al., 2015).",1.1. Challenges of the Single Index Models,[0],[0]
"But computationally the problem becomes nonconvex and one need to resort to either SDP based convex relaxations that are computationally expensive or do non-convex alternating minimization that require Gaussian assumptions on the data for successful initialization in the highdimensional setting (Cai et al., 2015).",1.1. Challenges of the Single Index Models,[0],[0]
"Furthermore, if the link function is changed, it might become challenging or impossible to obtain provably computable estimators.
",1.1. Challenges of the Single Index Models,[0],[0]
2.,1.1. Challenges of the Single Index Models,[0],[0]
"Knowledge of data distribution: Now suppose we want to be agnostic about the link function, i.e., we want to estimate the linear component for a general class of link functions.",1.1. Challenges of the Single Index Models,[0],[0]
Then it becomes necessary to make assumptions about the distribution from which the covariates are sampled from.,1.1. Challenges of the Single Index Models,[0],[0]
"In particular, assuming the covariate has Gaussian and symmetric elliptical distributions respectively, (Plan & Vershynin, 2016) and (Goldstein et al., 2016) propose estimators in the high-dimensional setting for a large class of unknown link functions.
",1.1. Challenges of the Single Index Models,[0],[0]
"As mentioned previously, our estimators are based on Stein’s Lemma for non-Gaussian distributions, which utilizes the score function.",1.1. Challenges of the Single Index Models,[0],[0]
Estimating with the score function is challenging due to their heavy tails.,1.1. Challenges of the Single Index Models,[0],[0]
"In order to illustrate that, consider the univariate histograms provided in Figure1.",1.1. Challenges of the Single Index Models,[0],[0]
"The dark shaded, more concentrated one corresponds to the histogram of 10000 i.i.d.",1.1. Challenges of the Single Index Models,[0],[0]
samples from Gamma distribution with scale and shape parameters set to 5 and 0.2 respectively.,1.1. Challenges of the Single Index Models,[0],[0]
The transparent histogram corresponds to the distribution of the score function of the same Gamma distribution.,1.1. Challenges of the Single Index Models,[0],[0]
"Note that even when the actual Gamma distribution is well concentrated, the distribution of the corresponding score function is well-spread and heavy-tailed.",1.1. Challenges of the Single Index Models,[0],[0]
"In the high dimensional setting, in order to estimate with the score functions, we require certain vectors or matrices based on the score functions to be well-concentrated in appropriate norms.",1.1. Challenges of the Single Index Models,[0],[0]
"In order to achieve that, we construct robust estimators via careful truncation arguments to balance the bias (due to thresholding)-variance (of the estimator) tradeoff and achieve the required concentration.",1.1. Challenges of the Single Index Models,[0],[0]
There is a significant body of work on SIMs in the lowdimensional setting.,1.2. Related Work,[0],[0]
They are based on assumptions on either the distribution of the covariate or the link functions.,1.2. Related Work,[0],[0]
"Assuming a monotonic link function, (Han, 1987; Sherman, 1993) propose the maximum rank correlation estimator exploiting the relationship between monotonic functions and rank-correlations.",1.2. Related Work,[0],[0]
"Furthermore, (Li & Duan, 1989) propose an estimator for a wide class of unknown link functions under the assumption that the covariate follows a symmetric elliptical distribution.",1.2. Related Work,[0],[0]
This assumption is restrictive as often times the covariates are not from a symmetric distribution.,1.2. Related Work,[0],[0]
"For example, in several economic applications where the covariates are usually highly skewed and heavy-tailed (Horowitz, 2009).",1.2. Related Work,[0],[0]
"A line of work for estimation in SIMs is proposed by Ker-Chau Li which is based on sliced inverse regression (Li, 1991) and principal Hessian directions (Li, 1992) .",1.2. Related Work,[0],[0]
"These estimators are based on similar symmetry assumptions and involve computing second-order (conditional and unconditional) moments which are difficult to estimate in high-dimensions without restrictive assumptions.
",1.2. Related Work,[0],[0]
"The success of Lasso and related linear estimators in highdimensions (Bühlmann & van de Geer, 2011), also enabled the exploration of high-dimensional SIMs.",1.2. Related Work,[0],[0]
"Although, this is very much work in progress.",1.2. Related Work,[0],[0]
"As mentioned previously, (Plan & Vershynin, 2016) show that the Lasso estimator works for the SIMs in high dimensions when the data is Gaussian.",1.2. Related Work,[0],[0]
"A more tighter albeit an asymptotic results under the same setting was proved in (Thrampoulidis et al., 2015).",1.2. Related Work,[0],[0]
"Very recently (Goldstein et al., 2016) extend
the results of (Li & Duan, 1989) to the high dimensional setting but it suffers from similar problems as mentioned in the low-dimensional setting.",1.2. Related Work,[0],[0]
"For the case of monotone nonparametric component, (Yang et al., 2015) analyze a non-convex least squares approach under the assumption that the data is sub-Gaussian.",1.2. Related Work,[0],[0]
"However, the success of their method hinges on the knowledge of the link function.",1.2. Related Work,[0],[0]
"Furthermore, (Jiang & Liu, 2014; Lin et al., 2015; Zhu et al., 2006) analyze the sliced inverse regression estimator in the high-dimensional setting concentrating mainly on support recovery and consistency properties.",1.2. Related Work,[0],[0]
"Similar to the low-dimensional case, the assumptions made on the covariate distribution restrict them from several real-world applications involving non-Gaussian or non-symmetric covariate, for example high-dimensional problems in economics (Fan et al., 2011).",1.2. Related Work,[0],[0]
"Furthermore, several results are established on a case-by-case basis for fixed link function.",1.2. Related Work,[0],[0]
"Specifically (Boufounos & Baraniuk, 2008; Ai et al., 2014) and (Davenport et al., 2014) consider 1-bit compressed sensing and matrix completion respectively, where the link is assumed to be the sign function.",1.2. Related Work,[0],[0]
"Also, (Waldspurger et al., 2015) and (Cai et al., 2015) propose and analyze convex and non-convex estimators for phase retrieval respectively, in which the link is the square function.",1.2. Related Work,[0],[0]
"All the above works, except (Ai et al., 2014) make Gaussian assumptions on the data and are specialized for the specific link functions.",1.2. Related Work,[0],[0]
"The non-asymptotic result obtained in (Ai et al., 2014) is under sub-Gaussian assumptions, but the estimator is not consistent.",1.2. Related Work,[0],[0]
"Finally, there is a line of work focussing on estimating both the parametric and the nonparametric component (Kalai & Sastry, 2009; Kakade et al., 2011; Alquier & Biau, 2013; Radchenko, 2015).",1.2. Related Work,[0],[0]
"We do not focus on this situation in this paper as mentioned before.
",1.2. Related Work,[0],[0]
"To summarize, all the above works require restrictive assumption on either the data distribution or on the link function.",1.2. Related Work,[0],[0]
We propose and analyze an estimator for a class of (unknown) link functions for the case when the covariates are drawn from a non-Gaussian distribution – under the assumption that we know the distribution a priori.,1.2. Related Work,[0],[0]
"Note that in several situations, one could fit specialized distributions, to real-world data that is often times skewed and heavytailed, so that it provides a good generative model of the data.",1.2. Related Work,[0],[0]
"Also, mixture of Gaussian distribution, with the number of components selected appropriately, approximates the set of all square integrable distributions to arbitrary accuracy (see for example (McLachlan & Peel, 2004)).",1.2. Related Work,[0],[0]
"Furthermore, since this is a density estimation problem it is unlabeled and there is no issue of label scarcity.",1.2. Related Work,[0],[0]
Hence it is possible to get accurate estimate of the distribution in most situations of interest.,1.2. Related Work,[0],[0]
Thus our work is complementary to the existing literature and provides an estimator for a class of models that is not addressed in the previous works.,1.2. Related Work,[0],[0]
"We conclude this section with a summary of our main contri-
butions in this paper:
• We propose estimators for the parametric component of a sparse SIM and low-rank SIM for a class of unknown link function under the assumption that the covariate distribution is non-Gaussian but known a priori.
",1.2. Related Work,[0],[0]
"• We show that it is possible to recover a s-sparse ddimensional vector and a rank-r, d1× d2 dimensional matrix with number of samples of the order of s log d and r(d1 +d2) log(d1 +d2) respectively under significantly mild moment assumptions in the SIM setting.
",1.2. Related Work,[0],[0]
• We provide numerical simulation results that confirm our theoretical predictions.,1.2. Related Work,[0],[0]
"In this section, we introduce the notation and define the single index models.",2. Single Index Models,[0],[0]
"Throughout this work, we use [n] to denote the set {1, . . .",2. Single Index Models,[0],[0]
", n}.",2. Single Index Models,[0],[0]
"In addition, for a vector v ∈",2. Single Index Models,[0],[0]
"Rd, we denote by ‖v‖p the `p-norm of v for any p ≥ 1.",2. Single Index Models,[0],[0]
"We use Sd−1 to denote the unit sphere in Rd, which is defined as Sd−1 = {v ∈ Rd : ‖v‖2 = 1}.",2. Single Index Models,[0],[0]
"In addition, we define the support of v ∈ Rd as supp(v) = {j ∈",2. Single Index Models,[0],[0]
"[d], vj 6= 0}.",2. Single Index Models,[0],[0]
"Moreover, we denote the nuclear norm, operator norm, and Frobenius norm of a matrixA ∈ Rd1×d2 by ‖·‖?, ‖·‖op, and ‖ · ‖fro, respectively.",2. Single Index Models,[0],[0]
"We denote by vec(A) the vectorization of matrix A, which is a vector in Rd1·d2 .",2. Single Index Models,[0],[0]
"For two matrices A,B ∈ Rd1×d2 we define the trace inner product as 〈A,B〉 = Trace(A>B).",2. Single Index Models,[0],[0]
Note that it can be viewed as the standard inner product between vec(A) and vec(B).,2. Single Index Models,[0],[0]
"In addition, for an univariate function g : R → R, we denote by g ◦ (v) and g ◦",2. Single Index Models,[0],[0]
"(A) the output of applying g to each element of a vector v and a matrixA, respectively.",2. Single Index Models,[0],[0]
"Finally, for a random variable X ∈ R with density p, we use p⊗d : Rd → R to denote the joint density of {X1, · · · , Xd}, which are d identical copies of X .
",2. Single Index Models,[0],[0]
Now we are ready to define the statistical model.,2. Single Index Models,[0],[0]
"Let f : R → R be an univariate function and β∗ be the parameter of interest, which is a structured vector or a matrix.",2. Single Index Models,[0],[0]
"The single index model in general is formulated as
Y = f(〈X,β∗〉) + , (2.1)
where X is the covariate, Y ∈ R is the response, and is the exogenous noise that is independent of X .",2. Single Index Models,[0],[0]
"We assume that is centered and has bounded fourth moment, i.e., Ep0( ) = 0 and E( 4) ≤ C for an absolute constant C > 0.",2. Single Index Models,[0],[0]
Note in particular that this allows for heavy-tailed noise as well.,2. Single Index Models,[0],[0]
"In addition, we assume that the entries of X are i.i.d. random variables with density p0.",2. Single Index Models,[0],[0]
"This assumption could be further relaxed using more sophisticated concentration arguments; here we focus on the i.i.d. setting to clearly present the main message of this paper.
",2. Single Index Models,[0],[0]
"Let {(Yi, Xi)}ni=1 be n i.i.d.",2. Single Index Models,[0],[0]
observations of the SIM.,2. Single Index Models,[0],[0]
Our goal is to consistently estimate β∗ without the knowledge of f .,2. Single Index Models,[0],[0]
"In particular, we focus on the case when β∗ is either sparse or low-rank, which are defined as follows.
",2. Single Index Models,[0],[0]
Sparse single index model:,2. Single Index Models,[0],[0]
"In this setting, we assume that β∗ = (β∗1 , · · · , β∗d)> is a sparse vector in Rd with s∗ nonzero entries, such that s∗ n d. Moreover, for the model in (2.1) to be identifiable, we further assume β∗ lies on the unit sphere Sd−1 as the norm of β∗ can always be absorbed in the unknown link function f .
",2. Single Index Models,[0],[0]
Low-rank single index model:,2. Single Index Models,[0],[0]
"In this setting, we assume that β∗ ∈ Rd1×d2 has rank r∗ min{d1, d2}.",2. Single Index Models,[0],[0]
"In this scenario, X ∈ Rd1×d2 and the inner product in (2.1) is 〈X,β∗〉 = Trace(X>β∗).",2. Single Index Models,[0],[0]
"For model identifiability, we further assume that ‖β∗‖F = 1, similar to the sparse case.",2. Single Index Models,[0],[0]
"Our estimator is primarily motivated by an interesting phenomenon illustrated in (Plan & Vershynin, 2016) for the Gaussian setting.",3. Estimation via Score Functions,[0],[0]
"Below, we first briefly summarize the result from (Plan & Vershynin, 2016) and then provide our alternative justification for the same result via Stein’s Lemma.",3. Estimation via Score Functions,[0],[0]
We mainly leverage this alternative justification and propose our estimators for the more general setting we consider.,3. Estimation via Score Functions,[0],[0]
"Assuming for simplicity, we work in the onedimensional setting and are given n i.i.d.",3. Estimation via Score Functions,[0],[0]
samples from the SIM.,3. Estimation via Score Functions,[0],[0]
"Consider the least-squares estimator
β̂LS = argmin β∈R
1
n n∑ i=1",3. Estimation via Score Functions,[0],[0]
"(Yi −Xiβ)2 .
Note that the above estimator is the standard least-squares estimator assuming a linear model (i.e., identity link function).",3. Estimation via Score Functions,[0],[0]
"The surprising observation from (Plan & Vershynin, 2016) is that, under the crucial assumption that X is standard Gaussian, β̂LS is a good estimator of β∗ (up to a scaling) even when the data is generated from the nonlinear SIM.",3. Estimation via Score Functions,[0],[0]
"The same holds true for the high-dimensional setting when the minimization is performed in an appropriately constrained norm-ball (for example, the `1-ball).",3. Estimation via Score Functions,[0],[0]
Hence the theory developed for the linear setting could be leveraged to understand the performance in the SIM setting.,3. Estimation via Score Functions,[0],[0]
"Below, we give an alternative justification for the above estimator as an implication of Stein’s Lemma in the Gaussian case, which is summarized as follows.",3. Estimation via Score Functions,[0],[0]
"Proposition 3.1 (Gaussian Stein’s Lemma (Stein, 1972)).",3. Estimation via Score Functions,[0],[0]
"Let X ∼ N(0, 1) and g : R → R be a continuos function such that E|g′(X)| ≤ ∞.",3. Estimation via Score Functions,[0],[0]
"Then we have E[g(X)X] = E[g′(X)].
",3. Estimation via Score Functions,[0],[0]
"Note that in our context for SIMs, we have E[f ′(X)] ∝ β∗ and E[f(X)X] = E[Y · X].",3. Estimation via Score Functions,[0],[0]
"Now consider the following
estimator, which is based on performing least-squares on the sample version of the above proposition:
β̂SL = argmin β∈R
1
n n∑ i=1",3. Estimation via Score Functions,[0],[0]
"(YiXi − β)2
Note that β̂LS and β̂SL are the same estimators assuming X ∼ N(0, 1), as n",3. Estimation via Score Functions,[0],[0]
"→ ∞. This observation leads to an alternative interpretation of the estimator proposed by (Plan & Vershynin, 2016) via Stein’s Lemma for Gaussian random variables.",3. Estimation via Score Functions,[0],[0]
Thus it provides an alternative justification for why the linear least-squares estimator should work in the SIM setting.,3. Estimation via Score Functions,[0],[0]
"This observation naturally leads to leveraging non-Gaussian versions of Stein’s Lemma for dealing with non-Gaussian covariates.
",3. Estimation via Score Functions,[0],[0]
We now describe our estimator for the non-Gaussian setting based on the above observation.,3. Estimation via Score Functions,[0],[0]
We first define the score function associate to a density.,3. Estimation via Score Functions,[0],[0]
Let p : Rd → R be a probability density function defined on Rd.,3. Estimation via Score Functions,[0],[0]
The score function,3. Estimation via Score Functions,[0],[0]
"Sp : Rd → R associated to p is defined as
Sp(x) = −∇x[log p(x)]",3. Estimation via Score Functions,[0],[0]
"= −∇xp(x)/p(x).
",3. Estimation via Score Functions,[0],[0]
"Note that in the above definition, the derivative is taken with respect to x.",3. Estimation via Score Functions,[0],[0]
This is different from the more traditional definition of the score function where the density belongs to a parametrized family and the derivative is taken with respect to the parameters.,3. Estimation via Score Functions,[0],[0]
"In the rest of the paper to simplify the notation, we omit the subscript x from ∇x.",3. Estimation via Score Functions,[0],[0]
"We also omit the subscript p from Sp when the underlying density p is clear from the context.
",3. Estimation via Score Functions,[0],[0]
We now describe a version of Stein’s Lemma that is applicable for non-Gaussian random variables.,3. Estimation via Score Functions,[0],[0]
"Note from the motivating example for the Gaussian case that while utilizing the Stein’s Lemma for SIM estimation, assumptions on the function in Stein’s Lemma translate directly to those on the link function in SIM.",3. Estimation via Score Functions,[0],[0]
"We now introduce a version of Stein’s Lemma that applies to non-Gaussian random variables and for continuously differentiable functions from (Stein et al., 2004).",3. Estimation via Score Functions,[0],[0]
"A more general version of the Stein’s Lemma that applies to a class of regular functions is available in (Stein et al., 2004).",3. Estimation via Score Functions,[0],[0]
"We assume continuously differentiable functions in the Stein’s Lemma below as they cover a wide range of practical SIM such as generalized linear models and single-layer neural networks.
",3. Estimation via Score Functions,[0],[0]
"Lemma 3.2 (Non-Gaussian Stein’s Lemma (Stein et al., 2004)).",3. Estimation via Score Functions,[0],[0]
"Let g : Rd → R be continuously differentiable function and X ∈ Rd be a random vector with density p : Rd → R, which is also continuously differentiable.",3. Estimation via Score Functions,[0],[0]
"Under the assumption that the expectations E[g(X) · S(X)] and E[∇g(X)] are both well-defined, we have the follow-
ing generalized Stein’s identity E[g(X) · S(X)]",3. Estimation via Score Functions,[0],[0]
"= − ∫ Rd g(x) · ∇p(x)dx
= ∫ Rd ∇g(x) · p(x)dx = E[∇g(X)].",3. Estimation via Score Functions,[0],[0]
"(3.1)
Recall that in the two single index models introduced in §2, X in (2.1) has i.i.d. entries with density p0.",3. Estimation via Score Functions,[0],[0]
"To unify both the vector and matrix settings, in the low-rank SIM, we identify X with vec(X) ∈ Rd where d = d1 · d2.",3. Estimation via Score Functions,[0],[0]
"In this case, X has density p = p⊗d0 and the corresponding score function S :",3. Estimation via Score Functions,[0],[0]
"Rd → Rd is given by
S(x) = −∇ log p(x) = −∇p(x)/p(x) = s0 ◦ (x), (3.2)
where the univariate function s0 = p′0/p0 is applied to each entry of x. Thus S(X) has i.i.d. entries.",3. Estimation via Score Functions,[0],[0]
"In addition, by Lemma 3.2, we have E[S(X)]",3. Estimation via Score Functions,[0],[0]
= 0,3. Estimation via Score Functions,[0],[0]
by setting g to be a constant function in (3.1).,3. Estimation via Score Functions,[0],[0]
"Moreover, in the context of SIMs specified in (2.1), we have
E[Y · S(X)]",3. Estimation via Score Functions,[0],[0]
"= E [ f(〈X,β∗〉) · S(X) ]",3. Estimation via Score Functions,[0],[0]
"= E[f ′(〈X,β∗〉)] · β∗,
as long as the density and the link function satisfy the conditions stated in Lemma 3.2.",3. Estimation via Score Functions,[0],[0]
"This implies that optimization problem
minimize β∈Rd
{ 〈β, β〉 − 2E[Y · 〈S(X), β〉] } (3.3)
has solution β = µ ·β∗, where µ = E[f ′(〈X,β∗〉)].",3. Estimation via Score Functions,[0],[0]
Hence the above program could be used to obtain the unknown β∗ as long as µ 6= 0.,3. Estimation via Score Functions,[0],[0]
"Before we proceed to describe the sample version of the above program, we make the following brief remark.",3. Estimation via Score Functions,[0],[0]
The requirement µ 6= 0 rules out in particular the use of our approach for non-Gaussian phase retrieval (where f(u) = u2) as in that case we have µ = 0 when X is centered.,3. Estimation via Score Functions,[0],[0]
"But we emphasize that the same holds true in the Gaussian and elliptical setting as well, as noted in (Plan & Vershynin, 2016) and (Goldstein et al., 2016).",3. Estimation via Score Functions,[0],[0]
Their methods also fail to recover the true β∗ when the SIM model corresponds to phase retrieval.,3. Estimation via Score Functions,[0],[0]
"We refer the reader to §6 for a discussion on overcoming this limitation.
",3. Estimation via Score Functions,[0],[0]
"Finally, we use a sample version of the above program as an estimator for the unknown β∗.",3. Estimation via Score Functions,[0],[0]
"In order to deal with the high-dimensional setting, we consider a regularized version of the above formulation.",3. Estimation via Score Functions,[0],[0]
"More specifically, we use the `1-norm and nuclear norm regularization in the vector and matrix settings respectively.",3. Estimation via Score Functions,[0],[0]
"However, a major difficulty in the sample setting for this procedure is that E[Y · S(X)] and its empirical counterpart may not be close enough due to a lack of concentration.",3. Estimation via Score Functions,[0],[0]
"Recall our discussion from §1.1 that even if the random variable X is light-tailed, its scorefunction S(x) might be arbitrarily heavy-tailed.",3. Estimation via Score Functions,[0],[0]
"Furthermore, bounded-fourth moment assumption on the noise, Y
too can be heavy-tailed.",3. Estimation via Score Functions,[0],[0]
"Thus the naive method of using the sample version of (3.3) to estimate β∗ leads to sub-optimal statistical rates of convergence.
",3. Estimation via Score Functions,[0],[0]
"To improve concentration and obtain optimal rates of convergence, we replace Y · S(X) with a transformed random variable T (Y,X), which will be defined precisely in §4 for the sparse and low-rank cases.",3. Estimation via Score Functions,[0],[0]
"In particular, T (Y,X) is a carefully truncated version of Y ·S(X), introduced and analyzed in (Catoni et al., 2012; Fan et al., 2016) for related problems, that enables us to obtain well-concentrated estimators.",3. Estimation via Score Functions,[0],[0]
"Thus our final estimator β̂ is defined as the solution to the following regularized optimization problem
minimize β∈Rd
L(β) + λ ·R(β), (3.4)
L(β) = 〈β, β〉 − 2 n n∑ i=1",3. Estimation via Score Functions,[0],[0]
"〈T (Yi, Xi), β 〉 ,
where λ > 0 is the regularization parameter which will be specified later and R(·) is the `1-norm in the vector case and the nuclear norm in the matrix case.",3. Estimation via Score Functions,[0],[0]
"In this section, we state our main results in Theorem 4.2 and Theorem 4.3,which establish the statistical rates of convergence of the estimator defined in §3.",4. Theoretical Results,[0],[0]
The proof for both Theorems is presented in the supplementary material.,4. Theoretical Results,[0],[0]
"Before doing so, we introduce our main moment assumption for the single index model.",4. Theoretical Results,[0],[0]
This assumption is made apart from the assumptions made on the noise and the link function in §2 and §3 respectively.,4. Theoretical Results,[0],[0]
Recall that each entry of the score function defined in (3.2) is equal to s0(u) = −p′0(u)/p0(u).,4. Theoretical Results,[0],[0]
"We first state the assumption and make a few remarks about it.
",4. Theoretical Results,[0],[0]
Assumption 4.1.,4. Theoretical Results,[0],[0]
There exists an absolute constant M > 0 such that E(Y 4) ≤ M and Ep0 [s40(U)],4. Theoretical Results,[0],[0]
"≤ M , where random variable U ∈ R has density p0.
Consider the assumption E(Y 4) ≤ M .",4. Theoretical Results,[0],[0]
By CauchySchwarz inequality we have E(Y 4) ≤,4. Theoretical Results,[0],[0]
"4E( 4) + 4E[f4(〈X,β∗)].",4. Theoretical Results,[0],[0]
"Note that we assum to be centered, independent of X and has bounded fourth moment (see §2).",4. Theoretical Results,[0],[0]
"If the covariate X has bounded fourth moment along the direction of true parameter, since f(·) is continuously differentiable, f(〈X,β∗〉) has bounded fourth moment as well if f(·) is defined on a compact subset of R.",4. Theoretical Results,[0],[0]
Hence the condition E(Y 4) ≤,4. Theoretical Results,[0],[0]
M is relatively easy to satisfy and significantly milder than assuming that Y is bounded or has lighter tails.,4. Theoretical Results,[0],[0]
"Furthermore, Ep0 [s40(U)] ≤ M is relatively mild and satisfied by a wide class of random variables.",4. Theoretical Results,[0],[0]
"Specifically random variables that are non-symmetric and non-Gaussian satisfy this property thereby allowing our approach to work with covariates not previously possible.
",4. Theoretical Results,[0],[0]
We believe it is highly non-trivial to weaken this condition without losing significantly in the rates of convergence that we discuss below.,4. Theoretical Results,[0],[0]
"Under the above assumptions, we first state our theorem on the sparse SIM.",4.1. Sparse Single Index Model,[0],[0]
"As discussed in §3, Y · S(X) can by heavy-tailed and hence we apply truncation to achieve concentration.",4.1. Sparse Single Index Model,[0],[0]
"Denote the j-th entry of the score function S in (3.2) as Sj : Rd → R, j ∈",4.1. Sparse Single Index Model,[0],[0]
[d].,4.1. Sparse Single Index Model,[0],[0]
"We define the truncated response and score function as
Ỹ = sign(Y ) · (|Y | ∧ τ), S̃j(x) = sign[Sj(x)] · [ |Sj(x)| ∧ τ ] , (4.1)
where τ > 0 is a predetermined threshold value.",4.1. Sparse Single Index Model,[0],[0]
"We define Ỹi similarly for all Yi, i ∈",4.1. Sparse Single Index Model,[0],[0]
[n].,4.1. Sparse Single Index Model,[0],[0]
Then we define the estimator β̂,4.1. Sparse Single Index Model,[0],[0]
"as the solution to the optimization problem in (3.4) with T (Yi, Xi) = Ỹi · S̃(Xi) and R(β) = ‖β‖1.",4.1. Sparse Single Index Model,[0],[0]
Here we apply elementwise truncation in T to ensure the sample average of T converges to E[Y ·S(X)] in the `∞-norm for an appropriately chosen τ .,4.1. Sparse Single Index Model,[0],[0]
Note that the `∞-norm is the dual norm of the `1-norm.,4.1. Sparse Single Index Model,[0],[0]
"Such a convergence requirement in the dual norm is standard in the analysis of regularized M - estimators (Negahban et al., 2012) to achieve optimal rates.",4.1. Sparse Single Index Model,[0],[0]
"The following theorem characterizes the convergence rates of β̂.
Theorem 4.2 (Signal recovery for the sparse single index model).",4.1. Sparse Single Index Model,[0],[0]
"For the sparse SIM defined in §2, we assume that β∗ ∈ Rd has s∗ nonzero entries.",4.1. Sparse Single Index Model,[0],[0]
"Under Assumption 4.1, we let τ = 2(M · log d/n)1/4 in (4.1) and set the regularization parameter λ in (3.4) as C √ M · log d/n, where C > 0 is an absolute constant.",4.1. Sparse Single Index Model,[0],[0]
"Then with probability at least 1−d−2, the `1-regularized estimator β̂ defined in (3.4) satisfies
‖β̂",4.1. Sparse Single Index Model,[0],[0]
− µβ∗‖2 ≤,4.1. Sparse Single Index Model,[0],[0]
"√ s∗ · λ, ‖β̂",4.1. Sparse Single Index Model,[0],[0]
"− µβ∗‖1 ≤ 4s∗ · λ.
From this theorem, the `1- and `2-convergence rates of β̂ are ‖β̂",4.1. Sparse Single Index Model,[0],[0]
− µβ∗‖1 = O(s∗ √ log d/n) and ‖β̂,4.1. Sparse Single Index Model,[0],[0]
"− µβ∗‖2 =
O( √ s∗ log d/n), respectively.",4.1. Sparse Single Index Model,[0],[0]
"These rates match the convergence rates of sparse generalized linear models (Loh & Wainwright, 2015) and sparse single index models with Gaussian and symmetric elliptical covariates (Plan & Vershynin, 2016; Goldstein et al., 2016) which are known to be minimax-optimal for this problem via matching lower bounds.",4.1. Sparse Single Index Model,[0],[0]
We next state our theorem for the low-rank SIM.,4.2. Low-rank Single Index Model,[0],[0]
"In this case, we apply the nuclear norm regularization to promote low-rankness.",4.2. Low-rank Single Index Model,[0],[0]
"Note that by definition, T is matrix-valued.
",4.2. Low-rank Single Index Model,[0],[0]
"Since the dual norm of the nuclear norm is the operator norm, we need the sample average of T to converge to E[Y ·S(X)] in the operator norm rapidly to achieve optimal rates of convergence.",4.2. Low-rank Single Index Model,[0],[0]
"To achieve such a goal, we leverage the truncation argument from (Catoni et al., 2012; Minsker, 2016) to construct T (Y,X).
",4.2. Low-rank Single Index Model,[0],[0]
"Let φ : R→ R be a non-decreasing function such that
− log(1−x+x2/2) ≤ φ(x) ≤ log(1+x+x2/2), ∀x",4.2. Low-rank Single Index Model,[0],[0]
"∈ R.
Based on φ, we define a linear mapping ψ :",4.2. Low-rank Single Index Model,[0],[0]
Rd1×d2 → Rd1×d2 as follows.,4.2. Low-rank Single Index Model,[0],[0]
"For any A ∈ Rd1×d2 , let
Ã = [ 0 A A> 0 ] and let ΥΛΥ> be the eigenvalue decomposition of Ã.",4.2. Low-rank Single Index Model,[0],[0]
"In addition, let B = Υ[φ ◦ (Λ)]Υ>, where ψ is applied elementwisely on Λ. Then we write B in block from as
B =",4.2. Low-rank Single Index Model,[0],[0]
[ B11 B12 B21 B22 ] and define ψ(A) = B12.,4.2. Low-rank Single Index Model,[0],[0]
"Finally, we define T (Y,X) = 1/κ · ψ",4.2. Low-rank Single Index Model,[0],[0]
"[ κ · Y · S(X) ] , where κ > 0 will be specified later.",4.2. Low-rank Single Index Model,[0],[0]
"Therefore, our final estimator β̂ ∈ Rd1×d2 is defined as the solution to the optimization problem in (3.4) with R(β) = ‖β‖?.",4.2. Low-rank Single Index Model,[0],[0]
We note here the minimization in (3.4) is taken over Rd1×d2 .,4.2. Low-rank Single Index Model,[0],[0]
"The following theorem quantifies the convergence rates of the proposed estimator.
",4.2. Low-rank Single Index Model,[0],[0]
Theorem 4.3 (Signal recovery for the low-rank single index model).,4.2. Low-rank Single Index Model,[0],[0]
"For the low-rank single index model defined in §2, we assume that rank(β∗) = r∗.",4.2. Low-rank Single Index Model,[0],[0]
"Under Assumption 4.1, we let
κ = 2 √ n ·",4.2. Low-rank Single Index Model,[0],[0]
"log(d1 + d2)√ (d1 + d2)M
in T (Y,X).",4.2. Low-rank Single Index Model,[0],[0]
"Moreover, the regularization parameter λ in (3.4) is set to C √ M · (d1 + d2) ·",4.2. Low-rank Single Index Model,[0],[0]
"log(d1 + d2)/n, where C > 0 is an absolute constant.",4.2. Low-rank Single Index Model,[0],[0]
"Then with probability at least 1− (d1 +d2)−2, the nuclear norm regularized estimator β̂ satisfies
‖β̂",4.2. Low-rank Single Index Model,[0],[0]
"− µβ∗‖fro ≤ 3 √ r∗ · λ, ‖β̂",4.2. Low-rank Single Index Model,[0],[0]
− µβ∗‖?,4.2. Low-rank Single Index Model,[0],[0]
≤ 12r∗,4.2. Low-rank Single Index Model,[0],[0]
·,4.2. Low-rank Single Index Model,[0],[0]
"λ.
By this theorem, we have ‖β̂",4.2. Low-rank Single Index Model,[0],[0]
− µβ∗‖fro = O( √ r∗(d1 + d2) ·,4.2. Low-rank Single Index Model,[0],[0]
log(d1 + d2)/n) and ‖β̂,4.2. Low-rank Single Index Model,[0],[0]
− µβ∗‖?,4.2. Low-rank Single Index Model,[0],[0]
"=
O(r∗ · √
(d1 + d2) ·",4.2. Low-rank Single Index Model,[0],[0]
log(d1 + d2)/n).,4.2. Low-rank Single Index Model,[0],[0]
Note that the rate obtained is minimax-optimal up to a logarithmic factor.,4.2. Low-rank Single Index Model,[0],[0]
"Furthermore, it matches the rates for low-rank single index models with Gaussian and symmetric elliptical distributions up to a logarithmic factor (Plan & Vershynin, 2016; Goldstein et al., 2016).",4.2. Low-rank Single Index Model,[0],[0]
We assess the finite sample performance of the proposed estimators on simulated data.,5. Numerical Experiments,[0],[0]
"Throughout this section, we let ∼ N(0, 1) and set the link function in (2.1) as one of f1(u) = 3u+10 sin(u) and f2(u) = √ 2u+4 exp(−2u2), which are plotted in Figure 2.",5. Numerical Experiments,[0],[0]
"We set p0 to be one of (i) Gamma distribution with shape parameter 5 and scale parameter 1, (ii) Student’s t-distribution with 5 degrees of freedom, and (iii) Rayleigh distribution with scale parameter 2.",5. Numerical Experiments,[0],[0]
"To measure the estimation accuracy, we use the cosine distance
cos θ(β̂, β∗) = 1−",5. Numerical Experiments,[0],[0]
"‖β̂‖−1• |〈β̂, β∗〉|,
where • stands for the Euclidean norm in the vector case and the Frobenius norm when β∗ is a matrix.",5. Numerical Experiments,[0],[0]
Here we report the cosine distance rather than ‖β̂,5. Numerical Experiments,[0],[0]
"− µβ∗‖• to compare the performances for X having different distributions, where µ may have different values.
",5. Numerical Experiments,[0],[0]
"For the vector case, we fix d = 2000, s∗ = 5 and vary n.",5. Numerical Experiments,[0],[0]
"The support of β∗ is chosen uniformly random among all subsets of {1, . . .",5. Numerical Experiments,[0],[0]
", d}.",5. Numerical Experiments,[0],[0]
"For each j ∈ supp(β∗), we set β∗j = 1/ √ s∗ · γj , where each γj is an i.i.d.",5. Numerical Experiments,[0],[0]
Rademacher random variable.,5. Numerical Experiments,[0],[0]
"In addition, the regularization parameter λ is set to 4 √ log d/n.",5. Numerical Experiments,[0],[0]
"We plot the cosine distance against
the signal strength √ s∗ log d/n in Figure 4-(a) and (b) for f1 and f2 respectively, based on 200 independent trials for each n. As shown in this figure, the estimation error grows sublinearly as a function of the signal strength.
",5. Numerical Experiments,[0],[0]
"As for the matrix case, we fix d1 = d2 = 20, r∗ = 3 and let n vary.",5. Numerical Experiments,[0],[0]
"The signal parameter β∗ is equal to USV >, where U, V ∈ Rd×d are random orthogonal matrices and S is a diagonal matrix with r∗ nonzero entries.",5. Numerical Experiments,[0],[0]
"Moreover, we set the nonzero diagonal entries of S as 1/ √ r∗, which implies ‖β∗‖fro = 1.",5. Numerical Experiments,[0],[0]
"We set the regularization parameter as λ = 2 √
(d1 + d2) log(d1 + d2)/n.",5. Numerical Experiments,[0],[0]
"Furthermore, we use the proximal gradient descent algorithm (with the learning rate fixed to 0.05) to solve the nuclear norm regularization problem in (3.4).",5. Numerical Experiments,[0],[0]
"To present the result, we plot the cosine distant against the signal strength √ r∗(d1 + d2) log(d1 + d2)/n in Figure 4-(b) based on 200 independent trials.",5. Numerical Experiments,[0],[0]
"As shown in this figure, the error is bounded by a linear function of the signal strength, which corroborates Theorem 4.3.",5. Numerical Experiments,[0],[0]
"In this paper, we consider SIMs in the high-dimensional non-Gaussian setting and proposed estimators based on Stein’s Lemma for a wider class of unknown link functions and covariate distributions.",6. Conclusion,[0],[0]
We consider both sparse and low-rank models and propose minimax rate-optimal estimators under fairly mild assumptions.,6. Conclusion,[0],[0]
"An interesting avenue of future work is the problem of phase retrieval
with non-Gaussian data.",6. Conclusion,[0],[0]
Our current approach requires that µ 6= 0,6. Conclusion,[0],[0]
which is not applicable.,6. Conclusion,[0],[0]
The main reason this happens is we use a first-order version of Stein’s Lemma.,6. Conclusion,[0],[0]
"Such a problem could overcome by second-order Stein’s Lemma (Janzamin et al., 2014).",6. Conclusion,[0],[0]
Obtaining rate-optimal estimators based on second-order score functions require addressing several challenges.,6. Conclusion,[0],[0]
Concentrating on phase retrieval (and sparse phase retrieval) we plan to report our results for the above problem in the near future.,6. Conclusion,[0],[0]
We consider estimating the parametric component of single index models in high dimensions.,abstractText,[0],[0]
"Compared with existing work, we do not require the covariate to be normally distributed.",abstractText,[0],[0]
"Utilizing Stein’s Lemma, we propose estimators based on the score function of the covariate.",abstractText,[0],[0]
"Moreover, to handle score function and response variables that are heavy-tailed, our estimators are constructed via carefully thresholding their empirical counterparts.",abstractText,[0],[0]
"Under a bounded fourth moment condition, we establish optimal statistical rates of convergence for the proposed estimators.",abstractText,[0],[0]
Extensive numerical experiments are provided to back up our theory.,abstractText,[0],[0]
High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation,title,[0],[0]
"Considerable advances have been made over the past decade on fitting high-dimensional structured linear models when the number of samples n is much smaller than the ambient dimensionality p (Banerjee et al., 2014; Negahban et al., 2012; Chandrasekaran et al., 2012).",1 Introduction,[0],[0]
"Most of the advances have been made for linear models: yi = 〈xi, θ∗〉 + ωi, i = 1, . . .",1 Introduction,[0],[0]
", n, where θ∗ ∈ Rp is assumed to be structured, e.g., sparse, group sparse, etc.",1 Introduction,[0],[0]
"Estimation of such structured θ is usually done using Lasso-type regularized estimators (Negahban et al., 2012; Banerjee et al., 2014) or Dantzig-type constrained estimators (Chandrasekaran et al., 2012; Chatterjee et al., 2014); other related estimators have also been explored (Hsu & Sabato,
1Department of Computer Science & Engineering, University of Minnesota, Twin Cities.",1 Introduction,[0],[0]
"Correspondence to: Vidyashankar Sivakumar <sivak017@umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016; Vershynin, 2015).",1 Introduction,[0],[0]
"Such models have been extended to generalized linear models (Banerjee et al., 2014; Negahban et al., 2012), matrix completion (Candès & Recht, 2009), vector auto-regressive models (Melnyk & Banerjee, 2016) among others.
",1 Introduction,[0],[0]
"In this paper, we consider the problem of structured quantile regression in high-dimensions, which can be posed as follows: given the response variable yi and covariates xi the τ th conditional quantile function of yi given xi is given by: F−1yi|xi(τ |xi) = 〈xi, θ ∗ τ",1 Introduction,[0],[0]
"〉, τ ∈ (0, 1) for some structured θ∗τ whose structure can be captured by a suitable atomic norm R(·), e.g., l1-norm for sparsity, l1/l2 norm for group sparsity, etc.",1 Introduction,[0],[0]
Here F−1yi|xi(·) is the inverse of the conditional distribution function of yi given xi.,1 Introduction,[0],[0]
"We consider the following regularized estimator for the structured quantile regression problem:
θ̂n",1 Introduction,[0],[0]
:= argminθ∈Rp Lτ (θ) +,1 Introduction,[0],[0]
"λnR(θ)
:= argminθ∈Rp 1
n n∑ i=1",1 Introduction,[0],[0]
"ρτ (yi − 〈xi, θ〉) + λnR(θ) ,
(1)
where ρτ (u) =",1 Introduction,[0],[0]
"(τ − I(u ≤ 0))u is the asymmetric absolute deviation function (Koenker, 2005), I(·) is the indicator function.",1 Introduction,[0],[0]
"The goal is to get nonasymptotic bounds on the estimation error ‖θ̂ − θ∗‖2.
",1 Introduction,[0],[0]
"Many previous papers analyze the asymptotic performance of the estimator in (1) (Li & Zhu, 2008; Kai et al., 2011; Wu & Liu, 2009; Zou & Yuan, 2008; Wang et al., 2012).",1 Introduction,[0],[0]
"In the non-asymptotic setting of interest in this paper, special cases of the estimator in (1) have been studied in recent literature (Belloni & Chernozhukov, 2011; Kato, 2011; Fan et al., 2014a), primarily focusing on specific norms like the l1-norm and nonoverlapping group sparse norm.",1 Introduction,[0],[0]
"In contrast, our formulation and analysis is applicable to any atomic norm R(·) giving considerable flexibility in choosing a suitable structure for real world problems, e.g., hierarchical sparsity, k-support norm, OWL norm etc.",1 Introduction,[0],[0]
"More recently (Alquier et al., 2017) consider the more general problem of norm regularized regression with Lipschitz loss functions which includes (1) as a special case.",1 Introduction,[0],[0]
"They derive similar results to ours for bounds on the estimation error, but their analysis differs significantly and, in our opinion, does not leverage and highlight the key geometric and sta-
tistical characteristics of the problem.
",1 Introduction,[0],[0]
"In the setting of norm regularized regression with square loss, including the widely studied Lasso estimator (Tibshirani, 1996; Negahban et al., 2012; Banerjee et al., 2014), the sample complexity n0 of the estimator gets determined by a certain restricted strong convexity (RSC) property which simplifies to the restricted eigenvalue (RE) condition on the matrix XTX (Bickel et al., 2009; Negahban et al., 2012); in the noiseless setting, i.e., when ωi = 0, the sample complexity determines a phase transition phenomenon so that the probability of recovering the structured θ∗ is minimal when n < n0, and one can exactly recover θ∗ with high probability when n > n0.",1 Introduction,[0],[0]
"Our work gives an equivalent sample complexity characterization for structured quantile regression, which was not available in prior work.",1 Introduction,[0],[0]
"The challenge in characterizing RSC in the context of quantile regression stems partly from the nonsmoothness of the objective, so one has to work with subgradients.",1 Introduction,[0],[0]
"However, the unique aspect stems from the geometry of quantile regression, or as the authoritative book on the topic puts it: “How quantile regression works?”",1 Introduction,[0],[0]
"(Koenker, 2005)[Section 2.2].",1 Introduction,[0],[0]
"In quantile regression, the n samples get divided into three subsets: ν samples which get exactly interpolated, i.e., yi = 〈xi, θ̂〉, (n − ν)τ samples which lie below the curve, i.e., yi < 〈xi, θ̂〉, and (n − ν)(1",1 Introduction,[0],[0]
"− τ) samples which lie above the curve, i.e., yi > 〈xi, θ̂〉.",1 Introduction,[0],[0]
Note that when ν = n,1 Introduction,[0],[0]
"all samples are interpolated, the loss is zero and the same θ̂ is a solution for all quantiles τ .",1 Introduction,[0],[0]
Quantile regression is clearly not working.,1 Introduction,[0],[0]
"The Number of InterPolated Samples (NIPS) ν is an important quantity, inherent to structure in θ∗, and determines the sample complexity of structured quantile regression (1).",1 Introduction,[0],[0]
"In fact, we show that when n > ν, the RSC condition associated with the estimator in (1) is satisfied.",1 Introduction,[0],[0]
"When there is no structure in θ∗, then ν = O(p), and quantile regression needs n > O(p) samples to work.",1 Introduction,[0],[0]
"However, when θ∗ has structure, such as sparsity or group sparsity, ν can be substantially smaller than p.",1 Introduction,[0],[0]
"Specifically we show that ν is of the order of square of Gaussian width of the error set (Talagrand, 2014; Chandrasekaran et al., 2012) for a class of atomic norms which includes l1, l1/l2 group sparse, ksupport (Argyriou et al., 2012) and the OWL (Bogdan et al., 2013) norms.",1 Introduction,[0],[0]
"The Gaussian width as a measure of complexity of a set has been extensively used in prior literature (Chandrasekaran et al., 2012; Tropp, 2015; Banerjee et al., 2014).",1 Introduction,[0],[0]
"For example, when θ∗ is sparse with s non-zero entries, we show that ν",1 Introduction,[0],[0]
"= cs log p.
",1 Introduction,[0],[0]
"When n > ν and the RSC condition is satisfied, building on recent developments in high-dimensional estimation (Negahban et al., 2012; Banerjee et al., 2014), we show that choosing λn ≥ 2R∗(∇Lτ (θ∗))), where R∗(·) is the dual norm of R(·), leads to non-asymptotic bounds on the estimation error ‖θ̂n",1 Introduction,[0],[0]
− θ∗‖2.,1 Introduction,[0],[0]
"While the specification of
λn looks complex, with its dependency on the dual norm and its dependency on θ∗, we show it is sufficient to set λn based on the Gaussian width (Talagrand, 2014) of the unit norm ball for R(·) (Banerjee et al., 2014; Sivakumar et al., 2015)‘.",1 Introduction,[0],[0]
"Our analysis and results on the estimation error bound for quantile regression, interestingly, has the same order as that for regularized least squares regression for general norms (Banerjee et al., 2014).",1 Introduction,[0],[0]
In contrast to the least squares loss the quantile loss is more robust as the estimation error is independent of the two norm of the noise.,1 Introduction,[0],[0]
"We discuss results for the l1, l1/l2 group sparse and k-support norms as examples, precisely characterizing the sample complexity for recovery and non-asymptotic error bounds.",1 Introduction,[0],[0]
"Specifically, our results for the l1-norm matches those from existing literature on sparse quantile regression (Belloni & Chernozhukov, 2011).
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we discuss the problem formulation along with assumptions, review the general framework for analyzing regularized estimation problems and discuss the three atomic norms used as examples throughout the paper.",1 Introduction,[0],[0]
"In Section 3, we analyze the number of interpolated samples and establish precise sample complexities for a class of atomic norms in terms of the Gaussian widths of sets.",1 Introduction,[0],[0]
In Section 4 we establish key ingredients of the analysis and provide the main bound.,1 Introduction,[0],[0]
We present experimental results in Section 5 and conclude in Section 6.,1 Introduction,[0],[0]
Problem formulation: We outline the assumptions on the data and estimator.,2 Background and Preliminaries,[0],[0]
"Similar conditions are present in all prior literature on quantile regression and we refer to Section 2.5 in Belloni & Chernozhukov (2011) for examples of data satisfying the conditions.
",2 Background and Preliminaries,[0],[0]
"We consider data is generated as y = Xθ + ω, X ∈ Rn×p is the design matrix, θ ∈ Rp and ω ∈",2 Background and Preliminaries,[0],[0]
Rn is the noise.,2 Background and Preliminaries,[0],[0]
We assume subGaussian design matrices X ∈ Rn×p which includes the class of all bounded random variables.,2 Background and Preliminaries,[0],[0]
Note that this is not a restrictive assumption as to avoid the quantile crossing phenomenon the covariate space has to be bounded.,2 Background and Preliminaries,[0],[0]
"Quantile crossing is when the value of the τ1th quantile is greater than the τ2th quantile for some τ1 < τ2 (See Section 2.5 in (Koenker, 2005)).",2 Background and Preliminaries,[0],[0]
We do not make any assumptions on the noise vector ω ∈,2 Background and Preliminaries,[0],[0]
Rn.,2 Background and Preliminaries,[0],[0]
"More specifically the noise can be sampled from a heavy tailed distribution, can be heteroscedastic as in the location-scale model where ωi = 〈xi, η〉 · i, η",2 Background and Preliminaries,[0],[0]
"∈ Rp, i is noise independent of xi, bimodal and so on and so forth.",2 Background and Preliminaries,[0],[0]
"Note that this setting is more general than for the least squares loss.
",2 Background and Preliminaries,[0],[0]
"We consider a parametric quantile regression model where the τ th conditional quantile function of the response vari-
able yi given any xi ∈",2 Background and Preliminaries,[0],[0]
"Rp is given by,
F−1yi|xi(τ |xi) = 〈xi, θ ∗ τ 〉, θ∗τ ∈",2 Background and Preliminaries,[0],[0]
"Rp, τ ∈ (0, 1) , (2)
where F−1yi|xi is the inverse of the conditional distribution function of yi given xi.",2 Background and Preliminaries,[0],[0]
"We will assume the conditional density of yi evaluated at the conditional quantile 〈xi, θ∗τ 〉 is bounded away from zero uniformly for all τ , that is, fyi|xi(〈xi, θ∗τ 〉) >",2 Background and Preliminaries,[0],[0]
f > 0 for all τ and all xi.,2 Background and Preliminaries,[0],[0]
The goal is to estimate parameter θ̂τ close to θ∗τ using n observations of the data when n <,2 Background and Preliminaries,[0],[0]
p.,2 Background and Preliminaries,[0],[0]
"The estimator in this paper belongs to the family of regularized estimators and is of the form:
θ̂λn,τ := argminθ∈Rp Lτ (θ) + λnR(θ) , (3)
",2 Background and Preliminaries,[0],[0]
where Lτ (θ) =,2 Background and Preliminaries,[0],[0]
1n,2 Background and Preliminaries,[0],[0]
∑n i=1,2 Background and Preliminaries,[0],[0]
"ρτ (yi−〈xi, θ〉), ρτ (·) is the quantile loss function and R(·) is any atomic norm.",2 Background and Preliminaries,[0],[0]
"Examples of atomic norms we consider in this paper are the l1, l1/l2 nonoverlapping group sparse norm and the k-support norm.",2 Background and Preliminaries,[0],[0]
"We present all results assuming any single τ ∈ (0, 1) and going forward drop the subscripts from θ∗τ and θ̂λn,τ .
",2 Background and Preliminaries,[0],[0]
Gaussian Width:,2 Background and Preliminaries,[0],[0]
All results will be in terms of the Gaussian width of suitable sets.,2 Background and Preliminaries,[0],[0]
"For any set A ∈ Rp, the Gaussian width of the set A is defined as (Gordon, 1985; Chandrasekaran et al., 2012):
w(A) =",2 Background and Preliminaries,[0],[0]
"Eg [ sup u∈A 〈g, u〉 ] .",2 Background and Preliminaries,[0],[0]
"(4)
where the expectation is over g ∼ N(0, Ip×p).",2 Background and Preliminaries,[0],[0]
"Gaussian widths have been widely used in prior literature on structured estimation (Chandrasekaran et al., 2012; Banerjee et al., 2014; Sivakumar et al., 2015; Tropp, 2015).
",2 Background and Preliminaries,[0],[0]
"High-dimensional estimation: Our analysis is built on developments over the past decade for high-dimensional structured regression for linear and generalized linear models using both regularized as well as constrained estimators (Candes & Tao, 2007; Bickel et al., 2009; Chandrasekaran et al., 2012; Negahban et al., 2012; Banerjee et al., 2014).",2 Background and Preliminaries,[0],[0]
For the regularized formulation considered in this work Banerjee et al. (2014); Negahban et al. (2012) have established a generalized analysis framework when the loss is least squares or more generally the maximum likelihood estimator for generalized linear models.,2 Background and Preliminaries,[0],[0]
"We give a brief overview of the main components of the analysis.
1.",2 Background and Preliminaries,[0],[0]
Regularization parameter:,2 Background and Preliminaries,[0],[0]
"In Banerjee et al. (2014); Negahban et al. (2012) the regularization parameter is assumed to satisfy the following assumption,
λn ≥ 2R∗(∇Lτ (θ∗)) .",2 Background and Preliminaries,[0],[0]
"(5)
With ΩR = {u|R(u) ≤ 1} denoting the unit norm ball, Banerjee et al. (2014) prove that with high probability a
value λn = O(w(ΩR)) satisfies the above condition for subGaussian design matrices, noise and the least squares loss.
2.",2 Background and Preliminaries,[0],[0]
"Error set: The assumption on the regularization parameter ensures that the error vector ∆ = θ̂− θ∗ lies in the following error set (Banerjee et al., 2014),
C := {
∆ | R(θ∗ + ∆) ≤ R(θ∗)",2 Background and Preliminaries,[0],[0]
"+ 1 2 R(∆)
} .",2 Background and Preliminaries,[0],[0]
"(6)
3.",2 Background and Preliminaries,[0],[0]
"The norm compatibility constant: It is defined as follows (Negahban et al., 2012; Banerjee et al., 2014),
Ψ(C) = sup u∈C
R(u)",2 Background and Preliminaries,[0],[0]
‖u‖2 .,2 Background and Preliminaries,[0],[0]
"(7)
4.",2 Background and Preliminaries,[0],[0]
Restricted Strong Convexity (RSC):,2 Background and Preliminaries,[0],[0]
"In Banerjee et al. (2014); Negahban et al. (2012) the loss function is shown to satisfy the following RSC condition with high probability once the number of samples is of the order of the square of the Gaussian width of the error set, that is, n = O(w2(C)).
inf u∈C δL(θ∗, u) = inf u∈C (L(θ∗ + u)− L(θ∗)− 〈∇L(θ∗), u〉)
≥ κ‖u‖2 .",2 Background and Preliminaries,[0],[0]
"(8)
For the squared loss, the RSC condition simplifies to the Restricted Eigenvalue (RE) condition (Bickel et al., 2009)
inf u∈C
1 n ‖Xu‖22 ≥ κ‖u‖22 .",2 Background and Preliminaries,[0],[0]
"(9)
5.",2 Background and Preliminaries,[0],[0]
"Recovery Bounds: When RSC and bounds on the regularization parameter are satisfied Banerjee et al. (2014) prove the following deterministic error bound,
‖∆‖2 = ‖θ̂",2 Background and Preliminaries,[0],[0]
− θ∗‖2 ≤,2 Background and Preliminaries,[0],[0]
c,2 Background and Preliminaries,[0],[0]
"Ψ(C)w(ΩR)
κ .",2 Background and Preliminaries,[0],[0]
"(10)
where c is any constant.
",2 Background and Preliminaries,[0],[0]
Atomic Norms: We consider the class of atomic norms for the regularizer.,2 Background and Preliminaries,[0],[0]
"Mathematically consider a set A ⊆ Rp the collection of atoms that is compact, centrally symmetric about the origin (that is, a ∈",2 Background and Preliminaries,[0],[0]
A =⇒ −a ∈,2 Background and Preliminaries,[0],[0]
A).,2 Background and Preliminaries,[0],[0]
"Let ‖θ‖A denote the gauge of A,
R(θ) = ‖θ‖A = inf{t > 0",2 Background and Preliminaries,[0],[0]
": θ ∈ tconv(A)} (11) = inf{ ∑ a∈A ca : θ = ∑ a∈A caa, ca ≥ 0,∀a ∈",2 Background and Preliminaries,[0],[0]
"A} .
(12)
",2 Background and Preliminaries,[0],[0]
For example when A = {±ei}pi=1 yields ‖θ‖A = ‖θ‖1.,2 Background and Preliminaries,[0],[0]
"Although the atomic set A may contain uncountably many vectors, we assume A can be decomposed as a union of m sets,A = Ai∪A2∪. . .",2 Background and Preliminaries,[0],[0]
"Am similar to the setting considered
in Chen & Banerjee (2015).",2 Background and Preliminaries,[0],[0]
"Such a decomposition assumption is satisfied by many popular atomic norms like the l1, l1/l2 group sparse norms, k-support norm, OWL norm etc.",2 Background and Preliminaries,[0],[0]
"Throughout the paper we will illustrate our results on the following norms.
1.",2 Background and Preliminaries,[0],[0]
l1 norm:,2 Background and Preliminaries,[0],[0]
"For the l1 norm we will consider that θ∗ is an s-sparse vector, that is, ‖θ∗‖0 = s.
2. l1/l2 nonoverlapping group sparse norm: Let {G1,G2, . . .",2 Background and Preliminaries,[0],[0]
", GNG} denote a collection of groups which are blocks of any vector θ ∈",2 Background and Preliminaries,[0],[0]
Rp.,2 Background and Preliminaries,[0],[0]
"Let θNG denote a vector with coordinates θiNG = θ
i if i ∈ GNG , else θiNG = 0.",2 Background and Preliminaries,[0],[0]
"The maximum size of any group is l = max
i∈[1,...,NG ] |Gi|.",2 Background and Preliminaries,[0],[0]
The norm is given as R(θ) = ∑NG i=1 ‖θi‖2.,2 Background and Preliminaries,[0],[0]
"Let SG ⊆ {1, 2, . . .",2 Background and Preliminaries,[0],[0]
", NG} with cardinality |SG | = sG .",2 Background and Preliminaries,[0],[0]
We consider the true parameter θ∗ ∈,2 Background and Preliminaries,[0],[0]
"Rp is sG-sparse, that is, θ∗NG =",2 Background and Preliminaries,[0],[0]
"~0,∀NG /∈",2 Background and Preliminaries,[0],[0]
"SG .
3.",2 Background and Preliminaries,[0],[0]
k-support norm:,2 Background and Preliminaries,[0],[0]
"The k-support norm can be expressed as an infimum convolution given by (Argyriou et al., 2012),
R(θ) = inf∑ i ui=θ {∑ i ‖ui‖2",2 Background and Preliminaries,[0],[0]
| ‖ui‖0 ≤ k } .,2 Background and Preliminaries,[0],[0]
"(13)
Clearly it is an atomic norm for which A = {a ∈",2 Background and Preliminaries,[0],[0]
Rp | ‖a‖0 ≤,2 Background and Preliminaries,[0],[0]
"k, ‖a‖2 = 1} and A is a union of ( p k ) subsets, that is, A = A1 ∪ A2 ∪ . . .",2 Background and Preliminaries,[0],[0]
A(pk).,2 Background and Preliminaries,[0],[0]
More results on the k-support norm can be looked up in Chatterjee et al. (2014); Chen & Banerjee (2015).,2 Background and Preliminaries,[0],[0]
We consider the setting where ‖θ∗‖0 = s and k < s.,2 Background and Preliminaries,[0],[0]
"For the results we require the Gaussian widths of the unit norm ball, error set and the norm compatibility constants for the norms.",2 Background and Preliminaries,[0],[0]
We provide them below for reference.,2 Background and Preliminaries,[0],[0]
"All values are in order notation.
",2 Background and Preliminaries,[0],[0]
Norm w(ΩR) w(C) Ψ(C) l1 c √ log p c,2 Background and Preliminaries,[0],[0]
√ s log p c,2 Background and Preliminaries,[0],[0]
"√ s
l1/l2 c",2 Background and Preliminaries,[0],[0]
√ l + logNG,2 Background and Preliminaries,[0],[0]
c √ lsG + sG logNG c √ sG,2 Background and Preliminaries,[0],[0]
k-sp,2 Background and Preliminaries,[0],[0]
c,2 Background and Preliminaries,[0],[0]
√ k + k,2 Background and Preliminaries,[0],[0]
"logd p
k e c
√ s+ s logd p
k e c
√ 2s/k",2 Background and Preliminaries,[0],[0]
We begin with intuitions on the geometry of the problem.,3 Number of InterPolated Samples (NIPS),[0],[0]
"In the high sample, low dimension setting n",3 Number of InterPolated Samples (NIPS),[0],[0]
>,3 Number of InterPolated Samples (NIPS),[0],[0]
"> p, when R(θ) = 0, the quantile loss is a linear program and hence the solutions are at the vertices, that is, where any p of the n samples are interpolated.",3 Number of InterPolated Samples (NIPS),[0],[0]
"Mathematically we define the quantity Z = {i : yi = 〈xi, θ̂〉 = 〈xi, θ∗ + u〉, u ∈ Rp} and note that ν",3 Number of InterPolated Samples (NIPS),[0],[0]
"= sup
u∈Rp |Z| = O(p).",3 Number of InterPolated Samples (NIPS),[0],[0]
"In the high di-
mensional setting considered in this paper n < p and hence with R(θ) = 0 the number of interpolated samples is ν = n.",3 Number of InterPolated Samples (NIPS),[0],[0]
From an optimization perspective there are multiple such solutions and all solutions are optimal for all quantile parameters τ .,3 Number of InterPolated Samples (NIPS),[0],[0]
But practically quantile regression is not working.,3 Number of InterPolated Samples (NIPS),[0],[0]
"Now introducing a regularizer
with a suitable choice for the regularization parameter ensures that the error vector lies in a restricted subset of Rp, C := { u ∣∣R(θ∗ + u) ≤ R(θ∗) + 12R(u) }",3 Number of InterPolated Samples (NIPS),[0],[0]
⊆ Rp.,3 Number of InterPolated Samples (NIPS),[0],[0]
We are now interested in characterizing ν = sup u∈C,3 Number of InterPolated Samples (NIPS),[0],[0]
"|Z|, Z = {i : yi = 〈xi, θ̂〉 = 〈xi, θ∗ + u〉, u ∈ C}, that is, the maximum number of interpolated samples with the error restricted to a particular subset of Rp.",3 Number of InterPolated Samples (NIPS),[0],[0]
"Again if ν = n, quantile regression is not working.",3 Number of InterPolated Samples (NIPS),[0],[0]
"Since there are no restrictions on the number of non-zero elements in the error vector u a first crude estimate will be ν ≤ min{n, p, ‖u‖0}, which implies quantile regression will not work unless we have a minimum of p samples.",3 Number of InterPolated Samples (NIPS),[0],[0]
"But intuitively ν should depend on properties of the error set C, which the initial crude estimate is failing to take advantage of.
",3 Number of InterPolated Samples (NIPS),[0],[0]
Below we state a result which reinforces the intuition of the relation between ν and the properties of the set C. Specifically we show that for the types of atomic norms considered in this work (which includes all popularly known vector norms) the number of interpolated samples does not exceed the product of the square of the norm compatibility constant and the square of the Gaussian width of the unit norm ball.,3 Number of InterPolated Samples (NIPS),[0],[0]
"For the norms considered, this is precisely the square of the Gaussian width of the error set C. For example for the l1 norm for an s-sparse vector this evaluates to an upper bound of ν = O(s log p).",3 Number of InterPolated Samples (NIPS),[0],[0]
"While the result statement considers subGaussian design matrices, the result will also hold for design matrices sampled from heavy-tailed distributions using arguments similar to Lecué & Mendelson (2014); Sivakumar et al. (2015).
",3 Number of InterPolated Samples (NIPS),[0],[0]
"Theorem 3.1 ConsiderX has isotropic subGaussian rows and θ∗ is an s-sparse vector that can be written as a linear combination of k atoms from an atomic set of cardinality m,
θ∗ = k∑ i=1",3 Number of InterPolated Samples (NIPS),[0],[0]
"ciai, ai ∈",3 Number of InterPolated Samples (NIPS),[0],[0]
"A, ci ≥ 0, |A| = m .",3 Number of InterPolated Samples (NIPS),[0],[0]
"(14)
For the regularized quantile regression problem penalized with the atomic norm R(θ) = ‖θ‖A,
θ̂ = arg min θ∈Rp Lτ (θ)+λR(θ) = arg min θ∈Rp
1
n n∑ i=1",3 Number of InterPolated Samples (NIPS),[0],[0]
"ρτ (θ)+λR(θ) ,
(15) let C := { u|R(θ∗ + u) ≤ R(θ∗)",3 Number of InterPolated Samples (NIPS),[0],[0]
"+ 12R(u) } denote the error set, let λ ≥ R∗(∇Lτ (θ∗)) and let n ≥ (c1s + c2Ψ
2(C)w(A)) where Ψ(C) = sup u∈C ‖u‖A",3 Number of InterPolated Samples (NIPS),[0],[0]
"‖u‖2 is the norm compatibility constant in the error set, w(A) is the Gaussian width of the unit norm ball and c1 and c2 are some constants.",3 Number of InterPolated Samples (NIPS),[0],[0]
Then with probability atleast 1 − exp(−c2k1 log(em))−2 exp(−ηw2(C)),3 Number of InterPolated Samples (NIPS),[0],[0]
"the number of in-
terpolated samples,
ν = sup u∈C |{i : yi = 〈xi, θ∗+u〉, u ∈ C}| ≤ cΨ2(C)w2(A) , (16) where c is a constant.
",3 Number of InterPolated Samples (NIPS),[0],[0]
To understand the intuition consider the case of the l1 norm.,3 Number of InterPolated Samples (NIPS),[0],[0]
For an error vector lying in a particular s-dimensional subspace the maximum number of interpolated samples is O(s) with high probability.,3 Number of InterPolated Samples (NIPS),[0],[0]
"Extending the argument to all such s-dimensional subspaces by a union bound argument on the ( p s ) subspaces, the maximum number of interpolated samples when the error vector is any s-sparse vector in pdimensional space is O(s log p).",3 Number of InterPolated Samples (NIPS),[0],[0]
"Finally the argument is extended to all vectors in the error set using the powerful Maurey’s empirical approximation argument previously employed in Sivakumar et al. (2015); Lecué & Mendelson (2014); Rudelson & Zhou (2013).
",3 Number of InterPolated Samples (NIPS),[0],[0]
"Suprisingly in prior literature on structured high dimensional quantile regression, the importance of ν has not been explicitly discussed.",3 Number of InterPolated Samples (NIPS),[0],[0]
"This intuition about the importance of ν also shows up in an elegant form in the analysis of the RSC condition in Section 4.2.
",3 Number of InterPolated Samples (NIPS),[0],[0]
"Below we provide results for the number of interpolated samples for the l1, l1/l2 nonoverlapping group sparse and k-support norms.",3 Number of InterPolated Samples (NIPS),[0],[0]
For the l1/l2 nonoverlapping group sparse norm and the k-support norm we first illustrate that they are atomic norms.,3 Number of InterPolated Samples (NIPS),[0],[0]
The results then follow from substituting known values for the norm compatibility constant and Gaussian width of unit norm ball for the different norms.,3 Number of InterPolated Samples (NIPS),[0],[0]
"For computation of these quantities for any general norm we refer the interested reader to work in Vershynin (2015); Chen & Banerjee (2015).
",3 Number of InterPolated Samples (NIPS),[0],[0]
"Corollary 1 For the l1 norm with θ∗ being an s-sparse vector, when n >",3 Number of InterPolated Samples (NIPS),[0],[0]
"(c1s + c2s log p) then with high probability,
ν = sup u∈C |{i : yi = 〈xi, θ∗ + u〉}| ≤ cs log p , (17)
for some constant c.
Before applying the result to the nonoverlapping group sparse norm note that for any vector θ ∈",3 Number of InterPolated Samples (NIPS),[0],[0]
"Rp,
θ = NG∑ i=1",3 Number of InterPolated Samples (NIPS),[0],[0]
"∑ j cijβij , (18)
where βij is any unit norm vector in subspace defined by the group i.",3 Number of InterPolated Samples (NIPS),[0],[0]
"For any group i, let θi denote the vector constructed from θ such that it has component k, θk = 0",3 Number of InterPolated Samples (NIPS),[0],[0]
if k /∈ i.,3 Number of InterPolated Samples (NIPS),[0],[0]
"Now by definition of atomic norm cij = ‖θi‖2 for θi
in the same direction of βij , otherwise cij = 0.",3 Number of InterPolated Samples (NIPS),[0],[0]
"Therefore the group sparse norm is an atomic norm with a hierarchical set structure with the number of elements m = NG in the outer set with each element of the outer set itself being a set of an infinite number of elements with any one element chosen for a particular vector θ, that is, cij 6= 0 for only one j amongst an infinite number of j’s.
",3 Number of InterPolated Samples (NIPS),[0],[0]
Corollary 2 Consider the l1/l2 nonoverlapping groupsparse norm with n >,3 Number of InterPolated Samples (NIPS),[0],[0]
(c1sGl + c2sG logNG).,3 Number of InterPolated Samples (NIPS),[0],[0]
"With high probability,
ν = sup u∈C |{i : yi = 〈xi, θ∗ + u〉}| ≤ c(lsG + sG logNG) ,
(19) for some constant c.
For the k-support norm ‖θ‖spk = inf∑ i ui=θ
{ ∑ i ‖ui‖2",3 Number of InterPolated Samples (NIPS),[0],[0]
"| ‖ui‖0 ≤ k}, can be similarly
expressed as,
θ =",3 Number of InterPolated Samples (NIPS),[0],[0]
"(pk)∑ i=1 ∑ j cijβij (20)
where βij is a unit vector in",3 Number of InterPolated Samples (NIPS),[0],[0]
k-dimensional subspace i.,3 Number of InterPolated Samples (NIPS),[0],[0]
The difference compared to the nonoverlapping group sparse norm is that many of the cij’s can now be non zero in the inner sum.,3 Number of InterPolated Samples (NIPS),[0],[0]
"This is comparably more complex than the group-sparse norm where the inner set becomes a singleton for some θ, but in terms of the analysis nothing changes.
",3 Number of InterPolated Samples (NIPS),[0],[0]
Corollary 3 Consider the k-support norm with n >,3 Number of InterPolated Samples (NIPS),[0],[0]
(c1s+ c2s logd pk e).,3 Number of InterPolated Samples (NIPS),[0],[0]
"With high probability,
ν = sup u∈C |{i : yi = 〈xi, θ∗ + u〉}| ≤ c(s+ s logdp/ke) ,
(21) for some constant c.",3 Number of InterPolated Samples (NIPS),[0],[0]
"In this section, we present results for the key components in the general analysis framework of Banerjee et al. (2014) which we briefly described in Section 2 of the paper.",4 Structured Quantile Regression,[0],[0]
We start with results on the regularization parameter by analyzing equation (5) before establishing sample complexity bounds when the restricted strong convexity condition in equation (8) is satisfied.,4 Structured Quantile Regression,[0],[0]
Finally an l2 bound on the error is obtained using (10).,4 Structured Quantile Regression,[0],[0]
We will consider subGaussian design matrices throughout.,4 Structured Quantile Regression,[0],[0]
All results are in terms of Gaussian widths of sets and the norm compatibility constant.,4 Structured Quantile Regression,[0],[0]
"Results for l1, l1/l2-nonoverlapping group sparse and ksupport norms are given for illustration purposes.",4 Structured Quantile Regression,[0],[0]
We analyze the bound in equation (5).,4.1 Regularization Parameter λn,[0],[0]
In prior literature a bound has been established specifically for the l1 norm in Belloni & Chernozhukov (2011) (See Theorem 1).,4.1 Regularization Parameter λn,[0],[0]
Below we consider the case of any general atomic norm and obtain a result in terms of the Gaussian width of the unit norm ball.,4.1 Regularization Parameter λn,[0],[0]
"The analysis follows from a similar result for the regularization parameter in Banerjee et al. (2014) for the least squares case.
",4.1 Regularization Parameter λn,[0],[0]
Theorem 4.1 Let X ∈ Rn×p be a design matrix with independent isotropic subGaussian rows with subGaussian norm |||xi|||ψ2,4.1 Regularization Parameter λn,[0],[0]
≤,4.1 Regularization Parameter λn,[0],[0]
κ.,4.1 Regularization Parameter λn,[0],[0]
Define ΩR = {u : R(u) ≤ 1} the unit norm ball and let φ =,4.1 Regularization Parameter λn,[0],[0]
"sup
u ‖u‖2/R(u).",4.1 Regularization Parameter λn,[0],[0]
"Then the following
holds E [R∗(∇Lτ (θ∗))]",4.1 Regularization Parameter λn,[0],[0]
"≤ c √ τ(1− τ)w(ΩR)√
n , (22)
where c is any fixed constant depending only on the subGaussian norm κ.",4.1 Regularization Parameter λn,[0],[0]
"Moreover with probability atleast 1 −
c1 exp
( − (
τ c2φκ
)2) − 2 exp ( −2t2 ) R∗(∇Lτ (θ∗)) ≤",4.1 Regularization Parameter λn,[0],[0]
"c √ τ(1− τ)w(ΩR) + t√
n , (23)
where c1, c2, t are absolute constants.
",4.1 Regularization Parameter λn,[0],[0]
"A major difference to the least squares loss setting, is the independence of the regularization parameter to assumptions on the noise vector (see for example Theorem 3 and Theorem 4 in Banerjee et al. (2014) where the noise is explicitly assumed to be subGaussian and homoscedastic and the noise enters the analysis through properties of ‖ω‖2).",4.1 Regularization Parameter λn,[0],[0]
This gives the flexibility of considering noise vectors which are heavy tailed or heteroscedastic.,4.1 Regularization Parameter λn,[0],[0]
"Indeed the most interesting applications of quantile regression arise in such settings.
",4.1 Regularization Parameter λn,[0],[0]
Below we provide bounds for the regularization parameter for different norms by substituting known values of the Gaussian width for the unit norm balls.,4.1 Regularization Parameter λn,[0],[0]
"The result for the l1 norm matches with Theorem 1 in Belloni & Chernozhukov (2011) for the regularization parameter.
",4.1 Regularization Parameter λn,[0],[0]
"Corollary 4 If R(·) is the l1 norm, with high probability R∗(∇Lτ (θ∗))",4.1 Regularization Parameter λn,[0],[0]
≤,4.1 Regularization Parameter λn,[0],[0]
c √ τ(1− τ),4.1 Regularization Parameter λn,[0],[0]
"√ log p+ t√
n .",4.1 Regularization Parameter λn,[0],[0]
"(24)
Corollary 5 If R(·) is the l1/l2 nonoverlapping group sparse norm, with high probability,
R∗(∇Lτ (θ∗))",4.1 Regularization Parameter λn,[0],[0]
≤,4.1 Regularization Parameter λn,[0],[0]
c √ τ(1− τ),4.1 Regularization Parameter λn,[0],[0]
"√ l + logNG + t√ n .
",4.1 Regularization Parameter λn,[0],[0]
"Corollary 6 If R(·) is the k-support norm, with high probability,
R∗(∇Lτ (θ∗))",4.1 Regularization Parameter λn,[0],[0]
≤,4.1 Regularization Parameter λn,[0],[0]
c √ τ(1− τ),4.1 Regularization Parameter λn,[0],[0]
√ k + k logd pk e+ t√ n .,4.1 Regularization Parameter λn,[0],[0]
The loss needs to satisfy the RSC condition in equation (8).,4.2 Restricted Strong Convexity (RSC),[0],[0]
"Prior literature on structured quantile regression has not discussed the RSC condition explicitly, though Fan et al. (2014b) considers it for the quantile huber loss function.
",4.2 Restricted Strong Convexity (RSC),[0],[0]
We start by providing an intuition for the RSC formulation for the quantile loss.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"The RSC condition equation (8) on the error set C evaluates to the following,
inf u∈C
1
n n∑ i=1 ∫",4.2 Restricted Strong Convexity (RSC),[0],[0]
"〈xi,u〉 0",4.2 Restricted Strong Convexity (RSC),[0],[0]
"(I(yi − 〈xi, θ∗〉 ≤",4.2 Restricted Strong Convexity (RSC),[0],[0]
"z)− I(yi − 〈xi, θ∗〉 ≤ 0))",4.2 Restricted Strong Convexity (RSC),[0],[0]
"dz .
(25) Let ν",4.2 Restricted Strong Convexity (RSC),[0],[0]
"= sup
u∈C |Z| = sup u∈C |{i|yi = 〈xi, θ∗ + u〉}| is the num-
ber of interpolated samples.",4.2 Restricted Strong Convexity (RSC),[0],[0]
"For any n < p if the model can interpolate all points, that is, ν = n",4.2 Restricted Strong Convexity (RSC),[0],[0]
then (25) evaluates to zero.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"In general, as shown in Section 3, ν gets determined by the structure.",4.2 Restricted Strong Convexity (RSC),[0],[0]
For example for the l1 norm ν,4.2 Restricted Strong Convexity (RSC),[0],[0]
= O(s log p) rather than the ambient dimensionality p.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"Thus, the sum over n points in (25) simply reduces to the sum over the (n−ν) points which are not interpolated, and will ensure the RSC condition when n",4.2 Restricted Strong Convexity (RSC),[0],[0]
> ν.,4.2 Restricted Strong Convexity (RSC),[0],[0]
The intuition of the NIPS property of Section 3 thus shows up elegantly in the RSC condition.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"In equation (25), let ξi = yi",4.2 Restricted Strong Convexity (RSC),[0],[0]
"− 〈xi, θ∗〉, vi =∫ 〈xi,u〉 0
(I(ξi ≤ z)− I(ξi ≤ 0))",4.2 Restricted Strong Convexity (RSC),[0],[0]
and consider 1 n,4.2 Restricted Strong Convexity (RSC),[0],[0]
"∑n i=1E[vi],
1
n n∑ i=1",4.2 Restricted Strong Convexity (RSC),[0],[0]
E[vi] = 1 n n∑ i=1 ∫,4.2 Restricted Strong Convexity (RSC),[0],[0]
"〈xi,u〉 0",4.2 Restricted Strong Convexity (RSC),[0],[0]
"(Fi(ξi + z)− Fi(ξ))
",4.2 Restricted Strong Convexity (RSC),[0],[0]
"= 1
n n∑ i=1 ∫",4.2 Restricted Strong Convexity (RSC),[0],[0]
"〈xi,u〉 0 fi(ξi)zdz + o(1)
",4.2 Restricted Strong Convexity (RSC),[0],[0]
"= 1
2n n∑ i=1",4.2 Restricted Strong Convexity (RSC),[0],[0]
"fi(ξi)〈xi, u〉2 + o(1)
≥",4.2 Restricted Strong Convexity (RSC),[0],[0]
"f
2n ‖Xu‖22 ≥ fκ‖u‖22 2 .
",4.2 Restricted Strong Convexity (RSC),[0],[0]
"The first line follows from the definition of the cumulative distribution function, the second line by a simple Taylor series expansion, the last line by the assumption that f ≤ fi(ξi),∀i and (1/n)‖Xu‖22 ≥ κ, where κ is the restricted eigenvalue (RE) constant.",4.2 Restricted Strong Convexity (RSC),[0],[0]
The RE condition is satisfied as the sample complexity bounds for satisfying the NIPS property is same as the RE condition.,4.2 Restricted Strong Convexity (RSC),[0],[0]
More generally RSC is a condition on the minimum eigenvalue of the Jacobian matrix,4.2 Restricted Strong Convexity (RSC),[0],[0]
1n,4.2 Restricted Strong Convexity (RSC),[0],[0]
∑n i=1,4.2 Restricted Strong Convexity (RSC),[0],[0]
"fi(ξi)〈xi,",4.2 Restricted Strong Convexity (RSC),[0],[0]
"u〉2 restricted to the error set
C. This quantity has also been considered in prior literature (see Section 4.2 in Koenker (2005) and the proof in page 121, also see condition D.1 in Belloni & Chernozhukov (2011)).",4.2 Restricted Strong Convexity (RSC),[0],[0]
"While the above analysis is in expectation of the quantity vi, we state the following result giving large deviation bounds for the above quantity.
",4.2 Restricted Strong Convexity (RSC),[0],[0]
Theorem 4.2 ConsiderX ∈ Rn×p has subGaussian rows.,4.2 Restricted Strong Convexity (RSC),[0],[0]
Let 0,4.2 Restricted Strong Convexity (RSC),[0],[0]
"< f < fi(〈xi, θ∗〉) be a uniform lower bound on the conditional density for all xi in the support of x.",4.2 Restricted Strong Convexity (RSC),[0],[0]
Let κ denote the RE constant satisfying 1n‖Xu‖ 2 2 ≥ κ‖u‖22.,4.2 Restricted Strong Convexity (RSC),[0],[0]
Let the number of samples n > cw2(C) where w(C) is the Gaussian width of the error set C and c is some constant.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"Then with probability atleast 1− exp(−τ2/2)− exp ( −φ 2 1fξ 2 n ) where φ1, ξ < 1 and τ are constants,
inf u∈C
δLτ (θ∗, u) ≥ c1κf‖u‖22 .",4.2 Restricted Strong Convexity (RSC),[0],[0]
"(26)
where c1 < 0 is a constant.
",4.2 Restricted Strong Convexity (RSC),[0],[0]
Below we provide results for different norms.,4.2 Restricted Strong Convexity (RSC),[0],[0]
"The sample complexity for the l1 norm matches the result in Belloni & Chernozhukov (2011) (see equation 2.10).
",4.2 Restricted Strong Convexity (RSC),[0],[0]
Corollary 7 For the l1 norm with n >,4.2 Restricted Strong Convexity (RSC),[0],[0]
"cs log p with high probability the following RSC condition is satisfied,
inf u∈C
δLτ (θ∗, u) ≥ cκf‖u‖22 .",4.2 Restricted Strong Convexity (RSC),[0],[0]
"(27)
Corollary 8 For the l1/l2 nonoverlapping group sparse norm with n > c(lsG + sG logNG) with high probability the following RSC condition is satisfied,
inf u∈C
δLτ (θ∗, u) ≥ κf‖u‖22 .",4.2 Restricted Strong Convexity (RSC),[0],[0]
"(28)
Corollary 9 For the k-support norm with n >",4.2 Restricted Strong Convexity (RSC),[0],[0]
"c(s + s logd pk e) with high probability the following RSC condition is satisfied,
inf u∈C
δLτ (θ∗, u) ≥ cκf‖u‖22 .",4.2 Restricted Strong Convexity (RSC),[0],[0]
(29),4.2 Restricted Strong Convexity (RSC),[0],[0]
"Following the general framework outlined in Banerjee et al. (2014) (see Theorem 2), we state the following high probability bound on the two norm of the error vector ∆ = θ̂−θ∗.
Theorem 4.3 For the quantile regression problem, when λn ≥ c1 √ τ(1−τ)w(ΩR)√
n , n > c2w2(C) for some constants
c1, c2 then with high probability, ‖∆‖2 ≤ c √ τ(1− τ)Ψ(C)w(ΩR)
fκ .",4.3 Recovery Bounds,[0],[0]
"(30)
where Ψ(C) is the norm compatibility constant in the error set.
",4.3 Recovery Bounds,[0],[0]
The two norm of the error depends on the two terms√ τ(1− τ) and f .,4.3 Recovery Bounds,[0],[0]
The √ τ(1− τ) term is minimized at the tails and hence has the effect of reducing the estimation error.,4.3 Recovery Bounds,[0],[0]
But typically this is dominated by the lower bound on the density f term which makes the estimate less precise in regions of low density.,4.3 Recovery Bounds,[0],[0]
This is to be expected as there are very few samples to make a very precise estimate in low density regions.,4.3 Recovery Bounds,[0],[0]
"While similar observations are made in page 72 of Koenker (2005), the results are asymptotic while we show non-asymptotic recovery bounds.",4.3 Recovery Bounds,[0],[0]
Another aspect we reiterate here is the independence of the results from the form of the noise.,4.3 Recovery Bounds,[0],[0]
"All results make no assumptions on the noise apart from an assumption on the lower bound of the noise density.
",4.3 Recovery Bounds,[0],[0]
"Below we provide recovery bounds for the different norms we consider in the paper.
",4.3 Recovery Bounds,[0],[0]
"Corollary 10 For the l1 norm when λn ≥ c1 √ τ(1−τ)
",4.3 Recovery Bounds,[0],[0]
"√ log p√
n and n >",4.3 Recovery Bounds,[0],[0]
"c2s log p with high proba-
bility
‖∆‖2 ≤",4.3 Recovery Bounds,[0],[0]
c,4.3 Recovery Bounds,[0],[0]
"√ s log p
fκ √ n
(31)
",4.3 Recovery Bounds,[0],[0]
"Corollary 11 For the l1/l2 nonoverlapping group sparse
norm when λn > c1 √ τ(1−τ)",4.3 Recovery Bounds,[0],[0]
"√ l+logNG√
n and n > c(lsG",4.3 Recovery Bounds,[0],[0]
"+
sG logNG)",4.3 Recovery Bounds,[0],[0]
"with high probability
‖∆‖2 ≤ c",4.3 Recovery Bounds,[0],[0]
"√ lsG + sG logNG
fκ √ n
(32)
",4.3 Recovery Bounds,[0],[0]
"Corollary 12 For the k-support norm when λn > c1 √ τ(1−τ)
√ k+k logd pk e√ n
and n > cs + s logd pk e with high probability
‖∆‖2 ≤ c",4.3 Recovery Bounds,[0],[0]
"√ s+ s logd pk e fκ √ n
(33)",4.3 Recovery Bounds,[0],[0]
We perform simulations with synthetic data.,5 Experiments,[0],[0]
Data is generated as y = Xθ∗ + ω.,5.1 Phase Transition,[0],[0]
θ∗ =,5.1 Phase Transition,[0],[0]
"[1, 1, 1, 1, 1, 1︸ ︷︷ ︸
6
, 0, 0, . .",5.1 Phase Transition,[0],[0]
.,5.1 Phase Transition,[0],[0]
", 0︸ ︷︷ ︸ p-6 ] ∈",5.1 Phase Transition,[0],[0]
"Rp for the l1 norm and
θ∗ =",5.1 Phase Transition,[0],[0]
"[1, . .",5.1 Phase Transition,[0],[0]
.,5.1 Phase Transition,[0],[0]
", 1︸ ︷︷ ︸ 5 , 1, . . .",5.1 Phase Transition,[0],[0]
", 1︸ ︷︷ ︸ 5 , 1, . . .",5.1 Phase Transition,[0],[0]
", 1︸ ︷︷ ︸ 5 , 0, . . .",5.1 Phase Transition,[0],[0]
", 0︸ ︷︷ ︸ 5 , . . .",5.1 Phase Transition,[0],[0]
", 0, . . .",5.1 Phase Transition,[0],[0]
", 0︸ ︷︷ ︸ 5 ] for the l1/l2 group sparse norm with p ∈",5.1 Phase Transition,[0],[0]
"[500, 750, 1000].",5.1 Phase Transition,[0],[0]
"The noise ωi ∼ N(0, 0.25),∀i ∈",5.1 Phase Transition,[0],[0]
[n] is Gaussian with zero mean and 0.25 variance.,5.1 Phase Transition,[0],[0]
"The design matrix X ∼
N(0, Ip×p) is multivariate Gaussian with identity covariance.",5.1 Phase Transition,[0],[0]
We vary n =,5.1 Phase Transition,[0],[0]
"[10, 20, 30, . .",5.1 Phase Transition,[0],[0]
.,5.1 Phase Transition,[0],[0]
", 120, 130].",5.1 Phase Transition,[0],[0]
For each n we generate 100 datasets with the probability of success defined as the fraction of times we are able to faithfully estimate the true parameter.,5.1 Phase Transition,[0],[0]
For p = 500 we run simulations for τ ∈,5.1 Phase Transition,[0],[0]
"[0.1, 0.5, 0.9] and for p ∈ [750, 1000] we run simulations only for τ = 0.5.",5.1 Phase Transition,[0],[0]
"For the optimization, we use the Alternating Direction Method of Multipliers (Boyd et al., 2010).",5.1 Phase Transition,[0],[0]
"The details of the updates can be found in the flare
documentation Li et al. (2015).",5.1 Phase Transition,[0],[0]
The code was implemented in Python.,5.1 Phase Transition,[0],[0]
The plots in Figure 1 clearly show a phase transition for both the l1 and l1/l2 group sparse norms for all quantiles exemplifying the NIPS property described earlier.,5.1 Phase Transition,[0],[0]
We showcase the robustness enjoyed by quantile regression over ordinary least squares estimation against heavytailed noise and outliers.,5.2 Robustness,[0],[0]
We consider the l1 norm with y = Xθ∗ + ω.,5.2 Robustness,[0],[0]
θ∗ =,5.2 Robustness,[0],[0]
"[1, 1, 1, 1, 1, 1︸ ︷︷ ︸
6
, 0, 0, . .",5.2 Robustness,[0],[0]
.,5.2 Robustness,[0],[0]
", 0︸ ︷︷ ︸ 494 ] ∈",5.2 Robustness,[0],[0]
Rp.,5.2 Robustness,[0],[0]
"For
heavy-tailed noise we consider the student t-distribution with different degrees of freedom, with lower degrees of freedom corresponding to heavier tailed data.",5.2 Robustness,[0],[0]
"To show the robustness to outliers we randomly pick a certain percentage of samples from the dataset and multiply the noise by 10, that is, ωi = 10 ∗ ωi for a certain proportion of the dataset.",5.2 Robustness,[0],[0]
We vary the proportion of contamination from 2.5% to 15%.,5.2 Robustness,[0],[0]
We fix n = 200 for this simulation.,5.2 Robustness,[0],[0]
"Again for both exercises, we run 100 simulations and plot the mean and standard deviation of the estimation error ‖θ̂ − θ∗‖2.",5.2 Robustness,[0],[0]
The plots in Figure 2 show 1.,5.2 Robustness,[0],[0]
the estimation error against varying degrees of freedom of the student tdistribution and 2. estimation error against the percent contamination.,5.2 Robustness,[0],[0]
The observations are in agreement with conventional wisdom on robustness of the quantile regression estimator to heavy-tailed noise and outliers.,5.2 Robustness,[0],[0]
The paper presents a general framework for the analysis of non-asymptotic error and structured recovery for norm regularized quantile regression for any atomic norm.,6 Conclusions,[0],[0]
Our results are based on extending the general analysis framework outlined in Banerjee et al. (2014); Negahban et al. (2012) using insights from the geometry of the problem.,6 Conclusions,[0],[0]
In particular we introduce the Number of InterPolated Samples (NIPS) as critical for determining the sample complexity for consistent recovery.,6 Conclusions,[0],[0]
"We prove that once the number of samples crosses the NIPS threshold, we start recovering the true parameter.",6 Conclusions,[0],[0]
This phase transition phenomena for norm regularized quantile regression problems has not been discussed in prior literature.,6 Conclusions,[0],[0]
"We also prove that NIPS is of the order of square of the Gaussian width of the error set for many atomic norms - which is the same order as that for regularized least squares regression and match results from previous work for the l1 norm (Belloni & Chernozhukov, 2011).
",6 Conclusions,[0],[0]
Acknowledgements: We thank reviewers for their valuable comments.,6 Conclusions,[0],[0]
"This work was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A.",6 Conclusions,[0],[0]
Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables.,abstractText,[0],[0]
In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation.,abstractText,[0],[0]
We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(·) and consider the norm regularized quantile regression estimator.,abstractText,[0],[0]
We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error.,abstractText,[0],[0]
"While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature.",abstractText,[0],[0]
We perform experiments on synthetic data which support the theoretical results.,abstractText,[0],[0]
High-Dimensional Structured Quantile Regression,title,[0],[0]
"As a popular algorithm for the estimation of latent variable models, the expectation-maximization (EM) algorithm (Dempster et al., 1977; Wu, 1983) has been widely used in machine learning and statistics (Jain et al., 1999; Tseng, 2004; Han et al., 2011; Little & Rubin, 2014).",1. Introduction,[0],[0]
"Although EM is well-known to often converge to an empirically good local estimator (Wu, 1983), finite sample theoretical guarantees for its performance have not been established until recent
1Facebook, Inc., Menlo Park, CA 94025 2Department of Computer Science, University of Virginia, Charlottesville, VA 22904, USA 3Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801.",1. Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qg5w@virginia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"studies (Balakrishnan et al., 2014; Wang et al., 2014; Yi & Caramanis, 2015).",1. Introduction,[0],[0]
"Specifically, the first local convergence theory and finite sample statistical rates of convergence for the conventional EM algorithm and its gradient ascent variant were established in Balakrishnan et al. (2014).",1. Introduction,[0],[0]
"Later on, Wang et al. (2014) extended the conventional EM algorithm as well as gradient ascent EM algorithm to the high-dimensional setting, where the number of parameters is comparable to or even larger than the sample size.",1. Introduction,[0],[0]
"A key idea used in their algorithms is an additional truncation step after the maximization step (M-step), which is able to exploit the intrinsic sparse structure of the high-dimensional latent variable models.",1. Introduction,[0],[0]
"Yi & Caramanis (2015) also proposed a high-dimensional EM algorithm, which, instead of using truncation, uses a regularized M-estimator in the M-step.",1. Introduction,[0],[0]
"In the high-dimensional setting, the gradient EM algorithm is especially appealing, because exact maximization based M-step can be very time consuming, or even ill-posed.",1. Introduction,[0],[0]
"Nonetheless, gradient EM algorithms can still be computationally prohibitive when the number of observations is also large, since they need to calculate the full gradient at each iteration, whose time complexity is linear in the sample size.
",1. Introduction,[0],[0]
"In this paper, we address the aforementioned computational challenge in the large-scale high-dimensional setting, by proposing a novel variance-reduced stochastic gradient EM algorithm with theoretical guarantees.",1. Introduction,[0],[0]
"Our algorithm is along the line of gradient EM algorithms (Balakrishnan et al., 2014; Wang et al., 2014), where the M-step is achieved by one-step gradient ascent rather than (regularized) exact maximization (Yi & Caramanis, 2015).",1. Introduction,[0],[0]
"Instead of using a full gradient at each iteration as in existing gradient EM algorithms, we significantly reduce the computational cost by utilizing stochastic variance-reduced gradient, which is inspired by recent advances in stochastic optimization (Roux et al., 2012; Johnson & Zhang, 2013; Shalev Shwartz & Zhang, 2013; Defazio et al., 2014; Zhang & Gu, 2016).",1. Introduction,[0],[0]
"To accommodate the special bivariate structure of the Qfunction (i.e., the expected value of the log likelihood function, with respect to the conditional distribution of the latent variable given the observed variable under the current estimate of the model parameter) in EM algorithm, we design a novel semi-stochastic variance-reduced gradient which sets
our work apart from all existing methods and greatly helps reduce the intrinsic variance of the stochastic gradient of the Q-function in the EM algorithm.",1. Introduction,[0],[0]
We apply our algorithm to two popular latent variable models and thorough numerical experiments are provided to backup our theory.,1. Introduction,[0],[0]
"In particular, we summarize our major contributions as follows:
• We propose a novel high-dimensional EM algorithm by incorporating variance reduction into the stochastic gradient method for EM.",1. Introduction,[0],[0]
"Specifically, we design a novel semi-stochastic gradient tailored to the bivariate structure of the Q-function in the EM algorithm.",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first work ever that brings variance reduction into the stochastic gradient EM algorithm in the high-dimensional scenario.",1. Introduction,[0],[0]
• We prove that our proposed algorithm converges at a linear rate to the unknown model parameter and achieves the best-known statistical rate of convergence with a mild condition on the initialization.,1. Introduction,[0],[0]
• We show that the proposed algorithm has an improved overall computational complexity over the state-of-theart algorithm.,1. Introduction,[0],[0]
"Specifically, to achieve an optimization error of , our algorithm needs O ( (N + bκ2) ·",1. Introduction,[0],[0]
"log(1/ )
) gradient evaluations1, where N is the sample size, b is the mini batch size that will be discussed later, and κ is the restricted condition number.",1. Introduction,[0],[0]
"In contrast, the gradient complexity of the state-of-the-art high-dimensional EM algorithm (Wang et al., 2014) is O ( κN log(1/ ) ) .",1. Introduction,[0],[0]
"As
long as κ ≤ N/b, which holds in most real cases, the overall gradient complexity of our algorithm is less than Wang et al. (2014).",1. Introduction,[0],[0]
"• Different from the proof technique used in existing work
(Balakrishnan et al., 2014; Wang et al., 2014; Yi & Caramanis, 2015), which analyzes both the population and sample versions of the Q-function, we directly analyze the sample version of the Q-function.",1. Introduction,[0],[0]
"Our proof is much simpler and provides a good interface to analyze the semistochastic gradient.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
"We introduce the related work in Section 2, and then present our algorithm and its applications to two representative latent variable models in Section 3.",1. Introduction,[0],[0]
"We demonstrate the main theoretical result as well as its implication to specific latent variable models in Section 4, followed by experimental results in Section 5.",1. Introduction,[0],[0]
"Finally, we conclude our paper and point out some future work in Section 6.
",1. Introduction,[0],[0]
Notation: Let A = [Aij ] ∈,1. Introduction,[0],[0]
"Rd×d be a matrix and v = (v1, . . .",1. Introduction,[0],[0]
", vd)
> ∈ Rd be a vector.",1. Introduction,[0],[0]
"We define the `q1Throughout this paper, we consider the calculation of the gradient of the Q-function over a data point as a unit gradient evaluation cost.",1. Introduction,[0],[0]
"And we use the gradient complexity, i.e., the number of gradient evaluation units, to fairly compare different algorithms.
norm (q ≥ 1) of v as ‖v‖q = (∑d j=1 |vj",1. Introduction,[0],[0]
"|q )1/q
.",1. Introduction,[0],[0]
"Specifically, ‖v‖0 denotes the number of nonzero entries of v, ‖v‖2 = √∑d j=1 v 2 j and ‖v‖∞",1. Introduction,[0],[0]
= maxj |vj |.,1. Introduction,[0],[0]
"For q ≥ 1, we define ‖A‖q as the operator norm of A. Specifically, ‖A‖2 is the spectral norm.",1. Introduction,[0],[0]
"We let ‖A‖∞,∞ = maxi,j |Aij |.",1. Introduction,[0],[0]
"For an integer d > 1, we define [d] = {1, . . .",1. Introduction,[0],[0]
", d}.",1. Introduction,[0],[0]
For an index set I ∈,1. Introduction,[0],[0]
"[d] and vector v ∈ Rd, we use vI ∈ Rd to denote the vector where [vI ]j = vj if j ∈",1. Introduction,[0],[0]
"I , and [vI ]j = 0",1. Introduction,[0],[0]
otherwise.,1. Introduction,[0],[0]
"We use supp(v) to denote the index set of its nonzero entries, and supp(v, s) to denote the index set of top s largest |vj |’s.",1. Introduction,[0],[0]
C is used to denote some absolute constants.,1. Introduction,[0],[0]
The values of these constants may be different from case to case. λmax(A) and λmin(A) are used to denote the largest and smallest eigenvalues of matrix A.,1. Introduction,[0],[0]
We use B(r;β) to denote the ball centered at β with radius r.,1. Introduction,[0],[0]
"In this section, we discuss some related work in detail.",2. Related Work,[0],[0]
"Even with its long history in theory and practice of the EM algorithm (Dempster et al., 1977; Wu, 1983; Tseng, 2004), the finite sample statistical guarantees on EM algorithm have not been pursued until recent research (Balakrishnan et al., 2014; Wang et al., 2014; Yi & Caramanis, 2015).",2. Related Work,[0],[0]
"In a pioneering work by Balakrishnan et al. (2014), both statistical and computational analysis of EM algorithm was conducted in the classical regime.",2. Related Work,[0],[0]
"Specifically, the authors treated EM algorithms as a special perturbed form of standard gradient methods, and they showed that with an appropriate initialization, their algorithm achieves a locally linear convergence rate to the unknown model parameter.",2. Related Work,[0],[0]
"However, their work is limited to the classical regime.",2. Related Work,[0],[0]
"While in the high-dimensional regime, when data dimension is much larger than the number of samples, the M-step is often intractable or even not well defined.",2. Related Work,[0],[0]
"In order to extend this work to the high-dimensional scenario, Wang et al. (2014) addressed this challenge by inserting a truncation step to enforce the sparsity of the parameter.",2. Related Work,[0],[0]
They proved that their algorithm also enjoys locally linear convergence to the model parameter up to certain statistical error.,2. Related Work,[0],[0]
"Yi & Caramanis (2015) proposed a high-dimensional extension of EM algorithms via a regularized M-estimator, and provided similar theoretical guarantees.",2. Related Work,[0],[0]
"In addition, both Balakrishnan et al. (2014) and Wang et al. (2014) proposed gradient variants of the EM algorithm, which can be computationally faster than exact maximization based EM.
",2. Related Work,[0],[0]
"Although the gradient based EM algorithms (Balakrishnan et al., 2014; Wang et al., 2014) have been proved to achieve guaranteed performance, these deterministic approaches can incur huge computational cost in big data and highdimensional scenario since they need costly calculation of full gradient at each iteration.",2. Related Work,[0],[0]
"Stochastic gradient methods are a common workaround to large scale optimization (Bot-
tou, 2010; Gemulla et al., 2011), because one only needs to calculate a mini-batch of the stochastic gradients each time.",2. Related Work,[0],[0]
"However, due to the intrinsic variance introduced by the stochastic gradient, these methods often have a slower convergence rate compared with full gradient methods.",2. Related Work,[0],[0]
"Therefore, a lot of variance reduction techniques have been proposed to reduce the variance of the stochastic gradient and pursue a faster convergence rate.",2. Related Work,[0],[0]
"One of the most popular methods is the stochastic variance-reduced gradient (SVRG) (Johnson & Zhang, 2013).",2. Related Work,[0],[0]
"Inspired by this method, various machine learning tasks (Li et al., 2016; Chen & Gu, 2016; Garber & Hazan, 2015) have utilized the stochastic variance reduction technique to provide improved performance of nonconvex optimization with univariate structures.",2. Related Work,[0],[0]
"Recently, Reddi et al. (2016); Allen-Zhu & Hazan (2016) also analyzed SVRG for the general univariate nonconvex finite-sum optimization.",2. Related Work,[0],[0]
"Motivated by all of these SVRG methods, one natural question is that, can we accelerate gradient based EM algorithms using SVRG?",2. Related Work,[0],[0]
We show in this work that the answer is in the affirmative.,2. Related Work,[0],[0]
"Since all the aforementioned SVRG methods can not be applied to the special bivariate structure of the Q-function, in order to incorporate the variance reduction technique into stochastic gradient based EM algorithms, we need to construct a new semi-stochastic gradient.",2. Related Work,[0],[0]
"In this section, we present our proposed algorithm.",3. Methodology,[0],[0]
"We first introduce the general framework of the EM method, and then give two representative high-dimensional latent variable models as examples before going into the details of our algorithm.",3. Methodology,[0],[0]
We now briefly review the latent variable model and the conventional EM algorithm.,3.1. Background,[0],[0]
"Let Y ∈ Y be the observed random variable and Z ∈ Z be the latent random variable with joint distribution fβ(y, z) and conditional distribution pβ(z|y), with the model parameter β ∈ Rd.",3.1. Background,[0],[0]
"Given N observations {yi}Ni=1 of Y , the EM algorithm aims at maximizing the Q-function
Q̄N (β;β ′)",3.1. Background,[0],[0]
"=
1
N N∑ i=1",3.1. Background,[0],[0]
∫,3.1. Background,[0],[0]
Z pβ′(z|yi) ·,3.1. Background,[0],[0]
"log fβ(yi, z) dz.
",3.1. Background,[0],[0]
"Particularly, in the l-th iteration of EM algorithm, we evaluate Q̄N (β;β(l)) in the E-step, and perform the maximization of Q̄N (β;β(l)) on β in the M-step.",3.1. Background,[0],[0]
"For example, in the standard gradient ascent implementation of EM algorithm, the M-step is given by
β(l+1) = β(l) + η∇1Q̄N (β(l);β(l)),
where∇1Q̄N (·; ·) denotes the gradient on the first variable and η is the learning rate.
",3.1. Background,[0],[0]
"In the high-dimensional regime, we assume β∗ ∈ Rd is sparse with ‖β‖0 ≤ s∗.",3.1. Background,[0],[0]
"In order to ensure the sparsity of the estimator, Wang et al. (2014) proposed to use a truncation step (i.e., T-step) following the M-step.",3.1. Background,[0],[0]
"We now introduce two representative latent variable models as running examples for high-dimensional EM algorithms.
",3.2. Illustrative Examples,[0],[0]
Example 3.1 (Sparse Gaussian Mixture Model).,3.2. Illustrative Examples,[0],[0]
"The random variable Y ∈ Rd is given by
Y = Z · β∗ + V ,
where Z is a random variable with P(Z = 1) =",3.2. Illustrative Examples,[0],[0]
"P(Z = −1) = 1/2, and V ∼N(0,Σ) is a Gaussian random vector, with Σ being the covariance matrix, V and Z are independent, and ‖β∗‖0 ≤ s∗.",3.2. Illustrative Examples,[0],[0]
"We assume Σ is known for simplicity.
",3.2. Illustrative Examples,[0],[0]
Example 3.2 (Mixture of Sparse Linear Regression).,3.2. Illustrative Examples,[0],[0]
"Let X ∈ Rd ∼ N(0,Σ) be a Gaussian random vector, and V ∼N(0, σ2) be a univariate normal random variable.",3.2. Illustrative Examples,[0],[0]
"The random variable Y ∈ R is given by
Y = Z ·X>β∗ + V,
where Z is a random variable with P(Z = 1) =",3.2. Illustrative Examples,[0],[0]
P(Z = −1) = 1/2.,3.2. Illustrative Examples,[0],[0]
"Here X , V and Z are independent, and ‖β‖0 ≤ s∗.",3.2. Illustrative Examples,[0],[0]
"In addition, we assume that σ is known.",3.2. Illustrative Examples,[0],[0]
Now we present our high-dimensional EM algorithm based on stochastic variance-reduced gradient ascent.,3.3. Proposed Algorithm,[0],[0]
"The outline of the proposed algorithm is described in Algorithm 1.
",3.3. Proposed Algorithm,[0],[0]
"Since our algorithm is based on stochastic gradient, we divide theN samples into nmini-batches {Di}ni=1, and define function {qi}ni=1 on these mini-batches, i.e., qi(β;β′) = 1/b ∑ j∈Di ∫ Z pβ′(z|yj) · log fβ(yj , z) dz, where we let b be the mini-batch size, and N = nb.",3.3. Proposed Algorithm,[0],[0]
Let Qn(β;β′),3.3. Proposed Algorithm,[0],[0]
= 1/,3.3. Proposed Algorithm,[0],[0]
n,3.3. Proposed Algorithm,[0],[0]
∑n i=1,3.3. Proposed Algorithm,[0],[0]
"qi(β;β
′).",3.3. Proposed Algorithm,[0],[0]
It is easy to show that Qn(β;β′),3.3. Proposed Algorithm,[0],[0]
"= Q̄N (β;β ′).
",3.3. Proposed Algorithm,[0],[0]
"Note that in Algorithm 1, to ensure the sparsity of the output estimator , we use the hard thresholding operator (Blumensath & Davies, 2009),Hs(v) = vsupp(v,s), which only keeps the largest s entries in magnitude of a vector v ∈ Rd.",3.3. Proposed Algorithm,[0],[0]
"The sparsity parameter s controls the sparsity level of the estimated parameter, and is critical to the estimation error as we will show later.
",3.3. Proposed Algorithm,[0],[0]
We can see that there are two layers of iterations in our algorithm.,3.3. Proposed Algorithm,[0],[0]
"For each outer iteration, we first conduct E-step,
Algorithm 1 Variance Reduced Stochastic Gradient EM Algorithm (VRSGEM) 1",3.3. Proposed Algorithm,[0],[0]
": Parameter: Sparsity Parameter s, Maximum Number of
Outer Iterations m, Number of Inner Iterations T , learning rate η
2: Initialization: β̃(0) = Hs(βinit), 3: For l = 0 to m− 1 4: E-step:
Evaluate Qn ( β; β̃(l) )",3.3. Proposed Algorithm,[0],[0]
"with the dataset
β̃ = β̃(l), µ̃ = ∇1Qn(β̃; β̃) 5: M-step:
β(0) = β̃ Randomly select jl uniformly from {0, . . .",3.3. Proposed Algorithm,[0],[0]
", T − 1}
6: For t = 0 to jl Randomly select i from [n] uniformly
7: v(t) = ∇1qi ( β(t); β̃ ) −∇1qi ( β̃; β̃ ) + µ̃, 8: β(t+0.5) = β(t) + ηv(t), 9: T-step: β(t+1) = Hs(β(t+0.5))
10: End For 11: β̃(l+1) = β(jl+1) 12: End For 13: Output: β̂ = β̃(m)
where we compute the averaged gradient µ̃ based on the whole dataset and the model parameter from last outer iteration.",3.3. Proposed Algorithm,[0],[0]
This averaged gradient will be used repetitively in the M-step for variance reduction.,3.3. Proposed Algorithm,[0],[0]
"In M-step, we have the inner iterations.",3.3. Proposed Algorithm,[0],[0]
"We first determine the number of inner iterations, which is randomly selected from [T ] uniformly.",3.3. Proposed Algorithm,[0],[0]
"At each inner iteration, we make use of the variance reduction technique.",3.3. Proposed Algorithm,[0],[0]
Note that we extend the variance reduction idea originally proposed by Johnson & Zhang (2013) to the bivariate structure of the Q-function.,3.3. Proposed Algorithm,[0],[0]
"Specifically, we design a novel semi-stochastic gradient on mini-batches as v(t) = ∇1qi(β(t); β̃)",3.3. Proposed Algorithm,[0],[0]
"− ∇1qi(β̃; β̃) + µ̃, which fixes the second variable within each outer iteration for the sake of convergence guarantee.",3.3. Proposed Algorithm,[0],[0]
"While the standard gradient implementation of EM algorithm (Wang et al., 2014) uses ∇1Q̄N (β(t);β(t)) to update the parameter at each iteration, our newly designed semi-stochastic gradient is proved to better reduce the variance and attain a lower gradient complexity.",3.3. Proposed Algorithm,[0],[0]
"After finishing all the inner iterations, we use the output from the last inner iteration as the output of this outer iteration.",3.3. Proposed Algorithm,[0],[0]
"We use the output from the last outer iteration as the final estimator.
",3.3. Proposed Algorithm,[0],[0]
"We believe our newly proposed semi-stochastic gradient is of independent interest for the stochastic optimization of functions with bivariate structures, to prove a faster rate of convergence.",3.3. Proposed Algorithm,[0],[0]
"In this section, we show the main theory on the theoretical guarantees of our proposed Algorithm 1.",4. Main Theoretical Results,[0],[0]
"We also present the implications of our algorithm applied to two models
described in Section 3.2.
",4. Main Theoretical Results,[0],[0]
"To facilitate the technical analysis of our algorithm, we focus on the resampling version of Algorithm 1 following the convention of previous work (Wang et al., 2014; Yi & Caramanis, 2015).",4. Main Theoretical Results,[0],[0]
The key difference between the resampling version and Algorithm 1 is that we split the whole dateset into m subsets and use one subset for each outer iteration.,4. Main Theoretical Results,[0],[0]
The details of the resampling version of our algorithm is provided in the longer version of this paper.,4. Main Theoretical Results,[0],[0]
"It is worth noting that the resampling version of our algorithm is able to decouple the dependence between consecutive outer iterations, and it is only used to simplify the technical proof.",4. Main Theoretical Results,[0],[0]
"In practice including our experiment, we use Algorithm 1 rather than the resampling version.
",4. Main Theoretical Results,[0],[0]
"Before we present the main results, we introduce three conditions that are essential for our analysis.
",4. Main Theoretical Results,[0],[0]
Condition 4.1 (Smoothness).,4. Main Theoretical Results,[0],[0]
"For any β,β1,β2 ∈ B(p‖β∗‖2;β∗), where p ∈ (0, 1) is a model-dependent constant, for any i ∈",4. Main Theoretical Results,[0],[0]
"[n], qi(·; ·) in Algorithm 1 satisfies the smoothness condition with respect to the first variable with parameter L:∥∥∇1qi(β1;β)−∇1qi(β2;β)∥∥2 ≤ L∥∥β1",4. Main Theoretical Results,[0],[0]
− β2∥∥2.,4. Main Theoretical Results,[0],[0]
Condition 4.1 says that the gradient of qi(·; ·) we use in each inner iteration is Lipschitz continuous with respect to the first variable when the first and second variables are within the ball B(p‖β∗‖2;β∗).,4. Main Theoretical Results,[0],[0]
"There exists a wide range of models with this condition holding.
",4. Main Theoretical Results,[0],[0]
Condition 4.2 (Concavity).,4. Main Theoretical Results,[0],[0]
"For all β,β1,β2 ∈ B(p‖β∗‖2;β∗), where p ∈ (0, 1) is a model-dependent constant, the function Qn(·; ·) satisfies the strong concavity condition with parameter µ:[
∇1Qn(β1;β)−∇1Qn(β2;β) ]",4. Main Theoretical Results,[0],[0]
">
(β1 − β2) ≤",4. Main Theoretical Results,[0],[0]
"−µ‖β2 − β1‖22.
",4. Main Theoretical Results,[0],[0]
Condition 4.2 requires Qn(·; ·) to be strongly concave with respect to the first variable when the first and second variables are within the ball B(p‖β∗‖2;β∗).,4. Main Theoretical Results,[0],[0]
"This is a reasonable requirement when N is large enough.
",4. Main Theoretical Results,[0],[0]
Condition 4.3 (First-order stability).,4. Main Theoretical Results,[0],[0]
"For the true model parameter β∗ and any β ∈ B(p‖β∗‖2;β∗), where p ∈ (0, 1) is a model-dependent constant, Qn ( ·; · )
satisfies the first-order stability with parameter γ:∥∥∇1Qn(β∗;β)−∇1Qn(β∗;β∗)∥∥2 ≤ γ∥∥β",4. Main Theoretical Results,[0],[0]
− β∗∥∥2.,4. Main Theoretical Results,[0],[0]
"Condition 4.3 requires that the gradient ∇1Qn(β∗; ·) is stable with regard to the second variable, with the second variable within the ball B(p‖β∗‖2;β∗).",4. Main Theoretical Results,[0],[0]
"There are actually
various versions of this condition in previous work (Yi & Caramanis, 2015; Balakrishnan et al., 2014) on population versionQ(·; ·) = E[Qn(·, ·)].",4. Main Theoretical Results,[0],[0]
"Here we impose the condition on the sample Q-function, i.e., Qn(·, ·), because our proof technique directly analyzes the sample Q-function.",4. Main Theoretical Results,[0],[0]
"Intuitively, when the sample size N is sufficiently large, Qn(·; ·)",4. Main Theoretical Results,[0],[0]
andQ(·; ·) should be close.,4. Main Theoretical Results,[0],[0]
"Therefore, this condition should hold for Qn(·; ·) as well.
",4. Main Theoretical Results,[0],[0]
"Due to the space limit, we verify the above conditions for the two examples in the longer version of this paper.",4. Main Theoretical Results,[0],[0]
We use κ = L/µ to denote the condition number .,4. Main Theoretical Results,[0],[0]
With the above conditions on qi(·; ·),4.1. Theory for the Generic Model,[0],[0]
"and Qn(·; ·), we have the following theorem to characterize the estimation error of our estimator β̃(r) returned by the resampling version of Algorithm 1.
",4.1. Theory for the Generic Model,[0],[0]
Theorem 4.4.,4.1. Theory for the Generic Model,[0],[0]
"Suppose qi(·; ·) satisfies Condition 4.1 and Qn(·; ·) satisfies Conditions 4.2, 4.3.",4.1. Theory for the Generic Model,[0],[0]
"We also assume that ∥∥βinit − β∗∥∥ 2 ≤ p ∥∥β∗∥∥ 2 , where p ∈ (0, 1).",4.1. Theory for the Generic Model,[0],[0]
"If η ≤ µ/(8L2), and T and s are chosen such that
ρ = 1
T (1− τ) +
2αη [ ηL2 + (2η + L/µ2)γ2 ] 1− τ",4.1. Theory for the Generic Model,[0],[0]
"< 1,
where τ = α(1−ηµ+ 2η2L2)",4.1. Theory for the Generic Model,[0],[0]
"and α = 1 + √ s∗/ √ s− s∗, then the estimator β̃(r) from the resampling version of Algorithm 1 satisfies
E ∥∥β̃(r)−β∗∥∥
2 ≤ ρr/2 ∥∥βinit",4.1. Theory for the Generic Model,[0],[0]
"− β∗∥∥ 2
+
√ 2s̃αη(2η + L/µ2)
(1− τ)(1− ρ) ∥∥∇1Qn(β∗;β∗)∥∥∞,
(4.1)
",4.1. Theory for the Generic Model,[0],[0]
"where s̃ = 2s+ s∗.
Remark 4.5.",4.1. Theory for the Generic Model,[0],[0]
"As suggested in Theorem 4.4 that by choosing an appropriate learning rate η, a sufficiently large number of inner iterations T , and sparsity parameter s such that ρ < 1, we can achieve a linear convergence rate.",4.1. Theory for the Generic Model,[0],[0]
Here we give an example to show that such ρ is achievable.,4.1. Theory for the Generic Model,[0],[0]
"If we choose step size η = µ/(8L2), and truncation parameter s satisfies
s >
[ 4(1−K)2
K2 + 1
] s∗,
where
K = 5µ2 96L2 − µ 2γ2 12L4 − γ 2 3Lµ > 0.
",4.1. Theory for the Generic Model,[0],[0]
"Then, we can get
α < 1
1− 5µ2/96L2 + µ2γ2/12L4",4.1. Theory for the Generic Model,[0],[0]
+,4.1. Theory for the Generic Model,[0],[0]
"γ2/3Lµ ,
and the contraction parameter ρ in Theorem 4.4 can be simplified as
ρ ≤ 1 T (1− τ) + 3 4 .
",4.1. Theory for the Generic Model,[0],[0]
"Therefore, if we choose T ≥ 256κ2/ ( 3(α − 1) ) , we can obtain ρ ≤ 7/8, ensuring the linear convergence rate.
",4.1. Theory for the Generic Model,[0],[0]
Remark 4.6.,4.1. Theory for the Generic Model,[0],[0]
The right hand side of (4.1) in Theorem 4.4 consists of two terms.,4.1. Theory for the Generic Model,[0],[0]
The first term stands for the optimization error.,4.1. Theory for the Generic Model,[0],[0]
The second term is the statistical error.,4.1. Theory for the Generic Model,[0],[0]
"According to Remark 4.5, we can ensure the linear convergence rate of our algorithm.",4.1. Theory for the Generic Model,[0],[0]
"Thus for any error bound > 0, we need r ≥ 2 logρ−1 [‖βinit − β∗‖2/ ]",4.1. Theory for the Generic Model,[0],[0]
iterations to let the optimization error ρr/2‖βinit,4.1. Theory for the Generic Model,[0],[0]
"− β∗‖2 ≤ , which basically requires O ( log(1/ ) )",4.1. Theory for the Generic Model,[0],[0]
outer iterations.,4.1. Theory for the Generic Model,[0],[0]
"For each outer iteration, we need to compute T gradients of qi(·, ·), and one full gradient.",4.1. Theory for the Generic Model,[0],[0]
"Since we have T = O(κ2), which is suggested in Remark 4.5, the gradient complexity of our algorithm would be O ( (N + bκ2) · log(1/ ) ) .",4.1. Theory for the Generic Model,[0],[0]
"Nevertheless, for the state-of-the-art gradient based high-dimensional EM algorithm (Wang et al., 2014), its gradient complexity is O ( κN log(1/ ) ) .",4.1. Theory for the Generic Model,[0],[0]
"As long as κ ≤ N/b, the gradient complexity of our algorithm is less than that of Wang et al. (2014).",4.1. Theory for the Generic Model,[0],[0]
"Since in big data scenarios, N is always very large and b as the batch size is relatively small, this condition is naturally satisfied in most real applications.
",4.1. Theory for the Generic Model,[0],[0]
"The second term on the right-hand side of (4.1) stands for the upper bound of the statistical error, which depends on specific models as we will introduce later.",4.1. Theory for the Generic Model,[0],[0]
Now we apply our algorithm to the two examples introduced in Section 3.2.,4.2. Implications for Specific Models,[0],[0]
"The next corollary gives the implication of our main theory for sparse Gaussian mixture models.
",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
Corollary 4.7.,4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Under the same conditions of Theorem 4.4 and suppose ∥∥βinit − β∗∥∥ 2
≤ ( √ λmin(Σ)/λmax(Σ)/4)",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
∥∥β∗∥∥ 2 .,4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Then with probability at least 1 − 2e/d, the estimator β̂ = β̃(m) from the resampling version of Algorithm 1 satisfies
E ∥∥β̂",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"− β∗∥∥
2 ≤ ρm/2 ∥∥βinit",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"− β∗∥∥ 2
+ CΦκ3/2 √ s∗ log d · logN
N , (4.2)
where Φ = λmin(Σ) ( ‖Σ−1β∗‖∞ + σλ−1/2min (Σ) ) and κ = L/µ.
Proof Sketch.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"For sparse Gaussian mixture model, we have Conditions 4.1 to 4.3 hold with parameters L = 1/λmin(Σ), µ = 1/λmax(Σ), and γ = 20(ξ2 + ξ + 1 + ξ−2)e−ξ
2/64/λmin(Σ), where ξ = ‖Σ−1/2β∗‖2 denotes the signal-to-noise ratio (SNR).",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Next, s̃ = 2s+ s∗ is of the same order as s∗.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"For the term ‖∇1Qn(β∗;β∗)‖∞ in (4.1), we have the following inequality holds with probability",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"at least 1− 2e/d
‖∇1Qn(β∗;β∗)‖∞ ≤ C ( ‖Σ−1β∗‖∞ +
σ√ λmin(Σ)
)",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"√ log d · logN
N .
",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"This completes the proof.
",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
Remark 4.8.,4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"We can see that the parameters in Conditions 4.1 and 4.2 are determined by the covariance matrix Σ, which is reasonable because Σ actually denotes the variance of the data.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"For Condition 4.3, we need to introduce the signal-to-noise ratio (SNR).",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
The concept of SNR in parameter estimation is also proposed in Balakrishnan et al. (2014); Dasgupta & Schulman (2007).,4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Since we have extended the covariance matrix of noise from identity matrix in previous work to any positive definite matrix, our SNR is also a little bit different from their definition.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Generally speaking, for GMM with lower SNR, the variance of the noise makes it harder or even impossible for the algorithm to converge.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Therefore, it is always reasonable to have a requirement for the SNR of GMM to be large enough for reliable parameter estimation.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Spectral method (Anandkumar et al., 2014) can be used to match the requirement on initialization for GMM, however, we find that random initialization also performs reasonably well in practice as we will show later.
",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"According to Remark 4.5, by choosing appropriate learning rate η, inner iterations T , and sparsity parameter s, we can ensure linear convergence rate of our algorithm.",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"Therefore, from Corollary 4.7, we know that after O ( log ( N/(s∗ log d logN) ))",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"number of iterations, the
output of our algorithm attainsO( √ s∗ log d · logN/N) statistical error, which matches the best-known error bound (Wang et al., 2014; Yi & Caramanis, 2015) for Gaussian mixture model up to a logarithmic factor logN .",4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
Note that the extra logarithmic factor is due to the resampling strategy.,4.2.1. SPARSE GAUSSIAN MIXTURE MODEL,[0],[0]
"The implication of our main theory for mixture of linear regression is presented in the following corollary.
",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
Corollary 4.9.,4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"Under the same conditions of Theorem 4.4 and suppose ∥∥βinit − β∗∥∥ 2
≤ ( √ λmin(Σ)/λmax(Σ)/32) ∥∥β∗∥∥ 2 .",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"Then with probability at least 1 − 2e/d, the estimator β̂ = β̃(m) from the
resampling version of Algorithm 1 satisfies E ∥∥β̂",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"− β∗∥∥
2 ≤ ρm/2 ∥∥βinit",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"− β∗∥∥ 2
+ Cκ3/2 ( ‖β∗‖2 +
σ√ λmax(Σ)
)",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"√ s∗ log d · logN
N ,
where κ = L/µ.
Proof Sketch.",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"For mixture of linear regression, we have Conditions 4.1 to 4.3 hold with parameters L = 2λmax(Σ), µ = λmin(Σ)/2, and γ = γ1λmax(Σ), where γ1 ∈ (0, 1/3) is a constant.",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"We also show that s̃ is of the same order as s∗. Next, for the term ‖∇1Qn(β∗;β∗)‖∞ in (4.1), we have the following inequality holds with probability at least 1−2e/d
‖∇1Qn(β∗;β∗)‖∞
≤ C ( λmax(Σ)‖β∗‖2 + λ1/2max(Σ)σ )",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"√ log d · logN N .
",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"This completes the proof.
",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
Remark 4.10.,4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"According to Remark 4.5, our algorithm can achieve a linear convergence rate with appropriate learning rate η, inner iterations T , and sparsity parameter s. Thus Corollary 4.9 tells us that after O ( log ( N/(s∗ log d logN) )) number of outer iterations, the output of our algorithm achieves O( √ s∗ log d · logN/N) statistical error, which matches the best-known statistical error (Yi & Caramanis, 2015) for mixture of linear regression up to a logarithmic factor from the resampling strategy.",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"Specifically, the dependence on ‖β∗‖2 is due to the fundamental limits of EM, which also appears in Balakrishnan et al. (2014); Yi & Caramanis (2015).",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"There is also a spectral method (Chaganty & Liang, 2013) helping the initialization of MLR, but we use random initialization which also performs well in our experiments.",4.2.2. MIXTURE OF SPARSE LINEAR REGRESSION,[0],[0]
"In this section, we present experiment results to validate our theory.",5. Experiments,[0],[0]
"For parameter estimation, we use Gaussian mixture model and mixture of linear regression, and compare our proposed variance-reduced stochastic gradient EM algorithm (VRSGEM) with two state-of-the-art highdimensional EM algorithms as baselines:
• (HDGEM) High-Dimensional Gradient EM algorithm proposed in Wang et al. (2014): the gradient variant of high-dimensional EM method enforcing sparsity structure.",5. Experiments,[0],[0]
"• (HDREM) High-Dimensional Regularized EM algorithm proposed in Yi & Caramanis (2015): the method based on decaying regularization.
",5. Experiments,[0],[0]
"Since high-dimensional scenario is much more challenging, we only compare our algorithm with high-dimensional EM algorithms.",5. Experiments,[0],[0]
"For each latent variable model, we compare both the optimization error ‖β̃(l) − β̂‖2 featuring the convergence of the estimator to the local optima, and the overall estimation error ‖β̃(l)−β∗‖2 featuring the overall estimation accuracy with regard to the true model parameter β∗.",5.1. Experimental Setup,[0],[0]
We also show the convergence comparison in terms of training time.,5.1. Experimental Setup,[0],[0]
"All the comparisons are under two different parameter settings: s∗ = 5, d = 256, b = 100, N = 5000 and s∗ = 10, d = 512, b = 200, N = 10000.",5.1. Experimental Setup,[0],[0]
"For VRSGEM, we choose m = 30, n = 50 and T = 50 across all settings and models.",5.1. Experimental Setup,[0],[0]
"Besides the comparison of different algorithms, we also verify our statistical rate of convergence by plotting the statistical error ‖β̂",5.1. Experimental Setup,[0],[0]
− β∗‖ against √ s∗ log d/N .,5.1. Experimental Setup,[0],[0]
"Specifically, we fix d = 512 and show the plots of three cases s∗ = 5, s∗ = 10 and s∗ = 15 with varying N .
",5.1. Experimental Setup,[0],[0]
"In each experiment setting, we run 100 trials and show the averaged results.",5.1. Experimental Setup,[0],[0]
The learning rate η is tuned by grid search and s is chosen by cross validation.,5.1. Experimental Setup,[0],[0]
We use random initialization.,5.1. Experimental Setup,[0],[0]
We test VRSGEM on Gaussian mixture models introduced in Section 3.2.,5.2. Gaussian Mixture Model,[0],[0]
"For the sake of simplicity and better matching the problem setting of the baseline methods, the co-
variance matrix Σ of V is chosen to be a diagonal matrix with all elements being 1.",5.2. Gaussian Mixture Model,[0],[0]
"We randomly set two elements to λmax(Σ) = 10, and another two elements to λmin(Σ) = 0.1.",5.2. Gaussian Mixture Model,[0],[0]
"The results are shown in Figures 1 and 2.
",5.2. Gaussian Mixture Model,[0],[0]
"From Figures 1(a) and 2(a), we can see that all three algorithms have linear convergence as Corollary 4.7 suggests.",5.2. Gaussian Mixture Model,[0],[0]
VRSGEM clearly enjoys a faster convergence rate than the baselines.,5.2. Gaussian Mixture Model,[0],[0]
"Moreover, as shown in Figures 1(b) and 2(b), the performance on overall estimation error of our algorithm is as good as HDGEM, which is far better than HDREM.",5.2. Gaussian Mixture Model,[0],[0]
"In terms of time consumption, our algorithm also enjoys a remarkable advantage over the baselines as shown in Figures 1(c), 1(d), 2(c) and 2(d).
",5.2. Gaussian Mixture Model,[0],[0]
The statistical error results are shown in Figure 5.,5.2. Gaussian Mixture Model,[0],[0]
"From Figure 5(a), we can see that statistical error of VRSGEM shows a linear dependency on √ s∗ log d/N across different settings of s∗, verifying results in Corollary 4.7.",5.2. Gaussian Mixture Model,[0],[0]
"Similar to the setting for GMM, we use the same covariance matrix Σ in Section 5.2 for X here.",5.3. Mixture of Linear Regression,[0],[0]
"For V , we let σ = 1.",5.3. Mixture of Linear Regression,[0],[0]
"We show the results in Figures 3 and 4.
",5.3. Mixture of Linear Regression,[0],[0]
"From Figures 3(a) and 4(a), we can see that VRSGEM achieves linear convergence which is consistent with Corollary 4.9, and our algorithm significantly outperforms the
baselines in terms of optimization error.",5.3. Mixture of Linear Regression,[0],[0]
"In terms of overall estimation error shown in Figures 3(b) and 4(b), VRSGEM is as good as HDGEM and beats HDREM by a remarkable margin.",5.3. Mixture of Linear Regression,[0],[0]
"Our algorithm also beats the baselines in time consumption for convergence as we can see in Figures 3(c), 3(d), 4(c) and 4(d).",5.3. Mixture of Linear Regression,[0],[0]
"Overall, VRSGEM achieves the best performance among all the methods.
",5.3. Mixture of Linear Regression,[0],[0]
"In addition, from Figure 5(b), we can see that for MLR, the statistical error of VRSGEM is of order √ s∗ log d/N , which supports Corollary 4.9.",5.3. Mixture of Linear Regression,[0],[0]
"We propose a novel accelerated stochastic gradient EM algorithm based on a uniquely constructed semi-stochastic
variance reduced gradient.",6. Conclusions and Future Work,[0],[0]
"We show that with an appropriate initialization, the proposed algorithm converges at a linear rate and attains the optimal statistical rate.",6. Conclusions and Future Work,[0],[0]
"We apply our proposed algorithm to two popular latent variable models in the high-dimensional regime and numerical experiments are provided to support our theory.
",6. Conclusions and Future Work,[0],[0]
"It is worth noting that, the proposed algorithm is directly applicable to the classical regime, by dropping the T-step.",6. Conclusions and Future Work,[0],[0]
"It will give rise to an accelerated stochastic extension of conventional EM algorithm, and the corresponding theory in this paper can be extended to the classical regime analogously (Balakrishnan et al., 2014).",6. Conclusions and Future Work,[0],[0]
We will investigate this by-product in our future work.,6. Conclusions and Future Work,[0],[0]
"We also plan to extend our algorithm to the estimation of high-dimensional latent variable models with low-rank parameters (Yi & Caramanis, 2015).",6. Conclusions and Future Work,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This research was sponsored in part by the National Science Foundation under Grant Numbers CNS-1513939, CNS-1027965, IIS-1629161, IIS-1618948, IIS-1652539.",Acknowledgments,[0],[0]
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.,Acknowledgments,[0],[0]
We propose a generic stochastic expectationmaximization (EM) algorithm for the estimation of high-dimensional latent variable models.,abstractText,[0],[0]
At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the Qfunction in the EM algorithm.,abstractText,[0],[0]
"Under a mild condition on the initialization, our algorithm is guaranteed to attain a linear convergence rate to the unknown parameter of the latent variable model, and achieve an optimal statistical rate up to a logarithmic factor for parameter estimation.",abstractText,[0],[0]
"Compared with existing high-dimensional EM algorithms, our algorithm enjoys a better computational complexity and is therefore more efficient.",abstractText,[0],[0]
"We apply our generic algorithm to two illustrative latent variable models: Gaussian mixture model and mixture of linear regression, and demonstrate the advantages of our algorithm by both theoretical analysis and numerical experiments.",abstractText,[0],[0]
We believe that the proposed semi-stochastic gradient is of independent interest for general nonconvex optimization problems with bivariate structures.,abstractText,[0],[0]
High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1150–1160, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
The accuracy of Semantic Role Labeling (SRL) systems depends strongly on the features used by the underlying classifiers.,1 Introduction,[0],[0]
"For instance, the top performing system on the CoNLL–2009 shared task employs over 50 language-specific templates for feature generation (Che et al., 2009).",1 Introduction,[0],[0]
"The templates
1Our code is available at https://github.com/ taolei87/SRLParser.
are manually created and thus offer specific means of incorporating prior knowledge into the method.",1 Introduction,[0],[0]
"However, finding compact, informative templates is difficult since the relevant signal may be spread over many correlated features.",1 Introduction,[0],[0]
"Moreover, the use of lexicalized features, which are inevitably sparse, leads to overfitting.",1 Introduction,[0],[0]
In this case it is advantageous to try to automatically compress the feature set to use a small number of underlying co-varying dimensions.,1 Introduction,[0],[0]
Dimensionality reduction of this kind can be incorporated into the classifier directly by utilizing tensor calculus.,1 Introduction,[0],[0]
"In this paper, we adopt this strategy.
",1 Introduction,[0],[0]
We start by building high-dimensional feature vectors that are subsequently mapped into a lowdimensional representation.,1 Introduction,[0],[0]
"Since this highdimensional representation has to reflect the interaction between different indicators of semantic relations, we construct it as a cross-product of smaller feature vectors that capture distinct facets of semantic dependence: predicate, argument, syntactic path and role label.",1 Introduction,[0],[0]
"By compressing this sparse representation into lower dimensions, we obtain dense representations for words (predicate, argument) and their connecting paths, uncovering meaningful interactions.",1 Introduction,[0],[0]
"The associated parameters are maintained as a four-way low-rank tensor, and optimized for SRL performance.",1 Introduction,[0],[0]
"Tensor modularity enables us to employ standard online algorithms for training.
",1 Introduction,[0],[0]
"Our approach to SRL is inspired by recent success of our tensor-based approaches in dependency parsing (Lei et al., 2014).",1 Introduction,[0],[0]
"Applying analogous techniques to SRL brings about new challenges, however.",1 Introduction,[0],[0]
"The scoring function needs to reflect the highorder interactions between the predicate, argument,
1150
their syntactic path and the corresponding role label.",1 Introduction,[0],[0]
"Therefore, we parametrize the scoring function as a four-way tensor.",1 Introduction,[0],[0]
Generalization to high-order tensors also requires new initialization and update procedures.,1 Introduction,[0],[0]
"For instance, the SVD initialization used in our dependency parsing work results in memory explosion when extending to our 4-way tensor.",1 Introduction,[0],[0]
"Instead, we employ the power method (De Lathauwer et al., 1995) to build the initial tensor from smaller pieces, one rank-1 component at a time.",1 Introduction,[0],[0]
"For learning, in order to optimize an overall non-convex objective function with respect to the tensor parameters, we modify the passive-aggressive algorithm to update all the low-rank components in one step.",1 Introduction,[0],[0]
"The update strategy readily generalizes to any high-order tensor.
",1 Introduction,[0],[0]
"We evaluate our tensor-based approach for SRL on the CoNLL–2009 shared task benchmark datasets of five languages: English, German, Chinese, Catalan and Spanish (Surdeanu et al., 2008).",1 Introduction,[0],[0]
"As a baseline, we use a simple SRL model that relies on a minimal set of standard features.",1 Introduction,[0],[0]
"Our results demonstrate that the tensor-based model outperforms the original SRL model by a significant margin, yielding absolute improvements of 2.1% F1 score.",1 Introduction,[0],[0]
"We also compare our results against the best performing system on this task (Zhao et al., 2009a).",1 Introduction,[0],[0]
"On three out of five languages, the tensor-based model outperforms this system.",1 Introduction,[0],[0]
These results are particularly notable because the system of Zhao et al. (2009a) employs a rich set of language-specific features carefully engineered for this task.,1 Introduction,[0],[0]
"Finally, we demonstrate that using four-way tensor yields better performance than its three-way counterpart, highlighting the importance of modeling the relation between role labels and properties of the path.",1 Introduction,[0],[0]
"A great deal of SRL research has been dedicated to designing rich, expressive features.",2 Related Work,[0],[0]
"The initial work by Gildea and Jurafsky (2002) already identified a compact core set of features, which were widely adopted by the SRL community.",2 Related Work,[0],[0]
"These features describe the predicate, the candidate argument, and the syntactic relation between them (path).",2 Related Work,[0],[0]
"Early systems primarily extended this core set by including local context lexicalized patterns (e.g., n-grams),
several extended representations of the path features, and some linguistically motivated syntactic patterns, as the syntactic frame (Surdeanu et al., 2003; Xue and Palmer, 2004; Pradhan et al., 2005).
",2 Related Work,[0],[0]
More recent approaches explored a broader range of features.,2 Related Work,[0],[0]
"Among others, Toutanova et al. (2008), Martins and Almeida (2014) and Yang and Zong (2014) have explored high-order features involving several arguments and even pairs of sentence predicates.",2 Related Work,[0],[0]
"Other approaches have focused on semantic generalizations of lexical features using selectional preferences, neural network embeddings or latent word language models (Zapirain et al., 2013; Collobert et al., 2011; Deschacht and Moens, 2009; Roth and Woodsend, 2014).",2 Related Work,[0],[0]
"To avoid the intensive feature engineering inherent in SRL, Moschitti et al. (2008) employ kernel learning.",2 Related Work,[0],[0]
"Although attractive from this perspective, the kernel-based approach comes with a high computational cost.",2 Related Work,[0],[0]
"In contrast to prior work, our approach effectively learns lowdimensional representation of words and their roles, eliminating the need for heavy manual feature engineering.",2 Related Work,[0],[0]
"Finally, system combination approaches such as reranking typically outperform individual systems (Björkelund et al., 2010).",2 Related Work,[0],[0]
"Our method can be easily integrated as a component in one of those systems.
",2 Related Work,[0],[0]
"In technical terms, our work builds on our recent tensor-based approach for dependency parsing (Lei et al., 2014).",2 Related Work,[0],[0]
"In that work, we use a three-way tensor to score candidate dependency relations within a first-order scoring function.",2 Related Work,[0],[0]
The tensor captures the interaction between words and their syntactic (headmodifier) relations.,2 Related Work,[0],[0]
"In contrast, the scoring function in SRL involves higher-order interactions between the path, argument, predicate and their associated role label.",2 Related Work,[0],[0]
"Therefore, we parametrized the scoring function with a four-way low-rank tensor.",2 Related Work,[0],[0]
"To help with this extension, we developed a new initialization and update strategy.",2 Related Work,[0],[0]
Our experimental results demonstrate that the new representation tailored to SRL outperforms previous approaches.,2 Related Work,[0],[0]
"Our setup follows the CoNLL–2009 shared task (Hajič et al., 2009).",3 Problem Formulation,[0],[0]
"Each token in sentence x is annotated with a predicted POS tag and predicted
UNESCO is holding its meetings in Paris
A0 A1
AM-LOC
word lemma.",3 Problem Formulation,[0],[0]
"Some tokens are also marked as predicates, i.e., argument-bearing tokens.",3 Problem Formulation,[0],[0]
The goal is to determine the semantic dependencies for each predicate pi (cf. upper part of Figure 1).,3 Problem Formulation,[0],[0]
These dependencies identify the arguments of each predicate and their role labels.,3 Problem Formulation,[0],[0]
"In this work, we focus only on the semantic side – that is, identification and classification of predicate arguments.",3 Problem Formulation,[0],[0]
"To this end, our system takes as input a syntactic dependency tree ysyn derived from a state-of-the-art parser (bottom part of Figure 1).
",3 Problem Formulation,[0],[0]
"More formally, let {pi} ⊂ x be the set of verbal and nominal predicates in the sentence.",3 Problem Formulation,[0],[0]
"For each predicate pi (e.g., “holding”), our goal is to predict tuples (pi, aij , rij) specifying the semantic dependency arcs, where aij ∈ x is one argument (e.g., “meetings”), and rij is the corresponding semantic role label (e.g., A1).",3 Problem Formulation,[0],[0]
The semantic parse is then the collection of predicted arcs zsem,3 Problem Formulation,[0],[0]
=,3 Problem Formulation,[0],[0]
"{(pi, aij , rij)}.
",3 Problem Formulation,[0],[0]
We decouple syntactic and semantic inference problems into two separate steps.,3 Problem Formulation,[0],[0]
We first run our syntactic dependency parser RBGParser2 to obtain the syntactic dependency tree ysyn.,3 Problem Formulation,[0],[0]
"The semantic parse zsem is then found conditionally on the syntactic part:
z∗sem = arg maxzsem Ssem(x,ysyn, zsem), (1)
",3 Problem Formulation,[0],[0]
Here Ssem(·) is the parametrized scoring function to be learned.,3 Problem Formulation,[0],[0]
"We build our scoring function by combining a traditional feature scoring function with a tensor-based scoring function.
2https://github.com/taolei87/RBGParser",3 Problem Formulation,[0],[0]
"In a typical feature-based approach (Johansson, 2009; Che et al., 2009), feature templates give rise to rich feature descriptions of the semantic structure.",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"The score Ssem(x,ysyn, zsem) is then defined as the inner product between the parameter vector and the feature vector.",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"In the first-order arc-factored case,
Ssem(x,ysyn, zsem) = w · φ(x,ysyn, zsem) = ∑ (p,a,r)∈zsem w · φ(p, a, r),
where w are the model parameters and φ(p, a, r) is the feature vector representing a single semantic arc (p, a, r) (we suppress its dependence on x and ysyn).",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"We also experiment with second order features, i.e., considering two arguments associated with the same predicate, or two predicates sharing the same token as argument.
",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"For the arc-factored model, there are mainly four types of atomic information that define the arc features in φ(p, a, r):
(a) the predicate token p (and its local context); (b) the argument token a (and its local context); (c) the dependency label path that connects p and
a in the syntactic tree; (d) the semantic role label r of the arc.
",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
These pieces of atomic information are either used directly or combined as unigram up to 4-gram features into traditional models.,3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"To avoid heavy feature engineering and overfitting, we use a light and compact feature set derived from the information in (a)–(d).",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"Table 1 shows the complete list of feature
templates, used as our first-order semantic baseline in the experiments.",3.1 Traditional Scoring Using Manually-designed Features,[0],[0]
"Now, we describe the tensor-based scoring function.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"We characterize each semantic arc (p, a, r) using the cross-product of atomic feature vectors associated with the four types of information described above: the predicate vector φ(p), the argument vector φ(a), the dependency path vector φ(path) and the semantic role label vector φ(r).",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"For example, in the simplest case φ(p),φ(a) ∈",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"[0, 1]n are one-hot indicator vectors, where n is the size of the vocabulary.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Similarly, φ(path) ∈",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"[0, 1]m and φ(r) ∈",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"[0, 1]l are indicator vectors where m is the number of unique paths (seen in the training set) and l is the number of semantic role labels.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Of course, we can add other atomic information into these atomic vectors.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"For example, φ(p) will not only indicate the word form of the current predicate p, but also the word lemma, POS tag and surrounding tokens as well.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"The crossproduct of these four vectors is an extremely highdimensional rank-1 tensor,
φ(p)⊗ φ(a)⊗ φ(path)⊗ φ(r) ∈",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Rn×n×m×l
in which each entry indicates the combination of four atomic features appearing in the semantic arc (p, a, r)3.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"The rank-1 tensor (cross-product) captures all possible combinations over atomic units, and therefore it is a full feature expansion over the manually selected feature set in Table 1.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Similar to the traditional scoring, the semantic arc score is the inner product between a 4-way parameter tensor A and this feature tensor:
A ∈",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Rn×n×m×l : vec(A) · vec (φ(p)⊗ φ(a)⊗ φ(path)⊗ φ(r)) ,
(2)
where vec(·) denotes the vector representation of a matrix / tensor.
",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Instead of reducing and pruning possible feature concatenations (e.g., by manual feature template
3We always add a bias term into these atomic vectors (e.g., a fixed “1” attached to the beginning of every vector).",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Therefore, their cross-product will contain all unigram to 4-gram concatenations, not just 4-gram concatenations.
construction as in the traditional approach), this tensor scoring method avoids parameter explosion and overfitting by assuming a low-rank factorization of the parameters A. Specifically, A is decomposed into the sum of k simple rank-1 components,
A = k∑
i=1
P (i)⊗Q(i)⊗R(i)⊗ S(i).",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"(3)
Here k is a small constant, P,Q ∈ Rk×n, R ∈ Rk×m and S ∈ Rk×l are parameter matrices, and P (i) (and similarly Q(i), R(i) and S(i)) represents the i-th row vector of matrix P .
",3.2 Low-rank Scoring via Projected Representations,[0],[0]
The advantages of this low-rank assumption are as follows.,3.2 Low-rank Scoring via Projected Representations,[0],[0]
"First, computing the score no longer requires maintaining and constructing extremely large tensors.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Instead, we can project atomic vectors via P , Q, R and S obtaining small dense vectors, and subsequently calculating the arc score by
k∑ i=1",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"[Pφ(p)]i [Qφ(a)]i [Rφ(path)]i [Sφ(r)]i .
Second, projecting atomic units such as words, POS tags and labels into dense, low-dimensional vectors can effectively alleviate the sparsity problem, and it enables the model to capture high-order feature interactions between atomic units, while avoiding the parameter explosion problem.",3.2 Low-rank Scoring via Projected Representations,[0],[0]
"Similar to our low-rank syntactic dependency parsing model (Lei et al., 2014), our final scoring function Ssem(x,ysyn, zsem) is the combination of the traditional scoring and the low-rank scoring,
Ssem(x,ysyn, zsem)",3.3 Combined System,[0],[0]
"=
γ w · φ(x,ysyn, zsem) +",3.3 Combined System,[0],[0]
"(1− γ) ∑
(p,a,r)∈zsem
k∑ i=1
",3.3 Combined System,[0],[0]
"[Pφ(p)]i [Qφ(a)]i [Rφ(path)]i [Sφ(r)]i .
where γ ∈",3.3 Combined System,[0],[0]
"[0, 1] is a hyper-parameter balancing the two scoring terms.",3.3 Combined System,[0],[0]
We tune this value on the development set.,3.3 Combined System,[0],[0]
"Finally, the set of parameters of our model is denoted as θ = {w, P,Q,R, S}.",3.3 Combined System,[0],[0]
Our goal is to optimize the weight vector w as well as the four projection matrices given the training set.,3.3 Combined System,[0],[0]
We now describe the learning method for our SRL model.,4 Learning,[0],[0]
"Let D = {(x̂(i), ˆysyn(i), ˆzsem(i))}Ni=1 be the collection of N training samples.",4 Learning,[0],[0]
"The values of the set of parameters θ = {w, P,Q,R, S} are estimated on the basis of this training set.",4 Learning,[0],[0]
"Following standard practice, we optimize the parameter values in a maximum soft-margin framework.",4 Learning,[0],[0]
"That is, for the given sentence x̂ and the corresponding syntactic tree ˆysyn, we adjust parameter values to separate gold semantic parse and other incorrect alternatives:
∀zsem ∈ Z(x̂, ˆysyn) :",4 Learning,[0],[0]
"Ssem(x̂, ˆysyn, ˆzsem) ≥ Ssem(x̂, ˆysyn, zsem) + cost( ˆzsem, zsem) (4)
where Z(x̂, ˆysyn) represent the set of all possible semantic parses, and cost( ˆzsem, zsem) is a non-negative function representing the structural difference between ˆzsem and zsem.",4 Learning,[0],[0]
"The cost is zero when zsem = ˆzsem, otherwise it becomes positive and therefore is the “margin” to separate the two parses.",4 Learning,[0],[0]
"Following previous work (Johansson, 2009; Martins and Almeida, 2014), this cost function is defined as the sum of arc errors – we add 1.0 for each false-positive arc, 2.0 for each false-negative arc (a missing arc) and 0.5 if the predicate-argument pair (p, a) is in both parses but the semantic role label r is incorrect.",4 Learning,[0],[0]
The parameters are updated successively after each training sentence.,4.1 Online Update,[0],[0]
Each update first checks whether the constraint (4) is violated.,4.1 Online Update,[0],[0]
"This requires “costaugmented decoding” to find the maximum violation with respect to the gold semantic parse:
˜zsem = arg max zsem Ssem(x̂, ˆysyn, zsem)
+ cost( ˆzsem, zsem)
",4.1 Online Update,[0],[0]
"When the constraint (4) is violated (i.e. ˜zsem 6= ˆzsem), we seek a parameter update ∆θ to fix this violation.",4.1 Online Update,[0],[0]
"In other words, we define the hinge loss for this example as follows,
loss(θ) = max{ 0, Ssem(x̂, ˆysyn, ˜zsem) + cost( ˆzsem, ˜zsem)− Ssem(x̂, ˆysyn, ˆzsem) }
and we revise the parameter values to minimize this loss function.
",4.1 Online Update,[0],[0]
"Since this loss function is neither linear nor convex with respect to the parameters θ (more precisely the low-rank component matrices P , Q, R and S), we can use the same alternating passive-aggressive (PA) update strategy in our previous work (Lei et al., 2014) to update one parameter matrix at one time while fixing the other matrices.",4.1 Online Update,[0],[0]
"However, as we demonstrated later, modifying the passiveaggressive algorithm slightly can give us a joint update over all components in θ.",4.1 Online Update,[0],[0]
Our preliminary experiment shows this modified version achieves better results compared to the alternating PA.,4.1 Online Update,[0],[0]
"The original passive-aggressive parameter update ∆θ is derived for a linear, convex loss function by solving a quadatic optimization problem.",4.2 Joint PA Update for Tensor,[0],[0]
"Although our scoring function Ssem(·) is not linear, we can simply approximate it with its first-order Taylor expansion:
S(x,y, z; θ + ∆θ)",4.2 Joint PA Update for Tensor,[0],[0]
"≈ S(x,y, z; θ) + dS dθ ·∆θ
",4.2 Joint PA Update for Tensor,[0],[0]
"In fact, by plugging this into the hinge loss function and the quadratic optimization problem, we get a joint closed-form update which can be simply described as,
∆θ = max { C,
loss(θ) ‖gθ‖2
} gθ
where
gθ = dS dθ (x̂, ˆysyn, ˆzsem)− dS dθ (x̂, ˆysyn, ˜zsem),
and C is a regularization hyper-parameter controlling the maximum step size of each update.",4.2 Joint PA Update for Tensor,[0],[0]
"Note that θ is the set of all parameters, the update jointly adjusts all low-rank matrices and the traditional weight vector.",4.2 Joint PA Update for Tensor,[0],[0]
The PA update is “adaptive” in the sense that its step size is propotional to the loss(θ) of the current training sample.,4.2 Joint PA Update for Tensor,[0],[0]
Therefore the step size is adaptively decreased as the model fits the training data.,4.2 Joint PA Update for Tensor,[0],[0]
"Since the scoring and loss function with high-order tensor components is highly non-convex, our model
performance can be impacted by the initialization of the matrices P , Q, R and S.",4.3 Tensor Initialization,[0],[0]
"In addition to intializing these low-rank components randomly, we also experiment with a strategy to provide a good guess of the low-rank tensor.
",4.3 Tensor Initialization,[0],[0]
"First, note that the traditional manually-selected feature set (i.e., φ(p, a, r) in our notation) is an expressive and informative subset of the huge feature expansion covered in the feature tensor.",4.3 Tensor Initialization,[0],[0]
We can train our model using only the manual feature set and then use the corresponding feature weights w to intialize the tensor.,4.3 Tensor Initialization,[0],[0]
"Specifically, we create a sparse tensor T ∈ Rn×n×m×l by putting each parameter weight in w into its corresponding entry in T .",4.3 Tensor Initialization,[0],[0]
"We then try to find a low-rank approximation of sparse tensor T by approximately minimizing the squared error:
min P,Q,R,S
‖T − ∑
i
P (i)⊗Q(i)⊗R(i)⊗ S(i)‖22
In the low-rank dependency parsing work (Lei et al., 2014), this is achieved by unfolding the sparse tensor T into a n× nml matrix and taking the SVD to get the top low-rank components.",4.3 Tensor Initialization,[0],[0]
"Unfortunately this strategy does not apply in our case (and other high-order tensor cases) because even the number of columns in the unfolded matrix is huge, nml > 1011, and simply taking the SVD would fail because of memory limits.
",4.3 Tensor Initialization,[0],[0]
"Instead, we adopt the generalized high-order power method, a.k.a. power iteration (De Lathauwer et al., 1995), to incrementally obtain the most important rank-1 component one-by-one – P (i), Q(i), R(i) and S(i) for each i = 1..",4.3 Tensor Initialization,[0],[0]
k.,4.3 Tensor Initialization,[0],[0]
This method is a very simple iterative algorithm and is used to find the largest eigenvalues and eigenvectors (or singular values and vectors in SVD case) of a matrix.,4.3 Tensor Initialization,[0],[0]
Its generalization directly applies to our high-order tensor case.,4.3 Tensor Initialization,[0],[0]
"Decoding Following Lluı́s et al. (2013), the decoding of SRL is formulated as a bipartite maximum assignment problem, where we assign arguments to semantic roles for each predicate.",5 Implementation Details,[0],[0]
"We use the maximum weighted assignment algorithm (Kuhn, 1955).",5 Implementation Details,[0],[0]
"For syntactic dependency parsing, we employ the randomized hill-climbing algorithm from our previous work (Zhang et al., 2014).
",5 Implementation Details,[0],[0]
Features Table 1 summarizes the first-order feature templates.,5 Implementation Details,[0],[0]
"These features are mainly drawn from previous work (Johansson, 2009).",5 Implementation Details,[0],[0]
"In addition, we extend each template with the argument label.
",5 Implementation Details,[0],[0]
Table 2 summarizes the atomic features used in φ(p) and φ(a) for the tensor component.,5 Implementation Details,[0],[0]
"For each predicate or argument, the feature vector includes its word form and POS tag, as well as the POS tags of the context words.",5 Implementation Details,[0],[0]
We also add unsupervised word embeddings learned on raw corpus.4,5 Implementation Details,[0],[0]
"For atomic vectors φ(path) and φ(r) representing the path and the semantic role label, we use the indicator feature and a bias term.",5 Implementation Details,[0],[0]
"Dataset We evaluate our model on the English dataset and other 4 datasets in the CoNLL-2009 shared task (Surdeanu et al., 2008).",6 Experimental Setup,[0],[0]
"We use the
4https://github.com/wolet/ sprml13-word-embeddings
official split for training, development and testing.",6 Experimental Setup,[0],[0]
"For English, the data is mainly drawn from the Wall Street Journal.",6 Experimental Setup,[0],[0]
"In addition, a subset of the Brown corpus is used as the secondary out-of-domain test set, in order to evaluate how well the model generalizes to a different domain.",6 Experimental Setup,[0],[0]
"Following the official practice, we use predicted POS tags, lemmas and morphological analysis provided in the dataset across all our experiments.",6 Experimental Setup,[0],[0]
The predicates in each sentence are also given during both training and testing.,6 Experimental Setup,[0],[0]
"However, we neither predict nor use the sense for each predicate.
",6 Experimental Setup,[0],[0]
"Systems for Comparisons We compare against three systems that achieve the top average performance in the joint syntactic and semantic parsing track of the CoNLL-2009 shared task (Che et al., 2009; Zhao et al., 2009a; Gesmundo et al., 2009).",6 Experimental Setup,[0],[0]
All approaches extensively explored rich features for the SRL task.,6 Experimental Setup,[0],[0]
"We also compare with the stateof-the-art parser (Björkelund et al., 2010) for English, an improved version of systems participated in CoNLL-2009.",6 Experimental Setup,[0],[0]
This system combines the pipeline of dependency parser and semantic role labeler with a global reranker.,6 Experimental Setup,[0],[0]
"Finally, we compare with the recent approach which employs distributional word representations for SRL (Roth and Woodsend, 2014).",6 Experimental Setup,[0],[0]
"We directly obtain the outputs of all these systems from the CoNLL-2009 website5 or the authors.
",6 Experimental Setup,[0],[0]
Model Variants,6 Experimental Setup,[0],[0]
"Our full model utilizes 4-way tensor component and a standard feature set
5http://ufal.mff.cuni.cz/conll2009-st/ results/results.php
from (Johansson, 2009).",6 Experimental Setup,[0],[0]
"We also compare against our model without the tensor component, as well as a variant with a 3-way tensor by combining the path and semantic role label parts into a single mode (dimension).
",6 Experimental Setup,[0],[0]
"Evaluation Measures Following standard practice in the SRL evaluation, we measure the performance using labeled F-score.",6 Experimental Setup,[0],[0]
"To this end, we apply the evaluation script provided on the official website.6 The standard evaluation script considers the predicate sense prediction as a special kind of semantic label.7 Since we are neither predicting nor using the predicate sense information, we exclude this information in most of the evaluation.",6 Experimental Setup,[0],[0]
"In addition, we combine the predicate sense classification output of (Björkelund et al., 2010) with our semantic role labeling output, to provide results directly comparable to previous reported numbers.
",6 Experimental Setup,[0],[0]
"Experimental Details Across all experiments, we fix the rank of the tensor to 50 and train our model for a maximum of 20 epochs.",6 Experimental Setup,[0],[0]
"Following common practice, we average parameters over all iterations.",6 Experimental Setup,[0],[0]
"For each experimental setting, we tune the hyper-parameter γ ∈ {0.3, 0.5, 0.7, 0.9} and C ∈ {0.01, 0.1, 1} on the development set and apply the best model on the test set.",6 Experimental Setup,[0],[0]
Each model is evaluated on the development set after every epoch to pick the the best number of training epoch.,6 Experimental Setup,[0],[0]
"For the experiments with random initialization on the tensor component, the vectors are initialized as random unit vectors.",6 Experimental Setup,[0],[0]
"We combine our SRL model with our syntactic dependency parser, RBGParser v1.1 (Lei et al., 2014), for joint syntactic and semantic parsing.",6 Experimental Setup,[0],[0]
"The labeled attachment score (LAS) of RBGParser is 90.4 on English, when we train the “standard” model type using the unsupervised word vectors.",6 Experimental Setup,[0],[0]
We first report the performance of our methods and other state-of-the-art SRL systems on English datasets (See Table 3).,7 Results,[0],[0]
"We single out performance
6http://ufal.mff.cuni.cz/conll2009-st/ scorer.html
7Note that the original script includes such prediction in the F-score calculation, although the predicate sense is typically predicted in a separate step before semantic label classification.
on English corpora because these datasets are most commonly used for system evalutation.",7 Results,[0],[0]
"As a single system without reranking, our model outperforms the five top performing systems (second block in Table 3) on both in-domain and out-of-domain datasets.",7 Results,[0],[0]
"The improvement from the F-score of 82.08% to our result 82.51% on the WSJ in-domain test set is significant with p < 0.05, which is computed using a randomized test tool8 based on Yeh (2000).",7 Results,[0],[0]
"For comparison purposes, we also report F-score performance when predicate senses are included in evaluation.",7 Results,[0],[0]
"The relative performance between the systems is consistent independent of whether the predicate senses are included or excluded.
",7 Results,[0],[0]
Table 4 shows the results of our system on other languages in the CoNLL-2009 shared task.,7 Results,[0],[0]
"Out of five languages, our model rivals the best performing system on three languages, achieving statistically significant gains on English and Chinese.",7 Results,[0],[0]
Note that our model uses the same feature configuration for all the languages.,7 Results,[0],[0]
"In contrast, Zhao et al. (2009b) rely on language-specific configurations obtained via “huge feature engineering” (as noted by the authors).
",7 Results,[0],[0]
"Results in Table 3 and 4 also highlight the con-
8http://www.nlpado.de/˜sebastian/ software/sigf.shtml
tribution of the tensor to the model performance, which is consistent across languages.",7 Results,[0],[0]
"Without the tensor component, our system trails the top two performing systems.",7 Results,[0],[0]
"However, adding the tensor component provides on average 2.1% absolute gain, resulting in competitive performance.",7 Results,[0],[0]
"The mode of the tensor also contributes to the performance – the 4-way tensor model performs better than the 3-way counterpart, demonstrating the importance of modeling the interactions between dependency paths and semantic role labels.
",7 Results,[0],[0]
Table 5 shows the impact of initialization on the performance of the tensor-based model.,7 Results,[0],[0]
"The initialization based on the power method yields superior results compared to random initialization, for both
3-way and 4-way tensors.",7 Results,[0],[0]
"However, random initialization still delivers reasonable performance, outperforming the tensor-free model by more than 1% in F-score.
",7 Results,[0],[0]
"Finally, we compare our tensor-based approach against a simpler model that captures interactions between predicate, argument and syntactic path using word embeddings (Roth and Woodsend, 2014).",7 Results,[0],[0]
Table 6 demonstrates that modeling feature interactions using tensor yields higher gains than using word embeddings alone.,7 Results,[0],[0]
"For instance, the highest gain achieved by Roth and Woodsend (2014) when the embeddings of the arguments are averaged is 0.5%, compared to 1.6% obtained by our model.",7 Results,[0],[0]
In this paper we introduce a tensor-based approach to SRL that induces a compact feature representation for words and their relations.,8 Conclusions,[0],[0]
"In this sense, our dimensionality reduction method provides a clear alternative to a traditional feature engineering approach used in SRL.",8 Conclusions,[0],[0]
"Augmenting a simple, yet competitive SRL model with the tensor component yields significant performance gains.",8 Conclusions,[0],[0]
We demonstrate that our full model outperforms the best performing systems on the CoNLL-2009 shared task.,8 Conclusions,[0],[0]
The authors acknowledge the support of the MURI program (W911NF-10-1-0533) and the DARPA BOLT program.,Acknowledgments,[0],[0]
This research is developed in a collaboration of MIT with the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Interactive sYstems for Answer Search (IYAS) project.,Acknowledgments,[0],[0]
We are grateful to Anders Bjökelund and Michael Roth for providing the outputs of their systems.,Acknowledgments,[0],[0]
"We thank Yu Xin, Tommi Jaakkola, the MIT NLP group and the ACL reviewers for their comments.",Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.",Acknowledgments,[0],[0]
This paper introduces a tensor-based approach to semantic role labeling (SRL).,abstractText,[0],[0]
"The motivation behind the approach is to automatically induce a compact feature representation for words and their relations, tailoring them to the task.",abstractText,[0],[0]
"In this sense, our dimensionality reduction method provides a clear alternative to the traditional feature engineering approach used in SRL.",abstractText,[0],[0]
"To capture meaningful interactions between the argument, predicate, their syntactic path and the corresponding role label, we compress each feature representation first to a lower dimensional space prior to assessing their interactions.",abstractText,[0],[0]
This corresponds to using an overall cross-product feature representation and maintaining associated parameters as a four-way low-rank tensor.,abstractText,[0],[0]
The tensor parameters are optimized for the SRL performance using standard online algorithms.,abstractText,[0],[0]
Our tensor-based approach rivals the best performing system on the CoNLL-2009 shared task.,abstractText,[0],[0]
"In addition, we demonstrate that adding the representation tensor to a competitive tensorfree model yields 2% absolute increase in Fscore.1",abstractText,[0],[0]
High-Order Low-Rank Tensors for Semantic Role Labeling,title,[0],[0]
"Deep neural networks (NNs) have achieved impressive performance in a wide variety of tasks in recent years, however, success is generally in terms of aggregated accuracy metrics.",1. Introduction,[0],[0]
"For many real-world applications, it is not enough that on average a model performs well, rather the uncertainty of each prediction must also be quantified.",1. Introduction,[0],[0]
"This can be particularly important where there is a large downside to an incorrect prediction: Examples can be found in prognostics, manufacturing, finance, weather, traffic and energy networks.",1. Introduction,[0],[0]
"There is therefore interest in how NNs can be modified to meet this requirement (Krzywinski & Altman, 2013; Gal, 2016).
",1. Introduction,[0],[0]
In this work the output of prediction intervals (PIs) in regression tasks is considered.,1. Introduction,[0],[0]
"Whilst NNs by default output point estimates, PIs directly communicate uncertainty, offering a lower and upper bound for a prediction and assurance that, with some high probability (e.g. 95% or 99%), the realised data point will fall between these bounds.",1. Introduction,[0],[0]
"Having this information allows for better-informed decisions.
1Department of Engineering, University of Cambridge, UK 2Alan Turing Institute, UK.",1. Introduction,[0],[0]
"Correspondence to: Tim Pearce <tp424@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"As an example, a point estimate stating that a machine will fail in 60 days may not be sufficient to schedule a repair, however given a PI of 45-65 days with 99% probability, timing of a repair is easily scheduled.
",1. Introduction,[0],[0]
"A diverse set of approaches have been developed to quantify NN uncertainty, ranging from fully Bayesian NNs (BNNs) (MacKay, 1992), to interpreting dropout as performing variational inference (Gal & Ghahramani, 2015).",1. Introduction,[0],[0]
"These require either high computational demands or strong assumptions.
",1. Introduction,[0],[0]
In this work we formulate PI output as a constrained optimisation problem.,1. Introduction,[0],[0]
"It is self evident that high-quality PIs should be as narrow as possible, whilst capturing some specified proportion of data points (hereafter referred to as the HQ principle).",1. Introduction,[0],[0]
"Indeed it is through these metrics that PI quality is often assessed (Papadopoulos et al., 2000; Khosravi et al., 2011b; Galván",1. Introduction,[0],[0]
"et al., 2017).",1. Introduction,[0],[0]
"We show how a loss function can be derived directly from this HQ principle, and used in an ensemble to produce PIs accounting for both model uncertainty and data noise variance.",1. Introduction,[0],[0]
"The key advantages of the method are its intuitive objective, low computational demand, robustness to outliers, and lack of distributional assumption.
",1. Introduction,[0],[0]
"Notably we build on the work of Khosravi et al. (2011a) who developed the Lower Upper Bound Estimation (LUBE) method, incorporating the HQ principle directly into the NN loss function for the first time.",1. Introduction,[0],[0]
"LUBE is gaining popularity in several communities, for example in the forecasting of energy demand and wind speed (section 2).",1. Introduction,[0],[0]
"However, we have identified several limitations of its current form.
",1. Introduction,[0],[0]
"• Gradient Descent - It was stated that the method was incompatible with gradient descent (GD), a belief carried forward, unchallenged, in all subsequent work (section 2).",1. Introduction,[0],[0]
"Implementations therefore require nongradient based methods for training, such as Simulated Annealing (SA) and Particle Swarm Optimisation (PSO).",1. Introduction,[0],[0]
"This is inconvenient since GD has become the standard training method for NNs (Goodfellow et al., 2016), used by all modern NN APIs.
",1. Introduction,[0],[0]
• Loss Form - Its current form suffers from several problems.,1. Introduction,[0],[0]
"The function is at a global minimum when all
PIs are reduced to zero.",1. Introduction,[0],[0]
"It was also designed through qualitative assessment of the desired behaviour rather than on a statistical basis.
",1. Introduction,[0],[0]
• Model Uncertainty - LUBE accounts only for datanoise variance and not model uncertainty (section 2.1).,1. Introduction,[0],[0]
"This is an oversimplification (Heskes, 1996), implicitly assuming that training data fully populates the input space, which is seldom the case.
",1. Introduction,[0],[0]
"In this work we develop a model addressing each of these issues - henceforth referred to as the quality-driven PI method (QD), and QD-Ens when explicitly referring to the ensembled form.
",1. Introduction,[0],[0]
"We link early literature on PIs for NNs (Tibshirani, 1996; Heskes, 1996; Papadopoulos et al., 2000; Khosravi et al., 2011a), with recent work on uncertainty in deep learning (Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Lakshminarayanan et al., 2017) - areas which have remained surprisingly distinct.",1. Introduction,[0],[0]
"We achieve this by following the same experimental procedure of recent work, assessing performance across ten benchmark regression datasets.",1. Introduction,[0],[0]
"We compare QD’s performance with the current best performing model, originally named Deep Ensembles (Lakshminarayanan et al., 2017), here referred to as MVE-Ens.",1. Introduction,[0],[0]
"We show that QD outperforms in PI quality metrics, achieving closer to the desired coverage proportion, and reducing average PI width by around 10%.",1. Introduction,[0],[0]
In this section we consider methods to quantify uncertainty in regression with NNs.,2. Related Work,[0],[0]
"Three review papers catalogued early work (Tibshirani, 1996; Papadopoulos et al., 2000; Khosravi et al., 2011b), the latter two specifically considering PIs.",2. Related Work,[0],[0]
"Three primary methods were presented:
•",2. Related Work,[0],[0]
"The Delta method adopts theory for building confidence intervals (CIs) used by general non-linear regression models, estimating model uncertainty.",2. Related Work,[0],[0]
"It is computationally demanding as it requires use of the Hessian matrix.
",2. Related Work,[0],[0]
"• Mean Variance Estimation (MVE) (Nix & Weigend, 1994) uses a NN with two output nodes - one representing the mean and the other the variance of a normal distribution, allowing estimation of data noise variance.",2. Related Work,[0],[0]
"The loss function used is the Negative Log Likelihood (NLL) of the predicted distribution given the data.
",2. Related Work,[0],[0]
"• The Bootstrap (Heskes, 1996) estimates model uncertainty.",2. Related Work,[0],[0]
It trains multiple NNs with different parameter initialisations on different resampled versions of the training dataset.,2. Related Work,[0],[0]
"It is easily combined with MVE to estimate total variance.
",2. Related Work,[0],[0]
"In addition, BNNs treat model parmeters as distributions rather than point estimates (MacKay, 1992), and hence can predict distributions rather than point estimates.",2. Related Work,[0],[0]
Their drawback is that the computational cost of running MCMC algorithms can be prohibitive.,2. Related Work,[0],[0]
"Recent work has focused on addressing this (Graves, 2011; Hernández-Lobato & Adams, 2015; Blundell et al., 2015), notably NNs with dropout may be interpreted as performing variational inference (Gal & Ghahramani, 2015).
",2. Related Work,[0],[0]
"Lakshminarayanan et al. (2017) produced a modernisation of Heskes’ work (1996), ensembling individual MVE NNs (without resampling the dataset - section 4), and including adversarial training examples.",2. Related Work,[0],[0]
We henceforth refer to this as MVE Ensemble (MVE-Ens).,2. Related Work,[0],[0]
"Another MVE extention encourages high uncertainty in data regions not observed by augmenting training data with synthetic ‘out-of-distribution’ samples of high variance (Malinin et al., 2017).
",2. Related Work,[0],[0]
"Many of these modern works complied with an experimental protocol laid out by Hernandez-Lobato & Adams (2015), assessing NLL & RMSE across ten benchmark regression datasets, with MVE-Ens the current best performer.",2. Related Work,[0],[0]
"By contrast, the PI literature reports metrics around coverage proportion and PI width.
LUBE (Khosravi et al., 2011a) was developed on the HQ principle.",2. Related Work,[0],[0]
"Originally it was proposed with SA as the training method, and much effort has gone toward trialling it with various non-gradient based training methods including Genetic Algorithms (Ak et al., 2013b), Gravitational Search Algorithms (Lian et al., 2016), PSO (Galván",2. Related Work,[0],[0]
"et al., 2017; Wang et al., 2017), Extreme Learning Machines (Sun et al., 2017), and Artificial Bee Colony Algorithms (Shen et al., 2018).",2. Related Work,[0],[0]
"Multi-objective optimisation has been found useful in considering the tradeoff between PI width and coverage (Galván et al., 2017; Shen et al., 2018).
",2. Related Work,[0],[0]
"LUBE has been used in a plethora of application-focused work: Particularly in energy load (Pinson & Kariniotakis, 2013; Quan et al., 2014) and wind speed forecasting (Wang et al., 2017; Ak et al., 2013b), but also prediction of landslide displacement (Lian et al., 2016), gas flow (Sun et al., 2017), solar energy (Galván et al., 2017), condition-based maintenance (Ak et al., 2013a), and others.",2. Related Work,[0],[0]
"All work has used LUBE as a single NN, making no attempt to account for model uncertainty (section 2.1).",2. Related Work,[0],[0]
"This section describes uncertainty in regression, it is an agglomeration of several prominent works (Tibshirani, 1996; Heskes, 1996; Papadopoulos et al., 2000; Shafer & Vovk, 2008; Mazloumi et al., 2011; Khosravi et al., 2011b; Lakshminarayanan et al., 2017), each of who presented similar concepts but under different guises and terminology.",2.1. The Uncertainty Framework,[0],[0]
"We
attempt to reconcile them here.
",2.1. The Uncertainty Framework,[0],[0]
"The philosophy behind regression is that some data generating function, f(x), exists, combined with additive noise, to produce observable target values y,
y = f(x) + .",2.1. The Uncertainty Framework,[0],[0]
"(1)
The component is termed irreducible noise or data noise.",2.1. The Uncertainty Framework,[0],[0]
"It may exist due to exclusion of (minor) explanatory variables in x, or due to an inherently stochastic process.",2.1. The Uncertainty Framework,[0],[0]
"Some models, for example the Delta method, assume is constant across the input space (homoskedastic), others allow for it to vary (heteroskedastic), for example MVE.
",2.1. The Uncertainty Framework,[0],[0]
"Generally the goal of regression is to produce an estimate f̂(x), which allows prediction of point estimates ( is assumed to have mean zero).",2.1. The Uncertainty Framework,[0],[0]
"However, when estimating the uncertainty of y, additional terms must be estimated.",2.1. The Uncertainty Framework,[0],[0]
"Given that both terms of eq. (1) have associated sources of uncertainty, and assuming they are independent, the total variance of observations is given by,
σ2y = σ 2 model + σ 2 noise, (2)
with σ2model termed model uncertainty or epistemic uncertainty - uncertainty in f̂(x) - and σ2noise irreducible variance, data noise variance, or aleatoric uncertainty.
",2.1. The Uncertainty Framework,[0],[0]
It is worth here distinguishing CIs from PIs.,2.1. The Uncertainty Framework,[0],[0]
"CIs consider the distribution Pr(f(x)|f̂(x)), and hence only require estimation of σ2model, whilst PIs consider Pr(y|f̂(x)) and must also consider σ2noise.",2.1. The Uncertainty Framework,[0],[0]
"PIs are necessarily wider than CIs.
",2.1. The Uncertainty Framework,[0],[0]
"Model uncertainty can be attributed to several factors.
",2.1. The Uncertainty Framework,[0],[0]
"• Model misspecification or bias - How closely f̂(x) is able to approximate f(x), assuming ideal parameters and plentiful training data.
",2.1. The Uncertainty Framework,[0],[0]
• Training data uncertainty or variance - Training data is a sample from an input distribution.,2.1. The Uncertainty Framework,[0],[0]
"There is uncertainty over how representative the sample is, and how sensitive the model is to other samples.
",2.1. The Uncertainty Framework,[0],[0]
•,2.1. The Uncertainty Framework,[0],[0]
"Parameter uncertainty - Uncertainty exists around the optimum parameters of the model, increasing in regions sparsely represented in the training data.
",2.1. The Uncertainty Framework,[0],[0]
Different model types have different weightings for each of these factors (bias-variance trade-off ).,2.1. The Uncertainty Framework,[0],[0]
"Provided the number of hidden neurons is large relative to the complexity of f(x), NNs are considered to have low bias and high variance.",2.1. The Uncertainty Framework,[0],[0]
"Work on uncertainty in NNs therefore generally ignores model misspecification, and only estimates training data uncertainty and parameter uncertainty (Heskes, 1996).
",2.1. The Uncertainty Framework,[0],[0]
"To construct PIs, σ2y must be estimated at each prediction point.",2.1. The Uncertainty Framework,[0],[0]
"In regions of the input space with more data, σ2model decreases, and σ2noise may become the larger component
1.",2.1. The Uncertainty Framework,[0],[0]
"In regions of the input space with little data, σ2model grows.
",2.1. The Uncertainty Framework,[0],[0]
"Lakshminarayanan et al. (2017) recognise this in more intuitive terms - that two sources of uncertainty exist.
1.",2.1. The Uncertainty Framework,[0],[0]
"Calibration - Data noise variance in regions which are well represented by the training data.
",2.1. The Uncertainty Framework,[0],[0]
2.,2.1. The Uncertainty Framework,[0],[0]
Out-of-distribution - Uniqueness of an input2 - inputs less similar to training data should lead to less certain estimates.,2.1. The Uncertainty Framework,[0],[0]
We now derive a loss function based on the HQ principle.,3.1. Derivation,[0],[0]
"Let the set of input covariates and target observations be X and y, for n data points, and with xi ∈",3.1. Derivation,[0],[0]
"RD denoting the ith D dimensional input corresponding to yi, for 1 ≤",3.1. Derivation,[0],[0]
i ≤ n.,3.1. Derivation,[0],[0]
"The predicted lower and upper PI bounds are ŷL, ŷU. A PI should capture some desired proportion of the observations, (1− α), common choices of α being 0.01 or 0.05,
",3.1. Derivation,[0],[0]
Pr(ŷLi ≤ yi ≤ ŷUi) ≥ (1− α).,3.1. Derivation,[0],[0]
"(3)
A vector, k, of length n represents whether each data point has been captured by the estimated PIs, with each element ki ∈ {0, 1} given by,
ki = { 1, if yLi ≤ yi ≤ yUi 0, else.
(4)
We define the total number of data points captured as c,
c",3.1. Derivation,[0],[0]
:= n∑ i=1,3.1. Derivation,[0],[0]
ki.,3.1. Derivation,[0],[0]
"(5)
Let Prediction Interval Coverage Probability (PICP ) and Mean Prediction Interval Width (MPIW ) be defined as,
PICP := c
n , (6)
",3.1. Derivation,[0],[0]
"MPIW := 1
n n∑ i=1",3.1. Derivation,[0],[0]
ŷUi,3.1. Derivation,[0],[0]
− ŷLi.,3.1. Derivation,[0],[0]
"(7)
1At the same time, σ2noise may be estimated with more certainty, although uncertainty of this value itself is not generally considered.
2Conformal prediction provides a framework to assess this.
",3.1. Derivation,[0],[0]
"According to the HQ principle, PIs should minimise MPIW subject to PICP ≥ (1 − α).",3.1. Derivation,[0],[0]
"To minimise MPIW , eq. (7) could simply be included in the loss function, however PIs that fail to capture their data point should not be encouraged to shrink further.",3.1. Derivation,[0],[0]
We therefore introduce captured MPIW as the MPIW of only those points for which ŷL ≤ y ≤,3.1. Derivation,[0],[0]
"ŷL holds,
MPIWcapt.",3.1. Derivation,[0],[0]
":= 1
c n∑ i=1",3.1. Derivation,[0],[0]
(ŷUi − ŷLi) · ki.,3.1. Derivation,[0],[0]
"(8)
Regarding PICP , we take a likelihood-based approach, seeking NN parameters, θ, that maximise,
Lθ := L(θ|k, α).",3.1. Derivation,[0],[0]
"(9)
Recognising that each element, ki, is a binary variable taking 1 with probability (1−α), we represent it as a Bernoulli random variable (one per prediction), ki ∼ Bernoulli(1− α).",3.1. Derivation,[0],[0]
We further assume that each ki is iid.,3.1. Derivation,[0],[0]
"This independence assumption may not hold for data points clustered close together, however we believe holds sufficiently for a randomly sampled subset of all data points, as used in mini-batches for GD.",3.1. Derivation,[0],[0]
"This iid assumption allows the total number of captured points, c, to be represented by a binomial distribution, c ∼ Binomial(n, (1− α)).",3.1. Derivation,[0],[0]
"Substituting in the pmf,
",3.1. Derivation,[0],[0]
"Lθ = ( n
c
) (1− α)cαn−c.",3.1. Derivation,[0],[0]
"(10)
The factorials in the binomial coefficient make computation inconvenient.",3.1. Derivation,[0],[0]
However using the central limit theorem (specifically the de Moivre-Laplace theorem) it can further be approximated by a normal distribution.,3.1. Derivation,[0],[0]
"For large n,
Binomial(n, (1− α))",3.1. Derivation,[0],[0]
"≈ N ( n(1− α), nα(1− α) ) (11)
= 1√
2πnα(1− α) exp− (c− n(1− α)) 2 2nα(1− α) .",3.1. Derivation,[0],[0]
"(12)
We consider this a mild assumption provided a mini-batch size of reasonable number, say > 50, is used.
",3.1. Derivation,[0],[0]
"It is common to minimise the NLL rather than maximise the likelihood, this simplifies eq.",3.1. Derivation,[0],[0]
"(12) to,
− logLθ ∝ (n(1− α)− c)2
nα(1− α) (13)
= n
α(1− α) ((1− α)− PICP )2. (14)
Remembering that a penalty should only occur in the case where PICP < (1− α) results in a one-sided loss.",3.1. Derivation,[0],[0]
"Combining with eq. (8) and adding a Lagrangian, λ, controlling the importance of width vs. coverage gives a new loss,
LossQD =
MPIWcapt. + λ n
α(1− α) max(0, (1− α)− PICP )2.
(15)",3.1. Derivation,[0],[0]
"The derived loss function in eq. (15) may be compared to the LUBE loss (Khosravi et al., 2011a),
LossLUBE =
MPIW
r
( 1 + exp ( λ max(0, (1− α)− PICP ) )) ,
(16)
where r = max(y)",3.2. Comparison to LUBE,[0],[0]
"− min(y), is the range of the target variable.
",3.2. Comparison to LUBE,[0],[0]
"Whilst still recognisable as having the same objective, the differences are significant, and are summarised as follows.
",3.2. Comparison to LUBE,[0],[0]
"• The inclusion of n intuitively makes sense since a larger sample size provides more confidence in the value of PICP , and hence a larger loss should be incurred.",3.2. Comparison to LUBE,[0],[0]
Similar arguments follow for α.,3.2. Comparison to LUBE,[0],[0]
"They remove the need to adjust λ based on batch size and target coverage.
",3.2. Comparison to LUBE,[0],[0]
• A squared term has replaced the exponential.,3.2. Comparison to LUBE,[0],[0]
"Whilst the RHS for both is minimised when PICP ≥ (1 − α), the squared term was derived based on likelihood whilst the exponential term was selected qualitatively.
",3.2. Comparison to LUBE,[0],[0]
• MPIW now has an additive rather than multiplicative effect.,3.2. Comparison to LUBE,[0],[0]
Multiplying has the attractive property of ensuring both terms are of the same magnitude.,3.2. Comparison to LUBE,[0],[0]
However it also means that a global minimum is found when all PIs are of zero width.,3.2. Comparison to LUBE,[0],[0]
"We found in practise that NNs occasionally did produce this undesirable solution.
",3.2. Comparison to LUBE,[0],[0]
• MPIW is no longer normalised by the range of y. Data for a NN should already be normalised during preprocessing.,3.2. Comparison to LUBE,[0],[0]
Further normalisation is therefore redundant.,3.2. Comparison to LUBE,[0],[0]
"It also merely scales the loss by a constant, having no overall effect on the optimisation.
",3.2. Comparison to LUBE,[0],[0]
• MPIWcapt. is used rather than MPIW .,3.2. Comparison to LUBE,[0],[0]
As discussed in section 3.1 this avoids the NN benefiting by further reduction of PI widths for missed data.,3.2. Comparison to LUBE,[0],[0]
"It was originally believed that the LUBE loss function was, “nonlinear, complex, and non-differentiable... gradient descent-based algorithms cannot be applied for its minimization” (Khosravi et al., 2011a).",3.3. Training QD with Gradient Descent,[0],[0]
"This belief has been carried forward, unchallenged, in all subsequent work - see section 2 for numerous examples.",3.3. Training QD with Gradient Descent,[0],[0]
"It is inconvenient since GD is the standard method for training NNs, so implementations require extra coding effort.
",3.3. Training QD with Gradient Descent,[0],[0]
"Regarding the quoted justification, most standard loss functions are nonlinear - e.g. L2 errors - and whilst the LUBE loss function is complex, this does not affect its compatibility with GD3.",3.3. Training QD with Gradient Descent,[0],[0]
The non-differentiability comment is partially valid.,3.3. Training QD with Gradient Descent,[0],[0]
"Because the loss function requires the use of step functions, it is not differentiable everywhere.",3.3. Training QD with Gradient Descent,[0],[0]
"But this is not an unsurmountable problem: ReLUs are a common choice of activation function in modern NNs, despite not being differentiable when the input is exactly zero4.",3.3. Training QD with Gradient Descent,[0],[0]
"LossQD can be directly implemented as shown in Algorithm 1 (LossH ), however it fails to converge to a minimum.",3.3.1. GD TOY EXAMPLE,[0],[0]
"We demonstrate why this is the case and how it can be remedied through a toy example.
",3.3.1. GD TOY EXAMPLE,[0],[0]
"Consider a NN as in figure 1 with one input and two output
3Modern NN APIs generally handle gradient computation automatically, through application of the chain rule to the predefined operations.",3.3.1. GD TOY EXAMPLE,[0],[0]
"Provided functions within the API library are used, gradient calculations are automatically handled.
",3.3.1. GD TOY EXAMPLE,[0],[0]
"4Software implementations return one of the derivatives either side of zero when the input corresponds to the undefined point rather than raising an error (Goodfellow et al., 2016).
",3.3.1. GD TOY EXAMPLE,[0],[0]
"neurons, linear activations and no bias.",3.3.1. GD TOY EXAMPLE,[0],[0]
"For purposes of clarity, one weight is fixed, w2 = 0.15, to create a one dimensional problem with a single trainable weight, w1.",3.3.1. GD TOY EXAMPLE,[0],[0]
"Given 10 data points evenly spaced at x = 1.0, and α = 0.2, the optimal value for ŷU (and therefore w1) is 0.9, which gives the lowest MPIW , subject to PICP ≥ 1−α = 0.8.
",3.3.1. GD TOY EXAMPLE,[0],[0]
LossQD is plotted in figure 2 (black line).,3.3.1. GD TOY EXAMPLE,[0],[0]
"Whilst the global minimum occurs at the desired point, this solution is not found through GD.",3.3.1. GD TOY EXAMPLE,[0],[0]
"Given the steepest descent weight update rule with some learning rate τ ,
w1,t+1 = w1,t − τ ∂LossQD ∂w1,t , (17)
the weight shrinks without converging.",3.3.1. GD TOY EXAMPLE,[0],[0]
"This is because the gradient, ∂Loss∂w1 , at any point is positive, except for the discontinuities which are never realised.
",3.3.1. GD TOY EXAMPLE,[0],[0]
"To remediate this, we introduce an approximation of the step function.",3.3.1. GD TOY EXAMPLE,[0],[0]
"The sigmoid function has been used in the past as a differentiable alternative (Yan et al., 2004).",3.3.1. GD TOY EXAMPLE,[0],[0]
"In eq. (4), k, the captured vector was defined.",3.3.1. GD TOY EXAMPLE,[0],[0]
"We redefine this as khard and introduce a relaxed version as follows,
ksoft =",3.3.1. GD TOY EXAMPLE,[0],[0]
σ(s(y,3.3.1. GD TOY EXAMPLE,[0],[0]
− ŷL)),3.3.1. GD TOY EXAMPLE,[0],[0]
"σ(s(ŷU − y)), (18)
where σ is the sigmoid function, and s > 0 is some softening factor.",3.3.1. GD TOY EXAMPLE,[0],[0]
"We further define PICPsoft and LossQD−soft by replacing khard with ksoft in equations (6) & (15) respectively - see also LossS in Algorithm 1.
",3.3.1. GD TOY EXAMPLE,[0],[0]
Figure 2 shows the result of using LossQD−soft (red & blue lines).,3.3.1. GD TOY EXAMPLE,[0],[0]
"By choosing an appropriate value for s, following the steepest gradient does lead to a minimum, making GD a
Algorithm 1 Construction of loss function using basic operations
Input: Target values, y, predictions of lower and upper bound, ŷL, ŷU, desired coverage, (1− α), and sigmoid softening factor, s, denotes the element-wise product.
",3.3.1. GD TOY EXAMPLE,[0],[0]
"# hard uses sign step fn, sign returns -1",3.3.1. GD TOY EXAMPLE,[0],[0]
"if -ve, +1 if +ve kHU = max(0, sign(ŷU − y))",3.3.1. GD TOY EXAMPLE,[0],[0]
"kHL = max(0, sign(y − ŷL)) kH",3.3.1. GD TOY EXAMPLE,[0],[0]
"= kHU kHL
# soft uses sigmoid fn kSU = sigmoid((ŷU − y) · s) kSL =",3.3.1. GD TOY EXAMPLE,[0],[0]
"sigmoid((y − ŷL) · s) kS = kSU kSL
MPIWc = reduce sum((ŷU − ŷL) kH)/reduce sum(kH)",3.3.1. GD TOY EXAMPLE,[0],[0]
PICPH = reduce mean(kH),3.3.1. GD TOY EXAMPLE,[0],[0]
"PICPS = reduce mean(kS) LossH = MPIWc+ λ · nα(1−α) ·max(0, (1− α)− PICPH) 2 LossS = MPIWc+ λ · nα(1−α) ·max(0, (1− α)− PICPS) 2
viable method.",3.3.1. GD TOY EXAMPLE,[0],[0]
"Setting s = 160 worked well in experiments in section 6, requiring no alteration across datasets.",3.3.1. GD TOY EXAMPLE,[0],[0]
"The original LUBE loss function, eq. (16), has been implemented with various evolutionary training schemes that do not require derivatives of the loss function.",3.4. Particle Swarm Optimisation,[0],[0]
"In order to test the efficacy of LossQD−soft with GD, we compared to an evolutionary-based training method (section 5.1).",3.4. Particle Swarm Optimisation,[0],[0]
"PSO (Kennedy & Eberhart, 1995) was chosen due to use in recent work with LUBE (Galván et al., 2017; Wang et al., 2017).",3.4. Particle Swarm Optimisation,[0],[0]
"We make the assumption that other evolutionary methods would offer similar performance (Jones, 2005).",3.4. Particle Swarm Optimisation,[0],[0]
See Kennedy & Eberhart (2001) for an introduction to PSO.,3.4. Particle Swarm Optimisation,[0],[0]
"In section 2.1, two components of uncertainty were defined; model uncertainty and data noise variance.",4. Ensembles to Estimate Model Uncertainty,[0],[0]
It appears that previous work assumed both were accounted for (section 2).,4. Ensembles to Estimate Model Uncertainty,[0],[0]
"In fact, LUBE & QD only estimate data noise variance, and there is a need to consider the uncertainty of these estimates themselves.",4. Ensembles to Estimate Model Uncertainty,[0],[0]
This becomes particularly important when new data is encountered.,4. Ensembles to Estimate Model Uncertainty,[0],[0]
"Consider a NN trained for the example in figure 1: Despite being capable of estimating the data noise variance at x = 1.0, if shown new data at x = 2.0 it would predict ŷU = 1.8, with little basis.
",4. Ensembles to Estimate Model Uncertainty,[0],[0]
Ensembling models provides a conceptually simple way to deal with this.,4. Ensembles to Estimate Model Uncertainty,[0],[0]
"Recall from section 2.1 that three sources of model uncertainty exist, and that the first, model misspecification, is assumed zero for NNs.",4. Ensembles to Estimate Model Uncertainty,[0],[0]
Parameter uncertainty can be measured by training multiple NNs with different parameter initialisations (parameter resampling).,4. Ensembles to Estimate Model Uncertainty,[0],[0]
"Training data uncertainty can be done similarly: Sub-sampling from the training set, and fitting a NN to each subset (bootstrap resampling).",4. Ensembles to Estimate Model Uncertainty,[0],[0]
"The resulting ensemble of NNs contains some diversity, and the variance of their predictions can be used as an estimate of model uncertainty.
",4. Ensembles to Estimate Model Uncertainty,[0],[0]
"Recent work reported that parameter resampling offered superior performance to both bootstrap resampling, and a combination of the two (Lee et al., 2015; Lakshminarayanan et al., 2017).",4. Ensembles to Estimate Model Uncertainty,[0],[0]
"No robust justification has been given for this.
",4. Ensembles to Estimate Model Uncertainty,[0],[0]
"Given an ensemble of m NNs trained with LossQD−soft, let ỹU, ỹL represent the ensemble’s upper and lower estimate of the PI.",4. Ensembles to Estimate Model Uncertainty,[0],[0]
"We calculate model uncertainty and hence the ensemble’s PIs as follows,
ȳUi = 1
m m∑ j=1 ŷUij , (19)
σ̂2model = σ 2 Ui =
1
m− 1 m∑ j=1 (ŷUij − ȳUi)2, (20)
ỹUi = ȳUi + 1.96σUi, (21)
where ŷUij represents the upper bound of the PI for data point i, for NN j. A similar procedure is followed for ỹLi, subtracting rather than adding 1.96σLi.",4. Ensembles to Estimate Model Uncertainty,[0],[0]
In this section behaviour of QD is qualitatively assessed on synthetic data.,5. Qualitative Experiments,[0],[0]
"Firstly, the GD method of training explained in section 3.3 is compared to PSO.",5. Qualitative Experiments,[0],[0]
"Next, the advantage of QD over MVE (section 2) in data with non-normal variance is shown.",5. Qualitative Experiments,[0],[0]
"Finally, the effectiveness of the ensembled QD approach at estimating model uncertainty is demonstrated.
",5. Qualitative Experiments,[0],[0]
"Supplementary material contains experimental details as well as numerical results for this synthetic data, covering a full permutation of loss functions and training methods [LUBE, QD, MVE]x[GD, PSO].",5. Qualitative Experiments,[0],[0]
"Comparison of evolutionary methods vs. GD for NN training is its own research topic, and as such analysis here is limited.",5.1. Training method: PSO vs. GD,[0],[0]
"Preliminary experiments showed that GD performed slightly better than PSO in terms of PICP and MPIW ,
producing smoother, tighter boundaries, both more consistently and with lower computational effort.",5.1. Training method: PSO vs. GD,[0],[0]
See figure 3 for a graphical comparison of typical PI boundaries.,5.1. Training method: PSO vs. GD,[0],[0]
"Data was generated by y = 0.3 sin(x) + 0.2 , with ∼ N(0, x4).",5.1. Training method: PSO vs. GD,[0],[0]
"Here, the advantage of a distribution-free loss function is demonstrated by comparing MVE, which assumes Gaussian data noise, to QD, which makes no such assumption, on two synthetic datasets.",5.2. Loss function: QD vs. MVE,[0],[0]
"The first was generated as in 5.1 with normal noise, the second with exponential noise, ∼ exp(1/x2).
",5.2. Loss function: QD vs. MVE,[0],[0]
"Figure 4 shows, unsurprisingly, that MVE outputs PIs very close to the ideal for normal noise, but struggles with exponential noise.",5.2. Loss function: QD vs. MVE,[0],[0]
"QD approximates both reasonably, though does not learn the boundaries well where data is sparse.",5.2. Loss function: QD vs. MVE,[0],[0]
"Though possible to alter MVE to assume an exponential distribution, this would require significant work.",5.2. Loss function: QD vs. MVE,[0],[0]
"With real data, the distribution would be unknown, and likely irregular, putting QD at an advantage.",5.2. Loss function: QD vs. MVE,[0],[0]
This experiment demonstrated the ability of ensembling to estimate model uncertainty.,5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
"Data was generated through y = 0.02x3 + 0.02 , with ∼ N(0, 32).",5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
Figure 5 shows ten individual QD PIs as well as the ensembled PIs.,5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
"The estimated model uncertainty, σ̂2model, calculated from eq. (20) is overlaid5.",5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
"Whilst it is difficult to reason about the correctness of the absolute value, its behaviour agrees with the intuition that uncertainty should increase in regions of the input space that are not represented in the training set, here x ∈",5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
"[−1, 1], and x > 4, x < −4.",5.3. Model Uncertainty Estimation: Ensembles,[0],[0]
"To compare QD to recent work on uncertainty in deep learning, we adopted their shared experimental procedure
5With uncertainty of the upper and lower bound averaged.
",6. Benchmarking Experiments,[0],[0]
"(Hernández-Lobato & Adams, 2015; Gal & Ghahramani, 2015; Lakshminarayanan et al., 2017).",6. Benchmarking Experiments,[0],[0]
Experiments were run across ten open-access datasets.,6. Benchmarking Experiments,[0],[0]
Models were asked to output 95% PIs and used five NNs per ensemble.,6. Benchmarking Experiments,[0],[0]
See supplementary material for full experimental details.,6. Benchmarking Experiments,[0],[0]
"Code is made available online6.
",6. Benchmarking Experiments,[0],[0]
Previous work reported NLL & RMSE metrics.,6. Benchmarking Experiments,[0],[0]
"However, the important metrics for PI quality areMPIW and PICP (section 1).",6. Benchmarking Experiments,[0],[0]
This meant that we had to reimplement a competing method.,6. Benchmarking Experiments,[0],[0]
"We chose to compare QD-Ens to MVE-Ens, since it had reported the best NLL results to date (Lakshminarayanan et al., 2017).",6. Benchmarking Experiments,[0],[0]
"We did not include LUBE since ensembling and GD had already been justified in section 5.
",6. Benchmarking Experiments,[0],[0]
"QD-Ens and MVE-Ens both output fundamentally different things; MVE-Ens a distribution, and QD-Ens upper and lower estimates of the PI.",6. Benchmarking Experiments,[0],[0]
To compute NLL & RMSE for QD-Ens is possible only by imposing a distribution on the PI.,6. Benchmarking Experiments,[0],[0]
This is not particularly fair since the attraction of the method is its lack of distributional assumption.,6. Benchmarking Experiments,[0],[0]
"Purely for comparison purposes we did this in the supplementary material.
",6. Benchmarking Experiments,[0],[0]
"A fairer comparison is to convert the MVE-Ens output distributions to PIs, and compute PI quality metrics.",6. Benchmarking Experiments,[0],[0]
"This was done by trimming the tails of the MVE-Ens output normal distributions by the appropriate amount, which allowed extraction of MPIW , PICP , and LossQD−soft.",6. Benchmarking Experiments,[0],[0]
"In our experiments we ensured MVE-Ens achieved NLL & RMSE scores at least as good as those reported in the original work, before PI metrics were calculated.
",6. Benchmarking Experiments,[0],[0]
6https://github.com/TeaPearce,6. Benchmarking Experiments,[0],[0]
"Full results of PI quality metrics are given in table 1, NLL & RMSE results are included in supplementary material.",6.1. Discussion,[0],[0]
"Given that LossQD−soft is representative of PI quality, QDEns outperformed MVE-Ens on all but one dataset.",6.1. Discussion,[0],[0]
"PICP was generally closer to the 95% target, and MPIW was on average 11.6% narrower.",6.1. Discussion,[0],[0]
The exception to this was the Kin8nm dataset.,6.1. Discussion,[0],[0]
"In fact, this dataset was synthetic (Danafar et al., 2010), and we suspect that Gaussian noise may have been used in its simulation, which would explain the superior performance of MVE-Ens.
",6.1. Discussion,[0],[0]
One drawback of QD-Ens was the fragility of the training process.,6.1. Discussion,[0],[0]
"Compared to MVE-Ens it required a lower learning rate, was more sensitive to decay rate, and hence needed from two to ten times more training epochs.
",6.1. Discussion,[0],[0]
Other comments are as follows.,6.1. Discussion,[0],[0]
We found λ a convenient lever providing some control over PICP .,6.1. Discussion,[0],[0]
"Bootstrap resampling gave worse performance than parameter resampling, which agrees with work discussed in 4 - we suspect it would work give a much larger ensemble size.",6.1. Discussion,[0],[0]
"We tried to establish a relationship between the normality of residual errors and improvement of QD-Ens over MVE-Ens, but due to the variable power of normality tests analysis was unreliable.",6.1. Discussion,[0],[0]
In this paper we derived a loss function for the output of PIs based on the assumption that high-quality PIs should be as narrow as possible subject to a given coverage proportion.,7. Conclusions and Future Work,[0],[0]
"We contrasted it with a previous work, justifying differences and showed that it can be used successfully with GD with only slight modification.",7. Conclusions and Future Work,[0],[0]
"We described why a single NN using the derived loss function underestimates uncertainty, and that this can be addressed by using the model in an ensemble.",7. Conclusions and Future Work,[0],[0]
"On ten benchmark regression datasets, the new model reduced PI widths by over 10%.
",7. Conclusions and Future Work,[0],[0]
"Several areas are worth further investigation: Why parameter resampling provides better performance than bootstrap resampling, how model uncertainty could be estimated through dropout or conformal prediction rather than ensembling, and the role that NN architecture plays in estimates of model uncertainty.",7. Conclusions and Future Work,[0],[0]
"The authors thank EPSRC for funding (EP/N509620/1), the Alan Turing Institute for accommodating the lead author during his work (TU/D/000016), and Microsoft for Azure credits.",Acknowledgements,[0],[0]
"Personal thanks to Mr Ayman Boustati, Mr HenryLouis de Kergorlay, and Mr Nicolas Anastassacos.",Acknowledgements,[0],[0]
This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks.,abstractText,[0],[0]
"It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data.",abstractText,[0],[0]
We derive a loss function directly from this axiom that requires no distributional assumption.,abstractText,[0],[0]
"We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form.",abstractText,[0],[0]
"Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10%.",abstractText,[0],[0]
"High-Quality Prediction Intervals for Deep Learning:  A Distribution-Free, Ensembled Approach",title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 687–692 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Recent coreference resolution systems have heavily relied on first order models (Clark and Manning, 2016a; Lee et al., 2017), where only pairs of entity mentions are scored by the model.",1 Introduction,[0],[0]
These models are computationally efficient and scalable to long documents.,1 Introduction,[0],[0]
"However, because they make independent decisions about coreference links, they are susceptible to predicting clusters that are locally consistent but globally inconsistent.",1 Introduction,[0],[0]
Figure 1 shows an example from Wiseman et al. (2016) that illustrates this failure case.,1 Introduction,[0],[0]
"The plurality of [you] is underspecified, making it locally compatible with both",1 Introduction,[0],[0]
"[I] and [all of you], while the full cluster would have mixed plurality, resulting in global inconsistency.
",1 Introduction,[0],[0]
We introduce an approximation of higher-order inference that uses the span-ranking architecture from Lee et al. (2017) in an iterative manner.,1 Introduction,[0],[0]
"At each iteration, the antecedent distribution is used as an attention mechanism to optionally update existing span representations, enabling later corefer-
ence decisions to softly condition on earlier coreference decisions.",1 Introduction,[0],[0]
"For the example in Figure 1, this enables the linking of [you] and [all of you] to depend on the linking of [I] and [you].
",1 Introduction,[0],[0]
"To alleviate computational challenges from this higher-order inference, we also propose a coarseto-fine approach that is learned with a single endto-end objective.",1 Introduction,[0],[0]
We introduce a less accurate but more efficient coarse factor in the pairwise scoring function.,1 Introduction,[0],[0]
This additional factor enables an extra pruning step during inference that reduces the number of antecedents considered by the more accurate but inefficient fine factor.,1 Introduction,[0],[0]
"Intuitively, the model cheaply computes a rough sketch of likely antecedents before applying a more expensive scoring function.
",1 Introduction,[0],[0]
Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark.,1 Introduction,[0],[0]
"We observe a significant increase in average F1 with a second-order model, but returns quickly diminish with a third-order model.",1 Introduction,[0],[0]
"Additionally, our analysis shows that the coarse-to-fine approach makes the model performance relatively insensitive to more aggressive antecedent pruning, compared to the distance-based heuristic pruning from previous work.
687",1 Introduction,[0],[0]
"Task definition We formulate the coreference resolution task as a set of antecedent assignments yi for each of span i in the given document, following Lee et al. (2017).",2 Background,[0],[0]
"The set of possible assignments for each yi is Y(i) = { , 1, . . .",2 Background,[0],[0]
", i − 1}, a dummy antecedent and all preceding spans.",2 Background,[0],[0]
Non-dummy antecedents represent coreference links between i and yi.,2 Background,[0],[0]
The dummy antecedent represents two possible scenarios: (1) the span is not an entity mention or (2) the span is an entity mention but it is not coreferent with any previous span.,2 Background,[0],[0]
"These decisions implicitly define a final clustering, which can be recovered by grouping together all spans that are connected by the set of antecedent predictions.
",2 Background,[0],[0]
"Baseline We describe the baseline model (Lee et al., 2017), which we will improve to address the modeling and computational limitations discussed previously.",2 Background,[0],[0]
"The goal is to learn a distribution P (yi) over antecedents for each span i :
P (yi) =",2 Background,[0],[0]
"es(i,yi)∑
y′∈Y(i)",2 Background,[0],[0]
"e s(i,y′)
",2 Background,[0],[0]
"(1)
where s(i, j) is a pairwise score for a coreference link between span i and span j. The baseline model includes three factors for this pairwise coreference score: (1) sm(i), whether span i is a mention, (2) sm(j), whether span j is a mention, and (3) sa(i, j) whether j is an antecedent of i:
s(i, j) = sm(i) + sm(j) + sa(i, j) (2)
",2 Background,[0],[0]
"In the special case of the dummy antecedent, the score s(i, ) is instead fixed to 0.",2 Background,[0],[0]
"A common component used throughout the model is the vector representations gi for each possible span i. These are computed via bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) that learn context-dependent boundary and head representations.",2 Background,[0],[0]
"The scoring functions sm and sa take these span representations as input:
sm(i) = w > m FFNNm(gi) (3) sa(i, j) =",2 Background,[0],[0]
"w > a FFNNa([gi, gj , gi ◦ gj , φ(i, j)]) (4)
where ◦ denotes element-wise multiplication, FFNN denotes a feed-forward neural network, and the antecedent scoring function sa(i, j) includes explicit element-wise similarity of each span gi ◦ gj and a feature vector φ(i, j) encoding speaker
and genre information from the metadata and the distance between the two spans.
",2 Background,[0],[0]
The model above is factored to enable a twostage beam search.,2 Background,[0],[0]
A beam of up to M potential mentions is computed (whereM is proportional to the document length) based on the spans with the highest mention scores sm(i).,2 Background,[0],[0]
"Pairwise coreference scores are only computed between surviving mentions during both training and inference.
",2 Background,[0],[0]
"Given supervision of gold coreference clusters, the model is learned by optimizing the marginal log-likelihood of the possibly correct antecedents.",2 Background,[0],[0]
This marginalization is required since the best antecedent for each span is a latent variable.,2 Background,[0],[0]
"The baseline above is a first-order model, since it only considers pairs of spans.",3 Higher-order Coreference Resolution,[0],[0]
First-order models are susceptible to consistency errors as demonstrated in Figure 1.,3 Higher-order Coreference Resolution,[0],[0]
"Unlike in sentence-level semantics, where higher-order decisions can be implicitly modeled by the LSTMs, modeling these decisions at the document-level requires explicit inference due to the potentially very large surface distance between mentions.
",3 Higher-order Coreference Resolution,[0],[0]
"We propose an inference procedure that allows the model to condition on higher-order structures, while being fully differentiable.",3 Higher-order Coreference Resolution,[0],[0]
"This inference involves N iterations of refining span representations, denoted as gni for the representation of span i at iteration n. At iteration n, gni is computed with an attention mechanism that averages over previous representations gn−1j weighted according to how likely each mention j is to be an antecedent for i, as defined below.
",3 Higher-order Coreference Resolution,[0],[0]
The baseline model is used to initialize the span representation at g1i .,3 Higher-order Coreference Resolution,[0],[0]
"The refined span representations allow the model to also iteratively refine the antecedent distributions Pn(yi):
Pn(yi) = es(g
n",3 Higher-order Coreference Resolution,[0],[0]
"i ,g n yi )
",3 Higher-order Coreference Resolution,[0],[0]
∑ y∈Y(i),3 Higher-order Coreference Resolution,[0],[0]
"e s(gni ,g n y ))
(5)
where s is the coreference scoring function of the baseline architecture.",3 Higher-order Coreference Resolution,[0],[0]
"The scoring function uses the same parameters at every iteration, but it is given different span representations.
",3 Higher-order Coreference Resolution,[0],[0]
"At each iteration, we first compute the expected antecedent representation ani of each span i by using the current antecedent distribution Pn(yi) as
an attention mechanism:
ani = ∑
yi∈Y(i) Pn(yi) · gnyi (6)
",3 Higher-order Coreference Resolution,[0],[0]
"The current span representation gni is then updated via interpolation with its expected antecedent representation ani :
",3 Higher-order Coreference Resolution,[0],[0]
fni = σ(Wf[g n,3 Higher-order Coreference Resolution,[0],[0]
"i ,a n i ]) (7) gn+1i = f n",3 Higher-order Coreference Resolution,[0],[0]
i ◦ gni + (1− fni ) ◦,3 Higher-order Coreference Resolution,[0],[0]
"ani (8)
The learned gate vector fni determines for each dimension whether to keep the current span information or to integrate new information from its expected antecedent.",3 Higher-order Coreference Resolution,[0],[0]
"At iteration n, gni is an element-wise weighted average of approximately n span representations (assuming Pn(yi) is peaked), allowing Pn(yi) to softly condition on up to n other spans in the predicted cluster.
",3 Higher-order Coreference Resolution,[0],[0]
"Span-ranking can be viewed as predicting latent antecedent trees (Fernandes et al., 2012; Martschat and Strube, 2015), where the predicted antecedent is the parent of a span and each tree is a predicted cluster.",3 Higher-order Coreference Resolution,[0],[0]
"By iteratively refining the span representations and antecedent distributions, another way to interpret this model is that the joint distribution∏ i PN (yi) implicitly models every directed path of up to length N +1 in the latent antecedent tree.",3 Higher-order Coreference Resolution,[0],[0]
The model described above scales poorly to long documents.,4 Coarse-to-fine Inference,[0],[0]
"Despite heavy pruning of potential mentions, the space of possible antecedents for every surviving span is still too large to fully consider.",4 Coarse-to-fine Inference,[0],[0]
"The bottleneck is in the antecedent score sa(i, j), which requires computing a tensor of size M ×M × (3|g|+ |φ|).
",4 Coarse-to-fine Inference,[0],[0]
"This computational challenge is even more problematic with the iterative inference from Section 3, which requires recomputing this tensor at every iteration.",4 Coarse-to-fine Inference,[0],[0]
"To reduce computation, Lee et al. (2017) heuristically consider only the nearest K antecedents of each span, resulting in a smaller input of size M ×K × (3|g|+ |φ|).
",4.1 Heuristic antecedent pruning,[0],[0]
The main drawback to this solution is that it imposes an a priori limit on the maximum distance of a coreference link.,4.1 Heuristic antecedent pruning,[0],[0]
"The previous work only considers up to K = 250 nearest mentions, whereas coreference links can reach much further in natural language discourse.",4.1 Heuristic antecedent pruning,[0],[0]
We instead propose a coarse-to-fine approach that can be learned end-to-end and does not establish an a priori maximum coreference distance.,4.2 Coarse-to-fine antecedent pruning,[0],[0]
"The key component of this coarse-to-fine approach is an alternate bilinear scoring function:
sc(i, j) =",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"g > i Wc gj (9)
where Wc is a learned weight matrix.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"In contrast to the concatenation-based sa(i, j), the bilinear sc(i, j) is far less accurate.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"A direct replacement of sa(i, j) with sc(i, j) results in a performance loss of over 3 F1 in our experiments.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"However, sc(i, j) is much more efficient to compute.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"Computing sc(i, j) only requires manipulating matrices of size M × |g| and M ×M .
",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"Therefore, we instead propose to use sc(i, j) to compute a rough sketch of likely antecedents.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"This is accomplished by including it as an additional factor in the model:
s(i, j) = sm(i) + sm(j)",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"+ sc(i, j) + sa(i, j) (10)
Similar to the baseline model, we leverage this additional factor to perform an additional beam pruning step.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"The final inference procedure involves a three-stage beam search:
First stage Keep the top M spans based on the mention score sm(i) of each span.
",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"Second stage Keep the top K antecedents of each remaining span i based on the first three factors, sm(i) + sm(j)",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"+ sc(i, j).
",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"Third stage The overall coreference s(i, j) is computed based on the remaining span pairs.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"The soft higher-order inference from Section 3 is computed in this final stage.
",4.2 Coarse-to-fine antecedent pruning,[0],[0]
"While the maximum-likelihood objective is computed over only the span pairs from this final stage, this coarse-to-fine approach expands the set of coreference links that the model is capable of learning.",4.2 Coarse-to-fine antecedent pruning,[0],[0]
It achieves better performance while using a much smaller K (see Figure 2).,4.2 Coarse-to-fine antecedent pruning,[0],[0]
"We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments.",5 Experimental Setup,[0],[0]
"The code for replicating these results is publicly available.1
Our models reuse the hyperparameters from Lee et al. (2017), with a few exceptions mentioned below.",5 Experimental Setup,[0],[0]
"In our results, we report two improvements that are orthogonal to our contributions.
",5 Experimental Setup,[0],[0]
"• We used embedding representations from a language model (Peters et al., 2018) at the input to the LSTMs (ELMo in the results).
",5 Experimental Setup,[0],[0]
"• We changed several hyperparameters:
1. increasing the maximum span width from 10 to 30 words.
",5 Experimental Setup,[0],[0]
"2. using 3 highway LSTMs instead of 1. 3. using GloVe word embeddings (Pen-
nington et al., 2014) with a window size
1https://github.com/kentonl/e2e-coref
of 2 for the head word embeddings and a window size of 10 for the LSTM inputs.
",5 Experimental Setup,[0],[0]
The baseline model considers up to 250 antecedents per span.,5 Experimental Setup,[0],[0]
"As shown in Figure 2, the coarse-to-fine model is quite insensitive to more aggressive pruning.",5 Experimental Setup,[0],[0]
"Therefore, our final model considers only 50 antecedents per span.
",5 Experimental Setup,[0],[0]
"On the development set, the second-order model (N = 2) outperforms the first-order model by 0.8 F1, but the third order model only provides an additional 0.1 F1 improvement.",5 Experimental Setup,[0],[0]
"Therefore, we only compute test results for the secondorder model.",5 Experimental Setup,[0],[0]
"We report the precision, recall, and F1 of the the MUC, B3, and CEAFφ4metrics using the official CoNLL-2012 evaluation scripts.",6 Results,[0],[0]
"The main evaluation is the average F1 of the three metrics.
",6 Results,[0],[0]
Results on the test set are shown in Table 1.,6 Results,[0],[0]
We include performance of systems proposed in the past 3 years for reference.,6 Results,[0],[0]
"The baseline relative to our contributions is the span-ranking model from Lee et al. (2017) augmented with both ELMo and hyperparameter tuning, which achieves 72.3 F1.",6 Results,[0],[0]
"Our full approach achieves 73.0 F1, setting a new state of the art for coreference resolution.
",6 Results,[0],[0]
"Compared to the heuristic pruning with up to 250 antecedents, our coarse-to-fine model only computes the expensive scores sa(i, j) for 50 antecedents.",6 Results,[0],[0]
"Despite using far less computation, it outperforms the baseline because the coarse scores
sc(i, j) can be computed for all antecedents, enabling the model to potentially predict a coreference link between any two spans in the document.",6 Results,[0],[0]
"As a result, we observe a much higher recall when adopting the coarse-to-fine approach.
",6 Results,[0],[0]
We also observe further improvement by including the second-order inference (Section 3).,6 Results,[0],[0]
"The improvement is largely driven by the overall increase in precision, which is expected since the higher-order inference mainly serves to rule out inconsistent clusters.",6 Results,[0],[0]
It is also consistent with findings from Martschat and Strube (2015) who report mainly improvements in precision when modeling latent trees to achieve a similar goal.,6 Results,[0],[0]
"In addition to the end-to-end span-ranking model (Lee et al., 2017) that our proposed model builds upon, there is a large body of literature on coreference resolvers that fundamentally rely on scoring span pairs (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a).
",7 Related Work,[0],[0]
"Motivated by structural consistency issues discussed above, significant effort has also been devoted towards cluster-level modeling.",7 Related Work,[0],[0]
"Since global features are notoriously difficult to define (Wiseman et al., 2016), they often depend heavily on existing pairwise features or architectures (Björkelund and Kuhn, 2014; Clark and Manning, 2015, 2016b).",7 Related Work,[0],[0]
We similarly use an existing pairwise span-ranking architecture as a building block for modeling more complex structures.,7 Related Work,[0],[0]
"In contrast to Wiseman et al. (2016) who use highly expressive recurrent neural networks to model clusters, we show that the addition of a relatively lightweight gating mechanism is sufficient to effectively model higher-order structures.",7 Related Work,[0],[0]
We presented a state-of-the-art coreference resolution system that models higher order interactions between spans in predicted clusters.,8 Conclusion,[0],[0]
"Additionally, our proposed coarse-to-fine approach alleviates the additional computational cost of higherorder inference, while maintaining the end-to-end learnability of the entire model.",8 Conclusion,[0],[0]
"The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award.",Acknowledgements,[0],[0]
We also thank the UW NLP group for helpful conversations and comments on the work.,Acknowledgements,[0],[0]
We introduce a fully differentiable approximation to higher-order inference for coreference resolution.,abstractText,[0],[0]
Our approach uses the antecedent distribution from a span-ranking architecture as an attention mechanism to iteratively refine span representations.,abstractText,[0],[0]
This enables the model to softly consider multiple hops in the predicted clusters.,abstractText,[0],[0]
"To alleviate the computational cost of this iterative process, we introduce a coarse-to-fine approach that incorporates a less accurate but more efficient bilinear factor, enabling more aggressive pruning without hurting accuracy.",abstractText,[0],[0]
"Compared to the existing state-of-the-art span-ranking approach, our model significantly improves accuracy on the English OntoNotes benchmark, while being far more computationally efficient.",abstractText,[0],[0]
Higher-order Coreference Resolution with Coarse-to-fine Inference,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2055–2061, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Entailment relations are of central importance in the enterprise of both formal and computational semantics.,1 Introduction,[0],[0]
"Traditionally, formal semanticists have concentrated on a relatively small set of linguistic inferences.",1 Introduction,[0],[0]
"However, since the emergence of statistical parsers based on sophisticated syntactic theories (Clark and Curran, 2007), an open domain system has been developed that supports certain degree of robust semantic interpretation with wide coverage (Bos et al., 2004).",1 Introduction,[0],[0]
"It is then reasonable to expect that a state-of-the-art formal semantics provides an accurate computational basis of natural language inferences.
",1 Introduction,[0],[0]
"However, there are still obstacles in the way of achieving this goal.",1 Introduction,[0],[0]
"One is that the statistical parsers on which semantic interpretations rely do not necessarily reflect the best syntactic analysis as assumed in the formal semantics literature (Honnibal et al., 2010).",1 Introduction,[0],[0]
"Another persistent problem is the gap between the logics employed in the two com-
munities; while it is generally assumed among formal semanticists that adequate semantic representations for natural language demand higher-order logic or type theory (Carpenter, 1997), the dominant view in computational linguistics is that inferences based on higher-order logic are hopelessly inefficient for practical applications (Bos, 2009a).",1 Introduction,[0],[0]
"Accordingly, it is claimed that some approximation of higher-order representations in terms of first-order logic (Hobbs, 1985), or a more efficient “natural logic” system based on surface structures is needed.",1 Introduction,[0],[0]
"However, it is often not a trivial task to give an approximation of rich higher-order information within a first-order language (Pulman, 2007).",1 Introduction,[0],[0]
"Moreover, the coverage of existing natural logic systems is limited to single-premise inferences (MacCartney and Manning, 2008).
",1 Introduction,[0],[0]
"In this paper, we first present an improved compositional semantics that fills the gap between the parser syntax and a composition derivation.",1 Introduction,[0],[0]
We then develop an inference system that is capable of higher-order inferences in natural languages.,1 Introduction,[0],[0]
"We combine a state-of-the-art higher-order proof system (Coq) with a wide-coverage parser based on a modern syntactic theory (Combinatory Categorial Grammar, CCG).",1 Introduction,[0],[0]
"The system is designed to handle multi-premise inferences as well as singlepremise ones.
",1 Introduction,[0],[0]
"We test our system on the FraCaS test suite (Cooper et al., 1994), which is suitable for evaluating the linguistic coverage of an inference system.",1 Introduction,[0],[0]
The experiments show that our higher-order system outperforms the state-of-the-art first-order system with respect to the speed and accuracy of making logical inferences.,1 Introduction,[0],[0]
"As an initial step of compositional semantics, we use the C&C parser (Clark and Curran, 2007), a statistical CCG parser trained on CCGbank (Hockenmaier and Steedman, 2007).",2 CCG and Compositional Semantics,[0],[0]
"Parser out-
2055
puts are mapped onto semantic representations in a standard way (Bos, 2008), using λ-calculus as an interface between syntax and semantics.
",2 CCG and Compositional Semantics,[0],[0]
The strategy we use to build a semantic lexicon is similar to that of Bos et al. (2004).,2 CCG and Compositional Semantics,[0],[0]
A lexical entry for each open word class consists of a syntactic category in CCG (possibly with syntactic features) and a semantic representation encoded as a λ-term.,2 CCG and Compositional Semantics,[0],[0]
"Fig. 1 gives an example.1 For a limited number of closed words such as logical or functional expressions, a λ-term is directly assigned to a surface form (see Fig. 2).",2 CCG and Compositional Semantics,[0],[0]
"The output formula is obtained by combining each λ-term in accordance with meaning composition rules and then by applying β-conversion.
",2 CCG and Compositional Semantics,[0],[0]
There is a non-trivial gap between the parser output and the standard CCG-syntax as presented in Steedman (2000).,2 CCG and Compositional Semantics,[0],[0]
"Due to this gap, it is not straightforward to obtain desirable semantic representations for a wide range of constructions.",2 CCG and Compositional Semantics,[0],[0]
"One major difference from the standard CCG-syntax is the treatment of post-NP modifiers; for instance, the relative clause who works is assigned not the category N \N , but the category NP\NP , which applies to the whole NP.",2 CCG and Compositional Semantics,[0],[0]
"To derive correct truthconditions for quantificational sentences, we assign to determiners a semantic term having an extra predicate variable as shown in Fig. 2, namely, λFλGλH .∀x",2 CCG and Compositional Semantics,[0],[0]
"(Fx ∧Gx→Hx ), in a similar way to the continuation semantics for event predicates (Bos, 2009b; Champollion, 2015).",2 CCG and Compositional Semantics,[0],[0]
The extra predicate variable G can be filled by the semantically empty predicate λx.True in a verb phrase (see Fig. 1).,2 CCG and Compositional Semantics,[0],[0]
"Fig. 3 gives an example derivation.
",2 CCG and Compositional Semantics,[0],[0]
"Note that the changes in the lexical entries as illustrated in Fig. 1 and Fig. 2 are made for the correct semantic parsing, namely, the compositional
1Here we use a non-standard semantics for intransitive verbs.",2 CCG and Compositional Semantics,[0],[0]
"We will explain it in the next paragraph.
derivation of semantic representations.",2 CCG and Compositional Semantics,[0],[0]
"Usually, inferences are conducted on those output semantic representations in which additional complexities, such as lambda operators and extra predicate variables, disappear.",2 CCG and Compositional Semantics,[0],[0]
"Accordingly, the changes in the lexical entries do not affect the efficiency of inferences.
",2 CCG and Compositional Semantics,[0],[0]
"The present analysis of post NP-modifiers can also handle non-restrictive relative clauses such as “the president, who ...”.",2 CCG and Compositional Semantics,[0],[0]
"In this case, the modifier “who ...” can be taken to apply to the whole NP the president, thus its syntactic category can be regarded as NP\NP , not as N \N .",2 CCG and Compositional Semantics,[0],[0]
"Thus, although the NP\NP analysis of relative clauses is a non-standard one, it has an advantage in that it provides a unified treatment of restrictive and nonrestrictive relative clauses.",2 CCG and Compositional Semantics,[0],[0]
We present a higher-order representation language and describe apparently higher-order phenomena that have received attention in formal semantics.,3 Representation and Inference in HOL,[0],[0]
"We use the language of higher-order logic (HOL) with two basic types, E for entities and Prop for propositions.",3.1 Semantic representations in HOL,[0],[0]
"Here we distinguish between propositions and truth-values, as is standard in modern type theory (Ranta, 1994; Luo, 2012).",3.1 Semantic representations in HOL,[0],[0]
Key higherorder constructs are summarized in Table 1.2 A first-order language can be taken as a fragment of this language.,3.1 Semantic representations in HOL,[0],[0]
"Thus, adopting a higher-order language does not lead to the loss of the expressive power of the first-order language.
",3.1 Semantic representations in HOL,[0],[0]
"Apart from sub-sentential utterances such as short answers to wh-questions (Ginzburg, 2005), there are important constructions that are naturally
2We write a function from objects of type A to objects of type B as A→B. Here → is right-associative: A→B→C means A→(B→C).",3.1 Semantic representations in HOL,[0],[0]
"We use the symbol → both for logical implication and function-type constructor, following the socalled Curry-Howard isomorphism (Carpenter, 1997).
represented in higher-order languages.3
Generalized quantifiers A classical example of non-first-orderizable expressions is a proportional generalized quantifier like most and half of (Barwise and Cooper, 1981).",3.1 Semantic representations in HOL,[0],[0]
"Model-theoretically, they denote relations between sets.",3.1 Semantic representations in HOL,[0],[0]
We represent them as a two-place higher-order predicate taking firstorder predicates as arguments.,3.1 Semantic representations in HOL,[0],[0]
"For instance, Most students work is represented as follows.
",3.1 Semantic representations in HOL,[0],[0]
"(1) most(λx.student(x), λx.work(x))
",3.1 Semantic representations in HOL,[0],[0]
"Here, most is a higher-order predicate in the sense that it takes first-order predicates λx.student(x) and λx.work(x) as arguments.",3.1 Semantic representations in HOL,[0],[0]
"We take the entailment patterns governing most as axioms, along the same lines of natural logic and monotonicity calculus (Icard and Moss, 2014), where determiners are taken as primitive two-place operators.
",3.1 Semantic representations in HOL,[0],[0]
Standard quantifiers like every and some could also be treated as binary operators in the same way as the binary most in (1).,3.1 Semantic representations in HOL,[0],[0]
"But we choose to adopt the first-order decomposition in such cases (see Fig. 2 for the lexical entry of every).
",3.1 Semantic representations in HOL,[0],[0]
"Modals Modal auxiliary expressions like might, must and can are represented as unary sentential operators.",3.1 Semantic representations in HOL,[0],[0]
"For instance, the sentence Some student might come is represented as:
(2) ∃x(student(x) ∧might(come(x))).",3.1 Semantic representations in HOL,[0],[0]
"An important inference role of such a modal operator is to distinguish modal contexts from actual contexts and thus block an inference from one context to another (might A does not entail A).
",3.1 Semantic representations in HOL,[0],[0]
"Alternatives to the higher-order approach include the first-order decomposition of modal operators using world variables (Blackburn et al., 2001) and the first-order modal semantic representations implemented in Boxer (Bos, 2005).",3.1 Semantic representations in HOL,[0],[0]
"We
3See also Blackburn and Bos (2005) for some discussion on inferences that go beyond first-order logic.
",3.1 Semantic representations in HOL,[0],[0]
"prefer the higher-order approach, because the firstorder approaches introduce additional quantifiers and variables at the level of the semantic representations on which one makes inferences.
",3.1 Semantic representations in HOL,[0],[0]
"Veridical and anti-veridical predicates A sentential operator O is veridical if O(A) entails A, and anti-veridical if O(A) entails ¬A.",3.1 Semantic representations in HOL,[0],[0]
"While modal auxiliary verbs like might are neither veridical nor anti-veridical, there is a class of expressions licensing these patterns of inference.",3.1 Semantic representations in HOL,[0],[0]
"Typical examples are adjectives taking an embedded proposition, such as true/correct and false/incorrect.",3.1 Semantic representations in HOL,[0],[0]
"Note that sentences like Everything/what he said is false involve a quantification over propositions, which is problematic for the first-order approach.
",3.1 Semantic representations in HOL,[0],[0]
"The so-called implicative verbs like manage and fail (Nairn et al., 2006) are also an instance of this class.",3.1 Semantic representations in HOL,[0],[0]
"For example, Some student manages to come is formalized as
(3) ∃x(student(x) ∧manage(x, come(x))) where manage is a veridical predicate taking a proposition as the second argument; it licenses an inference to ∃x(student(x) ∧ come(x)).",3.1 Semantic representations in HOL,[0],[0]
Attitude verbs A wide range of propositional attitude verbs such as believe and hope are similar to modals in that they do not license an inference from attitude contexts to actual contexts.,3.1 Semantic representations in HOL,[0],[0]
"But factives like know and remember are an exception; they are veridical.4
A first-order translation can be given along the lines of Hintikka (1962).",3.1 Semantic representations in HOL,[0],[0]
"(4) is translated as (5).
(4) know(john,∃x(student(x) ∧ come(x)))",3.1 Semantic representations in HOL,[0],[0]
"(5) ∀w1(Rjohnw0 w1 → ∃x(student(w1, x) ∧ come(w1, x)))",3.1 Semantic representations in HOL,[0],[0]
"4Factive predicates show the important inference patterns known as presupposition projection (van der Sandt, 1992), which are beyond the scope of this paper.
",3.1 Semantic representations in HOL,[0],[0]
"However, one drawback is that the compositional semantics becomes complicated, so we prefer the non-decomposition approach for attitude verbs.",3.1 Semantic representations in HOL,[0],[0]
"Following Chatzikyriakidis and Luo (2014), we use a proof-assistant Coq (Castéran and Bertot, 2004) to implement a specialized prover for higher-order features in natural languages, and combine it with efficient first-order inferences.",3.2 Inferences in HOL,[0],[0]
We use Coq’s built-in tactics for first-order inferences.,3.2 Inferences in HOL,[0],[0]
"Coq also has a language called Ltac for userdefined automated tactics (Delahaye, 2000).",3.2 Inferences in HOL,[0],[0]
The additional axioms and tactics specialized for natural language constructions are written in Ltac.,3.2 Inferences in HOL,[0],[0]
"We ran Coq fully automated, by feeding to its interactive mode a set of predefined tactics combined with user-defined proof-search tactics.
",3.2 Inferences in HOL,[0],[0]
Table 2 shows the axioms we implemented.,3.2 Inferences in HOL,[0],[0]
"Modals and non-veridical predicates (by which we mean predicates that are neither veridical nor antiveridical) do not have particular axioms, with the consequence that actual and hypothetical contexts are correctly distinguished.",3.2 Inferences in HOL,[0],[0]
"We evaluated our system on the FraCaS test suite (Cooper et al., 1994), a set of entailment problems that is designed to evaluate theories of formal semantics.5",4 Experiments,[0],[0]
We used the version provided by MacCartney and Manning (2007).,4 Experiments,[0],[0]
"The whole data set is divided into nine sections, each devoted to linguistically challenging problems.",4 Experiments,[0],[0]
"Of these, we used six sections, excluding three sections (nominal anaphora, ellipsis and temporal reference) that
5Our system will be publicly available at https://github.com/mynlp/ccg2lambda.
involve a task of resolving context-dependency, a task beyond the scope of this paper.",4 Experiments,[0],[0]
"Each problem consists of one or more premises, followed by a hypothesis.",4 Experiments,[0],[0]
"There are three types of answer: yes (the premise set entails the hypothesis), no (the premise set entails the negation of the hypothesis), and unknown (the premise set entails neither the hypothesis nor its negation).",4 Experiments,[0],[0]
"Fig. 4 shows some examples.
",4 Experiments,[0],[0]
"Currently, our system has 57 templates for general syntactic categories and 80 lexical entries for closed words.",4 Experiments,[0],[0]
"In a similar way to Bos et al. (2004), closed words are confined to a limited range of logical and functional expressions such as quantifiers and connectives.",4 Experiments,[0],[0]
These templates and lexical entries are not specific with respect to the FraCaS test suite.,4 Experiments,[0],[0]
"We use WordNet (Miller, 1995) as the knowledge base for antonymy; logical axioms relevant to given inferences are extracted from this knowledge base.
",4 Experiments,[0],[0]
"We compared our system with the state-ofthe-art CCG-based first-order system Boxer (Bos, 2008), which is one of the most well-known logicbased approaches to textual entailment.",4 Experiments,[0],[0]
We used the Nutcracker system based on Boxer that utilizes a first-order prover (Bliksem) and a model builder (Mace) with the option enabling access to WordNet.,4 Experiments,[0],[0]
"We did not use the option enabling modal semantics, since it did not improve the results.",4 Experiments,[0],[0]
"All experiments were run on a 4-core@1.8Ghz, 8GB RAM and SSD machine with Ubuntu.
",4 Experiments,[0],[0]
Experimental results are shown in Table 3.,4 Experiments,[0],[0]
Our system improved on Nutcracker.,4 Experiments,[0],[0]
"We set a timeout of 30 seconds, after which we output the label “unknown”.",4 Experiments,[0],[0]
"Nutcracker timed-out in one third of the problems (57 out of 181), whereas there was no time-out in our system.
",4 Experiments,[0],[0]
Table 4 shows parse times and inference times for the FraCaS test suite.,4 Experiments,[0],[0]
"The inference speed
of our system is significantly higher than that of Nutcracker.",4 Experiments,[0],[0]
"Our system’s total accuracy with higher-order rules is 69%, and drops to 59% when ablating the higher-order rules.
",4 Experiments,[0],[0]
We are aware of two other systems tested on FraCaS that are capable of multiple-premise inferences: the CCG-based first-order system of Lewis and Steedman (2013) and the dependency-based compositional semantics of Tian et al. (2014).,4 Experiments,[0],[0]
"These systems were only evaluated on the Quantifier section of FraCaS. As shown in Table 3, our results improve on the former and are comparable with the latter.
",4 Experiments,[0],[0]
"Other important studies on FraCaS are those based on natural logic (MacCartney and Manning, 2008; Angeli and Manning, 2014).",4 Experiments,[0],[0]
These systems are designed solely for single-premise inferences and hence are incapable of handling the general case of multiple-premise problems (which cover about 45% of the problems in FraCaS).,4 Experiments,[0],[0]
"Our system improves on these natural-logic-based systems by making multiple-premise inferences as well.
",4 Experiments,[0],[0]
"Main errors we found are due to various parse errors caused by the CCG parser, including the failure to handle multiwords like a lot of.",4 Experiments,[0],[0]
"The performance of our system will be further improved
with correct syntactic analyses.",4 Experiments,[0],[0]
Our experiments on FraCaS problems do not constitute an evaluation on real texts nor on unseen test data.,4 Experiments,[0],[0]
"Note, however, that a benefit of using a linguistically controlled set of entailment problems is that one can check not only whether, but also how each semantic phenomenon is handled by the system.",4 Experiments,[0],[0]
"In contrast to the widely held view that higher-order logic is less useful in computational linguistics, our results demonstrate the logical capacity of a higher-order inference system integrated with the CCG-based compositional semantics.",4 Experiments,[0],[0]
"We have presented a framework for a compositional semantics based on the wide-coverage CCG parser, combined with a higher-order inference system.",5 Conclusion,[0],[0]
The experimental results on the FraCaS test suite have shown that a reasonable number of lexical entries and non-first-order axioms enable various logical inferences in an efficient way and outperform the state-of-the-art first-order system.,5 Conclusion,[0],[0]
"Future work will focus on incorporating a robust model of lexical knowledge (Lewis and Steedman, 2013; Tian et al., 2014) to our framework.",5 Conclusion,[0],[0]
"We are grateful to Chris Worth, Ribeka Tanaka, and the three anonymous reviewers for their helpful comments and suggestions.",Acknowledgments,[0],[0]
This research has been supported by the JST CREST program (Establishment of Knowledge-Intensive Structural Natural Language Processing and Construction of Knowledge Infrastructure).,Acknowledgments,[0],[0]
We present a higher-order inference system based on a formal compositional semantics and the wide-coverage CCG parser.,abstractText,[0],[0]
We develop an improved method to bridge between the parser and semantic composition.,abstractText,[0],[0]
The system is evaluated on the FraCaS test suite.,abstractText,[0],[0]
"In contrast to the widely held view that higher-order logic is unsuitable for efficient logical inferences, the results show that a system based on a reasonably-sized semantic lexicon and a manageable number of non-first-order axioms enables efficient logical inferences, including those concerned with generalized quantifiers and intensional operators, and outperforms the state-of-the-art firstorder inference system.",abstractText,[0],[0]
Higher-order logical inference with compositional semantics,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2369",text,[0],[0]
The ability to perform reasoning and inference over natural language is an important aspect of intelligence.,1 Introduction,[0],[0]
The task of question answering (QA) provides a quantifiable and objective way to test the reasoning ability of intelligent systems.,1 Introduction,[0],[0]
"To this end, a few large-scale QA datasets have been proposed, which sparked significant progress in this direction.",1 Introduction,[0],[0]
"However, existing datasets have limitations that hinder further advancements of machine reasoning over natural language, especially in testing QA systems’ ability to perform multi-hop reasoning, where the system has to reason with information taken from more than one document to arrive at the answer.",1 Introduction,[0],[0]
∗These authors contributed equally.,1 Introduction,[0],[0]
The order of authorship is decided through dice rolling.,1 Introduction,[0],[0]
"†Work done when WWC was at CMU.
",1 Introduction,[0],[0]
"First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning.",1 Introduction,[0],[0]
"For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph.",1 Introduction,[0],[0]
"As a result, it has fallen short at testing systems’ ability to reason over a larger context.",1 Introduction,[0],[0]
"TriviaQA (Joshi et al., 2017) and SearchQA",1 Introduction,[0],[0]
"(Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs.",1 Introduction,[0],[0]
"Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g.,
over multiple paragraphs).",1 Introduction,[0],[0]
"Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and COMPLEXWEBQUESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs).",1 Introduction,[0],[0]
"As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited.
",1 Introduction,[0],[0]
"Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it.",1 Introduction,[0],[0]
"This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions.
",1 Introduction,[0],[0]
"To address the above challenges, we aim at creating a QA dataset that requires reasoning over multiple documents, and does so in natural language, without constraining itself to an existing knowledge base or knowledge schema.",1 Introduction,[0],[0]
"We also want it to provide the system with strong supervision about what text the answer is actually derived from, to help guide systems to perform meaningful and explainable reasoning.
",1 Introduction,[0],[0]
"We present HOTPOTQA1, a large-scale dataset that satisfies these desiderata.",1 Introduction,[0],[0]
"HOTPOTQA is collected by crowdsourcing based on Wikipedia articles, where crowd workers are shown multiple supporting context documents and asked explicitly to come up with questions requiring reasoning about all of the documents.",1 Introduction,[0],[0]
"This ensures it covers multi-hop questions that are more natural, and are not designed with any pre-existing knowledge base schema in mind.",1 Introduction,[0],[0]
"Moreover, we also ask the crowd workers to provide the supporting facts they use to answer the question, which we also provide as part of the dataset (see Figure 1 for an example).",1 Introduction,[0],[0]
"We have carefully designed a data collection pipeline for HOTPOTQA, since the collection of high-quality multi-hop questions is nontrivial.",1 Introduction,[0],[0]
We hope that this pipeline also sheds light on future work in this direction.,1 Introduction,[0],[0]
"Finally, we also collected a novel type of questions—comparison questions—as part of HOTPOTQA, in which we require systems to compare two entities on some shared properties to test their understanding of both language and common concepts such as numerical magnitude.",1 Introduction,[0],[0]
"We make HOTPOTQA publicly available at https://HotpotQA.github.io.
1The name comes from the first three authors’ arriving at the main idea during a discussion at a hot pot restaurant.",1 Introduction,[0],[0]
The main goal of our work is to collect a diverse and explainable question answering dataset that requires multi-hop reasoning.,2 Data Collection,[0],[0]
"One way to do so is to define reasoning chains based on a knowledge base (Welbl et al., 2018; Talmor and Berant, 2018).",2 Data Collection,[0],[0]
"However, the resulting datasets are limited by the incompleteness of entity relations and the lack of diversity in the question types.",2 Data Collection,[0],[0]
"Instead, in this work, we focus on text-based question answering in order to diversify the questions and answers.",2 Data Collection,[0],[0]
"The overall setting is that given some context paragraphs (e.g., a few paragraphs, or the entire Web) and a question, a QA system answers the question by extracting a span of text from the context, similar to Rajpurkar et al. (2016).",2 Data Collection,[0],[0]
"We additionally ensure that it is necessary to perform multi-hop reasoning to correctly answer the question.
",2 Data Collection,[0],[0]
It is non-trivial to collect text-based multi-hop questions.,2 Data Collection,[0],[0]
"In our pilot studies, we found that simply giving an arbitrary set of paragraphs to crowd workers is counterproductive, because for most paragraph sets, it is difficult to ask a meaningful multi-hop question.",2 Data Collection,[0],[0]
"To address this challenge, we carefully design a pipeline to collect text-based multi-hop questions.",2 Data Collection,[0],[0]
"Below, we will highlight the key design choices in our pipeline.
",2 Data Collection,[0],[0]
Building a Wikipedia Hyperlink Graph.,2 Data Collection,[0],[0]
"We use the entire English Wikipedia dump as our corpus.2 In this corpus, we make two observations: (1) hyper-links in the Wikipedia articles often naturally entail a relation between two (already disambiguated) entities in the context, which could potentially be used to facilitate multi-hop reasoning; (2) the first paragraph of each article often contains much information that could be queried in a meaningful way.",2 Data Collection,[0],[0]
"Based on these observations, we extract all the hyperlinks from the first paragraphs of all Wikipedia articles.",2 Data Collection,[0],[0]
"With these hyperlinks, we build a directed graph G, where each edge (a, b) indicates there is a hyperlink from the first paragraph of article a to article b.
Generating Candidate Paragraph Pairs.",2 Data Collection,[0],[0]
"To generate meaningful pairs of paragraphs for multihop question answering with G, we start by considering an example question “when was the singer and songwriter of Radiohead born?”",2 Data Collection,[0],[0]
"To
2https://dumps.wikimedia.org/
answer this question, one would need to first reason that the “singer and songwriter of Radiohead” is “Thom Yorke”, and then figure out his birthday in the text.",2 Data Collection,[0],[0]
We call “Thom Yorke” a bridge entity in this example.,2 Data Collection,[0],[0]
"Given an edge (a, b) in the hyperlink graph G, the entity of b can usually be viewed as a bridge entity that connects a and b.",2 Data Collection,[0],[0]
"As we observe articles b usually determine the theme of the shared context between a and b, but not all articles b are suitable for collecting multihop questions.",2 Data Collection,[0],[0]
"For example, entities like countries are frequently referred to in Wikipedia, but don’t necessarily have much in common with all incoming links.",2 Data Collection,[0],[0]
"It is also difficult, for instance, for the crowd workers to ask meaningful multihop questions about highly technical entities like the IPv4 protocol.",2 Data Collection,[0],[0]
"To alleviate this issue, we constrain the bridge entities to a set of manually curated pages in Wikipedia (see Appendix A).",2 Data Collection,[0],[0]
"After curating a set of pages B, we create candidate paragraph pairs by sampling edges (a, b) from the hyperlink graph such that b ∈ B.
Comparison Questions.",2 Data Collection,[0],[0]
"In addition to questions collected using bridge entities, we also collect another type of multi-hop questions— comparison questions.",2 Data Collection,[0],[0]
"The main idea is that comparing two entities from the same category usually results in interesting multi-hop questions, e.g., “Who has played for more NBA teams, Michael Jordan or Kobe Bryant?”",2 Data Collection,[0],[0]
"To facilitate collecting this type of question, we manually curate 42 lists of similar entities (denoted as L) from Wikipedia.3 To generate candidate paragraph pairs, we randomly sample two paragraphs from the same list and present them to the crowd worker.
",2 Data Collection,[0],[0]
"To increase the diversity of multi-hop questions, we also introduce a subset of yes/no questions in comparison questions.",2 Data Collection,[0],[0]
This complements the original scope of comparison questions by offering new ways to require systems to reason over both paragraphs.,2 Data Collection,[0],[0]
"For example, consider the entities Iron Maiden (from the UK) and AC/DC (from Australia).",2 Data Collection,[0],[0]
"Questions like “Is Iron Maiden or AC/DC from the UK?” are not ideal, because one would deduce the answer is “Iron Maiden” even if one only had access to that article.",2 Data Collection,[0],[0]
"With yes/no questions, one may ask “Are Iron Maiden and AC/DC from the same country?”, which re-
3This is achieved by manually curating lists from the Wikipedia “List of lists of lists” (https://wiki.sh/ y8qv).",2 Data Collection,[0],[0]
"One example is “Highest Mountains on Earth”.
",2 Data Collection,[0],[0]
"Algorithm 1 Overall data collection procedure Input: question type ratio r1 = 0.75, yes/no ratio r2 = 0.5 while not finished do
if random() <",2 Data Collection,[0],[0]
"r1 then Uniformly sample an entity b ∈ B Uniformly sample an edge (a, b) Workers ask a question about paragraphs a and b else Sample a list from L, with probabilities weighted by list sizes Uniformly sample two entities (a, b) from the list if random() < r2 then
Workers ask a yes/no question to compare a and b else Workers ask a question with a span answer to compare a and b
end if end if Workers provide the supporting facts
end while
quires reasoning over both paragraphs.",2 Data Collection,[0],[0]
"To the best of our knowledge, text-based comparison questions are a novel type of questions that have not been considered by previous datasets.",2 Data Collection,[0],[0]
"More importantly, answering these questions usually requires arithmetic comparison, such as comparing ages given birth dates, which presents a new challenge for future model development.
",2 Data Collection,[0],[0]
Collecting Supporting Facts.,2 Data Collection,[0],[0]
"To enhance the explainability of question answering systems, we want them to output a set of supporting facts necessary to arrive at the answer, when the answer is generated.",2 Data Collection,[0],[0]
"To this end, we also collect the sentences that determine the answers from crowd workers.",2 Data Collection,[0],[0]
These supporting facts can serve as strong supervision for what sentences to pay attention to.,2 Data Collection,[0],[0]
"Moreover, we can now test the explainability of a model by comparing the predicted supporting facts to the ground truth ones.
",2 Data Collection,[0],[0]
The overall procedure of data collection is illustrated in Algorithm 1.,2 Data Collection,[0],[0]
"We collected 112,779 valid examples in total on Amazon Mechanical Turk4 using the ParlAI interface (Miller et al., 2017) (see Appendix A).To isolate potential single-hop questions from the desired multi-hop ones, we first split out a subset of data called train-easy.",3 Processing and Benchmark Settings,[0],[0]
"Specifically, we randomly sampled questions (∼3–10 per Turker) from top-contributing turkers, and categorized all
4https://www.mturk.com/
their questions into the train-easy set if an overwhelming percentage in the sample only required reasoning over one of the paragraphs.",3 Processing and Benchmark Settings,[0],[0]
We sampled these turkers because they contributed more than 70% of our data.,3 Processing and Benchmark Settings,[0],[0]
"This train-easy set contains 18,089 mostly single-hop examples.
",3 Processing and Benchmark Settings,[0],[0]
"We implemented a question answering model based on the current state-of-the-art architectures, which we discuss in detail in Section 5.1.",3 Processing and Benchmark Settings,[0],[0]
"Based on this model, we performed a three-fold cross validation on the remaining multi-hop examples.",3 Processing and Benchmark Settings,[0],[0]
"Among these examples, the models were able to correctly answer 60% of the questions with high confidence (determined by thresholding the model loss).",3 Processing and Benchmark Settings,[0],[0]
"These correctly-answered questions (56,814 in total, 60% of the multi-hop examples) are split out and marked as the train-medium subset, which will also be used as part of our training set.
",3 Processing and Benchmark Settings,[0],[0]
"After splitting out train-easy and train-medium, we are left with hard examples.",3 Processing and Benchmark Settings,[0],[0]
"As our ultimate goal is to solve multi-hop question answering, we focus on questions that the latest modeling techniques are not able to answer.",3 Processing and Benchmark Settings,[0],[0]
Thus we constrain our dev and test sets to be hard examples.,3 Processing and Benchmark Settings,[0],[0]
"Specifically, we randomly divide the hard examples into four subsets, train-hard, dev, test-distractor, and test-fullwiki.",3 Processing and Benchmark Settings,[0],[0]
Statistics about the data split can be found in Table 1.,3 Processing and Benchmark Settings,[0],[0]
"In Section 5, we will show that combining train-easy, train-medium, and trainhard to train models yields the best performance, so we use the combined set as our default training set.",3 Processing and Benchmark Settings,[0],[0]
"The two test sets test-distractor and testfullwiki are used in two different benchmark settings, which we introduce next.
",3 Processing and Benchmark Settings,[0],[0]
We create two benchmark settings.,3 Processing and Benchmark Settings,[0],[0]
"In the first setting, to challenge the model to find the true supporting facts in the presence of noise, for each example we employ bigram tf-idf (Chen et al., 2017)
to retrieve 8 paragraphs from Wikipedia as distractors, using the question as the query.",3 Processing and Benchmark Settings,[0],[0]
We mix them with the 2 gold paragraphs (the ones used to collect the question and answer) to construct the distractor setting.,3 Processing and Benchmark Settings,[0],[0]
The 2 gold paragraphs and the 8 distractors are shuffled before they are fed to the model.,3 Processing and Benchmark Settings,[0],[0]
"In the second setting, we fully test the model’s ability to locate relevant facts as well as reasoning about them by requiring it to answer the question given the first paragraphs of all Wikipedia articles without the gold paragraphs specified.",3 Processing and Benchmark Settings,[0],[0]
"This full wiki setting truly tests the performance of the systems’ ability at multi-hop reasoning in the wild.5 The two settings present different levels of difficulty, and would require techniques ranging from reading comprehension to information retrieval.",3 Processing and Benchmark Settings,[0],[0]
"As shown in Table 1, we use separate test sets for the two settings to avoid leaking information, because the gold paragraphs are available to a model in the distractor setting, but should not be accessible in the full wiki setting.
",3 Processing and Benchmark Settings,[0],[0]
We also try to understand the model’s good performance on the train-medium split.,3 Processing and Benchmark Settings,[0],[0]
"Manual analysis shows that the ratio of multi-hop questions in train-medium is similar to that of the hard examples (93.3% in train-medium vs. 92.0% in dev), but one of the question types appears more frequently in train-medium compared to the hard splits (Type II: 32.0% in train-medium vs. 15.0% in dev, see Section 4 for the definition of Type II questions).",3 Processing and Benchmark Settings,[0],[0]
"These observations demonstrate that given enough training data, existing neural architectures can be trained to answer certain types and certain subsets of the multi-hop questions.",3 Processing and Benchmark Settings,[0],[0]
"However, train-medium remains challenging when not just the gold paragraphs are present—we show in Appendix C that the retrieval problem on these examples are as difficult as that on their hard cousins.",3 Processing and Benchmark Settings,[0],[0]
"In this section, we analyze the types of questions, types of answers, and types of multi-hop reasoning covered in the dataset.
",4 Dataset Analysis,[0],[0]
Question Types.,4 Dataset Analysis,[0],[0]
We heuristically identified question types for each collected question.,4 Dataset Analysis,[0],[0]
"To identify the question type, we first locate the central question word (CQW) in the question.",4 Dataset Analysis,[0],[0]
"Since HOTPOTQA contains comparison questions and
5As we required the crowd workers to use complete entity names in the question, the majority of the questions are unambiguous in the full wiki setting.
",4 Dataset Analysis,[0],[0]
"yes/no questions, we consider as question words WH-words, copulas (“is”, “are”), and auxiliary verbs (“does”, “did”).",4 Dataset Analysis,[0],[0]
"Because questions often involve relative clauses beginning with WH-words, we define the CQW as the first question word in the question if it can be found in the first three tokens, or the last question word otherwise.",4 Dataset Analysis,[0],[0]
"Then, we determine question type by extracting words up to 2 tokens away to the right of the CQW, along with the token to the left if it is one of a few common prepositions (e.g., in the cases of “in which” and “by whom”).
",4 Dataset Analysis,[0],[0]
"We visualize the distribution of question types in Figure 2, and label the ones shared among more than 250 questions.",4 Dataset Analysis,[0],[0]
"As is shown, our dataset covers a diverse variety of questions centered around entities, locations, events, dates, and numbers, as well as yes/no questions directed at comparing two entities (“Are both A and B ...?”), to name a few.
",4 Dataset Analysis,[0],[0]
Answer Types.,4 Dataset Analysis,[0],[0]
"We further sample 100 examples from the dataset, and present the types of answers in Table 2.",4 Dataset Analysis,[0],[0]
"As can be seen, HOTPOTQA covers a broad range of answer types, which matches our initial analysis of question types.",4 Dataset Analysis,[0],[0]
"We find that a majority of the questions are about entities in the articles (68%), and a non-negligible amount of questions also ask about various properties like date (9%) and other descriptive properties such as numbers (8%) and adjectives (4%).
",4 Dataset Analysis,[0],[0]
Multi-hop Reasoning Types.,4 Dataset Analysis,[0],[0]
We also sampled 100 examples from the dev and test sets and manually classified the types of reasoning required to answer each question.,4 Dataset Analysis,[0],[0]
"Besides comparing two entities, there are three main types of multi-hop reasoning required to answer these questions, which we show in Table 3 accompanied with examples.
",4 Dataset Analysis,[0],[0]
Most of the questions require at least one supporting fact from each paragraph to answer.,4 Dataset Analysis,[0],[0]
"A majority of sampled questions (42%) require chain reasoning (Type I in the table), where the reader must first identify a bridge entity before the second hop can be answered by filling in the bridge.",4 Dataset Analysis,[0],[0]
One strategy to answer these questions would be to decompose them into consecutive single-hop questions.,4 Dataset Analysis,[0],[0]
The bridge entity could also be used implicitly to help infer properties of other entities related to it.,4 Dataset Analysis,[0],[0]
"In some questions (Type III), the entity in question shares certain properties with a bridge entity (e.g., they are collocated), and we can infer its properties through the bridge entity.",4 Dataset Analysis,[0],[0]
Another type of question involves locating the answer entity by satisfying multiple properties simultaneously (Type II).,4 Dataset Analysis,[0],[0]
"Here, to answer the question, one could find the set of all entities that satisfy each of the properties mentioned, and take an intersection to arrive at the final answer.",4 Dataset Analysis,[0],[0]
"Questions comparing two entities (Comparison) also require the system to understand the properties in question about the two entities (e.g., nationality), and sometimes require arithmetic such as counting (as seen in the table) or comparing numerical values (“Who is older, A or B?”).",4 Dataset Analysis,[0],[0]
"Finally, we find that sometimes the questions require more than two supporting facts to answer (Other).",4 Dataset Analysis,[0],[0]
"In our analysis, we also find that for all of the examples shown in the table, the supporting facts provided by the Turkers match exactly with the limited context shown here,
showing that the supporting facts collected are of high quality.
",4 Dataset Analysis,[0],[0]
"Aside from the reasoning types mentioned above, we also estimate that about 6% of the sampled questions can be answered with one of the two paragraphs, and 2% of them unanswerable.",4 Dataset Analysis,[0],[0]
"We also randomly sampled 100 examples from train-medium and train-hard combined, and the proportions of reasoning types are: Type I 38%, Type II 29%, Comparison 20%, Other 7%, Type III 2%, single-hop 2%, and unanswerable 2%.",4 Dataset Analysis,[0],[0]
"To test the performance of leading QA systems on our data, we reimplemented the architecture described in Clark and Gardner (2017) as our baseline model.",5.1 Model Architecture and Training,[0],[0]
We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1).,5.1 Model Architecture and Training,[0],[0]
"Our implemented model subsumes the latest techni-
cal advances on question answering, including character-level models, self-attention (Wang et al., 2017), and bi-attention (Seo et al., 2017).",5.1 Model Architecture and Training,[0],[0]
"Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures (Liu et al., 2018; Clark and Gardner, 2017; Wang et al., 2017; Seo et al., 2017; Pan et al., 2017; Salant and Berant, 2018; Xiong et al., 2018) on SQuAD can be viewed as similar to our implemented model.",5.1 Model Architecture and Training,[0],[0]
"To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of “yes”, “no”, and span-based answers.",5.1 Model Architecture and Training,[0],[0]
"During decoding, we first use the 3-way output to determine whether the answer is “yes”, “no”, or a text span.",5.1 Model Architecture and Training,[0],[0]
"If it is a text span, we further search for the most probable span.
",5.1 Model Architecture and Training,[0],[0]
Supporting Facts as Strong Supervision.,5.1 Model Architecture and Training,[0],[0]
"To evaluate the baseline model’s performance in predicting explainable supporting facts, as well as how much they improve QA performance, we additionally design a component to incorporate
such strong supervision into our model.",5.1 Model Architecture and Training,[0],[0]
"For each sentence, we concatenate the output of the selfattention layer at the first and last positions, and use a binary linear classifier to predict the probability that the current sentence is a supporting fact.",5.1 Model Architecture and Training,[0],[0]
We minimize a binary cross entropy loss for this classifier.,5.1 Model Architecture and Training,[0],[0]
"This objective is jointly optimized with the normal question answering objective in a multi-task learning setting, and they share the same low-level representations.",5.1 Model Architecture and Training,[0],[0]
"With this classifier, the model can also be evaluated on the task of supporting fact prediction to gauge its explainability.",5.1 Model Architecture and Training,[0],[0]
Our overall architecture is illustrated in Figure 3.,5.1 Model Architecture and Training,[0],[0]
"Though it is possible to build a pipeline system, in this work we focus on an end-to-end one, which is easier to tune and faster to train.",5.1 Model Architecture and Training,[0],[0]
We evaluate our model in the two benchmark settings.,5.2 Results,[0],[0]
"In the full wiki setting, to enable efficient tfidf retrieval among 5,000,000+ wiki paragraphs, given a question we first return a candidate pool of at most 5,000 paragraphs using an inverted-indexbased filtering strategy6 and then select the top 10 paragraphs in the pool as the final candidates using bigram tf-idf.7 Retrieval performance is shown in
6See Appendix C for details.",5.2 Results,[0],[0]
"7We choose the number of final candidates as 10 to stay consistent with the distractor setting where candidates are 2
Table 5.",5.2 Results,[0],[0]
"After retrieving these 10 paragraphs, we then use the model trained in the distractor setting to evaluate its performance on these final candidate paragraphs.
",5.2 Results,[0],[0]
"Following previous work (Rajpurkar et al., 2016), we use exact match (EM) and F1 as two evaluation metrics.",5.2 Results,[0],[0]
"To assess the explainability of the models, we further introduce two sets of metrics involving the supporting facts.",5.2 Results,[0],[0]
"The first set focuses on evaluating the supporting facts directly, namely EM and F1 on the set of supporting fact sentences as compared to the gold set.",5.2 Results,[0],[0]
The second set features joint metrics that combine the evaluation of answer spans and supporting facts as follows.,5.2 Results,[0],[0]
"For each example, given its precision and recall on the answer span (P (ans), R(ans)) and the supporting facts (P (sup), R(sup)), respectively, we calculate joint F1 as
P (joint) = P (ans)P (sup), R(joint) = R(ans)R(sup),
Joint F1 = 2P (joint)R(joint)
P (joint) +R(joint) .
",5.2 Results,[0],[0]
Joint EM is 1 only if both tasks achieve an exact match and otherwise 0.,5.2 Results,[0],[0]
"Intuitively, these metrics penalize systems that perform poorly on either task.",5.2 Results,[0],[0]
"All metrics are evaluated example-byexample, and then averaged over examples in the evaluation set.
",5.2 Results,[0],[0]
"The performance of our model on the benchmark settings is reported in Table 4, where all numbers are obtained with strong supervision over supporting facts.",5.2 Results,[0],[0]
"From the distractor setting to the full wiki setting, expanding the scope of the context increases the difficulty of question answering.",5.2 Results,[0],[0]
"The performance in the full wiki setting is substantially lower, which poses a challenge to existing techniques on retrieval-based question answering.",5.2 Results,[0],[0]
"Overall, model performance in all settings is significantly lower than human performance as shown in Section 5.3, which indicates that more technical advancements are needed in future work.
",5.2 Results,[0],[0]
We also investigate the explainability of our model by measuring supporting fact prediction performance.,5.2 Results,[0],[0]
"Our model achieves 60+ supporting fact prediction F1 and ∼40 joint F1, which indicates there is room for further improvement in terms of explainability.
",5.2 Results,[0],[0]
"In Table 6, we break down the performance on different question types.",5.2 Results,[0],[0]
"In the distractor setting, comparison questions have lower F1 scores
gold paragraphs plus 8 distractors.
than questions involving bridge entities (as defined in Section 2), which indicates that better modeling this novel question type might need better neural architectures.",5.2 Results,[0],[0]
"In the full wiki setting, the performance of bridge entity questions drops significantly while that of comparison questions decreases only marginally.",5.2 Results,[0],[0]
"This is because both entities usually appear in the comparison questions, and thus reduces the difficulty of retrieval.",5.2 Results,[0],[0]
"Combined with the retrieval performance in Table 5, we believe that the deterioration in the full wiki setting in Table 4 is largely due to the difficulty of retrieving both entities.
",5.2 Results,[0],[0]
"We perform an ablation study in the distractor setting, and report the results in Table 7.",5.2 Results,[0],[0]
"Both selfattention and character-level models contribute notably to the final performance, which is consistent with prior work.",5.2 Results,[0],[0]
This means that techniques targeted at single-hop QA are still somewhat effective in our setting.,5.2 Results,[0],[0]
"Moreover, removing strong supervision over supporting facts decreases performance, which demonstrates the effectiveness of our approach and the usefulness of the supporting facts.",5.2 Results,[0],[0]
"We establish an estimate of the upper bound of strong supervision by only considering the supporting facts as the oracle context input to our
model, which achieves a 10+ F1 improvement over not using the supporting facts.",5.2 Results,[0],[0]
"Compared with the gain of strong supervision in our model (∼2 points in F1), our proposed method of incorporating supporting facts supervision is most likely suboptimal, and we leave the challenge of better modeling to future work.",5.2 Results,[0],[0]
"At last, we show that combining all data splits (train-easy, train-medium, and train-hard) yields the best performance, which is adopted as the default setting.",5.2 Results,[0],[0]
"To establish human performance on our dataset, we randomly sampled 1,000 examples from the dev and test sets, and had at least three additional Turkers provide answers and supporting facts for these examples.",5.3 Establishing Human Performance,[0],[0]
"As a baseline, we treat the original Turker during data collection as the prediction, and the newly collected answers and supporting facts as references, to evaluate human performance.",5.3 Establishing Human Performance,[0],[0]
"For each example, we choose the answer and supporting fact reference that maximize the F1 score to report the final metrics to reduce the effect of ambiguity (Rajpurkar et al., 2016).
",5.3 Establishing Human Performance,[0],[0]
"As can be seen in Table 8, the original crowd worker achieves very high performance in both finding supporting facts, and answering the question correctly.",5.3 Establishing Human Performance,[0],[0]
"If the baseline model were provided with the correct supporting paragraphs to begin with, it achieves parity with the crowd worker in finding supporting facts, but still falls short at finding the actual answer.",5.3 Establishing Human Performance,[0],[0]
"When distractor paragraphs are present, the performance gap between the baseline model and the crowd worker on both tasks is enlarged to ∼30% for both EM and F1.
",5.3 Establishing Human Performance,[0],[0]
"We further establish the upper bound of human performance in HOTPOTQA, by taking the maximum EM and F1 for each example.",5.3 Establishing Human Performance,[0],[0]
"Here, we use each Turker’s answer in turn as the prediction, and evaluate it against all other workers’ answers.",5.3 Establishing Human Performance,[0],[0]
"As can be seen in Table 8, most of the metrics are close to 100%, illustrating that on most examples, at least a subset of Turkers agree with each other, showing high inter-annotator agreement.",5.3 Establishing Human Performance,[0],[0]
"We also note that crowd workers agree less on supporting facts, which could reflect that this task is inherently more subjective than answering the question.",5.3 Establishing Human Performance,[0],[0]
"Various recently-proposed large-scale QA datasets can be categorized in four categories.
",6 Related Work,[0],[0]
Single-document datasets.,6 Related Work,[0],[0]
"SQuAD (Rajpurkar et al., 2016, 2018) questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer.
",6 Related Work,[0],[0]
Multi-document datasets.,6 Related Work,[0],[0]
"TriviaQA (Joshi et al., 2017) and SearchQA",6 Related Work,[0],[0]
"(Dunn et al., 2017) contain question answer pairs that are accompanied with more than one document as the context.",6 Related Work,[0],[0]
This further challenges QA systems’ ability to accommodate longer contexts.,6 Related Work,[0],[0]
"However, since the
supporting documents are collected after the question answer pairs with information retrieval, the questions are not guaranteed to involve interesting reasoning between multiple documents.
",6 Related Work,[0],[0]
KB-based multi-hop datasets.,6 Related Work,[0],[0]
"Recent datasets like QAngaroo (Welbl et al., 2018) and COMPLEXWEBQUESTIONS (Talmor and Berant, 2018) explore different approaches of using pre-existing knowledge bases (KB) with pre-defined logic rules to generate valid QA pairs, to test QA models’ capability of performing multi-hop reasoning.",6 Related Work,[0],[0]
The diversity of questions and answers is largely limited by the fixed KB schemas or logical forms.,6 Related Work,[0],[0]
"Furthermore, some of the questions might be answerable by one text sentence due to the incompleteness of KBs.
",6 Related Work,[0],[0]
Free-form answer-generation datasets.,6 Related Work,[0],[0]
MS MARCO,6 Related Work,[0],[0]
"(Nguyen et al., 2016) contains 100k user queries from Bing Search with human generated answers.",6 Related Work,[0],[0]
Systems generate free-form answers and are evaluated by automatic metrics such as ROUGE-L and BLEU-1.,6 Related Work,[0],[0]
"However, the reliability of these metrics is questionable because they have been shown to correlate poorly with human judgement (Novikova et al., 2017).",6 Related Work,[0],[0]
"We present HOTPOTQA, a large-scale question answering dataset aimed at facilitating the development of QA systems capable of performing explainable, multi-hop reasoning over diverse natural language.",7 Conclusions,[0],[0]
We also offer a new type of factoid comparison questions to test systems’ ability to extract and compare various entity properties in text.,7 Conclusions,[0],[0]
This work is partly funded by the Facebook ParlAI Research Award.,Acknowledgements,[0],[0]
"ZY, WWC, and RS are supported by a Google grant, the DARPA grant D17AP00001, the ONR grants N000141512791, N000141812861, and the Nvidia NVAIL Award.",Acknowledgements,[0],[0]
"SZ and YB are supported by Mila, Université de Montréal.",Acknowledgements,[0],[0]
PQ and CDM are supported by the National Science Foundation under Grant No. IIS1514268.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgements,[0],[0]
A.1 Data,A Data Collection Details,[0],[0]
"Preprocessing We downloaded the dump of English Wikipedia of October 1, 2017, and extracted text and hyperlinks with WikiExtractor.8 We use Stanford CoreNLP 3.8.0",A Data Collection Details,[0],[0]
"(Manning et al., 2014) for word and sentence tokenization.",A Data Collection Details,[0],[0]
"We use the resulting sentence boundaries for collection of supporting facts, and use token boundaries to check whether Turkers are providing answers that cover spans of entire tokens to avoid nonsensical partial-word answers.
",A Data Collection Details,[0],[0]
A.2 Further Data Collection Details Details on Curating Wikipedia Pages.,A Data Collection Details,[0],[0]
"To make sure the sampled candidate paragraph pairs are intuitive for crowd workers to ask high-quality multi-hop questions about, we manually curate 591 categories from the lists of popular pages by WikiProject.9 For each category, we sample (a, b) pairs from the graph G where b is in the considered category, and manually check whether a multi-hop question can be asked given the pair (a, b).",A Data Collection Details,[0],[0]
"Those categories with a high probability of permitting multi-hop questions are selected.
",A Data Collection Details,[0],[0]
Bonus Structures.,A Data Collection Details,[0],[0]
"To incentivize crowd workers to produce higher-quality data more efficiently, we follow Yang et al. (2018), and employ bonus structures.",A Data Collection Details,[0],[0]
We mix two settings in our data collection process.,A Data Collection Details,[0],[0]
"In the first setting, we reward the top (in terms of numbers of examples) workers every 200 examples.",A Data Collection Details,[0],[0]
"In the second setting, the workers get bonuses based on their productivity (measured as the number of examples per hour).
",A Data Collection Details,[0],[0]
A.3 Crowd Worker Interface,A Data Collection Details,[0],[0]
"Our crowd worker interface is based on ParlAI (Miller et al., 2017), an open-source project that facilitates the development of dialog systems and data collection with a dialog interface.",A Data Collection Details,[0],[0]
We adapt ParlAI for collecting question answer pairs by converting the collection workflow into a systemoriented dialog.,A Data Collection Details,[0],[0]
"This allows us to have more control over the turkers input, as well as provide turkers with in-the-loop feedbacks or helpful hints to help Turkers finish the task, and therefore speed up the collection process.
",A Data Collection Details,[0],[0]
"Please see Figure 4 for an example of the worker interface during data collection.
",A Data Collection Details,[0],[0]
"8https://github.com/attardi/ wikiextractor
9https://wiki.sh/y8qu",A Data Collection Details,[0],[0]
"To further look into the diversity of the data in HOTPOTQA, we further visualized the distribution of question lengths in the dataset in Figure 5.",B Further Data Analysis,[0],[0]
"Besides being diverse in terms of types as is show in the main text, questions also vary greatly in length, indicating different levels of complexity and details covered.",B Further Data Analysis,[0],[0]
"C.1 The Inverted Index Filtering Strategy
In the full wiki setting, we adopt an efficient inverted-index-based filtering strategy for preliminary candidate paragraph retrieval.",C Full Wiki Setting Details,[0],[0]
"We provide details in Algorithm 2, where we set the control threshold N = 5000 in our experiments.",C Full Wiki Setting Details,[0],[0]
"For some of the question q, its corresponding gold para-
Algorithm 2 Inverted Index Filtering Strategy Input: question text q, control threshold N , ngram-toWikidoc inverted index D Inintialize: Extract unigram + bigram set rq from q Ncand = +∞ Cgram = 0 while Ncands > N do
Cgram = Cgram + 1 Set Soverlap to be an empty dictionary for w ∈ rq do
for d ∈ D[w] do if d not in Soverlap then
Soverlap[d] = 1 else
Soverlap[d] = Soverlap[d] + 1 end if
end for end for Scand = ∅ for d in Soverlap do
if Soverlap[d] ≥ Cgram then",C Full Wiki Setting Details,[0],[0]
Scand =,C Full Wiki Setting Details,[0],[0]
"Scand ∪ {d}
end if end for Ncands = |Scand|
end while return Scand
graphs may not be included in the output candidate pool Scand, we set such missing gold paragraph’s rank as |Scand|+1 during the evaluation, so MAP and Mean Rank reported in this paper are upper bounds of their true values.
",C Full Wiki Setting Details,[0],[0]
"C.2 Compare train-medium Split to Hard Ones
Table 9 shows the comparison between trainmedium split and hard examples like dev and test under retrieval metrics in full wiki setting.",C Full Wiki Setting Details,[0],[0]
"As we can see, the performance gap between trainmedium split and its dev/test is close, which implies that train-medium split has a similar level of difficulty as hard examples under the full wiki setting in which a retrieval model is necessary as the first processing step.",C Full Wiki Setting Details,[0],[0]
Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers.,abstractText,[0],[0]
"We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison.",abstractText,[0],[0]
"We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",abstractText,[0],[0]
"HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",title,[0],[0]
"The covariance matrix C of an n-dimensional distribution is an integral part of data analysis, with numerous occurrences in machine learning and signal processing.",1 Introduction,[0],[0]
"It is therefore crucial to understand how close is the sample covariance, i.e., the matrix C̃ estimated from a finite number of samples m, to the actual covariance matrix.",1 Introduction,[0],[0]
"Following developments in the tools for the concentration of measure, (Vershynin, 2012) showed that a sample size of m = O(n) is up to iterated logarithmic factors sufficient for all distributions with finite fourth moment supported in a centered Euclidean ball of radius O( √ n).",1 Introduction,[0],[0]
"Similar results hold for sub-exponential (Adamczak et al., 2010) and finite second moment distributions (Rudelson, 1999).
",1 Introduction,[0],[0]
"We take an alternative standpoint and ask if we can do
1École Polytechnique Fédérale de Lausanne, Switzerland.",1 Introduction,[0],[0]
"Correspondence to: Andreas Loukas<andreas.loukas@epfl.ch>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
better when only a subset of the spectrum is of interest.",1 Introduction,[0],[0]
"Concretely, our objective is to characterize how many samples are sufficient to guarantee that an eigenvector and/or eigenvalue of the sample and actual covariance matrices are, respectively, sufficiently close.",1 Introduction,[0],[0]
Our approach is motivated by the observation that methods that utilize the covariance commonly prioritize the estimation of principal eigenspaces.,1 Introduction,[0],[0]
"For instance, in (local) principal component analysis we are usually interested in the direction of the first few eigenvectors (Berkmann & Caelli, 1994; Kambhatla & Leen, 1997), where in linear dimensionality reduction one projects the data to the span of the first few eigenvectors (Jolliffe, 2002; Frostig et al., 2016).",1 Introduction,[0],[0]
"In the nonasymptotic regime, an analysis of these methods hinges on characterizing how close are the principal eigenvectors and eigenspaces of the sample and actual covariance matrices.
",1 Introduction,[0],[0]
Our finding is that the “spectral leaking” occurring in the eigenvector estimation is strongly concentrated along the eigenvalue axis.,1 Introduction,[0],[0]
"In other words, the eigenvector ũi of the sample covariance is far less likely to lie in the span of an eigenvector uj of the actual covariance when the eigenvalue distance |λi − λj | is large, and the concentration of the distribution in the direction of uj is small.",1 Introduction,[0],[0]
"This agrees with the intuition that principal components are easier to estimate, exactly because they are more likely to appear in the samples of the distribution.
",1 Introduction,[0],[0]
We provide a mathematical argument confirming this phenomenon.,1 Introduction,[0],[0]
"Under fairly general conditions, we prove that
m = O
( k2j
(λi − λj)2
) and m = O ( k2i λ2i ) (1)
samples are asymptotically almost surely (a.a.s.)",1 Introduction,[0],[0]
"sufficient to guarantee that |〈ũi, uj〉| and |λ̃i−λi|/λi, respectively, is small for all distributions with finite second moment.",1 Introduction,[0],[0]
"Here, k2j is a measure of the concentration of the distribution in the direction of uj .",1 Introduction,[0],[0]
We also attain a high probability bound for sub-gaussian distributions supported in a centered Euclidean ball.,1 Introduction,[0],[0]
"Interestingly, our results lead to sample estimates for linear dimensionality reduction, and suggest that linear reduction is feasible even from few samples.
",1 Introduction,[0],[0]
"To the best of our knowledge, these are the first nonasymptotic results concerning the eigenvectors of the sam-
ple covariance of non-Normal distributions.",1 Introduction,[0],[0]
"Previous studies have intensively investigated the limiting distribution of the eigenvalues of a sample covariance matrix (Silverstein & Bai, 1995; Bai, 1999), such as the smallest and largest eigenvalues (Bai & Yin, 1993) and the eigenvalue support (Bai & Silverstein, 1998).",1 Introduction,[0],[0]
"Eigenvectors and eigenprojections have attracted less attention; the main research thrust entails using tools from the theory of large-dimensional matrices to characterize limiting distributions (Anderson, 1963; Girko, 1996; Schott, 1997; Bai et al., 2007) and it has limited applicability in the nonasymptotic setting where the sample size m is small and n cannot be arbitrary large.
",1 Introduction,[0],[0]
"Differently, we use techniques from perturbation analysis and non-asymptotic concentration of measure.",1 Introduction,[0],[0]
"However, in contrast to arguments commonly used to reason about eigenspaces (Davis & Kahan, 1970; Yu et al., 2015; Huang et al., 2009; Hunter & Strohmer, 2010), our bounds can characterize weighted linear combinations of 〈ũi, uj〉2 over i and j, and do not depend on the minimal eigenvalue gap separating two eigenspaces but rather on all eigenvalue differences.",1 Introduction,[0],[0]
"The latter renders them useful to many real datasets, where the eigenvalue gap is not significant but the eigenvalue magnitudes decrease sufficiently fast.
",1 Introduction,[0],[0]
We also note two recent works targeting the nonassymptotic regime of Normal distributions.,1 Introduction,[0],[0]
"Shaghaghi and Vorobyov recently characterized the first two moments of the subspace projection error, a result which implies sample estimates (Shaghaghi & Vorobyov, 2015), but is restricted to specific projectors.",1 Introduction,[0],[0]
"A refined concentration analysis for spectral projectors of Normal distributions was also presented in (Koltchinskii & Lounici, 2015).",1 Introduction,[0],[0]
"Finally, we remark that there exist alternative estimators for the spectrum of the covariance with better asymptotic properties (Ahmed, 1998; Mestre, 2008).",1 Introduction,[0],[0]
"Instead, we here focus on the standard estimates, i.e., the eigenvalues and eigenvectors of the sample covariance.",1 Introduction,[0],[0]
"Let x ∈ Cn be a sample of a multivariate distribution and denote by x1, x2, . . .",2 Problem Statement and Main Results,[0],[0]
", xm the m independent samples used
to form the sample covariance, defined as
C̃ = m∑ p=1 (xp − x̄)(xp",2 Problem Statement and Main Results,[0],[0]
"− x̄)∗ m , (2)
where x̄ is the sample mean.",2 Problem Statement and Main Results,[0],[0]
"Denote by ui the eigenvector of C associated with eigenvalue λi, and correspondingly for the eigenvectors ũi and eigenvalues λ̃i of C̃, such that λ1 ≥ λ2 ≥ . . .",2 Problem Statement and Main Results,[0],[0]
≥ λn.,2 Problem Statement and Main Results,[0],[0]
"We ask:
Problem 1.",2 Problem Statement and Main Results,[0],[0]
"How many samples are sufficient to guarantee that the inner product |〈ũi, uj〉| = |ũ∗i uj | and the eigenvalue gap |δλi| = |λ̃i",2 Problem Statement and Main Results,[0],[0]
"− λi| is smaller than some constant t with probability larger than ?
",2 Problem Statement and Main Results,[0],[0]
"Clearly, when asking that all eigenvectors and eigenvalues of the sample and actual covariance matrices are close, we will require at least as many samples as needed to ensure that ‖C̃",2 Problem Statement and Main Results,[0],[0]
− C‖2 ≤ t.,2 Problem Statement and Main Results,[0],[0]
"However, we might do better when only a subset of the spectrum is of interest.",2 Problem Statement and Main Results,[0],[0]
"The reason is that inner products |〈ũi, uj〉| are strongly concentrated along the eigenvalue axis.",2 Problem Statement and Main Results,[0],[0]
"To illustrate this phenomenon, let us consider the distribution constructed by the n = 784 pixel values of digit ‘1’ in the MNIST database.",2 Problem Statement and Main Results,[0],[0]
"Figure 1, compares the eigenvectors uj of the covariance computed from all 6742 images, to the eigenvectors ũi of the sample covariance matrices C̃ computed from a random subset of m = 10, 100, 500, and 1000 samples.",2 Problem Statement and Main Results,[0],[0]
"For each i = 1, 4, 20, 100, we depict at λj the average of |〈ũi, uj〉| over 100 sampling draws.",2 Problem Statement and Main Results,[0],[0]
"We observe that: (i) The magnitude of 〈ũi, uj〉 is inversely proportional to their eigenvalue gap |λi−λj |.",2 Problem Statement and Main Results,[0],[0]
"(ii) Eigenvector ũj mostly lies in the span of eigenvectors uj over which the distribution is concentrated.
",2 Problem Statement and Main Results,[0],[0]
We formalize these statements in two steps.,2 Problem Statement and Main Results,[0],[0]
"First, we work in the setting of Hermitian matrices and notice the following inequality:
Theorem 3.2.",2.1 Perturbation arguments,[0],[0]
"For Hermitian matricesC and C̃ = δC+C, with eigenvectors uj and ũi respectively, the inequality
|〈ũi, uj〉| ≤ 2 ‖δCuj‖2",2.1 Perturbation arguments,[0],[0]
"|λi − λj | ,
holds for sgn(λi − λj) 2λ̃i >",2.1 Perturbation arguments,[0],[0]
sgn(λi,2.1 Perturbation arguments,[0],[0]
"− λj)(λi + λj) and λi 6= λj .
",2.1 Perturbation arguments,[0],[0]
"The above stands out from standard eigenspace perturbation results, such as the sin(Θ) Theorem (Davis & Kahan, 1970) and its variants (Huang et al., 2009; Hunter & Strohmer, 2010; Yu et al., 2015) for three main reasons:
First, Theorem 3.2 characterizes the angle between any pair of eigenvectors allowing us to jointly bound any linear combination of inner-products.",2.1 Perturbation arguments,[0],[0]
"Though this often proves handy (c.f. Section 5), it is infeasible using sin(Θ)-type arguments.",2.1 Perturbation arguments,[0],[0]
"Second, classical bounds are not appropriate for a probabilistic analysis as they feature ratios of dependent random variables (corresponding to perturbation terms).",2.1 Perturbation arguments,[0],[0]
"In the analysis of spectral clustering, this complication was dealt with by assuming that |λi − λj | ≤ |λ̃i",2.1 Perturbation arguments,[0],[0]
"− λj | (Hunter & Strohmer, 2010).",2.1 Perturbation arguments,[0],[0]
We weaken this condition at a cost of a multiplicative factor.,2.1 Perturbation arguments,[0],[0]
"In contrast to previous work, we also prove that the condition is met a.a.s.",2.1 Perturbation arguments,[0],[0]
"Third, previous bounds are expressed in terms of the minimal eigenvalue gap between eigenvectors lying in the interior and exterior of the subspace of interest.",2.1 Perturbation arguments,[0],[0]
This is a limiting factor in practice as it renders the results only amenable to situations where there is a very large eigenvalue gap separating the subspaces.,2.1 Perturbation arguments,[0],[0]
The proposed result improves upon this by considering every eigenvalue difference.,2.1 Perturbation arguments,[0],[0]
The second part of our analysis focuses on the covariance and has a statistical flavor.,2.2 Concentration of measure,[0],[0]
"In particular, we give an answer to Problem 1 for various families of distributions.
",2.2 Concentration of measure,[0],[0]
"In the context of distributions with finite second moment, we prove in Section 4.1 that:
Theorem 4.1.",2.2 Concentration of measure,[0],[0]
"For any two eigenvectors ũi and uj of the sample and actual covariance respectively, and for any real number t > 0:
P(|〈ũi, uj〉| ≥ t) ≤ 1
m ( 2kj t |λi − λj | )2 , (3)
subject to the same conditions as Theorem 3.2.
",2.2 Concentration of measure,[0],[0]
"For eigenvalues, we have the following corollary:
Corollary 2.1.",2.2 Concentration of measure,[0],[0]
"For any eigenvalues λi and λ̃i of C and C̃, respectively, and for any t > 0, we have
P
( |λ̃i",2.2 Concentration of measure,[0],[0]
"− λi|
λi ≥ t )",2.2 Concentration of measure,[0],[0]
"≤ 1 m ( ki λi t )2 .
",2.2 Concentration of measure,[0],[0]
Term kj =,2.2 Concentration of measure,[0],[0]
"(E [ ‖xx∗uj‖22 ] − λ2j )1/2 captures the tendency of the distribution to fall in the span of uj : the smaller the tail in the direction of uj the less likely we are going to confuse ũi with uj .
",2.2 Concentration of measure,[0],[0]
"For normal distributions, we have that k2j = λ",2.2 Concentration of measure,[0],[0]
"2 j + λj tr(C) and the number of samples needed for |〈ũi, uj〉| to be small is m = O(tr(C)/λ2i )",2.2 Concentration of measure,[0],[0]
when λj = O(1) and m = O(λ −2,2.2 Concentration of measure,[0],[0]
i ) when λj = O(tr(C)−1).,2.2 Concentration of measure,[0],[0]
"Thus for normal distributions, principal components ui and uj with min{λi/λj , λi} = Ω(tr(C)1/2) can be distinguished given a constant number of samples.",2.2 Concentration of measure,[0],[0]
"On the other hand, estimating λi with small relative error requires m = O(tr(C)/λi) samples and can thus be achieved from very few samples when λi is large1.
",2.2 Concentration of measure,[0],[0]
"In Section 4.2, we also give a sharp bound for the family of distributions supported within a ball (i.e., ‖x‖ ≤ r a.s.).
",2.2 Concentration of measure,[0],[0]
Theorem 4.2.,2.2 Concentration of measure,[0],[0]
"For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c s.t. for any real number t > 0,
P(|〈ũi, uj〉| ≥ t) ≤ exp",2.2 Concentration of measure,[0],[0]
"( 1− cmΦij(t) 2
λj ‖x‖2Ψ2
) , (4)
where Φij(t) =",2.2 Concentration of measure,[0],[0]
"|λi−λj | t−2λj 2 (r2/λj−1)1/2
−2 ‖x‖Ψ2 and subject to the same conditions as Theorem 3.2.
",2.2 Concentration of measure,[0],[0]
"Above, ‖x‖Ψ2 is the sub-gaussian norm, for which we usually have ‖x‖Ψ2 = O(1) (Vershynin, 2010).",2.2 Concentration of measure,[0],[0]
"As such, the theorem implies that, whenever λi λj = O(1), the sample requirement is with high probability m = O(r2/λ2i ).
",2.2 Concentration of measure,[0],[0]
"These theorems solidify our experimental findings shown in Figure 1 and provide a concrete characterization of the relation between the spectrum of the sample and actual covariance matrix as a function of the number of samples, the eigenvalue gap, and the distribution properties.",2.2 Concentration of measure,[0],[0]
"As exemplified in Section 5 for linear dimensionality reduction, we believe that our results carry strong implications for the non-asymptotic analysis of PCA-based methods.",2.2 Concentration of measure,[0],[0]
"Before focusing on the sample covariance matrix, it helps to study 〈ũi, uj〉 in the setting of Hermitian matrices.",3 Perturbation Arguments,[0],[0]
The presentation of the results is split in three parts.,3 Perturbation Arguments,[0],[0]
"Section 3.1 starts by studying some basic properties of inner products of the form 〈ũi, uj〉, for any i and j. The results are used in Section 3.2 to provide a first bound on the angle between two eigenvectors, and refined in Section 3.3.",3 Perturbation Arguments,[0],[0]
"We start by noticing an exact relation between the angle of a perturbed eigenvector and the actual eigenvectors of C.
Lemma 3.1.",3.1 Basic observations,[0],[0]
"For every i and j in 1, 2, . . .",3.1 Basic observations,[0],[0]
", n, the relation (λ̃i − λj) (ũ∗i uj) = ∑n `=1(ũ ∗",3.1 Basic observations,[0],[0]
"i u`) (u ∗ jδCu`) holds .
",3.1 Basic observations,[0],[0]
"1Though the same cannot be stated about the absolute error |δλi|, that is smaller for small λi.
Proof.",3.1 Basic observations,[0],[0]
The proof follows from a modification of a standard argument in perturbation theory.,3.1 Basic observations,[0],[0]
"We start from the definition C̃ ũi = λ̃i ũi and write
(C + δC)",3.1 Basic observations,[0],[0]
(ui + δui) = (λi + δλi) (ui + δui).,3.1 Basic observations,[0],[0]
"(5)
Expanded, the above expression becomes
Cδui + δCui + δCδui
= λiδui + δλiui + δλiδui, (6)
where we used the fact that Cui = λiui to eliminate two terms.",3.1 Basic observations,[0],[0]
"To proceed, we substitute δui = ∑n j=1 βijuj , where βij = δu∗i uj , into (6) and multiply from the left by u∗j , resulting to:
n∑ `=1 βiju ∗ jCu",3.1 Basic observations,[0],[0]
"` + u ∗ jδCui + n∑ `=1 βiju ∗ jδCu`
= λi n∑ `=1 βiju ∗ ju` + δλiu ∗",3.1 Basic observations,[0],[0]
"jui + δλi n∑ `=1 βiju ∗ ju` (7)
Cancelling the unnecessary terms and rearranging, we have
δλiu ∗ jui",3.1 Basic observations,[0],[0]
"+ (λi + δλi − λj)βij
= u∗jδCui + n∑ `=1 βiju ∗ jδCu`.",3.1 Basic observations,[0],[0]
"(8)
At this point, we note that (λi + δλi − λj) = λ̃i",3.1 Basic observations,[0],[0]
− λj and furthermore that βij,3.1 Basic observations,[0],[0]
= ũ∗i uj − u∗i uj .,3.1 Basic observations,[0],[0]
"With this in place, equation (8) becomes
δλiu ∗",3.1 Basic observations,[0],[0]
jui +,3.1 Basic observations,[0],[0]
(,3.1 Basic observations,[0],[0]
"λ̃i − λj) (ũ∗i uj − u∗i uj)
= u∗jδCui + n∑ `=1 (ũ∗i u`)u ∗ jδCu` − u∗jδCui. (9)
",3.1 Basic observations,[0],[0]
"The proof completes by noticing that, in the left hand side, all terms other than (λ̃i − λj) ũ∗i uj fall-off, either due to u∗i uj = 0, when i 6= j, or because δλi = λ̃i−λj , o.w.
",3.1 Basic observations,[0],[0]
"As the expression reveals, 〈ũi, uj〉 depends on the orientation of ũi with respect to all other u`.",3.1 Basic observations,[0],[0]
"Moreover, the angles between eigenvectors depend not only on the minimal gap between the subspace of interest and its complement space (as in the sin(Θ) theorem), but on every difference λ̃i−λj .",3.1 Basic observations,[0],[0]
"This is a crucial ingredient to a tight bound, that will be retained throughout our analysis.",3.1 Basic observations,[0],[0]
"We proceed to decouple the inner products.
",3.2 Bounding arbitrary angles,[0],[0]
Theorem 3.1.,3.2 Bounding arbitrary angles,[0],[0]
"For any Hermitian matrices C and C̃ = δC +C, with eigenvectors uj and ũi respectively, we have that |λ̃i",3.2 Bounding arbitrary angles,[0],[0]
"− λj | |〈ũi, uj〉| ≤ ‖δC uj‖2.
",3.2 Bounding arbitrary angles,[0],[0]
Proof.,3.2 Bounding arbitrary angles,[0],[0]
"We rewrite Lemma 3.1 as
(λ̃i − λj)2(ũ∗i uj)2 = ( n∑ `=1 (ũ∗i u`) (u ∗ jδCu`) )2 .",3.2 Bounding arbitrary angles,[0],[0]
"(10)
We now use the Cauchy-Schwartz inequality
(λ̃i − λj)2(ũ∗i uj)2 ≤ n∑ `=1 (ũ∗i u`) 2 n∑ `=1 (u∗jδCu`) 2
= n∑ `=1 (u∗jδCu`) 2 = ‖δC uj‖22, (11)
where in the last step we exploited Lemma 3.2.",3.2 Bounding arbitrary angles,[0],[0]
The proof concludes by taking a square root at both sides of the inequality.,3.2 Bounding arbitrary angles,[0],[0]
Lemma 3.2.,3.2 Bounding arbitrary angles,[0],[0]
n∑̀ =1 (u∗jδCu`) 2 = ‖δC uj‖22.,3.2 Bounding arbitrary angles,[0],[0]
Proof.,3.2 Bounding arbitrary angles,[0],[0]
We first notice that u∗jδCu` is a scalar and equal to its transpose.,3.2 Bounding arbitrary angles,[0],[0]
"Moreover, δC is Hermitian as the difference of two Hermitian matrices.",3.2 Bounding arbitrary angles,[0],[0]
"We therefore have that n∑ `=1 (u∗jδCu`) 2 = n∑ `=1 u∗jδCu`u ∗ `δCuj
= u∗jδC n∑ `=1 (u`u ∗ ` )",3.2 Bounding arbitrary angles,[0],[0]
δCuj = u ∗ jδCδCuj,3.2 Bounding arbitrary angles,[0],[0]
"= ‖δCuj‖22,
matching our claim.",3.2 Bounding arbitrary angles,[0],[0]
"As a last step, we move all perturbation terms to the numerator, at the expense of a multiplicative constant factor.
Theorem 3.2.",3.3 Refinement,[0],[0]
"For Hermitian matricesC and C̃ = δC+C, with eigenvectors uj and ũi respectively, the inequality
|〈ũi, uj〉| ≤ 2 ‖δCuj‖2",3.3 Refinement,[0],[0]
"|λi − λj | ,
holds for sgn(λi − λj) 2λ̃i >",3.3 Refinement,[0],[0]
sgn(λi,3.3 Refinement,[0],[0]
"− λj)(λi + λj) and λi 6= λj .
",3.3 Refinement,[0],[0]
Proof.,3.3 Refinement,[0],[0]
"Adding and subtracting λi from the left side of the expression in Lemma 3.1 and from definition we have
(δλi + λi − λj) (ũ∗i uj) = n∑ `=1 (ũ∗i u`) (u ∗ jδCu`).",3.3 Refinement,[0],[0]
"(12)
",3.3 Refinement,[0],[0]
"For λi 6= λj , the above expression can be re-written as
|ũ∗i uj | =
∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`)− δλi (ũ∗i uj) ∣∣∣∣ |λi − λj |
≤ 2 max  ∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`)",3.3 Refinement,[0],[0]
"∣∣∣∣ |λi − λj | , |δλi| |ũ∗i uj | |λi − λj |  .",3.3 Refinement,[0],[0]
"(13)
Let us examine the right-hand side inequality carefully.",3.3 Refinement,[0],[0]
"Obviously, when the condition |λi − λj | ≤ 2 |δλi| is not met, the right clause of (13) is irrelevant.",3.3 Refinement,[0],[0]
"Therefore, for |δλi| < |λi − λj | /2",3.3 Refinement,[0],[0]
"the bound simplifies to
|ũ∗i uj | ≤ 2
∣∣∣∣ n∑̀ =1 (ũ∗i u`) (u ∗ jδCu`) ∣∣∣∣ |λi − λj | .",3.3 Refinement,[0],[0]
"(14)
Similar to the proof of Theorem 3.1, applying the CauchySchwartz inequality we have that
|ũ∗i uj | ≤ 2
√ n∑̀ =1 (ũ∗i u`) 2 n∑̀ =1 (u∗jδCu`) 2
|λi − λj | = 2 ‖δCuj‖2 |λi",3.3 Refinement,[0],[0]
"− λj | ,
(15)
where in the last step we used Lemma 3.2.",3.3 Refinement,[0],[0]
"To finish the proof we notice that, due to Theorem 3.2, whenever |λi − λj | ≤ |λ̃i",3.3 Refinement,[0],[0]
"− λj |, one has
|ũ∗i uj | ≤ ‖δC uj‖2 |λ̃i",3.3 Refinement,[0],[0]
− λj | ≤ ‖δC uj‖2 |λi − λj | < 2 ‖δCuj‖2,3.3 Refinement,[0],[0]
|λi,3.3 Refinement,[0],[0]
− λj | .,3.3 Refinement,[0],[0]
"(16)
Our bound therefore holds for the union of intervals |δλi| < |λi − λj | /2 and |λi− λj | ≤ |λ̃i−",3.3 Refinement,[0],[0]
"λj |, i.e., for λ̃i >",3.3 Refinement,[0],[0]
(λi + λj)/2 when λi > λj and for λ̃i < (λi + λj)/2 when λi < λj .,3.3 Refinement,[0],[0]
"This section builds on the perturbation results of Section 3 to characterize how far any inner product 〈ũi, uj〉 and eigenvalue λ̃i are from the ideal estimates.
",4 Concentration of Measure,[0],[0]
"Before proceeding, we remark on some simplifications employed in the following.",4 Concentration of Measure,[0],[0]
"W.l.o.g., we will assume that the mean E[x] is zero.",4 Concentration of Measure,[0],[0]
"In addition, we will assume the perspective of Theorem 3.2, for which the inequality sgn(λi− λj) 2λ̃i > sgn(λi−λj)(λi+λj) holds.",4 Concentration of Measure,[0],[0]
This event is shown to occur a.a.s.,4 Concentration of Measure,[0],[0]
"when the gap and the sample size are sufficiently large, but it is convenient to assume that it happens almost surely.",4 Concentration of Measure,[0],[0]
"In fact, removing this assumption is possible (see Section 4.1.2), but it is largely not pursued here as it leads to less elegant and sharp estimates.",4 Concentration of Measure,[0],[0]
"Our first flavor of results is based on a variant of the Tchebichef inequality and holds for any distribution with finite second moment, though only with moderate probability estimates.",4.1 Distributions with finite second moment,[0],[0]
"We start with the concentration of inner-products |〈ũi, uj〉|.
Theorem 4.1.",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"For any two eigenvectors ũi and uj of the sample and actual covariance respectively, with λi 6= λj , and for any real number t > 0, we have
P(|〈ũi, uj〉| ≥ t) ≤ 1
m ( 2 kj t |λi − λj | )2 for sgn(λi − λj) 2λ̃i >",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
sgn(λi,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
− λj)(λi + λj) and kj =( E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ ‖xx∗uj‖22 ],4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λ2j )1/2 .
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
Proof.,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"According to a variant of Tchebichef’s inequality (Sarwate, 2013), for any random variable X and for any real numbers t > 0 and α:
P(|X",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− α| ≥ t) ≤ Var[X] + (E[X]− α) 2
t2 .",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"(17)
Setting X = 〈ũi, uj〉 and α = 0, we have
P(|〈ũi, uj〉| ≥ t) ≤",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"Var[〈ũi, uj〉] + E[〈ũi, uj〉]2
t2 = E",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"[ 〈ũi, uj〉2 ]",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
t2 ≤ 4E [ ‖δCuj‖22 ] t2(λi,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λj)2 , (18)
where the last inequality follows from Theorem 3.2.",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"We continue by expanding δC using the definition of the eigenvalue decomposition and substituting the expectation.
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ ‖δCuj‖22 ] = E [ ‖C̃uj − λjuj‖22 ] = E [ u∗j (C̃ − λj)(C̃,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λj)uj
] = E [ u∗j C̃ 2uj ]",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"+ λ2j − 2λju∗jE [ C̃ ] uj
= E [ u∗j C̃ 2uj ]",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
− λ2j .,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"(19)
In addition, E [ u∗j C̃ 2uj ]",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"= m∑ p,q=1",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
u∗j E [ (xpx ∗ p)(xqx ∗ q) ],4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"m2 uj
= ∑ p 6=q u∗j E [ xpx ∗ p ]",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ xqx ∗ q,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
] m2 uj + m∑ p=1 u∗j,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ xpx ∗ pxpx ∗ p ],4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"m2 uj
= m(m− 1)
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"m2 λ2j +
1 m u∗jE[xx ∗xx∗]uj
= (1− 1 m )λ2j + 1 m u∗jE[xx ∗xx∗]uj (20)
and therefore
E [ ‖δCuj‖22 ] = (1− 1
m )λ2j +
1 m u∗jE[xx ∗xx∗]uj − λ2j
= u∗jE[xx ∗xx∗]uj",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λ2j m
= E",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ ‖xx∗uj‖22 ],4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λ2j
m .
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"Putting everything together, the claim follows.
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"The following corollary will be very useful when applying our results.
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
Corollary 4.1.,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"For any weights wij and real t > 0:
P ∑ i 6=j wij〈ũi, uj〉2 > t  ≤∑ i6=j
4wij k 2 j
mt (λi − λj)2 ,
where kj = ( E [ ‖xx∗uj‖22 ] − λ2j )1/2 and wij 6= 0 when
λi 6= λj and sgn(λi − λj) 2λ̃i >",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
sgn(λi,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"− λj)(λi + λj).
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
Proof.,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"We proceed as in the proof of Theorem 4.1:
P (∑ i 6=j wij〈ũi, uj〉2 ) 1 2 > t  ≤",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"[∑ i6=j wij〈ũi, uj〉2 ] t2
≤ 4 t2 ∑ i 6=j wij E",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"[ ‖δCuj‖22 ] (λi − λj)2
(21)
",4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
The claim follows by computing E,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
[ ‖δCuj‖22 ] (as before) and squaring both terms within the probability.,4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES,[0],[0]
"Though perhaps less sharp than what is currently known (e.g., see (Silverstein & Bai, 1995; Bai & Silverstein, 1998) for the asymptotic setting), it might be interesting to observe that a slight modification of the same argument can be used to characterize the eigenvalue relative difference, and as a consequence the main condition of Theorem 4.1.
",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
Corollary 4.2.,4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"For any eigenvalues λi and λ̃i of C and C̃, respectively, and for any t > 0, we have
P
( |λ̃i",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"− λi|
λi ≥ t )",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"≤ 1 m ( ki λi t )2 ,
where ki =",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"(E [ ‖xx∗ui‖22 ] − λi)1/2.
Proof.",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"Directly from the Bauer-Fike theorem (Bauer & Fike, 1960) one sees that
|δλi| ≤ ‖C̃ui",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
− λiui‖2 = ‖δCui‖2.,4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"(22)
",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"The proof is then identical to that of Theorem 4.1.
",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"Using this, we find that the eventE = {sgn(λi−λj) 2λ̃i > sgn(λi",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"− λj)(λi + λj)} occurs with probability at least
P(E) ≥ P ( |λ̃i",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
−,4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"λi| <
|λi − λj | 2
) > 1− 2k 2 i
m|λi − λj | .
",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"Therefore, one eliminates the condition from Theorem 4.1’s statement by relaxing the bound to
P(|〈ũi, uj〉| ≥ t) ≤ P(|〈ũi, uj〉| ≥ t |E)",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"+ (1−P(E))
",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"< 2
m|λi − λj | ( 2k2j t2|λi − λj | + k2i ) .",4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
(23),4.1.2 EIGENVALUE CONCENTRATION,[0],[0]
"As seen by the straightforward inequality E [ ‖xx∗uj‖22 ] ≤
E [ ‖x‖42 ] , kj connects to the kurtosis of the distribution.",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"However, it also captures the tendency of the distribution to fall in the span of uj .
",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"To see this, we will work with the whitened random vectors ε = C+1/2x, where C+ denotes the Moore–Penrose pseudoinverse of C. In particular,
k2j = E",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
[ u∗jC 1/2εε∗Cεε∗C1/2uj ],4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"− λ2j
= λj(E",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
[ ‖Λ1/2U∗εε∗uj‖22 ],4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"− λj)
= λj ( n∑ `=1 λ`E [ ε̂(`)2ε̂(j)2 ]",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"− λj ) , (24)
where ε̂ = U∗ε.",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
It is therefore easier to untangle the spaces spanned by ũi and uj when the variance of the distribution along the latter space is small (the expression is trivially minimized when λj → 0) or when the variance is entirely contained along that space (the expression is also small when λi = 0 for all i 6= j).,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"In addition, it can be seen that distributions with fast decaying tails allow for better principal component identification (E [ ε̂(j)4 ] is a measure of kurtosis over the direction of uj).
",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"For the particular case of a Normal distribution, we provide a closed-form expression.
",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
Corollary 4.3.,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"For a Normal distribution, we have k2j = λj",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"(λj + tr(C)).
",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
Proof.,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"For a centered and normal distribution with identity covariance, the choice of basis is arbitrary and the vector ε̂ = U∗ε is also zero mean with identity covariance.",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"Moreover, for every ` 6= j we can write E",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"[ ε̂(`)2ε̂(j)2 ] =
E [ ε̂(`)2 ]",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
E,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
[ ε̂(j)2 ] = 1.,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"This implies that
E [ ‖xx∗uj‖22 ] = λ2j E [ ε̂(j)4 ] + λj n∑ ` 6=j λ`
= λ2j (3− 1) +",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
λj tr(C) = 2λ2j +,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"λj tr(C) (25)
and, accordingly, k2j =",4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
λj,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
(λj + tr(C)).,4.1.3 THE INFLUENCE OF THE DISTRIBUTION,[0],[0]
"Our last result provides a sharper probability estimate for the family of sub-gaussian distributions supported in a centered Euclidean ball of radius r, with their Ψ2-norm
‖x‖Ψ2 = sup y∈Sn−1 ‖〈x, y〉‖ψ2 , (26)
where Sn−1 is the unit sphere and with the ψ2-norm of a random variable X defined as
‖X‖ψ2 = sup p≥1
p−1/2E[|X|p]1/p .",4.2 Distributions supported in a Euclidean ball,[0],[0]
"(27)
",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Our setting is therefore similar to the one used to study covariance estimation (Vershynin, 2012).",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Due to space constraints, we refer the reader to the excellent review article (Vershynin, 2010) for an introduction to sub-gaussian distributions as a tool for non-asymptotic analysis of random matrices.
",4.2 Distributions supported in a Euclidean ball,[0],[0]
Theorem 4.2.,4.2 Distributions supported in a Euclidean ball,[0],[0]
"For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c, independent of the sample size, such that for any real number t > 0,
P(|〈ũi, uj〉| ≥ t) ≤ exp ( 1− cmΦij(t) 2
λj ‖x‖2Ψ2
) , (28)
where Φij(t) =",4.2 Distributions supported in a Euclidean ball,[0],[0]
"|λi−λj | t−2λj 2 (r2/λj−1)1/2
− 2 ‖x‖Ψ2 , λi 6= λj and sgn(λi",4.2 Distributions supported in a Euclidean ball,[0],[0]
− λj) 2λ̃i >,4.2 Distributions supported in a Euclidean ball,[0],[0]
sgn(λi,4.2 Distributions supported in a Euclidean ball,[0],[0]
"− λj)(λi + λj).
",4.2 Distributions supported in a Euclidean ball,[0],[0]
Proof.,4.2 Distributions supported in a Euclidean ball,[0],[0]
"We start from the simple observation that, for every upper bound B of |〈ũi, uj〉| the relation P(|〈ũi, uj〉| > t) ≤",4.2 Distributions supported in a Euclidean ball,[0],[0]
P(B > t) holds.,4.2 Distributions supported in a Euclidean ball,[0],[0]
To proceed therefore we will construct a bound with a known tail.,4.2 Distributions supported in a Euclidean ball,[0],[0]
"As we saw in Sections 3.3 and 4.1,
|〈ũi, uj〉| ≤ 2 ‖δCuj‖2",4.2 Distributions supported in a Euclidean ball,[0],[0]
"|λi − λj |
= 2 ∥∥∥(1/m)∑mp=1(xpx∗puj",4.2 Distributions supported in a Euclidean ball,[0],[0]
"− λjuj)∥∥∥ 2
|λi − λj | ≤ 2 ∑m p=1 ∥∥xpx∗puj",4.2 Distributions supported in a Euclidean ball,[0],[0]
"− λjuj∥∥2 m |λi − λj |
= 2 ∑m p=1 √ (u∗jxp)
2(x∗pxp)− 2λj(u∗jxp)2",4.2 Distributions supported in a Euclidean ball,[0],[0]
"+ λ2j m |λi − λj |
= 2 ∑m p=1 √ (u∗jxp)
2(‖xp‖22",4.2 Distributions supported in a Euclidean ball,[0],[0]
− λj) +,4.2 Distributions supported in a Euclidean ball,[0],[0]
"λ2j m |λi − λj |
(29)
",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Assuming further that ‖x‖2 ≤ r, and since the numerator is minimized when ‖xp‖22 approaches λj , we can write for every sample x = C1/2ε:√
(u∗jx) 2(‖x‖22",4.2 Distributions supported in a Euclidean ball,[0],[0]
− λj) + λ2j ≤,4.2 Distributions supported in a Euclidean ball,[0],[0]
√ (u∗jx) 2(r2,4.2 Distributions supported in a Euclidean ball,[0],[0]
"− λj) + λ2j
= √ λj(u∗jε) 2(r2",4.2 Distributions supported in a Euclidean ball,[0],[0]
"− λj) + λ2j
≤ |u∗jε|",4.2 Distributions supported in a Euclidean ball,[0],[0]
"√ λjr2 − λ2j + λj , (30)
which is a shifted and scaled version of the random variable |ε̂(j)| = |u∗jε|.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Setting a = (λjr2 − λ2j )1/2, we have
P(|〈ũi, uj〉| ≥ t) ≤ P",4.2 Distributions supported in a Euclidean ball,[0],[0]
"( 2 ∑m p=1(|ε̂p(j)| a+ λj) m |λi − λj | ≥ t )
",4.2 Distributions supported in a Euclidean ball,[0],[0]
"= P ( m∑ p=1 (|ε̂p(j)| a+ λj) ≥ 0.5mt |λi − λj | )
= P ( m∑ p=1 |ε̂p(j)| ≥ m (0.5 t |λi − λj | − λj) a ) .",4.2 Distributions supported in a Euclidean ball,[0],[0]
"(31)
By Lemma 4.1 however, the left hand side is a sum of independent sub-gaussian variables.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Since the summands are not centered, we expand each |ε̂p(j)| = zp + E[|ε̂p(j)|] in terms of a centered sub-gaussian zp with the same ψ2norm.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Furthermore, by Jensen’s inequality and Lemma 4.1
E[|ε̂p(j)|] ≤ E",4.2 Distributions supported in a Euclidean ball,[0],[0]
"[ ε̂p(j) 2 ]1/2 ≤ 2
λj ‖x‖Ψ2 .",4.2 Distributions supported in a Euclidean ball,[0],[0]
"(32)
Therefore, if we set Φij(t)",4.2 Distributions supported in a Euclidean ball,[0],[0]
"= (0.5 |λi−λj | t−λj)
(r2/",4.2 Distributions supported in a Euclidean ball,[0],[0]
λj−1)1/2,4.2 Distributions supported in a Euclidean ball,[0],[0]
"− 2 ‖x‖Ψ2
P(|〈ũi, uj〉| ≥ t)",4.2 Distributions supported in a Euclidean ball,[0],[0]
≤ P,4.2 Distributions supported in a Euclidean ball,[0],[0]
( m∑ p=1 zp ≥ mΦij(t) λj ) .,4.2 Distributions supported in a Euclidean ball,[0],[0]
"(33)
Moreover, by the rotation invariance principle, the left hand side of the last inequality is a sub-gaussian with ψ2-norm smaller than (c1 ∑m p=1 ‖zp‖ 2 ψ2 )1/2 = (c1m) 1/2 ‖z‖ψ2 ≤ (c1m/λj) 1/2 ‖x‖Ψ2 , for some absolute constant c1.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"As a consequence, there exists an absolute constant c2, such that for each θ > 0",4.2 Distributions supported in a Euclidean ball,[0],[0]
":
P (∣∣∣∣∣ m∑ p=1 zp ∣∣∣∣∣",4.2 Distributions supported in a Euclidean ball,[0],[0]
≥ θ ) ≤ exp ( 1− c2 θ 2λj m ‖x‖2Ψ2 ) .,4.2 Distributions supported in a Euclidean ball,[0],[0]
"(34)
Substituting θ = mΦij(t)/λj , we have
P(|〈ũi, uj〉| ≥ t) ≤ exp",4.2 Distributions supported in a Euclidean ball,[0],[0]
"( 1− c2m 2 Φij(t) 2λj
mλ2j ‖x‖ 2 Ψ2
)
= exp ( 1− c2mΦij(t) 2
λj ‖x‖2Ψ2
) , (35)
which is the desired bound.
",4.2 Distributions supported in a Euclidean ball,[0],[0]
Lemma 4.1.,4.2 Distributions supported in a Euclidean ball,[0],[0]
"If x is a sub-gaussian random vector and ε = C+1/2x, then for every i, the random variable ε̂(i) =",4.2 Distributions supported in a Euclidean ball,[0],[0]
"u∗i ε is also sub-gaussian, with ‖ε̂(i)‖ψ2 ≤ ‖x‖Ψ2 / √ λi.
Proof.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"Notice that
‖x‖Ψ2 = sup y∈Sn−1 ‖〈x, y〉‖ψ2= sup y∈Sn−1 ∥∥∥∥∥∥",4.2 Distributions supported in a Euclidean ball,[0],[0]
n∑ j=1 λ 1/2 j (u ∗ jy)(u ∗ jε) ∥∥∥∥∥∥,4.2 Distributions supported in a Euclidean ball,[0],[0]
"ψ2
≥ ∥∥∥∥∥∥",4.2 Distributions supported in a Euclidean ball,[0],[0]
n∑ j=1 λ 1/2 j (u ∗ jui)ε̂(j) ∥∥∥∥∥∥ ψ2 = λ 1/2,4.2 Distributions supported in a Euclidean ball,[0],[0]
"i ‖ε̂(i)‖ψ2 , (36)
where, for the last inequality, we set y = ui.",4.2 Distributions supported in a Euclidean ball,[0],[0]
"To emphasize the utility of our results, in the following we consider the practical example of linear dimensionality reduction.",5 Application to Dimensionality Reduction,[0],[0]
"We show that a direct application of our bounds leads to upper estimates on the sample requirement.
",5 Application to Dimensionality Reduction,[0],[0]
"In terms of mean squared error, the optimal way to reduce the dimension of a sample x of a distribution is by projecting it over the subspace of the covariance with maximum variance.",5 Application to Dimensionality Reduction,[0],[0]
Denote by Ik the diagonal matrix with the first k diagonal entries equal to one and the rest zero.,5 Application to Dimensionality Reduction,[0],[0]
"When the actual covariance is known, the expected energy loss induced by the Pkx = IkU∗x projection is
loss(Pk) =",5 Application to Dimensionality Reduction,[0],[0]
E,5 Application to Dimensionality Reduction,[0],[0]
[ ‖x‖22 − ‖Pkx‖22 ] E[‖x‖22] = ∑ i>k λi tr(C) .,5 Application to Dimensionality Reduction,[0],[0]
"(37)
However, when the projector P̃k = IkŨ∗ is constructed from the sample covariance, we have
loss(P̃k) =",5 Application to Dimensionality Reduction,[0],[0]
E [ ‖x‖22 − ‖P̃kx‖22 ],5 Application to Dimensionality Reduction,[0],[0]
"E[‖x‖22]
=
∑n i=1 λi − tr(IkŨ∗UΛU∗Ũ)
tr(C)
",5 Application to Dimensionality Reduction,[0],[0]
"=
∑n i=1",5 Application to Dimensionality Reduction,[0],[0]
λi,5 Application to Dimensionality Reduction,[0],[0]
"− ∑ i≤k,j(ũ ∗",5 Application to Dimensionality Reduction,[0],[0]
"i uj) 2λj
tr(C) (38)
",5 Application to Dimensionality Reduction,[0],[0]
"with the expectation taken over the to-be-projected vectors x, but not the samples used to estimate the covariance.",5 Application to Dimensionality Reduction,[0],[0]
"After slight manipulation, one finds that
loss(P̃k) = loss(Pk) +
∑ i≤k,j 6=i (ũ∗i uj) 2(λi − λj)
tr(C) .",5 Application to Dimensionality Reduction,[0],[0]
"(39)
",5 Application to Dimensionality Reduction,[0],[0]
"The loss difference has an intuitive interpretation: when reducing the dimension with P̃k one looses either by discarding useful energy (terms j > k), or by displacing kept components within the permissible eigenspace (terms j ≤ k).",5 Application to Dimensionality Reduction,[0],[0]
Note also that all terms with j <,5 Application to Dimensionality Reduction,[0],[0]
"i are negative and can be excluded from the sum if we are satisfied we an upper estimate2.
",5 Application to Dimensionality Reduction,[0],[0]
"It is an implication of (39) and Corollary 4.1 that, when its conditions hold, for any distribution and t > 0
P ( loss(P̃k) > loss(Pk) + t
tr(C) )",5 Application to Dimensionality Reduction,[0],[0]
≤,5 Application to Dimensionality Reduction,[0],[0]
∑ i≤k,5 Application to Dimensionality Reduction,[0],[0]
j>i 4 k2j mt |λi,5 Application to Dimensionality Reduction,[0],[0]
"− λj | .
Observe that the loss difference becomes particularly small whenever k is small: (i) the terms in the sum are fewer and (ii) the magnitude of each term decreases (due to |λi−λj |).
",5 Application to Dimensionality Reduction,[0],[0]
"2A similar approach could also be utilized to derive a lower bound of the quantity loss(P̃k)− loss(Pk).
",5 Application to Dimensionality Reduction,[0],[0]
This phenomenon is also numerically verified in Figure 2 for the distribution of the images featuring digit ‘3’ in MNIST (total 6131 images with n = 784 pixels each).,5 Application to Dimensionality Reduction,[0],[0]
"The figure depicts for different k how many samples are required such that the loss difference is smaller than a tolerance threshold, here 0.02, 0.05, and 0.1.",5 Application to Dimensionality Reduction,[0],[0]
Each point in the figure corresponds to an average over 10 sampling draws.,5 Application to Dimensionality Reduction,[0],[0]
The trends featured in these numerical results agree with our theoretical intuition.,5 Application to Dimensionality Reduction,[0],[0]
"Moreover they illustrate that for modest k the sample requirement is far smaller than n.
It is also interesting to observe that for covariance matrices that are (approximately) low-rank, we obtain estimates reminiscent of compressed sensing (Candès et al., 2011), in the sense that the sample requirement becomes a function of the non-zero eigenvalues.",5 Application to Dimensionality Reduction,[0],[0]
"Though intuitive, with the exception of (Koltchinskii et al., 2016), this dependency of the estimation accuracy on the rank was not transparent in known results for covariance estimation (Rudelson, 1999; Adamczak et al., 2010; Vershynin, 2012).",5 Application to Dimensionality Reduction,[0],[0]
"The main contribution of this paper was the derivation of non-asymptotic bounds for the concentration of innerproducts |〈ũi, uj〉| involving eigenvectors of the sample and actual covariance matrices.",6 Conclusions,[0],[0]
"We also showed how these results can be extended to reason about eigenvalues and we applied them to the non-asymptotic analysis of linear dimensionality reduction.
",6 Conclusions,[0],[0]
We have identified two interesting directions for further research.,6 Conclusions,[0],[0]
The first has to do with obtaining tighter estimates.,6 Conclusions,[0],[0]
"Especially with regards to our perturbation arguments, we believe that our current bounds on inner products could be sharpened by at least a constant multiplicative factor.",6 Conclusions,[0],[0]
"The second direction involves using our results for the analysis of methods that utilize the eigenvectors of the covariance, such that principal component projection and regression (Jolliffe, 1982; Frostig et al., 2016).",6 Conclusions,[0],[0]
How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix?,abstractText,[0],[0]
"For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples.",abstractText,[0],[0]
Our findings imply non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications.,abstractText,[0],[0]
"For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.",abstractText,[0],[0]
How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 889–898, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Text compression exploits redundancy in human language to store documents compactly, and transmit them quickly.",1 Introduction,[0],[0]
"It is natural to think about compressing bilingual texts, which have even more redundancy:
“From an information theoretic point of view, accurately translated copies of the original text would be expected to contain almost no extra information if the original text is available, so in principle it should be possible to store and transmit these texts with very little extra cost.”",1 Introduction,[0],[0]
"(Nevill and Bell, 1992)
",1 Introduction,[0],[0]
"Of course, if we look at actual translation data (Figure 1), we see that there is quite a bit of unpredictability.",1 Introduction,[0],[0]
But the intuition is sound.,1 Introduction,[0],[0]
"If there were a million equally-likely translations of a short sentence, it would only take us log2(1m) = 20 bits to specify which one.
",1 Introduction,[0],[0]
"By finding and exploiting patterns in bilingual data, we want to provide an upper bound for this question: How much information does a human translator add to the original?",1 Introduction,[0],[0]
"We do this in the context of building a practical compressor for bilingual text.
",1 Introduction,[0],[0]
上个星期的战斗至少夺取12个人的生命。 At least 12 people were killed in the battle last week.,1 Introduction,[0],[0]
Last week’s fight took at least 12 lives.,1 Introduction,[0],[0]
The fighting last week killed at least 12.,1 Introduction,[0],[0]
The battle of last week killed at least 12 persons.,1 Introduction,[0],[0]
At least 12 people lost their lives in last week’s fighting.,1 Introduction,[0],[0]
At least 12 persons died in the fighting last week.,1 Introduction,[0],[0]
At least 12 died in the battle last week.,1 Introduction,[0],[0]
At least 12 people were killed in the fighting last week.,1 Introduction,[0],[0]
"During last week’s fighting, at least 12 people died.",1 Introduction,[0],[0]
Last week at least twelve people died in the fighting.,1 Introduction,[0],[0]
"Last week’s fighting took the lives of twelve people.
",1 Introduction,[0],[0]
"Figure 1: Eleven human translations of the same source sentence (LDC2002T01).
",1 Introduction,[0],[0]
"We adopt the same scheme used in monolingual text compression benchmark evaluations, such as the Hutter Prize (Hutter, 2006), a competition to compress a 100m-word extract of English Wikipedia.",1 Introduction,[0],[0]
"A valid entry is an executable, or self-extracting archive, that prints out Wikipedia, byte-for-byte.",1 Introduction,[0],[0]
"Decompression code, dictionaries, and/or other resources must be embedded in the executable—we cannot assume that the recipient of the compressed file has access to those resources.",1 Introduction,[0],[0]
"This view of compression goes by the name of algorithmic information theory (or Kolmogorov complexity).
",1 Introduction,[0],[0]
Any executable is permitted.,1 Introduction,[0],[0]
"For example, if our job were to compress the first million digits of π, then we might submit a very short piece of code that prints those digits.",1 Introduction,[0],[0]
The brevity of that compression would demonstrate our understanding of the sequence.,1 Introduction,[0],[0]
"Of course, in our application, we will find it useful to develop generic algorithms that can compress any text.
",1 Introduction,[0],[0]
Our approach will be as follows.,1 Introduction,[0],[0]
"Given a bilingual text (file1 and file2), we develop this compression interface:
% compress file1 > file1.exe
% bicompress file2",1 Introduction,[0],[0]
"file1 > file2.exe
The second command compresses file2 while looking at file1.",1 Introduction,[0],[0]
"We take the size of file1.exe
889
as the amount of information in the original text.",1 Introduction,[0],[0]
"We bound how much information the translator adds to the original by: |file2.exe| / |file1.exe| We can say that bilingual compression is more effective that monolingual compression if: |file2.exe| < |file3.exe|, where % compress file2 > file3.exe",1 Introduction,[0],[0]
Our decompression interface is: % file1.exe >,1 Introduction,[0],[0]
"file1
% file2.exe file1 > file2
The second command decompresses file2 while looking at (uncompressed) file1.
",1 Introduction,[0],[0]
The contributions of this paper are: 1.,1 Introduction,[0],[0]
"We provide a new quantitative bound for how
much information a translator adds to an original text.",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
We present practical software to compress bilingual text with compression rates that exceed the previous state-of-the-art.,1 Introduction,[0],[0]
3.,1 Introduction,[0],[0]
We set up a public benchmark bilingual text compression challenge to stimulate new researchers to find and exploit patterns in bilingual text.,1 Introduction,[0],[0]
"Ultimately, we want to feed those ideas into practical machine translation systems.",1 Introduction,[0],[0]
"We propose the widely accessible Spanish/English Europarl corpus v7 (Koehn, 2005) as a benchmark for bilingual text compression (Figure 2).",2 Data,[0],[0]
"Portions of this large corpus have been used in previous compression work (Sánchez-Martı́nez et al., 2012).",2 Data,[0],[0]
The Spanish side is in UTF-8.,2 Data,[0],[0]
"For English, we have removed accent marks and further eliminated all but the 95 printable ASCII characters (Brown et al., 1992), plus newline.
",2 Data,[0],[0]
"Our task is to compress the data “as is”: un-
tokenized, but already segment aligned.",2 Data,[0],[0]
"We also include a tokenized version with 334 manually word-aligned segment pairs (Lambert et al., 2005) distributed throughout the corpus.
",2 Data,[0],[0]
"For rapid development and testing, we have arranged a smaller corpus that is 10% the size of the full corpus (Figure 3).",2 Data,[0],[0]
Compression captures patterns in data.,3 Monolingual compression,[0],[0]
"Language modeling also captures patterns, but at first blush, these two areas seem distinct.",3 Monolingual compression,[0],[0]
"In compression, we seek a small executable that prints out a text, while in language modeling, we seek an executable that assigns low perplexity to held-out test data.1",3 Monolingual compression,[0],[0]
"Actually, the two areas have much more in common, as a review of compression algorithms reveals.
",3 Monolingual compression,[0],[0]
Huffman coding.,3 Monolingual compression,[0],[0]
"A well-known compression technique is to create a binary Huffman tree whose leaves are characters in the text,2 and whose edges are labeled 0 or 1 (Huffman and others, 1952).",3 Monolingual compression,[0],[0]
The tree is arranged so that frequent characters have short binary codes (edge sequences).,3 Monolingual compression,[0],[0]
"It is very important that the Huffman tree for a particular text be included at the beginning of the compressed file, so that decompression knows how to process the compressed bit string.
",3 Monolingual compression,[0],[0]
Adaptive Huffman.,3 Monolingual compression,[0],[0]
"Actually, we can avoid shipping the Huffman tree inside the compressed file, by building the tree adaptively, as the compressor processes the input text.",3 Monolingual compression,[0],[0]
"If we start with a uniform distribution, the first few characters may not compress very well, but soon we will converge onto a good tree and good compression.",3 Monolingual compression,[0],[0]
"It is very
1File size has advantages, as perplexity computations are often buggy, and they usually gloss over how probability is apportioned to out-of-vocabulary items.
2Or other symbols, such as words, bytes, or unicode sequences.
important that the decompressor exactly recapitulate the same sequence of Huffman trees that the compressor made.",3 Monolingual compression,[0],[0]
"It can do this by counting characters as it outputs them, just as the compressor counted characters as it consumed them.
",3 Monolingual compression,[0],[0]
"Adaptive compression can also nicely accommodate shifting topics in text, if we give higher counts to recent events.",3 Monolingual compression,[0],[0]
"By its single-pass nature, it is also good for streaming data.
",3 Monolingual compression,[0],[0]
Arithmetic coding.,3 Monolingual compression,[0],[0]
Huffman coding exploits a predictive unigram distribution over the next character.,3 Monolingual compression,[0],[0]
"If we use more context, we can make sharper distributions.",3 Monolingual compression,[0],[0]
"An n-gram table is one way to map contexts onto predictions.
",3 Monolingual compression,[0],[0]
How do we convert good predictions into good compression?,3 Monolingual compression,[0],[0]
"The solution is called arithmetic coding (Rissanen and Langdon Jr., 1981; Witten et al., 1987).",3 Monolingual compression,[0],[0]
Figure 4 sketches the technique.,3 Monolingual compression,[0],[0]
"We produce context-dependent probability intervals, and each time we observe a character, we move to its interval.",3 Monolingual compression,[0],[0]
"Our working interval becomes smaller and smaller, but the better our predictions, the wider it stays.",3 Monolingual compression,[0],[0]
A document’s compression is the shortest bit string that fits inside the final interval.,3 Monolingual compression,[0],[0]
"In practice, we do the bit-coding as we navigate probability intervals.
",3 Monolingual compression,[0],[0]
"Arithmetic coding separates modeling and compression, making our job similar to language modeling, where we use try to use context to predict the next symbol.",3 Monolingual compression,[0],[0]
"PPM is the most well-known adaptive, predictive compression technique (Cleary and Witten, 1984).",3.1 PPM,[0],[0]
PPM updates character n-gram tables (usually n=1..5) as it compresses.,3.1 PPM,[0],[0]
"In a given context, an n-gram table may predict only a subset of characters, so PPM reserves some probability mass for
an escape (ESC), after which it executes a hard backoff to the (n-1)-gram table.",3.1 PPM,[0],[0]
"In PPMA, P(ESC) is 1/(1+D), where D is the number of times the context has been seen.",3.1 PPM,[0],[0]
"PPMB uses q/D, where q is the number of distinct character types seen in the context.",3.1 PPM,[0],[0]
"PPMC uses q/(q+D), aka Witten-Bell.",3.1 PPM,[0],[0]
"PPMD uses q/2D.
PPM* uses the shortest previously-seen deterministic context, which may be quite long.",3.1 PPM,[0],[0]
"If there is no deterministic context, PPM* goes to the longest matching context and starts PPMD.",3.1 PPM,[0],[0]
"Instead of the longest context, PPMZ rates all contexts between lengths 0 and 12 according to each context’s most probable character.",3.1 PPM,[0],[0]
"PPMZ also implements an adaptive P(ESC) that combines context length, number of previous ESC in the context, etc.
",3.1 PPM,[0],[0]
We use our own C++ implementation of PPMC for monolingual compression experiments in this paper.,3.1 PPM,[0],[0]
"When we pass over a set of characters in favor of ESC, we remove those characters from the hard backoff.",3.1 PPM,[0],[0]
"PAQ (Mahoney, 2005) is a family of state-of-theart compression algorithms and a perennial Hutter Prize winner.",3.2 PAQ,[0],[0]
PAQ combines hundreds of models with a logistic unit when making a prediction.,3.2 PAQ,[0],[0]
This is most efficient when predictions are at the bit-level instead of the character-level.,3.2 PAQ,[0],[0]
"The unit’s model weights are adaptively updated by:
wi ← wi + ηxi(correct− P(1)), where xi = ln(Pi(1)/(1− Pi(1))",3.2 PAQ,[0],[0]
η,3.2 PAQ,[0],[0]
"= fixed learning rate Pi(1) = ith model’s prediction
PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of characters with ASCII > 32), a bigram model, and a skip-bigram model.",3.2 PAQ,[0],[0]
"Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase corpora, such as different English versions of the Bible.
Conley and Klein (2008) and Conley and Klein (2013) compress a target text that has been wordaligned to a source text, to which they add a lemmatizer and bilingual glossary.",4 Bilingual Compression: Prior Work,[0],[0]
"They obtain a 1%- 6% improvement over monolingual compression,
without counting the cost of auxiliary files needed for decompression.
",4 Bilingual Compression: Prior Work,[0],[0]
"Martı́nez-Prieto et al. (2009), Adiego et al. (2009), Adiego et al. (2010) rewrite bilingual text by first interleaving source words with their translations, then compressing this sequence of biwords.",4 Bilingual Compression: Prior Work,[0],[0]
Sánchez-Martı́nez et al. (2012) improve the interleaving scheme and include offsets to enable decompression to reconstruct the original word order.,4 Bilingual Compression: Prior Work,[0],[0]
They also compare several characterbased and word-based compression schemes for biword sequences.,4 Bilingual Compression: Prior Work,[0],[0]
"On Spanish-English Europarl data, they reach an 18.7% compression rate on word-interleaved text, compared to 20.1% for concatenated texts, a 7.2% improvement.
",4 Bilingual Compression: Prior Work,[0],[0]
"Al-Onaizan et al. (1999) study the perplexity of learned translation models, i.e., the probability assigned to the target corpus given the source corpus.",4 Bilingual Compression: Prior Work,[0],[0]
They observed iterative training to improve training-set perplexity (as guaranteed) but degrade test-set perplexity.,4 Bilingual Compression: Prior Work,[0],[0]
"They hypothesized that an increasingly tight, unsmoothed translation dictionary might exclude word translations needed to explain test-set data.",4 Bilingual Compression: Prior Work,[0],[0]
"Subsequently, research moved to extrinsic evaluation of translation models, in the context of end-to-end machine translation.
",4 Bilingual Compression: Prior Work,[0],[0]
Foster et al. (2002) and others have used prediction to propose auto-completions to speed up human translation.,4 Bilingual Compression: Prior Work,[0],[0]
"As we have seen, prediction and compression are highly related.",4 Bilingual Compression: Prior Work,[0],[0]
"Our algorithm compresses target-language file2 while looking at source-language file1:
% bicompress file2",5 Predictive Bilingual Compression,[0],[0]
"file1 > file2.exe
To make use of arithmetic coding, we consider the task of predicting the next target character, given the source sentence and target string so far:3
P(ej |f1 . . .",5 Predictive Bilingual Compression,[0],[0]
"fl, e1 . . .",5 Predictive Bilingual Compression,[0],[0]
ej−1),5 Predictive Bilingual Compression,[0],[0]
"If we are able to accurately predict what a human translator will type next, then we should be able to build a good machine translator.",5 Predictive Bilingual Compression,[0],[0]
"Here is an example of the task:
Spanish: Pido que hagamos un minuto de silencio.",5 Predictive Bilingual Compression,[0],[0]
"English so far: I should like to ob
3We predict e from f in this paper, reversed from Brown et al. (1993), who predict f from e.",5 Predictive Bilingual Compression,[0],[0]
Let us first work at the word level instead of the character level.,5.1 Word alignment,[0],[0]
"If we are predicting the jth English word, and we know that it translates fi (“aligns to fi”), and if fi has only a handful of translations, then we may be able to specify ej with just a few bits.",5.1 Word alignment,[0],[0]
"We may therefore suppose that a set of Viterbi word alignments may be useful for compression (Conley and Klein, 2008; SánchezMartı́nez et al., 2012).
",5.1 Word alignment,[0],[0]
We consider unidirectional alignments that link each target position j to a single source position i (including the null word at i = 0).,5.1 Word alignment,[0],[0]
"Such alignments can be computed automatically using EM (Brown et al., 1993), and stored in one of two formats:
Absolute: 1 2 5 5 7 0 3 6 . . .",5.1 Word alignment,[0],[0]
Relative: +1 +1,5.1 Word alignment,[0],[0]
+3 0 +2 null -4,5.1 Word alignment,[0],[0]
"+3 . . .
",5.1 Word alignment,[0],[0]
"In order to interpret the bits produced by the compressor, our decompressor must also have access to the same Viterbi alignments.",5.1 Word alignment,[0],[0]
"Therefore, we must include those alignments at the beginning of the compressed file.",5.1 Word alignment,[0],[0]
"So let’s compress them too.
",5.1 Word alignment,[0],[0]
How compressible are alignment sequences?,5.1 Word alignment,[0],[0]
Figure 5 gives results for Viterbi alignments derived from our large parallel Spanish/English corpus.,5.1 Word alignment,[0],[0]
"First, some interesting facts: • Huffman works better on relative offsets, be-
cause the common “+1” gets a short bit code.",5.1 Word alignment,[0],[0]
"• PPMC’s use of context makes it impressively
insensitive to alignment format.",5.1 Word alignment,[0],[0]
"• PPMC beats Huffman on relative offsets.
",5.1 Word alignment,[0],[0]
"This would not happen if relative offset integers were independent of one another, as assumed by (Brown et al., 1993) and (Vogel et al., 1996).",5.1 Word alignment,[0],[0]
"Bigram statistics bear this out:
P(+1 | -2) = 0.20 P(+1 | +1) = 0.59 P(+1 | -1) = 0.20 P(+1 | +2) = 0.49 P(+1 | 0) = 0.52
",5.1 Word alignment,[0],[0]
"So this small compression experiment already
suggests that translation aligners might want to model more context than just P(offset).
",5.1 Word alignment,[0],[0]
"However, the main point of Figure 5 is that the compressed alignment file requires 12.4 Mb!",5.1 Word alignment,[0],[0]
"This is too large for us to prepend to our compressed file, for the sake of enabling decompression.",5.1 Word alignment,[0],[0]
Another approach is to forget Viterbi alignments and instead exploit a probabilistic translation dictionary table t(e|f).,5.2 Translation dictionary,[0],[0]
"To predict the next target word ej , we admit the possibility that ej might be translating any of the source tokens.",5.2 Translation dictionary,[0],[0]
"IBM Model 2 (Brown et al., 1993) tells us how to do this:
Given f1 . . .",5.2 Translation dictionary,[0],[0]
fl: 1.,5.2 Translation dictionary,[0],[0]
Choose English length m (m|l) 2.,5.2 Translation dictionary,[0],[0]
"For j = 1..m, choose alignment aj a(aj |j, l) 3.",5.2 Translation dictionary,[0],[0]
"For j = 1..m, choose translation ej t(ej |faj )
which, via the “IBM trick” implies: P(e1 . . .",5.2 Translation dictionary,[0],[0]
em|f1 . . .,5.2 Translation dictionary,[0],[0]
"fl) = (m|l) ∏mj=1 ∑li=0 a(i|j, l)t(ej |fi)",5.2 Translation dictionary,[0],[0]
"In compression, we must predict English words incrementally, before seeing the whole string.",5.2 Translation dictionary,[0],[0]
"Furthermore, we must predict P(STOP ) to end the English sentence.",5.2 Translation dictionary,[0],[0]
"We can adapt IBM Model 2 to make incremental predictions:
P(STOP |f1 . . .",5.2 Translation dictionary,[0],[0]
"fl, e1 . . .",5.2 Translation dictionary,[0],[0]
"ej−1) ∼ P(STOP |j, l) = (j − 1|l)/∑maxk=j−1 (k|l)
",5.2 Translation dictionary,[0],[0]
P(ej |f1 . . .,5.2 Translation dictionary,[0],[0]
"fl, e1 . . .",5.2 Translation dictionary,[0],[0]
ej−1) ∼ P(ej |f1 . . .,5.2 Translation dictionary,[0],[0]
"fl) =
[1− P(STOP |j, l)] ∑li=0 a(i|j, l)t(ej |fi)",5.2 Translation dictionary,[0],[0]
"We can train t, a, and on our bilingual text using EM (Brown et al., 1993).",5.2 Translation dictionary,[0],[0]
"However, the t-table is still too large to prepend to the compressed English file.",5.2 Translation dictionary,[0],[0]
"Instead, inspired by PPM, we build up translation tables in RAM, during a single pass of our compressor.",5.3 Adaptive translation modeling,[0],[0]
"Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string.
",5.3 Adaptive translation modeling,[0],[0]
"Neal and Hinton (1998) describe online EM, which updates probability tables after each training example.",5.3 Adaptive translation modeling,[0],[0]
"Liang and Klein (2009) and Levenberg et al. (2010) apply online EM to a number of language tasks, including word alignment.",5.3 Adaptive translation modeling,[0],[0]
"Here we concentrate on the single-pass case.
",5.3 Adaptive translation modeling,[0],[0]
"We initialize a uniform translation model, use it to collect fractional counts from the first segment
pair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on.",5.3 Adaptive translation modeling,[0],[0]
"Because we pass through the data only once, we hope to converge quickly to high-quality tables for compressing the bulk of the text.
",5.3 Adaptive translation modeling,[0],[0]
"Unlike in batch EM, we need not keep separate count and probability tables.",5.3 Adaptive translation modeling,[0],[0]
"We only need count tables, including summary counts for normalization groups, so memory savings are significant.",5.3 Adaptive translation modeling,[0],[0]
"Whenever we need a probability, we compute it on the fly.",5.3 Adaptive translation modeling,[0],[0]
"To avoid zeroes being immediately locked in, we invoke add-λ smoothing every time we compute a probability from counts:4
t(e|f)",5.3 Adaptive translation modeling,[0],[0]
"= count(e,f)+λtcount(f)+λt|VE | a(i|j, l) = count(i,j,l)+λacount(j,l)+λa(l+1)
where |VE | is the size of the English vocabulary.",5.3 Adaptive translation modeling,[0],[0]
We determine |VE,5.3 Adaptive translation modeling,[0],[0]
"| via a quick initial pass through the data, then include it at the top of our compressed file.
",5.3 Adaptive translation modeling,[0],[0]
"In batch EM, we usually run IBM Model 1 for a few iterations before Model 2, gripped by an atavistic fear that the a probabilities will enforce rigid alignments before word co-occurrences have a chance to settle in.",5.3 Adaptive translation modeling,[0],[0]
It turns out this fear is justified in online EM!,5.3 Adaptive translation modeling,[0],[0]
"Because the a table initially learns to align most words to null, we smooth it more heavily (λa = 102, λt = 10−4).
",5.3 Adaptive translation modeling,[0],[0]
"We also implement a single-pass HMM alignment model (Vogel et al., 1996).",5.3 Adaptive translation modeling,[0],[0]
"In the IBM models, we can either collect fractional counts after we have compressed a whole sentence, or we can do it word-by-word.",5.3 Adaptive translation modeling,[0],[0]
"In the HMM model, alignment choices are no longer independent of one another:
Given f1 . . .",5.3 Adaptive translation modeling,[0],[0]
fl: 1.,5.3 Adaptive translation modeling,[0],[0]
Choose English length m w/prob (m|l) 2.,5.3 Adaptive translation modeling,[0],[0]
"For j = 1..m:
2a. set aj to null w/prob p1, or
2b.",5.3 Adaptive translation modeling,[0],[0]
choose non-null aj w/prob (1− p1)o(aj − ak) 3.,5.3 Adaptive translation modeling,[0],[0]
"For j = 1..m, choose translation ej w/prob t(ej |faj )",5.3 Adaptive translation modeling,[0],[0]
In the expression o(aj,5.3 Adaptive translation modeling,[0],[0]
"− ak), k is the maximum English index (k < j) such that ak 6= 0.",5.3 Adaptive translation modeling,[0],[0]
"The relative offset o-table learns to encourage adjacent English words to align to adjacent Spanish words.
",5.3 Adaptive translation modeling,[0],[0]
"Batch HMM performs poorly under uniform initialization, with two causes of failure.",5.3 Adaptive translation modeling,[0],[0]
"First, EM training sets o(0) too high, leading to absolute alignments like “1 2 2 2 5 5 5 5 . . .",5.3 Adaptive translation modeling,[0],[0]
”.,5.3 Adaptive translation modeling,[0],[0]
"We avoid
4In their online EM Model 1 aligner, Liang and Klein (p.c.)",5.3 Adaptive translation modeling,[0],[0]
"skirt the smoothing issue by running an epoch of batch EM to initialize a full set of probabilities before starting.
",5.3 Adaptive translation modeling,[0],[0]
"this with a standard schedule of 5 IBM1 iterations, 5 IBM2 iterations, then 5 HMM iterations.",5.3 Adaptive translation modeling,[0],[0]
"However, HMM still learns a very high value for p1, aligning most tokens to null, so we fix p1 = 0.1 for the duration of training.
",5.3 Adaptive translation modeling,[0],[0]
"Single-pass, online HMM suffers the same two problems, both solved when we smooth differentially (λo = 102, λt = 10−4) and fix p1 = 0.1.
",5.3 Adaptive translation modeling,[0],[0]
"Two quick asides before we examine the effectiveness of our online methods: • Translation researchers often drop long seg-
ment pairs that slow down HMM model processing.",5.3 Adaptive translation modeling,[0],[0]
"In compression, we cannot drop any of the text.",5.3 Adaptive translation modeling,[0],[0]
"Therefore, if the source segment contains more than 50 words, we use only monolingual PPMC to compress the target.",5.3 Adaptive translation modeling,[0],[0]
This affects 26.5% of our word tokens.,5.3 Adaptive translation modeling,[0],[0]
"• We might assist an online aligner by permut-
ing our n segment pairs to place shorter, less ambiguous ones at the top.",5.3 Adaptive translation modeling,[0],[0]
"However, we would have to communicate the permutation to the decompressor, at a prohibitive cost of log2(n!)/(8 · 106) = 4.8 Mb.",5.3 Adaptive translation modeling,[0],[0]
We next look at alignment accuracy (f-score) on our large Spanish/English corpus (Figure 6).,5.3 Adaptive translation modeling,[0],[0]
We evaluate against both a silver standard (Batch EM Viterbi alignments5) and a gold standard of 334 human-aligned segment pairs distributed throughout the corpus.,5.3 Adaptive translation modeling,[0],[0]
We see that online methods generate competitive translation dictionaries.,5.3 Adaptive translation modeling,[0],[0]
"Because single-pass alignment is significantly faster than traditional multi-pass, we also investigate its impact on an overall Moses pipeline for phrase-based
5We confirm that our Batch HMM implementation gives f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2, p=85.5, r=61.0), and its differently parameterized HMM.
machine translation (Koehn et al., 2007).",5.3 Adaptive translation modeling,[0],[0]
"Figure 7 shows that we can achieve competitive translation accuracy using fast, single-pass alignment, speeding up the system development cycle.",5.3 Adaptive translation modeling,[0],[0]
"For this use case, we can get an additional +0.3 alignment fscore (just as fast) if we print Viterbi alignments in a second pass instead of during training.",5.3 Adaptive translation modeling,[0],[0]
"We now want our continuously-improving translation model (TM) to predict target text, and to combine its predictions with PPM’s.",5.4 Word tokenization,[0],[0]
"For that to happen, our TM will need to predict the exact text, including spurious double-spaces, how parentheses combine with quotation marks, and so on.
",5.4 Word tokenization,[0],[0]
"We devise a tokenization scheme that records spacing information in the word tokens, which allows us to recover the original text uniquely.",5.4 Word tokenization,[0],[0]
"First, we identify word tokens as subsequences of [a-zAZ]*, [0-9]*, and [other]*, appending to each token the number of spaces following it (e.g., “...@2”).",5.4 Word tokenization,[0],[0]
"Next, we remove all “@1”, which leaves unique
recoverability intact.",5.4 Word tokenization,[0],[0]
"Finally, we move any suffix on an alpha-numeric word i to become a prefix on a non-alpha-numeric word i+ 1.",5.4 Word tokenization,[0],[0]
This reduces the vocabulary size for TM learning.,5.4 Word tokenization,[0],[0]
"An example: ""String-theory?"" he asked.
",5.4 Word tokenization,[0],[0]
<=> S@0,5.4 Word tokenization,[0],[0]
"""@0 String@0 -@0 theory@0 ?@0 ""@1 he@2 asked@0",5.4 Word tokenization,[0],[0]
".@0
<=> S@0",5.4 Word tokenization,[0],[0]
"""@0 String@0 -@0 theory@0 ?@0 "" he@2 asked@0 .@0
<=> S@0",5.4 Word tokenization,[0],[0]
"""@0 String @0-@0 theory @0?@0 "" he@2 asked @0.@0",5.4 Word tokenization,[0],[0]
"Under this tokenization scheme, we now ask our TM to give us a probability distribution over possible next words.",5.5 Predicting target words,[0],[0]
The TM knows the entire source word sequence f1...fl and the target words e1...,5.5 Predicting target words,[0],[0]
ej−1 seen so far.,5.5 Predicting target words,[0],[0]
"As candidates, we consider target words that can be produced, via the current t-table, from any (non-NULL) source words with probability greater than 10−4.
",5.5 Predicting target words,[0],[0]
"For HMM, we compute a prediction lattice that gives a distribution over possible source alignment positions for the current word we are predicting.",5.5 Predicting target words,[0],[0]
"Intuitively, the prediction lattice tells us “where we currently are” in translating the source string, and it prefers translations of source words in that vicinity.",5.5 Predicting target words,[0],[0]
"We efficiently reuse the lattice as we make predictions for each subsequent target word.
",5.5 Predicting target words,[0],[0]
"To make the TM’s prediction more accurate, we weight its prediction for each word with a smoothed, adapted English bigram word language model (LM).",5.5 Predicting target words,[0],[0]
This discourages the TM from trying to predict the first character of a word by simply using the most frequent source words.,5.5 Predicting target words,[0],[0]
We found that exponentiating the LM’s score by 0.2 before weighting keeps it from overpowering the HMM predictions.,5.5 Predicting target words,[0],[0]
"To convert word predictions into character predictions, we combine scores for words that share the next character.",5.6 Predicting target characters,[0],[0]
"For example, if the TM predicts ”monkey 0.4, car 0.3, cat 0.2, dog 0.1”, then we have ”P(c) 0.5, P(m) 0.4, P(d) 0.1”.",5.6 Predicting target characters,[0],[0]
"Additionally, we restrict ourselves to words prefixed by the portion of ej already observed.",5.6 Predicting target characters,[0],[0]
"The TM predicts the space character when a predicted word fully matches the observed prefix.
",5.6 Predicting target characters,[0],[0]
We also adjust PPM to produce a full distribution over the 96 possible next characters.,5.6 Predicting target characters,[0],[0]
"PPM
normally computes a distribution over only characters previously seen in the current context (plus ESC).",5.6 Predicting target characters,[0],[0]
"We now back off to the lowest context for every prediction.
",5.6 Predicting target characters,[0],[0]
We interpolate PPM and TM probabilities: P(ek|f1 . . .,5.6 Predicting target characters,[0],[0]
"fl, e1 . . .",5.6 Predicting target characters,[0],[0]
ek−1) = µ PPPM (ek|e1 . . .,5.6 Predicting target characters,[0],[0]
ek−1)+ (1− µ) PTM (ek|f1 . .,5.6 Predicting target characters,[0],[0]
.,5.6 Predicting target characters,[0],[0]
"fl, e1 . . .",5.6 Predicting target characters,[0],[0]
"ek−1)
",5.6 Predicting target characters,[0],[0]
"We adjust µ dynamically based on the relative confidence of the models:
µ = max(PPM) 2.5
max(PPM)2.5+max(HMM)2.5 Here, max(model) refers to the highest probability assigned to any character in the current context by the model.",5.6 Predicting target characters,[0],[0]
This yields better compression rates than simply setting µ to a constant.,5.6 Predicting target characters,[0],[0]
"When the TM is unable to extend a word, we set µ = 1.",5.6 Predicting target characters,[0],[0]
Figure 8 shows that monolingual PPM compresses the Spanish side of our corpus to 15.8% of the original.,6 Results,[0],[0]
Figure 9 (Main results) shows results for the English side of the corpus.,6 Results,[0],[0]
"Monolingual PPM compresses to 16.5%, while our HMM-based bilingual compression compresses to 11.9%.6
We can say that a human translation is characterized by an additional 0.95 bits per byte on top of the original, rather than the 1.32 bits per byte we
6For this result, we divide the English corpus into two pieces and compress them in parallel, and we further increase the sentence length threshold from 50 to 60, incurring a speed penalty.",6 Results,[0],[0]
"Our fictional Weissman score is 0.676.
would need if the English were independent text.",6 Results,[0],[0]
"Assuming our Spanish compression is good, we can also say that the human translator produces at most 68.1% (35.0/51.4) of the information that the original Spanish author produced.",6 Results,[0],[0]
"Intuitively, we feel this bound is high and should be reduced with better translation modeling.
",6 Results,[0],[0]
Figure 9 also reports our Shannon game experiments in which bilingual humans guessed subsequent characters of the English text.,6 Results,[0],[0]
"As suggested by Shannon, we upper-bound bpb as the crossentropy of a unigram model over a human guess sequence (e.g., 1 1 2 5 17 1 1 . . .",6 Results,[0],[0]
),6 Results,[0],[0]
", which records how many guesses it took to identify each subsequent English character, given context.",6 Results,[0],[0]
"For a 502- character English sequence, a team of four bilinguals working together gave us an upper-bound bpb of 0.51.",6 Results,[0],[0]
"This team had access to the original Spanish, plus a Google translation.",6 Results,[0],[0]
Monolinguals guessing on the same data (minus the Spanish and Google translation) yielded an upper-bound bpb of 1.61.,6 Results,[0],[0]
"These human-level models indicate that human translators are actually only adding ∼ 32% more information on top of the original, and that our current translation models are only capturing some fraction of this redundancy.7
Figure 10 shows compression of the entire bilingual corpus, allowing us to compare with the previous state-of-the-art (Sánchez-Martı́nez et al., 2012), which compresses a single, wordinterleaved bilingual corpus.",6 Results,[0],[0]
"It shows how PPMC
7Machine models can also generate guess sequences, and we see that entropy of a 30m-character PPMC guess sequence (1.43) upper-bounds actual PPMC bpb (1.28).
does on a concatenated Spanish/English file.",6 Results,[0],[0]
Uncompressed English (294.5 Mb) is 90.6% the size of uncompressed Spanish (324.9 Mb).,6 Results,[0],[0]
"Huffman narrows this gap to 93.0%, and PPM narrows it further to 94.4%, consistent with Behr et al. (2003) and Liberman (2008).",6 Results,[0],[0]
Spanish redundancies like adjective-noun agreement and balanced question marks (“¿ . . . ?”) may remain unexploited.,6 Results,[0],[0]
We have created a bilingual text compression challenge web,7 Conclusion,[0],[0]
site.8,7 Conclusion,[0],[0]
"This web site contains standard bilingual data, specifies what a valid compression is, and maintains benchmark results.
",7 Conclusion,[0],[0]
There are many future directions to pursue.,7 Conclusion,[0],[0]
"First, we would like to develop and exploit better predictive translation modeling.",7 Conclusion,[0],[0]
We have so far adapted machine translation technology circa only 1996.,7 Conclusion,[0],[0]
"For example, the HMM alignment model cannot “cross off” a source word and stop trying to translate it.",7 Conclusion,[0],[0]
"Also possible are phrase-based translation, neural nets, or as-yet-unanticipated patternfinding algorithms.",7 Conclusion,[0],[0]
"We only require an executable that prints the bilingual text.
",7 Conclusion,[0],[0]
Our current method requires segment-aligned input.,7 Conclusion,[0],[0]
"To work with real-life bilingual corpora, the compressor should take care of segment alignment, in a way that allows decompression back to the original text.",7 Conclusion,[0],[0]
"Similarly, we are currently restricted to texts written in the Latin alphabet, per our definition of “word.”
More broadly, we would also like to import more compression ideas into NLP.",7 Conclusion,[0],[0]
"Compression has so far appeared sporadically in NLP tasks like native language ID (Bobicev, 2013), text input methods (Powers and Huang, 2004), word segmentation (Teahan et al., 2000; Sornil and Chaiwanarom, 2004; Hutchens and Alder, 1998), alignment (Liu et al., 2014), and text categorization (Caruana & Lang, unpub. 1995).
",7 Conclusion,[0],[0]
"Translation researchers may also view bilingual compression as an alternate, reference-free evaluation metric for translation models.",7 Conclusion,[0],[0]
We anticipate that future ideas from bilingual compression can be brought back into translation.,7 Conclusion,[0],[0]
"Like Brown et al. (1992), with their gauntlet thrown down and fury of competitive energy, we hope that crossfertilizing compression and translation will bring fresh ideas to both areas.
",7 Conclusion,[0],[0]
8www.isi.edu/natural-language/compression,7 Conclusion,[0],[0]
This work was supported by a USC Provost Fellowship and ARO grant W911NF-10-1-0533.,Acknowledgements,[0],[0]
"We ask how much information a human translator adds to an original text, and we provide a bound.",abstractText,[0],[0]
"We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator?",abstractText,[0],[0]
We develop new compression algorithms and establish a benchmark task.,abstractText,[0],[0]
How Much Information Does a Human Translator Add to the Original?,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5010–5015 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
5010",text,[0],[0]
"Recently, reading comprehension (RC) has emerged as a popular task, with researchers proposing various end-to-end deep learning algorithms to push the needle on a variety of benchmarks.",1 Introduction,[0],[0]
"As characterized by Hermann et al. (2015); Onishi et al. (2016), unlike prior work addressing question answering from general structured knowledge, RC requires that a model extract information from a given, unstructured passage.",1 Introduction,[0],[0]
It’s not hard to imagine how such systems could be useful.,1 Introduction,[0],[0]
"In contrast to generic text summarization, RC systems could answer targeted questions about specific documents, efficiently extracting facts and insights.
",1 Introduction,[0],[0]
"While many RC datasets have been proposed over the years (Hirschman et al., 1999; Breck et al., 2001; Peñas et al., 2011; Peñas et al., 2012;
Sutcliffe et al., 2013; Richardson et al., 2013; Berant et al., 2014), more recently, larger datasets have been proposed to accommodate the dataintensiveness of deep learning.",1 Introduction,[0],[0]
"These vary both in the source and size of their corpora and in how they cast the prediction problem—as a classification task (Hill et al., 2016; Hermann et al., 2015; Onishi et al., 2016; Lai et al., 2017; Weston et al., 2016; Miller et al., 2016), span selection (Rajpurkar et al., 2016; Trischler et al., 2017), sentence retrieval (Wang et al., 2007; Yang et al., 2015), or free-form answer generation (Nguyen et al.,",1 Introduction,[0],[0]
"2016).1 Researchers have steadily advanced on these benchmarks, proposing myriad neural network architectures aimed at attending to both questions and passages to produce answers.
",1 Introduction,[0],[0]
"In this paper, we argue that amid this rapid progress on empirical benchmarks, crucial steps are sometimes skipped.",1 Introduction,[0],[0]
"In particular, we demonstrate that the level of difficulty for several of these tasks is poorly characterized.",1 Introduction,[0],[0]
"For example, for many RC datasets, it’s not reported, either in the papers introducing the datasets, or in those proposing models, how well one can perform while ignoring either the question or the passage.",1 Introduction,[0],[0]
"In other datasets, although the passage might consist of many lines of text, it’s not clear how many are actually required to answer the question, e.g., the answer may always lie in the first or the last sentence.
",1 Introduction,[0],[0]
"We describe several popular RC datasets and models proposed for these tasks, analyzing their performance when provided with question-only (Q-only) or passage-only (P-only) information.",1 Introduction,[0],[0]
"We show that on many tasks, the results obtained are surprisingly strong, outperforming many base-
1 We note several other QA datasets (Yang et al., 2015; Miller et al., 2016; Nguyen et al., 2016; Paperno et al., 2016; Clark and Etzioni, 2016; Lai et al., 2017; Trischler et al., 2017; Joshi et al., 2017) not addressed in this paper.
lines, and sometimes even surpassing the same models, supplied with both questions and passages.
",1 Introduction,[0],[0]
We note that similar problems were shown for datasets in visual question answering by Goyal et al. (2017) and for natural language inference by Gururangan et al. (2018); Poliak et al. (2018); Glockner et al. (2018).,1 Introduction,[0],[0]
"Several other papers have discussed the weaknesses of various RC benchamrks (Chen et al., 2016; Lee et al., 2016).",1 Introduction,[0],[0]
We discuss these studies in the paragraphs introducing the corresponding datasets below.,1 Introduction,[0],[0]
"In the following section, we provide context on each dataset that we investigate and then describe our process for corrupting the data as required by our question- and passage-only experiments.
",2 Datasets,[0],[0]
CBT Hill et al. (2016) prepared a cloze-style (fill in the blank) RC dataset by using passages from children’s books.,2 Datasets,[0],[0]
"In their dataset, each passage consists of 20 consecutive sentences, and each question is the 21st sentence with one word removed.",2 Datasets,[0],[0]
The missing word then serves as the answer.,2 Datasets,[0],[0]
"The dataset is split into four categories of answers: Named Entities (NE), Common Nouns (CN), Verbs (V) and Prepositions (P).",2 Datasets,[0],[0]
"The training corpus contains over 37, 000 candidates and each question is associated with 10 candidates, POSmatched to the correct answer.",2 Datasets,[0],[0]
"The authors established LSTM/embedding-based Q-only baselines but did not present the results obtained by their best model using Q-only or P-only information.
",2 Datasets,[0],[0]
"CNN Hermann et al. (2015) introduced the CNN/Daily Mail datasets containing more than 1 million news articles, each associated with several highlight sentences.",2 Datasets,[0],[0]
"Also adopting the clozestyle dataset preparation, they remove an entity (answer) from a highlight (question).",2 Datasets,[0],[0]
"They anonymize all entities to ensure that models rely on information contained in the passage, vs memorizing characteristics of given entities across examples, and thus ignoring passages.",2 Datasets,[0],[0]
"On average, passages contain 26 entities, with over 500 total possible answer candidates.",2 Datasets,[0],[0]
Chen et al. (2016) analyzed the difficulty of the CNN and Daily Mail tasks.,2 Datasets,[0],[0]
"They hand-engineered a set of eight features for each entity e (does e occur in the question, in the passage, etc.), showing that this simple classifier outperformed many earlier deep learning
results.
",2 Datasets,[0],[0]
"Who-did-What Onishi et al. (2016) extracted pairs of news articles, each pair referring to the same events.",2 Datasets,[0],[0]
"Adopting the cloze-style, they remove a person’s name (the answer) from the first sentence of one article (the question).",2 Datasets,[0],[0]
"A model must predict the answer based on the question, together with the other article in the pair (passage).",2 Datasets,[0],[0]
"Unlike CNN, Who-did-What does not anonymize entities.",2 Datasets,[0],[0]
"On average, each question is associated with 3.5 candidate answers.",2 Datasets,[0],[0]
"The authors removed several questions from their dataset to thwart simple strategies such as always predicting the name that occurs most (or first) in the passage.
",2 Datasets,[0],[0]
bAbI Weston et al. (2016) presented a set of 20 tasks to help researchers identify and rectify the failings of their reading comprehension systems.,2 Datasets,[0],[0]
"Unlike the datasets discussed so far, the questions in this task are not cloze-style and are synthetically generated using templates.",2 Datasets,[0],[0]
This restricts the diversity in clauses appearing in the passages.,2 Datasets,[0],[0]
"Further, this also restricts the dataset vocabulary to just 150 words, in contrast, CNN dataset has a vocabulary made of close to 120, 000 words.",2 Datasets,[0],[0]
"Memory Networks with adaptive memory, n-grams and non-linear matching were shown to obtain 100% accuracy on 12 out of 20 bAbI tasks.",2 Datasets,[0],[0]
"We note that Lee et al. (2016) previously identified that bAbI tasks might fall short as a measure of “AI-complete question answering”, proposing two models based on tensor product representations that achieve 100% accuracy on many bAbI tasks.
SQuAD",2 Datasets,[0],[0]
"More recently, Rajpurkar et al. (2016) released the Stanford Question Answering Dataset (SQuAD) containing over 100, 000 crowd-sourced questions addressing 536 passages.",2 Datasets,[0],[0]
Each question is associated with a paragraph (passage) extracted from an article.,2 Datasets,[0],[0]
These passages are shorter than those in CNN and Who-did-What datasets.,2 Datasets,[0],[0]
"Models choose answers by selecting (varying-length) spans from these passages.
",2 Datasets,[0],[0]
"Generating Corrupt Data To void any information in either the questions or the passages, while otherwise leaving each architecture intact, we create corrupted versions of each dataset by assigning either questions randomly, while preserving the correspondence between passage and answer, or by randomizing the passage.",2 Datasets,[0],[0]
"For
tasks where question-answering requires selecting spans or candidates from the passage, we create passages that contain the candidates in random locations but otherwise consist of random gibberish.",2 Datasets,[0],[0]
"In our investigations of the various RC benchmarks, we rely upon the following three recentlyproposed models: key-value memory networks, gated attention readers, and QA nets.",3 Models,[0],[0]
"Although space constraints preclude a full discussion of each architecture, we provide references to the source papers and briefly discuss any implementation decisions necessary to reproduce our results.
",3 Models,[0],[0]
"Key-Value Memory Networks We implement a Key-Value Memory Network (KV-MemNet) (Miller et al., 2016), applying it to bAbI and CBT.",3 Models,[0],[0]
"KV-MemNets are based on Memory Networks (Sukhbaatar et al., 2015), shown to perform well on both datasets.",3 Models,[0],[0]
"For bAbI tasks, the keys and values both encode the passage as a bag-of-words (BoW).",3 Models,[0],[0]
"For CBT, the key is a BoW-encoded 5- word window surrounding a candidate answer and the value is the candidate itself.",3 Models,[0],[0]
"We fixed the number of hops to 3 and the embedding size to 128.
",3 Models,[0],[0]
"Gated Attention Reader Introduced by Dhingra et al. (2017), the Gated Attention Reader (GAR)2 performs multiple hops over a passage, like MemNets.",3 Models,[0],[0]
"The word representations are refined over each hop and are mapped by an attention-sum module (Kadlec et al., 2016) to a probability distribution over the candidate answer set in the last hop.",3 Models,[0],[0]
"The model nearly matches bestreported results on many cloze-style RC datasets, and thus we apply it to Who-did-What, CNN, CBTNE and CBT-CN.
",3 Models,[0],[0]
"QA Net Recently introduced by (Yu et al., 2018), the QA-Net3 was recently demonstrated to outperform all previous models on the SQuAD dataset4.",3 Models,[0],[0]
Passages and questions are passed as input to separate encoders consisting of depth-wise separable convolutions and global self-attention.,3 Models,[0],[0]
"This is followed by a passage-question attention layer, followed by stacked encoders.",3 Models,[0],[0]
"The outputs
2https://github.com/bdhingra/ga-reader 3We use the implementation available at https://github.com/NLPLearn/QANet 4At the time of publication, an ensemble of QA-Net models was at the top of the leader board.",3 Models,[0],[0]
"A single QA-Net was ranked 4th.
from these encoders are used to predict an answer span inside the passage.",3 Models,[0],[0]
bAbI tasks Table 1 shows the results obtained by a Key-Value Memory Network on bAbI tasks by nullifying the information present in either questions or passages.,4 Experimental Results,[0],[0]
"On tasks 2, 7, 13 and 20, Ponly models obtain over 80% accuracy with questions randomly assigned.",4 Experimental Results,[0],[0]
"Moreover, on tasks 3, 13, 16, and 20, P-only models match performance of those trained on the full dataset.",4 Experimental Results,[0],[0]
"On task 18, Qonly models achieve an accuracy of 91%, nearly matching the best performance of 93% achieved by the full model.",4 Experimental Results,[0],[0]
"These results show that some of bAbI tasks are easier than one might think.
",4 Experimental Results,[0],[0]
"Children’s Books Test On the NE and CN CBT tasks, Q-only KV-MemNets obtain an accuracy close to the full accuracy and on the Verbs (V) and Prepositions (P) tasks, Q-only models outperform the full model (Table 2).",4 Experimental Results,[0],[0]
"Q-only Gated attention readers reach accuracy of 50.6% and 54% on Named Entities (NE) and Common Nouns (CN) tasks, respectively, while P-only models reach accuracies of 40.8% and 36.7%, respectively.",4 Experimental Results,[0],[0]
We note that our models can outperform 16 of the 19 reported results on the NE task in Hill et al. (2016) using Q-only information.,4 Experimental Results,[0],[0]
"Table 3 shows that if we make use of just last sentence instead of all 20 sentences in the passage, our sentence memory based KV-MemNet achieve comparable or better performance w.r.t the full model on most subtasks.
",4 Experimental Results,[0],[0]
"CNN Table 2, shows the performance of Gated Attention Reader on the CNN dataset.",4 Experimental Results,[0],[0]
"Q-only and P-only models obtained 25.6% and 38.3% accuracies respectively, compared to 77.8% on the true dataset.",4 Experimental Results,[0],[0]
This drop in accuracy could be due to the anonymization of entities which prevents models from building entity-specific information.,4 Experimental Results,[0],[0]
"Notwithstanding the deficiencies noted by Chen et al. (2016), we found that out CNN, out all the cloze-style RC datasets that we evaluated, appears to be the most carefully designed.
",4 Experimental Results,[0],[0]
"Who-did-What P-only models achieve greater than 50% accuracy in both the strict and relaxed setting, reaching within 15% of the accuracy of the full model in the strict setting.",4 Experimental Results,[0],[0]
Q-only models also achieve 50% accuracy on the relaxed setting while achieving an accuracy of 41.8% on the strict setting.,4 Experimental Results,[0],[0]
"Our P-only model also outperforms all the
suppressed baselines and 5 additional baselines reported by Onishi et al. (2016).",4 Experimental Results,[0],[0]
"We suspect that
the models memorize attributes of specific entities, justifying the entity-anonymization used by Hermann et al. (2015) to construct the CNN dataset.
SQuAD",4 Experimental Results,[0],[0]
Our results suggest that SQuAD is an unusually carefully-designed and challenging RC task.,4 Experimental Results,[0],[0]
The span selection mode of answering requires that models consider the passage thus the abysmal performance of the Q-only QANet (Table 4).,4 Experimental Results,[0],[0]
"Since SQuAD requires answering by span selection, we construct Q-only variants here by placing answers from all relevant questions in random order, filling the gaps with random words.",4 Experimental Results,[0],[0]
"Moreover, Q-only and P-only models achieve F1 scores of only 4% and 14.8% resp.",4 Experimental Results,[0],[0]
"(Table 4), significantly lower than 79.1 on the proper task.",4 Experimental Results,[0],[0]
"We briefly discuss our findings, offer some guiding principles for evaluating new benchmarks and algorithms, and speculate on why some of these problems may have gone under the radar.",5 Discussion,[0],[0]
"Our goal is not to blame the creators of past datasets but instead to support the community by offering practical guidance for future researchers.
",5 Discussion,[0],[0]
"Provide rigorous RC baselines Published RC datasets should contain reasonable baselines that characterize the difficulty of the task, and specifically, the extent to which questions and passages are essential.",5 Discussion,[0],[0]
"Moreover, follow-up papers reporting improvements ought to report performance both on the full task and variations omitting questions and passages.",5 Discussion,[0],[0]
"While many proposed technical innovations purportedly work by better matching up information in questions and passages, absent these baselines one cannot tell whether gains come for the claimed reason or if the models just do a better job of passage classification (disregarding questions).
",5 Discussion,[0],[0]
"Test that full context is essential Even on tasks where both questions and passages are required, problems might appear harder than they really are.",5 Discussion,[0],[0]
"On first glance the the length-20 passages in CBT, might suggest that success requires reasoning over all 20 sentences to identify the correct answer to each question.",5 Discussion,[0],[0]
"However, it turns out that for some models, comparable performance can be achieved by considering only the last sentence.",5 Discussion,[0],[0]
"We recommend that researchers provide reasonable ablations to characterize the amount of context that each model truly requires.
",5 Discussion,[0],[0]
Caution with cloze-style RC datasets We note that cloze-style datasets are often created programatically.,5 Discussion,[0],[0]
"Thus it’s possible for a dataset to be produced, published, and incorporated into many downstream studies, all without many personhours spent manually inspecting the data.",5 Discussion,[0],[0]
"We speculate that, as a result, these datasets tend be subject to less contemplation of what’s involved in answering these questions and are therefore especially susceptible to the sorts of overlooked weaknesses described in our study.
",5 Discussion,[0],[0]
A note on publishing incentives We express some concern that the recommended experimental rigor might cut against current publishing incentives.,5 Discussion,[0],[0]
We speculate that papers introducing datasets may be more likely to be accepted at conferences by omitting unfavorable ablations than by including them.,5 Discussion,[0],[0]
"Moreover, with reviewers often demanding architectural novelty, methods papers may find an easier path to acceptance by providing unsubstantiated stories about the reasons why a given architecture works than by providing rigorous ablation studies stripping out spurious explanations and unnecessary model components.",5 Discussion,[0],[0]
"For
more general discussions of misaligned incentives and empirical rigor in machine learning research, we point the interested reader to Lipton and Steinhardt (2018) and Sculley et al. (2018).",5 Discussion,[0],[0]
"Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples.",abstractText,[0],[0]
"Presumably, a model must combine information from both questions and passages to predict corresponding answers.",abstractText,[0],[0]
"However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered.",abstractText,[0],[0]
"In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Whodid-What datasets, finding that questionand passage-only models often perform surprisingly well.",abstractText,[0],[0]
"On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50% accuracy, sometimes matching the full model.",abstractText,[0],[0]
"Interestingly, while CBT provides 20-sentence passages, only the last is needed for comparably accurate prediction.",abstractText,[0],[0]
"By comparison, SQuAD and CNN appear better-constructed.",abstractText,[0],[0]
How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122–2132, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model’s generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",text,[0],[0]
"An important aspect of dialogue response generation systems, which are trained to produce a reasonable utterance given a conversational context, is how to evaluate the quality of the generated response.",1 Introduction,[0],[0]
"Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; Möller et al., 2006; Kamm, 1995), which are relevant when the dialogue is task-focused.",1 Introduction,[0],[0]
"We call models optimized for such supervised objectives supervised dialogue models, while those that do not are unsupervised dialogue models.
",1 Introduction,[0],[0]
"This paper focuses on unsupervised dialogue response generation models, such as chatbots.",1 Introduction,[0],[0]
"These
∗Denotes equal contribution.
",1 Introduction,[0],[0]
"models are receiving increased attention, particularly using end-to-end training with neural networks (Serban et al., 2016; Sordoni et al., 2015; Vinyals and Le, 2015).",1 Introduction,[0],[0]
"This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive.",1 Introduction,[0],[0]
"However, automatically evaluating the quality of these models remains an open question.",1 Introduction,[0],[0]
"Automatic evaluation metrics would help accelerate the deployment of unsupervised response generation systems.
",1 Introduction,[0],[0]
"Faced with similar challenges, other natural language tasks have successfully developed automatic evaluation metrics.",1 Introduction,[0],[0]
"For example, BLEU (Papineni et al., 2002a) and METEOR (Banerjee and Lavie, 2005) are now standard for evaluating machine translation models, and ROUGE (Lin, 2004) is often used for automatic summarization.",1 Introduction,[0],[0]
"These metrics have recently been adopted by dialogue researchers (Ritter et al., 2011; Sordoni et al., 2015; Li et al., 2015; Galley et al., 2015b; Wen et al., 2015; Li et al., 2016).",1 Introduction,[0],[0]
However these metrics assume that valid responses have significant word overlap with the ground truth responses.,1 Introduction,[0],[0]
"This is a strong assumption for dialogue systems, where there is significant diversity in the space of valid responses to a given context.",1 Introduction,[0],[0]
"This is illustrated in Table 1, where two reasonable responses are proposed to the context, but these responses do not share any words in common and do not have the same semantic meaning.
",1 Introduction,[0],[0]
"In this paper, we investigate the correlation between the scores from several automatic evaluation metrics and human judgements of dialogue response quality, for a variety of response generation models.",1 Introduction,[0],[0]
"We consider both statistical word-overlap similar-
2122
ity metrics such as BLEU, METEOR, and ROUGE, and word embedding metrics derived from word embedding models such as Word2Vec (Mikolov et al., 2013).",1 Introduction,[0],[0]
"We find that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models (see above, and Lasguido et al. (2014)).",1 Introduction,[0],[0]
"In particular, we show that these metrics have only a small positive correlation on the chitchat oriented Twitter dataset, and no correlation at all on the technical Ubuntu Dialogue Corpus.",1 Introduction,[0],[0]
"For the word embedding metrics, we show that this is true even though all metrics are able to significantly distinguish between baseline and state-of-the-art models across multiple datasets.",1 Introduction,[0],[0]
"We further highlight the shortcomings of these metrics using: a) a statistical analysis of our survey’s results; b) a qualitative analysis of examples from our data; and c) an exploration of the sensitivity of the metrics.
",1 Introduction,[0],[0]
"Our results indicate that a shift must be made in the research community away from these metrics, and highlight the need for a new metric that correlates more strongly with human judgement.",1 Introduction,[0],[0]
"We focus on metrics that are model-independent, i.e. where the model generating the response does not also evaluate its quality; thus, we do not consider word perplexity, although it has been used to evaluate unsupervised dialogue models (Serban et al., 2015).",2 Related Work,[0],[0]
"This is because it is not computed on a per-response basis, and cannot be computed for retrieval models.",2 Related Work,[0],[0]
"Further, we only consider metrics that can be used to evaluate proposed responses against ground-truth responses, so we do not consider retrieval-based metrics such as recall, which
has been used to evaluate dialogue models (Schatzmann et al., 2005; Lowe et al., 2015).",2 Related Work,[0],[0]
"We also do not consider evaluation methods for supervised evaluation methods.1
Several recent works on unsupervised dialogue systems adopt the BLEU score for evaluation.",2 Related Work,[0],[0]
Ritter et al. (2011) formulate the unsupervised learning problem as one of translating a context into a candidate response.,2 Related Work,[0],[0]
"They use a statistical machine translation (SMT) model to generate responses to various contexts using Twitter data, and show that it outperforms information retrieval baselines according to both BLEU and human evaluations.",2 Related Work,[0],[0]
Sordoni et al. (2015) extend this idea using a recurrent language model to generate responses in a context-sensitive manner.,2 Related Work,[0],[0]
"They also evaluate using BLEU, however they produce multiple ground truth responses by retrieving 15 responses from elsewhere in the corpus, using a simple bag-of-words model.",2 Related Work,[0],[0]
Li et al. (2015) evaluate their proposed diversity-promoting objective function for neural network models using BLEU score with only a single ground truth response.,2 Related Work,[0],[0]
"A modified version of BLEU, deltaBLEU (Galley et al., 2015b), which takes into account several humanevaluated ground truth responses, is shown to have a weak to moderate correlation to human judgements using Twitter dialogues.",2 Related Work,[0],[0]
"However, such human annotation is often infeasible to obtain in practice.",2 Related Work,[0],[0]
"Galley et al. (2015b) also show that, even with several ground truth responses available, the standard BLEU metric does not correlate strongly with human judgements.
",2 Related Work,[0],[0]
"There has been significant previous work that evaluates how well automatic metrics correlate with human judgements in in both machine translation (Callison-Burch et al., 2010; Callison-Burch et al., 2011; Bojar et al., 2014; Graham et al., 2015) and natural language generation (NLG) (Stent et al., 2005; Cahill, 2009; Reiter and Belz, 2009; Espinosa et al., 2010).",2 Related Work,[0],[0]
"There has also been work criticizing the usefulness of BLEU in particular for machine translation (Callison-Burch et al., 2006).",2 Related Work,[0],[0]
"While many of the criticisms in these works apply to dialogue generation, we note that generating dialogue responses conditioned on the conversational
1Evaluation methods in the supervised setting have been well studied, see (Walker et al., 1997; Möller et al., 2006; Jokinen and McTear, 2009).
context is in fact a more difficult problem.",2 Related Work,[0],[0]
This is because most of the difficulty in automatically evaluating language generation models lies in the large set of correct answers.,2 Related Work,[0],[0]
"Dialogue response generation given solely the context intuitively has a higher diversity (or entropy) than translation given text in a source language, or surface realization given some intermediate form (Artstein et al., 2009).",2 Related Work,[0],[0]
"Given a dialogue context and a proposed response, our goal is to automatically evaluate how appropriate the proposed response is to the conversation.",3 Evaluation Metrics,[0],[0]
We focus on metrics that compare it to the ground truth response of the conversation.,3 Evaluation Metrics,[0],[0]
"In particular, we investigate two approaches: word based similarity metrics and word-embedding based similarity metrics.",3 Evaluation Metrics,[0],[0]
We first consider metrics that evaluate the amount of word-overlap between the proposed response and the ground-truth response.,3.1 Word Overlap-based Metrics,[0],[0]
"We examine the BLEU and METEOR scores that have been used for machine translation, and the ROUGE score that has been used for automatic summarization.",3.1 Word Overlap-based Metrics,[0],[0]
"While these metrics have been shown to correlate with human judgements in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been thoroughly investigated for dialogue systems.2
We denote the ground truth response as r (thus we assume that there is a single candidate ground truth response), and the proposed response as r̂. The j’th token in the ground truth response r is denoted by wj , with ŵj denoting the j’th token in the proposed response r̂.
BLEU.",3.1 Word Overlap-based Metrics,[0],[0]
"BLEU (Papineni et al., 2002a) analyzes the co-occurrences of n-grams in the ground truth and the proposed responses.",3.1 Word Overlap-based Metrics,[0],[0]
"It first computes an n-gram precision for the whole dataset (we assume that there is a single candidate ground truth response
2To the best of our knowledge, only BLEU has been evaluated in the dialogue system setting quantitatively by Galley et al. (2015a) on the Twitter domain.",3.1 Word Overlap-based Metrics,[0],[0]
"However, they carried out their experiments in a very different setting with multiple ground truth responses, which are rarely available in practice, and without providing any qualitative analysis of their results.
per context):
Pn(r, r̂) =
∑ k min(h(k, r), h(k, r̂i))∑
k h(k, ri)
where k indexes all possible n-grams of length n and h(k, r) is the number of n-grams k in r.3",3.1 Word Overlap-based Metrics,[0],[0]
"To avoid the drawbacks of using a precision score, namely that it favours shorter (candidate) sentences, the authors introduce a brevity penalty.",3.1 Word Overlap-based Metrics,[0],[0]
"BLEU-N, where N is the maximum length of n-grams considered, is defined as:
BLEU-N := b(r, r̂) exp( N∑
n=1
βn logPn(r, r̂))
",3.1 Word Overlap-based Metrics,[0],[0]
"βn is a weighting that is usually uniform, and b(·) is the brevity penalty.",3.1 Word Overlap-based Metrics,[0],[0]
The most commonly used version of BLEU uses N = 4.,3.1 Word Overlap-based Metrics,[0],[0]
"Modern versions of BLEU also use sentence-level smoothing, as the geometric mean often results in scores of 0 if there is no 4-gram overlap (Chen and Cherry, 2014).",3.1 Word Overlap-based Metrics,[0],[0]
"Note that BLEU is usually calculated at the corpus-level, and was originally designed for use with multiple reference sentences.
",3.1 Word Overlap-based Metrics,[0],[0]
METEOR.,3.1 Word Overlap-based Metrics,[0],[0]
"The METEOR metric (Banerjee and Lavie, 2005) was introduced to address several weaknesses in BLEU.",3.1 Word Overlap-based Metrics,[0],[0]
It creates an explicit alignment between the candidate and target responses.,3.1 Word Overlap-based Metrics,[0],[0]
"The alignment is based on exact token matching, followed by WordNet synonyms, stemmed tokens, and then paraphrases.",3.1 Word Overlap-based Metrics,[0],[0]
"Given a set of alignments, the METEOR score is the harmonic mean of precision and recall between the proposed and ground truth sentence.
",3.1 Word Overlap-based Metrics,[0],[0]
ROUGE.,3.1 Word Overlap-based Metrics,[0],[0]
"ROUGE (Lin, 2004) is a set of evaluation metrics used for automatic summarization.",3.1 Word Overlap-based Metrics,[0],[0]
"We consider ROUGE-L, which is a F-measure based on the Longest Common Subsequence (LCS) between a candidate and target sentence.",3.1 Word Overlap-based Metrics,[0],[0]
"The LCS is a set of words which occur in two sentences in the same order; however, unlike n-grams the words do not have to be contiguous, i.e. there can be other words in between the words of the LCS.
3Note that the min in this equation is calculating the number of co-occurrences of n-gram k between the ground truth response r and the proposed response r̂, as it computes the fewest appearances of k in either response.",3.1 Word Overlap-based Metrics,[0],[0]
"An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding, which assigns a vector to each word.",3.2 Embedding-based Metrics,[0],[0]
"Methods such as Word2Vec (Mikolov et al., 2013) calculate these embeddings using distributional semantics; that is, they approximate the meaning of a word by considering how often it co-occurs with other words in the corpus.4 These embeddingbased metrics usually approximate sentence-level embeddings using some heuristic to combine the vectors of the individual words in the sentence.",3.2 Embedding-based Metrics,[0],[0]
"The sentence-level embeddings between the candidate and target response are compared using a measure such as cosine distance.
",3.2 Embedding-based Metrics,[0],[0]
Greedy Matching.,3.2 Embedding-based Metrics,[0],[0]
Greedy matching is the one embedding-based metric that does not compute sentence-level embeddings.,3.2 Embedding-based Metrics,[0],[0]
"Instead, given two sequences r and r̂, each token w ∈ r is greedily matched with a token ŵ ∈ r̂ based on the cosine similarity of their word embeddings (ew), and the total score is then averaged across all words:
G(r, r̂) =
∑ w∈r; maxŵ∈r̂ cos sim(ew, eŵ)
|r|
GM(r, r̂) = G(r, r̂) +G(r̂, r)
2
",3.2 Embedding-based Metrics,[0],[0]
"This formula is asymmetric, thus we must average the greedy matching scores G in each direction.",3.2 Embedding-based Metrics,[0],[0]
"This was originally introduced for intelligent tutoring systems (Rus and Lintean, 2012).",3.2 Embedding-based Metrics,[0],[0]
"The greedy approach favours responses with key words that are semantically similar to those in the ground truth response.
",3.2 Embedding-based Metrics,[0],[0]
Embedding Average.,3.2 Embedding-based Metrics,[0],[0]
"The embedding average metric calculates sentence-level embeddings using additive composition, a method for computing the meanings of phrases by averaging the vector representations of their constituent words (Foltz et al., 1998; Landauer and Dumais, 1997; Mitchell and Lapata, 2008).",3.2 Embedding-based Metrics,[0],[0]
"This method has been widely used in other domains, for example in textual similarity
4To maintain statistical independence between the task and each performance metric, it is important that the word embeddings used are trained on corpora which do not overlap with the task corpus.
tasks (Wieting et al., 2015).",3.2 Embedding-based Metrics,[0],[0]
"The embedding average, ē, is defined as the mean of the word embeddings of each token in a sentence r:
ēr =
∑ w∈r ew
|∑w′∈r ew′",3.2 Embedding-based Metrics,[0],[0]
"| .
",3.2 Embedding-based Metrics,[0],[0]
"To compare a ground truth response r and retrieved response r̂, we compute the cosine similarity between their respective sentence level embeddings: EA := cos(ēr, ēr̂).
",3.2 Embedding-based Metrics,[0],[0]
Vector Extrema.,3.2 Embedding-based Metrics,[0],[0]
"Another way to calculate sentence-level embeddings is using vector extrema (Forgues et al., 2014).",3.2 Embedding-based Metrics,[0],[0]
"For each dimension of the word vectors, take the most extreme value amongst all word vectors in the sentence, and use that value in the sentence-level embedding:
erd = { maxw∈r ewd if ewd > |minw′∈r",3.2 Embedding-based Metrics,[0],[0]
"ew′d| minw∈r ewd otherwise
where d indexes the dimensions of a vector; ewd is the d’th dimensions of ew (w’s embedding).",3.2 Embedding-based Metrics,[0],[0]
"The min in this equation refers to the selection of the largest negative value, if it has a greater magnitude than the largest positive value.
",3.2 Embedding-based Metrics,[0],[0]
Similarity between response vectors is again computed using cosine distance.,3.2 Embedding-based Metrics,[0],[0]
"Intuitively, this approach prioritizes informative words over common ones; words that appear in similar contexts will be close together in the vector space.",3.2 Embedding-based Metrics,[0],[0]
"Thus, common words are pulled towards the origin because they occur in various contexts, while words carrying important semantic information will lie further away.",3.2 Embedding-based Metrics,[0],[0]
"By taking the extrema along each dimension, we are thus more likely to ignore common words.",3.2 Embedding-based Metrics,[0],[0]
"In order to determine the correlation between automatic metrics and human judgements of response quality, we obtain response from a diverse range of response generation models in the recent literature, including both retrieval and generative models.",4 Dialogue Response Generation Models,[0],[0]
"Ranking or retrieval models for dialogue systems are typically evaluated based on whether they can retrieve the correct response from a corpus of predefined responses, which includes the ground truth
response to the conversation (Schatzmann et al., 2005).",4.1 Retrieval Models,[0],[0]
Such systems can be evaluated using recall or precision metrics.,4.1 Retrieval Models,[0],[0]
"However, when deployed in a real setting these models will not have access to the correct response given an unseen conversation.",4.1 Retrieval Models,[0],[0]
"Thus, in the results presented below we remove one occurrence of the ground-truth response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances.",4.1 Retrieval Models,[0],[0]
"Note that this does not mean the correct response will not appear in the corpus at all; in particular, if there exists another context in the dataset with an identical ground-truth response, this will be available for selection by the model.
",4.1 Retrieval Models,[0],[0]
We then evaluate each model by comparing the retrieved response to the ground truth response of the conversation.,4.1 Retrieval Models,[0],[0]
"This closely imitates real-life deployment of these models, as it tests the ability of the model to generalize to unseen contexts.
",4.1 Retrieval Models,[0],[0]
TF-IDF.,4.1 Retrieval Models,[0],[0]
"We consider a simple Term Frequency - Inverse Document Frequency (TF-IDF) retrieval model (Lowe et al., 2015).",4.1 Retrieval Models,[0],[0]
"TF-IDF is a statistic that intends to capture how important a given word is to some document, which is calculated as: tfidf(w, c, C) = f(w, c)× log N|{c∈C:w∈c}| , where C is the set of all contexts in the corpus, f(w, c) indicates the number of times word w appeared in context c, N is the total number of dialogues, and the denominator represents the number of dialogues in which the word w appears.
",4.1 Retrieval Models,[0],[0]
"In order to apply TF-IDF as a retrieval model for dialogue, we first compute the TF-IDF vectors for each context and response in the corpus.",4.1 Retrieval Models,[0],[0]
"We then return the response with the largest cosine similarity in the corpus, either between the input context and corpus contexts (C-TFIDF), or between the input context and corpus responses (R-TFIDF).
",4.1 Retrieval Models,[0],[0]
Dual Encoder.,4.1 Retrieval Models,[0],[0]
"Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al., 2015).",4.1 Retrieval Models,[0],[0]
"The DE model consists of two RNNs which respectively compute the vector representation of an input context and response, c, r ∈ Rn.",4.1 Retrieval Models,[0],[0]
"The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: p(r is correct|c, r,M) =",4.1 Retrieval Models,[0],[0]
σ(cTMr + b) where M is a matrix of learned parameters and b is a bias.,4.1 Retrieval Models,[0],[0]
"The model is trained using negative sampling to minimize the cross-entropy error of all (context, response) pairs.",4.1 Retrieval Models,[0],[0]
"To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel.",4.1 Retrieval Models,[0],[0]
"In addition to retrieval models, we also consider generative models.",4.2 Generative Models,[0],[0]
"In this context, we refer to a model as generative if it is able to generate entirely new sentences that are unseen in the training set.
",4.2 Generative Models,[0],[0]
LSTM language model.,4.2 Generative Models,[0],[0]
"The baseline model is an LSTM language model (Hochreiter and Schmidhuber, 1997) trained to predict the next word in the (context, response) pair.",4.2 Generative Models,[0],[0]
"During test time, the model is given a context, encodes it with the LSTM and generates a response using a greedy beam search procedure (Graves, 2013).
HRED.",4.2 Generative Models,[0],[0]
Finally we consider the Hierarchical Recurrent Encoder-Decoder (HRED),4.2 Generative Models,[0],[0]
"(Serban et al., 2015).",4.2 Generative Models,[0],[0]
"In the traditional Encoder-Decoder framework, all utterances in the context are concatenated together before encoding.",4.2 Generative Models,[0],[0]
"Thus, information from previous utterances is far outweighed by the most recent utterance.",4.2 Generative Models,[0],[0]
"The HRED model uses a hierarchy of encoders; each utterance in the context passes through an ‘utterance-level’ encoder, and the
output of these encoders is passed through another ‘context-level’ encoder, which enables the handling of longer-term dependencies.",4.2 Generative Models,[0],[0]
"When evaluation metrics are not explicitly correlated to human judgement, it is possible to draw misleading conclusions by examining how the metrics rate different models.",4.3 Conclusions from an Incomplete Analysis,[0],[0]
"To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which contains technical vocabulary and where conversations are often oriented towards solv-
ing a particular problem, and a non-technical Twitter corpus collected following the procedure of Ritter et al. (2010).",4.3 Conclusions from an Incomplete Analysis,[0],[0]
"We consider these two datasets since they cover contrasting dialogue domains, i.e. technical help vs casual chit-chat, and because they are amongst the largest publicly available corpora, making them good candidates for building data-driven dialogue systems.
",4.3 Conclusions from an Incomplete Analysis,[0],[0]
Results on the proposed embedding metrics are shown in Table 2.,4.3 Conclusions from an Incomplete Analysis,[0],[0]
"For the retrieval models, we observe that the DE model significantly outperforms both TFIDF baselines on all metrics across both datasets.",4.3 Conclusions from an Incomplete Analysis,[0],[0]
"Further, the HRED model significantly outperforms the basic LSTM generative model in both domains, and appears to be of similar strength as the DE model.",4.3 Conclusions from an Incomplete Analysis,[0],[0]
"Based on these results, one might be tempted to conclude that there is some information being captured by these metrics, that significantly differentiates models of different quality.",4.3 Conclusions from an Incomplete Analysis,[0],[0]
"However, as we show in the next section, the embedding-based metrics correlate only weakly with human judgements on the Twitter corpus, and not at all on the Ubuntu Dialogue Corpus.",4.3 Conclusions from an Incomplete Analysis,[0],[0]
This demonstrates that metrics that have not been specifically correlated with human judgements on a new task should not be used to evaluate that task.,4.3 Conclusions from an Incomplete Analysis,[0],[0]
Data Collection.,5 Human Correlation Analysis,[0],[0]
"We conducted a human survey to determine the correlation between human judgements on the quality of responses, and the score assigned by each metric.",5 Human Correlation Analysis,[0],[0]
"We aimed to follow the procedure for the evaluation of BLEU (Papineni et al.,
2002a).",5 Human Correlation Analysis,[0],[0]
"25 volunteers from the Computer Science department at the author’s institution were given a context and one proposed response, and were asked to judge the response quality on a scale of 1 to 5.5; a 1 indicates that the response is not appropriate or sensible given the context, and a 5 indicates that the response is very reasonable.",5 Human Correlation Analysis,[0],[0]
"Out of the 25 respondents, 23 had Cohen’s kappa scores κ > 0.2 w.r.t.",5 Human Correlation Analysis,[0],[0]
"the other respondents, which is a standard measure for inter-rater agreement (Cohen, 1968).",5 Human Correlation Analysis,[0],[0]
"The 2 respondents with κ < 0.2, indicating slight agreement, were excluded from the analysis below.",5 Human Correlation Analysis,[0],[0]
"The median κ score was approximately 0.55, roughly indicating moderate to strong annotator agreement.
",5 Human Correlation Analysis,[0],[0]
Each volunteer was given 100 questions per dataset.,5 Human Correlation Analysis,[0],[0]
"These questions correspond to 20 unique contexts, with 5 different responses: one utterance
5Studies asking humans to evaluate text often rate different aspects separately, such as ‘adequacy’, ‘fluency’ and ‘informativeness’ of the text (Hovy, 1999; Papineni et al., 2002b)",5 Human Correlation Analysis,[0],[0]
Our evaluation focuses on adequacy.,5 Human Correlation Analysis,[0],[0]
We did not consider fluency because 4 out of the 5 proposed responses to each context were generated by a human.,5 Human Correlation Analysis,[0],[0]
"We did not consider informativeness because in the domains considered, it is not necessarily important (in Twitter), or else it seems to correlate highly with adequacy (in Ubuntu).
randomly drawn from elsewhere in the test set, the response selected from each of the TF-IDF, DE, and HRED models, and a response written by a human annotator.",5 Human Correlation Analysis,[0],[0]
"These were chosen as they cover the range of qualities almost uniformly (see Figure 1).
",5 Human Correlation Analysis,[0],[0]
Survey Results.,5 Human Correlation Analysis,[0],[0]
We present correlation results between the human judgements and each metric in Table 3.,5 Human Correlation Analysis,[0],[0]
"We compute the Pearson correlation, which estimates linear correlation, and Spearman correlation, which estimates any monotonic correlation.
",5 Human Correlation Analysis,[0],[0]
"The first observation is that in both domains the BLEU-4 score, which has previously been used to evaluate unsupervised dialogue systems, shows very weak if any correlation with human judgement.",5 Human Correlation Analysis,[0],[0]
"In fact we found that the BLEU-3 and BLEU-4 scores were near-zero for a majority of response pairs; for BLEU-4, only four examples had a score > 10−9.",5 Human Correlation Analysis,[0],[0]
"Despite this, they still correlate with human judgements on the Twitter Corpus at a rate similar to BLEU-2.",5 Human Correlation Analysis,[0],[0]
"This is because of the smoothing constant, which gives a tiny weight to unigrams and bigrams despite the absence of higher-order n-grams.",5 Human Correlation Analysis,[0],[0]
"BLEU-3 and BLEU-4 behave as a scaled, noisy version of BLEU-2; thus, if one is to evaluate dialogue
responses with BLEU, we recommend the choice of N = 2 over N = 3 or 4.",5 Human Correlation Analysis,[0],[0]
"Note that using a test corpus larger than the size reported in this paper may lead to stronger correlations for BLEU-3 and BLEU4, due to a higher number of non-zero scores.
",5 Human Correlation Analysis,[0],[0]
"It is interesting to note that, while some of the embedding metrics and BLEU show small positive correlation in the non-technical Twitter domain, there is no metric that significantly correlates with humans on the Ubuntu Dialogue Corpus.",5 Human Correlation Analysis,[0],[0]
This is likely because the correct Ubuntu responses contain specific technical words that are less likely to be produced by our models.,5 Human Correlation Analysis,[0],[0]
"Further, it is possible that responses in the Ubuntu Dialogue Corpus have intrinsically higher variability (or entropy) than Twitter when conditioned on the context, making the evaluation problem significantly more difficult.
",5 Human Correlation Analysis,[0],[0]
Figure 1 illustrates the relationship between metrics and human judgements.,5 Human Correlation Analysis,[0],[0]
"We include only the best performing metric using word-overlaps, i.e. the BLEU-2 score (left), and the best performing metric using word embeddings, i.e. the vector average (center).",5 Human Correlation Analysis,[0],[0]
"These plots show how weak the correlation is: in both cases, they appear to be random noise.",5 Human Correlation Analysis,[0],[0]
It seems as though the BLEU score obtains a positive correlation because of the large number of responses that are given a score of 0 (bottom left corner of the first plot).,5 Human Correlation Analysis,[0],[0]
"This is in stark contrast to the inter-rater agreement, which is plotted between two randomly sampled halves of the raters (right-most plots).",5 Human Correlation Analysis,[0],[0]
We also calculated the BLEU scores after removing stopwords and punctuation from the responses.,5 Human Correlation Analysis,[0],[0]
"As shown in Table 4, this weakens the cor-
relation with human judgements for BLEU-2 compared to the values in Table 3, and suggests that BLEU is sensitive to factors that do not change the semantics of the response.
",5 Human Correlation Analysis,[0],[0]
"Finally, we examined the effect of response length on the metrics, by considering changes in scores when the ground truth and proposed response had a large difference in word counts.",5 Human Correlation Analysis,[0],[0]
"Table 4 shows that BLEU and METEOR are particularly sensitive to this aspect, compared to the Embedding Average metric and human judgement.
",5 Human Correlation Analysis,[0],[0]
Qualitative Analysis.,5 Human Correlation Analysis,[0],[0]
"In order to determine specifically why the metrics fail, we examine qualitative samples where there is a disagreement between the metrics and human rating.",5 Human Correlation Analysis,[0],[0]
"Although these only show inconsistencies at the example-level, they provide some intuition as to why the metrics don’t correlate with human judgements at the corpuslevel.",5 Human Correlation Analysis,[0],[0]
"We present in Figure 2 two examples where all of the embedding-based metrics and BLEU-1 score the proposed response significantly differently than the humans.
",5 Human Correlation Analysis,[0],[0]
"The left of Figure 2 shows an example where the embedding-based metrics score the proposed response lowly, while humans rate it highly.",5 Human Correlation Analysis,[0],[0]
It is clear from the context that the proposed response is reasonable – indeed both responses intend to express gratitude.,5 Human Correlation Analysis,[0],[0]
"However, the proposed response has a different wording than the ground truth response, and therefore the metrics are unable to separate the salient words from the rest.",5 Human Correlation Analysis,[0],[0]
"This suggests that the embedding-based metrics would ben-
efit from a weighting of word saliency.",5 Human Correlation Analysis,[0],[0]
"The right of the figure shows the reverse scenario: the embedding-based metrics score the proposed response highly, while humans do not.",5 Human Correlation Analysis,[0],[0]
"This is most likely due to the frequently occurring ‘i’ token, and the fact that ‘happy’ and ‘welcome’ may be close together in the embedding space.",5 Human Correlation Analysis,[0],[0]
"However, from a human perspective there is a significant semantic difference between the responses as they pertain to the context.",5 Human Correlation Analysis,[0],[0]
Metrics that take into account the context may be required in order to differentiate these responses.,5 Human Correlation Analysis,[0],[0]
"Note that in both responses in Figure 2, there are no overlapping n-grams greater than unigrams between the ground truth and proposed responses; thus, all of BLEU-2,3,4 would assign a score near 0 to the response.",5 Human Correlation Analysis,[0],[0]
We have shown that many metrics commonly used in the literature for evaluating unsupervised dialogue systems do not correlate strongly with human judgement.,6 Discussion,[0],[0]
"Here we elaborate on important issues arising from our analysis.
",6 Discussion,[0],[0]
Constrained tasks.,6 Discussion,[0],[0]
Our analysis focuses on relatively unconstrained domains.,6 Discussion,[0],[0]
"Other work, which separates the dialogue system into a dialogue planner and a natural language generation component for applications in constrained domains, may find stronger correlations with the BLEU metric.",6 Discussion,[0],[0]
"For example, Wen et al. (2015) propose a model to map from dialogue acts to natural language sentences and use BLEU to evaluate the quality of the generated sentences.",6 Discussion,[0],[0]
"Since the mapping from dialogue acts to natural language sentences has lower diversity and is more similar to the machine translation task, it seems likely that BLEU will correlate better with human judgements.",6 Discussion,[0],[0]
"However, an empirical investigation is still necessary to justify this.
",6 Discussion,[0],[0]
Incorporating multiple responses.,6 Discussion,[0],[0]
Our correlation results assume that only one ground truth response is available given each context.,6 Discussion,[0],[0]
"Indeed, this is the common setting in most of the recent literature on training end-to-end conversation models.",6 Discussion,[0],[0]
"There has been some work on using a larger set of automatically retrieved plausible responses when evaluating with BLEU (Galley et al., 2015b).",6 Discussion,[0],[0]
"However,
there is no standard method for doing this in the literature.",6 Discussion,[0],[0]
"Future work should examine how retrieving additional responses affects the correlation with word-overlap metrics.
",6 Discussion,[0],[0]
Searching for suitable metrics.,6 Discussion,[0],[0]
"While we provide evidence against existing metrics, we do not yet provide good alternatives for unsupervised evaluation.",6 Discussion,[0],[0]
"Despite the poor performance of the word embedding-based metrics in this survey, we believe that metrics based on distributed sentence representations hold the most promise for the future.",6 Discussion,[0],[0]
"This is because word-overlap metrics will simply require too many ground-truth responses to find a significant match for a reasonable response, due to the high diversity of dialogue responses.",6 Discussion,[0],[0]
"As a simple example, the skip-thought vectors of Kiros et al. (2015) could be considered.",6 Discussion,[0],[0]
"Since the embedding-based metrics in this paper only consist of basic averages of vectors obtained through distributional semantics, they are insufficiently complex for modeling sentence-level compositionality in dialogue.",6 Discussion,[0],[0]
"Instead, these metrics can be interpreted as calculating the topicality of a proposed response (i.e. how on-topic the proposed response is, compared to the ground-truth).
",6 Discussion,[0],[0]
"All of the metrics considered in this paper directly compare a proposed response to the ground-truth, without considering the context of the conversation.",6 Discussion,[0],[0]
"However, metrics that take into account the context could also be considered.",6 Discussion,[0],[0]
Such metrics could come in the form of an evaluation model that is learned from data.,6 Discussion,[0],[0]
"This model could be either a discriminative model that attempts to distinguish between model and human responses, or a model that uses data collected from the human survey in order to provide human-like scores to proposed responses.",6 Discussion,[0],[0]
"Finally, we must consider the hypothesis that learning such models from data is no easier than solving the problem of dialogue response generation.",6 Discussion,[0],[0]
"If this hypothesis is true, we must concede and always use human evaluations together with metrics that only roughly approximate human judgements.",6 Discussion,[0],[0]
"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available.",abstractText,[0],[0]
Recent works in response generation have adopted metrics from machine translation to compare a model’s generated response to a single target response.,abstractText,[0],[0]
"We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain.",abstractText,[0],[0]
"We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.",abstractText,[0],[0]
How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation,title,[0],[0]
"Given a function f : Rd → R, a gradient descent aims to minimize the function via the following iteration:
xt+1 = xt",1. Introduction,[0],[0]
"− η∇f(xt),
where η > 0 is a step size.",1. Introduction,[0],[0]
"Gradient descent and its variants (e.g., stochastic gradient) are widely used in machine learning applications due to their favorable computational properties.",1. Introduction,[0],[0]
"This is notably true in the deep learning setting, where gradients can be computed efficiently via backpropagation (Rumelhart et al., 1988).
",1. Introduction,[0],[0]
"Gradient descent is especially useful in high-dimensional settings because the number of iterations required to reach
1University of California, Berkeley 2Duke University 3Microsoft Research India 4University of Washington.",1. Introduction,[0],[0]
"Correspondence to: Chi Jin <chijin@berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
a point with small gradient is independent of the dimension (“dimension-free”).",1. Introduction,[0],[0]
"More precisely, for a function that is `- gradient Lipschitz (see Definition 1), it is well known that gradient descent finds an -first-order stationary point (i.e., a point x with ‖∇f(x)‖ ≤ ) within `(f(x0)",1. Introduction,[0],[0]
"− f?)/ 2 iterations (Nesterov, 1998), where x0 is the initial point and f? is the optimal value of f .",1. Introduction,[0],[0]
"This bound does not depend on the dimension of x. In convex optimization, finding an -first-order stationary point is equivalent to finding an approximate global optimum.
",1. Introduction,[0],[0]
"In non-convex settings, however, convergence to first-order stationary points is not satisfactory.",1. Introduction,[0],[0]
"For non-convex functions, first-order stationary points can be global minima, local minima, saddle points or even local maxima.",1. Introduction,[0],[0]
"Finding a global minimum can be hard, but fortunately, for many non-convex problems, it is sufficient to find a local minimum.",1. Introduction,[0],[0]
"Indeed, a line of recent results show that, in many problems of interest, all local minima are global minima (e.g., in tensor decomposition (Ge et al., 2015), dictionary learning (Sun et al., 2016a), phase retrieval (Sun et al., 2016b), matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016), matrix completion (Ge et al., 2016), and certain classes of deep neural networks (Kawaguchi, 2016)).",1. Introduction,[0],[0]
"Moreover, there are suggestions that in more general deep networks most of the local minima are as good as global minima (Choromanska et al., 2014).
",1. Introduction,[0],[0]
"On the other hand, saddle points (and local maxima) can correspond to highly suboptimal solutions in many problems (see, e.g., Jain et al., 2015; Sun et al., 2016b).",1. Introduction,[0],[0]
"Furthermore, Dauphin et al. (2014) argue that saddle points are ubiquitous in high-dimensional, non-convex optimization problems, and are thus the main bottleneck in training neural networks.",1. Introduction,[0],[0]
"Standard analysis of gradient descent cannot distinguish between saddle points and local minima, leaving open the possibility that gradient descent may get stuck at saddle points, either asymptotically or for a sufficiently long time so as to make training times for arriving at a local minimum infeasible.",1. Introduction,[0],[0]
"Ge et al. (2015) showed that by adding noise at each step, gradient descent can escape all saddle points in a polynomial number of iterations, provided that the objective function satisfies the strict saddle property (see Assumption A2).",1. Introduction,[0],[0]
"Lee et al. (2016) proved that under similar conditions, gradient descent with random initialization avoids saddle points even without adding
Algorithm 1 Perturbed Gradient Descent (Meta-algorithm) for t = 0, 1, . . .",1. Introduction,[0],[0]
"do
if perturbation condition holds then xt ← xt + ξt, ξt uniformly ∼ B0(r) xt+1",1. Introduction,[0],[0]
← xt,1. Introduction,[0],[0]
"− η∇f(xt)
noise.",1. Introduction,[0],[0]
"However, this result does not bound the number of steps needed to reach a local minimum.
",1. Introduction,[0],[0]
"Previous work explains why gradient descent avoids saddle points in the nonconvex setting, but not why it is efficient—all of them have runtime guarantees with high polynomial dependency in dimension d. For instance, the number of iterations required in Ge et al. (2015) is at least Ω(d4), which is prohibitive in high dimensional setting such as deep learning (typically with millions of parameters).",1. Introduction,[0],[0]
"Therefore, we wonder whether gradient descent type of algorithms are fundamentally slow in escaping saddle points, or it is the lack of our theoretical understanding while gradient descent is indeed efficient.",1. Introduction,[0],[0]
"This motivates the following question: Can gradient descent escape saddle points and converge to local minima in a number of iterations that is (almost) dimension-free?
",1. Introduction,[0],[0]
"In order to answer this question formally, this paper investigates the complexity of finding -second-order stationary points.",1. Introduction,[0],[0]
"For ρ-Hessian Lipschitz functions (see Definition 5), these points are defined as (Nesterov & Polyak, 2006):
‖∇f(x)‖ ≤ , and λmin(∇2f(x))",1. Introduction,[0],[0]
≥,1. Introduction,[0],[0]
"− √ ρ .
",1. Introduction,[0],[0]
"Under the assumption that all saddle points are strict (i.e., for any saddle point xs, λmin(∇2f(xs))",1. Introduction,[0],[0]
"< 0), all second-order stationary points ( = 0) are local minima.",1. Introduction,[0],[0]
"Therefore, convergence to second-order stationary points is equivalent to convergence to local minima.
",1. Introduction,[0],[0]
"This paper studies a simple variant of gradient descent (with phasic perturbations, see Algorithm 1).",1. Introduction,[0],[0]
"For `-smooth functions that are also Hessian Lipschitz, we show that perturbed gradient descent will converge to an -second-order stationary point in Õ(`(f(x0)− f?)/ 2), where Õ(·) hides polylog factors.",1. Introduction,[0],[0]
"This guarantee is almost dimension free (up to polylog(d) factors), answering the above highlighted question affirmatively.",1. Introduction,[0],[0]
"Note that this rate is exactly the same as the well-known convergence rate of gradient descent to first-order stationary points (Nesterov, 1998), up to log factors.",1. Introduction,[0],[0]
"Furthermore, our analysis admits a maximal step size of up to Ω(1/`), which is the same as that in analyses for first-order stationary points.
",1. Introduction,[0],[0]
"As many real learning problems present strong local geometric properties, similar to strong convexity in the global setting (see, e.g. Bhojanapalli et al., 2016; Sun & Luo, 2016; Zheng & Lafferty, 2016), it is important to note that our analysis naturally takes advantage of such local struc-
ture.",1. Introduction,[0],[0]
"We show that when local strong convexity is present, the -dependence goes from a polynomial rate, 1/ 2, to linear convergence, log(1/ ).",1. Introduction,[0],[0]
"As an example, we show that sharp global convergence rates can be obtained for matrix factorization as a direct consequence of our analysis.",1. Introduction,[0],[0]
"This paper presents the first sharp analysis that shows that (perturbed) gradient descent finds an approximate secondorder stationary point in at most polylog(d) iterations, thus escaping all saddle points efficiently.",1.1. Our Contributions,[0],[0]
"Our main technical contributions are as follows:
• For `-gradient Lipschitz, ρ-Hessian Lipschitz functions (possibly non-convex), gradient descent with appropriate perturbations finds an -second-order stationary point in Õ(`(f(x0)−f?)/ 2) iterations.",1.1. Our Contributions,[0],[0]
"This rate matches the well-known convergence rate of gradient descent to first-order stationary points up to log factors.
",1.1. Our Contributions,[0],[0]
"• Under a strict-saddle condition (see Assumption A2), the same convergence result applies for local minima.",1.1. Our Contributions,[0],[0]
"This means that gradient descent can escape all saddle points with only logarithmic overhead in runtime.
",1.1. Our Contributions,[0],[0]
"• When the function has local structure, such as local strong convexity (see Assumption A3.a), the above results can be further improved to linear convergence.",1.1. Our Contributions,[0],[0]
"We give sharp rates that are comparable to previous problem-specific local analysis of gradient descent with smart initialization (see Section 1.2).
",1.1. Our Contributions,[0],[0]
• All the above results rely on a new characterization of the geometry around saddle points: points from where gradient descent gets stuck at a saddle point constitute a thin “band.”,1.1. Our Contributions,[0],[0]
We develop novel techniques to bound the volume of this band.,1.1. Our Contributions,[0],[0]
"As a result, we can show that after a random perturbation the current point is very unlikely to be in the “band”; hence, efficient escape from the saddle point is possible (see Section 5).",1.1. Our Contributions,[0],[0]
"Over the past few years, there have been many problemspecific convergence results for non-convex optimization.",1.2. Related Work,[0],[0]
"One line of work requires a smart initialization algorithm to provide a coarse estimate lying inside a local neighborhood, from which popular local search algorithms enjoy fast local convergence (see, e.g., Netrapalli et al., 2013; Candes et al., 2015; Sun & Luo, 2016; Bhojanapalli et al., 2016).",1.2. Related Work,[0],[0]
"While there are not many results that show global convergence for non-convex problems, Jain et al. (2015) show that gradient descent yields global convergence rates for matrix square-root problems.",1.2. Related Work,[0],[0]
"Although these results
give strong guarantees, the analyses are heavily tailored to specific problems, and it is unclear how to generalize them to a wider class of non-convex functions.
",1.2. Related Work,[0],[0]
"For general non-convex optimization, there are a few previous results on finding second-order stationary points.",1.2. Related Work,[0],[0]
"These results can be divided into the following three categories, where, for simplicity of presentation, we only highlight dependence on dimension d and , assuming that all other problem parameters are constant from the point of view of iteration complexity:
Hessian-based: Traditionally, only second-order optimization methods were known to converge to second-order stationary points.",1.2. Related Work,[0],[0]
These algorithms rely on computing the Hessian to distinguish between first- and second-order stationary points.,1.2. Related Work,[0],[0]
Nesterov & Polyak (2006) designed a cubic regularization algorithm which converges to an -secondorder stationary point in O(1/ 1.5) iterations.,1.2. Related Work,[0],[0]
"Trust region algorithms (Curtis et al., 2014) can also achieve the same performance if the parameters are chosen carefully.",1.2. Related Work,[0],[0]
"These algorithms typically require the computation of the inverse of the full Hessian per iteration, which can be very expensive.
",1.2. Related Work,[0],[0]
Hessian-vector-product-based: A number of recent papers have explored the possibility of using only Hessianvector products instead of full Hessian information in order to find second-order stationary points.,1.2. Related Work,[0],[0]
"These algorithms require a Hessian-vector product oracle: given a function f , a point x and a direction u, the oracle returns ∇2f(x) · u. Agarwal et al. (2016) and Carmon et al. (2016) presented accelerated algorithms that can find an -second-order stationary point in O(log d/ 7/4) steps.",1.2. Related Work,[0],[0]
"Also, Carmon & Duchi (2016) showed by running gradient descent as a subroutine to solve the subproblem of cubic regularization
(which requires Hessian-vector product oracle), it is possible to find an -second-order stationary pointinO(log d/ 2) iterations.",1.2. Related Work,[0],[0]
"In many applications such an oracle can be implemented efficiently, in roughly the same complexity as the gradient oracle.",1.2. Related Work,[0],[0]
"Also, when the function has a Hessian Lipschitz property such an oracle can be approximated by differentiating the gradients at two very close points (although this may suffer from numerical issues, thus is seldom used in practice).
",1.2. Related Work,[0],[0]
Gradient-based: Another recent line of work shows that it is possible to converge to a second-order stationary point without any use of the Hessian.,1.2. Related Work,[0],[0]
"These methods feature simple computation per iteration (only involving gradient operations), and are closest to the algorithms used in practice.",1.2. Related Work,[0],[0]
"Ge et al. (2015) showed that stochastic gradient descent could converge to a second-order stationary point in poly(d/ ) iterations, with polynomial of order at least four.",1.2. Related Work,[0],[0]
This was improved in Levy (2016) to O(d3 · poly(1/ )) using normalized gradient descent.,1.2. Related Work,[0],[0]
"The current paper improves on both results by showing that perturbed gradient descent can actually find an -second-order stationary point in O(polylog(d)/ 2) steps, which matches the guarantee for converging to first-order stationary points up to polylog factors.",1.2. Related Work,[0],[0]
"In this section, we will first introduce our notation, and then present some definitions and existing results in optimization which will be used later.",2. Preliminaries,[0],[0]
"We use bold upper-case letters A,B to denote matrices and bold lower-case letters x,y to denote vectors.",2.1. Notation,[0],[0]
"Aij means the (i, j)th entry of matrix A. For vectors we use ‖·‖ to denote the `2-norm, and for matrices we use ‖·‖ and ‖·‖F to denote spectral norm and Frobenius norm respectively.",2.1. Notation,[0],[0]
"We use σmax(·), σmin(·), σi(·) to denote the largest, the smallest and the i-th largest singular values respectively, and λmax(·), λmin(·), λi(·) for corresponding eigenvalues.
",2.1. Notation,[0],[0]
"For a function f : Rd → R, we use ∇f(·) and ∇2f(·) to denote its gradient and Hessian, and f? to denote the global minimum of f(·).",2.1. Notation,[0],[0]
"We use notation O(·) to hide only absolute constants which do not depend on any problem parameter, and notation Õ(·) to hide only absolute constants and log factors.",2.1. Notation,[0],[0]
"We let B(d)x (r) denote the d-dimensional ball centered at x with radius r; when it is clear from context, we simply denote it as Bx(r).",2.1. Notation,[0],[0]
We use PX (·) to denote projection onto the set X .,2.1. Notation,[0],[0]
Distance and projection are always defined in a Euclidean sense.,2.1. Notation,[0],[0]
The theory of gradient descent often takes its point of departure to be the study of convex optimization.,2.2. Gradient Descent,[0],[0]
Definition 1.,2.2. Gradient Descent,[0],[0]
A differentiable function f(·) is `-smooth (or `-gradient Lipschitz),2.2. Gradient Descent,[0],[0]
"if:
∀x1,x2, ‖∇f(x1)−∇f(x2)‖ ≤ `‖x1 − x2‖.
Definition 2.",2.2. Gradient Descent,[0],[0]
"A twice-differentiable function f(·) is αstrongly convex if ∀x, λmin(∇2f(x))",2.2. Gradient Descent,[0],[0]
"≥ α
Such smoothness guarantees imply that the gradient can not change too rapidly, and strong convexity ensures that there is a unique stationary point (and hence a global minimum).",2.2. Gradient Descent,[0],[0]
Standard analysis using these two properties shows that gradient descent converges linearly to a global optimum x?,2.2. Gradient Descent,[0],[0]
"(see e.g. (Bubeck et al., 2015)).",2.2. Gradient Descent,[0],[0]
Theorem 1.,2.2. Gradient Descent,[0],[0]
Assume f(·) is `-smooth and α-strongly convex.,2.2. Gradient Descent,[0],[0]
"For any > 0, if we run gradient descent with step size η = 1` , iterate xt will be -close to x ?",2.2. Gradient Descent,[0],[0]
"in iterations:
2` α log ‖x0",2.2. Gradient Descent,[0],[0]
"− x?‖
In a more general setting, we no longer have convexity, let alone strong convexity.",2.2. Gradient Descent,[0],[0]
"Though global optima are difficult to achieve in such a setting, it is possible to analyze convergence to first-order stationary points.",2.2. Gradient Descent,[0],[0]
Definition 3.,2.2. Gradient Descent,[0],[0]
"For a differentiable function f(·), we say that x is a first-order stationary point if ‖∇f(x)‖ = 0; we also say x is an -first-order stationary point if ‖∇f(x)‖ ≤ .
",2.2. Gradient Descent,[0],[0]
"Under an `-smoothness assumption, it is well known that by choosing the step size η",2.2. Gradient Descent,[0],[0]
"= 1` , gradient descent converges to first-order stationary points.",2.2. Gradient Descent,[0],[0]
"Theorem 2 ((Nesterov, 1998)).",2.2. Gradient Descent,[0],[0]
Assume that the function f(·) is `-smooth.,2.2. Gradient Descent,[0],[0]
"Then, for any > 0, if we run gradient descent with step size η = 1` and termination condition ‖∇f(x)‖ ≤ , the output will be -first-order stationary point, and the algorithm will terminate within the following number of iterations:
`(f(x0)− f?) 2 .
",2.2. Gradient Descent,[0],[0]
"Note that the iteration complexity does not depend explicitly on intrinsic dimension; in the literature this is referred to as “dimension-free optimization.”
Note that a first-order stationary point can be either a local minimum or a saddle point or a local maximum.",2.2. Gradient Descent,[0],[0]
"For minimization problems, saddle points and local maxima are undesirable, and we abuse nomenclature to call both of them “saddle points” in this paper.",2.2. Gradient Descent,[0],[0]
"The formal definition is as follows:
Definition 4.",2.2. Gradient Descent,[0],[0]
"For a differentiable function f(·), we say that x is a local minimum if x is a first-order stationary point, and there exists > 0",2.2. Gradient Descent,[0],[0]
"so that for any y in the - neighborhood of x, we have f(x) ≤ f(y); we also say x is a saddle point if x is a first-order stationary point but not a local minimum.",2.2. Gradient Descent,[0],[0]
"For a twice-differentiable function f(·), we further say a saddle point x is strict (or nondegenerate) if λmin(∇2f(x))",2.2. Gradient Descent,[0],[0]
"< 0.
",2.2. Gradient Descent,[0],[0]
"For a twice-differentiable function f(·), we know a saddle point x must satify λmin(∇2f(x)) ≤ 0.",2.2. Gradient Descent,[0],[0]
"Intuitively, for saddle point x to be strict, we simply rule out the undetermined case λmin(∇2f(x))",2.2. Gradient Descent,[0],[0]
"= 0, where Hessian information alone is not enough to check whether x is a local minimum or saddle point.",2.2. Gradient Descent,[0],[0]
"In most non-convex problems, saddle points are undesirable.
",2.2. Gradient Descent,[0],[0]
"To escape from saddle points and find local minima in a general setting, we move both the assumptions and guarantees in Theorem 2 one order higher.",2.2. Gradient Descent,[0],[0]
"In particular, we require the Hessian to be Lipschitz:
Definition 5.",2.2. Gradient Descent,[0],[0]
"A twice-differentiable function f(·) is ρHessian Lipschitz if:
∀x1,x2, ‖∇2f(x1)−∇2f(x2)‖ ≤ ρ‖x1",2.2. Gradient Descent,[0],[0]
"− x2‖.
That is, Hessian can not change dramatically in terms of spectral norm.",2.2. Gradient Descent,[0],[0]
"We also generalize the definition of firstorder stationary point to higher order:
Definition 6.",2.2. Gradient Descent,[0],[0]
"For a ρ-Hessian Lipschitz function f(·), we say that x is a second-order stationary point if ‖∇f(x)‖ = 0 and λmin(∇2f(x))",2.2. Gradient Descent,[0],[0]
"≥ 0; we also say x is -second-order stationary point if:
‖∇f(x)‖ ≤ , and λmin(∇2f(x))",2.2. Gradient Descent,[0],[0]
≥,2.2. Gradient Descent,[0],[0]
"− √ ρ
Second-order stationary points are very important in nonconvex optimization because when all saddle points are strict, all second-order stationary points are exactly local minima.
",2.2. Gradient Descent,[0],[0]
"Note that the literature sometime defines -second-order stationary point by two independent error terms; i.e., letting ‖∇f(x)‖ ≤ g and λmin(∇2f(x)) ≥",2.2. Gradient Descent,[0],[0]
− H .,2.2. Gradient Descent,[0],[0]
We instead follow the convention of Nesterov & Polyak (2006) by choosing H = √ ρ g to reflect the natural relations between the gradient and the Hessian.,2.2. Gradient Descent,[0],[0]
"In this section we show that it possible to modify gradient descent in a simple way so that the resulting algorithm will provably converge quickly to a second-order stationary point.
",3. Main Result,[0],[0]
"Algorithm 2 Perturbed Gradient Descent: PGD(x0, `, ρ, , c, δ,∆f )
",3. Main Result,[0],[0]
"χ← 3 max{log(d`∆fc 2δ ), 4}, η ← c",3. Main Result,[0],[0]
"` , r ←
√ c χ2 · `
gthres ← √ c χ2 · , fthres ← c χ3 ·
√ 3
ρ , tthres ← χ c2 · √̀ρ
tnoise ← −tthres",3. Main Result,[0],[0]
"− 1 for t = 0, 1, . . .",3. Main Result,[0],[0]
"do
if ‖∇f(xt)‖ ≤ gthres and t− tnoise > tthres then x̃t ← xt, tnoise ← t xt",3. Main Result,[0],[0]
"← x̃t + ξt, ξt uniformly ∼ B0(r) if t − tnoise = tthres and f(xt)",3. Main Result,[0],[0]
"− f(x̃tnoise) > −fthres then
return x̃tnoise",3. Main Result,[0],[0]
xt+1,3. Main Result,[0],[0]
← xt,3. Main Result,[0],[0]
"− η∇f(xt)
The algorithm that we analyze is a perturbed form of gradient descent (see Algorithm 2).",3. Main Result,[0],[0]
The algorithm is based on gradient descent with step size η.,3. Main Result,[0],[0]
"When the norm of the current gradient is small (≤ gthres) (which indicates that the current iterate x̃t is potentially near a saddle point), the algorithm adds a small random perturbation to the gradient.",3. Main Result,[0],[0]
"The perturbation is added at most only once every tthres iterations.
",3. Main Result,[0],[0]
To simplify the analysis we choose the perturbation ξt to be uniformly sampled from a d-dimensional ball1.,3. Main Result,[0],[0]
The use of the threshold tthres ensures that the dynamics are mostly those of gradient descent.,3. Main Result,[0],[0]
"If the function value does not decrease enough (by fthres) after tthres iterations, the algorithm outputs x̃tnoise .",3. Main Result,[0],[0]
"The analysis in this section shows that under this protocol, the output x̃tnoise is necessarily “close” to a second-order stationary point.
",3. Main Result,[0],[0]
We first state the assumptions that we require.,3. Main Result,[0],[0]
Assumption A1.,3. Main Result,[0],[0]
"Function f(·) is both `-smooth and ρHessian Lipschitz.
",3. Main Result,[0],[0]
"The Hessian Lipschitz condition ensures that the function is well-behaved near a saddle point, and the small perturbation we add will suffice to allow the subsequent gradient updates to escape from the saddle point.",3. Main Result,[0],[0]
"More formally, we have: Theorem 3.",3. Main Result,[0],[0]
Assume that f(·) satisfies A1.,3. Main Result,[0],[0]
"Then there exists an absolute constant cmax such that, for any δ > 0, ≤ ` 2
ρ , ∆f ≥ f(x0)",3. Main Result,[0],[0]
"− f ?, and constant c ≤
cmax, PGD(x0, `, ρ, , c, δ,∆f ) will output an -secondorder stationary point, with probability 1−δ, and terminate in the following number of iterations:
O
( `(f(x0)− f?)
2 log4 ( d`∆f 2δ )) .
",3. Main Result,[0],[0]
"1Note that uniform sampling from a d-dimensional ball can be done efficiently by sampling U
1 d × Y‖Y‖ where U ∼
Uniform([0, 1]) and Y ∼ N (0, Id) (Harman & Lacko, 2010).
",3. Main Result,[0],[0]
"Strikingly, Theorem 3 shows that perturbed gradient descent finds a second-order stationary point in almost the same amount of time that gradient descent takes to find first-order stationary point.",3. Main Result,[0],[0]
The step size η is chosen as O(1/`) which is in accord with classical analyses of convergence to first-order stationary points.,3. Main Result,[0],[0]
"Though we state the theorem with a certain choice of parameters for simplicity of presentation, our result holds even if we vary the parameters up to constant factors.
",3. Main Result,[0],[0]
"Without loss of generality, we can focus on the case ≤ `2/ρ, as in Theorem 3.",3. Main Result,[0],[0]
"In the case > `2/ρ, standard gradient descent without perturbation—Theorem 2—easily solves the problem.",3. Main Result,[0],[0]
"This is because by A1, we always have λmin(∇2f(x))",3. Main Result,[0],[0]
≥,3. Main Result,[0],[0]
"−` ≥ − √ ρ , which means that all - second-order stationary points are -first order stationary points.
",3. Main Result,[0],[0]
"We believe that the dependence on at least one log d factor in the iteration complexity is unavoidable in the nonconvex setting, as our result can be directly applied to the principal component analysis problem, for which the best known runtimes (for the power method or Lanczos method) incur a log d factor due to random initialization.",3. Main Result,[0],[0]
"Establishing this formally is still an open question however.
",3. Main Result,[0],[0]
"To provide some intuition for Theorem 3, consider an iterate xt which is not yet an -second-order stationary point.",3. Main Result,[0],[0]
"By definition, either (1) the gradient ∇f(xt) is large, or (2) the Hessian ∇2f(xt) has a significant negative eigenvalue.",3. Main Result,[0],[0]
Traditional analysis works in the first case.,3. Main Result,[0],[0]
The crucial step in the proof of Theorem 3 involves handling the second case: when the gradient is small ‖∇f(xt)‖ ≤ gthres and the Hessian has a significant negative eigenvalue λmin(∇2f(x̃t)) ≤,3. Main Result,[0],[0]
"− √ ρ , then adding a perturbation, followed by standard gradient descent for tthres steps, decreases the function value by at least fthres, with high probability.",3. Main Result,[0],[0]
"The proof of this fact relies on a novel characterization of geometry around saddle points (see Section 5)
If we are able to make stronger assumptions on the objective function we are able to strengthen our main result.",3. Main Result,[0],[0]
This further analysis is presented in the next section.,3. Main Result,[0],[0]
"In many real applications, objective functions further admit the property that all saddle points are strict (Ge et al., 2015; Sun et al., 2016a;b; Bhojanapalli et al., 2016; Ge et al., 2016).",3.1. Functions with Strict Saddle Property,[0],[0]
"In this case, all second-order stationary points are local minima and hence convergence to second-order stationary points (Theorem 3) is equivalent to convergence to local minima.
",3.1. Functions with Strict Saddle Property,[0],[0]
"To state this result formally, we introduce a robust version of the strict saddle property (cf. Ge et al., 2015): Assumption A2.",3.1. Functions with Strict Saddle Property,[0],[0]
"Function f(·) is (θ, γ, ζ)-strict saddle.
",3.1. Functions with Strict Saddle Property,[0],[0]
"That is, for any x, at least one of following holds:
• ‖∇f(x)‖ ≥ θ.
• λmin(∇2f(x)) ≤ −γ.
",3.1. Functions with Strict Saddle Property,[0],[0]
• x is ζ-close to X ?,3.1. Functions with Strict Saddle Property,[0],[0]
"— the set of local minima.
",3.1. Functions with Strict Saddle Property,[0],[0]
"Intuitively, the strict saddle assumption states that the Rd space can be divided into three regions: 1) a region where the gradient is large; 2) a region where the Hessian has a significant negative eigenvalue (around saddle point); and 3) the region close to a local minimum.",3.1. Functions with Strict Saddle Property,[0],[0]
"With this assumption, we immediately have the following corollary:",3.1. Functions with Strict Saddle Property,[0],[0]
Corollary 4.,3.1. Functions with Strict Saddle Property,[0],[0]
Let f(·) satisfy A1 and A2.,3.1. Functions with Strict Saddle Property,[0],[0]
"Then, there exists an absolute constant cmax such that, for any δ > 0,∆f",3.1. Functions with Strict Saddle Property,[0],[0]
≥ f(x0),3.1. Functions with Strict Saddle Property,[0],[0]
"− f?, constant c ≤ cmax, and letting ̃ = min(θ, γ2/ρ), PGD(x0, `, ρ, ̃, c, δ,∆f ) will output a point ζ-close to X ?, with probability 1 − δ, and terminate in the following number of iterations:
O
( `(f(x0)− f?)
̃2 log4 ( d`∆f ̃2δ )) .
",3.1. Functions with Strict Saddle Property,[0],[0]
"Corollary 4 shows after finding ̃-second-order stationary point by Theorem 3 where ̃ = min(θ, γ2/ρ), the output is also in the ζ-neighborhood of some local minimum.
",3.1. Functions with Strict Saddle Property,[0],[0]
Note although Corollary 4 only explicitly asserts that the output will lie within some fixed radius ζ from a local minimum.,3.1. Functions with Strict Saddle Property,[0],[0]
"In many real applications, we further have that ζ can be written as a function ζ(θ) which decreases linearly or polynomially depending on θ, while γ will be nondecreasing w.r.t θ.",3.1. Functions with Strict Saddle Property,[0],[0]
"In these cases, the above corollary further gives a convergence rate to a local minimum.",3.1. Functions with Strict Saddle Property,[0],[0]
"The convergence rate in Theorem 3 is polynomial in , which is similar to that of Theorem 2, but is worse than the rate of Theorem 1 because of the lack of strong convexity.",3.2. Functions with Strong Local Structure,[0],[0]
"Although global strong convexity does not hold in the nonconvex setting that is our focus, in many machine learning problems the objective function may have a favorable local structure in the neighborhood of local minima (Ge et al., 2015; Sun et al., 2016a;b; Sun & Luo, 2016).",3.2. Functions with Strong Local Structure,[0],[0]
Exploiting this property can lead to much faster convergence (linear convergence) to local minima.,3.2. Functions with Strong Local Structure,[0],[0]
One such property that ensures such convergence is a local form of smoothness and strong convexity: Assumption A3.a.,3.2. Functions with Strong Local Structure,[0],[0]
"In a ζ-neighborhood of the set of local minima X ?, the function f(·) is α-strongly convex, and β-smooth.
",3.2. Functions with Strong Local Structure,[0],[0]
Here we use different letter β to denote the local smoothness parameter (in contrast to the global smoothness parameter `).,3.2. Functions with Strong Local Structure,[0],[0]
Note that we always have β ≤,3.2. Functions with Strong Local Structure,[0],[0]
"`.
",3.2. Functions with Strong Local Structure,[0],[0]
"Algorithm 3 Perturbed Gradient Descent with Local Improvement: PGDli(x0, `, ρ, , c, δ,∆f , β)
x0",3.2. Functions with Strong Local Structure,[0],[0]
"← PGD(x0, `, ρ, , c, δ,∆f ) for t = 0, 1, . .",3.2. Functions with Strong Local Structure,[0],[0]
.,3.2. Functions with Strong Local Structure,[0],[0]
do xt+1,3.2. Functions with Strong Local Structure,[0],[0]
← xt,3.2. Functions with Strong Local Structure,[0],[0]
"− 1β∇f(xt)
",3.2. Functions with Strong Local Structure,[0],[0]
"However, often even local α-strong convexity does not hold.",3.2. Functions with Strong Local Structure,[0],[0]
"We thus introduce the following relaxation:
Assumption A3.b.",3.2. Functions with Strong Local Structure,[0],[0]
"In a ζ-neighborhood of the set of local minima X ?, the function f(·) satisfies a (α, β)-regularity condition if for any x in this neighborhood:
〈∇f(x),x−PX?(x)〉 ≥ α
2 ‖x− PX?(x)‖2+
1
2β ‖∇f(x)‖2.
",3.2. Functions with Strong Local Structure,[0],[0]
"(1)
HerePX?(·) is the projection on to the setX ?.",3.2. Functions with Strong Local Structure,[0],[0]
"Note (α, β)regularity condition is more general and is directly implied by standard β-smooth and α-strongly convex conditions.",3.2. Functions with Strong Local Structure,[0],[0]
"This regularity condition commonly appears in low-rank problems such as matrix sensing and matrix completion, and has been used in Bhojanapalli et al. (2016); Zheng & Lafferty (2016), where local minima form a connected set, and where the Hessian is strictly positive only with respect to directions pointing outside the set of local minima.
",3.2. Functions with Strong Local Structure,[0],[0]
Gradient descent naturally exploits local structure very well.,3.2. Functions with Strong Local Structure,[0],[0]
"In Algorithm 3, we first run Algorithm 2 to output a point within the neighborhood of a local minimum, and then perform standard gradient descent with step size 1β .",3.2. Functions with Strong Local Structure,[0],[0]
"We can then prove the following theorem:
Theorem 5.",3.2. Functions with Strong Local Structure,[0],[0]
"Let f(·) satisfy A1, A2, and A3.a (or A3.b).",3.2. Functions with Strong Local Structure,[0],[0]
"Then there exists an absolute constant cmax such that, for any δ > 0, > 0,∆f ≥ f(x0)",3.2. Functions with Strong Local Structure,[0],[0]
"− f?, constant c ≤ cmax, and letting ̃ = min(θ, γ2/ρ), PGDli(x0, `, ρ, ̃, c, δ,∆f , β) will output a point that is - close to X ?, with probability 1−δ, in the following number of iterations:
O
( `(f(x0)− f?)
̃2 log4 ( d`∆f ̃2δ ) + β α log ζ ) .
",3.2. Functions with Strong Local Structure,[0],[0]
"Theorem 5 says that if strong local structure is present, the convergence rate can be boosted to linear convergence (log 1 ).",3.2. Functions with Strong Local Structure,[0],[0]
In this theorem we see that sequence of iterations can be decomposed into two phases.,3.2. Functions with Strong Local Structure,[0],[0]
"In the first phase, perturbed gradient descent finds a ζ-neighborhood by Corollary 4.",3.2. Functions with Strong Local Structure,[0],[0]
"In the second phase, standard gradient descent takes us from ζ to -close to a local minimum.",3.2. Functions with Strong Local Structure,[0],[0]
"Standard gradient descent and Assumption A3.a (or A3.b) make sure that the iterate never steps out of a ζ-neighborhood in this second phase, giving a result similar to Theorem 1 with linear convergence.",3.2. Functions with Strong Local Structure,[0],[0]
"As a simple example to illustrate how to apply our general theorems to specific non-convex optimization problems, we consider a symmetric low-rank matrix factorization problem, based on the following objective function:
min U∈Rd×r
f(U) = 1
2 ‖UU> −M?‖2F, (2)
",4. Example — Matrix Factorization,[0],[0]
where M? ∈ Rd×d.,4. Example — Matrix Factorization,[0],[0]
"For simplicity, we assume rank(M?)",4. Example — Matrix Factorization,[0],[0]
"= r, and denote σ?1 := σ1(M
?), σ?r := σr(M
?).",4. Example — Matrix Factorization,[0],[0]
"Clearly, in this case the global minimum of function value is zero, which is achieved at V? = TD1/2 where TDT> is the SVD of the symmetric real matrix M?.
",4. Example — Matrix Factorization,[0],[0]
The following two lemmas show that the objective function in Eq.,4. Example — Matrix Factorization,[0],[0]
"(2) satisfies the geometric assumptions A1, A2,and A3.b.",4. Example — Matrix Factorization,[0],[0]
"Moreover, all local minima are global minima.",4. Example — Matrix Factorization,[0],[0]
Lemma 6.,4. Example — Matrix Factorization,[0],[0]
"For any Γ ≥ σ?1 , the function f(U) defined in Eq.",4. Example — Matrix Factorization,[0],[0]
"(2) is 8Γ-smooth and 12Γ1/2-Hessian Lipschitz, inside the region {U|‖U‖2 < Γ}.",4. Example — Matrix Factorization,[0],[0]
Lemma 7.,4. Example — Matrix Factorization,[0],[0]
For function f(U) defined in Eq.,4. Example — Matrix Factorization,[0],[0]
"(2), all local minima are global minima.",4. Example — Matrix Factorization,[0],[0]
The set of global minima is X ?,4. Example — Matrix Factorization,[0],[0]
= {V?R|RR> =,4. Example — Matrix Factorization,[0],[0]
R>R = I}.,4. Example — Matrix Factorization,[0],[0]
"Furthermore, f(U) is ( 124 (σ ?",4. Example — Matrix Factorization,[0],[0]
"r ) 3/2, 13σ ?",4. Example — Matrix Factorization,[0],[0]
"r , 1 3 (σ ?",4. Example — Matrix Factorization,[0],[0]
"r )
",4. Example — Matrix Factorization,[0],[0]
1/2)-strict saddle; and satisfies a ( 23σ ?,4. Example — Matrix Factorization,[0],[0]
"r , 10σ ?",4. Example — Matrix Factorization,[0],[0]
1)-regularity condition in a 1 3 (σ ?,4. Example — Matrix Factorization,[0],[0]
"r )
",4. Example — Matrix Factorization,[0],[0]
"1/2- neighborhood of X ?.
",4. Example — Matrix Factorization,[0],[0]
"One caveat is that since the objective function is actually a fourth-order polynomial with respect to U, the smoothness and Hessian Lipschitz parameters from Lemma 6 naturally depend on ‖U‖.",4. Example — Matrix Factorization,[0],[0]
"Fortunately, we can further show that gradient descent (even with perturbation) does not increase ‖U‖ beyond O(max{‖U0‖, (σ?1)1/2}).",4. Example — Matrix Factorization,[0],[0]
"Then, applying Theorem 5 gives: Theorem 8.",4. Example — Matrix Factorization,[0],[0]
There exists an absolute constant cmax such that the following holds.,4. Example — Matrix Factorization,[0],[0]
For the objective function in Eq.,4. Example — Matrix Factorization,[0],[0]
"(2), for any δ > 0 and constant c ≤ cmax, and for Γ1/2 := 2 max{‖U0‖, 3(σ?1)1/2}, the output of PGDli(U0, 8Γ, 12Γ1/2, (σ?r ) 2
108Γ1/2 , c, δ, rΓ
2
2 , 10σ ?",4. Example — Matrix Factorization,[0],[0]
"1), will be -
close to the global minimum set X ?, with probability 1− δ, after the following number of iterations:
",4. Example — Matrix Factorization,[0],[0]
"O ( r ( Γ
σ?r
)4 log4 ( dΓ
δσ?r )",4. Example — Matrix Factorization,[0],[0]
"+ σ?1 σ?r log σ?r ) .
",4. Example — Matrix Factorization,[0],[0]
"Theorem 8 establishes global convergence of perturbed gradient descent from an arbitrary initial point U0, including exact saddle points.",4. Example — Matrix Factorization,[0],[0]
"Suppose we initialize at U0 = 0, then our iteration complexity becomes:
O ( r(κ?)4 log4(dκ?/δ) + κ? log(σ?r/ ) ) ,
where κ?",4. Example — Matrix Factorization,[0],[0]
= σ?1/σ ?,4. Example — Matrix Factorization,[0],[0]
r is the condition number of the matrix M?.,4. Example — Matrix Factorization,[0],[0]
"We see that in the first phase, to move from a neighborhood of the solution, our method requires a number of
iterations scaling as Õ(r(κ?)4).",4. Example — Matrix Factorization,[0],[0]
We suspect that this strong dependence on condition number arises from our generic assumption that the Hessian Lipschitz is uniformly upper bounded; it may well be the case that this dependence can be reduced in the special case of matrix factorization via a finer analysis of the geometric structure of the problem.,4. Example — Matrix Factorization,[0],[0]
In this section we will present the key ideas underlying the main result of this paper (Theorem 3).,5. Proof Sketch for Theorem 3,[0],[0]
We will first argue the correctness of Theorem 3 given two important intermediate lemmas.,5. Proof Sketch for Theorem 3,[0],[0]
"Then we turn to the main lemma, which establishes that gradient descent can escape from saddle points quickly.",5. Proof Sketch for Theorem 3,[0],[0]
"We present full proofs of all these results in Appendix A. Throughout this section, we use η, r, gthres, fthres and tthres as defined in Algorithm 2.",5. Proof Sketch for Theorem 3,[0],[0]
"Recall that an -second-order stationary point is a point with a small gradient, and where the Hessian does not have a significant negative eigenvalue.",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
Suppose we are currently at an iterate xt,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"that is not an -second-order stationary point; i.e., it does not satisfy the above properties.",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
There are two possibilities: (1) The gradient is large: ‖∇f(xt)‖ ≥ gthres; or (2) Around the saddle point we have ‖∇f(xt)‖ ≤ gthres and λmin(∇2f(xt)) ≤,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"− √ ρ .
",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
The following two lemmas address these two cases respectively.,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"They guarantee that perturbed gradient descent will decrease the function value in both scenarios.
",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
Lemma 9 (Gradient).,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
Assume that f(·) satisfies A1.,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"Then for gradient descent with stepsize η < 1` , we have f(xt+1) ≤ f(xt)− η2‖∇f(xt)‖ 2.
",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
Lemma 10 (Saddle).,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"(informal) Assume that f(·) satisfies A1, If xt satisfies ‖∇f(xt)‖ ≤ gthres and λmin(∇2f(xt)) ≤",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"− √ ρ , then adding one perturbation step followed by tthres steps of gradient descent, we have f(xt+tthres)− f(xt) ≤ −fthres with high probability.
",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
We see that Algorithm 2 is designed so that Lemma 10 can be directly applied.,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"According to these two lemmas, perturbed gradient descent will decrease the function value either in the case of a large gradient, or around strict saddle points.",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
Computing the average decrease in function value yields the total iteration complexity.,5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
"Since Algorithm 2 only terminate when the function value decreases too slowly, this guarantees that the output must be -second-order stationary point (see Appendix A for formal proofs).",5.1. Exploiting Large Gradient or Negative Curvature,[0],[0]
The proof of Lemma 9 is straightforward and follows from traditional analysis.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"The key technical contribution of this paper is the proof of Lemma 10, which gives a new characterization of the geometry around saddle points.
",5.2. Escaping from Saddle Points Quickly,[0],[0]
Consider a point x̃ that satisfies the the preconditions of Lemma 10 (‖∇f(x̃)‖ ≤ gthres and λmin(∇2f(x̃)) ≤ −√ρ ).,5.2. Escaping from Saddle Points Quickly,[0],[0]
"After adding the perturbation (x0 = x̃+ξ), we can view x0 as coming from a uniform distribution over Bx̃(r), which we call the perturbation ball.",5.2. Escaping from Saddle Points Quickly,[0],[0]
We can divide this perturbation ball Bx̃(r) into two disjoint regions: (1) an escaping region Xescape which consists of all the points x ∈ Bx̃(r) whose function value decreases by at least fthres after tthres steps; (2) a stuck region Xstuck = Bx̃(r)−Xescape.,5.2. Escaping from Saddle Points Quickly,[0],[0]
Our general proof strategy is to show that Xstuck consists of a very small proportion of the volume of perturbation ball.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"After adding a perturbation to x̃, point x0 has a very small chance of falling in Xstuck, and hence will escape from the saddle point efficiently.
",5.2. Escaping from Saddle Points Quickly,[0],[0]
Let us consider the nature of Xstuck.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"For simplicity, let us imagine that x̃ is an exact saddle point whose Hessian has only one negative eigenvalue, and d − 1 positive eigenvalues.",5.2. Escaping from Saddle Points Quickly,[0],[0]
Let us denote the minimum eigenvalue direction as e1.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"In this case, if the Hessian remains constant (and we
have a quadratic function), the stuck region Xstuck consists of points x such that x− x̃ has a small e1 component.",5.2. Escaping from Saddle Points Quickly,[0],[0]
This is a straight band in two dimensions and a flat disk in high dimensions.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"However, when the Hessian is not constant, the shape of the stuck region is distorted.",5.2. Escaping from Saddle Points Quickly,[0],[0]
"In two dimensions, it forms a “narrow band” as plotted in Figure 2 on top of the gradient flow.",5.2. Escaping from Saddle Points Quickly,[0],[0]
"In three dimensions, it forms a “thin pancake” as shown in Figure 1.
",5.2. Escaping from Saddle Points Quickly,[0],[0]
The major challenge here is to bound the volume of this high-dimensional non-flat “pancake” shaped region Xstuck.,5.2. Escaping from Saddle Points Quickly,[0],[0]
"A crude approximation of this “pancake” by a flat “disk” loses polynomial factors in the dimensionalilty, which gives a suboptimal rate.",5.2. Escaping from Saddle Points Quickly,[0],[0]
Our proof relies on the following crucial observation:,5.2. Escaping from Saddle Points Quickly,[0],[0]
"Although we do not know the explicit form of the stuck region, we know it must be very “thin,” therefore it cannot have a large volume.",5.2. Escaping from Saddle Points Quickly,[0],[0]
"The informal statement of the lemma is as follows:
Lemma 11.",5.2. Escaping from Saddle Points Quickly,[0],[0]
"(informal) Suppose x̃ satisfies the precondition of Lemma 10, and let e1 be the smallest eigendirection of ∇2f(x̃).",5.2. Escaping from Saddle Points Quickly,[0],[0]
"For any δ ∈ (0, 1/3] and any two points w,u ∈ Bx̃(r), if w − u = µre1 and µ ≥ δ/(2 √ d), then at least one of w,u is not in the stuck region Xstuck.
",5.2. Escaping from Saddle Points Quickly,[0],[0]
Using this lemma it is not hard to bound the volume of the stuck region: we can draw a straight line along the e1 direction which intersects the perturbation ball (shown as purple line segment in Figure 2).,5.2. Escaping from Saddle Points Quickly,[0],[0]
"For any two points on this line segment that are at least δr/(2 √ d) away from each other (shown as red points w,u in Figure 2), by Lemma 11, we know at least one of them must not be in Xstuck.",5.2. Escaping from Saddle Points Quickly,[0],[0]
"This implies if there is one point ũ ∈ Xstuck on this line segment, then Xstuck on this line can be at most an interval of length δr/ √ d around ũ. This establishes the “thickness” of Xstuck in the e1 direction, which is turned into an upper bound on the volume of the stuck region Xstuck by standard calculus.",5.2. Escaping from Saddle Points Quickly,[0],[0]
This paper presents the first (nearly) dimension-free result for gradient descent in a general non-convex setting.,6. Conclusion,[0],[0]
"We present a general convergence result and show how it can be further strengthened when combined with further structure such as strict saddle conditions and/or local regularity/convexity.
",6. Conclusion,[0],[0]
There are still many related open problems.,6. Conclusion,[0],[0]
"First, in the presence of constraints, it is worthwhile to study whether gradient descent still admits similar sharp convergence results.",6. Conclusion,[0],[0]
Another important question is whether similar techniques can be applied to accelerated gradient descent.,6. Conclusion,[0],[0]
"We hope that this result could serve as a first step towards a more general theory with strong, almost dimension free guarantees for non-convex optimization.",6. Conclusion,[0],[0]
"This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”).",abstractText,[0],[0]
"The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors.",abstractText,[0],[0]
"When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free.",abstractText,[0],[0]
"Our results can be directly applied to many machine learning applications, including deep learning.",abstractText,[0],[0]
"As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization.",abstractText,[0],[0]
"Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.",abstractText,[0],[0]
How to Escape Saddle Points Efficiently,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1569–1575, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
User-generated passwords tend to be memorable, but not secure. A random, computergenerated 60-bit string is much more secure. However, users cannot memorize random 60- bit strings. In this paper, we investigate methods for converting arbitrary bit strings into English word sequences (both prose and poetry), and we study their memorability and other properties.",text,[0],[0]
"Passwords chosen by users (e.g., “Scarlet%2”) are easy to remember, but not secure (Florencio and Herley, 2007).",1 Introduction,[0],[0]
"A more secure method is to use a system-assigned 60-bit random password, such as 0010100010100...00101001.",1 Introduction,[0],[0]
"However, this string is hard to memorize.",1 Introduction,[0],[0]
"In this paper, we convert such strings into English phrases, in order to improve their memorability, using natural language processing to select fluent passphrases.
",1 Introduction,[0],[0]
"Our methods are inspired by an XKCD cartoon1 that proposes to convert a randomly-chosen 44-bit password into a short, nonsensical sequence of English words.",1 Introduction,[0],[0]
"The proposed system divides the 44-bit password into four 11-bit chunks, and each chunk provides an index into a 2048-word English dictionary.",1 Introduction,[0],[0]
"XKCD’s example passphrase is correct horse battery staple:
1http://xkcd.com/936
44-bit password English phrase --------------- -------------- 10101101010 -> correct 10010110101 -> horse 01010101010 -> battery 10110101101 -> staple
The four-word sequence is nonsense, but it is easier to memorize than the 44-bit string, and XKCD hypothesizes that users can improve memorability by building an image or story around the four words.
",1 Introduction,[0],[0]
"In this paper, we investigate other methods for converting a system-generated bit string into a memorable sequence of English words.",1 Introduction,[0],[0]
"Our methods produce whole sentences, e.g.
Fox news networks are seeking views from downtown streets.
as well as short poems, e.g.
Diversity inside replied, Soprano finally reside.
",1 Introduction,[0],[0]
"We also move to 60-bit passwords, for better security.",1 Introduction,[0],[0]
"One source claims:
As of 2011, available commercial products claim the ability to test up to 2,800,000,000 passwords a second on a standard desktop computer using a highend graphics processor.2
If this is correct, a 44-bit password would take one hour to crack, while a 60-bit password would take 11.3 years.
",1 Introduction,[0],[0]
"Our concrete task is as follows: 2http://en.wikipedia.org/wiki/Password cracking
1569
•",1 Introduction,[0],[0]
"Input: A random, system-generated 60-bit password.
",1 Introduction,[0],[0]
"• Output: An English word sequence with two properties:
– It is memorable.",1 Introduction,[0],[0]
–,1 Introduction,[0],[0]
"We can deterministically recover the orig-
inal input 60-bit string from it.
",1 Introduction,[0],[0]
This implies that we map 260 distinct bit strings into 260 distinct English sequences.,1 Introduction,[0],[0]
"If a user memorizes the English word sequence supplied to them, then they have effectively memorized the 60-bit string.",1 Introduction,[0],[0]
"We now describe our baseline password generation method, followed by four novel methods.",2 Password Generation Methods,[0],[0]
In Section 3 we experimentally test their memorability.,2 Password Generation Methods,[0],[0]
Our baseline is a version of XKCD.,2.1 XKCD Baseline,[0],[0]
"Instead of a 2048-word dictionary, we use a 32,7868-word dictionary.",2.1 XKCD Baseline,[0],[0]
"We assign each word a distinct 15-bit code.
",2.1 XKCD Baseline,[0],[0]
"At runtime, we take a system-assigned 60-bit code and split it into four 15-bit sequences.",2.1 XKCD Baseline,[0],[0]
We then substitute each 15-bit segment with its corresponding word.,2.1 XKCD Baseline,[0],[0]
"By doing this, we convert a random 60-bit code into a 4-word password.
",2.1 XKCD Baseline,[0],[0]
"The first row of Table 1 shows three sample XKCD passwords, along with other information, such as the average number of characters (including spaces).",2.1 XKCD Baseline,[0],[0]
"XKCD passwords are short but nonsensical, so we now look into methods that instead create longer but fluent English sentences.",2.2 First Letter Mnemonic,[0],[0]
"We might think to guarantee fluency by selecting sentences from an alreadyexisting text corpus, but no corpus is large enough to contain 260 (∼ 1018) distinct sentences.",2.2 First Letter Mnemonic,[0],[0]
"Therefore, we must be able to synthesize new English strings.
",2.2 First Letter Mnemonic,[0],[0]
"In our first sentence generation method (First Letter Mnemonic), we store our input 60-bit code in the first letters of each word.",2.2 First Letter Mnemonic,[0],[0]
"We divide the 60-bit code into 4-bit sections, e.g., ‘0100-1101-1101-...’.",2.2 First Letter Mnemonic,[0],[0]
"Every 4-bit sequence type corresponds to an English letter
or two, per Table 2.",2.2 First Letter Mnemonic,[0],[0]
"We build a word-confusion network (or “sausage lattice”) by replacing each 4-bit code with all English words that start with a corresponding letter, e.g.: 0100 1101 1111 ... 0011",2.2 First Letter Mnemonic,[0],[0]
----,2.2 First Letter Mnemonic,[0],[0]
----,2.2 First Letter Mnemonic,[0],[0]
----,2.2 First Letter Mnemonic,[0],[0]
---- income my frog ... octopus is miner feast ... of inner priest gratuitous ... oregon ... ...,2.2 First Letter Mnemonic,[0],[0]
"... ...
",2.2 First Letter Mnemonic,[0],[0]
"This yields about 1074 paths, some good (is my frog. . . )",2.2 First Letter Mnemonic,[0],[0]
and some bad (income miner feast. . . ).,2.2 First Letter Mnemonic,[0],[0]
"To select the most fluent path, we train a 5-gram language model with the SRILM toolkit (Stolcke, 2002) on the English Gigaword",2.2 First Letter Mnemonic,[0],[0]
"corpus.3 SRILM also includes functionality for extracting the best path from a confusion network.
",2.2 First Letter Mnemonic,[0],[0]
Table 1 shows sample sentences generated by the method.,2.2 First Letter Mnemonic,[0],[0]
"Perhaps surprisingly, even though the sentences are much longer than XKCD (15 words versus 4 words), the n-gram language model (LM) score is a bit better.",2.2 First Letter Mnemonic,[0],[0]
"The sentences are locally fluent, but not perfectly grammatical.
",2.2 First Letter Mnemonic,[0],[0]
We can easily reconstruct the original 60-bit code by extracting the first letter of each word and applying the Table 2 mapping in reverse.,2.2 First Letter Mnemonic,[0],[0]
"Most of the characters in the previous methods seem “wasted”, as only the word-initial letters bear information relevant to reconstructing the original 60-
3https://catalog.ldc.upenn.edu/LDC2011T07
bit string.",2.3 All Letter Method,[0],[0]
"Our next technique (All Letter Method) non-deterministically translates every bit into an English letter, per Table 3.",2.3 All Letter Method,[0],[0]
"Additionally, we nondeterministically introduce a space (or not) between each pair of letters.
",2.3 All Letter Method,[0],[0]
"This yields 4 · 1084 possible output strings per input, 3 ·1056 of which consist of legal English words.",2.3 All Letter Method,[0],[0]
"From those 3 · 1056 strings, we choose the one that yields the best word 5-gram score.
",2.3 All Letter Method,[0],[0]
It is not immediately clear how to process a letterbased lattice with a word-based language model.,2.3 All Letter Method,[0],[0]
We solve this search problem by casting it as one of machine translation from bit-strings to English.,2.3 All Letter Method,[0],[0]
"We create a phrase translation table by pairing each English word with a corresponding “bit phrase”, using Table 3 in reverse.",2.3 All Letter Method,[0],[0]
"Sample entries include:
din ||| 1 0 1 through ||| 1 0 0 0 0 0 0 yields |||",2.3 All Letter Method,[0],[0]
"1 0 0 1 1 1
We then use the Moses machine translation toolkit (Koehn et al., 2007) to search for the 1-best translation of our input 60-bit string, using the phrase table and a 5-gram English LM, disallowing re-ordering.
",2.3 All Letter Method,[0],[0]
"Table 1 shows that these sentences are shorter than the mnemonic method (11.8 words versus 15 words), without losing fluency.
",2.3 All Letter Method,[0],[0]
"Given a generated English sequence, we can deterministically reconstruct the original 60-bit input string, using the above phrase table in reverse.",2.3 All Letter Method,[0],[0]
Sentence passwords from the previous method contain 70.8 characters on average (including spaces).,2.4 Frequency Method,[0],[0]
Classic studies by Shannon (1951) and others estimate that printed English may ultimately be compressible to about one bit per character.,2.4 Frequency Method,[0],[0]
"This implies we might be able to produce shorter output (60 characters, including space) while maintaining normal English fluency.
",2.4 Frequency Method,[0],[0]
"Our next technique (Frequency Method) modifies the phrase table by assigning short bit codes to frequent words, and long bit codes to infrequent words.",2.4 Frequency Method,[0],[0]
"For example:
din ||| 0 1 1 0 1 0 1 0 0 through ||| 1 1 1 1 yields ||| 0 1 0 1 1 1 0 1
Note that the word din is now mapped to a 9-bit sequence rather than a 3-bit sequence.",2.4 Frequency Method,[0],[0]
"More precisely, we map each word to a random bit sequence of length bmax(1,−α × log P(word) + β)c.",2.4 Frequency Method,[0],[0]
"By changing variables α and β we can vary between smooth but long sentences (α = 1 and β = 0) to XKCD-style phrases (α = 0 and β = 15).
",2.4 Frequency Method,[0],[0]
"Table 1 shows example sentences we obtain with α = 2.5 and β = −2.5, yielding sentences of 9.7 words on average.",2.4 Frequency Method,[0],[0]
"In ancient times, people recorded long, historical epics using poetry, to enhance memorability.",2.5 Poetry,[0],[0]
"We follow this idea by turning each system-assigned 60-bit string into a short, distinct English poem.",2.5 Poetry,[0],[0]
"Our format is the rhyming iambic tetrameter couplet:
•",2.5 Poetry,[0],[0]
"The poem contains two lines of eight syllables each.
",2.5 Poetry,[0],[0]
"• Lines are in iambic meter, i.e., their syllables have the stress pattern 01010101, where 0 represents an unstressed syllable, and 1 represents a stressed syllable.",2.5 Poetry,[0],[0]
"We also allow 01010100, to allow a line to end in a word like Angela.
",2.5 Poetry,[0],[0]
• The two lines end in a pair of rhyming words.,2.5 Poetry,[0],[0]
Words rhyme if their phoneme sequences match from the final stressed vowel onwards.,2.5 Poetry,[0],[0]
"We obtain stress patterns and phoneme sequences from the CMU pronunciation dictionary.4
Monosyllabic words cause trouble, because their stress often depends on context (Greene et al., 2010).",2.5 Poetry,[0],[0]
"For example, eighth is stressed in eighth street, but not in eighth avenue.",2.5 Poetry,[0],[0]
This makes it hard to guarantee that automatically-generated lines will scan as intended.,2.5 Poetry,[0],[0]
"We therefore eject all monosyllabic words
4http://www.speech.cs.cmu.edu/cgi-bin/cmudict
from the vocabulary, except for six unstressed ones (a, an, and, the, of, or).
",2.5 Poetry,[0],[0]
"Here is a sample poem password:
",2.5 Poetry,[0],[0]
The le-gen-da-ry Ja-pan-ese ↓ ↑ ↓,2.5 Poetry,[0],[0]
↑ ↓,2.5 Poetry,[0],[0]
↑ ↓,2.5 Poetry,[0],[0]
↑,2.5 Poetry,[0],[0]
Sub-si-di-ar-ies ov-er-seas ↓ ↑,2.5 Poetry,[0],[0]
↓,2.5 Poetry,[0],[0]
↑ ↓,2.5 Poetry,[0],[0]
↑ ↓,2.5 Poetry,[0],[0]
"↑
Meter and rhyme constraints make it difficult to use the Moses machine translation toolkit to search for fluent output, as we did above; the decoder state must be augmented with additional short- and longdistance information (Genzel et al., 2010).
",2.5 Poetry,[0],[0]
"Instead, we build a large finite-state acceptor (FSA) with a path for each legal poem.",2.5 Poetry,[0],[0]
"In each path, the second line of the poem is reversed, so that we can enforce rhyming locally.
",2.5 Poetry,[0],[0]
The details of our FSA construction are as follows.,2.5 Poetry,[0],[0]
"First, we create a finite-state transducer (FST) that maps each input English word onto four sequences that capture its essential properties, e.g.:
create -> 0 1 create -> 0",2.5 Poetry,[0],[0]
1,2.5 Poetry,[0],[0]
"EY-T create -> 1r 0r create -> EY-T 1r 0r
Here, EY-T represents the rhyme-class of words like create and debate.",2.5 Poetry,[0],[0]
"The r indicates a stress pattern in the right-to-left direction.
",2.5 Poetry,[0],[0]
We then compose this FST with an FSA that only accepts sequences of the form: 0 1 0 1 0 1 0 1 X X 1r 0r,2.5 Poetry,[0],[0]
"1r 0r 1r 0r 1r 0r where X and X are identical rhyme classes (e.g., EYT and EY-T).
",2.5 Poetry,[0],[0]
It remains to map an arbitrary 60-bit string onto a path in the FSA.,2.5 Poetry,[0],[0]
Let k be the integer representation of the 60-bit string.,2.5 Poetry,[0],[0]
"If the FSA contains exactly 260 paths, we can easily select the kth path using the following method.",2.5 Poetry,[0],[0]
"At each node N of the FSA, we store the total number of paths from N to the final state—this takes linear time if we visit states in reverse topological order.",2.5 Poetry,[0],[0]
"We then traverse the FSA deterministically from the start state, using k to guide the path selection.
",2.5 Poetry,[0],[0]
"Our FSA actually contains 279 paths, far more than the required 260.",2.5 Poetry,[0],[0]
We can say that the information capacity of the English rhyming iambic tetrameter couplet is 79 bits!,2.5 Poetry,[0],[0]
"Some are very good:
Sophisticated potentates misrepresenting Emirates.
",2.5 Poetry,[0],[0]
"The supervisor notified the transportation nationwide.
",2.5 Poetry,[0],[0]
"Afghanistan, Afghanistan, Afghanistan, and Pakistan.
",2.5 Poetry,[0],[0]
"while others are very bad: The shirley emmy plebiscite complete suppressed unlike invite
The shirley emmy plebiscite complaints suppressed unlike invite
The shirley emmy plebiscite complaint suppressed unlike invite
Fortunately, because our FSA contains over a million times the required 260 paths, we can avoid these bad outputs.",2.5 Poetry,[0],[0]
"For any particular 60-bit string, we have a million poems to choose from, and we output only the best one.
",2.5 Poetry,[0],[0]
"More precisely, given a 60-bit input string k, we extract not only the kth FSA path, but also the k + i · 260 paths, with i ranging from 1 to 999,999.",2.5 Poetry,[0],[0]
"We explicitly list out these paths, reversing the second half of each, and score them with our 5-gram LM.",2.5 Poetry,[0],[0]
We output the poem with the 1-best LM score.,2.5 Poetry,[0],[0]
"Table 1 shows sample outputs.
",2.5 Poetry,[0],[0]
"To reconstruct the original 60-bit string k, we first find the FSA path corresponding to the user-recalled English string (with second half reversed).",2.5 Poetry,[0],[0]
We use depth-first search to find this path.,2.5 Poetry,[0],[0]
"Once we have the path, it is easy to determine which numbered path it is, lexicographically speaking, using the nodelabeling scheme above to recover k.",2.5 Poetry,[0],[0]
"We designed two experiments to compare our methods.
",3 Experiments,[0],[0]
The first experiment tests the memorability of passwords.,3 Experiments,[0],[0]
We asked participants to memorize a password from a randomly selected method5 and recall it two days later.,3 Experiments,[0],[0]
"To give more options to users,
5In all experiments, we omit the First Letter Mnemonic, due to its low performance in early tests.
",3 Experiments,[0],[0]
we let them select from the 10-best passwords according to the LM score for a given 60-bit code.,3 Experiments,[0],[0]
"Note that this flexibility is not available for XKCD, which produces only one password per code.
62 users participated in this experiment, 44 returned to recall the password, and 22 successfully recalled the complete password.",3 Experiments,[0],[0]
"Table 4 shows that the Poetry and XKCD methods yield passwords that are easiest to remember.
",3 Experiments,[0],[0]
"In the second experiment, we present a separate set of users with passwords from each of the four methods.",3 Experiments,[0],[0]
"We ask which they would prefer to use, without requiring any memorization.",3 Experiments,[0],[0]
"Table 5 shows that users prefer sentences over poetry, and poetry over XKCD.",3 Experiments,[0],[0]
Table 4 shows that the Poetry and XKCD methods yield passwords that are easiest to memorize.,4 Analysis,[0],[0]
Complete sentences generated by the All Letter and Frequency Methods are harder to memorize.,4 Analysis,[0],[0]
"At the same time Table 5 shows that people like the sentences better than XKCD, so it seems that they overestimate their ability to memorize a sentence of 10- 12 words.",4 Analysis,[0],[0]
"Here are typical mistakes (S = system-
generated, R = as recalled by user):
(S) Still looking for ruben sierra could be in central michigan (R)",4 Analysis,[0],[0]
"I am still looking for ruben sierra in central michigan
(S) That we were required to go to college more than action movies (R)",4 Analysis,[0],[0]
"We are required to go to college more than action movies
(S)",4 Analysis,[0],[0]
"No dressing allowed under canon law in the youth group (R) No dresses allowed under canon law for youth groups
Users remember the gist of a sentence very well, but have trouble reproducing the exact wording.",4 Analysis,[0],[0]
Post-experiment interview reveal this to be partly an effect of overconfidence.,4 Analysis,[0],[0]
"Users put little mental work into memorizing sentences, beyond choosing among the 10-best alternatives presented to them.",4 Analysis,[0],[0]
"By contrast, they put much more work into memorizing an XKCD phrase, actively building a mental image or story to connect the four otherwise unrelated words.",4 Analysis,[0],[0]
"Actually, we can often automatically determine that a user-recalled sequence is wrong.",5 Future Directions,[0],[0]
"For example, when we go to reconstruct the 60-bit input string from a user-recalled sequence, we may find that we get a 62-bit string instead.",5 Future Directions,[0],[0]
"We can then automatically prod the user into trying again, but we find that this is not effective in practice.",5 Future Directions,[0],[0]
"An intriguing direction is to do automatic error-correction, i.e., take the user-recalled sequence and find the closest match among the 260 English sequences producible by the method.",5 Future Directions,[0],[0]
"Of course, it is a challenge to do this with 1-best outputs of an MT system that uses heuristic beam search, and we must also ensure that security is maintained.
",5 Future Directions,[0],[0]
We may also investigate new ways to re-rank nbest lists.,5 Future Directions,[0],[0]
"Language model scoring is a good start, but we may prefer vivid, concrete, or other types of words, or we may use text data associated with the user (papers, emails) for secure yet personalized password generation.",5 Future Directions,[0],[0]
"Gasser (1975), Crawford and Aycock (2008), and Shay et al. (2012) describe systems that produce meaningless but pronounceable passwords, such as “tufritvi” .",6 Related Work,[0],[0]
"However, their systems can only assign ∼ 230 distinct passwords.
Jeyaraman and Topkara (2005) suggest generating a random sequence of characters, and finding a mnemonic for it in a text corpus.",6 Related Work,[0],[0]
A limited corpus means they again have a small space of systemassigned passwords.,6 Related Work,[0],[0]
"We propose a similar method in Section 2.2, but we automatically synthesize a new mnemonic word sequence.
Kurzban (1985) and Shay et al. (2012) use a method similar to XKCD with small dictionaries.",6 Related Work,[0],[0]
This leads to longer nonsense sequences that can be difficult to remember.,6 Related Work,[0],[0]
We introduced several methods for generating secure passwords in the form of English word sequences.,7 Conclusion,[0],[0]
"We learned that long sentences are seemingly easy to remember, but actually hard to reproduce, and we also learned that our poetry method produced relatively short, memorable passwords that are liked by users.",7 Conclusion,[0],[0]
"We would like to thank James Bedell, Aliya Deri, Tomer Levinboim, Jonathan May, Nima Pourdamghani and the anonymous reviewers for their very helpful comments.",Acknowledgments,[0],[0]
This work was supported in part by DARPA contract FA-8750-13-2-0045.,Acknowledgments,[0],[0]
"User-generated passwords tend to be memorable, but not secure.",abstractText,[0],[0]
"A random, computergenerated 60-bit string is much more secure.",abstractText,[0],[0]
"However, users cannot memorize random 60bit strings.",abstractText,[0],[0]
"In this paper, we investigate methods for converting arbitrary bit strings into English word sequences (both prose and poetry), and we study their memorability and other properties.",abstractText,[0],[0]
How to Memorize a Random 60-Bit String,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1146–1155 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
We pose the general task of user-factor adaptation — adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it. We introduce a continuous adaptation technique, suited for real-valued user factors that are common in social science and bringing us closer to personalized NLP, adapting to each user uniquely. We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and stance detection. Adaptation provides statistically significant benefits for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points for stance.",text,[0],[0]
Language use is personal.,1 Introduction,[0],[0]
Knowing who wrote a piece of text can help to better understand it.,1 Introduction,[1.0],['Knowing who wrote a piece of text can help to better understand it.']
"For instance, knowing the age and gender groups of authors has been shown to improve document classification (Hovy, 2015) and sentiment analysis (Volkova et al., 2013).
",1 Introduction,[0.9999999674632591],"['For instance, knowing the age and gender groups of authors has been shown to improve document classification (Hovy, 2015) and sentiment analysis (Volkova et al., 2013).']"
"However, putting people into discrete groups (e.g. age groups, binary gender) often relies on arbitrary boundaries which may not correspond to meaningful changes in language use.",1 Introduction,[1.0],"['However, putting people into discrete groups (e.g. age groups, binary gender) often relies on arbitrary boundaries which may not correspond to meaningful changes in language use.']"
"A wealth of psychological research suggests people should not be characterized as discrete types (or domains) but rather as mixtures of continuous factors (McCrae
and Costa Jr., 1989; Ruscio and Ruscio, 2000; Widiger and Samuel, 2005).
",1 Introduction,[1.0000000293190274],"['A wealth of psychological research suggests people should not be characterized as discrete types (or domains) but rather as mixtures of continuous factors (McCrae and Costa Jr., 1989; Ruscio and Ruscio, 2000; Widiger and Samuel, 2005).']"
"Here, we ask how one can adapt NLP models to real-valued human factors – continuous valued attributes that capture fine-grained differences between users (e.g. real-valued age, continuous gender scores).",1 Introduction,[1.0],"['Here, we ask how one can adapt NLP models to real-valued human factors – continuous valued attributes that capture fine-grained differences between users (e.g. real-valued age, continuous gender scores).']"
"We refer to this problem as userfactor adaptation, and investigate a solution to it in the context of social media, a genre where language is generated by a particularly diverse set of users (Duggan and Smith, 2013).",1 Introduction,[0],[0]
"Importantly, user-factor adaptation brings us closer to personalized NLP in that with real-valued factors we can now adapt uniquely for each user.
",1 Introduction,[0],[0]
"Our approach composes user factor information with the linguistic features, similar to feature augmentation (Daumé III, 2007), a widely used domain adaptation technique which allows for easy integration with most feature-based learning models.",1 Introduction,[1.0],"['Our approach composes user factor information with the linguistic features, similar to feature augmentation (Daumé III, 2007), a widely used domain adaptation technique which allows for easy integration with most feature-based learning models.']"
"Since relevant user information often is not explicitly available, we use a background of tweets from the user to infer user factors.",1 Introduction,[1.0],"['Since relevant user information often is not explicitly available, we use a background of tweets from the user to infer user factors.']"
"We evaluate our approach over five tasks — POS tagging, PPattachment, sentiment analysis, sarcasm detection, and stance detection — and with a variety of inferred user factors including (a) known factors: age, gender, and personality traits, as well as (b) latent factors derived from past user tweets.
Contributions.",1 Introduction,[0],[0]
"The main contributions of this work include (a) adaptation based simply on background language (e.g. past tweets; no required a priori user knowledge or “domain”), (b) a method for adapting models based on continuous variables, (c) adaptation to other user attributes beyond age and gender (personality and latent factors), and (d) empirical evidence that standard NLP models can often be improved by user-factor adaptation with a range of inferred factors.
1146",1 Introduction,[0],[0]
"User-factor adaptation is especially critical for social media, where content is generated by a diverse user base (Duggan and Smith, 2013).",2 User-Factor Adaptation,[0],[0]
"Adaptation requires two components: 1) a user factor representation that captures salient traits indicative of language differences between users, and 2) an adaptation technique that uses this representation to modify learning appropriately.
",2 User-Factor Adaptation,[0],[0]
"User factors, even simple ones such as age and gender, may not always be readily available.",2 User-Factor Adaptation,[0],[0]
"The messages posted by users, however, are often public and can be used to infer many known linguistically relevant user traits including personality, as well as latent language factors (described next).",2 User-Factor Adaptation,[0],[0]
"Given this background information about users, the user-factor adaptation problem is to learn a single model that is sensitive to both the variations and commonalities in language across different users.",2 User-Factor Adaptation,[0],[0]
The first step in our adaptation approach is to create a representation of users that relates to their language use.,3 User Factors,[0],[0]
"To this end, we explore two sets of factors: 1) inferred demographics and personality traits, and 2) latent language factors that directly capture language use variations among users.
",3 User Factors,[0],[0]
"Different from prior work, we model these human attributes as real-valued factors, as is common in psychology literature.",3 User Factors,[0],[0]
"Although they may refer to discrete classes such as cluster membership, a factor representation is able to capture more nuanced differences and characteristics that are best understood as a continuum (McCrae and Costa Jr., 1989; Ruscio and Ruscio, 2000; Widiger and Samuel, 2005).",3 User Factors,[0],[0]
This is critical for our goal of moving beyond group-level adaptation toward personalization.,3 User Factors,[0],[0]
"Many studies have linked language variations with demographic (Argamon et al., 2007; Cheshire, 2005), occupational (Preoţiuc-Pietro et al., 2016) and other psychosocial variables such as personality (Schwartz et al., 2013).",3.1 Demographic and Personality Factors,[0],[0]
"We investigate the relevance of a subset of these social variables as user factors for adaptation.
",3.1 Demographic and Personality Factors,[0],[0]
"However, we may not have direct access to such information.",3.1 Demographic and Personality Factors,[0],[0]
"Unlike the tweets posted by a user,
their demographic and personality traits are not always publicly available.",3.1 Demographic and Personality Factors,[0],[0]
"We use automatic classification models for obtaining real-valued age and gender estimates (Sap et al., 2014) and personality traits (Park et al., 2015).",3.1 Demographic and Personality Factors,[0],[0]
"In addition to being reasonably accurate (e.g. age prediction has a Pearson r of .83 with true age), language based estimation of factor scores may capture linguistic preferences more clearly.",3.1 Demographic and Personality Factors,[0],[0]
"For instance, Bamman et al. (2014b) found that perceived gender was strongly linked to the gender makeup of a user’s social network, and may be a better descriptor of linguistic preferences than self-reported gender.",3.1 Demographic and Personality Factors,[0],[0]
We also explore methods to derive latent factors that capture language use similarities and variations across users.,3.2 Latent Language Factors,[0],[0]
The main idea is to derive a latent d-dimensional representation of each user using their background tweets.,3.2 Latent Language Factors,[0],[0]
"While there are many choices here, we explore a factorization technique (generative factor analysis), a clustering technique (k-means with TF-IDF), and a hybrid (word2vec with k-means).
",3.2 Latent Language Factors,[0],[0]
Generative Factor Analysis.,3.2 Latent Language Factors,[0],[0]
Factorization methods allow us to build latent representations of users by finding low-rank approximations of the original high-dimensional representations of their text.,3.2 Latent Language Factors,[0],[0]
"We use a general method called factor analysis (FA) (Lawley and Maxwell, 1971).",3.2 Latent Language Factors,[0],[0]
"Intuitively, FA seeks to capture the variability across correlated variables as a weighted linear combination of a given number of latent dimensions, thus allowing a low-dimensional representation of words1.
",3.2 Latent Language Factors,[0],[0]
"Formally, let M|U|×|V| denote the user-term matrix, whose entries Mij indicate the number of times word j is used by user i. FA factorizes this high-dimensional representation into two matrices F and L as follows: M = FL + E where E is an error matrix consisting of residual errors not captured by FL and where the residual noise is assumed to be Gaussian distributed with zero mean.
",3.2 Latent Language Factors,[0],[0]
Clustering.,3.2 Latent Language Factors,[0],[0]
We also explore commonly used text clustering-based methods to derive latent factors from the users’ tweets.,3.2 Latent Language Factors,[0],[0]
The idea is to cluster the users based on their tweets.,3.2 Latent Language Factors,[0],[0]
"In one case we use TF-IDF based representations, and in the other we use word2vec embeddings (Mikolov et al., 2013).",3.2 Latent Language Factors,[0],[0]
"1In this sense FA is a more flexible method than singular value decomposition in that it allows factors to be correlated.
",3.2 Latent Language Factors,[0],[0]
We produce a k-means clustering on this reduced dimensional space to create clusters of users who have similar language use.,3.2 Latent Language Factors,[0],[0]
We derive realvalued factors from these clusters using the distance of the user to the centers of each cluster.,3.2 Latent Language Factors,[0],[0]
Cluster membership yields the discrete representation.,3.2 Latent Language Factors,[0],[0]
Refer to section 5.1 for implementation details.,3.2 Latent Language Factors,[0],[0]
"Given a factor representation of each user, the adaptation task is to learn a model that is sensitive to both the differences and commonalities across all users.",4 Adaptation Models,[0],[0]
"This is similar to the objective for domain adaptation tasks, where the task data is drawn from one or more underlying domains and learning needs to account for both the similarities and differences in the domains.",4 Adaptation Models,[0],[0]
"We formulate user-factor adaptation as a domain adaptation technique based on feature augmentation (Daumé III, 2007) but rather than force users into discrete domains, we develop a continuous formulation that allows us to make good use of the real-valued user factors.
",4 Adaptation Models,[0],[0]
Here we first describe a direct discrete formulation of feature augmentation and then describe our proposed continuous formulation.,4 Adaptation Models,[0],[0]
Feature augmentation uses domain information to transform instances into a new augmented space such that instances from the same domain have higher similarity in the augmented space compared to instances from different domains.,4.1 Discrete Adaptation,[0],[0]
"A learner operating over this augmented space can now learn to model both domain-specific and domain-general influences of the features.
",4.1 Discrete Adaptation,[0],[0]
"The discrete adaptation method is a direct application of this idea, where the training and test instances are mapped into domains based on some grouping that we induce from the user factors.",4.1 Discrete Adaptation,[1.0],"['The discrete adaptation method is a direct application of this idea, where the training and test instances are mapped into domains based on some grouping that we induce from the user factors.']"
"For example, the user factor age induces three discrete domains: low (age < 24), middle (24 < age < 28), and high (age > 28).
",4.1 Discrete Adaptation,[0],[0]
"Given the instance domain mapping, feature augmentation transforms the instances based on their domain.",4.1 Discrete Adaptation,[0],[0]
"Suppose the original instances have n features and suppose there are d discrete factor classes (F1, · · · , Fd) i.e., d domains.",4.1 Discrete Adaptation,[0],[0]
"Given an instance which is mapped to a factor class Fi, augmentation creates a new feature vector that has
d + 1 feature sets of length n each.",4.1 Discrete Adaptation,[0],[0]
The original features are copied over to the first feature set for all instances regardless of their domain.,4.1 Discrete Adaptation,[0],[0]
"For instances from domain i, the original features are copied over to feature set i + 1.",4.1 Discrete Adaptation,[1.0],"['For instances from domain i, the original features are copied over to feature set i + 1.']"
The other feature sets are zeroes.,4.1 Discrete Adaptation,[0],[0]
"Table 1 shows some examples of this augmentation strategy for a single instance, x, under different factor class mappings.
",4.1 Discrete Adaptation,[0],[0]
These augmented instances are used for training and testing without any further modifications to the original learning formulation.,4.1 Discrete Adaptation,[0],[0]
Discrete adaptation ignores the continuous nature of user factors.,4.2 Continuous Adaptation,[0],[0]
"Unlike the commonly considered domains, people don’t fit neatly into discrete bins.",4.2 Continuous Adaptation,[0],[0]
"Many psychological studies have shown the ineffectiveness of treating user factors as discrete types (McCrae and Costa Jr., 1989); we expect an adaptation method which does so to be similarly ineffective.",4.2 Continuous Adaptation,[0],[0]
"For most factors the boundaries for determining classes is unclear, and such arbitrarily-drawn boundaries may not correspond to big changes in language use.
",4.2 Continuous Adaptation,[0],[0]
Figure 1 illustrates the advantage of continuous adaptation for a single feature — whether the current instance contains an intensifier — using sarcasm detection as an example.,4.2 Continuous Adaptation,[1.0],['Figure 1 illustrates the advantage of continuous adaptation for a single feature — whether the current instance contains an intensifier — using sarcasm detection as an example.']
"The colored shapes show the feature values for instances from four users, with green squares representing “sarcastic” tweets and yellow circles representing “not sarcastic” ones.",4.2 Continuous Adaptation,[1.0],"['The colored shapes show the feature values for instances from four users, with green squares representing “sarcastic” tweets and yellow circles representing “not sarcastic” ones.']"
The model is unable to distinguish between sarcastic and non-sarcastic tweets in the no adaptation and discrete adaptation case.,4.2 Continuous Adaptation,[1.0],['The model is unable to distinguish between sarcastic and non-sarcastic tweets in the no adaptation and discrete adaptation case.']
"While discrete adaptation could induce some separability, in this case it fails to account for the variations between differently-aged over 30 users.",4.2 Continuous Adaptation,[0],[0]
"On the other hand, if we use features values that are proportional to the actual age, it can result in a better
separation as shown in the figure.",4.2 Continuous Adaptation,[0],[0]
"A compositional function c combines d user factor scores fu,d with original feature values x:
Φ(x, u) = 〈x, c(fu,1,x), c(fu,2,x), · · · , c(fu,d,x)〉",4.2 Continuous Adaptation,[0],[0]
"Thus, a version of each feature exists with and without the factor information integrated.",4.2 Continuous Adaptation,[0],[0]
"We will explore a simple multiplicative compositional function (i.e., c(fu,d,x) = fu,d · x) but others can be imagined (e.g. additive, multiplicative with kernel functions).
",4.2 Continuous Adaptation,[0],[0]
"Multiplicative composition has the property of reducing Φ to discrete adaptation when the factors are binary i.e., c(fu,d,x) = x",4.2 Continuous Adaptation,[0],[0]
"when u ∈ Fd and c(fu,d,x) = 0 otherwise.",4.2 Continuous Adaptation,[0],[0]
"As with discrete adaptation, learning then proceeds unmodified with these augmented instances.
",4.2 Continuous Adaptation,[0.9999999845638583],"['As with discrete adaptation, learning then proceeds unmodified with these augmented instances.']"
"The augmented training data (trainaug) is thus associated with the features x of the tweet, the task labels y, and the user information u.",4.2 Continuous Adaptation,[1.0],"['The augmented training data (trainaug) is thus associated with the features x of the tweet, the task labels y, and the user information u.']"
"Following the feature augmentation formulation, any supervised learning task of finding a parametrized function hθ over the original labeled training data can now be specified in terms of the augmented training data along with the transformed instances:
arg min θ ∑ (x,y,u)∈trainaug loss (hθ (Φ(x, u), y))
",4.2 Continuous Adaptation,[0],[0]
For test instances we apply the same transformation function Φ before prediction.,4.2 Continuous Adaptation,[0],[0]
"We apply user-factor adaptation to five popular NLP tasks: part-of-speech tagging, prepositional-
phrase attachment, sentiment analysis, sarcasm detection, and stance detection.",5 Evaluation,[0],[0]
"These represent both syntactic and semantic tasks; include some of the key steps in an NLP application pipeline; and use different types of learning formulations including logistic regression, conditional random fields, and support vector machines.
",5 Evaluation,[0],[0]
We demonstrate the value of user-factor adaptation on strong baselines for each task.,5 Evaluation,[0],[0]
Table 2 provides the specific details for each task including the systems used and their configurations.,5 Evaluation,[0],[0]
"We learn factors from a user’s background language, or past tweets2.",5.1 Implementation Details,[1.0],"['We learn factors from a user’s background language, or past tweets2.']"
"To do so, we collect up to 200 tweets per user; users with fewer than 20 tweets were excluded.",5.1 Implementation Details,[0],[0]
"Retweets were not included and all tweets were tokenized using the Happier Fun Tokenizer3.
",5.1 Implementation Details,[0],[0]
Demographics and Personality.,5.1 Implementation Details,[0],[0]
We derive real-valued demographics and personality scores using the models introduced in section 3.1.,5.1 Implementation Details,[0],[0]
"For demographics, our model predicts continuous age and a gender score where higher values imply more “femaleness”.",5.1 Implementation Details,[0],[0]
"For personality, these scores represent the Big Five personality traits: openness to experience, conscientiousness, extraversion, agreeableness, and neuroticism (Goldberg, 1990; McCrae and Costa Jr., 1997).",5.1 Implementation Details,[0],[0]
"Age, gender, and the five personality dimensions are each a single factor.
2Factor inference code is available at:",5.1 Implementation Details,[0],[0]
"https://stonybrooknlp.github.io/user-factor-adaptation/ 3https://github.com/dlatk/happierfuntokenizing
Latent Language Factors.",5.1 Implementation Details,[0],[0]
We use three methods to derive latent factors: (1) tf-idf:,5.1 Implementation Details,[0],[0]
"The TF-IDF approach uses unigrams, bigrams, and trigrams occurring in more than 20% but fewer than 80% of documents.",5.1 Implementation Details,[0],[0]
(2) word2vec,5.1 Implementation Details,[0],[0]
:,5.1 Implementation Details,[0],[0]
"The skip-grams algorithm (Mikolov et al., 2013) was used to produce 50-dimensional word embeddings.",5.1 Implementation Details,[0],[0]
"(3) userembed: d-dimensional user embeddings from generative factor analysis (Child, 1990) over relative frequencies of n-grams per user-background.",5.1 Implementation Details,[0],[0]
The TF-IDF and word2vec representations are then clustered to produce a low-dimensional representation of the users.,5.1 Implementation Details,[0],[0]
Each dimension is a single factor.,5.1 Implementation Details,[0],[0]
"We primarily report results for d=5 for all latent factors, although we explore alternate values in Section 5.3.
",5.1 Implementation Details,[0],[0]
Discrete Adaptation.,5.1 Implementation Details,[0],[0]
Each user is mapped to a single “domain” per factor.,5.1 Implementation Details,[0],[0]
"For inferred age, we select three equally-sized domains: age < 24, 24 < age < 28, and age > 28.",5.1 Implementation Details,[0],[0]
TF-IDF and word2vec define their domains based on cluster membership.,5.1 Implementation Details,[0],[0]
"Gender, personality, and user embeddings have two domains, above and below the mean, which is done on a per-dimension basis.
",5.1 Implementation Details,[0],[0]
Continuous Adaptation.,5.1 Implementation Details,[0],[0]
We apply transformations to the raw factor scores before using them for adaptation.,5.1 Implementation Details,[0],[0]
"For demographic and personality factors, we apply a min-max transformation.",5.1 Implementation Details,[0],[0]
"Because language often does not vary linearly with age (Pennebaker and Stone, 2003), we additionally use the square root of the predicted age.",5.1 Implementation Details,[0],[0]
"For the cluster based latent factors, we use the inverse of the Euclidean distance of the user-background
from the cluster centroid, amplifying the power of those users who are closest to each cluster.",5.1 Implementation Details,[0],[0]
User embeddings from factor analysis are used without any transforms since they naturally produce a Gaussian distribution.,5.1 Implementation Details,[0],[0]
Table 3 presents the main adaptation results.,5.2 Results,[0],[0]
"We compare the performance of adaptation techniques against two baselines: no inclusion of additional factors or adaptation, and models with factors randomly drawn from a Gaussian distribution – a situation requiring learning the same number of parameters as our most augmented models.",5.2 Results,[0],[0]
"For the random factor baseline, we take the average performance across five iterations for both discrete and continuous adaptation.",5.2 Results,[0],[0]
"To establish significance of difference in error between adaptation results and the no-adaptation baseline, we use permutation testing for stance detection and McNemar’s test for the others.",5.2 Results,[0],[0]
Our findings follow.,5.2 Results,[0],[0]
"While these conclusions were drawn from our own experiments, we encourage future researchers to see what works best on their own tasks and datasets.",5.2 Results,[0],[0]
(i) Adaptation improves over unadapted baselines:,5.2 Results,[0],[0]
"The results show significant gains with adaptation for PP-attachment (+1.0), sentiment (+1.0), sarcasm (+3.4), and stance (+3.0).",5.2 Results,[0],[0]
"Adaptation yields better results for sarcasm and stance, semantic tasks where we’d expect user preferences to be an important factor.",5.2 Results,[1.0],"['Adaptation yields better results for sarcasm and stance, semantic tasks where we’d expect user preferences to be an important factor.']"
"While prior studies have shown POS variations across demographic factors (Pennebaker and Stone, 2003; Schwartz
et al., 2013), we hypothesize that the ambiguity in POS reduces greatly when conditioning on local context compared to demographic preferences.",5.2 Results,[0],[0]
"This coupled with the ceiling effect in a strong baseline may explain the lack of improvements.
",5.2 Results,[0],[0]
"(ii) Continuous is better than discrete: For PPattachment, sarcasm, and sentiment, continuous adaptation is better than discrete in all but three of the eighteen configurations.",5.2 Results,[0],[0]
Binning people into discrete groups is hard and lossy; using continuous weights helps avoid this issue.,5.2 Results,[0],[0]
"Stance, however, is the one task where discrete works better for most factors.",5.2 Results,[1.0],"['Stance, however, is the one task where discrete works better for most factors.']"
"As we show in Section 5.4, demographics and personality scores are themselves highly predictive of stances on issues; we believe this makes the simpler binning approach more reliable than the continuous model.
",5.2 Results,[0.9999999208913535],"['As we show in Section 5.4, demographics and personality scores are themselves highly predictive of stances on issues; we believe this makes the simpler binning approach more reliable than the continuous model.']"
"(iii) Both known and latent factors are helpful: Sarcasm benefits from age, gender and personality based adaptations, while stance benefits from personality.",5.2 Results,[1.0],"['(iii) Both known and latent factors are helpful: Sarcasm benefits from age, gender and personality based adaptations, while stance benefits from personality.']"
The demographic and personality factors do not help PP-attachment.,5.2 Results,[0],[0]
"Language factors help all tasks except POS tagging.
",5.2 Results,[0],[0]
"(iv) Latent factors provide best gains: The latent language factors provide the best observed gains for all of the tasks where we saw a significant improvement: PP-attachment, sentiment, sarcasm, and stance.",5.2 Results,[0],[0]
"The language factors model users directly in terms of the similarities (and differences) in their entire language use, whereas the inferred demographic and personality factors focus more on a subset of their language as it relates to the particular attribute.
",5.2 Results,[0],[0]
(v),5.2 Results,[0],[0]
"Expanding feature space alone is not enough:
One possible explanation for the gains with factors are that the expanded feature space could somehow provide more capacity for learners to generalize.",5.2 Results,[0],[0]
"However, adapting to random factors typically lowered results, suggesting that models using more features but not more information naturally take a hit.",5.2 Results,[1.0],"['However, adapting to random factors typically lowered results, suggesting that models using more features but not more information naturally take a hit.']"
"The amount of background available directly affects the factor measurement, which in turn may impact adaptation effectiveness.",5.3 Background Size and Number of Factors,[0],[0]
Figure 2a shows how varying the background size affects adaptation effectiveness for sarcasm.,5.3 Background Size and Number of Factors,[0],[0]
"In general, larger backgrounds lead to bigger gains as expected.",5.3 Background Size and Number of Factors,[0],[0]
"Even with small amounts of background (50 tweets) adaptation can provide gains, suggesting that even with imperfect predictions of the user attributes, there is still some benefit to adaptation.
",5.3 Background Size and Number of Factors,[0],[0]
Figure 2b compares how performance varies with the number of latent factors for sarcasm.,5.3 Background Size and Number of Factors,[0],[0]
We see gains for all d sizes we explored.,5.3 Background Size and Number of Factors,[0],[0]
Performance improves with d first and then tapers off; its best is +3.4 at d=5 and 7.,5.3 Background Size and Number of Factors,[1.0],['Performance improves with d first and then tapers off; its best is +3.4 at d=5 and 7.']
"As the number of factors increases, there is greater potential for a finegrained characterization of language use differences.",5.3 Background Size and Number of Factors,[0],[0]
"However, this is offset by the increased risk of overwhelming the learner with too many parameters to learn during adaptation.",5.3 Background Size and Number of Factors,[0],[0]
"We also find that the impact of number of factors also varies with the type of task (e.g., for PP-attachment we find d=3 gives the best performance of 72.2, a +1.2 gain over the baseline).
",5.3 Background Size and Number of Factors,[0],[0]
"(a) Background size effects for cases with large adaptation gains: sarcasm when using personality and user-embedding factors.
",5.3 Background Size and Number of Factors,[0],[0]
"(b) Gains over unadapted baseline for sarcasm using TF-IDF, user embeddings, and word2vec with varying number of factors.
",5.3 Background Size and Number of Factors,[0],[0]
Figure 2: Adaptation performance compared against background size and number of factors.,5.3 Background Size and Number of Factors,[0],[0]
"One way to use the factors is to add them as direct features to the instances, without adaptation.",5.4 Factors as Direct Features,[1.0],"['One way to use the factors is to add them as direct features to the instances, without adaptation.']"
"Table 4 compares how the most beneficial known factor, personality, performs when added directly as a feature to the two tasks where it had the highest impact.
",5.4 Factors as Direct Features,[0],[0]
"For sarcasm, adding personality as a direct feature itself leads to a strong improvement on par with using it for adaptation.",5.4 Factors as Direct Features,[0],[0]
"For stance, however, we see that while there is an improvement over the baseline, it is not as large as that from adaptation.",5.4 Factors as Direct Features,[1.0],"['For stance, however, we see that while there is an improvement over the baseline, it is not as large as that from adaptation.']"
"We observed little-to-no improvement for POS tagging, sentiment or PP-attachment when using personality as direct features.",5.4 Factors as Direct Features,[0],[0]
This reflects the relative complexity of the relationships between user factors and class labels for each task.,5.4 Factors as Direct Features,[0],[0]
"Note that while direct features provide benefits, the overall possible gain with adaptation using any factor (shown in best column) is larger.
",5.4 Factors as Direct Features,[0],[0]
"Including user factors as direct features is beneficial when there is a linear relationship with the class label, such as with gender and sarcasm use.",5.4 Factors as Direct Features,[0],[0]
"In contrast, user-factor adaptation can capture more complex relationships between user groups and their language expression.",5.4 Factors as Direct Features,[0],[0]
"Figure 3, for in-
stance, shows a three-way interaction between gender scores, adjective use and sarcasm.",5.4 Factors as Direct Features,[0],[0]
Increase in the number of adjectives is a positive indicator of sarcasm for women (high gender scores) but is a negative indicator for men (low gender scores).,5.4 Factors as Direct Features,[1.0],['Increase in the number of adjectives is a positive indicator of sarcasm for women (high gender scores) but is a negative indicator for men (low gender scores).']
We observe similar trends for age: phrases such as “thanks” and “love it” tend to be meant sarcastically by younger users and sincerely by older ones.,5.4 Factors as Direct Features,[0],[0]
User-factor adaptation can model these interaction relationships when direct features alone may not.,5.4 Factors as Direct Features,[1.0],['User-factor adaptation can model these interaction relationships when direct features alone may not.']
"To understand the advantage of continuous latent adaptation, we look at how well discrete and continuous factor representations capture meaningful information about users.",5.5 Comparison of Latent Representations,[1.0],"['To understand the advantage of continuous latent adaptation, we look at how well discrete and continuous factor representations capture meaningful information about users.']"
"To do so, we select two dimensions from the TF-IDF latent factors for stance detection and examine the extent to which they differentiate users based on their attributes (i.e. age) and posting behavior (i.e. URL use).",5.5 Comparison of Latent Representations,[0],[0]
This is shown in Figure 4.,5.5 Comparison of Latent Representations,[0],[0]
"The top row gives the
discrete representation: kernel density plots show age and URL use distributions for users binned into the two factor dimensions, shown here in red and blue.",5.5 Comparison of Latent Representations,[0],[0]
"The bottom gives the continuous representation: scatter plots show the relationship between age and URL use and the factor score for each dimension.
",5.5 Comparison of Latent Representations,[0],[0]
"In the discrete view, age distributions are similar for both factors; there is no apparent relationship between factor membership and age.",5.5 Comparison of Latent Representations,[0],[0]
"However, in the continuous view there is a clear negative correlation for age with the factor score for blue and a positive one for red.",5.5 Comparison of Latent Representations,[0],[0]
"This indicates that the factors are capturing meaningful information about user age: those with a high factor score for blue tend to be younger, whereas those with a low factor score are older.",5.5 Comparison of Latent Representations,[0],[0]
The reverse is true for red.,5.5 Comparison of Latent Representations,[0],[0]
"The URL use shows some difference between the two dimensions in the discrete view, and again we see strong and differing linear relationships with the continuous view.
",5.5 Comparison of Latent Representations,[0],[0]
"Overall, the latent factors appear to capture both user attributes and posting behavior, with the continuous version providing additional benefits in characterizing these relationships.",5.5 Comparison of Latent Representations,[0],[0]
The lack of a clear differentiation in the discrete case hints at the difficulty in capturing linguistic variance by splitting users into discrete groups.,5.5 Comparison of Latent Representations,[0],[0]
"Modeling users has a long history of successful applications in providing personalized information access (Dou et al., 2007; Teevan et al., 2005) and recommendations (Guy et al., 2009; Li et al., 2010; Morales et al., 2012).",6 Related Work,[0],[0]
"In contrast, this work models users to better understand their content via language processing tasks following ideas from demographics-aware and domain adaptation.
",6 Related Work,[0],[0]
"User-level language variance affects lexical choices (Preoţiuc-Pietro et al., 2016) and even syntactic aspects of language (Johannsen et al., 2015).",6 Related Work,[0],[0]
"Such variations can result in demographics-based bias in low-level tasks such as POS tagging (Hovy and Søgaard, 2015) and can also impact high-level applications such as sentiment analysis (Volkova et al., 2013) and machine translation (Mirkin et al., 2015), motivating demographics and personality-based adaptations.
",6 Related Work,[0],[0]
"Consequently, recent works have explored demographics-aware NLP (Volkova et al., 2013; Bamman et al., 2014a; Kulkarni et al., 2016; Hovy, 2015; Yang and Eisenstein, 2015).",6 Related Work,[0],[0]
Volkova et al. (2013) propose a gender-aware model and demonstrate superior performance over a genderagnostic model on the task of sentiment analysis.,6 Related Work,[0],[0]
Bamman et al. (2014a) and Kulkarni et al. (2016) analyze regional linguistic variation using region-specific word embeddings on online social media.,6 Related Work,[0],[0]
"Hovy (2015) advances this line of research further and learns group-specific word embeddings, showing improvements over general embeddings on three types of text classification tasks.",6 Related Work,[0],[0]
"When author demographics are not available, Yang and Eisenstein (2015) show that learning community-specific embeddings using social networks can help improve sentiment analysis.",6 Related Work,[0],[0]
"A similar approach with a social theory-based optimization also showed improvements for sentiment analysis (Hu et al., 2013).",6 Related Work,[0],[0]
"For sarcasm detection, historical information about the author and their past context (e.g. entities they discuss) have been shown to be helpful (Bamman and Smith, 2015; Khattri et al., 2015; Rajadesingan et al., 2015).
",6 Related Work,[0],[0]
Our work builds on these ideas and explores the general task of user-factor adaptation.,6 Related Work,[0],[0]
"Compared to past work, our method (a) is more general – working with both continuous and discrete factors, (b) uses factors beyond demographics – characteristics like personality are known to influence language beyond demographics (Schwartz
et al., 2013), and (c) only requires a background of language – by using inferred factors from a background of language, we require no a priori knowledge of user traits.",6 Related Work,[0],[0]
Language on social media reflects the diversity in its user base and motivates the need for robust models that can handle the resulting variations by user attributes.,7 Conclusion,[0],[0]
"We have introduced user-factor adaptation, a method to adapt typical supervised language classifiers based on factors of the user authoring the language.",7 Conclusion,[0],[0]
Our approach requires nothing more than a background of language by the user and only needs access to the features used by the supervised learner.,7 Conclusion,[0],[0]
"Since it requires no other modifications to the learner, our approach can be easily applied to many NLP tasks.
",7 Conclusion,[0],[0]
"To the best of our knowledge, this represents the first work to use the idea of continuousvalued variables for language processing adaptation.",7 Conclusion,[0],[0]
"Continuous adaptation to a variety of user factors brings us closer to personalized NLP and outperforms discrete adaptation over four different tasks: part-of-speech tagging, preposition-phrase attachment, sentiment analysis, and sarcasm detection.",7 Conclusion,[0],[0]
Adaptations with data-driven latent factors produced the largest gains.,7 Conclusion,[0],[0]
"We see this work as part of a growing trend to put language not just within its document-wide context, but also within the context of the human being that wrote it.",7 Conclusion,[0],[0]
"This publication was made possible, in part, through the support of a grant from the Templeton Religion Trust – TRT0048.",Acknowledgments,[0],[0]
"We wish to thank the following colleagues for their annotation help for the PP-attachment task: Chetan Naik, Heeyoung Kwon, Ibrahim Hammoud, Jun Kang, Masoud Rouhizadeh, Mohammadzaman Zamani, and Samuel Louvan.",Acknowledgments,[0],[0]
"We pose the general task of user-factor adaptation — adapting supervised learning models to real-valued user factors inferred from a background of their language, reflecting the idea that a piece of text should be understood within the context of the user that wrote it.",abstractText,[0],[0]
"We introduce a continuous adaptation technique, suited for real-valued user factors that are common in social science and bringing us closer to personalized NLP, adapting to each user uniquely.",abstractText,[0],[0]
"We apply this technique with known user factors including age, gender, and personality traits, as well as latent factors, evaluating over five tasks: POS tagging, PP-attachment, sentiment analysis, sarcasm detection, and stance detection.",abstractText,[0],[0]
"Adaptation provides statistically significant benefits for 3 of the 5 tasks: up to +1.2 points for PP-attachment, +3.4 points for sarcasm, and +3.0 points for stance.",abstractText,[0],[0]
Human Centered NLP with User-Factor Adaptation,title,[0],[0]
