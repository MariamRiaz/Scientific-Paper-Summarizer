{"sections": [{"heading": "1. Introduction", "text": "Many real-world processes of interest, ranging from climate variables to brain signals, are spatio-temporal in nature, cf. Cressie & Wikle (2011). That is, they can be described as a random quantity that varies over some fixed spatial and temporal domain. Suppose we obtain n training points from a real-valued spatio-temporal process,\nDn = { (s1, t1, y1), . . . , (sn, tn, yn) } ,\nwhere yi denotes the quantity of interest observed at the ith training point, with spatial coordinate si and time ti. For notational convenience, let (s, t, y) denote an unobserved test point in space-time where y is unknown. Then a common goal is to predict y in unobserved space-time regions (s, t) using Dn. Specifically, certain spatial regions may have limited data coverage over extended periods of time, as illustrated in Figure 1. In real-world applications, Dn need not be gathered in a single batch but obtained in parts over time from various sensors, stations, satellites,\n*Equal contribution 1Uppsala University, Sweden. Correspondence to: Muhammad Osama <muhammad.osama@it.uu.se>, Dave Zachariah <dave.zachariah@it.uu.se>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\netc. That is, the dataset is augmented sequentially, i.e., n = 1, 2, . . . , N . In these streaming data scenarios, we are interested in continuous refinement of the prediction of y at (s, t) as new data is augmented into Dn+1.\nThe unknown data-generating process is often assumed to belong to a class of data models indexed by a parameter \u03b8. Each model \u03b8 in the class yields a predictor y\u0302\u03b8(s, t) of y at test point (s, t). A specific set of model parameters \u03b8\u0302 is learned using Dn. Examples of commonly used model classes include Gaussian Processes (GP) (Rasmussen & Williams, 2006), spatio-temporal random effects models (Cressie et al., 2010), dynamic factor analysis models (Lopes et al., 2008; Fox & Dunson, 2015), spatial random effect models extended to incorporate time as an additional dimension (Zammit-Mangion & Cressie, 2017) (cf. related work section below). For many spatio-temporal applications, the model class should be capable of expressing temporal patterns that change across different spatial regions. Moreover, for streaming data scenarios, the learned parameter \u03b8\u0302 and the resulting predictor y\u0302\u03b8\u0302(s, t) should be updated in a sequential manner.\nOur contribution in this paper is two-fold:\n\u2022 we develop a non-stationary, localized covariance model capable of capturing temporal patterns that change across space, as illustrated in Figure 2 below.\n\u2022 we show how to sequentially learn the covariance model parameters and update the predictor from streaming spatio-temporal data, with a runtime that is linear in n.\nIn Section 2, we relate our work to already existing approaches and introduce a commonly used model class in Section 3. In Section 4 we develop a localized spatiotemporal covariance model to be used in conjunction with a covariance-fitting learning approach. Finally, the proposed method is evaluated using synthetic and real climate data in Sections 5 and 6, respectively.\nNotation: col{s1, s2} stacks both elements into a single column vector. \u2297, \u03b4(\u00b7), \u2016 \u00b7 \u2016W and \u2020 denote the Kronecker product, Kronecker delta function, weighted `2-norm and Moore-Penrose inverse, respectively. Finally, the sample mean is denoted by E\u0302[si] = 1n \u2211n i=1 si."}, {"heading": "2. Related work", "text": "A popular model class is the family of GPs, specified by a mean and covariance function (Rasmussen & Williams, 2006). This approach is computationally prohibitive in its basic form since both learning the model parameters \u03b8 and implementing the predictor y\u0302\u03b8(s, t) requires a runtime on the order of O(N3), where N is typically large in spatio-temporal applications. The predictor implementation can be approximated using various techniques. One popular approach is to approximate the training data using m N inducing points which reduces the runtime toO(m2N) (Quin\u0303onero-Candela & Rasmussen, 2005; Bijl et al., 2015). Moreover, by assuming Kronecker covariance functions it is possible to obtain even shorter runtimes by utilizing the Kronecker structure of the GP covariance matrix (Saatc\u0327i, 2012). If the model class is restricted to stationary covariance functions, the runtimes can be reduced further, cf. (Saatc\u0327i, 2012; Wilson et al., 2014). In the spacetime domain, such models are also equivalent to dynamical system models so that y\u0302\u03b8(s, t) can be approximated using a basis expansion and implemented by a Kalman smoother (Sa\u0308rkka\u0308 et al., 2013). In the above cases, however, \u03b8 and y\u0302\u03b8(s, t) are not updated jointly when obtaining streaming data.\nThe restriction to stationary covariance models is, moreover, not always adequate to capture temporal patterns that differ across spatial regions. This modeling limitation is addressed by Cressie et al. (2010), where a discrete-time model class is partially specified using a spatial basis function expansion with time-varying expansion coefficients. These are modeled as a first-order vector auto-regressive process. The coefficients thus determine a spatial pattern of the process that evolves at each discrete time-instant. This model class can capture patterns localized to specific regions in space, unlike stationary covariance models. The predictor y\u0302\u03b8(s, t) can be viewed as a spatial fixed-rank kriging method that is updated via a Kalman filter and thus applicable to streaming data (cf. Cressie & Johannesson (2008)). The model parameter \u03b8, however, is learned us-\ning a moment-fitting approach and operates on batch rather than streaming datasets. Other work using dynamic factor analysis models (Lopes et al., 2008; Fox & Dunson, 2015) similarly allow for time-varying coefficients but with more flexible data-adaptive basis. However, they are implemented using Markov Chain Monte Carlo methods which are computationally prohibitive for the scenarios considered herein.\nMoreover, a first-order auto-regressive structure may not accurately capture more complex temporal patterns observed in real spatio-temporal processes. The approach taken by Zammit-Mangion & Cressie (2017) circumvents this limitation using basis functions that are localized in both space and time. Time locality cannot, however, capture periodic patterns or trends necessary for interpolation over longer periods. The model parameters are learned using an expectation-maximization method which is not readily applicable to streaming data scenarios."}, {"heading": "3. Spatio-temporal model class", "text": "We begin by defining the data vector y = col{y1, y2, . . . , yn} obtained from Dn. For the test point (s, t), we consider the unbiased predictor of y as a linear combination of the data (Stein, 2012):\ny\u0302(s, t) = \u03bb>(s, t)y, (1)\nwhere \u03bb>(s, t) is a vector of n weights which naturally depend on the test point (s, t). The weight vector is defined as the minimizer of the conditional mean square prediction error. That is,\n\u03bb(s, t) , arg min \u03bb\nE [ (y \u2212 \u03bb>y)2 \u2223\u2223 s, t ]. (2)\nSince the conditional error is determined by the unknown distribution p(y,y|s, t, s1, t1, . . . , sn, tn), we specify a class of data-generating models, using only the mean and covariance (Cressie & Wikle, 2011):{\nE[y] = u>(s, t)\u03b7, Cov[y, y\u2032] = \u03c6>(s, t)\u0398\u03c6(s\u2032, t\u2032) + \u03b80\u03b4(s, s \u2032)\u03b4(t, t\u2032).\n(3) The function u(s, t) captures the expected trend of the entire spatio-temporal process y, and when there is no such general trend we set u(s, t) \u2261 1. The function \u03c6(s, t) captures the smoothness of the process in space-time and is of dimension p \u00d7 1. The parameter matrix \u0398 is diagonal and specifies the relevance of each dimension of \u03c6(s, t) similar to the way in which automatic relevance determination is sometimes used within the GP (Tipping, 2001; Faul & Tipping, 2002). Taken together, (3) specifies a class of models, each of which is indexed by the parameters (\u03b7,\u0398, \u03b80). The p+1 covariance parameters (\u0398, \u03b80), which we collectively\ndenote by \u03b8 = col{\u03b80, \u03b81, . . . , \u03b8p} for notational convenience, determine the spatio-temporal covariance structure Cov\u03b8[y, y\n\u2032] which depends on the function \u03c6(s, t). In the next section, we will specify \u03c6(s, t) to develop a suitable covariance model to capture local spatial and periodic temporal patterns.\nFor a given model in the class, the optimal weights (2) are given in closed form (Stein, 2012) as\n\u03bb\u03b8(s, t) = K \u221211(1>K\u221211)\u2020 + K\u22121\u03a0\u22a5\u03a6\u0398\u03c6(s, t),\nwhere the subindex highlights the model parameter dependence. The quantities in \u03bb\u03b8(s, t) are determined by the regressor matrix\n\u03a6 = [ \u03c6(s1, t1) . . . \u03c6(sn, tn) ]> and the following covariance matrix\nK\u03b8 = Cov[y,y] = \u03a6\u0398\u03a6 > + \u03b80I 0, (5)\nwith \u03a0\u22a5 = I\u2212 1(1>K\u221211)\u20201K\u22121 being an oblique projector onto span(1)\u22a5.\nThe optimal weights are invariant to the mean parameters \u223c \u03b7 and to uniform scaling of the p + 1 covariance parameters \u03b8. By learning \u03b8 up to an arbitrary scale factor, the predictor (1) is given by the linear combiner weights \u03bb\u03b8(s, t). If we assume that the process is Gaussian, the model can be learned using the maximum likelihood framework. However, this yields neither a convex problem nor one that is readily solved in a sequential manner as Dn is augmented sequentially. In the next section, we apply a convex covariance-fitting framework to learn the spatiotemporal model using streaming data."}, {"heading": "4. Proposed method", "text": "Below we specify the function \u03c6(s, t) in (3) such that the spatio-temporal covariance structure Cov\u03b8[y, y\u2032] can express local spatial patterns with varying temporal periodicities as illustrated in Figure 2. Subsequently, we apply a covariance-fitting methodology for learning the model parameters such that the predictor (1) can be updated sequentially for each new observation (Zachariah et al., 2017)."}, {"heading": "4.1. Local-periodic space-time basis", "text": "The function \u03c6(s, t) varies over a space-time domain S \u00d7 T \u2282 Rd+1 and its elements can be thought of as basis functions. It is formulated as a Kronecker product of a time and space bases,\n\u03c6(s, t) = \u03c8(t)\u2297\u03d5(s), (6)\nfor compactness.\nWe begin by specifying the spatial function as\n\u03d5(s) = \u03d51(s1)\u2297 \u00b7 \u00b7 \u00b7 \u2297\u03d5d(sd), (7)\nwhere the basis vector for the ith spatial dimension,\n\u03d5i(si) = col{ \u03d5i,1(si), \u00b7 \u00b7 \u00b7 , \u03d5i,Ns(si) } (8)\nis composed of Ns localized components with a finite support L. For notational simplicity, we consider Ns and L to be same for each dimension i. Based on their computational attractiveness and local approximation properties we use a cubic spline basis (Rasmussen & Williams, 2006; Wasserman, 2006). Then (8) is given by (4), where c determines the location of a component. Figure 3a illustrates the components as a function of its spatial dimension. We place the centers c of each component uniformly across the spatial dimensions.\nUsing \u03d5(s) allows for covariance structures that are localized in space in such a way that neighbouring points have a nonnegative correlation and points far from each other have no correlation as determined by the support size L. Hence for a given L, the resulting covariance structure can capture local spatial patterns of a certain scale and can easily be extended to cover multiple scales by replacing (6) with for example\n\u03c6(s, t) = \u03c8(t)\u2297 [ \u03d5L1(s) \u03d5L2(s) ] that accommodates two different support sizes L1 and L2. The number of basis functions Ns is chosen such that adjacent localized components \u03d5i(s) have overlapping support to cater for points in between them. This requirement is\n\u03d5(s) =  1 6f(s) 3 (c\u22122)L 4 \u2264 s < (c\u22121)L 4 \u22121 2 f(s) 3 + 2f(s)2 \u2212 2f(s) + 23 (c\u22121)L 4 \u2264 s < Lc 4 1 2f(s) 3 \u2212 4f(s)2 + 10f(s)\u2212 223 Lc 4 \u2264 s < (c+1)L 4 \u22121 6 f(s) 3 + 2f(s)2 \u2212 8f(s) + 323 (c+1)L 4 \u2264 s \u2264 (c+2)L 4\n0 otherwise\nwhere f(s) = 4s\nL \u2212c+2\n(4)\nfulfilled by choosing Ns > RsL where Rs is the range of the spatial dimension. For example when Ns = 2RsL , the adjacent component \u03d5i(s) have 50 percent overlap. The maximum value of Ns is limited by the number of training points and the computational resources that are available.\nThe temporal function \u03c8(t) is also specified by a basis\n\u03c8(t) = col{ \u03c80(t), \u03c81(t), . . . , \u03c8Nt(t) }. (9)\nHowever, to be able to predict missing data of the type illustrated in Figure 1 we cannot rely on a localized basis for extended interpolations over space-time. Due to its good approximating properties we instead apply the periodic basis developed by Solin & Sa\u0308rkka\u0308 (2014) defined over a range T = [0, Rt]:\n\u03c8k(t) =\n{ 1, k = 0,\n1\u221a Rt sin (k\u03c0 t+Rt2Rt ), otherwise, (10)\nSimilar to a Fourier basis, \u03c8(t) allows for periodic covariance structures that capture both fixed and periodic patterns in the data along time with different frequencies. Moreover, as Nt grows, any temporally stationary covariance structure can be captured, cf. (Solin & Sa\u0308rkka\u0308, 2014). Using (10), the maximum frequency in the model is Nt4Rt . Hence,\ndepending on the data and the highest frequency periodic patterns we may expect in it, an appropriate value of Nt can be chosen.\nIn summary, the proposed spatio-temporal basis \u03c6(s, t) in (6) is of dimension p = Nds (Nt+1) and yields a covariance function Cov\u03b8[y, y\u2032] that may vary temporally with different frequencies specific to different spatial regions, as illustrated in Figure 2. The covariance structure is determined by the parameter \u03b8, which we learn using a covariancefitting methodology considered next."}, {"heading": "4.2. Learning method for streaming data", "text": "We describe a covariance-fitting approach for learning the model parameter \u03b8, up to an arbitrary scale factor, from streaming data. Given a training dataset Dn, this approach enables us to update the predictor y\u0302\u03b8(s, t) = \u03bb>\u03b8 (s, t)y from (1) in a streaming fashion as n = 1, 2, . . . . We consider fitting the model covariance structure of the training data y, which is parameterized by \u03b8 in (5), to the empirical structure. Let us first define a normalized sample covariance matrix of the training data,\nK\u0303 = (y \u2212 1\u03b7)(y \u2212 1\u03b7)>\n\u2016y \u2212 1\u03b7\u20162 .\nHere 1 corresponds to using u(s, t) \u2261 1. Then the optimal model parameters are given by a covariance-fitting criterion (cf. (Cressie, 1985; Anderson, 1989; Cressie & Johannesson, 2008; Stoica et al., 2011)) with minimizer:\n\u03b8\u0302 = arg min \u03b8 \u2225\u2225K\u0303\u2212K\u03b8\u2225\u22252K\u22121\u03b8 (11) Here the matrix norm corresponds to a weighted norm which penalizes correlated residuals. The learned parameter \u03b8\u0302 is invariant with respect to the mean parameter \u03b7 and can be rescaled by an arbitrary scale factor (Zachariah et al., 2017). Moreover, the resulting predictor corresponding to \u03b8\u0302 in equation (1) can be written in the equivalent form:\ny\u0302\u03b8\u0302(s, t) \u2261 \u03b1 >(s, t)w? (12)\nwhere \u03b1(s, t) = col{1,\u03c6(s, t)}. The (p + 1)-dimensional weight vector w? is defined as the minimizer\nw? = arg min w\n\u221a E\u0302 [ |yi \u2212\u03b1>(si, ti)w|2 ] +\n1\u221a N \u2016\u03b6 w\u20161\n(13) where the elements of \u03b6 are given by\n\u03b6j =\n{ 1\u221a N \u2016[\u03a6]j\u22121\u20162, j > 1,\n0, otherwise,\nFor proofs of these relations and a derivation of its computational properties, see Zachariah et al. (2017).\nThe resulting predictor in (12) is called the SPICE (sparse iterative covariance-based estimation) predictor. It is computed via a convex and sparsifying regularized minimization problem that can be solved using coordinate descent with recursively updated quantities at each new training point (sn, tn, yn). By exploiting this structure, our predictor y\u0302\u03b8\u0302(s, t) can now be updated with streaming data as n = 1, 2, . . . . A pseudocode implementation is provided in Algorithm 1. The key recursively updated quantities passed from one update to the next are the symmetric matrix \u0393 and the vectors \u03c1 and w\u030c of dimension p+1 along with the scalar \u223c \u03ba. Here w\u030c is the weight vector at sample n\u2212 1, which is initialized at zero along with the above variables in Algorithm 1. The runtime is linear in n and constant in memory. That is, for a fixed training data sizeN , the total runtime of the algorithm is on the order O(Np2) and its memory requirement is O(p2). For further details, we refer the reader to the supplementary material. Code available at github."}, {"heading": "5. Synthetic data", "text": "The proposed method has been derived for predictions using large and/or streaming data sets. We now demonstrate its predictive properties using synthetic data and for the sake of reference compare it with a GPR (Gaussian process regression) method using different covariance functions Cov[y, y\u2032].\nAlgorithm 1 Learning from streaming datasets Input: (sn, tn, yn) and w\u030c \u0393 := \u0393 +\u03b1(sn, tn)\u03b1\n>(sn, tn) \u03c1 := \u03c1+\u03b1(sn, tn)yn \u03ba := \u03ba+ y2n := \u03ba+ w\u030c>\u0393w\u030c \u2212 2w\u030c>\u03c1 \u03c4 := \u03c1\u2212 \u0393w\u030c repeat j = 1, . . . , p+ 1 cj := \u03c4j + \u0393jjw\u030cj if j = 1 then wj :=\ncj \u0393jj\nelse aj := + \u0393jjw\u030c 2 j + 2w\u030cj\u03c4j\ns\u0302j := sign(cj) r\u0302j := |cj | \u0393jj \u2212 1\u0393jj\n\u221a aj\u0393jj\u2212|cj |2\nn\u22121\nwj :=\n{ s\u0302j r\u0302j \u221a n\u2212 1|cj | > \u221a aj\u0393jj \u2212 |cj |2\n0 otherwise end if := + \u0393jj(w\u030cj \u2212 w?j )2 + 2(w\u030cj \u2212 w?j )\u03c4j \u03c4 := \u03c4 + [\u0393]j(w\u030cj \u2212 w?j )\nuntil number of iterations equal L Output: w? = w\u030c"}, {"heading": "5.1. Damped planar wave", "text": "To illustrate a dynamically evolving process, we consider planar a wave in one-dimensional space and time, cf. Figure 4a. The unknown process is generated according to:\ny(s, t) = cos\n( 2\u03c0\n\u03bbs (s\u2212 vst)\n) exp ( \u2212 s\n20\n) + \u03b5, (14)\nwhere vs is the speed of the wave along space in units per second, \u03bbs is the wavelength in units of space and \u03b5 is a zero-mean white Gaussian process with standard deviation \u03c3.\nNote that the process decays exponentially as it propagates through space. For our experiments, we set vs = 3 [spatial units/sec], \u03bbs = 9 [spatial units] and \u03c3 = 0.3. Synthetic data is generated over a uniform grid and a subset of N = 700 training points are used. Different contiguous space-time blocks are selected as test regions to resemble realistic scenarios in which the coverage of sensors, satellites or other measurement equipment is incomplete. For example, the dashed white boxes in Figure 4 emulate cases where data over a small region is missing most of the time. By contrast, the dashed black boxes correspond to cases when data over large spatial regions is missing some of the time.\nThe process in these test regions as well as at other randomly missing points is predicted using the proposed\nmethod with Nt = 25, Ns = 15 and a spatial basis support set to L = 5 spatial units. This results in \u03c6(s, t) being of dimension p = Ns(Nt + 1) = 390. The mean-square error (MSE) of the prediction is shown in Figure 4b and evaluated using 25 Monte Carlo simulations. The region in the white box extends over almost the entire time dimension, hence there are very few neighbouring training points in time to draw upon for prediction and no information about the periodicities in the region. Instead our method leverages the neighbouring spatial information to obtain a good prediction resulting in a low MSE. Both black boxes are test regions that have neighbouring training points that provide temporal information about the process. However, left region has training points both before and after whereas the right region only has points before, yielding a more challenging prediction problem. Nevertheless, the proposed method is able to learn both the periodic and the local damping patterns to provide accurate predictions in both regions.\nWe include also the MSE of GPR using two different covariance functions learned by a numerical maximum likelihood search. While this method is not applicable to the streaming data of interest here, it provides a performance reference. First, we use a Mate\u0301rn ARD covariance model (Rasmussen & Nickisch, 2010) to carefully adapt both space and time dimensions. In Figure 4c it is seen that the resulting prediction errors are markedly worse for the large missing spatial regions and the method naturally fails to capture the periodic pattern of the process. Next, we use a periodic Mate\u0301rn ARD covariance model to also capture space-time periodicity. However, the MSE (Figure 4d) is degraded throughout, which is possibly due to the non-convex optimization problem used to learn the model parameters. It may lead to local minima issues, including learning erroneous periods."}, {"heading": "5.2. Varying seasonalities across space", "text": "Here we generate a process that emulates scenarios of temporal periodicities which may vary across spatial regions. This occurs e.g. in climate data. Figure 5a shows a realization of a process generated according to\ny(s, t) = cos\n( 2\u03c0\nT (s) t\n) + \u03b5 (15)\nwhere the period T (s) differs across space and \u03b5 is zeromean white Gaussian process with standard deviation \u03c3 = 0.3. In the upper region of the spatial domain T (s) = \u221e, i.e., the process has a constant mean. In the middle and bottom regions T (s) is large and small, respectively. The data is generated over a uniform grid and a subset of N = 600 points is used for training. A contiguous space-time block, marked by the dashed black box in Figure 5, forms a test region to emulate scenarios where data can be missing\nover a large spatial region for some time.\nFor the proposed method we use Nt = 35, Ns = 15 and a support of L = 3 for the spatial basis, so that p = Ns(Nt + 1) = 540. For the GPR we use the periodic Mate\u0301rn ARD kernel. Figures 5b and 5c show the MSE performance of the proposed method and GPR respectively which were obtained using 25 Monte Carlo simulations. The MSE of the proposed method is overall lower than that of GPR, both in the dashed test region as well as outside it. Unlike the proposed method, GPR has one parameter to fit to an overall periodic pattern and is thus unable to learn spatially localized patterns. Thus after learning, the process is predicted to be be nearly constant along time for all parts of the spatial region."}, {"heading": "6. Real data", "text": "We now demonstrate the proposed method for much larger, and possibly streaming, real-world datasets."}, {"heading": "6.1. Pacific Sea Surface Temperature", "text": "As a first application example, we use tropical pacific Sea Surface Temperature (SST) data (Wikle, 2011). These data represent gridded monthly SST anomalies, in \u25e6C, from January 1970 through March 2003 over a spatial region from 29\u25e6S to 29\u25e6N and 124\u25e6E to 70\u25e6W. The spatial resolution of the data is 2\u25e6 in both latitude and longitude.\nHere we consider data from the first 36 months, making the total number of space-time data points equal to 36 \u00d7 2 520 = 90 720. In the first experiment, training points are sampled randomly across space-time and the missing data constitute the test points. Here we set N = 63 503 as the number of training points. For the proposed method we set Nt = 100, Ns = 8 and the spatial support L to be half of each spatial dimension. Then p = N2s (Nt+ 1) = 6 464. Figure 6a shows the prediction error histogram of all test points across the spatio-temporal domain. We see that it is centered around zero and its dispersion is considerably narrower than the dynamic range of the data.\nIn the second experiment, we select a contiguous spacetime block as a test region in addition to other test points to evaluate the performance in scenarios where data over entire spatial regions are missing for a period of time. Data falling within the spatial region marked by the black dashed box in Figure 6c is missing beyond month 26, as indicated by the black dashed line in Figure 6d. Here N = 18 144 are the number of training points. The prediction error histogram for this second experiment is shown in Figure 6b and remains fairly narrow. Figure 6c illustrates the predicted SST anomalies [\u25e6C] for a spatial slice at month t = 30. We pick a spatial point in a region where the El Nin\u0303o effect, i.e., the periodic warming of the equatorial Pa-\ncific Sea Surface (Sarachik & Cane, 2010), is known to be noticeable. The prediction of the SST anomalies at this spatial location across time along with the true SST is illustrated with Figure 6d. Note that the predictor is able to track the rising temperature deviation also for the missing data."}, {"heading": "6.2. Precipitation data", "text": "As a second application example, we use precipitation data from the Climate Research Unit (CRU) time series datasets of climate variations (Jones & Harris, 2013). The precipitation data consists of monthly rainfall in millimeter over a period from 1901 to 2012 obtained with high spatial resolution (0.5 by 0.5 degree) over the whole planet. Here we consider a five year period from 2001 to 2005 and between spatial coordinates 95\u25e6W to 107\u25e6W and 40\u25e6N to 50\u25e6N. This yields a total number of 28 800 data points.\nThe spatial region indicated by the black dashed box in Figure 7b beyond month t = 47, as seen in Figure 7c, constitutes a contiguous test region, in addition to other randomly selected test points. The remaining N = 14 400 points are used for training.\nFor the proposed method we set Nt = 300, Ns = 6 and the spatial support L to be half of each spatial dimension. Then p = N2s (Nt + 1) = 10 836. Figure 7a shows the\nprediction error histogram for the precipitation test data. It is centered around zero and its dispersion is narrower than the dynamic range of the data. Figure 7b shows the contour plot of predicted precipitation for a spatial slice at month t = 54. The red cross and plus marker indicate spatial points whose actual and predicted time series are compared in Figures 7c and 7d, respectively. Note that the estimated precipitation tracks the true precipitation well everywhere even to the right of the black dashed line where the data was not seen during training. Note the ability of the predictor to track the different seasonal patterns in the missing regions."}, {"heading": "7. Conclusion", "text": "We proposed a method in which a spatio-temporal predictor y\u0302\u03b8\u0302(s, t) can be learned and updated sequentially as spatio-temporal data is obtained as a stream. It is capable of capturing spatially varying temporal patterns, using a non-stationary covariance model that is learned using a covariance-fitting approach. We demonstrated, using both simulated and real climate data, that it is capable of producing accurate predictions in large unobserved space-time test regions. In future work, we intend to further improve the computational efficiency of the method by exploiting the spatially localized structure of the covariance model.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2\nPrediction error\n0\n500\n1000\n1500\n2000\n2500\nN u m\nb e\nr o\nf p o\nin ts\n(a)\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2\nPrediction error\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nN u\nm b\ne r\no f\np o\nin ts\n(b)\n140 o E 160 o E 180 o E 160 o W 140 o W 120 o W 100 o W 80 o W\nLongitude\n25 o S\n20 o S\n15 o S\n10 o S\n5 o S\n0\n5 o N\n10 o N\n15 o N\n20 o N\n25 o N\nL a\nti tu\nd e\n-0.5\n0\n0.5\n1"}, {"heading": "8. Acknowledgements", "text": "NewLEADS - New Directions in Learning Dynamical Systems (contract number: 621-2016-06079), funded by the Swedish Research Council and the project ASSEMBLE (contract number: RIT15-0012), funded by the Swedish Foundation for Strategic Research (SSF)."}], "year": 2018, "references": [{"title": "Linear latent variable models and covariance structures", "authors": ["T.W. Anderson"], "venue": "Journal of Econometrics,", "year": 1989}, {"title": "Online sparse Gaussian process regression using FITC and PITC", "authors": ["H. Bijl", "van Wingerden", "J.-W", "T.B. Sch\u00f6n", "M. Verhaegen"], "venue": "approximations. IFAC-PapersOnLine,", "year": 2015}, {"title": "Fitting variogram models by weighted least squares", "authors": ["N. Cressie"], "venue": "Journal of the International Association for Mathematical Geology,", "year": 1985}, {"title": "Fixed rank kriging for very large spatial data sets", "authors": ["N. Cressie", "G. Johannesson"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2008}, {"title": "Statistics for spatio-temporal data", "authors": ["N. Cressie", "C.K. Wikle"], "year": 2011}, {"title": "Fixed rank filtering for spatio-temporal data", "authors": ["N. Cressie", "T. Shi", "E.L. Kang"], "venue": "Journal of Computational and Graphical Statistics,", "year": 2010}, {"title": "Analysis of sparse Bayesian learning", "authors": ["A.C. Faul", "M.E. Tipping"], "venue": "In Advances in neural information processing systems,", "year": 2002}, {"title": "Bayesian nonparametric covariance regression", "authors": ["E.B. Fox", "D.B. Dunson"], "venue": "Journal of Machine Learning Research,", "year": 2015}, {"title": "Climatic research unit (CRU) timeseries (ts) version 3.21 of high resolution gridded data of month-by-month variation in climate (jan", "authors": ["P. Jones", "I. Harris"], "venue": "NCAS British Atmospheric Data Centre,", "year": 1901}, {"title": "Spatial dynamic factor analysis", "authors": ["H.F. Lopes", "E. Salazar", "D. Gamerman"], "venue": "Bayesian Analysis,", "year": 2008}, {"title": "A unifying view of sparse approximate Gaussian process regression", "authors": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "Journal of Machine Learning Research,", "year": 1959}, {"title": "Gaussian processes for machine learning (gpml) toolbox", "authors": ["C.E. Rasmussen", "H. Nickisch"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Gaussian processes for machine learning, volume 1", "authors": ["C.E. Rasmussen", "C. Williams"], "venue": "MIT press Cambridge,", "year": 2006}, {"title": "Scalable inference for structured Gaussian process models", "authors": ["Y. Saat\u00e7i"], "venue": "PhD thesis, Citeseer,", "year": 2012}, {"title": "The El Nino-southern oscillation phenomenon", "authors": ["E.S. Sarachik", "M.A. Cane"], "year": 2010}, {"title": "Spatiotemporal learning via infinite-dimensional Bayesian filtering and smoothing: A look at Gaussian process regression through Kalman filtering", "authors": ["S. S\u00e4rkk\u00e4", "A. Solin", "J. Hartikainen"], "venue": "IEEE Signal Processing Magazine,", "year": 2013}, {"title": "Hilbert space methods for reduced-rank Gaussian process regression", "authors": ["A. Solin", "S. S\u00e4rkk\u00e4"], "venue": "arXiv preprint arXiv:1401.5508,", "year": 2014}, {"title": "Interpolation of spatial data: some theory for kriging", "authors": ["M.L. Stein"], "venue": "Springer Science & Business Media,", "year": 2012}, {"title": "New method of sparse parameter estimation in separable models and its use for spectral analysis of irregularly sampled data", "authors": ["P. Stoica", "P. Babu", "J. Li"], "venue": "IEEE Trans. Signal Processing,", "year": 2011}, {"title": "Sparse Bayesian learning and the relevance vector machine", "authors": ["M.E. Tipping"], "venue": "Journal of machine learning research,", "year": 2001}, {"title": "Sea surface temperature anomaly", "authors": ["C.K. Wikle"], "year": 2011}, {"title": "Online learning for distribution-free prediction", "authors": ["D. Zachariah", "P. Stoica", "T.B. Sch\u00f6n"], "venue": "arXiv preprint arXiv:1703.05060,", "year": 2017}, {"title": "FRK: An R package for spatial and spatio-temporal prediction with large datasets", "authors": ["A. Zammit-Mangion", "N. Cressie"], "venue": "arXiv preprint arXiv:1705.08105,", "year": 2017}], "id": "SP:1c7b01eef2126baf50ec3ab46aa07eb92dfde2c6", "authors": [{"name": "Muhammad Osama", "affiliations": []}, {"name": "Dave Zachariah", "affiliations": []}, {"name": "Thomas Sch\u00f6n", "affiliations": []}], "abstractText": "We address the problem of predicting spatiotemporal processes with temporal patterns that vary across spatial regions, when data is obtained as a stream. That is, when the training dataset is augmented sequentially. Specifically, we develop a localized spatio-temporal covariance model of the process that can capture spatially varying temporal periodicities in the data. We then apply a covariance-fitting methodology to learn the model parameters which yields a predictor that can be updated sequentially with each new data point. The proposed method is evaluated using both synthetic and real climate data which demonstrate its ability to accurately predict data missing in spatial regions over time.", "title": "Learning Localized Spatio-Temporal Models From Streaming Data"}