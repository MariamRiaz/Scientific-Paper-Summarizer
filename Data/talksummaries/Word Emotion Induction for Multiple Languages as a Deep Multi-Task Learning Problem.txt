5	138	In the meantime, a shift towards more expressive representation models for emotion can be observed that heavily draws inspirations from psychological theory, e.g., Basic Emotions (Ekman, 1992) or the Valence-Arousal-Dominance model (Bradley and Lang, 1994).
6	54	Though this change turned out to be really beneficial for sentiment analysis in NLP, a large variety of mutually incompatible encodings schemes for emotion and, consequently, annotation formats for emotion metadata in corpora have emerged that hinder the interoperability of these resources and their subsequent reuse, e.g., on the basis of alignments or mergers (Buechel and Hahn, 2017).
9	34	MTL has been shown to greatly decrease the risk of overfitting (Baxter, 1997), work well for various NLP tasks (Setiawan et al., 2015; Liu et al., 2015; Søgaard and Goldberg, 2016; Cummins et al., 2016; Liu et al., 2017; Peng et al., 2017), and practically increases sample size, thus making it a natural choice for small-sized data sets typically found in the area of word emotion induction.
13	15	Our MTL model surpasses the current state-of-the-art for each of them, and even performs competitive relative to human reliability.
101	37	Since we treat emotion prediction as a regression problem, the activation on the output layer aout (where out is the number of non-input layers in the network) is computed as the affine transformation a(out) := W (out)a(out−1) + b(out) (11) Boosting is a general machine learning technique where several weak estimators are combined to form a strong estimator.
107	16	That is, they fit one individual model per VAD dimension without sharing parameters between them.
108	20	In contradistinction, the key feature of our approach is that we fit a single FFNN model to predict all VAD dimensions jointly, thus applying multi-task learning to word emotion induction.
109	48	Hence, we treat the prediction of Valence, Arousal and Dominance as three independent tasks.
111	35	However, the activation in our two hidden layers (of 256 and 128 units, respectively) is shared across all VAD dimensions, and so are the associated weights and biases.
112	38	Thus, while we train our MTLNN model it is forced to learn intermediate representations of the input which are generally informative for all VAD dimensions.
117	19	We use leaky ReLU activation (LReLU) as nonlinearity (Maas et al., 2013).
119	24	For regularization, dropout (Srivastava et al., 2014) is applied during training with a probability of .2 on the embedding layer and .5 on the hidden layers.
127	27	The main hypothesis of this contribution is that an MTL set-up is superior to single-task learning for word emotion induction.
133	16	SepNN is equivalent to fitting our proposed model (but with only one output unit) to the different VAD dimensions individually, one after the other.
136	17	We will run MTLNN against SepNN on the EN and the EN+ data set (the former is very small, the latter relatively large; see Table 1) using the following set-up: for each gold lexicon and model, we randomly split the data 9/1 and train for 15, 000 iterations on the larger split (the same number of steps is used for the main experiment).
138	30	This process will be repeated 20 times and the performance figures at each one-thousand iterations step will be averaged.
142	26	Overall, performance is higher for the smaller EN lexicon.
145	58	As hypothesized, the MTLNN model does indeed outperform the single task model on both data sets.
151	19	We combined each of the selected lexicon data sets (Table 1) with each of the applicable publicly available embedding models (Section 2; the embedding model provided by Sedoc et al. (2017) will be used separately) for a total of 15 conditions, i.e, the rows in Table 4.
153	22	For conciseness, we present only the average correlation over the respective affective dimensions in Table 4 (Valence and Arousal for ES+ and ZH, VAD for the others).
154	73	Note that the methods we compare ourselves against comprise the current state-of-the art in both polarity and emotion induction (as described in Section 2).
155	29	As can be seen, our proposed MTLNN model outperforms all other approaches in each of the 15 conditions.
167	28	Warriner et al. (2013) (who created EN+) report an inter-study reliability (ISR; i.e., the correlation of the aggregated ratings from two different studies) between the EN and the EN+ lexicon of r = .953, .759 and .795 for VAD, respectively.
169	16	Thus, our proposed method did actually outperform human reliability for Dominance and is competitive for Valence and Arousal, as well.
172	65	Again, our MTLNN model performs very competitive with r = .870, .674 and .758, respectively using the COMMON embeddings.
173	60	In this paper, we propose multi-task learning (MTL) as a simple, yet surprisingly efficient method to improve the performance and, at the same time, to deal with existing data limitations in word emotion induction—the task to predict a complex emotion score for an individual word.
174	52	We validated our claim that MTL is superior to single-task learning by achieving better results with our proposed method in performance as well as training time compared to its single-task counterpart.
175	44	We performed an extensive evaluation of our model on 9 typologically diverse languages, using different kinds of word embedding models for a total 15 conditions.
176	22	Comparing our approach to state-of-the-art methods from word polarity and word emotion induction, our model turns out to be superior in each condition, thus setting a novel state-of-the-art performance for both polarity and emotion induction.
177	72	Moreover, our results are even competitive to human annotation reliability in terms of inter-study as well as split-half reliability.
