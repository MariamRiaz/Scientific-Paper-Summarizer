0	29	Hierarchical relationships play a central role in knowledge representation and reasoning.
6	12	However, a well-known problem of Hearst-like patterns is their extreme sparsity: words must co-occur in exactly the right configuration, or else no relation can be detected.
9	10	The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear.
10	53	Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance.
12	25	Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016).
13	20	While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora.
15	11	We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH.
30	22	However, sparsity is one of the main issues when using Hearst patterns, as a necessarily incomplete set of extraction rules will lead inevitably to missing extractions.
52	16	Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation.
65	19	Direction: In direction prediction, the task is to identify which term is broader in a given pair of words.
66	18	For this task, we evaluate all models on three datasets described by Kiela et al. (2015): On BLESS, the task is to predict the direction for all 1337 positive pairs in the dataset.
78	12	For all models, we report Spearman’s rank correlation ρ.
80	19	Pattern-based models: We extract Hearst patterns from the concatenation of Gigaword andWikipedia, and prepare our corpus by tokenizing, lemmatizing, and POS tagging using CoreNLP 3.8.0.
82	16	Our selected patterns match prototypical Hearst patterns, like “animals such as cats,” but also include broader patterns like “New Year is the most important holiday.” Leading and following noun phrases are allowed to match limited modifiers (compound nouns, adjectives, etc.
84	43	Dur- ing postprocessing, we remove pairs which were not extracted by at least two distinct patterns.
85	32	We also remove any pair (y, x) if p(y, x) < p(x, y).
89	51	Distributional models: For the distributional baselines, we employ the large, sparse distributional space of Shwartz et al. (2017), which is computed from UkWaC andWikipedia, and is known to have strong performance on several of the detection tasks.
90	20	The corpus was POS tagged and dependency parsed.
96	28	Particularly strong improvements can be observed on BLESS (0.76 average precision vs 0.19) and WBLESS (0.96 vs. 0.69) for the detection tasks and on all directionality tasks.
97	34	For directionality prediction on BLESS, the SVD models surpass even the state-of-the-art supervised model of Vulić and Mrkšić (2017).
98	29	Moreover, both SVD models perform generally better than their sparse counterparts on all tasks and datasets except on HYPERLEX.
99	35	We performed a posthoc analysis of the validation sets comparing the ppmi and spmi models, and found that the truncated SVD improved recall via its matrix completion properties.
100	23	We also found that the spmi model downweighted many high-scoring outlier pairs composed of rare terms.
101	45	When comparing the p(x, y) and ppmi models to distributional models, we observe mixed results.
103	57	On EVAL, Hearst-pattern based methods get penalized by OOV words, due to the large number of verbs and adjectives in the dataset, which are not captured by our patterns.
104	51	However, in 7 of the 9 datasets, at least one of the sparse models outperforms all distributional measures, showing that Hearst patterns can provide strong performance on large corpora.
106	47	Our results show that the pattern-based methods substantially outperform DIH-based methods on several challenging benchmarks.
107	64	We find that embedding methods alleviate sparsity concerns of pattern-based approaches and substantially improve coverage.
108	62	We conclude that Hearst patterns provide important contexts for the detection of hypernymy relations that are not yet captured in DIH models.
109	42	Our code is available at https://github.com/ facebookresearch/hypernymysuite.
