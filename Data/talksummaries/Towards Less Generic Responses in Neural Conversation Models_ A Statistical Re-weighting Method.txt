0	21	Many recent works have been proposed to use neural networks to generate responses for opendomain dialogue systems (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,c; Serban et al., 2017; Shen et al., 2017; Li et al., 2017; Yu et al., 2017; Xu et al., 2017).
2	20	They aim at maximizing the probability of generating a response given an input query, and generally use the maximum likelihood estimation (MLE) as their objective function.
3	15	However, various problems occur when Seq2Seq models are used for dialogue generation tasks.
4	33	One of the most important problems is that such models are inclined to generate generic and dull responses (e.g., I don’t know), rather than meaningful and specific answers (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,c; Kannan et al., 2016; Li et al., 2017; Xie, 2017; Wei et al., 2017; Mou et al., 2017).
7	14	They further proposed a fast diverse decoding approach (Li et al., 2016b), which modifies the beam search to re-rank meaningful responses into higher positions.
9	19	In the reinforcement learning framework (Li et al., 2016c), the reward function used in the decoding considers the ease of answering, which is measured by a distance towards a set of 8 generic responses.
12	29	All these works tried to add extra optimized terms in the encoding or decoding modules in Seq2Seq, making the training or prediction more complicated.
27	10	Standard Seq2Seq models for NMT and dialogue generation aim at estimating the conditional probability p(y|x) where x = (x1, .
28	29	, xT ) is an input sequence and y = (y1, .
29	25	, yT ′) is its corresponding output sequence whose length T ′ may differ from T .
30	12	During training, we learn all the model parameters θ by summing the negative log likelihood of each sample pair (x,y) in the training corpus C: `(x,y, θ) = − T ′∑ t=1 log p(yt|x,y[t−1];θ), (1) L(C, θ) = ∑ (x,y)∈C `(x,y, θ).
31	29	(2) Recall that generic responses are those that are safe and universal for many queries and thus frequently appear in the training corpus.
32	36	Hence, if we have two responses of x in which one is generic and the other one contains more meaningful content, using L(C, θ) in Eq.
34	14	Therefore, L(C, θ) contains a large amount of patterns from the generic responses, thus it is not surprised to see that the trained models are stuck into local optimum that are inclined to generate these patterns or their combinations.
36	18	Here, we propose a reweighting method for responses of a query x.
38	12	In the implementation, we make the normalization of this loss at the mini-batch level for better computational efficiency.
50	24	A response with a higher frequency will be assigned with a smaller E(y).
53	50	In practice, we have tried to use long responses (longer than average length) to fine-tune the Seq2Seq model.
60	8	6 are effective to weight the responses, Table 1 shows the weights of 8 responses for a query “其实 单身也挺好的 (It’s pretty good to be single)”.
63	14	Conventional metrics such as BLEU (Papineni et al., 2002) and perplexity, are improper to be used for response generation tasks.
65	12	We randomly sample 500 queries (not used in training) as our test samples, and recruit 3 annotators to evaluate each generated response from two aspects: • Fluency: 0 (unreadable), 1 (readable but with some grammar mistakes), 2 (fluent); • Relevance: 0 (not relevant at all), 1 (relevant at a distant level), 2 (relevant, including the generic responses), 3 (relevant as well as interesting).
82	9	Specifically, the average percentage of the generated responses that are assigned to relevance rating 2 (relevant, including the generic responses) and 3 (relevant as well as interesting) are presented in Table 4.
84	17	To validate that our method is effective to reduce the number of generated generic responses, we calculated the distinct-1 and distinct-2 (Li et al., 2016a) for the compared methods respectively, which are the number of distinct unigrams and bigrams divided by total number of generated words respectively.
87	20	We further randomly sample another 100K queries (not used in training) and use the various models to generate responses.
88	24	We compare the frequencies of several common generic responses appearing in the generated results, as shown in Table 6.
91	39	)” and 77% of the case “我也想知道 (I want to know, too)” to be generated.
92	23	In this paper, we propose a statistical re-weighting method to weight multiple responses differently and optimize the MLE objective function.
93	12	The weight of each response is calculated based on two terms according to the similarity frequency and its length.
94	46	Experiments show that our approach improves the performance over the baseline models and reduces the number of generated generic responses significantly.
95	29	It indicates that mismatching issue of objective function can be alleviated through such similar re-weighting methods, by which current encoder-decoder architectures can take full use of the m-to-n training corpus and model the dialogue generation tasks better.
