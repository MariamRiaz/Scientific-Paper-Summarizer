8	39	In this work, we incorporate entities into neural text generation models; each entity in a story is given its own vector representation, which is updated as the story unfolds.
14	50	The choice of entity generated next in the sentence will change what language should follow that mention and will shape and drive the direction of the story.
16	20	Of course, entities are not the only context needed for coherent language generation; previously generated content remains an important source of information.
22	69	We further conduct a human evaluation in which our model’s generated sentences are compared to a strong baseline model.
27	56	The current state of the entities mentioned in the document so far Each of these types of information is encoded in vector form, following extensive past work on recurrent neural network (RNN) language models.
31	35	As noted, our starting point is a sequence-tosequence model (Sutskever et al., 2014); the last hidden state from the previous sentence offers a representation of the preceding context.
35	35	Unlike the definition of attention in Bahdanau et al. (2015), here we use the bilinear product in Equation 2 to encourage correlation between ht,i and ht−1,j for coherence in text generation.
40	30	Recently, Ji et al. (2017) introduced a language model, ENTITYNLM, that adds explicit tracking of entities, which have their own representations that are updated as the document progresses.2 That model was introduced for analysis tasks, such as language modeling and coreference resolution, where the texts (and their coreference information) are given, and the model is used to score the texts to help resolve coreference relationships.3 ENTITYNLM’s strong performance on language modeling suggests the potential of distributed entity representations as another source of contextual information for text generation.
42	40	In general, every entity in a document (e.g., EMILY in Figure 1) is assigned a vector representation; this vector is updated every time the entity is mentioned.
44	21	When we generate text, the model will have access to the current representation of every participant (i.e., every entity) in the story at that time (denoted by ei,t for entity i at timestep t).
49	21	If the model decided the current word should not refer to an entity, then ecurrent is still used and will be the representation of the most recently mentioned entity.
50	19	If the choice is a new, previously unmentioned entity, then ecurrent is initialized with a new embedding randomly generated from a normal distribution: u ∼ N (r, σ2I), (4) where σ = 0.01 and r is a parameter vector that is used to determine whether the next word should refer to an entity.
51	34	Once the word wt has been generated, the entity representation is updated based on the new hidden state information, ht.
53	22	Both provide a representation of context: respectively, the previous sentence’s representation (pt) and the most salient entity’s representation (ecurrent).
57	19	We use a max-pooling function to form a context vector ct with the same dimensionality as ht−1 (and, of course, pt and ecurrent).
82	19	We trained all models on 312 adventure books from the Toronto Book Corpus (Zhu et al., 2015), with development and test sets of an additional 39 books each.
105	38	So if the model was choosing the next entity mention to be generated in Figure 1, it would select between all the previous entity mentions (Emily, the dragon, Seth, and her) and the correct mention (she).
115	19	The first mention generation choice is a trivial one, with a single candidate that is by definition correct.
116	43	As more entity mentions are observed, the number of options will increase.4 To enable aggregation across contexts of all lengths, we report the mean average precision (MAP) of the correct candidates, where the language model scores are used to rank candidates.
129	20	In our version of this task, we provide a model with n − 1 = 49 sentences of preceding context, and offer two choices for the nth (50th) sentence: the actual 50th sentence or a distractor sentence randomly chosen from the next 50 sentences.
133	32	All of the sentences share lexical and entity information with the last line of the context.
151	22	We presented Amazon Mechanical Turkers5 with a short excerpt from a story and two generated sentences, one generated by ENGEN and one generated by the entity-unaware S2SA.
173	21	When asked to explain why they selected the sentence they did, a few Turkers attributed their choices to connections between pronouns in ENGEN’s suggestions to characters mentioned in the story excerpt.
175	67	For example, one Turker said they chose ENGEN’s sentence because the S2SA sentence began with “she,” and there were no female characters in the context.
181	19	Others made decisions based on whether they thought a sentence of dialogue or a descriptive sentence was more appropriate, or a statement versus a question.
183	204	For example, in the second story listed in Table 3, one Turker used social knowledge to choose the S2SA sentence because “the introduction makes the man sound like he is a stranger, so ‘I’m proud of you’ seems out of place.” In this case, even though the sentence from ENGEN correctly generated pronouns that refer to entities in the context, the mismatch in the social aspects of the context and ENGEN’s sentence contributed to 7 out of 11 Turkers choosing the vaguer S2SA sentence.
184	56	While neither S2SA nor ENGEN explicitly encodes these types of information, these qualities are important to human evaluators of generated text and should influence future work on narrative text generation.
207	43	Inspired by Centering Theory and the importance of characters in stories, we propose a neural model for text generation that incorporates context via entities.
209	43	By collecting human evaluations of sentences generated with entity information, we find that while coherently referring back to entities in the context was cited by several Turkers as a factor in their decision, the introduction of new entities and moving the narrative forward were also valued.
210	48	Therefore, while entities are a useful structure to incorporate in story generation, other structures may also prove useful, including other aspects of discourse (e.g., discourse relations or planning) or story-related structures (e.g., narrative structure).
