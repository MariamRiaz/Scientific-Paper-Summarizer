2	17	Linguistically, metaphor is defined as a language expression that uses one or several words to represent another concept, rather than taking their literal meanings of the given words in the context (Lagerwerf and Meijers, 2008).
8	21	These models follow a similar paradigm in which input sentences are first parsed into phrases and then the metaphoricity of the phrases is identified; they do not tackle word-level metaphor.
12	18	E.g., “This young man knows how to climb the social ladder.” (Mohammad et al., 2016) is a metaphorical expression.
14	133	In this paper, we propose an unsupervised metaphor processing model which can identify and interpret linguistic metaphors at the wordlevel.
15	24	Specifically, our model is built upon word embedding methods (Mikolov et al., 2013) and uses WordNet (Fellbaum, 1998) for lexical re- lation acquisition.
74	24	Our second hypothesis (H2) is that the literal senses of words occur more commonly in corpora than their metaphoric senses (Cameron, 2003; Martin, 2006; Steen et al., 2010; Shutova, 2016).
77	32	Step (1) involves training word embeddings based on a Wikipedia dump3 for obtaining input and output vectors of words.
81	17	In Step (3), we identify the best fit word, which is defined as the word that represents the literal sense that the target word is most likely taking given its context.
86	18	The intuition is that the best fit word will represent the literal sense that the target word is most likely taking.
92	20	The candidate word with the highest similarity to the context is then selected as the best fit word.
103	22	If the similarity is higher than a threshold (τ ) the target word is considered as literal, otherwise, metaphorical (based on H1).
105	19	Such a feature is useful for supporting other NLP tasks such as Machine Translation, which we will explore in §6.
112	47	We then construct a candidate word setW by including the synonyms and direct hypernyms of the target word from WordNet, and then augmenting the set with the inflections of the extracted synonyms and hypernyms, as well as the target word devour and its inflections.
118	22	However, we found that using input vectors to measure cosine similarity between two words with different POS types in a phrase is sub- optimal, as words with different POS normally have different semantics.
120	116	Taking Skip-gram for example, empirically, input vectors of words with the same POS, occurring within the same contexts tend to be close in the vector space (Mikolov et al., 2013), as they are frequently updated by back propagating the errors from the same context words.
122	18	Our observation is also in line with Nalisnick et al. (2016), who examine IN-IN, OUT-OUT and IN-OUT vectors to measure similarity between two words.
124	103	For illustrative purpose, we visualize the CBOW and Skip-gram updates between 4- dimensional input and output vectors by Wevi4 (Rong, 2014), using a two-sentence corpus, “Drink apple juice.” and “Drink orange juice.”.
125	21	We feed these two sentences to CBOW and Skipgram with 500 iterations.
135	68	Note that while Shutova et al. and Rei et al. detect metaphors at the phrase level by identifying metaphorical phrases, Melamud et al.’s model can perform metaphor identification and interpretation on sentences.
138	40	There is a verbal target word annotated by 10 annotators in each sentence.
151	19	We use Stanford CoreNLP (Manning et al., 2014) lemmatized Wikipedia to train word embeddings for phrase level evaluation, which is in line with Shutova et al. (2016).
163	87	When comparing to the baselines, our model SIM-SGI+O significantly outperforms the word embedding based approach by Shutova et al. (2016), and gives the same performance as the deep supervised method (Rei et al., 2017) which requires a large amount of labelled data for training and cost in training time.
180	24	1-2 are the original sentence translations, translated by Google Translate (GT) and Bing Translator (BT).
198	26	The interpretation of the identified metaphorical words given by our model also contributes to Google and Bing translation systems with 11% and 9% accuracy improvements.
199	19	The experiments show that using words’ hypernyms and synonyms in WordNet can paraphrase metaphors into their literal counterparts, so that the metaphors can be correctly identified and translated.
200	43	To our knowledge, this is the first study that evaluates a metaphor processing method on Machine Translation.
201	131	We believe that compared with simply identifying metaphors, metaphor processing applied in practical tasks, can be more valuable in the real world.
202	168	Additionally, our experiments demonstrate that using a candidate word output vector instead of its input vector to model the similarity between the candidate word and its context yields better results in the best fit word (the literal counterpart of the metaphor) identification.
203	37	CBOW and Skip-gram do not consider the distance between a context word and a centre word in a sentence, i.e., context word contributes to predict the centre word equally.
204	99	Future work will introduce weighted CBOW and Skip-gram to learn positional information within sentences.
