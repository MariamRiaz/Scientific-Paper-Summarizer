{"sections": [{"heading": "1. Introduction", "text": "We consider high-dimensional statistical models where the ambient dimension p is much larger than the number of observations n. Under such high-dimensional scaling, it is still possible to obtain consistent estimators by imposing low-dimensional structural constraints upon the statistical models, such as sparsity (e.g. in compressed sens-\n1School of Computing, KAIST, Daejeon, South Korea 2AItrics, Seoul, South Korea 3IBM T.J. Watson Research Center, Yorktown Heights, NY, USA. Correspondence to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano <aclozano@us.ibm.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ning (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), lowrank structure (Recht et al., 2007; Negahban & Wainwright, 2010), sparse graphical model structure (Friedman et al., 2007; Ravikumar et al., 2008), and sparse additive structure for non-parametric models (Ravikumar et al., 2009). A widely used approach to structured learning is via specific regularization functions. For instance, `1-regularization is employed for sparse models (Tibshirani, 1996), `1/`q norms for group sparsity (Yuan & Lin, 2006), and nuclear norm for low-rank matrix-structured models (Cande\u0300s & Tao, 2010). Much attention has been devoted to the study of these structured norms and their theoretical properties.\nSuch a \u201cclean\u201d regularization approach, however, might be too stringent in practice. For instance in linear regression, a blend of element-wise sparsity and group-sparsity might be more appropriate than a purely sparse or purely groupsparse solution. In multitask learning, while some parameters might be shared across tasks, others might only be relevant to a subset of tasks or a single task. To overcome this limitation, a line of work on so-called dirty models has emerged, which addresses this caveat by \u201cmixing and matching\u201d different structures. One basic approach consists in decomposing the model parameters as a sum of two components, each penalized separately: one component captures the common structure across tasks and the other task-specific characteristics (Jalali et al., 2010; Gong et al., 2012). For instance the dirty model in Jalali et al. (2010) employs `1,1 and `1,\u221e regularizers to the two components. Chandrasekaran et al. (2011) consider the problem of recovering unknown low-rank and sparse matrices, given the sum of their sum, with application such as optical imaging systems. Robust principal component analysis and related extensions (Cande\u0300s et al., 2011; Agarwal et al., 2012; Hsu et al., 2011) estimate a covariance matrix that is the sum of a low-rank matrix and a structured (e.g. sparse, column sparse) matrix.\nA general framework for studying dirty models was recently proposed in Yang & Ravikumar (2013), which bridges and extends several analyses for specific pairs of superposition structures and specific statistical models (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011); Cande\u0300s et al. (2011); Agarwal et al. (2012); Hsu et al.\n(2011)) Specifically, this framework applies to a general class of M -estimators employing a so-called hybrid regularization function, which is the infimal convolution of weighted regularization functions, one for each structural component. This formulation is equivalent to an M - estimator that combines a loss function applied to the sum of multiple parameter vectors (one per structural component) and a weighted sum of regularization functions (one per parameter vector).\nFor the sparse + group sparse decomposition, however existing analyses are highly problematic. The key weakness is that they require some form of structural incoherence condition which captures the interaction between the different structured components. While such a structural incoherence is a reasonable assumption for e.g. sparse + low rank superposition, it is what too stringent for the sparse+group sparse case because the two structures are completely coherent for this case! This yields a key motivating question for this paper: Under the sparse + group sparse setting, can we bypass structural incoherence conditions and yet obtain tight error bounds?\nIn this paper we provide a positive answer by developing a novel proof technique. Prior analyses require \u2018local\u2019 restricted strong convexity conditions (RSC): one condition for the sparse component and one for the group sparse component. The use of structural incoherence between sparse and group sparse components in then needed to show \u2018global\u2019 RSC for the vector concatenating sparse and group sparse components. To avoid the need for structural incoherence, we use RSC in the summed space directly (namely for the summed sparse + group-sparse structure). However, this brings in a new issue: in this case, the dirty regularizer for the parameter vector is not decomposable. To circumvent this issue, our key ingredient is to introduce \u201csurrogate\u201d sparse and group sparse components depending on our estimators such that i) their sum equals the sum of the true parameter components and ii) corresponding error vectors are decomposable even though the regularizer itself is not decomposable. Using the decomposability of error vectors, we are then able to show `2 consistency for general loss functions.\nAs an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to non-convex regularizers, and show their `\u221e consistency. Interestingly, these models are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models. In particular, our `\u221e consistency results require neither incoherence in the loss function nor structural incoherence between sparse and group sparse parameters. We illustrate the practical impact of this superior theoretical results with simulation experiments.\nThe remainder of this paper is organized as follows. In Section 2 we review sparse+group-sparse dirty models with convex penalties and introduce their non-convex counterparts. In Section 3 we discuss the incoherence assumption required by prior analyses and explain why such an assumption is unreasonable. Section 4 introduces the key ingredient of our novel proof technique. Section 5 presents the convergence bounds for models with convex penalties. Those for non-convex penalties are stated in Section 6. Finally, simulation experiments are provided in Section 7 to illustrate the remarkable practical advantage of non-convex penalties, agreeing with their superior convergence rates."}, {"heading": "2. Sparse + Group-Sparse Dirty Models: Setup and Formulations", "text": "Consider a data collection Z = {Z1, . . . , Zn}, where each element is drawn independently from distribution P, and a loss function L(\u00b7 ;Z) : \u2126 \u2192 R where L(\u03b8 ;Z) measures the goodness of fit of parameter \u03b8 \u2208 \u2126 to the given data collection Z. Typically \u2126 = Rp (parameters are vectors) or Rp\u00d7r (parameters are matrices). Assume there are some known groups G = {G1, . . . , Gq} that partition the parameter index set: Gi \u2229Gj = \u03c6 and \u222aqg=1Gg = {1, . . . , p}.\nWe aim at recovering parameter \u03b8\u2217 which is the unique minimizer of the population risk: \u03b8\u2217 := argmin\u03b8\u2208\u2126 EZ [L(\u03b8;Z)] in cases where\n\u03b8\u2217 = \u03b1\u2217 + \u03b2\u2217, (1)\nwhere \u03b1\u2217 is a sparse component and \u03b2\u2217 is a group-sparse component obeying the group structure G. For that purpose, we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity. We consider both convex and non-convex regularizers as follows."}, {"heading": "2.1. Dirty models with convex regularizers", "text": "We focus on regularized M -estimators of the form\nminimize \u03b1,\u03b2\nL(\u03b1+ \u03b2;Z) + \u03bb1\u2016\u03b1\u20161 + \u03bb2\u2016\u03b2\u20161,a, (2)\nwhere the loss function L(\u00b7 ;Z) is possibly nonconvex. Here, given known parameter groups G = {G1, G2, . . . , Gq}, the group regularizer is defined as \u2016\u03b2\u20161,a := \u2211q t=1 \u2016\u03b2Gt\u2016a for a \u2265 2, where \u03b2Gt denotes the parameter subset in group Gt. The constant a determines how the elements within each group are combined.\nWe provide examples for the popular settings of linear regression and inverse covariance estimation.\nLinear regression. Consider the standard linear model y = X\u03b8\u2217 +w where y \u2208 Rn is the observation vector, \u03b8\u2217\nis the true regression parameter which is the sum of sparse \u03b1\u2217 and group sparse \u03b2\u2217, X \u2208 Rn\u00d7p is the design matrix, and w \u2208 Rn is the observation noise. The \u201cdirty\u201d regularized least squares solves\nminimize \u03b1,\u03b2\u2208Rp\n1\n2n \u2016y \u2212X\n( \u03b1+ \u03b2)\u201622 + \u03bb1\u2016\u03b1\u20161 + \u03bb2\u2016\u03b2\u20161,a\n(3)\nwhere groups are defined within a (single) parameter vector space via \u03b2. The formulation can be seamlessly extended to cover the dirty multitask learning setting of Jalali et al. (2010):\nminimize \u03b1,\u03b2\u2208Rp\u00d7m m\u2211 k=1 1 2n \u2225\u2225y(k) \u2212X(k)([\u03b1+ \u03b2](\u00b7,k))\u2225\u222522 (4) +\u03bb1\u2016\u03b1\u20161 + \u03bb2\u2016\u03b2\u20161,\u221e\nwhere we have m related tasks in columns: \u03b1,\u03b2 \u2208 Rp\u00d7m, and the groups can be defined across tasks in rows. E.g. for predictor j, \u03b2(j,1), . . . ,\u03b2(j,m) belong to the same group. Here, [\u03b1 + \u03b2](\u00b7,k) indicates k-th column of matrix input \u03b1+ \u03b2.\nGraphical Model Estimation. Another key example is a modified graphical Lasso where the goal is to estimate the structure of the underlying graphs representing conditional independences across variables. Assume that there are some known set of edge groups and that the true parameter \u0398\u2217 has only a small number of active edge groups plus some individual edges. To recover \u0398\u2217 we solve\nminimize S+B 0\ntrace ( (S +B) \u03a3\u0302 ) \u2212 log det(S +B) (5)\n+\u03bb1\u2016S\u20161 + \u03bb2\u2016B\u20161,a\nwhere \u03a3\u0302 is the sample covariance matrix and regularizers are applied to off-diagonal entries of S and B. As done in (4) for the linear model, the formulation (5) can be seamlessly extended to the multitask setting where we wish to estimate multiple precision matrices jointly, encouraging similar structure while allowing for some discrepancy across them. This estimator is discussed in Hara & Washio (2013).\nEquivalent Program. As shown in Yang & Ravikumar (2013), the formulation (2) can be rewritten as:\nminimize \u03b8\nL(\u03b8;Z) + \u2016\u03b8\u2016\u03bb (6)\nwhere \u2016\u03b8\u2016\u03bb is the infimal convolution of two regularizers\n\u2016\u03b8\u2016\u03bb := inf \u03b1,\u03b2\n{ \u03bb1\u2016\u03b1\u20161 + \u03bb2\u2016\u03b2\u20161,a : \u03b1+ \u03b2 = \u03b8 } . (7)\nIt is known that \u2016 \u00b7 \u2016\u03bb is a norm and its dual is defined as \u2016\u03b8\u2016\u2217\u03bb := max{\u2016\u03b8\u2016\u221e/\u03bb1, \u2016\u03b8\u2016\u221e,a\u2217/\u03bb2} where 1/a + 1/a\u2217 = 1 so that \u2016 \u00b7 \u2016\u221e,a\u2217 is the dual norm of \u2016 \u00b7 \u20161,a (see Yang & Ravikumar (2013) for details)."}, {"heading": "2.2. Dirty models with non-convex regularizers", "text": "In this paper, we introduce and study estimators of the form\nminimize \u03b1,\u03b2\nL(\u03b1+ \u03b2) + \u03c1\u03bb1 ( \u03b1 ) + \u03c6\u03bb2,a ( \u03b2). (8)\nHere \u03c1\u03bb1(\u00b7) is any regularizer inducing sparsity beyond the `1-norm (note that the notation encapsulates the regularization parameter \u03bb1 itself within the regularizer) satisfying the following conditions (Loh & Wainwright, 2014):\n(C1) \u03c1\u03bb1(0) = 0 and is symmetric. For t > 0, \u03c1\u03bb1(t) is non-decreasing but \u03c1\u03bb1(t)/t is non-increasing in t. Besides, \u03c1\u03bb1(t) is differentiable for t 6= 0 with limt\u21920+ \u03c1 \u2032 \u03bb1 (t) = \u03bb1, and is \u03c1\u03bb1(t) + \u00b5 2 t\n2 is convex for some \u00b5 > 0.\n(C2) There exists some scalar \u03b3 \u2208 (0,\u221e) such that \u03c1\u2032\u03bb1(t) = 0 when t \u2265 \u03b3\u03bb1.\nFollowing the notation of Loh & Wainwright (2014), we call \u03c1\u03bb1(\u00b7) \u00b5-amenable if it satisfies (C1) and (\u00b5, \u03b3)amenable if it additionally satisfies (C2). The popular non-convex regularizers SCAD (Fan & Li, 2001) and MCP (Zhang, 2010) are both (\u00b5, \u03b3)-amenable (Loh & Wainwright, 2014).\nThe regularizer \u03c6\u03bb2,a(\u00b7) a non-convex counterpart of the group regularizer \u03bb2\u2016 \u00b7 \u20161,a employed in (2) where we use \u03c1\u03bb2(\u00b7) instead of \u03bb2\u2016 \u00b7 \u20161, over groups:\n\u03c6\u03bb2,a(\u03b2) := \u03c1\u03bb2(G(\u03b2))\nwhere G(\u03b2) := (\u2016\u03b2G1\u2016a, . . . , \u2016\u03b2Gq\u2016a)>. Example of non-convex regularizers include the Group-SCAD and Group-MCP penalties where SCAD and MCP penalties are respectively used on the norm of each group.\nRemarkably, the proof techniques developed in this paper make it possible to provide not only `2-error bounds under milder conditions than prior work on convex problem (2), but also support set recovery guarantees for non-convex one (8). In fact we shall see that dirty models with nonconvex regularizers (8) enjoy strictly better statistical guarantees than their convex counterpart (2), with practical consequences."}, {"heading": "3. Structural Incoherence: essential in prior work, yet an unreasonable assumption", "text": "As our starting point, we focus on the case of convex dirty models in (2) or equivalently in (6). A key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer (Negahban et al., 2012). However, considering the form of regularizer in (6), it is not obvious to find the model space and its orthogonal complement with which we could directly derive\nerror bounds with optimal rates. To circumvent this problem, Yang & Ravikumar (2013) utilize the decomposability of each component separately, but this requires restricted strong convexity (RSC) to hold jointly for all component parameters. In order to have the \u201cjoint\u201d RSC property from \u201clocal\u201d RSC with respect to each individual component, Yang & Ravikumar (2013) assume a structural incoherence condition. Even if the loss function is strongly convex with respect to each component, such incoherence across components is essential for the joint RSC due to the linearity across components. To see this more clearly, suppose we have the function z2 for z \u2208 R, which is obviously strongly convex. If we assume, however, that z is the sum of two components x, y \u2208 R, then one can immediately see that (x + y)2 is not strongly convex jointly in x and y because x and y are completely coherent in this one dimensional example.\nThe problem is that the structural incoherence condition for the `1+`1,a setting is way too restrictive because the sparse and the group-sparse structures essentially share the same model and its orthogonal spaces1. In order to see this, we consider the popular linear model setting (3) for example. Let s\u2217 (and b\u2217) be the support set of true sparse (groupsparse) component and sc be its complement. Furthermore, [ 1nX\n>X](sc\u2229b\u2217) denotes the projection of the sample covariance onto sc \u2229 b\u2217-coordinate space (j-th coordinate becomes zero if j /\u2208 sc \u2229 b\u2217). Projections on other spaces are defined similarly. Then, the structural incoherence condition for joint RSC can be reduced as: for all (s, b) \u2208 {(sc, b\u2217), (s\u2217, bc), (sc, bc)},\n\u03c3max ( [ 1nX >X](s\u2229b) ) \u2264 C\u03ba1 (9)\nwhere \u03c3max(\u00b7) is the maximum singular value of a matrix, \u03ba1 is the curvature of (restricted) eigenvalue condition, and C is some fixed constant. Informally, this condition requires the maximum singular value of sample covariance (modulo the projection onto the true model and its orthogonal space) to be smaller than its minimum singular value (Note that for linear models, the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance). This condition can be easily shown to fail in many cases. For instance consider the popular setting where the design matrix X is a set of samples from Gaussian ensemble with covariance \u03a3, and the true parameter is the sum of group sparse + a single nonzero component as depicted in Figure 1. Then, the incoherence condition in (9) implies maxi,j |[ 1nX\n>X]ij | \u2264 1/128\u03c3min(\u03a3), which can be easily violated in many natural setting of \u03a3 because the minimum eigenvalue of \u03a3 is smaller than the maximum element of \u03a3\n1Note that the sparse + group sparse setting is outstanding. The structural incohence assumption makes sense in other dirty models settings, e.g. sparse + low rank dirty models.\nby the Rayleigh quotient.\nThis naturally leads to the following question:\nCan we provide tight error bounds for the problem (2) not requiring the joint RSC across individual structures and hence bypassing the incoherence condition?"}, {"heading": "4. Our key strategy: Constructing surrogate components that are always decomposable.", "text": "In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components dependent of our estimation. Consider arbitrary target parameter \u03b8\u2217 such that \u03b8\u2217 = \u03b1\u2217 + \u03b2\u2217. Note that we do not impose additional constraints on defining the sparse component \u03b1\u2217 and the group sparse component \u03b2\u2217, hence the possible combination of (\u03b1\u2217,\u03b2\u2217) is not unique. As we will see later, we provide estimation error bounds that depend on the selection of (\u03b1\u2217,\u03b2\u2217)\u2013more precisely on the sparsity level of \u03b1\u2217 and the group sparsity level of \u03b2\u2217. In that sense our theorems provide sets of estimation bounds. However, it is important to note that we still do not need to worry about the identifiability between structures, because we only care about the `2 and `\u221e error rates of the final or \u201csummed\u201d estimator (we do not recover (nor care about) the individual components).\nSuppose we compute \u03b8\u0303 from the program (6) where \u03b1\u0303 and \u03b2\u0303 are minimizing its dirty regularizer (7). Then, rather than directly deriving error bounds of \u03b8\u0303 \u2212 \u03b8\u2217 from \u03b1\u0303\u2212\u03b1\u2217 and \u03b2\u0303 \u2212 \u03b2\u2217, which are not decomposable, we introduce an additional set of vectors, \u03b1\u0304, \u03b2\u0304 and \u03b8\u0304 from the following rules:\n1. If \u03b1\u2217j = \u03b2 \u2217 j = 0, then \u03b1\u0304j = \u03b2\u0304j := 0.\n2. If \u03b1\u2217j 6= 0 and \u03b2\u2217j = 0, then \u03b2\u0304j := \u03b2\u0303j , and \u03b1\u0304j := \u03b8\u2217j \u2212 \u03b2\u0303j .\n3. If \u03b2\u2217j 6= 0, then \u03b1\u0304j := \u03b1\u0303j and \u03b2\u0304j := \u03b8\u2217j \u2212 \u03b1\u0303j .\n4. \u03b8\u0304 is defined as the sum of \u03b1\u0304 and \u03b2\u0304.\nSparse + Group Sparse Dirty Models\n3 1 1 1 2 1 0 0 1 0 1 0 0 0 0\nTheta^*\n(a) \u03b8\u2217\n0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\na^*\n(b) \u03b1\u2217\n3 1 1 1 2 1 0 0 0 0 0 0 0 0 0\nb^*\n(c) \u03b2\u2217\n1.9 0 0.2 0 1.1 0 0.1 0 1 0 0.9 0.2 0.1 0 0\na_hat\n(d) \u03b1\u0303\n0.8 0.8 0.8 1.1 1.1 1.1 0.2 -0.2 0.2 0 0 0 0 0 0\nb_hat\n(e) \u03b2\u0303\n1.9 0 0.2 0 1.1 0 0.1 0 0 0 -0.1 0.2 0.1 0 0\nDel^*\n(f) \u03b1\u0303\u2212\u03b1\u2217\n-2.2 -0.2 -0.2 0.1 -0.9 0.1 0.2 -0.2 0.2 0 0 0 0 0 0\nGam^*\n(g) \u03b2\u0303 \u2212 \u03b2\u2217\n1.9 0 0.2 0 1.1 0 0 0 0.8 0 1 0 0 0 0\na_bar\n(h) \u03b1\u0304"}, {"heading": "1.1 1 0.8", "text": "1 0.9 1 0 0 0.2 0 0 0 0 0 0\nb_bar\n(i) \u03b2\u0304\n0 0 0 0 0 0 0.1 0 0.2 0 -0.1 0.2 0.1 0 0\nDel^bar\n(j) \u03b1\u0303\u2212 \u03b1\u0304\n-0.3 -0.2 0 0.1 0.2 0.1 0.2 -0.2 0 0 0 0 0 0 0\nGam^bar\n(k) \u03b2\u0303 \u2212 \u03b2\u0304\nFigure 2. Example of constructing surrogate target parameters given (b) \u03b1\u2217, (c) \u03b2\u2217, (d) \u03b1\u0303 and (e) \u03b2\u0303 via transformation T (\u00b7). Error vectors based on surrogates are decomposable (see text for details).\nBy construction, \u03b8\u0304 is same as \u03b8\u2217, but \u03b1\u0304 has different sparsity pattern and values from\u03b1\u2217, depending on \u03b1\u0303. The same holds for \u03b2\u0304 as well. We denote the above transformation as (\u03b1\u0304, \u03b2\u0304) := T (\u03b1\u2217,\u03b2\u2217; \u03b1\u0303, \u03b2\u0303).\nIt turns out that the error vectors computed based on the surrogate \u03b1\u0304 and \u03b2\u0304 are always decomposable as described in the following proposition, and the consequence of this decomposability plays a key role for showing i) `2-error bounds without incoherence condition and ii) support set recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).\nProposition 1. Consider any local optimum \u03b8\u0303 of convex or non-convex dirty models, and corresponding \u03b8\u0304 := T (\u03b1\u2217,\u03b2\u2217; \u03b1\u0303, \u03b2\u0303). Then, the error vectors for individual components, \u2206 := \u03b1\u0303 \u2212 \u03b1\u0304 and \u0393 := \u03b2\u0303 \u2212 \u03b2\u0304, are decomposable in the sense that\n\u2223\u2223[\u2206 + \u0393]j\u2223\u2223 = |\u2206j |+ |\u0393j | for all j, and the overall `2 error \u2016\u03b8\u0303 \u2212 \u03b8\u2217\u20162 is lower bounded as follows:\n\u2016\u03b8\u0303 \u2212 \u03b8\u2217\u201622 \u2265 \u2016\u2206\u201622 + \u2016\u0393\u201622 . (10)\nMoreover, let s\u2217 := supp(\u03b1\u2217) (the support set of\u03b1\u2217), s\u0304 := supp(\u03b1\u2217) \u222a supp(\u03b1\u0304) and U := supp(\u03b1\u2217) \u222a supp(\u03b2\u2217). Similarly, we also define b\u2217 := supp(\u03b2\u2217) and b\u0304 := supp(\u03b2\u0304). Then, by construction of T , s\u2217 \u2286 s\u0304 \u2286 U and b\u2217 \u2286 b\u0304 \u2286 U . However, it is always guaranteed that\n\u2206s\u2217 = \u2206s\u0304 = \u2206U and \u0393b\u2217 = \u0393b\u0304 = \u0393U (11)\nwhere \u2206s\u2217 represents the projection of \u2206 onto the s\u2217coordinate space; that is, [\u2206s\u2217 ]j is \u2206j if j \u2208 s\u2217, and 0 otherwise.\nIllustrative example. Figure 2 describes an example: consider a 5\u00d7 3 matrix parameter with 5 known groups in rows. Suppose (i) the target parameter is given by (a), (ii) we define (b) and (c) as the sparse and group sparse components of \u03b8\u2217, and (iii) the minimizer of program (6) are computed as in (d) and (e), respectively for \u03b1\u0303 and \u03b2\u0303. Then, (f) and (g) show the error vectors for sparse and group-sparse components, which are not decomposable ((10) does not hold for (f) and (g)). On the other hand, for \u03b1\u0304 in (h) and \u03b2\u0304 in (i) derived from T (\u00b7), we can verify that surrogate error vectors (shown in (j) and (k)) are decomposable; at every position, \u03b1\u0303 \u2212 \u03b1\u0304 and \u03b2\u0303 \u2212 \u03b2\u0304 are sign consistent (or at least one of them is zero)."}, {"heading": "5. Statistical Guarantees of Models with Convex Regularizers", "text": "Throughout our analysis, we assume that the loss function L(\u00b7) is twice differentiable and and satisfies the restricted strong convexity condition\n(RSC) For any vector \u03b81,\u03b82 \u2208 Rp, the loss function L(\u00b7) satisfies \u2329\n\u2207L(\u03b81 + \u03b82)\u2212\u2207L(\u03b81),\u03b82 \u232a\n\u2265 { \u03ba1\u2016\u03b82\u201622 \u2212 \u03c41\u2016\u03b82\u20162\u03b7 , for all \u2016\u03b82\u20162 \u2264 1 , (12) \u03ba2\u2016\u03b82\u20162 \u2212 \u03c42\u2016\u03b82\u2016\u03b7 , for all \u2016\u03b82\u20162 \u2265 1 . (13)\nRSC of the loss is also used to guarantee `2-consistency (Negahban et al., 2012; Loh & Wainwright, 2015) or `\u221e- consistency (Loh & Wainwright, 2014) of \u201cclean\u201d structurally constrained problems (i.e. problems with a single\nstructure). Note that there are slight variations in the definition of RSC conditions in the literature. Here we adopt the form with tolerance terms in Loh & Wainwright (2014; 2015), to allow for a wide class of non quadratic and/or non-convex loss functions. We will show that RSC with tolerance in dirty norm holds with high probability under the popular setting of Gaussian ensembles, as an example.\nFor the analysis, we consider a slight modification of the program (6):\nminimize \u2016\u03b8\u2016\u03b7\u2264r\nL(\u03b8) + \u2016\u03b8\u2016\u03bb (14)\nwhere L is possibly non-convex, but satisfies (RSC). The additional constraint \u2016\u03b8\u2016\u03b7 \u2264 r also involves the dirty norm (7) but with a different parameter vector \u03b7. This constraint is a safety radius commonly used for analyzing non-convex problems to ensure that the global minimum exists (see e.g. Loh & Wainwright (2014; 2015)). In practice, we can disregard this additional constraint.\nTheorem 1. Consider the dirty model for problem (14) where L(\u00b7) is possibly non-convex but satisfies the restricted strong convexity (RSC). Suppose that \u03b8\u2217 is feasible and the regularization parameters are set so that \u03bb1 \u2265 4\u2016\u2207L(\u03b8\u2217)\u2016\u221e and \u03bb2 \u2265 4\u2016\u2207L(\u03b8\u2217)\u2016\u221e,a\u2217 . Suppose furthermore that r \u2264 min { \u03ba2 4\u03c42 , \u03ba25 max{\u03bb1/\u03b71,\u03bb2/\u03b72} , \u03bb1 8\u03c41\u03b71 , \u03bb28\u03c41\u03b72 } . Then, any local optimum \u03b8\u0303 of (14) is guaranteed to be `2 consistent:\u2225\u2225\u03b8\u0303 \u2212 \u03b8\u2217\u2225\u2225 2 \u2264 3 \u03ba1 max { \u03bb1 \u221a s , \u03bb2 \u221a sG } (15)\nwhere s is the number of nonzero elements in \u03b1\u2217 and sG is the number of nonzero groups in \u03b2\u2217.\nRemarks. The error bound in (15) scales with (n, p, s, sG) at the same rate as previous analysis (Yang & Ravikumar, 2013) for the sparse plus group sparse setting, which required a much stringent incoherence condition, as we already discussed. It is also instructive to note that Theorem 1 holds for any combination of (\u03b1\u2217,\u03b2\u2217) such that \u03b1\u2217 + \u03b2\u2217 = \u03b8\u2217, but different views of (\u03b1\u2217,\u03b2\u2217) constructing \u03b8\u2217 give different bounds depending on sparsity/group sparsity levels of (\u03b1\u2217,\u03b2\u2217) (i.e. s and sG). In this sense, Theorem 1 provides a set of `2 estimation upper bounds.\nLinear model and modified graphical Lasso. In the following corollaries, we apply Theorem 1 to the linear model (3) and the modified graphical Lasso problem (5) and derive their corresponding `2 estimation bounds.\nCorollary 1. Consider the linear model (3). Assume that (i) each row Xi of the observation matrix X is independently sampled fromN(0,\u03a3), (ii)X is (group) column normalized by scaling as in (Negahban et al., 2012), and (iii)\nw is independent sub-Gaussian with parameter \u03c3. Now suppose that in (14) we set a := 2 (where a is the parameter for the group norm both for \u2016\u03b8\u2016\u03b7 and \u2016\u03b8\u2016\u03bb), r constant (r only depends on \u03a3 and \u03c3), \u03bb1 = \u03b71 := 8\u03c3 \u221a log p/n\nand \u03bb2 = \u03b72 := 8\u03c3( \u221a m/n+ \u221a log q/n) for q groups and maximum group size m (maxg=1,...,q |Gg|). Suppose that \u03b8\u2217 is feasible to program (14) with these settings. Then with probability at least 1\u2212 c1 exp(\u2212c2n\u03bb2s)\u2212 c3/q2, any local optimum \u03b8\u0303 satisfies\n\u2016\u03b8\u0303 \u2212 \u03b8\u2217\u20162 \u2264 24\u03c3\n\u03ba1 max\n{\u221a s log p\nn ,\n\u221a sGm\nn +\n\u221a sG log q\nn\n} .\nwhere \u03ba1 is some constant depending on \u03a3.\nCorollary 2. Consider the modified graphical Lasso (5) to estimate inverse covariance \u0398\u2217. Suppose we set the parameters of (14) as \u03bb1 = \u03b71 := 4 maxi 6=j \u2223\u2223\u03a3\u0302ij \u2212 (\u0398\u2217)\u22121ij \u2223\u2223, \u03bb2 = \u03b72 := 4 maxg\u2208G\n\u2225\u2225\u03a3\u0302g \u2212 (\u0398\u2217)\u22121g \u2225\u2225a\u2217 and r \u2264 1\n5(|||\u0398\u2217|||2+1)2 where ||| \u00b7 |||2 is the spectral norm of the matrix and a \u2265 2. In addition, assume that \u0398\u2217 is feasible for this problem. Then, any local optimum \u0398\u0303 satisfies\n\u2016\u0398\u0303\u2212\u0398\u2217\u2016F \u2264 3(|||\u0398\u2217|||2 + 1)2 max { \u03bb1 \u221a s , \u03bb2 \u221a sG } .\n(16)\nRemark. Since \u2016\u03b8\u2016\u03b7 scales with 1\u221an for the specified values of \u03b7, the constraint \u2016\u03b8\u2016\u03b7 \u2264 r gets milder as n increases. It is also important to note that this constraint is no more stringent than those of non-convex analyses with a single regularizer (Loh & Wainwright, 2015; 2014): their constraints can be written as \u03b71\u2016\u03b8\u20161 \u2264 r (since \u03b71 \u221a\nlog p/n for linear models for example.) in our notation, which directly implies \u2016\u03b8\u2016\u03b7 \u2264 r since \u2016\u03b8\u2016\u03b7 \u2264 \u03b71\u2016\u03b8\u20161 by the definition of \u2016 \u00b7 \u2016\u03b7 ."}, {"heading": "6. Statistical Guarantees of Models with Non-convex Penalties", "text": "A natural extension of (14) is to incorporate non-convex regularizers that have some advantages such as unbiasedness. For that purpose, in this section we consider the following formulation\nminimize \u2016\u03b8\u2016\u03b7\u2264r\nL(\u03b8) +R(\u03b8;\u03bb) (17)\nwhereR(\u03b8;\u03bb) = inf\u03b1,\u03b2{\u03c1\u03bb1 ( \u03b1 ) + \u03c6\u03bb2,a ( \u03b2 )\n: \u03b1+ \u03b2 = \u03b8}. While the `2 analysis in Theorem 1 can be extended to non-convex regularizers following proof techniques recently developed in Loh & Wainwright (2015), using nonconvex unbiased regularizers has no benefit in terms of asymptotic convergence rates of `2 estimation errors. Instead, we here investigate the `\u221e-norm bound and related support set recovery guarantees where non-convex unbiased regularizers help. To derive `\u221e bounds, we use the\nprimal-dual witness method described in the supplementary materials. Theorem 2. Consider the dirty program with nonconvex penalties in (17), under (RSC). Suppose 2r(\u03c42 + 2 max{\u03bb1\u03b71 , \u03bb2 \u03b72 }) \u2264 1. Also suppose that for some\n\u03b4 \u2208 [ max{ 4r\u03c41\u03b71\u03bb1 , 4r\u03c41\u03b72 \u03bb2 }, 1 ] , the strict dual feasibility of primal-dual witness holds. Then, any stationary point \u03b8\u0303 of (17) is supported by U (recall U := supp(\u03b1\u2217) \u222a supp(\u03b2\u2217)) if the number of samples is large enough to satisfy max{\u03bb1 \u221a s, \u03bb2 \u221a sG}2 < c for some constant c depending only on \u03ba1, \u03c41 and \u03b4.\nAs in Theorem 1, the decomposability in (10) and (11) with respect to the surrogates \u03b1\u0304 and \u03b2\u0304, plays a crucial role in establishing the support set recovery guarantee of any local optimum in Theorem 2.\nBased on Theorem 2, we can derive the `\u221e bounds following the standard steps in (Loh & Wainwright, 2014): Corollary 3. Suppose the assumptions in Theorem 2 hold. Then,\n1. If \u03ba1\u2212\u00b52 \u2265 \u03c41 ( max{\u03b71 \u221a s, \u03b72 \u221a sG} )2\nholds for large enough sample size n, the program (17) has a unique stationary point \u03b8\u0302, specified by the construction of the primal dual witness.\n2. Letting Q\u0302 := \u222b 1\n0 \u22072L\n( \u03b8\u2217 + t(\u03b8\u0302 \u2212 \u03b8\u2217) ) dt, it holds\nthat \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e \u2264 \u2225\u2225(Q\u0302UU)\u22121\u2207L(\u03b8\u2217)U\u2225\u2225\u221e +\nmin{\u03bb1, \u03bb2} \u2223\u2223\u2223\u2223\u2223\u2223(Q\u0302UU)\u22121\u2223\u2223\u2223\u2223\u2223\u2223\u221e where ||| \u00b7 |||\u221e denotes a matrix induced norm (maximum absolute row sum).\n3. Moreover, if \u03c1 is (\u00b5, \u03b3)-amenable, and the minimum absolute value \u03b8\u2217min := minj |\u03b8\u2217j | is lowerbounded by \u03b8\u2217min \u2265 \u2225\u2225(Q\u0302UU)\u22121\u2207L(\u03b8\u2217)U\u2225\u2225\u221e + min{\u03bb1, \u03bb2}\n\u2223\u2223\u2223\u2223\u2223\u2223(Q\u0302UU)\u22121\u2223\u2223\u2223\u2223\u2223\u2223\u221e + 2 max{\u03bb1, \u03bb2}\u03b3. Then, the error bound in the statement 2 is reduced to tighter bound as \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u2016\u221e \u2264\u2225\u2225(Q\u0302UU)\u22121\u2207L(\u03b8\u2217)U\u2225\u2225\u221e.\nMulti-task high-dimensional linear regression. We consider the multi-task high-dimensional linear regression, as a concrete example of using non-convex dirty regularizers. This is the counterpart of model (4) which uses convex dirty regularizer. In the following corollary, we analyze the sparsistency of dirty multi-task linear regression with nonconvex regularizers:\nminimize \u0398\u2208Rp\u00d7ms.t.\u2016\u0398\u2016\u03b7\u2264r m\u2211 k=1 1 2n \u2225\u2225y(k) \u2212X(k)\u0398(\u00b7,k)\u2225\u222522 +R(\u0398;\u03bb) (18)\nwhere R(\u0398;\u03bb) = inf\u03b1,\u03b2{\u03c1\u03bb1 ( \u03b1 ) + \u03c6\u03bb2,a ( \u03b2 )\n: \u03b1+ \u03b2 = \u0398}. Now, we derive a corollary for this particular nonconvex dirty model.\nCorollary 4. Consider the multitask regression model. Suppose that for each task, design matrix X(k) is a zeromean Gaussian ensemble and is column normalized, w(k) is independent sub-Gaussian with parameter \u03c3. Now suppose we set parameters of (18) as a :=\u221e, r constant (only depends on \u03a3 and \u03c3.), \u03bb1 := c1\u03c3 \u221a log(pm)/n, \u03bb2 :=\nc2\u03c3 \u221a\n(log p+m log 2)/n, and \u0398\u2217 is feasible to program (14) with these settings. Then, for any local optimum \u0398\u0303, with probability at least 1 \u2212 c1 exp(\u2212c2 log(pm)) \u2212 c3 exp ( \u2212c4(log p+m log 2) ) (which is approaching to 1) for some positive constants c1 \u2212 c4,\n1. supp(\u0398\u0303) \u2286 supp(\u0398\u2217),\n2. if additionally the regularizer \u03c1\u03bb is (\u00b5, \u03b3)-amenable with \u00b5 < \u03bbmin(\u03a3) (the minimum eigenvalue of \u03a3) and Cmin := mink=1,...,m \u03bbmin ( \u03a3 (k) UkUk ) > 0, then\nsupp(\u0398\u0303) = supp(\u0398\u2217) and the element-wise difference is bounded as follows:\n|||\u0398\u0303\u2212\u0398\u2217|||max := max i,j |[\u0398\u0303\u2212\u0398\u2217]i,j | \u2264 \u03c3\n\u221a 100 log(pm)\nnCmin\nprovided that \u03b8\u2217min \u2265 \u03c3 \u221a 100 log(pm) nCmin + min{\u03bb1, \u03bb2} maxk=1,...,m |||(\u03a3(k)UkUk) \u22121|||\u221e + 2 max{\u03bb1, \u03bb2}\u03b3.\nRemark. In order to highlight the benefit of using (\u00b5, \u03b3)-amenable regularizers, we briefly compare the result of Corollary 4 with that of `1 + `1,a case in (Jalali et al., 2010). Not only the result in (Jalali et al., 2010) requires the incoherence on X (specifically, maxj\u2208Uc \u2211m k=1\n\u2225\u2225\u03a3(k)j,Uk(\u03a3(k)j,Uk)\u22121\u2225\u22251 < 1), but it also has an additional s\u03bb1\nCmin \u221a n term in |||\u0398\u0303 \u2212 \u0398\u2217|||max bound. Moreover, the required \u03bb1 and \u03bb2 there can converge\nto zero more slowly: \u03bb1 \u221a\nlog(pm) \u221a n\u2212 \u221a s log(pm)\nand \u03bb2 \u221a m(m+log p)\n\u221a n\u2212 \u221a sm(m+log p) ."}, {"heading": "7. Experiments", "text": "To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties, we perform experiments on both simulated and real-world data and compare convex and non-convex dirty models for sparse + group-sparse structures.\nSimulated data. We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings of parameters (s, sG) \u2208 {(2p/10, p/20), (p/10, p/10)} with respectively less / more support overlap across tasks (recall s and sG are the number of nonzero elements in \u03b1\u2217 and the number of nonzero groups in \u03b2\u2217, respectively). The\nrows of the design matrices X are sampled i.i.d. from a zero-mean Gaussian distribution with correlation of 0.2 between feature pairs. For each set of parameters (s, sG), we generate 100 instances of the problem where for each instance the non-zero entries of the true model parameter matrix are i.i.d. zero-mean Gaussian to agree with s and sG . Gaussian error with standard deviation of 4 is added to each observation. For varying sample size n we measure the `\u221e error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty. We also evaluate the following baselines: Lasso, MCP, SCAD, Group-Lasso, Group-MCP and Group-SCAD 2. The regularization parameters of each method are tuned via 5-folds cross-validation. The results are presented in Figure 3 (To avoid cluttering the graphs, we do not display standard errors as these are much lower than the gaps between the pertinent groups of methods, and we only display the best group of baselines for each setting). As can be seen from the figure, dirty models with non-convex penalties enjoy superior performance over their counterparts with convex penalties as a function of the sample size. In terms of computational cost, (Group) coordinate descent steps for (group) lasso, (group) MCP and (group) SCAD all have simple closed-form expressions (Huang et al., 2012), similarly for proximal-based approaches. We noticed that for a wide range of (\u03bb1, \u03bb2), non-convex procedures took less time and converged faster (See supplements). As future work it would be interesting to study their theoretical numerical convergence rates.\nReal data analysis. We consider the problem of predicting biological activities of molecules given features extracted from their chemical structures. We analyze three biological activity datasets from the \u201cmolec-\n2Our theorem on `\u221e consistency requires the sample size to be larger than the maximum of two terms, which precludes from presenting graphs with curve alignment across p (by rescaling the x-axis with a control parameter as in Jalali et al. (2010)).\nular activity challenge\u201d (http://www.kaggle.com/ c/MerckActivity). Specifically we consider multitask regression with three tasks corresponding to predicting the raw value (\u2212 log(IC50)) of three different types of biological activities : \u2018binding to cannabinoid receptor 1\u2019, \u2018inhibition of dipeptidyl peptidase 4\u2019 and \u2018time dependent 3A4 inhibitions\u2019. For each task we used n = 200 observations with p = 3000 molecular features. We consider 20 random data splits into training and validation sets, using 2/3 of the data for tranining and 1/3 for validation, and report the average R2 over these random splits. As shown in table1, dirty models outperformed \u201cclean\u201d models suggesting the importance to strike a balanc e between task specificity and sharing for this data. Non-convex dirty models achieved the best R2, which illustrate their capability as a valuable tool for high-dimensional data analysis."}, {"heading": "8. Concluding Remarks", "text": "This paper finally resolved the outstanding case of sparse + group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not require implausible assumptions, thereby fully justifying their practical success. In addition we proposed and studied dirty models with non-convex penalties and showed that they enjoy superior theoretical guarantees that translate into significant practical impact. An interesting direction for future work is to investigate whether our proof technique might be applicable to other dirty models and beyond."}, {"heading": "Acknowledgments", "text": "E.Y. acknowledges the support of MSIP/NRF (National Research Foundation of Korea) via NRF2016R1A5A1012966 and MSIP/IITP (Institute for Information & Communications Technology Promotion of Korea) via ICT R&D program 2016-0-00563, 2017-0-00537."}], "year": 2017, "references": [{"title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions", "authors": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"], "venue": "The Annals of Statistics,", "year": 2012}, {"title": "Compressive sensing", "authors": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine,", "year": 2007}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "authors": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "Information Theory, IEEE Transactions on,", "year": 2010}, {"title": "Robust principal component analysis", "authors": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "authors": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optimization,", "year": 2011}, {"title": "Variable selection via non-concave penalized likelihood and its oracle properties", "authors": ["J. Fan", "R. Li"], "venue": "Jour. Amer. Stat. Ass.,", "year": 2001}, {"title": "Sparse inverse covariance estimation with the graphical Lasso", "authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "year": 2007}, {"title": "Multi-stage multi-task feature learning", "authors": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "Advances in Neural Information Processing Systems", "year": 2012}, {"title": "Learning a common substructure of multiple graphical gaussian models", "authors": ["S. Hara", "T. Washio"], "venue": "Neural Networks,", "year": 2013}, {"title": "Robust matrix decomposition with sparse corruptions", "authors": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "Information Theory, IEEE Transactions on,", "year": 2011}, {"title": "A selective review of group selection in high-dimensional models", "authors": ["J. Huang", "P. Breheny", "S. Ma"], "venue": "Statist. Sci., 27(4):481\u2013499,", "year": 2012}, {"title": "A dirty model for multi-task learning", "authors": ["A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan"], "venue": "In Neur. Info. Proc. Sys. (NIPS),", "year": 2010}, {"title": "Support recovery without incoherence: A case for nonconvex regularization", "authors": ["P. Loh", "M.J. Wainwright"], "venue": "Arxiv preprint arXiv:1412.5632,", "year": 2014}, {"title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "authors": ["P. Loh", "M.J. Wainwright"], "venue": "Journal of Machine Learning Research (JMLR),", "year": 2015}, {"title": "Estimation of (near) low-rank matrices with noise and high-dimensional scaling", "authors": ["S. Negahban", "M.J. Wainwright"], "venue": "In Inter. Conf. on Machine learning (ICML),", "year": 2010}, {"title": "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers", "authors": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science,", "year": 2012}, {"title": "Restricted eigenvalue properties for correlated gaussian designs", "authors": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "Journal of Machine Learning Research (JMLR),", "year": 2010}, {"title": "Model selection in gaussian graphical models: Highdimensional consistency of `1-regularized mle", "authors": ["P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu"], "venue": "In Neur. Info. Proc. Sys. (NIPS),", "year": 2008}, {"title": "Sparse additive models", "authors": ["P. Ravikumar", "J. Lafferty", "H. Liu", "L. Wasserman"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) (JRSSB),", "year": 2009}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "authors": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "Allerton Conference", "year": 2007}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1996}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (Lasso)", "authors": ["M.J. Wainwright"], "venue": "IEEE Trans. Information Theory,", "year": 2009}, {"title": "Dirty statistical models", "authors": ["E. Yang", "P. Ravikumar"], "venue": "In Neur. Info. Proc. Sys. (NIPS),", "year": 2013}, {"title": "Graphical models via univariate exponential family distributions", "authors": ["E. Yang", "P. Ravikumar", "G.I. Allen", "Z. Liu"], "venue": "Journal of Machine Learning Research (JMLR),", "year": 2015}, {"title": "Model selection and estimation in regression with grouped variables", "authors": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society B,", "year": 2006}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "authors": ["C.H. Zhang"], "venue": "Annals of Statistics,", "year": 2010}], "id": "SP:d7be502dd2754557fc71c91a4ba03a2c2ac1df14", "authors": [{"name": "Eunho Yang", "affiliations": []}, {"name": "Aur\u00e9lie C. Lozano", "affiliations": []}, {"name": "Aurelie C. Lozano", "affiliations": []}], "abstractText": "Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization that is more realistic for many real-world problems. For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity. Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components. In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions. We also study non-convex counterparts of sparse + groupsparse models. Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.", "title": "Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity"}