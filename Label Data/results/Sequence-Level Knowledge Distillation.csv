0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1914",text,[0],[0]
Deep learning models work best when trained on large amounts of labeled data.,1 Introduction,[0],[0]
"However, acquiring labels is costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples.",1 Introduction,[0],[0]
"A widely successful semi-supervised learning strategy for neural NLP is pre-training word vectors (Mikolov et al., 2013).",1 Introduction,[0],[0]
"More recent work trains a Bi-LSTM sentence encoder to do language modeling and then incorporates its context-sensitive representations into supervised models (Dai and Le, 2015; Peters et al.,
1Code will be made available at https: //github.com/tensorflow/models/tree/ master/research/cvt_text
2018).",1 Introduction,[0],[0]
"Such pre-training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training.
",1 Introduction,[0],[0]
A key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task.,1 Introduction,[0],[0]
Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data.,1 Introduction,[0],[0]
"Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models.",1 Introduction,[0],[0]
"This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models.
",1 Introduction,[0],[0]
"In self-training, the model learns as normal on labeled examples.",1 Introduction,[0],[0]
"On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions.",1 Introduction,[0],[0]
"Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on.",1 Introduction,[0],[0]
"Recent research on computer vision addresses this by adding noise to the student’s input, training the model so it is robust to input perturbations (Sajjadi et al., 2016; Wei et al., 2018).",1 Introduction,[0],[0]
"However, applying noise is difficult for discrete inputs like text.
",1 Introduction,[0],[0]
"As a solution, we take inspiration from multiview learning (Blum and Mitchell, 1998; Xu et al., 2013) and train the model to produce consistent predictions across different views of the input.",1 Introduction,[0],[0]
"Instead of only training the full model as a student, CVT adds auxiliary prediction modules – neural networks that transform vector representations into predictions – to the model and also trains them as students.",1 Introduction,[0],[0]
"The input to each student prediction module is a subset of the model’s intermediate rep-
resentations corresponding to a restricted view of the input example.",1 Introduction,[0],[0]
"For example, one auxiliary prediction module for sequence tagging is attached to only the “forward” LSTM in the model’s first BiLSTM layer, so it makes predictions without seeing any tokens to the right of the current one.
",1 Introduction,[0.9514181885840282],"['This also explains the success of greedy decoding for Seq-KD models—since we are only modeling around the teacher’s mode, the student’s distribution is more peaked and therefore the argmax is much easier to find.']"
CVT works by improving the model’s representation learning.,1 Introduction,[0],[0]
"The auxiliary prediction modules can learn from the full model’s predictions because the full model has a better, unrestricted view of the input.",1 Introduction,[0],[0]
"As the auxiliary modules learn to make accurate predictions despite their restricted views of the input, they improve the quality of the representations they are built on top of.",1 Introduction,[0],[0]
"This in turn improves the full model, which uses the same shared representations.",1 Introduction,[0],[0]
"In short, our method combines the idea of representation learning on unlabeled data with classic self-training.
",1 Introduction,[0],[0]
"CVT can be applied to a variety of tasks and neural architectures, but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi-LSTM encoder.",1 Introduction,[0],[0]
"We propose auxiliary prediction modules that work well for sequence taggers, graph-based dependency parsers, and sequence-to-sequence models.",1 Introduction,[0],[0]
"We evaluate our approach on English dependency parsing, combinatory categorial grammar supertagging, named entity recognition, partof-speech tagging, and text chunking, as well as English to Vietnamese machine translation.",1 Introduction,[0],[0]
CVT improves over previously published results on all these tasks.,1 Introduction,[0],[0]
"Furthermore, CVT can easily and effectively be combined with multi-task learning: we just add additional prediction modules for the different tasks on top of the shared Bi-LSTM encoder.",1 Introduction,[0],[0]
Training a unified model to jointly perform all of the tasks except machine translation improves results (outperforming a multi-task ELMo model) while decreasing the total training time.,1 Introduction,[0],[0]
We first present Cross-View Training and describe how it can be combined effectively with multi-task learning.,2 Cross-View Training,[0],[0]
See Figure 1 for an overview of the training method.,2 Cross-View Training,[0],[0]
"Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} represent a labeled dataset and Dul = {x1, x2, ..., xM} represent an unlabeled dataset We use pθ(y|xi) to denote the output distribution over classes pro-
duced by the model with parameters θ on input xi.",2.1 Method,[0],[0]
"During CVT, the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples.",2.1 Method,[0],[0]
"For labeled examples, CVT uses standard cross-entropy loss:
Lsup(θ) = 1 |Dl| ∑
xi,yi∈Dl
CE(yi, pθ(y|xi))
",2.1 Method,[0],[0]
"CVT adds k auxiliary prediction modules to the model, which are used when learning on unlabeled examples.",2.1 Method,[0],[0]
"A prediction module is usually a small neural network (e.g., a hidden layer followed by a softmax layer).",2.1 Method,[0],[0]
"Each one takes as input an intermediate representation hj(xi) produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model).",2.1 Method,[0],[0]
It outputs a distribution over labels pjθ(y|xi).,2.1 Method,[0],[0]
"Each h
j is chosen such that it only uses a part of the input xi; the particular choice
can depend on the task and model architecture.",2.1 Method,[0],[0]
We propose variants for several tasks in Section 3.,2.1 Method,[0],[0]
"The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces pθ.
",2.1 Method,[0],[0]
"On an unlabeled example, the model first produces soft targets pθ(y|xi) by performing inference.",2.1 Method,[0],[0]
"CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing LCVT(θ) = 1|Dul| ∑ xi∈Dul ∑k j=1D(pθ(y|xi), p j θ(y|xi))
where D is a distance function between probability distributions (we use KL divergence).",2.1 Method,[0],[0]
"We hold the primary module’s prediction pθ(y|xi) fixed during training (i.e., we do not back-propagate through it) so the auxiliary modules learn to imitate the primary one, but not vice versa.",2.1 Method,[0],[0]
CVT works by enhancing the model’s representation learning.,2.1 Method,[0],[0]
"As the auxiliary modules train, the representations they take as input improve so they are useful for making predictions even when some of the model’s inputs are not available.",2.1 Method,[0],[0]
"This in turn improves the primary prediction module, which is built on top of the same shared representations.
",2.1 Method,[0],[0]
"We combine the supervised and CVT losses into the total loss, L = Lsup + LCVT, and minimize it with stochastic gradient descent.",2.1 Method,[0],[0]
"In particular, we alternate minimizing Lsup over a minibatch of labeled examples and minimizing LCVT over a minibatch of unlabeled examples.
",2.1 Method,[0],[0]
"For most neural networks, adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations (such as an RNN or CNN).",2.1 Method,[0],[0]
Therefore our method contributes little overhead to training time over other self-training approaches for most tasks.,2.1 Method,[0],[0]
CVT does not change inference time or the number of parameters in the fullytrained model because the auxiliary prediction modules are only used during training.,2.1 Method,[0],[0]
CVT can easily be combined with multi-task learning by adding additional prediction modules for the other tasks on top of the shared Bi-LSTM encoder.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"During supervised learning, we randomly select a task and then update Lsup using a minibatch of labeled data for that task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"When learning on the unlabeled data, we optimize LCVT
jointly across all tasks at once, first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"As before, the model alternates training on minibatches of labeled and unlabeled examples.
",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Examples labeled across many tasks are useful for multi-task systems to learn from, but most datasets are only labeled with one task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
A benefit of multi-task CVT is that the model creates (artificial) all-tasks-labeled examples from unlabeled data.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
This significantly improves the model’s data efficiency and training time.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Since running prediction modules is computationally cheap, computing LCVT is not much slower for many tasks than it is for a single one.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"However, we find the all-tasks-labeled examples substantially speed up model convergence.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
CVT relies on auxiliary prediction modules that have restricted views of the input.,3 Cross-View Training Models,[0],[0]
"In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning.",3 Cross-View Training Models,[0],[0]
"All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
It takes as input a sequence of words xi =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[x1i , x 2 i , ..., x T i ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v =",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[v1, v2, ..., vT ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking vt as input at each step t) and the backward direction (taking vT−t+1 at each step) to produce vector sequences [ −→ h 11, −→ h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"−→ h T1 ] and [ ←− h 11, ←− h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
←− h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
The output of the Bi-LSTM is the concatenation of these vectors: h1 =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[ −→ h 11 ⊕←−
h 11, ..., −→ h T1 ⊕ ←−",3.1 Bi-LSTM Sentence Encoder,[0],[0]
h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The second Bi-LSTM layer
works the same, producing outputs h2, except it takes h1 as input instead of v.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"In sequence tagging, each token xti has a corresponding label yti .",3.2 CVT for Sequence Tagging,[0],[0]
"The primary prediction module for sequence tagging produces a probability distribution over classes for the tth label using a onehidden-layer neural network applied to the corresponding encoder outputs:
p(yt|xi) = NN(ht1 ⊕ ht2) = softmax(U · ReLU(W (ht1 ⊕ ht2))",3.2 CVT for Sequence Tagging,[0],[0]
"+ b)
",3.2 CVT for Sequence Tagging,[0],[0]
"The auxiliary prediction modules take −→ h 1(xi) and ←− h 1(xi), the outputs of the forward and backward LSTMs in the first2 Bi-LSTM layer, as inputs.",3.2 CVT for Sequence Tagging,[0],[0]
"We add the following four auxiliary prediction modules to the model (see Figure 2):
pfwdθ (y t|xi)",3.2 CVT for Sequence Tagging,[0],[0]
= NNfwd( −→ h t1(xi)),3.2 CVT for Sequence Tagging,[0],[0]
pbwdθ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
"NNbwd( ←− h t1(xi))
",3.2 CVT for Sequence Tagging,[0],[0]
pfutureθ,3.2 CVT for Sequence Tagging,[0],[0]
"(y t|xi) = NNfuture( −→ h t−11 (xi))
",3.2 CVT for Sequence Tagging,[0],[0]
p past θ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
NNpast( ←− h,3.2 CVT for Sequence Tagging,[0],[0]
"t+11 (xi))
The “forward” module makes each prediction without seeing the right context of the current token.",3.2 CVT for Sequence Tagging,[0],[0]
The “future” module makes each prediction without the right context or the current token itself.,3.2 CVT for Sequence Tagging,[0],[0]
"Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next.",3.2 CVT for Sequence Tagging,[0],[0]
The “backward” and “past” modules are analogous.,3.2 CVT for Sequence Tagging,[0],[0]
"In a dependency parse, words in a sentence are treated as nodes in a graph.",3.3 CVT for Dependency Parsing,[0],[0]
"Typed directed edges connect the words, forming a tree structure describing the syntactic structure of the sentence.",3.3 CVT for Dependency Parsing,[0],[0]
"In particular, each word xti in a sentence",3.3 CVT for Dependency Parsing,[0],[0]
"xi = x 1 i , ..., x T i receives exactly one in-going edge (u, t, r) going from word xui (called the “head”) to it (the “dependent”) of type r (the “relation”).",3.3 CVT for Dependency Parsing,[0],[0]
We use a graph-based dependency parser similar to the one from Dozat and Manning (2017).,3.3 CVT for Dependency Parsing,[0],[0]
This treats dependency parsing as a classification task where the goal is to predict which in-going edge yti =,3.3 CVT for Dependency Parsing,[0],[0]
"(u, t, r) connects to each word x t i.
First, the representations produced by the encoder for the candidate head and dependent are
2Modules taking inputs from the second Bi-LSTM layer would not have restricted views because information about the whole sentence gets propagated through the first layer.
",3.3 CVT for Dependency Parsing,[0],[0]
"LSTM LSTM ŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast  Backward LSTM Forward LSTM Predict LSTM LSTM LSTM LSTM Auxiliary Prediction Modules Primary Prediction Module
passed through separate hidden layers.",3.3 CVT for Dependency Parsing,[0],[0]
A bilinear classifier applied to these representations produces a score for each candidate edge.,3.3 CVT for Dependency Parsing,[0],[0]
"Lastly, these scores are passed through a softmax layer to produce probabilities.",3.3 CVT for Dependency Parsing,[0],[0]
"Mathematically, the probability of an edge is given as:
pθ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es(h u 1 (xi)⊕hu2 (xi),ht1(xi)⊕ht2(xi),r)
where s is the scoring function:
s(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr",3.3 CVT for Dependency Parsing,[0],[0]
"+W )
",3.3 CVT for Dependency Parsing,[0],[0]
"ReLU(Wdepz2 + bdep)
",3.3 CVT for Dependency Parsing,[0],[0]
The bilinear classifier uses a weight matrix Wr specific to the candidate relation as well as a weight matrix W shared across all relations.,3.3 CVT for Dependency Parsing,[0],[0]
"Note that unlike in most prior work, our dependency parser only takes words as inputs, not words and part-of-speech tags.
",3.3 CVT for Dependency Parsing,[0],[0]
"We add four auxiliary prediction modules to our model for cross-view training:
pfwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-fwd(
−→ h u1 (xi), −→ h t1(xi),r)
pfwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-bwd(
−→ h u1 (xi), ←− h t1(xi),r)
pbwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-fwd(
←− h u1 (xi), −→ h t1(xi),r)
pbwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-bwd(
←− h u1 (xi), ←− h t1(xi),r)
",3.3 CVT for Dependency Parsing,[0],[0]
Each one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent.,3.3 CVT for Dependency Parsing,[0],[0]
"We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Each example consists of an input (source) sequence xi = x1i , ..., x T i and output (target) sequence yi = y1i , ..., y K i .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as αj ∝ eh
jWαh̄t where h̄t is the decoder’s current hidden state.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The source hidden states weighted by the attention distribution form a context vector: ct = ∑ j αjh
j .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Next, the context vector and current hidden state are combined into an attention vector at = tanh(Wa[ct, ht]).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Lastly, a softmax layer predicts the next token in the output sequence: p(yti |y<ti , xi) = softmax(Wsat).
",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
We add two auxiliary decoders when applying CVT.,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The auxiliary decoders share embedding and LSTM parameters with the primary decoder, but have different parameters for the attention mechanisms and softmax layers.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"For the first one, we restrict its view of the input by applying attention dropout, randomly zeroing out a fraction of its attention weights.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The second one is trained to predict the next word in the target sequence rather than the current one: pfutureθ (y t i |y<ti , xi) = softmax(W futures a future t−1 ).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016).,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"We compare Cross-View Training against several strong baselines on seven tasks:
Combinatory Categorial Grammar (CCG)",4 Experiments,[0],[0]
"Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007).
",4 Experiments,[0],[0]
"Text Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000).
",4 Experiments,[0],[0]
"Named Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003).
",4 Experiments,[0],[0]
"Fine-Grained NER (FGN): We use the OntoNotes (Hovy et al., 2006) dataset.
",4 Experiments,[0],[0]
Part-of-Speech (POS),4 Experiments,[0],[0]
"Tagging: We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).
",4 Experiments,[0],[0]
"Dependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0.
Machine Translation: We use the EnglishVietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015).",4 Experiments,[0],[0]
"We report (tokenized) BLEU scores on the tst2013 test set.
",4 Experiments,[0],[0]
"We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning.",4 Experiments,[0],[0]
"We apply dropout during training, but not when running the primary prediction module to produce soft targets on unlabeled examples.",4.1 Model Details and Baselines,[0],[0]
"In addition to the auxiliary prediction modules listed in Section 3, we find it slightly improves results to add another one that sees the whole input rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations).",4.1 Model Details and Baselines,[0],[0]
"Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers.",4.1 Model Details and Baselines,[0],[0]
See the supplementary material for full training details and hyperparameters.,4.1 Model Details and Baselines,[0],[0]
"We compare CVT with the following other semi-supervised learning algorithms:
Word Dropout.",4.1 Model Details and Baselines,[0],[0]
"In this method, we only train the primary prediction module.",4.1 Model Details and Baselines,[0],[0]
"When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token.",4.1 Model Details and Baselines,[0],[0]
This is similar to CVT in that it exposes the model to a restricted view of the input.,4.1 Model Details and Baselines,[0],[0]
"However, it is less data efficient.",4.1 Model Details and Baselines,[0],[0]
"By carefully designing the auxiliary prediction modules, it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once, rather than just one view at a time.
",4.1 Model Details and Baselines,[0],[0]
Virtual Adversarial Training (VAT).,4.1 Model Details and Baselines,[0],[0]
"VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words.",4.1 Model Details and Baselines,[0],[0]
"Notably, the noise is chosen adversarially so it most changes the model’s prediction.",4.1 Model Details and Baselines,[0],[0]
"This method was applied successfully to semi-supervised text classification
by Miyato et al. (2017).
ELMo.",4.1 Model Details and Baselines,[0],[0]
ELMo incorporates the representations from a large separately-trained language model into a task-specific model.,4.1 Model Details and Baselines,[0],[0]
Our implementaiton follows Peters et al. (2018).,4.1 Model Details and Baselines,[0],[0]
"When combining ELMo with multi-task learning, we allow each task to learn its own weights for the ELMo embeddings going into each prediction module.",4.1 Model Details and Baselines,[0],[0]
We found applying dropout to the ELMo embeddings was crucial for achieving good performance.,4.1 Model Details and Baselines,[0],[0]
Results are shown in Table 1.,4.2 Results,[0],[0]
CVT on its own outperforms or is comparable to the best previously published results on all tasks.,4.2 Results,[0],[0]
"Figure 3 shows an example win for CVT over supervised learning.
",4.2 Results,[0],[0]
"Of the prior results listed in Table 1, only TagLM and ELMo are semi-supervised.",4.2 Results,[0],[0]
These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier.,4.2 Results,[0],[0]
"Our base models use 1024 hidden units in their LSTMs (compared to 4096 in ELMo), require fewer training steps (around one pass over the billion-word benchmark rather than
many passes), and do not require a pipelined training procedure.",4.2 Results,[0],[0]
"Therefore, although they perform on par with ELMo, they are faster and simpler to train.",4.2 Results,[0],[0]
Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi-task ones.,4.2 Results,[0],[0]
"We suspect there could be further gains from combining our method with language model pre-training, which we leave for future work.
",4.2 Results,[0],[0]
CVT + Multi-Task.,4.2 Results,[0],[0]
We train a single sharedencoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones).,4.2 Results,[0],[0]
"Multi-task learning improves results on all of the tasks except fine-grained NER, sometimes by large margins.",4.2 Results,[0],[0]
Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms.,4.2 Results,[0],[0]
"Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data.
",4.2 Results,[0],[0]
"Interestingly, multi-task learning works better in conjunction with CVT than with ELMo.",4.2 Results,[0],[0]
"We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks.",4.2 Results,[0],[0]
"We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other ones, a well-known problem in many-task learning (Kirkpatrick et al., 2017).",4.2 Results,[0],[0]
"During multi-task CVT, the model makes predictions about unlabeled examples across all tasks, creating (artificial) all-tasks-labeled examples, so the model does not only see one task at a time.",4.2 Results,[0],[0]
"In fact, multi-task learning plus self training is similar to the Learning without Forgetting algorithm (Li and Hoiem, 2016), which trains the model to keep its predictions on an old task unchanged when learning a new task.",4.2 Results,[0],[0]
"To test the value of all-tasks-labeled examples, we trained a multi-task CVT model that only computes LCVT on one task at a time (chosen randomly for each unlabeled minibatch) instead of for all tasks in parallel.",4.2 Results,[0],[0]
"The one-at-a-time model performs substantially worse (see Table 2).
",4.2 Results,[0],[0]
Model Generalization.,4.2 Results,[0],[0]
"In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our different methods as they learn (see Figure 4).",4.2 Results,[0],[0]
"Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning.",4.2 Results,[0],[0]
"Interestingly, CVT continues to improve
in dev set accuracy while close to 100% train accuracy for CCG, Chunking, and NER, perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set.",4.2 Results,[0],[0]
We also show results for a smaller multi-task + CVT model.,4.2 Results,[0],[0]
"Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier.",4.2 Results,[0],[0]
"This suggests it is important to use sufficiently large neural networks for multitask learning: otherwise the model does not have the capacity to fit to all the training data.
",4.2 Results,[0],[0]
Auxiliary Prediction Module Ablation.,4.2 Results,[0],[0]
We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3.,4.2 Results,[0],[0]
"We find that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps because they see a more restricted and challenging view of the input.
",4.2 Results,[0],[0]
Training Models on Small Datasets.,4.2 Results,[0],[0]
"We explore how CVT scales with dataset size by varying the amount of training data the model has ac-
cess to.",4.2 Results,[0],[0]
"Unsurprisingly, the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases (see Figure 5, left).",4.2 Results,[0],[0]
"Using only 25% of the labeled data, our approach already performs as well or better than a fully supervised model using 100% of the training data, demonstrating that CVT is particularly useful on small datasets.
",4.2 Results,[0],[0]
Training Larger Models.,4.2 Results,[0],[0]
"Most sequence taggers and dependency parsers in prior work use small LSTMs (hidden state sizes of around 300) because larger models yield little to no gains in performance (Reimers and Gurevych, 2017).",4.2 Results,[0],[0]
We found our own supervised approaches also do not benefit greatly from increasing the model size.,4.2 Results,[0],[0]
"In contrast, when using CVT accuracy scales better with model size (see Figure 5, right).",4.2 Results,[0],[0]
"This finding suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for NLP tasks with limited amounts of labeled data.
",4.2 Results,[0],[0]
Generalizable Representations.,4.2 Results,[0],[0]
"Lastly, we explore training the CVT+multi-task model on five tasks, freezing the encoder, and then only training a prediction module on the sixth task.",4.2 Results,[0],[0]
This tests whether the encoder’s representations generalize to a new task not seen during its training.,4.2 Results,[0],[0]
Only training the prediction module is very fast because (1) the encoder (which is by far the slowest part of the model) has to be run over each example only once and (2) we do not back-propagate into the encoder.,4.2 Results,[0],[0]
"Results are shown in Table 4.
",4.2 Results,[0],[0]
"Training only a prediction module on top of multi-task representations works remarkably well,
outperforming ELMo embeddings and sometimes even a vanilla supervised model, showing the multi-task model is building up effective representations for language.",4.2 Results,[0],[0]
"In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning.",4.2 Results,[0],[0]
Unsupervised Representation Learning.,5 Related Work,[0],[0]
"Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP.",5 Related Work,[0],[0]
"Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018).",5 Related Work,[0],[0]
"Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017).",5 Related Work,[0],[0]
"Other approaches train
“thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning.
",5 Related Work,[0],[0]
Self-Training.,5 Related Work,[0],[0]
"One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006).",5 Related Work,[0],[0]
"In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set.",5 Related Work,[0],[0]
"Then, acting as a “student,” it is retrained on the new training set.",5 Related Work,[0],[0]
"Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015).",5 Related Work,[0],[0]
"It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018).
",5 Related Work,[0],[0]
Consistency Regularization.,5 Related Work,[0],[0]
"Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally flipping an image) to the student’s inputs.",5 Related Work,[0],[0]
"This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model.",5 Related Work,[0],[0]
"Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017).",5 Related Work,[0],[0]
"However, stochastic input alterations are more difficult to apply to discrete data like text, making consistency regularization less used for natural language processing.",5 Related Work,[0],[0]
"One solution is to add noise to the model’s word embeddings (Miyato et al., 2017); we compare against this approach in our experiments.",5 Related Work,[0],[0]
"CVT is easily applicable to text because it does not require changing the student’s inputs.
",5 Related Work,[0],[0]
Multi-View Learning.,5 Related Work,[0],[0]
"Multi-view learning on data where features can be separated into distinct subsets has been well studied (Xu et al., 2013).",5 Related Work,[0],[0]
"Particularly relevant are co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Belkin, 2005), which trains two models with disjoint views of the input.",5 Related Work,[0],[0]
"On unlabeled data, each one acts as a “teacher” for the other model.",5 Related Work,[0],[0]
"In contrast to these methods, our approach trains a single unified model where auxiliary prediction modules see different, but not necessarily indepen-
dent views of the input.
",5 Related Work,[0],[0]
Self Supervision.,5 Related Work,[0],[0]
Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels.,5 Related Work,[0],[0]
"Recent work has jointly trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017).",5 Related Work,[0],[0]
"Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input.
",5 Related Work,[0],[0]
Multi-Task Learning.,5 Related Work,[0],[0]
"There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017).",5 Related Work,[0],[0]
"For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017).",5 Related Work,[0],[0]
Manytask systems are less commonly developed.,5 Related Work,[0],[0]
"Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks.",5 Related Work,[0],[0]
"We propose Cross-View Training, a new method for semi-supervised learning.",6 Conclusion,[0],[0]
"Our approach allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available.",6 Conclusion,[0],[0]
"We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning.",6 Conclusion,[0],[0]
"We thank Abi See, Christopher Clark, He He, Peng Qi, Reid Pryzant, Yuaho Zhang, and the anonymous reviewers for their thoughtful comments and suggestions.",Acknowledgements,[0],[0]
We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER.,Acknowledgements,[0],[0]
Kevin is supported by a Google PhD Fellowship.,Acknowledgements,[0],[0]
"Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text.",abstractText,[0],[0]
"However, the supervised models only learn from taskspecific labeled data during the main training phase.",abstractText,[0],[0]
"We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data.",abstractText,[0],[0]
"On labeled examples, standard supervised learning is used.",abstractText,[0],[0]
"On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input.",abstractText,[0],[0]
"Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model.",abstractText,[0],[0]
"Moreover, we show that CVT is particularly effective when combined with multitask learning.",abstractText,[0],[0]
"We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.1",abstractText,[0],[0]
Semi-Supervised Sequence Modeling with Cross-View Training,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2009–2019 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2009",text,[0],[0]
"User geolocation, the task of identifying the “home” location of a user, is an integral component of many applications ranging from public health monitoring (Paul and Dredze, 2011; Chon et al., 2015; Yepes et al., 2015) and regional studies of sentiment, to real-time emergency awareness systems (De Longueville et al., 2009; Sakaki et al., 2010), which use social media as an implicit information resource about people.
",1 Introduction,[0],[0]
"Social media services such as Twitter rely on IP addresses, WiFi footprints, and GPS data to geolocate users.",1 Introduction,[0],[0]
"Third-party service providers don’t have easy access to such information, and have to rely on public sources of geolocation information such as the profile location field, which is noisy and difficult to map to a location (Hecht et al., 2011), or geotagged tweets, which are publicly available for only 1% of tweets (Cheng et al., 2010; Morstatter et al., 2013).",1 Introduction,[0],[0]
"The scarcity of publicly available
location information motivates predictive user geolocation from information such as tweet text and social interaction data.
",1 Introduction,[0],[0]
"Most previous work on user geolocation takes the form of either supervised text-based approaches (Wing and Baldridge, 2011; Han et al., 2012) relying on the geographical variation of language use, or graph-based semi-supervised label propagation relying on location homophily in user–user interactions (Davis Jr et al., 2011; Jurgens, 2013).
",1 Introduction,[0],[0]
Both text and network views are critical in geolocating users.,1 Introduction,[0],[0]
"Some users post a lot of local content, but their social network is lacking or is not representative of their location; for them, text is the dominant view for geolocation.",1 Introduction,[0],[0]
"Other users have many local social interactions, and mostly use social media to read other people’s comments, and for interacting with friends.",1 Introduction,[0],[0]
Single-view learning would fail to accurately geolocate these users if the more information-rich view is not present.,1 Introduction,[0],[0]
"There has been some work that uses both the text and network views, but it either completely ignores unlabelled data (Li et al., 2012a; Miura et al., 2017), or just uses unlabelled data in the network view (Rahimi et al., 2015b; Do et al., 2017).",1 Introduction,[0],[0]
"Given that the 1% of geotagged tweets is often used for supervision, it is crucial for geolocation models to be able to leverage unlabelled data, and to perform well under a minimal supervision scenario.
",1 Introduction,[0],[0]
"In this paper, we propose GCN, an end-to-end user geolocation model based on Graph Convolutional Networks (Kipf and Welling, 2017) that jointly learns from text and network information to classify a user timeline into a location.",1 Introduction,[0],[0]
"Our contributions are: (1) we evaluate our model under a minimal supervision scenario which is close to real world applications and show that GCN outperforms two strong baselines; (2) given sufficient supervision, we show that GCN is competitive, although the much simpler MLP-TXT+NET outper-
forms state-of-the-art models; and (3) we show that highway gates play a significant role in controlling the amount of useful neighbourhood smoothing in GCN.1",1 Introduction,[0],[0]
"We propose a transductive multiview geolocation model, GCN, using Graph Convolutional Networks (“GCN”: Kipf and Welling (2017)).",2 Model,[0],[0]
"We also introduce two multiview baselines: MLP-TXT+NET based on concatenation of text and network, and DCCA based on Deep Canonical Correlation Analysis (Andrew et al., 2013).",2 Model,[0],[0]
"Let X ∈ R|U |×|V | be the text view, consisting of the bag of words for each user in U using vocabulary V , and A ∈ 1|U",2.1 Multivew Geolocation,[0],[0]
"|×|U | be the network view, encoding user–user interactions.",2.1 Multivew Geolocation,[0],[0]
"We partition U = US ∪ UH into a supervised and heldout (unlabelled) set, US and UH , respectively.",2.1 Multivew Geolocation,[0],[0]
"The goal is to infer the location of unlabelled samples YU , given the location of labelled samples YS , where each location is encoded as a one-hot classification label, yi ∈ 1c with c being the number of target regions.
2.2 GCN GCN defines a neural network model f(X,A) with each layer:
Â = D̃− 1 2 (A+ λI)D̃− 1 2 H(l+1) = σ",2.1 Multivew Geolocation,[0],[0]
"( ÂH(l)W (l) + b ) ,
(1)
where D̃ is the degree matrix of A + λI; hyperparameter λ controls the weight of a node against its neighbourhood, which is set to 1 in the original model (Kipf and Welling, 2017); H0 = X and the din × dout matrix W (l) and dout ×",2.1 Multivew Geolocation,[0],[0]
1 matrix b are trainable layer parameters; and σ is an arbitrary nonlinearity.,2.1 Multivew Geolocation,[0],[0]
"The first layer takes an average of each sample and its immediate neighbours (labelled and unlabelled) using weights in Â, and performs a linear transformation using W and b followed by a nonlinear activation function (σ).",2.1 Multivew Geolocation,[0],[0]
"In other words, for user ui, the output of layer l is computed by:
~hl+1i = σ",2.1 Multivew Geolocation,[0],[0]
"(∑ j∈nhood(i) Âij~h l jW l + bl ) , (2)
1Code and data available at https://github.com/ afshinrahimi/geographconv
Highway GCN:
Highway GCN: ,
Output GCN:
X = BoWtext
Â
Â
Â tanh
tanh
softmax
H0
H1
Hl−1
Hl
predict location: ŷ
W l−1, bl−1, W l−1h , b l−1 h
W 1, b1, W 1h , b 1 h
W l, bl
Figure 1: The architecture of GCN geolocation model with layer-wise highway gates (W ih, b i h).",2.1 Multivew Geolocation,[0],[0]
"GCN is applied to a BoW model of user content over the @-mention graph to predict user location.
where W l and bl are learnable layer parameters, and nhood(i) indicates the neighbours of user ui.",2.1 Multivew Geolocation,[0],[0]
Each extra layer in GCN extends the neighbourhood over which a sample is smoothed.,2.1 Multivew Geolocation,[0],[0]
"For example a GCN with 3 layers smooths each sample with its neighbours up to 3 hops away, which is beneficial if location homophily extends to a neighbourhood of this size.",2.1 Multivew Geolocation,[0],[0]
"Expanding the neighbourhood for label propagation by adding multiple GCN layers can improve geolocation by accessing information from friends that are multiple hops away, but it might also lead to propagation of noisy information to users from an exponentially increasing number of expanded neighbourhood members.",2.2.1 Highway GCN,[0],[0]
"To control the required balance of how much neighbourhood information should be passed to a node, we use layer-wise gates similar to highway networks.",2.2.1 Highway GCN,[0],[0]
"In highway networks (Srivastava et al., 2015), the output of a layer is summed with its input with gating weights T (~hl):
T (~hl) = σ",2.2.1 Highway GCN,[0],[0]
( W lt ~hl + blt ) ~hl+1,2.2.1 Highway GCN,[0],[0]
= ~hl+1 ◦,2.2.1 Highway GCN,[0],[0]
T (~hl) +,2.2.1 Highway GCN,[0],[0]
"~hl ◦ (1− T (~hl)) , (3)
where ~hl is the incoming input to layer l + 1, (W lt , b l t) are gating weights and bias variables, ◦ is elementwise multiplication, and σ is the Sigmoid function.
",2.2.1 Highway GCN,[0],[0]
"2.3 DCCA Given two views X and Â (from Equation 1) of data samples, CCA (Hotelling, 1936), and its deep version (DCCA) (Andrew et al., 2013) learn functions f1(X) and f2(Â) such that the correlation between the output of the two functions is maximised:
ρ = corr(f1(X), f2(Â)) .",2.2.1 Highway GCN,[0],[0]
"(4)
The resulting representations of f1(X) and f2(Â) are the compressed representations of the two views where the uncorrelated noise between them is reduced.",2.2.1 Highway GCN,[0],[0]
"The new representations ideally represent user communities for the network view, and the language model of that community for the text view, and their concatenation is a multiview representation of data, which can be used as input for other tasks.
",2.2.1 Highway GCN,[0],[0]
"In DCCA, the two views are first projected to a lower dimensionality using a separate multilayer perceptron for each view (the f1 and f2 functions of Equation 4), the output of which is used to estimate the CCA cost:
maximise: tr(W T1 Σ12W2) subject to: W T1 Σ11W1 = W T 2 Σ22W2 =",2.2.1 Highway GCN,[0],[0]
"I (5)
where Σ11 and Σ22 are the covariances of the two outputs, and Σ12 is the cross-covariance.",2.2.1 Highway GCN,[0],[0]
"The weights W1 and W2 are the linear projections of the MLP outputs, which are used in estimating the CCA cost.",2.2.1 Highway GCN,[0],[0]
"The optimisation problem is solved by SVD, and the error is backpropagated to train the parameters of the two MLPs and the final linear projections.",2.2.1 Highway GCN,[0],[0]
"After training, the two networks are used to predict new projections for unseen data.",2.2.1 Highway GCN,[0],[0]
"The two projections of unseen data — the outputs of the two networks — are then concatenated to form a multiview sample representation, as shown in Figure 2.",2.2.1 Highway GCN,[0],[0]
"We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), (2) TWITTER-US (Roller et al., 2012), and (3) TWITTER-WORLD (Han et al., 2012).",3.1 Data,[0],[0]
These datasets have been used widely for training and evaluation of geolocation models.,3.1 Data,[0],[0]
"They are all pre-partitioned into training, development and test
sets.",3.1 Data,[0],[0]
"Each user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.",3.1 Data,[0],[0]
"GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively.",3.1 Data,[0],[0]
"The labels are the discretised geographical coordinates of the training points using a k-d tree following Roller et al. (2012), with the number of labels equal to 129, 256, and 930 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.1 Data,[0],[0]
"We build matrix Â as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user.",3.2 Constructing the Views,[0],[0]
"The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples.",3.2 Constructing the Views,[0],[0]
"For GCN, we use highway layers to control the amount of neighbourhood information passed to a node.",3.3 Model Selection,[0],[0]
"We use 3 layers in GCN with size 300, 600, 900 for GEOTEXT, TWITTER-US and TWITTERWORLD respectively.",3.3 Model Selection,[0],[0]
"Note that the final softmax layer is also graph convolutional, which sets the radius of the averaging neighbourhood to 4.",3.3 Model Selection,[0],[0]
"The
k-d tree bucket size hyperparameter which controls the maximum number of users in each cluster is set to 50, 2400, and 2400 for the respective datasets, based on tuning over the validation set.",3.3 Model Selection,[0],[0]
"The architecture of GCN-LP is similar, with the difference that the text view is set to zero.",3.3 Model Selection,[0],[0]
"In DCCA, for the unsupervised networks we use a single sigmoid hidden layer with size 1000 and a linear output layer with size 500 for the three datasets.",3.3 Model Selection,[0],[0]
"The loss function is CCA loss, which maximises the output correlations.",3.3 Model Selection,[0],[0]
"The supervised multilayer perceptron has one hidden layer with size 300, 600, 1000 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively, which we set by tuning over the development sets.",3.3 Model Selection,[0],[0]
"We evaluate the models using Median error, Mean error, and Acc@161, accuracy of predicting a user within 161km or 100 miles from the known location.",3.3 Model Selection,[0],[0]
"We also compare DCCA and GCN with two baselines:
GCN-LP is based on GCN, but for input, instead of text-based features , we use one-hot encoding of a user’s neighbours, which are then convolved with their k-hop neighbours using the GCN.",3.4 Baselines,[0],[0]
"This approach is similar to label propagation in smoothing the label distribution of a user with that of its neighbours, but uses graph convolutional networks which have extra layer parameters, and also a gating mechanism to control the smoothing neighbourhood radius.",3.4 Baselines,[0.9502767455342983],['We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher’s mode) instead of ‘wasting’ parameters on trying to model the entire space of translations.']
"Note that for unlabelled samples, the predicted labels are used for input after training accuracy reaches 0.2.
",3.4 Baselines,[0],[0]
"MLP-TXT+NET is a simple transductive supervised model based on a single layer multilayer perceptron where the input to the network is the concatenation of the text view X , the user content’s bag-of-words and Â (Equation 1), which represents the network view as a vector input.",3.4 Baselines,[0],[0]
"For the hidden layer we use a ReLU nonlinearity, and sizes 300, 600, and 600 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.4 Baselines,[0],[0]
"Deep CCA and GCN are able to provide an unsupervised data representation in different ways.
",4.1 Representation,[0],[0]
"Deep CCA takes the two text-based and networkbased views, and finds deep non-linear transformations that result in maximum correlation between the two views (Andrew et al., 2013).",4.1 Representation,[0],[0]
"The representations can be visualised using t-SNE, where we hope that samples with the same label are clustered together.",4.1 Representation,[0],[0]
"GCN, on the other hand, uses graph convolution.",4.1 Representation,[0],[0]
The representations of 50 samples from each of 4 randomly chosen labels of GEOTEXT are shown in Figure 3.,4.1 Representation,[0],[0]
"As shown, Deep CCA seems to slightly improve the representations from pure concatenation of the two views.",4.1 Representation,[0],[0]
"GCN, on the other hand, substantially improves the representations.",4.1 Representation,[0],[0]
"Further application of GCN results in more samples clumping together, which might be desirable when there is strong homophily.",4.1 Representation,[0],[0]
"To achieve good performance in supervised tasks, often large amounts of labelled data are required, which is a big challenge for Twitter geolocation, where only a small fraction of the data is geotagged (about 1%).",4.2 Labelled Data Size,[0],[0]
The scarcity of supervision indicates the importance of semi-supervised learning where unlabelled (e.g. non-geotagged) tweets are used for training.,4.2 Labelled Data Size,[0],[0]
"The three models we propose (MLP-TXT+NET, DCCA, and GCN) are all transductive semi-supervised models that use unlabelled data, however, they are different in terms of how much labelled data they require to achieve acceptable performance.",4.2 Labelled Data Size,[0],[0]
"Given that in a real-world scenario, only a small fraction of data is geotagged, we conduct an experiment to analyse the effect of labelled samples on the performance of the three geolocation models.",4.2 Labelled Data Size,[0],[0]
"We provided the three models with different fractions of samples that are labelled (in terms of % of dataset samples) while using the remainder as unlabelled data, and analysed their Median error performance over the development set of GEOTEXT, TWITTER-US, and TWITTER-WORLD.",4.2 Labelled Data Size,[0],[0]
"Note that the text and network view, and the development set, remain fixed for all the experiments.",4.2 Labelled Data Size,[0],[0]
"As shown in Figure 4, when the fraction of labelled samples is less than 10% of all the samples, GCN and DCCA outperform MLP-TXT+NET, as a result of having fewer parameters, and therefore, lower supervision requirement to optimise them.",4.2 Labelled Data Size,[0],[0]
"When enough training data is available (e.g. more than 20% of all the samples), GCN and MLP-TXT+NET clearly outperform DCCA, possibly as a result of directly modelling the
interactions between network and text views.",4.2 Labelled Data Size,[0],[0]
"When all the training samples of the two larger datasets (95% and 98% for TWITTER-US and TWITTERWORLD, respectively) are available to the models, MLP-TXT+NET outperforms GCN.",4.2 Labelled Data Size,[0],[0]
Note that the number of parameters increases from DCCA to GCN and to MLP-TXT+NET.,4.2 Labelled Data Size,[0],[0]
"In 1% for GEOTEXT, DCCA outperforms GCN as a result of having fewer parameters and just a few labelled samples, insufficient to train the parameters of GCN.",4.2 Labelled Data Size,[0],[0]
"Adding more layers to GCN expands the graph neighbourhood within which the user features are averaged, and so might introduce noise, and consequently decrease accuracy as shown in Figure 5 when no gates are used.",4.3 Highway Gates,[0],[0]
"We see that by adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without
much change.",4.3 Highway Gates,[0],[0]
The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6.,4.3 Highway Gates,[0],[0]
"The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1.",4.4 Performance,[0],[0]
"The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017).",4.4 Performance,[0],[0]
"MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important.",4.4 Performance,[0],[0]
"MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on GEO-
TEXT.",4.4 Performance,[0],[0]
"However, it’s difficult to make a fair comparison as they use timezone data in their feature set.",4.4 Performance,[0],[0]
"MLP-TXT+NET outperforms GCN over TWITTERUS and TWITTER-WORLD, which are very large, and have large amounts of labelled data.",4.4 Performance,[0],[0]
In a scenario with little supervision (1% of the total samples are labelled),4.4 Performance,[0],[0]
"DCCA and GCN clearly outperform MLP-TXT+NET, as they have fewer pa-
rameters.",4.4 Performance,[0],[0]
"Except for Acc@161 over GEOTEXT where the number of labelled samples in the minimal supervision scenario is very low, GCN outperforms DCCA by a large margin, indicating that for a medium dataset where only 1% of samples are labelled (as happens in random samples of Twitter) GCN is superior to MLP-TXT+NET and DCCA, consistent with Section 4.2.",4.4 Performance,[0],[0]
"Both MLP-TXT+NET and GCN achieve state of the art results compared
to network-only, text-only, and hybrid models.",4.4 Performance,[0],[0]
"The network-based GCN-LP model, which does label propagation using Graph Convolutional Networks, outperforms Rahimi et al. (2015a), which is based on location propagation using Modified Adsorption (Talukdar and Crammer, 2009), possibly because the label propagation in GCN is parametrised.",4.4 Performance,[0],[0]
"Although the performance of MLP-TXT+NET is better than GCN and DCCA when a large amount of labelled data is available (Table 1), under a scenario where little labelled data is available (1% of data), DCCA and GCN outperform MLP-TXT+NET, mainly because the number of parameters in MLP-TXT+NET grows with the number of samples, and is much larger than GCN and DCCA.",4.5 Error Analysis,[0],[0]
"GCN outperforms DCCA and MLP-TXT+NET using 1% of data, however, the distribution of errors in the development set of TWITTER-US indicates higher error for smaller states such as Rhode Island (RI), Iowa (IA), North Dakota (ND), and Idaho (ID), which is simply because the number of labelled samples in those states is insufficient.
",4.5 Error Analysis,[0],[0]
"Although we evaluate geolocation models with Median, Mean, and Acc@161, it doesn’t mean that the distribution of errors is uniform over all locations.",4.5 Error Analysis,[0],[0]
"Big cities often attract more local online discussions, making the geolocation of users in those areas simpler.",4.5 Error Analysis,[0],[0]
"For example users in LA are more likely to talk about LA-related issues such as their sport teams, Hollywood or local events than users in the state of Rhode Island (RI), which lacks large sport teams or major events.",4.5 Error Analysis,[0],[0]
"It is also possible that people in less densely populated areas are further apart from each other, and therefore, as a result of discretisation fall in different clusters.",4.5 Error Analysis,[0],[0]
The non,4.5 Error Analysis,[0],[0]
"-uniformity in local discussions results in lower geolocation performance in less densely populated areas like Midwest U.S., and higher performance in densely populated areas such as NYC and LA as shown in Figure 7.",4.5 Error Analysis,[0],[0]
"The geographical distribution of error for GCN, DCCA and MLP-TXT+NET under the minimal supervision scenario is shown in the supplementary material.
",4.5 Error Analysis,[0],[0]
"To get a better picture of misclassification between states, we built a confusion matrix based on known state and predicted state for development users of TWITTER-US using GCN using only 1% of labelled data.",4.5 Error Analysis,[0],[0]
"There is a tendency for users to be wrongly predicted to be in CA, NY, TX, and surpris-
ingly OH.",4.5 Error Analysis,[0],[0]
"Particularly users from states such as TX, AZ, CO, and NV, which are located close to CA, are wrongly predicted to be in CA, and users from NJ, PA, and MA are misclassified as being in NY.",4.5 Error Analysis,[0],[0]
The same goes for OH and TX where users from neighbouring smaller states are misclassified to be there.,4.5 Error Analysis,[0],[0]
"Users from CA and NY are also misclassified between the two states, which might be the result of business and entertainment connections that exist between NYC and LA/SF.",4.5 Error Analysis,[0],[0]
"Interestingly, there are a number of misclassifications to FL for users from CA, NY, and TX, which might be the effect of users vacationing or retiring to FL.",4.5 Error Analysis,[0],[0]
The full confusion matrix between the U.S. states is provided in the supplementary material.,4.5 Error Analysis,[0],[0]
"In Table 2, local terms of a few regions detected by GCN under minimal supervision are shown.",4.6 Local Terms,[0],[0]
The terms that were present in the labelled data are excluded to show how graph convolutions over the social graph have extended the vocabulary.,4.6 Local Terms,[0],[0]
"For example, in case of Seattle, #goseahawks is an important term not present in the 1% labelled data but present in the unlabelled data.",4.6 Local Terms,[0],[0]
The convolution over the social graph is able to utilise such terms that don’t exist in the labelled data.,4.6 Local Terms,[0],[0]
"Previous work on user geolocation can be broadly divided into text-based, network-based and multiview approaches.
",5 Related Work,[0],[0]
Text-based geolocation uses the geographical bias in language use to infer the location of users.,5 Related Work,[0],[0]
"There are three main text-based approaches to geolocation: (1) gazetteer-based models which map geographical references in text to location, but ignore non-geographical references and vernacular uses of language (Rauch et al., 2003; Amitay et al., 2004; Lieberman et al., 2010); (2) geographical topic models that learn region-specific topics, but don’t scale to the magnitude of social media (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013); and (3) supervised models which are often framed as text classification (Serdyukov et al., 2009; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or text regression (Iso et al., 2017; Rahimi et al., 2017a).",5 Related Work,[0],[0]
"Supervised models scale well and can achieve good performance with sufficient supervision, which is not available in a real world scenario.
",5 Related Work,[0],[0]
Network-based methods leverage the location homophily assumption: nearby users are more likely to befriend and interact with each other.,5 Related Work,[0],[0]
"There are four main network-based geolocation approaches: distance-based, supervised classification, graph-based label propagation, and node embedding methods.",5 Related Work,[0],[0]
"Distance-based methods model the probability of friendship given the distance (Backstrom et al., 2010; McGee et al., 2013; Gu et al., 2012; Kong et al., 2014), supervised models use neighbourhood features to classify a user into a location (Rout et al., 2013; Malmi et al., 2015), and graph-based label-propagation models propagate the location information through the user–user graph to estimate unknown labels (Davis Jr et al., 2011; Jurgens, 2013; Compton et al., 2014).",5 Related Work,[0],[0]
"Node embedding methods build heterogeneous graphs between user–user, user–location and location– location, and learn an embedding space to minimise the distance of connected nodes, and maximise the distance of disconnected nodes.",5 Related Work,[0],[0]
"The embeddings are then used in supervised models for geolocation (Wang et al., 2017).",5 Related Work,[0],[0]
Network-based models fail to geolocate disconnected users: Jurgens et al. (2015) couldn’t geolocation 37% of users as a result of disconnectedness.,5 Related Work,[0],[0]
"Previous work on hybrid text and network methods can be broadly categorised into three main approaches: (1) incorporating text-based information such as toponyms or locations predicted from a textbased model as auxiliary nodes into the user–user graph, which is then used in network-based models (Li et al., 2012a,b; Rahimi et al., 2015b,a); (2) ensembling separately trained text- and networkbased models (Gu et al., 2012; Ren et al., 2012; Jayasinghe et al., 2016; Ribeiro and Pappa, 2017); and (3) jointly learning geolocation from several information sources such as text and network information (Miura et al., 2017; Do et al., 2017), which can capture the complementary information in text and network views, and also model the interactions between the two.",5 Related Work,[0],[0]
"None of the previous
multiview approaches — with the exception of Li et al. (2012a) and Li et al. (2012b) that only use toponyms — effectively uses unlabelled data in the text view, and use only the unlabelled information of the network view via the user–user graph.
",5 Related Work,[0],[0]
"There are three main shortcomings in the previous work on user geolocation that we address in this paper: (1) with the exception of few recent works (Miura et al., 2017; Do et al., 2017), previous models don’t jointly exploit both text and network information, and therefore the interaction between text and network views is not modelled; (2) the unlabelled data in both text and network views is not effectively exploited, which is crucial given the small amounts of available supervision; and (3) previous models are rarely evaluated under a minimal supervision scenario, a scenario which reflects real world conditions.",5 Related Work,[0],[0]
"We proposed GCN, DCCA and MLP-TXT+NET, three multiview, transductive, semi-supervised geolocation models, which use text and network information to infer user location in a joint setting.",6 Conclusion,[0],[0]
"We showed that joint modelling of text and network information outperforms network-only, text-only, and hybrid geolocation models as a result of modelling the interaction between text and network information.",6 Conclusion,[0],[0]
We also showed that GCN and DCCA are able to perform well under a minimal supervision scenario similar to real world applications by effectively using unlabelled data.,6 Conclusion,[0],[0]
"We ignored the context in which users interact with each other, and assumed all the connections to hold location homophily.",6 Conclusion,[0],[0]
"In future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g. using user–user gates).",6 Conclusion,[0],[0]
Social media user geolocation is vital to many applications such as event detection.,abstractText,[0],[0]
"In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context.",abstractText,[0],[0]
"We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the stateof-the-art over three benchmark geolocation datasets when sufficient supervision is available.",abstractText,[0],[0]
"We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines.",abstractText,[0],[0]
We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.,abstractText,[0],[0]
Semi-supervised User Geolocation via Graph Convolutional Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2016",text,[0],[0]
"Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning.",1 Introduction,[0],[0]
"It has attracted much attention recently as it could make texts more accessible to wider audiences (Aluı́sio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; Štajner and Popović, 2016).
",1 Introduction,[0],[0]
"However, the state-of-the-art ATS systems still do not reach satisfying performances and require some human post-editing (Štajner and Popović, 2016).",1 Introduction,[0],[0]
"While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Specia, 2010; Štajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training.",1 Introduction,[0],[0]
"The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia
(EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015).",1 Introduction,[0],[0]
"For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches.",1 Introduction,[0],[0]
"The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaš and Štajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning.",1 Introduction,[0],[0]
"However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far.
",1 Introduction,[0],[0]
"The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015).",1 Introduction,[0],[0]
"Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS systems, due to the lack of its sentence (and paragraph) alignments.",1 Introduction,[0],[0]
"Such alignments, between various text complexity levels, would offer large training datasets for modelling different levels of simplification, i.e. ‘mild’ simplifications (using the alignments from the neighbouring levels) and ‘heavy’ simplifications (using the alignments of level pairs: 0–3, 0–4, 1–4).
Contributions.",1 Introduction,[0],[0]
"We: (1) provide several methods for paragraph- and sentence alignment of parallel texts, and for assessing similarity level between pairs of text snippets, as freely avail-
1Freely available: https://newsela.com/data/
97
able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications.",1 Introduction,[0],[0]
"The current state-of-the-art systems for automatic sentence-alignment of original and manually simplified texts are the GSWN method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMMbased method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015).
",2 Related Work,[0],[0]
The HMM-based method can be applied to any language as it does not require any languagespecific resources.,2 Related Work,[0],[0]
"It is based on two hypotheses: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has at least one corresponding ‘original’ sentence (it can have more than one in the case of ‘n-1’ or ‘nm’ alignments).
",2 Related Work,[0],[0]
"As Simple Wikipedia does not represent direct simplification of the ‘original’ Wikipedia articles (‘simple’ articles were written independently of the ‘original’ ones), GSWN method does not assume H1 or H2.",2 Related Work,[0],[0]
"The main limitations of this method are that it only allows for ‘1-1’ sentence alignments – which is very restricting for TS as it does not allow for sentence splitting (‘1-n’), and summarisation and compression (‘n-1’ and ‘n-m’) alignments – and it is language-dependent as it requires English Wiktionary.
",2 Related Work,[0],[0]
"Unlike the GSWN method, all the methods we apply are language-independent, resource-light and allow for ‘1-n’, ‘n-1’, and ‘n-m’ alignments.",2 Related Work,[0],[0]
"Similar to the HMM-method, our methods assume the hypothesis H2.",2 Related Work,[0],[0]
"We provide them in both variants, using the hypothesis H1 and without it (Section 3.1).
",2 Related Work,[0],[0]
2https://github.com/neosyon/ SimpTextAlign,2 Related Work,[0],[0]
"Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si, cj), where si ∈ S, cj ∈",3 Approach,[0],[0]
C.,3 Approach,[0],[0]
"Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or sentences).",3 Approach,[0],[0]
"Most Similar Text (MST): Given one of the similarity methods (Section 3.2), MST compares similarity scores of all possible pairs (si, cj), and aligns each si ∈ S with the closest one in C. MST with Longest Increasing Sequence (MSTLIS): MST-LIS uses the hypothesis H1.",3.1 Alignment strategies,[0],[0]
"It first uses the MST strategy, and then postprocess the output by extracting – from all obtained alignments – only those alignments li ∈ L, which contain the longest increasing sequence of offsets jk in C. In order to allow for ‘1–n’ alignments (i.e. sentence splitting), we allow for repeated offsets of C (‘complex’ text snippets) in L. The ‘simple’ text snippets not contained in L are included in the set U of unaligned snippets.",3.1 Alignment strategies,[0],[0]
"Finally, we align each um ∈ U by restricting the search space in C to those offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets.",3.1 Alignment strategies,[0],[0]
"For instance, if L = {(s1, c4), (s3, c7)} and U = {s2}, then the search space for the alignments of s2 is reduced to {c4...c7}.",3.1 Alignment strategies,[0],[0]
"We denote this strategy with an ‘*’ in the results (Table 2), e.g. C3G*.",3.1 Alignment strategies,[0],[0]
C3G: We employ the Character N -Gram,3.2 Similarity Methods,[0],[0]
"(CNG) (Mcnamee and Mayfield, 2004) similarity model (for n = 3) with log TF-IDF weighting (Salton and McGill, 1986) and compare vectors using the cosine similarity.",3.2 Similarity Methods,[0],[0]
WAVG:,3.2 Similarity Methods,[0],[0]
"We use the continuous skip-gram model (Mikolov et al., 2013b) of the TensorFlow toolkit3 to process the whole English Wikipedia and generate continuous representations of its words.4 For each text snippet, we average its word vectors to obtain a single representation of its content as this setting has shown good results
3https://www.tensorflow.org/ 4We use 300-dimensional vectors, context windows of size 10, and 20 negative words for each sample, in all our continuous word-based models.
",3.2 Similarity Methods,[0],[0]
"in other NLP tasks (e.g. for selecting out-of-thelist words (Mikolov et al., 2013a)).",3.2 Similarity Methods,[0],[0]
"Finally, the similarity between text snippets is estimated using the cosine similarity.",3.2 Similarity Methods,[0],[0]
CWASA:,3.2 Similarity Methods,[0],[0]
"We employ the Continuous Word Alignment-based Similarity Analysis (CWASA) model (Franco-Salvador et al., 2016), which finds the optimal word alignment by computing cosine similarity between continuous representations of all words (instead of averaging word vectors as in the case of WAVG).",3.2 Similarity Methods,[0],[0]
"It was originally proposed for plagiarism detection with excellent results, especially for longer text snippets.",3.2 Similarity Methods,[0],[0]
"To compare the performances of different alignment methods, we randomly selected 10 original texts (Level 0) and their corresponding simpler versions at Levels 1, 3 and 4.",4 Manual Evaluation,[0],[0]
"Instead of creating a ‘gold standard’ and then automatically evaluating the performances, we asked two annotators to rate each pair of automatically aligned paragraphs and sentences – by each of the possible six alignment methods and the HMM-based method (Bott and Saggion, 2011) – for three pairs of text complexity levels (0–1, 0–4, and 3–4) on a 0–2 scale, where: 0 – no semantic overlap in the content; 1 – partial semantic overlap (partial matches); 2 – same semantic content (good matches).",4 Manual Evaluation,[0],[0]
"This resulted in a total of 1526 paragraph- and 1086 sentence-alignments for the 0–1 pairs, and 1218 paragraph- and 1266 sentence-alignments for the 0–4 and 3–4 pairs.",4 Manual Evaluation,[0],[0]
"In the context of TS, both good- and partial matches
are important.",4 Manual Evaluation,[0],[0]
"While full semantic overlap models full paraphrases (‘1-1’ alignments), partial overlap models sentence splitting (“1-n” alignments), deleting irrelevant sentence parts, adding explanations, or summarizing (‘n-m’ alignments).",4 Manual Evaluation,[0],[0]
"Several examples of full and partial matches from the EW–SEW dataset (Hwang et al., 2015) are given in Table 1.
",4 Manual Evaluation,[0],[0]
"We expect that the automatic-alignment task is the easiest between the 0–1 text complexity levels, and much more difficult between the 0-4 levels (Level 4 is obtained after four stages of simplification and thus contains stronger paraphrases and less lexical overlap with Level 0 than Level 1 has).",4 Manual Evaluation,[0],[0]
"We also explore whether the task is equally difficult whenever we align two neighbouring levels, or the difficulty of the task depends on the level complexity (0–1 vs. 3–4).",4 Manual Evaluation,[0],[0]
"The obtained interannotator agreement, weighted Cohen’s κ (on 400 double-annotated instances) was between 0.71 and 0.74 depending on the task and levels.
",4 Manual Evaluation,[0],[0]
"The results of the manual analysis (Table 2) showed that: (1) all applied methods significantly (p < 0.001) outperformed the HMM method on both paragraph- and sentence-alignment tasks;5 (2) the methods which do not assume hypothesis H1 (C3G, CWASA, and WAVG) led to (not significantly) higher percentage of correct alignments than their counterparts which do assume
5Although some of our methods share the same percentage of good+partial matches with the HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches).
",4 Manual Evaluation,[0],[0]
"H1 (C3G*, CWASA*, WAVG*); (3) the difference in the performances of the lexical approach (C3G) and semantic approaches (CWASA and WAVG) was significant only in the 0–4 sentencealignment task, where CWASA performed significantly worse (p < 0.001) than the other two methods, and in the 0–4 paragraph-alignment task, where WAVG performed significantly worse than C3G; (4) the 2-step C3G alignment-method (C3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method.",4 Manual Evaluation,[0],[0]
"Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb.",5 Extrinsic Evaluation,[0],[0]
"and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007).",5 Extrinsic Evaluation,[0],[0]
"We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence
6Given that the performance of the HMM-method was poor for non-neighboring levels (Table 2).
",5 Extrinsic Evaluation,[0],[0]
"7GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), refinement and phraseextraction heuristics (Koehn et al., 2003), the minimum error rate training (Och, 2003) for tuning, and 5-gram LMs with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002).
pairs of their test set8 for our human evaluation.",5 Extrinsic Evaluation,[0],[0]
"Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available.",5 Extrinsic Evaluation,[0],[0]
"We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS system (Glavaš and Štajner, 2015).9
We perform three types of human evaluation on the outputs of all systems.",5 Extrinsic Evaluation,[0],[0]
"First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. “become defunct” → “was dissolved”) as one change.",5 Extrinsic Evaluation,[0],[0]
"We mark as Correct those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers).10 Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each sentence with at least one change on a 1–5 Likert scale (1 – very bad; 5 – very good).",5 Extrinsic Evaluation,[0],[0]
"Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences (one pair at the time) and asked whether the target sentence is: +2 – much simpler; +1 – somewhat simpler; 0 – equally difficult; -1 – somewhat more difficult; -2 – much more difficult, than the reference sentence.",5 Extrinsic Evaluation,[0],[0]
"While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.
",5 Extrinsic Evaluation,[0],[0]
"Adding our sentence-aligned Newsela corpus
8Both freely available from: https://github.com/ cocoxu/simplification/
9We use the output of the original SBMT (Xu et al., 2016) and LightLS (Glavaš and Štajner, 2015) systems, obtained from the authors.
",5 Extrinsic Evaluation,[0],[0]
"10Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority.
",5 Extrinsic Evaluation,[0],[0]
(either neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2l or all C3G-2l) to the currently best sentence-aligned Wiki corpus (Hwang et al., 2015) in a standard PBSMT setup significantly11 improves grammaticality (G) and meaning preservation (M), and increases the percentage of correct changes (Table 3).",5 Extrinsic Evaluation,[0],[0]
"It also significantly outperforms the state-of-the-art ATS systems by simplicity rankings (S), meaning preservation (M), and number of correct changes (Correct), while achieving almost equally good grammaticality (G).
",5 Extrinsic Evaluation,[0],[0]
The level of simplification applied in the training dataset (Newsela neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2s vs. Newsela all C3G-2s) significantly influences G and M scores.
",5 Extrinsic Evaluation,[0],[0]
"The use of the HMM-method for aligning Newsela (instead of ours) lead to significantly worse simplifications by all five criteria.
",5 Extrinsic Evaluation,[0],[0]
"11Wilcoxon’s signed rank test, p < 0.001.
",5 Extrinsic Evaluation,[0],[0]
An example of the outputs of different ATS systems is presented in Table 4.,5 Extrinsic Evaluation,[0],[0]
"We provided several methods for paragraphand sentence-alignment from parallel TS corpora, made the software publicly available, and showed that the use of the new sentence-aligned (freely available) Newsela dataset leads to state-of-the-art ATS systems even in a basic PBSMT setup.",6 Conclusions,[0],[0]
"We also showed that lexically-based C3G method is superior to semantically-based methods (CWASA and WAVG) in aligning paraphraphs and sentences with ‘heavy’ simplifications (0–4 alignments), and that 2-step sentence alignment (aligning first paragraphs and then sentences within the paragraphs) lead to more correct alignments than the ‘direct’ sentence alignment.",6 Conclusions,[0],[0]
"This work has been partially supported by the SFB 884 on the Political Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG), and also by the SomEMBED TIN2015-71147-C2-1-P MINECO research project.",Acknowledgments,[0],[0]
We provide several methods for sentencealignment of texts with different complexity levels.,abstractText,[0],[0]
"Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems.",abstractText,[0],[0]
"We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems.",abstractText,[0],[0]
Sentence Alignment Methods for Improving Text Simplification Systems,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 360–368, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence.,1 Introduction,[0],[0]
"Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few).
",1 Introduction,[0],[0]
Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output.,1 Introduction,[0],[0]
"A common approach is to use only some syntactic information (Jing, 2000; Clarke & Lapata, 2008,
among others) or use syntactic features as signals in a statistical model (McDonald, 2006).",1 Introduction,[0],[0]
"It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight & Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013, among others).",1 Introduction,[0],[0]
"Unfortunately, this makes such systems vulnerable to error propagation as there is no way to recover from an incorrect parse tree.",1 Introduction,[0],[0]
"With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy (Zhang & McDonald, 2014), the problem is not a negligible one.",1 Introduction,[0.950224614952629],"['Sequence-Level Knowledge Distillation (Seq-KD) Student is trained on the teacher-generated data, which is the result of running beam search and taking the highest-scoring sequence with the teacher model.']"
"To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization.
",1 Introduction,[0],[0]
In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information?,1 Introduction,[0],[0]
"While phenomena like long-distance relations may seem to make generation of grammatically correct compressions impossible, we are going to present an evidence to the contrary.",1 Introduction,[0],[0]
"In particular, we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models (LSTMs) to output surprisingly readable and informative compressions.",1 Introduction,[0],[0]
"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings, in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges.",1 Introduction,[0],[0]
"We believe that this is an important result as it may suggest a new direction for sentence compression research which is less tied to modeling linguistic
360
structures, especially syntactic ones, than the compression work so far.
",1 Introduction,[0],[0]
The paper is organized as follows: Section 3 presents a competitive baseline which implements the system of McDonald (2006) for large training sets.,1 Introduction,[0],[0]
The LSTM model and its three configurations are introduced in Section 4.,1 Introduction,[0],[0]
The evaluation set-up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions.,1 Introduction,[0],[0]
"The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero.",2 Related Work,[0],[0]
"The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence.",2 Related Work,[0],[0]
"That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013).",2 Related Work,[0],[0]
"Thus, the relation to existing compression work is that we also use the deletion approach.
",2 Related Work,[0],[0]
Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence.,2 Related Work,[0],[0]
"Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well.",2 Related Work,[0],[0]
"Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba & Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs).",2 Related Work,[0],[0]
"Thus, with regard to this line of research, our work comes closest to the recent machine translation work.",2 Related Work,[0],[0]
"An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions.
",2 Related Work,[0],[0]
"A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have
not been many examples yet (Cohn & Lapata, 2008).",2 Related Work,[0],[0]
"However, in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries.",2 Related Work,[0],[0]
We leave experiments with paraphrasing models to future work.,2 Related Work,[0],[0]
We compare our model against the system of McDonald (2006) which also formulates sentence compression as a binary sequence labeling problem.,3 Baseline,[0],[0]
"In contrast to our proposal, it makes use of a large set of syntactic features which are treated as soft evidence.",3 Baseline,[0],[0]
The presence or absence of these features is treated as signals which do not condition the output that the model can produce.,3 Baseline,[0],[0]
"Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences.
",3 Baseline,[0],[0]
The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used for model fitting.,3 Baseline,[0],[0]
"The first change was related to the learning procedure and the second one to the family of features used.
",3 Baseline,[0],[0]
"Regarding the learning procedure, the original model uses a large-margin learning framework, namely MIRA (Crammer & Singer, 2003), but with some minor changes as presented by McDonald et al. (2005).",3 Baseline,[0],[0]
"In this set-up, online learning is performed, and at each step an optimization procedure is made where K constraints are included, which correspond to the top-K solutions for a given training observation.",3 Baseline,[0],[0]
"This optimization step is equivalent to a Quadratic Programming problem if K > 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model.",3 Baseline,[0],[0]
"Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model.",3 Baseline,[0],[0]
"Consequently, and for the sake of being able to successfully train the model with largescale data, the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing (McDonald et al., 2010), where each shard is processed with MIRA and K is set to 1.
Setting K = 1 will only affect the weight update described on line 4 of Figure 3 of McDonald
(2006), which is now expressed as:
w(i+1) ← w(i)",3 Baseline,[0],[0]
"+ τ × eyt,y′ where τ = max ( 0, L(yt,y′)−w · eyt,y′ ||eyt,y′ ||2 )
eyt,y′ =",3 Baseline,[0],[0]
"F (xt,yt)− F (xt,y′) y′",3 Baseline,[0],[0]
= best(x; w(i)),3 Baseline,[0],[0]
"F (x,y) = |y|∑ j=2 f(x, I(yj−1), I(yj))
",3 Baseline,[0],[0]
The second change concerns the feature set used.,3 Baseline,[0],[0]
"While McDonald’s original model contains deep syntactic features coming from both dependency and constituency parse trees, we use only dependency-based features.",3 Baseline,[0],[0]
"Additionally, and to better compare the baseline with the LSTM models, we have included as an optional feature a 256-dimension embedding-vector representation of each input word and its syntactic parent.",3 Baseline,[0],[0]
"The vectors are pre-trained using the Skipgram model1 (Mikolov et al., 2013).",3 Baseline,[0],[0]
"Ultimately, our implementation of McDonald’s model contained 463,614 individual features, summarized in three categories: • PoS features: Joint PoS tags of selected to-
kens.",3 Baseline,[0],[0]
"Unigram, bigram and trigram PoS context of selected and dropped tokens.",3 Baseline,[0],[0]
All the previous features conjoined with one indicating if the last two selected tokens are adjacent.,3 Baseline,[0],[0]
•,3 Baseline,[0],[0]
"Deep syntactic features: Dependency labels
of taken and dropped tokens and their parent dependencies.",3 Baseline,[0],[0]
"Boolean features indicating syntactic relations between selected tokens (i.e., siblings, parents, leaves, etc.).",3 Baseline,[0],[0]
Dependency label of the least common ancestor in the dependency tree between a batch of dropped tokens.,3 Baseline,[0],[0]
All the previous features conjoined with the PoS tag of the involved tokens.,3 Baseline,[0],[0]
"• Word features: Boolean features indicating
if a group of dropped nodes contain a complete or incomplete parenthesization.",3 Baseline,[0],[0]
Wordembedding vectors of selected and dropped tokens and their syntactic parents.,3 Baseline,[0],[0]
"The model is fitted over ten epochs on the whole training data, and for model selection a small development set consisting of 5,000 previously unseen sentences is used (none of them belonging to
1https://code.google.com/p/word2vec/
the evaluation set).",3 Baseline,[0],[0]
The automated metric used for this selection was accuracy@1 which is the proportion of golden compressions which could be fully reproduced.,3 Baseline,[0],[0]
The performance on the development set plateaus when getting close to the last epoch.,3 Baseline,[0],[0]
Our approach is largely based on the sequence to sequence paradigm proposed in Sutskever et al. (2014).,4 The LSTM model,[0],[0]
We train a model that maximizes the probability of the correct output given the input sentence.,4 The LSTM model,[0],[0]
"Concretely, for each training pair (X,Y ), we will learn a parametric model (with parameters θ), by solving the following optimization problem:
θ∗ = arg max θ ∑ X,Y log p(Y |X; θ) (1)
where the sum is assumed to be over all training examples.",4 The LSTM model,[0],[0]
"To model the probability p, we use the same architecture described by Sutskever et al. (2014).",4 The LSTM model,[0],[0]
"In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997), designed to avoid vanishing gradients and to remember some long-distance dependences from the input sequence.",4 The LSTM model,[0],[0]
Figure 1 shows a basic LSTM architecture.,4 The LSTM model,[0],[0]
"The RNN is fed with input words Xi (one at a time), until we feed a special symbol “GO”.",4 The LSTM model,[0],[0]
"It is now a common practice (Sutskever et al., 2014; Li & Jurafsky, 2015) to start feeding the input in reversed order, as it has been shown to perform better empirically.",4 The LSTM model,[0],[0]
"During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right predictions when the second pass starts, after the “GO” symbol is read.
",4 The LSTM model,[0],[0]
"We can apply the chain rule to decompose Equation (1) as follows:
p(Y |X; θ) = T∏ t=1 p(Yt|Y1, . . .",4 The LSTM model,[0],[0]
", Yt−1, X; θ) (2)
noting that we made no independence assumptions.",4 The LSTM model,[0],[0]
"Once we find the optimal θ∗, we construct our estimated compression Ŷ as:
Ŷ = arg max Y
p(Y |X; θ∗) (3)
LSTM cell: Let us review the sequence-tosequence LSTM model.",4 The LSTM model,[0],[0]
The Long Short Term Memory model of Hochreiter & Schmidhuber (1997) is defined as follows.,4 The LSTM model,[0],[0]
"Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, given a sequence of inputs (x1, . . .",4 The LSTM model,[0],[0]
", xT ), the LSTM computes the h-sequence (h1, . . .",4 The LSTM model,[0],[0]
", hT ) and the m-sequence (m1, . . .",4 The LSTM model,[0],[0]
",mT ) as follows
it =",4 The LSTM model,[0],[0]
"sigm(W1xt +W2ht−1) i′t = tanh(W3xt +W4ht−1) ft = sigm(W5xt +W6ht−1) ot = sigm(W7xt +W8ht−1) mt = mt−1 ft + it i′t ht = mt ot
The operator denotes element-wise multiplication, the matrices W1, . . .",4 The LSTM model,[0],[0]
",W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise.
",4 The LSTM model,[0],[0]
Stochastic gradient descent is used to maximize the training objective (Eq. (1)),4 The LSTM model,[0],[0]
w.r.t.,4 The LSTM model,[0],[0]
"all the LSTM parameters.
",4 The LSTM model,[0],[0]
Network architecture:,4 The LSTM model,[0],[0]
In these experiments we have used the architecture depicted in Figure 3.,4 The LSTM model,[0],[0]
"Following Vinyals et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014).",4 The LSTM model,[0],[0]
"The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three
labels: 1, if a word is to be retained in the compression, 0 if a word is to be deleted, or EOS, which is the output label used for the “GO” input and the end-of-sentence final period.
",4 The LSTM model,[0],[0]
"Input representation: In the simplest implementation, that we call LSTM, the input layer has 259 dimensions.",4 The LSTM model,[0],[0]
"The first 256 contain the embedding-vector representation of the current in-
put word, pre-trained using the Skipgram model2",4 The LSTM model,[0],[0]
"(Mikolov et al., 2013).",4 The LSTM model,[0],[0]
"The final three dimensions contain a one-hot-spot representation of the goldstandard label of the previous word (during training), or the generated label of the previous word (during decoding).
",4 The LSTM model,[0],[0]
"For the LSTM+PAR architecture we first parse the input sentence, and then we provide as input, for each input word, the embedding-vector representation of that word and its parent word in the dependency tree.",4 The LSTM model,[0],[0]
"If the current input is the root node, then a special parent embedding is constructed with all nodes set to zero except for one node.",4 The LSTM model,[0],[0]
In these settings we want to test the hypothesis whether knowledge about the parent node can be useful to decide if the current constituent is relevant or not for the compression.,4 The LSTM model,[0],[0]
The dimensionality of the input layer in this case is 515.,4 The LSTM model,[0],[0]
"Similarly to McDonald (2006), syntax is used here as a soft feature in the model.
",4 The LSTM model,[0],[0]
"For the LSTM+PAR+PRES architecture, we again parse the input sentence, and use a 518-sized embedding vector, that includes: • The embedding vector for the current word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The embedding vector for the parent word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The label predicted for the last word (3 di-
mensions).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has 2https://code.google.com/p/word2vec/
already been seen and kept in the compression (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has
already been seen but discarded (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word
comes later in the input (1 dimension).
",4 The LSTM model,[0],[0]
Decoding: Eq. (3) involves searching through all possible output sequences (given X).,4 The LSTM model,[0],[0]
"Contrary to the baseline, in the case of LSTMs the complete previous history is taken into account for each prediction and we cannot simplify Eq.",4 The LSTM model,[0],[0]
(2) with a Markov assumption.,4 The LSTM model,[0],[0]
"Therefore, the search space at decoding time is exponential on the length of the input, and we have used a beam-search procedure as described in Figure 2.
",4 The LSTM model,[0],[0]
"Fixed parameters: For training, we unfold the network 120 times and make sure that none of our training instances is longer than that.",4 The LSTM model,[0],[0]
"The learning rate is initialized at 2, with a decay factor of 0.96 every 300,000 traning steps.",4 The LSTM model,[0],[0]
The dropping probability for the dropout layers is 0.2.,4 The LSTM model,[0],[0]
The number of nodes in each LSTM layer is always identical to the number of nodes in the input layer.,4 The LSTM model,[0],[0]
We have not tuned these parameters nor the number of stacked layers.,4 The LSTM model,[0],[0]
Both the LSTM systems we introduced and the baseline require a training set of a considerable size.,5.1 Data,[0],[0]
"In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight & Marcu, 2000; Clarke & Lapata, 2006).",5.1 Data,[0],[0]
"Following the method of Filippova & Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input.",5.1 Data,[0],[0]
"For testing, we use the publicly released set of 10,000 sentence-compression pairs3.",5.1 Data,[0],[0]
"We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation.",5.1 Data,[0],[0]
We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters.,5.2 Experiments,[0],[0]
The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression.,5.2 Experiments,[0],[0]
"The former covers the grammatical correctness, comprehensibility and fluency of the output while the latter measures the amount of important content preserved in the compression.
",5.2 Experiments,[0],[0]
"Additionally, for experiments on the development set, we used two metrics for automatic evaluation: per-sentence accuracy (i.e., how many compressions could be fully reproduced) and word-based F1-score.",5.2 Experiments,[0],[0]
The latter differs from the RASP-based relation F-score by Riezler et al. (2003) in that we simply compute the recall and precision in terms of tokens kept in the golden and the generated compressions.,5.2 Experiments,[0],[0]
"We report these results for completeness although it is the results of the human evaluation from which we draw our conclusions.
",5.2 Experiments,[0],[0]
Compression ratio:,5.2 Experiments,[0],[0]
The three versions of our system (LSTM*) and the baseline (MIRA) have comparable compression ratios (CR) which are defined as the length of the compression in characters divided over the sentence length.,5.2 Experiments,[0],[0]
"Since the
3http://storage.googleapis.com/ sentencecomp/compressiondata.json
ratios are very close, a comparison of the systems’ scores is justified (Napoles et al., 2011).
",5.2 Experiments,[0],[0]
"Automatic evaluation: A total of 1,000 sentence pairs from the test set4 were used in the automatic evaluation.",5.2 Experiments,[0],[0]
"The results are summarized in Table 1.
",5.2 Experiments,[0],[0]
"There is a significant difference in performance of the MIRA baseline and the LSTM models, both in terms of F1-score and in accuracy.",5.2 Experiments,[0],[0]
More than 30% of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20% of MIRA.,5.2 Experiments,[0],[0]
"The differences in F-score between the three versions of LSTM are not significant, all scores are close to 0.81.
",5.2 Experiments,[0],[0]
"Evaluation with humans: The first 200 sentences from the set of 1,000 used in the automatic evaluation were compressed by each of the four systems.",5.2 Experiments,[0],[0]
"Every sentence-compression pair was rated by three raters who were asked to select a rating on a five-point Likert scale, ranging from one to five.",5.2 Experiments,[0],[0]
In very few cases (around 1%),5.2 Experiments,[0],[0]
"the ratings were inconclusive (i.e., 1, 3, 5 were given to the same pair) and had to be skipped.",5.2 Experiments,[0],[0]
"Table 2 summarizes the results.
",5.2 Experiments,[0],[0]
The results indicate that the LSTM models produce more readable and more informative compressions.,5.2 Experiments,[0],[0]
"Interestingly, there is no benefit in using the syntactic information, at least not with
4We used the very first 1,000 instances.
",5.2 Experiments,[0],[0]
the amount of parallel data we had at our disposal.,5.2 Experiments,[0],[0]
"The simple LSTM model which only uses token embeddings to generate a sequence of deletion decisions significantly outperforms the baseline which was given not only embeddings but also syntactic and other features.
",5.2 Experiments,[0],[0]
Discussion: What are the wins and losses of the LSTM systems?,5.2 Experiments,[0],[0]
Figure 4 presents some of the evaluated sentence-compression pairs.,5.2 Experiments,[0],[0]
"In terms of readability, the basic LSTM system performed surprisingly well.",5.2 Experiments,[0],[0]
Only in a few cases (out of 200) did it get an average score of two or three.,5.2 Experiments,[0],[0]
"Sentences which pose difficulty to the model are the ones with quotes, intervening commas, or other uncommon punctuation patterns.",5.2 Experiments,[0],[0]
"For example, in the second sentence in Figure 4, if one removes from the input the age modifiers and the preceding commas, the words and Chris Martin are not
dropped and the output compression is grammatical, preserving both conjoined elements.
",5.2 Experiments,[0],[0]
"With regard to informativeness, the difficult cases are those where there is very little to be removed and where the model still removed more than a half to achieve the compression ratio it observed in the training data.",5.2 Experiments,[0],[0]
"For example, the only part that can be removed from the fourth sentence in Figure 4 is the modifier of police, everything else being important content.",5.2 Experiments,[0],[0]
"Similarly, in the fifth sentence the context of the event must be retained in the compression for the event to be interpreted correctly.
",5.2 Experiments,[0],[0]
"Arguably, such cases would also be difficult for other systems.",5.2 Experiments,[0],[0]
"In particular, recognizing when the context is crucial is a problem that can be solved only by including deep semantic and discourse features which has not been attempted yet.",5.2 Experiments,[0],[0]
"And
sentences with quotes (direct speech, a song or a book title, etc.) are challenging for parsers which in turn provide important signals for most compression systems.
",5.2 Experiments,[0],[0]
The bottom of Figure 4 contains examples of good compressions.,5.2 Experiments,[0],[0]
"Even though for a significant number of input sentences the compression was a continuous subsequence of tokens, there are many discontinuous compressions.",5.2 Experiments,[0],[0]
"In particular, the LSTM model learned to drop appositions, no matter how long they are, temporal expressions, optional modifiers, introductory clauses, etc.
",5.2 Experiments,[0],[0]
"Our understanding of why the extended model (LSTM+PAR+PRES) performed worse in the human evlauation than the base model is that, in the absence of syntactic features, the basic LSTM learned a model of syntax useful for compression, while LSTM++, which was given syntactic information, learned to optimize for the particular way the ”golden” set was created (tree pruning).",5.2 Experiments,[0],[0]
"While the automatic evaluation penalized all deviations from the single golden variant, in human evals there was no penalty for readable alternatives.",5.2 Experiments,[0],[0]
"We presented, to our knowledge, a first attempt at building a competitive compression system which is given no linguistic features from the input.",6 Conclusions,[0],[0]
"The two important components of the system are (1) word embeddings, which can be obtained by anyone either pre-trained, or by running word2vec on a large corpus, and (2) an LSTM model which draws on the very recent advances in research on RNNs.",6 Conclusions,[0],[0]
"The training data of about two million sentence-compression pairs was collected automatically from the Internet.
",6 Conclusions,[0],[0]
Our results clearly indicate that a compression model which is not given syntactic information explicitly in the form of features may still achieve competitive performance.,6 Conclusions,[0],[0]
The high readability and informativeness scores assigned by human raters support this claim.,6 Conclusions,[0],[0]
"In the future, we are planning to experiment with more “interesting” paraphrasing models which translate the input not into a zero-one sequence but into words.",6 Conclusions,[0],[0]
"We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions.",abstractText,[0],[0]
"We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated.",abstractText,[0],[0]
We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features.,abstractText,[0],[0]
In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.,abstractText,[0],[0]
Sentence Compression by Deletion with LSTMs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2453",text,[0],[0]
Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency.,1 Introduction,[0],[0]
"The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001).
",1 Introduction,[0],[0]
"The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013).",1 Introduction,[0],[0]
"Irrespective of how
1Publicly available for download at https://github. com/Jmallins/MOSS
the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words.",1 Introduction,[0],[0]
"More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016).",1 Introduction,[0],[0]
Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997).,1 Introduction,[0],[0]
"Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g., part-of-speech tags, parse trees).",1 Introduction,[0],[0]
"In order to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller.",1 Introduction,[0],[0]
"For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products.",1 Introduction,[0],[0]
"Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences).",1 Introduction,[0],[0]
Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles.,1 Introduction,[0],[0]
"More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts).
",1 Introduction,[0],[0]
"2Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million.
",1 Introduction,[0],[0]
"Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015).",1 Introduction,[0],[0]
"As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics).",1 Introduction,[0],[0]
"And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions.
",1 Introduction,[0],[0]
Our work addresses the paucity of data for sentence compression models.,1 Introduction,[0],[0]
"We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements).",1 Introduction,[0],[0]
"We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016).",1 Introduction,[0],[0]
"Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.",1 Introduction,[0],[0]
"We also demonstrate that models trained on multilingual data perform well out-of-domain.
",1 Introduction,[0],[0]
"Although our approach does not employ compression corpora for training, for evaluation purposes, we create MOSS, a new Multilingual Compression dataset for English, French, and German.",1 Introduction,[0],[0]
"MOSS is a parallel corpus containing documents from the European parliament proceedings, TED talks, news commentaries, and the EU bookshop.",1 Introduction,[0],[0]
"Each document is written in English, French, and German, and compressed by native speakers of the respective language who process a document at a time.",1 Introduction,[0],[0]
"We obtain five compressions per document leading to 2,000 long-short sentence pairs per language.",1 Introduction,[0],[0]
"Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010)",1 Introduction,[0],[0]
"our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents.
",1 Introduction,[0],[0]
There has been relatively little interest in compressing languages other than English.,1 Introduction,[0],[0]
"A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese.",1 Introduction,[0],[0]
"There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language.",1 Introduction,[0],[0]
"Overall, there are no standardized datasets in languages other than English, either for training or testing.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is available in the bilingual parallel data; and the release of a multilingual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling.",1 Introduction,[0],[0]
"In our pivot-based sentence compression model an input sequence is first translated into a foreign language, and then back into the source language.",2 Pivot-based Neural Compression,[0],[0]
"Unlike previous paraphrasing pivoting models (Mallinson et al., 2017), we parameterize our translation models with a length feature, which allows us to produce compressed output.",2 Pivot-based Neural Compression,[0],[0]
"We define two models, performing compression in one step or alternatively in two steps which affords more flexibility in model output.",2 Pivot-based Neural Compression,[0],[0]
"In the neural encoder-decoder framework for MT (Bahdanau et al., 2015; Sutskever et al., 2014), an encoder takes in a source X =",2.1 NMT Background,[0],[0]
"(x1, ...,xTx) of length Tx and the decoder generates a target sequence (y1, ...,yTy) of length Ty.",2.1 NMT Background,[0],[0]
"Let hi be the hidden state of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, hi =",2.1 NMT Background,[0],[0]
[ −→ hi ; ←−,2.1 NMT Background,[0],[0]
hi ].,2.1 NMT Background,[0],[0]
"We deviate from previous work (Bahdanau et al., 2015; Sutskever et al., 2014) in that we initialize the decoder with the average of the hidden states, following Sennrich et al. (2017):
s0 = tanh(Winit ∑Txi=1",2.1 NMT Background,[0],[0]
"hi
Tx ) (1)
where Winit is a learnt parameter.",2.1 NMT Background,[0],[0]
"Our decoder is a conditional recurrent neural network, specifically a gated recurrent unit (GRU, Cho et al., 2014) with attention, which we denote as cGRUatt .",2.1 NMT Background,[0],[0]
"cGRUatt takes as input the previous hidden state s j−1, the source annotations C = h1, ...,hTx , and the previously decoded symbol y j−1 in order to update its hidden state s j, which is used to decode symbol y j at position j:
s j = cGRUatt(s j−1,y j−1,C)",2.1 NMT Background,[0],[0]
"(2)
cGRUatt consists of three components.",2.1 NMT Background,[0],[0]
The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an intermediate representation s′j.,2.1 NMT Background,[0],[0]
"The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s′j in order to compute the context vector c j:
c j = AT T (C,s′j) =",2.1 NMT Background,[0],[0]
"Tx
∑ i αi jhi (3)
",2.1 NMT Background,[0],[0]
"αi j = exp(ei j)
∑Txk=1 exp(ek j) (4)
ei j = f",2.1 NMT Background,[0],[0]
"(s′j,hi) (5)
Where αi j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.
",2.1 NMT Background,[0],[0]
"Finally, we generate s j, the hidden state of cGRUatt , by using the intermediate representation s′j and",2.1 NMT Background,[0],[0]
"the context vector c j. Given s j, y j−1, and c j",2.1 NMT Background,[0],[0]
"the output probability p(y j|s j,y j−1,c j) is computed using a feedforward neural network with a softmax activation.",2.1 NMT Background,[0],[0]
"We define the probability of sequence y as:
P(y|x;θ) =",2.1 NMT Background,[0],[0]
"Ty
∏ j=1 p(y j|s j,y j−1,c j) (6)",2.1 NMT Background,[0],[0]
"To be able to produce compressed sentences, we parameterize our model with a length vector which allows to control the output length.",2.2 Length Control,[0],[0]
"Our approach is similar to the LenInit model of Kikuchi et al. (2016), however we use a GRU instead of an LSTM.",2.2 Length Control,[0],[0]
"The hidden state of the decoder consists of the average of the encoder’s hidden states but also a length vector LV , a learnt parameter, which is scaled by the desired target length Ty′ .",2.2 Length Control,[0],[0]
"We therefore rewrite Equation (1) as follows:
s′0 = tanh ( Winit [∑Txi=1 hi
Tx ;LV ·Ty′
]) (7)
As such we now define our model as:
P(y|x,Ty′ ;θ) (8)
During training, the target length is set to Ty′ = Ty.",2.2 Length Control,[0],[0]
"However, at test time, the target length generally varies according to the domain, genre, and language at hand.",2.2 Length Control,[0],[0]
We determine the target length experimentally based on a small validation set.,2.2 Length Control,[0],[0]
"Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language.",2.3 Pivoting,[0],[0]
"The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016).
",2.3 Pivoting,[0],[0]
"We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form.",2.3 Pivoting,[0],[0]
"We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1, ..., fK}.",2.3 Pivoting,[0],[0]
"The probability of generating compression y = y1...yTy is decomposed as:
P(y|x) =",2.3 Pivoting,[0],[0]
"Fx
∑ f
P(y| f ; −→ θ ) ·P( f |x; ←− θ ) (9)
which we approximate as the tokenwise weighted average of the pivots:
P(y|x)≈ Ty
∏ j=1
Fx ∑ f P(y j|y< j, f )P( f |x) (10)
where y< j = y1, ...y j .",2.3 Pivoting,[0],[0]
"To ensure a probability distribution, we normalize the K-best list Fx, such that the translation probabilities sum to one.",2.3 Pivoting,[0],[0]
We use beam search to decode tokens by conditioning on multiple pivoting sentences.,2.3 Pivoting,[0],[0]
"The results with the best decoding scores are considered candidate compressions.
",2.3 Pivoting,[0],[0]
"To ensure the model produces compressed output, we extend the pivoting approach in two ways.",2.3 Pivoting,[0],[0]
"In single step compression, one of the translation
models is parameterized with length information:
P(y|x,Ty′)",2.3 Pivoting,[0],[0]
"≈ F
∑ f
P(y| f ,Ty′ ; −→ θ ) · P( f |x; ←− θ )
",2.3 Pivoting,[0],[0]
"In dual-step compression, we parameterize both translation models with length information:
P(y|x,Ty′ ,Ty′′)≈ F
∑ f
P(y| f ,Ty′ ;",2.3 Pivoting,[0],[0]
"−→ θ )·P( f |x,Ty′′ ; ←− θ )
",2.3 Pivoting,[0],[0]
"We find that dual-compression performs better when the system is expected to drastically compress the source sentence (e.g., in a headline generation task).",2.3 Pivoting,[0],[0]
Imposing a high compression ratio from the start tends to produce unintelligible text.,2.3 Pivoting,[0],[0]
"The model attempts to reduce the length of the source at all costs, even at the expense of being semantically faithful to the input.",2.3 Pivoting,[0],[0]
"Performing two moderate compressions in succession reduces both length and content conservatively and as a result produces more meaningful text.
",2.3 Pivoting,[0],[0]
In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions.,2.3 Pivoting,[0],[0]
We show the output of a single-step compression model on three languages initialized with varying compression rates3,2.3 Pivoting,[0],[0]
(see Section 4 for details on how the models were trained and tested).,2.3 Pivoting,[0],[0]
"The compression rate (CR) is used to determine length parameter of Equation (8):
Ty′ =",2.3 Pivoting,[0],[0]
"Tx ·CR (11)
",2.3 Pivoting,[0],[0]
"The figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source
3The term refers to the percentage of words retained from the source sentence in the compression.
language (Mallinson et al., 2017).",2.3 Pivoting,[0],[0]
We can see that the majority of sentences are generated with length close to the desired compression rate.,2.3 Pivoting,[0],[0]
"For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French.",3 The MOSS Dataset,[0],[0]
The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre.,3 The MOSS Dataset,[0],[0]
"We sampled five documents each from:
1.",3 The MOSS Dataset,[0],[0]
"Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment).
2.",3 The MOSS Dataset,[0],[0]
"The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues.
3.",3 The MOSS Dataset,[0],[0]
"The EU bookshop corpus (Skadiņš et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel.
4.",3 The MOSS Dataset,[0],[0]
"The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs).
",3 The MOSS Dataset,[0],[0]
We obtained compressions using the Crowdflower platform.,3 The MOSS Dataset,[0],[0]
Crowdworkers were given instructions that explained the task and defined sentence compression with the aid of examples.,3 The MOSS Dataset,[0],[0]
"They
were asked to compress while preserving the most important information, ensuring the sentences remained grammatical and meaning preserving.",3 The MOSS Dataset,[0],[0]
"Annotators were encouraged to use any rewriting operations that seemed appropriate, e.g., to delete words, add new words, substitute them, or reorder them.",3 The MOSS Dataset,[0],[0]
"Annotation proceeded on a document-bydocument basis, line-by-line.",3 The MOSS Dataset,[0],[0]
Crowdworkers compressed the first twenty lines of each document and we elicited five compression per document.,3 The MOSS Dataset,[0],[0]
"Example compressions are shown in Table 1.
",3 The MOSS Dataset,[0],[0]
Table 2 presents various statistics on our corpus.,3 The MOSS Dataset,[0],[0]
"As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between.",3 The MOSS Dataset,[0],[0]
"We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances.",3 The MOSS Dataset,[0],[0]
"Overall, French speakers seem more conservative when shortening sentences compared to English and German.",3 The MOSS Dataset,[0],[0]
"In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED).",3 The MOSS Dataset,[0],[0]
"We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure com-
monly used to automatically evaluate the quality of machine translation output.",3 The MOSS Dataset,[0],[0]
We used TER to compute the (average) number of edits required to change a long sentence to shorter output.,3 The MOSS Dataset,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences.",3 The MOSS Dataset,[0],[0]
We observe that crowdworkers perform a fair amount of rewriting across corpora and languages.,3 The MOSS Dataset,[0],[0]
"The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions.",3 The MOSS Dataset,[0],[0]
"Neural Machine Translation Training Nematus (Sennrich et al., 2017) was used as the machine translation system for all our experiments.",4 Experimental Setup,[0],[0]
We generally used the default settings and training procedures as specified within Nematus.,4 Experimental Setup,[0],[0]
"All networks have a hidden layer size of 1,000, and an embedding layer size of 512.",4 Experimental Setup,[0],[0]
"In addition, layer normalization (Ba et al., 2016) was used.",4 Experimental Setup,[0],[0]
"During training, we used ADAM (Kingma and Ba, 2014), a minibatch size of 80, and the training set was reshuffled between epochs.",4 Experimental Setup,[0],[0]
"We also employed early stopping.
",4 Experimental Setup,[0],[0]
"We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19).",4 Experimental Setup,[0],[0]
German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task.,4 Experimental Setup,[0],[0]
"The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively.",4 Experimental Setup,[0],[0]
"We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems.",4 Experimental Setup,[0],[0]
"The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007).",4 Experimental Setup,[0],[0]
"Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b).",4 Experimental Setup,[0],[0]
"The BPE operations are shared between language directions.
",4 Experimental Setup,[0],[0]
We experimented with various model variants using one or multiple pivots.,4 Experimental Setup,[0],[0]
The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs).,4 Experimental Setup,[0],[0]
"Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2.
",4 Experimental Setup,[0],[0]
"4BLEU scores were calculated using mteval-v13a.pl.
",4 Experimental Setup,[0],[0]
"Comparison Systems We compared our model against ABS, a sequence-to-sequence attentionbased model, developed by Rush et al. (2015).",4 Experimental Setup,[0],[0]
"This model was trained on a monolingual dataset extracted from the Annotated English Gigaword corpus (Napoles et al., 2011).",4 Experimental Setup,[0],[0]
The dataset consists of approximately 4 million pairs of the first sentence from each source document and its headline.,4 Experimental Setup,[0],[0]
"We also trained LenInit (Kikuchi et al., 2016) on the same corpus which is conceptually similar to ABS but additionally controls the output length using a length embedding vector (as described in Section 2.2).5",4 Experimental Setup,[0],[0]
"Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale.",4 Experimental Setup,[0],[0]
An obvious workaround is to translate Gigaword to French and German and then train compression models on the translated data.,4 Experimental Setup,[0],[0]
"As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German.",4 Experimental Setup,[0],[0]
"Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set.",4 Experimental Setup,[0],[0]
"MOSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006).",5 Results,[0],[0]
"These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1).",5 Results,[0],[0]
"We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples.
",5 Results,[0],[0]
Table 3(a) reports results on English with a model which controls the output length (L) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10).,5 Results,[0],[0]
We experimented with French (fr) or German (de) as pivot languages.,5 Results,[0],[0]
All pivot-based models perform compression in a single step (see Section 2.3).,5 Results,[0],[0]
"Dual-step compres-
5We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016).
",5 Results,[0],[0]
"6We add a begin-of-sentence marker at the start of the candidate and reference sentences.
",5 Results,[0],[0]
sion obtained inferior results which we omit for the sake of brevity.,5 Results,[0],[0]
"As can be seen, models which use a single pivot are better than those using multiple ones (German is better than French; see SPde vs SP f r).",5 Results,[0],[0]
"More pivots might introduce noise at the expense of translation quality.
",5 Results,[0],[0]
"Overall, pivot-based models outperform ABS and LenInit.",5 Results,[0],[0]
This is perhaps to be expected since these models are tested on out of domain data with different vocabulary and writing conventions; MOSS does not contain any newspaper articles.,5 Results,[0],[0]
"Unfortunately, it is not possible to train ABS and LenInt on in-domain data as compression data only exists for the headlines-first sentences pairs.",5 Results,[0],[0]
"As an upper bound, we also report how well humans agree with each other, treating one (randomly selected) reference as system output and computing how it agrees with the rest (row Gold in Table 3).",5 Results,[0],[0]
"All models lag significantly behind human performance on this task.
",5 Results,[0],[0]
"Tables 3(b) and 3(c) report results on French and German, respectively.",5 Results,[0],[0]
"For these languages, we obtained best results with English as pivot, using a single-step compression model.",5 Results,[0],[0]
"ABS and LenInit perform poorly when trained directly on translations of Gigaword into French and German; their performance improves considerably when they are trained on the Gigaword and used to compress English translations of French or German (ABSen, LenIniten).",5 Results,[0],[0]
"Again, we observe that our models (SPL ,en, MPL ,en) outperform the comparison systems across all metrics and that using a single pivot yields better compressions.",5 Results,[0],[0]
Example compressions are given in Table 4 where we show output produced by ABS and SP for each language (see the supplementary material for more examples).,5 Results,[0],[0]
"Finally, notice that automatic scores for the prefix baseline across languages are misleadingly high, since it simply repeats the source sentence up to a fixed length without performing any rewriting.
",5 Results,[0],[0]
We also elicited human judgments through the Crowdflower platform.,5 Results,[0],[0]
We asked crowdworkers to rate the grammaticality of the target compressions and whether they preserved the most important information from the source.,5 Results,[0],[0]
"In both cases, they used a five-point rating scale where a high number indicates better performance.",5 Results,[0],[0]
"We randomly selected 25 sentences from each corpus from the test portion of MOSS, i.e., 100 long-short sentence pairs per language.",5 Results,[0],[0]
"We compared compressions generated by our model (SPL ), with ABS models for the three languages, the prefix baseline, and (randomly selected) gold-standard reference (Ref) compressions from MOSS.",5 Results,[0],[0]
All systems used the length parameter to allow comparisons with approximately the same compression rates.,5 Results,[0],[0]
We collected five ratings per compression.,5 Results,[0],[0]
Our results are summarized in Table 5.,5 Results,[0],[0]
"We show mean ratings for grammaticality (Gram), importance (Imp) and their combination (column Avg).",5 Results,[0],[0]
"Across languages our model (SPL) significantly (p < 0.05) outperforms comparison systems (Pfix, ABS) on both dimensions of grammaticality and importance (significance tests were performed using a student t-test).",5 Results,[0],[0]
"All systems are significantly worse (p < 0.05) than the human reference compressions.
",5 Results,[0],[0]
"Finally, in Table 6 we analyze the output of our best model (SPL ) using the same statistics we applied to the human compressions (see Table 2).",5 Results,[0],[0]
"As can be seen, the model generally compressess more aggressively and applies more ed-
its than the crowdworkers (both compression rates and TER scores are higher for all three languages).",5 Results,[0],[0]
"Although the rate of deletions is similar to humans, insertions, substitutions and shifts happen to a greater extent for our model, indicating that it performs a good amount of paraphrasing.
",5 Results,[0],[0]
"DUC-2004 Evaluation Besides MOSS, we evaluated our model on the benchmark DUC-2004 task-1 dataset.",5 Results,[0],[0]
"In this task, the aim is to create a very short summary (75 bytes) for a document.",5 Results,[0],[0]
The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries.,5 Results,[0],[0]
"We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary.",5 Results,[0],[0]
"To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.
",5 Results,[0],[0]
Our results are shown in Table 7.,5 Results,[0],[0]
"To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE-L).9",5 Results,[0],[0]
We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants.,5 Results,[0],[0]
"We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword.",5 Results,[0],[0]
"ABS+ applies minimum error rate (MERT) training as a copy-
7Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35
8Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70
9We used ROUGE version 1.5.5 with the original DUC-2004 ROUGE parameters.
",5 Results,[0],[0]
ing mechanism.,5 Results,[0],[0]
"LenEmb and LenInit include a length parameter (Kikuchi et al., 2016), whereas RAS uses a specialized recurrent neural network architecture (Elman, 1990).",5 Results,[0],[0]
We also report how well DUC-2004 abstractors agree with each other (row Gold in Table 7).,5 Results,[0],[0]
"Example compressions are given in Table 8, where we show output produced by SPL ,de and a human reference (see the supplementary material for further examples).
",5 Results,[0],[0]
Using automatic metrics we see that our model generally performs worse compared to these systems and that German is the best pivot for English.,5 Results,[0],[0]
"Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is interesting to see that our model is able to compress out-of-domain.",5 Results,[0],[0]
"We do not have access to headline-first sentence pairs, while all comparison systems do.",5 Results,[0],[0]
We also elicited human judgments on the compressions of 100 lead sentences whose documents were randomly selected from the DUC-2004 test set.,5 Results,[0],[0]
"We compared the prefix baseline, our model (SPL ,de), ABS+ (Rush et al., 2015), LenEmb (Kikuchi et al., 2016), Topiary (Zajic et al., 2004), and a randomly selected reference.",5 Results,[0],[0]
Topiary came top in almost all measures in the DUC-2004 evaluation; it first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords.,5 Results,[0],[0]
"Crowdworkers rated grammaticality and importance, using a five-point scale; we collected five ratings per compression.
",5 Results,[0],[0]
As shown in Table 9 ABS+ has the lead with our system following suit.,5 Results,[0],[0]
"In terms of grammaticality, ABS+ and SPL ,de are not significantly different from the gold standard or from each other (Pfix, Topiary, and LenEmb are significantly worse than Gold; p < 0.05).",5 Results,[0],[0]
"In terms of importance, pairwise differences between systems and the gold standard are not significant.",5 Results,[0],[0]
"Overall, we observe that SPL ,de performs comparably to ABS+ even though it was
not trained on any compression specific data.",5 Results,[0],[0]
Inspection of system output reveals that our model performs more paraphrasing than comparison systems (a conclusion also confirmed by the statistics in Table 6).,5 Results,[0],[0]
In this paper we have shown that multilingual corpora can be used to bootstrap compression models across languages and text genres.,6 Conclusions,[0.9557622803635702],['We anticipate that methods described in this paper can be used to similarly train smaller models in other domains.']
Our approach adapts existing neural machine translation machinery to the compression task coupled with methods which decode the output to a desired length.,6 Conclusions,[0],[0]
"An interesting direction for future work would be to train our model using reinforcement learning (Ranzato et al., 2016; Zhang and Lapata, 2017) in order to control the compression output more directly.",6 Conclusions,[0],[0]
"Moreover, although we do not use any direct supervision in our experiments, it would be interesting to incorporate it as a means of domain adaptation (Cheng et al., 2016).
",6 Conclusions,[0],[0]
Acknowledgments The authors gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1; Mallinson) and the European Research Council (award number 681760; Lapata).,6 Conclusions,[0],[0]
In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models.,abstractText,[0],[0]
Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length.,abstractText,[0],[0]
Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data.,abstractText,[0],[0]
"We release1 MOSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.",abstractText,[0],[0]
Sentence Compression for Arbitrary Languages via Multilingual Pivoting,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning.",1 Introduction,[0],[0]
"The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).",1 Introduction,[0],[0]
"For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).",1 Introduction,[0],[0]
"Automatic simplification would also benefit people with low-literacy skills (Watanabe et al., 2009), such as children and
1Our code and data are publicly available at https:// github.com/XingxingZhang/dress.
non-native speakers as well as individuals with autism (Evans et al., 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al., 2013).
",1 Introduction,[0],[0]
"The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014).",1 Introduction,[0],[0]
Earlier work focused on individual aspects of the simplification problem.,1 Introduction,[0],[0]
"For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).
",1 Introduction,[0],[0]
Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation.,1 Introduction,[0],[0]
Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia.,1 Introduction,[0],[0]
"For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting).",1 Introduction,[0],[0]
"Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.",1 Introduction,[0],[0]
"Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs.",1 Introduction,[0],[0]
"During inference, the K-best outputs of the PBMT model are reranked according
584
to their dis-similarity to the (complex) input sentence.",1 Introduction,[0],[0]
The hybrid model developed in Narayan and Gardent (2014) also operates in two phases.,1 Introduction,[0],[0]
"Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007).",1 Introduction,[0],[0]
The resulting sentences are further simplified by a model similar to Wubben et al. (2012).,1 Introduction,[0],[0]
"Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.
",1 Introduction,[0],[0]
"In this paper we propose a simplification model which draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014).",1 Introduction,[0],[0]
Central to this approach is an encoderdecoder architecture implemented by recurrent neural networks.,1 Introduction,[0],[0]
The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence.,1 Introduction,[0],[0]
"Although our model uses the encoder-decoder architecture as its backbone, it must also meet constraints imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical.",1 Introduction,[0],[0]
"To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints.",1 Introduction,[0],[0]
"Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016).
",1 Introduction,[0],[0]
"We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b).",1 Introduction,[0],[0]
We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets.,1 Introduction,[0],[0]
"We will first define a basic encoder-decoder model for sentence simplification and then explain how to embed it in a reinforcement learning
framework.",2 Neural Encoder-Decoder Model,[0],[0]
Given a (complex) source sentence X =,2 Neural Encoder-Decoder Model,[0],[0]
"(x1, x2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", x|X|), our model learns to predict its simplified target Y = (y1, y2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", y|Y |).",2 Neural Encoder-Decoder Model,[0],[0]
"Inferring the target Y given the sourceX is a typical sequence to sequence learning problem, which can be modeled with attention-based encoderdecoder models (Bahdanau et al., 2015; Luong et al., 2015).",2 Neural Encoder-Decoder Model,[0],[0]
"Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve splitting operations.",2 Neural Encoder-Decoder Model,[0],[0]
"For example, a long source sentence (In 1883, Faur married Marie Fremiet, with whom he had two sons.)",2 Neural Encoder-Decoder Model,[0],[0]
"can be simplified as two sentences (In 1883, Faur married Marie Fremiet.",2 Neural Encoder-Decoder Model,[0],[0]
They had two sons.).,2 Neural Encoder-Decoder Model,[0],[0]
"Nevertheless, we still view the target as a sequence, i.e., two or more sequences concatenated with full stops.
",2 Neural Encoder-Decoder Model,[0],[0]
The encoder-decoder model has two parts (see left hand side in Figure 1).,2 Neural Encoder-Decoder Model,[0],[0]
"The encoder transforms the source sentence X into a sequence of hidden states (hS1 ,h S 2 , . . .",2 Neural Encoder-Decoder Model,[0],[0]
",h S |X|)",2 Neural Encoder-Decoder Model,[0],[0]
"with a Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word yt+1 at a time in the simplified target Y .",2 Neural Encoder-Decoder Model,[0],[0]
"Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct, which encodes the source sentence:
P (Y |X) = |Y |∏ t=1 P (yt|y1:t−1, X) (1)
P (yt+1|y1:t, X) = softmax(g(hTt , ct))",2 Neural Encoder-Decoder Model,[0],[0]
"(2)
where g(·) is a one-hidden-layer neural network with the following parametrization:
g(hTt , ct) =",2 Neural Encoder-Decoder Model,[0],[0]
"Wo tanh(Uhh T t + Whct) (3)
where Wo ∈ R|V |×d, Uh ∈ Rd×d, and Wh ∈ Rd×d; |V",2 Neural Encoder-Decoder Model,[0],[0]
| is the output vocabulary size and d the hidden unit size.,2 Neural Encoder-Decoder Model,[0],[0]
"hTt is the hidden state of the decoder LSTM which summarizes y1:t, i.e., what has been generated so far:
hTt = LSTM(yt,h T t−1) (4)
",2 Neural Encoder-Decoder Model,[0],[0]
"The dynamic context vector ct is the weighted sum of the hidden states of the source sentence:
ct = |X|∑ i=1",2 Neural Encoder-Decoder Model,[0],[0]
"αtihSi (5)
whose weights αti are determined by an attention mechanism:
αti = exp(hTt · hSi )∑ i exp(h T t · hSi )
(6)
where · is the dot product between two vectors.",2 Neural Encoder-Decoder Model,[0],[0]
We use the dot product here mainly for efficiency reasons; alternative ways to compute attention scores have been proposed in the literature and we refer the interested reader to Luong et al. (2015).,2 Neural Encoder-Decoder Model,[0],[0]
The model sketched above is usually trained by minimizing the negative log-likelihood of the training source-target pairs.,2 Neural Encoder-Decoder Model,[0],[0]
"In this section we present DRESS, our Deep REinforcement Sentence Simplification model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence simplification.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Although a number of rewrite operations (e.g., copying, deletion, substitution, word reordering) can be used to simplify text, copying is by far the most common.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
We empirically found that 73% of the target words are copied from the source in the Newsela dataset.,3 Reinforcement Learning for Sentence Simplification,[0],[0]
This number further increases to 83% when considering Wikipedia-based datasets (we provide details on these datasets in Section 5).,3 Reinforcement Learning for Sentence Simplification,[0],[0]
"As a result, a generic encoder-decoder model learns to copy all too well at the expense of other rewrite operations, often parroting back the source or making only a few trivial changes.
",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"To encourage a wider variety of rewrite operations while remaining fluent and faithful to the meaning of the source, we employ a reinforcement learning framework (see Figure 1).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"We view the encoder-decoder model as an agent which first reads the source sentence X; then at each step, it takes an action ŷt ∈ V (where V is the output vocabulary) according to a policy PRL(ŷt|ŷ1:t−1, X) (see Equation (2)).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The agent continues to take actions until it produces an End Of Sentence (EOS) token yielding the action sequence Ŷ = (ŷ1, ŷ2, . . .",3 Reinforcement Learning for Sentence Simplification,[0],[0]
", ŷ|Ŷ |), which is also the simplified output of our model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"A reward r is then received and the REINFORCE algorithm (Williams, 1992) is used to update the agent.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"In the following, we first introduce our reward and then present the details of the REINFORCE algorithm.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The reward r(Ŷ ) for system output Ŷ is the weighted sum of the three components aimed at capturing key aspects of the target output, namely simplicity, relevance, and fluency:
r(Ŷ ) = λS rS + λR rR + λF",3.1 Reward,[0],[0]
"rF (7)
where λS , λR, λF ∈",3.1 Reward,[0],[0]
"[0, 1]; r(Ŷ ) is a shorthand for r(X,Y, Ŷ )",3.1 Reward,[0],[0]
"whereX is the source, Y the reference (or target), and Ŷ the system output.",3.1 Reward,[0],[0]
"rS , rR, and rF are shorthands for simplicity rS(X,Y, Ŷ ), relevance rR(X, Ŷ ), and fluency rF (Ŷ ).",3.1 Reward,[0],[0]
"We provide details for each reward summand below.
",3.1 Reward,[0],[0]
"Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence.",3.1 Reward,[0],[0]
"SARI is the arithmetic average of n-gram precision and recall of three rewrite operations: addition, copying, and deletion.",3.1 Reward,[0],[0]
It rewards addition operations where system output was not in the input but occurred in the references.,3.1 Reward,[0],[0]
"Analogously, it rewards words retained/deleted in both the system output and the references.",3.1 Reward,[0],[0]
"In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.
",3.1 Reward,[0],[0]
One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification.,3.1 Reward,[0],[0]
"Xu et al. (2016) provide eight references for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training.",3.1 Reward,[0],[0]
The majority of existing simplification datasets (see Section 5 for details) have a single reference for each source sentence.,3.1 Reward,[0],[0]
"Moreover, they are unavoidably noisy as they are mostly constructed automatically, e.g., by aligning sentences from the ordinary and simple English Wikipedias.",3.1 Reward,[0],[0]
"When relying solely on a single reference, SARI will try to reward accidental",3.1 Reward,[0],[0]
n-grams that should never have occurred in it.,3.1 Reward,[0],[0]
"To countenance the effect of noise, we apply SARI(X, Ŷ , Y ) in the expected direction, with X as the source, Ŷ the system output, and Y the reference as well as in the reverse direction with Y as the system output and Ŷ as the reference.",3.1 Reward,[0],[0]
"Assuming our system can produce reasonably good simplifications, by swapping the output
and the reference, reverse SARI can be used to estimate how good a reference is with respect to the system output.",3.1 Reward,[0],[0]
"Our first reward is therefore the weighted sum of SARI and reverse SARI:
rS=β SARI(X, Ŷ , Y )+(1−β)",3.1 Reward,[0],[0]
"SARI(X,Y, Ŷ ) (8)
Relevance",3.1 Reward,[0],[0]
"While the simplicity-based reward rS tries to encourage the model to make changes, the relevance reward rR ensures that the generated sentences preserve the meaning of the source.",3.1 Reward,[0],[0]
We use an LSTM sentence encoder to convert the source X and the predicted target Ŷ into two vectors qX and qŶ .,3.1 Reward,[0],[0]
"The relevance reward rR is simply the cosine similarity between these two vectors:
rR = cos(qX ,qŶ )",3.1 Reward,[0],[0]
= qX · qŶ ||qX || ||qŶ,3.1 Reward,[0],[0]
"||
(9)
",3.1 Reward,[0],[0]
We use a sequence auto-encoder (SAE; Dai and Le 2015) to train the LSTM sentence encoder on both the complex and simple sentences.,3.1 Reward,[0],[0]
"Specifically, the SAE uses sentence X =",3.1 Reward,[0],[0]
"(x1, . . .",3.1 Reward,[0],[0]
", x|X|) to infer itself via an encoder-decoder model (without an attention mechanism).",3.1 Reward,[0],[0]
"Firstly, an encoder LSTM convertsX into a sequence of hidden states (h1, . . .",3.1 Reward,[0],[0]
",h|X|).",3.1 Reward,[0],[0]
"Then, we use h|X| to initialize the hidden state of the decoder LSTM and recover/generate X one word at a time.
",3.1 Reward,[0],[0]
"Fluency Xu et al. (2016) observe that SARI correlates less with fluency compared to other metrics such as BLEU (Papineni et al., 2002).",3.1 Reward,[0],[0]
The fluency reward rF models the well-formedness of the generated sentences explicitly.,3.1 Reward,[0],[0]
"It is the normalized sentence probability assigned by an LSTM
language model trained on simple sentences:
rF = exp  1 |Ŷ | |Ŷ |∑ i=1",3.1 Reward,[0],[0]
"logPLM (ŷi|ŷ0:i−1)  (10)
",3.1 Reward,[0],[0]
We take the exponential of Ŷ ’s perplexity to ensure that rF ∈,3.1 Reward,[0],[0]
"[0, 1] as is the case with rS and rR.",3.1 Reward,[0],[0]
The goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The training loss for one sequence is its negative expected reward:
L(θ) = −E(ŷ1,...,ŷ|Ŷ |)∼PRL(·|X)[r(ŷ1, . .",3.2 The REINFORCE Algorithm,[0],[0]
"., ŷ|Ŷ",3.2 The REINFORCE Algorithm,[0],[0]
"|)]
where PRL is our policy, i.e., the distribution produced by the encoder-decoder model (see Equation(2)) and r(·) is the reward function of an action sequence Ŷ = (ŷ1, . . .",3.2 The REINFORCE Algorithm,[0],[0]
", ŷ|Ŷ |), i.e., a generated simplification.",3.2 The REINFORCE Algorithm,[0],[0]
"Unfortunately, computing the expectation term is prohibitive, since there is an infinite number of possible action sequences.",3.2 The REINFORCE Algorithm,[0],[0]
"In practice, we approximate this expectation with a single sample from the distribution of PLR(·|X).",3.2 The REINFORCE Algorithm,[0],[0]
We refer to Williams (1992) for the full derivation of the gradients.,3.2 The REINFORCE Algorithm,[0],[0]
The gradient of L(θ) is: ∇L(θ) ≈∑|Ŷ,3.2 The REINFORCE Algorithm,[0],[0]
"|
t=1∇ logPRL(ŷt|ŷ1:t−1, X)[r(ŷ1:|Ŷ |)− bt]
To reduce the variance of gradients, we also introduce a baseline linear regression model bt to estimate the expected future reward at time t (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
bt takes the concatenation of hTt and ct as input and outputs a real value as the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The parameters of the regressor are
trained by minimizing mean squared error.",3.2 The REINFORCE Algorithm,[0],[0]
"We do not back-propagate this error to hTt or ct during training (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
"Presented in its original form, the REINFORCE algorithm starts learning with a random policy.",3.3 Learning,[0],[0]
"This assumption can make model training challenging for generation tasks like ours with large vocabularies (i.e., action spaces).",3.3 Learning,[0],[0]
"We address this issue by pre-training our agent (i.e., the encoderdecoder model) with a negative log-likelihood objective (see Section 2), making sure it can produce reasonable simplifications, thereby starting off with a policy which is better than random.",3.3 Learning,[0],[0]
"We follow prior work (Ranzato et al., 2016) in adopting a curriculum learning strategy.",3.3 Learning,[0],[0]
"In the beginning of training, we give little freedom to our agent allowing it to predict the last few words for each target sentence.",3.3 Learning,[0],[0]
"For every target sequence, we use negative log-likelihood to train the first L (initially, L = 24) tokens and apply the reinforcement learning algorithm to the (L + 1)th tokens onwards.",3.3 Learning,[0],[0]
"Every two epochs, we set L = L− 3 and the training terminates when L is 0.",3.3 Learning,[0],[0]
"Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification (Specia et al., 2012).",4 Lexical Simplification,[0],[0]
The model presented so far learns lexical substitution and other rewrite operations jointly.,4 Lexical Simplification,[0],[0]
"In some cases, words are predicted because they seem natural in the their context, but are poor substitutes for the content of the complex sentence.",4 Lexical Simplification,[0],[0]
"To countenance this, we learn lexical simplifications explicitly and integrate them with our reinforcement learning-based model.
",4 Lexical Simplification,[0],[0]
"We use an pre-trained encoder-decoder model (which is trained on a parallel corpus of complex and simple sentences) to obtain probabilistic word alignments, aka attention scores (see αt in Equation (6)).",4 Lexical Simplification,[0],[0]
Let X =,4 Lexical Simplification,[0],[0]
"(x1, x2, . . .",4 Lexical Simplification,[0],[0]
", x|X|) denote a source sentence and Y = (y1, y2, . . .",4 Lexical Simplification,[0],[0]
", y|Y |) a target sentence.",4 Lexical Simplification,[0],[0]
"We convert X into |X| hidden states (v1,v2, . . .",4 Lexical Simplification,[0],[0]
",v|X|) with an LSTM.",4 Lexical Simplification,[0],[0]
Note that vt ∈ Rd×1 corresponds to the context dependent representation of xt.,4 Lexical Simplification,[0],[0]
"Let αt denote the alignment scores αt1, αt2, . . .",4 Lexical Simplification,[0],[0]
", αt|X|.",4 Lexical Simplification,[0],[0]
"The lexical simplification probability of yt given the source sentence
and the alignment scores is:
PLS(yt|X,αt) =",4 Lexical Simplification,[0],[0]
"softmax(Wl st) (11)
where Wl ∈ R|V |×d and st represents the source:
st = |X|∑ i=1",4 Lexical Simplification,[0],[0]
"αtivi (12)
",4 Lexical Simplification,[0],[0]
"The lexical simplification model on its own encourages lexical substitutions, without taking into account what has been generated so far (i.e., y1:t−1) and as a result fluency could be compromised.",4 Lexical Simplification,[0],[0]
"A straightforward solution is to integrate lexical simplification with our reinforcement learning trained model (Section 3) using linear interpolation, where η ∈",4 Lexical Simplification,[0],[0]
"[0, 1]:
P (yt|y1:t−1, X) = (1− η)PRL(yt|y1:t−1, X) +",4 Lexical Simplification,[0],[0]
"η PLS(yt|X,αt) (13)",4 Lexical Simplification,[0],[0]
In this section we present our experimental setup for assessing the performance of the simplification model described above.,5 Experimental Setup,[0],[0]
"We give details on our datasets, model training, evaluation protocol, and the systems used for comparison.
",5 Experimental Setup,[0],[0]
Datasets We conducted experiments on three simplification datasets.,5 Experimental Setup,[0],[0]
"WikiSmall (Zhu et al., 2010) is a parallel corpus which has been extensively used as a benchmark for evaluating text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).",5 Experimental Setup,[0],[0]
It contains automatically aligned complex and simple sentences from the ordinary and simple English Wikipedias.,5 Experimental Setup,[0],[0]
The test set consists of 100 complex-simple sentence pairs.,5 Experimental Setup,[0],[0]
"The training set contains 89,042 sentence pairs (after removing duplicates and test sentences).",5 Experimental Setup,[0],[0]
"We randomly sampled 205 pairs for development and used the remaining sentences for training.
",5 Experimental Setup,[0],[0]
"We also constructed WikiLarge, a larger Wikipedia corpus by combining previously created simplification corpora.",5 Experimental Setup,[0],[0]
"Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu’s (2010) WikiSmall dataset described above.",5 Experimental Setup,[0],[0]
We used the development and test sets created in Xu et al. (2016).,5 Experimental Setup,[0],[0]
These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers.,5 Experimental Setup,[0],[0]
"The dataset
contains 8 (reference) simplifications for 2,359 sentences partitioned into 2,000 for development and 359 for testing.",5 Experimental Setup,[0],[0]
"After removing duplicates and sentences in development and test sets, the resulting training set contains 296,402 sentence pairs.
",5 Experimental Setup,[0],[0]
"Our third dataset is Newsela, a corpus collated by Xu et al. (2015b) who argue that Wikipediabased resources are suboptimal due to the automatic sentence alignment which unavoidably introduces errors, and their uniform writing style which leads to systems that generalize poorly.",5 Experimental Setup,[0],[0]
"Newsela2 consists of 1,130 news articles, each rewritten four times by professional editors for children at different grade levels (0 is the most complex level and 4 is simplest).",5 Experimental Setup,[0],[0]
Xu et al. (2015b) provide multiple aligned complex-simple pairs within each article.,5 Experimental Setup,[0],[0]
"We removed sentence pairs corresponding to levels 0–1, 1–2, and 2–3, since they were too similar to each other.",5 Experimental Setup,[0],[0]
"The first 1,070 documents were used for training (94,208 sentence pairs), the next 30 documents for development (1,129 sentence pairs) and the last 30 documents for testing (",5 Experimental Setup,[0],[0]
"1,076 sentence pairs).3 We are not aware of any published results on this dataset.
",5 Experimental Setup,[0],[0]
Training Details,5 Experimental Setup,[0],[0]
We trained our models on an Nvidia GPU card.,5 Experimental Setup,[0],[0]
We used the same hyperparameters across datasets.,5 Experimental Setup,[0],[0]
"We first trained an encoder-decoder model, and then performed reinforcement learning training (Section 3), and trained the lexical simplification model (Section 4).",5 Experimental Setup,[0],[0]
"Encoder-decoder parameters were uniformly initialized to [−0.1, 0.1].",5 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.001; the first momentum coefficient was set to 0.9 and the second momentum coefficient to 0.999.",5 Experimental Setup,[0],[0]
"The gradient was rescaled when the norm exceeded 5 (Pascanu et al., 2013).",5 Experimental Setup,[0],[0]
Both encoder and decoder LSTMs have two layers with 256 hidden neurons in each layer.,5 Experimental Setup,[0],[0]
"We regularized all LSTMs with a dropout rate of 0.2 (Zaremba et al., 2014).",5 Experimental Setup,[0],[0]
"We initialized the encoder and decoder word embedding matrices with 300 dimensional Glove vectors (Pennington et al., 2014).
",5 Experimental Setup,[0],[0]
"During reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01.",5 Experimental Setup,[0],[0]
"We set β = 0.1, λS = 1, λR = 0.25 and λF = 0.5.4 Training details for the lexical
2https://newsela.com 3If a sentence has multiple references in the development or test set, we use the reference with highest simplicity level.",5 Experimental Setup,[0],[0]
"4Weights were tuned on the development set of the Newsela dataset and kept fixed for the other two datasets.
simplification model are identical to the encoderdecoder model except that word embedding matrices were randomly initialized.",5 Experimental Setup,[0],[0]
"The weight of the lexical simplification model was set to η = 0.1.
",5 Experimental Setup,[0],[0]
"To reduce vocabulary size, named entities were tagged with the Stanford CoreNLP (Manning et al., 2014) and anonymized with a NE@N token, where NE ∈ {PER,LOC,ORG,MISC} and N indicates NE@N is the N -th distinct NE typed entity.",5 Experimental Setup,[0],[0]
"For example, “John and Bob are . . .",5 Experimental Setup,[0],[0]
” becomes “PER@1 and PER@2 are . . .,5 Experimental Setup,[0],[0]
”.,5 Experimental Setup,[0],[0]
"At test time, we de-anonymize NE@N tokens in the output by looking them up in their source sentences.",5 Experimental Setup,[0],[0]
"Note that the de-anonymization may fail, but the chance is small (around 2% of the time on the Newsela development set).",5 Experimental Setup,[0],[0]
We replaced words occurring three times or less in the training set with UNK.,5 Experimental Setup,[0],[0]
"At test time, when our models predict UNK, we adopt the UNK replacement method proposed in Jean et al. (2015).
",5 Experimental Setup,[0],[0]
"Evaluation Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016)",5 Experimental Setup,[0],[0]
we evaluated system output automatically adopting metrics widely used in the simplification literature.,5 Experimental Setup,[0],[0]
"Specifically, we used BLEU5 (Papineni et al., 2002) to assess the degree to which generated simplifications differed from gold standard references and the Flesch-Kincaid Grade Level index (FKGL; Kincaid et al. 1975) to measure the readability of the output (lower FKGL6 implies simpler output).",5 Experimental Setup,[0],[0]
"In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and reference simplifications.7 BLEU, FKGL, and SARI are all measured at corpus-level.",5 Experimental Setup,[0],[0]
We also evaluated system output by eliciting human judgments via Amazon’s Mechanical Turk.,5 Experimental Setup,[0],[0]
"Specifically (selfreported) native English speakers were asked to rate simplifications on three dimensions: Fluency (is the output grammatical and well formed?),",5 Experimental Setup,[0],[0]
Adequacy (to what extent is the meaning expressed in the original sentence preserved in the output?) and Simplicity (is the output simpler than the original sentence?).,5 Experimental Setup,[0],[0]
"All ratings were obtained using a five point Likert scale.
",5 Experimental Setup,[0],[0]
Comparison Systems We compared our model against several systems previously proposed in the literature.,5 Experimental Setup,[0],[0]
"These include PBMT-R, a mono-
5With the default mtevalv13a.pl settings.",5 Experimental Setup,[0],[0]
6FKGL implementation at http://goo.gl/OHP7k3.,5 Experimental Setup,[0],[0]
"7We used he implementation of SARI in Xu et al. (2016).
",5 Experimental Setup,[0],[0]
"lingual phrase-based machine translation system with a reranking post-processing step8 (Wubben et al., 2012) and Hybrid, a model which first performs sentence splitting and deletion operations over discourse representation structures and then further simplifies sentences with PBMT-R (Narayan and Gardent, 2014).",5 Experimental Setup,[0],[0]
Hybrid9 is state of the art on the WikiSmall dataset.,5 Experimental Setup,[0],[0]
"Comparisons with SBMT-SARI, a syntax-based translation model trained on PPDB (Ganitkevitch et al., 2013) and tuned with SARI (Xu et al., 2016), are problematic due to the size of PPDB which is considerably larger than any of the datasets used in this work (it contains 106 million sentence pairs with 2 billion words).",5 Experimental Setup,[0],[0]
"Nevertheless, we compare10 against SBMT-SARI, but only models trained on Wikilarge, our largest dataset.",5 Experimental Setup,[0],[0]
"Since Newsela contains high quality simplifications created by professional editors, we performed the bulk of our experiments on this dataset.",6 Results,[0],[0]
"Specifically, we set out to answer two questions: (a) which neural model performs best and (b) how do neural models which are resource lean and do not have access to linguistic annotations fare against more traditional systems.",6 Results,[0],[0]
"We therefore compared the basic attention-based encoder-
8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).
",6 Results,[0],[0]
"9We are grateful to Shashi Narayan for running his system on our three datasets.
",6 Results,[0],[0]
"10The output of SBMT-SARI is publicly available.
",6 Results,[0],[0]
"decoder model (EncDecA), with the deep reinforcement learning model (DRESS; Section 3), and a linear combination of DRESS and the lexical simplification model (DRESS-LS; Section 4).",6 Results,[0],[0]
"Neural models were further compared against two strong baselines, PBMT-R and Hybrid.",6 Results,[0],[0]
"Table 3 shows example output of all models on the Newsela dataset.
",6 Results,[0],[0]
The top block in Table 1 summarizes the results of our automatic evaluation.,6 Results,[0],[0]
"As can be seen, all neural models obtain higher BLEU, lower FKGL and higher SARI compared to PBMT-R. Hybrid has the lowest FKGL and highest SARI.",6 Results,[0],[0]
"Compared to EncDecA, DRESS scores lower on FKGL and higher on SARI, which indicates that the model has indeed learned to optimize the reward function which includes SARI.",6 Results,[0],[0]
"Integrating lexical simplification (DRESS-LS) yields better BLEU, but slightly worse FKGL and SARI.
",6 Results,[0],[0]
The results of our human evaluation are presented in the top block of Table 2.,6 Results,[0],[0]
We elicited judgments for 100 randomly sampled test sentences.,6 Results,[0],[0]
"Aside from comparing system output (PBMT-R, Hybrid, EncDecA, DRESS, and DRESS-LS), we also elicited ratings for the gold standard Reference as an upper bound.",6 Results,[0],[0]
"We report results for Fluency, Adequacy, and Simplicity individually and in combination (All is the average rating of the three dimensions).",6 Results,[0],[0]
"As can be seen, DRESS and DRESS-LS outperform PBMT-R and
Hybrid on Fluency, Simplicity, and overall.",6 Results,[0],[0]
"The fact that neural models (EncDecA, DRESS and DRESS-LS) fare well on Fluency, is perhaps not surprising given the recent success of LSTMs in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).
",6 Results,[0],[0]
Neural models obtain worse ratings on Adequacy but are closest to the human references on this dimension.,6 Results,[0],[0]
"DRESS-LS (and DRESS) are significantly better (p < 0.01) on Simplicity than EncDecA, PBMT-R, and Hybrid which indicates that our reinforcement learning based model is effective at creating simpler output.",6 Results,[0],[0]
Combined ratings (All) for DRESS-LS are significantly different compared to the other models but not to DRESS and the Reference.,6 Results,[0],[0]
"Nevertheless, integration of the lexical simplification model boosts performance as ratings increase almost across the board (Simplicity is slightly worse).",6 Results,[0],[0]
"Returning to our original questions, we find that neural models are more fluent than comparison systems, while performing non-trivial rewrite operations (see the SARI
scores in Table 1) which yield simpler output (see the Simplicity column in Table 2).",6 Results,[0],[0]
"Based on our judgment elicitation study, neural models trained with reinforcement learning perform best, with DRESS-LS having a slight advantage.
",6 Results,[0],[0]
We further analyzed model performance by computing various statistics on the simplified output.,6 Results,[0],[0]
We measured average sentence length and the degree to which DRESS and comparison systems perform rewriting operations.,6 Results,[0],[0]
"We approximated the latter with Translation Error Rate (TER; Snover et al. 2006), a measure commonly used to automatically evaluate the quality of machine translation output.",6 Results,[0],[0]
We used TER to compute the (average) number of edits required to change an original complex sentence to simpler output.,6 Results,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences.
",6 Results,[0],[0]
"As shown in Table 4, Hybrid obtains the highest TER, followed by our models (DRESS and
DRESS-LS), which indicates that they actively perform rewriting.",6 Results,[0],[0]
"Perhaps Hybrid is too aggressive when simplifying a sentence, it obtains low Fluency and Adequacy scores in human evaluation (Table 2).",6 Results,[0],[0]
"There is a strong correlation between sentence length and number of deletion operations (i.e., more deleteions lead to shorter sentences) and PBMT-R performs very few deletions.",6 Results,[0],[0]
"Overall, reinforcement learning encourages deletion (see DRESS and DRESS-LS), while performing a reasonable amount of additional operations (e.g., substitutions and shifts) compared to EncDecA and PBMT-R.
The middle blocks in Tables 1 and 2 report results on the WikiSmall dataset.",6 Results,[0],[0]
FKGL and SARI follow a similar pattern as on Newsela.,6 Results,[0],[0]
"BLEU scores for PBMT-R, Hybrid, and EncDecA are much higher compared to DRESS and DRESS-LS.",6 Results,[0],[0]
"Hybrid obtains best BLEU and SARI scores, while DRESS and DRESS-LS do very well on FKGL.",6 Results,[0],[0]
"In human evaluation, we elicited judgments on the entire WikiSmall test set (100 sentences).",6 Results,[0],[0]
"We compared DRESS-LS, with PBMT-R, Hybrid, and gold standard Reference simplifications.",6 Results,[0],[0]
"As human experiments are time consuming and expensive, we did not include other neural models besides DRESS-LS based on our Newsela study which showed that EncDecA is inferior to variants trained with reinforcement learning and that DRESS-LS is the better performing model (however, we do compare all models in Table 1).",6 Results,[0],[0]
"DRESS-LS is significantly better on Simplicity than PBMT-R, Hybrid, and the Reference.",6 Results,[0],[0]
It performs on par with PBMT-R on Fluency and worse on Adequacy (but still closer to the human Reference than PBMT-R or Hybrid).,6 Results,[0],[0]
"When combining all ratings (All in Table 2), DRESS-LS is significantly better than PBMT-R, Hybrid, and the Reference.
",6 Results,[0],[0]
The bottom blocks in Tables 1 and 2 report results on Wikilarge.,6 Results,[0],[0]
"We compared our models with PBMT-R, Hybrid, and SBMT-SARI (Xu et al., 2016).",6 Results,[0],[0]
The FKGL follows a similar pattern as in the previous datasets.,6 Results,[0],[0]
"PBMT-R and our models are best in terms of BLEU while SBMT-SARI outperforms all other systems on SARI.11 Because there are 8 references for each complex sentence in the test set, BLEU scores are much higher compared to Newsela and WikiSmall.",6 Results,[0],[0]
"In human evaluation, we again elicited judgments for 100 randomly sampled test sentences.",6 Results,[0],[0]
We randomly selected one of the 8 references as the Reference upper bound.,6 Results,[0],[0]
"On Simplicity, DRESS-LS is significantly better than all comparison systems, except Hybrid.",6 Results,[0],[0]
"On Adequacy, it is better than Hybrid but significantly worse than other comparison systems.",6 Results,[0],[0]
"On Fluency, it is on par with PBMT-R12 but better than Hybrid and SBMT-SARI.",6 Results,[0],[0]
On All dimension DRESS-LS significantly outperforms all comparison systems.,6 Results,[0],[0]
"We developed a reinforcement learning-based text simplification model, which can jointly model simplicity, grammaticality, and semantic fidelity to the input.",7 Conclusions,[0],[0]
We also proposed a lexical simplification component that further boosts performance.,7 Conclusions,[0],[0]
"Overall, we find that reinforcement learning offers a great means to inject prior knowledge to the simplification task achieving good results across three datasets.",7 Conclusions,[0],[0]
"In the future, we would like to explicitly model sentence splitting and simplify entire documents (rather than individual sentences).",7 Conclusions,[0],[0]
"Beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression (Chopra et al., 2016), generation of programming code (Ling et al., 2016), or poems (Zhang and Lapata, 2014).
",7 Conclusions,[0],[0]
"Acknowledgments We would like to thank Li Dong, Jianpeng Cheng, Shashi Narayan and the EMNLP reviewers for their valuable feedback.",7 Conclusions,[0],[0]
We are also grateful to Shashi Narayan for supplying us with the output of his system and Wei Xu for her help with this work.,7 Conclusions,[0],[0]
"The authors acknowledge the support of the European Research Council (award number 681760).
",7 Conclusions,[0],[0]
"11BLEU and SARI scores reported in Xu et al. (2016) are 72.36 and 37.91, and measured at sentence-level.
",7 Conclusions,[0],[0]
12We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.,7 Conclusions,[0],[0]
Sentence simplification aims to make sentences easier to read and understand.,abstractText,[0],[0]
Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences.,abstractText,[0],[0]
We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework.,abstractText,[0],[0]
"Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input.",abstractText,[0],[0]
Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1,abstractText,[0],[0]
Sentence Simplification with Deep Reinforcement Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1156–1168 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Sentences with gapping (Ross, 1970) such as Paul likes coffee and Mary tea are characterized by having one or more conjuncts that contain multiple arguments or modifiers of an elided predicate.",1 Introduction,[0],[0]
"In this example, the predicate likes is elided for the relation Mary likes tea.",1 Introduction,[0],[0]
"While these sentences appear relatively infrequently in most written texts, they are often used to convey a lot of factual information that is highly relevant for language understanding (NLU) tasks such as open information extraction and semantic parsing.",1 Introduction,[0],[0]
"For example, consider the following sentence from the WSJ portion of the Penn Treebank (Marcus et al., 1993).
",1 Introduction,[0],[0]
"(1) Unemployment has reached 27.6% in Azerbaijan, 25.7% in Tadzhikistan, 22.8% in Uzbekistan, 18.8% in Turkmenia, 18% in Armenia and 16.3% in Kirgizia, [...]
To extract the information about unemployment rates in the various countries, an NLU system has to identify that the percentages indicate unemployment rates and the locational modifiers indicate the corresponding country.",1 Introduction,[0],[0]
"Given only this sentence, or this sentence and a strict surface syntax representation that does not indicate elided predicates, this is a challenging task.",1 Introduction,[0],[0]
"However, given a dependency graph that reconstructs the elided predicate for each conjunct, the problem becomes much easier and methods developed to extract information from dependency trees of clauses with canonical structures are much more likely to extract the correct information from a gapped clause.
",1 Introduction,[0],[0]
"While gapping constructions receive a lot of attention in the theoretical syntax literature (e.g., Ross 1970; Jackendoff 1971; Steedman 1990; Coppock 2001; Osborne 2006; Johnson 2014; Toosarvandani 2016; Kubota and Levine 2016), they have been almost entirely neglected by the NLP community so far.",1 Introduction,[0],[0]
"The Penn Treebank explicitly annotates gapping constructions, by coindexing arguments in the clause with a predicate and the clause with the gap, but these co-indices are not included in the standard parsing metrics
1156
and almost all parsers ignore them.1",1 Introduction,[0],[0]
"Despite the sophisticated analysis of gapping within CCG (Steedman, 1990), sentences with gapping were deemed too difficult to represent within the CCGBank (Hockenmaier and Steedman, 2007).",1 Introduction,[0],[0]
"Similarly the treebanks for the Semantic Dependencies Shared Task (Oepen et al., 2015) exclude all sentences from the Wall Street Journal that contain gapping.",1 Introduction,[0],[0]
"Finally, while the tectogrammatical layer of the Prague Dependency Treebank (Bejček et al., 2013) as well as the enhanced Universal Dependencies (UD) representation (Nivre et al., 2016) provide an analysis with reconstructed nodes for gapping constructions, there exist no methods to automatically parse to these representations.
",1 Introduction,[0],[0]
"Here, we provide the first careful analysis of parsing of gapping constructions, and we present two methods for reconstructing elided predicates in sentences with gapping within the UD framework.",1 Introduction,[0],[0]
"As illustrated in Figure 1, we first parse to a dependency tree and then reconstruct the elided material.",1 Introduction,[0],[0]
The methods differ in how much information is encoded in the dependency tree.,1 Introduction,[0],[0]
"The first method adapts an existing procedure for parsing sentences with elided function words (Seeker et al., 2012), which uses composite labels that can be deterministically turned into dependency graphs in most cases.",1 Introduction,[0],[0]
"The second method is a novel procedure that relies on the parser only to identify a gap, and then employs an unsupervised method to reconstruct the elided predicates and reattach the arguments to the reconstructed predicate.",1 Introduction,[0],[0]
We find that both methods can reconstruct elided predicates with very high accuracy from gold standard dependency trees.,1 Introduction,[0],[0]
"When applied to the output of a parser, which often fails to identify gapping, our methods achieve a sentence-level accuracy of 32% and 34%, significantly outperforming the recently proposed constituent parser by Kummerfeld and Klein (2017).",1 Introduction,[0],[0]
"Gapping constructions in English come in many forms that can be broadly classified as follows.
",2.1 Gapping constructions,[0],[0]
"1 To the best of our knowledge, the parser by Kummerfeld and Klein (2017) is the only parser that tries to output the co-indexing of constituents in clauses with gapping but they lack an explicit evaluation of their co-indexing prediction accuracy.
(2) Single predicate gaps: John bought books, and Mary flowers.
(3) Contiguous predicate-argument gap (including ACCs): Eve gave flowers to Al and Sue to Paul.",2.1 Gapping constructions,[0],[0]
"Eve gave a CD to Al and roses to Sue.
(4) Non-contiguous predicate-argument gap: Arizona elected Goldwater Senator, and Pennsylvania Schwelker .
",2.1 Gapping constructions,[0],[0]
"(Jackendoff, 1971)
(5) Verb cluster gap: I want to try to begin to write a novel and
...",2.1 Gapping constructions,[0],[0]
Mary a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to write a play.
...",2.1 Gapping constructions,[0],[0]
Mary to begin to write a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to try to begin to write a play.
",2.1 Gapping constructions,[0],[0]
"(Ross, 1970)
",2.1 Gapping constructions,[0],[0]
The defining characteristic of gapping constructions is that there is a clause that lacks a predicate (the gap) but still contains two or more arguments or modifiers of the elided predicate (the remnants or orphans).,2.1 Gapping constructions,[0],[0]
"In most cases, the remnants have a corresponding argument or modifier (the correspondent) in the clause with the overt predicate.
",2.1 Gapping constructions,[0],[0]
These types of gapping also make up the majority of attested constructions in other languages.,2.1 Gapping constructions,[0],[0]
"However, Wyngaerd (2007) notes that Dutch permits gaps in relative clauses, and Farudi (2013) notes that Farsi permits gaps in finite embedded clauses even if the overt predicate is not embedded.2",2.1 Gapping constructions,[0],[0]
"We work within the UD framework, which aims to provide cross-linguistically consistent dependency annotations that are useful for NLP tasks.",2.2 Target representation,[0],[0]
"UD defines two types of representation: the basic UD representation which is a strict surface syntax dependency tree and the enhanced UD representation (Schuster and Manning, 2016) which may be a graph instead of a tree and may contain additional nodes.",2.2 Target representation,[0],[0]
"The analysis of gapping in the enhanced representation makes use of copy nodes for elided predicates and additional edges for elided arguments, which we both try to automatically reconstruct in this paper.",2.2 Target representation,[0],[0]
"In the simple case in which only one predicate was elided, there is exactly one
2See Johnson (2014) or Schuster et al. (2017) for a more comprehensive overview of cross-linguistically attested gapping constructions.
copy node for the elided predicate, which leads to a structure that is identical to the structure of the same sentence without a gap.3
John bought books and Mary bought′ flowers
nsubj",2.2 Target representation,[0],[0]
"obj cc nsubj
conj
obj
If a clause contains a more complex gap, the enhanced representation contains copies for all content words that are required to attach the remnants.
... and Mary wanted′ try′ begin′ write′",2.2 Target representation,[0],[0]
"a play
cc conj
xcomp xcomp xcomp det
obj
The motivation behind this analysis is that the semantically empty markers to are not needed for interpreting the sentence and minimizing the number of copy nodes leads to less complex graphs.
",2.2 Target representation,[0],[0]
"Finally, if a core argument was elided along with the predicate, we introduce additional dependencies between the copy nodes and the shared arguments, as for example, the open clausal complement (xcomp) dependency between the copy node and Senator in the following example.
",2.2 Target representation,[0],[0]
"AZ elected G. Senator and PA elected′ S.
nsubj",2.2 Target representation,[0],[0]
"obj
xcomp cc
nsubj
conj
obj
xcomp
The rationale for not copying all arguments is again to keep the graph simple, while still encoding all relations between content words.",2.2 Target representation,[0],[0]
"Arguments can be arbitrarily complex and it seems misguided to copy entire subtrees of arguments which, e.g., could contain multiple adverbial clauses.",2.2 Target representation,[0],[0]
Note that linking to existing nodes would not work in the case of verb clusters because they do not satisfy the subtree constraint.,2.2 Target representation,[0],[0]
"Our first method adapts one of the procedures by Seeker et al. (2012), which represents gaps in dependency trees by attaching dependents of an elided predicate with composite relations.",3.1 Composite relations,[0],[0]
"These relations represent the dependency path that would
3To enhance the readability of our examples, we place the copy node in the sentence where the elided predicate would have been pronounced.",3.1 Composite relations,[0],[0]
"However, as linear order typically does not matter for extracting information with dependency patterns, our procedures only try to recover the structure of canonical sentences but not their linear order.
have existed if nothing had been elided.",3.1 Composite relations,[0],[0]
"For example, in the following sentence, the verb bought, which would have been attached to the head of the first conjunct with a conj relation, was elided from the second conjunct and hence all nodes that would have depended on the elided verb, are attached to the first conjunct using a composite relation consisting of conj and the type of argument.
",3.1 Composite relations,[0],[0]
"John bought books and Mary flowers
nsubj",3.1 Composite relations,[0],[0]
"obj
conj>cc
conj>nsubj
conj>obj
The major advantage of this approach is that the dependency tree contains information about the types of arguments and so it should be straightforward to turn dependency trees of this form into enhanced UD graphs.",3.1 Composite relations,[0],[0]
"For most dependency trees, one can obtain the enhanced UD graph by splitting the composite relations into its atomic parts and inserting copy nodes at the splitting points.4
At the same time, this approach comes with the drawback of drastically increasing the label space.",3.1 Composite relations,[0],[0]
"For sentences with more complex gaps as in (5), one has to use composite relations that consist of more than two atomic relations and theoretically, the number of composite relations is unbounded:
... and Mary a play
det
conj>xcomp>xcomp>xcomp>obj
conj>nsubj
conj>cc",3.1 Composite relations,[0],[0]
"Our second method also uses a two-step approach to resolve gaps, but compared to the previous method, it puts less work on the parser.",3.2 Orphan procedure,[0],[0]
"We first parse sentences to the basic UD v2 representation, which analyzes gapping constructions as follows.",3.2 Orphan procedure,[0],[0]
One remnant is promoted to be the head of the clause and all other remnants are attached to the promoted phrase.,3.2 Orphan procedure,[0],[0]
"For example, in this sentence, the subject of the second clause, Mary, is the head of the clause and the other remnant, flowers, is attached to Mary with the special orphan relation:
John bought books and Mary flowers
nsubj",3.2 Orphan procedure,[0],[0]
"obj cc conj orphan
4Note that this representation does not indicate conjunct boundaries, and for sentences with multiple gapped conjuncts, it is thus unclear how many copy nodes are required.
",3.2 Orphan procedure,[0],[0]
"This analysis can also be used for more complex gaps, as in the example with a gap that consists of a chain of non-finite embedded verbs in (5).
... and Mary a play
cc
conj
det
orphan
When parsing to this representation, the parser only has to identify that there is a gap but does not have to recover the elided material or determine the type of remnants.",3.2 Orphan procedure,[0],[0]
"As a second step, we use an unsupervised procedure to determine which nodes to copy and how and where to attach the remnants.",3.2 Orphan procedure,[0],[0]
"In developing this procedure, we made use of the fact that in the vast majority of cases, all arguments and modifiers that are expressed in gapped conjunct are also expressed in the full conjunct.",3.2 Orphan procedure,[0],[0]
The problem of determining which nodes to copy and which relations to use can thus be reduced to the problem of aligning arguments in the gapped conjunct to arguments in the full conjunct.,3.2 Orphan procedure,[0],[0]
"We apply the following procedure to all sentences that contain at least one orphan relation.
1.",3.2 Orphan procedure,[0],[0]
"Create a list F of arguments of the head of the full conjunct by considering all core argument dependents of the conjunct’s head as well as clausal and nominal non-core dependents, and adverbial modifiers.
2.",3.2 Orphan procedure,[0],[0]
"Create a list G of arguments in the gapped conjunct that contains the head of the gapped conjunct and all its orphan dependents.
3.",3.2 Orphan procedure,[0],[0]
"Find the highest-scoring monotonic alignment of arguments in G to arguments in F .
4.",3.2 Orphan procedure,[0],[0]
"Copy the head of the full conjunct and attach the copy node c to the head of the full conjunct with the original relation of the head of the gapped conjunct (usually conj).
5.",3.2 Orphan procedure,[0],[0]
"For each argument g ∈ G that has been aligned to f ∈ F , attach g to c with the same relation as the parent relation of f , e.g., if f is attached to the head of the full conjunct with an nsubj relation, also attach g to c with an nsubj relation.",3.2 Orphan procedure,[0],[0]
"Attach arguments g′ ∈ G that were not aligned to any token in F to c using the general dep relation.
6.",3.2 Orphan procedure,[0],[0]
"For each copy node c, add dependencies to all core arguments of the original node which do not have a corresponding remnant in the gapped clause.",3.2 Orphan procedure,[0],[0]
"For example, if the full conjunct contains a subject, an object, and an
oblique modifier but the clause with the gap, only a subject and an oblique modifier, add an object dependency between the copy node and the object in the full conjunct.
",3.2 Orphan procedure,[0],[0]
"A crucial step is the third step, determining the highest-scoring alignment.",3.2 Orphan procedure,[0],[0]
"This can be done straightforwardly with the sequence alignment algorithm by Needleman and Wunsch (1970) if one defines a similarity function sim(g, f) that returns a similarity score between the arguments g and f .",3.2 Orphan procedure,[0],[0]
"We defined sim based on the intuitions that often, parallel arguments are of the same syntactic category, that they are introduced by the same function words (e.g., the same preposition), and that they are closely related in meaning.",3.2 Orphan procedure,[0],[0]
"The first intuition can be captured by penalizing mismatching POS tags, and the other two by computing the distance between argument embeddings.",3.2 Orphan procedure,[0],[0]
We compute these embeddings by averaging over the 100- dim.,3.2 Orphan procedure,[0],[0]
"pretrained GloVe (Pennington et al., 2014) embeddings for each token in the argument.",3.2 Orphan procedure,[0],[0]
"Given the POS tags tg and tf and the argument embeddings vg and vf , sim is defined as follows.5
sim(g, f) = −‖vg",3.2 Orphan procedure,[0],[0]
− vf‖2 + 1,3.2 Orphan procedure,[0],[0]
"[tg = tf ] × pos_mismatch_penalty
We set pos_mismatch_penalty, a parameter that penalizes mismatching POS tags, to −2.6
This procedure can be used for almost all sentences with gapping constructions.",3.2 Orphan procedure,[0],[0]
"However, if parts of an argument were elided along with the main predicate, it can become necessary to copy multiple nodes.",3.2 Orphan procedure,[0],[0]
We therefore consider the alignment not only between complete arguments in the full clause and the gapped clause but also between partial arguments in the full clause and the complete arguments in the gapped clause.,3.2 Orphan procedure,[0],[0]
"For example, for the sentence “Mary wants to write a play and Sue a book” the complete arguments of the full clause are {Mary, to write a play} and the arguments of the gapped clause are {Sue, a book}.",3.2 Orphan procedure,[0],[0]
"In this case, we also consider the partial arguments {Mary, a play} and if the arguments of the gapped
5As suggested by one of the reviewers, we also ran a posthoc experiment with a simpler similarity score function without the embedding distance term, which only takes into account whether the POS tags match.",3.2 Orphan procedure,[0],[0]
"We found that quantitatively, the embeddings do not lead to significant better scores on the test set according to our metrics but qualitatively, they lead to better results for the examples with verb cluster gaps.
6We optimized this parameter on the training set by trying integer values from −1 to −15.
conjunct align better to the partial arguments, we use this alignment.",3.2 Orphan procedure,[0],[0]
"However, now that the token write is part of the dependency path between want and play, we also have to make a copy of write to reconstruct the UD graph of the gapped clause.",3.2 Orphan procedure,[0],[0]
Both methods rely on a dependency parser followed by a post-processing step.,4 Experiments,[0],[0]
We evaluated the individual steps and the end-to-end performance.,4 Experiments,[0],[0]
"We used the UD English Web Treebank v2.1 (henceforth EWT; Silveira et al., 2014; Nivre et al., 2017) for training and evaluating parsers.",4.1 Data,[0],[0]
"As the treebank is relatively small and therefore only contains very few sentences with gapping, we also extracted gapping constructions from the WSJ and Brown portions of the PTB (Marcus et al., 1993) and the GENIA corpus (Ohta et al., 2002).",4.1 Data,[0],[0]
"Further, we copied sentences from the Wikipedia page on gapping7 and from published papers on gapping.",4.1 Data,[0],[0]
"The sentences in the EWT already contain annotations with the orphan relation and copy nodes for the enhanced representation, and we manually added both of these annotations for the remaining examples.",4.1 Data,[0],[0]
"The composite relations can
7https://en.wikipedia.org/wiki/Gapping, accessed on Aug 24, 2017.
be automatically obtained from the enhanced representation by removing the copy nodes and concatenating the dependency labels, which we did to build the training and test corpus for the composite relation procedure.",4.1 Data,[0],[0]
"Table 1 shows properties of the data splits of the original treebank, the additional sentences with gapping, and their combination; Table 2 shows the number of sentences in our corpus for each of the gap types.",4.1 Data,[0],[0]
Parser We used the parser by Dozat and Manning (2017) for parsing to the two different intermediate dependency representations.,4.2 Parsing experiments,[0],[0]
"This parser is a graph-based parser (McDonald et al., 2005) that uses a biLSTM to compute token representations and then uses a multi-layer perceptron with biaffine attention to compute arc and label scores.
",4.2 Parsing experiments,[0],[0]
"Setup We trained the parser on the COMBINED training corpus with gold tokenization, and predicted fine-grained and universal part-of-speech tags, for which we used the tagger by Dozat et al. (2017).",4.2 Parsing experiments,[0],[0]
We trained the tagger on the COMBINED training corpus.,4.2 Parsing experiments,[0],[0]
"As pre-trained embeddings, we used the word2vec (Mikolov et al., 2013) embeddings that were provided for the CoNLL 2017 Shared Task (Zeman et al., 2017), and we used the same hyperparameters as Dozat et al. (2017).
",4.2 Parsing experiments,[0],[0]
Evaluation We evaluated the parseability of the two dependency representations using labeled and unlabeled attachment scores (LAS and UAS).,4.2 Parsing experiments,[0],[0]
"Further, to specifically evaluate how well parsers are able to parse gapping constructions according to the two annotation schemes, we also computed the LAS and UAS just for the head tokens of remnants (LASg and UASg).",4.2 Parsing experiments,[0],[0]
"For all our metrics, we excluded punctuation tokens.",4.2 Parsing experiments,[0],[0]
"To determine sta-
tistical significance of pairwise comparisons, we performed two-tailed approximate randomization tests (Noreen, 1989; Yeh, 2000) with an adapted version of the sigf package (Padó, 2006).
",4.2 Parsing experiments,[0],[0]
Results Table 3 shows the overall parsing results on the development and test sets of the two treebanks.,4.2 Parsing experiments,[0],[0]
"There was no significant difference between the parser that was trained on the UD representation (ORPHAN) and the parser trained on the composite representation (COMPOSITE) when tested on the EWT data sets, which is not surprising considering that there is just one sentence with gapping each in the development and the test split.",4.2 Parsing experiments,[0],[0]
"When evaluated on the GAPPING datasets, the ORPHAN parser performs significantly better (p < 0.01) in terms of labeled attachment score, which suggests that the parser trained on the COMPOSITE representation is indeed struggling with the greatly increased label space.",4.2 Parsing experiments,[0],[0]
This is further confirmed by the attachment scores of the head tokens of remnants (Table 4).,4.2 Parsing experiments,[0],[0]
The labeled attachment score of remnants is significantly higher for the ORPHAN parser than for the COMPOSITE parser.,4.2 Parsing experiments,[0],[0]
"Further, the unlabeled attachment score on the test set is also higher for the ORPHAN parser, which suggests that the COMPOSITE parser is sometimes struggling with finding the right attachment for the
multiple long-distance composite dependencies.",4.2 Parsing experiments,[0],[0]
Our second set of experiments concerns the recovery of the elided material and the reattachment of the orphans.,4.3 Recovery experiments,[0],[0]
We conducted two experiments: an oracle experiment that used gold standard dependency trees and an end-to-end experiment that used the output of the parser as input.,4.3 Recovery experiments,[0],[0]
"For all experiments, we used the COMBINED treebank.
",4.3 Recovery experiments,[0],[0]
"Evaluation Here, we evaluated dependency graphs and therefore used the labeled and unlabeled precision and recall metrics.",4.3 Recovery experiments,[0],[0]
"However, as our two procedures are only changing the attachment of orphans, we only computed these metrics for copy nodes and their dependents.",4.3 Recovery experiments,[0],[0]
"Further, we excluded punctuation and coordinating conjunctions as their attachment is usually trivial and including them would inflate scores.",4.3 Recovery experiments,[0],[0]
"Lastly, we computed the sentence-level accuracy for all sentences with gapping.",4.3 Recovery experiments,[0],[0]
"For this metric, we considered a sentence to be correct if all copy nodes and their dependents of a sentence were attached to the correct head with the correct label.
",4.3 Recovery experiments,[0],[0]
Oracle results The top part of Table 5 shows the results for the oracle experiment.,4.3 Recovery experiments,[0],[0]
Both methods are able to reconstruct the elided material and the canonical clause structure from gold dependency trees with high accuracy.,4.3 Recovery experiments,[0],[0]
"This was expected for the COMPOSITE procedure, which can make use of the composite relations in the dependency trees, but less so for the ORPHAN procedure which has to recover the structure and the types of relations.",4.3 Recovery experiments,[0],[0]
"The two methods work equally well in terms of all metrics except for the sentence-level accuracy, which is significantly higher for the COMPOSITE procedure.",4.3 Recovery experiments,[0],[0]
This difference is caused by a difference in the types of mistakes.,4.3 Recovery experiments,[0],[0]
All errors of the COMPOSITE procedure are of a structural nature and stem from copying the wrong number of nodes while the dependency labels are always correct because they are part of the dependency tree.,4.3 Recovery experiments,[0],[0]
"The majority of errors of the ORPHAN procedure stem from incorrect dependency labels, and these mistakes are scattered across more examples, which leads to the lower sentence-level accuracy.
",4.3 Recovery experiments,[0],[0]
End-to-end results The middle part of Table 5 shows the results for the end-to-end experiment.,4.3 Recovery experiments,[0],[0]
"The performance of both methods is considerably lower than in the oracle experiment, which is pri-
marily driven by the much lower recall.",4.3 Recovery experiments,[0],[0]
"Both methods assume that the parser detects the existence of a gap and if the parser fails to do so, neither method attempts to reconstruct the elided material.",4.3 Recovery experiments,[0],[0]
"In general, precision tends to be a bit higher for the ORPHAN procedure whereas recall tends to be a bit higher for the COMPOSITE method but overall and in terms of sentence-level accuracy both methods seem to perform equally well.
",4.3 Recovery experiments,[0],[0]
"Error analysis For both methods, the primary issue is low recall, which is a result of parsing errors.",4.3 Recovery experiments,[0],[0]
"When the parser correctly predicts the orphan relation, the main sources of error for the ORPHAN procedure are missing correspondents for remnants (e.g., [for good] has no correspondent in They had left the company, many for good) or that the types of argument of the remnant and its correspondent differ (e.g., in She was convicted of selling unregistered securities in Florida and of unlawful phone calls in Ohio,",4.3 Recovery experiments,[0],[0]
"[of selling unregistered securities] is an adverbial clause whereas [of unlawful phone calls] is an oblique modifier).
",4.3 Recovery experiments,[0],[0]
"Apart from the cases where the COMPOSITE procedure leads to an incorrect structure, the remaining errors are all caused by the parser predicting the wrong composite relation.",4.3 Recovery experiments,[0],[0]
Kummerfeld and Klein (henceforth K&K; 2017) recently proposed a one-endpoint-crossing graph parser that is able to directly parse to PTB-style trees with traces.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
They also briefly discuss gapping constructions and their parser tries to output the co-indexing that is used for gapping constructions in the PTB.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The EWT and all the sentences that we took from the WSJ, Brown, and GENIA treebanks already come with constituency tree annotations, and we manually annotated the remaining sentences according to the PTB guide-
lines (Bies et al., 1995).",4.4 Comparison to Kummerfeld and Klein,[0],[0]
This allowed us to train the K&K parser with exactly the same set of sentences that we used in our previous experiments.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"As this parser outputs constituency trees, we could not compute dependency graph metrics for this method.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For the sentence-level accuracy, we considered an example to be correct if a) each argument in the gapped conjunct was the child of a single constituent node, which in return was the sibling of the full clause/verb phrase, and b) the coindexing of each argument in the gapped conjunct was correct.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For example, the following bracketing would be considered correct despite the incorrect internal structure of the first conjunct: [S[S[NP-1 Al ] likes [NP-2 coffee ]] and [S[NP=1 Sue",4.4 Comparison to Kummerfeld and Klein,[0],[0]
],4.4 Comparison to Kummerfeld and Klein,[0],[0]
"[NP=2 tea ]]]
The last row of Table 5 shows the results of the K&K parser.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
The parser failed to output the correct constituency structure or co-indexing for every single example in the development and test sets.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The parser struggled in particular with outputting the correct co-indices: For 32.5% of the test sentences with gapping, the bracketing of the gapped clause was correct but one or more of the co-indices were missing from the output.
",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Overall these results suggest that our dependency-based approach is much more reliable at identifying gapping constructions than the parser by K&K, which, in their defense, was optimized to output traces for other phenomena.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Our method is also faster and took only seconds to parse the test set, while the K&K parser took several hours.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
One of the appeals of the ORPHAN procedure is that it can be easily applied to other languages even if there exist no annotated enhanced dependency graphs.8,5 Resolving gaps in other languages,[0],[0]
"On the one hand, this is because
8There is no theoretical reason that would prevent one from using the COMPOSITE procedure for other languages
our method does not make use of lexical information, and on the other hand, this is because we developed our method on top of the UD annotation scheme, which has already been applied to many languages and for which many treebanks exist.
",5 Resolving gaps in other languages,[0],[0]
"Currently, all treebanks but the English one lack copy nodes for gapping constructions and many of them incorrectly use the orphan relation (Droganova and Zeman, 2017) and therefore we could not evaluate our method on a large variety of languages.",5 Resolving gaps in other languages,[0],[0]
"In order to demonstrate that our method can be applied to other languages, we therefore did a case study on the Swedish UD treebank.",5 Resolving gaps in other languages,[0],[0]
"The Swedish UD treebank is an automatic conversion from a section of the Talbanken (Einarsson, 1976) with extensive manual corrections.",5 Resolving gaps in other languages,[0],[0]
"While the treebank is overall of high quality, we noticed conversion errors that led to incorrect uses of the orphan relation in 11 of the 29 sentences with orphan relations, which we excluded from our evaluation.",5 Resolving gaps in other languages,[0],[0]
We applied our gapping resolution procedure without any modifications to the remaining 18 sentences.,5 Resolving gaps in other languages,[0],[0]
We used the Swedish word2vec embeddings that were prepared for the CoNLL 2017 Shared Task.,5 Resolving gaps in other languages,[0],[0]
"Our method correctly predicts the insertion of 29 copy nodes and is able to predict the correct structure of the enhanced representation in all cases, including complex ones with elided verb clusters such as the example in Figure 2.",5 Resolving gaps in other languages,[0],[0]
"It also predicts the correct dependency label for 108/110 relations, leading to a labeled precision and labeled recall of 98.18%, which are both higher than the English numbers despite the fact that we optimized our procedure for English.",5 Resolving gaps in other languages,[0],[0]
"The main reason for the higher performance seems to be that many of the Swedish examples come from informational texts from public organizations, which are more likely to be written to be clear and unambiguous.",5 Resolving gaps in other languages,[0],[0]
"Further, the Swedish data does not contain challenging examples from the linguistic literature.
",5 Resolving gaps in other languages,[0],[0]
"As Swedish is a Germanic language like English and thus shares many structural properties, we cannot conclude that our method is applicable to any language based on just this experiment.",5 Resolving gaps in other languages,[0],[0]
"However, given that our method does not rely on language-specific structural patterns, we expect it to work well for a wide range of languages.
",5 Resolving gaps in other languages,[0],[0]
"but given that UD treebanks are annotated with orphan relations, using the the COMPOSITE procedure would require additional manual annotations in practice.",5 Resolving gaps in other languages,[0],[0]
"Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes.",6 Related work,[0],[0]
"Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing.",6 Related work,[0],[0]
These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping.,6 Related work,[0],[0]
"Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB.",6 Related work,[0],[0]
"While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions.",6 Related work,[0],[0]
"Moreover, they did not reconstruct gapped ACC clauses.",6 Related work,[0],[0]
"Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms would be needed for reconstruction.",6 Related work,[0],[0]
"In practice, though, dealing with gapping in a grammar-based framework is not straightforward and can lead to a combinatorial explosion that slows down parsing in general, as has been noted for the English Resource Grammar (Flickinger, 2017, p.c.) and for an HPSG implementation for Norwegian (Haugereid, 2017).",6 Related work,[0],[0]
"The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage.
",6 Related work,[0],[0]
"There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins,
1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017).",6 Related work,[0],[0]
"However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions.",6 Related work,[0],[0]
"And again, none of these works try to reconstruct elided material.
",6 Related work,[0],[0]
"Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions.",6 Related work,[0],[0]
We presented two methods to recover elided predicates in sentences with gapping.,7 Conclusion,[0],[0]
Our experiments suggest that both methods work equally well in a realistic end-to-end setting.,7 Conclusion,[0],[0]
"While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase.
",7 Conclusion,[0],[0]
We also demonstrated that our method can be used to automatically add the enhanced UD representation to UD treebanks in other languages than English.,7 Conclusion,[0],[0]
"Apart from being useful in a parsing pipeline, we therefore also expect our method to be useful for building enhanced UD treebanks.
",7 Conclusion,[0],[0]
"Reproducibility
All data, pre-trained models, system outputs as well as a package for running the enhancement procedure are available from https:// github.com/sebschu/naacl-gapping.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their thoughtful feedback.,Acknowledgments,[0],[0]
"Also thanks to Vera Gribanova and Boris Harizanov for continuous feedback throughout this project, and to Matthew Lamm for help with annotating the data.",Acknowledgments,[0],[0]
"This work was supported in part by gifts from Google, Inc. and IPSoft, Inc.",Acknowledgments,[0],[0]
The first author is also supported by a Goodan Family Graduate Fellowship.,Acknowledgments,[0],[0]
"Sentences with gapping, such as Paul likes coffee and Mary tea, lack an overt predicate to indicate the relation between two or more arguments.",abstractText,[0],[0]
"Surface syntax representations of such sentences are often produced poorly by parsers, and even if correct, not well suited to downstream natural language understanding tasks such as relation extraction that are typically designed to extract information from sentences with canonical clause structure.",abstractText,[0],[0]
"In this paper, we present two methods for parsing to a Universal Dependencies graph representation that explicitly encodes the elided material with additional nodes and edges.",abstractText,[0],[0]
We find that both methods can reconstruct elided material from dependency trees with high accuracy when the parser correctly predicts the existence of a gap.,abstractText,[0],[0]
We further demonstrate that one of our methods can be applied to other languages based on a case study on Swedish.,abstractText,[0],[0]
Sentences with Gapping: Parsing and Reconstructing Elided Predicates,title,[0],[0]
"Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text.",1 Introduction,[0],[0]
Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level.,1 Introduction,[0],[0]
"Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",1 Introduction,[0],[0]
"In practice, however, differ-
ent types of sentiment-oriented relations frequently coexist in documents.",1 Introduction,[0],[0]
"In particular, we found that more than 38% of the sentences in our test corpus contain more than one type of relations.",1 Introduction,[0],[0]
The isolated analysis approach is inappropriate because i) it sacrifices acuracy by ignoring the intricate interplay among different types of relations; ii) it could lead to conflicting predictions such as estimating a relation candidate as both negative and comparative.,1 Introduction,[0],[0]
"Therefore, in this paper, we identify instances of both sentiment polarities and comparative relations for entities of interest simultaneously.",1 Introduction,[0],[0]
"We assume that all the mentions of entities and attributes are given, and entities are disambiguated.",1 Introduction,[0],[0]
"It is a widely used assumption when evaluating a module in a pipeline system that the outputs of preceding modules are error-free.
",1 Introduction,[0],[0]
"To the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009).",1 Introduction,[0],[0]
We argue that it is better to tackle the task by using a unified model with structured outputs.,1 Introduction,[0],[0]
It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints.,1 Introduction,[0],[0]
"For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time.",1 Introduction,[0],[0]
"As a result, the system extracts a set of correlated instances of sentiment-oriented relations from a given sentence.",1 Introduction,[0],[0]
"For example, with the sentence about the camera Canon 7D, “The sensor is great, but the price is higher than Nikon D7000.”",1 Introduction,[0],[0]
"the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155–168.",1 Introduction,[0],[0]
Action Editor: Janyce Wiebe.,1 Introduction,[0],[0]
Submitted 6/2013; Revised 11/2013; Published 4/2014.,1 Introduction,[0],[0]
"c©2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textitprice).
",1 Introduction,[0],[0]
"However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background.",1 Introduction,[0],[0]
"We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence.",1 Introduction,[0],[0]
"Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions.
",1 Introduction,[0],[0]
The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training.,1 Introduction,[0],[0]
"For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the nearest relation candidate is insufficient, especially when the sentiment is not explicitly expressed.
",1 Introduction,[0],[0]
"In this paper, we propose SENTI-LSSVM, a latent structural SVM based model for sentiment-oriented relation extraction.",1 Introduction,[0],[0]
"SENTI-LSSVM is applied to find the most likely set of the relation instances expressed in a given sentence, where the latent variables are used to assign the most appropriate textual evidences to the respective instances.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are the following:
• We propose SENTI-LSSVM: the first unified statistical model with the capability of extracting instances of both binary and ternary sentimentoriented relations.
",1 Introduction,[0],[0]
"• We design a task-specific integer linear programming (ILP) formulation for inference.
",1 Introduction,[0],[0]
"• We construct a new SRG corpus as a valuable asset for the evaluation of sentiment relation
extraction.
",1 Introduction,[0],[0]
"• We conduct extensive experiments with online reviews and forum posts, showing that SENTI-LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems.",1 Introduction,[0],[0]
"There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation.
",2 Related Work,[0],[0]
Most prior approaches for fine-grained sentiment analysis focus on polarity classification.,2 Related Work,[0],[0]
"Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010).",2 Related Work,[0],[0]
"However, the corresponding annotation process is time-consuming.",2 Related Work,[0],[0]
"Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012).",2 Related Work,[0],[0]
"Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012).",2 Related Work,[0],[0]
"In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations.
",2 Related Work,[0],[0]
"Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",2 Related Work,[0],[0]
Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations.,2 Related Work,[0],[0]
"Therefore, these methods fall short of extracting comparative relations based on domain dependent information.
",2 Related Work,[0],[0]
Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs.,2 Related Work,[0],[0]
"However, they focus only on polarity classification
of expressions and require annotation of sentimentbearing expressions for training as well.
While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations.",2 Related Work,[0],[0]
This section gives an overview of the whole system for extracting sentiment-oriented relation instances.,3 System Overview,[0],[0]
"Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences.",3 System Overview,[0],[0]
Entity.,3.1 Concepts and Definitions,[0],[0]
"An entity is an abstract or concrete thing, which needs not be of material existence.",3.1 Concepts and Definitions,[0],[0]
An entity in this paper refers to either a product or a brand.,3.1 Concepts and Definitions,[0],[0]
Attribute.,3.1 Concepts and Definitions,[0],[0]
"An attribute is an object closely associated with or belonging to an entity, such as the lens of digital camera.",3.1 Concepts and Definitions,[0],[0]
Sentiment-Oriented Relation.,3.1 Concepts and Definitions,[0],[0]
"A sentimentoriented relation is either a sentiment polarity or a comparative relation, defined on tuples of entities and attributes.",3.1 Concepts and Definitions,[0],[0]
"A sentiment polarity relation conveys either a positive or a negative attitude towards entities or their attributes, whereas a comparative relation indicates the preference of one entity over the other entity w.r.t.",3.1 Concepts and Definitions,[0],[0]
an attribute.,3.1 Concepts and Definitions,[0],[0]
Relation Instance.,3.1 Concepts and Definitions,[0],[0]
"An instance of sentiment polarity takes the form r(entity, attribute) with r ∈ {positive, negative}, such as positive(Canon 7D, sensor).",3.1 Concepts and Definitions,[0],[0]
"The polarity instances expressed in the form of unary relations, such as “Nikon D7000 is excellent.”, are denoted as binary relations r(entity, whole), where the attribute whole indicates the entity as a whole.",3.1 Concepts and Definitions,[0],[0]
"In contrast, an instance of comparative relation is in the form of preferred{entity, entity, attribute}, e.g. preferred(Canon 7D, Nikon D7000, price).",3.1 Concepts and Definitions,[0],[0]
"For brevity, we refer to an instance set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances of the remaining relations, we represent them as other{entity, attribute}, such as textitpartOf{wheel, car}.",3.1 Concepts and Definitions,[0],[0]
These relations include objective relations and the subjective relations other than sentimentoriented relations.,3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Instances.,3.1 Concepts and Definitions,[0],[0]
A mentionbased relation instance refers to a tuple of entity mentions with a certain relation.,3.1 Concepts and Definitions,[0],[0]
"This concept is introduced as the representation of instances in a sentence by replacing entities with the corresponding entity mentions, such as positive(“Canon SD880i”, “wide angle view”).
",3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
A mention-based relation graph (or MRG ) represents a collection of mention-based relation instances expressed in a sentence.,3.1 Concepts and Definitions,[0],[0]
"As illustrated in Figure 1, an MRG is a directed hypergraph G = 〈M,E〉 with a vertex set M and an edge set E. A vertex mi ∈ M denotes a mention of an entity or an attribute occurring either within the sentence or in its context.",3.1 Concepts and Definitions,[0],[0]
We say that a mention is from the context if it is mentioned in the previous sentence or is an attribute implied in the current sentence.,3.1 Concepts and Definitions,[0],[0]
"An instance of a binary relation in an MRG takes the form of a binary edge el = (mi,ma), where mi and ma denote an entity mention and an attribute mention respectively, and the type l ∈ {positive, negative, other}.",3.1 Concepts and Definitions,[0],[0]
"A ternary edge el indicating comparative relation is represented as el = (mi,mj ,ma), where two entity mentions mi and mj are compared with respect to the attribute mention ma.",3.1 Concepts and Definitions,[0],[0]
"We define the type l ∈ {better,worse} to indicate two possible directions of the relation and assume mi occurs before mj .",3.1 Concepts and Definitions,[0],[0]
"As a result, we have a set L of five relation types: positive, negative, better, worse or other.",3.1 Concepts and Definitions,[0],[0]
"According to these definitions, the annotations in the SRG corpus are actually MRGs and disambiguated entities.",3.1 Concepts and Definitions,[0],[0]
"If there are multiple mentions referring to the same entity, annotators are asked to choose the
most obvious one because it saves annotation time and is less demanding for the entity recognition and diambiguation modules.
",3.1 Concepts and Definitions,[0],[0]
Evidentiary Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
"An evidentiary mention-based relation graph, coined eMRG , extends an MRG by associating each edge with a textual evidence to support the corresponding relation assertions (see Figure 2).",3.1 Concepts and Definitions,[0],[0]
"Consequently, an edge in an eMRG is denoted by a pair (a, c), where a represents a mention-based relation instance and c is the associated textual evidence.",3.1 Concepts and Definitions,[0],[0]
It is also referred to as an evidentiary edge.,3.1 Concepts and Definitions,[0],[0]
"represented as el = (mi,mj ,ma), an MRG as an evidentiary MRG (eMRG) and the edges of eMRGs as evidentiary edges, as shown in Figure 2.",3.1 Concepts and Definitions,[0],[0]
"As illustrated by Figure 3, at the core of our system is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs from sentences.",3.2 System Architecture,[0],[0]
"For a given sentence with known entity mentions, we select all possible mention sets as relation candidates, where each set includes at least one entity mention.",3.2 System Architecture,[0],[0]
Then we associate each relation candidate with a set of constituents or the whole sentence as the textual evidence candidates (cf. Section 6.1).,3.2 System Architecture,[0],[0]
"Subsequently, the inference component aims to find the most likely eMRG from all possible combinations of mention-based relation instances and their textual evidences (cf. Section 6.2).",3.2 System Architecture,[0],[0]
The representation eMRG is chosen because it characterizes exactly the model outputs by letting each edge correspond to an instance of mention-based relation and the associated textual evidence.,3.2 System Architecture,[0],[0]
"Finally, the model parameters of this model are learned by an online algorithm (cf. Section 7).
",3.2 System Architecture,[0],[0]
"Since instance sets of sentiment-oriented relations (sSoRs) are the expected outputs, we can obtain sSoRs from MRGs by using a simple rule-based algorithm.",3.2 System Architecture,[0],[0]
The algorithm essentially maps the mentions from an MRG into entities and attributes in an sSoR and label the corresponding tuples with the relation types of the edges from an MRG.,3.2 System Architecture,[0],[0]
"For instances of comparative relation, the label better or worse is mapped to the relation type preferred.",3.2 System Architecture,[0],[0]
The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence.,4 SENTI-LSSVM Model,[0],[0]
"Since sSoRs are derived from the corresponding MRGs as described in Section 3, the task is reduced to find the most likely MRG for each sentence.",4 SENTI-LSSVM Model,[0],[0]
"Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high.
",4 SENTI-LSSVM Model,[0],[0]
"To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004).",4 SENTI-LSSVM Model,[0],[0]
"The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs.",4 SENTI-LSSVM Model,[0],[0]
"As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text.",4 SENTI-LSSVM Model,[0],[0]
"How-
ever, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, as shown in Figure 2.",4 SENTI-LSSVM Model,[0],[0]
"As a consequence, the instances of different relations represented by overly similar features can easily confuse the learning algorithm.",4 SENTI-LSSVM Model,[0],[0]
"Thus, it is critical to select proper constituents or sentences as textual evidences for each relation candidate in both training and testing.
",4 SENTI-LSSVM Model,[0],[0]
"Consequently, we divide the task of sentimentoriented relation extraction into two subtasks : i) identifying the most likely MRGs; ii) assigning proper textual evidences to each edge of MRGs to support their relation assertions.",4 SENTI-LSSVM Model,[0],[0]
It is desirable to carry out the two subtasks jointly as these two subtasks could enhance each other.,4 SENTI-LSSVM Model,[0],[0]
"First, the identification of relation types requires proper textual evidences; second, the soft and hard constraints imposed by the correlated relation instances facilitate the recognition of the corresponding textual evidences.",4 SENTI-LSSVM Model,[0],[0]
"Since the eMRGs are created by attaching every MRG with a set of textual evidences, tackling the two subtasks simultaneously is equivalent to selecting the most likely eMRG from a set of eMRG candidates.",4 SENTI-LSSVM Model,[0],[0]
"It is challenging because our SRG corpus does not contain any annotation of textual evidences.
",4 SENTI-LSSVM Model,[0],[0]
"Formally, let X denote the set of all available sentences, and we define y ∈ Y(x)(x ∈ X ) as the set of labeled edges of an MRG and Y = ∪x∈XY(x).",4 SENTI-LSSVM Model,[0],[0]
"Since the assignments of textual evidences are not observed, an assignment of evidences to y is denoted by a latent variable h ∈ H(x) and H = ∪x∈XH(x).",4 SENTI-LSSVM Model,[0],[0]
"Then (y, h) corresponds to an eMRG, and (a, c) ∈ (y, h) is a labeled edge a attached with a textual evidence c.",4 SENTI-LSSVM Model,[0],[0]
"Given a labeled dataset D = {(x1, y1), ..., (xn, yn)} ∈ (X × Y)n, we aim to learn a discriminant function f : X",4 SENTI-LSSVM Model,[0],[0]
"→ Y×H that outputs the optimal eMRG (y, h) ∈ Y(x)×H(x) for a given sentence x.
Due to the introduction of latent variables, we adopt the latent structural SVM (Yu and Joachims, 2009) for structural classification.",4 SENTI-LSSVM Model,[0],[0]
"Our discriminant function is defined as
f(x) =",4 SENTI-LSSVM Model,[0],[0]
"argmax(y,h)∈Y(x)×H(x)β >Φ(x, y, h) (1)
where Φ(x, y, h) is the feature function of an eMRG (y, h) and β is the corresponding weight vector.
",4 SENTI-LSSVM Model,[0],[0]
"To ensure tractability, we also employ edge-based factorization for our model.",4 SENTI-LSSVM Model,[0],[0]
"Let Mp denote a set of entity mentions and yr(mi) be a set of edges labeled with sentiment-oriented relations incident to mi, the factorization of Φ(x, y, h) is given as
Φ(x, y, h) = ∑
(a,c)∈(y,h) Φe(x, a, c) + (2)
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ Φc(a, a
′)
where Φe(x, a, c) is a local edge feature function for a labeled edge a attached with a textual evidence c and Φc(a, a′) is a feature function capturing cooccurrence of two labeled edges ami and a ′ mi incident to an entity mention mi.",4 SENTI-LSSVM Model,[0],[0]
"The following features are used in the feature functions (Equation 2):
Unigrams: As mentioned before, a textual evidence attached to an edge in MRG is either a word, phrase or sentence.",5 Feature Space,[0],[0]
"We consider all lemmatized unigrams in the textual evidence as unigram features.
",5 Feature Space,[0],[0]
"Context: Since web users usually express related sentiments about the same entity across sentence boundaries, we describe the sentiment flow using a set of contextual binary features.",5 Feature Space,[0],[0]
"For example, if entity A is mentioned in both the previous sentence and the current sentence, a set of contextual binary features are used to indicate all possible combinations of the current and the previous mentioned sentimentoriented relations regarding to entity A.
Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc(a, a
′).",5 Feature Space,[0],[0]
It captures the co-occurrence of two labeled edges incident to the same entity mention.,5 Feature Space,[0],[0]
"Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges.
",5 Feature Space,[0],[0]
"Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence.",5 Feature Space,[0],[0]
"The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”,
“negative”, “better” or “worse”.",5 Feature Space,[0],[0]
"We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features.
",5 Feature Space,[0],[0]
Others: The commonly used part-of-speech tags are also included as features.,5 Feature Space,[0],[0]
"Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions.",5 Feature Space,[0],[0]
"For instance, a binary feature indicates whether an edge is a binary edge related to an entity mentioned in context.",5 Feature Space,[0],[0]
"To characterize the syntactic dependencies between two adjacent entity mentions, we use the path in the dependency tree between the heads of the corresponding constituents, the number of words and other mentions in-between as features.",5 Feature Space,[0],[0]
"Additionally, if the textual evidence is a constituent, its feature w.r.t.",5 Feature Space,[0],[0]
an edge is the dependency path to the closest mention of the edge that does not overlap with this constituent.,5 Feature Space,[0],[0]
"In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences.",6 Structural Inference,[0],[0]
We formulate this task as an Integer Linear Programming (ILP).,6 Structural Inference,[0],[0]
"Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate.",6 Structural Inference,[0],[0]
"Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003).",6.1 Textual Evidence Candidates Selection,[0],[0]
"For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate starting from the attribute men-
tion.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting two entity mentions, we consider type I candidates starting from both mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Moreover, for a comparative ternary edge, we consider both type I and type II candidates starting from the attribute mention.",6.1 Textual Evidence Candidates Selection,[0],[0]
This strategy is based on our observation that these candidates often cover the most important information w.r.t.,6.1 Textual Evidence Candidates Selection,[0],[0]
the covered entity mentions.,6.1 Textual Evidence Candidates Selection,[0],[0]
"We formulate the inference problem of finding the best eMRG as an ILP problem due to its convenient integration of both soft and hard constraints.
",6.2 ILP Formulation,[0],[0]
"Given the model parameters β, we reformulate the score of an eMRG in the discriminant function (1) as follows,
β>Φ(x, y, h) = ∑
(a,c)∈(y,h) saczac +
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ saa′zaa′
where sac = β>Φe(x, a, c) denotes the score of a labeled edge a attached with a textual evidence c, saa′ = β
>Φc(a, a′) is the edge co-occurrence score, the binary variable zac indicates the presence or absence of the corresponding edge, and zaa′ indicates if two edges co-occurr.",6.2 ILP Formulation,[0],[0]
"As not every edge set can form an eMRG, we require that a valid eMRG should satisfy a set of linear constraints, which form our constraint space.",6.2 ILP Formulation,[0],[0]
"Then function (1) is equivalent to
max z∈B
s>z + µzd
s.t.",6.2 ILP Formulation,[0],[0]
A   z η,6.2 ILP Formulation,[0],[0]
τ   ≤,6.2 ILP Formulation,[0],[0]
"d
z,η, τ ∈ B
where B = 2S with S = {0, 1}, and η and τ are auxiliary binary variables that help define the constraint space.",6.2 ILP Formulation,[0],[0]
"The above optimization problem takes exactly the form of an ILP because both the constraints and the objective function are linear, and all variables take only integer values.
",6.2 ILP Formulation,[0],[0]
"In the following, we consider two types of constraint space, 1) an eMRG with only binary edges and 2) an eMRG with both binary and ternary edges.
eMRG with only Binary Edges: An eMRG has only binary edges if a sentence contains no attribute mention or at most one entity mention.",6.2 ILP Formulation,[0],[0]
We expect that each edge has only one relation type and is supported by a single textual evidence.,6.2 ILP Formulation,[0],[0]
"To facilitate the formulation of constraints, we introduce ηel to denote the presence or absence of a labeled edge el, and ηec to indicate if a textual evidence c is assigned to an unlabeled edge e. Then the binary variable for the corresponding evidentiary edge zelc = ηec ∧ ηel , where the ILP formulation of conjunction can be found in (Martins et al., 2009).
",6.2 ILP Formulation,[0],[0]
Let Ce denote the set of textual evidence candidates of an unlabeled edge e.,6.2 ILP Formulation,[0],[0]
"The constraint of at most one textual evidence per edge is formulated as:
∑
c∈Ce ηec ≤ 1 (3)
",6.2 ILP Formulation,[0],[0]
"Once a textual evidence is assigned to an edge, their relation labels should match and the number of labeled edges must agree with the number of attached textual evidences.",6.2 ILP Formulation,[0],[0]
"Further, we assume that a textual evidence c conveys at most one relation so that an evidence will not be assigned to the relations of different types, which is the main problem for the structural SVM based model.",6.2 ILP Formulation,[0],[0]
"Let ηcl indicate that the textual evidence c is labeled by the relation type l. The corresponding constraints are expressed as,
∑ l∈Le ηel = ∑ c∈Ce ηec; zelc ≤ ηcl; ∑ l∈L ηcl ≤ 1
where Le denotes the set of all possible labels for an unlabeled edge e, and L is the set of all relation types of MRGs (cf. Section 3).
",6.2 ILP Formulation,[0],[0]
"In order to avoid a textual evidence being overly reused by multiple relation candidates, we first penalize the assignment of a textual evidence c to a labeled edge a by associating the corresponding zac with a fixed negative cost −µ in the objective function.",6.2 ILP Formulation,[0],[0]
"Then the selection of one textual evidence per edge a is encouraged by associating µ to zdc in the objective function, where zdc = ∨ e∈Sc ηec and Sc is the set of edges that the textual evidence c serves as a candidate.",6.2 ILP Formulation,[0],[0]
"The disjunction zdc is expressed as:
zdc ≥ ηe, e ∈",6.2 ILP Formulation,[0],[0]
"Sc zdc ≤ ∑
e∈Sc ηe
This soft constraint not only encourages one textual evidence per edge, but also keeps it eligible for multiple assignments.
",6.2 ILP Formulation,[0],[0]
"For any two labeled edge a and a′ incident to the same entity mention, the edge-to-edge cooccurrence is described by zca,a′ = za",6.2 ILP Formulation,[0],[0]
"∧ za′ .
eMRG with both Binary and Ternary Edges: If there are more than one entity mentions and at least one attribute mention in a sentence, an eMRG can potentially have both binary and ternary edges.",6.2 ILP Formulation,[0],[0]
"In this case, we assume that each mention of attributes can participate either in binary relations or in ternary relations.",6.2 ILP Formulation,[0],[0]
"The assumption holds in more than 99.9% of the sentences in our SRG corpus, thus we describe it as a set of hard constraints.",6.2 ILP Formulation,[0],[0]
"Geometrically, the assumption can be visualized as the selection between two alternative structures incident to the same attribute mention, as shown in Figure 4.",6.2 ILP Formulation,[0],[0]
"Note that, in the binary edge structure, we include not only the edges incident to the attribute mention but also the edge between the two entity mentions.
",6.2 ILP Formulation,[0],[0]
Let Sbmi be the set of all possible labeled edges in a binary edge structure of an attribute mention mi.,6.2 ILP Formulation,[0],[0]
Variable τ,6.2 ILP Formulation,[0],[0]
"bmi = ∨ el∈Sbmi
ηel indicates whether the attribute mention is associated with a binary edge structure or not.",6.2 ILP Formulation,[0],[0]
"In the same manner, we use τ tmi = ∨ el∈Stmi
ηel to indicate the association of the an attribute mention mi with an ternary edge structure from the set of all incident ternary edges Stmi .",6.2 ILP Formulation,[0],[0]
"The selection between two alternative structures is
formulated as τ bmi + τ",6.2 ILP Formulation,[0],[0]
t mi = 1.,6.2 ILP Formulation,[0],[0]
"As this influences only the edges incident to an attribute mention, we keep all the constraints introduced in the previous section unchanged except for constraint (3), which is modified as
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"bmi ;
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"tmi
Therefore, we can have either binary edges or ternary edges for an attribute mention.",6.2 ILP Formulation,[0],[0]
"Given a set of training sentences D = {(x1, y1), . . .",7 Learning Model Parameters,[0],[0]
", (xn, yn)}, the best weight vector β of the discriminant function (1) is found by solving the following optimization problem:
min β
1
n
n∑
i=1
[ max (ŷ,ĥ)∈Y(x)×H(x)
(β>Φ(x, ŷ, ĥ)+δ(ĥ, ŷ, y))
",7 Learning Model Parameters,[0],[0]
"− max h̄∈H(x)
β>Φ(x, y, h̄)]",7 Learning Model Parameters,[0],[0]
"+ ρ|β|] (4)
where δ(ĥ, ŷ, y) is a loss function measuring the discrepancies between an eMRG (y, h̄) with gold standard edge labels y and an eMRG (ŷ, ĥ) with inferred labeled edges ŷ and textual evidences ĥ. Due to the sparse nature of the lexical features, we apply L1 regularizer to the weight vector β, and the degree of sparsity is controlled by the hyperparameter ρ.
",7 Learning Model Parameters,[0],[0]
"Since the L1 norm in the above optimization problem is not differentiable at zero, we apply the online forward-backward splitting (FOBOS) algorithm (Duchi and Singer, 2009).",7 Learning Model Parameters,[0],[0]
"It requires two steps for updating the weight vector β by using a single training sentence x on each iteration t.
βt+ 12 = βt",7 Learning Model Parameters,[0],[0]
"− εt∆t
βt+1",7 Learning Model Parameters,[0],[0]
"= arg min β
1 2 ‖β",7 Learning Model Parameters,[0],[0]
"− βt‖2 + εtρ|β|
where ∆t is the subgradient computed without considering the L1 norm and εt is the learning rate.",7 Learning Model Parameters,[0],[0]
"For a labeled sentence x, ∆t = Φ(x, ŷ∗, ĥ∗) − Φ(x, y, h̄∗), where the feature functions of the corresponding eMRGs are inferred by solving (ŷ∗, ĥ∗) = arg max(ĥ,ŷ)∈H(x)×Y(x)[β >Φ(x, ŷ, ĥ) +",7 Learning Model Parameters,[0],[0]
"δ(ĥ, ŷ, y)] and (y, h̄∗) = arg maxh̄∈H(x) β",7 Learning Model Parameters,[0],[0]
>,7 Learning Model Parameters,[0],[0]
"Φ(x, y, h̄), as indicated in the optimization problem (4).
",7 Learning Model Parameters,[0],[0]
The former inference problem is similar to the one we considered in the previous section except the inclusion of the loss function.,7 Learning Model Parameters,[0],[0]
"We incorporate the loss function into the ILP formulation by defining the loss between an MRG (y, h) and a gold standard MRG as the sum of per-edge costs.",7 Learning Model Parameters,[0],[0]
"In our experiments, we consider a positive cost ϕ for each wrongly labeled edge a, so that if an edge a has a different label from the gold standard, we add ϕ to the coefficient sac of the corresponding variable zac in the objective function of the ILP formulation.
",7 Learning Model Parameters,[0],[0]
"In addition, since the non-positive weights of edge labels in the initial learning phrase often lead to eMRGs with many unlabeled edges, which harms the learning performance, we fix it by adding a constraint for the minimal number of labeled edges in an eMRG, ∑
a∈A
∑
c∈Ca ηac ≥ ζ (5)
where A is the set of all labeled edge candidates and ζ denotes the minimal number of labeled edges.
",7 Learning Model Parameters,[0],[0]
"Empirically, the best way to determine ζ is to make it equal to the maximal number of labeled edges in an eMRG with the restriction that a textual evidence can be assigned to at most one edge.",7 Learning Model Parameters,[0],[0]
"By considering all the edge candidates A and all the textual evidence candidates C as two vertex sets in a bipartite graph Ĝ = 〈V = (A,C), E〉 (with edges in E indicating which textual evidence can be assigned to which edge), ζ corresponds to exactly the size of a maximum matching of the bipartite graph1.
",7 Learning Model Parameters,[0],[0]
"To find the optimal eMRG (y, h̄∗), for the gold label k of each edge, we consider the following set of constraints for inference since the labels of the edges are known for the training data,
∑ c∈Ce ηec ≤ 1; ηec ≤ lck ∑ k′∈L lck′ ≤ 1; ∑ e∈Sc ηec ≤ 1
",7 Learning Model Parameters,[0],[0]
"We include also the soft constraints, which avoid a textual evidence being overly reused by multiple relations, and the constraints similar to (5) to ensure a minimal number of labeled edges and a minimal number of sentiment-oriented relations.
",7 Learning Model Parameters,[0],[0]
"1It is computed by the Hopcroft-Karp algorithm (Hopcroft and Karp, 1973) in our implementation.",7 Learning Model Parameters,[0],[0]
"For evaluation we constructed the SRG corpus, which in total consists of 1686 manually annotated online reviews and forum posts in the digital camera and movie domains2.",8 SRG Corpus,[0],[0]
"For each domain, we maintain a set of attributes and a list of entity names.
",8 SRG Corpus,[0],[0]
The annotation scheme for the sentiment representation asserts minimal linguistic knowledge from our annotators.,8 SRG Corpus,[0],[0]
"By focusing on the meanings of the sentences, the annotators make decisions based on their language intuition, not restricted by specific syntactic structures.",8 SRG Corpus,[0],[0]
"Taking the example in Figure 2, the annotators only need to mark the mentions of entities and attributes from both the sentences and the context, disambiguate them, and label (“Canon 7D”, “Nikon D7000”, price) as worse and (“Canon 7D”, “sensor”) as positive, whereas in prior work, people have annotated the sentiment-bearing expressions such as “great” and link them to the respective relation instances as well.",8 SRG Corpus,[0],[0]
"This also enables them to annotate instances of both sentiment polarity and comparative relaton, which are conveyed by not only explicit sentiment-bearing expressions like “excellent performance”, but also factual expressions implying evaluations such as “The 7V has 10x optical zoom and the 9V has 16x.”.
",8 SRG Corpus,[0],[0]
14 annotators participated in the annotation project.,8 SRG Corpus,[0],[0]
"After a short training period, annotators worked on randomly assigned documents one at a time.",8 SRG Corpus,[0],[0]
"For product reviews, the system lists all relevant information about the entity and the predefined attributes.",8 SRG Corpus,[0],[0]
"For forum posts, the system shows only the attribute list.",8 SRG Corpus,[0],[0]
"For each sentence in a document, the annotator first determines if it refers to an entity of interest.",8 SRG Corpus,[0],[0]
"If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Amazon.com; the 667 camera forum posts are downloaded from forum.digitalcamerareview.com; the 138 movie reviews and 774 forum posts are from imdb.com and boards.ie respectively
as off-topic.",8 SRG Corpus,[0],[0]
"Otherwise, the annotator will identify the most obvious mentions, disambiguate them, and mark the MRGs.",8 SRG Corpus,[0],[0]
"We evaluate the inter-annotator agreement on sSoRs in terms of Cohen’s Kappa (κ) (Cohen, 1968).",8 SRG Corpus,[0],[0]
"An average Kappa value of 0.698 was achieved on a randomly selected set consisting of 412 sentences.
",8 SRG Corpus,[0],[0]
Table 1 shows the corpus distribution after normalizing them into sSoRs.,8 SRG Corpus,[0],[0]
Camera forum posts contain the largest proportion of comparisons because they are mainly about the recommendation of digital cameras.,8 SRG Corpus,[0],[0]
"In contrast, web users are much less interested in comparing movies, in both reviews and forums.",8 SRG Corpus,[0],[0]
"In all subsets, positive relations play a dominant role since web users intend to express more positive attitudes online than negative ones (Pang and Lee, 2007).",8 SRG Corpus,[0],[0]
This section describes the empirical evaluation of SENTI-LSSVM together with two competitive baselines on the SRG corpus.,9 Experiments,[0],[0]
"We implemented a rule-based baseline (DINGRULE) and a structural SVM (Tsochantaridis et al., 2004) baseline (SENTI-SSVM) for comparison.",9.1 Experimental Setup,[0],[0]
"The former system extends the work of Ding et al. (2009), which designed several linguisticallymotivated rules based on a sentiment polarity lexicon for relation identification and assumes there is only one type of sentiment relation in a sentence.",9.1 Experimental Setup,[0],[0]
"In our implementation, we keep all the rules of (Ding et al., 2009) and add one phrase-level rule when there are more than one mention in a sentence.",9.1 Experimental Setup,[0],[0]
The additional rule assigns sentiment-bearing words and negators to its nearest relation candidates based on the absolute surface distance between the words and the corresponding mentions.,9.1 Experimental Setup,[0],[0]
"In this case, the phraselevel sentiment-oriented relations depend only on the assigned sentiment words and negators.",9.1 Experimental Setup,[0],[0]
The latter system is based on a structural SVM and does not consider the assignment of textual evidences to relation instances during inference.,9.1 Experimental Setup,[0],[0]
"The textual features of a relation candidate are all lexical and sentiment predictor features within a surface distance of four words from the mentions of the candidate.
",9.1 Experimental Setup,[0],[0]
"Thus, this baseline does not need the inference constraints of SENTI-LSSVM for the selection of textual evidences.",9.1 Experimental Setup,[0],[0]
"To gain more insights into the model, we also evaluate the contribution of individual features of SENTI-LSSVM.",9.1 Experimental Setup,[0],[0]
"In addition, to show if identifying sentiment polarities and comparative relations jointly works better than tackling each task on its own, we train SENTI-LSSVM for each task separately and combine their predictions according to compatibility rules and the corresponding graph scores.
",9.1 Experimental Setup,[0],[0]
"For each domain and text genre, we withheld 15% documents for development and use the remaining for cross validation.",9.1 Experimental Setup,[0],[0]
The hyperparameters of all systems are tuned on the development datasets.,9.1 Experimental Setup,[0],[0]
"For all experiments of SENTI-LSSVM, we use ρ = 0.0001 for the L1 regularizer in Eq.(4) and ϕ = 0.05 for the loss function; and for SENTI-SSVM, ρ = 0.0001 and ϕ = 0.01.",9.1 Experimental Setup,[0],[0]
"Since the relation type of off-topic sentences is certainly other, we evaluate all systems with 5-fold cross-validation only on the on-topic sentences in the evaluation dataset.",9.1 Experimental Setup,[0],[0]
"Since the same sSoR can have several equivalent MRGs and the relation type other is not of our interest, we evaluate the sSoRs in terms of precision, recall and F-measure.",9.1 Experimental Setup,[0],[0]
All reported numbers are averages over the 5 folds.,9.1 Experimental Setup,[0],[0]
Table 2 shows the complete results of all systems.,9.2 Results,[0],[0]
Here our model SENTI-LSSVM outperformed all baselines in terms of the average F-measure scores and recalls by a large margin.,9.2 Results,[0],[0]
The F-measure on movie reviews is about 14% over the best baseline.,9.2 Results,[0],[0]
The rule-based system has higher precision than recall in most cases.,9.2 Results,[0],[0]
"However, simply increasing the coverage of the domain independent sentiment polarity lexicon might lead to worse performance (Taboada et al., 2011) because many sentiment oriented relations are conveyed by domain dependent expressions and factual expressions implying evaluations, such as “This camera does not have manual control.”",9.2 Results,[0],[0]
"Compared to DING-RULE, SENTI-SSVM performs better in the camera domain but worse for the movies due to many misclassification of negative relation instances as other.",9.2 Results,[0],[0]
It also wrongly predicted more positive instances as other than SENTI-LSSVM.,9.2 Results,[0],[0]
"We found that the recalls of these instances are low because they often have overly similar features with the instances of the type
other linking to the same mentions.",9.2 Results,[0],[0]
The problem gets worse in the movie domain since i),9.2 Results,[0],[0]
many sentences contain no explicit sentiment-bearing words; ii) the prior polarity of the sentiment-bearing words do not agree with their contextual polarity in the sentences.,9.2 Results,[0],[0]
Consider the following example from a forum post about the movie “Superman Returns”: “Have a look at Superman: the Animated Series or Justice League Unlimited . . .,9.2 Results,[0],[0]
that is how the characters of Superman and Lex Luthor should be.”.,9.2 Results,[0],[0]
"In contrast, our model minimizes the overlapping features by assigning them to the most likely relation candidates.",9.2 Results,[0],[0]
This leads to significantly better performance.,9.2 Results,[0],[0]
"Although SENTI-SSVM has low recall for both positive and negative relations, it achieves the highest recall for the comparative relation among all systems in the movie domain and camera reviews.",9.2 Results,[0],[0]
"Since less than 1% of all instances are for comparative relations in these document sets and all models are trained to optimize the overall accuracy, SENTILSSVM intends to trade off the minority class for the overall better performance.",9.2 Results,[0],[0]
"This advantage disappears on the camera forum posts, where the number of instances of comparative relation is 12 times more than that in the other data sets.
",9.2 Results,[0],[0]
All systems perform better in predicting positive relations than the negative ones.,9.2 Results,[0],[0]
"This corresponds well to the empirical findings in (Wilson, 2008) that people intend to use more complex expressions for negative sentiments than their affirmative counterparts.",9.2 Results,[0],[0]
It is also in accordance with the distribution of these relations in our SRG corpus which is randomly sampled from the online documents.,9.2 Results,[0],[0]
"For learning systems, it can also be explained by the fact that the training data for positive relations are considerably more than those for negative ones.",9.2 Results,[0],[0]
"The comparative relation is the hardest one to process since we found that many corresponding expressions do not contain explicit keywords for comparison.
",9.2 Results,[0],[0]
"To understand the performance of the key feature groups in our model better, we remove each group from the full SENTI-LSSVM system and evaluate the variations with movie reviews and camera forum posts, which have relatively balanced distribution of relation types.",9.2 Results,[0],[0]
"As shown in Table 3, the features from the sentiment predictors make significant contributions for both datasets.",9.2 Results,[0],[0]
"The different drops of the performance indicate that the po-
larities predicted by rules are more consistent in camera forum posts than in movie reviews.",9.2 Results,[0],[0]
Due to the complexity of expressions in the movie reviews our model cannot benefit from the unigram features but these features are a good compensation for the sentiment predictor features in camera forum posts.,9.2 Results,[0],[0]
The sharp drop by removing the context features from our model on movie reviews indicates that the sentiments in movie reviews depend highly on the relations of the previous sentences.,9.2 Results,[0],[0]
"In contrast, the sentiment-oriented relations of the previous sentences could be a reason of overfitting for camera forum data.",9.2 Results,[0],[0]
The edge co-occurrence features do not play an important role in our model since the number of co-occurred sentiment-oriented relations in the sentences with contrast conjunctions like “but” is small.,9.2 Results,[0],[0]
"However, we found that allowing the co-occurrence of any sentiment-oriented relations would harm the performance of the model.
",9.2 Results,[0],[0]
"In addition, our experiments showed that the sep-
arated approach, which trains a model for sentiment polarities and comparative relations respectively, leads to a decrease by almost 1% in terms of the F-measure averaged over all four datasets.",9.2 Results,[0.9500508807175201],['Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal.11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU.']
"The largest drop of F-measure is 3% on camera forum posts, since this dataset contains the largest proportion of comparative relations.",9.2 Results,[0],[0]
We found that the errors are increased when the trained models make conflicting predictions.,9.2 Results,[0],[0]
"In this case, the joint approach can take all factors into account and make more consistent decisions than the separated approaches.",9.2 Results,[0],[0]
We proposed SENTI-LSSVM model for extracting instances of both sentiment polarities and comparative relations.,10 Conclusion,[0],[0]
"For evaluating and training the model, we created an SRG corpus by using a lightweight annotation scheme.",10 Conclusion,[0],[0]
We showed that our model can automatically find textual evidences to support its relation predictions and achieves significantly better F-measure scores than alternative state-of-the-art methods.,10 Conclusion,[0],[0]
Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis.,abstractText,[0],[0]
"Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program.",abstractText,[0],[0]
"Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest.",abstractText,[0],[0]
The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts).,abstractText,[0],[0]
The gold standard corpus that we built will also be a valuable resource for the community.,abstractText,[0],[0]
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3654–3663 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3654",text,[0],[0]
"Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages.",1 Introduction,[0],[0]
"It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015).",1 Introduction,[0],[0]
"As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016).
",1 Introduction,[0],[0]
"∗Corresponding author
Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao.",1 Introduction,[0],[0]
"In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s).",1 Introduction,[0],[0]
"With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a).",1 Introduction,[0],[0]
"Comparing to the traditional reviews, the QA style reviews can be more informative and convincing.",1 Introduction,[0],[0]
"More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful.
",1 Introduction,[0],[0]
"Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016).",1 Introduction,[0],[0]
"However, because of the significant differences between QA-style and classical reviews, existing review mining algorithms, e.g., text-based sentiment analysis/classification, should not be di-
rectly applied to this new kind of QA-style data.",1 Introduction,[0],[0]
"More detailed reasons can be found as the followings.
",1 Introduction,[0],[0]
"First, in QA-style text, the question and answer text are more likely to be two parallel units rather than a sequence form.",1 Introduction,[0],[0]
"On the one hand, for instance, in Figure 1, sentence “It’s a nice phone with high-quality screen.”",1 Introduction,[0],[0]
in Answer 1 actually does not follow sentence “How is the battery?”,1 Introduction,[0],[0]
"in Question 1 , but corresponds to sentence “Is the screen clear?”",1 Introduction,[0],[0]
in Question 1.,1 Introduction,[0],[0]
"Therefore, when the question text and answer text are presented as two units in a sequence, it is rather difficult to capture the relationship between the question and its corresponding answer due to the possible long distance between them.",1 Introduction,[0],[0]
"On the other hand, there often exists both positive and negative sentiments in answer text according to different parts of question, and this specific case should be categorized as another category named conflict.",1 Introduction,[0],[0]
"For instance, in Figure 1, Answer 1",1 Introduction,[0],[0]
“It’s a nice phone with high-quality screen.,1 Introduction,[0],[0]
But the battery is not durable.”,1 Introduction,[0],[0]
is a conflict answer to Question 1.,1 Introduction,[0],[0]
"However, when this answer text is considered as a sequence, it is highly possible to be predicted as the category of positive or negative rather than conflict.",1 Introduction,[0],[0]
"In order to address these problems, a more appropriate approach is to segment both the question and answer text into some parallel sentences, and then construct the [Q-sentence, A-sentence] units in each QA text pair to detect in-depth sentiment information.
",1 Introduction,[0],[0]
"Second, although the main sentiment polarity is usually expressed from the answer text, the question text could also carry important sentiment tips to predict the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
"For instance, in Figure 1, we could hardly estimate the sentiment polarity solely based on Answer 2.",1 Introduction,[0],[0]
"However, when we take Question 2, “Is the sun cream really effective?”, into consideration, it can be easier to label this QA text pair with a negative tag.",1 Introduction,[0],[0]
"In this study, we propose an approach to match the sentences inside the question and answer text bidirectionally.
",1 Introduction,[0],[0]
"Third, in each QA text pair, the importance degrees of different [Q-sentence, A-sentence] units can be different.",1 Introduction,[0],[0]
"For instance, in Figure 1, the [Qsentence, A-sentence] unit, i.e., sentence “Summer is coming, I’m afraid of getting darker.”",1 Introduction,[0],[0]
"in Answer 2 and sentence “No, just depending on my own experience.”",1 Introduction,[0],[0]
"in Question 2, makes tiny contribution to imply the sentiment polarity for the
QA text pair.",1 Introduction,[0],[0]
"Therefore, a well-behaved network approach should consider the importance degrees of different [Q-sentence, A-sentence] units for predicting the sentiment polarity of a QA text pair.
",1 Introduction,[0],[0]
The contribution of this paper is twofold.,1 Introduction,[0],[0]
"First, we propose a novel problem, QA-style sentiment analysis, and build a large-scale annotated corpus tailed for this task.",1 Introduction,[0],[0]
The dataset is released to motivate future investigations for this track of research.,1 Introduction,[0],[0]
"Second, we propose a hierarchical matching network model to address the challenges of QA-style sentiment classification.",1 Introduction,[0],[0]
"Specifically, we first segment both the question and answer text into sentences and construct the [Q-sentence, A-sentence] units for each QA text pair.",1 Introduction,[0],[0]
"Then, by using a QA bidirectional matching layer, we encode each [Q-sentence, A-sentence] unit for exploring sentiment information.",1 Introduction,[0],[0]
"Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification.,1 Introduction,[0],[0]
Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002).,2 Related Work,[0],[0]
"In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level.
",2 Related Work,[0],[0]
Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources.,2 Related Work,[0],[0]
Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits.,2 Related Work,[0],[0]
"Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995).",2 Related Work,[0],[0]
"More recently, some studies aim to learn better word embedding of a word rather than its polarity.",2 Related Work,[0],[0]
"Tang et al. (2014) developed three neural networks to learn word em-
bedding by incorporating sentiment polarities of text in loss functions.",2 Related Work,[0],[0]
"Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding.
",2 Related Work,[0],[0]
Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
"On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011).",2 Related Work,[0],[0]
"On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification.",2 Related Work,[0],[0]
Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification.,2 Related Work,[0],[0]
Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts.,2 Related Work,[0],[0]
"More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification.
",2 Related Work,[0],[0]
Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task.,2 Related Work,[0],[0]
"Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence.",2 Related Work,[0],[0]
Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance.,2 Related Work,[0],[0]
"Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect.
",2 Related Work,[0],[0]
"Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification.",2 Related Work,[0],[0]
"To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level.",2 Related Work,[0],[0]
"We collect QA text pairs from “Asking All” in Taobao (Alibaba)1, which is the world’s biggest ecommerce company.",3 Data Collection and Annotation,[0],[0]
"The QA text pairs are mainly from Beauty, Shoe and Electronic domains and each domain contains 10,000 QA text pairs.
",3 Data Collection and Annotation,[0],[0]
"We define four sentiment-related categories, i.e., positive, negative, conflict (both positive and negative sentiment) and neutral (neither positive nor negative sentiment).",3 Data Collection and Annotation,[0],[0]
"To guarantee a high annotation agreement, we propose some annotation guidelines after several times of annotation processes on a small size of data.",3 Data Collection and Annotation,[0],[0]
"Then, we ask more coders to annotate the whole data set according to these annotation guidelines.
",3 Data Collection and Annotation,[0],[0]
The annotation guidelines contain two main groups.,3 Data Collection and Annotation,[0],[0]
"One contains the guidelines which aim to distinguish the categories of neutral and nonneutral, i.e., (a) A QA text pair in which the question and the answer do not match is annotated as a neutral sample.",3 Data Collection and Annotation,[0],[0]
"In this type of samples, the answer does not reply to the question correctly.",3 Data Collection and Annotation,[0],[0]
"E1 is an example of this type where the question talks about the screen while the answer talks about the battery.
",3 Data Collection and Annotation,[0],[0]
E1: Q: Is the screen clear?,3 Data Collection and Annotation,[0],[0]
"A: The battery life is decent.
",3 Data Collection and Annotation,[0],[0]
(b) A QA text pair with an unknown or uncertain answer is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E2 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E2: Q: What about these sneakers?,3 Data Collection and Annotation,[0],[0]
"A: I don’t know, I bought it for my dad.
",3 Data Collection and Annotation,[0],[0]
(c),3 Data Collection and Annotation,[0],[0]
A QA text pair with only objective description is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E3 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E3: Q: What’s the operation system of the phone?,3 Data Collection and Annotation,[0],[0]
"A: Android.
",3 Data Collection and Annotation,[0],[0]
(d),3 Data Collection and Annotation,[0],[0]
A QA text pair which compares two different products is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"In this type of samples, two products are involved and it
1https://www.taobao.com/
is sometimes difficult to tell the sentiment orientation of one product.",3 Data Collection and Annotation,[0],[0]
"E4 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E4: Q: How about this phone when compared to iPhone 6s?,3 Data Collection and Annotation,[0],[0]
"A: It’s up to you, and they’re not comparable.
",3 Data Collection and Annotation,[0],[0]
"The other group contains the guidelines which aim to distinguish the categories of positive and negative, i.e., (e) If the answer text contains sentimental expressions to question like “disappointed”, “terrible”, and so on, we annotate it as negative.",3 Data Collection and Annotation,[0],[0]
"E5 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E5: Q: How is the rock climbing shoe?,3 Data Collection and Annotation,[0],[0]
"A: I am so disappointed, my feet felt hurt when I wore them.
",3 Data Collection and Annotation,[0],[0]
(f),3 Data Collection and Annotation,[0],[0]
"If the answer text contains sentimental expressions to question like “perfect”, “satisfied”, and so on, we annotate it as positive.",3 Data Collection and Annotation,[0],[0]
"E6 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E6: Q: How about the fragrance?,3 Data Collection and Annotation,[0],[0]
"A: I am so satisfied, it smells distinctive.
",3 Data Collection and Annotation,[0],[0]
"(g) If we cannot confirm the polarity of a QA text pair only depending on answer text, we annotate the polarity according to both the question and answer text.",3 Data Collection and Annotation,[0],[0]
"For instance, E7 is an example with positive polarity, while E8 is an example with negative polarity.
",3 Data Collection and Annotation,[0],[0]
E7: Q: Will the phone get hot when gaming?,3 Data Collection and Annotation,[0],[0]
"A: No.
E8: Q: Is the sun cream really economic?",3 Data Collection and Annotation,[0],[0]
"A: No.
We assign two annotators to annotate each QA text pair, and the Kappa consistency check value of the annotation is 0.84.",3 Data Collection and Annotation,[0],[0]
"When annotators cannot reach an agreement, an expert will make the final decision, ensuring the quality of data annotation.",3 Data Collection and Annotation,[0],[0]
Table 1 shows the category distribution of the corpus.,3 Data Collection and Annotation,[0],[0]
"To motivate other scholars to investigate this novel but important task, we share the data via Github2.",3 Data Collection and Annotation,[0],[0]
"In this section, we introduce the proposed hierarchical matching network approach for QAstyle sentiment classification.",4 Methodology,[0],[0]
Figure 2 depicts the overview of the proposed approach.,4 Methodology,[0],[0]
Word Encoding Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After sentence segmentation, the question text in a QA text pair contains N sentences, SQi represents the i-th sentence in the question text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Similarly, the answer text in this QA text pair contains M sentences, SAj represents the j-th sentence in the answer text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then construct [Q-sentence, A-sentence] units by pairing one sentence in the question text and one sentence in the answer text, and we obtain N*M [Q-sentence, A-sentence] units at last.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given a [SQi , SAj ] unit in this QA text pair, i.e., Q-sentence SQi with words wi,n, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈
2https://github.com/clshenNLP/QASC/
[1, Ni] and A-sentence SAj with words wj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ], we first convert the words to their respective word embeddings (xi,n ∈ Rd, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, Ni] and xj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ]).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then use Bi-directional LSTM (namely Bi-LSTM), which can efficiently make use of past features (via forward states) and future features (via backward states) for a specific time step, to get contextual representations of SQi and SAj individually.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
The representation of each word is formed by concatenating the forward and backward hidden states.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For simplicity, we note contextual representation of SQi asHQi , and contextual representation of SAj as HAj respectively:
HQi =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hi,1, hi,2, ..., hi,n, ..., hi,Ni ] (1)
HAj =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hj,1, hj,2, ..., hj,m, ..., hj,Mj ] (2)
where hi,n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Rd ′
denotes the word representation in SQi at time step n, hj,m ∈ Rd ′ denotes the word representation in SAj at time step m, and d ′ is the dimensionality of word representation.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
QA Bidirectional Matching Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"General neural network could not capture sentiment matching information in a [SQi , SAj ] unit well.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For the sake of solving this problem, we introduce the QA bidirectional matching layer to encapsulate the clues and interactions between SQi and SAj synchronously (Tay et al., 2017; McCann et al., 2017).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Figure 3 depicts the detail architecture of QA bidirectional matching mechanism.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Specifically, we first calculate the bidirectional pair-wise matching matrix by using the fol-
lowing formula:
D[i,j] = (HQi)",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"> · (HAj ) (3)
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"where D[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"RNi×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given the bidirectional matching matrix D[i,j], we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
•,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Answer-to-Question Attention: We employ row-wise operations to compute the attention weight vector αr[i,j] as follows:
U r[i,j] = tanh(Wr ·D >",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j]) (4)
αr[i,j] = softmax(w > r · U r[i,j]) (5)
where αr[i,j] ∈ R Ni is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Rd′×Mj and wr ∈ Rd ′ are weight matrices.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After computing the Answer-to-Question attention weight vector, we can get the Answer-to-Question matching vector V r[i,j] ∈ R d′ as follows:
V r[i,j] = (HQi) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α r,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (6)
• Question-to-Answer Attention: Simultaneously, we employ column-wise operations to calculate the attention weight vector αc[i,j] as follows:
U c[i,j] = tanh(Wc ·D[i,j]) (7) αc[i,j] =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"softmax(w > c · U c[i,j]) (8)
where αc[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"R Mj is the Question-to-Answer attention weight vector regarding the importance degrees of all words in A-sentence SAj , Wc ∈ Rd′×Ni and wc ∈ Rd ′ are weight matrices.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After calculating the Question-to-Answer attention weight vector, we can get the Question-to-Answer matching vector V c[i,j] ∈ R d′ as follows:
V c[i,j] = (HAj ) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α c,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (9)
Then, we combine Answer-to-Question and Question-to-Answer matching vectors to represent
the final bidirectional matching vector of the [SQi , SAj ] unit:
V[i,j] = V r",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] ⊕ V c",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (10)
where ⊕ denotes the concatenate operator, and V[i,j] denotes the bidirectional matching vector which integrates SQi and SAj with each other.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Through the QA bidirectional matching layer, informative bidirectional matching vectors are generated to pinpoint the sentiment matching information in each [Q-sentence, A-sentence] unit.",4.2 Self-Matching Attention Mechanism,[0],[0]
"Intuitively, each matching vector for [Q-sentence, Asentence] unit holds different importance to a QA text pair.",4.2 Self-Matching Attention Mechanism,[0],[0]
"To better aggregate the evidence from these vectors for inferring the sentiment polarity of the QA text pair, we propose a self-matching attention layer, matching these informative vectors against themselves.",4.2 Self-Matching Attention Mechanism,[0],[0]
Self-Matching Attention Layer:,4.2 Self-Matching Attention Mechanism,[0],[0]
"As aforementioned, we have obtained N*M bidirectional matching vectors through QA bidirectional matching layer, then we calculate the attention weight vector α with these matching vectors by following formulas:
V =",4.2 Self-Matching Attention Mechanism,[0],[0]
"[V[1,1], V[1,2], ..., V[i,j], ..., V[N,M ]] (11)
",4.2 Self-Matching Attention Mechanism,[0],[0]
U = tanh(Wh · V ) (12) α =,4.2 Self-Matching Attention Mechanism,[0],[0]
"softmax(w>h · U) (13)
where α is the attention weight vector which measures the importance of these matching vectors, Wh and wh are the weight matrices.
",4.2 Self-Matching Attention Mechanism,[0],[0]
"Finally, we can get the QA text pair representation R as follows:
R = V · α (14)",4.2 Self-Matching Attention Mechanism,[0],[0]
QA text pair representationR is a high level representation which can be used for classification.,4.3 Classification Model,[0],[0]
"In our approach, we feed R to a softmax classifier:
p = softmax(Wl",4.3 Classification Model,[0],[0]
"·R+ bl) (15)
where p is a set of predicted distribution of the sentiment categories, i.e., positive, negative, neutral, and conflict.",4.3 Classification Model,[0],[0]
"Wl is the weight matrix and bl is the bias.
",4.3 Classification Model,[0],[0]
"To learn the whole model, we train an end-toend model given the training data, and the goal of
training is to minimize the cross-entropy loss, i.e.,
L(θ) =",4.3 Classification Model,[0],[0]
"− S∑
s=1 K∑ k=1",4.3 Classification Model,[0],[0]
yks ·,4.3 Classification Model,[0],[0]
"logŷks + λ‖θ‖ 2 2 (16)
where S is the number of training data.",4.3 Classification Model,[0],[0]
ys is the true sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
ŷs is the predicted sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
K is number of all sentiment categories.,4.3 Classification Model,[0],[0]
"λ is a L2regularization term, θ is the parameter set.",4.3 Classification Model,[0],[0]
"In the above equation, the model parameters are optimized by using Adam (Kingma and Ba, 2014).",4.3 Classification Model,[0],[0]
"In this section, we evaluate the performances of the proposed approach for QA-style sentiment classification.",5 Experimentation,[0],[0]
"• Data Sets: As introduced in Section 3, the annotated QA text pairs cover three different domains.",5.1 Experimental Settings,[0],[0]
"In each domain, we randomly split the data into a training set (80% in each category) and a test set (20% in each category).",5.1 Experimental Settings,[0],[0]
"In addition, we set aside 10% from the training set as the development data for parameters tuning.",5.1 Experimental Settings,[0],[0]
"• Word Segmentation and Embeddings: FudanNLP3 (Qiu et al., 2013) is employed to segment text into Chinese words and word2vec4 (Mikolov et al., 2013) is employed to pre-train word embeddings.",5.1 Experimental Settings,[0],[0]
The vector dimensionality is set to be 100.,5.1 Experimental Settings,[0],[0]
"• Sentence Segmentation: CoreNLP5 (Manning et al., 2014) is employed to segment both the question and answer text into sentences.",5.1 Experimental Settings,[0],[0]
"• Hyper-parameters: In the experiment, all outof-vocabulary words are initialized by sampling from the uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
"All weight matrices are given their initial values by sampling from uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
The LSTM hidden states are set to be 128 and all models are trained by mini-batch of 32 instances.,5.1 Experimental Settings,[0],[0]
The dropout rate is set to 0.2.,5.1 Experimental Settings,[0],[0]
The other hyper-parameters are tuned according to the development data.,5.1 Experimental Settings,[0],[0]
•,5.1 Experimental Settings,[0],[0]
"Evaluation Metric: The performance is evaluated using standard Accuracy and Macro-F1.
3https://github.com/FudanNLP/fnlp/ 4https://code.google.com/archive/p/word2vec/ 5http://stanfordnlp.github.io/CoreNLP/",5.1 Experimental Settings,[0],[0]
The following baseline approaches are employed for comparison.,5.2 Experimental Results,[0],[0]
Note that all the approaches share the same word embeddings for fair comparison.,5.2 Experimental Results,[0],[0]
• SVM:,5.2 Experimental Results,[0],[0]
This baseline employs support vector machine along with word embedding features.,5.2 Experimental Results,[0],[0]
The question and answer text in a QA text pair are chained as a sequence.,5.2 Experimental Results,[0],[0]
• LSTM:,5.2 Experimental Results,[0],[0]
A standard LSTM model utilizes word embeddings and concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bi-LSTM:,5.2 Experimental Results,[0],[0]
A bidirectional LSTM model which concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bidirectional-Match:,5.2 Experimental Results,[0],[0]
"This approach employs QA bidirectional matching mechanism, without taking the sentence segmentation strategy and selfmatching attention mechanism.",5.2 Experimental Results,[0],[0]
• AtoQ-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Answer-to-Question attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
We average the Answer-to-Question matching vectors to represent the QA text pair.,5.2 Experimental Results,[0],[0]
•,5.2 Experimental Results,[0],[0]
QtoA-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Question-to-Answer attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
"• Bidirectional-Match QA: This approach takes the sentence segmentation strategy, and employs QA bidirectional matching mechanism, but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"This is our hierarchical matching network model which takes the sentence segmentation strategy and employs both QA bidirectional matching mechanism and self-matching attention mechanism.
",5.2 Experimental Results,[0],[0]
"Table 2 summarizes the experimental results of all the approaches above, and we can find that:
(1) All LSTM-based approaches are superior to SVM, indicating the effectiveness of neural network for this task.",5.2 Experimental Results,[0],[0]
"(2) The proposed approaches, with novel QA contextual representation, outperform the other baseline approaches.",5.2 Experimental Results,[0],[0]
"(3) When only employing QA bidirectional matching mechanism, Bidirectional-Match QA, which takes the sentence segmentation strategy, consistently outperforms Bidirectional-Match (without sentence segmentation) in all domains.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that sentence segmentation helps to extract the sentiment matching information between the question and answer.,5.2 Experimental Results,[0],[0]
"(4) When comparing to QA unidirectional matching mechanism, Bidirectional-Match QA, which employs QA bidirectional matching mechanism, performs better than AtoQMatch and QtoA-Match.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that both the question and answer information contribute to sentiment polarity of the QA text pair.,5.2 Experimental Results,[0],[0]
"(5) Impressively, the proposed approach HMN significantly outperforms all the other approaches in all domains (p-value<0.05 via ttest).",5.2 Experimental Results,[0],[0]
"It verifies the advantages of both QA bidirectional matching mechanism and selfmatching attention mechanism for this task.
",5.2 Experimental Results,[0],[0]
"Besides, we also implement some more recent state-of-the-art approaches for sentiment classification, which are illustrated in Table 3.",5.2 Experimental Results,[0],[0]
"This result also supports the earlier findings.
",5.2 Experimental Results,[0],[0]
"• CNN-Tensor (Lei et al., 2015):",5.2 Experimental Results,[0],[0]
"This is a stateof-the-art approach to sentence-level sentiment classification, which models n-gram interactions based on tensor product and evaluates all non-
consecutive n-gram vectors as a feature mapping operator for CNNs.",5.2 Experimental Results,[0],[0]
"• Attention-LSTM (Wang et al., 2016):",5.2 Experimental Results,[0],[0]
This is a state-of-the-art approach to aspect-level sentiment classification.,5.2 Experimental Results,[0],[0]
"In our implementation, we ignore the aspect embedding and directly use the outputs of LSTM to yield the attention.",5.2 Experimental Results,[0],[0]
• BiMPM,5.2 Experimental Results,[0],[0]
"(Wang et al., 2017):",5.2 Experimental Results,[0],[0]
"This is a state-ofthe-art approach to QA matching, which matches the question and answer from multiple perspectives.",5.2 Experimental Results,[0],[0]
"In our implementation, we use the matching representation to perform QA-style sentiment classification with a softmax classifier.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"The proposed hierarchical matching network which employs both QA bidirectional matching mechanism and self-matching attention mechanism, and takes the sentence segmentation strategy.
",5.2 Experimental Results,[0],[0]
Table 3 shows the comparison results of these strong baseline approaches and the proposed approach (HMN) in all domains.,5.2 Experimental Results,[0],[0]
"From this table, we can find that: (1) the approaches that take matching strategy, i.e., BiMPM and our approach (HMN), outperform other approaches.",5.2 Experimental Results,[0],[0]
"(2) The proposed approach (HMN) significantly outperforms all the other baseline approaches in terms of both Macro-F1 and Accuracy (p-value<0.05 via ttest), which confirms the initial hypotheses of this study.",5.2 Experimental Results,[0],[0]
"Table 4 shows some examples, along with the predicted categories via different approaches.",5.3 Case Study,[0],[0]
"We can find that: (1) the approaches based on matching strategy (BiMPM and HMN) are well-performed, as shown in E9, when question and answer carrying different kinds of information.",5.3 Case Study,[0],[0]
"This is a unique challenge for QA-style sentiment mining, and traditional sentiment classification approaches can hardly address this problem.",5.3 Case Study,[0],[0]
"(2) The proposed approach (HMN) performs better than other approaches when dealing with conflict instances, as shown in E10.",5.3 Case Study,[0],[0]
"To get a better understanding of our proposed hierarchical matching network for QA-style sentiment classification, we picture the attention weights obtained from Equations (5), (8) and (13).",5.4 Visualization of Attention,[0],[0]
"For
simplicity, we directly use the English translation of E11 for illustration and adopt the visualization approach presented by Yang et al. (2016), as shown in Figure 2.",5.4 Visualization of Attention,[0],[0]
"Specifically, each line is a [Qsentence, A-sentence] unit, where the red denotes the [Q-sentence, A-sentence] unit weight, the blue denotes the word weight in each [Q-sentence, Asentence], and the color depth indicates the importance of attention weights (the darker the more important).
",5.4 Visualization of Attention,[0],[0]
"From Figure 4, we can see that the QA bidirectional matching layer always assigns reasonable attention weights to words in each [Q-sentence, Asentence] unit which makes sentence from question and sentence from answer match correctly.",5.4 Visualization of Attention,[0],[0]
"In addition, the self-matching attention layer is able to select informative",5.4 Visualization of Attention,[0],[0]
"[Q-sentence, A-sentence] unit for predicting true sentiment polarity of this example.",5.4 Visualization of Attention,[0],[0]
"In this paper, we propose a novel but important sentiment analysis task, i.e., QA-style sentiment mining, and we build a large-scale highquality human annotated corpus for experiment.",6 Conclusion,[0],[0]
The dataset is shared to encourage other scholars to investigate this interesting problem.,6 Conclusion,[0],[0]
"Moreover, we propose a hierarchical matching neural network model to enable QA bidirectional matching mechanism and self-matching attention mechanism for this task.",6 Conclusion,[0],[0]
"Empirical studies show that the proposed approach significantly outperforms other strong baseline approaches in all the test domains for QA-style sentiment classification.
",6 Conclusion,[0],[0]
"In the future, we would like to investigate some other network structures to explore deeper information in each QA text pair.",6 Conclusion,[0],[0]
"Besides, we would like to test the effectiveness of the proposed approach to QA-style sentiment classification in some other languages.",6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work is partially supported by the National Key R&D Program of China under Grant No.2017YFB1002101 and two NSFC grants No.61331011, No.61672366.",Acknowledgments,[0],[0]
This work is also supported by the joint research project of Alibaba Group and Soochow University.,Acknowledgments,[0],[0]
"In an e-commerce environment, user-oriented question-answering (QA) text pair could carry rich sentiment information.",abstractText,[0],[0]
"In this study, we propose a novel task/method to address QA sentiment analysis.",abstractText,[0],[0]
"In particular, we create a high-quality annotated corpus with speciallydesigned annotation guidelines for QA-style sentiment classification.",abstractText,[0],[0]
"On the basis, we propose a three-stage hierarchical matching network to explore deep sentiment information in a QA text pair.",abstractText,[0],[0]
"First, we segment both the question and answer text into sentences and construct a number of [Q-sentence, Asentence] units in each QA text pair.",abstractText,[0],[0]
"Then, by leveraging a QA bidirectional matching layer, the proposed approach can learn the matching vectors of each [Q-sentence, A-sentence] unit.",abstractText,[0],[0]
"Finally, we characterize the importance of the generated matching vectors via a selfmatching attention layer.",abstractText,[0],[0]
"Experimental results, comparing with a number of state-ofthe-art baselines, demonstrate the impressive effectiveness of the proposed approach for QA-style sentiment classification.",abstractText,[0],[0]
Sentiment Classification towards Question-Answering with Hierarchical Matching Network,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2860–2865 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
NLP research relies heavily on annotated datasets for training and evaluation.,1 Introduction,[0],[0]
"The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.
",1 Introduction,[0],[0]
"When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (Hovy et al., 2006), methods for aggregating annotations (Passonneau and Carpenter, 2014), and intuitive user interfaces (Stenetorp et al., 2012).
",1 Introduction,[0],[0]
"Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al.,
2008; Callison-Burch and Dredze, 2010; Graham et al., 2017), although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (CallisonBurch and Dredze, 2010; Hoßfeld et al., 2014).",1 Introduction,[0],[0]
"In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (Koehn and Monz, 2006; Bojar et al., 2016).
",1 Introduction,[0],[0]
"When crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a “HIT” (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes.",1 Introduction,[0],[0]
"The order of items is generally fixed across all annotators of an individual HIT (Snow et al., 2008; Graham et al., 2017).
",1 Introduction,[0],[0]
"In this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals’ assessment of an item.",1 Introduction,[0],[0]
"Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).
",1 Introduction,[0],[0]
"Possible explanations for sequence effects include:
Gambler’s fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution.",1 Introduction,[0],[0]
"In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True).",1 Introduction,[0],[0]
"So if they assign 0 to an item,
2860
they may approach the next item with a prior belief that it is more likely to be a 1 than a 0.",1 Introduction,[0],[0]
"Chen et al. (2016) showed evidence for the gambler’s fallacy in decisions of loan officers, asylum judges, and baseball umpires.
",1 Introduction,[0],[0]
Sequential contrast effects: A high quality item may raise the bar for the next item.,1 Introduction,[0],[0]
"On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear)
",1 Introduction,[0],[0]
"Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item.",1 Introduction,[0],[0]
"If they focus on similarities between the previous and current instance, the annotations show an assimilation effect (Geiselman et al., 1984; Damisch et al., 2006).",1 Introduction,[0],[0]
"Anchoring effects may decrease as people gain experience and expertise in the task (Wilson et al., 1996).",1 Introduction,[0],[0]
"We test whether the annotation of an instance is correlated with the annotation on previous instances, conditioned on control variables such as the gold standard (i.e. expert annotations1), based on the following linear model:
Yi,t = β0 + β1Yi,t−1 + β2Gold + η (1)
where Yi,t is the annotation given by an annotator i to an instance t, and η is white Gaussian noise with zero mean.",2 Methodology,[0],[0]
We use linear regression for continuous data and logistic regression for binary data.2,2 Methodology,[0],[0]
"If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive β2 value) and bias term (β0), with β1 set to zero.",2 Methodology,[0],[0]
"When we use the ground truth as a control, if β1 is non-zero, it is evidence of mistakes being made by annotators due to sequential bias.",2 Methodology,[0],[0]
"A positive value of β1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler’s fallacy.",2 Methodology,[0],[0]
"Accordingly, we test the statistical significance of
1For the Machine Translation dataset described in Section 3.3, we use the mean of at least fifteen crowd workers as a proxy for expert annotations.
",2 Methodology,[0],[0]
"2η is not included in the case of logistic regression
the β1 6= 0",2 Methodology,[0],[0]
to determine whether sequencing effects are present in crowdsourced text corpora.,2 Methodology,[0],[0]
"We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation.",3 Experiments,[0],[0]
"First, we examine the recognising textual entailment (“RTE”) and event temporal ordering (“TEMPORAL”) datasets from Snow et al. (2008).",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"With the TEMPORAL dataset, they are shown two sentences describing events, and asked to indicate which of the two events occurred first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
Both datasets include both expert annotations and crowdsourced annotations constructed using Amazon Mechanical Turk (“MTurk”).,3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"For both tasks, each HIT was annotated by 10 workers.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"Results We use logistic regression on worker labels against labels on the previous instance in the current HIT, with the expert judgements as a control variable.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also add an additional control, namely the percentage of True labels assigned by the worker overall, which accounts for the overall annotator bias.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"To calculate this, we use scores by the worker excluding the current score, to avoid giving the model any information about the current instance.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"As shown in Table 1, over all workers (“All”), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"One possibility
is that this is biased by opportunistic workers who assign the same label to all instances in the HIT, for which we would not expect any sequential bias effects.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"When we exclude these workers (“Moderate”), the autocorrelation increases, and is highly statistically significant.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also show results for workers with at least 60% accuracy when compared to expert annotations (“Good”), and observe a similar effect.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the affective text analysis task (“AFFECTIVE”), annotators are asked to rate news headlines for anger, disgust, fear, joy, sadness, and surprise on a continuous scale of 0–100.",3.2 Affective text analysis,[0],[0]
"Besides these emotions, they are asked to rate sentences for (emotive) valence, i.e., how strongly negative or positive they are (−100 to +100).",3.2 Affective text analysis,[0],[0]
"In this dataset, there are 100 headlines divided into 10 HITs, with 10 workers annotating each HIT (Snow et al., 2008).",3.2 Affective text analysis,[0],[0]
"We test for autocorrelation of scores of each aspect individually, controlling for the expert scores and worker correlation with the expert scores.",3.2 Affective text analysis,[0],[0]
"We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5.
",3.2 Affective text analysis,[0],[0]
"Results For individual emotions, we do not observe any significant autocorrelation (p ≥ 0.05).",3.2 Affective text analysis,[0],[0]
"As there are only 1000 annotations per emotion, we also look at results when combining data for all aspects.",3.2 Affective text analysis,[0],[0]
"Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2).",3.2 Affective text analysis,[0],[0]
"Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects.",3.2 Affective text analysis,[0],[0]
"When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output.",3.3 Machine Translation Adequacy,[0],[0]
"In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100- point sliding scale which is initialised at the mid point.",3.3 Machine Translation Adequacy,[0],[0]
There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration.,3.3 Machine Translation Adequacy,[0],[0]
"They are given no other instructions or
guidelines.",3.3 Machine Translation Adequacy,[0],[0]
"In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire data from WMT 2013 (Bojar et al., 2013).",3.3 Machine Translation Adequacy,[0],[0]
"The dataset consists of 12 HITS of 100 sentence pairs each; each HIT is annotated by at least 15 workers.
",3.3 Machine Translation Adequacy,[0],[0]
HITs are designed to include quality control items to filter out poor quality scores.,3.3 Machine Translation Adequacy,[0],[0]
"In addition to 70 MT system translations, each HIT contains degraded versions of 10 of these translations, 10 reference translations by a human expert corresponding to 10 of these translations, and repeats of another 10 translations.",3.3 Machine Translation Adequacy,[0],[0]
"Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations.",3.3 Machine Translation Adequacy,[0],[0]
Workers who submitted scores of clearly bad quality were rejected.,3.3 Machine Translation Adequacy,[0],[0]
"For the remaining workers, the Wilcoxon rank-sum test is used to test whether the score difference between the repeat judgements is less than the score difference between translations and the corresponding degraded versions.",3.3 Machine Translation Adequacy,[0],[0]
"We divide these workers into “good” and “moderate” based on the threshold of p < 0.05.
",3.3 Machine Translation Adequacy,[0],[0]
"To eliminate differences due to different internal scales, every individual worker’s scores are standardised by subtracting the mean and dividing by the standard deviation of their scores.",3.3 Machine Translation Adequacy,[0],[0]
"Following Graham et al. (2015), we use the average of standardised scores of at least 15 good workers as the ground truth.
",3.3 Machine Translation Adequacy,[0],[0]
"We refer to the final dataset as “MTadeq”.
Results As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control.",3.3 Machine Translation Adequacy,[0],[0]
"We also controlled for worker correlation with mean score, and position of the sentence in the HIT, but these were not significant and did not affect the autocorrelation.",3.3 Machine Translation Adequacy,[0],[0]
"As seen in Table 3, we see a small but significant positive autocorrelation for good workers.",3.3 Machine Translation Adequacy,[0],[0]
"The bias is much stronger with
bad (rejected) workers.",3.3 Machine Translation Adequacy,[0],[0]
"An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task.",3.3 Machine Translation Adequacy,[0],[0]
"Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 4, for good and moderate workers, the bias is stronger in the first group of sentences annotated, decreases in the second, and is much smaller in the last.",3.3 Machine Translation Adequacy,[0],[0]
"This could be because workers are familiarising themselves with the task earlier on, and calibrating their scale.",3.3 Machine Translation Adequacy,[0],[0]
"There is no such trend with bad quality scores, possibly because the workers are not putting in sufficient effort to produce accurate scores.
",3.3 Machine Translation Adequacy,[0],[0]
Next we assess the impact of the bias in the worst case situation.,3.3 Machine Translation Adequacy,[0],[0]
"We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 5 we can see that the sentences in the “low” partition and the “high” partition have a difference of 0.18, which is highly significant;3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation.",3.3 Machine Translation Adequacy,[0],[0]
"The bias remains even when we increase the number of workers and use the average score, as all workers scored the translations in the same order.",3.3 Machine Translation Adequacy,[0],[0]
"This shows that the mean is also affected by
3p < 0.001 using Welch’s two-sample t-test
sequence bias.",3.3 Machine Translation Adequacy,[0],[0]
"Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system’s computed score by ordering a HIT such that the system’s output is seen consistently immediately after a bad (or good) output.",3.3 Machine Translation Adequacy,[0],[0]
"We have shown significant sequence effects across several independent crowdsourced datasets: a negative autocorrelation in the RTE and TEMPORAL datasets, and a positive autocorrelation in the MTadeq dataset.",4 Discussion and Conclusions,[0],[0]
The negative autocorrelation can be attributed either to sequential contrast effects or the gambler’s fallacy.,4 Discussion and Conclusions,[0],[0]
"These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects.",4 Discussion and Conclusions,[0],[0]
"It is also possible that the dataset is too small to obtain statistical significance.
",4 Discussion and Conclusions,[0],[0]
"MT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale.",4 Discussion and Conclusions,[0],[0]
"We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect.",4 Discussion and Conclusions,[0],[0]
"Since the ordering of the systems is random, system scores obtained by averaging scores of all sentences translated by the system would be unbiased, assuming a sufficiently large sample of sentences.",4 Discussion and Conclusions,[0],[0]
Thus we do not expect sequential bias to have a marked effect on system rankings or other macro-level conclusions on the basis of this data.,4 Discussion and Conclusions,[0],[0]
"However, the scores of in-
dividual translations remain biased, which augurs poorly for the use of these annotations at the sentence level, such as when used in error analysis or for training automatic metrics.
",4 Discussion and Conclusions,[0],[0]
"Sequence problems can be easily addressed by adequate randomisation — providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data.",4 Discussion and Conclusions,[0],[0]
"In this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased.
",4 Discussion and Conclusions,[0],[0]
"This study has shown that sequence bias is real, and can distort evaluation and annotation exercises with crowd-workers.",4 Discussion and Conclusions,[0],[0]
"We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006).",4 Discussion and Conclusions,[0],[0]
"Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.",4 Discussion and Conclusions,[0],[0]
We thank the anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by the Australian Research Council.,Acknowledgments,[0],[0]
Manual data annotation is a vital component of NLP research.,abstractText,[0],[0]
"When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation.",abstractText,[0],[0]
"In this paper, we explore sequence effects where annotations of an item are affected by the preceding items.",abstractText,[0],[0]
"Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next.",abstractText,[0],[0]
"During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively.",abstractText,[0],[0]
We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets.,abstractText,[0],[0]
We then recommend a simple way to minimise sequence effects.,abstractText,[0],[0]
Sequence Effects in Crowdsourced Annotations,title,[0],[0]
"The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",Introduction,[0],[0]
"Comprised of elements s
t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",Introduction,[0],[0]
", sT q P X has length T which can vary between different instances.",Introduction,[0],[0]
"Sentences are a popular example of such data, where each s
j is a word from the language.",Introduction,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.
1MIT Computer Science & Artificial Intelligence Laboratory.",Introduction,[0],[0]
"Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.
",Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",Introduction,[0],[0]
"Copyright 2017 by the author(s).
",Introduction,[0],[0]
those which appear realistic),Introduction,[0],[0]
.,Introduction,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.
",Introduction,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",Introduction,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D
n",Introduction,[0],[0]
"“ tpx i , y i qun i“1 iid„ p XY
of sequence-outcome pairs.",Introduction,[0],[0]
"The marginal distribution p X
is assumed as a generative model of the natural sequences, and may be concentrated in a small subspace of X .",Introduction,[0],[0]
"Throughout this paper, p denotes both density and distribution functions depending on the referenced variable.
",Introduction,[0],[0]
"After fitting models to D n , we are presented a new sequence x0 P X (with unknown outcome), and our goal is to quickly identify a revised version that is expected to have superior outcome.",Introduction,[0],[0]
"Formally, we seek the revised sequence:
x˚ “ argmax xPC
x0
ErY | X “ xs (1)
Here, we want the set C x0 of feasible revisions to ensure that x˚ remains natural and is merely a minor revision of x0.",Introduction,[0],[0]
"Under a generative modeling perspective, these two goals are formalized as the following desiderata: p
X px˚q is not too small, and x˚ and x0 share similar underlying latent characteristics.",Introduction,[0],[0]
"When revising a sentence for example, it is imperative that the revision reads naturally (has reasonable likelihood under the distribution of realistic sentences) and retains the semantics of the original.
",Introduction,[0],[0]
This optimization is difficult because the constraint-set and objective may be highly complex and are both unknown (must be learned from data).,Introduction,[0],[0]
"For many types of sequence such as sentences, standard distance measures applied directly in the space of X or S (eg. Levenshtein distance or TF-IDF similarity) are inadequate to capture meaningful similarities, even though these can be faithfully reflected by a simple metric over an appropriately learned space of continuous latent factors (Mueller & Thyagarajan, 2016).",Introduction,[0],[0]
"In this work, we introduce a generative-modeling framework which transforms (1) into a simpler differentiable optimiza-
tion by leveraging continuous-valued latent representations learned using neural networks.",Introduction,[0],[0]
"After the generative model has been fit, our proposed procedure can efficiently revise any new sequence in a manner that satisfies the aforementioned desiderata (with high probability).",Introduction,[0],[0]
"Unlike imitation learning, our setting does not require availability of improved versions of a particular sequence.",Related Work,[0],[0]
"This prevents direct application of a sequence-to-sequence model (Sutskever et al., 2014).",Related Work,[0],[0]
"Similar to our approach, Gómez-Bombarelli et al. (2016) also utilize latent autoencoder representations in order to propose novel chemical structures via Bayesian optimization.",Related Work,[0],[0]
"However, unlike sequential bandit/reinforcement-learning settings, our learner sees no outcomes outside of the training data, neither for the new sequence it is asked to revise, nor for any of its proposed revisions of said sequence (Mueller et al., 2017).",Related Work,[0],[0]
"Our methods only require an easily-assembled dataset of sequence-outcome pairs and are thus widely applicable.
",Related Work,[0],[0]
"Combinatorial structures are often optimized via complex search heuristics such as genetic programming (Zaefferer et al., 2014).",Related Work,[0],[0]
"However, search relies on evaluating isolated changes in each iteration, whereas good revisions of a sequence are often made over a larger context (ie. altering a phrase in a sentence).",Related Work,[0],[0]
"From the vast number of possibilities, such revisions are unlikely to be found by search-procedures, and it is generally observed that such methods are outperformed by gradient-based optimization in high-dimensional continuous settings.",Related Work,[0],[0]
"Unlike combinatorial search, our framework leverages gradients in order to efficiently find good revisions at test time.",Related Work,[0],[0]
"Simonyan et al. (2014) and Nguyen et al. (2015) also proposed gradientbased optimization of inputs with respect to neural predictions, but work in this vein has been focused on conditional generation (rather than revision) and is primarily restricted to the continuous image domain (Nguyen et al., 2016).",Related Work,[0],[0]
"To identify good revisions, we first map our stochastic combinatorial optimization problem into a continuous space where the objective and constraints exhibit a simpler form.",Methods,[0],[0]
"We assume the data are generated by the probabilistic graphical model in Figure 1A. Here, latent factors Z P Rd specify a (continuous) configuration of the generative process for X,Y (both sequences and outcomes), and we adopt the prior p
Z “ Np0, Iq.",Methods,[0],[0]
"Relationships between these variables are summarized by the maps F,E,D which we parameterize using three neural networks F ,E ,D trained to enable efficient approximate inference under this model.
",Methods,[0],[0]
"The first step of our framework is to fit this model to D n
by learning the parameters of these inference networks: the encoder E , the decoder D , and the outcome-predictor F .",Methods,[0],[0]
"A good model that facilitates high-quality revision under our framework will possess the following properties: (1) Y can efficiently be inferred from Z and this relationship obeys a smooth functional form, (2) the map D produces a realistic sequence x given any z with reasonable prior probability, (3) the distribution of natural sequences is geometrically simple in the latent Z-space.",Methods,[0],[0]
"We explicitly encourage (1) by choosing F as a fairly simple feedforward network, (2) by defining D as the most-likely x given z, and (3) by endowing Z with our simple Np0, Iq prior.",Methods,[0],[0]
Another characteristic desired of our Z-representations is that they encode meaningful sequence-features such that two fundamentally similar sequences are likely to have been generated from neighboring z-values.,Methods,[0],[0]
"Applied to image data, VAE models similar to ours have been found to learn latent representations that disentangle salient characteristics such as scale, rotation, and other independent visual concepts (Higgins et al., 2016).",Methods,[0],[0]
"The latent representations of recurrent architectures trained on text (similar to the models used here) have also been shown to encode meaningful semantics, with a strong correlation between distances in the latent space and human-judged similarity between texts (Mueller & Thyagarajan, 2016).",Methods,[0],[0]
"By exploiting such simplified geometry, a basic shift in the latent vector space may be able to produce higher-quality revisions than attempts to directly manipulate the combinatorial space of sequence elements.
",Methods,[0],[0]
"After fitting a model with these desirable qualities, our strategy to revise a given sequence x0 P X is outlined in Figure 1B. First, we compute its latent representation z0 “ Epx0q using a trained encoding map.",Methods,[0],[0]
"As the latent representations z are continuous, we can employ efficient gradient-based optimization to find a nearby local optimum z˚ of F pzq (within a simple constraint-set around z0 defined later on).",Methods,[0],[0]
"To z˚, we subsequently apply a simple decoding map D (defined with respect to our learned model) in order to obtain our revised sequence x˚. Under our
assumed model, the optimization in latent representationspace attempts to identify a generative configuration which produces large values of Y (as inferred via F ).",Methods,[0],[0]
The subsequent decoding step seeks the most likely sequence produced by the optimized setting of the latent factors.,Methods,[0],[0]
"For approximate inference in the X,Z relationship, we leverage the variational autoencoder (VAE) model of Kingma & Welling (2014).",Variational Autoencoder,[0],[0]
"In our VAE, a generative model of sequences is specified by our prior over the latent values z combined with a likelihood function p
D px | zq which our decoder network D outputs in order to evaluate the likelihood of any sequence x given z P Rd.",Variational Autoencoder,[0],[0]
"Given any sequence x, our encoder network E outputs a variational approximation q
E pz | xq of the true posterior over the latent-values ppz | xq9 p
D px | zqp Z pzq.",Variational Autoencoder,[0],[0]
"As advocated by Kingma & Welling (2014) and Bowman et al. (2016), we employ the variational family q
E pz",Variational Autoencoder,[0],[0]
| xq,Variational Autoencoder,[0],[0]
"“ Npµ z|x,⌃z|x)",Variational Autoencoder,[0],[0]
"with diag-
onal covariance",Variational Autoencoder,[0],[0]
.,Variational Autoencoder,[0],[0]
"Our revision methodology employs the encoding procedure Epxq “ µ
z|x which maps a sequence to the maximum a posteriori (MAP) configuration of the latent values z (as estimated by the encoder network E ).",Variational Autoencoder,[0],[0]
"The parameters of E ,D are learned using stochastic variational inference to maximize a lower bound for the marginal likelihood of each observation in the training data:
log p X pxq • ´ “ Lrecpxq ` Lpripxq ‰ (2)",Variational Autoencoder,[0],[0]
"Lrecpxq “ ´E q
E pz|xq rlog",Variational Autoencoder,[0],[0]
"pDpx | zqs Lpripxq “ KLpqEpz | xq|| pZq
Defining z|x",Variational Autoencoder,[0],[0]
“,Variational Autoencoder,[0],[0]
"diagp⌃z|xq, the prior-enforcing KullbackLeibler divergence has a differentiable closed form expression when q
E , p Z are diagonal Gaussian distributions.",Variational Autoencoder,[0],[0]
"The reconstruction term Lrec (ie. negative log-likelihood under the decoder model) is efficiently approximated using just one Monte-Carlo sample z „ q
E pz | xq.",Variational Autoencoder,[0],[0]
"To optimize the variational lower bound over our data D
n with respect to the parameters of neural networks E ,D , we use stochastic gradients of (2) obtained via backpropagation and the reparameterization trick of Kingma & Welling (2014).
",Variational Autoencoder,[0],[0]
"Throughout, our encoder/decoder models E ,D are recurrent neural networks (RNN).",Variational Autoencoder,[0],[0]
"RNNs adapt standard feedforward neural networks for sequence data x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q, where at each time-step t P t1, . . .",Variational Autoencoder,[0],[0]
", T u, a fixed size hiddenstate vector h
t P Rd is updated based on the next element in the input sequence.",Variational Autoencoder,[0],[0]
"To produce the approximate posterior for a given x, our encoder network E appends the following additional layers to the final RNN hidden-state (parameterized by W
µ
,W ,W v , b µ , b , b v ):
µ z|x “ WµhT ` bµ P Rd
z|x",Variational Autoencoder,[0],[0]
"“ expp´|W v ` b |q, v “ ReLUpWvhT ` bvq (3)
",Variational Autoencoder,[0],[0]
The (squared) elements of z|x P Rd form the diagonal of our approximate-posterior covariance ⌃ z|x.,Variational Autoencoder,[0],[0]
"Since Lpri is minimized at z|x “ ~1 and Lrec is likely to worsen with additional variance in encodings (as our posterior approximation is unimodal), we simply do not consider
z|x values that exceed 1 in our variational family.",Variational Autoencoder,[0],[0]
This restriction results in more stable training and also encourages the encoder and decoder to co-evolve such that the true posterior is likely closer to unimodal with variance § 1.,Variational Autoencoder,[0],[0]
"To evaluate the likelihood of a sequence, RNN D computes not only its hidden state h
t
, but also the additional output:
⇡ t “ softmaxpW ⇡ h t ` b ⇡ q (4)
",Variational Autoencoder,[0],[0]
"At each position t, ⇡ t estimates pps t | s1, . . .",Variational Autoencoder,[0],[0]
", st´1q by relying on h
t to summarize the sequence history.",Variational Autoencoder,[0],[0]
"By the factorization pps1, . . .",Variational Autoencoder,[0],[0]
", sT q “ ± T
t“1 ppst | st´1, . . .",Variational Autoencoder,[0],[0]
", s1q, we have p
D px",Variational Autoencoder,[0],[0]
"| zq “ ±T t“1 ⇡trsts, which is calculated by
specifying an initial hidden-state h0 “ z and feeding x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q into D .",Variational Autoencoder,[0],[0]
"From a given latent configuration z, our revisions are produced by decoding a sequence via the most-likely observation, which we denote as the map:
Dpzq “ argmax",Variational Autoencoder,[0],[0]
xPX,Variational Autoencoder,[0],[0]
"p D px | zq (5)
",Variational Autoencoder,[0],[0]
"While the most-likely decoding in (5) is itself a combinatorial problem, beam search can exploit the sequentialfactorization of ppx | zq to efficiently find a good approximate solution (Wiseman & Rush, 2016; Sutskever et al., 2014).",Variational Autoencoder,[0],[0]
"For x˚ “ Dpzq P X , this decoding strategy seeks to ensure neither p
X
px˚q nor ppz | x˚q is too small.",Variational Autoencoder,[0],[0]
"In addition to the VAE component, we fit a compositional outcome-prediction model which uses a standard feed forward neural network F to implement the map F : Rd Ñ R. It is assumed that F pzq “ ErY | Z “ zs under our generative model.",Compositional Prediction of Outcomes,[0],[0]
"Rather than integrating over Z to compute ErY | X “ xs “ ≥ F pzqq
E pz | xqdz, we employ the first-order Taylor approximation F pEpxqq, where the approximation-error shrinks the more closely F resembles an affine transformation.",Compositional Prediction of Outcomes,[0],[0]
"To ensure this approximateinference step accurately estimates the conditional expectation, we jointly train E and F with the loss:
Lmsepx, yq “ ry ´ F pEpxqqs2 (6)
",Compositional Prediction of Outcomes,[0],[0]
"If the architecture of networks E ,F is specified with sufficient capacity to capture the underlying conditional relationship, then we should have F pEpxqq « ErY | X “ xs after properly learning the network parameters from a sufficiently large dataset (even F is a nonlinear map).
",Compositional Prediction of Outcomes,[0],[0]
"Enforcing Invariance
In theory, it is possible that some dimensions of z pertain solely to the outcome y and do not have any effect on the decoded sequence Dpzq.",Compositional Prediction of Outcomes,[0],[0]
"Happening to learn this sort of latent representation would be troubling, since subsequent optimization of the inferred y with respect to z might not actually lead to a superior revised sequence.",Compositional Prediction of Outcomes,[0],[0]
"To mitigate this issue, we carefully ensure the dimensionality d of our latent Z does not significantly exceed the bottleneck capacity needed to produce accurate outcome-predictions and VAE reconstructions (Gupta et al., 2016).",Compositional Prediction of Outcomes,[0],[0]
"We explicitly suppress this undesirable scenario by adding the following loss to guide training of our neural networks:
Linv “ Ez„p Z
“ F pzq ´ F pEpDpzqqq ‰2",Compositional Prediction of Outcomes,[0],[0]
"(7)
When optimizing neural network parameters with respect to this loss, we treat the parameters of D and the lefthand F pzq term as fixed, solely backpropagating Monte-Carlo estimated gradients into E ,F .",Compositional Prediction of Outcomes,[0],[0]
Driving Linv toward 0 ensures our outcome-predictions remain invariant to variation introduced by the encoding-decoding process (and this term also serves as a practical regularizer to enforce additional smoothness in our learned functions).,Compositional Prediction of Outcomes,[0],[0]
"The parameters of all components of this model (q E , p D , and F ) are learned jointly in an end-to-end fashion.",Joint Training,[0],[0]
"Training is done via stochastic gradient descent applied to minimize the following objective over the examples in D
n
:
Lpx, yq",Joint Training,[0],[0]
"“ Lrec ` priLpri ` mse 2 Y Lmse ` inv 2 Y Linv (8)
where 2 Y denotes the (empirical) variance of the outcomes, and the • 0 are constants chosen to balance the relative weight of each goal so that the overall framework produces maximally useful revisions.",Joint Training,[0],[0]
"By setting mse “ inv “ 0 at first, we can optionally leverage a separate large corpus of unlabeled examples to initially train only the VAE component of our architecture, as in the unsupervised pretraining strategy used successfully by Kiros et al. (2015); Erhan et al. (2010).
",Joint Training,[0],[0]
"In practice, we found the following training strategy to work well, in which numerous mini-batch stochastic gradient updates (typically 10-30 epochs) are applied within every one of these steps:
Step 1: Begin with inv “ pri “ 0, so Lrec and Lmse are the only training objectives.",Joint Training,[0],[0]
"We found that regardless of the precise value specified for mse, both Lrec and Lmse were often driven to their lowest possible values during this joint optimization (verified by training individually against each objective).
",Joint Training,[0],[0]
"Step 2: Grow pri from 0 to 1 following the sigmoid annealing schedule proposed by Bowman et al. (2016), which is needed to ensure the variational sequence to sequence model does not simply ignore the encodings z",Joint Training,[0],[0]
(note that the formal variational lower bound is attained at pri “ 1).,Joint Training,[0],[0]
"Step 3: Gradually increase inv linearly until Linv becomes small on average across our Monte-Carlo samples z „ p
Z .",Joint Training,[0],[0]
"Here, p
D is treated as constant with respect to Linv, and each mini-batch used in stochastic gradient descent is chosen to contain the same number of Monte-Carlo samples for estimating Linv as (sequence, outcome) pairs.",Joint Training,[0],[0]
"While the aforementioned training procedure is computationally intensive, once learned, our neural networks can be leveraged for efficient inference.",Proposing Revisions,[0],[0]
"Given user-specified constant ↵ ° 0 and a to-be-revised sequence x0, we propose the revision x˚ output by the following procedure.
",Proposing Revisions,[0],[0]
"REVISE Algorithm Input: sequence x0 P X , constant ↵ P p0, |2⇡⌃ z|x0 |´ 1 2 q Output: revised sequence x˚ P X 1)",Proposing Revisions,[0],[0]
"Use E to compute q
E pz | x0q 2)",Proposing Revisions,[0],[0]
"Define C
x0 “ z P Rd :",Proposing Revisions,[0],[0]
"q E pz | x0q • ↵ (
3) Find z˚ “ argmax zPC
x0
F pzq (gradient ascent)
4) Return x˚ “ Dpz˚q (beam search)
",Proposing Revisions,[0],[0]
"Intuitively, the level-set constraint C x0 Ñ Rd ensures that z˚, the latent configuration from which we decode x˚, is likely similar to the latent characteristics responsible for the generation of x0.",Proposing Revisions,[0],[0]
Assuming x0 and x˚ share similar latent factors implies these sequences are fundamentally similar according to the generative model.,Proposing Revisions,[0],[0]
"Note that z˚ “ Epx0q is always a feasible solution of the latent-factor optimization over z P C
x0 (for any allowed value of ↵).",Proposing Revisions,[0],[0]
"Furthermore, this constrained optimization is easy under our Gaussian approximate-posterior, since C
x0 forms a simple ellipsoid centered around Epx0q.",Proposing Revisions,[0],[0]
"To find z˚ in Step 3 of the REVISE procedure, we use gradient ascent initialized at z “ Epx0q, which can quickly reach a local maximum if F is parameterized by a simple feedforward network.",Proposing Revisions,[0],[0]
"Starting the search at Epx0q makes most sense for unimodal posterior approximations like our Gaussian q
E .",Proposing Revisions,[0],[0]
"To ensure all iterates remain in the feasible region C
x0 , we instead take gradient steps with respect to a penalized objective F pzq ` µ ¨ Jpzq where:
Jpzq “ log ” K ´ pz ´ Epx0qqT ⌃´1 z|x0pz ´ Epx0qq ı
K “ ´2 logrp2⇡qd{2|⌃ z|x|1{2↵s (9)
and 0 † µ !",Proposing Revisions,[0],[0]
"1 is gradually decreased toward 0 to en-
sure the optimization can approach the boundary of C x0 .",Proposing Revisions,[0],[0]
"In terms of resulting revision quality, we found this log barrier method outperformed other standard first-order techniques for constrained optimization such as the projected gradient and Franke-Wolfe algorithms.
",Proposing Revisions,[0],[0]
"In principle, our revision method can operate on the latent representations of a traditional deterministic autoencoder for sequences, such as the seq2seq models of Sutskever et al. (2014) and Cho et al. (2014).",Proposing Revisions,[0],[0]
"However, the VAE offers numerous practical advantages, some of which are highlighted by Bowman et al. (2016) in the context of generating more-coherent sentences.",Proposing Revisions,[0],[0]
The posterior uncertainty of the VAE encourages the network to smoothly spread the training examples across the support of the latent distribution.,Proposing Revisions,[0],[0]
"In contrast, central regions of the latent space under a traditional autoencoder can contain holes (to which no examples are mapped), and it is not straightforward to avoid these in our optimization of z˚.",Proposing Revisions,[0],[0]
"Furthermore, we introduce an adaptive variant of our decoder in §S1 which is designed to avoid poor revisions in cases where the initial sequence is already not reconstructed properly: DpEpx0qq ‰ x0.",Proposing Revisions,[0],[0]
"Here, we theoretically characterize properties of revisions obtained via our REVISE procedure (all proofs are relegated to §S3 in the Supplementary Material).",Theoretical Properties of Revision,[0],[0]
"Our results imply that in an ideal setting where our neural network inference approximations are exact, the revisions proposed by our method are guaranteed to satisfy our previously stated desiderata: x˚ is associated with an expected outcome-increase, x˚ appears natural (has nontrivial probability under p
X whenever x0 is a natural sequence), and x˚ is likely to share similar latent characteristics as x0 (since x˚ is the most likely observation generated from z˚ and q
E pz˚ | x0q • ↵ by design).",Theoretical Properties of Revision,[0],[0]
"Although exact approximations are unrealistic in practice, our theory precisely quantifies the expected degradation in the quality of proposed revisions that accompanies a decline in either the accuracy of our approximate inference techniques or the marginal likelihood of the original sequence to revise.
",Theoretical Properties of Revision,[0],[0]
"Theorems 1 and 2 below ensure that for an initial sequence x0 drawn from the natural distribution, the likelihood of the revised sequence x˚ output by our REVISE procedure under p
X has lower bound determined by the user-parameter ↵ and the probability of the original sequence p
X px0q.",Theoretical Properties of Revision,[0],[0]
"Thus, when revising a sequence x0 which looks natural (has substantial probability under p
X ), our procedure is highly likely to produce a revised sequence x˚ which also looks natural.",Theoretical Properties of Revision,[0],[0]
"The strength of this guarantee can be precisely controlled by choosing ↵ appropriately large in applications where this property is critical.
",Theoretical Properties of Revision,[0],[0]
"In each high probability statement, our bounds assume the
initial to-be-revised sequence x0 stems from the natural distribution p
X , and each result holds for any fixed constant ° 0.",Theoretical Properties of Revision,[0],[0]
"We first introduce the following assumptions: (A1) For ° 0,↵ ° 0, there exists 0 † § 1 such that:
i. With probability • 1 ´ {2 (over x „ p X ):
ppz | xq • ¨ q E pz | xq",Theoretical Properties of Revision,[0],[0]
"whenever q E pz | xq • ↵
ii.",Theoretical Properties of Revision,[0],[0]
"PrpZ R B R{2p0qq • ¨ Prp rZ R BR{2p0qq
where Z „ Np0, Iq, and rZ „ q Z
, the average encoding distribution defined by Hoffman & Johnson (2016) as:
q Z pzq",Theoretical Properties of Revision,[0],[0]
"“ E x„p
X
rq E pz | xqs (10) B
R p0q “ tz P Rd : ||z|| § Ru denotes the Euclidean ball centered around 0 with radius R defined here as: R “ maxtR1, R2u (11) with R1 “ a ´8 logr↵ ¨ p2⇡qd{2s
R2 “ maxt rR2, 2u, rR2 “ c 8 ´ 14d log ´ 8 ¯
(A2)",Theoretical Properties of Revision,[0],[0]
There exists ⌘ ° 0 (depends on ) such that with probability • 1 ´ {2 (over x0 „ pX ): ppz˚ | x˚q,Theoretical Properties of Revision,[0],[0]
§ ⌘,Theoretical Properties of Revision,[0],[0]
"This means the latent posterior is bounded at x˚, z˚ (as defined in REVISE), where both depend upon the initial tobe-revised sequence x0.",Theoretical Properties of Revision,[0],[0]
Theorem 1.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, (A1) and (A2) imply:
p X px˚q",Theoretical Properties of Revision,[0],[0]
"• ↵ ⌘ ¨ p X
px0q with probability • 1 ´ (over x0 „ pX ).
",Theoretical Properties of Revision,[0],[0]
"Condition (A1) forms a generalization of absolute continuity, and is required since little can be guaranteed about our inference procedures if the variational posterior is too inaccurate.",Theoretical Properties of Revision,[0],[0]
"Equality holds in (A1) with probability 1 if the variational distributions q
E exactly represent the true posterior ( Ñ 1 as the variational approximations become more accurate over the measure p
X ).",Theoretical Properties of Revision,[0],[0]
"In practice, minimization of the reverse KL divergence (Lpri) used in our VAE formulation ensures that q
E pz",Theoretical Properties of Revision,[0],[0]
"| xq is small wherever the true posterior ppz | xq takes small values (Blei et al., 2017).",Theoretical Properties of Revision,[0],[0]
"While the bound in Theorem 1 has particularly simple form, this result hinges on assumption (A2).",Theoretical Properties of Revision,[0],[0]
One can show for example that the inequality in (A2) is satisfied if the posteriors ppz,Theoretical Properties of Revision,[0],[0]
| x˚q are Lipschitz continuous functions of z at z˚ (sharing one Lipschitz constant over all possible x˚).,Theoretical Properties of Revision,[0],[0]
"In general however, (A2) heavily depends on both the data distribution p
X and decoder model p D .",Theoretical Properties of Revision,[0],[0]
"Therefore, we provide a similar lower bound guarantee on the likelihood of our revision x˚ under p
X , which instead only relies on weaker assumption (A3) below.
",Theoretical Properties of Revision,[0],[0]
"(A3) There exists L ° 0 such that for each x P X : p D px | zq is a L-Lipschitz function of z over B R`1p0q.
",Theoretical Properties of Revision,[0],[0]
"Here, L depends on (through R), and we assume L • 1 without loss of generality.",Theoretical Properties of Revision,[0],[0]
(A3) is guaranteed to hold in the setting where we only consider sequences of finite length § T .,Theoretical Properties of Revision,[0],[0]
"This is because the probability output by our decoder model, p
D px | zq, is differentiable with bounded gradients over all z P B
R p0q under any sequence-to-sequence RNN architecture which can be properly trained using gradient methods.",Theoretical Properties of Revision,[0],[0]
"Since B
R`1p0q Ä Rd is a closed interval, p D
px | zq must be Lipschitz continuous over this set, for a given value of x.",Theoretical Properties of Revision,[0],[0]
We can simply define L to be the largest Lipschitz constant over the |S|T possible choices of x P X (|S| “ size of the vocabulary).,Theoretical Properties of Revision,[0],[0]
"In the next theorem below, user-specified constant ↵ ° 0 is defined in REVISE, and L, , R all depend on .",Theoretical Properties of Revision,[0],[0]
Theorem 2.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if (A1) and (A3) hold, then with probability • 1 ´ (over x0 „ pX ):
p X
px˚q •",Theoretical Properties of Revision,[0],[0]
"Ce ´R
Ld ¨
“ ¨ ↵ ¨ p
X px0q ‰ d`1
where constant C “ ⇡ d{2 pd2 ` 1q ¨ pd ` 1q d pd ` 2qd`1
Our final result, Theorem 3, ensures that our optimization of z˚ with respect to F is tied to the expected outcomes at x˚ “ Dpz˚q, so that large improvements in the optimization objective: F pz˚q ´ F pEpx0qq imply that our revision procedure likely produces large expected improvements in the outcome: ErY | X “ x˚s ´ ErY | X “ x0s.",Theoretical Properties of Revision,[0],[0]
"For this result, we make the following assumptions:
(A4) For any ° 0, there exists  ° 0",Theoretical Properties of Revision,[0],[0]
"such that PrpX P Kq • 1 ´ {2, where we define:
K “ tx P X : x0 “ x ùñ pXpx˚q • u (12)
as the subset of sequences whose improved versions produced by our REVISE procedure remain natural with likelihood • .",Theoretical Properties of Revision,[0],[0]
"Note that either Theorem 1 or 2 (with the corresponding assumptions) ensures that one can suitably define  such that (A4) is satisfied (by considering a sufficiently large finite subset of X ).
",Theoretical Properties of Revision,[0],[0]
(A5),Theoretical Properties of Revision,[0],[0]
"For any  ° 0, there exists ✏mse ° 0 such that PrpX P Emseq",Theoretical Properties of Revision,[0],[0]
"° 1 ´ , where we define:
Emse“ tx P X : |F",Theoretical Properties of Revision,[0],[0]
"pEpxqq ´ ErY |X “ xs| § ✏mseu (13)
(A6) For any ° 0, there exists ✏inv ° 0 such that:
|F pzq ´ F pEpDpzqqq| § ✏inv for all z P BRp0q",Theoretical Properties of Revision,[0],[0]
"Ä Rd
where R is defined in (11) and depends on .
",Theoretical Properties of Revision,[0],[0]
"Here, ✏mse and ✏inv quantify the approximation error of our neural networks for predicting expected outcomes and ensuring encoding-decoding invariance with respect to F .
",Theoretical Properties of Revision,[0],[0]
"Standard learning theory implies both ✏mse, ✏inv will be driven toward 0 if we use neural networks with sufficient capacity to substantially reduce Lmse and Linv over a large training set.",Theoretical Properties of Revision,[0],[0]
Theorem 3.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if conditions (A1), (A4), (A5), and (A6) hold, then with probability • 1 ´ ´ :
z ˚ ´ ✏ § F pz˚q ´ F pEpx0qq § z ˚ ` ✏ (14)
where
z ˚ “ ErY | X “ x˚s ´ ErY | X “ x0s ✏ “ ✏
inv ` 2✏ mse
Here, , ✏inv are defined in terms of as specified in (A4), (A6), and ✏mse is defined in terms of  as specified in (A5).",Theoretical Properties of Revision,[0],[0]
"All of our RNNs employ the Gated Recurrent Unit (GRU) of Cho et al. (2014), which contains a simple gating mechanism to effectively learn long-range dependencies across a sequence.",Experiments,[0],[0]
"Throughout, F is a simple feedforward network with 1 hidden layer and tanh activations (note that the popular ReLU activation is inappropriate for F since it has zero gradient over half its domain).",Experiments,[0],[0]
"Decoding with respect to p
D is simply done entirely greedily (ie.",Experiments,[0],[0]
a beam-search of size 1) to demonstrate our approach is not reliant on search heuristics.,Experiments,[0],[0]
§S2 contains additional details for each analysis.,Experiments,[0],[0]
"To study our methods in a setting where all aspects of performance can be quantified, we construct a natural distribution p
X over sequences of lengths 10-20 whose elements stem from the vocabulary S “ tA,B, . . .",Simulation Study,[0],[0]
", I, Ju.",Simulation Study,[0],[0]
Each sequence is generated via the probabilistic grammar of Table S1.,Simulation Study,[0],[0]
"For each sequence, the associated outcome y is simply the number of times A appears in the sequence (a completely deterministic relationship).",Simulation Study,[0],[0]
"Since A often follows C and is almost always followed by B under p
X , a procedure to generate natural revisions cannot simply insert/substitute A symbols at random positions.
",Simulation Study,[0],[0]
Table 1 compares various methods for proposing revisions.,Simulation Study,[0],[0]
"Letting
Y denote the standard deviation of outcomes in D
n , we evaluate each proposed x˚ using a rescaled version of the actual underlying outcome-improvement:
Y px˚q “ ´1 Y pErY",Simulation Study,[0],[0]
| X “ x˚s ´ ErY | X “ x0sq.,Simulation Study,[0],[0]
"Except where sample size is explicitly listed, all models were trained using n “ 10, 000 (sequence, outcome) pairs sampled from the generative grammar.",Simulation Study,[0],[0]
"Wherever appropriate, the different methods all make use of the same neural network components with latent dimension d “ 128.",Simulation Study,[0],[0]
"Other than ↵, all hyperparameters of each revision method described below were chosen so that over 1000 revisions, the Levenshtein (edit) distance dpx˚, x0q « 3.3 on average.
",Simulation Study,[0],[0]
"All three results above the line in Table 1 are based on the full model described in our joint training procedure, with new sequences proposed via our REVISE algorithm (using the setting log↵ “ ´10000).",Simulation Study,[0],[0]
"In the latter two results, this model was only trained on a smaller subset of the data.",Simulation Study,[0],[0]
We also generated revisions via this same procedure with the more conservative choice log↵ “ ´1.,Simulation Study,[0],[0]
"ADAPTIVE denotes the same approach (with log↵ “ ´10000), this time using the adaptive decoding D
x0 introduced in §S1, which is intended to slightly bias revisions toward x0.",Simulation Study,[0],[0]
The model with inv “ pri “ 0 is a similar method using a deterministic sequence-to-sequence autoencoder rather than our probabilistic VAE formulation (no variational posterior approximation or invariance-enforcing) where the latent encodings are still jointly trained to predict outcomes via F .,Simulation Study,[0],[0]
"Under this model, a revision is proposed by starting at Epx0q in the latent space, taking 1000 (unconstrained) gradient steps with respect to F , and finally applying D to the resulting z.
The above methods form an ablation study of the various components in our framework.",Simulation Study,[0],[0]
"SEARCH is a different combinatorial approach where we randomly generate 100 revisions by performing 4 random edits in x0 (each individual edit is randomly selected as one of: substitution, insertion, deletion, or no change).",Simulation Study,[0],[0]
"In this approach, we separately learn a language-model RNN L on our training sequences (Mikolov et al., 2010).",Simulation Study,[0],[0]
"Sharing the same GRU architecture as our decoder model, L directly estimates the likelihood of any given sequence under p
X .",Simulation Study,[0],[0]
"Of the randomly generated revisions, we only retain those sequences x for which Lpxq • 1|S|Lpx0q (in this case, those which are not estimated to be † 10 times less likely than the original sequence x0 under pX ).",Simulation Study,[0],[0]
"Finally, we score each remaining candidate (including x0) using the outcome-prediction model F pEpxqq, and the best is chosen as x˚. Table 1 shows that our probabilistic VAE formulation outperforms the alternative approaches, both in terms of outcome-improvement achieved as well as ensuring revi-
sions follow p X .",Simulation Study,[0],[0]
"For comparison, ´ log p X px0q had an average value of 26.8 (over these 1000 starting sequences), and changing one randomly-selected symbol in each sequence to A results in an average negative log-probability of 32.8.",Simulation Study,[0],[0]
"Thus, all of our revision methods clearly account for p
X to some degree.",Simulation Study,[0],[0]
We find that all components used in our REVISION procedure are useful in achieving superior revisions.,Simulation Study,[0],[0]
"While individual standard deviations seem large, nearly all average differences in
Y or ´ log p X values produced by different methods are statistically significant considering they are over 1000 revisions.
",Simulation Study,[0],[0]
"From Supplementary Figure S1, it is clear that ↵ controls how conservative the changes proposed by our REVISE procedure tend to be, in terms of both ´ log p
X px˚q and the edit distance dpx0, x˚q.",Simulation Study,[0],[0]
"The red curve in Figure S1A suggests that our theoretical lower bounds for p
X px˚q are overly stringent in practice (although only the averagecase is depicted in the figure).",Simulation Study,[0],[0]
"The relationship between log p
X px0q and log pXpx˚q (see Figure S1B) is best-fit by a line of slope 1.2, indicating that the linear dependence on p
X px0q in the Theorem 1 bound for pXpx˚q is reasonably accurate.",Simulation Study,[0],[0]
Figure S1C shows that the magnitude of changes in the latent space (arising from z-optimization during our REVISE procedure) only exhibits a weak correlation with the edit distance between the resulting revision and the original sequence.,Simulation Study,[0],[0]
This implies that a fixed shift in different directions in the latent space can produce drastically different degrees of change in the sequence space.,Simulation Study,[0],[0]
"To ensure a high-quality revision, it is thus crucial to carefully treat the (variational) posterior landscape when performing manipulations of Z.",Simulation Study,[0],[0]
"Next, we apply our model to „1M reviews from BeerAdvocate (McAuley et al., 2012).",Improving Sentence Positivity,[0],[0]
"Each beer review is parsed into separate sentences, and each sentence is treated as an individual sequence of words.",Improving Sentence Positivity,[0],[0]
"In order to evaluate methods using an outcome that can be obtained for any proposed revision, we choose y P r0, 1s as the VADER sentiment compound score of a given sentence (Hutto & Gilbert,
2014).",Improving Sentence Positivity,[0],[0]
"VADER is a complex rule-based sentiment analysis tool which jointly estimates polarity and intensity of English text, and larger VADER scores correspond to text that humans find more positive with high fidelity.
",Improving Sentence Positivity,[0],[0]
We applied all aforementioned approaches to produce revisions for a held-out set of 1000 test sentences.,Improving Sentence Positivity,[0],[0]
"As p
X
underlying these sentences is unknown, we report estimates thereof obtained from a RNN language-model L learned on the sentences in D
n .",Improving Sentence Positivity,[0],[0]
Table 2 demonstrates that our VAE approach achieves the greatest outcome-improvement.,Improving Sentence Positivity,[0],[0]
"Moreover, Tables 3 and S2 show that our probabilisticallyconstrained VAE revision approach produces much more coherent sentences than the other strategies.",Improving Sentence Positivity,[0],[0]
"For our final application, we assemble a dataset of „100K short sentences which are either from Shakespeare or a more contemporary source (details in §S2.3).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"In this training data, each sentence is labeled with outcome y “ 0.9
if it was authored by Shakespeare and y “ 0.1 otherwise (these values are chosen to avoid the flat region of the sigmoid output layer used in network F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"When applied in this domain, our REVISE procedure thus attempts to alter a sentence so that the author is increasingly expected to be Shakespeare rather than a more contemporary source.
",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Tables 4 and S3 show revisions (of held-out sentences) proposed by our REVISE procedure with adaptive decoding (see §S1), together with sentences generated by applying the adaptive decoder at various points along an unconstrained gradient-ascent path in latent Z space (following gradients of F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Since the data lack similar versions of a sentence written in both contemporary and Shakespearean language, this revision task is an ambitious application of our ideas.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Without observing a continuous spectrum of outcomes or leveraging specially-designed style transfer features (Gatys et al., 2016), our REVISE procedure has to alter the underlying semantics in order to nontrivially increase the expected outcome of the revised sentence under F .",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Nevertheless, we find that many of the revised sentences look realistic and resemble text written by Shakespeare.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Furthermore, these examples demonstrate how the probabilistic constraint in our REVISE optimization prevents the revision-generating latent Z configurations from straying into regions where decodings begin to look very unnatural.",Revising Modern Text in the Language of Shakespeare,[0],[0]
This paper presents an efficient method for optimizing discrete sequences when both the objective and constraints are stochastically estimated.,Discussion,[0],[0]
"Leveraging a latent-variable generative model, our procedure does not require any examples of revisions in order to propose natural-looking sequences with improved outcomes.",Discussion,[0],[0]
These characteristics are proven to hold with high probability in a theoretical analysis of VAE behavior under our controlled latent-variable manipulations.,Discussion,[0],[0]
"However, ensuring semantic similarity in textrevisions remains difficult for this approach, and might be improved via superior VAE models or utilizing additional similarity labels to shape the latent geometry.",Discussion,[0],[0]
"D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Empirical Methods on Natural Language Processing, 2014.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Eck, D. and Schmidhuber, J. A first look at music composition using lstm recurrent neural networks.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"IDSIA Technical Report, 2002.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. Why does unsupervised pretraining help deep learning?","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Journal of Machine Learning Research, 11:625–660, 2010.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
transfer using convolutional neural networks.,"Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"Computer Vision and Pattern Recognition, 2016.","Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"J. M., Aguilera-Iparraguirre, J., , Hirzel, T., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1610.02415, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Graves, A. Generating sequences with recurrent neural networks.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1308.0850, 2013.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Gupta, P., Banchs, R. E., and Rosso, P. Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Neurocomputing, 175:1001– 1008, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. Early visual concept learning with unsupervised deep learning.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1606.05579, 2016.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
another way to carve up the variational evidence lower bound.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"NIPS Workshop on Advances in Approximate Bayesian Inference, 2016.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Hutto, C.J. and Gilbert, E. Vader: A parsimonious rulebased model for sentiment analysis of social media text.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Eighth International Conference on Weblogs and Social
Media, 2014.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Karpathy, A.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
The unreasonable effectiveness of recurrent neural networks.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Andrej Karpathy blog, 2015.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
URL karpathy.github.io.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
bayes.,"Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"International Conference on Learning Representations, 2014.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., and Fidler, S. Skip-thought vectors.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Advances in Neural Information Processing Systems, 2015.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"McAuley, J., Leskovec, J., and Jurafsky, D. Learning attitudes and attributes from multi-aspect reviews.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"IEEE International Conference on Data Mining, 2012.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Khudanpur, S. Recurrent neural network based language model.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Interspeech, 2010.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J. and Thyagarajan, A. Siamese recurrent architectures for learning sentence similarity.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
Proc.,"Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"AAAI Conference on Artificial Intelligence, 2016.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J., Reshef, D. N., Du, G., and Jaakkola, T. Learning optimal interventions.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Artificial Intelligence and Statistics, 2017.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Nguyen, A., Yosinski, J., and Clune, J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Computer Vision and Pattern Recognition, 2015.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Clune, J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
"Advances in Neural Information Processing Systems, 2016.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
convolutional networks: Visualising image classification models and saliency maps.,"Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"ICLR Workshop Proceedings, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Sutskever, I., Vinyals, O., and Le, Q.V. Sequence to sequence learning with neural networks.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Advances in Neural Information Processing Systems, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Wiseman, S. and Rush, A. M. Sequence-to-sequence learning as beam-search optimization.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Empirical Methods in Natural Language Processing, 2016.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"B., and Bartz-Beielstein, T. Efficient global optimization for combinatorial problems.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"Genetic and Evolutionary Computation Conference, 2014.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome.",abstractText,[0],[0]
"Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions.",abstractText,[0],[0]
"To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module.",abstractText,[0],[0]
"Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes.",abstractText,[0],[0]
"By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural.",abstractText,[0],[0]
"These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.",abstractText,[0],[0]
"Introduction The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",abstractText,[0],[0]
"Comprised of elements s t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",abstractText,[0],[0]
", sT q P X has length T which can vary between different instances.",abstractText,[0],[0]
"Sentences are a popular example of such data, where each s j is a word from the language.",abstractText,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.",abstractText,[0],[0]
MIT Computer Science & Artificial Intelligence Laboratory.,abstractText,[0],[0]
Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.,abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
those which appear realistic),abstractText,[0],[0]
.,abstractText,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.",abstractText,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",abstractText,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D",abstractText,[0],[0]
Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,title,[0],[0]
"The approach of training sequence generation models using likelihood maximization suffers from known failure modes, and it is notoriously difficult to ensure multi-step generated sequences have coherent global structure.",1. Introduction,[0],[0]
"For example, long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks trained to predict the next character in sequences of text may produce text that has correct
1Google Brain, Mountain View, USA 2Massachusetts Institute of Technology, Cambridge, USA 3University of Cambridge, Cambridge, UK 4Max Planck Institute for Intelligent Systems, Stuttgart, Germany 5Université de Montréal, Montréal, Canada.",1. Introduction,[0],[0]
"Correspondence to: Natasha Jaques <jaquesn@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"spelling, punctuation, and even a semblance of grammar, but the generated text shifts so rapidly from topic to topic, that it is almost completely nonsensical (see (Graves, 2013) for an example).",1. Introduction,[0],[0]
"Similar networks trained to predict the next note in a melody suffer from the same problem; the generated music has no consistent theme or structure, and appears wandering and random.",1. Introduction,[0],[0]
"In addition, these models are prone to excessively repeating the same output token, a problem that has also been noted in the context of recurrent dialog generation models (Li et al., 2016).
",1. Introduction,[0],[0]
"To ameliorate these problems we propose Sequence Tutor, a novel approach which uses RL to impose structure on a sequence generation RNN via task-specific rewards, while simultaneously ensuring that information learned from data is retained.",1. Introduction,[0],[0]
"This is accomplished by maintaining a fixed copy of a sequence generation RNN pre-trained on data, which is termed the Reward RNN.",1. Introduction,[0],[0]
"Rather than simply using the Reward RNN to supply part of the rewards to our model, we derive novel off-policy RL methods for sequence generation from KL-control that allow us to directly penalize Kullback Leibler (KL) divergence from the policy defined by the Reward RNN.",1. Introduction,[0],[0]
As a byproduct of minimizing KL our objective includes an entropy regularization term that encourages high entropy in the distribution of the RL model.,1. Introduction,[0],[0]
"This is ideal for sequence generation tasks such as text, music, or molecule generation, in which maintaining diversity in the samples generated by the model is critical.
",1. Introduction,[0],[0]
"Sequence Tutor effectively combines both data and taskrelated goals, without relying on either as a perfect metric of task success.",1. Introduction,[0],[0]
This is an important novel direction of research.,1. Introduction,[0],[0]
"Much previous work on combining RL and MLE has used MLE training simply as a way to bootstrap the training of an RL model (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016), since training with RL from scratch is difficult.",1. Introduction,[0],[0]
"However, this approach does not encourage diversity of the generated samples, and can be problematic when task-specific rewards are incomplete or imperfect.",1. Introduction,[0],[0]
"Designing an appropriate reward definition is highly non-trivial, and often the hand-crafted rewards cannot be fully trusted (Vedantam et al., 2015; Liu et al., 2016).",1. Introduction,[0],[0]
"And yet, relying on data alone can be insufficient when the data itself contains biases, as has been shown for text data
ar X
iv :1
61 1.
02 79
6v 9
[ cs
.L G
] 1
6 O
ct 2
01 7
(Caliskan-Islam et al., 2016), or when domain-specific constraints cannot be encoded directly into MLE training.",1. Introduction,[0],[0]
"By learning a policy that trades off staying close to the data distribution while improving performance on specific metrics, Sequence Tutor reduces both of these problems.
",1. Introduction,[0],[0]
"This paper contributes to the sequence training and RL literature by a) proposing a novel method for combining MLE and RL training; b) showing the connection between KL control and sequence generation; c) deriving the explicit relationships among a generalized variant of Ψ-learning (Rawlik et al., 2012), G-learning (Fox et al., 2015), and Q-learning with log prior augmentation, and being the first to empirically compare these methods and use them with deep neural networks.
",1. Introduction,[0],[0]
We explore the usefulness of our approach for two sequence generation applications.,1. Introduction,[0],[0]
"The first, music generation, is a difficult problem in which the aesthetic beauty of generated sequences cannot be fully captured in a known reward function, but in which models trained purely on data cannot produce well-structured sequences.",1. Introduction,[0],[0]
"Through an empirical study, we show that by imposing rules of music theory on a melody generation model, Sequence Tutor is able to produce melodies which are varied, yet more harmonious, interesting, and rated as significantly more subjectively pleasing than those of the MLE model.",1. Introduction,[0],[0]
"Further, Sequence Tutor is able to significantly reduce unwanted behaviors and failure modes of the original RNN.",1. Introduction,[0],[0]
"The effectiveness of Sequence Tutor is also demonstrated for computational molecular generation, a task in which the goal is to generate novel drug-like molecules with desirable properties by outputting a string representation of the molecule encoding.",1. Introduction,[0],[0]
"However, generating valid molecules can prove difficult, as it is hard for probabilistic models to learn all the constraints that define physically realizable molecules directly from data (Gómez-Bombarelli et al., 2016).",1. Introduction,[0],[0]
"We show that Sequence Tutor is able to yield a higher percentage of valid molecules than the baseline MLE RNN, and the generated molecules score higher on metrics of druglikeness and ease of synthesis.",1. Introduction,[0],[0]
Recent work has attempted to use both MLE and RL in the context of structured prediction.,2. Related Work,[0],[0]
"While the attempts were successful, the problems of maintaining information about the data distribution and diversity in the generated samples were not addressed.",2. Related Work,[0],[0]
"MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) uses BLEU score as a reward signal to gradually introduce a RL loss to a text translation model.",2. Related Work,[0],[0]
"Bahdanau et al. (2016) applies an actor-critic method and uses BLEU score directly to train a critic network to output the value of each word, where the actor is again initialized with the policy of an
RNN trained with next-step prediction.",2. Related Work,[0],[0]
Li et al. (2016) use RL to improve a pre-trained dialog model with heuristic rewards.,2. Related Work,[0],[0]
These approaches assume that the complete task reward specification is available.,2. Related Work,[0],[0]
"They pre-train a good policy with supervised learning so that RL can be used to learn the true task objective, since it can be difficult to reach convergence when training with pure RL.",2. Related Work,[0],[0]
"However, the original MLE policy of these models is overwritten by the RL training process.",2. Related Work,[0],[0]
"In contrast, Sequence Tutor uses rewards to correct certain properties of the generated data, while learning most information from data and maintaining this information; an important ability when the true reward function is not available or imperfect.
",2. Related Work,[0],[0]
"Reward augmented maximum likelihood (RAML) (Norouzi et al., 2016) is an approach designed to improve MLE training of a translation model by augmenting the ground truth targets with additional outputs that are within a small edit distance, and performing MLE training against those as well.",2. Related Work,[0],[0]
"The authors show that their approach is equivalent to minimizing KL-divergence between an RL exponentiated payoff distribution based on edit distance, and the MLE distribution.",2. Related Work,[0],[0]
"In contrast, our goal is generation rather than prediction, and we train an RL rather than MLE model.",2. Related Work,[0],[0]
"The RAML approach, while an important contribution, is only viable if it is possible to generate additional MLE training samples that are similar in terms of the reward function to the ground truth (i.e. samples within a small edit distance).",2. Related Work,[0],[0]
"However in some domains, including the two explored in this paper, generating similar samples with high reward is not only not possible, but in fact constitutes the entire problem under investigation.
",2. Related Work,[0],[0]
"Finally, our approach is related to KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012), a branch of stochastic optimal control (SOC) (Stengel, 1986).",2. Related Work,[0],[0]
"There is also a connection between this work and Maximum Entropy Inverse RL (Ziebart et al., 2008), which can be seen as KL control with a flat, improper prior.",2. Related Work,[0],[0]
"From KL control, we take inspiration from two off-policy, model-free methods, Ψ-learning (Rawlik et al., 2012) and G-learning (Fox et al., 2015).",2. Related Work,[0],[0]
"Both approaches are derived from a KLregularized RL objective, where an agent maximizes the reward while incurring additional penalty for divergence from some prior policy.",2. Related Work,[0],[0]
"While our methods rely on similar derivations presented in these papers, our methods have different motivations and forms from the original papers.",2. Related Work,[0],[0]
"The original Ψ-learning (Rawlik et al., 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KLregularized policy updates, similar to conservative policy gradient methods (?Peters et al., 2010; Schulman et al., 2015).",2. Related Work,[0],[0]
"The original G-learning (Fox et al., 2015) penalizes divergence from a simple uniform prior policy in order to cope with over-estimation of target Q values.",2. Related Work,[0],[0]
"These tech-
niques have not been applied to deep learning techniques or with RNNs, or as a way to improve a pre-trained MLE model.",2. Related Work,[0],[0]
"Our work is the first to explore these methods in such a context, and includes a Q-learning model with additional cross-entropy reward as a comparable alternative.",2. Related Work,[0],[0]
"To the best of our knowledge, our work is the first to provide comparisons among these three approaches.
",2. Related Work,[0],[0]
There has also been prior work in the domain of generative modeling of music.,2. Related Work,[0],[0]
"Using RNNs for this purpose has been explored in a variety of contexts, including generating Celtic folk music (Sturm et al., 2016), or improvising the blues (Eck & Schmidhuber, 2002).",2. Related Work,[0],[0]
"Often, this involves training the RNN to predict the next note in a monophonic melody; however, as mentioned above, the melodies generated by this model tend to wander and lack musical structure.",2. Related Work,[0],[0]
"Some authors have experimented with encoding musical structure into a hierarchical RNN with layers dedicated to generated the melody, drums, and chords (Chu et al., 2016).",2. Related Work,[0],[0]
"Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).",2. Related Work,[0],[0]
"Recently, Wavenet produced impressive performance in generating music from raw audio using convolutional neural networks with receptive fields at various time scales (van den Oord et al., 2016).",2. Related Work,[0],[0]
"However, the authors themselves note that “even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, and sound quality” (p. 8).
",2. Related Work,[0],[0]
"Finally, prior work has successfully performed computational molecular generation with deep neural networks.",2. Related Work,[0],[0]
Segler et al. (2017) demonstrated that an LSTM trained on sets of biologically active molecules can be used to generate novel molecules with similar properties.,2. Related Work,[0],[0]
GómezBombarelli,2. Related Work,[0],[0]
et al. (2016) trained a variational autoencoder to learn a compact embedding of molecules encoded using the SMILES notation.,2. Related Work,[0],[0]
"By interpolating in the embedding space and optimizing for desirable metrics of drug quality, the authors were able to decode molecules with high scores on these metrics.",2. Related Work,[0],[0]
"However, producing embeddings that led to valid molecules was difficult; in some cases, as little as 1% of generated sequences proved to be a valid molecule encoding.",2. Related Work,[0],[0]
"In RL, an agent interacts with an environment.",3. Background,[0],[0]
"Given the state of the environment at time t, st, the agent takes an action at according to its policy π(at|st), receives a reward r(st, at), and the environment transitions to state, st+1.The agent’s goal is to maximize reward over a sequence of actions, with a discount factor of γ applied to future rewards.",3. Background,[0],[0]
"The optimal deterministic policy π∗ is known to satisfy the
following Bellman optimality equation,
Q(st, at;π ∗)",3. Background,[0],[0]
"= r(st, at) (1)
+ γEp(st+1|st,at)[maxat+1 Q(st+1, at+1;π
∗)]
where Qπ(st, at) = Eπ[ ∑∞ t′=t",3. Background,[0],[0]
"γ
t′−tr(st′ , at′)] is the Q function of a policy π.",3. Background,[0],[0]
"In Deep Q-learning (Mnih et al., 2013), a neural network called the deep Q-network (DQN) is trained to approximate Q(s, a; θ), using the following objective,
L(θ) =",3. Background,[0],[0]
"Eβ [(r(s, a) + γmax a′ Q(s′, a′; θ−)−Q(s, a; θ))2] (2)
where β is the exploration policy, and θ− is the parameters of the target Q-network (Mnih et al., 2013) that is held fixed during the gradient computation.",3. Background,[0],[0]
"The target Q-network is updated more slowly than the Q-network; for example the moving average of θ can be used as θ−, as proposed by Lillicrap et al. (2015).",3. Background,[0],[0]
Exploration can be performed with either the -greedy method or Boltzmann sampling.,3. Background,[0],[0]
"Additional techniques such as a replay memory (Mnih et al., 2013) are used to stabilize and improve learning.",3. Background,[0],[0]
"Given a trained sequence generation RNN, we would like to impose domain-specific rewards based on the structure and quality of generated sequences, while still maintaining information about typical sequences learned from data.",4. Sequence Tutor,[0],[0]
"Therefore, we treat the trained model as a black-box prior policy, and focus on developing a method that can tune some properties of the model without interfering with the original probability distribution learned from data.",4. Sequence Tutor,[0],[0]
"The separation between the trained sequence model and the tuning method is important, as it prevents RL training from overwriting the original policy.",4. Sequence Tutor,[0],[0]
"To accomplish this task, we propose Sequence Tutor.",4. Sequence Tutor,[0],[0]
"An LSTM trained on data supplies the initial weights for three networks in the model: a recurrent Q-network and target Q-network, and a Reward RNN.",4. Sequence Tutor,[0],[0]
"The Reward RNN is held fixed during training, and treated as a prior policy which can supply the probability of a given token in a sequence as originally learned from data.
",4. Sequence Tutor,[0],[0]
"To apply RL to sequence generation, generating the next token in the sequence is treated as an action a.",4. Sequence Tutor,[0],[0]
"The state of the environment consists of all of the tokens generated so far, i.e. st = {a1, a2, ...at−1}.",4. Sequence Tutor,[0],[0]
"Given action at, we would like the reward rt to combine information about the prior policy p(at|st) as output by the Reward RNN, as well as some domain- or task-specific rewards rT .",4. Sequence Tutor,[0],[0]
Figure 1 illustrates these ideas.,4. Sequence Tutor,[0],[0]
The simplest and most naı̈ve way to incorporate information about the prior policy is to directly augment the taskspecific rewards with the output of the Reward RNN.,4.1. Q-learning with log prior augmentation,[0],[0]
"In this case, the total reward given at time t becomes:
r(s, a) = log p(a|s) + rT (a, s)/c (3)
where c is a constant controlling the emphasis placed on the task-specific rewards.",4.1. Q-learning with log prior augmentation,[0],[0]
"Given the DQN objective in Eq. 2 and modified reward function in Eq. 3, the objective and learned policy are:
L(θ) =",4.1. Q-learning with log prior augmentation,[0],[0]
Eβ,4.1. Q-learning with log prior augmentation,[0],[0]
"[(log p(a|s) + rMT (a, s)/c (4) +",4.1. Q-learning with log prior augmentation,[0],[0]
"γmax
a′ Q(s′, a′; θ−)−Q(s, a; θ))2]
πθ(a|s) = δ(a = arg max a Q(s, a; θ)).",4.1. Q-learning with log prior augmentation,[0],[0]
"(5)
",4.1. Q-learning with log prior augmentation,[0],[0]
"This modified objective forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have high probability in the original data.",4.1. Q-learning with log prior augmentation,[0],[0]
"However, the DQN learns a deterministic policy (as shown in Eq. 5), which is not ideal for sequence generation.",4.1. Q-learning with log prior augmentation,[0],[0]
"Therefore, after the model is trained, we generate sequences by sampling from the softmax function applied to the predicted Q-values.",4.1. Q-learning with log prior augmentation,[0],[0]
"If we cast sequence generation as a sequential decisionmaking problem and the desired sequence properties in terms of target rewards, the problem can be expressed as a KL control problem for a non-Markovian system.",4.2. KL Control for Sequence Generation,[0],[0]
"KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012) is a branch of stochastic optimal control (SOC) (Stengel, 1986), which studies an RL, or control, problem in which the agent tries maximizing its task reward while minimizing deviation from a prior policy.",4.2. KL Control for Sequence Generation,[0],[0]
"For our purposes, we treat a trained MLE sequence model as the prior policy, and thus the objective is to train a new policy, or sequence model, to maximize some rewards while keeping close to the original MLE model.",4.2. KL Control for Sequence Generation,[0],[0]
"We show that such KL control formulation allows us to derive additional
variants of Q-learning with minimal modifications, which give rise to different properties.",4.2. KL Control for Sequence Generation,[0],[0]
Let τ,4.2. KL Control for Sequence Generation,[0],[0]
=,4.2. KL Control for Sequence Generation,[0],[0]
{,4.2. KL Control for Sequence Generation,[0],[0]
"a1, a2, ..., at−1} represent the sequence, r(τ) the reward of the sequence, p(τ) be the prior distribution over τ given by the trained sequence model, and q(τ) be the policy of the Sequence Tutor model.",4.2. KL Control for Sequence Generation,[0],[0]
"The objective is then to maximize the following expression with respect to q(τ), where DKL represents the KL divergence of distributions:
L(q) = Eq(τ)[r(τ)]/c−DKL[q(τ)||p(τ)].",4.2. KL Control for Sequence Generation,[0],[0]
"(6)
We express q(τ) in terms of a parametrized recurrent policy πθ(at|st), i.e. q(τ) = ∏T t=1 πθ(at|st) where st = {a1, a2, ..., at−1}, indicates that the system is nonMarkovian.",4.2. KL Control for Sequence Generation,[0],[0]
The prior policy is expressed similarly p(τ) =∏T t=1 p(at|st).,4.2. KL Control for Sequence Generation,[0],[0]
"The reinforcement learning objective is the following, where Eπ[·] below indicates expectation with respect to sequences sampled from π, L(θ) =",4.2. KL Control for Sequence Generation,[0],[0]
"Eπ[ ∑
t
r(st, at)/c+ log p(at|st)− log πθ(at|st)]
",4.2. KL Control for Sequence Generation,[0],[0]
"The difference between this equation and Eq. 4 is that an entropy regularizer is now included, and thus the optimal policy is no longer deterministic.",4.2. KL Control for Sequence Generation,[0],[0]
"Below, we derive general temporal-difference based methods for the KL-control problem for sequence generation.",4.2. KL Control for Sequence Generation,[0],[0]
"Let V π(st) define the recurrent value function of the policy πθ, given by,
V π(st) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eπ[ ∞∑
t′=t
r(st′ , at′)/c+ log p(at′ |st′) (7)
− log π(at′ |st′)]
We define the generalized Ψ function, analogous toQ function for KL control, as below.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"We call this generalized Ψ function, as it was introduced in deriving Ψ-learning (Rawlik et al., 2012), and the following derivation is a generalization to the Ψ-learning algorithm.
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"r(st, at)/c+ log p(at|st) + V π(st+1) (8)
Note that the state st+1 is given deterministically by st = {a1, a2, ..., at−1} and at for sequence modeling, and thus the expressions do not contain the usual stochastic dynamics p(st+1|st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
The value function V π(st),4.3. Recurrent Generalized Ψ-learning,[0],[0]
"can be recursively expressed in terms of Ψπ ,
V π(st) = Eπ[Ψπ(st, at)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"+ H[π(.|st)] (9) = Eπ[Ψπ(st, at)− log π(at|st)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(10)
Fixing Ψ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) and constraining π to be a probability distribution, the optimal greedy policy update
π∗ can be derived, along with the corresponding optimal value function,
π∗(at|st) ∝ eΨ(st,at) (11) V ∗(st) = log ∑
at
eΨ(st,at) (12)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Given Eq. 8 and 12, the following Bellman optimality equation for generalized Ψ function is derived.
Ψ∗(st, at) = r(st, at)/c+ log p(at|st) + log ∑
at+1
exp(Ψ∗(st+1, at+1))",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(13)
The Ψ-learning loss directly follows:
LΨ(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eβ [(Ψθ(st, at)− yt)2] where (14) yt = log p(at|st) + r(st, at)/c+ γ log ∑
a′
eΨ −(st+1,a ′)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"β corresponds to sampling sequence trajectories from an arbitrary distribution; in practice, the experience replay (Mnih et al., 2013).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Ψ− indicates that it uses the target network.,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψθ, i.e. πθ, is parametrized with recurrent neural networks, and for discrete actions, πθ is effectively a softmax layer on top of Ψθ.
4.4.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Recurrent G-learning
We can derive another algorithm by parametrizing Ψθ indirectly by Ψθ(st, at) = log p(at|st) +",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Gθ(st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Substituting into above equations, we get a different temporaldifference method:
LG(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Eβ,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"[(Gθ(st, at)− yt)2] where (15) yt = r(st, at)/c+ γ log ∑
a′
p(a′|st+1)eG −(st+1,a ′) and
πθ(at|st) ∝",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"p(at|st) exp(Gθ(st, at))
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"This formulation corresponds to G-learning (Fox et al., 2015), which can thus be seen as a special case of generalized Ψ-learning.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Unlike Ψ learning, which directly builds knowledge about the prior policy into the Ψ function, theG-function does not give the policy directly but instead needs to be dynamically mixed with the prior policy probabilities.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"While this computation is straight-forward for discrete action domains as here, extensions to continuous action domains require additional considerations such as normalizability of Ψ-function parametrization (Gu et al., 2016).
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"The KL control-based derivation also has another benefit in that the stochastic policies can be directly used as an exploration strategy, instead of heuristics such as -greedy or additive noise (Mnih et al., 2013; Lillicrap et al., 2015).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Following from the above derivations, we compare three methods for implementing Sequence Tutor: Q-learning with log prior augmentation (based on Eq. 4), generalized Ψ-learning (based on Eq. 14), and G-learning (based on Eq. 15).",4.5. Sequence Tutor implementation,[0],[0]
"A pre-trained sequence generation LSTM is used as the Reward RNN, to supply the cross entropy reward in Q-learning and the prior policy in G- and generalized Ψlearning.",4.5. Sequence Tutor implementation,[0],[0]
"These approaches are compared to both the original performance of the MLE RNN, and a model trained using only RL and no prior policy.",4.5. Sequence Tutor implementation,[0],[0]
"Model evaluation is performed every 100,000 training epochs, by generating 100 sequences and assessing the average rT and log p(a|s).",4.5. Sequence Tutor implementation,[0],[0]
"The code for Sequence Tutor, including a checkpointed version of the trained melody RNN is available at https://github.com/tensorflow/magenta/ tree/master/magenta/models/rl_tuner.",4.5. Sequence Tutor implementation,[0],[0]
"Music compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge.",5. Experiment I: Melody Generation,[0],[0]
"For example, music theory tells that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases.",5. Experiment I: Melody Generation,[0],[0]
"Our research question is therefore whether such constraints can be learned by an RNN, while still allowing it to maintain note probabilities learned from data.
",5. Experiment I: Melody Generation,[0],[0]
"To test this hypothesis, we developed several rules that we believe describe pleasant-sounding melodies, taking inspiration from a text on melodic composition (Gauldin, 1995).",5. Experiment I: Melody Generation,[0],[0]
"We do not claim these characteristics are exhaustive or strictly necessary for good composition; rather, they are an incomplete measure of task success that can simply guide the model towards traditional composition structure.",5. Experiment I: Melody Generation,[0],[0]
It is therefore crucial that the Sequence Tutor approach allows the model to retain knowledge learned from real songs in the training data.,5. Experiment I: Melody Generation,[0],[0]
"The rules comprising the music-specific reward function rT (a, s) encourage melodies to: stay in key, start with the tonic note, resolve melodic leaps, have a unique maximum and minimum note, prefer harmonious intervals, play motifs and repeat them, have a low autocorrelation at a lag of 1, 2, and 3 beats, and avoid excessively repeating notes.",5. Experiment I: Melody Generation,[0],[0]
"Interestingly, while excessively repeating tokens is a common problem in RNN sequence generation models, avoiding this behavior is also Gauldin’s first rule of melodic composition (p. 42).
",5. Experiment I: Melody Generation,[0],[0]
"To train the model, we begin by extracting monophonic melodies from a corpus of 30,000 MIDI songs and encoding them as one-hot sequences of notes1.",5. Experiment I: Melody Generation,[0],[0]
"These melodies
1More information about both the note encoding and the reward metrics is available in the supplementary material.
",5. Experiment I: Melody Generation,[0],[0]
are then used to train an LSTM with one layer of 100 cells.,5. Experiment I: Melody Generation,[0],[0]
"Optimization was performed with Adam (Kingma & Ba, 2014), a batch size of 128, initial learning rate of .5, and a stepwise learning rate decay of 0.85 every 1000 steps.",5. Experiment I: Melody Generation,[0],[0]
"Gradients were clipped to ensure the L2 norm was less than 5, and weight regularization was applied with β = 2.5×10−5.",5. Experiment I: Melody Generation,[0],[0]
"Finally, the losses for the first 8 notes of each sequence were not used to train the model, since it cannot reasonably be expected to accurately predict them with no context.",5. Experiment I: Melody Generation,[0],[0]
The trained RNN eventually obtained a validation accuracy of 92% and a log perplexity score of .2536.,5. Experiment I: Melody Generation,[0],[0]
"This model was used as described above to initialize the three sub-networks in the Sequence Tutor model.
",5. Experiment I: Melody Generation,[0],[0]
"The Sequence Tutor model was trained using a similar configuration to the one above, except with a batch size of 32, and a reward discount factor of γ=.5.",5. Experiment I: Melody Generation,[0],[0]
"The TargetQ-network’s weights θ− were gradually updated towards those of the Q-network (θ) according to the formula (1 − η)θ− + ηθ, where η = .01 is the Target-Q-network update rate.",5. Experiment I: Melody Generation,[0],[0]
"A strength of our model is that the influence of data and task-specific rewards can be explicitly controlled by adjusting the temperature parameter c. We replicated our results for a number of settings for c; we present results for c=.5 below because we believe them to be most musically pleasing, however additional results are available at https://goo.gl/cTZy8r.",5. Experiment I: Melody Generation,[0],[0]
"Similarly, we replicated the results using both -greedy and Boltzmann exploration, and present the results using -greedy exploration below.",5. Experiment I: Melody Generation,[0],[0]
"Table 1 provides quantitative results in the form of performance on the music theory rules to which we trained the model to adhere; for example, we can assess the fraction of notes played by the model which belonged to the correct key, or the fraction of melodic leaps that were resolved.",5.1. Results,[0],[0]
"The statistics were computed by randomly generating 100,000 melodies from each model.
",5.1. Results,[0],[0]
"The results above demonstrate that the application of RL is able to correct almost all of the targeted “bad behaviors” of the MLE RNN, while improving performance on the desired metrics.",5.1. Results,[0],[0]
"For example, the original LSTM model was extremely prone to repeating the same note; after applying RL, we see that the number of notes belonging to some excessively repeated segment has dropped from 63% to nearly 0% in all of the Sequence Tutor models.",5.1. Results,[0],[0]
"While the metrics for the G model did not improve as consistently, the Q and Ψ models successfully learned to adhere to most of the imposed rules.",5.1. Results,[0],[0]
The degree of improvement on these metrics is related to the magnitude of the reward given for the behavior.,5.1. Results,[0],[0]
"For example, a strong penalty of -100 was applied each time a note was excessively repeated, while a reward of only 3 was applied at the end of a melody
for unique extrema notes (which most likely explains the lack of improvement on this metric).",5.1. Results,[0],[0]
"The reward values could be adjusted to improve the metrics further, however we found that these values produced pleasant melodies.
",5.1. Results,[0],[0]
"While the metrics indicate that the targeted behaviors of the RNN have improved, it is not clear whether the models have retained information about the training data.",5.1. Results,[0],[0]
"Figure 2a plots the average log p(a|s) as produced by the Reward RNN for melodies generated by the models every 100,000 training epochs; Figure 2b plots the average rT .",5.1. Results,[0],[0]
"Included in the plot is an RL only model trained using only the music theory rewards, with no information about log p(a|s).",5.1. Results,[0],[0]
"Since each model is initialized with the weights of the trained MLE RNN, we see that as the models quickly learn to adhere to the music theory constraints, log p(a|s) falls from its initial point.",5.1. Results,[0],[0]
"For the RL only model, log p(a|s) reaches an average of -3.65, which is equivalent to an average p(a|s) of approximately 0.026, or essentially a random policy over the 38 actions with respect to the distribution defined by the Reward RNN.",5.1. Results,[0],[0]
"Figure 2a shows that each of our models (Q, Ψ, and G) attain higher log p(a|s) values than this baseline, indicating they have maintained information about the data distribution, even over 3,000,000 training steps.",5.1. Results,[0],[0]
"TheG-learning implementation scores highest on this metric, at the cost of slightly lower average rT .",5.1. Results,[0],[0]
This compromise between data probability and adherence to music theory could explain the difference in the G model’s performance on the music theory metrics in Table 1.,5.1. Results,[0],[0]
"Finally, we have verified that by increasing the c parameter it is possible to train all the models to have even higher average log p(a|s), but found that c = 0.5 produced melodies that sounded better subjectively.
",5.1. Results,[0],[0]
The question remains whether the RL-tutored models actually produce more pleasing melodies.,5.1. Results,[0],[0]
"The sample melodies used for the study are available here: goo.gl/XIYt9m;
we encourage readers to judge their quality for themselves.",5.1. Results,[0],[0]
"To more formally answer this question, we conducted a user study via Amazon Mechanical Turk in which participants were asked to rate which of two randomly selected melodies they preferred on a Likert scale.",5.1. Results,[0],[0]
A total of 192 ratings were collected; each model was involved in 92 of these comparisons.,5.1. Results,[0],[0]
Figure 3 plots the number of comparisons in which a melody from each model was selected as the most musically pleasing.,5.1. Results,[0],[0]
"A Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models, χ2(3) = 109.480, p < 0.001.",5.1. Results,[0],[0]
"Mann-Whitney U post-hoc tests revealed that the melodies from all three Sequence Tuner models (Q, Ψ, and G) had significantly higher ratings than the melodies of the MLE RNN, p < .001.",5.1. Results,[0],[0]
"The Q and Ψ melodies were also rated as significantly more pleasing than those of the G model, but did not differ significantly from each other.",5.1. Results,[0],[0]
"Listening to the samples produced by the MLE RNN reveals that they are sometimes dischordant and usually dull; the model tends to place rests frequently, repeat the same
0 10 20 30 40 50 60 70 80 90
Number of times preferred
Ψ
Q
G
Note RNN
M o d e l
Figure 3: The number of times a melody from each model was selected as most musically pleasing.",5.2. Discussion,[0],[0]
Error bars reflect the std. dev.,5.2. Discussion,[0],[0]
"of a binomial distribution fit to the binary win/loss data from each model.
note, and produce melodies with little variation.",5.2. Discussion,[0],[0]
"In contrast, the melodies produced by the Sequence Tutor models are more varied and interesting.",5.2. Discussion,[0],[0]
"The G model tends to produce energetic and chaotic melodies, which include sequences of repeated notes.",5.2. Discussion,[0],[0]
"This repetition is likely because theG policy as defined in Eq. 15 directly mixes p(a|s) with the output of the G network, and the MLE RNN strongly favours repeating notes.",5.2. Discussion,[0],[0]
The most pleasant melodies are generated by the Q and Ψ models.,5.2. Discussion,[0],[0]
"These melodies stay firmly in key and frequently choose more harmonious interval steps, leading to melodic and pleasant melodies.",5.2. Discussion,[0],[0]
"However, it is clear they have retained information about the training data; for example, the sample q2.wav in the sample directory ends with a seemingly familiar riff.
",5.2. Discussion,[0],[0]
"While we acknowledge that the monophonic melodies generated by these models — which are based on highly simplistic rules of melodic composition — do not approach the level of artistic merit of human composers, we believe this study provides a proof-of-concept that encoding even incomplete and partially specified domain knowledge using our method can help the outputs of an LSTM adhere to a more consistent structure.",5.2. Discussion,[0],[0]
"The musical complexity of the songs is limited not just by the heuristic rules, but also by the simple monophonic encoding, which cannot represent the dynamics and expressivity of a musical performance.",5.2. Discussion,[0],[0]
"Although these melodies cannot surpass those of human musicians, attempting to train a model to generate aesthetically pleasing outputs in the absence of a better metric of human taste than log-likelihood is a problem of broader interest to the artificial intelligence community.",5.2. Discussion,[0],[0]
"As a follow-on experiment, we tested the effectiveness of Sequence Tutor for generating a higher yield of synthet-
ically accessible drug-like molecules.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Organic molecules can be encoded using the commonly used SMILES representation (Weininger, 1970).",6. Experiment II: Computational Molecular Generation,[0],[0]
"For example, amphetamine can be encoded as ‘CC(N)Cc1ccccc1’, while creatine is ‘CN(CC(=O)O)C(=N)N’.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Using this character encoding, it is straightforward to train an MLE RNN to generate sequences of SMILES characters; we trained such a model using the same settings as described above for the melody MLE RNN.",6. Experiment II: Computational Molecular Generation,[0],[0]
"However, only about a third of the molecules generated using this simple approach are actually valid SMILES encodings.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Further, this approach does not directly optimize for metrics of molecule or drug quality.",6. Experiment II: Computational Molecular Generation,[0],[0]
"These metrics include: a) the water-octanol partition coefficient (logP), which is important in assessing the druglikeness of a molecule; b) synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), a score from 1-10 that is lower if the molecule is easier to synthesize; and c) Quantitative Estimation of Drug-likeness (QED) (Bickerton et al., 2012), a more subjective measure of drug-likeness based on abstract ideas of medicinal aesthetics.
",6. Experiment II: Computational Molecular Generation,[0],[0]
"To optimize for these metrics, while simultaneously improving the percent yield of valid molecules from the RNN, we constructed a reward function that incentivizes validity, logP, SA, and QED using an open-source library called RDkit (http://www.rdkit.org/).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Included in the reward function was a penalty for molecules with unrealistically large carbon rings (size larger than 6), as per previous work (Gómez-Bombarelli et al., 2016).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Finally, after observing that the model could exploit the reward function by generating the simple molecule ‘N’ repeatedly, or ‘CCCCC...’ (which produces an unrealistically high logP value), we added penalties for sequences shorter than, or with more consecutive carbon atoms than, any sequence in the training data.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Sequence Tutor was then trained using these rewards, the pre-trained MLE RNN, and similar settings to the first experiment, except with -greedy exploration with = .01, a batch size of 512, and discount factor γ = .95.",6. Experiment II: Computational Molecular Generation,[0],[0]
"For this experiment, we also made use of prioritized experience replay (Schaul et al., 2015) to allow the model to more frequently learn from relatively rare valid samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"A value of c = 2.85 led to a higher yield of valid molecules with high metrics, but still encouraged the diversity of generated samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"As the Ψ algorithm produced the best results for the music generation task, we focused on using this technique for generating molecules.",6.1. Results and discussion,[0],[0]
"Table 2 shows the performance of this model against the original MLE model according to metrics of validity, drug-likeness, and synthetic accessibility.",6.1. Results and discussion,[0],[0]
"Once again, Sequence Tutor is able to significantly improve almost all of the targeted metrics.",6.1. Results and discussion,[0],[0]
"However, it should be noted that the Sequence Tutor model
tends to produce simplistic molecules involving more carbon atoms than the MLE baseline; e.g. Sequence Tutor may produce ‘SNCc1ccccc1’, while the MLE produces ‘C(=O)c1ccc(S(=O)(=O)N(C)C)c(Cl)c1’, which is the reason for the Sequence Tutor model’s lower QED scores.",6.1. Results and discussion,[0],[0]
"This effect is due to the fact that simple sequences are more likely to be valid, have high logP and SA scores, and carbon is highly likely under the distribution learned by the MLE model.",6.1. Results and discussion,[0],[0]
A higher reward for QED and further improvement of the task-specific rewards based on domain knowledge could help to alleviate these problems.,6.1. Results and discussion,[0],[0]
"Overall, the fact that Sequence Tutor can improve the percentage of valid molecules produced as well as the logP and synthetic accessibility scores serves as a proof-of-concept that Sequence Tutor may be valuable in a number of domains for imparting domain knowledge onto a sequence predictor.",6.1. Results and discussion,[0],[0]
"We have derived a novel sequence learning framework which uses RL to correct properties of sequences generated by an RNN, while maintaining information learned from MLE training on data, and ensuring the diversity of generated samples.",7. Conclusion and Future Work,[0],[0]
"By demonstrating a connection between our sequence generation approach and KL-control, we have derived three novel RL-based methods for optimizing sequence generation models.",7. Conclusion and Future Work,[0],[0]
"These methods were empirically compared in the context of a music generation task, and further demonstrated on a computational molecular generation task.",7. Conclusion and Future Work,[0],[0]
"Sequence Tutor showed promising results in terms of both adherence to task-specific rules, and subjective quality of the generated sequences.
",7. Conclusion and Future Work,[0],[0]
"We believe the Sequence Tutor approach of using RL to refine RNN models could be promising for a number of applications, including the reduction of bias in deep learning models.",7. Conclusion and Future Work,[0],[0]
"While manually writing a domain-specific reward function may seem unappealing, that approach is limited by the quality of the data that can be collected, and besides, even state-of-the-art sequence models often fail to learn all the aspects of high-level structure (van den Oord et al., 2016; Graves, 2013).",7. Conclusion and Future Work,[0],[0]
"Further, the data may contain hidden biases, as has been demonstrated for popular language models (Caliskan-Islam et al., 2016).",7. Conclusion and Future Work,[0],[0]
"In contrast to relying solely on possibly biased data, our approach allows
for encoding high-level domain knowledge into the RNN, providing a general, alternative tool for training sequence models.",7. Conclusion and Future Work,[0],[0]
"This work was supported by Google Brain, the MIT Media Lab Consortium, and Canada’s Natural Sciences and Engineering Research Council (NSERC).",ACKNOWLEDGMENTS,[0],[0]
"We thank Greg Wayne, Sergey Levine, and Timothy Lillicrap for helpful discussions on RL and stochastic optimal control and Kyle Kastner and Tim Cooijmans for valuable insight into training RNNs.",ACKNOWLEDGMENTS,[0],[0]
"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity.",abstractText,[0],[0]
"An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy.",abstractText,[0],[0]
Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN.,abstractText,[0],[0]
"To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control.",abstractText,[0],[0]
"The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation.",abstractText,[0],[0]
"For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",abstractText,[0],[0]
Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control,title,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) is a deep learningbased method for translation that has recently shown promising results as an alternative to statistical ap-
proaches.",1 Introduction,[0],[0]
"NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on the source sentence and previously generated target words.
",1 Introduction,[0],[0]
"While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and Zhou et al.",1 Introduction,[0.9675527830353055],"['While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and Zhou et al. (2016) obtained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer.']"
(2016) obtained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer.,1 Introduction,[0],[0]
"The sheer size of the models requires cutting-edge hardware for training and makes using the models on standard setups very challenging.
",1 Introduction,[0],[0]
"This issue of excessively large networks has been observed in several other domains, with much focus on fully-connected and convolutional networks for multi-class classification.",1 Introduction,[0],[0]
"Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process (Denil et al., 2013).",1 Introduction,[0],[0]
Therefore compressing deep models into smaller networks has been an active area of research.,1 Introduction,[0],[0]
"As deep learning systems obtain better results on NLP tasks, compression also becomes an important practical issue with applications such as running deep learning models for speech and translation locally on cell phones.
",1 Introduction,[0],[0]
Existing compression methods generally fall into two categories: (1) pruning and (2) knowledge distillation.,1 Introduction,[0],[0]
"Pruning methods (LeCun et al., 1990; He et al., 2014; Han et al., 2016), zero-out weights or entire neurons based on an importance criterion: LeCun et al. (1990) use (a diagonal approximation to)
",1 Introduction,[0],[0]
"ar X
iv :1
60 6.
07 94
7v 4
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
2 Se
p 20
the Hessian to identify weights whose removal minimally impacts the objective function, while Han et al. (2016) remove weights based on thresholding their absolute values.",1 Introduction,[0],[0]
"Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.
",1 Introduction,[0],[0]
"In this work, we investigate knowledge distillation in the context of neural machine translation.",1 Introduction,[1.0],"['In this work, we investigate knowledge distillation in the context of neural machine translation.']"
We note that NMT differs from previous work which has mainly explored non-recurrent models in the multiclass prediction setting.,1 Introduction,[0],[0]
"For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.",1 Introduction,[1.0],"['For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.']"
"With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.",1 Introduction,[0],[0]
"This sequence-level approximation leads to a simple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher network.
",1 Introduction,[0],[0]
"We run experiments to compress a large state-ofthe-art 4 × 1000 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a 2× 500 LSTM that roughly matches the performance of the full system.",1 Introduction,[0],[0]
We see similar results compressing a 2 × 500 model down to 2 × 100 on a smaller data set.,1 Introduction,[0],[0]
"Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time.",1 Introduction,[0],[0]
As a result we are able to perform greedy decoding on the 2 × 500 model 10 times faster than beam search on the 4 × 1000 model with comparable performance.,1 Introduction,[0],[0]
"Our student models can even be run efficiently on a standard smartphone.1 Finally, we apply weight pruning on top of the student network to obtain a model that has 13× fewer parameters than the original teacher model.",1 Introduction,[0],[0]
"We have released all the code for the models described in this paper.2
1https://github.com/harvardnlp/nmt-android 2https://github.com/harvardnlp/seq2seq-attn",1 Introduction,[0],[0]
"Let s = [s1, . . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
", sI ] and t =",2.1 Sequence-to-Sequence with Attention,[0],[0]
"[t1, . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
.,2.1 Sequence-to-Sequence with Attention,[0],[0]
", tJ ] be (random variable sequences representing)",2.1 Sequence-to-Sequence with Attention,[0],[0]
"the source/target sentence, with I and J respectively being the source/target lengths.",2.1 Sequence-to-Sequence with Attention,[0],[0]
"Machine translation involves finding the most probable target sentence given the source:
argmax t∈T
p(t | s)
where T is the set of all possible sequences.",2.1 Sequence-to-Sequence with Attention,[0],[0]
NMT models parameterize p(t | s) with an encoder neural network which reads the source sentence and a decoder neural network which produces a distribution over the target sentence (one word at a time) given the source.,2.1 Sequence-to-Sequence with Attention,[0],[0]
"We employ the attentional architecture from Luong et al. (2015), which achieved state-ofthe-art results on English→ German translation.3",2.1 Sequence-to-Sequence with Attention,[0],[0]
Knowledge distillation describes a class of methods for training a smaller student network to perform better by learning from a larger teacher network (in addition to learning from the training data set).,2.2 Knowledge Distillation,[0],[0]
"We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student.",2.2 Knowledge Distillation,[1.0],"['We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student.']"
Knowledge distillation suggests training by matching the student’s predictions to the teacher’s predictions.,2.2 Knowledge Distillation,[0],[0]
"For classification this usually means matching the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or by crossentropy (Li et al., 2014; Hinton et al., 2015).
",2.2 Knowledge Distillation,[0],[0]
"Concretely, assume we are learning a multi-class classifier over a data set of examples of the form (x, y) with possible classes V .",2.2 Knowledge Distillation,[0],[0]
"The usual training criteria is to minimize NLL for each example from the training data,
LNLL(θ) =",2.2 Knowledge Distillation,[0],[0]
"− |V|∑ k=1 1{y = k} log p(y = k |x; θ)
where 1{·} is the indicator function and p the distribution from our model (parameterized by θ).
",2.2 Knowledge Distillation,[0],[0]
"3Specifically, we use the global-general attention model with the input-feeding approach.",2.2 Knowledge Distillation,[0],[0]
"We refer the reader to the original paper for further details.
",2.2 Knowledge Distillation,[0],[0]
"This objective can be seen as minimizing the crossentropy between the degenerate data distribution (which has all of its probability mass on one class) and the model distribution p(y |x; θ).
",2.2 Knowledge Distillation,[0],[0]
"In knowledge distillation, we assume access to a learned teacher distribution q(y |x; θT ), possibly trained over the same data set.",2.2 Knowledge Distillation,[0],[0]
"Instead of minimizing cross-entropy with the observed data, we instead minimize the cross-entropy with the teacher’s probability distribution,
LKD(θ; θT )",2.2 Knowledge Distillation,[0],[0]
"=− |V|∑ k=1 q(y = k |x; θT )×
log p(y = k |x; θ)
where θT parameterizes the teacher distribution and remains fixed.",2.2 Knowledge Distillation,[0],[0]
"Note the cross-entropy setup is identical, but the target distribution is no longer a sparse distribution.4 Training on q(y |x; θT ) is attractive since it gives more information about other classes for a given data point (e.g. similarity between classes) and has less variance in gradients (Hinton et al., 2015).
",2.2 Knowledge Distillation,[0.9543209238420507],"['Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses, L(θ; θT ) = (1− α)LNLL(θ) + αLKD(θ; θT ) where α is mixture parameter combining the one-hot distribution and the teacher distribution.']"
4 In some cases the entropy of the teacher/student distribution is increased by annealing it with a temperature term τ,2.2 Knowledge Distillation,[0],[0]
"> 1
p̃(y |x) ∝",2.2 Knowledge Distillation,[0],[0]
"p(y |x) 1 τ
After testing τ ∈ {1, 1.5, 2} we found that τ = 1 worked best.
",2.2 Knowledge Distillation,[0],[0]
"Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses,
L(θ; θT )",2.2 Knowledge Distillation,[0],[0]
= (1− α)LNLL(θ),2.2 Knowledge Distillation,[0],[0]
"+ αLKD(θ; θT )
where α is mixture parameter combining the one-hot distribution and the teacher distribution.",2.2 Knowledge Distillation,[0],[0]
The large sizes of neural machine translation systems make them an ideal candidate for knowledge distillation approaches.,3 Knowledge Distillation for NMT,[0],[0]
In this section we explore three different ways this technique can be applied to NMT.,3 Knowledge Distillation for NMT,[0],[0]
"NMT systems are trained directly to minimize word NLL, LWORD-NLL, at each position.",3.1 Word-Level Knowledge Distillation,[0],[0]
"Therefore if we have a teacher model, standard knowledge distillation for multi-class cross-entropy can be applied.",3.1 Word-Level Knowledge Distillation,[0],[0]
"We define this distillation for a sentence as,
LWORD-KD = − J∑
j=1 |V|∑ k=1 q(tj = k | s, t<j)×
log p(tj = k",3.1 Word-Level Knowledge Distillation,[0],[0]
"| s, t<j)
where V is the target vocabulary set.",3.1 Word-Level Knowledge Distillation,[0],[0]
"The student can further be trained to optimize the mixture of
LWORD-KD and LWORD-NLL.",3.1 Word-Level Knowledge Distillation,[0],[0]
"In the context of NMT, we refer to this approach as word-level knowledge distillation and illustrate this in Figure 1 (left).",3.1 Word-Level Knowledge Distillation,[0],[0]
Word-level knowledge distillation allows transfer of these local word distributions.,3.2 Sequence-Level Knowledge Distillation,[1.0],['Word-level knowledge distillation allows transfer of these local word distributions.']
"Ideally however, we would like the student model to mimic the teacher’s actions at the sequence-level.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence distribution is particularly important for NMT, because wrong predictions can propagate forward at testtime.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"First, consider the sequence-level distribution specified by the model over all possible sequences t ∈ T ,
p(t | s) = J∏
j=1
p(tj | s, t<j)
for any length J .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences,
LSEQ-NLL = − ∑ t∈T 1{t = y} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0.9516611996918485],"['The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences, LSEQ-NLL = − ∑ t∈T 1{t = y} log p(t | s) = − J∑ j=1 |V|∑ k=1 1{yj = k} log p(tj = k | s, t<j) = LWORD-NLL where y = [y1, .']"
"− J∑
j=1 |V|∑ k=1 1{yj = k} log p(tj = k",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s, t<j)
= LWORD-NLL
where y =",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"[y1, . . .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
", yJ ] is the observed sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Of course, this just shows that from a negative log likelihood perspective, minimizing word-level NLL and sequence-level NLL are equivalent in this model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
But now consider the case of sequence-level knowledge distillation.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"As before, we can simply replace the distribution from the data with a probability distribution derived from our teacher model.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"However, instead of using a single word prediction, we use q(t | s) to represent the teacher’s sequence distribution over the sample space of all possible sequences,
LSEQ-KD = − ∑ t∈T q(t | s) log p(t | s)
Note that LSEQ-KD is inherently different from LWORD-KD, as the sum is over an exponential number of terms.",3.2 Sequence-Level Knowledge Distillation,[1.000000045317194],"['However, instead of using a single word prediction, we use q(t | s) to represent the teacher’s sequence distribution over the sample space of all possible sequences, LSEQ-KD = − ∑ t∈T q(t | s) log p(t | s) Note that LSEQ-KD is inherently different from LWORD-KD, as the sum is over an exponential number of terms.']"
"Despite its intractability, we posit
that this sequence-level objective is worthwhile.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
It gives the teacher the chance to assign probabilities to complete sequences and therefore transfer a broader range of knowledge.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"We thus consider an approximation of this objective.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Our simplest approximation is to replace the teacher distribution q with its mode,
q(t | s) ∼",3.2 Sequence-Level Knowledge Distillation,[0],[0]
1{t,3.2 Sequence-Level Knowledge Distillation,[0],[0]
=,3.2 Sequence-Level Knowledge Distillation,[0],[0]
argmax,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"t∈T q(t | s)}
Observing that finding the mode is itself intractable, we use beam search to find an approximation.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The loss is then
LSEQ-KD",3.2 Sequence-Level Knowledge Distillation,[0],[0]
≈,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"− ∑ t∈T 1{t = ŷ} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0],[0]
− log p(t = ŷ,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s)
where ŷ is now the output from running beam search with the teacher model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with a single sample.",3.2 Sequence-Level Knowledge Distillation,[1.0],"['Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with a single sample.']"
"However, previous results showing the effectiveness of beam search decoding for NMT lead us to belief that a large portion of q’s mass lies in a single output sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"In fact, in experiments we find that with beam of size 1, q(ŷ | s) (on average) accounts for 1.3% of the distribution for German→ English, and 2.3% for Thai→ English (Table 1: p(t = ŷ)).5
To summarize, sequence-level knowledge distillation suggests to: (1) train a teacher model, (2) run beam search over the training set with this model, (3) train the student network with cross-entropy on this new dataset.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Step (3) is identical to the word-level NLL process except now on the newly-generated data set.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This is shown in Figure 1 (center).
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
5Additionally there are simple ways to better approximate q(t | s).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"One way would be to consider a K-best list from beam search and renormalizing the probabilities,
q(t | s) ∼ q(t | s)∑ t∈TK q(t | s)
where TK is the K-best list from beam search.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This would increase the training set by a factor of K. A beam of size 5 captures 2.8% of the distribution for German → English, and 3.8% for Thai → English.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Another alternative is to use a Monte Carlo estimate and sample from the teacher model (since LSEQ-KD = Et∼q(t | s)[− log p(t | s) ]).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
However in practice we found the (approximate) mode to work well.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training data (LSEQ-NLL),
L = (1− α)LSEQ-NLL + αLSEQ-KD = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0.9807152764439432],"['Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training data (LSEQ-NLL), L = (1− α)LSEQ-NLL + αLSEQ-KD = −(1− α) log p(y | s)− α ∑ t∈T q(t | s) log p(t | s) where y is the gold target sequence.']"
s)−,3.3 Sequence-Level Interpolation,[0],[0]
α ∑,3.3 Sequence-Level Interpolation,[0],[0]
"t∈T q(t | s) log p(t | s)
where y is the gold target sequence.",3.3 Sequence-Level Interpolation,[0],[0]
"Since the second term is intractable, we could again apply the mode approximation from the previous section,
L = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0],[0]
"s)− α log p(ŷ | s)
and train on both observed (y) and teachergenerated (ŷ) data.",3.3 Sequence-Level Interpolation,[0],[0]
"However, this process is nonideal for two reasons: (1) unlike for standard knowledge distribution, it doubles the size of the training data, and (2) it requires training on both the teachergenerated sequence and the true sequence, conditioned on the same source input.",3.3 Sequence-Level Interpolation,[0],[0]
"The latter concern is particularly problematic since we observe that y and ŷ are often quite different.
",3.3 Sequence-Level Interpolation,[0],[0]
"As an alternative, we propose a single-sequence approximation that is more attractive in this setting.",3.3 Sequence-Level Interpolation,[0],[0]
"This approach is inspired by local updating (Liang et al., 2006), a method for discriminative training in statistical machine translation (although to our knowledge not for knowledge distillation).",3.3 Sequence-Level Interpolation,[0],[0]
"Local updating suggests selecting a training sequence which is close to y and has high probability under the teacher model,
ỹ = argmax t∈T
sim(t,y)q(t | s)
where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU (Papineni et al., 2002)).",3.3 Sequence-Level Interpolation,[1.0000000514251564],"['Local updating suggests selecting a training sequence which is close to y and has high probability under the teacher model, ỹ = argmax t∈T sim(t,y)q(t | s) where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU (Papineni et al., 2002)).']"
"Following local updating, we can approximate this sequence by running beam search and choosing
ỹ",3.3 Sequence-Level Interpolation,[0],[0]
≈ argmax t∈TK,3.3 Sequence-Level Interpolation,[0],[0]
"sim(t,y)
where TK is the K-best list from beam search.",3.3 Sequence-Level Interpolation,[0],[0]
"We take sim to be smoothed sentence-level BLEU (Chen and Cherry, 2014).
",3.3 Sequence-Level Interpolation,[0],[0]
"We justify training on ỹ from a knowledge distillation perspective with the following generative process: suppose that there is a true target sequence (which we do not observe) that is first generated from the underlying data distributionD. And further suppose that the target sequence that we observe (y) is a noisy version of the unobserved true sequence: i.e. (i) t ∼ D, (ii) y ∼ (t), where (t) is, for example, a noise function that independently replaces each element in t with a random element in V with some small probability.6 In such a case, ideally the student’s distribution should match the mixture distribution,
DSEQ-Inter ∼ (1− α)D + αq(t | s)
",3.3 Sequence-Level Interpolation,[0],[0]
"In this setting, due to the noise assumption,D now has significant probability mass around a neighborhood of y (not just at y), and therefore the argmax of the mixture distribution is likely something other than y (the observed sequence) or ŷ",3.3 Sequence-Level Interpolation,[0],[0]
(the output from beam search).,3.3 Sequence-Level Interpolation,[0],[0]
We can see that ỹ is a natural approximation to the argmax of this mixture distribution between D and q(t | s) for some α.,3.3 Sequence-Level Interpolation,[0],[0]
We illustrate this framework in Figure 1 (right) and visualize the distribution over a real example in Figure 2.,3.3 Sequence-Level Interpolation,[0],[0]
"To test out these approaches, we conduct two sets of NMT experiments: high resource (English → German) and low resource (Thai→ English).
",4 Experimental Setup,[0],[0]
The English-German data comes from WMT 2014.7 The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set.,4 Experimental Setup,[0],[0]
"We keep the top 50k most frequent words, and replace the rest with UNK.",4 Experimental Setup,[0],[0]
The teacher model is a 4 × 1000 LSTM (as in Luong et al. (2015)) and we train two student models: 2× 300 and 2× 500.,4 Experimental Setup,[0],[0]
"The Thai-English data comes from IWSLT 2015.8 There are 90k sentences in the
6While we employ a simple (unrealistic) noise function for illustrative purposes, the generative story is quite plausible if we consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement of words with synonyms, etc.",4 Experimental Setup,[0],[0]
"One could view translation having two sources of variance that should be modeled separately: variance due to the source sentence (t ∼ D), and variance due to the individual translator (y ∼ (t)).
",4 Experimental Setup,[0],[0]
"7http://statmt.org/wmt14 8https://sites.google.com/site/iwsltevaluation2015/mt-track
training set",4 Experimental Setup,[0],[0]
"and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k.",4 Experimental Setup,[0],[0]
"Size of the teacher model is 2×500 (which performed better than 4×1000, 2×750 models), and the student model is 2×100.",4 Experimental Setup,[0],[0]
"Other training details mirror Luong et al. (2015).
",4 Experimental Setup,[0],[0]
"We evaluate on tokenized BLEU with multi-bleu.perl, and experiment with the following variations:
Word-Level Knowledge Distillation (Word-KD) Student is trained on the original data and additionally trained to minimize the cross-entropy of the teacher distribution at the word-level.",4 Experimental Setup,[0],[0]
"We tested α ∈ {0.5, 0.9} and found α = 0.5 to work better.
",4 Experimental Setup,[0],[0]
"Sequence-Level Knowledge Distillation (Seq-KD) Student is trained on the teacher-generated data, which is the result of running beam search and taking the highest-scoring sequence with the teacher model.",4 Experimental Setup,[0],[0]
"We use beam size K = 5 (we did not see improvements with a larger beam).
",4 Experimental Setup,[0],[0]
Sequence-Level Interpolation (Seq-Inter) Student is trained on the sequence on the teacher’s beam that had the highest BLEU (beam size K = 35).,4 Experimental Setup,[1.0],['Sequence-Level Interpolation (Seq-Inter) Student is trained on the sequence on the teacher’s beam that had the highest BLEU (beam size K = 35).']
"We
adopt a fine-tuning approach where we begin training from a pretrained model (either on original data or Seq-KD data) and train with a smaller learning rate (0.1).",4 Experimental Setup,[0],[0]
"For English-German we generate SeqInter data on a smaller portion of the training set (∼ 50%) for efficiency.
",4 Experimental Setup,[0],[0]
The above methods are complementary and can be combined with each other.,4 Experimental Setup,[0],[0]
"For example, we can train on teacher-generated data but still include a word-level cross-entropy term between the teacher/student (Seq-KD + Word-KD in Table 1), or fine-tune towards Seq-Inter data starting from the baseline model trained on original data (Baseline + Seq-Inter in Table 1).9",4 Experimental Setup,[0.959260469521387],"['Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but finetuned towards Seq-Inter data (Baseline + Seq-Inter).']"
Results of our experiments are shown in Table 1.,5 Results and Discussion,[0],[0]
"We find that while word-level knowledge distillation (Word-KD) does improve upon the baseline, sequence-level knowledge distillation (SeqKD) does better on English → German and performs similarly on Thai → English.",5 Results and Discussion,[0],[0]
"Combining them (Seq-KD + Word-KD) results in further gains for the 2 × 300 and 2 × 100 models (although not for the 2 × 500 model), indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level.
",5 Results and Discussion,[0],[0]
"Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but finetuned towards Seq-Inter data (Baseline + Seq-Inter).",5 Results and Discussion,[0],[0]
"In fact, greedy decoding with this fine-tuned model has similar performance (19.6) as beam search with the original model (19.5), allowing for faster decoding even with an identically-sized model.
",5 Results and Discussion,[0],[0]
"We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher’s mode) instead of ‘wasting’ parameters on trying to model the entire
9For instance, ‘Seq-KD + Seq-Inter + Word-KD’ in Table 1 means that the model was trained on Seq-KD data and finetuned towards Seq-Inter data with the mixture cross-entropy loss at the word-level.
space of translations.",5 Results and Discussion,[0],[0]
Our results suggest that this is indeed the case: the probability mass that SeqKD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: p(t = ŷ)).,5 Results and Discussion,[0],[0]
"For example, on English → German the (approximate) argmax for the 2 × 500 Seq-KD model (on average) accounts for 16.9% of the total probability mass, while the corresponding number is 0.9% for the baseline.",5 Results and Discussion,[0],[0]
"This also explains the success of greedy decoding for Seq-KD models—since we are only modeling around the teacher’s mode, the student’s distribution is more peaked and therefore the argmax is much easier to find.",5 Results and Discussion,[0],[0]
"Seq-Inter offers a compromise between the two, with the greedily-decoded sequence accounting for 7.6% of the distribution.
",5 Results and Discussion,[0],[0]
"Finally, although past work has shown that models with lower perplexity generally tend to have
higher BLEU, our results indicate that this is not necessarily the case.",5 Results and Discussion,[0],[0]
"The perplexity of the baseline 2 × 500 English→ German model is 8.2 while the perplexity of the corresponding Seq-KD model is 22.7, despite the fact that Seq-KD model does significantly better for both greedy (+4.2 BLEU) and beam search (+1.4 BLEU) decoding.",5 Results and Discussion,[0],[0]
Run-time complexity for beam search grows linearly with beam size.,5.1 Decoding Speed,[0],[0]
"Therefore, the fact that sequencelevel knowledge distillation allows for greedy decoding is significant, with practical implications for running NMT systems across various devices.",5.1 Decoding Speed,[0],[0]
"To test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the average number of source words translated per second (Table 2).",5.1 Decoding Speed,[0],[0]
We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone.,5.1 Decoding Speed,[1.0],['We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone.']
"We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.",5.1 Decoding Speed,[1.0],"['We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.']"
"Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (Table 1: Params), due to the word embeddings which dominate most of the parameters.10 For example, on the
10Word embeddings scale linearly while RNN parameters scale quadratically with the dimension size.
2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters.",5.2 Weight Pruning,[0.9952349370495186],"['Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (Table 1: Params), due to the word embeddings which dominate most of the parameters.10 For example, on the 2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters.']"
"The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model.
",5.2 Weight Pruning,[1.0000001088891974],['The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model.']
We therefore focus next on reducing the memory footprint of the student models further through weight pruning.,5.2 Weight Pruning,[0],[0]
"Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance.",5.2 Weight Pruning,[1.0],"['Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance.']"
We take our best English→ German student model (2× 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values.,5.2 Weight Pruning,[1.0],['We take our best English→ German student model (2× 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values.']
We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1.,5.2 Weight Pruning,[1.0],['We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1.']
"As observed by See et al. (2016), retraining proved to be crucial.",5.2 Weight Pruning,[1.0],"['As observed by See et al. (2016), retraining proved to be crucial.']"
"The results are shown in Table 3.
",5.2 Weight Pruning,[0],[0]
Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal.11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU.,5.2 Weight Pruning,[0.9672832110640099],"['While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.']"
"While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is
11To our knowledge combining pruning and knowledge distillation has not been investigated before.
drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.",5.2 Weight Pruning,[0],[0]
"• For models trained with word-level knowledge
distillation, we also tried regressing the student network’s top-most hidden layer at each time step to the teacher network’s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.",5.3 Further Observations,[1.0000000131759987],"['• For models trained with word-level knowledge distillation, we also tried regressing the student network’s top-most hidden layer at each time step to the teacher network’s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.']"
"We found this to give comparable results to standard knowledge distillation and hence did not pursue this further.
",5.3 Further Observations,[0],[0]
"• There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables (Ling et al., 2015a; Kim et al., 2016; Ling et al., 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016).",5.3 Further Observations,[0],[0]
Combining such methods with knowledge distillation/pruning to further reduce the memory footprint of NMT systems remains an avenue for future work.,5.3 Further Observations,[0],[0]
Compressing deep learning models is an active area of current research.,6 Related Work,[0],[0]
Pruning methods involve pruning weights or entire neurons/nodes based on some criterion.,6 Related Work,[0],[0]
"LeCun et al. (1990) prune weights based on an approximation of the Hessian, while Han et al. (2016) show that a simple magnitude-based pruning works well.",6 Related Work,[0],[0]
Prior work on removing neurons/nodes include Srinivas and Babu (2015) and Mariet and Sra (2016).,6 Related Work,[0],[0]
"See et al. (2016) were the first to apply pruning to Neural Machine Translation, observing that that different parts of the architecture (input word embeddings, LSTM matrices, etc.) admit different levels of pruning.",6 Related Work,[0],[0]
"Knowledge distillation approaches train a smaller student model to mimic a larger teacher model, by minimizing the loss between the teacher/student predictions (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015).",6 Related Work,[0],[0]
"Romero et al. (2015) additionally regress on the intermediate hidden layers of the
student/teacher network as a pretraining step, while Mou et al. (2015) obtain smaller word embeddings from a teacher model via regression.",6 Related Work,[0],[0]
There has also been work on transferring knowledge across different network architectures: Chan et al. (2015b) show that a deep non-recurrent neural network can learn from an RNN; Geras et al. (2016) train a CNN to mimic an LSTM for speech recognition.,6 Related Work,[0],[0]
"Kuncoro et al. (2016) recently investigated knowledge distillation for structured prediction by having a single parser learn from an ensemble of parsers.
",6 Related Work,[0],[0]
"Other approaches for compression involve low rank factorizations of weight matrices (Denton et al., 2014; Jaderberg et al., 2014; Lu et al., 2016; Prabhavalkar et al., 2016), sparsity-inducing regularizers (Murray and Chiang, 2015), binarization of weights (Courbariaux et al., 2016; Lin et al., 2016), and weight sharing (Chen et al., 2015; Han et al., 2016).",6 Related Work,[0],[0]
"Finally, although we have motivated sequence-level knowledge distillation in the context of training a smaller model, there are other techniques that train on a mixture of the model’s predictions and the data, such as local updating (Liang et al., 2006), hope/fear training (Chiang, 2012), SEARN (Daumé III et al., 2009), DAgger (Ross et al., 2011), and minimum risk training (Och, 2003; Shen et al., 2016).",6 Related Work,[0],[0]
"In this work we have investigated existing knowledge distillation methods for NMT (which work at the word-level) and introduced two sequence-level variants of knowledge distillation, which provide improvements over standard word-level knowledge distillation.
",7 Conclusion,[0],[0]
"We have chosen to focus on translation as this domain has generally required the largest capacity deep learning models, but the sequence-to-sequence framework has been successfully applied to a wide range of tasks including parsing (Vinyals et al., 2015a), summarization (Rush et al., 2015), dialogue (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016), NER/POS-tagging (Gillick et al., 2016), image captioning (Vinyals et al., 2015b; Xu et al., 2015), video generation (Srivastava et al., 2015), and speech recognition (Chan et al., 2015a).",7 Conclusion,[0],[0]
We anticipate that methods described in this paper can be used to similarly train smaller models in other domains.,7 Conclusion,[0],[0]
Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches.,abstractText,[0],[0]
"However to reach competitive performance, NMT models need to be exceedingly large.",abstractText,[0],[0]
"In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT.",abstractText,[0],[0]
"We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model).",abstractText,[0],[0]
Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance.,abstractText,[0],[0]
It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.,abstractText,[0],[0]
"Applying weight pruning on top of knowledge distillation results in a student model that has 13× fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",abstractText,[0],[0]
Sequence-Level Knowledge Distillation,title,[0],[0]
