0	104	Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014).
1	38	Usually, due to the large number of parameters these neural models need a large-scale corpus.
2	41	It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge.
7	21	Multi-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performance relative to learning each task independently.
8	28	Inspired by the success of multi-task learning (Caruana, 1997), several neural network based models (Collobert and Weston, 2008; Liu et al., 2015b) are proposed for NLP tasks, which utilized multi-task learning to jointly learn several tasks with the aim of mutual benefit.
11	12	In this paper, we propose two deep architectures of sharing information among several tasks in multitask learning framework.
15	9	Different with NTM, we use a deep fusion strategy to integrate the information from the external memory into taskspecific LSTM, in which a fusion gate controls the 118 information flowing flexibly and enables the model to selectively utilize the shared information.
17	20	Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.
19	29	Two proposed models are complementary to prior multi-task neural networks.
21	36	• As a by-product, the fusion gate enables us to better understand how the external shared memory helps specific task.
23	59	Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.
25	44	Architecturally speaking, the memory state and output state are explicitly separated by activation gates (Wang and Cho, 2015).
38	20	Recently, there are some works to augment LSTM with an external memory, such as neural Turing machine (Graves et al., 2014) and memory network (Sukhbaatar et al., 2015), called memory enhanced LSTM (ME-LSTM).
41	24	To better control shared information and understand how it is utilized from external memory, we propose a deep fusion strategy for ME-LSTM.
42	23	As shown in Figure 1, ME-LSTM consists the original LSTM and an external memory which is maintained by reading and writing operations.
43	20	The LSTM not only interacts with the input and output information but accesses the external memory using selective read and write operations.
49	15	Reading The read operation is to read information rt ∈ RM from memory Mt−1.
50	17	rt = αtMt−1, (6) where rt denotes the reading vector and αt ∈ RK represents a distribution over the set of segments of memory Mt−1, which controls the amount of information to be read from and written to the memory.
54	13	Writing The memory can be written by two operations: erase and add.
58	26	To better control signals flowing from external memory, inspired by (Wang and Cho, 2015), we propose a deep fusion strategy to keep internal and external memories interacting closely without being conflated.
59	42	In detail, the state ht of LSTM at step t depends on both the read vector rt from external memory, and internal memory ct, which is computed by ht = ot tanh(ct + gt (Wfrt)), (11) where Wf is parameter matrix, and gt is a fusion gate to select information from external memory, which is computed by gt = σ(Wrrt + Wcct), (12) where Wr and Wc are parameter matrices.
66	29	This unsupervised pre-training is effective to improve the final performance, but it does not directly optimize the desired task.
67	89	Motivated by the success of multi-task learning (Caruana, 1997), we propose two deep architectures with shared external memory to leverage supervised data from many related tasks.
68	98	Deep neural model is well suited for multi-task learning since the features learned from a task may be useful for other tasks.
69	44	Figure 2 gives an illustration of our proposed architectures.
71	231	More formally, given an input text x, the task-specific output h(m)t of taskm at step t is defined as (h (m) t ,M (s) t , c (m) t ) = ME-LSTM(h (m) t−1, M (s) t−1, c (m) t−1,xt, θ (m) p , θ (s) q ), (14) where xt represents word embeddings of word xt; the superscript s represents the parameters are shared across different tasks; the superscript m represents that the parameters or variables are taskspecific for task m. Here all tasks share single global memory M(s), meaning that all tasks can read information from it and have the duty to write their shared or taskspecific information into the memory.
72	37	M (s) t = fwrite(M (s) t−1, α (s) t ,h (m) t ) (15) After calculating the task-specific representation of text h(m)T for task m, we can predict the probability distribution over classes.
75	65	More generally, for task m, we assign each taskspecific LSTM with a local memory M(m), followed by a global memory M(s), which is shared across different tasks.
78	9	ŷ(m) = softmax(W(m)h(m) + b(m)), (20) where ŷ(m) is prediction probabilities for task m. Given M related tasks, our global cost function is the linear combination of cost function for all tasks.
79	15	φ = M∑ m=1 λmL(ŷ (m), y(m)) (21) where λm is the weights for each task m respectively.
