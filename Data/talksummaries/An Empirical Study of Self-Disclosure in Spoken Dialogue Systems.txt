1	14	The contributions to a conversation can be categorized as those which serve propositional functions by adding new information to the dialog, those which serve interactional functions by driving the interaction and those which serve interpersonal functions, by building up the relationship between the involved parties.
2	17	When fulfilling interpersonal functions, people either consciously or sub-consciously employ social conversational strategies in order to connect and build relationships with each other (Laurenceau et al., 1998; Won-Doornink, 1985).
6	17	We consider the definition of self-disclosure within the theoretical framework of social penetration theory, where it is defined as the voluntary sharing of opinions, thoughts, beliefs, experiences, preferences, values and personal history (Altman and Taylor, 1973).
8	82	Several studies have shown that selfdisclosure reciprocity characterizes initial social interactions between people (Ehrlich and Graeven, 1971; Sprecher and Hendrick, 2004) and further, that disclosure promotes disclosure (Dindia et al., 2002).
10	25	A subtle but crucial aspect is that humans are aware that machines do not have feelings or experiences of their own, so any attempt at self-disclosure on the part of the machine is inherently disingenuous.
11	17	However, Nass et al. (1994) suggests that humans tend to view computers as social actors, and interact with them in much the same way they do with humans.
22	15	Reciprocity is the phenomenon by which selfdisclosure by one participant in a dyadic social interaction results in self-disclosure from the other participant in response.
24	48	While the exact cause of this phenomena is not known, it has been suggested that selfdisclosure can be viewed as a social exchange, where the party receiving self-disclosure feels obligated to self-disclose in return (Archer, 1979), or as a social conversational norm (Derlega et al., 1993), or from the point of view of social trustattraction (Vittengl and Holt, 2000) where people self-disclose to people who disclose to them, as they consider self-disclosure to be a sign of trust and liking.
35	18	The data for this study was collected by having users from the real-world interact with our open-domain dialog agent.
36	62	The dialog agent was hosted on Amazon Alexa devices as part of the AlexaPrize competition (Ram et al., 2018) and was one of sixteen socialbots that could be invoked by any user within the United States through the command ‘Let’s chat!’.
40	38	The users were also free to end the interaction at any time, and thus had no motivation for continuing the conversation besides their own entertainment.
41	16	To control the direction of the conversation and bot utterance, we utilize a finite state transducer-based dialog system that chats with the user about movies and TV shows, as well as plays games and supports open-domain conversation (Prabhumoye et al., 2017).
44	47	In this way, the socialbot leads the user through the following topics, conditioned on user interest as shown in Figure 3: Greeting : In this phase, our dialog agent greets the user and asks them about their day.
46	29	TV Shows: The next phase involves chit chat about popular TV shows.
53	14	The dialog system response is determined by a retrieval model.
56	28	319 users rated the socialbot (Group 1) and 1507 users interacted with our system in total (Group 2).
62	34	Out of the 5216 human dialog utterances, 13.8% featured some form of self-disclosure.
64	27	To estimate this, we randomly sample 100 utterances from the dataset and annotate these utterances for whether they contained an ASR mistake, and if the sentence meaning was still apparent either from context or from the utterance itself.
92	15	We observe that considering the word in the context of the machine utterance is most helpful in identifying self-disclosure, indicating possibly that it helps us capture the notion of selfdisclosure being a voluntary phenomena whereby the user reveals information about himself or herself, by separating instances of direct answers to questions from turns where users disclose more than what is asked.
95	75	However many of these utterances were in fact direct responses to questions, or questions to the bot itself prefaced with a personal pronoun, and thus not really instances of self-disclosure.
100	95	H: Really wanna grab a smoke).
101	32	In the future, real world knowledge and a larger amount of training data might help mitigate some of these error classes.
102	54	We analyze common markers of reciprocity (Jourard and Jaffe, 1970; Harper and Harper, 2006), such as the usage of personal pronouns, word overlap with the previous sentence (normalized by length of previous utterance) and average user utterance length between two groups of usersones who were shown a bot that self-disclosed ini- tially and a bot which only self-disclosed later (Table 2.).
103	62	Within the data which consists of only rated conversations, we observe how many turns where the machine self-disclosed were also met with human self-disclosure (“Rated” in Table.
104	38	We then tag all user utterances 7 with our SVM classifier as either being instances of self-disclosure or not being instances of self disclosure (“All” in Table.
105	74	We find that 10.6% of all user utterances contain self disclosure, and 21.6% of machine utterances that contained self-disclosure were followed by a human utterance that contained self-disclosure, compared to the 7.4% of cases where a user selfdisclosed without the machine self-disclosing (p < 0.05).
122	23	We find that on average, users who do not choose to self-disclose initially are significantly less likely to self-disclose (p<0.05) even later on in the conversation, only revealing information in 9% of their turns as compared to the 24.6% of turns of other users.
129	28	We utilize the user ratings based on the question ‘Would you talk to this socialbot again’ as a proxy for likability of the dialog agent, and examine whether conversations where the user self-disclosed often were given higher ratings than ones where they didn’t.
133	18	Lastly, we analyze the effect of reciprocity and self-disclosure, by analyzing the ratings of users who self-disclosed in response to bot disclosure but find no significant difference in the ratings of such users (3.34 to 3.27).
157	16	We find that indicators of reciprocity occur even in conversations with dialog systems, and that user behavior can be characterized by self-disclosure patterns in the initial stages of the conversation.
