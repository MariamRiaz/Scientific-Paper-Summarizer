0	12	Recently, deep learning has emerged as a powerful and popular class of machine learning algorithms.
1	13	Well-known examples include the convolutional neural network (LeCun et al., 1998), long short term memory (Hochreiter & Schmidhuber, 1997), memory network (Weston et al., 2014), and deep Q-network (Mnih et al., 2015).
2	8	These models have achieved remarkable performance on various difficult tasks such as image classification (He et al., 2016), speech recognition (Graves et al., 2013), natural language understanding (Bahdanau et al., 2015; Sukhbaatar et al., 2015), and game playing (Silver et al., 2016).
13	6	However, SGD requires careful stepsize tuning, which is difficult as different weights have vastly different gradients (in terms of both magnitude and direction).
18	6	When used on deep networks, Adagrad also demonstrates significantly better performance than SGD (Dean et al., 2012).
30	6	Section 2 first gives a brief review on FTRL and other solvers for deep learning.
35	100	For a matrix X , X2 = XX , and diag(X) is a vector with the diagonal of X as its elements.
72	11	However, for highly nonconvex models such as the deep network, the parameter iterate may move from one local basin to another.
73	21	Pi’s that are due to samples in the distant past are less informative than those from the recent ones.
74	40	To alleviate this problem, one may consider only Pi’s in a recent window.
76	76	A simpler alternative is by using an exponential moving average of the Pi’s: Si = β1Si−1 + (1 − β1)Pi, where β1 ∈ [0, 1) and S0 = 0.
79	11	The denominator 1− βt1 plays a similar role as bias correction in Adam.
94	12	Instead, the negative entries of σi encourage the corresponding entries of θ to move away from those of θi−1.
96	16	Hence, the objective in (13) is still strongly convex.
97	6	Moreover, the following Proposition shows that θt in (13) has a simple closed-form solution.
100	21	, Pt} are considered in each round, the update depends only the current gradient gt and parameter θt−1.
101	6	It can be easily seen that FTML is easy to implement, memory-efficient and has low per-iteration complexity.
106	6	The following Theorem shows that FTML also has a similar gradient descent update.
108	15	(14) When β1 = 0 and bias correction for the variance is not used, (14) reduces to RMSprop in (7).
111	15	Moreover, as demonstrated in Adam, bias correction of the variance can be very important.
118	10	(15) Compared with (13), the regularization in (15) is more aggressive as it encourages θt to be close only to the last iterate θt−1.
129	13	(Summary) RMSprop and Adam are improvements over Adagrad in training deep networks.
132	9	The proposed FTML combines the nice properties of the two.
133	23	In this section, experiments are performed on a number of deep learning models, including convolutional neural networks (Section 4.1), deep residual networks (Section 4.2), memory networks (Section 4.3), neural conversational model (Section 4.4), deep Q-network (Section 4.5), and long short-term memory (LSTM) (Section 4.6).
134	24	A summary of the empirical performance of the various deep learning optimizers is presented in Section 4.7.
135	39	The following state-of-the-art optimizers for deep learning models will be compared: (i) Adam (Kingma & Ba, 2015); (ii) RMSprop (Tieleman & Hinton, 2012); (iii) Adadelta (Zeiler, 2012); and (iv) Nesterov accelerated gradient (NAG) (Sutskever et al., 2013).
136	33	For FTML, we set β1 = 0.6, β2 = 0.999, and a constant t = = 10−8 for all t. For FTML, Adam, RMSprop, and NAG, η is selected by monitoring performance on the training set (note that Adadelta does not need to set η).
137	29	The learning rate is chosen from {0.5, 0.25, 0.1, .
139	15	We then pick the rate that leads to the smallest final training loss.
143	10	Finally, there is a fully-connected layer with ReLU activation and a dropout rate of 0.5.
