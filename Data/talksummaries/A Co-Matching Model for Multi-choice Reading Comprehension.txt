0	33	Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016).
1	74	Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China.
2	7	Figure 1 shows an example passage and two related questions from RACE.
3	60	The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figure 1.
9	10	As illustrated by Q2 in Figure 1, the model may need to recognize what “he” and “it” in candidate answer (c) refer to in the question, in order to select (c) as the correct answer.
11	17	In this paper, we propose a new model to match a question-answer pair to a given passage.
12	53	Our comatching approach explicitly treats the question and the candidate answer as two sequences and jointly matches them to the given passage.
13	31	Specifically, for each position in the passage, we compute two attention-weighted vectors, where one is from the question and the other from the candidate answer.
14	142	Then, two matching representations are constructed: the first one matches the passage with the question while the second one matches the passage with the candidate answer.
24	50	Let us use P ∈ Rd×P , Q ∈ Rd×Q and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector.
27	15	For each candidate answer, our model constructs a vector that represents the matching of P with both Q and A.
46	5	We refer to c ∈ R2l, which is a single column of C, as a co-matching state that concurrently matches a passage state with both the question and the candidate answer.
47	12	In order to capture the sentence structure of the passage, we further modify the model presented earlier and build a hierarchical LSTM (Tang et al., 2015) on top of the co-matching states.
49	17	,PN to represent these sentences, where N is the number of sentences in the passage.
56	40	For each candidate answer Ai, we can build its matching representation hti ∈ Rl with the question and the passage through Eqn.
60	7	We compare our model with a number of baseline models.
68	16	Besides, we also report the following two results as reference points: Turkers is the performance of Amazon Turkers on a randomly sampled subset of the RACE test set.
71	14	We can see that our proposed complete model, Hier-CoMatching, achieved the best performance among all the public results.
74	11	In this study, we are mainly interested in the contribution of each component introduced in this work to our final results.
75	17	We studied two key factors: (1) the comatching module and (2) the hierarchical aggregation approach.
76	47	We observed a 4 percentage performance decrease by replacing the co-matching module with a single matching state (i.e., only Ma in Eqn (3)) by directly concatenating the question with each candidate answer (Yin et al., 2016).
77	28	We also observe about 2 percentage decrease when we treat the passage as a plain sequence, and run a two-layer LSTM (to ensure the numbers of parameters are comparable) over the whole passage instead of the hierarchical LSTM.
81	8	We can see that the performance of our model on different types of questions in the RACE dataset is quite similar.
83	101	In order to answer questions that require summarization, inference or reasoning, we still need to further explore the dataset and improve the model.
86	14	Similarly, on statement-justification questions with the keyword “true”, our model could achieve better accuracy of 51% than 47%.
87	30	In this paper, we proposed a co-matching model for multi-choice reading comprehension.
88	12	The model consists of a co-matching component and a hierarchical aggregation component.
89	8	We showed that our model could achieve state-of-the-art performance on the RACE dataset.
90	5	In the future, we will adapt the idea of co-matching and hierarchical aggregation to the standard open-domain QA setting for answer candidate reranking (Wang et al., 2017).
