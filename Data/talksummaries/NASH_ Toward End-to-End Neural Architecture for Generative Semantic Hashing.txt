0	94	The problem of similarity search, also called nearest-neighbor search, consists of finding documents from a large collection of documents, or corpus, which are most similar to a query document of interest.
3	63	By representing every document in the corpus as a similaritypreserving discrete (binary) hashing code, the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing codes, i.e., the number of bits that are different between two codes.
6	59	These methods can be generally divided into two categories: (i) binary codes for documents are first learned in an unsupervised manner, then l binary classifiers are trained via supervised learning to predict the l-bit hashing code (Zhang et al., 2010; Xu et al., 2015); (ii) continuous text representations are first inferred, which are binarized as a second (separate) step during testing (Wang et al., 2013; Chaidaroon and Fang, 2017).
18	49	Summarizing, the contributions of this paper are: (i) to the best of our knowledge, we present the first semantic hashing architecture that can be trained in an end-to-end manner; (ii) we propose a neural variational inference framework to learn compact (regularized) binary codes for documents, achieving promising results on both unsupervised and supervised text hashing; (iii) the connection between our method and rate-distortion theory is established, from which we demonstrate the advantage of injecting data-dependent noise into the latent variable during training.
33	28	Intuitively, latent codes learned from a model that accounts for the generative term should naturally encapsulate key semantic information from x because the generation/reconstruction objective is a function of p(x|z).
35	30	We define a generative model that simultaneously accounts for both the encoding distribution, p(z|x), and decoding distribution, p(x|z), by defining approximations qφ(z|x) and qθ(x|z), via inference and generative networks, gφ(x) and gθ(z), parameterized by φ and θ, respectively.
41	36	To tailor the NVI framework for semantic hashing, we cast z as a binary latent variable and assume a multivariate Bernoulli prior on z: p(z) ∼ Bernoulli(γ) = ∏l i=1 γ zi i (1 − γi)1−zi , where γi ∈ [0, 1] is component i of vector γ.
42	29	Thus, the encoding (approximate posterior) distribution qφ(z|x) is restricted to take the form qφ(z|x) = Bernoulli(h), where h = σ(gφ(x)), σ(·) is the sigmoid function, and gφ(·) is the (nonlinear) inference network specified as a multilayer perceptron (MLP).
44	31	Suppose z is a l-bit hash code, for the deterministic binarization, we have, for i = 1, 2, ......, l: zi = 1σ(giφ(x))>0.5 = sign(σ(giφ(x)− 0.5) + 1 2 , (1) where z is the binarized variable, and zi and giφ(x) denote the i-th dimension of z and gφ(x), respectively.
47	63	Because of this sampling process, we do not have to assume a predefined threshold value like in (1).
50	25	Instead, we maximize a variational lower bound.
56	29	Moreover, discrete latent variables are inherently incompatible with backpropagation, since the derivative of the sign function is zero for almost all input values.
61	26	With the ST gradient estimator, the first loss term in (3) can be backpropagated into the encoder network to fine-tune the hash function gφ(x).
68	32	To reconstruct text data x from sampled binary representation z, a deterministic decoder is typically utilized (Miao et al., 2016; Chaidaroon and Fang, 2017).
69	34	Inspired by the success of employing stochastic decoders in image hashing applications (Dai et al., 2017; Theis et al., 2017), in our experiments, we found that injecting random Gaussian noise into z makes the decoder a more favorable regularizer for the binary codes, which in practice leads to stronger retrieval performance.
77	35	For the case of stochastic latent variable z, the objective function in (3) can be written in a form similar to the rate-distortion tradeoff: minEqφ(z|x) − log qφ(z|x)︸ ︷︷ ︸ Rate + 1 2σ2︸︷︷︸ β ||x− Ez||22︸ ︷︷ ︸ Distortion +C  , (8) where C is a constant that encapsulates the prior distribution p(z) and the Gaussian distribution normalization term.
86	51	The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing, where a mapping from latent variable z to labels y is learned, here parametrized by a twolayer MLP followed by a fully-connected softmax layer.
107	29	Similar documents to the query in the corresponding training set need to be retrieved based on the Hamming distance of their hashing codes, i.e. number of different bits.
109	25	Specifically, during testing, for a query document, we first retrieve the 100 nearest/closest documents according to the Hamming distances of the corresponding hash codes (i.e., the number of different bits).
113	51	We experimented with four variants for our NASH model: (i) NASH: with deterministic decoder; (ii) NASH-N: with fixed random noise injected to decoder; (iii) NASH-DN: with data-dependent noise injected to decoder; (iv) NASH-DN-S: NASH-DN with supervised information during training.
116	37	It can be also observed that the injection of noise into the decoder networks has improved the robustness of learned binary representations, resulting in better retrieval performance.
117	79	More importantly, by making the variances of noise adaptive to the specific input, our NASH-DN achieves even better results, compared with NASH-N, highlighting the importance of exploring/learning the trade-off between rate and distortion objectives by the data itself.
119	84	Another observation is that the retrieval results tend to drop a bit when we set the length of hashing codes to be 64 or larger, which also happens for some baseline models.
120	54	This phenomenon has been reported previously in Wang et al. (2012); Liu et al. (2012); Wang et al. (2013); Chaidaroon and Fang (2017), and the reasons could be twofold: (i) for longer codes, the number of data points that are assigned to a certain binary code decreases exponentially.
122	107	However, even with longer hashing codes, our NASH models perform stronger than the baselines in most cases (except for the 20Newsgroups dataset), suggesting that NASH can effectively allocate documents to informative/meaningful hashing codes even with limited training data.
129	29	We compare these two types of binarization functions in the case of unsupervised hashing.
130	31	As illustrated in Figure 3, stochastic sampling shows stronger retrieval results on all three datasets, indicating that endowing the sampling process of latent variables with more stochasticity improves the learned representations.
149	94	On the contrary, the hashing codes for documents with different topics exhibit much larger Hamming distance.
153	48	Motivated by the connections between the proposed method and rate-distortion theory, we inject data-dependent noise into the Bernoulli latent variable at the training stage.
154	28	The effectiveness of our framework is demonstrated with extensive experiments.
