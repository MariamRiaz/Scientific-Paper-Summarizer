{"sections": [{"heading": "1. Introduction", "text": "Property testing is the study of algorithms that query their input a small number of times and distinguish between whether their input satisfies a given property or is \u201cfar\u201d from satisfying that property. The quest for efficient testing algorithms was initiated by (Blum et al., 1993) and (Babai et al., 1991) and later explicitly formulated by (Rubinfeld & Sudan, 1996) and (Goldreich et al., 1998). Property testing can be viewed as a relaxation of the traditional notion of a decision problem, where the relaxation is quantified in terms of a distance parameter. There has been extensive work in this area over the last couple of decades; see, for instance, the surveys (Ron, 2008) and (Rubinfeld & Shapira, 2006) for some different perspectives.\n*Equal contribution 1Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India. Correspondence to: Siddharth Barman <barman@iisc.ac.in>, Arnab Bhattacharyya <arnabb@iisc.ac.in>, Suprovat Ghoshal <suprovat@iisc.ac.in>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nAs evident from these surveys, research in property testing has largely focused on properties of combinatorial and algebraic structures, such as bipartiteness of graphs, linearity of Boolean functions on the hypercube, membership in errorcorrecting codes or representability of functions as concise Boolean formulae. In this work, we study the question of testing properties of continuous structures, specifically properties of vectors and matrices over the reals.\nOur computational model is a natural extension of the standard property testing framework by allowing queries to be linear measurements of the input. Let P \u2282 Rd be a property of real vectors. Let dist : Rd \u2192 R>0 be a \u201cdistance\u201d function such that dist(x) = 0 for all x \u2208 P . We say that an algorithm A is a tester for P with respect to dist and with parameters \u03b5, \u03b4 > 0 if for any input y \u2208 Rn, the algorithm A observes My where M \u2208 Rq\u00d7d is a randomized matrix and has the following guarantee:\n(i) If y \u2208 P , PrM[A(My) accepts] > 1\u2212 \u03b4.\n(ii) If dist(y) > \u03b5, PrM[A(My) accepts] 6 \u03b4.\nWe call each inner product between the rows of M and y a (linear) query, and the number of rows q = q(\u03b5, \u03b4) is the query complexity of the tester. The running time of the tester A is its running time on the outcome of its queries. As typical in property testing, we do not count the time needed to evaluate the queries. If P \u2282 Rd\u00d7p is a property of real matrices with an associated distance function dist : Rd\u00d7p \u2192 R>0, testing is defined similarly: given an input matrix Y \u2208 Rd\u00d7p, the algorithm observes MY for a random matrix M \u2208 Rq\u00d7d with analogous completeness and soundness properties. A linear projection of an input vector or matrix to a low-dimensional space is also called a linear sketch or a linear measurement. The technique of obtaining small linear sketches of high-dimensional vectors has been used to great effect in algorithms for streaming (e.g., (Alon et al., 1996; McGregor, 2014)) and numerical linear algebra (see (Woodruff, 2014) for an excellent survey). Because GPUs are specially designed to optimize matrix-vector computation, many modern optimization and learning algorithms work with linear sketches of their input.\nWe focus on testing whether a vector is sparse with respect\nto some basis.1 A vector x is said to be k-sparse if it has at most k nonzero coordinates. Sparsity is a structural characteristic of signals of interest in a diverse range of applications. It is a pervasive concept throughout modern statistics and machine learning, and algorithms to solve inverse problems under sparsity constraints are among the most successful stories of the optimization community (see the book (Hastie et al., 2015)). The natural property testing question we consider is whether there exists a solution to a linear inverse problem under a sparsity constraint.\nThere are two settings in which we investigate the sparsity testing problem.\n(a) In the first setting, the basis is not known in advance. For input vectors y1,y2, . . . ,yp \u2208 Rd, the property to test is whether there exists a matrix A \u2208 Rd\u00d7m and k-sparse unit vectors x1,x2, . . .xp \u2208 Rm such that yi = Axi for all i \u2208 [p]. Note that m is specified as a parameter and could be much larger than d (the overcomplete case). In this setting, we restrict the unknown A to be a (\u03b5, k)-RIP matrix which means that (1 \u2212 \u03b5)\u2016x\u2016 6 \u2016Ax\u2016 6 (1 + \u03b5)\u2016x\u2016 for any ksparse x. This is a standard assumption made in many related works (see Section 1.2 for details).\nIn this setting, we design an efficient tester for this property that projects the inputs toO(\u03b5\u22122 log p) dimensions and, informally speaking, rejects if for all (\u03b5, k)-RIP matrices A, there is some yi such that yi \u2212Axi has large norm for all \u201capproximately sparse\u201d xi.\n(b) In the second setting, a design matrix A \u2208 Rd\u00d7m is known explicitly, and the property to test is whether a given input vector y \u2208 Rd equals Ax for a k-sparse vector x \u2208 Rm. For instance, A can be the Fourier basis or an overcomplete dictionary in an image processing application. We approach this problem in full generality, without putting any restriction on the structure of A.\nInformally, our main result in this setting is that for any design matrix A, there exists a tester projecting the input y toO(k logm) dimensions that rejects if y\u2212Ax has large norm for any O(k)-sparse x. The running time of the tester is polynomial inm. As we describe in Section 1.2, previous work in numerical linear algebra yields a tester with the same query complexity and with qualitatively similar soundness guarantees but which requires running time exponential in m or assumptions about the matrix A.\nRemark 1.1 (Problem Formulation). Note that the settings considered in the known and unknown design matrix settings\n1With slight abuse of notation, we use the term basis to denote the set of columns of a design matrix. The columns might not be linearly independent.\nare quite different from each other. In particular, for the known design setting, the input is a single vector. However, given a single input vector y \u2208 Rd, the analogous unknown design testing question would be moot, since one can always consider the vector y to be the design matrix A, in which it trivially admits a 1-sparse representation. For the same reason, unknown design testing is interesting only when the number of vectors p exceeds m.\nIn both of the above tests, the measurement matrix is a random matrix with iid gaussian entries, chosen so as to preserve norms and certain other geometric properties upon dimensionality reduction.2 In particular, our testers are oblivious to the input. It is a very interesting open question as to whether non-oblivious testers can strengthen the above results."}, {"heading": "1.1. Our Results", "text": "We now present our results more formally. For integer m > 0, let Sm\u22121 = {x \u2208 Rm : \u2016x\u2016 = 1}, and let Spmk = {x \u2208 Sm\u22121 : \u2016x\u20160 6 k}.3\nTheorem 1.2 (Unknown Design Matrix). Fix \u03b5, \u03b4 \u2208 (0, 1) and positive integers d, k,m and p, such that (k/m)1/8 < \u03b5 < 1100 and k > 10 log 1 \u03b5 . There exists a tester with query complexity O(\u03b5\u22122 log (p/\u03b4)) which, given as input vectors y1,y2, . . . ,yp \u2208 Rd, has the following behavior (where Y is the matrix having y1,y2, . . . ,yp as columns):\n\u2013 Completeness: If Y admits a decomposition Y = AX, where A \u2208 Rd\u00d7m satisfies (\u03b5, k)-RIP and X \u2208 Rm\u00d7p with each column of X in Spmk , then the tester accepts with probability > 1\u2212 \u03b4.\n\u2013 Soundness: Suppose Y does not admit a decomposition Y = A(X + Z) + W with\n1. The design matrix A \u2208 Rd\u00d7m being (\u03b5, k)-RIP, with \u2016ai\u2016 = 1 for every i \u2208 [m]. 2. The coefficient matrix X \u2208 Rm\u00d7p being column wise `-sparse, where ` = O(k/\u03b54).\n3. The error matrices Z \u2208 Rm\u00d7p and W \u2208 Rd\u00d7p satisfying\n\u2016zi\u2016\u221e 6 \u03b52, \u2016wi\u20162 6 O(\u03b51/4) for all i \u2208 [p].\nThen the tester rejects with probability > 1\u2212 \u03b4. 2If evaluating the queries efficiently was an objective, one could also use sparse dimension reduction matrices (Dasgupta et al., 2010; Kane & Nelson, 2014; Bourgain et al., 2015), but we do not pursue this direction here.\n3Here, \u2016x\u20160 denotes the the sparsity of the vector, \u2016x\u20160 := |{i \u2208 [m] | xi 6= 0}|. Without any subscript, \u2016 \u00b7 \u2016 denotes the `2-norm: \u2016x\u2016 := \u221a\u2211 i x 2 i .\nThe contrapositive of the soundness guarantee from the above theorem states that if the tester accepts, then matrix Y admits a factorization of the form Y = A(X+Z)+W, with error matrices Z and W having `\u221e and `2 error bounds. The matrix X+Z is a sparse matrix with `\u221e-based thresholding, and W is an additive `2-error term.4\nTheorem 1.3 (Known Design Matrix). Fix \u03b5, \u03b4 \u2208 (0, 1) and positive integers d, k,m and a matrix A \u2208 Rd\u00d7m such that \u2016ai\u2016 = 1 for every i \u2208 [m]. There exists a tester with query complexity O(k\u03b5\u22122 log(m/\u03b4)) that behaves as follows for an input vector y \u2208 Rd:\n\u2013 Completeness: If y = Ax for some x \u2208 Spmk , then the tester accepts with probability 1.\n\u2013 Soundness: If \u2016Ax\u2212 y\u20162 > \u03b5 for every x : \u2016x\u20160 6 K, then the tester rejects with probability > 1 \u2212 \u03b4. Here, K = O(k/\u03b52).\nThe running time of the tester is poly(m, k, 1/\u03b5).\nA different way of stating the result is that the tester, using O(k\u03b5\u22122 log(m/\u03b4)) linear queries, accepts with probability 1 if y = Ax for a k-sparse x \u2208 Rm and rejects with probability 1\u2212 \u03b4 if \u2016Ax\u2212 y\u2016 > \u03b5\u2016x\u2016 for every O(k/\u03b52)sparse x. To complement this result, we show that a better tradeoff between the sparsity and reconstruction error is likely to be impossible.\nTheorem 1.4 (Hardness). Assume SAT does not have nO(log logn)-time algorithms, and let \u03b7 be any constant less than 1. Then, there does not exist a polynomial time algorithm that, given input A \u2208 Rd\u00d7m (where \u2016ai\u2016 = 1 for every i \u2208 [m]), y \u2208 Rd and \u03b5 > 0, distinguishes with constant probability between the following two cases: (i) y = Ax for a k-sparse x, and (ii) \u2016y \u2212Ax\u2016 > \u03b5\u2016x\u2016\u03b7 for every (k/\u03b52)-sparse x.\nNote that the above hardness applies to any polynomial time algorithm, not just sketching algorithms.\nWe also give tolerant variants of these testers (Theorems H.1 and H.2) which can handle bounded noise for the completeness case. Moreover, the tester for the known design case can be converted into a new sketching algorithm for sparse recovery (Theorem D.1).\nFinally, we also give an algorithm for testing dimensionality, which is based on similar techniques.\nTheorem 1.5 (Testing Dimensionality). Fix \u03b5, \u03b4 \u2208 (0, 1), positive integers d, k and p, where k > 10\u03b52 log d. There exists a tester with query complexity O(p log \u03b4\u22121), which\n4Theorem 1.2 can be restated in terms of incoherent (instead of RIP) design matrices as well. This follows from the fact that the incoherence and RIP constants of a matrix are order-wise equivalent. This observation is formalized in Appendix F.\ngives as input vectors y1, . . . ,yp \u2282 Sd\u22121, has the following behavior:\n\u2013 Completeness: If rank(Y ) 6 k, then the tester accepts with probability > 1\u2212 \u03b4.\n\u2013 Soundness: If rank\u03b5(Y ) > k\u2032, then the tester rejects with probability > 1\u2212 \u03b4. Here, k\u2032 = 20k/\u03b52\nThe soundness criteria in the above Theorem is stated in terms of the \u03b5-approximate rank of a matrix (see Definition E.1). This is a well-studied relaxation of the standard definition of rank, and has applications in approximation algorithms, communication complexity and learning theory (see (Alon et al., 2013) and references therein)."}, {"heading": "1.2. Related Work", "text": "Although, to the best of our knowledge, the testing problems we consider have not been explicitly investigated before, there are several related areas of study that frame our results in their proper context.\nUnknown Design setting. In the setting of the unknown design matrix, the question of recovering the design matrix and the sparse representation (as opposed to our problem of testing their existence) is called the dictionary learning or sparse coding problem. The first work to give a dictionary learning algorithm with provable guarantees was (Spielman et al., 2012) where the dictionary was restricted to be square. For the more common overcomplete setting, (Arora et al., 2014) and (Agarwal et al., 2014) independently gave algorithms with provable guarantees for dictionaries satisfying incoherence and RIP respectively. All of these (as well as other more recent) works assume distributions from which the input samples are generated in an i.i.d fashion. In contrast, our work is in the agnostic setting and hence, is incomparable with these results.\nIt is known that the dictionary learning problem is NP-hard, even for square dictionaries (Razaviyayn et al., 2014; Tillmann, 2015). In fact, (Tillmann, 2015) shows that unless SAT has a quasi-polynomial time algorithm, it is impossible, given Y \u2208 Rd\u00d7p, to approximate in polynomial time the minimum k upto a factor 2log\n1\u2212\u03b5 d (for any \u03b5 > 0) such that Y = AX where each column of X \u2208 Rd\u00d7p is k-sparse. This motivates our bicriteria relaxation of both the sparsity as well as the additive error in Theorem 1.2.\nKnown Design setting. Some results about testing sparsity in the known design setting are implicit in recent work on streaming algorithms and oblivious subspace embeddings. Of particular interest are the following results:\nTheorem 1.6 (Implicit in (Kane et al., 2010)). Fix \u03b5 \u2208 (0, 1), positive integers m, k and an invertible matrix A \u2208\nRm\u00d7m. Then, there is a tester with query complexity O(\u03b5\u22122 log(m)) that, for an input y \u2208 Rm, accepts with probability at least 2/3 if y = Ax for some k-sparse x \u2208 Zm, and rejects with probability 2/3 if y 6= Ax for all (1 + \u03b5)k-sparse x \u2208 Zm. The running time of the algorithm is poly(m, 1/\u03b5). Theorem 1.7 (Implicit in prior work, see (Woodruff, 2014)). Fix \u03b5, \u03b4 \u2208 (0, 1) and positive integers d, k,m and a matrix A \u2208 Rd\u00d7m. Then, there is a tester with query complexity O(k\u03b5\u22122 log(m/\u03b4)) that, for an input vector y \u2208 Rd, accepts with probability 1 if y = Ax for some k-sparse x and rejects with probability at least 1\u2212 \u03b4 if \u2016y \u2212Ax\u2016 > \u03b5 for all k-sparse x. The running time of the tester is the time required to solve the following optimization problem:\nx\u0302 = arg min x\u2032\u2208K \u2016SAx\u2032 \u2212 Sy\u2016 = arg min x\u2032\u2208K \u2016S(Ax\u2032 \u2212 y)\u2016\n(1) where S \u2208 Rq\u00d7d is a random sketch matrix (where q d) and K = {x : \u2016x\u20160 6 k}\nDetailed descriptions of the algorithms and proof sketches for the above Theorems are given in Section B.4. The algorithms from the above theorems come with significant limitations. In particular, the guarantees for Theorem 1.6 hold only when the design matrix is invertible. On the other hand, the running time for the algorithm in Theorem 1.7 is the cost of solving the optimization problem in Equation (1), which is known to be NP-hard for general matrices.\nThe problem of testing sparsity has also been studied in non-sketching settings as well, where the algorithm is allowed access to the entire input. In particular, (Natarajan, 1995) gave a bicriteria-approximation algorithm, where the blowup in the sparsity is proportional to \u2016A\u2020\u201622 (which can be large if A is ill conditioned).\nTesting Dimensionality. In (Czumaj et al., 2000), some problems in computational geometry were studied from the property testing perspective, but the problems involved only discrete structures. (Krauthgamer & Sasson, 2003) studied the problem of testing dimensionality, but their notion of farness from being low-dimensional is different from ours5. (Chierichetti et al., 2017) gave approximation algorithms for computing approximate rank of the matrix, in the setting where the algorithms have full access to the input."}, {"heading": "1.3. Discussion", "text": "A standard approach to designing a testing algorithm for a property P is the following: we identify an alternative property P \u2032 which can be tested efficiently and exactly, while satisfying the following:\n5In their setup, a sequence of vectors y1, . . . ,yp is \u03b5-far from being d-dimensional if at least \u03b5p vectors need to be removed to make it be of dimension d\n(i) Completeness: If an instance satisfies P , then it satisfies P \u2032.\n(ii) Soundness: If an instance satisfies P \u2032, the it is close to satisfying P .\nIn other words, we reduce the property testing problem to that of finding a efficiently testable property P \u2032, which can be interpreted as a surrogate for property P . The inherent geometric nature of the problems looked at in this paper motivate us to look for P \u2032s which are based around convex geometry and high dimensional probability.\nFor the unknown design setting, we are intuitively looking for a P \u2032 based on a quantity \u03c9 that robustly captures sparsity and is easily computable using linear queries, in the sense that \u03c9 is small when the input vectors have a sparse coding and large when they are \u201cfar\u201d from any sparse coding. Moreover, \u03c9 needs to be invariant with respect to isometries and nearly invariant with respect to near-isometries. A natural and widely-used measure of structure that satisfies the above mentioned properties is the gaussian width.\nDefinition 1.8. The gaussian width of a set S \u2286 Rd is: \u03c9(S) = Eg[supv\u2208S\u3008g,v\u3009] where g \u2208 Rd is a random vector drawn from N(0, 1)d, i.e., a vector of independent standard normal variables.\nThe gaussian width of S measures how well on average the vectors in S correlate with a randomly chosen direction. It is invariant under orthogonal transformations of S as the distribution of g is spherically symmetric. It is a well-studied quantity in high-dimensional geometry ((Vershynin, 2015; Mendelson & Vershynin, 2002)), optimization ((Chandrasekaran et al., 2012; Amelunxen et al., 2013)) and statistical learning theory ((Bartlett & Mendelson, 2002)). The following bounds are well-known.\nLemma 1.9 (See, for example, (Rudelson & Vershynin, 2008; Vershynin, 2015)).\n(i) If S is a finite subset of Sd\u22121, then \u03c9(S) 6 \u221a 2 log |S|.\n(ii) \u03c9(Sd\u22121) 6 \u221a d\n(iii) If S \u2286 Sd\u22121 is of dimension k, then \u03c9(S) 6 \u221a k.\n(iv) \u03c9(Spdk) 6 2 \u221a 3k log(d/k) when d/k > 2 and k > 4.\nIn the context of Theorems 1.2 and 1.5, one can observe that whenever a given set satisfies sparsity or dimensionality constraints, the gaussian width of such sets are small (points (iii) and (iv) from the above Lemma). Therefore, one can hope to test dimensionality or sparsity by computing an empirical estimate of the gaussian width and comparing the estimate to the results in Lemma 1.9. While completeness of such testers would follow directly from concentration of measure, establishing soundness would require us to show\nthat approximate converses of points (iii) and (iv) hold as well i.e., whenever the gaussian width of the set S is small, it can be approximated by sets which are approximately sparse in some design matrix (or have low rank).\nFor the soundness direction of Theorem 1.2, the above arguments are made precise using Lemma 3.3 and Theorem 3.2, which show that small gaussian width sets can be approximated by random projections of sparse vectors and vectors with small `\u221e-norm. For Theorem 1.5, we use lemma E.2 which shows that sets with small gaussian width have small approximate rank.\nFor the known design setting, we are looking for aP \u2032, which would ensure that if a given point y \u2208 Rd satisfies P \u2032, then it is close to having a sparse representation in the matrix A. Towards this end, the approximate Carathe\u0301odory\u2019s theorem states that if a point y \u2208 Rd belonging to the convex-hull of A, then it is close to another point which admits a sparse representation. On the other hand, if a unit vector x \u2208 Sd\u22121 \u2229Rd+ were k-sparse to begin with , then it can be seen that the corresponding y = Ax would belong to the convex hull of \u221a k \u00b7 A. These observations taken together, seem to suggest that one can take P \u2032 to be membership in the convex-hull of \u221a k \u00b7A. This intuition is made precise in the analysis of the tester in Section 4."}, {"heading": "1.4. Organization", "text": "Section 2 introduces notations and preliminaries used in the rest of the paper. In Sections 3 and 4, we design and analyze the testers for the unknown and known basis setting respectively. Section 5 contains empirical results which supplement Section 3. In Section B we prove additional lemmas used in the proof of Theorem 3.2, and in Section A we prove Theorem 3.2. In Section C, we prove Theorem C.1, a stronger version of Theorem 1.4. In Section D, we show that Theorem 1.3 yields a sketching algorithm for sparse recovery. In Section E, we design and analyze the dimensionality tester. In Section G, we describe the results for testing sparsity in the known case implicit in previous work. Finally, in Section H, we give noise tolerant testers for the known and unknown basis settings."}, {"heading": "2. Preliminaries", "text": "Given S \u2282 Rd, we shall use conv(S) to denote the convex hull of S. For a vector x \u2208 Rd, we use \u2016 \u00b7 \u2016p to denote its `p-norm, and we will drop the indexing when p = 2. We denote the `2-distance of the point x to the set S by dist(x, S). We recall the definition of \u03b5-isometry:\nDefinition 2.1. Given sets S \u2282 Rm and S\u2032 \u2282 Rn (for some m,n \u2208 N), we say that S\u2032 is an \u03b5-isometry of S, if there exists a mapping \u03c8 : S 7\u2192 S\u2032 which satisfies the following\nproperty:\n\u2200x,y \u2208 S : (1\u2212\u03b5)\u2016x\u2212y\u2016 6 \u2016\u03c8(x)\u2212\u03c8(y)\u2016 6 (1+\u03b5)\u2016x\u2212y\u2016\nFor the unknown design setting, we shall require the notion of Restricted Isometry Property, which is defined as follows:\nDefinition 2.2 ((\u03b5, k)-RIP). A matrix A \u2208 Rd\u00d7m satisfies (\u03b5, k)-RIP, if for every x \u2208 Spmk the following holds:\n(1\u2212 \u03b5)\u2016x\u2016 6 \u2016Ax\u2016 6 (1 + \u03b5)\u2016x\u2016 (2)\nWe use the following version of Gordon\u2019s Theorem repeatedly in this work.\nTheorem 2.3 (Gordon\u2019s Theorem (Gordon, 1985)). Given S \u2282 SD\u22121 and a random gaussian matrix G \u223c 1\u221a d\u2032 N(0, 1)d \u2032\u00d7D, we have\nE G [ max x\u2208S \u2016Gx\u20162 ] 6 1 + \u03c9(S)\u221a d\u2032\nIt directly implies the following generalization of the Johnson-Lindenstrauss lemma.\nTheorem 2.4 (Generalized Johnson-Lindenstrauss lemma). Let S \u2286 Sn\u22121. Then there exists linear transformation \u03a6 : Rn 7\u2192 Rd\u2032 , for d\u2032 = O ( \u03c9(S)2\n\u03b52\n) , such that \u03a6 is an\n\u03b5-isometry on S. Moreover, \u03a6 \u223c 1\u221a d\u2032 N(0, 1)d \u2032\u00d7n is an \u03b5-isometry on S with high probability.\nIt can be easily verified that the quantity maxx\u2208S \u2016Gx\u20162 is 1-Lipschitz with respect to G. Therefore, using Gaussian concentration for Lipschitz functions, we get the following corollary :\nCorollary 2.5. Let S and G be as in Theorem 2.3. Then for all \u03b5 > 0, we have\nPr G ( max x\u2208S \u2016Gx\u20162 > 1 + ( 1 + \u03b5 )\u03c9(S)\u221a d\u2032 ) 6 exp ( \u2212O(\u03b5\u03c9(S))2\n) The following lemma gives concentration for the gaussian width:\nLemma 2.6 (Concentration on the gaussian width (Boucheron et al., 2013)). Let S \u2282 Rd. Let W = supv\u2208S\u3008g,v\u3009 where g is drawn from N(0, 1)d. Then:\nPr[|W \u2212EW | > u] < 2e\u2212 u2 2\u03c32\nwhere \u03c32 = supv\u2208S ( \u2016v\u201622 ) . Notice that the bound is dimension independent.\nLastly, we shall use the `2-variant of the approximate Carathe\u0301odory\u2019s Theorem:\nTheorem 2.7. (Theorem 0.1.2 (Vershynin, 2016) ) Given X = {w1, . . . ,wp} where \u2016wi\u2016 6 1 for every i \u2208 [p]. Then for every choice z \u2208 conv ( X )\nand k \u2208 N, there exists wi1 ,wi2 , . . . ,wik such that\u2225\u2225\u2225\u22251k \u2211\nj\u2208[k]\nwij \u2212 z \u2225\u2225\u2225\u2225 6 2\u221ak (3)"}, {"heading": "2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector", "text": "We record here simple lemmas bounding the number of linear queries needed to estimate the gaussian width of a set and the length of a vector.\nLemma 2.8 (Estimating Gaussian Width using linear queries). For any u > 4, \u03b5 \u2208 (0, 1/2) and \u03b4 > 0, there is a randomized algorithm that given a set S \u2286 Rd and \u2016v\u2016 \u2208 [1 \u00b1 \u03b5] for all v \u2208 S, computes \u03c9\u0302 such that \u03c9(S)\u2212 u 6 \u03c9\u0302 6 \u03c9(S) + u with probability at least 1\u2212 \u03b4. The algorithm makes O(log(1/\u03b4) \u00b7 |S|) linear queries to S.\nProof. By Lemma 2.6, for a random g \u223c N(0, 1)d, supv\u2208S\u3008g,v\u3009 is away from \u03c9(S) by u with probability at most 2e\u221216/4.5 < 0.1. By the Chernoff bound, the median of O(log \u03b4\u22121) trials will satisfy the conditions required of \u03c9\u0302 with probability at least 1\u2212 \u03b4. Lemma 2.9 (Estimating norm using linear queries). Given \u03b5 \u2208 (0, 1/2) and \u03b4 > 0, for any vector x \u2208 Rd , only O(\u03b5\u22122 log \u03b4\u22121) linear queries to x suffice to decide whether \u2016x\u2016 \u2208 [1\u2212 \u03b5, 1 + \u03b5] with success probability 1\u2212 \u03b4.\nProof. It is easy to verify that Eg\u223cN(0,1)d [\u3008g,x\u30092] = \u2016x\u20162. Therefore, it can be estimated to a multiplicative error of (1 \u00b1 \u03b5/2) by taking the average of the squares of linear measurements using O ( 1 \u03b52 log 1 \u03b4 ) -queries. For the case \u2016x\u20162 6 2, a multiplicative error (1\u00b1 \u03b5/2) implies an additive error of \u03b5. Furthermore, when \u2016x\u20162 > 2, a multiplicative error of (1\u00b1 \u03b5/2) implies that L > 2(1\u2212 \u03b5/2) > 1 + \u03b5 for \u03b5 < 1/2."}, {"heading": "3. Analysis for Unknown Design setting", "text": "In this section, we prove Theorem 1.2. Let S denote the set {y1, . . . ,yp}. Our testing algorithm is shown in Algorithm 1.\nThe number of linear queries made by the tester is O(p\u03b5\u22122 log(p/\u03b4)) in Line 1 and O(p log \u03b4\u22121) in Line 2."}, {"heading": "3.1. Completeness", "text": "Assume that for each i \u2208 [p], yi = Axi for a matrix A \u2208 Rd\u00d7m satisfying (\u03b5, k)-RIP and xi \u2208 Spmk . By definition\nAlgorithm 1 SparseTestUnknown 1: Use Lemma 2.9 to decide with probability at least 1\u2212 \u03b4/2 if there exists yi such that \u2016yi\u2016 6\u2208 [1\u2212 2\u03b5, 1 + 2\u03b5]. Reject if so.\n2: Use Lemma 2.8 to obtain \u03c9\u0302, an estimate of \u03c9(S) within additive error \u221a 3k log(m/k) with probability at least\n1\u2212 \u03b4/2. 3: Accept if \u03c9\u0302 6 4 \u221a 3k log(m/k), else reject.\nof RIP, we know that 1\u2212 \u03b5 6 \u2016yi\u2016 6 1 + \u03b5, so that Line 1 of the algorithm will pass with probability at least 1\u2212 \u03b4/2. From Lemma 1.9, we know that \u03c9({x1, . . .xp}) 6 2 \u221a\n3k log(m/k). Lemma 3.1 shows that the gaussian width of S is approximately the same; its proof, deferred to the appendix (Section B.4), uses Slepian\u2019s Lemma (Lemma B.3).\nLemma 3.1. LetX \u2282 Sm\u22121 be a finite set, and let S \u2282 Rd be an \u03b5-isometric embedding of X . Then\n(1\u2212 \u03b5)\u03c9(X) 6 \u03c9(S) 6 (1 + \u03b5)\u03c9(X) (4)\nHence, the gaussian width of y1, . . . ,yp is at most 2(1 + \u03b5) \u221a\n3k log(m/k). Taking into account the additive error in Line 2, we see that with probability at least 1 \u2212 \u03b4/2, \u03c9\u0302 6 (3 + 2\u03b5) \u221a 3k log(m/k) 6 4 \u221a 3k log(m/k). Hence, the tester accepts with probability at least 1\u2212 \u03b4."}, {"heading": "3.2. Soundness", "text": "As mentioned before, in order to prove soundness we need to show that whenever the gaussian width of the set S is small, it is close to some sparse point-set. Let \u03c9\u2217 = 4 \u221a 3k log mk . We shall break the analysis into two cases: Case (i) { \u03c9\u2217 > (\u03b5/C)2 \u221a d } : For this case, we use the\nfact random projection of discretized sparse point-sets (Definition A.1) form an appropriated cover of S. This is formalized in the following theorem, which in a sense shows an approximate inverse of Gordon\u2019s Theorem for sparse vectors:\nTheorem 3.2. Given \u03b5 > 0 and integers C, d, k and m, let n = O ( k \u03b52 log(m/k) ) . Suppose m > k/\u03b58. Let \u03a6 : Rm 7\u2192 Rn be drawn from 1\u221a n N(0, 1)n\u00d7m. Then, for ` = O(k\u03b5\u22124), with high probability, the set \u03a6norm(S\u0302p m\n` ) is an O(\u03b5 1/4)-cover of Sn\u22121, where\n\u03a6norm(x) = \u03a6(x)/\u2016\u03a6(x)\u20162.\nThe proof of the above Theorem is deferred to Section A. From the choice of parameters we have d 6 C\n\u2032k \u03b52 log m k\nTherefore, using the above Theorem we know that there\nexists (\u03b5, k)-RIP matrix \u03a6 \u2208 Rd\u00d7m such that \u03a6norm ( Spm` ) is an O(\u03b51/4)-cover of Sd\u22121 (and therefore it is a \u03b51/4cover of S). Therefore, there exists X \u2208 Rm\u00d7p such that Y = \u03a6(X) + E where the columns of X and E satisfy the respective \u2016 \u00b7 \u20160 and \u2016 \u00b7 \u20162-upper bounds respectively. Case (ii) { \u03c9\u2217 6 (\u03b5/C)2 \u221a d } : For this case, we use the\nfollowing result on the concentration of `\u221e-norm: Lemma 3.3. Given S \u2282 Sd\u22121, we have\nPr R\u223cOd\n[ max\ny\u2208R(S) \u2016y\u2016\u221e 6 C\n\u03c9(S)\nd1/2\n] > 1\n2\nwhere Od is the orthogonal group in Rd i.e., R is a uniform random rotation.\nAlthough this concentration bound is known, for completeness we give a proof in the appendix (Section B.7). From the above lemma, it follows that there exists R \u2208 Od such that for any z \u2208 Z := R(S) we have \u2016z\u2016\u221e 6 \u03b52 and therefore Y = R\u22121Z. Furthermore, since R is orthogonal, therefore the matrix R\u22121 is also orthogonal, and therefore it satisfies (\u03b5, k)-RIP.\nTo complete the proof, we observe that even though the given factorization has inner dimension d, we can trivially extend it to one with inner dimension m. This can be done by constructing \u03a6 = [ R\u22121 G ] with G \u223c 1\u221a d N(0, 1)d\u00d7m\u2212d. Since \u03c9\u2217 d, from Theorem 2.4 it follows that with high probability G (and consequently \u03a6) will satisfy (\u03b5, k)-RIP. Finally, we construct Z\u0302 \u2208 Rm\u00d7n by padding Z with m \u2212 d rows of zeros. Therefore, by construction Y = \u03a6 \u00b7 Z\u0302, where for every i \u2208 [p] we have \u2016zi\u2016\u221e 6 \u03b52. Hence the claim follows."}, {"heading": "4. Analysis for the Known Design setting", "text": "In this section, we describe and analyze the tester for the known design matrix case. The algorithm itself is a simple convex-hull membership test, which can be solved using a linear program.\nAlgorithm 2 SparseTest-KnownDesign 1: Set n = 100klog m\u03b4 , sample projection matrix \u03a6 \u223c\n1\u221a n N(0, 1)n\u00d7d\n2: Observe linear sketch y\u0303 = \u03a6(y) 3: Let A\u00b1 = A \u222a \u2212A 4: Accept iff y\u0303 \u2208 \u221a k \u00b7 conv ( \u03a6(A\u00b1) ) We shall now prove the completeness and soundness guarantees of the above tester. The running time bound follows because convex hull membership reduces to linear programming.\nCompleteness Let y = Ax where A \u2208 Rd\u00d7m is an arbitrary matrix with \u2016ai\u2016 = 1 for every i \u2208 [m]. Furthermore \u2016x\u20162 = 1 and \u2016x\u20160 6 k. Therefore, by Cauchy-Schwartz we have \u2016x\u20161 6 \u221a k\u2016x\u20162 = \u221a k. Hence, it follows that\ny \u2208 \u221a k \u00b7 conv(A\u00b1). Since \u03a6 : Rm 7\u2192 Rd is a linear trans-\nformation, we have \u03a6(y) \u2208 \u221a k \u00b7 conv(\u03a6(A\u00b1)). Therefore, the tester accepts with probability 1.\nSoundness Consider the set A\u03b5/\u221ak which is the set of all (2k/\u03b52)-uniform convex combinations of \u221a k(A\u00b1) i.e.,\nA\u03b5/ \u221a k = { \u2211 vi\u2208\u2126 \u03b52 2k vi : multiset \u2126 \u2208 (\u221a k.A\u00b1 )2k/\u03b52} (5)\nThen, from the approximate Carathe\u0301odory theorem, it follows that A\u03b5/\u221ak is an \u03b5-cover of \u221a k \u00b7 conv ( A\u00b1 ) . Furthermore, |A\u03b5/\u221ak| 6 (2m) 2k/\u03b52 . By our choice of n, with\nprobability at least 1 \u2212 \u03b4/2, the set \u03a6 ( {y} \u222a A\u03b5/\u221ak ) is \u03b5-isometric to {y} \u222aA\u03b5/\u221ak.\nLet A\u0303\u03b5/\u221ak = \u03a6 ( A\u03b5/ \u221a k ) . Again, by the approximate Carathe\u0301odory\u2019s theorem, the set A\u0303\u03b5/\u221ak is an \u03b5-cover of\n\u03a6 (\u221a k \u00b7 conv(A\u00b1) ) . Now suppose the test accepts y with probability at least \u03b4. Then, with probability at least \u03b4/2, the test accepts and the above \u03b5-isometry conditions hold simultaneously. Then,\ny\u0303 \u2208 \u221a k \u00b7 conv ( \u03a6(A\u00b1) ) 1\u21d2 dist ( y\u0303, A\u0303\u03b5/ \u221a k ) 6 \u03b5\n2\u21d2 dist ( y, A\u03b5/ \u221a k ) 6 \u03b5(1\u2212 \u03b5)\u22121 6 2\u03b5\n\u21d2 dist ( y, \u221a k \u00b7 conv(A\u00b1) ) 6 2\u03b5\nwhere step 1 follows from the \u03b5-cover guarantee of A\u0303\u03b5/\u221ak, step 2 follows from the \u03b5-isometry guarantee. Invoking the approximate Carathe\u0301odory theorem, we get that there exists y\u0302 = Ax\u0302 \u2208 \u221a k \u00b7 conv(\u00b1A) such that \u2016x\u0302\u20160 6 O(k/\u03b52) and \u2016y\u0302 \u2212 y\u2016 6 O(\u03b5). This completes the soundness direction."}, {"heading": "5. Experimental Results", "text": "Our algorithm for the unknown design setting is based on the principle that the property of sparse representability in some basis admits an approximate characterization in terms of gaussian width. This section provides experimental evidence which supplements our theoretical results. For the empirical study, we use the classic Barbara image (which is of size 512\u00d7 512 pixels). Specifically, we consider 9 subimages of size 100 \u00d7 100 pixels each (see Figure 1). For each such sub-image, we compute a matrix representation (by the standard technique of subdividing the images into patches, see, e.g., (Elad & Aharon, 2006)). In particular,\neach sub-image is represented as a matrix Y of dimension 64 \u00d7 8649. Then, for each matrix Y corresponding to a sub-image, we estimate the gaussian width of the `2-column normalized matrix. In addition, setting the number of atoms m = 100 and sparsity k = 10, we run the k-SVD algorithm for 50 iterations and record the reconstruction error.6\nFigure 2 shows the comparison between gaussian width and reconstruction error, in which we observe that there is an approximate correlation between the two quantities. In particular, for sub-images 2,7 and 8\u2014which mostly consist of background\u2014both the gaussian width and the reconstruction error is small. On the other hand, images 3, 6 and 9, which consist of intricate patterns and objects, have large gaussian width as well as large reconstruction error. Consequently, we can deduce that for sub-images with large gaussian width, in order to achieve low reconstruction error, one would have consider a larger number of atoms m or larger sparsity k."}, {"heading": "6. Conclusion and Open Questions", "text": "In this paper, we studied the problem of testing sparsity with respect to unknown and known bases. While the optimization variants of these problems (namely Dictionary Learning and Sparse Recovery) are known to be NP-hard in the worst case, our results show that under appropriate relaxations, these problems admit efficient property testing algorithms. Future work include designing testing algorithms for sparsity over an unknown basis with stronger\n6For a matrix Y \u2208 Rd\u00d7n approximated by overcomplete basis A and coefficient matrix X, the reconstruction error is equal to \u2016Y \u2212AX\u20162F /(n.d).\nguarantees or developing impossibility results. We also hope that this paper leads to study of property testing of other widely studied hypotheses in machine learning such as nonnegative rank and VC-dimension."}, {"heading": "Acknowledgements", "text": "We would like to thank David Woodruff for showing us the sketching-based tester described in Section 1.2."}], "year": 2018, "references": [{"title": "Learning sparsely used overcomplete dictionaries", "authors": ["A. Agarwal", "A. Anandkumar", "P. Jain", "P. Netrapalli", "R. Tandon"], "venue": "In Proc. 27th Annual ACM Workshop on Computational Learning Theory, pp", "year": 2014}, {"title": "The space complexity of approximating the frequency moments", "authors": ["N. Alon", "Y. Matias", "M. Szegedy"], "venue": "In Proc. 28th Annual ACM Symposium on the Theory of Computing,", "year": 1996}, {"title": "The approximate rank of a matrix and its algorithmic applications: approximate rank", "authors": ["N. Alon", "T. Lee", "A. Shraibman", "S. Vempala"], "venue": "In Proc. 45th Annual ACM Symposium on the Theory of Computing,", "year": 2013}, {"title": "Living on the edge: A geometric theory of phase transitions in convex optimization", "authors": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "CoRR, abs/1303.6672,", "year": 2013}, {"title": "Non-deterministic exponential time has two-prover interactive protocols", "authors": ["L. Babai", "L. Fortnow", "C. Lund"], "venue": "Computational Complexity,", "year": 1991}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "authors": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "year": 2002}, {"title": "Sparse approximation via generating point sets", "authors": ["A. Blum", "S. Har-Peled", "B. Raichel"], "venue": "In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms,", "year": 2016}, {"title": "Selftesting/correcting with applications to numerical problems", "authors": ["M. Blum", "M. Luby", "R. Rubinfeld"], "venue": "J. Comp. Sys. Sci.,", "year": 1993}, {"title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "authors": ["S. Boucheron", "G. Lugosi", "P. Massart"], "venue": "OUP Oxford,", "year": 2013}, {"title": "Toward a unified theory of sparse dimensionality reduction in euclidean space", "authors": ["J. Bourgain", "S. Dirksen", "J. Nelson"], "venue": "Geometric and Functional Analysis,", "year": 2015}, {"title": "The convex geometry of linear inverse problems", "authors": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics,", "year": 2012}, {"title": "Algorithms for $\\ell p$ low-rank approximation", "authors": ["F. Chierichetti", "S. Gollapudi", "R. Kumar", "S. Lattanzi", "R. Panigrahy", "D.P. Woodruff"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Property testing in computational geometry", "authors": ["A. Czumaj", "C. Sohler", "M. Ziegler"], "venue": "In Proc. 8th European Symposium on Algorithms,", "year": 2000}, {"title": "A sparse JohnsonLindenstrauss transform", "authors": ["A. Dasgupta", "R. Kumar", "T. Sarl\u00f3s"], "venue": "In Proc. 42nd Annual ACM Symposium on the Theory of Computing,", "year": 2010}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "authors": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image processing,", "year": 2006}, {"title": "Variable selection is hard", "authors": ["D. Foster", "H. Karloff", "J. Thaler"], "venue": "In Conference on Learning Theory, pp", "year": 2015}, {"title": "Property testing and its connection to learning and approximation", "authors": ["O. Goldreich", "S. Goldwasser", "D. Ron"], "venue": "J. ACM,", "year": 1998}, {"title": "Some inequalities for gaussian processes and applications", "authors": ["Y. Gordon"], "venue": "Israel Journal of Mathematics,", "year": 1985}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "authors": ["T. Hastie", "R. Tibshirani", "M. Wainwright"], "venue": "CRC press,", "year": 2015}, {"title": "Sparser johnson-lindenstrauss transforms", "authors": ["D.M. Kane", "J. Nelson"], "venue": "Journal of the ACM (JACM),", "year": 2014}, {"title": "An optimal algorithm for the distinct elements problem", "authors": ["D.M. Kane", "J. Nelson", "D.P. Woodruff"], "venue": "In Proc. 29th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of database systems,", "year": 2010}, {"title": "Property testing of data dimensionality", "authors": ["R. Krauthgamer", "O. Sasson"], "venue": "In Proc. 14th ACM-SIAM Symposium on Discrete Algorithms,", "year": 2003}, {"title": "Graph stream algorithms: a survey", "authors": ["A. McGregor"], "venue": "ACM SIGMOD Record,", "year": 2014}, {"title": "Entropy, combinatorial dimensions and random averages", "authors": ["S. Mendelson", "R. Vershynin"], "venue": "In Computational Learning Theory, 15th Annual Conference on Computational Learning Theory, COLT", "year": 2002}, {"title": "Sparse approximate solutions to linear systems", "authors": ["B.K. Natarajan"], "venue": "SIAM journal on computing,", "year": 1995}, {"title": "Dictionary learning for sparse representation: Complexity and algorithms", "authors": ["M. Razaviyayn", "Tseng", "H.-W", "Luo", "Z.-Q"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2014}, {"title": "Property testing: A learning theory perspective", "authors": ["D. Ron"], "venue": "Foundations and Trends in Machine Learning,", "year": 2008}, {"title": "Sublinear time algorithms", "authors": ["R. Rubinfeld", "A. Shapira"], "venue": "In Proc. International Congress of Mathematicians 2006,", "year": 2006}, {"title": "Robust characterizations of polynomials with applications to program testing", "authors": ["R. Rubinfeld", "M. Sudan"], "venue": "SIAM J. on Comput.,", "year": 1996}, {"title": "On sparse reconstruction from fourier and gaussian measurements", "authors": ["M. Rudelson", "R. Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "year": 2008}, {"title": "The one-sided barrier problem for gaussian noise", "authors": ["D. Slepian"], "venue": "The Bell System Technical Journal,", "year": 1962}, {"title": "Exact recovery of sparsely-used dictionaries", "authors": ["D.A. Spielman", "H. Wang", "J. Wright"], "venue": "In Proc. 25th Annual ACM Workshop on Computational Learning Theory, pp", "year": 2012}, {"title": "On the computational intractability of exact and approximate dictionary learning", "authors": ["A.M. Tillmann"], "venue": "IEEE Signal Processing Letters,", "year": 2015}, {"title": "Lectures in geometric functional analysis", "authors": ["R. Vershynin"], "venue": "Preprint, University of Michigan,", "year": 2011}, {"title": "Estimation in high dimensions: a geometric perspective", "authors": ["R. Vershynin"], "venue": "In Sampling Theory, a Renaissance,", "year": 2015}, {"title": "High dimensional probability, 2016", "authors": ["R. Vershynin"], "year": 2016}, {"title": "Sketching as a tool for numerical linear algebra", "authors": ["D.P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "year": 2014}], "id": "SP:97e5b98168c1cb784806b70ca70db8c9635d5dca", "authors": [{"name": "Siddharth Barman", "affiliations": []}, {"name": "Arnab Bhattacharyya", "affiliations": []}, {"name": "Suprovat Ghoshal", "affiliations": []}], "abstractText": "Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications. In this work, we describe property testing algorithms for sparsity that observe a lowdimensional projection of the input. We consider two settings. In the first setting, we test sparsity with respect to an unknown basis: given input vectors y1, . . . ,yp \u2208 R whose concatenation as columns forms Y \u2208 Rd\u00d7p, does Y = AX for matrices A \u2208 Rd\u00d7m and X \u2208 Rm\u00d7p such that each column of X is k-sparse, or is Y \u201cfar\u201d from having such a decomposition? In the second setting, we test sparsity with respect to a known basis: for a fixed design matrix A \u2208 Rd\u00d7m, given input vector y \u2208 R, is y = Ax for some ksparse vector x or is y \u201cfar\u201d from having such a decomposition? We analyze our algorithms using tools from high-dimensional geometry and probability.", "title": "Testing Sparsity over Known and Unknown Bases"}