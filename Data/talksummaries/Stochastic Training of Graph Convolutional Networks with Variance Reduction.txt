14	7	However, there is no theoretical guarantee on the convergence of the stochastic training algorithm with NS.
16	12	In this paper, we develop novel control variate-based stochastic approximation algorithms for GCN by utilizing the historical activations of nodes as a control variate.
17	57	Our algorithms have new theoretical results on (1) variance reduction from the magnitude of the activation to the magnitude of the difference between current-and-historical activations; (2) exact (zero-variance) predictions at testing time; (3) convergence to a local optimum of GCN during training regardless of the neighbor sampling size D(l), with an asymptotically unbiased stochastic gradient.
18	43	The theoretical properties allow us to significantly reduce the time complexity of stochastic training by sampling only D(l) = 2 neighbors per node, yet still retain the quality of the model.
19	8	We empirically test our algorithms on six graph datasets, and the results match with the theory.
20	130	Comparing with NS, our Input Layer 1 Layer 2 (a) Exact Input Layer 1 Layer 2 (b) Neighbour sampling Input Layer 1 Layer 2 (c) Control variate Latest activation Historical activation Input GraphConv Dropout Dropout GraphConv (1)H (2)H GraphConv GraphConv (1) (2) (d) CVD network Figure 1.
21	76	Two-layer graph convolutional networks, and the receptive field of a single vertex.
22	38	algorithms significantly reduce the bias and variance of the gradient.
23	98	Comparing with the exact algorithm which considers all the neighbors, our algorithms with only D(l) = 2 neighbors still get the same accuracy at testing time, and achieve similar predictive performance during training in a comparable number of epochs, with a much lower time complexity, while these results are not achievable by NS.
24	9	On the largest Reddit dataset, the training time of our algorithm is 7 times shorter than that of the best-performing competitor among exact, neighbor sampling and importance sampling (Chen et al., 2018) algorithms.
25	48	We briefly review graph convolutional networks (GCNs), stochastic training, neighbor sampling, and importance sampling in this section.
27	19	However, the algorithm is neither limited to the task nor the model.
28	10	Our algorithm is applicable to other models including GraphSAGE-mean (Hamilton et al., 2017a) and graph attention networks (GAT) (Veličković et al., 2018), and other tasks (Kipf & Welling, 2016; Berg et al., 2017; Schlichtkrull et al., 2017; Hamilton et al., 2017b), as long as the model aggregates neighbor activations by averaging.
29	17	In the node classification task, we have an undirected graph G = (V, E) with V = |V| vertices and E = |E| edges, where each vertex v consists of a feature vector xv and a label yv.
31	2	The goal is to predict the labels for the rest vertices VU := V\VL.
34	11	H(0) = X is the input feature matrix, W (l) is a trainable weight matrix, and σ(·) is an activation function.
36	2	The training loss is defined as L = 1 |VL| ∑ v∈VL f(yv, z (L) v ), (2) where f(·, ·) is a loss function.
37	5	A graph convolution layer propagates information to nodes from their neighbors by computing the neighbor averaging PH(l).
39	40	The neighbor averaging of node u, (PH(l))u =∑V v=1 Puvh (l) v = ∑ v∈n(u) Puvh (l) v , is a weighted sum of neighbors’ activations.
40	21	Then, a fully-connected layer is applied on all the nodes, with a shared weight matrix W (l) across all the nodes.
41	25	We denote the receptive field of a node u as all the activations h(l)v on layer l needed for computing z (L) u .
42	5	If the layer l is not explicitly mentioned, it is the input layer 0.
43	9	Intuitively, the receptive field of node u is just all its L-hop neighbors, i.e., nodes that are reachable from u within L hops, as illustrated in Fig.
49	6	For instance, as shown in Table 1, the number of 2-hop neighbors on the NELL dataset is averagely 1,597, which means in a 2-layer GCN, computing the gradient even for a single node needs 1, 597/65, 755 ≈ 2.4% nodes of the entire graph.
53	10	NS randomly chooses D(l) neighbors for each node at layer l and develops an estimator NS(l)u of (PH (l))u based on Monte-Carlo approximation: (PH(l))u ≈ NS(l)u := n(u) D(l) ∑ v∈n̂(l)(u) Puvh (l) v , where n̂(l)(u) ⊂ n(u) is a subset of D(l) random neighbors.
55	5	We refer NS(l)u as the NS estimator of (PH (l))u, and (PH(l))u itself as the exact estimator.
59	7	The approximated gradient has two sources of randomness: the random selection of minibatch VB ⊂ VL, and the random selection of neighbors.
60	21	Though P̂ (l) is an unbiased estimator of P , σ(P̂ (l)H(l)W (l)) is not an unbiased estimator of σ(PH(l)W (l)), due to the non-linearity of σ(·).
61	16	In the sequel, both the prediction Z(L) and gradient ∇f(yv, z(L)v ) obtained by NS are biased, and the convergence of SGD is not guaranteed, unless the sample size D(l) goes to infinity.
63	12	Hamilton et al. (2017a) choose D(1) = 10 and D(2) = 25, and the receptive field size D(1) ×D(2) = 250 is much larger than one, so the training is still expensive.
