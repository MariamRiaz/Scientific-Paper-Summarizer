4	21	In this paper, we are interested in a fundamental problem in NLP, namely named entity recognition (NER) and mention detection (MD).
6	19	NER and MD are tasks of identifying entities (named and/or nominal) from raw text, and classifying the detected entities into one of the pre-defined categories such as person (PER), organization (ORG), location (LOC), etc.
9	34	For example, [Sue]PER and her [brother]PER N studied in [University of [Toronto]LOC ]ORG.
78	24	It encodes any sequence of variable length composed by words in V .
80	19	The FOFE of each partial sequence zt from the first word to the t-th word is recursively defined as: zt = { 0, if t = 0 α · zt−1 + et, otherwise (1) where the constant α is called forgetting factor, and it is picked between 0 and 1 exclusively.
81	18	Obviously, the size of zt is |V |, and it is irrelevant to the length of original sequence, T .
86	12	If the forgetting factor α satisfies 0 < α ≤ 0.5, FOFE is unique for any countable vocabulary V and any finite value T .
87	23	For 0.5 < α < 1, given any finite value T and any countable vocabulary V , FOFE is almost unique everywhere, except only a finite set of countable choices of α.
95	50	This matrix can be viewed as a single-channel image.
98	29	Any word, phrase or fragment can be viewed as a sequence of characters.
103	62	As described above, our FOFE-based local detection approach for NER, called FOFE-NER hereafter, is motivated by the way how human actually infers whether a word segment in text is an entity or mention, where the entity types of the other entities in the same sentence is not a must.
105	38	Whether a fragment is an entity or not, and what class it may belong to, largely depend on the internal structure of the fragment itself as well as the left and right contexts in which it appears.
107	17	Contexts play a very important role in NER or MD when it involves multi-sense words/phrases or out-of-vocabulary (OOV) words.
109	21	For each fragment, it uses the FOFE method to fully encode the underlying fragment itself, its left context and right context into some fixed-size representations, which are in turn fed to an FFNN to predict whether the current fragment is NOT a valid entity mention (NONE), or its correct entity type (PER, LOC, ORG and so on) if it is a valid mention.
110	11	This method is appealing because the FOFE codes serves as a theoretically lossless representation of the hypothesis and its full contexts.
121	26	(1), all of the above FOFE codes can be jointly computed for one sentence or document in a very efficient manner.
133	21	• Partial-overlap with an entity label, e.g., “for the Toronto”.
135	30	For all exact-matched fragments, we generate the corresponding outputs based on the types of the matched entities in the training set.
156	17	For example, for a sentence of “w1 w2 w3 w4 w5”, if the model first generates the prediction results after the global pruning, as [“w2w3”, PER, 0.7], [“w3w4”, LOC, 0.8], [“w1w2w3w4”, ORG, 0.9], if we choose to run highest-first, it will generate the first entity label as [“w1w2w3w4”, ORG, 0.9].
183	43	As we can see from Table 2, our system (highest-first decoding) yields very strong performance (90.85 in F1 score) in this task, outperforming most of neural network models reported on this dataset.
184	35	More importantly, we have not used any hand-crafted features in our systems, and all features (either word or char level) are automatically derived from the data.
187	24	Given a document collection in three languages (English, Chinese and Spanish), the KBP2015 trilingual EDL task (Ji et al., 2015) requires to automatically identify entities (including nested entities) from a source collection of textual documents in multiple languages as in Table 3, and classify them into one of the following pre-defined five types: Person (PER), Geo-political Entity (GPE), Organization (ORG), Location (LOC) and Facility (FAC).
191	18	Three sets of word embeddings of 128 dimensions are derived from English Gigaword (Parker et al., 2011), Chinese Gigaword (Graff and Chen, 2005) and Spanish Gigaword (Mendonca et al., 2009) respectively.
214	71	After adding the weakly labeled data, WIKI, we can see the entity discovery performance is improved to 0.718 in F1 score.
215	67	Moreover, we can see that it yields even better performance by using the KBP2015 data and the in-house data sets to train our models, giving 0.750 in F1 score.
216	14	The official best results of our system are summarized in Table 5.
218	59	Our system, achieving 0.718 in F1 score in the KBP2016 trilingual EDL track, ranks second among all participants.
219	23	Note that our result is produced by a single system while the top system is a combination of two different models, each of which is based on 5-fold cross-validation (Liu et al., 2016).
220	14	In this paper, we propose a novel solution to NER and MD by applying FFNN on top of FOFE features.
221	57	This simple local-detection based approach has achieved almost state-of-the-art performance on various NER and MD tasks, without using any external knowledge or feature engineering.
