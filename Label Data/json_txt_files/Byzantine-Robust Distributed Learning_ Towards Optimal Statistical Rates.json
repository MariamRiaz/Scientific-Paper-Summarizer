{"sections": [{"heading": "1. Introduction", "text": "Many tasks in computer vision, natural language processing and recommendation systems require learning complex prediction rules from large datasets. As the scale of the datasets in these learning tasks continues to grow, it is crucial to utilize the power of distributed computing and storage. In such large-scale distributed systems, robustness and security issues have become a major concern. In particular, individual computing units\u2014known as worker machines\u2014may exhibit abnormal behavior due to crashes, faulty hardware, stalled computation or unreliable communication channels. Security issues are only exacerbated in the so-called Federated Learning setting, a modern distributed learning paradigm that is more decentralized, and that uses the data owners\u2019 devices (such as mobile phones and personal computers)\n1Department of EECS, UC Berkeley 2School of ORIE, Cornell University 3Department of Statistics, UC Berkeley. Correspondence to: Dong Yin <dongyin@berkeley.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nas worker machines (McMahan & Ramage, 2017; Konec\u030cny\u0300 et al., 2016). Such machines are often more unpredictable, and in particular may be susceptible to malicious and coordinated attacks.\nDue to the inherent unpredictability of this abnormal (sometimes adversarial) behavior, it is typically modeled as Byzantine failure (Lamport et al., 1982), meaning that some worker machines may behave completely arbitrarily and can send any message to the master machine that maintains and updates an estimate of the parameter vector to be learned. Byzantine failures can incur major degradation in learning performance. It is well-known that standard learning algorithms based on naive aggregation of the workers\u2019 messages can be arbitrarily skewed by a single Byzantine-faulty machine. Even when the messages from Byzantine machines take only moderate values\u2014hence difficult to detect\u2014and when the number of such machines is small, the performance loss can still be significant. We demonstrate such an example in our experiments in Section 7.\nIn this paper, we aim to develop distributed statistical learning algorithms that are provably robust against Byzantine failures. While this objective is considered in a few recent works (Feng et al., 2014; Blanchard et al., 2017; Chen et al., 2017), a fundamental problem remains poorly understood, namely the optimal statistical performance of a robust learning algorithm. A learning scheme in which the master machine always outputs zero regardless of the workers\u2019 messages, is certainly not affected by Byzantine failures, but it will not return anything statistically useful either. On the other hand, many standard distributed algorithms that achieve good statistical performance in the absence of Byzantine failures, become completely unreliable otherwise. Therefore, a main goal of this work is to understand the following questions: what is the best achievable statistical performance while being Byzantine-robust, and how to design an algorithm that achieves such performance?\nTo formalize this question, we consider a standard statistical setting of empirical risk minimization (ERM). Here nm data points are sampled independently from some distribution and distributed evenly among m machines, \u03b1m of which are Byzantine. The goal is to learn a parametric model by minimizing some loss function defined by the data. In this statistical setting, one expects that the error in learning\nthe parameter, measured in an appropriate metric, should decrease when the amount of data nm becomes larger and the fraction of Byzantine machines \u03b1 becomes smaller. In fact, we can show that, at least for strongly convex problems, no algorithm can achieve an error lower than\n\u2126\u0303 ( \u03b1\u221a n + 1\u221a nm ) = \u2126\u0303 ( 1\u221a n ( \u03b1+ 1\u221a m )) ,\nregardless of communication costs;1 see Observation 1 in Section 6. Intuitively, the above error rate is the optimal rate that one should target for, as 1\u221a\nn is the effective standard\ndeviation for each machine with n data points, \u03b1 is the bias effect of Byzantine machines, and 1\u221a\nm is the averaging\neffect of m normal machines. When there are no or few Byzantine machines, we see the usual scaling 1\u221a\nmn with\nthe total number of data points; when some machines are Byzantine, their influence remains bounded, and moreover is proportional to \u03b1. If an algorithm is guaranteed to attain this bound, we are assured that we do not sacrifice the quality of learning when trying to guard against Byzantine failures\u2014we pay a price that is unavoidable, but otherwise achieve the best possible statistical accuracy in the presence of Byzantine failures.\nAnother important consideration for us is communication efficiency. As communication between machines is costly, one cannot simply send all data to the master machine. This constraint precludes direct application of standard robust learning algorithms (such as M-estimators (Huber, 2011)), which assume access to all data. Instead, a desirable algorithm should involve a small number of communication rounds as well as a small amount of data communicated per round. We consider a setting where in each round a worker or master machine can only communicate a vector of size O(d), where d is the dimension of the parameter to be learned. In this case, the total communication cost is proportional to the number of communication rounds.\nTo summarize, we aim to develop distributed learning algorithms that simultaneously achieve two objectives:\n\u2022 Statistical optimality: attain an O\u0303( \u03b1\u221a n + 1\u221a nm ) rate.\n\u2022 Communication efficiency: O(d) communication per round, with as few rounds as possible.\nTo the best of our knowledge, no existing algorithm achieves these two goals simultaneously. In particular, previous robust algorithms either have unclear or sub-optimal statistical guarantees, or incur a high communication cost and hence not applicable in a distributed setting\u2014we discuss related work in more detail in Section 2.\n1Throughout the paper, unless otherwise stated, \u2126(\u00b7) and O(\u00b7) hide universal multiplicative constants; \u2126\u0303(\u00b7) and O\u0303(\u00b7) further hide terms that are independent of \u03b1, n,m or logarithmic in n,m.\nOur Contributions We propose two robust distributed gradient descent (GD) algorithms, one based on coordinatewise median, and the other on coordinate-wise trimmed mean. We establish their statistical error rates for strongly convex, non-strongly convex, and non-convex population loss functions. Particularly for strongly convex losses, we show that these algorithms achieve order-optimal statistical rates under mild conditions. We further propose a medianbased robust algorithm that only requires one communication round, and show that it also achieves the optimal rate for strongly convex quadratic losses. The statistical error rates of these three algorithms are summarized as follows.\n\u2022 Median-based GD: O\u0303( \u03b1\u221a n + 1\u221a nm + 1n ), order-optimal for strongly convex loss if n & m.\n\u2022 Trimmed-mean-based GD: O\u0303( \u03b1\u221a n + 1\u221a nm ), orderoptimal for strongly convex loss.\n\u2022 Median-based one-round algorithm: O\u0303( \u03b1\u221a n + 1\u221a nm\n+ 1 n ), order-optimal for strongly convex quadratic loss if n & m.\nA major technical challenge in our statistical setting here is as follows: the nm data points are sampled once and fixed, and each worker machine has access to the same set of data throughout learning process. This creates complicated probabilistic dependency across the iterations of the algorithms. Worse yet, Byzantine machines may create further unspecified dependency. We overcome this difficulty by proving certain uniform bounds via careful covering arguments. Furthermore, for the analysis of median-based algorithms, we cannot simply adapt standard techniques (such as those in Minsker et al. (2015)), which can only show that the output of the master machine is as accurate as that of one normal machine, leading to a sub-optimal O( 1\u221a\nn ) rate even without Byzantine failures. Instead, we make use of a more delicate argument based on normal approximation and Berry-Esseen-type inequalities."}, {"heading": "2. Related Work", "text": "Outlier-robust estimation in non-distributed settings is a classical topic in statistics (Huber, 2011). Particularly relevant to us is the so-called median-of-means method, in which one partitions the data m subsets, computes an estimate from each sub-dataset, and finally takes the median of these m estimates. This idea is studied in Nemirovskii et al. (1983); Jerrum et al. (1986); Alon et al. (1999); Lerasle & Oliveira (2011); Minsker et al. (2015), and has been applied to bandit and least square regression problems (Bubeck et al., 2013; Lugosi & Mendelson, 2016; Kogler & Traxler, 2016) as well as problems involving heavy-tailed distributions (Hsu & Sabato, 2016; Lugosi & Mendelson, 2017). In a very recent work, Minsker & Strawn (2017) provide new analysis of median-of-means using normal approximation. We borrow some techniques from this paper, but need to address\na significant harder problem: 1) we deal with the Byzantine setting with arbitrary/adversarial outliers, which is not considered in their paper; 2) we study iterative algorithms for general multi-dimensional problems with convex and non-convex losses, while they mainly focus on one-shot algorithms for mean-estimation-type problems.\nThe median-of-means method is used in the context of Byzantine-robust distributed learning in two recent papers. In particular, the work of Feng et al. (2014) considers a simple one-shot application of median-of-means, and only proves a sub-optimal O\u0303( 1\u221a\nn ) error rate as mentioned. The\nwork of Chen et al. (2017) considers only strongly convex losses, and seeks to circumvent the above issue by grouping the worker machines into mini-batches; however, their rate O\u0303( \u221a \u03b1\u221a n + 1\u221a nm\n) still falls short of being optimal, and in particular their algorithm fails even when there is only one Byzantine machine in each mini-batch.\nOther methods have been proposed for Byzantine-robust distributed learning and optimization; e.g., Su & Vaidya (2016a;b). These works consider optimizing fixed functions and do not provide guarantees on statistical error rates. Most relevant is the work by Blanchard et al. (2017), who propose to aggregate the gradients from worker machines using a robust procedure. Their optimization setting\u2014which is at the level of stochastic gradient descent and assumes unlimited, independent access to a strong stochastic gradient oracle\u2014is fundamentally different from ours; in particular, they do not provide a characterization of the statistical errors given a fixed number of data points.\nCommunication efficiency has been studied extensively in non-Byzantine distributed settings (McMahan et al., 2016; Yin et al., 2017). An important class of algorithms are based on one-round aggregation methods (Zhang et al., 2012; 2015; Rosenblatt & Nadler, 2016). More sophisticated algorithms have been proposed in order to achieve better accuracy than the one-round approach while maintaining lower communication costs; examples include DANE (Shamir et al., 2014), Disco (Zhang & Lin, 2015), distributed SVRG (Lee et al., 2015) and their variants (Reddi et al., 2016; Wang et al., 2017). Developing Byzantine-robust versions of these algorithms is an interesting future direction.\nFor outlier-robust estimation in non-distributed settings, much progress has been made recently in terms of improved performance in high-dimensional problems (Diakonikolas et al., 2016; Lai et al., 2016; Bhatia et al., 2015) as well as developing list-decodable and semi-verified learning schemes when a majority of the data points are adversarial (Charikar et al., 2017). These results are not directly applicable to our distributed setting with general loss functions, but it is nevertheless an interesting future problem to investigate their potential extension for our problem."}, {"heading": "3. Problem Setup", "text": "In this section, we formally set up our problem and introduce a few concepts key to our the algorithm design and analysis. Suppose that training data points are sampled from some unknown distribution D on the sample space Z . Let f(w; z) be a loss function of a parameter vector w \u2208 W \u2286 Rd associated with the data point z, whereW is the parameter space, and F (w) := Ez\u223cD[f(w; z)] is the corresponding population loss function. Our goal is to learn a model defined by the parameter that minimizes the population loss:\nw\u2217 = arg min w\u2208W F (w). (1)\nThe parameter spaceW is assumed to be convex and compact with diameter D, i.e., \u2016w \u2212w\u2032\u20162 \u2264 D,\u2200w,w\u2032 \u2208 W . We consider a distributed computation model with one master machine and m worker machines. Each worker machine stores n data points, each of which is sampled independently from D. Denote by zi,j the j-th data on the i-th worker machine, and Fi(w) := 1n \u2211n j=1 f(w; z\ni,j) the empirical risk function for the i-th worker. We assume that an \u03b1 fraction of the m worker machines are Byzantine, and the remaining 1\u2212 \u03b1 fraction are normal. With the notation [N ] := {1, 2, . . . , N}, we index the set of worker machines by [m], and denote the set of Byzantine machines by B \u2282 [m] (thus |B| = \u03b1m). The master machine communicates with the worker machines using some predefined protocol. The Byzantine machines need not obey this protocol and can send arbitrary messages to the master; in particular, they may have complete knowledge of the system and learning algorithms, and can collude with each other.\nWe introduce the coordinate-wise median and trimmed mean operations, which serve as building blocks for our algorithm. Definition 1 (Coordinate-wise median). For vectors xi \u2208 Rd, i \u2208 [m], the coordinate-wise median g := med{xi : i \u2208 [m]} is a vector with its k-th coordinate being gk = med{xik : i \u2208 [m]} for each k \u2208 [d], where med is the usual (one-dimensional) median. Definition 2 (Coordinate-wise trimmed mean). For \u03b2 \u2208 [0, 12 ) and vectors x\ni \u2208 Rd, i \u2208 [m], the coordinate-wise \u03b2trimmed mean g := trmean\u03b2{xi : i \u2208 [m]} is a vector with its k-th coordinate being gk = 1(1\u22122\u03b2)m \u2211 x\u2208Uk x for each k \u2208 [d]. Here Uk is a subset of {x1k, . . . , xmk } obtained by removing the largest and smallest \u03b2 fraction of its elements. For the analysis, we need several standard definitions concerning random variables/vectors. Definition 3 (Variance of random vectors). For a random vector x, define its variance as Var(x) := E[\u2016x\u2212 E[x]\u201622]. Definition 4 (Absolute skewness). For a one-dimensional random variable X , define its absolute skewness2 as\n2Note the difference with the usual skewness E[(X\u2212E[X]) 3]\nVar(X)3/2 .\n\u03b3(X) := E[|X\u2212E[X]| 3]\nVar(X)3/2 . For a d-dimensional random vec-\ntor x, we define its absolute skewness as the vector of the absolute skewness of each coordinate of x, i.e., \u03b3(x) := [\u03b3(x1) \u03b3(x2) \u00b7 \u00b7 \u00b7 \u03b3(xd)]>. Definition 5 (Sub-exponential random variables). A random variable X with E[X] = \u00b5 is called v-sub-exponential if E[e\u03bb(X\u2212\u00b5)] \u2264 e 12v2\u03bb2 , \u2200 |\u03bb| < 1v . Finally, we need several standard concepts from convex analysis regarding a differentiable function h(\u00b7) : Rd \u2192 R. Definition 6 (Lipschitz). h is L-Lipschitz if |h(w)\u2212 h(w\u2032)| \u2264 L\u2016w \u2212w\u2032\u20162,\u2200 w,w\u2032. Definition 7 (Smoothness). h is L\u2032-smooth if \u2016\u2207h(w)\u2212\u2207h(w\u2032)\u20162 \u2264 L\u2032\u2016w \u2212w\u2032\u20162,\u2200 w,w\u2032. Definition 8 (Strong convexity). h is \u03bb-strongly convex if h(w\u2032) \u2265 h(w)+\u3008\u2207h(w),w\u2032\u2212w\u3009+ \u03bb2 \u2016w \u2032\u2212w\u201622,\u2200w,w\u2032."}, {"heading": "4. Robust Distributed Gradient Descent", "text": "We describe two robust distributed gradient descent algorithms, one based on coordinate-wise median and the other on trimmed mean. These two algorithms are formally given in Algorithm 1 as Option I and Option II, respectively, where the symbol \u2217 represents an arbitrary vector.\nIn each parallel iteration of the algorithms, the master machine broadcasts the current model parameter to all worker machines. The normal worker machines compute the gradients of their local loss functions and then send the gradients back to the master machine. The Byzantine machines may send any messages of their choices. The master machine then performs a gradient descent update on the model parameter with step-size \u03b7, using either the coordinate-wise median or trimmed mean of the received gradients. The Euclidean projection \u03a0W(\u00b7) ensures that the model parameter stays in the parameter spaceW .\nBelow we provide statistical guarantees on the error rates of these algorithms, and compare their performance. Throughout we assume that each loss functions f(w; z) and the population loss function F (w) are smooth:\nAssumption 1 (Smoothness of f and F ). For any z \u2208 Z , the partial derivative of f(\u00b7; z) with respect to the k-th coordinate of its first argument, denoted by \u2202kf(\u00b7; z), is Lk-Lipschitz for each k \u2208 [d], and the function f(\u00b7; z) is L-smooth. Let L\u0302 := ( \u2211d k=1 L 2 k)\n1/2. Also assume that the population loss function F (\u00b7) is LF -smooth.\nIt is easy to see hat LF \u2264 L \u2264 L\u0302. We note that L\u0302 appears because we have coordinate-wise operations and the L\u0302 quantity combines the smoothness parameter of all the d partial derivatives. When the dimension of w is high, the quantity L\u0302 may be large. However, we will soon see that L\u0302 only appears in the logarithmic factors in our bounds and thus does not have a significant impact.\nAlgorithm 1 Robust Distributed Gradient Descent\nRequire: Initialize parameter vector w0 \u2208 W , algorithm parameters \u03b2 (for Option II), \u03b7 and T . for t = 0, 1, 2, . . . , T \u2212 1 do\nMaster machine: send wt to all the worker machines. for i \u2208 [m] in parallel do\nWorker machine i: compute local gradient\ngi(wt)\u2190 { \u2207Fi(wt) normal machines, \u2217 Byzantine machines,\nsend gi(wt) to master machine. end for Master machine: compute aggregate gradient\ng(wt)\u2190 { med{gi(wt) : i \u2208 [m]} Option I trmean\u03b2{gi(wt) : i \u2208 [m]} Option II\nupdate model parameter wt+1 \u2190 \u03a0W(wt \u2212 \u03b7g(wt)). end for\nIn addition, when F (\u00b7) is convex, we assume that w\u2217, the minimizer of F (\u00b7) inW , is also the minimizer of F (\u00b7) in Rd. Formally, we have Assumption 2 (minimizer in W). Suppose that F (w) is convex, and let w\u2217 = arg minw\u2208W F (w). We assume that \u2207F (w\u2217) = 0."}, {"heading": "4.1. Median-based Gradient Descent", "text": "We first consider our median-based algorithm, namely Algorithm 1 with Option I. We impose the assumptions that the gradient of the loss function f has bounded variance, and each coordinate of the gradient has coordinate-wise bounded absolute skewness:\nAssumption 3 (Bounded variance of gradient). For any w \u2208 W , Var(\u2207f(w; z)) \u2264 V 2. Assumption 4 (Bounded skewness of gradient). For any w \u2208 W , \u2016\u03b3(\u2207f(w; z))\u2016\u221e \u2264 S. These assumptions are satisfied in many learning problems with small values of V 2 and S. Below we provide a concrete example in terms of a linear regression problem.\nProposition 1. Suppose that each data point z = (x, y) \u2208 Rd \u00d7 R is generated by y = x>w\u2217 + \u03be with some w\u2217 \u2208 W . Assume that the elements of x are independent and uniformly distributed in {\u22121, 1}, and that the noise \u03be \u223c N (0, \u03c32) is independent of x. With the quadratic loss function f(w;x, y) = 12 (y \u2212 x\nTw)2, we have Var(\u2207f(w;x, y)) = (d\u2212 1)\u2016w \u2212w\u2217\u201622 + d\u03c32, and \u2016\u03b3(\u2207f(w;x, y))\u2016\u221e \u2264 480.\nWe prove Proposition 1 in Appendix A.1. In this example, the upper bound V on Var(\u2207f(w;x, y)) depends on dimension d and the diameter of the parameter space, and if the diameter is a constant, we have V = O( \u221a d). Moreover,\nthe gradient skewness is bounded by a universal constant S regardless of the size of the parameter space.\nWe now state our main technical results on the median-based algorithm, namely statistical error guarantees for strongly convex, non-strongly convex, and smooth non-convex population loss functions F . Strongly Convex Losses: We first consider the case where the population loss function F (\u00b7) is strongly convex. Note that we do not require strong convexity of the individual loss function f(\u00b7; z). Theorem 1. Consider Option I in Algorithm 1. Suppose that Assumptions 1, 2, 3, and 4 hold, F (\u00b7) is \u03bbF -strongly convex, and the fraction \u03b1 of Byzantine machines satisfies\n\u03b1+\n\u221a d log(1 + nmL\u0302D)\nm(1\u2212 \u03b1) + 0.4748 S\u221a n \u2264 1 2 \u2212 (2)\nfor some > 0. Choose step-size \u03b7 = 1/LF . Then, with probability at least 1\u2212 4d\n(1+nmL\u0302D)d , after T parallel itera-\ntions, we have\n\u2016wT \u2212w\u2217\u20162 \u2264 (1\u2212 \u03bbF\nLF + \u03bbF )T \u2016w0 \u2212w\u2217\u20162 +\n2\n\u03bbF \u2206,\nwhere \u2206 := O ( C V ( \u03b1\u221a n +\n\u221a d log(nmL\u0302D)\nnm + S n\n)) , (3)\nand C is defined as\nC := \u221a 2\u03c0 exp (1\n2 (\u03a6\u22121(1\u2212 ))2\n) , (4)\nwith \u03a6\u22121(\u00b7) being the inverse of the cumulative distribution function of the standard Gaussian distribution \u03a6(\u00b7). We prove Theorem 1 in Appendix B. In (3), we hide universal constants and a higher order term that scales as 1nm , and the factor C is a function of ; as a concrete example, C \u2248 4 when = 16 . Theorem 1 together with the inequality log(1\u2212 x) \u2264 \u2212x, guarantees that after running T \u2265 LF +\u03bbF\u03bbF log( \u03bbF 2\u2206\u2016w\n0 \u2212w\u2217\u20162) parallel iterations, with high probability we can obtain a solution w\u0302 = wT with error \u2016w\u0302 \u2212w\u2217\u20162 \u2264 4\u03bbF \u2206.\nHere we achieve an the error rate (defined as the distance between w\u0302 and the optimal solution w\u2217) of the form O\u0303( \u03b1\u221a\nn + 1\u221a nm + 1n ). In Section 6, we provide a\nlower bound showing that the error rate of any algorithm is \u2126\u0303( \u03b1\u221a\nn + 1\u221a nm ). Therefore the first two terms in the upper bound cannot be improved. The third term 1n is due to the dependence of median on the skewness of the gradients. When each worker machine has a sufficient amount of data, more specifically n & m, we achieve an order-optimal error rate up to logarithmic factors.\nNon-strongly Convex Losses: We next consider the case where the population risk function F (\u00b7) is convex, but not necessarily strongly convex. In this case, we need a mild technical assumption on the size of the parameter spaceW .\nAssumption 5 (Size ofW). The parameter spaceW contains the following `2 ball centered at w\u2217: {w \u2208 Rd : \u2016w \u2212w\u2217\u20162 \u2264 2\u2016w0 \u2212w\u2217\u20162}.\nThis assumption (and Assumption 6 below) ensures that the iterates wt always stay inW without projection. Doing so streamlines our analysis, as our main focus is on robustness. We then have the following result on the convergence rate in terms of the value of the population risk function.\nTheorem 2. Consider Option I in Algorithm 1. Suppose that Assumptions 1, 2, 3, 4 and 5 hold, and that the population loss F (\u00b7) is convex, and \u03b1 satisfies (2) for some > 0. Define \u2206 as in (3), and choose step-size \u03b7 = 1/LF . Then, with probability at least 1\u2212 4d(1+nmL\u0302D)d , after T = LF\u2206 \u2016w 0 \u2212w\u2217\u20162 parallel iterations, we have\nF (wT )\u2212 F (w\u2217) \u2264 16\u2016w0 \u2212w\u2217\u20162\u2206 ( 1 + 1 2LF \u2206 ) .\nWe prove Theorem 2 in Appendix C. We observe that the error rate, defined as the excess risk F (wT )\u2212F (w\u2217), again has the form O\u0303 ( \u03b1\u221a n + 1\u221a nm + 1n ) .\nNon-convex Losses: When F (\u00b7) is non-convex but smooth, we need a somewhat different technical assumption on the size ofW . Assumption 6 (Size of W). Suppose that \u2200 w \u2208 W , \u2016\u2207F (w)\u20162 \u2264M . We assume thatW contains the `2 ball {w \u2208 Rd : \u2016w\u2212w0\u20162 \u2264 2\u22062 (M+\u2206)(F (w\n0)\u2212F (w\u2217))}, where \u2206 is defined as in (3).\nWe have the following guarantees on the rate of convergence to a critical point of the population loss F (\u00b7). Theorem 3. Consider Option I in Algorithm 1. Suppose that Assumptions 1 3, 4 and 6 hold, and \u03b1 satisfies (2) for some > 0. Define \u2206 as in (3), and choose step-size \u03b7 = 1/LF . With probability at least 1\u2212 4d(1+nmL\u0302D)d , after T = 2LF\u22062 (F (w 0)\u2212 F (w\u2217)) parallel iterations, we have\nmin t=0,1,...,T\n\u2016\u2207F (wt)\u20162 \u2264 \u221a 2\u2206.\nWe prove Theorem 3 in Appendix D. We again obtain an O\u0303( \u03b1\u221a\nn + 1\u221a nm + 1n ) error rate in terms of the gap to a critical\npoint of F (w)."}, {"heading": "4.2. Trimmed-mean-based Gradient Descent", "text": "We next analyze the robust distributed gradient descent algorithm based on coordinate-wise trimmed mean, namely Option II in Algorithm 1. Here we need stronger assumptions on the tail behavior of the partial derivatives of the loss functions\u2014in particular, sub-exponentiality.\nAssumption 7 (Sub-exponential gradients). We assume that for all k \u2208 [d] and w \u2208 W , the partial derivative of f(w; z) with respect to the k-th coordinate of w, \u2202kf(w; z), is vsub-exponential.\nThe sub-exponential property implies that all the moments of the derivatives are bounded. This is a stronger assumption than the bounded absolute skewness (hence bounded third moments) required by the median-based GD algorithm.\nWe use the same example as in Proposition 1 and show that the derivatives of the loss are indeed sub-exponential.\nProposition 2. Consider the regression problem in Proposition 1. For all k \u2208 [d] and w \u2208 W , the partial derivative \u2202kf(w; z) is \u221a \u03c32 + \u2016w \u2212w\u2217\u201622-sub-exponential.\nProposition 2 is proved in Appendix A.3. We now proceed to establish the statistical guarantees of the trimmed-meanbased algorithm, for different loss function classes. When the population loss F (\u00b7) is convex, we again assume that the minimizer of F (\u00b7) inW is also its minimizer in Rd. The next three theorems are analogues of Theorems 1\u20133 for the median-based GD algorithm.\nStrongly Convex Losses: We have the following result. Theorem 4. Consider Option II in Algorithm 1. Suppose that Assumptions 1, 2, and 7 hold, F (\u00b7) is \u03bbF -strongly convex, and \u03b1 \u2264 \u03b2 \u2264 12 \u2212 for some > 0. Choose step-size \u03b7 = 1/LF . Then, with probability at least 1\u2212 4d(1+nmL\u0302D)d , after T parallel iterations, we have\n\u2016wT \u2212w\u2217\u20162 \u2264 (\n1\u2212 \u03bbF LF + \u03bbF\n)T \u2016w0 \u2212w\u2217\u20162 + 2\n\u03bbF \u2206\u2032,\nwhere \u2206\u2032 := O (vd ( \u03b2\u221a n + 1\u221a nm )\u221a log(nmL\u0302D) ) . (5)\nWe prove Theorem 4 in Appendix E. In (5), we hide universal constants and higher order terms that scale as \u03b2n or\n1 nm . By running T \u2265 LF +\u03bbF \u03bbF log( \u03bbF2\u2206\u2032 \u2016w 0 \u2212w\u2217\u20162) parallel iterations, we can obtain a solution w\u0302 = wT satisfying \u2016w\u0302 \u2212 w\u2217\u20162 \u2264 O\u0303( \u03b2\u221an + 1\u221a nm\n). Note that one needs to choose the parameter for trimmed mean to satisfy \u03b2 \u2265 \u03b1. If we set \u03b2 = c\u03b1 for some universal constant c \u2265 1, we can achieve an order-optimal error rate O\u0303( \u03b1\u221a\nn + 1\u221a nm ).\nNon-strongly Convex Losses: Again imposing Assumption 5 on the size ofW , we have the following guarantee. Theorem 5. Consider Option II in Algorithm 1. Suppose that Assumptions 1, 2, 5 and 7 hold, F (\u00b7) is convex, and \u03b1 \u2264 \u03b2 \u2264 12 \u2212 for some > 0. Choose step-size \u03b7 = 1/LF , and define \u2206\u2032 as in (5). Then, with probability at least 1\u2212 4d\n(1+nmL\u0302D)d , after T = LF\u2206\u2032 \u2016w 0 \u2212w\u2217\u20162 parallel iterations, we have\nF (wT )\u2212 F (w\u2217) \u2264 16\u2016w0 \u2212w\u2217\u20162\u2206\u2032 ( 1 + 1 2LF \u2206\u2032 ) .\nThe proof of Theorem 5 is similar to that of Theorem 2, and we refer readers to Remark 1 in Appendix E. Again, by choosing \u03b2 = c\u03b1 (c \u2265 1), we obtain the O\u0303( \u03b1\u221a\nn + 1\u221a nm )\nerror rate in the function value of F (w).\nNon-convex Losses: In this case, imposing a version of Assumption 6 on the size ofW , we have the following. Theorem 6. Consider Option II in Algorithm 1, and define \u2206\u2032 as in (5). Suppose that Assumptions 1 and 7 hold, Assumption 6 holds with \u2206 replaced by \u2206\u2032, and \u03b1 \u2264 \u03b2 \u2264 12 \u2212 for some > 0. Choose step-size \u03b7 = 1/LF . Then, with probability at least 1\u2212 4d(1+nmL\u0302D)d , after T = 2LF\u2206\u20322 (F (w\n0) \u2212 F (w\u2217)) parallel iterations, we have\nmin t=0,1,...,T\n\u2016\u2207F (wt)\u20162 \u2264 \u221a 2\u2206\u2032.\nThe proof of Theorem 6 is similar to that of Theorem 3; see Remark 1 in Appendix E. By choosing \u03b2 = c\u03b1 with c \u2265 1, we again achieve the statistical rate O\u0303( \u03b1\u221a\nn + 1\u221a nm )."}, {"heading": "4.3. Comparisons", "text": "We compare the performance guarantees of the above two robust distribute GD algorithms. The trimmed-mean-based algorithm achieves the statistical error rate O\u0303( \u03b1\u221a\nn + 1\u221a nm ),\nwhich is order-optimal for strongly convex loss. In comparison, the rate of the median-based algorithm is O\u0303( \u03b1\u221a\nn +\n1\u221a nm + 1n ), which has an additional 1 n term and is only optimal when n & m. In particular, the trimmed-meanbased algorithm has better rates when each worker machine has small local sample size\u2014the rates are meaningful even in the extreme case n = O(1). On the other hand, the median-based algorithm requires milder tail/moment assumptions on the loss derivatives (bounded skewness) than its trimmed-mean counterpart (sub-exponentiality). Finally, the trimmed-mean operation requires an additional parameter \u03b2, which can be any upper bound on the fraction \u03b1 of Byzantine machines in order to guarantee robustness. Using an overly large \u03b2 may lead to a looser bound and sub-optimal performance. In contrast, median-based GD does not require knowledge of \u03b1. We summarize these observations in Table 1. We see that the two algorithms are complementary to each other, and our experiment results corroborate this point."}, {"heading": "5. Robust One-round Algorithm", "text": "As mentioned, in our distributed computing framework, the communication cost is proportional to the number of parallel iterations. The above two GD algorithms both require a number iterations depending on the desired accuracy. Can we further reduce the communication cost while keeping the algorithm Byzantine-robust and statistically optimal?\nA natural candidate is the so-called one-round algorithm. Previous work has considered a standard one-round scheme where each local machine computes the empirical risk minimizer (ERM) using its local data and the master machine receives all workers\u2019 ERMs and computes their average (Zhang et al., 2012). Clearly, a single Byzantine machine can arbitrary skew the output of this algorithm. We instead consider a Byzantine-robust one-round algorithm. As detailed in Algorithm 2, we employ the coordinate-wise median operation to aggregate all the ERMs.\nAlgorithm 2 Robust One-round Algorithm for i \u2208 [m] in parallel do\nWorker machine i: compute & send to master machine:\nw\u0302i \u2190 { arg minw\u2208W Fi(w) normal machines \u2217 Byzantine machines\nend for Master machine: compute w\u0302\u2190 med{w\u0302i : i \u2208 [m]}.\nOur main result is a characterization of the error rate of Algorithm 2 in the presence of Byzantine failures. We are only able to establish such a guarantee when the loss functions are quadratic andW = Rd. However, one can implement this algorithm in problems with other loss functions.\nDefinition 9 (Quadratic loss function). The loss function f(w; z) is quadratic if it can be written as\nf(w; z) = 1\n2 wTHw + pTw + c,\nwhere z = (H,p, c), H, and p, and c are drawn from the distributions DH , Dp, and Dc, respectively. Denote by HF , pF , and cF the expectations of H, p, and c, respectively. Thus the population risk function takes the form F (w) = 12w THFw + p T Fw + cF .\nWe need a technical assumption which guarantees that each normal worker machine has unique ERM.\nAssumption 8 (Strong convexity of Fi). With probability 1, the empirical risk minimization function Fi(\u00b7) on each normal machine is strongly convex.\nNote that this assumption is imposed on Fi(w), rather than on the individual loss f(w; z) associated with a single data point. This assumption is satisfied, for example, when all f(\u00b7; z)\u2019s are strongly convex, or in the linear regression problems with the features x drawn from some continuous distribution (e.g. isotropic Gaussian) and n \u2265 d. We have the following guarantee for the robust one-round algorithm.\nTheorem 7. Suppose that \u2200 z \u2208 Z , the loss function f(\u00b7; z) is convex and quadratic, F (\u00b7) is \u03bbF -strongly convex, and Assumption 8 holds. Assume that \u03b1 satisfies\n\u03b1+\n\u221a log(nmd)\n2m(1\u2212 \u03b1) + C\u0303\u221a n \u2264 1 2 \u2212\nfor some > 0, where C\u0303 is a quantity that depends on DH , Dp, \u03bbF and is monotonically decreasing in n. Then, with probability at least 1\u2212 4nm , the output w\u0302 of the robust one-round algorithm satisfies\n\u2016w\u0302 \u2212w\u2217\u20162 \u2264 C \u221a n \u03c3\u0303 ( \u03b1+\n\u221a log(nmd)\n2m(1\u2212 \u03b1) + C\u0303\u221a n\n) ,\nwhere C is defined as in (4) and \u03c3\u03032 := E [ \u2016H\u22121F ( (H\u2212HF )H\u22121F pF \u2212 (p\u2212 pF ) ) \u201622 ] ,\nwith H and p drawn from DH and Dp, respectively. We prove Theorem 7 and provide an explicit expression of C\u0303 in Appendix F. In terms of the dependence on \u03b1, n, and m, the robust one-round algorithm achieves the same error rate as the robust gradient descent algorithm based on coordinate-wise median, i.e., O\u0303( \u03b1\u221a\nn + 1\u221a nm + 1n ), for\nquadratic problems. Again, this rate is optimal when n & m. Therefore, at least for quadratic loss functions, the robust one-round algorithm has similar theoretical performance as the robust gradient descent algorithm with significantly less communication cost. Our experiments show that the one-round algorithm has good empirical performance for other losses as well."}, {"heading": "6. Lower Bound", "text": "In this section, we provide a lower bound on the error rate for strongly convex losses, which implies that the \u03b1\u221a\nn + 1\u221a nm\nterm is unimprovable. This lower bound is derived using a mean estimation problem, and is an extension of the lower bounds in the robust mean estimation literature such as Chen et al. (2015); Lai et al. (2016).\nWe consider the problem of estimating the mean \u00b5 of some random variable z \u223c D, which is equivalent to solving the following minimization problem:\n\u00b5 = arg min w\u2208W\nEz\u223cD[\u2016w \u2212 z\u201622], (6)\nNote that this is a special case of the general learning problem (1). We consider the same distributed setting as in Section 4, with a minor technical difference regarding the Byzantine machines. We assume that each of the m worker machines is Byzantine with probability \u03b1, independently of each other. The parameter \u03b1 is therefore the expected fraction of Byzantine machines. In this setting we have the lower bound in Observation 1. In Appendix G, we also discuss how we can translate this average-case bound to a lower bound holds under the setting of our main theorems, that is, an unknown set of \u03b1m Byzantine machines are selected without any assumption.\nObservation 1. Consider the distributed mean estimation problem in (6) with Byzantine failure probability \u03b1, and suppose that Z is Gaussian distribution with mean \u00b5 and covariance matrix \u03c32I (\u03c3 = O(1)). Then, any algorithm\nthat computes an estimation \u00b5\u0302 of the mean from the data has a constant probability of error \u2016\u00b5\u0302\u2212\u00b5\u20162 = \u2126( \u03b1\u221an + \u221a d nm ).\nWe prove Observation 1 in Appendix G. According to this observation, we see that the \u03b1\u221a\nn + 1\u221a nm dependence cannot\nbe avoided, which in turn implies the order-optimality of the results in Theorem 1 (when n & m) and Theorem 4."}, {"heading": "7. Experiments", "text": "We conduct experiments to show the effectiveness of the median and trimmed mean operations. Our experiments are implemented with Tensorflow (Abadi et al., 2016) on Microsoft Azure system. We use the MNIST (LeCun et al., 1998) dataset and randomly partition the 60,000 training data into m subsamples with equal sizes. We use these subsamples to represent the data on m machines.\nIn the first experiment, we compare the performance of distributed gradient descent algorithms in the following 4 settings: 1) \u03b1 = 0 (no Byzantine machines), using vanilla distributed gradient descent (aggregating the gradients by taking the mean), 2) \u03b1 > 0, using vanilla distributed gradient descent, 3) \u03b1 > 0, using median-based algorithm, and 4) \u03b1 > 0, using trimmed-mean-based algorithm. We generate the Byzantine machines in the following way: we replace every training label y on these machines with 9\u2212y, e.g., 0 is replaced with 9, 1 is replaced with 8, etc, and the Byzantine machines simply compute gradients based on these data. We also note that when generating the Byzantine machines, we do not simply add extreme values in the features or gradients; instead, the Byzantine machines send messages to the master machine with moderate values.\nWe train a multi-class logistic regression model and a convolutional neural network (CNN) using distributed gradient descent, and for each model, we compare the test accuracies in the aforementioned 4 settings. For the convolutional neural network model, we use the stochastic version of the distributed gradient descent algorithm; more specifically, in every iteration, each worker machine computes the gradient using 10% of its local data. We plot the test error as a function of the number of parallel iterations (i.e., communication rounds) in Figure 1. The final test accuracies are presented in Tables 2 and 3.\nAs we can see, in the adversarial settings, the vanilla distributed gradient descent algorithm suffers from severe performance loss, and using the median and trimmed mean operations, we observe significant improvement in test accuracy. This shows these two operations indeed have strong ability in defense against Byzantine failures.\nIn the second experiment, we compare the performance of distributed one-round algorithms in the following 3 settings: 1) \u03b1 = 0, mean aggregation, 2) \u03b1 > 0, mean aggregation, and 3) \u03b1 > 0, median aggregation. To show that our algorithm is able to defend against different types of adversarial behavior, we generate the Byzantine machines differently from the first experiment\u2014the training labels are i.i.d. uniformly sampled from {0, . . . , 9}, and these machines train models using the faulty data. We choose the multi-class logistic regression model, and the test accuracies are presented in Table 4.\nAs we can see, for the one-round algorithm, although the theoretical guarantee is only proved for quadratic loss, in practice, the median-based one-round algorithm still improves the test accuracy in problems with other loss functions, such as the logistic loss here."}, {"heading": "Acknowledgements", "text": "We gratefully acknowledge the support of the NSF through grant IIS-1619362, CIF award 1703678, CRII award 1657420 and grant 1704828. We also acknowledge the support of Berkeley DeepDrive Industry Consortium and Gift award from Huawei. Cloud computing resources are provided by a Microsoft Azure for Research award."}], "year": 2018, "references": [{"title": "Tensorflow: A system for large-scale machine learning", "authors": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M Isard"], "venue": "In OSDI,", "year": 2016}, {"title": "The space complexity of approximating the frequency moments", "authors": ["N. Alon", "Y. Matias", "M. Szegedy"], "venue": "Journal of Computer and system sciences,", "year": 1999}, {"title": "Robust regression via hard thresholding", "authors": ["K. Bhatia", "P. Jain", "P. Kar"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Bandits with heavy tail", "authors": ["S. Bubeck", "N. Cesa-Bianchi", "G. Lugosi"], "venue": "IEEE Transactions on Information Theory,", "year": 2013}, {"title": "Learning from untrusted data", "authors": ["M. Charikar", "J. Steinhardt", "G. Valiant"], "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,", "year": 2017}, {"title": "Robust covariance matrix estimation via matrix depth", "authors": ["M. Chen", "C. Gao", "Z. Ren"], "venue": "arXiv preprint arXiv:1506.00691,", "year": 2015}, {"title": "Distributed statistical machine learning in adversarial settings: Byzantine gradient descent", "authors": ["Y. Chen", "L. Su", "J. Xu"], "venue": "arXiv preprint arXiv:1705.05491,", "year": 2017}, {"title": "Robust estimators in high dimensions without the computational intractability", "authors": ["I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2016}, {"title": "Distributed robust learning", "authors": ["J. Feng", "H. Xu", "S. Mannor"], "venue": "arXiv preprint arXiv:1409.5937,", "year": 2014}, {"title": "Loss minimization and parameter estimation with heavy tails", "authors": ["D. Hsu", "S. Sabato"], "venue": "The Journal of Machine Learning Research,", "year": 2016}, {"title": "Robust statistics", "authors": ["P.J. Huber"], "venue": "In International Encyclopedia of Statistical Science,", "year": 2011}, {"title": "Random generation of combinatorial structures from a uniform distribution", "authors": ["M.R. Jerrum", "L.G. Valiant", "V.V. Vazirani"], "venue": "Theoretical Computer Science,", "year": 1986}, {"title": "Efficient and robust median-ofmeans algorithms for location and regression. In Symbolic and Numeric Algorithms for Scientific Computing (SYNASC), 2016", "authors": ["A. Kogler", "P. Traxler"], "venue": "18th International Symposium on,", "year": 2016}, {"title": "Federated learning: Strategies for improving communication efficiency", "authors": ["J. Kone\u010dn\u1ef3", "H.B. McMahan", "F.X. Yu", "P. Richt\u00e1rik", "A.T. Suresh", "D. Bacon"], "venue": "arXiv preprint arXiv:1610.05492,", "year": 2016}, {"title": "Agnostic estimation of mean and covariance", "authors": ["K.A. Lai", "A.B. Rao", "S. Vempala"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2016}, {"title": "The byzantine generals problem", "authors": ["L. Lamport", "R. Shostak", "M. Pease"], "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),", "year": 1982}, {"title": "Gradientbased learning applied to document recognition", "authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "year": 1998}, {"title": "Distributed stochastic variance reduced gradient methods and a lower bound for communication complexity", "authors": ["J.D. Lee", "Q. Lin", "T. Ma", "T. Yang"], "venue": "arXiv preprint arXiv:1507.07595,", "year": 2015}, {"title": "Robust empirical mean estimators", "authors": ["M. Lerasle", "R.I. Oliveira"], "venue": "arXiv preprint arXiv:1112.3914,", "year": 2011}, {"title": "Risk minimization by medianof-means tournaments", "authors": ["G. Lugosi", "S. Mendelson"], "venue": "arXiv preprint arXiv:1608.00757,", "year": 2016}, {"title": "Sub-gaussian estimators of the mean of a random vector", "authors": ["G. Lugosi", "S. Mendelson"], "venue": "arXiv preprint arXiv:1702.00482,", "year": 2017}, {"title": "Federated learning: Collaborative machine learning without centralized training data. https://research.googleblog.com/2017/ 04/federated-learning-collaborative", "authors": ["B. McMahan", "D. Ramage"], "year": 2017}, {"title": "Communication-efficient learning of deep networks from decentralized data", "authors": ["H.B. McMahan", "E. Moore", "D. Ramage", "S Hampson"], "venue": "arXiv preprint arXiv:1602.05629,", "year": 2016}, {"title": "Distributed statistical estimation and rates of convergence in normal approximation", "authors": ["S. Minsker", "N. Strawn"], "venue": "arXiv preprint arXiv:1704.02658,", "year": 2017}, {"title": "Geometric median and robust estimation in banach", "authors": ["S Minsker"], "venue": "spaces. Bernoulli,", "year": 2015}, {"title": "Problem complexity and method efficiency in optimization", "authors": ["A. Nemirovskii", "D.B. Yudin", "E.R. Dawson"], "year": 1983}, {"title": "Aide: Fast and communication efficient distributed optimization", "authors": ["S.J. Reddi", "J. Kone\u010dn\u1ef3", "P. Richt\u00e1rik", "B. P\u00f3cz\u00f3s", "A. Smola"], "venue": "arXiv preprint arXiv:1608.06879,", "year": 2016}, {"title": "On the optimality of averaging in distributed statistical learning", "authors": ["J.D. Rosenblatt", "B. Nadler"], "venue": "Information and Inference: A Journal of the IMA,", "year": 2016}, {"title": "Communicationefficient distributed optimization using an approximate newton-type method", "authors": ["O. Shamir", "N. Srebro", "T. Zhang"], "venue": "In International conference on machine learning,", "year": 2014}, {"title": "Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms", "authors": ["L. Su", "N.H. Vaidya"], "venue": "In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,", "year": 2016}, {"title": "Non-bayesian learning in the presence of byzantine agents", "authors": ["L. Su", "N.H. Vaidya"], "venue": "In International Symposium on Distributed Computing,", "year": 2016}, {"title": "Giant: Globally improved approximate newton method for distributed optimization", "authors": ["S. Wang", "F. Roosta-Khorasani", "P. Xu", "M.W. Mahoney"], "venue": "arXiv preprint arXiv:1709.03528,", "year": 2017}, {"title": "Gradient diversity: a key ingredient for scalable distributed learning", "authors": ["D. Yin", "A. Pananjady", "M. Lam", "D. Papailiopoulos", "K. Ramchandran", "P. Bartlett"], "venue": "arXiv preprint arXiv:1706.05699,", "year": 2017}, {"title": "Disco: Distributed optimization for self-concordant empirical loss", "authors": ["Y. Zhang", "X. Lin"], "venue": "In International conference on machine learning,", "year": 2015}, {"title": "Communication-efficient algorithms for statistical optimization", "authors": ["Y. Zhang", "M.J. Wainwright", "J.C. Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Divide and conquer kernel ridge regression: A distributed algorithm with minimax optimal rates", "authors": ["Y. Zhang", "J. Duchi", "M. Wainwright"], "venue": "The Journal of Machine Learning Research,", "year": 2015}], "id": "SP:9e691d9417c2e0b4cf4ef8a63c71f10aba7dc782", "authors": [{"name": "Dong Yin", "affiliations": []}, {"name": "Yudong Chen", "affiliations": []}, {"name": "Kannan Ramchandran", "affiliations": []}, {"name": "Peter Bartlett", "affiliations": []}], "abstractText": "In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failures\u2014arbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, nonstrongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.", "title": "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates"}