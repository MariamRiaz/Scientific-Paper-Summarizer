0	53	Deep neural networks have achieved state-of-the-art performance on many machine learning tasks in areas such as natural language processing (Wu et al., 2016), computer vision (Krizhevsky et al., 2012) and speech recognition (Hinton et al., 2012).
1	20	Training of such networks is often successfully performed by minimizing a high-dimensional nonconvex objective function, using simple first-order methods such as stochastic gradient descent.
2	26	Nonetheless, the success of deep learning from an optimization perspective is poorly understood theoretically.
3	12	Current results are mostly pessimistic, suggesting that even training a 3-node neural network is NP-hard (Blum & Rivest, 1993), and that the objective function of a single neuron can admit exponentially many local minima (Auer et al., 1996; Safran & Shamir, 2016).
5	46	Several works focus on the geometric properties of loss functions that neural networks attempt to minimize.
12	29	A no-overlap network can be viewed as a simple convolution layer with non overlapping filters, followed by a ReLU activation function, and then average pooling.
13	23	Formally, let w ∈ Rm denote the filter coefficients, and assume the input x is in Rd.
16	17	Finally, define σ to be the ReLU activation function, namely σ (z) = max{0, z}.
17	23	Then the output of the network in Figure 1 is given by: f(x;w) = 1 k ∑ i σ (w · x[i]) (1) We note that such architectures have been used in several works (Lin et al., 2013; Milletari et al., 2016), but we view them as important firstly because they capture key properties of general convolutional networks.
20	18	,xn from a distribution D, and assigning them labels using y = f(x;w∗).
21	29	The learning problem is then to find a w that minimizes the squared loss.
23	8	We believe the population risk captures the key characteristics of the problem, since the large data regime is the one of interest.
25	23	Specifically, in Section 4, we show that learning No-Overlap Networks is NP complete via a reduction from a variant of the set splitting problem.
71	10	In our analysis in Section 5 and Section 7.1 we assume that the input feature x ∈ Rd is a vector of IID Gaussian random variables with zero mean and variance one.2 Denote this distribution by G. We consider networks with one hidden layer, and k hidden units.
84	14	This simplifies the loss considerably, since for all i: g(wi,wi) = 1 2 ‖w‖ 2, and for all i 6= j: g(wi,wj) = 1 2π ‖w‖ 2 and g(wi,w∗j ) = 1 2π ‖w‖ ‖w ∗‖.
132	13	Then wTm( ∑ i∈Cj ei) > |Cj | 2k , which im- plies that f(d( ∑ i∈Cj ei);w) = ∑k l=1 σ(w T l ( ∑ i∈Cj ei)) k > |Cj | 2k2 ≥ 1 2k2 , a contradiction.
138	35	In order to analyze convergence of gradient descent on `, we need a characterization of all the critical and non-differentiable points.
139	27	We show that ` has a nondifferentiable point and a degenerate saddle point.6 Therefore, recent methods for showing global convergence of gradient-based optimizers on non-convex objectives (Lee et al., 2016; Ge et al., 2015) cannot be used in our case, because they assume all saddles are strict 7 and the objective function is continuously differentiable everywhere.
146	37	(c) A degenerate saddle point at w = −( k 2−k k2+(π−1)k )w ∗.
147	38	For k = 1, w = 0 is not a local maximum and the unique global minimum w∗ is the only differentiable critical point 8.
151	22	We note that the dependence of the convergence rate on is similar to standard results on convergence of gradient descent to stationary points (e.g., see discussion in Allen-Zhu & Hazan, 2016).
152	31	Assume ‖w∗‖ = 1.9 For any δ > 0 and 0 < < δ sinπδk , there exists 0 < λ < 1 10 such that with probability at least 1− δ, gradient descent initialized randomly from the unit sphere with learning rate λ will get to a point w such that `(w) ≤ O( ) in O( 1 2 ) iterations.
156	57	First we note that the gradient of `(w) at wt is given by: ∇`(wt) = −c1(wt,w∗)wt − c2(wt,w∗)w∗ , (14) where c1 and c2 are two functions such that c1 ≥ −1, c2 ≥ 0 and c2 = 0 if and only if θwt,w∗ = π.
158	15	At iteration t+ 1 we have: wt+1 = (1 + λc1(wt,w ∗))wt + λc2(wt,w ∗)w∗ (15) It follows that for λ < 1 and θwt,w∗ 6= π, we have θwt+1,w∗ < θwt,w∗ .
159	17	Therefore, if θw0,w∗ 6= π , we will never converge to the saddle point in Lemma 5.1.
160	39	Next, assuming ‖w0‖ > 0 and that θw0,w∗ ≤ (1 − δ)π (which occurs with probability 1− δ), it can be shown that the norm of wt is always bounded away from zero by a constant M = Ω̃(1).12 The proof is quite technical and follows from the fact that w = 0 is a local maximum.13 The fact that wt stays away from the problematic points allows us to show that `(w) has a Lipschitz continuous gradient on the line between wt and wt+1, with constant L = Õ(1).12 By standard optimization analysis (Nesterov, 2004) it follows that after T = O( 1 2 ) iterations we will have ‖∇l(wt)‖ ≤ O( ) for some 0 ≤ t ≤ T .
182	11	A natural question is then what happens when overlaps are allowed (namely, the stride is smaller than the filter size).
184	15	Here we show that this is in fact not the case, and that with probability greater than 14 gradient descent will get stuck in a sub-optimal region.
185	62	In Section 7.1 we analyze this setting for a two dimensional example and provide bounds on the level of suboptimality.
187	38	Our results suggest that by restarting gradient descent a constant number of times, it will converge to the global minimum with high probability.
