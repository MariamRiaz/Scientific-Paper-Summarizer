6	13	In this edition, new tasks have been added in order to adapt to social requirements.
7	18	There were the traditional Multilingual Multi-document and Single-document Summarization (MMS and MSS), coming from previous events, but also new summarization tasks related to Online Fora (OnForumS) - on how to deal with reader comments- and Call Center Conversation (CCCS) - from spoken conversations to textual synopses.
8	7	Taking into consideration the interest that multilingual summarization approaches is gaining among the research community, and the positive impact and benefits it may have for the society, the objective of this paper is to present a multilingual summarization approach within the MultiLing 2015 competition, discussing its potentials and limitations, and providing some insights of the future of this type of summarization based on the average results obtained by us and other participants as well.
9	11	The remaining of the paper is organized as follows.
10	7	In Section 2 we review the most relevant multilingual summarization approaches, some of them participating in previous MultiLing events.
12	10	Section 4 describes the 2013  250 task in which we participated, and the experiments performed.
37	7	In this Section, we present our proposed multilingual summarization approach (i.e., UA-DLSI ap- proach).
41	16	It has been often employed in conjunction with other data mining techniques, such as Semantic Vector Space model (Vikas et al., 2008) or Singular Value Decomposition (Lee et al., 2005), using term-based frequency methods.
44	44	For developing our UA-DLSI approach, we relied on the summary process stages outlined in (Sparck-Jones, 1999): 1) interpretation, 2) transformation and, finally, 3) the summary generation.
45	23	The first stage of our approach includes a linguistic and lexical-semantic processing (this latter part is optional).
46	35	For the linguistic processing, sentence segmentation, tokenization and stopwords removal is applied.
47	8	For the lexicalsemantic processing, a named entity recognizer (Standford Named Entity Recognizer4) and semantic resources, such as WordNet (Miller, 1995) and EuroWordNet (Vossen, 2004) are employed.
48	24	Whereas named entity recognizers mainly provide the identification of person, organization and place names in a document (Tjong et al., 2003), the semantic resources used comprises a set of synonyms grouped by means of the synsets that allow us to work with concept better than just with terms.
49	5	In this manner, we group a set of synonyms under the same concept.
53	29	The result of this stage is to build an initial lexical-semantic matrix, where for each sentence (rows in our matrix), we identify the units that will be later taken into account (i.e., terms, named entities, and/or concepts) which will correspond to the columns.
56	22	Once PCA has been applied over the covariance matrix, the principal components (eigenvectors) and its corresponding weight (eigenvalue) are obtained.
57	4	The eigenvectors are composed by the contribution of each variable, which determines the importance of the variable in the eigenvector.
59	14	In this manner, an eigenvector with high eigenvalue carries a great amount of information.
63	14	Two strategies were proposed for selecting and ordering the most relevant sentences from the document, leading to two types of summaries: one generic and one topic-focused.
64	42	In this manner, taking into account the element with the highest value for each eigenvector from the PCA matrix, we select and extract: â€¢ one sentence (searching in order of appearance in the original text) in which such concept6 appears.
69	7	This section describes the MultiLing 2015 task in which we participated, together with the dataset employed, and the explanation of the different variants of our approach submitted to the competition.
70	61	The Multilingual Single Document Summarization task was initially proposed in MultiLing 2013, targeting the same goal in the current edition: to evaluate the performance of participant systems whose work is focused on generating a single document summary for all the given Wikipedia articles in some of the languages provided (at least the participants should select three languages).
71	29	In the context of MultiLing 2015, two datasets were provided for the MSS task: a training dataset, containing 30 articles for each of the 38 available languages with their corresponding human-generated summaries; and a test dataset, which contains the same number of documents per language, but different from the training dataset, the human summaries were not provided.
81	32	As it can be seen, the length of the summaries compared to the original length of the Wikipedia articles (i.e., compression ratio) is very short, always below 10%.
83	67	Having provided the information about the general multilingual summarization process in Section 3, and since each participant in the MSS task was allowed to submit up to six approaches, different versions of our approach were set to participate in MultiLing 2015.
84	75	Apart of the two types of summaries that could be generated with our approach (T1: generic summary; T3: topic-focused summary), the incorporation of lexical-semantic knowledge was an optional substage, so we decided to test our approach also without any type of semantic knowledge, other than a list of stopwords for each language (LI: language-independent; LEX: using lexical knowlege (named entity recognition); SEM: using semantic knowledge (i.e., WordNet and EuroWordNet)).
85	109	This way the performance of a fully language-independent summarization approach based on PCA could be also analyzed.
86	79	Moreover, due to the nature of the test dataset (Wikipedia articles), all documents included headings for structuring different sections within them, so we opt for taking advantage of this information, considering only the words in these headings for the matrix construction (OWFH), instead of working with all words in the document, except stopwords (AW).
87	24	Headings usually contain important concepts that reflect the main topic of the section that follows.
88	13	Considering only this words, we also reduce the amount of information we have to process by 99% of the PCA matrix.
