0	39	When disaster strikes, news and social media are invaluable sources of information, allowing humanitarian organizations to rapidly mitigate crisis situations and save lives (Vieweg et al., 2010; Neubig et al., 2011; Starbird et al., 2012).
2	20	In these cases, machine translation (MT) technology can be a valuable tool, with one widely-heralded success story being the deployment of Haitian Creoleto-English translation systems during the earthquakes in Haiti (Lewis, 2010; Munro, 2010).
3	55	However, data-driven MT systems, particularly neural machine translation (NMT; Kalchbrenner and Blunsom (2013); Bahdanau et al. (2015)), require large amounts of training data, and creating high-quality systems in low-resource languages (LRLs) is a difficult challenge where research efforts have just begun (Gu et al., 2018).
5	12	In a crisis situation, time is of the essence, and systems that require days or weeks of training will not be desirable or even feasible.
7	13	To examine this question we propose NMT methods at the intersection of cross-lingual transfer learning (Zoph et al., 2016) and multilingual training (Johnson et al., 2016), two paradigms that, to our knowledge, have not been used together in previous work.
12	19	The results are sometimes surprising – we first find that a single monolithic model trained on 57 languages can achieve BLEU scores as high as 15.5 with no training data in the new source language whatsoever.
14	6	In this paper, we consider the setting where we have a source LRL of interest, and we want to translate into English.2 All of our adaptation methods are based on first training on larger data including other languages, then fine-tuning the model to be specifically tailored to the LRL.
15	50	We first discuss a few multilingual training paradigms from previous literature (§2.1), then discuss our proposed adaptation methods (§2.2).
17	16	This method is straightforward and the resulting model will be most highly tailored to the final test language pair, but the method also has the obvious disadvantage that training data is very sparse.
23	8	model that has wide coverage of vocabulary and syntax of a large number of languages, but also has the drawback in that a single model must be able to express information about all the languages in the training set within its limited parameter budget.
29	5	Within our experiments, we will test this setting, but also make two distinctions between the types of adaptation: Seed Model Variety: Zoph et al. (2016) performed experiments taking a bilingual system trained on a different language (e.g. French) and adapting it to a new LRL (e.g. Uzbek).
33	5	Intuitively, we expect warm-start training to perform better, as having access to the LRL of interest during the training of the original model will ensure that it can handle the LRL to some extent.
34	8	However, the cold-start scenario is also of interest: we may want to spend large amounts of time training a strong model, then quickly adapt to a new language that we have never seen before in our training data as data becomes available.
41	26	We perform experiments on the 58-language-toEnglish TED corpus (Qi et al., 2018), which is ideal for our purposes because it has a wide variety of languages over several language families, some high-resourced and some low-resourced.
43	10	These languages are all paired with a similar HRL: Turkish (tur), Russian (rus), Portuguese (por), and Czech (ces) respectively.
46	45	The model consists of an attentional neural machine translation model (Bahdanau et al., 2015), using bi-directional LSTM encoders, 128-dimensional word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder.
47	6	Following standard practice (Sennrich et al., 2016; Denkowski and Neubig, 2017), we break low-frequency words into subwords using the sentencepiece toolkit.6 There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model.
48	27	Previous work on multilingual training has preferred the former (Nguyen and Chiang, 2017), but in this paper we use the latter for two reasons: (1) because data in the LRL will not affect the subword units from the other languages, in the cold-start scenario we can postpone creation of subword units for the LRL until directly before we start training on the LRL itself, and (2) we need not be concerned with the LRL being “overwhelmed” by the higher-resourced languages when calculating statistics used in the creation of subword units, because all languages get an equal share.7 In the experiments, we use a subword vocabulary of 8,000 for each language.
49	15	We also compare with two additional baselines: phrase-based MT implemented in Moses,8 and unsupervised NMT implemented in undreamt.9 Moses is trained on the bilingual data only (training multilingually reduced average accuracy), and undreamt is trained on all monolingual data available for the LRL and English.
55	52	Comparing with the phrase-based baseline, as noted by Koehn and Knowles (2017) NMT tends to underperform on low-resource settings when trained only on the data available for these languages.
57	9	More interestingly, examining the cold-start results, we can see that even systems with no data in the target language are able to achieve nontrivial accuracies, up to 15.5 BLEU on glg-eng.
59	22	In contrast, the unsupervised NMT model struggles, achieving a BLEU score of around 0 for all language pairs – this is because unsupervised NMT requires high-quality monolingual embeddings from the same distribution, which can be trained easily in English, but are not available in the low-resource languages we are considering.
63	38	Finally, in our data setting, corpus concatenation outperforms balanced sampling in both the coldstart and warm-start scenarios.
64	38	How Can We Adapt Most Efficiently?
65	113	Finally, we revisit adapting to new languages efficiently, with Figure 1 showing BLEU vs. hours training for the aze/tur and bel/rus source language pairs (others were similar).
66	18	We can see that in all cases the cold-start models (All− →) either outperform or are comparable in final accuracy to the fromscratch single-source and bi-source models.
68	176	Comparing the cold-start adaptation strategies, we can see that in general, the higher the density of target language training data, the faster the training converges to a solution, but the worse the final solution is.
69	57	This suggests that there is a speed/accuracy tradeoff in the amount of similar language regularization we apply during fine-tuning.
75	66	This paper examined methods to rapidly adapt MT systems to new languages by fine-tuning.
76	35	In both warm-start and cold-start scenarios, the best results were obtained by adapting a pre-trained universal model to the low-resource language while regularizing with similar languages.
