0,1,label2,summary_sentences
Neural networks trained through stochastic gradient descent (SGD) can memorize their training data.,1. Introduction,[0],[0]
"Although practitioners have long been aware of this phenomenon, Zhang et al. (2017) recently brought attention to it by showing that standard SGD-based training on AlexNet gets close to zero training error on a modification of the ImageNet dataset even when the labels are randomly permuted.",1. Introduction,[0],[0]
"This leads to an interesting question: If neural nets have sufficient capacity to memorize random training sets why do
1Two Sigma, New York, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Satrajit Chatterjee <satrajit.chatterjee@twosigma.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
they generalize on real data?,1. Introduction,[0],[0]
A natural hypothesis is that nets behave differently on real data than on random data.,1. Introduction,[0],[0]
Arpit et al. (2017) study this question experimentally and show that there are apparent differences in behavior.,1. Introduction,[0],[0]
"They conclude that generalization and memorization depend not just on the network architecture and optimization procedure but on the dataset itself.
",1. Introduction,[0],[0]
"But what if networks fundamentally do not behave differently on real data than on random data, and, in both cases, are simply memorizing?",1. Introduction,[0],[0]
This is a difficult question to explore for two reasons.,1. Introduction,[0],[0]
"First, it is hard to provide a direct answer.",1. Introduction,[0],[0]
Whereas it is easy to tell when a net is memorizing random data (the training error goes to zero!),1. Introduction,[0],[0]
", there is no easy way to tell when a network is memorizing real data as opposed to “learning”.",1. Introduction,[0],[0]
"Second, and perhaps more importantly, it contradicts the intuitive notion—inherent in the preceding discussion—that memorization and generalization are at odds.",1. Introduction,[0],[0]
This work attempts to shed light on this second difficulty by investigating the following: How much can you learn if memorization is all you can do?,1. Introduction,[0],[0]
"Is generalization even possible in this setting?
",1. Introduction,[0],[0]
"At first, generalization in such a setting of pure memorization may seem hopeless: the simplest way to memorize would be to build a lookup table from the training data.",1. Introduction,[0],[0]
"Although this approach works for special cases where the input population is finite and small, it fails in general since the examples seen during training are unlikely to match test examples.",1. Introduction,[0],[0]
One way to get around this limitation is to use k-Nearest Neighbors (k-NN) or any of its variants at test time.,1. Introduction,[0],[0]
"While k-NNs work well on many problems, they fail on problems where it is not easy to construct a semantically meaningful distance function on the input space.",1. Introduction,[0],[0]
"In such cases, the obvious syntactic distance functions (e.g., say Euclidean distance between images viewed as vectors in Rd) do not work well.",1. Introduction,[0],[0]
"Indeed some of the most interesting results from deep learning have been the discovery—through learning—of semantically meaningful distance functions (via embeddings).
",1. Introduction,[0],[0]
"Therefore, in this work we do not allow ourselves a distance function.",1. Introduction,[0],[0]
"Instead, we get around the problem by applying the notion of depth, which has been wildly successful in improving the performance of neural networks, to direct memorization.",1. Introduction,[0],[0]
"We build a network of lookup tables (also
called “luts”) where the luts are arranged in successive layers much like a neural network.",1. Introduction,[0],[0]
"However, unlike a neural network, training happens through memorization and does not involve backpropagation, gradient descent, or any explicit search.",1. Introduction,[0],[0]
"Now, since in contrast to a neuron, the function implemented by a lut can be arbitrarily complex, without some means to control the complexity, the notion of depth is vacuous.",1. Introduction,[0],[0]
We control the complexity of a function learned by a lut in the simplest possible way: we limit the support (and thereby the size) of the lut.,1. Introduction,[0],[0]
"Each lut in a layer receives inputs from only a few luts in the previous layer, which are picked at random when the network is constructed.",1. Introduction,[0],[0]
This kind of restriction on local function complexity is similar to what is found to work well in deep neural networks.,1. Introduction,[0],[0]
"For example, a convolutional filter is obviously support-limited, and a fully connected layer although not support-limited is nevertheless limited in expressivity.",1. Introduction,[0],[0]
"Furthermore, the learned weight matrices in neural networks are often sparse or can be made so with no loss in accuracy (Han et al., 2015).
",1. Introduction,[0],[0]
We need two restrictions before we can proceed to an algorithm.,1. Introduction,[0],[0]
"First, for simplicity, we focus our attention on binary classification problems.",1. Introduction,[0],[0]
"Second, because lookup tables work naturally with discrete inputs, in this work we limit ourselves to discrete signals.",1. Introduction,[0],[0]
"In fact, the inputs and all intermediate signals in the network of lookup tables are binary.",1. Introduction,[0],[0]
The restriction is not as extreme as it may appear.,1. Introduction,[0],[0]
"There are a number of results in quantized and binary neural networks showing that limited precision is often sufficient (e.g. Rastegari et al., 2016).",1. Introduction,[0],[0]
"Furthermore, even in real-valued neural networks, we need mechanisms such as convolution and pooling to ensure that certain types of small changes in the inputs (e.g., a small displacement) do not lead to large changes in output.",1. Introduction,[0],[0]
"In principle, similar mechanisms could be used in a fully discrete setting to handle real-valued quantities.
",1. Introduction,[0],[0]
"With these restrictions in place, we are now ready to proceed.",1. Introduction,[0],[0]
"Let B = {0, 1} and consider the problem of learning a function f :",2. A Single Lookup Table,[0],[0]
"Bk → B from a list of training examples where each example is an (x, y) pair.",2. A Single Lookup Table,[0],[0]
"Since we want to learn by memorizing, we construct a lookup table with 2k rows (one for each possible bit pattern p ∈",2. A Single Lookup Table,[0],[0]
Bk that can appear at the input) and two columns y0 and y1.,2. A Single Lookup Table,[0],[0]
"The y0 entry for the row corresponding to pattern p (denoted by cp0) counts how many times p is associated with output 0 in the training set, i.e., the number of occurrences of (p, 0) in the training set.",2. A Single Lookup Table,[0],[0]
"Similarly, the y1 entry for row p (denoted by cp1) counts how many times the pattern p is associated with the output 1 in the training set, i.e., the number of occurrences of (p, 1)
in the training set.",2. A Single Lookup Table,[0],[0]
"Note that for a pattern p it is possible for both cp0 and cp1 to be greater than zero since due to Bayes error both (p, 0) and (p, 1) may be present in the training examples.",2. A Single Lookup Table,[0],[0]
It is also possible for both cp0 and cp1 to be zero if the input p never appears in the training examples.,2. A Single Lookup Table,[0],[0]
"We call such a lookup table a k-input lookup table or a k-lut since the inputs are bit vectors of length k.1
Next, we associate a boolean function f̂ : Bk → B with the lookup table in the following manner:
f̂(p) =  1 if cp1 > cp0,0 if cp1 < cp0, b if cp1 = cp0
where b ∈ B is picked uniformly at random when fixing f̂ in order to break ties.",2. A Single Lookup Table,[0],[0]
"In other words, f̂ maps an input p to the output that is most often associated with it in the training set (breaking ties randomly).",2. A Single Lookup Table,[0],[0]
"We say that f̂ is the function learned by the lookup table.
",2. A Single Lookup Table,[0],[0]
Example 1.,2. A Single Lookup Table,[0],[0]
Let k = 3 and consider learning a function f : B3 → B from 7 examples shown on the left below.,2. A Single Lookup Table,[0],[0]
"The lookup table that we learn is shown in the middle, and the truth table of the learned function f̂ is shown on the right.",2. A Single Lookup Table,[0],[0]
"The entries in the truth table which have been picked randomly to break ties are indicated by an asterisk.
",2. A Single Lookup Table,[0],[0]
"x x0x1x2
y
000 0 000 1 000 1 001 1 100 0 110 0 110 1
p x0x1x2
y0 y1
000 1 2 001 0 1 010 0 0 011 0 0 100 1 0 101 0 0 110 1 1 111 0 0
p f̂
000 1 001 1 010 0∗ 011 1∗ 100 0 101 1∗ 110 1∗ 111 0∗
Note that f̂ gets all training examples correct except for the first and sixth.",2. A Single Lookup Table,[0],[0]
"This is the best we can do on this set of training examples because the Bayes error rate is non-zero.
",2. A Single Lookup Table,[0],[0]
"If we measure training error as the average 0–1 loss on the training set, this procedure to learn f̂ has the following properties:
1.",2. A Single Lookup Table,[0],[0]
Optimality.,2. A Single Lookup Table,[0],[0]
"The learned function f̂ is Bayes-optimal on the training set, i.e., there is no function g :",2. A Single Lookup Table,[0],[0]
Bk → B with training error strictly less than that of f̂ .,2. A Single Lookup Table,[0],[0]
"In particular, the training error is zero iff the training set has zero Bayes error.
2.",2. A Single Lookup Table,[0],[0]
Monotonicity.,2. A Single Lookup Table,[0],[0]
"If we have more information for each x in the training set, i.e., we augment each training
1 Typically k is small (less than 10) and so the the table can be stored explicitly.",2. A Single Lookup Table,[0],[0]
"The input bit vector (viewed as an integer) can be used to directly index into the table.
example with m extra bits of information (keeping the labels fixed) and use the above procedure to now learn a new function ĝ : Bk+m → B, then the training error of ĝ is no more than that of f̂ .
",2. A Single Lookup Table,[0],[0]
Proof Sketch.,2. A Single Lookup Table,[0],[0]
Optimality is easy to see since the total training error is the sum of the training error for each possible pattern p which is minimized by choosing the majority class for each p.,2. A Single Lookup Table,[0],[0]
"Monotonicity holds since if not, then we can compose the obvious projection Bk+m",2. A Single Lookup Table,[0],[0]
"→ Bk with f̂ to get a contradiction with the optimality of ĝ.
Note that monotonicity implies in particular that the training accuracy at the output of a lut is no worse than that at any of its inputs.",2. A Single Lookup Table,[0.9505583329296992],"['Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree) it is often possible to completely determine or at least place strong constraints on the semantics of the third.']"
"Furthermore, as we make the luts larger, the training error cannot increase but only decrease.",2. A Single Lookup Table,[0],[0]
This is interesting since there are no restrictions on the m extra bits: they could be completely non-informative.,2. A Single Lookup Table,[0],[0]
"These properties will prove useful in the next section as we consider networks of luts.
",2. A Single Lookup Table,[0],[0]
"To summarize, the procedure described to learn a single lookup table in this section is essentially memorization in the presence of Bayes error, where the idea is to simply remember the output that is most commonly associated with an input in the training set.",2. A Single Lookup Table,[0],[0]
"Now consider a binary classification task on MNIST (LeCun & Cortes, 2010) of separating the digits ‘0’ through ‘4’ (we map these to the 0 class) from the digits ‘5’ through ‘9’ (the 1 class) where the pixels are 1-bit quantized.",3. A Network of Lookup Tables,[0],[0]
"Thus the task is to learn a function f : B28×28 → B. We call this the Binary-MNIST task (overloading binary here to mean both binary classification and binary inputs).
",3. A Network of Lookup Tables,[0],[0]
"In principle, we could use the procedure in Section 2 to learn this function.",3. A Network of Lookup Tables,[0],[0]
"However, since we have only 60,000 training examples in MNIST, most of the 228×28 rows in the lookup table would have 0 entries in both columns, and hence the function learned would be mostly random and have very poor generalization to inputs outside the training set.
",3. A Network of Lookup Tables,[0],[0]
"As discussed in the introduction, we get around this problem by introducing depth.",3. A Network of Lookup Tables,[0],[0]
"Instead of learning a giant lookup table with 228×28 entries, we learn a network of (much) smaller lookup tables.",3. A Network of Lookup Tables,[0],[0]
The network consists of d layers with each layer l (1 ≤ l ≤ d) having nl k-input lookup tables.,3. A Network of Lookup Tables,[0],[0]
Each lut in first layer (l = 1) receives its inputs from a krandom subset of the network inputs.,3. A Network of Lookup Tables,[0],[0]
A lut in a layer,3. A Network of Lookup Tables,[0],[0]
l > 1 receives inputs from a k-random subset of the luts in layer l − 1.,3. A Network of Lookup Tables,[0],[0]
The connectivity is fixed at network creation time and does not change during training or inference.,3. A Network of Lookup Tables,[0],[0]
"The final layer of the network has a single lookup table (i.e., nd = 1) which is the output of the network.",3. A Network of Lookup Tables,[0],[0]
"By analogy with neural
networks, we call the final layer the output layer and the other layers hidden layers.
",3. A Network of Lookup Tables,[0],[0]
"We train the lookup tables layer by layer, where the target of each lookup table is the final output.",3. A Network of Lookup Tables,[0],[0]
We start from the first layer and work our way to the output.,3. A Network of Lookup Tables,[0],[0]
"Once a layer has been learned, we use the functions associated with its luts (the f̂s of Section 2) to map its inputs to outputs.",3. A Network of Lookup Tables,[0],[0]
"These outputs serve as the inputs for the next layer, which is learned next.",3. A Network of Lookup Tables,[0],[0]
"Continuing our analogy with neural networks, we call the output values of a layer activations.
",3. A Network of Lookup Tables,[0],[0]
"Inference is similar to training: We start from the inputs and evaluate each layer in order using the functions learned at each lut to map inputs to outputs.
",3. A Network of Lookup Tables,[0],[0]
Example 2.,3. A Network of Lookup Tables,[0],[0]
We modify Example 1.,3. A Network of Lookup Tables,[0],[0]
"Instead of learning a single lut with k = 3 inputs, we learn a network of k = 2 luts.",3. A Network of Lookup Tables,[0],[0]
The network shown in Figure 1 has d = 2 layers.,3. A Network of Lookup Tables,[0],[0]
"The first layer has 2 luts (i.e., n1 = 2) which are connected to inputs x0 and x1 of the network.",3. A Network of Lookup Tables,[0],[0]
"The second layer (which is also the output layer) has 1 lut (i.e., n2 = 1) which is connected to the outputs of the two luts in the first layer.",3. A Network of Lookup Tables,[0],[0]
(The connections were made randomly when the network was created.),3. A Network of Lookup Tables,[0],[0]
"Using the procedure in Section 2, the two lookup tables learned in the first layer (using y as the target) along with their corresponding functions f̂10 and f̂11 are:
p x0x1
y0 y1 f̂10
00 1 3 1 01 0 0 1∗ 10 1 0 0 11 1 1 1∗
p x0x2
y0 y1 f̂11
00 1 2 1 01 0 1 1 10 2 1 0 11 0 0 1∗
Let the output of the luts in the first layer be w10 and w11, i.e., w10 = f̂10(x0x1) and w11 = f̂11(x0x2).",3. A Network of Lookup Tables,[0],[0]
The learning problem for the lut in the second layer is shown in the tables below.,3. A Network of Lookup Tables,[0],[0]
"For convenience, on the left we show the primary inputs x0, x1 and x2, the first layer activations w10 and w11 (which are the inputs of the lut), and the target output y.",3. A Network of Lookup Tables,[0],[0]
"On the right we show the table and the learned function f̂20:
x x0x1x2
w10w11 y
000 11 0 000 11 1 000 11 1 001 11 1 100 00 0 110 10 0 110 10 1
p w10w11
y0 y1 f̂20
00 1 0 0 01 0 0 1∗ 10 1 1 0∗ 11 1 3 1
In this case the function implemented by the network of 2-luts has the same performance on the training set as the function learned by the 3-lut in Example 1.",3. A Network of Lookup Tables,[0],[0]
"Since there are fewer possible patterns in the case of smaller luts, we expect better pattern coverage during training and hence better generalization.
",3. A Network of Lookup Tables,[0],[0]
Implementation.,3. A Network of Lookup Tables,[0],[0]
"The memorization procedure described here is linear in the size of the training data, requiring two passes over the training set.",3. A Network of Lookup Tables,[0],[0]
It is computationally efficient since it only involves counting and dense table lookups and does not require floating point.,3. A Network of Lookup Tables,[0],[0]
"It is also easy to parallelize since each lut in a given layer is independent, and the counts can be computed on disjoint subsets of the training data and then combined (using, for example, a reduction tree).",3. A Network of Lookup Tables,[0.9528172054616597],"['Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.']"
Note that using this property it is possible to execute the algorithm on extremely large datasets where all the training examples may not fit on a single machine with only the summary statistics of the data (the counts in the lookup tables) being exchanged across machines.,3. A Network of Lookup Tables,[0],[0]
Experiment 1.,4. Experiments,[0],[0]
"In the first experiment, we apply the above procedure to the Binary-MNIST task (as defined in Section 3) to see if this approach to memorization can generalize.",4. Experiments,[0],[0]
"For this experiment, we construct a network with 5 hidden layers of 1024 luts and 1 lut in the output layer.",4. Experiments,[0],[0]
"We set k = 8, i.e., each lut in the network takes 8 inputs.
",4. Experiments,[0],[0]
"The network achieves a training accuracy of 0.89 on this task, which is perhaps not so surprising since we are memorizing the training data after all.",4. Experiments,[0],[0]
"But what is surprising is that the network achieves an accuracy of 0.87 on a heldout set (the 10,000 test images in MNIST) which indicates generalization.
",4. Experiments,[0],[0]
"This result is not state-of-the-art on this variant of MNIST (see Experiment 4), but that is not the point.",4. Experiments,[0],[0]
"It is significantly above the 0.5 accuracy that would be expected by chance, and this is achieved by an algorithm that only memorizes and performs no explicit search.
",4. Experiments,[0],[0]
The training and test accuracies are stable: there is very little variation from run to run.,4. Experiments,[0],[0]
"In other words, very little depends on the actual random choices made when deciding the topology of the network.",4. Experiments,[0],[0]
"To understand why this is
the case, we look at training accuracies of the luts in the network.",4. Experiments,[0],[0]
"Since the target for each lut in the network is the final classification target, we can examine the accuracy of a lut as a function of its layer.
",4. Experiments,[0],[0]
Table 1 shows the summary statistics for the accuracies of luts in each layer.,4. Experiments,[0],[0]
We observe that as depth increases the average accuracy of the luts in a layer goes up.,4. Experiments,[0],[0]
"In other words, depth helps.",4. Experiments,[0],[0]
"Some intuition for this is provided by the monotonicity property of the luts: the output of a lut cannot have lower accuracy than any of its inputs (Section 2).
",4. Experiments,[0],[0]
"Furthermore, we observe in Table 1 the dispersion in accuracy across the luts (measured either by standard deviation (std) or the difference between max and min) goes down.",4. Experiments,[0],[0]
"Therefore, as depth increases the specifics of the connectivity matters less and the network automatically becomes more stable with respect to the random choices made during construction.",4. Experiments,[0.9516656576227484],"['Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.']"
"Indeed we can say something stronger: we have seen in our experiments (not shown in Table 1) that as depth increases the activations of the luts in a layer become more correlated with each other, and hence become more interchangeable.",4. Experiments,[0],[0]
"While this correlation is good for stability with respect to connectivity, it causes diminishing returns with additional depth.
",4. Experiments,[0.9530032352653836],"['The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.']"
Remark.,4. Experiments,[0],[0]
The perceptive reader looking at Table 1 will also notice that we are wasting computation: the single output lut in layer 6 receives input from only 8 of the 1024 luts in layer 5 and these in turn can at most receive inputs from 64 luts from layer 4.,4. Experiments,[0],[0]
"Although a different topology would be more computationally efficient, this specific choice allows us to compare the different layers more easily.",4. Experiments,[0.9502539148069216],['Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.']
"We have not optimized this aspect since it typically takes less than 30 seconds using a single threaded unoptimized implementation (Python with NumPy) to run an experiment.
",4. Experiments,[0],[0]
Experiment 2.,4. Experiments,[0],[0]
"As discussed in the introduction and in Section 3, we do not expect unbridled memorization in the form of a large lookup table (say k = 28 × 28 in the case of Binary-MNIST) to generalize at all.",4. Experiments,[0],[0]
"This motivated our
exploration of a network of smaller lookup tables parameterized by k (the number of inputs of each lut).",4. Experiments,[0],[0]
We now vary k to see if we can control the amount of memorization and to see the effect it has on generalization.,4. Experiments,[0],[0]
"To avoid changing too much at once, we keep the number of layers and the number of luts per layer the same as in Experiment 1.
",4. Experiments,[0],[0]
The results are shown in the first 3 columns of Table 2.,4. Experiments,[0],[0]
"With small values of k, the network finds it difficult to memorize the training data.",4. Experiments,[0],[0]
"As intuitively expected (see also the monotonicity property in Section 2), as k increases the training accuracy goes up with perfect memorization at k = 14, i.e., long before 28×28.",4. Experiments,[0],[0]
"However, larger luts generalize less well, and the best test accuracy of 0.90 is achieved at k = 12 though with substantially good memorization of the training data (0.99).",4. Experiments,[0],[0]
"Interestingly, there is a clear monotonic increase in the generalization gap measured as the difference between training and test accuracy with increasing k.
Experiment 3.",4. Experiments,[0],[0]
In this experiment—along the lines of those performed in Zhang et al. (2017)—we randomly permute the labels in the training set and repeat Experiment 2 on this “random” dataset.,4. Experiments,[0],[0]
The results are shown in columns 4 and 5 of Table 2.,4. Experiments,[0],[0]
"As expected, with increasing k the network gets better at memorizing the training data, and the test accuracy hovers around chance (0.5) though with significant variation (± 0.05).",4. Experiments,[0],[0]
"This may be viewed as empirical evidence that the Rademacher complexity goes up with k.
However, and this may be surprising for a pure memorization algorithm, memorizing random data turns out to be harder than memorizing real data (columns 2 and 3 of Table 2) in the sense that a larger k is required to get the same accuracy with random data than with real data.",4. Experiments,[0],[0]
"For example, it takes until k = 12 to get comparable training accuracy on random data as k = 4 gets on real data.",4. Experiments,[0],[0]
"This result corroborates the findings in Arpit et al. (2017, §3 and §4) that real data is easier to fit than random data.",4. Experiments,[0],[0]
But it also means that we cannot conclude that any such difference observed in neural networks is because they do not use brute force memorization on real data.,4. Experiments,[0],[0]
"As this experiment shows, such
differences can appear even with brute force memorization.
",4. Experiments,[0],[0]
"Finally, at k = 12 we have a network that is able to memorize random data (random training accuracy of 0.82) and yet generalizes to test data when trained on real data (real test accuracy of 0.90).",4. Experiments,[0],[0]
This is very similar to findings of Zhang et al. (2017) in the context of neural networks.,4. Experiments,[0],[0]
"Kawaguchi et al. (2017, §3) argue that this phenomenon is universal and our result may be viewed as further empirical evidence for their claim showing that this phenomenon can happen even in the simplified setting of just memorization.
",4. Experiments,[0],[0]
Experiment 4.,4. Experiments,[0],[0]
"For completeness, we compare memorization with several standard methods and the results are shown in Table 3.",4. Experiments,[0],[0]
We have not specifically tuned the other methods since our goal is not to beat the state-of-the-art but to get a sense of how memorization alone does when compared to the standard methods.,4. Experiments,[0],[0]
"The best performance is obtained by a LENET-style convolutional network with 2 convolutions (64 and 32 filters respectively) each followed by a corresponding max pool layer, and 3 fully connected layers (256, 128 and 2 units respectively) with softmax output.",4. Experiments,[0],[0]
"The net is trained for 6 epochs with stochastic gradient descent and dropout.
",4. Experiments,[0],[0]
"Once again, compared to random guessing which has 0.50 test accuracy, memorization does quite well with a test accuracy of 0.90 (using the k = 12 configuration from Experiment 2) and beats logistic regression and naı̈ve Bayes.",4. Experiments,[0],[0]
"Interestingly, 1- and 5-Nearest Neighbors do well too (test accuracy of 0.97) though recall that they are provided with a distance function which memorization does not have access to and must in a sense discover.
",4. Experiments,[0],[0]
Experiment 5.,4. Experiments,[0],[0]
"We now consider the task of separating the i-th digit in MNIST from the j-th digit, which gives us( 10 2 ) = 45 binary classification tasks, which we collectively call Pairwise-MNIST.",4. Experiments,[0],[0]
"The images are binarized as before.
",4. Experiments,[0],[0]
"Figure 2 shows the training accuracy and the test accuracy for each of those 45 experiments for 8 different values of k.
As in Experiment 2, we find that as k increases, the training accuracy increases (reaching 1.0), but the test accuracy falls off.",4. Experiments,[0],[0]
"If we look at the best test accuracies for a given task (across k), on 31 out of the 45 tasks, we do better than 0.98.",4. Experiments,[0],[0]
The worst of these is 0.95 which is the best memorization can do for separating ‘4’ and ‘9’.,4. Experiments,[0],[0]
This is still significantly better than the 0.5 we would expect by chance.,4. Experiments,[0],[0]
"Typically the best test accuracies are achieved at k = 6 and k = 8.
",4. Experiments,[0],[0]
Experiment 6.,4. Experiments,[0],[0]
In Experiment 5 we notice that the variation is quite high for k = 2.,4. Experiments,[0],[0]
This indicates that the depth of the network is insufficient for proper mixing.,4. Experiments,[0],[0]
"To investigate this further, we keep k = 2 and vary the number of hidden layers from 20 to 25.",4. Experiments,[0],[0]
Each hidden layer still has 1024 luts.,4. Experiments,[0],[0]
Figure 3 shows how the training and test accuracies vary with the depth of the network.,4. Experiments,[0],[0]
"It is interesting to note that the test accuracy continues to improve even for relatively deep networks (16 or 32 hidden layers), and we get very high test accuracies even with such small lookup tables.",4. Experiments,[0],[0]
"Furthermore, we note that the variation in the generalization error (difference between training and test accuracies) decreases with increasing depth.
",4. Experiments,[0],[0]
Experiment 7.,4. Experiments,[0],[0]
Next we look at memorization on CIFAR-10 which is a collection of 32 pixel by 32 pixel color images belonging to 10 classes.,4. Experiments,[0],[0]
"As with Binary-MNIST, we quantize each color channel to 1 bit and try to separate the classes 0 through 4 from classes 5 through 9.",4. Experiments,[0],[0]
"This gives us the Binary-CIFAR-10 task where we have to learn a function f : B3×32×32 → B from 50,000 images.",4. Experiments,[0],[0]
"Incidentally, the quantization of each color channel to 1-bit significantly degrades the signal making it a difficult task for humans.
",4. Experiments,[0],[0]
"For this task, we construct a network with 5 hidden layers each with 1024 luts and one output layer with 1 output.",4. Experiments,[0],[0]
We set k = 10 for the luts.,4. Experiments,[0],[0]
This network is able to achieve a training accuracy of 0.79 and a test accuracy of 0.63.,4. Experiments,[0],[0]
"Although not as impressive in absolute terms as the memoriza-
tion result on Binary-MNIST, it is still significantly above chance (0.50).",4. Experiments,[0],[0]
"Furthermore, as before, the result is very stable and does not depend on a specific random topology chosen when the network is constructed.
",4. Experiments,[0],[0]
We compare memorization with several standard methods in Table 4.,4. Experiments,[0],[0]
By comparing Table 4 with Table 3 it is clear that Binary-CIFAR-10 is a harder task than Binary-MNIST since all the methods perform significantly worse on it.,4. Experiments,[0],[0]
"The best test accuracy of 0.71 is again from a LENET-style network similar to the one used in Experiment 4, but with 40 epochs of training.",4. Experiments,[0],[0]
"We believe a ResNet-style architecture (He et al., 2016) may potentially do better here but since our goal is not to achieve state-of-the-art but see how memorization does, we leave this to future work.",4. Experiments,[0],[0]
"For the same reason we don’t explore data augmentation here which is a standard technique for CIFAR-10.
Once again, memorization compares favorably on test accuracy with the other methods, and compared to Binary-MNIST it does relatively better here since it ties with the nearest neighbor searches.
",4. Experiments,[0],[0]
Experiment 8.,4. Experiments,[0],[0]
"In this experiment, we consider the Pairwise-CIFAR-10 tasks which are defined analogously to Pairwise-MNIST.",4. Experiments,[0],[0]
We use the same network architecture as in Experiment 7 instead of optimizing specifically for these tasks.,4. Experiments,[0],[0]
Training accuracies are generally 0.95 and above whereas the test accuracies range from 0.61 (CAT v/s DOG) to 0.85,4. Experiments,[0],[0]
"(FROG v/s SHIP) with an average test accuracy of 0.76 which is significantly above chance.
",4. Experiments,[0],[0]
Experiment 9.,4. Experiments,[0],[0]
"To get qualitative insight into the decision boundaries learned with different levels of memorization, we classify points in the region [−2, 2] ×",4. Experiments,[0],[0]
"[−2, 2] ∈ R2 as being inside or outside the circle",4. Experiments,[0],[0]
x2,4. Experiments,[0],[0]
+,4. Experiments,[0],[0]
y2 ≤ 1.62.,4. Experiments,[0],[0]
"Our dataset consists of points on a 100 × 100 grid in this region which has been partitioned into equal test and training sets (Figure 4, leftmost column).",4. Experiments,[0],[0]
To make this a hard problem we encode each point as pair of 10-bit fixed-point numbers.,4. Experiments,[0],[0]
"We learn this function f : B20 → B using networks with 32 layers each with 2048 luts and vary k. With k = 10 (rightmost column), the training set is memorized perfectly but (as seen on test)",4. Experiments,[0],[0]
the concept is not learned.,4. Experiments,[0],[0]
"However, memorizing with k = 2, we learn a simpler concept that is not faithful around the “corners” (as can be seen by zooming in) but one that generalizes almost perfectly to test.",4. Experiments,[0],[0]
"Finally, k = 6 provides a satisfactory compromise between the two extremes.",4. Experiments,[0],[0]
"Thus, once again, we see that memorization if done carefully can lead to good generalization.",4. Experiments,[0],[0]
"It is instructive to compare our memorization procedure with a few commonly used procedures for learning.
",5. Comparison with Other Methods,[0],[0]
k-Nearest Neighbors.,5. Comparison with Other Methods,[0],[0]
"The key difference, as noted in the introduction, is that k-NNs require a user-specified distance function which is often syntactic notion of distance such that induced by treating an image as a vector in Rd.",5. Comparison with Other Methods,[0],[0]
These syntactic notions of distance do not work well on more challenging tasks and one may view such a learning problem as essentially that of discovering a semantically meaningful distance function.,5. Comparison with Other Methods,[0],[0]
"We see this in our experiments: the
tr ai
ni ng
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
te st
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
−0.5,5. Comparison with Other Methods,[0],[0]
"0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
ground truth k = 2 k = 6 k = 10
Figure 4.",5. Comparison with Other Methods,[0],[0]
"The decision boundaries learned in Experiment 9.
distance function helps more with Binary-MNIST (Experiment 4) than it does with Binary-CIFAR-10 (Experiment 7).",5. Comparison with Other Methods,[0],[0]
"Furthermore, in a separate experiment we found that augmenting the table lookup with 1-NN search at test time did not significantly improve test accuracy for Binary-CIFAR-10 where memorization was already tied with k-NNs.
Additionally, k-NN requires storing the entire training set and is typically computationally more expensive at test time.",5. Comparison with Other Methods,[0],[0]
"For example, on Binary-MNIST the standard k-NN implementation in scikit-learn (Pedregosa et al., 2011) took more than an hour to evaluate performance on the training and test sets (as opposed to seconds with memorization).",5. Comparison with Other Methods,[0],[0]
"There has been work on speeding up nearest neighbor search by using locality sensitive hashing (Indyk & Motwani, 1998) and, more recently, with random projections (Li & Malik, 2016).",5. Comparison with Other Methods,[0],[0]
"In that context, one may view each lookup table as implementing a trivial locality sensitive hash function where the distance metric arises from exact equality, and the network as an ensemble through cascading of such nearest neighbors classifiers.
",5. Comparison with Other Methods,[0],[0]
Neural Networks.,5. Comparison with Other Methods,[0],[0]
"The initial motivation for this work was to understand neural networks better; particularly to explore with a model the idea that perhaps SGD is a sophisticated way to memorize training data in a manner that generalizes and that perhaps there are simpler ways to memorize data
as well that may yet generalize.",5. Comparison with Other Methods,[0.9521632485198867],"['Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.']"
"However, a key difference is that gradient descent-based training can learn useful intermediate representations or targets for hidden layers.",5. Comparison with Other Methods,[0],[0]
"In this work we have side stepped that question, by simply setting the intermediate target to be the final output.",5. Comparison with Other Methods,[0],[0]
It is an interesting line of research to see if we can find a way to learn useful intermediate signals in this setting perhaps by purely combinatorial methods.,5. Comparison with Other Methods,[0],[0]
"Practically, that would give us a method to learn purely binary neural networks without using floating point at all, which is useful in resource constrained environments.
",5. Comparison with Other Methods,[0],[0]
Random Forests.,5. Comparison with Other Methods,[0],[0]
"Trees in a random forest are constructed over a subset of the data by iteratively evaluating different input variables to optimize purity after splitting on the variable (Breiman, 2001).",5. Comparison with Other Methods,[0],[0]
"In contrast, memorization uses the whole dataset and does not solve any optimization problem (which makes it more computationally efficient).",5. Comparison with Other Methods,[0],[0]
"Furthermore, random forests combine the tree predictions using voting whereas memorization uses cascading.
",5. Comparison with Other Methods,[0],[0]
Cascading and Stacked Generalization.,5. Comparison with Other Methods,[0],[0]
"A recent extension of random forests are Deep Forests (Zhi-Hua Zhou, 2017) where multiple random forests are constructed at each level and then cascaded using the idea of stacked generalization (Wolpert, 1992) which is a generalization of cross-validation.",5. Comparison with Other Methods,[0],[0]
"In contrast, layers of luts are far simpler, and memorization propagates outputs based on what has been memorized over the entire training data.",5. Comparison with Other Methods,[0],[0]
"Due to the manner in which we construct the lookup tables and the corresponding functions (using the counts of the patterns) it is not clear to us that stacked generalization will help.
",5. Comparison with Other Methods,[0],[0]
Spectral Methods.,5. Comparison with Other Methods,[0],[0]
"There is a rich literature on the theory of learning boolean functions (f : Bk → B in our notation) (Mansour, 1994) which looks at theoretical learning guarantees under assumptions on the input distribution (typically uniform) and on the spectrum of the function (e.g. f can be approximated by a sparse and low degree polynomial in the boolean fourier basis).",5. Comparison with Other Methods,[0],[0]
"Recently, Hazan et al. (2017) have used these techniques in hyperparameter optimization where they find them to be practically useful (the distributional assumption is not fatal for this application).",5. Comparison with Other Methods,[0],[0]
"This line of work does not deal with depth, but only linear combinations of the basis functions.",5. Comparison with Other Methods,[0],[0]
"However, there is similarity in having a low degree in the fourier basis and our notion of support-limited memorization.",5. Comparison with Other Methods,[0],[0]
"These are similar structural priors and our results and those of Hazan et al. may be viewed as evidence that real world functions satisfy these priors.
",5. Comparison with Other Methods,[0],[0]
Learning Boolean Circuits.,5. Comparison with Other Methods,[0],[0]
"There is relatively little prior work in directly learning boolean circuits (Oliveira & Sangiovanni-Vincentelli, 1994; Tapp, 2014).",5. Comparison with Other Methods,[0],[0]
"However, it is interesting to note that the memorization algorithm in Section 3 although developed independently and from different
considerations is similar to the greedy algorithm described by Tapp.2",5. Comparison with Other Methods,[0],[0]
"An important difference is that instead of learning a single tree, we learn a network which makes learning more stable (as seen in Experiment 1).",5. Comparison with Other Methods,[0],[0]
"The experiments of Zhang et al. (2017) and Arpit et al. (2017) on training with random data lead naturally to the question that if neural networks can memorize random data and yet generalize on real data, are they perhaps doing something different in the two cases.",6. Conclusion,[0],[0]
This work started with the opposite thought: What if in both cases they are simply memorizing?,6. Conclusion,[0],[0]
"This, in turn, leads to the question of whether it is even possible to generalize from pure memorization.",6. Conclusion,[0],[0]
Naı̈ve memorization with a lookup table is too simplistic a model,6. Conclusion,[0],[0]
"but, as we saw, a slightly more complex model in the form of a network of support-limited lookup tables does significantly better than chance and is closer to the standard algorithms on a number of binary classification problems from MNIST and CIFAR-10.",6. Conclusion,[0],[0]
"(To investigate if this result holds on other datasets is an important area of future work.)
",6. Conclusion,[0],[0]
"Furthermore, this model replicates some of the key observations with neural networks: the performance of a network improves with depth; it memorizes random data and yet generalizes on real data; and memorizing random data is harder than real data.",6. Conclusion,[0],[0]
"In particular, the last observation implies that we cannot rule out memorization based on differences in the hardness of learning between real and random data.
",6. Conclusion,[0],[0]
"For future work, we would like to understand why memorization generalizes.",6. Conclusion,[0],[0]
"Now, since the size of the hypothesis space is bounded by 2n2 k
(where n is the number of k-luts in the network), we can use results from PAC-learning to bound the generalization gap, but these bounds are typically weak or vacuous.3 Rademacher complexity may be useful for small k (say 2), but for moderate k—where the Rademacher complexity is high yet there is generalization— we would need a different approach, perhaps one based on stability (Bousquet & Elisseeff, 2002).",6. Conclusion,[0],[0]
"In this connection, we expect the results in Devroye & Wagner (1979) to apply to a single lut, but extensions are needed to handle networks of luts, i.e., depth.",6. Conclusion,[0],[0]
"Furthermore, these would have to incorporate details of the construction since not every network of luts generalizes (even for k = 2).
",6. Conclusion,[0],[0]
"Finally, given the computational efficiency of memorization, we would like to extend it to a practically useful algorithm for learning, but that would likely involve introducing some form of explicit optimization or search.
",6. Conclusion,[0.9530920517554438],"['This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to the training set.']"
2We thank David Krueger for noticing the connection.,6. Conclusion,[0],[0]
3,6. Conclusion,[0],[0]
"For example, using Theorem 2.2 in Mohri et al. (2012) for the experiments in Table 2 (with δ = 0.01 for concreteness) bounds the gap to 0.34 for k = 2.",6. Conclusion,[0],[0]
The bound doubles as k increases by 2.,6. Conclusion,[0],[0]
"I thank Ben Rossi, Vinod Valsalam, Rhys Ulerich, and Eric Allen for many useful discussions and Larry Rudolph and Steve Heller for their feedback on the paper.",Acknowledgments,[0],[0]
"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization.",abstractText,[0],[0]
"In this work, we examine to what extent this tension exists, by exploring if it is possible to generalize by memorizing alone.",abstractText,[0],[0]
"Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10.",abstractText,[0],[0]
"Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data.",abstractText,[0],[0]
The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.,abstractText,[0],[0]
Learning and Memorization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 332–344 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1031",text,[0],[0]
"There is a growing interest in automated processing of historical documents, as evidenced by the growing field of digital humanities and the increasing number of digitally available collections of historical documents.",1 Introduction,[0],[0]
"A common approach to deal with the high amount of variance often found in this type of data is to perform spelling normalization (Piotrowski, 2012), which is the mapping of historical spelling variants to standardized/modernized forms (e.g. vnd→ und ‘and’).
",1 Introduction,[0],[0]
"Training data for supervised learning of historical text normalization is typically scarce, making it a challenging task for neural architectures, which typically require large amounts of labeled data.",1 Introduction,[0],[0]
"Nevertheless, we explore framing the
spelling normalization task as a character-based sequence-to-sequence transduction problem, and use encoder–decoder recurrent neural networks (RNNs) to induce our transduction models.",1 Introduction,[0],[0]
"This is similar to models that have been proposed for neural machine translation (e.g., Cho et al. (2014)), so essentially, our approach could also be considered a specific case of character-based neural machine translation.
",1 Introduction,[0],[0]
"By basing our model on individual characters as input, we keep the vocabulary size small, which in turn reduces the model’s complexity and the amount of data required to train it effectively.",1 Introduction,[0],[0]
Using an encoder–decoder architecture removes the need for an explicit character alignment between historical and modern wordforms.,1 Introduction,[0],[0]
"Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models.
",1 Introduction,[0],[0]
"We propose several architectures, including multi-task learning architectures taking advantage of the auxiliary data, and evaluate them across 44 small datasets from Early New High German.
",1 Introduction,[0],[0]
Contributions,1 Introduction,[0],[0]
"Our contributions are as follows:
• We are, to the best of our knowledge, the first to propose and evaluate encoder-decoder architectures for historical text normalization.
",1 Introduction,[0],[0]
"• We evaluate several such architectures across 44 datasets of Early New High German.
",1 Introduction,[0],[0]
"• We show that such architectures benefit from bidirectional encoding, beam search, and attention.
",1 Introduction,[0],[0]
"• We also show that MTL with pronunciation as an auxiliary task improves the performance of architectures without attention.
",1 Introduction,[0],[0]
"332
• We analyze the above architectures and show that the MTL architecture learns attention from the auxiliary task, making the attention mechanism largely redundant.
",1 Introduction,[0],[0]
"• We make our implementation publicly available at https://bitbucket.org/ mbollmann/acl2017.
",1 Introduction,[0],[0]
"In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning.",1 Introduction,[0],[0]
"Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.1",2 Datasets,[0],[0]
"The corpus is a collection of manuscripts and prints of the same core text, a religious treatise.",2 Datasets,[0],[0]
"Although the texts are semi-parallel and share some vocabulary, they were written in different time periods (between the 14th and 16th century) as well as different dialectal regions, and show quite diverse spelling characteristics.",2 Datasets,[0],[0]
"For example, the modern German word Frau ‘woman’ can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fraüwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.2
All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in Krasselt et al. (2015).",2 Datasets,[0],[0]
"For our experiments, we excluded texts from the corpus that are shorter than 4,000 tokens, as well as a few for which annotations were not yet available at the time of writing (mostly Low German and Dutch versions).",2 Datasets,[0],[0]
"Nonetheless, the remaining 44 texts are still quite short for machine-learning standards, ranging from about 4,200 to 13,200 tokens, with an average length of 7,350 tokens.
",2 Datasets,[0],[0]
"For all texts, we removed tokens that consisted solely of punctuation characters.",2 Datasets,[0],[0]
"We also lowercase all characters, since it helps keep the size of the vocabulary low, and uppercasing of words is usually not very consistent in historical texts.",2 Datasets,[0],[0]
"Tokenization was not an issue for pre-processing these texts, since modern token boundaries have already been marked by the transcribers.
",2 Datasets,[0],[0]
"1https://www.linguistics.rub.de/ anselm/
2We refer to individual texts using the same internal IDs that are found in the Anselm corpus (cf.",2 Datasets,[0],[0]
"the website).
",2 Datasets,[0],[0]
Grapheme-to-phoneme mappings We use learning to pronounce as our auxiliary task.,2 Datasets,[0],[0]
This task consists of learning mappings from sequences of graphemes to the corresponding sequences of phonemes.,2 Datasets,[0],[0]
"We use the German part of the CELEX lexical database (Baayen et al., 1995), particularly the database of phonetic transcriptions of German wordforms.",2 Datasets,[0],[0]
"The database contains a total of 365,530 wordforms with transcriptions in DISC format, which assigns one character to each distinct phonological segment (including affricates and diphthongs).",2 Datasets,[0],[0]
"For example, the word Jungfrau ‘virgin’ is represented as ’jUN-frB.",2 Datasets,[0],[0]
"We propose several architectures that are extensions of a base neural network architecture, closely following the sequence-to-sequence model proposed by Sutskever et al. (2014).",3.1 Base model,[0],[0]
"It consists of the following:
• an embedding layer that maps one-hot input vectors to dense vectors;
• an encoder RNN that transforms the input sequence to an intermediate vector of fixed dimensionality;
• a decoder RNN whose hidden state is initialized with the intermediate vector, and which is fed the output prediction of one timestep as the input for the next one; and
• a final dense layer with a softmax activation which takes the decoder’s output and generates a probability distribution over the output classes at each timestep.
",3.1 Base model,[0],[0]
"For the encoder/decoder RNNs, we use long short-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997).",3.1 Base model,[0],[0]
"LSTMs are designed to allow recurrent networks to better learn long-term dependencies, and have proven advantageous to standard RNNs on many tasks.",3.1 Base model,[0],[0]
"We found no significant advantage from stacking multiple LSTM layers for our task, so we use the simplest competitive model with only a single LSTM unit for both encoder and decoder.
",3.1 Base model,[0],[0]
"By using this encoder–decoder model, we avoid the need to generate explicit alignments between the input and output sequences, which would bring up the question of how to deal with input/output
pairs of different lengths.",3.1 Base model,[0],[0]
"Another important property is that the model does not start to generate any output until it has seen the full input sequence, which in theory allows it to learn from any part of the input, without being restricted to fixed context windows.",3.1 Base model,[0],[0]
An example illustration of the unrolled network is shown in Fig. 1.,3.1 Base model,[0],[0]
"During training, the encoder inputs are the historical wordforms, while the decoder inputs correspond to the correct modern target wordforms.",3.2 Training,[0],[0]
"We then train each model by minimizing the crossentropy loss across all output characters; i.e., if y = (y1, ..., yn) is the correct output word (as a list of one-hot vectors of output characters) and ŷ",3.2 Training,[0],[0]
"= (ŷ1, ..., ŷn) is the model’s output, we minimize the mean loss−∑ni=1 yi log ŷi over all training samples.",3.2 Training,[0],[0]
"For the optimization, we use the Adam algorithm (Kingma and Ba, 2015) with a learning rate of 0.003.
",3.2 Training,[0],[0]
"To reduce computational complexity, we also set a maximum word length of 14, and filter all training samples where either the input or output word is longer than 14 characters.",3.2 Training,[0],[0]
"This only affects 172 samples across the whole dataset, and is only done during training.",3.2 Training,[0],[0]
"In other words, we evaluate our models across all the test examples.",3.2 Training,[0],[0]
"For prediction, our base model generates output character sequences in a greedy fashion, selecting the character with the highest probability at each timestep.",3.3 Decoding,[0],[0]
"This works fairly well, but the greedy approach can yield suboptimal global picks, in which each individual character is sensibly derived from the input, but the overall word is non-
sensical.",3.3 Decoding,[0],[0]
"We therefore also experiment with beam search decoding, setting the beam size to 5.
",3.3 Decoding,[0],[0]
"Finally, we also experiment with using a lexical filter during the decoding step.",3.3 Decoding,[0],[0]
"Here, before picking the next 5 most likely characters during beam search, we remove all characters that would lead to a string not covered by the lexicon.",3.3 Decoding,[0],[0]
This is again intended to reduce the occurrence of nonsensical outputs.,3.3 Decoding,[0],[0]
"For the lexicon, we use all word forms from CELEX (cf. Sec. 2) plus the target word forms from the training set.3",3.3 Decoding,[0],[0]
"In our base architecture, we assume that we can decode from a single vector encoding of the input sequence.",3.4 Attention,[0],[0]
"This is a strong assumption, especially with long input sequences.",3.4 Attention,[0],[0]
Attention mechanisms give us more flexibility.,3.4 Attention,[0],[0]
"The idea is that instead of encoding the entire input sequence into a fixedlength vector, we allow the decoder to “attend” to different parts of the input character sequence at each time step of the output generation.",3.4 Attention,[0],[0]
"Importantly, we let the model learn what to attend to based on the input sequence and what it has produced so far.
",3.4 Attention,[0],[0]
Our implementation is identical to the decoder with soft attention described by Xu et al. (2015).,3.4 Attention,[0],[0]
"If a = (a1, ..., an) is the encoder’s output and ht is the decoder’s hidden state at timestep t, we first calculate a context vector ẑt as a weighted combination of the output vectors ai:
ẑt =
n∑
i=1
αiai (1)
3We observe that due to this filtering, we cannot reach 2.25% of the targets in our test set, most of which are Latin word forms.
",3.4 Attention,[0],[0]
"The weights αi are derived by feeding the encoder’s output and the decoder’s hidden state from the previous timestep into a multilayer perceptron, called the attention model (fatt):
α = softmax(fatt(a, ht−1))",3.4 Attention,[0],[0]
"(2)
We then modify the decoder by conditioning its internal states not only on the previous hidden state ht−1 and the previously predicted output character yt−1, but also on the context vector ẑt:
it = σ(Wi[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bi)
ft = σ(Wf [ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bf )
ot = σ(Wo[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bo)
gt = tanh(Wg[ht−1, yt−1, ẑt] + bg)
ct = ft ct−1",3.4 Attention,[0],[0]
"+ it gt ht = ot tanh(ct)
(3)
",3.4 Attention,[0],[0]
"In Eq. 3, we follow the traditional LSTM description consisting of input gate it, forget gate ft, output gate ot, cell state ct and hidden state ht, where W and b are trainable parameters.
",3.4 Attention,[0],[0]
"For all experiments including an attentional decoder, we use a bi-directional encoder, comprised of one LSTM layer that reads the input sequence normally and another LSTM layer that reads it backwards, and attend over the concatenated outputs of these two layers.
",3.4 Attention,[0],[0]
"While a precise alignment of input and output sequences is sometimes difficult, most of the time the sequences align in a sequential order, which can be exploited by an attentional component.",3.4 Attention,[0],[0]
"Finally, we introduce a variant of the base architecture, with or without beam search, that does multi-task learning (Caruana, 1993).",3.5 Multi-task learning,[0],[0]
"The multitask architecture only differs from the base architecture in having two classifier functions at the outer layer, one for each of our two tasks.",3.5 Multi-task learning,[0],[0]
Our auxiliary task is to predict a sequence of phonemes as the correct pronunciation of an input sequence of graphemes.,3.5 Multi-task learning,[0],[0]
"This choice is motivated by the relationship between phonology and orthography, in particular the observation that spelling variation often stems from phonological variation.
",3.5 Multi-task learning,[0],[0]
"We train our multi-task learning architecture by alternating between the two tasks, sampling one instance of the auxiliary task for each training sample of the main task.",3.5 Multi-task learning,[0],[0]
"We use the encoderdecoder to generate a corresponding output se-
quence, whether a modern word form or a pronunciation.",3.5 Multi-task learning,[0],[0]
"Doing so, we suffer a loss with respect to the true output sequence and update the model parameters.",3.5 Multi-task learning,[0],[0]
"The update for a sample from a specific task affects the parameters of corresponding classifier function, as well as all the parameters of the shared hidden layers.",3.5 Multi-task learning,[0],[0]
We used a single manuscript (B) for manually evaluating and setting the hyperparameters.,3.6 Hyperparameters,[0],[0]
This manuscript is left out of the averages reported below.,3.6 Hyperparameters,[0],[0]
"We believe that using a single manuscript for development, and using the same hyperparameters across all manuscripts, is more realistic, as we often do not have enough data in historical text normalization to reliably tune hyperparameters.
",3.6 Hyperparameters,[0],[0]
"For the final evaluation, we set the size of the embedding and the recurrent LSTM layers to 128, applied a dropout of 0.3 to the input of each recurrent layer, and trained the model on mini-batches with 50 samples each for a total of 50 epochs (in the multi-task learning setup, mini-batches contain 50 samples of each task, and epochs are counted by the size of the training set for the main task only).",3.6 Hyperparameters,[0],[0]
All these parameters were set on the B manuscript alone.,3.6 Hyperparameters,[0],[0]
"We implemented all of the models in Keras (Chollet, 2015).",3.7 Implementation,[0],[0]
Any parameters not explicitly described here were left at their default values in Keras v1.0.8.,3.7 Implementation,[0],[0]
"We split up each text into three parts, using 1,000 tokens each for a test set and a development set (that is not currently used), and the remainder of the text (between 2,000 and 11,000 tokens) for training.",4 Evaluation,[0],[0]
"We then train and evaluate on each of the 43 texts (excluding the B text that was used for hyper-parameter tuning) individually.
",4 Evaluation,[0],[0]
Baselines We compare our architectures to several competitive baselines.,4 Evaluation,[0],[0]
"Our first baseline is an averaged perceptron model trained to predict output character n-grams for each input character, after using Levenshtein alignment with generated segment distances (Wieling et al., 2009, Sec. 3.3) to align input and output characters.",4 Evaluation,[0],[0]
"Our second baseline uses the same alignment, but trains a
deep bi-LSTM sequential tagger, following Bollmann and Søgaard (2016).",4 Evaluation,[0],[0]
We evaluate this tagger using both standard and multi-task learning.,4 Evaluation,[0],[0]
"Finally, we compare our model to the rule-based and Levenshtein-based algorithms provided by the Norma tool (Bollmann, 2012).4",4 Evaluation,[0],[0]
We use word-level accuracy as our evaluation metric.,4.1 Word accuracy,[0],[0]
"While we also measure character-level metrics, minor differences on character level can cause large differences in downstream applications, so we believe that perfectly matching the output sequences is more useful.",4.1 Word accuracy,[0],[0]
"Average scores across all 43 texts are presented in Table 1 (see Appendix A for individual scores).
",4.1 Word accuracy,[0],[0]
We first see that almost all our encoder-decoder architectures perform significantly better than the four state-of-the-art baselines.,4.1 Word accuracy,[0],[0]
"All our architectures perform better than Norma and the averaged perceptron, and all the MTL architectures outperform Bollmann and Søgaard (2016).
",4.1 Word accuracy,[0],[0]
"We also see that beam search, filtering, and attention lead to cumulative gains in the context of the single-task architecture – with the best architecture outperforming the state-of-the-art by almost 3% in absolute terms.
",4.1 Word accuracy,[0],[0]
"For our multi-task architecture, we also observe gains when we add beam search and filtering, but
4https://github.com/comphist/norma
importantly, adding attention does not help.",4.1 Word accuracy,[0],[0]
"In fact, attention hurts the performance of our multitask architecture quite significantly.",4.1 Word accuracy,[0],[0]
"Also note that the multi-task architecture without attention performs on-par with the single-task architecture with attention.
",4.1 Word accuracy,[0],[0]
"We hypothesize that the reason for this pattern, which is not only observed in the average scores in Table 1, but also quite consistent across the individual results in Appendix A, is that our multi-task learning already learns how to focus attention.
",4.1 Word accuracy,[0],[0]
"This is the hypothesis that we will try to validate in Sec. 5: That multi-task learning can induce strategies for focusing attention comparable to attention strategies for recurrent neural networks.
",4.1 Word accuracy,[0],[0]
Sample predictions A small selection of predictions from our models is shown in Table 2.,4.1 Word accuracy,[0],[0]
"They serve to illustrate the effects of the various settings; e.g., the base model with greedy search tends to produce more nonsense words (ters, ünsget) than the others.",4.1 Word accuracy,[0],[0]
"Using a lexical filter helps the most in this regard: the base model with filtering correctly normalizes ergieng to erging ‘(he) fared’, while decoding without a filter produces the non-word erbiggen.",4.1 Word accuracy,[0],[0]
"Even for herczenlichen (modern herzlichen ‘heartfelt’), where no model finds the correct target form, only the model with filtering produces a somewhat reasonable alternative (herzgeliebtes ‘heartily loved’).
",4.1 Word accuracy,[0],[0]
"In some cases (such as gewarnet ‘warned’),
only the models with attention or multi-task learning produce the correct normalization, but even when they are wrong, they often agree on the prediction (e.g. dicke, herzel).",4.1 Word accuracy,[0],[0]
We will investigate this property further in Sec. 5.,4.1 Word accuracy,[0],[0]
"To gain further insights into our model, we created t-SNE projections (Maaten and Hinton, 2008) of vector representations learned on the M4 text.
",4.2 Learned vector representations,[0],[0]
Fig. 2 shows the learned character embeddings.,4.2 Learned vector representations,[0],[0]
"In the representations from the base model (Fig. 2a), characters that are often normalized to the same target character are indeed grouped closely together: e.g., historical <v> and <u> (and, to a smaller extent, <f>) are often used interchangeably in the M4 text.",4.2 Learned vector representations,[0],[0]
Note the wide separation of <n>,4.2 Learned vector representations,[0],[0]
"and <m>, which is a feature of M4 that does not hold true for all of the texts, as these do not always display a clear distinction between nasals.",4.2 Learned vector representations,[0],[0]
"On the other hand, the MTL model shows a better generalization of the training data (Fig. 2b): here, <u> is grouped closer to other vowel characters and far away from <v>/<f>.",4.2 Learned vector representations,[0],[0]
"Also, <n> and <m> are now in close proximity.
",4.2 Learned vector representations,[0],[0]
We can also visualize the internal word representations that are produced by the encoder (Fig. 3).,4.2 Learned vector representations,[0],[0]
"Here, we chose words that demonstrate the interchangeable use of <u> and <v>.",4.2 Learned vector representations,[0],[0]
"Historical vnd, vns, vmb become modern und, uns, um, changing the <v> to <u>.",4.2 Learned vector representations,[0],[0]
"However, the representation of vmb learned by the base model is closer to forms like von, vor, uor, all starting with <v> in the target normalization.",4.2 Learned vector representations,[0],[0]
"In the MTL model, however, these examples are indeed clustered together.",4.2 Learned vector representations,[0],[0]
Table 1 shows that models which employ either an attention mechanism or multi-task learning obtain similar improvements in word accuracy.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"However, we observe a decline in word accuracy for models that combine multi-task learning with attention.
",5 Analysis: Multi-task learning helps focus attention,[0],[0]
"A possible interpretation of this counterintuitive pattern might be that attention and MTL, to some degree, learn similar functions of the input data, a conjecture by Caruana (1998).",5 Analysis: Multi-task learning helps focus attention,[0],[0]
We put this hypothesis to the test by closely investigating properties of the individual models below.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"First, we are interested in the weight parameters of the final layer that transforms the decoder output to class probabilities.",5.1 Model parameters,[0],[0]
"We consider these parameters for our standard encoder-decoder model and compare them to the weights that are learned by the attention and multi-task models, respectively.5
Note that hidden layer parameters are not necessarily comparable across models, but with a fixed seed, differences in parameters over a reference model may be (and are, in our case).",5.1 Model parameters,[0],[0]
"With a fixed seed, and iterating over data points in the same order, it is conceivable the two non-baselines end up in roughly the same alternative local optimum (or at least take comparable routes).
",5.1 Model parameters,[0],[0]
"We observe that the weight differences between the standard and the attention model correlate with the differences between the standard and multitask model by a Pearson’s r of 0.346, averaged across datasets, with a standard deviation of 0.315; on individual datasets, correlation coefficient is as
5For the multi-task models, this analysis disregards those dimensions that do not correspond to classes in the main task.
high as 96.",5.1 Model parameters,[0],[0]
Figure 4 illustrates these highly parallel weight changes for the different models when trained on the N4 dataset.,5.1 Model parameters,[0],[0]
"Next, we compare the effect that employing either an attention mechanism or multi-task learning has on the actual output of our system.",5.2 Final output,[0],[0]
"We find that out of the 210.9 word errors that the base model produces on average across all test sets (comprising 1,000 tokens each), attention resolves 47.7, while multi-task learning resolves an average of 45.4 errors.",5.2 Final output,[0],[0]
"Crucially, the overlap of errors that are resolved by both the attention and the MTL model amounts to 27.7 on average.
",5.2 Final output,[0],[0]
"Attention and multi-task also introduce new errors compared to the base model (26.6 and 29.5 per test set, respectively), and again we can observe a relatively high agreement of the models (11.8 word errors are introduced by both models).
",5.2 Final output,[0],[0]
"Finally, the attention and multi-task models display a word-level agreement of κ=0.834 (Cohen’s kappa), while either of these models is less strongly correlated with the base model (κ=0.817 for attention and κ=0.814 for multi-task learning).",5.2 Final output,[0],[0]
Our last analysis regards the saliency of the input timesteps with respect to the predictions of our models.,5.3 Saliency analysis,[0],[0]
We follow Li et al. (2016) in calculating first-derivative saliency for given input/output pairs and compare the scores from the different models.,5.3 Saliency analysis,[0],[0]
"The higher the saliency of an input timestep, the more important it is in determining the model’s prediction at a given output timestep.",5.3 Saliency analysis,[0],[0]
"Therefore, if two models produce similar saliency
matrices for a given input/output pair, they have learned to focus on similar parts of the input during the prediction.",5.3 Saliency analysis,[0],[0]
"Our hypothesis is that the attentional and the multi-task learning model should be more similar in terms of saliency scores than either of them compared to the base model.
",5.3 Saliency analysis,[0.9588288477609425],"['Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.']"
Figure 5 shows a plot of the saliency matrices generated from the word pair czeychen – zeichen ‘sign’.,5.3 Saliency analysis,[0],[0]
"Here, the scores for the attentional and the MTL model indeed correlate by ρ = 0.615, while those for the base model do not correlate with either of them.",5.3 Saliency analysis,[0],[0]
"A systematic analysis across 19,000 word pairs (where all models agree on the output) shows that this effect only holds for longer input sequences (≥ 7 characters), with a mean ρ = 0.303 (±0.177) for attentional vs. MTL model, while the base model correlates with either of them by ρ < 0.21.",5.3 Saliency analysis,[0],[0]
"Many traditional approaches to spelling normalization of historical texts use edit distances or some form of character-level rewrite rules, handcrafted (Baron and Rayson, 2008) or learned automatically (Bollmann, 2013; Porta et al., 2013).
",6 Related Work,[0],[0]
"A more recent approach is based on characterbased statistical machine translation applied to historical text (Pettersson et al., 2013; SánchezMartínez et al., 2013; Scherrer and Erjavec, 2013; Ljubešić et al., 2016) or dialectal data (Scherrer and Ljubešić, 2016).",6 Related Work,[0],[0]
"This is conceptually very similar to our approach, except that we substitute the classical SMT algorithms for neural networks.",6 Related Work,[0],[0]
"Indeed, our models can be seen as a form of character-based neural MT (Cho et al., 2014).
",6 Related Work,[0],[0]
"Neural networks have rarely been applied to
historical spelling normalization so far.",6 Related Work,[0],[0]
Azawi et al. (2013) normalize old Bible text using bidirectional LSTMs with a layer that performs alignment between input and output wordforms.,6 Related Work,[0],[0]
"Bollmann and Søgaard (2016) also use bi-LSTMs to frame spelling normalization as a characterbased sequence labelling task, performing character alignment as a preprocessing step.
",6 Related Work,[0],[0]
"Multi-task learning was shown to be effective for a variety of NLP tasks, such as POS tagging, chunking, named entity recognition (Collobert et al., 2011) or sentence compression (Klerke et al., 2016).",6 Related Work,[0],[0]
"It has also been used in encoderdecoder architectures, typically for machine translation (Dong et al., 2015; Luong et al., 2016), though so far not with attentional decoders.",6 Related Work,[0.9569958955881707],"['However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.']"
"We presented an approach to historical spelling normalization using neural networks with an encoder-decoder architecture, and showed that it consistently outperforms several existing baselines.",7 Conclusion and Future Work,[0],[0]
"Encouragingly, our work proves to be fully competitive with the sequence-labeling approach by Bollmann and Søgaard (2016), without requiring a prior character alignment.
",7 Conclusion and Future Work,[0],[0]
"Specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task.",7 Conclusion and Future Work,[0],[0]
We included a multifaceted analysis of the effects that MTL introduces to our models and the resemblance that it bears to attention mechanisms.,7 Conclusion and Future Work,[0],[0]
"We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention.
",7 Conclusion and Future Work,[0],[0]
"Finally, many improvements to the presented approach are conceivable, most notably introducing some form of token context to the model.",7 Conclusion and Future Work,[0],[0]
"Currently, we only consider word forms in isolation, which is problematic for ambiguous cases (such as jn, which can normalize to in ‘in’ or ihn ‘him’) and conceivably makes the task harder for others.",7 Conclusion and Future Work,[0],[0]
Reranking the predictions with a language model could be one possible way to improve on this.,7 Conclusion and Future Work,[0],[0]
Ljubešić,7 Conclusion and Future Work,[0],[0]
"et al. (2016), for example, experiment with segment-based normalization, using a character-based SMT model with character input derived from segments (essentially, token ngrams) instead of single tokens, which also intro-
duces context.",7 Conclusion and Future Work,[0],[0]
"Such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text.",7 Conclusion and Future Work,[0],[0]
"Marcel Bollmann was supported by Deutsche Forschungsgemeinschaft (DFG), Grant DI 1558/4.",Acknowledgments,[0],[0]
This research is further supported by ERC Starting Grant LOWLANDS,Acknowledgments,[0],[0]
"No. 313695, as well as by Trygfonden.",Acknowledgments,[0],[0]
"For interested parties, we provide our full evaluation results for each single text in our dataset.",A Supplementary Material,[0],[0]
"Table 3 shows token counts, a rough classification of each text’s dialectal region, and the results for the baseline methods.",A Supplementary Material,[0],[0]
Table 4 presents the full results for our encoder-decoder models.,A Supplementary Material,[0],[0]
Automated processing of historical texts often relies on pre-normalization to modern word forms.,abstractText,[0],[0]
"Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task.",abstractText,[0],[0]
"We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-theart by an absolute 2% increase in performance.",abstractText,[0],[0]
We analyze the induced models across 44 different texts from Early New High German.,abstractText,[0],[0]
"Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms.",abstractText,[0],[0]
"This, we believe, is an important step toward understanding how MTL works.",abstractText,[0],[0]
Learning attention for historical text normalization by learning to pronounce,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 313–322 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Today, machine learning is centered around algorithms that can be trained on available taskspecific labeled and unlabeled training samples.",1 Introduction,[0],[0]
"Although learning paradigms like Transfer Learning (Pan and Yang, 2010) attempt to incorporate
∗equal contribution †Main work done during internship at Accenture Technol-
ogy Labs
knowledge from one task into another, these techniques are limited in scalability and are specific to the task at hand.",1 Introduction,[0],[0]
"On the other hand, humans have the intrinsic ability to elicit required past knowledge from the world on demand and infuse it with newly learned concepts to solve problems.
",1 Introduction,[0],[0]
"The question that we address in this paper is the following: Is it possible to develop learning models that can be trained in a way that it is able to infuse a general body of world knowledge for prediction apart from learning based on training data?
",1 Introduction,[0],[0]
"By world knowledge, we mean structured general purpose knowledge that need not be domain specific.",1 Introduction,[0],[0]
"Knowledge Graphs (Nickel et al., 2016a) are a popular source of such structured world knowledge.",1 Introduction,[0],[0]
"Knowledge Graphs represent information in the form of fact triplets, consisting of a subject entity, relation and object entity (example: <Italy, capital, Rome>).",1 Introduction,[0],[0]
The entities represent the nodes of the graph and their relations act as edges.,1 Introduction,[0],[0]
"A fact triple (subject entity, relation, object relation) is represented as (h, r, t).",1 Introduction,[0],[0]
"Practical knowledge bases congregate information from secondary databases or extract facts from unstructured text using various statistical learning mechanisms, examples of such systems are NELL (Mitchell et al., 2015) and DeepDive (Niu
313
et al., 2012).",1 Introduction,[0],[0]
"There are human created knowledge bases as well, like Freebase (FB15k) (Bollacker et al., 2008) and WordNet (Miller et al., 1990).",1 Introduction,[0],[0]
"The knowledge present in these knowledge bases includes common knowledge and partially covers common-sense knowledge and domain knowledge (Song and Roth, 2017).",1 Introduction,[0],[0]
"Knowledge Graphs and Knowledge Bases are conceptually equivalent for our purpose and we will use the name interchangeably in this paper.
",1 Introduction,[0],[0]
We illustrate the significance of world knowledge using a few examples.,1 Introduction,[0],[0]
"For the example of a Natural Language Inference (NLI) problem (MacCartney, 2009), consider the two following statements, A: The couple is walking on the sea shore and B: The man and woman are wide awake.",1 Introduction,[0],[0]
"Here, for a learning model to infer B from A, it should have access to the common knowledge that “The man and woman and The couple means the same” since this information may not be specific for a particular inference.",1 Introduction,[0],[0]
"Further, it is not possible for a model to learn all such correlations from just the labeled training data available for the task.
",1 Introduction,[0],[0]
"Consider another example of classifying the news snippet, Donald Trump offered his condolences towards the hurricane victims and their families in Texas.",1 Introduction,[0],[0]
"We cannot classify it as a political news unless we know the facts <Donald Trump, president, United States> and <Texas, state, United States>.",1 Introduction,[0],[0]
"We posit that machine learning models, apart from training them on data with the ground-truth can also be trained to fetch relevant information from structured knowledge bases in order to enhance their performance.
",1 Introduction,[0],[0]
"In this work, we propose a deep learning model that can extract relevant support facts on demand from a knowledge base (Mitchell et al., 2015) and incorporate it in the feature space along with the features learned from the training data (shown in Figure 1).",1 Introduction,[0],[0]
"This is a challenging task, as knowledge bases typically have millions of fact triples.",1 Introduction,[0],[0]
Our proposed model involves a deep learning mechanism to jointly model this look-up scheme along with the task specific training of the model.,1 Introduction,[0],[0]
The look-up mechanism and model is generic enough so that it can be augmented to any task specific learning model to boost the learning performance.,1 Introduction,[0],[0]
"In this paper, we have established superior performance of the proposed KG-augmented models
over vanilla model on text classification and natural language inference.
",1 Introduction,[0],[0]
"Although there is a plethora of work on knowledge graph representation (Nickel et al., 2016a)",1 Introduction,[0],[0]
"(Mitchell et al., 2015) (Niu et al., 2012) from natural language text, no attempt to augment learning models with knowledge graph information have been done.",1 Introduction,[0],[0]
To the best of our knowledge this is the first attempt to incorporate world knowledge from a knowledge base for learning models.,1 Introduction,[0],[0]
Knowledge Graph entities/relations need to be encoded into a numerical representation for processing.,2 Knowledge Graph Representations,[0],[0]
"Before describing the model, we provide a brief overview of graph encoding techniques.",2 Knowledge Graph Representations,[0],[0]
"Various KG embedding techniques can be classified at a high level into: Structure-based embeddings and Semantically-enriched embeddings.
",2 Knowledge Graph Representations,[0],[0]
"Structure-based embeddings: TransE (Bordes et al., 2013) is the introductory work on knowledge graph representation, which translated subject entity to object entity using one-dimensional relation vector (h + r = t).",2 Knowledge Graph Representations,[0],[0]
"Variants of the TransE (Bordes et al., 2013) model uses translation of the entity vectors over relation specific subspaces.",2 Knowledge Graph Representations,[0],[0]
"TransH (Wang et al., 2014b) introduced the relation-specific hyperplane to translate the entities.",2 Knowledge Graph Representations,[0],[0]
Similar work utilizing only the structure of the graph include ManifoldE,2 Knowledge Graph Representations,[0],[0]
"(Xiao et al., 2015b), TransG (Xiao et al., 2015a), TransD",2 Knowledge Graph Representations,[0],[0]
"(Ji et al., 2015), TransM (Fan et al., 2014), HolE (Nickel et al., 2016b) and ProjE (Shi and Weninger, 2017).
",2 Knowledge Graph Representations,[0],[0]
Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic information.,2 Knowledge Graph Representations,[0],[0]
"Neural Tensor Network(NTN) (Socher et al., 2013) was the pioneering work in this field which initialized entity vectors with the average word embeddings followed by tensor-based operations.",2 Knowledge Graph Representations,[0],[0]
"Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017).",2 Knowledge Graph Representations,[0],[0]
"DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model.",2 Knowledge Graph Representations,[0],[0]
Pretrained word2vec,2 Knowledge Graph Representations,[0],[0]
"(Mikolov et al., 2013) is used to form the entity representation by passing through a Convolutional Neural Network (CNN) (Kim, 2014) architecture constraining the relationships to hold.
",2 Knowledge Graph Representations,[0],[0]
"In our experiments we have used the DKRL (Xie et al., 2016) encoding scheme as it emphasizes on the semantic description of the text.",2 Knowledge Graph Representations,[0],[0]
"Moreover, DKRL fundamentally uses TransE (Bordes et al., 2013) method for encoding structural information.",2 Knowledge Graph Representations,[0],[0]
"Therefore, we can retrieve relevant entities & relation and obtain the complete the fact using t = h+",2 Knowledge Graph Representations,[0],[0]
r.,2 Knowledge Graph Representations,[0],[0]
"This reduces the complexity of fact retrieval as the number of entities/relations is much less compared to the number of facts, thus making the retrieval process faster.",2 Knowledge Graph Representations,[0],[0]
"Conventional supervised learning models with parameters Θ, given training data x and label y, tries to maximize the following function
max Θ
P (y|x,Θ)
The optimized parameters Θ are given as,
Θ = argmax Θ
logP (y|x,Θ)
",3 The Proposed Model,[0],[0]
"In this work, we propose to augment the supervised learning process by incorporation of world knowledge features xw.",3 The Proposed Model,[0],[0]
"The world knowledge features are retrieved using the data x, using a separate model where, xw = F (x,Θ(2)).",3 The Proposed Model,[0],[0]
"Thus, our modified objective function can be expressed as
max Θ
P (y|x, xw,Θ(1))
where, Θ = {Θ(1),Θ(2)}.",3 The Proposed Model,[0],[0]
"The optimized parameters can be obtained using the equation
Θ = argmax Θ
logP (y|x, F (x,Θ(2)),Θ(1))
",3 The Proposed Model,[0],[0]
The subsequent sections focus on the formulation of the function F which is responsible for fact triple retrieval using the data sample x.,3 The Proposed Model,[0],[0]
"Here it is important to note that, we are not assuming any structural form for P based on F .",3 The Proposed Model,[0],[0]
"So the method is generic and applicable to augment any supervised learning setting with any form for P , only constraint being P should be such that the error gradient can be computed with respect to F .",3 The Proposed Model,[0],[0]
"In the experiments we have used softmax using the LSTM (Greff et al., 2015) encodings of the input as the form for P .",3 The Proposed Model,[0],[0]
"As for F , we use soft attention (Luong et al., 2015; Bahdanau et al., 2014) using the LSTM encodings of the input and appropriate representations of the fact(s).",3 The Proposed Model,[0],[0]
"Based on the
representation used for the facts, we propose two models (a) Vanilla Model (b) Convolution-based entity/relation cluster representation, for fact retrieval in the subsequent sections.",3 The Proposed Model,[0],[0]
"The entities and relationships of KG are encoded using DKRL, explained earlier.",3.1 Vanilla Model,[0],[0]
Let ei ∈ Rm stand for the encoding of the entity i and rj ∈ Rm stands for jth relationship in the KG.,3.1 Vanilla Model,[0],[0]
"The input text in the form of concatenated word vectors, x = (x1, x2, . . .",3.1 Vanilla Model,[0],[0]
", xT ) is first encoded using an LSTM (Greff et al., 2015) module as follows,
ht = f(xt, ht−1)
and
o = 1
T
T∑
t=1
ht,
ht ∈",3.1 Vanilla Model,[0],[0]
"Rn is the hidden state of the LSTM at time t, f is a non-linear function and T is the sequence length.",3.1 Vanilla Model,[0],[0]
"Then a context vector is formed from o as follows,
C = ReLU(oTW ),
where, W ∈ Rn×m represent the weight parameters.",3.1 Vanilla Model,[0],[0]
"The same procedure is duplicated with separate LSTMs to form two seperate context vectors, one for entity retrieval (CE) and one for relationship retrieval (CR).
",3.1 Vanilla Model,[0],[0]
"As the number of fact triples in a KG is in the order of millions in the vanilla model, we resort to generating attention over the entity and relation space separately.",3.1 Vanilla Model,[0],[0]
The fact is then formed using the retrieved entity and relation.,3.1 Vanilla Model,[0],[0]
"The attention for the entity, ei using entity context vector is given by
αei = exp(CTEei)",3.1 Vanilla Model,[0],[0]
|E|∑ j=0,3.1 Vanilla Model,[0],[0]
"exp(CTEej)
where |E| is the number of entities in the KG.",3.1 Vanilla Model,[0],[0]
"Similarly the attention for a relation vector ri is computed as
αri = exp(CTRri) |R|∑ j=0 exp(CTRrj)
where |R| is the number of relations in the KG.",3.1 Vanilla Model,[0],[0]
"The final entity and relation vector retrieval is computed by the weighted sum with the attention
values of individual retrieved entity/relation vectors.
",3.1 Vanilla Model,[0],[0]
"e =
|E|∑
i=0
αeiei r =
|R|∑
i=0
αriri
Figure 2 shows the schematic diagram for entity/relation retrieval.",3.1 Vanilla Model,[0],[0]
"After the final entity and relation vectors are computed, we look forward to completion of the fact triple.",3.1 Vanilla Model,[0],[0]
The KG embedding technique used for the experiment is DKRL which inherently uses the TransE model assumption (h+r ≈ t).,3.1 Vanilla Model,[0],[0]
"Therefore, using the subject entity and relation we form the object entity as t = e+r.",3.1 Vanilla Model,[0],[0]
Thus the fact triplet retrieved is F =,3.1 Vanilla Model,[0],[0]
"[e, r, e + r], where F ∈ R3m. This retrieved fact information is concatenated along with the context vector (C) of input x obtained using LSTM module.",3.1 Vanilla Model,[0],[0]
"The final classification label y is computed as follows,
F ′ = ReLU(FTV )
y = softmax([F ′",3.1 Vanilla Model,[0],[0]
": C]TU) where, V ∈ R3m×u and U ∈ R2u×u are model parameters to be learned.",3.1 Vanilla Model,[0],[0]
y is used to compute the cross entropy loss.,3.1 Vanilla Model,[0],[0]
"We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent (Bottou, 2012).",3.1 Vanilla Model,[0],[0]
"The final prediction y, now includes information from both dataset specific samples and world knowledge to aid in en-
hanced performance.",3.1 Vanilla Model,[0],[0]
While jointly training the attention mechanism tunes itself to retrieve relevant facts that are required to do the final classification.,3.1 Vanilla Model,[0],[0]
The vanilla model attends over the entire entity/relation space which is not a good approach as we observe that the gradient for each attention value gets saturated easily.,3.2 Pre-training KG Retrieval,[0],[0]
"While training the classification and retrieval module together, the model tends to ignore the KG part and gradient propagates only through the classification module.",3.2 Pre-training KG Retrieval,[0],[0]
"This is expected to an extent as the most pertinent information for the task at hand comes from the training samples, only background aiding information comes from KG.",3.2 Pre-training KG Retrieval,[0],[0]
"After few epochs of training, the KG retrieved fact always converged to a fixed vector.",3.2 Pre-training KG Retrieval,[0],[0]
"To overcome this problem, we attempted pretraining KG retrieval part separately.",3.2 Pre-training KG Retrieval,[0],[0]
"A pre-trained KG model is used to retrieve the facts and then concatenate with the classification module, while we allow error to be propagate through the pretrained model, at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
We infer that KG doesn’t return noise and has essential information for the task as the separate KG part alone shows significant performance (59% for News20 & 66% for SNLI).,3.2 Pre-training KG Retrieval,[0],[0]
Figure 3 depicts the entire training scheme.,3.2 Pre-training KG Retrieval,[0],[0]
"This procedure solved the issue of gradient saturation in the KG retrieval part
at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
"However, the key problem of attention mechanism having to cover a large span of entities/relation, remained.",3.2 Pre-training KG Retrieval,[0],[0]
"In this section, we propose a mechanism to reduce the large number of entities/relationships over which attention has to be generated in the knowledge graph.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We propose to reduce the attention space by learning the representation of similar entity/relation vectors and attending over them.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In order to cluster similar entity/relation vectors, we used k-means clustering (Bishop, 2006) and formed l clusters with equal number of entity/relation vectors in each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Each of the clusters were then encoded using convolutional filters.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the k-means clustering is a sequence of entity/relation vectors {eT1 , eT2 , · · · , eTq }, where ei ∈ Rm.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"For each cluster these vectors were stacked to form E as the 2- D input to the CNN encoder, where E ∈ Rm×q.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"During experimentation for finding a suitable fil-
ter shape, it was observed that using 2-D filters the model failed to converge at all.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Therefore, we inferred that the latent representation of two different indices in the vector ei, should not be tampered using convolution.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We then resorted to use 1-D convolution filters which slide along only the columns of E , as shown Figure 4.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
The stride length along y-axis is the window length k.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the convolution layer is expressed as,
E ′(i, j) = W T",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"[ei,j , ei+1,j , . . .",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
", ei+k−1,j ]T
where, E ′(i, j) is the (i, j)th element of the output matrix E ′ and W ∈",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Rk is the convolution weight filter.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"A pooling layer followed the convolution layer in order to reduce the parameter space, we used 1-D window only along the y-axis similar to the convolutional kernel mentioned above.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We used a two layered convolution network with the stride length k & max-pool windows n is adjusted to obtain output Ei ∈ Rm, where i is the cluster index.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Similar procedure of clustering followed by the encoding of the cluster entities is done for relations as well.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Thus both the entity and relation space were reduced to contain fewer elements, one each for each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"After the compact entity space E and relation space R is formed, we followed the same steps as earlier for forming the attention, but now the training was more effective as the gradient was propagating effectively and was not choked by the large space.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"As the convolution architecture is also simultaneously trained, attention mechanism was not burdened as before, to learn over the large space of entities and relations.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Another point that needs to be mentioned here is regarding ranking/ordering items in the clusters, we have done experiments to verify the ordering does not affect the final result.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We have verified this by randomly shuffling the entities/relationships in every clusters and the ac-
curacy output remained within an error bound of ±0.5%.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In various permutations, the representations learned by the convolution operator for clusters varies, but it does not affect the overall results.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Regarding the interpretation of what convolution operator learns, the operator is applied along each dimension of the entity/relationship vector, to learn a representation of the clusters.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"This representation includes information from relevant entities in the cluster, as the relevant entities varies across tasks, the representation learned using convolution also adapts accordingly.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"It is analogous to learning relevant features from an image, in our case the convolution layer learns the features focusing on relevant entities/relations in a cluster pertaining to the task.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Our experiments were designed to analyze whether a deep learning model is being improved when it has access to KG facts from a relevant source.,4 Experiments and Evaluations,[0],[0]
"The selection of knowledge graph has to be pertinent to the task at hand, as currently there is no single knowledge base that contains multiple kinds of information and can cater to all tasks.",4 Experiments and Evaluations,[0],[0]
We illustrate with results that the performance of a deep learning model improves when it has access to relevant facts.,4 Experiments and Evaluations,[0],[0]
"We also illustrate that as the model learns faster with access to knowledge bases, we can train deep learning models with substantially less training data, without compromising on the accuracy.",4 Experiments and Evaluations,[0],[0]
"In the subsequent section we briefly describe the datasets and associated Knowledge Bases used.
Datasets and Relevant Knowledge Graphs
In our experiments, we have mainly used the popular text classification dataset 20Newsgroups (Lichman, 2013) and the Natural Language Inference dataset, Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).",4 Experiments and Evaluations,[0],[0]
"We have also done experiments on DBPedia ontology classification dataset1, with a very strong baseline.",4 Experiments and Evaluations,[0],[0]
"These datasets are chosen as they share domain knowledge with two most popular knowledge bases, Freebase (FB15k) (Bollacker et al., 2008) and WordNet (WN18) (Bordes et al., 2013).",4 Experiments and Evaluations,[0],[0]
"The training and test size of the datasets are mentioned in Table 1.
1http://wiki.dbpedia.org/ services-resources/dbpedia-data-set-2014
Freebase (FB15k) (Bollacker et al., 2008) contains facts about people, places and things (contains 14904 entities, 1345 relations & 4.9M fact triples), which is useful for text classification in 20Newsgroups (Lichman, 2013) dataset.",4 Experiments and Evaluations,[0],[0]
"On the other hand, WordNet (WN18) (Bordes et al., 2013) (has 40943 entities, 18 relations & 1.5M fact triples) contains facts about common day-to-day things (example: furniture includes bed), which can help in inference tasks like SNLI.",4 Experiments and Evaluations,[0],[0]
"Both the knowledge bases are directed graphs, due to fewer number of relations WN18 the entities are more likely to be connected using the same type of relations.",4 Experiments and Evaluations,[0],[0]
For experiments relating to both the datasets 20Newsgroups & SNLI we used the standard LSTM as the classification module.,4 Experiments and Evaluations,[0],[0]
"As iterated earlier, our KG based fact retrieval is independent of the base model used.",4 Experiments and Evaluations,[0],[0]
We show improvement in performance using the proposed models by KG fact retrieval.,4 Experiments and Evaluations,[0],[0]
We use classification accuracy of the test set as our evaluation metric.,4 Experiments and Evaluations,[0],[0]
All experiments were carried on a Dell Precision Tower 7910 server with Quadro M5000 GPU with 8 GB of memory.,4.1 Experimental Setup,[0],[0]
"The models were trained using the Adam’s Optimizer (Kingma and Ba, 2014) in a stochastic gradient descent (Bottou, 2012) fashion.",4.1 Experimental Setup,[0],[0]
"The models were implemented using TensorFlow (Abadi et al., 2015).",4.1 Experimental Setup,[0],[0]
The relevant hyperparameters are listed in Table 2.,4.1 Experimental Setup,[0],[0]
"The word embeddings for the experiments were obtained using the pre-trained GloVe (Pennington et al., 2014)2 vectors.",4.1 Experimental Setup,[0],[0]
"For words missing in the pre-trained vectors, the local GloVe vectors which was trained on the corresponding dataset was used.",4.1 Experimental Setup,[0],[0]
Table 3 shows the results of test accuracy of the various methods proposed on the datasets News20 & SNLI.,4.2 Results & Discussion,[0],[0]
"We observe that incorporation of KG facts using the basic vanilla model improves the performance slightly, as the retrieval model was
2http://nlp.stanford.edu/data/glove.840B.300d.zip
not getting trained effectively.",4.2 Results & Discussion,[0],[0]
The convolutionbased model shows significant improvement over the normal LSTM classification.,4.2 Results & Discussion,[0],[0]
While tuning the parameters of the convolution for clustered entities/relations it was observed that smaller stride length and longer max-pool window improved performance.,4.2 Results & Discussion,[0],[0]
"For News20 dataset we show an improvement of almost 3% and for SNLI an improvement of almost 5%.
",4.2 Results & Discussion,[0],[0]
The work is motivated more from the perspective of whether incorporation of world knowledge will improve any deep learning model rather than beating the state-of-the-art performance.,4.2 Results & Discussion,[0],[0]
"Although LSTM is used to encode the input for the model as well as the retrieval vector, as mentioned earlier, these two modules need not be same.",4.2 Results & Discussion,[0],[0]
For encoding the input any complex state-of-the-art model can be used.,4.2 Results & Discussion,[0],[0]
LSTM has also been used to generate the retrieval vector.,4.2 Results & Discussion,[0],[0]
"For DBPedia ontology classification dataset, we have used a strong baseline of 98.6%, and after augmenting it with KG (Freebase) using convolution based model we saw an improvement of ∼0.2%.",4.2 Results & Discussion,[0],[0]
"As the baseline is stronger, the improvement quantum has decreased.",4.2 Results & Discussion,[0],[0]
This is quite intuitive as complex models are selfsufficient in learning from the data by itself and therefore the room available for further improvement is relatively less.,4.2 Results & Discussion,[0],[0]
"The improvement as observed in the experiments is significant in weaker learning models, however it is also capable of improving stronger baselines as is evident from the results of DBPedia dataset.",4.2 Results & Discussion,[0],[0]
"We hypothesized that as Knowledge Graph is feeding more information to the model, we can achieve better performance with less training data.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
To verify this we have performed experiments on varying dataset fractions for 20Newsgroups dataset as shown in Figure 5.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"From the plot, we observe that KG augmented LSTM with 70% data outperforms the baseline model with full dataset support, thereby reducing the dependency on labeled data by 30%.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
We also designed an experiment to compare the accuracy of the baseline model trained on full training data and compared it with the accuracy of the KG augmented model trained with just 70% of the training data for 20Newsgroups and SNLI datasets.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
The accuracy and training loss plots across training epochs is given in Figure 6.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"Even with just 70% of the data, KG augmented model is able to achieve better accuracy compared to the vanilla LSTM model trained on the full data.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
This clearly indicates that relevant information pertaining to the task is retrieved from the knowledge graph and the training loss reduction is not due to lesser data only.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
Also note that training loss is substantially less for KG LSTM compared to normal LSTM when the dataset size is reduced.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"This result is very promising, to reduce the large labeled training data requirement of large deep learning
models, which is hard to come by.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"The basic idea of infusing general world knowledge for learning tasks, especially for natural language processing, has not been attempted before.",5 Relevant Previous Work,[0],[0]
"For multi-label image classification, the use of KGs has been pursued recently by (Marino et al., 2016).",5 Relevant Previous Work,[0],[0]
"In this work, they first obtain labels of the input data (using a different model), use these labels to populate features from the KG and in turn use these features back for the final classification.",5 Relevant Previous Work,[0.9536812998217147],"['In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).']"
"For NLP tasks the information needed may not necessarily depend on the final class, and we are directly using all the information available in the input for populating the relevant information from the knowledge graphs.",5 Relevant Previous Work,[0],[0]
"Our attempt is very different from Transfer Learning (Pan and Yang, 2010).
",5 Relevant Previous Work,[0],[0]
In Transfer Learning the focus is on training the model for one task and tuning the trained model to use it for another task.,5 Relevant Previous Work,[0],[0]
This is heavily dependent on the alignment between source task and destination task and transferred information is in the model.,5 Relevant Previous Work,[0],[0]
"In our case, general world knowledge is being infused into the learning model for any given task.",5 Relevant Previous Work,[0],[0]
"By the same logic, our work is different from domain adaptation (Glorot et al., 2011) as well.",5 Relevant Previous Work,[0],[0]
"There has been attempts to use world knowledge (Song and Roth, 2017) for creating more labeled training data and providing distant supervision etc.",5 Relevant Previous Work,[0],[0]
"Incorporating Inductive Biases (Ridgeway, 2016) based on the known information about a domain onto the structure of the learned models, is an active area of research.",5 Relevant Previous Work,[0],[0]
However our motivation and approach is fundamentally different from these works.,5 Relevant Previous Work,[0],[0]
In this work we have illustrated the need for incorporating world knowledge in training task specific models.,6 Conclusion & Future Work,[0],[0]
We presented a novel convolutionbased architecture to reduce the attention space over entities and relations that outperformed other models.,6 Conclusion & Future Work,[0],[0]
"With significant improvements over the vanilla baselines for two well known datasets, we have illustrated the efficacy of our proposed methods in enhancing the performance of deep learning models.",6 Conclusion & Future Work,[0],[0]
We showcased that the proposed method can be used to reduce labeled training data requirements of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Although in this work we focused only on NLP tasks and using LSTM as the baseline model, the proposed approach is applicable for other domain tasks as well, with more complicated deep learning models as baseline.",6 Conclusion & Future Work,[0],[0]
To the best of our knowledge this is the first attempt at infusing general world knowledge for task specific training of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Being the first work of its kind, there is a lot of scope for improvement.",6 Conclusion & Future Work,[0],[0]
A more sophisticated model which is able to retrieve facts more efficiently from millions of entries can be formulated.,6 Conclusion & Future Work,[0],[0]
"Currently we have focused only on a flat attention structure, a hierarchical attention mechanism would be more suitable.",6 Conclusion & Future Work,[0],[0]
The model uses soft attention to enable training by simple stochastic gradient descent.,6 Conclusion & Future Work,[0],[0]
Hard attention over facts using reinforcement learning can be pursued further.,6 Conclusion & Future Work,[0],[0]
"This will further help in selection of multifacts, that are not of similar type but relevant to the task.",6 Conclusion & Future Work,[0],[0]
"The convolution based model, helped to reduce the space over entities and relationships over which attention had to be generated.",6 Conclusion & Future Work,[0],[0]
"However more sophisticated techniques using similarity based search (Wang et al., 2014a; Mu and Liu, 2017) can be pursued towards this purpose.",6 Conclusion & Future Work,[0],[0]
"The results from the initial experiments illustrates the effectiveness of our proposed approach, advocating further investigations in these directions.",6 Conclusion & Future Work,[0],[0]
"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data.",abstractText,[0],[0]
"Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand.",abstractText,[0],[0]
"In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks.",abstractText,[0],[0]
Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism.,abstractText,[0],[0]
We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space.,abstractText,[0],[0]
We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task.,abstractText,[0],[0]
"Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset.",abstractText,[0],[0]
"We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",abstractText,[0],[0]
Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1042",text,[0],[0]
Multilingual word embeddings have attracted a lot of attention in recent times.,1 Introduction,[0],[0]
"In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012).
",1 Introduction,[0],[0]
"Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs.",1 Introduction,[0],[0]
"A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al.,
2015; Vulić and Moens, 2016; Mogadala and Rettinger, 2016), but large amounts of such corpora are not always available for some language pairs.
",1 Introduction,[0],[0]
"An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries (Mikolov et al., 2013a; Artetxe et al., 2016).",1 Introduction,[0],[0]
"However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages.
",1 Introduction,[0],[0]
"In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries.",1 Introduction,[0],[0]
"Our method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved.",1 Introduction,[0],[0]
"The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilingual word embeddings without any real bilingual data.",1 Introduction,[0],[0]
"In either case, we obtain very competitive results, comparable to other state-of-the-art methods that make use of much richer bilingual resources.
",1 Introduction,[0],[0]
"The proposed method is an extension of existing mapping techniques, where the dictionary is used to learn the embedding mapping and the embedding mapping is used to induce a new dictionary iteratively in a self-learning fashion (see Figure 1).",1 Introduction,[0],[0]
"In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of independently trained embeddings.
",1 Introduction,[0],[0]
We analyze previous work in Section 2.,1 Introduction,[0],[0]
"Section 3 describes the self-learning framework, while Section 4 presents the experiments.",1 Introduction,[0],[0]
"Section 5 analyzes the underlying optimization objective, and Section 6 presents an error analysis.
",1 Introduction,[0],[0]
451,1 Introduction,[0],[0]
"We will first focus on bilingual embedding mappings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings.",2 Related work,[0],[0]
"Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary.
",2.1 Bilingual embedding mappings,[0],[0]
"The first of such methods is due to Mikolov et al. (2013a), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries.",2.1 Bilingual embedding mappings,[0],[0]
"The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal.",2.1 Bilingual embedding mappings,[0],[0]
"Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping.",2.1 Bilingual embedding mappings,[0],[0]
"Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling.
",2.1 Bilingual embedding mappings,[0],[0]
"Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space.",2.1 Bilingual embedding mappings,[0],[0]
"Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn non-linear transformations.
",2.1 Bilingual embedding mappings,[0],[0]
"Artetxe et al. (2016) propose a general framework that clarifies the relation between Mikolov et al. (2013a), Xing et al. (2015), Faruqui and Dyer (2014) and Zhang et al. (2016) as variants of the
same core optimization objective, and show that a new variant is able to surpass them all.",2.1 Bilingual embedding mappings,[0.9537372822994665],"['The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).']"
"While most of the previous methods use gradient descent, Artetxe et al. (2016) propose an efficient analytical implementation for those same methods, recently extended by Smith et al. (2017) to incorporate dimensionality reduction.
",2.1 Bilingual embedding mappings,[0],[0]
"A prominent application of bilingual embedding mappings, with a direct application in machine translation (Zhao et al., 2015), is bilingual lexicon extraction, which is also the main evaluation method.",2.1 Bilingual embedding mappings,[0],[0]
"More specifically, the learned mapping is used to induce the translation of source language words that were missing in the original dictionary, usually by taking their nearest neighbor word in the target language according to cosine similarity, although Dinu et al. (2015) and Smith et al. (2017) propose alternative retrieval methods to address the hubness problem.",2.1 Bilingual embedding mappings,[0],[0]
"As mentioned before, our method works with as little as 25 word pairs, while the methods discussed previously use thousands of pairs.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The only exception in this regard is the work by Zhang et al. (2016), who only use 10 word pairs with good results on transfer learning for part-of-speech tagging.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our experiments will show that, although their method captures coarse-grained relations, it fails on finer-grained tasks like bilingual lexicon induction.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Bootstrapping methods similar to ours have been previously proposed for traditional countbased vector space models (Peirsman and Padó, 2010; Vulić and Moens, 2013).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, while previous techniques incrementally build a high-
Algorithm 1 Traditional framework Input: X (source embeddings)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: W ← LEARN MAPPING(X , Z, D) 2: D ← LEARN DICTIONARY(X , Z, W ) 3: EVALUATE DICTIONARY(D)
dimensional model where each axis encodes the co-occurrences with a specific word and its equivalent in the other language, our method works with low-dimensional pre-trained word embeddings, which are more widely used nowadays.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
A practical aspect for reducing the need of bilingual supervision is on the design of the seed dictionary.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"This is analyzed in depth by Vulić and Korhonen (2016), who propose using documentaligned corpora to extract the training dictionary.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"A more common approach is to rely on shared words and cognates (Peirsman and Padó, 2010; Smith et al., 2017), eliminating the need of bilingual data in practice.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our use of shared numerals exploits the same underlying idea, but relies on even less bilingual evidence and should thus generalize better to distant language pairs.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Miceli Barone (2016) and Cao et al. (2016) go one step further and attempt to learn bilingual embeddings without any bilingual evidence.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The former uses adversarial autoencoders (Makhzani et al., 2016), combining an encoder that maps the source language embeddings into the target language, a decoder that reconstructs the original embeddings, and a discriminator that distinguishes mapped embeddings from real target language embeddings, whereas the latter adds a regularization term to the training of word embeddings that pushes the mean and variance of each dimension in different languages close to each other.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Although promising, the reported performance in both cases is poor in comparison to other methods.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Finally, the induction of bilingual knowledge from monolingual corpora is closely related to the decipherment scenario, for which models that incorporate word embeddings have also been proposed (Dou et al., 2015).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, decipherment is only concerned with translating text from one language to another and relies on complex statistical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Algorithm 2 Proposed self-learning framework Input: X (source embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: repeat 2:",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"W ← LEARN MAPPING(X , Z, D) 3: D ← LEARN DICTIONARY(X , Z, W ) 4: until convergence criterion 5: EVALUATE DICTIONARY(D)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"As discussed in Section 2.1, a common evaluation task (and practical application) of bilingual embedding mappings is to induce bilingual lexicons, that is, to obtain the translation of source words that were missing in the training dictionary, which are then compared to a gold standard test dictionary for evaluation.",3 Proposed self-learning framework,[0],[0]
"This way, one can say that the seed (train) dictionary is used to learn a mapping, which is then used to induce a better dictionary (at least in the sense that it is larger).",3 Proposed self-learning framework,[0],[0]
"Algorithm 1 summarizes this framework.
",3 Proposed self-learning framework,[0],[0]
"Following this observation, we propose to use the output dictionary in Algorithm 1 as the input of the same system in a self-learning fashion which, assuming that the output dictionary was indeed better than the original one, should serve to learn a better mapping and, consequently, an even better dictionary the second time.",3 Proposed self-learning framework,[0],[0]
The process can then be repeated iteratively to obtain a hopefully better mapping and dictionary each time until some convergence criterion is met.,3 Proposed self-learning framework,[0],[0]
"Algorithm 2 summarizes this alternative framework that we propose.
",3 Proposed self-learning framework,[0],[0]
Our method can be combined with any embedding mapping and dictionary induction technique (see Section 2.1).,3 Proposed self-learning framework,[0],[0]
"However, efficiency turns out to be critical for a variety of reasons.",3 Proposed self-learning framework,[0],[0]
"First of all, by enclosing the learning logic in a loop, the total training time is increased by the number of iterations.",3 Proposed self-learning framework,[0],[0]
"Even more importantly, our framework requires to explicitly build the entire dictionary at each iteration, whereas previous work tends to induce the translation of individual words ondemand later at runtime.",3 Proposed self-learning framework,[0],[0]
"Moreover, from the second iteration onwards, it is this induced, full dictionary that has to be used to learn the embedding mapping, and not the considerably smaller seed dictionary as it is typically done.",3 Proposed self-learning framework,[0],[0]
"In the following two subsections, we respectively describe the embedding mapping method and the dictionary in-
duction method that we adopt in our work with these efficiency requirements in mind.",3 Proposed self-learning framework,[0],[0]
"As discussed in Section 2.1, most previous methods to learn embedding mappings use variants of gradient descent.",3.1 Embedding mapping,[0],[0]
"Among the more efficient exact alternatives, we decide to adopt the one by Artetxe et al. (2016) for its simplicity and good results as reported in their paper.",3.1 Embedding mapping,[0],[0]
"We next present their method, adapting the formalization to explicitly incorporate the dictionary as required by our self-learning algorithm.
",3.1 Embedding mapping,[0],[0]
Let X and Z denote the word embedding matrices in two languages so that Xi∗ corresponds to the ith source language word embedding and Zj∗ corresponds to the jth target language embedding.,3.1 Embedding mapping,[0],[0]
"While Artetxe et al. (2016) assume these two matrices are aligned according to the dictionary, we drop this assumption and represent the dictionary explicitly as a binary matrix D, so that Dij = 1 if the ith source language word is aligned with the jth target language word.",3.1 Embedding mapping,[0],[0]
The goal is then to find the optimal mapping matrix W ∗ so that the sum of squared Euclidean distances between the mapped source embeddings Xi∗W and target embeddings Zj∗,3.1 Embedding mapping,[0],[0]
"for the dictionary entries Dij is minimized:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg min W
∑
i
∑
j
Dij ||Xi∗W",3.1 Embedding mapping,[0],[0]
"− Zj∗||2
Following Artetxe et al. (2016), we length normalize and mean center the embedding matrices X and Z in a preprocessing step, and constrain W to be an orthogonal matrix (i.e. WW T = W TW = I), which serves to enforce monolingual invariance, preventing a degradation in monolingual performance while yielding to better bilingual mappings.",3.1 Embedding mapping,[0],[0]
"Under such orthogonality constraint, minimizing the squared Euclidean distance becomes equivalent to maximizing the dot product, so the above optimization objective can be reformulated as follows:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg max W
Tr ( XWZTDT )
where Tr (·) denotes the trace operator (the sum of all the elements in the main diagonal).",3.1 Embedding mapping,[0],[0]
"The optimal orthogonal solution for this problem is given by W ∗ = UV T , where",3.1 Embedding mapping,[0],[0]
XTDZ = UΣV T is the singular value decomposition of XTDZ.,3.1 Embedding mapping,[0],[0]
"Since the dictionary matrix D is sparse, this can be efficiently computed in linear time with respect to the number of dictionary entries.",3.1 Embedding mapping,[0],[0]
"As discussed in Section 2.1, practically all previous work uses nearest neighbor retrieval for word translation induction based on embedding mappings.",3.2 Dictionary induction,[0],[0]
"In nearest neighbor retrieval, each source language word is assigned the closest word in the target language.",3.2 Dictionary induction,[0],[0]
"In our work, we use the dot product between the mapped source language embeddings and the target language embeddings as the similarity measure, which is roughly equivalent to cosine similarity given that we apply length normalization followed by mean centering as a preprocessing step (see Section 3.1).",3.2 Dictionary induction,[0],[0]
"This way, following the notation in Section 3.1, we set Dij = 1 if j = argmaxk (Xi∗W )",3.2 Dictionary induction,[0],[0]
·Zk∗ and,3.2 Dictionary induction,[0],[0]
"Dij = 0 otherwise1.
",3.2 Dictionary induction,[0],[0]
"While we find that independently computing the similarity measure between all word pairs is prohibitively slow, the computation of the entire similarity matrix XWZT can be easily vectorized using popular linear algebra libraries, obtaining big performance gains.",3.2 Dictionary induction,[0],[0]
"However, the resulting similarity matrix is often too large to fit in memory when using large vocabularies.",3.2 Dictionary induction,[0],[0]
"For that reason, instead of computing the entire similarity matrix XWZT in a single step, we iteratively compute submatrices of it using vectorized matrix multiplication, find their corresponding maxima each time, and then combine the results.",3.2 Dictionary induction,[0],[0]
"In this section, we experimentally test the proposed method in bilingual lexicon induction and crosslingual word similarity.",4 Experiments and results,[0],[0]
"Subsection 4.1 describes the experimental settings, while Subsections 4.2 and 4.3 present the results obtained in each of the tasks.",4 Experiments and results,[0],[0]
The code and resources necessary to reproduce our experiments are available at https://github.com/artetxem/ vecmap.,4 Experiments and results,[0],[0]
"For easier comparison with related work, we evaluated our mappings on bilingual lexicon induction using the public English-Italian dataset by Dinu et al. (2015), which includes monolingual word embeddings in both languages together with a bilingual dictionary split in a training set and a
1Note that we induce the dictionary entries starting from the source language words.",4.1 Experimental settings,[0],[0]
"We experimented with other alternatives in development, with minor differences.
test set2.",4.1 Experimental settings,[0],[0]
"The embeddings were trained with the word2vec toolkit with CBOW and negative sampling (Mikolov et al., 2013b)3, using a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC) and a 1.6 billion word corpus for Italian (itWaC).",4.1 Experimental settings,[0],[0]
"The training and test sets were derived from a dictionary built form Europarl word alignments and available at OPUS (Tiedemann, 2012), taking 1,500 random entries uniformly distributed in 5 frequency bins as the test set and the 5,000 most frequent of the remaining word pairs as the training set.
",4.1 Experimental settings,[0],[0]
"In addition to English-Italian, we selected two other languages from different language families with publicly available resources.",4.1 Experimental settings,[0],[0]
We thus created analogous datasets for English-German and English-Finnish.,4.1 Experimental settings,[0],[0]
"In the case of German, the embeddings were trained on the 0.9 billion word corpus SdeWaC, which is part of the WaCky collection (Baroni et al., 2009) that was also used for English and Italian.",4.1 Experimental settings,[0],[0]
"Given that Finnish is not included in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 20164 instead, which we tokenized using the Stanford Tokenizer (Manning et al., 2014).",4.1 Experimental settings,[0],[0]
"In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as Dinu et al. (2015).
",4.1 Experimental settings,[0],[0]
"Given that the main focus of our work is on small seed dictionaries, we created random subsets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries.",4.1 Experimental settings,[0],[0]
"This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries.
",4.1 Experimental settings,[0],[0]
"In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.",4.1 Experimental settings,[0],[0]
"For that purpose, we created numeral dictionaries, consisting of words matching the [0-9]+ regular expression in both vocabularies (e.g. 1-1, 2-2, 3-3, 1992-1992
2http://clic.cimec.unitn.it/ ˜georgiana.dinu/down/
3The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10, and the vocabulary was restricted to the 200,000 most frequent words
4http://www.statmt.org/wmt16/ translation-task.html
etc.).",4.1 Experimental settings,[0],[0]
"The resulting dictionary had 2772 entries for English-Italian, 2148 for English-German, and 2345 for English-Finnish.",4.1 Experimental settings,[0],[0]
"While more sophisticated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as Arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian).
",4.1 Experimental settings,[0],[0]
"While bilingual lexicon induction is a standard evaluation task for seed dictionary based methods like ours, it is unsuitable for bilingual corpus based methods, as statistical word alignment already provides a reliable way to derive dictionaries from bilingual corpora and, in fact, this is how the test dictionary itself is built in our case.",4.1 Experimental settings,[0],[0]
"For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing to compare it to systems that use richer bilingual data.",4.1 Experimental settings,[0],[0]
"There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for EnglishItalian as published by Camacho-Collados et al. (2015) 5.
",4.1 Experimental settings,[0],[0]
"As for the convergence criterion, we decide to stop training when the improvement on the average dot product for the induced dictionary falls below a given threshold from one iteration to the next.",4.1 Experimental settings,[0],[0]
"After length normalization, the dot product ranges from -1 to 1, so we decide to set this threshold at 1e-6, which we find to be a very conservative value yet enough that training takes a reasonable amount of time.",4.1 Experimental settings,[0],[0]
"The curves in the next section confirm that this was a reasonable choice.
",4.1 Experimental settings,[0],[0]
"This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a modest desktop computer (Intel Core i5-4670 CPU with 8GiB of RAM), including the induction of a dictionary of 200,000 words at each iteration.",4.1 Experimental settings,[0],[0]
"For the experiments on bilingual lexicon induction, we compared our method with those proposed by Mikolov et al. (2013a), Xing et al. (2015), Zhang et al. (2016) and Artetxe et al. (2016), all of them implemented as part of the framework proposed by the latter.",4.2 Bilingual lexicon induction,[0],[0]
"The results ob-
5http://lcl.uniroma1.it/ similarity-datasets/
tained with the 5,000 entry, 25 entry and the numerals dictionaries for all the 3 language pairs are given in Table 1.
",4.2 Bilingual lexicon induction,[0],[0]
"The results for the 5,000 entry dictionaries show that our method is comparable or even better than the other systems.",4.2 Bilingual lexicon induction,[0],[0]
"As another reference, the best published results using nearest-neighbor retrieval are due to Lazaridou et al. (2015), who report an accuracy of 40.20% for the full EnglishItalian dictionary, almost at pair with our system (39.67%).
",4.2 Bilingual lexicon induction,[0],[0]
"In any case, the main focus of our work is on smaller dictionaries, and it is under this setting that our method really stands out.",4.2 Bilingual lexicon induction,[0],[0]
"The 25 entry and numerals columns in Table 1 show the results for this setting, where all previous methods drop dramatically, falling below 1% accuracy in all cases.",4.2 Bilingual lexicon induction,[0],[0]
"The method by Zhang et al. (2016) also obtains poor results with small dictionaries, which reinforces our hypothesis in Section 2.2 that their method can only capture coarse-grain bilingual relations for small dictionaries.",4.2 Bilingual lexicon induction,[0],[0]
"In contrast, our proposed method obtains very competitive results for all dictionaries, with a difference of only 1-2 points between the full dictionary and both the 25 entry dictionary and the numerals dictionary in all three languages.",4.2 Bilingual lexicon induction,[0],[0]
"Figure 2 shows the curve of the English-Italian accuracy for different seed dictionary sizes, confirming this trend.
",4.2 Bilingual lexicon induction,[0],[0]
"Finally, it is worth mentioning that, even if all the three language pairs show the same general behavior, there are clear differences in their absolute accuracy numbers, which can be attributed to the linguistic proximity of the languages involved.",4.2 Bilingual lexicon induction,[0],[0]
"In particular, the results for English-Finnish are about 10 points below the rest, which is explained by the fact that Finnish is a non-indoeuropean agglutinative language, making the task considerably more difficult for this language pair.",4.2 Bilingual lexicon induction,[0],[0]
"In this regard, we believe that the good results with small dictionaries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual ev-
idence even for distant language pairs where the structural similarity of the embedding spaces is presumably weaker.",4.2 Bilingual lexicon induction,[0],[0]
"In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by Luong et al. (2015), which is the state-of-the-art for bilingual word embeddings based on parallel corpora (Upadhyay et al., 2016)6.",4.3 Crosslingual word similarity,[0],[0]
"As this method is an extension of word2vec, we used the same hyperparameters as for the monolingual embeddings when possible (see Section 4.1), and leave the default ones otherwise.",4.3 Crosslingual word similarity,[0],[0]
"We used Europarl as our parallel corpus to train this method as done by the authors, which consists of nearly 2 million parallel sentences.
",4.3 Crosslingual word similarity,[0],[0]
"As shown in the results in Table 2, our method obtains the best results in all cases, surpassing the rest of the dictionary-based methods by 1-3 points depending on the dataset.",4.3 Crosslingual word similarity,[0],[0]
"But, most importantly, it does not suffer from any significant degradation for using smaller dictionaries and, in fact, our method gets better results using the 25 entry dictionary or the numeral list as the only bilingual evidence than any of the baseline systems using much richer resources.
",4.3 Crosslingual word similarity,[0],[0]
"The relatively poor results of Luong et al. (2015) can be attributed to the fact that the dictionary based methods make use of much bigger monolingual corpora, while methods based on parallel corpora are restricted to smaller corpora.",4.3 Crosslingual word similarity,[0],[0]
"However, it is not clear how to introduce monolingual corpora on those methods.",4.3 Crosslingual word similarity,[0],[0]
"We did run some experiments with BilBOWA (Gouws et al., 2015), which supports training in monolingual corpora in addition to bilingual corpora, but obtained very poor results7.",4.3 Crosslingual word similarity,[0],[0]
"All in all, our experiments show
6We also tested English-German pre-trained embeddings from Klementiev et al. (2012) and Chandar A P et al. (2014).",4.3 Crosslingual word similarity,[0],[0]
"They both had coverage problems that made the results hard to compare, and, when considering the correlations for the word pairs in their vocabulary, their performance was poor.
",4.3 Crosslingual word similarity,[0],[0]
"7Upadhyay et al. (2016) report similar problems using
that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone.",4.3 Crosslingual word similarity,[0],[0]
"It might seem somehow surprising at first that, as seen in the previous section, our simple selflearning approach is able to learn high quality bilingual embeddings from small seed dictionaries instead of falling in degenerated solutions.",5 Global optimization objective,[0],[0]
"In this section, we try to shed light on our approach, and give empirical evidence supporting our claim.
",5 Global optimization objective,[0],[0]
"More concretely, we argue that, for the embedding mapping and dictionary induction methods described in Section 3, the proposed selflearning framework is implicitly solving the following global optimization problem8:
W ∗ = arg max W
∑
i
max j
(Xi∗W ) · Zj∗
s.t. WW T = W TW",5 Global optimization objective,[0],[0]
=,5 Global optimization objective,[0],[0]
"I
Contrary to the optimization objective for W in Section 3.1, the global optimization objective does not refer to any dictionary, and maximizes the similarity between each source language word and its closest target language word.",5 Global optimization objective,[0],[0]
"Intuitively, a random solution would map source language embeddings to seemingly random locations in the target language space, and it would thus be unlikely that
BilBOWA.",5 Global optimization objective,[0],[0]
"8While we restrict our formal analysis to the embedding mapping and dictionary induction method that we use, the general reasoning should be valid for other choices as well.
",5 Global optimization objective,[0],[0]
"they have any target language word nearby, making the optimization value small.",5 Global optimization objective,[0],[0]
"In contrast, a good solution would map source language words close to their translation equivalents in the target language space, and they would thus have their corresponding embeddings nearby, making the optimization value large.",5 Global optimization objective,[0],[0]
"While it is certainly possible to build degenerated solutions that take high optimization values for small subsets of the vocabulary, we think that the structural similarity between independently trained embedding spaces in different languages is strong enough that optimizing this function yields to meaningful bilingual mappings when the size of the vocabulary is much larger than the dimensionality of the embeddings.
",5 Global optimization objective,[0],[0]
The reasoning for how the self-learning framework is optimizing this objective is as follows.,5 Global optimization objective,[0],[0]
"At the end of each iteration, the dictionary D is updated to assign, for the current mapping W , each source language word to its closest target language word.",5 Global optimization objective,[0],[0]
"This way, when we update W to maximize the average similarity of these dictionary entries at the beginning of the next iteration, it is guaranteed that the value of the optimization objective will improve (or at least remain the same).",5 Global optimization objective,[0],[0]
"The reason is that the average similarity between each word and what were previously the closest words will be improved if possible, as this is what the updated W directly optimizes (see Section 3.1).",5 Global optimization objective,[0],[0]
"In addition to that, it is also possible that, for some source words, some other target words get closer after the update.",5 Global optimization objective,[0],[0]
"Thanks to this, our self-learning algorithm is guaranteed to converge to a local optimum of the above global objective, behaving like an alternating optimization algorithm for it.
",5 Global optimization objective,[0],[0]
"It is interesting to note that the above reasoning is valid no matter what the the initial solution is, and, in fact, the global optimization objective does not depend on the seed dictionary nor any other
bilingual resource.",5 Global optimization objective,[0],[0]
"For that reason, it should be possible to use a random initialization instead of a small seed dictionary.",5 Global optimization objective,[0],[0]
"However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough.
",5 Global optimization objective,[0],[0]
"The general behavior of our method is reflected in Figure 3, which shows the learning curve for different seed dictionaries according to both the objective function and the accuracy on bilingual lexicon induction.",5 Global optimization objective,[0],[0]
"As it can be seen, the objective function is improved from iteration to iteration and converges to a local optimum just as expected.",5 Global optimization objective,[0],[0]
"At the same time, the learning curves show a strong correlation between the optimization objective and the accuracy, as it can be clearly observed that improving the former leads to an improvement of the latter, confirming our explanations.",5 Global optimization objective,[0],[0]
"Regarding random initialization, the figure shows that the algorithm gets stuck in a poor local optimum of the objective function, which is the reason of the bad performance (0% accuracy) on bilingual lexicon induction, but the proposed optimization objective itself seems to be adequate.
",5 Global optimization objective,[0],[0]
"Finally, we empirically observe that our algorithm learns similar mappings no matter what the seed dictionary was.",5 Global optimization objective,[0],[0]
"We first repeated our experiments on English-Italian bilingual lexicon induction for 5 different dictionaries of 25 entries, obtaining an average accuracy of 38.15% and a standard deviation of only 0.75%.",5 Global optimization objective,[0],[0]
"In addition to that, we observe that the overlap between the predictions made when starting with the full dictionary and the numerals dictionary is 76.00% (60.00% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"At the same time,
37.00% of the test cases are correctly solved by both instances, and it is only 5.07% of the test cases that one of them gets right and the other wrong (34.00% and 8.94% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"This suggests that our algorithm tends to converge to similar solutions even for disjoint seed dictionaries, which is in line with our view that we are implicitly optimizing an objective that is independent from the seed dictionary, yet a seed dictionary is necessary to build a good enough initial solution to avoid getting stuck in poor local optima.",5 Global optimization objective,[0],[0]
"For that reason, it is likely that better methods to tackle this optimization problem would allow learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research.",5 Global optimization objective,[0],[0]
"So as to better understand the behavior of our system, we performed an error analysis of its output in English-Italian bilingual lexicon induction when starting with the 5,000 entry, the 25 entry and the numeral dictionaries in comparison with the baseline method of Artetxe et al. (2016) with the 5,000 entry dictionary.",6 Error analysis,[0],[0]
"For that purpose, we took 100 random examples from the test set in the [1-5K] frequency bin, another 100 from the [5K20K] frequency bin and 30 from the [100K-200K] frequency bin, and manually analyzed each of the errors made by all the 4 different variants.
",6 Error analysis,[0],[0]
"Our analysis first reveals that, in all the cases, about a third of the translations taken as erroneous according to the gold standard are not so in real-
ity.",6 Error analysis,[0],[0]
This corresponds to both different morphological variants of the gold standard translations (e.g. dichiarato/dichiarò) and other valid translations that were missing in the gold standard (e.g. climb → salita instead of the gold standard scalato).,6 Error analysis,[0],[0]
"This phenomenon is considerably more pronounced in the first frequency bins, which already have a much higher accuracy according to the gold standard.
",6 Error analysis,[0],[0]
"As for the actual errors, we observe that nearly a third of them correspond to named entities for all the different variants.",6 Error analysis,[0],[0]
"Interestingly, the vast majority of the proposed translations in these cases are also named entities (e.g. Ryan→ Jason, John→ Paolo), which are often highly related to the original ones (e.g. Volvo→ BMW, Olympus→ Nikon).",6 Error analysis,[0],[0]
"While these are clear errors, it is understandable that these methods are unable to discriminate between named entities to this degree based solely on the distributional hypothesis, in particular when it comes to common proper names (e.g. John, Andy), and one could design alternative strategies to address this issue like taking the edit distance as an additional signal.
",6 Error analysis,[0],[0]
"For the remaining errors, all systems tend to propose translations that have some degree of relationship with the correct ones, including nearsynonyms (e.g. guidelines → raccomandazioni), antonyms (e.g. sender→ destinatario) and words in the same semantic field (e.g. nominalism→ intuizionismo / innatismo, which are all philosophical doctrines).",6 Error analysis,[0],[0]
"However, there are also a few instances where the relationship is weak or unclear (e.g. loch→ giardini, sweep→ serrare).",6 Error analysis,[0],[0]
"We also observe a few errors that are related to multiwords or collocations (e.g. carrier→ aereo, presumably related to the multiword air carrier / linea aerea), as well as some rare word that is repeated across many translations (Ferruzzi), which could be attributed to the hubness problem (Dinu et al., 2015; Lazaridou et al., 2015).
",6 Error analysis,[0],[0]
"All in all, our error analysis reveals that the baseline method of Artetxe et al. (2016) and the proposed algorithm tend to make the same kind of errors regardless of the seed dictionary used by the latter, which reinforces our interpretation in the previous section regarding an underlying optimization objective that is independent from any training dictionary.",6 Error analysis,[0],[0]
"Moreover, it shows that the quality of the learned mappings is much better than what the raw accuracy numbers might sug-
gest, encouraging the incorporation of these techniques in other applications.",6 Error analysis,[0],[0]
"In this work, we propose a simple self-learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique.",7 Conclusions and future work,[0],[0]
"Our experiments on bilingual lexicon induction and crosslingual word similarity show that our method is able to learn high quality bilingual embeddings from as little bilingual evidence as a 25 word dictionary or an automatically generated list of numerals, obtaining results that are competitive with state-of-the-art systems using much richer bilingual resources like larger dictionaries or parallel corpora.",7 Conclusions and future work,[0],[0]
"In spite of its simplicity, a more detailed analysis shows that our method is implicitly optimizing a meaningful objective function that is independent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsupervised manner.
",7 Conclusions and future work,[0],[0]
"In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all.",7 Conclusions and future work,[0],[0]
"In addition to that, we would like to explore non-linear transformations (Lu et al., 2015) and alternative dictionary induction methods (Dinu et al., 2015; Smith et al., 2017).",7 Conclusions and future work,[0],[0]
"Finally, we would like to apply our model in the decipherment scenario (Dou et al., 2015).",7 Conclusions and future work,[0],[0]
"We thank the anonymous reviewers for their insightful comments and Flavio Merenda for his help with the error analysis.
",Acknowledgements,[0],[0]
"This research was partially supported by a Google Faculty Award, the Spanish MINECO (TUNER TIN2015-65308-C5-1-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the Basque Government (MODELA KK-2016/00082) and the UPV/EHU (excellence research group).",Acknowledgements,[0],[0]
Mikel Artetxe enjoys a doctoral grant from the Spanish MECD.,Acknowledgements,[0],[0]
"Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs.",abstractText,[0],[0]
"This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead.",abstractText,[0],[0]
"In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique.",abstractText,[0],[0]
"Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.",abstractText,[0],[0]
Learning bilingual word embeddings with (almost) no bilingual data,title,[0],[0]
"In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1.",1. Introduction,[0],[0]
"The hidden layer, h = (h1, . . .",1. Introduction,[0],[0]
", hd), consists of d binary random variables with an unknown joint distribution Ph : {0, 1}d",1. Introduction,[0],[0]
→,1. Introduction,[0],[0]
"[0, 1].",1. Introduction,[0],[0]
"The observed vector x ∈ Rm with m ≥ d features is modeled as
x = W>h+ σξ, (1)
whereW ∈ Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ",1. Introduction,[0],[0]
"≥ 0 is the noise level and ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d.",1. Introduction,[0],[0]
"zero mean and unit variance random variables.
",1. Introduction,[0],[0]
1Dept.,1. Introduction,[0],[0]
"of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
"2Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel.",1. Introduction,[0],[0]
"3Program of Applied Mathematics, Yale University, New Haven, CT 06511, USA.",1. Introduction,[0],[0]
"Correspondence to: Ariel Jaffe <ariel.jaffe@weizmann.ac.il>, Roi Weiss <roi.weiss@weizmann.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"For simplicity we assume it is Gaussian, though our method can be modified to handle other noise distributions.
",1. Introduction,[0],[0]
"The model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997).",1. Introduction,[0],[0]
"A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012).",1. Introduction,[0],[0]
"G-RBMs were used, e.g., in modeling human motion (Taylor et al., 2007) and natural image patches (Melchior et al., 2017).
",1. Introduction,[0],[0]
Given n i.i.d.,1. Introduction,[0],[0]
"samples x1, . . .",1. Introduction,[0],[0]
",xn from model (1), the goal is to estimate the weight matrixW .",1. Introduction,[0],[0]
A common approach for learning W is by maximum likelihood.,1. Introduction,[0],[0]
"As this function is non-convex, common optimization schemes include the EM algorithm and alternating least squares (ALS).",1. Introduction,[0],[0]
"In addition, several works developed iterative methods specialized to GRBMs (Hinton, 2010; Cho et al., 2011).",1. Introduction,[0],[0]
"All these methods, however, often lack consistency guarantees and may not be well suited for large datasets due to their potential slow convergence.",1. Introduction,[0],[0]
"This is not surprising, as learning W under model (1) is believed to be computationally hard; see for example Mossel & Roch (2005).
",1. Introduction,[0],[0]
"Over the past years, several works considered variants and specific instances of model (1) under additional assumptions on the distribution Ph or on the weight matrix W .",1. Introduction,[0],[0]
"For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyvärinen et al., 2004).",1. Introduction,[0],[0]
"In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyvärinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014).",1. Introduction,[0],[0]
"Similarly, when the hidden units are mutually exclusive, namely Ph has support h ∈ {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means.",1. Introduction,[0],[0]
"Efficient and consistent algorithms were derived for this case as well (Moitra & Valiant, 2010; Anandkumar et al., 2012a;b; Hsu & Kakade, 2013).",1. Introduction,[0],[0]
"Among those, most relevant to this work
are orthogonal tensor decomposition methods (Anandkumar et al., 2014).",1. Introduction,[0],[0]
"Interestingly, these methods can learn some additional latent models, with hidden units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models (Arabshahi & Anandkumar, 2017).
",1. Introduction,[0],[0]
Learning W given the observed data {xj}nj=1 can also be viewed as a noisy matrix factorization problem.,1. Introduction,[0],[0]
"If W is known to be non-negative, then various non-negative matrix factorization methods can be used.",1. Introduction,[0],[0]
"Moreover, under appropriate conditions, some of these methods were proven to be computationally efficient and consistent (Donoho & Stodden, 2004; Arora et al., 2012).",1. Introduction,[0],[0]
"For general full rank W , the matrix factorization method in Slawski et al. (2013) (SHL) exactly recovers W when σ = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and has no consistency guarantees when σ > 0.
",1. Introduction,[0],[0]
A tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed data.,1. Introduction,[0],[0]
"We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n − 12 ) for any noise level σ ≥ 0.
",1. Introduction,[0],[0]
The non-degeneracy conditions we pose are significantly weaker than those required by the previous tensor decomposition methods mentioned above.,1. Introduction,[0],[0]
"In particular, their assumptions and resulting methods can be viewed as specific cases of our more general approach.
",1. Introduction,[0],[0]
"Similarly to the matrix factorization method in Slawski et al. (2013), our algorithm has runtime linear in n, polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime is spent on computing the eigenpairs of a d× d× d tensor.",1. Introduction,[0],[0]
"Practically, our method, implemented without any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC.",1. Introduction,[0],[0]
"Furthermore, the overall runtime can be significantly reduced, since the step of computing the tensor eigenpairs can be embarrassingly parallelized.
",1. Introduction,[0],[0]
Paper outline In the next section we give necessary background on tensor eigenpairs.,1. Introduction,[0],[0]
"In Section 3 we introduce our
method in the case σ = 0.",1. Introduction,[0],[0]
The case σ ≥ 0 is treated in Section 4.,1. Introduction,[0],[0]
Experiments with our method and comparison to other approaches appear in Section 5.,1. Introduction,[0],[0]
All proofs are deferred to the supplementary material.,1. Introduction,[0],[0]
Notation Denote,2. Preliminaries,[0],[0]
"[d] = {1, . . .",2. Preliminaries,[0],[0]
", d} and ei as the i-th unit vector.",2. Preliminaries,[0],[0]
"We slightly abuse notation and view a matrixW also as the set of its columns, namely w ∈ W is some column of W and span(W ) is the span of all its columns.",2. Preliminaries,[0],[0]
"The unit sphere is denoted by Sd−1 = {u ∈ Rd : ‖u‖ = 1}.
",2. Preliminaries,[0],[0]
"A tensor T ∈ Rd×d×d is symmetric if Tijk = Tπ(i,j,k) for all permutations π of i, j, k.",2. Preliminaries,[0],[0]
"Here, we consider only symmetric tensors.",2. Preliminaries,[0],[0]
"T can also be seen as a multi-linear operator: for matrices W 1,W 2,W 3 with W i ∈ Rd×di , the tensor-mode product, denoted T (W 1,W 2,W 3), is a d1 × d2 × d3 tensor whose (i1, i2, i3)-th entry is∑
j1,j2,j3∈[d]
W 1j1i1W 2 j2i2W 3 j3i3Tj1j2j3 .
",2. Preliminaries,[0],[0]
Tensor eigenpairs Several types of eigenpairs of a tensor have been proposed.,2. Preliminaries,[0],[0]
"Here, we consider the following definition, termed Z-eigenpairs by Qi (2005) and l2-eigenpairs by Lim (2005).",2. Preliminaries,[0],[0]
"Henceforth we just call them eigenpairs.
",2. Preliminaries,[0],[0]
Definition 1.,2. Preliminaries,[0],[0]
"(u, λ) ∈ Rd × R is an eigenpair of T if
T (I,u,u) =",2. Preliminaries,[0],[0]
λu and ‖u‖ = 1.,2. Preliminaries,[0],[0]
"(2)
Note that if (u, λ) is an eigenpair then the eigenvalue is simply λ = T (u,u,u).",2. Preliminaries,[0],[0]
"In addition, (−u,−λ) is also an eigenpair.",2. Preliminaries,[0],[0]
"Following common practice we treat these two pairs as one and make the convention that λ ≥ 0.
",2. Preliminaries,[0],[0]
"In contrast to the matrix case, the number of eigenvalues {λ} of a tensor T ∈ Rd×d×d can be much larger than d. As shown by Cartwright & Sturmfels (2013), for a d× d× d tensor, there can be at most 2d",2. Preliminaries,[0],[0]
− 1 of them.,2. Preliminaries,[0],[0]
"With precise definitions appearing in Cartwright & Sturmfels (2013), for a generic tensor, all its eigenvalues have multiplicity one and the number of eigenpairs {(u, λ)} is at most 2d",2. Preliminaries,[0],[0]
"− 1.
",2. Preliminaries,[0],[0]
"In principle, enumerating the set of all eigenpairs of a general symmetric tensor is a #P problem (Hillar & Lim, 2013).",2. Preliminaries,[0],[0]
"Nevertheless, several methods have been proposed for computing at least some eigenpairs, including iterative higherorder power methods (Kolda & Mayo, 2011; 2014), homotopy continuation (Chen et al., 2016), semidefinite programming (Cui et al., 2014), and iterative Newton-based methods (Jaffe et al., 2017; Guo et al., 2017).",2. Preliminaries,[0],[0]
"We conclude this section with the definition of Newton-stable eigenpairs (Jaffe et al., 2017) which are most relevant to our work.
",2. Preliminaries,[0],[0]
"Newton-stable eigenpairs Equivalently to (2), eigenpairs of T can also be characterized by the function g : Rd → Rd,
g(u)",2. Preliminaries,[0],[0]
"= T (I,u,u)− T (u,u,u) · u. (3)
It is easy to verify that a pair (u, λ) with ‖u‖ = 1 is an eigenpair of T if and only if g(u) = 0 and λ = T (u,u,u).",2. Preliminaries,[0],[0]
"The stability of an eigenpair is determined by its Jacobian matrix ∇g(u) ∈ Rd×d, more precisely, by its projection into the d− 1 dimensional subspace orthogonal to u. Formally, let Lu ∈ Rd×(d−1) be a matrix with d− 1 orthonormal columns that span the subspace orthogonal to u and define the (d− 1)× (d− 1) projected Jacobian matrix
Jp(u) = L > u∇g(u)Lu.",2. Preliminaries,[0],[0]
"(4)
Definition 2.",2. Preliminaries,[0],[0]
"An eigenpair (u, λ) of T ∈ Rd×d×d is Newton-stable if the matrix Jp(u) has full rank d− 1.
",2. Preliminaries,[0],[0]
The homotopy continuation method in Chen et al. (2016) is guaranteed to compute all the Newton-stable eigenpairs of a tensor.,2. Preliminaries,[0],[0]
"Alternatively, all the Newton-stable eigenpairs can be computed by the iterative orthogonal Newton correction method (O–NCM) in Jaffe et al. (2017) as these are the attracting fixed points for this algorithm.",2. Preliminaries,[0],[0]
"Moreover, O–NCM converges to any Newton-stable eigenpair at a quadratic rate given a sufficiently close initial guess.",2. Preliminaries,[0],[0]
"Finally, for a generic tensor, all its eigenpairs are Newton-stable.",2. Preliminaries,[0],[0]
To motivate our approach for estimating the matrix W it is instructive to first consider the ideal noiseless case where σ = 0.,3. Learning in the noiseless case,[0],[0]
"In this case, model (1) takes the form x = W>h.",3. Learning in the noiseless case,[0],[0]
Our problem then becomes that of factorizing the observed matrix X =,3. Learning in the noiseless case,[0],[0]
"[x1, . . .",3. Learning in the noiseless case,[0],[0]
",xn] ∈",3. Learning in the noiseless case,[0],[0]
"Rm×n of n samples into a product of real and binary low-rank matrices,1
Find W ∈ Rd×m, H ∈ {0, 1}d×n s.t. X",3. Learning in the noiseless case,[0],[0]
"= W>H. (5)
To be able to recover W we first need conditions under which the decomposition of X into W and H is unique.",3. Learning in the noiseless case,[0],[0]
"Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore this degeneracy.",3. Learning in the noiseless case,[0],[0]
"A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid.",3. Learning in the noiseless case,[0],[0]
"Formally, H ∈ {0, 1}d×n is rigid if any non-trivial linear combination of its rows yields a non-binary vector: ∀u 6= 0,
u>H ∈ {0, 1}n ⇔ u ∈ {ei}di=1.",3. Learning in the noiseless case,[0],[0]
"(6)
Condition (6) is satisfied, for example, when the columns of H include ei and ei + ej for all i",3. Learning in the noiseless case,[0],[0]
6= j ∈,3. Learning in the noiseless case,[0],[0]
[d].,3. Learning in the noiseless case,[0],[0]
"If there
1Note that this is different from the problem known as “Boolean matrix factorization”, where X and W are assumed to be binary as well; see Miettinen & Vreeken (2014) and references therein.
exists a positive constant p0 > 0 such that Ph(ei) ≥ p0 and Ph(ei + ej) ≥ p0, then for a sample size n >",3. Learning in the noiseless case,[0],[0]
"2 log(d)/p0 the matrix H is rigid with high probability.
",3. Learning in the noiseless case,[0],[0]
"The following proposition, similar in nature to the (affine constrained) uniqueness guarantee in Slawski et al. (2013), shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary constraints.
",3. Learning in the noiseless case,[0],[0]
Proposition 1.,3. Learning in the noiseless case,[0],[0]
"Let X = W>H with H ∈ {0, 1}d×n rigid and W ∈ Rd×m full rank with m ≥ d.",3. Learning in the noiseless case,[0],[0]
Let W † ∈ Rm×d be the unique right pseudo-inverse of W so WW † = Id.,3. Learning in the noiseless case,[0],[0]
"Then W and H are unique and for all v ∈ span(X) \ {0},
v>X ∈ {0, 1}n ⇔ v ∈W †.",3. Learning in the noiseless case,[0],[0]
"(7)
Hence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of finding the unique set W † = {v∗1 , . .",3. Learning in the noiseless case,[0],[0]
.,3. Learning in the noiseless case,[0],[0]
",v∗d} ⊆ span(X) of d non-zero vectors that satisfy the binary constraints v∗i",3. Learning in the noiseless case,[0],[0]
">X ∈ {0, 1}n.",3. Learning in the noiseless case,[0],[0]
The weight matrix is then W =,3. Learning in the noiseless case,[0],[0]
"(W †)†.
Algorithm outline We recover W † via a two step procedure.",3. Learning in the noiseless case,[0],[0]
"First, a finite set V = {v1,v2, . . .",3. Learning in the noiseless case,[0],[0]
} ⊆ span(X) of candidate vectors is computed with a guarantee that W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"Specifically, V is computed from the set of eigenpairs of a d× d× d tensor, constructed from the low order moments of X .",3. Learning in the noiseless case,[0],[0]
"Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v ∈ V that satisfy v>X ∈ {0, 1}n.
",3. Learning in the noiseless case,[0],[0]
Before describing the two steps in more detail we first state the additional non-degeneracy conditions we pose.,3. Learning in the noiseless case,[0],[0]
"To this end, denote the unknown first, second, and third order moments of the latent binary vector h by
p = E[h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd, C = E[h⊗ h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd×d, C = E[h⊗ h⊗ h] ∈ Rd×d×d.",3. Learning in the noiseless case,[0],[0]
"(8)
Non-degeneracy conditions We assume the following:
(I) H is rigid.
",3. Learning in the noiseless case,[0],[0]
"(II) rank(2C(I, I, ei)− C) =",3. Learning in the noiseless case,[0],[0]
d for all i ∈,3. Learning in the noiseless case,[0],[0]
"[d].
",3. Learning in the noiseless case,[0],[0]
Condition (I) implies that both rank(HH>),3. Learning in the noiseless case,[0],[0]
= d and rank(C) = d. This in turn implies pi = E[hi],3. Learning in the noiseless case,[0],[0]
> 0,3. Learning in the noiseless case,[0],[0]
for all i ∈,3. Learning in the noiseless case,[0],[0]
[d] and that at most one variable hi has pi = 1.,3. Learning in the noiseless case,[0],[0]
"Such an “always on” variable can model a fixed bias to x. As far as we know, condition (II) is new and its nature will become clear shortly.
",3. Learning in the noiseless case,[0],[0]
"We now describe each step of our algorithm in more detail.
",3. Learning in the noiseless case,[0],[0]
"Computing the candidate set To compute a set V that is guaranteed to include the columns of W † we make use of
the second and third order moments of x,
M = E[x⊗ x] ∈ Rm×m, M = E[x⊗ x⊗ x] ∈ Rm×m×m.
(9)
",3. Learning in the noiseless case,[0],[0]
"Given a large number of samples n 1, these can be easily and accurately estimated from the sample X .",3. Learning in the noiseless case,[0],[0]
"For simplicity, in this section we consider the population setting where n → ∞, so M andM are known exactly.",3. Learning in the noiseless case,[0],[0]
"M andM are related to the unknown second and third order moments of h in (8) via (Anandkumar et al., 2014)
M = W>CW, M = C(W,W,W ).",3. Learning in the noiseless case,[0],[0]
"(10)
Since both C and W are full rank, the number of latent units can be deduced by rank(M) =",3. Learning in the noiseless case,[0],[0]
"d. Since C is positive definite, there is a whitening matrix K ∈ Rm×d such that
K>MK = Id. (11)
Such a K can be computed, for example, by an eigendecomposition of M .",3. Learning in the noiseless case,[0],[0]
"Although K is not unique, any K ⊆ span(M) that satisfies (11) suffices for our purpose.",3. Learning in the noiseless case,[0],[0]
"Define the d× d× d lower dimensional whitened tensor
W =M(K,K,K).",3. Learning in the noiseless case,[0],[0]
"(12)
Denote the set of eigenpairs ofW by
U = {(u, λ) ∈",3. Learning in the noiseless case,[0],[0]
"Sd−1 × R+ :W(I,u,u) = λu}.",3. Learning in the noiseless case,[0],[0]
"(13)
Our set of candidates is then
V = {Ku/λ : (u, λ) ∈ U with λ ≥ 1} ⊆ Rm. (14)
",3. Learning in the noiseless case,[0],[0]
The following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W †.,3. Learning in the noiseless case,[0],[0]
Lemma 1.,3. Learning in the noiseless case,[0],[0]
LetW be the tensor in (12) corresponding to model (1) with σ = 0 and let V be as in (14).,3. Learning in the noiseless case,[0],[0]
If condition (I) holds then W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"In particular, each (ui, λi) in the set of d relevant eigenpairs
U∗ = {(u, λ) ∈ U : Ku/λ ∈W †} (15)
has the eigenvalue λi = 1/ √ pi ≥ 1 where pi = E[hi] > 0.
",3. Learning in the noiseless case,[0],[0]
"Computing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W † by first calculating the set U of eigenpairs ofW .",3. Learning in the noiseless case,[0],[0]
"Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013).",3. Learning in the noiseless case,[0],[0]
"Moreover, besides the d columns of W †, the set V in (14) may contain many spurious candidates, as the number of eigenpairs of W is typically O(2d)",3. Learning in the noiseless case,[0],[0]
"d (Cartwright & Sturmfels, 2013).
",3. Learning in the noiseless case,[0],[0]
"Nevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs of a tensor under appropriate stability conditions.",3. Learning in the noiseless case,[0],[0]
"The following lemma highlights the importance of condition (II) for the stability of the eigenpairs in U∗. Note that conditions (I)-(II) do not depend on W , but only on the distribution of h.
Lemma 2.",3. Learning in the noiseless case,[0],[0]
Let W be the whitened tensor in (12) corresponding to model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then all (u, λ) ∈ U∗ are Newton-stable eigenpairs ofW .
",3. Learning in the noiseless case,[0],[0]
"Hence, under conditions (I)-(II), the homotopy method in Chen et al. (2016), or alternatively the O–NCM with a sufficiently large number of random initializations (Jaffe et al., 2017), are guaranteed to compute a candidate set V which includes all the columns of W †.",3. Learning in the noiseless case,[0],[0]
"The next step is to extract W † out of V .
",3. Learning in the noiseless case,[0],[0]
Filtering As suggested by Eq.,3. Learning in the noiseless case,[0],[0]
"(7) we select the subset of vectors V̄ ⊆ V that satisfy the binary constraints,
V̄",3. Learning in the noiseless case,[0],[0]
"= {v ∈ V : vTX ∈ {0, 1}n}.",3. Learning in the noiseless case,[0],[0]
"(16)
Indeed, under condition (I), Proposition 1 implies that V̄ = W † and the weight matrix is thus W = V̄ †.
Algorithm 2 in Appendix C summarizes our method for the noiseless case and has the following recovery guarantee.",3. Learning in the noiseless case,[0],[0]
Theorem 1.,3. Learning in the noiseless case,[0],[0]
Let X be a matrix of n samples from model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then the above method recovers W exactly.
",3. Learning in the noiseless case,[0],[0]
"We note that when σ = 0 and conditions (I)-(II) hold for the empirical latent moments Ĉ and Ĉ (rather than C and C), the above procedure exactly recovers W when M and M are replaced by their finite sample estimates.",3. Learning in the noiseless case,[0],[0]
The matrix factorization method SHL in Slawski et al. (2013) also exactly recovers W in the case σ = 0.,3. Learning in the noiseless case,[0],[0]
"While its runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach.",3. Learning in the noiseless case,[0],[0]
"This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d × d × d tensor.",3. Learning in the noiseless case,[0],[0]
"However, SHL does not take advantage of the large number of samples n, since only m× d sub-matrices of the m×n sample matrix X are used for constructing its candidate set.",3. Learning in the noiseless case,[0],[0]
"Indeed, in the noisy case where σ > 0, SHL has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels of noise.",3. Learning in the noiseless case,[0],[0]
In the next section we derive a modified version of our method that consistently estimates W for any noise level σ ≥ 0.,3. Learning in the noiseless case,[0],[0]
The method in Section 3 to estimateW is clearly inadequate when σ > 0.,4. Learning in the presence of noise,[0],[0]
"However, we now show that by making several adjustments, the two steps of computing the candidate set and its filtering can be both made robust to noise, yielding a consistent estimator of W for any σ ≥ 0.
",4. Learning in the presence of noise,[0],[0]
"Computing the candidate set As in the case σ = 0, our goal in the first step is to compute a finite candidate set
Vσ ⊆ Rm that is guaranteed to contain accurate estimates for the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment µ = E[x] and define the following noise corrected moments,
Mσ = M − σ2Im,
Mσ = M− σ2 m∑ i=1",4. Learning in the presence of noise,[0],[0]
"( µ⊗ ei ⊗ ei
+ ei ⊗ µ⊗ ei + ei ⊗ ei ⊗ µ ) .
",4. Learning in the presence of noise,[0],[0]
"(17)
By assumption, the noise satisfies E[ξ3i ] = 0.",4. Learning in the presence of noise,[0],[0]
"Thus, similarly to the moment equations in (10), the modified moments in (17) are related to these ofh by (Anandkumar et al., 2014)
Mσ = W >CW, Mσ = C(W,W,W ).",4. Learning in the presence of noise,[0],[0]
"(18)
Hence, if Mσ andMσ were known exactly, a candidate set Vσ that contains W † could be obtained exactly as in the noiseless case, but with M andM replaced with Mσ and Mσ; namely, first calculate the whitening matrix Kσ such that K>σMσKσ =",4. Learning in the presence of noise,[0],[0]
Id,4. Learning in the presence of noise,[0],[0]
"and then compute the eigenpairs of the population whitened tensor
Wσ =Mσ(Kσ,Kσ,Kσ).",4. Learning in the presence of noise,[0],[0]
"(19)
In practice, σ2, d, µ,M andM are all unknown and need to be estimated from the sample matrix X .",4. Learning in the presence of noise,[0],[0]
"Assuming m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009).",4. Learning in the presence of noise,[0],[0]
"For simplicity, we assume they are known exactly.",4. Learning in the presence of noise,[0],[0]
"Similarly, µ, M ,M are consistently estimated by their empirical means, µ̂, M̂ , and M̂.",4. Learning in the presence of noise,[0],[0]
"So, after computing the plugin estimates K̂σ such that K̂>σ M̂σK̂σ = Id and Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ), we compute the set Ûσ of eigenpairs of Ŵσ and for some small 0 < τ",4. Learning in the presence of noise,[0],[0]
"= O(n− 1 2 ) take our candidate set as
V̂σ = {K̂σu/λ : (u, λ) ∈ Ûσ with λ ≥ 1−τ}.",4. Learning in the presence of noise,[0],[0]
"(20)
The following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations.",4. Learning in the presence of noise,[0],[0]
"Namely, for perturbations of order δ 1 inWσ and Kσ , the method computes a candidate set V̂σ that contains a subset of d vectors that are O(δ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"Furthermore, these d vectors all correspond to Newton-stable eigenpairs of the perturbed tensor and are Ω(1) separated from the other candidates in V̂σ .
",4. Learning in the presence of noise,[0],[0]
Lemma 3.,4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let K̂σ, Ŵσ be their perturbed versions, inducing the candidate set V̂σ in (20).",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) hold, then there are c, δ0, δ1 > 0",4. Learning in the presence of noise,[0],[0]
"such that for all 0 ≤ δ ≤ δ0 the following holds: If the perturbed versions satisfy
max{‖Ŵσ −Wσ‖F , ‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F } ≤ δ, (21)
",4. Learning in the presence of noise,[0],[0]
"then any v∗ ∈W † has a unique v̂ ∈ V̂σ such that
‖v̂ − v∗‖ ≤ cδ.",4. Learning in the presence of noise,[0],[0]
"(22)
",4. Learning in the presence of noise,[0],[0]
"Moreover, v̂ corresponds to a Newton-stable eigenpair of Ŵσ with eigenvalue λ ≥ 1− cδ and for all ṽ ∈ V̂σ \ {v̂},
‖ṽ",4. Learning in the presence of noise,[0],[0]
− v∗‖ ≥ δ1 > 2cδ.,4. Learning in the presence of noise,[0],[0]
"(23)
",4. Learning in the presence of noise,[0],[0]
"The proof is based on the implicit function theorem (Hubbard & Hubbard, 2015); small perturbations to a tensor result in small perturbations to its Newton-stable eigenpairs.
",4. Learning in the presence of noise,[0],[0]
"Now, by the delta method, the plugin estimates K̂σ and Ŵσ are both OP (n− 1 2 ) close to their population quantities,
‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F = OP (n − 12 ),
‖Ŵσ −Wσ‖F = OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
"(24)
By (24), we have that (21) holds with δ = OP (n− 1 2 ).",4. Learning in the presence of noise,[0],[0]
"Hence, by Lemma 3, the eigenpairs of Ŵσ provide a candidate set V̂σ that contains d vectors that are OP (n− 1 2 ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"In addition, any irrelevant candidate is ΩP (1) far away from W †.",4. Learning in the presence of noise,[0],[0]
"As we show next, these properties ensure that with high probability the d relevant candidates can be identified in V̂σ .
",4. Learning in the presence of noise,[0],[0]
"Filtering Given the candidate set V̂σ computed in the first step, our goal now is to find a set V̄σ ⊆ V̂σ of d vectors that accurately estimate the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To simplify the theoretical analysis, we assume the filtering step is done using a sample X of size n that is independent of V̂σ.",4. Learning in the presence of noise,[0],[0]
"This can be achieved by first splitting a given sample of size 2n into two sets of size n, one for each step.
",4. Learning in the presence of noise,[0],[0]
"Recall that for x from model (1) and any v ∈ Rm,
v>x = v>W>h+",4. Learning in the presence of noise,[0],[0]
"σv>ξ. (25)
",4. Learning in the presence of noise,[0],[0]
"Obviously, when σ > 0, the filtering procedure in (16) for the noiseless case is inadequate, as typically no v∗ ∈ W † will exactly satisfy v∗>X ∈ {0, 1}n.",4. Learning in the presence of noise,[0],[0]
"Nevertheless, we expect that for a sufficiently small noise level σ, any v ∈ V̂σ that is close to some v∗ ∈ W † will result in v>X that is close to being binary, while any v sufficiently far from W † will result in v>X that is far from being binary.",4. Learning in the presence of noise,[0],[0]
"A natural measure for how v>X is “far from being binary”, similar to the one used for filtering in Slawski et al. (2013), is simply its deviation from its binary rounding,
min b∈{0,1}n
‖vTX − b‖2
n‖v‖2 .",4. Learning in the presence of noise,[0],[0]
"(26)
Eq. (26) works extremely well for small σ, but fails for high noise levels.",4. Learning in the presence of noise,[0],[0]
"Here we instead propose a filtering procedure based on the classical Kolmogorov-Smirnov goodness of fit
test (Lehmann & Romano, 2006).",4. Learning in the presence of noise,[0],[0]
"As we show below, this approach gives consistent estimates of W for any σ > 0.
",4. Learning in the presence of noise,[0],[0]
"Before describing the test, we first introduce the probabilistic analogue of the rigidity condition (6).",4. Learning in the presence of noise,[0],[0]
"For any u ∈ Rd, define its corresponding expected binary rounding error,
r(u) = Eh∼Ph",4. Learning in the presence of noise,[0],[0]
"[
min b∈{0,1}
(u>h− b)2 ] .
",4. Learning in the presence of noise,[0],[0]
"Clearly, r(0) = 0 and r(ei) = 0 for all i ∈",4. Learning in the presence of noise,[0],[0]
[d].,4. Learning in the presence of noise,[0],[0]
"We pose the following expected rigidity condition: for all u 6= 0,
r(u)",4. Learning in the presence of noise,[0],[0]
= 0 ⇔ u ∈ {ei}di=1.,4. Learning in the presence of noise,[0],[0]
"(27)
Analogously to the deterministic rigidity condition in (6), condition (27) is satisfied, for example, when Ph(ei) > 0",4. Learning in the presence of noise,[0],[0]
and Ph(ei + ej) > 0,4. Learning in the presence of noise,[0],[0]
for all i 6=,4. Learning in the presence of noise,[0],[0]
j ∈,4. Learning in the presence of noise,[0],[0]
"[d].
To introduce our filtering test, recall that under model (1), ξ ∼ N (0, Im).",4. Learning in the presence of noise,[0],[0]
"Hence, for any fixed v, the random variable v>x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),
v>x ∼ ∑
h∈{0,1}d Ph(h) · N",4. Learning in the presence of noise,[0],[0]
"(v>W>h, σ2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(28)
",4. Learning in the presence of noise,[0],[0]
Denote the cumulative distribution function of v>x by Fv .,4. Learning in the presence of noise,[0],[0]
"For general v, this mixture may have up to 2d distinct components.",4. Learning in the presence of noise,[0],[0]
"However, for v∗ ∈W †, it reduces to a mixture of two components with means at 0 and 1.",4. Learning in the presence of noise,[0],[0]
"More precisely, for any candidate v with corresponding eigenvalue λ(v)",4. Learning in the presence of noise,[0],[0]
"≥ 1, define the GMM with two components
(1− 1λ(v)2 ) · N (0, σ 2‖v‖2)",4. Learning in the presence of noise,[0],[0]
"+ 1λ(v)2 · N (1, σ 2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(29)
Denote its cumulative distribution function by Gv.",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that under condition (27), Gv fully characterizes the columns of W †.
Lemma 4.",4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let Vσ be the set of population candidates as computed from the eigenpairs of Wσ.",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then for any v ∈",4. Learning in the presence of noise,[0],[0]
"Vσ with corresponding eigenvalue λ(v),
Fv = Gv ⇔ v ∈W †.
",4. Learning in the presence of noise,[0],[0]
"Given the empirical candidate set V̂σ, Lemma 4 suggests ranking all v̂ ∈ V̂σ according to their goodness of fit to Gv̂ and taking the d candidates with the best fit.",4. Learning in the presence of noise,[0],[0]
"More precisely, given a sample X =",4. Learning in the presence of noise,[0],[0]
"[x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn] that is independent of V̂σ , for each candidate v̂ ∈ V̂σ we compute the empirical cumulative distribution function, F̂v̂(t) =",4. Learning in the presence of noise,[0],[0]
1n,4. Learning in the presence of noise,[0],[0]
"∑n j=1 1{v̂>xj ≤ t}, t ∈ R, and calculate its Kolmogorov-Smirnov score
∆n(v̂) = sup t∈R |F̂v̂(t)−Gv̂(t)|.",4. Learning in the presence of noise,[0],[0]
"(30)
Algorithm 1 Estimate W when σ > 0 and n <∞",4. Learning in the presence of noise,[0],[0]
"Input: sample matrix X ∈ Rm×n and 0 < τ 1
1: estimate number of hidden units d and noise level σ2 2: compute empirical moments µ̂, M̂ and M̂ and plugin moments M̂σ and M̂σ of (17) 3: compute K̂σ such that K̂>σ M̂σK̂σ = Id 4: construct Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ) 5: compute the set Ûσ of eigenpairs of Ŵσ 6: compute the candidate set V̂σ in (20) 7: for each v̂ ∈ V̂σ compute its KS score ∆n(v̂) in (30) 8: select V̄σ ⊆ V̂σ of d vectors with smallest ∆n(v̂) 9: return the pseudo-inverse Ŵ = V̄ †σ
Our estimator V̄σ ⊆ V̂σ for W † is then the set of d vectors with the smallest scores ∆n(v̂).",4. Learning in the presence of noise,[0],[0]
"The estimator for W is the pseudo-inverse, Ŵ = V̄ †σ .
",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that for sufficiently large n, ∆n(v̂) accurately distinguishes between v̂ ∈ V̂σ that are close to the columns of W † from these that are not.
",4. Learning in the presence of noise,[0],[0]
Lemma 5.,4. Learning in the presence of noise,[0],[0]
"Let v∗ ∈ W † and v̂(1), v̂(2), . . .",4. Learning in the presence of noise,[0],[0]
"a sequence of random vectors such that ‖v̂(n) − v∗‖ = OP (n
− 12 ).",4. Learning in the presence of noise,[0],[0]
"Then, ∆n(v̂(n))",4. Learning in the presence of noise,[0],[0]
= oP (1).,4. Learning in the presence of noise,[0],[0]
"In contrast, if minv∗∈W † ‖v̂(n) − v∗‖ = ΩP (1), then ∆n(v̂(n)) = ΩP (1), provided the expected rigidity condition (27) holds.
",4. Learning in the presence of noise,[0],[0]
"Lemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example Lehmann & Romano (2006); Billingsley (2013).
",4. Learning in the presence of noise,[0],[0]
Algorithm 1 summarizes our method for estimating W in the general case where σ > 0 and n <,4. Learning in the presence of noise,[0],[0]
"∞. The following theorem establishes its consistency.
",4. Learning in the presence of noise,[0],[0]
Theorem 2.,4. Learning in the presence of noise,[0],[0]
"Let x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn be n i.i.d.",4. Learning in the presence of noise,[0],[0]
samples from model (1).,4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then the estimator Ŵ computed by Algorithm 1 is consistent, achieving the parametric rate,
Ŵ = W +OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
Runtime The runtime of Algorithm 1 is composed of three main parts.,4. Learning in the presence of noise,[0],[0]
"First, O(nm3) operations are needed to compute all the relevant moments from the data and to construct the d× d× d whitened tensor Ŵσ .",4. Learning in the presence of noise,[0],[0]
"The most time consuming task is computing the eigenpairs of Ŵσ, which can be done by either the homotopy method or O–NCM.",4. Learning in the presence of noise,[0],[0]
"Currently, no runtime guarantees are available for either of these methods.",4. Learning in the presence of noise,[0],[0]
"In practice, since there are O(2d) eigenpairs, these methods spend O(2d · poly(d)) operations in total.",4. Learning in the presence of noise,[0],[0]
"Finally, since there are O(2d) candidates and each KS test takes O(dn) operations (Gonzalez et al., 1977), the filtering procedure runtime is O(d2dn).
",4. Learning in the presence of noise,[0],[0]
Power-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact that the set UN of Newton-stable eigenpairs ofWσ is typically O(2d).,4. Learning in the presence of noise,[0],[0]
"However, in some cases, the set U∗ of d relevant eigenpairs has additional structure so that a smaller candidate set may be computed instead of UN .",4. Learning in the presence of noise,[0],[0]
Consider the subset UP ⊆ UN of power-stable eigenpairs ofWσ:,4. Learning in the presence of noise,[0],[0]
Definition 3.,4. Learning in the presence of noise,[0],[0]
"An eigenpair (u, λ) is power-stable if its projected Jacobian Jp(u) is either positive or negative definite.
",4. Learning in the presence of noise,[0],[0]
"Typically, the number of power-stable eigenpairs is significantly smaller than the number of Newton-stable eigenpairs.2 In addition, UP can be computed by the shifted higher-order power method (Kolda & Mayo, 2011; 2014).
",4. Learning in the presence of noise,[0],[0]
"Similarly to Lemma 2, one can show that UP is guaranteed to contain U∗ whenever the following stronger version of condition (II) holds: for all (ui, λi) ∈ U∗, the matrix
(WKLui) >(2C(I, I, ei)− C)(WKLui) (31)
is either positive-definite or negative-definite.
",4. Learning in the presence of noise,[0],[0]
"As an example, consider the case where Ph has the support h ∈",4. Learning in the presence of noise,[0],[0]
Id.,4. Learning in the presence of noise,[0],[0]
Then model (1) corresponds to a GMM with d spherical components with linearly independent means.,4. Learning in the presence of noise,[0],[0]
"In this case, bothC and C are diagonal with p on their diagonal.",4. Learning in the presence of noise,[0],[0]
"Thus, the matrices in (31) take the form −L>ei diag(p)Lei , which are all negative-definite when p > 0.",4. Learning in the presence of noise,[0],[0]
"In fact, in this case, Wσ has an orthogonal CP decomposition and the d orthogonal eigenpairs in U∗ are the only negative-definite power-stable eigenpairs ofWσ (Anandkumar et al., 2014).",4. Learning in the presence of noise,[0],[0]
"Similarly, when Ph is a product distribution, the same orthogonal structure appears if the centered moments of x are used instead of M andM. As shown in Anandkumar et al. (2014), the power method, accompanied with a deflation procedure, decomposes an orthogonal tensor in polynomial time, thus implying an efficient algorithm in these cases.",4. Learning in the presence of noise,[0],[0]
"However, under the much weaker conditions we pose on Ph, the relevant eigenpairs in U∗ are not necessarily powerstable and the CP decomposition ofWσ does not necessarily include U∗.",4. Learning in the presence of noise,[0],[0]
We demonstrate our method in three scenarios: (I) simulations from the exact binary model (1); (II) learning a common population genetic admixture model; (III) learning the proportion matrix of a cell mixture from DNA methylation levels.,5. Experiments,[0],[0]
"Due to lack of space, (III) is deferred to Appendix N. Code to reproduce the simulation results can be found at https://github.com/arJaffe/ BinaryLatentVariables.
",5. Experiments,[0],[0]
2We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d.,5. Experiments,[0],[0]
"We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and Gaussian noise ξ ∼ N (0, Im).",5.1. Simulations,[0],[0]
The m columns of W were drawn uniformly from the unit sphere Sd−1.,5.1. Simulations,[0],[0]
"Fixing a mean vector a ∈ Rd and a covariance matrix R ∈ Rd×d, each hidden vector h was generated independently by first drawing r ∼ N (a, R) and then taking its binary rounding.
",5.1. Simulations,[0],[0]
"Figure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and σ (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.
",5.1. Simulations,[0],[0]
"As one can see, as opposed to SHL, our method is consistent for σ > 0 and achieves an error rate O(n−",5.1. Simulations,[0],[0]
1 2 ) corresponding to a slope of −1 in the upper panel of Fig. 2.,5.1. Simulations,[0],[0]
"In addition, as seen in the lower panel of Fig. 2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate.",5.1. Simulations,[0],[0]
"Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error
3Code from https://sites.google.com/site/ slawskimartin/code.",5.1. Simulations,[0],[0]
"For each realization X , we made 50 runs of SHL and chose H,W minimizing",5.1. Simulations,[0],[0]
‖X −W>,5.1. Simulations,[0],[0]
"H‖F .
for high noise levels.",5.1. Simulations,[0],[0]
A comparison between the runtime of SHL and the spectral method appears in Appendix I.,5.1. Simulations,[0],[0]
"We present an application of our method to a fundamental problem in population genetics, known as admixture (see Fig. 3).",5.2. Population genetic admixture,[0],[0]
"Admixture refers to the mixing of d ≥ 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008).",5.2. Population genetic admixture,[0],[0]
"The observed data X is an m× n matrix where m is the number of modern “admixed” individuals and n is the number of relevant locations in their DNA, known as SNPs.",5.2. Population genetic admixture,[0],[0]
Each SNP corresponds to two alleles and individuals may have different alleles.,5.2. Population genetic admixture,[0],[0]
"Fixing a reference allele for each location, Xij takes values in {0, 12 , 1} according to the number of reference alleles appearing in the genotype of individual",5.2. Population genetic admixture,[0],[0]
i ∈,5.2. Population genetic admixture,[0],[0]
[m] at locus j ∈,5.2. Population genetic admixture,[0],[0]
"[n].
",5.2. Population genetic admixture,[0],[0]
"Given the genotypes X , an important problem in population genetics is to estimate the following two quantities.",5.2. Population genetic admixture,[0],[0]
The allele frequency matrix H ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus j ∈",5.2. Population genetic admixture,[0],[0]
[n] in ancestral population k ∈,5.2. Population genetic admixture,[0],[0]
[d]; and the admixture proportion matrix W ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×m whose columns sum to 1 and its entry Wki is the proportion of individual",5.2. Population genetic admixture,[0],[0]
"i’s genome that was inherited from population k.
A common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {0, 1, 2} is the sum of two i.i.d.",5.2. Population genetic admixture,[0],[0]
"Bernoulli random variables with success probability Fij = ∑d k=1WkiHkj , namely, Xij |H ∼ 1 2 · Binomial(2, Fij).",5.2. Population genetic admixture,[0],[0]
"Note that under this model
E[X|H] = F = W>H. (32)
",5.2. Population genetic admixture,[0],[0]
"Although (32) has similar form to model (1), there are two main differences; the noise is not normally distributed and the matrix H is non-binary.",5.2. Population genetic admixture,[0],[0]
"Yet, the binary model (1) serves as a good approximation whenever various alleles are rare in some populations but abundant in others.",5.2. Population genetic admixture,[0],[0]
"Specifically, for ancestral populations that have been long separated, some alleles may become fixed in one population (i.e., reach frequency of 1) while being totally absent in others.
",5.2. Population genetic admixture,[0],[0]
"Simulating genetic admixture We followed a standard simulation scheme apllied, for example, in Xue et al. (2017); Gravel (2012); Price et al. (2009).",5.2. Population genetic admixture,[0],[0]
"First, using SCRM (Staab et al., 2015), we simulated d = 3 ancestral populations separated for 4000 generations and generated the genomes of 40 individuals for each.",5.2. Population genetic admixture,[0],[0]
H was then computed as the frequency of the reference alleles in each population.,5.2. Population genetic admixture,[0],[0]
"Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter α ≥ 0.",5.2. Population genetic admixture,[0],[0]
"Finally, the genomes of m = 50 admixed individuals were generated as mosaics of genomic segments of individuals from the ancestral populations with proportions W .",5.2. Population genetic admixture,[0],[0]
"The mosaic nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs (Xue et al., 2017).",5.2. Population genetic admixture,[0],[0]
"A detailed description is in Appendix M.
We compare our algorithm to two methods.",5.2. Population genetic admixture,[0],[0]
"The first is Admixture (Alexander et al., 2009), one of the most widely used algorithms in population genetics, which aims to maximize the likelihood of X .",5.2. Population genetic admixture,[0],[0]
"The second is the recently proposed spectral method ALStructure (Cabreros & Storey, 2017), where an estimation of span(W>) via Chen & Storey (2015) is followed by constrained ALS iterations of W and H .",5.2. Population genetic admixture,[0],[0]
"For our method, two modification are needed for Algorithm 1.",5.2. Population genetic admixture,[0],[0]
"First, since the distribution ofXij−wTi hj is not Gaussian, the corrected moments M̂σ,M̂σ as calculated by (17) do not satisfy (18).",5.2. Population genetic admixture,[0],[0]
"Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details.",5.2. Population genetic admixture,[0],[0]
"In addition, the filtering process described in Section 4 is no longer valid.",5.2. Population genetic admixture,[0],[0]
"However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.
",5.2. Population genetic admixture,[0],[0]
"Figure 4 compares the results of the 3 methods for α = 0.1, 1, 10.",5.2. Population genetic admixture,[0],[0]
"The spectral method outperforms Admixture and ALStructure for α = 1, 10 and performs similarly to Admixture for α = 0.1.
",5.2. Population genetic admixture,[0],[0]
Acknowledgements This research was funded in part by NIH Grant 1R01HG008383-01A1.,5.2. Population genetic admixture,[0],[0]
Uniqueness of the factorization readily follows from (7) so we proceed to prove (7).,A. Proof of Proposition 1,[0],[0]
First note that span(X) = span(W>) =,A. Proof of Proposition 1,[0],[0]
span(W †).,A. Proof of Proposition 1,[0],[0]
"Since W is full rank, we have WW † = Id.",A. Proof of Proposition 1,[0],[0]
"Hence,
(W †)>X = (WW †)>H = H ∈ {0, 1}d×n.
",A. Proof of Proposition 1,[0],[0]
So any v∗ ∈ W † satisfies the binary constraint,A. Proof of Proposition 1,[0],[0]
"v∗>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"For the other direction, let v ∈ span(X)",A. Proof of Proposition 1,[0],[0]
"\ {0} be such that v>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"Since v>X = (Wv)>H , the rigidity condition (6) implies Wv ∈ {ei}di=1.",A. Proof of Proposition 1,[0],[0]
"Since W is full rank and v ∈ span(W †), v must be a column of W †.",A. Proof of Proposition 1,[0],[0]
"Since the vector h is binary, its second and third order moments are related as follows.",B. Proof of Lemma 1,[0],[0]
"For all i, j ∈",B. Proof of Lemma 1,[0],[0]
"[d],
Ciij = Ciji = Cjii = E[h2ihj",B. Proof of Lemma 1,[0],[0]
] = E[hihj ] = Cij .,B. Proof of Lemma 1,[0],[0]
"(33)
",B. Proof of Lemma 1,[0],[0]
"Since W is full rank, WW † = Id.",B. Proof of Lemma 1,[0],[0]
"Hence, applying W † multi-linearly on the moment equations in (10) we obtain
C =",B. Proof of Lemma 1,[0],[0]
"(W †)>MW †,
C = M(W †,W †,W †).
",B. Proof of Lemma 1,[0],[0]
"Thus, the equality in (33) is equivalent to
[M(W †,W †,W †)]iij =",B. Proof of Lemma 1,[0],[0]
[(W †)>MW †]ij .,B. Proof of Lemma 1,[0],[0]
"(34)
Let Y ∗ ∈ Rd×d be the full rank matrix that satisfies W † = KY ∗ where K is the whitening matrix in (11).",B. Proof of Lemma 1,[0],[0]
"Then,
M(W †,W †,W †) = M(KY ∗,KY ∗,KY ∗) = W(Y ∗, Y ∗, Y ∗)
whereW is the whitened tensor in (12).",B. Proof of Lemma 1,[0],[0]
"Similarly, by (11),
(W †)>MW † = (Y ∗)>(KTMK)(Y ∗) =",B. Proof of Lemma 1,[0],[0]
"(Y ∗)>Y ∗.
Inserting these into (34), the matrix Y ∗ must satisfy
[W(Y ∗, Y ∗, Y ∗)]iij =",B. Proof of Lemma 1,[0],[0]
"[(Y ∗)>(Y ∗)]ij , ∀i, j ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"(35)
The following lemma, proved in Appendix H, shows that Eq. (35) is nothing but a tensor eigen-problem.",B. Proof of Lemma 1,[0],[0]
"Specifically, the columns of Y ∗, up to scaling, are eigenvectors ofW .",B. Proof of Lemma 1,[0],[0]
Lemma 6.,B. Proof of Lemma 1,[0],[0]
Let W ∈ Rd×d×d be an arbitrary symmetric tensor.,B. Proof of Lemma 1,[0],[0]
"Then, a matrix Y =",B. Proof of Lemma 1,[0],[0]
"[y1, . . .",B. Proof of Lemma 1,[0],[0]
",yd] ∈ Rd×d of rank d satisfies (35) if and only if for all k ∈",B. Proof of Lemma 1,[0],[0]
"[d], yk = uk/λk, where (uk, λk)dk=1 are d eigenpairs of W with linearly independent {uk}dk=1.
",B. Proof of Lemma 1,[0],[0]
"By Lemma 6, the set of scaled eigenpairs {y = u/λ} of W is guaranteed to contain the d columns of Y ∗. Since W † = KY ∗, the set {Ky} is guaranteed to contain W †.
To show that each y = u/λ",B. Proof of Lemma 1,[0],[0]
∈,B. Proof of Lemma 1,[0],[0]
"Y ∗ has λ ≥ 1, note that the vector Ky is a column of W †, so WKy = ei for some i ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"Hence, by the definition of the whitened tensor (12) and the moment equation (10),
W(y,y,y) = M(Ky,Ky,Ky) = C(WKy,WKy,WKy) = C(ei, ei, ei) = Ciii = E[hi] ≤ 1.
",B. Proof of Lemma 1,[0],[0]
"On the other hand, since (u, λ) is an eigenpair ofW with eigenvalue λ =W(u,u,u),
W(y,y,y) = 1 λ3 W(u,u,u) = 1 λ2 .
",B. Proof of Lemma 1,[0],[0]
"By convention, λ ≥ 0.",B. Proof of Lemma 1,[0],[0]
"Hence, λ = 1/ √ E[hi] ≥ 1,
concluding the proof.",B. Proof of Lemma 1,[0],[0]
Algorithm 2 Recover W when σ = 0,C. Recovery algorithm - noiseless case,[0],[0]
"Input: sample matrix X
1: estimate second and third order moments M ,M 2: set d = rank(M) 3: compute K ⊆ span(M) such that K>MK",C. Recovery algorithm - noiseless case,[0],[0]
=,C. Recovery algorithm - noiseless case,[0],[0]
"Id 4: compute whitened tensorW =M(K,K,K) 5: compute the set U of eigenpairs ofW 6: compute the candidate set V in (14) 7: filter V̄ = {v ∈ V : v>X ∈ {0, 1}n} 8: return the pseudo-inverse W = V̄ †",C. Recovery algorithm - noiseless case,[0],[0]
"Let (u, λ) ∈ U∗ be an eigenpair of W such that v∗ = Ku/λ ∈W †.",D. Proof of Lemma 2,[0],[0]
"To show Newton-stability we need to show that under conditions (I)-(II) the projected Jacobian matrix Jp(u) = L > u∇g(u)Lu in (4) is full rank d− 1.
",D. Proof of Lemma 2,[0],[0]
"The Jacobian matrix∇g(u) is
∇g(u) = 2W(I, I,u)− 3uW(I,u,u)>
−W(u,u,u)Id = 2W(I, I,u)− 3λuu>",D. Proof of Lemma 2,[0],[0]
− λId.,D. Proof of Lemma 2,[0],[0]
"(36)
Since L>uu = 0, the second term in (36) does not contribute to Jp(u).",D. Proof of Lemma 2,[0],[0]
"For the first term in (36), by (12) and (10),
W(I, I,u) =M(K,K,Ku) = C(WK,WK,WKu).
",D. Proof of Lemma 2,[0],[0]
"Since v∗ = Ku/λ is a column of W †, WKu =",D. Proof of Lemma 2,[0],[0]
λei for some,D. Proof of Lemma 2,[0],[0]
i ∈,D. Proof of Lemma 2,[0],[0]
[d].,D. Proof of Lemma 2,[0],[0]
"Thus,
W(I, I,u) = λC(WK,WK, ei) = λK>W>C(I, I, ei)WK.
",D. Proof of Lemma 2,[0],[0]
"For the third term in (36), by the definition of K in (11),
Id = K >MK = K>W>CWK.
Putting the last two equalities in (36) and applying the projection Lu we obtain
Jp(u) = L > u∇g(u)Lu
= λL>uK >W>(2C(I, I, ei)− C)WKLu.
",D. Proof of Lemma 2,[0],[0]
"Since λ ≥ 1 and W and K are full rank, condition (II) implies that Jp(u) is full rank as well.",D. Proof of Lemma 2,[0],[0]
"Thus, (u, λ) is a Newton-stable eigenpair ofW .",D. Proof of Lemma 2,[0],[0]
Lemma 3 follows from the following lemma which establishes the stability of Newton-stable eigenpairs of a tensor W to small perturbations W̃ =W + ∆W .,E. Proof of Lemma 3,[0],[0]
Lemma 7.,E. Proof of Lemma 3,[0],[0]
"Let (u, λ) be a Newton-stable eigenpair ofW with λ ≥ 1.",E. Proof of Lemma 3,[0],[0]
"There are c1, c2, ε0 > 0 such that for all sufficiently small ε > 0",E. Proof of Lemma 3,[0],[0]
the following holds.,E. Proof of Lemma 3,[0],[0]
"For any W̃ such that ‖W̃ −W‖F ≤ ε there exists a unique eigenpair (ũ, λ̃) of W̃ such that
‖u− ũ‖ ≤ c1ε and |λ̃− λ| ≤ c2ε.
",E. Proof of Lemma 3,[0],[0]
"In addition, (ũ, λ̃) is Newton-stable and any other eigenvector ṽ of W̃ satisfies ‖ṽ",E. Proof of Lemma 3,[0],[0]
"− u‖ ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
Proof of Lemma 7.,E. Proof of Lemma 3,[0],[0]
For a tensor T ∈ Rd×d×d let t ∈ Rs be the vector of s = d3 entries {Tijk}.,E. Proof of Lemma 3,[0],[0]
"Define the function Q : Rd+s → Rd by
Q(v, t)",E. Proof of Lemma 3,[0],[0]
= T,E. Proof of Lemma 3,[0],[0]
"(I,v,v)− T (v,v,v) ·",E. Proof of Lemma 3,[0],[0]
"v.
Note that for any t ∈ Rs and (v, β) ∈ Rd × R with v 6= 0 and β 6= 0, we have thatQ(v, t) = 0",E. Proof of Lemma 3,[0],[0]
"if and only if (v, β) is an eigenpair of t with eigenvalue β",E. Proof of Lemma 3,[0],[0]
"= T (v,v,v).4 Denote the gradients ofQ with respect to v and t by
A(v, t) = ∇vQ(v, t) ∈ Rd×d, B(v, t) = ∇tQ(v, t) ∈ Rd×s.
",E. Proof of Lemma 3,[0],[0]
"Let w ∈ Rs be the vectorization of W and let (u, λ) ∈",E. Proof of Lemma 3,[0],[0]
Sd−1 × R+ be a Newton-stable eigenpair of w with λ ≥ 1.,E. Proof of Lemma 3,[0],[0]
Since u is Newton-stable and λ,E. Proof of Lemma 3,[0],[0]
"> 0, A(u,w) is invertible.",E. Proof of Lemma 3,[0],[0]
"In addition, the following (d+ s)× (d+ s) matrix is invertible,
D(u,w) =
( A(u,w) B(u,w)
0",E. Proof of Lemma 3,[0],[0]
"Is
) .
",E. Proof of Lemma 3,[0],[0]
"4This does not precisely hold when β = 0 since Q(v, t) = 0 does not imply ‖v‖ = 1 in this case, but only that v is proportional to an eigenvector.
",E. Proof of Lemma 3,[0],[0]
"Let γD = 1/‖D(u,w)−1‖ > 0 be the smallest singular value of D(u,w) and let LD < ∞ be the Lipschitz constant of ∇Q(v, t) =",E. Proof of Lemma 3,[0],[0]
"[A(v, t), B(v, t)] ∈ Rd×(d+s) in a small neighborhood of (u,w), namely, ∀(v, t), (ṽ, t̃) in the neighborhood,
‖∇Q(v, t)−∇Q(ṽ, t̃)‖ ≤ LD‖(v, t)− (ṽ, t̃)‖.
Let Bε(w) ⊂ Rs be the ball of radius ε centered at w.",E. Proof of Lemma 3,[0],[0]
"Then by the implicit function theorem (Hubbard & Hubbard, 2015), for any ε ≤ ε1 := γ2D/(2LD), there exists a unique continuously differentiable mapping ũ : Bε(w)→ B2ε/γD (u) such that Q(ũ(w̃), w̃) = 0 for all w̃ ∈ Bε(w).",E. Proof of Lemma 3,[0],[0]
"In other words, for any w̃ such that ‖w̃ −w‖ ≤ ε, there exist a unique vector ũ in all B2ε/γD (u) that is an eigenpair of w̃. Equivalently, for W̃ such that ‖W̃ −W‖F ≤ ε, there exists a unique eigenvector ũ of W̃ such that
‖ũ− u‖ ≤ 2ε/γD",E. Proof of Lemma 3,[0],[0]
:= c1ε.,E. Proof of Lemma 3,[0],[0]
"(37)
",E. Proof of Lemma 3,[0],[0]
The bound on |λ̃ − λ| readily follows from (37).,E. Proof of Lemma 3,[0],[0]
"Indeed, let q : Rd+s → R be q(v, t) = T (v,v,v) and let Lλ be the Lipschitz constant of q in the neighborhood of (u,w).",E. Proof of Lemma 3,[0],[0]
"Then,
|λ̃− λ| = |q(ũ, w̃)− q(u,w)| ≤ Lλ √ ‖ũ− u‖2 + ‖w̃ −w‖2
≤",E. Proof of Lemma 3,[0],[0]
"Lλ √ 2
γD + 1 · ε",E. Proof of Lemma 3,[0],[0]
":= c2ε.
",E. Proof of Lemma 3,[0],[0]
"As for the Newton-stability of ũ, let r : Rd+s → R+ be r(v, t) = 1/‖A(v, t)−1‖, the minimal singular value of A(v, t).",E. Proof of Lemma 3,[0],[0]
"Since (u, λ) is a Newton-stable eigenpair of w, ∃γA > 0",E. Proof of Lemma 3,[0],[0]
"such that r(u,w) ≥ γA.",E. Proof of Lemma 3,[0],[0]
"Let Lγ be the Lipschitz constant of r(v, t) in the neighborhood (Golub & Van Loan, 2012).",E. Proof of Lemma 3,[0],[0]
"Then, for ε ≤ ε2 := γ/(2Lγ), we have r(ũ, w̃) ≥",E. Proof of Lemma 3,[0],[0]
"γA/2 > 0, so (ũ, λ̃) is a Newton-stable eigenpair of w̃.
Finally, we show that any other eigenvector ṽ of W̃ is apart from u. Since ũ is Newton-stable, there exists ε0 > 0",E. Proof of Lemma 3,[0],[0]
such that ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ ≥ 2ε0 for any other eigenvector ṽ. Hence, for ε ≤ ε0,
‖ṽ",E. Proof of Lemma 3,[0],[0]
− u‖ ≥ ∣∣‖ṽ − ũ‖ − ‖ũ−,E. Proof of Lemma 3,[0],[0]
u‖∣∣ ≥ ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ − ε ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
"Taking ε ≤ min{ε0, ε1, ε2} and c1, c2, ε0 as above concludes the proof of the lemma.
",E. Proof of Lemma 3,[0],[0]
"Lastly, for completeness, we show that γD ≥ γA√ γ2A+d .
γ−1D = ‖D(u,w) −1‖",E. Proof of Lemma 3,[0],[0]
"≤ √ ‖A(u,w)−1‖2(1 + ‖B(u,w)‖2) + ‖Is‖2
≤ √ 1 + 1 + ‖B(v,w)‖2
γ2A .",E. Proof of Lemma 3,[0],[0]
"(38)
",E. Proof of Lemma 3,[0],[0]
"To bound ‖B(u,w)‖, note thatQ(u,w) is linear in w and its i-th entry is given by
[Q(u,w)]i = ∑ k,l wiklukul",E. Proof of Lemma 3,[0],[0]
"− ( ∑ j,k,l wjklujukul)ui.
",E. Proof of Lemma 3,[0],[0]
"Thus, the d×m matrix B(u,w) has entries
[B(u,w)]i,(jkl) =",E. Proof of Lemma 3,[0],[0]
"[∇wQ(u,w)]i,(jkl) = (δij−uiuj)ukul,
which is independent of w. Recalling that ‖u‖ = 1,
‖B(u)‖2 ≤",E. Proof of Lemma 3,[0],[0]
"‖B(u)‖2F = d∑
i,j,k,l=1
(δij",E. Proof of Lemma 3,[0],[0]
"− uiuj)2u2ku2l
= d∑",E. Proof of Lemma 3,[0],[0]
"i,j=1 (δ2ij − 2δijuiuj + u2iu2j ) = d− 1.
Putting this bound in (38), we obtain γD ≥ γA√ γ2A+d .",E. Proof of Lemma 3,[0],[0]
Let v∗ ∈ W †.,F. Proof of Lemma 4,[0],[0]
Then ∃i ∈,F. Proof of Lemma 4,[0],[0]
"[d] such that v∗>W>h = hi ∈ {0, 1}.",F. Proof of Lemma 4,[0],[0]
"Hence, by (28), the c.d.f.",F. Proof of Lemma 4,[0],[0]
"Fv∗ of v∗>x corresponds to the two component GMM
(1− pi) · N (0, σ2‖v∗‖2) + pi · N (1, σ2‖v∗‖2).
",F. Proof of Lemma 4,[0],[0]
By Lemma 1 we have pi = 1/λ(v∗)2.,F. Proof of Lemma 4,[0],[0]
"Thus, Fv∗ = Gv∗ .
",F. Proof of Lemma 4,[0],[0]
"For the other direction, let v ∈",F. Proof of Lemma 4,[0],[0]
Vσ \W †.,F. Proof of Lemma 4,[0],[0]
"Since W is full rank, the d-dimensional vector u> = v>W> /∈",F. Proof of Lemma 4,[0],[0]
{e>i }di=1.,F. Proof of Lemma 4,[0],[0]
"Moreover, by Eq. (23) of Lemma 3,
inf v∈Vσ\W † min v∗∈W †
‖v − v∗‖ ≥ δ1 > 0.
",F. Proof of Lemma 4,[0],[0]
"Hence, there exists ε0 > 0",F. Proof of Lemma 4,[0],[0]
"such that
min i∈[d] ‖u− ei‖ ≥ ε0.
",F. Proof of Lemma 4,[0],[0]
"So by the expected rigidity condition (27), there exists η0 > 0",F. Proof of Lemma 4,[0],[0]
such that r(u) ≥ η0.,F. Proof of Lemma 4,[0],[0]
It follows that Fv has a component with mean that is bounded away from both 0 and 1 and thus Fv 6=,F. Proof of Lemma 4,[0],[0]
Gv .,F. Proof of Lemma 4,[0],[0]
"In particular, there exists η1 > 0 such that
sup t∈R |Fv(t)−Gv(t)| ≥ η1.",F. Proof of Lemma 4,[0],[0]
Recall that our sample of size 2n was split into two separate parts each of size,G. Proof of Lemma 5,[0],[0]
"n. The first n samples were used to estimate the tensor eigenvectors, and the last n samples to estimate the empirical cdf’s of their projections onto the eigenvectors.
",G. Proof of Lemma 5,[0],[0]
"For any v̂ that is close to a vector v, we bound ∆n(v̂) = ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
−Gv̂‖∞,G. Proof of Lemma 5,[0],[0]
"by the triangle inequality,
‖F̂v̂ −Gv̂‖∞ ≤ ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ + ‖Fv̂,G. Proof of Lemma 5,[0],[0]
− Fv‖∞ (39) + ‖Fv −Gv‖∞ + ‖Gv,G. Proof of Lemma 5,[0],[0]
"−Gv̂‖∞.
We now consider each of the four terms separately, starting with the first one.",G. Proof of Lemma 5,[0],[0]
"Since σ > 0, the cdf Fv̂ : R → [0, 1] is continuous and the distribution of ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
"− Fv̂‖∞ is independent of v̂. Then, by the Dvoretzky-Kiefer-Wolfowitz inequality, ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ is w.h.p.,G. Proof of Lemma 5,[0],[0]
"of order O(1/ √ n) for any v̂, and in particular tends to zero as n→ 0.
",G. Proof of Lemma 5,[0],[0]
"As for the second term, write v̂ = v + η.",G. Proof of Lemma 5,[0],[0]
"Then,
v̂>x = v>x+ η>x.
Recall that x = W>h + σξ.",G. Proof of Lemma 5,[0],[0]
"Hence, |η>x| ≤",G. Proof of Lemma 5,[0],[0]
‖W‖2 √ d‖η‖ + σ|η>ξ|.,G. Proof of Lemma 5,[0],[0]
The term η>ξ is simply a zero mean Gaussian random variable with standard deviation σ‖η‖.,G. Proof of Lemma 5,[0],[0]
"So, there exists",G. Proof of Lemma 5,[0],[0]
Kn > √ d‖W‖2 +,G. Proof of Lemma 5,[0],[0]
"σn1/3 such that with probability tending to one as n→∞, for all n samples xj ∈ X , |η>xj | ≤ Kn‖η‖.",G. Proof of Lemma 5,[0],[0]
"Thus, |v̂>x − v>x| can be bounded by Kn‖v̂ − v‖.",G. Proof of Lemma 5,[0],[0]
"This, in turn, implies that
‖Fv̂ − Fv‖∞ ≤ LKn‖v̂",G. Proof of Lemma 5,[0],[0]
"− v‖,
where L = maxt F ′v(t), which is finite for any σ > 0.",G. Proof of Lemma 5,[0],[0]
"Now, suppose the sequence v̂(n) converges to some v at rate OP (1/ √ n).",G. Proof of Lemma 5,[0],[0]
"Since Kn grows much more slowly with n, this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Let us next consider the fourth term, and leave the third term to the end.",G. Proof of Lemma 5,[0],[0]
Here note that Gv is continuous in its parameter v.,G. Proof of Lemma 5,[0],[0]
"So if the sequence v̂(n) converges to some v, then this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Finally, consider the third term.",G. Proof of Lemma 5,[0],[0]
"If the limiting vector v belongs to the correct set, namely v∗ ∈W †, then Fv = Gv , and thus overall ‖F̂v̂ −Gv̂‖∞ tends to zero as required.
",G. Proof of Lemma 5,[0],[0]
"In contrast, if v̂ converges to a vector v /∈W †, then instead of Eq.",G. Proof of Lemma 5,[0],[0]
"(39) we invoke the following inequality:
‖F̂v̂ −Gv̂‖∞ ≥ ‖Fv −Gv‖∞",G. Proof of Lemma 5,[0],[0]
− ‖Fv − Fv̂‖∞ −‖Fv̂ − F̂v̂‖∞,G. Proof of Lemma 5,[0],[0]
"− ‖Gv̂ −Gv‖∞.
Here ‖Fv −Gv‖∞ is strictly larger than zero whereas the three other remaining terms tend to zero as n → ∞ as above.",G. Proof of Lemma 5,[0],[0]
Multiplying (35) from the right by the full rank matrix Y −1,H. Proof of Lemma 6,[0],[0]
"we obtain the equations
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[Y >]ij , ∀i, j ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Note that for all i ∈",H. Proof of Lemma 6,[0],[0]
"[d],
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[W(yi,yi, I)]j .
SinceW is symmetric, we thus have
W(I,yi,yi) = yi, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Writing yi = ui/λi we obtain the eigenpair equation
W(I,ui,ui) = λiui, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
",H. Proof of Lemma 6,[0],[0]
The other direction readily follows from the definition of eigenpairs.,H. Proof of Lemma 6,[0],[0]
Figure 5 shows the simulation runtime of the spectral approach and that of SHL vs. the number of samples n.,I. Simulation runtime,[0],[0]
The setup is similar to the one described in section 5.,I. Simulation runtime,[0],[0]
"The runtime of SHL increases linearly with n, as expected.",I. Simulation runtime,[0],[0]
"For our spectral method, for lower values of n the dominant factor is the computation of tensor eigenvectors, which does not depend on n. For large n, the dominant factor is the computation of the correlation tensor, linear in n.",I. Simulation runtime,[0],[0]
"In Algorithm 1, we modify the diagonal elements of M,M by (17).",J. Matrix and tensor denoising,[0],[0]
"This modification is suited for additive Gaussian noise, but is not applicable for the case where X = binomial(2,WTH).",J. Matrix and tensor denoising,[0],[0]
"Instead, we implemented a method derived in (Jain & Oh, 2014) for a similar setup.
",J. Matrix and tensor denoising,[0],[0]
"First, we treat the diagonal elements of Mσ as missing data, and complete them with the following iterative steps.",J. Matrix and tensor denoising,[0],[0]
"(i) compute the first d eigenpairs {vi, λi} of R(k); and (ii) update the diagonal elements by R(k+1)jj =",J. Matrix and tensor denoising,[0],[0]
"( ∑ i λiviv > i )jj .
",J. Matrix and tensor denoising,[0],[0]
"Next, instead of computingMσ via (17) and thenWσ via (19), we compute Wσ directly by solving the following system of linear equations.",J. Matrix and tensor denoising,[0],[0]
"Let K† be the pseudo-inverse
matrix of K, and PΩ(T ) denote a masking operation over the tensor T such that,
PΩ(T ) =",J. Matrix and tensor denoising,[0],[0]
{ Tijk i 6=,J. Matrix and tensor denoising,[0],[0]
j 6= k 0,J. Matrix and tensor denoising,[0],[0]
"o.w
We estimateW by the following minimization problem,
Ŵ = argmin W
‖PΩ ( W(K†,K†,K†) )",J. Matrix and tensor denoising,[0],[0]
"− PΩ(M)‖2F
This method depends only on the off-diagonal elements of M and M and hence is applicable whenever E[X|H] = WTH and the noise has bounded variance.",J. Matrix and tensor denoising,[0],[0]
"In section 5, we compare the results of algorithm 1 with and without an additional single weighted least square step.",K. Adding a weighted least square step to the spectral method,[0],[0]
"Given an estimate Ŵ , for each observed instance xj we calculate the conditional likelihood L(xj |h) for the 2d possible binary vectors h ∈ {0, 1}d,
L(xj |h) = 1√
2πσ2 exp
( − ‖xj − ŴTh‖2 / (2σ2) )",K. Adding a weighted least square step to the spectral method,[0],[0]
"For each instance xj , we keep the top K = 6 vectors h1j , . . .",K. Adding a weighted least square step to the spectral method,[0],[0]
",hKj with the highest likelihood.",K. Adding a weighted least square step to the spectral method,[0],[0]
Let Π ∈,K. Adding a weighted least square step to the spectral method,[0],[0]
"[0, 1]K×n be a weight matrix such that Πkj is proportional to L(xj |hkj), and ∑ k Πkj = 1 for all j. The new estimate Ŵwls is the minimizer of the weighted least square problem,
Ŵwls = argmin W n∑ j=1 K∑ k=1 Πkj‖xj −WThkj‖2.",K. Adding a weighted least square step to the spectral method,[0],[0]
"In section 5, we compare the results of the spectral approach to the following ALS iterations, with a random starting point.
",L. Alternating least squares for W and H,[0],[0]
W (k) = argmin,L. Alternating least squares for W and H,[0],[0]
"W∈Rd×m ‖X −WTH(k−1)‖2F
Ĥ(k) = argmin H∈Rd×n",L. Alternating least squares for W and H,[0],[0]
‖X,L. Alternating least squares for W and H,[0],[0]
"− (W (k))TH‖2F
H(k) = argmin H∈{0,1}d×n ‖H − Ĥ(k)‖2F ,",L. Alternating least squares for W and H,[0],[0]
"The simulated admixture data was generated via the following steps:
1.",M. Genetic admixture simulations,[0],[0]
"We used SCRM (Staab et al., 2015) to simulate a split between d = 3 ancestral populations, with separation
time of 4000 generations.",M. Genetic admixture simulations,[0],[0]
The simulator generated 40 chromosomes of length 250 · 106 for each of the three populations.,M. Genetic admixture simulations,[0],[0]
"The simulation parameters were determined as N0 = 104 effective population size, 10−8 mutation rate (per base pair per generation), and 10−8 recombination rate (per base pair per generation).
2.",M. Genetic admixture simulations,[0],[0]
"We sampled the proportion matrix W from a Dirichlet distribution with parameter α.
3.",M. Genetic admixture simulations,[0],[0]
"Two chromosomes of length 250 · 106 were created for each of the m = 50 admixed individuals with the following steps: (i) An ancestral population was sampled according to W, say, population hA. (ii)",M. Genetic admixture simulations,[0],[0]
"One of the 40 chromosomes was sampled from hA, say hA(k) (iii) A block length l was sampled from an exponential distribution with rate 20 per Morgan corresponding admixture event happening 20 generations ago (in our case, 1 Morgan was 108 base pairs).",M. Genetic admixture simulations,[0],[0]
(iv) A block of length l was copied from chromosome hA(k) to the corresponding locations in the new admixed chromosome.,M. Genetic admixture simulations,[0],[0]
"We repeated steps (i)-(iv) until completion of the chromosome.
",M. Genetic admixture simulations,[0],[0]
"N. Analysis of DNA methylation data The dataset is part of the supplementary material of (Houseman et al., 2012).",M. Genetic admixture simulations,[0],[0]
"The observed matrix X ∈
[0, 1]m×n consists of m = 12 blood samples, each with the DNA methylation measurements in n = 500 sites (called CpGs).
",M. Genetic admixture simulations,[0],[0]
The statistical model for X is similar to that of Eq.,M. Genetic admixture simulations,[0],[0]
(1).,M. Genetic admixture simulations,[0],[0]
"We assume that each blood cell is a mixture of d = 4 cell types, with unknown proportions.",M. Genetic admixture simulations,[0],[0]
The latent variables h correspond to the presence or absence of methylation in each site for the 4 cell types.,M. Genetic admixture simulations,[0],[0]
"Given the DNA methylation array, the task is to estimate the proportion matrix W .
",M. Genetic admixture simulations,[0],[0]
The upper and lower panels of Figure 6 correspond to the real and estimated mixture matrix.,M. Genetic admixture simulations,[0],[0]
"For comparison, we performed the steps described in detail for this dataset in Slawski et al. (2013, Section 4).",M. Genetic admixture simulations,[0],[0]
"For both methods we measured the l1 distance between the real and estimated mixture matrices,
1
mn ∑ ij |Wij",M. Genetic admixture simulations,[0],[0]
"− Ŵij |
The l1 distance were equal to 0.003 for SHL and 0.00056 for the spectral approach.",M. Genetic admixture simulations,[0],[0]
Latent variable models with hidden binary units appear in various applications.,abstractText,[0],[0]
"Learning such models, in particular in the presence of noise, is a challenging computational problem.",abstractText,[0],[0]
"In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data.",abstractText,[0],[0]
"We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate.",abstractText,[0],[0]
"Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive.",abstractText,[0],[0]
We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.,abstractText,[0],[0]
Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 377–387 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1035",text,[0],[0]
"Detection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems.",1 Introduction,[0],[0]
"Traditional sentiment analyzers and
sarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c).",1 Introduction,[0],[0]
"Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns).",1 Introduction,[0],[0]
"It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels.",1 Introduction,[0],[0]
"For example, the sentence I really love my job.",1 Introduction,[0],[0]
I work 40 hours a week to be this poor.,1 Introduction,[0],[0]
requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job.,1 Introduction,[0],[0]
"In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.
",1 Introduction,[0.9570016724699036],"['This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se- Work started when M. Allamanis was at Edinburgh.']"
"Mishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better.",1 Introduction,[0],[0]
The hypothesis here is that human gaze activities are related to the cognitive processes in the brain that combine the “external knowledge” that the reader possesses with textual clues that she / he perceives.,1 Introduction,[0],[0]
"While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification, and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c).",1 Introduction,[0],[0]
"These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately
377
capture all forms of textual subtleties (b) Eyemovement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult.",1 Introduction,[0],[0]
"So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Network (CNN).",1 Introduction,[0],[0]
"We test our technique on two publicly available datasets enriched with eyemovement information, used for binary classification tasks of sentiment polarity and sarcasm detection.",1 Introduction,[0],[0]
Our experiments show that the automatically extracted features often help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.,1 Introduction,[0],[0]
"The datasets used in our experiments, resources and other relevant pointers are available at http://www.cfilt.iitb.ac.in/ cognitive-nlp
The rest of the paper is organized as follows.",1 Introduction,[0],[0]
Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting.,1 Introduction,[0],[0]
"In Section 3, we argue why CNN is preferred over other available alternatives for feature extraction.",1 Introduction,[0],[0]
The CNN architecture is proposed and discussed in Section 4.,1 Introduction,[0],[0]
Section 5 describes our experimental setup and results are discussed in Section 6.,1 Introduction,[0],[0]
We provide a detailed analysis of the results along with some insightful observations in Section 7.,1 Introduction,[0],[0]
"Section 8 points to relevant literature followed by Section 9 that concludes the paper.
",1 Introduction,[0],[0]
Terminology A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest.,1 Introduction,[0],[0]
Forward and backward saccades are called progressions and regressions respectively.,1 Introduction,[0],[0]
A scanpath is a line graph that contains fixations as nodes and saccades as edges.,1 Introduction,[0],[0]
"Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity",2 Eye-movement and Linguistic Subtleties,[0],[0]
"or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive
load results in longer fixation duration (Kliegl et al., 2004).
",2 Eye-movement and Linguistic Subtleties,[0],[0]
Mishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1).,2 Eye-movement and Linguistic Subtleties,[0],[0]
"For sentiment bearing texts, highly subtle eyemovement patterns are observed for semantically/pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations, etc.) than the simple ones (Mishra et al., 2016b).",2 Eye-movement and Linguistic Subtleties,[0],[0]
The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs.,2 Eye-movement and Linguistic Subtleties,[0],[0]
"In our work, CNN takes the onus of feature engineering.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012).",3 Why Convolutional Neural Network?,[0],[0]
"Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours, and removal of redundant backgrounds.",3 Why Convolutional Neural Network?,[0],[0]
"We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features.",3 Why Convolutional Neural Network?,[0],[0]
"For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical il-
lustration).",3 Why Convolutional Neural Network?,[0],[0]
"With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to.",3 Why Convolutional Neural Network?,[0],[0]
Figure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The components are explained below.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The text component is quite similar to the one proposed by Kim (2014) for sentence classification.,4.1 Text Component,[0],[0]
Words (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK).,4.1 Text Component,[0],[0]
"As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static throughout training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT).",4.1 Text Component,[0],[0]
"We separately experiment with static, non-static and multi-channel variants.
",4.1 Text Component,[0],[0]
"For each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever
necessary to tackle length variations) by concatenating the word embeddings.
",4.1 Text Component,[0],[0]
"x1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)
where ⊕ is the concatenation operator.",4.1 Text Component,[0],[0]
"To extract local features1, convolution operation is applied.",4.1 Text Component,[0],[0]
"Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words.",4.1 Text Component,[0],[0]
"A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output.",4.1 Text Component,[0],[0]
"Mathematically,
ci = f(W.xi:i+H−1 + b) (2)
where b ∈ R is the bias and f is the non-linear function.",4.1 Text Component,[0],[0]
"This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .
",4.1 Text Component,[0],[0]
c =,4.1 Text Component,[0],[0]
"[c1, c2, c3, ..., cN−H+1] (3)
A global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map.",4.1 Text Component,[0],[0]
"The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.
",4.1 Text Component,[0],[0]
We have described the process by which one feature is extracted from one filter (red bordered portions in Figure 3 illustrate the case of H = 2).,4.1 Text Component,[0],[0]
The model uses multiple filters for each filter size to obtain multiple features representing the text.,4.1 Text Component,[0],[0]
"In the MULTICHANNELTEXT variant, for a window of H words, the convolution operation is separately applied on both the embedding channels.",4.1 Text Component,[0],[0]
Local features learned from both the channels are concatenated before applying max-pooling.,4.1 Text Component,[0],[0]
The gaze component deals with scanpaths of multiple participants annotating the same text.,4.2 Gaze Component,[0],[0]
"Scanpaths can be pre-processed to extract two sequences3 of gaze data to form separate channels of input: (1) A sequence of normalized4 durations of fixations (in milliseconds) in the order in which
1features specific to a region in case of images or window of words in case of text
2mean pooling does not perform well.",4.2 Gaze Component,[0],[0]
"3like text-input, gaze sequences are padded where necessary 4scaled across participants using min-max normalization to reduce subjectivity
they appear in the scanpath, and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath.",4.2 Gaze Component,[0],[0]
These channels are related to two fundamental gaze attributes such as fixation and saccade respectively.,4.2 Gaze Component,[0],[0]
"With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.
",4.2 Gaze Component,[0],[0]
"For each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence).",4.2 Gaze Component,[0],[0]
"Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of the ith participant.",4.2 Gaze Component,[0],[0]
"Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2).",4.2 Gaze Component,[0],[0]
"For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,
cij = f(W.gij:(i+J−1)(j+J−1) + b) (4)
where b ∈ R is the bias and f is a non-linear func-
tion.",4.2 Gaze Component,[0],[0]
"This operation is applied to each possible window of size J2 to produce a feature map (c),
c =[c11, c12, c13, ..., c1(G−J+1),
c21, c22, c23, ..., c2(G−J+1), ...,
c(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]
(5)
",4.2 Gaze Component,[0],[0]
A global feature is then obtained by applying max pooling operation.,4.2 Gaze Component,[0],[0]
"Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2).",4.2 Gaze Component,[0],[0]
"For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible",4.2 Gaze Component,[0],[0]
"i, j.
We have described the process by which one feature is extracted from one filter (of 2D window size J2 and the max-pooling window size of M2).",4.2 Gaze Component,[0],[0]
"In Figure 3, red and blue bordered portions illustrate the cases of J2 =",4.2 Gaze Component,[0],[0]
"[3, 3] and M2 = [2, 2] respectively.",4.2 Gaze Component,[0],[0]
"Like the text component, the gaze component also uses multiple filters for each filter size to obtain multiple features representing the gaze input.",4.2 Gaze Component,[0],[0]
"In the MULTICHANNELGAZE variant, for a 2D window of J2, the convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before maxpooling is applied.
",4.2 Gaze Component,[0],[0]
"Once the global features are learned from both the text and gaze components, they are merged
and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.
",4.2 Gaze Component,[0],[0]
"The gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others.",4.2 Gaze Component,[0],[0]
"The only way we can think of for addressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering.",4.2 Gaze Component,[0],[0]
"Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge.",4.2 Gaze Component,[0],[0]
"As of now, training and testing are carried out by keeping the order of the input constant.",4.2 Gaze Component,[0],[0]
We now share several details regarding our experiments below.,5 Experiment Setup,[0],[0]
We conduct experiments for two binaryclassification tasks of sentiment and sarcasm using two publicly available datasets enriched with eye-movement information.,5.1 Dataset,[0],[0]
Dataset 1 has been released by Mishra et al. (2016a).,5.1 Dataset,[0],[0]
It contains 994 text snippets with 383 positive and 611 negative examples.,5.1 Dataset,[0],[0]
"Out of the 994 snippets, 350 are sarcastic.",5.1 Dataset,[0],[0]
"Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive, and 400 are negative.",5.1 Dataset,[0],[0]
Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively.,5.1 Dataset,[0],[0]
"With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1).",5.2 CNN Variants,[0],[0]
"Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2).",5.2 CNN Variants,[0],[0]
"With both text and gaze components, 9 more variants could thus beexperimented with.",5.2 CNN Variants,[0],[0]
"For text component, we experiment with filter widths (H) of [3, 4].",5.3 Hyper-parameters,[0],[0]
"For the gaze component, 2D filters (J2) set to [3× 3], [4× 4] respectively.",5.3 Hyper-parameters,[0],[0]
"The
max pooling 2D window, M2, is set to [2× 2].",5.3 Hyper-parameters,[0],[0]
"In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window.",5.3 Hyper-parameters,[0],[0]
These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system.,5.3 Hyper-parameters,[0],[0]
"Tuning of hyper-parameters might help in improving the performance of our framework, which is on our future research agenda.",5.3 Hyper-parameters,[0],[0]
"For regularization dropout is employed both on the embedding and the penultimate layers with a constraint on l2-norms of the weight vectors (Hinton et al., 2012).",5.4 Regularization,[0],[0]
"Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units during forward propagation.",5.4 Regularization,[0],[0]
We set p to 0.25.,5.4 Regularization,[0],[0]
"We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1.",5.5 Training,[0],[0]
The input batch size is set to 32 and number of training iterations (epochs) is set to 200.,5.5 Training,[0],[0]
10% of the training data is used for validation.,5.5 Training,[0],[0]
"Initializing the embedding layer with of pretrained embeddings can be more effective than random initialization (Kim, 2014).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"In our experiments, we have used embeddings learned using the movie reviews with one sentence per review dataset (Pang and Lee, 2005).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"It is worth noting that, for a small dataset like ours, using a small data-set like the one from (Pang and Lee, 2005) helps in reducing the number model parameters resulting in faster learning of embeddings.",5.6 Use of Pre-trained Embeddings:,[0],[0]
The results are also quite close to the ones obtained using word2vec facilitated by Mikolov et al. (2013).,5.6 Use of Pre-trained Embeddings:,[0],[0]
"For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features.",5.7 Comparison with Existing Work,[0],[0]
"For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm).",5.7 Comparison with Existing Work,[0],[0]
We follow the same 10-fold train-test configuration as these existing works for consistency.,5.7 Comparison with Existing Work,[0],[0]
"In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks.",6 Results,[0],[0]
Table 1 presents results for sentiment analysis task.,6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%.",6.1 Results for Sentiment Analysis Task,[0],[0]
This improvement is statistically significant of p < 0.05 as confirmed by McNemar test.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).
",6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 2, CNN based approaches do not perform better than manual feature based approaches.",6.1 Results for Sentiment Analysis Task,[0],[0]
"However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%.",6.1 Results for Sentiment Analysis Task,[0],[0]
"We observe that for dataset 2, training accuracy reaches 100 within 25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting.",6.1 Results for Sentiment Analysis Task,[0],[0]
Tuning the regularization parameters specific to dataset 2 may help here.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Even though CNN might
not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants.",6.1 Results for Sentiment Analysis Task,[0],[0]
"For sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2).",6.2 Results for Sarcasm Detection Task,[0],[0]
"However, the improvement by adding the gaze component to the CNN network is just 1.34%, which is statistically insignificant over CNN with text component.",6.2 Results for Sarcasm Detection Task,[0],[0]
"While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset.",6.2 Results for Sarcasm Detection Task,[0],[0]
"This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings.",6.2 Results for Sarcasm Detection Task,[0],[0]
"In this section, some important observations from our experiments are discussed.",7 Discussion,[0],[0]
"Embedding dimension has proven to have a deep impact on the performance of neural systems (dos Santos and Gatti, 2014; Collobert et al., 2011).
",7.1 Effect of Embedding Dimension Variation,[0],[0]
We repeated our experiments by varying the embedding dimensions in the range of [50-300]5 and observed that reducing embedding dimension improves the F-scores by a little margin.,7.1 Effect of Embedding Dimension Variation,[0],[0]
Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small.,7.1 Effect of Embedding Dimension Variation,[0],[0]
"We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component.",7.1 Effect of Embedding Dimension Variation,[0],[0]
"Non-static embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples).",7.2 Effect of Static / Non-static Text Channels,[0],[0]
"For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due
5a standard range (Liu et al., 2015; Melamud et al., 2016)
to the presence of irony / sarcasm) better.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Fixation channel does not help much, may be because of higher variance in fixation duration.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"For sarcasm detection, fixation and saccade channels perform with similar accuracy when employed separately.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and nonsarcastic classes, as opposed to sentiment classes.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"To examine how good the features learned by the CNN are, we analyzed the features for a few example cases.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Figure 4 presents some of the example test cases for the task of sarcasm detection.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Output of the hidden layer after merge layer is considered as features learned by the network.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"We plot the features, in the form of color-bars, following Li et al. (2016) - denser col-
ors representing feature with higher magnitude.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In Figure 4, we show only two representative model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"As one can see, addition of gaze information helps to generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"This difference is not clear in feature vectors learned by text-only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 4 is incorrectly classified by both the systems, perhaps due to lack of context.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time.,8 Related Work,[0],[0]
"Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc.",8 Related Work,[0],[0]
"(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011).",8 Related Work,[0],[0]
"For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015)",8 Related Work,[0],[0]
"(b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014).",8 Related Work,[0],[0]
Recent systems are based on variants of deep neural network built on the top of embeddings.,8 Related Work,[0],[0]
"A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi-
tecture (Wang et al., 2016).",8 Related Work,[0],[0]
"Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.
",8 Related Work,[0],[0]
"Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks.",8 Related Work,[0],[0]
"Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features.",8 Related Work,[0],[0]
The closest works to ours are by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze and text data for sentiment and sarcasm detection tasks.,8 Related Work,[0],[0]
These recent advancements motivate us to explore the cognitive NLP paradigm.,8 Related Work,[0],[0]
"In this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification.",9 Conclusion and Future Directions,[0],[0]
"On multiple published datasets for which gaze information is available, our systems could often achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.",9 Conclusion and Future Directions,[0],[0]
An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers.,9 Conclusion and Future Directions,[0],[0]
"Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.) to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient.",9 Conclusion and Future Directions,[0],[0]
"We thank Anoop Kunchukuttan, Joe Cheri Ross, and Sachin Pawar, research scholars of the Center for Indian Language Technology (CFILT), IIT Bombay for their valuable inputs.",Acknowledgments,[0],[0]
"Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc..",abstractText,[0],[0]
Such extraction of features is typically manual.,abstractText,[0],[0]
"We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system.",abstractText,[0],[0]
We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection.,abstractText,[0],[0]
Our proposed framework is based on Convolutional Neural Network (CNN).,abstractText,[0],[0]
The CNN learns features from both gaze and text and uses them to classify the input text.,abstractText,[0],[0]
"We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",abstractText,[0],[0]
Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1839–1848 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1839",text,[0],[0]
"In the last few years, convolutional neural networks (CNNs) have demonstrated remarkable progress in various natural language processing applications (Collobert et al., 2011), including sentence/document classification (Kim, 2014; Zhang et al., 2015; Wang et al., 2018), text sequence matching (Hu et al., 2014; Yin et al., 2016; Shen et al., 2017), generic text representations (Gan et al., 2016; Tang et al., 2018), language modeling (Dauphin et al., 2017), machine translation (Gehring et al., 2017) and abstractive sentence summarization (Gehring et al., 2017).",1 Introduction,[0],[0]
"CNNs are typically applied to tasks where feature extrac-
tion and a corresponding supervised task are approached jointly (LeCun et al., 1998).",1 Introduction,[0],[0]
"As an encoder network for text, CNNs typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014).",1 Introduction,[0],[0]
"Different filter sizes n may be used within the same model, exploiting meaningful semantic features from different n-gram fragments.
",1 Introduction,[0],[0]
"The learned weights of CNN filters, in most cases, are assumed to be fixed regardless of the input text.",1 Introduction,[0],[0]
"As a result, the rich contextual information inherent in natural language sequences may not be fully captured.",1 Introduction,[0],[0]
"As demonstrated in Cohen and Singer (1999), the context of a word tends to greatly influence its contribution to the final supervised tasks.",1 Introduction,[0],[0]
"This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.
",1 Introduction,[0],[0]
Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations.,1 Introduction,[0],[0]
"One common strategy is the attention mechanism, which is typically employed on top of a CNN (or Long ShortTerm Memory (LSTM)) layer to guide the extraction of semantic features.",1 Introduction,[0],[0]
"For the embedding of a single sentence, Lin et al. (2017) proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations.",1 Introduction,[0],[0]
"However, their model needs considerably more parameters to achieve performance gains over traditional CNNs.",1 Introduction,[0],[0]
"To match sentence pairs, Yin et al. (2016) introduced an attentionbased CNN model, which re-weights the convolution inputs or outputs, to extract interdepen-
dent sentence representations.",1 Introduction,[0],[0]
Wang et al. (2016); Wang and Jiang (2017) explore a compare and aggregate framework to directly capture the wordby-word matching between two paired sentences.,1 Introduction,[0],[0]
"However, these approaches suffer from the problem of high matching complexity, since a similarity matrix between pairwise words needs to be computed, and thus it is computationally inefficient or even prohibitive when applied to long sentences (Mou et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we propose a generic approach to learn context-aware convolutional filters for natural language understanding.",1 Introduction,[0],[0]
"In contrast to traditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity.",1 Introduction,[0],[0]
"Specifically, we introduce a meta network to generate a set of contextaware filters, conditioned on specific input sentences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences.",1 Introduction,[0],[0]
"In this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.
",1 Introduction,[0],[0]
"Moreover, since the generated filters in our framework can adapt to different conditional information available (labels or paired sentences), they can be naturally generalized to model sentence pairs.",1 Introduction,[0],[0]
"In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-aware representations.
",1 Introduction,[0],[0]
"We investigate the effectiveness of our Adaptive Context-sensitive CNN (ACNN) framework on several text processing tasks: ontology classification, sentiment analysis, answer sentence selection and paraphrase identification.",1 Introduction,[0],[0]
We show that the proposed methods consistently outperforms the standard CNN and attention-based CNN baselines.,1 Introduction,[0],[0]
"Our work provides a new perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even better performance in the future.",1 Introduction,[0],[0]
"Learning deep text representations has attracted much attention recently, since they can potentially benefit a wide range of NLP applications (Collobert et al., 2011; Kim, 2014; Wang et al., 2017a; Shen et al., 2018a; Tang and de Sa, 2018; Zhang
et al., 2018).",2 Related Work,[0],[0]
CNNs have been extensively investigated as the encoder networks of natural language.,2 Related Work,[0],[0]
"Our work is in line with previous efforts on improving the adaptivity and flexibility of convolutional neural networks (Jeon and Kim, 2017; De Brabandere et al., 2016).",2 Related Work,[0],[0]
Jeon and Kim (2017) proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation.,2 Related Work,[0],[0]
"De Brabandere et al. (2016) introduced an architecture to generate the future frames conditioned on given image(s), by adapting the CNN filter weights to the motion within previous video frames.",2 Related Work,[0],[0]
"Although CNNs have been widely adopted in a large number of NLP applications, improving the adaptivity of vanilla CNN modules has been considerably less studied.",2 Related Work,[0],[0]
"To the best of our knowledge, the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences.
",2 Related Work,[0],[0]
"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks (Ha et al., 2017), and dynamic parameter-prediction for video-frame generation (De Brabandere et al., 2016).",2 Related Work,[0],[0]
"In contrast to these works that focus on generation problems, our model is based on context-aware CNN filters and is aimed at abstracting more informative and predictive sentence features.",2 Related Work,[0],[0]
"Most similar to our work, Liu et al. (2017) designed a meta network to generate compositional functions over tree-structured neural networks for encapsulating sentence features.",2 Related Work,[0],[0]
"However, their model is only suitable for encoding individual sentences, while our framework can be readily generalized to capture the interactions between sentence pairs.",2 Related Work,[0],[0]
"Moreover, our framework is based on CNN models, which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential-based models.",2 Related Work,[0],[0]
"The CNN architectures in (Kim, 2014; Collobert et al., 2011) are typically utilized for extracting sentence representations, by a composition of a convolutional layer and a max-pooling operation over all resulting feature maps.",3.1 Basic CNN for text representations,[0],[0]
"Let the words of a sentence of length T (padded where necessary) be x1, x2, ... , xT .",3.1 Basic CNN for text representations,[0],[0]
"The sentence can be represented
as a matrix X ∈ Rd×T , where each column represents a d-dimensional embedding of the corresponding word.
",3.1 Basic CNN for text representations,[0],[0]
"In the convolutional layer, a set of filters with weights W ∈ RK×h×d is convolved with every window of h words within the sentence, i.e., {x1:h, x2:h+1, . . .",3.1 Basic CNN for text representations,[0],[0]
", xT−h+1:T }, where K is the number of output feature maps (and filters).",3.1 Basic CNN for text representations,[0],[0]
"In this manner, feature maps p for these h-gram text fragments are generated as:
pi = f(W × xi:i+h−1 + b) (1)
where i = 1, 2, ..., T − h + 1 and × denotes the convolution operator at the ith shift location.",3.1 Basic CNN for text representations,[0],[0]
"Parameter b ∈ RK is the bias term and f(·) is a non-linear function, implemented as a rectified linear unit (ReLU) in our experiments.",3.1 Basic CNN for text representations,[0],[0]
"The output feature maps of the convolutional layer, i.e., p ∈ RK×(T−h+1) are then passed to the pooling layer, which takes the maximum value in every row of p, forming a K-dimensional vector, z.",3.1 Basic CNN for text representations,[0],[0]
This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments.,3.1 Basic CNN for text representations,[0],[0]
"Moreover, the max-over-time nature of the pooling operation (Collobert et al., 2011) guarantees that the size of the obtained representation is independent of the sentence length.
",3.1 Basic CNN for text representations,[0],[0]
"Note that in basic CNN sentence encoders, filter weights are the same for different inputs, which may be suboptimal for feature extraction (De Brabandere et al., 2016), especially in the case where conditional information is available.",3.1 Basic CNN for text representations,[0],[0]
"The proposed architecture to learn contextsensitive filters is composed of two principal modules: (i) a filter generation module, which produces a set of filters conditioned on the input sentence; and (ii) an adaptive convolution module, which applies the generated filters to an input sentence (this sentence may be either the same as or different from the first input, as discussed further in Section 3.3).",3.2 Learning context-sensitive filters,[0],[0]
"The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",3.2 Learning context-sensitive filters,[0],[0]
"Since the generated filters are sample-specific, our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder.",3.2 Learning context-sensitive filters,[0],[0]
"The general ACNN framework is shown schematically in Figure 1.
Filter generation module Instead of utilizing fixed filter weightsW for different inputs (as (1)), our model generates a set of filters conditioned on the input sentence X .",3.2 Learning context-sensitive filters,[0],[0]
"Given an input X , the filter-generation module can be implemented, in principle, as any deep (differentiable) architecture.",3.2 Learning context-sensitive filters,[0],[0]
"However, in order to handle input sentences of variable length common in natural language, we design a generic filter generation module to produce filters with a predefined size.
",3.2 Learning context-sensitive filters,[0],[0]
"First, the input X is encapsulated into a fixedlength vector (code) z with the dimension of l, via a basic CNN model, where one convolutional layer is employed along with the pooling operation (as described in Section 3.1).",3.2 Learning context-sensitive filters,[0],[0]
"On top of this hidden representation z, a deconvolutional layer, which performs transposed operations of convolutions (Radford et al., 2016), is further applied to produce a unique set of filters forX (as illustrated in Figure 1):
z = CNN(X;θe), (2)
f = DCNN(z;θd) , (3)
where θe and θd are the learned parameters in each layer of the filter-generating module, respectively.",3.2 Learning context-sensitive filters,[0],[0]
"Specifically, we convolve z with a filter of size (fs, l, kx, ky), where fs is the number of generated filters and the kernel size is (kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"The output will be a tensor of shape (fs, kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"Since the dimension of hidden representation z is independent of input-sentence length, this framework guarantees that the generated filters are of the same shape and size for every sentence.",3.2 Learning context-sensitive filters,[0],[0]
"Intuitively, the encoding part of filter generation module abstracts the information from sentenceX into z. Based on this representation, the deconvolutional up-sampling layer determines a set of fixedsize, fine-grained filters f for the specific input.
",3.2 Learning context-sensitive filters,[0],[0]
Adaptive convolution module The adaptive convolution module takes as inputs the generated filters f and an input sentence.,3.2 Learning context-sensitive filters,[0],[0]
This sentence and the input to the filter-generation module may be identical (as in Figure 1) or different (as in Figure 2).,3.2 Learning context-sensitive filters,[0],[0]
"With the sample-specific filters, the input sentence is adaptively encoded, again, via a basic CNN architecture as in Section 3.1, i.e., one convolutional and one pooling layer.",3.2 Learning context-sensitive filters,[0],[0]
"Notably, there are no additional parameters in the adaptive convolution module (no bias term is employed).
",3.2 Learning context-sensitive filters,[0],[0]
"Our ACNN framework can be seen as a generalization of the basic CNN, which can be represented as an ACNN by setting the outputs of the filter-generation module to a constant, regardless of the contextual information from input sentence(s).",3.2 Learning context-sensitive filters,[0],[0]
"Because of the learning-to-learn (Thrun and Pratt, 2012) nature of the proposed ACNN framework, it tends to have greater representational power than the basic CNN.",3.2 Learning context-sensitive filters,[0],[0]
"Considering the ability of our ACNN framework to generate context-aware filters, it can be naturally generalized to the task of text sequence matching.",3.3 Extension to text sequence matching,[0],[0]
"In this section, we will describe the proposed Adaptive Question Answering (AdaQA) model in the context of answer sentence selection task.",3.3 Extension to text sequence matching,[0],[0]
"Note that the corresponding model can be readily adapted to other sentence matching problems as well (see Section 5.2).
",3.3 Extension to text sequence matching,[0],[0]
"Given a factual question q (associated with a list of candidate answers {a1, a2, . . .",3.3 Extension to text sequence matching,[0],[0]
", am} and their corresponding labels y = {y1, y2, . . .",3.3 Extension to text sequence matching,[0],[0]
", ym}), the goal of the model is to identify the correct answers from the set of candidates.",3.3 Extension to text sequence matching,[0],[0]
"For i = 1, 2, . . .",3.3 Extension to text sequence matching,[0],[0]
",m, if ai correctly answers q, then yi = 1, and otherwise yi = 0.",3.3 Extension to text sequence matching,[0],[0]
"Therefore, the task can be cast as a classification problem where, given an unlabeled question-answer pair (qi, ai), we seek to predict the judgement yi.
",3.3 Extension to text sequence matching,[0],[0]
"Conventionally, a question q and an answer a are independently encoded by two basic CNNs to fixed-length vector representations, denoted hq and ha, respectively.",3.3 Extension to text sequence matching,[0],[0]
They are then directly employed to predict the judgement y.,3.3 Extension to text sequence matching,[0],[0]
"This strategy could be suboptimal, since no communication (information sharing) occurs between the questionanswer pair until the top prediction layer.",3.3 Extension to text sequence matching,[0],[0]
"Intuitively, while the model is inferring the representation for a question, if the meaning of the answer is
taken into account, those features that are relevant for final prediction are more likely to be extracted.",3.3 Extension to text sequence matching,[0],[0]
"So motivated, we propose an adaptive CNN-based question-answer (AdaQA) model for this problem.",3.3 Extension to text sequence matching,[0],[0]
"The AdaQA model can be divided into three modules: filter generation, adaptive convolution, and matching modules, as depicted schematically in Figure 2.",3.3 Extension to text sequence matching,[0],[0]
"Assume there is a question-answer pair to be matched, represented by word-embedding matrices, i.e. Q ∈ RTq×d and A ∈ RTa×d, where d is the embedding dimension and Tq and Ta are respective sentence lengths.",3.3 Extension to text sequence matching,[0],[0]
"First, they are passed to two filter-generation modules, to produce two sets of filters that encapsulate features of the corresponding input sentences.",3.3 Extension to text sequence matching,[0],[0]
"Similar to the setup in Section 3.2, we also employ a two-step process to produce the filters.",3.3 Extension to text sequence matching,[0],[0]
"For a question Q, the generating process is:
zq = CNN(Q;θqe), (4) f q = DCNN(zq;θ q d) (5)
where CNN and DCNN denote the basic CNN unit and deconvolution layer, respectively, as discussed in Section 2.1.",3.3 Extension to text sequence matching,[0],[0]
Parameters θqe and θ q d are to be learned.,3.3 Extension to text sequence matching,[0],[0]
"The same process can be utilized to produce encodings za and filters fa for the answer input,A, with parameters θae and θ a d, respectively.
",3.3 Extension to text sequence matching,[0],[0]
"The two sets of filter weights are then passed to adaptive convolution modules, along with Q and A, to obtain the extracted question and answer embeddings.",3.3 Extension to text sequence matching,[0],[0]
"That is, the question embedding is convolved with the filters produced by the answer and vise versa (ψq and ψa are the bias terms to be learned).",3.3 Extension to text sequence matching,[0],[0]
"The key idea is to abstract information from the answer (or question) that is pertinent to the corresponding question (or answer).
",3.3 Extension to text sequence matching,[0],[0]
"Compared to a Siamese CNN architecture (Bromley et al., 1994), our model selectively encapsulates the most important features for judgement prediction, removing less vital information.",3.3 Extension to text sequence matching,[0],[0]
We then employ the question and answer representations hq ∈,3.3 Extension to text sequence matching,[0],[0]
"Rnh , ha ∈",3.3 Extension to text sequence matching,[0],[0]
Rnh as inputs to the matching module (where nh is the dimension of question/answer embeddings).,3.3 Extension to text sequence matching,[0],[0]
"Following Mou et al. (2016), the matching function is defined as:
t =",3.3 Extension to text sequence matching,[0],[0]
"[hq;ha;hq − ha;hq ha] (6) p(y = 1|hq,ha) = MLP(t;η′) (7)
where − and denote an element-wise subtraction and element-wise product, respectively.",3.3 Extension to text sequence matching,[0],[0]
[ha;hb] indicates that ha and hb are stacked as column vectors.,3.3 Extension to text sequence matching,[0],[0]
"The resulting matching vector t ∈ R4nh is then sent through an MLP layer (with sigmoid activation function and parameters η′ to be learned) to model the desired conditional distribution p(yi = 1|hq,ha).
",3.3 Extension to text sequence matching,[0],[0]
"Notably, we share the weights of filter generating networks for both the question and answer, so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters.",3.3 Extension to text sequence matching,[0],[0]
All three modules in AdaQA model are jointly trained end-to-end.,3.3 Extension to text sequence matching,[0],[0]
"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks, such as paraphrase identification (see Section 5.2).",3.3 Extension to text sequence matching,[0],[0]
"The adaptive context-aware filter generation mechanism proposed here bears close resemblance to attention mechanism (Yin et al., 2016; Bahdanau et al., 2015; Xiong et al., 2017) widely adopted in the NLP community, in the sense that both methods intend to incorporate rich contextual information into text representations.",3.4 Connections to attention mechanism,[0],[0]
"However, attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers, and assigns different weights to each unit according to a context vector.",3.4 Connections to attention mechanism,[0],[0]
"By contrast, in our context-aware filter generation mechanism, the contextual information is inherently encoded into the convolutional filters, which directly interact with the input sentence during the convolution encoding operation.",3.4 Connections to attention mechanism,[0],[0]
"Notably, according to our experiments, the proposed filter generation module can be readily combined with (standard) attention mechanisms to further enhance the modeling expressiveness of CNN encoder.",3.4 Connections to attention mechanism,[0],[0]
Datasets We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks.,4 Experimental Setup,[0],[0]
"Specifically, we consider two large-scale document classification datasets: Yelp Reviews Polarity, and DBPedia ontology datasets (Zhang et al., 2015).",4 Experimental Setup,[0],[0]
"For Yelp reviews, we seek to predict a binary label (positive or negative) regarding one review about a restaurant.",4 Experimental Setup,[0],[0]
"DBpedia is extracted from Wikipedia by crowd-sourcing and is categorized into 14 non-overlapping ontology classes, including Company, Athlete, Natural Place, etc.",4 Experimental Setup,[0],[0]
"We sample 15% of the training data as the validation set, to select hyperparameters for our models and perform early stopping.",4 Experimental Setup,[0],[0]
"For sentence matching, we evaluate the AdaQA model on two datasets for open-domain question answering: WikiQA (Yang et al., 2015) and SelQA (Jurczyk et al., 2016).",4 Experimental Setup,[0],[0]
"Given a question, the task is to rank the corresponding candidate answers, which, in the case of WikiQA, are sentences extracted from the summary section of a related Wikipedia article.",4 Experimental Setup,[0],[0]
"To facilitate comparison with existing results (Yin et al., 2016; Yang et al., 2015; Shen et al., 2018b), we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset.",4 Experimental Setup,[0],[0]
"We also consider the task of paraphrase identification with the Quora Question Pairs dataset, with the same data splits as in (Wang et al., 2017b).",4 Experimental Setup,[0],[0]
"A summary of all datasets is presented in Table 1.
",4 Experimental Setup,[0],[0]
"Training Details For the document classification experiments, we randomly initialize the word embeddings uniformly within [−0.001, 0.001] and update them during training.",4 Experimental Setup,[0],[0]
"For the generated filters, we set the window size as h = 5, with K = 100 feature maps (the dimension of z is set as 100).",4 Experimental Setup,[0],[0]
"For direct comparison, we employ the same filter shape/size settings as in our basic CNN implementation.",4 Experimental Setup,[0],[0]
"A one-layer architecture is utilized for both the CNN baseline and the ACNN model, since we did not observe significant
performance gains with a multilayer architecture.",4 Experimental Setup,[0],[0]
"The minibatch size is set as 128, and a dropout rate of 0.2 is utilized on the embedding layer.",4 Experimental Setup,[0],[0]
"We observed that a larger dropout rate (e.g., 0.5) will hurt performance on document classifications and make training significantly slower.
",4 Experimental Setup,[0],[0]
"For the sentence matching tasks, we initialized the word embeddings with 50-dimensional Glove (Pennington et al., 2014) word vectors pretrained from Wikipedia 2014 and Gigaword 5 (Pennington et al., 2014) for all model variants.",4 Experimental Setup,[0],[0]
"As for the filters, we set the window size as h = 5, with K = 300 feature maps.",4 Experimental Setup,[0],[0]
"As described in Section 3.3, the vector t, output from the matching module, is fed to the prediction layer, implemented as a one-layer MLP followed by the sigmoid function.",4 Experimental Setup,[0],[0]
"We use Adam (Kingma and Ba, 2014) to train the models, with a learning rate of 3 × 10−4.",4 Experimental Setup,[0],[0]
"Dropout (Srivastava et al., 2014), with a rate of 0.5, is employed on the word embedding layer.",4 Experimental Setup,[0],[0]
The hyperparameters are selected by choosing the best model on the validation set.,4 Experimental Setup,[0],[0]
"All models are implemented with TensorFlow (Abadi et al., 2016) and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12GB memory.
",4 Experimental Setup,[0],[0]
"Baselines For document classification, we consider several baseline models: (i) ngrams (Zhang et al., 2015), a bag-of-means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams (up to 5-grams) from the training set and use their corresponding counts as features; (ii) small/large word CNN (Zhang et al., 2015): 6 layer word-based convolutional networks, with 256/1024 features at each layer, denoted as small/large, respectively; (iii) deep CNN (Conneau et al., 2016): deep convolutional neural networks with 9/17/29 layers.",4 Experimental Setup,[0],[0]
"To evaluate the effectiveness of proposed AdaQA model, we compare it with several CNN-based sequence matching baselines, including Vanilla CNN (Jurczyk et al., 2016; Santos et al., 2017), attentive pooling networks (dos Santos et al., 2016), and ABCNN (Yin et al., 2016) (where an attention mechanism is employed over the two sentence representations).
",4 Experimental Setup,[0],[0]
"Evaluation Metrics For document categorization and paraphrase identification tasks, we employ the percentage of correct predictions on the test set to evaluate and compare different models.
",4 Experimental Setup,[0],[0]
"For the answer sentence selection task, mean average precision (MAP) and mean reciprocal rank (MRR) are utilized as the corresponding evaluation metrics.",4 Experimental Setup,[0],[0]
"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation, we perform a comparison between the basic CNN and ACNN models with only a single filter, which are denoted as SCNN, S-ACNN, respectively (this setting may not yield best overall performance, since only a single filter is used, but it allows us to isolate the impact of adaptivity).",5.1 Document Classification,[0],[0]
"As illustrated in Table 2, SACNN significantly outperforms S-CNN on both datasets, demonstrating the advantage of the filtergeneration module in our ACNN framework.",5.1 Document Classification,[0],[0]
"As a result, with only one convolutional filter and thus very limited modeling capacity, our S-ACNN model tends to be much more expressive than the basic CNN model, due to the flexibility of applying different filters to different sentences.
",5.1 Document Classification,[0],[0]
We further experiment on both ACNN and CNN models with multiple filters.,5.1 Document Classification,[0],[0]
The corresponding document categorization accuracies are presented in Table 2.,5.1 Document Classification,[0],[0]
"Although we only use one convolution layer for our ACNN model, it already outperforms other CNN baseline methods with much deeper architectures.",5.1 Document Classification,[0],[0]
"Moreover, our method ex-
hibits higher accuracy than n-grams, which is a very strong baseline as shown in (Zhang et al., 2015).",5.1 Document Classification,[0],[0]
We attribute the superior performance of the ACNN framework to its stronger (adaptive) feature-extraction ability.,5.1 Document Classification,[0],[0]
"Moreover, our MACNN also achieves slightly better performance than self-attentive sentence embeddings proposed in Lin et al. (2017), which requires significant more parameters than our method.
",5.1 Document Classification,[0],[0]
"Effect of number of filters To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework, we implement the basic CNN model with different numbers of filter sizes, ranging from 1 to 1000.",5.1 Document Classification,[0],[0]
"As illustrated in Figure 3(a), when the filter size is larger than 100, the test accuracy of the standard CNN model does not show any noticeable improvement with more filters.",5.1 Document Classification,[0],[0]
"More importantly, even with a filter size of 1000, the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100.",5.1 Document Classification,[0],[0]
"Given these observations, we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework.",5.1 Document Classification,[0],[0]
"To elucidate the role of different parts (modules) in our AdaQA model, we implement several model variants for comparison: (i) a “vanilla” CNN model that independently encodes two sentence representations for matching; (ii) a self-adaptive ACNN-based model where the question/answer sentence generates adaptive filters only to convolve with the input itself; (iii) a one-way ACNN model where only the answer sentence representation is extracted with adaptive filters, which
are generated conditioned on the question; (iv) a two-way AdaQA model as described in Section 2.4, where both sentences are adaptively encoded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the proposed context-aware filter generation mechanism with the multi-perspective attention layer introduced in (Wang et al., 2017b).
",5.2 Answer Sentence Selection,[0],[0]
"Tables 3 and 4 show experimental results of our models on WikiQA and SelQA datasets, along with other state-of-the-art methods.",5.2 Answer Sentence Selection,[0],[0]
"Note that the self-adaptive ACNN model variant, which generates filters only for the input itself (without any interactions before the top matching module), slightly outperforms the vanilla CNN Siamese model.",5.2 Answer Sentence Selection,[0],[0]
"Combined with the results in document categorization experiments, we believe that our ACNN framework, in its simplest form, can be utilized as a powerful feature extractor for transforming natural language sentences into fixed-length vectors.",5.2 Answer Sentence Selection,[0],[0]
"More importantly, our two-way AdaQA model exhibits superior results compared with the one-way variant as well as other CNN-based baseline models on the WikiQA dataset.",5.2 Answer Sentence Selection,[0],[0]
This observation indicates that the bidirectional filter generation mechanism is strongly associated with the performance gains.,5.2 Answer Sentence Selection,[0],[0]
"While combined with the multi-perspective attention layers, adopted after the ACNN encoding layer, our two-way AdaQA model achieves even better performance.",5.2 Answer Sentence Selection,[0],[0]
"This suggests that the proposed strategy is complemen-
tary, in terms of the incorporation of rich contextual information, to the standard attention mechanism.",5.2 Answer Sentence Selection,[0],[0]
"The same trend is also observed on the SelQA dataset (as shown in Table 4), which is a much larger dataset than WikiQA.
",5.2 Answer Sentence Selection,[0],[0]
"Notably, our model yields significantly better results than an attentive pooling network and ABCNN (attention-based CNN) baselines.",5.2 Answer Sentence Selection,[0],[0]
"We attribute the improvement to two potential advantages of our AdaQA model: (i) for the two previous baseline methods, the interaction between question and answer takes place either before or after convolution.",5.2 Answer Sentence Selection,[0],[0]
"However, in our AdaQA model, the communication between two sentences is inherent in the convolution operation, and thus can provide the abstracted features with more flexibility; (ii) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer, which could enable the model to recover from initial local maxima corresponding to incorrect predictions (Xiong et al., 2017).
",5.2 Answer Sentence Selection,[0],[0]
"Paragraph Identification Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems, we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset.",5.2 Answer Sentence Selection,[0],[0]
"To ensure a fair comparison, we employ the same data splits as in (Wang et al., 2017b).",5.2 Answer Sentence Selection,[0],[0]
"As illustrated in Table 5, our twoway AdaQA model again exhibits superior performances compared with basic CNN models (as reported in (Wang et al., 2017b)).",5.2 Answer Sentence Selection,[0],[0]
"Reasoning ability To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model, we further categorize the questions in the WikiQA test set into 5 types containing: ‘What’, ‘Where’, ‘How’, ‘When’ or ‘Who’.",5.3 Discussion,[0],[0]
We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types.,5.3 Discussion,[0],[0]
"Similar to the findings in (Miao et al., 2016), we observe that the ‘How’ question is the hardest to answer, with the lowest MAP scores.",5.3 Discussion,[0],[0]
"However, our AdaQA model improves most over the basic CNN on the ‘How’ type question, see Figure 3(b).",5.3 Discussion,[0],[0]
"Further comparing our results with NASM in (Miao et al., 2016), our AdaQA model (with a MAP score of 0.579) outperforms their reported ‘How’ question MAP scores (0.524) by a large margin, indicating that the adaptive convolutional filter-generation mechanism improves the model’s ability to read and reason over natural language sentences.
",5.3 Discussion,[0],[0]
"Filter visualization To better understand what information has been encoded into our contextaware filters, we visualize one of the filters for sentences within the test set (on the DBpedia dataset) with t-SNE.",5.3 Discussion,[0],[0]
The corresponding results are shown in Figure 3(c).,5.3 Discussion,[0],[0]
"It can be observed that the filters for documents with the same label (ontology) are grouped into clusters, indicating that for different types of document, ACNN has leveraged distinct convolutional filters for better feature extraction.",5.3 Discussion,[0],[0]
"We presented a context-aware convolutional filtergeneration mechanism, introducing a meta network to adaptively produce a set of input-aware filters.",6 Conclusions,[0],[0]
"In this manner, the filter weights vary from sample to sample, providing the CNN encoder network with more modeling flexibility and capacity.
",6 Conclusions,[0],[0]
"This framework is further generalized to model question-answer sentence pairs, leveraging a twoway feature abstraction process.",6 Conclusions,[0],[0]
"We evaluate our models on several document-categorization and sentence matching benchmarks, and they consistently outperform the standard CNN and attentionbased CNN baselines, demonstrating the effectiveness of our framework.
",6 Conclusions,[0],[0]
"Acknowledgments This research was supported in part by DARPA, DOE, NIH, ONR and NSF.",6 Conclusions,[0],[0]
Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP).,abstractText,[0],[0]
"Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences.",abstractText,[0],[0]
"In this paper, we consider an approach of using a small meta network to learn contextaware convolutional filters for text processing.",abstractText,[0],[0]
The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters.,abstractText,[0],[0]
"We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations.",abstractText,[0],[0]
"In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-aware filters, consistently outperforms the standard CNN and attentionbased CNN baselines.",abstractText,[0],[0]
"By visualizing the learned context-aware filters, we further validate and rationalize the effectiveness of proposed framework.",abstractText,[0],[0]
Learning Context-Aware Convolutional Filters for Text Processing,title,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",1. Introduction,[0],[0]
This is particularly important while dealing with exponentially large domains such as source code and logical expressions.,1. Introduction,[0],[0]
Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.,1. Introduction,[0],[0]
"Although symbolic reasoning is very powerful, it also tends to be hard.",1. Introduction,[0],[0]
"For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse.",1. Introduction,[0],[0]
"This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se-
Work started when M. Allamanis was at Edinburgh.",1. Introduction,[0],[0]
This work was done while P. Kohli was at Microsoft.,1. Introduction,[1.0],['This work was done while P. Kohli was at Microsoft.']
"1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
mantic relationships.",1. Introduction,[0],[0]
"However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.",1. Introduction,[0],[0]
"In this work, we explore the direction of learning continuous semantic representations of symbolic expressions.",1. Introduction,[0],[0]
"The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.",1. Introduction,[0],[0]
"Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space.
",1. Introduction,[0],[0]
"In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions.",1. Introduction,[0],[0]
"Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.",1. Introduction,[0],[0]
"This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature.",1. Introduction,[0],[0]
"As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.
",1. Introduction,[0],[0]
"Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.",1. Introduction,[0],[0]
"They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",1. Introduction,[0],[0]
"While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",1. Introduction,[0],[0]
"In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different.",1. Introduction,[0],[0]
"The underlying conceptual problem is how to develop a con-
1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.
",1. Introduction,[0],[0]
"tinuous representation that follows syntax but not too much, that respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.
",1. Introduction,[0],[0]
"To tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET).",1. Introduction,[0],[0]
"EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.",1. Introduction,[1.0],"['EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.']"
"As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions.",1. Introduction,[0],[0]
"The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree.",1. Introduction,[0],[0]
"To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors.",1. Introduction,[0],[0]
"Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.
",1. Introduction,[0],[0]
"To summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task.",1. Introduction,[0],[0]
"(b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space.",1. Introduction,[0],[0]
"(c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives.",1. Introduction,[0],[0]
Code and data are available at groups.inf.ed.ac.uk/cup/semvec.,1. Introduction,[0],[0]
"In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class.",2. Model,[0],[0]
"Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.
",2. Model,[0],[0]
Problem Hardness.,2. Model,[0],[0]
Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse.,2. Model,[0],[0]
"For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability — an NPcomplete problem.",2. Model,[0],[0]
"Of course, we do not expect to circum-
vent an NP-complete problem with neural networks.",2. Model,[0],[0]
A network for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP .,2. Model,[0],[0]
"Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set.",2. Model,[0],[0]
"The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.
",2. Model,[0],[0]
Notation and Framework.,2. Model,[0],[0]
"To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN)",2. Model,[0],[0]
"(Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.",2. Model,[0],[0]
"Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network.",2. Model,[0],[0]
We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes.,2. Model,[0],[0]
"We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type “a” referring to the variable a or of type “and” referring to a node of the logical AND (∧) operation.",2. Model,[0],[0]
We refer to the type of a node n as τn.,2. Model,[0],[0]
"In pseudocode, TREENNs retrieve the representation of a tree T rooted at node ρ, by invoking the function TREENN(ρ) that returns a vector representation rρ ∈ RD, i.e., a SEMVEC.",2. Model,[0],[0]
"The function is defined as TREENN (current node n)
if n is not a leaf then rn ← COMBINE(TREENN(c0), . . .",2. Model,[0],[0]
", TREENN(ck), τn), where (c0, . . .",2. Model,[0.9760096000399016],"[', TREENN(ck), τn), where (c0, .']"
", ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn)
return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.",2. Model,[0.9999999896818311],"[', ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn) return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.']"
"Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.",2. Model,[0],[0]
"As discussed next, these will both prove to be serious limitations in our setting.",2. Model,[0],[0]
"To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).",2. Model,[1.0],"['To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).']"
"Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.",2.1. Neural Equivalence Networks,[0],[0]
"In this work, we find that standard neural architectures do not handle well this challenge.",2.1. Neural Equivalence Networks,[1.0],"['In this work, we find that standard neural architectures do not handle well this challenge.']"
"To represent semantics from syntax, we need to learn to recursively
compose and decompose semantic representations and remove syntactic “noise”.",2.1. Neural Equivalence Networks,[0],[0]
"Any syntactic operation may significantly change semantics (e.g. negation, or appending ∧FALSE) while we may reach the same semantic state through many possible operations.",2.1. Neural Equivalence Networks,[0],[0]
This necessitates using high-curvature operations over the semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. ¬¬A and A should have an identical SEMVECs).",2.1. Neural Equivalence Networks,[0],[0]
"Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).",2.1. Neural Equivalence Networks,[1.0],"['Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).']"
"Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements.",2.1. Neural Equivalence Networks,[0],[0]
"The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.
",2.1. Neural Equivalence Networks,[0],[0]
The first extension that we introduce is to the network structure at each layer in the tree.,2.1. Neural Equivalence Networks,[1.0],['The first extension that we introduce is to the network structure at each layer in the tree.']
"Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.",2.1. Neural Equivalence Networks,[0],[0]
"During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations.",2.1. Neural Equivalence Networks,[0],[0]
Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear.,2.1. Neural Equivalence Networks,[0],[0]
For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Instead, we turn to multi-layer neural
networks.",2.1. Neural Equivalence Networks,[0],[0]
"In particular, we define the network as shown in the function COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children.,2.1. Neural Equivalence Networks,[0],[0]
"Each node type τn, e.g., each logical operator, has a different set of weights.",2.1. Neural Equivalence Networks,[0],[0]
"We experimented with deeper networks but this did not yield any improvements.
",2.1. Neural Equivalence Networks,[0],[0]
"However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.",2.1. Neural Equivalence Networks,[1.0],"['However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.']"
"This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to “echo” its own errors and producing unstable feedback loops.",2.1. Neural Equivalence Networks,[0],[0]
"We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.",2.1. Neural Equivalence Networks,[1.0],"['We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.']"
We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.,2.1. Neural Equivalence Networks,[1.0],['We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.']
"That is, we set LOOKUPLEAFEMBEDDING(τn) = Cτn/ ‖Cτn‖2 , and we normalize the output of the final layer of COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
"The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).",2.1. Neural Equivalence Networks,[0],[0]
"Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.",2.1. Neural Equivalence Networks,[0],[0]
"As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.
",2.1. Neural Equivalence Networks,[0.9999999831938527],"['As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.']"
"Although these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good
performance.",2.1. Neural Equivalence Networks,[0],[0]
"In our early experiments, we noticed that the networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar.",2.1. Neural Equivalence Networks,[0],[0]
"At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.
",2.1. Neural Equivalence Networks,[0],[0]
"Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.",2.1. Neural Equivalence Networks,[0],[0]
We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE).,2.1. Neural Equivalence Networks,[0],[0]
We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility.,2.1. Neural Equivalence Networks,[0],[0]
"Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information.",2.1. Neural Equivalence Networks,[0],[0]
"To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example.",2.1. Neural Equivalence Networks,[0],[0]
"Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree)",2.1. Neural Equivalence Networks,[0],[0]
it is often possible to completely determine or at least place strong constraints on the semantics of the third.,2.1. Neural Equivalence Networks,[0],[0]
"For example, consider a boolean formula F (a, b) = F1(a, b) ∨ F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b.",2.1. Neural Equivalence Networks,[0],[0]
"Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true.",2.1. Neural Equivalence Networks,[0],[0]
"More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.
",2.1. Neural Equivalence Networks,[0],[0]
"Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children.",2.1. Neural Equivalence Networks,[0],[0]
"More specifically, given any node p in the tree with children c0 . . .",2.1. Neural Equivalence Networks,[0],[0]
"ck, we can define a parent-children tuple",2.1. Neural Equivalence Networks,[0],[0]
"[rc0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", rck , rp] containing the (computed) SEMVECs of the children and parent nodes.",2.1. Neural Equivalence Networks,[0],[0]
What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder.,2.1. Neural Equivalence Networks,[0],[0]
"We then seek to minimize the reconstruction error of the child representations (r̃c0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from the reconstructed children.",2.1. Neural Equivalence Networks,[0],[0]
"More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with κ percent of its elements set to zero.",2.1. Neural Equivalence Networks,[0],[0]
Note that the encoder is specific to the parent node type τp.,2.1. Neural Equivalence Networks,[0],[0]
"Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs
in two major ways.",2.1. Neural Equivalence Networks,[0],[0]
"First, SUBEXPAE autoencodes on the entire parent-children representation tuple, rather than the child representations alone.",2.1. Neural Equivalence Networks,[0],[0]
"Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.
",2.1. Neural Equivalence Networks,[0],[0]
Subexpression autoencoding has several desirable effects.,2.1. Neural Equivalence Networks,[0],[0]
"First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions.",2.1. Neural Equivalence Networks,[0],[0]
"Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.",2.1. Neural Equivalence Networks,[1.0],"['Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.']"
"Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.",2.1. Neural Equivalence Networks,[0],[0]
"The goal is to nudge the network to learn consistent, reversible semantics.",2.1. Neural Equivalence Networks,[0],[0]
"Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class.",2.1. Neural Equivalence Networks,[0],[0]
"To illustrate this point, imagine two semantically equivalent c′0 and c ′′ 0 child nodes of different expressions that
have distant SEMVECs, i.e. ∥∥rc′0 − rc′′0 ∥∥2 although COMBINE(rc′0 , . . . )",2.1. Neural Equivalence Networks,[0],[0]
"≈ COMBINE(rc′′0 , . . . ).",2.1. Neural Equivalence Networks,[0],[0]
"In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).",2.1. Neural Equivalence Networks,[0],[0]
"Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.",2.1. Neural Equivalence Networks,[1.0],"['Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.']"
We train EQNETs from a dataset of expressions whose semantic equivalence is known.,2.2. Training,[1.0],['We train EQNETs from a dataset of expressions whose semantic equivalence is known.']
Given a training set T = {T1 . . .,2.2. Training,[0],[0]
"TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . .",2.2. Training,[0.9913775843008837],"['TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 .']"
eJ}.,2.2. Training,[0],[0]
"We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N .",2.2. Training,[0],[0]
"Given an expression tree T that belongs to the equivalence class ei ∈ E , we compute the probability
P (ei|T ) =",2.2. Training,[0],[0]
"exp
( TREENN(T )>qei + bi )∑ j exp ( TREENN(T )>",2.2. Training,[0],[0]
qej,2.2. Training,[0],[0]
"+ bj
) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms.",2.2. Training,[0],[0]
"Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions.",2.2. Training,[0],[0]
"This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to
the training set.",2.2. Training,[0],[0]
"To train the model, we use a max-margin objective that maximizes classification accuracy, i.e.
LACC(T, ei) = max (
0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2)
where m > 0 is a scalar margin.",2.2. Training,[0.9999999505481005],"['To train the model, we use a max-margin objective that maximizes classification accuracy, i.e. LACC(T, ei) = max ( 0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2) where m > 0 is a scalar margin.']"
"And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei ∈ E is
L(T, ei) = LACC(T, ei) + µ |Q| ∑ n∈Q SUBEXPAE(ch(n), n)
(3)
",2.2. Training,[0],[0]
"where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight.",2.2. Training,[0],[0]
"We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics.",2.2. Training,[0],[0]
"So, for each epoch t, we set µ = 1− 10−νt with ν ≥ 0.",2.2. Training,[0],[0]
"Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.",2.2. Training,[0],[0]
"In practice, we found the optimization to be very unstable, yielding suboptimal performance.",2.2. Training,[0],[0]
We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity.,2.2. Training,[0],[0]
Datasets.,3. Evaluation,[0],[0]
We generate datasets of expressions grouped into equivalence classes from two domains.,3. Evaluation,[0],[0]
The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions.,3. Evaluation,[0],[0]
"In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression.",3. Evaluation,[0],[0]
"When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c · (a · a+ b) and f ·(d·d+e) are not equivalent.",3. Evaluation,[0],[0]
"For each domain, we generate “simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more complex operators.",3. Evaluation,[0],[0]
We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.,3. Evaluation,[0],[0]
All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly.,3. Evaluation,[0],[0]
Table 1 shows the datasets we generated.,3. Evaluation,[0],[0]
In the supplementary material we present some sample expressions.,3. Evaluation,[0],[0]
"For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) — although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.",3. Evaluation,[0],[0]
"Learning SEMVECs for boolean expressions
is already a hard problem; with n boolean variables, there are 22 n
equivalence classes (i.e. one for each possible truth table).",3. Evaluation,[0],[0]
"We split the datasets into training, validation and test sets.",3. Evaluation,[0],[0]
"We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS).",3. Evaluation,[0],[0]
It is easiest to describe UNSEENEQCLASS first.,3. Evaluation,[0],[0]
"To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set.",3. Evaluation,[0],[0]
We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class.,3. Evaluation,[0],[0]
We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset.,3. Evaluation,[0],[0]
"Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%–15%–25%.",3. Evaluation,[0],[0]
"We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.
Baselines.",3. Evaluation,[0],[0]
"To compare the performance of our model, we train the following baselines.",3. Evaluation,[0],[0]
"TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses).",3. Evaluation,[0],[0]
This captures topical/declarative knowledge but is unable to capture procedural knowledge.,3. Evaluation,[0],[0]
GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.,3. Evaluation,[0],[0]
Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.,3. Evaluation,[0],[0]
We also include two recursive neural networks (TREENN).,3. Evaluation,[0],[0]
The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014).,3. Evaluation,[0],[0]
"We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.",3. Evaluation,[0],[0]
"This shows the effect of SEMVEC normalization and subexpression autoencoder.
Hyperparameters.",3. Evaluation,[0],[0]
"We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . .",3. Evaluation,[0],[0]
", 15) statistics (described next).",3. Evaluation,[0],[0]
The selected hyperparameters are detailed in the supplementary material.,3. Evaluation,[0],[0]
Metrics.,3.1. Quantitative Evaluation,[0],[0]
To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class.,3.1. Quantitative Evaluation,[0],[0]
"More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the
score as
scorek(q) = |Nk(q) ∩ c| min(k, |c|) .",3.1. Quantitative Evaluation,[0],[0]
"(4)
To report results for a given testset, we simply average scorek(q) for all expressions q in the testset.",3.1. Quantitative Evaluation,[0],[0]
"We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation.,3.1. Quantitative Evaluation,[0],[0]
Figure 2 presents the average per-model precision-recall curves across the datasets.,3.1. Quantitative Evaluation,[0],[0]
Table 1 shows score5 of UNSEENEQCLASS.,3.1. Quantitative Evaluation,[0],[0]
Detailed plots are found in the supplementary material.,3.1. Quantitative Evaluation,[0],[0]
"EQNET performs better for all datasets, by a large margin.",3.1. Quantitative Evaluation,[0],[0]
"The only exception is POLY5, where the 2-L TREENN performs better.",3.1. Quantitative Evaluation,[0],[0]
"However, this may have to do with the small size of the dataset.",3.1. Quantitative Evaluation,[0],[0]
The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn.,3.1. Quantitative Evaluation,[0],[0]
"Understandably, introducing more variables increases the size of the represented space reducing performance.",3.1. Quantitative Evaluation,[0],[0]
"The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used.",3.1. Quantitative Evaluation,[0],[0]
Similar observations can be made for sequence models.,3.1. Quantitative Evaluation,[0],[0]
The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs.,3.1. Quantitative Evaluation,[0],[0]
"Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.",3.1. Quantitative Evaluation,[0],[0]
"Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation of Compositionality.,3.1. Quantitative Evaluation,[0],[0]
"We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of
a small size.",3.1. Quantitative Evaluation,[0],[0]
"To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8.",3.1. Quantitative Evaluation,[0],[0]
We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b.,3.1. Quantitative Evaluation,[0],[0]
"These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.
",3.1. Quantitative Evaluation,[0],[0]
"Impact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here.",3.1. Quantitative Evaluation,[0],[0]
"First, SUBEXPAE improves performance.",3.1. Quantitative Evaluation,[0],[0]
"When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS.",3.1. Quantitative Evaluation,[0],[0]
"This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average.",3.1. Quantitative Evaluation,[0],[0]
"However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets.",3.1. Quantitative Evaluation,[0],[0]
The second key difference to traditional TREENNs is the output normalization and the residual connections.,3.1. Quantitative Evaluation,[0],[0]
"Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC).",3.1. Quantitative Evaluation,[0],[0]
"We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs.",3.1. Quantitative Evaluation,[0],[0]
Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class.,3.2. Qualitative Evaluation,[0],[0]
"Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is
Table 2.",3.2. Qualitative Evaluation,[0],[0]
Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8.,3.2. Qualitative Evaluation,[0],[0]
"A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.
",3.2. Qualitative Evaluation,[0],[0]
Expr a ∧ (a ∧ (a ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (a+ c)) ((a+ c) ·,3.2. Qualitative Evaluation,[0],[0]
"c) + a (b · b)− b
tfidf c",3.2. Qualitative Evaluation,[0],[0]
∧,3.2. Qualitative Evaluation,[0],[0]
((a ∧ a) ∧ (¬a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
a+ (c+ a) ·,3.2. Qualitative Evaluation,[0],[0]
c (c · a) + (a+ c) b · (b− b) GRU X a ∧ (a ∧ (c ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
b+ (c · (a+ c)) ((b+ c) · c) + a (b+ b) · b− b 1L-TREENN a ∧ (a ∧ (a ∧ (¬b))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬b))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬b)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (b+ c)) ((b+ c) ·,3.2. Qualitative Evaluation,[0],[0]
c) + a (a− c) · b− b EQNET X X (¬(b⇒ (b ∨ c))),3.2. Qualitative Evaluation,[0],[0]
"∧ a X X (b · b) · b− b
involved.",3.2. Qualitative Evaluation,[0],[0]
"In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10.",3.2. Qualitative Evaluation,[0],[0]
"As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.
",3.2. Qualitative Evaluation,[0],[0]
Figure 4 shows a visualization of score5 for each node in the expression tree.,3.2. Qualitative Evaluation,[0],[0]
"One may see that as EQNET knows how
¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b)))",3.2. Qualitative Evaluation,[0],[0]
"((c ∨ (¬b))⇒ a) ∧ (a ⇒ a)
((b ⊕ (¬c))",3.2. Qualitative Evaluation,[0],[0]
"∧ b)⊕ (a ∨ b) ((b · a)− a) · b
a − ((a + b) · a) ((c · b) · c) · a b + ((b · b) · b)
Figure 4.",3.2. Qualitative Evaluation,[0],[0]
Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET.,3.2. Qualitative Evaluation,[0],[0]
"The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.
to compose expressions that achieve good score, even if the subexpressions achieve a worse score.",3.2. Qualitative Evaluation,[0],[0]
"This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network.",3.2. Qualitative Evaluation,[0],[0]
"Larger scale interactive t-SNE visualizations can be found online.
",3.2. Qualitative Evaluation,[0],[0]
Figure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives.,3.2. Qualitative Evaluation,[0],[0]
It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space.,3.2. Qualitative Evaluation,[0],[0]
"Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and −a, topbottom between polynomials with−b and b and the diagonal parellelt to y = −x between c and−c.",3.2. Qualitative Evaluation,[0],[0]
We observe a similar behavior in Figure 5a for boolean expressions.,3.2. Qualitative Evaluation,[0],[0]
"Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-
mann, 1994).",4. Related Work,[0],[0]
One can consider a serialized version of the resulting neural network as a representation of the expression.,4. Related Work,[0],[0]
"However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.
",4. Related Work,[0],[0]
"Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.",4. Related Work,[0],[0]
Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.,4. Related Work,[0],[0]
"EQNET’s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.",4. Related Work,[0],[0]
"In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.
",4. Related Work,[0],[0]
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.,4. Related Work,[0],[0]
"Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics.",4. Related Work,[0],[0]
Piech et al. (2015) also learn distributed matrix representations of student code submissions.,4. Related Work,[0],[0]
"However, to learn the representations, they use input and output program states and do not test for program equivalence.",4. Related Work,[0],[0]
"Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs.",4. Related Work,[0],[0]
"Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.
",4. Related Work,[0],[0]
"More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.",4. Related Work,[0],[0]
"In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better
represent equivalence, and do not use search when looking for equivalent expressions.",4. Related Work,[0],[0]
Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.,4. Related Work,[0],[0]
"In the future, SEMVECs may be useful within this area.
",4. Related Work,[0],[0]
"Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).",4. Related Work,[0],[0]
"In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated.",4. Related Work,[0],[0]
"In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.
",5. Discussion & Conclusions,[0],[0]
We show that EQNETs perform significantly better than state-of-the-art alternatives.,5. Discussion & Conclusions,[0],[0]
"But further improvements are needed, especially for more robust training of compositional models.",5. Discussion & Conclusions,[0],[0]
"In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented.",5. Discussion & Conclusions,[0],[0]
"Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"In the future, to represent more complex procedures, variable-sized representations would seem to be required.",5. Discussion & Conclusions,[0],[0]
This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].,Acknowledgments,[0],[0]
We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources.,Acknowledgments,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",abstractText,[0],[0]
"As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions.",abstractText,[0],[0]
"These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different.",abstractText,[0],[0]
"The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures.",abstractText,[0],[0]
"We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",abstractText,[0],[0]
Learning Continuous Semantic Representations of Symbolic Expressions,title,[0],[0]
