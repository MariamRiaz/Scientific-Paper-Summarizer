37	27	The actions available in a history is A(h) and the player who acts at that history is P (h) ∈ P ∪ c, where c denotes chance.
38	23	Chance plays an action a ∈ A(h) with a fixed probability.
41	52	For each player i ∈ P , there is a payoff function ui : Z → < where u1 = −u2.
43	34	Imperfect information is represented by information sets for each player i ∈ P by a partition Ii of h ∈ H : P (h) = i.
44	43	For any information set I ∈ Ii, all histories h, h′ ∈ I are indistinguishable to player i, so A(h) = A(h′).
64	23	In two-player zero-sum games, if σi and σ−i are both Nash equilibrium strategies, then 〈σi, σ−i〉 is a Nash equilibrium.
66	45	Counterfactual Regret Minimization (CFR) is a popular algorithm for extensive-form games in which the strategy vector for each information set is determined according to a regret-minimization algorithm (Zinkevich et al., 2007).
67	65	We use regret matching (RM) (Hart & Mas-Colell, 2000) as the regret-minimization algorithm, but the material presented in this paper also applies to other regret minimizing algorithms such as Hedge (Brown et al., 2017).
82	67	In vanilla CFR, the entire game tree is traversed separately for each player historyby-history.
83	249	On each traversal, the regret for each action of a history’s information set is updated based on the expected value for that action on that iteration, weighed by the probability of opponents taking actions to reach the history (that is, weighed by πσ t −i(h)).
84	21	However, if a history h is reached on iteration t in which πσ t −i(h) = 0, then from (1) and (2) the strategy at h contributes nothing on iteration t to the regret of I(h) (or to the information sets above it).
88	19	While partial pruning allows one to prune paths that an opponent reaches with zero probability, Regret-Based Pruning allows one to also prune paths that the traverser reaches with zero probability (Brown & Sandholm, 2015a).
95	52	Moreover, since player i never plays action a with positive probability between iterations t and t′, that means every other player can apply partial pruning on that part of the game tree for iterations t′ − t, and skip it completely.
96	102	This, in turn, means that player i has free rein to play whatever they want in D(I, a) without affecting the regrets of the other players.
98	40	A CBR to the average of the opponent strategies on the t′ − t iterations would qualify as such a zero-regret strategy.
100	41	Thus, in RM, if Rt0(I, a) < 0, we can prune the game tree beyond action a from iteration t0 until iteration t1 so long as ∑t0 t=1 v σt(I, a) + ∑t1 t=t0+1 πσ t −i(I)U(I, a) ≤∑t1 t=1 v σt(I).
126	16	In practice, rather than check whether (5) is met for an action on every iteration, one could only check actions that σt(I) and either∑T t=1 v σt(I, a) or RT (I, a) are stored.
134	25	That space need only be re-allocated once (9) ceases to hold and we cannot immediately begin pruning again (that is, (5) does not hold).
135	50	Theorem 2 proves that for any information set I and action a ∈ A(I) that is not part of a best response to a Nash equilibrium, there is an iteration TI,a such that for all T ≥ TI,a, action a in information set I (and its descendants) can be pruned.3 Thus, once this TI,a is reached, it will never be necessary to allocate space for regret in D(I, a) again.
136	27	In a two-player zero-sum game, if for every opponent Nash equilibrium strategy σ∗−P (I), CBV σ ∗ −P (I)(I, a) < CBV σ ∗ −P (I)(I), then there exists a TI,a and δI,a > 0 such that after T ≥ TI,a iterations of CFR, CBV σ̄ T −i(I, a)− ∑T t=1 v σt (I) T ≤ −δI,a.
137	23	While such a constant TI,a exists for any suboptimal action, BRP cannot determine whether or when TI,a is reached.
138	17	Thus, it is still necessary to check whether (5) is satisfied whenever (9) no longer holds, and to recalculate how much longer D(I, a) can safely be pruned.
152	18	In a two-player zero-sum game with some threshold on the average strategy C√ T for C > 0, after a finite number of iterations CFR with BRP requires only O ( |IS ||A| ) space.
157	24	Section 3 proved that CFR with BRP converges in the same number of iterations as CFR alone.
202	16	Regret-Based Pruning with CFR+ does significantly better, while BRP with CFR using RM+ sees no improve- ment over BRP with CFR.
205	52	We introduced BRP, a new form of pruning that provably reduces both the space needed to solve an imperfectinformation game and the time needed to reach an -Nash equilibrium.
207	55	Experimentally, BRP reduced the space needed to solve a game by a factor of 7, with the reduction factor increasing with game size.
208	45	While the early iterations may still be slow and require the same amount of space as CFR without BRP, these early iterations can be skipped by warm starting CFR with an abstraction of the game (Brown & Sandholm, 2016).
209	32	This paper focused on the theory of BRP when applied to CFR, the most popular algorithm for solving imperfectinformation games.
210	23	However, BRP can also be applied to Fictitious Play (Heinrich et al., 2015) and likely extends to other iterative algorithms as well (Hoda et al., 2010).
