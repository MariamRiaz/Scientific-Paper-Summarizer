{"sections": [{"heading": "1. Introduction", "text": "Over the last several years, the world has witnessed the emergence of data sets of an unprecedented scale across different scientific disciplines. This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data. In this paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.\nSeeding \u2014 the task of finding initial cluster centers \u2014 is critical to finding good clusterings for k-Means. In fact, the seeding step of the state of the art algorithm k-means++ (Arthur & Vassilvitskii, 2007) provides the\n1Department of Computer Science, ETH Zurich. Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ntheoretical guarantee on the solution quality while the subsequent refinement using Lloyd\u2019s algorithm (Lloyd, 1982) only guarantees that the quality does not deteriorate. While the k-means++ seeding step guarantees a solution that is O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through the data set. This makes it unsuitable for the massive data setting where the data set is distributed across machines and computation has to occur in parallel.\nAs a remedy, Bahmani et al. (2012) propose the k-means\u2016 algorithm which produces seedings for kMeans with a reduced number of sequential iterations. Whereas k-means++ only samples a single cluster center in each of k rounds, k-means\u2016 samples in expectation ` points in each of t iterations. Provided t is small enough, this makes k-means\u2016 suitable for a distributed setting as the number of synchronizations is reduced.\nOur contributions. We provide a novel analysis of k-means\u2016 that bounds the expected solution quality for any number of rounds t and any oversampling factor ` \u2265 k, the two parameters that need to be chosen in practice. Our bound on the expected quantization error includes both a \u201ctraditional\u201d multiplicative error term based on the optimal solution as well as a scale-invariant additive error term based on the variance of the data. The key insight is that this additive error term vanishes at a rate of ( k e` )t if t or ` is increased. This shows that k-means\u2016 provides provably good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon that k-means\u2016 works very well even for small t.\nWe further provide a hard instance on which k-means\u2016 provably incurs an additive error based on the variance of the data and for which an exclusively multiplicative error guarantee cannot be achieved. This implies that an additive error term such as the one in our analysis is in fact necessary if less than k \u2212 1 rounds are employed."}, {"heading": "2. Background & related work", "text": "k-Means clustering. Let X denote a set of points in Rd. The k-Means clustering problem is to find a set C of k cluster centers in Rd that minimizes the quantization error\n\u03c6X (C) = \u2211 x\u2208X d(x,C)2 = \u2211 x\u2208X min q\u2208C \u2016x\u2212 q\u201622.\nAlgorithm 1 k-means++ seeding Require: weighted data set (X , w), number of clusters k\n1: C \u2190 sample single x \u2208 X with probability wx\u2211 x\u2032\u2208X wx\u2032 2: for i = 2, . . . , k do 3: Sample x \u2208 X with probability wx d(x,C)\n2\u2211 x\u2032\u2208X wx\u2032 d(x \u2032,C)2\n4: C \u2190 C \u222a {x} 5: Return C\nWe denote the optimal quantization error by \u03c6OPT(X ) while the variance of the data is defined as Var(X ) = \u03c6X ({\u00b5(X )}) where \u00b5(X ) is the mean of X .\nk-means++ seeding. Given a data set X and any set of cluster centers C \u2282 X , the D2-sampling strategy selects a new center by sampling each point x \u2208 X with probability\np(x) = d(x,C)2\u2211\nx\u2032\u2208X d(x \u2032, C \u2032)2\n.\nThe seeding step of k-means++ (Arthur & Vassilvitskii, 2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k \u2212 1 cluster centers using D2 sampling whereby C is always the set of previously sampled centers. Arthur & Vassilvitskii (2007) show that the solution quality \u03c6k-means++ of k-means++ seeding is bounded in expectation by\nE[\u03c6k-means++] \u2264 8 (log2 k + 2)\u03c6OPT(X ).\nThe computational complexity of k-means++ seeding is O(nkd) where n is the number of data points and d the dimensionality. Unfortunately, the iterations in k-means++ seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data. This makes the algorithm unsuitable for the distributed setting.\nk-means\u2016 seeding. As a remedy, Bahmani et al. (2012) propose the algorithm k-means\u2016 which aims to reduce the number of sequential iterations. The key component of k-means\u2016 is detailed in Algorithm 2 in what we call k-means\u2016 overseeding: First, a data point is sampled as the first cluster center uniformly at random. Then, in each of t sequential rounds, each data point x \u2208 X is independently sampled with probability min ( 1, ` d(x,C) 2\n\u03c6X (C)\n) and\nadded to the set of sampled centers C at the end of the round. The parameter ` \u2265 1 is called the oversampling factor and determines the expected number of sampled points in each iteration.\nAt the end of Algorithm 2, one obtains an oversampled solution with t` cluster centers in expectation. The full k-means\u2016 seeding algorithm as detailed in Algorithm 3 reduces such a solution to k centers as follows: First, each of the centers in the oversampled solution is weighted by the number of data points which are closer to it than the\nAlgorithm 2 k-means\u2016 overseeding Require: data set X , # rounds t, oversampling factor `\n1: C \u2190 sample a point uniformly at random from X 2: for i = 1, 2, . . . , t do 3: C \u2032 \u2190 \u2205 4: for x \u2208 X do 5: Add x to C \u2032 with probability min ( 1, ` d(x,C) 2\n\u03c6X (C) ) 6: C \u2190 C \u222a C \u2032 7: Return C\nAlgorithm 3 k-means\u2016 seeding Require: data set X , # rounds t, oversampling factor `\n1: B \u2190 Result of Algorithm 2 applied to (X , t, `) 2: for c \u2208 B do 3: Xc \u2190 points x \u2208 X whose closest center in B is c (ties broken arbitrarily but consistently) 4: wc \u2190 |Xc| 5: C \u2190 Result of Algorithm 1 applied to (B,w) 6: Return C\nother centers. Then, k-means++ seeding is run on the weighted oversampled solution to produce a set of k final centers. The total computational complexity of Algorithm 3 is O(nt`d) in expectation.\nThe key intuition behind k-means\u2016 is that, if we choose a large oversampling factor `, the number of rounds t can be small \u2014 certainly much smaller than k, preferably even constant. The step in lines 4 and 5 in Algorithm 2 can be distributed over several machines and after each round the set C can be synchronized. Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently run in a distributed setting.1\nOther related work. Celebi et al. (2013) provide an overview over different seeding methods for k-Means. D2sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and Arthur & Vassilvitskii (2007). This research direction has led to polynomial time approximation schemes based on D2-sampling (Jaiswal et al., 2014; 2015), constant factor approximations based on sampling more than k centers (Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Ro\u0308glin, 2011). Recently, algorithms to approximate k-means++ seeding based on Markov Chain Monte Carlo have been proposed by Bachem et al. (2016b;a). Finally, k-means++ has been used to construct coresets \u2014 small data set summaries \u2014 for k-Means clustering (Lucic et al., 2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic et al., 2017).\n1A popular choice is the MLLib library of Apache Spark (Meng et al., 2016) which uses k-means\u2016 by default."}, {"heading": "3. Intuition and key results", "text": "In this section, we provide the intuition and the main results behind our novel analysis of k-means\u2016 and defer the formal statements and the formal proofs to Section 4."}, {"heading": "3.1. Solution quality of k-means\u2016", "text": "Solution quality of Algorithm 2. We first consider Algorithm 2 as it largely determines the final solution quality. Algorithm 3 with its use of k-means++ to obtain the final k cluster centers, only adds an additional O(log k) factor as shown in Theorem 1. Our key result is Lemma 4 (see Section 4) which guarantees that, for ` \u2265 k, the expected error of solutions computed by Algorithm 2 is at most\nE[\u03c6X (C)] \u2264 2 ( k\ne`\n)t Var(X ) + 26\u03c6OPT(X ). (1)\nThe first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error \u03c6OPT(X ). It is scale-invariant since both the variance and the quantization error are scaled by \u03bb2 if we scale the data setX by \u03bb > 0. The second term is a \u201ctraditional\u201d multiplicative error term based on the optimal quantization error.\nGiven a fixed oversampling factor `, the additive error term decreases exponentially if the number of rounds t is increased. Similarly, for a fixed number of rounds t, it decreases polynomially at a rate O ( 1 `t ) if the over sampling factor ` is increased. This result implies that even for a constant number of rounds one may obtain good clusterings by increasing the oversampling factor `. This explains the empirical observation that often even a low number of rounds t is sufficient and that increasing ` increases the solution quality (Bahmani et al., 2012). The practical implications of this result are non-trivial: Even for the choice of t = 5 and ` = 5k one retains at most 0.0004% of the variance as an additive error. Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error term (Bachem et al., 2017).\nComparison to previous result. Bahmani et al. (2012) show the following result: Let C be the set returned by Algorithm 2 with t rounds. For \u03b1 = exp ( \u2212(1\u2212 e\u2212`/(2k)) ) \u2248 e\u2212 `2k , Corollary 3 of Bahmani et al. (2012) bounds the expected quality of C by\nE[\u03c6X (C)] \u2264 ( 1 + \u03b1\n2\n)t \u03c8 + 16\n1\u2212 \u03b1 \u03c6OPT(X ), (2)\nwhere \u03c8 denotes the quantization error of X based on the first, uniformly sampled center in k-means\u2016. The key difference compared to our result is as follows: First, even as we increase `, the factor \u03b1 is always non-negative. Hence, regardless of the choice of `, the additive \u03c8 term is reduced\nby at most 12 per round. 2 This means that, given the analysis in Bahmani et al. (2012), one would always obtain a constant additive error for a constant number of rounds t, even as ` is increased.\nGuarantee for Algorithm 3. Our main result \u2014 Theorem 1 \u2014 bounds the expected quality of solutions produced by Algorithm 3. As in Bahmani et al. (2012), one loses another factor ofO(ln k) compared to (1) due to Algorithm 3. Theorem 1. Let k \u2208 N, t \u2208 N and ` \u2265 k. Let X be a data set in Rd and C be the set returned by Algorithm 3. Then,\nE[\u03c6X (C)] \u2264 O\n(( k\ne`\n)t ln k ) Var(X )+O(ln k)\u03c6OPT(X )."}, {"heading": "3.2. A hard instance for k-means\u2016", "text": "We consider the case t < k\u22121 which captures the scenario where k-means\u2016 is useful in practice as for t \u2265 k one may simply use k-means++ instead.\nTheorem 2. For any \u03b2 > 0, k \u2208 N, t < k \u2212 1 and ` \u2265 1, there exists a data set X of size 2(t+ 1) such that\nE[\u03c6X (C)] \u2265 1\n4 (4`t)\n\u2212t Var(X ),\nwhere C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `. Furthermore,\nVar(X ) > 0, \u03c6OPT(X ) = 0 and n\u22062 \u2264 \u03b2\nwhere \u2206 is the largest distance between any points in X .\nTheorem 2 shows that there exists a data set on which k-means\u2016 provably incurs a non-negligible error even if the optimal quantization error is zero. This implies that k-means\u2016 with t < k \u2212 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets. We thus argue that an additive error bound such as the one in Theorem 1 is required. We note that the upper bound in (1) and the lower bound in Theorem 2 exhibit the same 1`t dependence on the oversampling factor ` for a given number of rounds t.\nFurthermore, Theorem 2 implies that, for general data sets, k-means\u2016 cannot achieve the multiplicative error of O(log k) in expectation as claimed by Bahmani et al. (2012).3 In particular, if the optimal quantization error is\n2Note that E[\u03c8] \u2264 2Var(X ) (Arthur & Vassilvitskii, 2007). 3To see this, let \u03c8 = \u03c6X (c1) be the quantization error of the first sampled center in Algorithm 2 and choose \u03b2 small enough such that the choice of t \u2208 O(log\u03c8) leads to t < k \u2212 1. For X in Theorem 2, \u03c6OPT(X ) = 0 which implies that the desired multiplicative guarantee would require E[\u03c6X (C)] = 0. However, the non-negligible, additive error in Theorem 2 and Var(X ) > 0 implies that E[\u03c6X (C)] > 0.\nzero, then k-means\u2016 would need to return a solution with quantization error zero. While we are guaranteed to remove a constant fraction of the error in each round, the number of required iterations may be unbounded."}, {"heading": "4. Theoretical analysis", "text": "Proof of Theorem 1. The proof is divided into four steps: First, we relate k-means\u2016-style oversampling to k-means++-style D2-sampling in Lemmas 1 and 2. Second, we analyze a single iteration of Algorithm 2 in Lemma 3. Third, we bound the expected solution quality of Algorithm 2 in Lemma 4. Finally, we use this to bound the expected solution quality of Algorithm 3 in Theorem 1.\nLemma 1. Let A be a finite set and let f : 2A \u2192 R be a set function that is non-negative and monotonically decreasing, i.e., f(V ) \u2265 f(U) \u2265 0, for all V \u2286 U .\nLet P be a probability distribution where, for each a \u2208 A, Ea denotes an independent event that occurs with probability qa \u2208 [0, 1]. Let C be the set of elements a \u2208 A for which the event Ea occurs.\nLet Q be the probability distribution on A where a single a \u2208 A is sampled with probability qa/ \u2211 a\u2208A qa.\nThen, with \u2205 denoting the empty set, we have that\nEP [f(C)] \u2264 EQ[f({a})] + e\u2212 \u2211 a\u2208A qaf(\u2205).\nProof. To prove the claim, we first construct a series of sub-events of the events {Ea}a\u2208A and then use them to recursively bound EP [f(C)].\nLet m \u2208 N. For each a \u2208 A, let ia be an independent random variable drawn uniformly at random from {1, 2, . . . ,m}. For each a \u2208 A and i = 1, 2, . . . ,m, let Fai be an independent event that occurs with probability\nP[Fai] = (\n1\u2212 qa m\n)i\u22121 .\nFor each a \u2208 A and i = 1, 2, . . . ,m, denote by Eai the event that occurs if i = ia and both Ea and Fai occur. By design, all these events are independent and thus\nP[Eai] = P[Ea]P[Fai]P[ia = i] = qa m\n( 1\u2212 qa\nm\n)i\u22121 (3)\nfor each a \u2208 A and i = 1, 2, . . . ,m. Furthermore, for any a, a\u2032 \u2208 A with a 6= a\u2032 and any i, i\u2032 \u2208 {1, 2, . . . ,m}, the events Eai and Ea\u2032i\u2032 are independent.\nFor i = 1, 2, . . . ,m let Gi be the event that none of the events {Eai\u2032}a\u2208A,i\u2032\u2264i occur, i.e.,\nGi = \u22c2 i\u2032\u2264i \u22c2 a\u2208A Eai\u2032\nwhere A denotes the complement of A. For convenience, let G0 be the event that occurs with probability one.\nLet (a1, a2, . . . , a|A|) be any enumeration of A. For i = 1, 2, . . . ,m and j = 1, 2, . . . , |A|+1, define the event\nGi,j = Gi\u22121 \u2229 \u22c2\n0<j\u2032<j\nEaj\u2032 i.\nWe note that by definitionGi,1 = Gi\u22121 andGi,|A|+1 = Gi for i = 1, 2, . . . ,m.\nFor i = 1, 2, . . . ,m and j = 1, 2, . . . , |A|, we have E[f(C)|Gi,j ] =P [ Eaji | Gi,j ] E [ f(C) | Eaji \u2229Gij ] + P [ Eaj\u2032 i | Gij ] E[f(C) | Gi,j+1].\n(4)\nWe now bound the individual terms. The event Gi,j implies that the events {Eaji\u2032}i\u2032<i did not occur. Furthermore, Eaji is independent of the events {Eaj\u2032 i\u2032}i\u2032=1,2,...,m for j\u2032 6= j. Hence, we have\nP [ Eaji | Gi,j ] = P [ Eaji | G0 \u2229 \u22c2 i\u2032<i Eaji\u2032 ]\n= P [ Eaji ] P [ G0 \u2229 \u22c2 i\u2032<iEaji\u2032\n] = P [ Eaji ] 1\u2212 P [\u22c3 i\u2032<iEaji\u2032\n] = P [ Eaji ] 1\u2212 \u2211 i\u2032<i P [ Eaji\u2032 ] ,\n(5)\nwhere the last equality follows since the events {Eaji\u2032}i\u2032<i are disjoint. Using (3), we observe that \u2211 i\u2032<i P [ Eaji\u2032 ] is a sum of a finite geometric series and we have\u2211 i\u2032<i P [ Eaji\u2032 ] = \u2211 i\u2032<i qa m ( 1\u2212 qa m\n)i\u2032\u22121 = qa m 1\u2212 ( 1\u2212 qam )i\u22121 1\u2212 ( 1\u2212 qam\n) = 1\u2212 ( 1\u2212 qa\nm\n)i\u22121 .\nTogether with (3) and (5), this implies\nP [ Eaji | Gi,j ] = qa m\n( 1\u2212 qam )i\u22121( 1\u2212 qam\n)i\u22121 = qam. (6) The event Eaji implies that C contains aj . Hence, since f is monotonically decreasing, we have\nE [ f(C) | Eaji \u2229Gij ] \u2264 f({aj}).\nUsing (4) and (6), this implies\nE[f(C)|Gi,j ] \u2264 qaj m f({aj})+\n( 1\u2212\nqaj m\n) E[f(C) | Gi,j+1].\nApplying this result iteratively for j = 1, 2, . . . , |A| implies E[f(C)|Gi,1] = |A|\u2211 j=1 qaj m \u220f j\u2032<j ( 1\u2212 qaj\u2032 m ) f({aj}) +\n |A|\u220f j=1 ( 1\u2212 qaj m )E[f(C) | Gi,|A|+1]. Note that 0 \u2264 1 \u2212 qam \u2264 1 for all a \u2208 A and that f is non-negative. This implies that for i = 1, 2, . . . ,m\nE[f(C)|Gi,1] \u2264 \u2211 a\u2208A qa m f({a}) + c \u00b7 E [ f(C)|Gi,|A|+1 ] where\nc = \u220f a\u2208A ( 1\u2212 qa m ) .\nSince Gi,1 = Gi\u22121 and Gi,|A|+1 = Gi, we have for i = 1, 2, . . . ,m\nE[f(C)|Gi\u22121] \u2264 \u2211 a\u2208A qa m f({a}) + c \u00b7 E[f(C)|Gi].\nApplying this result iteratively, we obtain\nE[f(C)] \u2264 ( m\u2211 i=1 ci\u22121 )\u2211 a\u2208A qa m f({a}) + cm \u00b7 f(\u2205).\nSince 0 \u2264 c \u2264 1, we have m\u2211 i=1 ci\u22121 \u2264 \u221e\u2211 i=1 ci\u22121 = 1 1\u2212 c .\nFor x \u2208 [\u22121, 0] it holds that log(1 + x) \u2264 x and hence\ncm = \u220f a\u2208A ( 1\u2212 qa m )m = exp ( m \u2211 a\u2208A log ( 1\u2212 qa m ))\n\u2264 exp ( \u2212m\n\u2211 a\u2208A qa m\n) = e\u2212 \u2211 a\u2208A qa .\nThis implies that\nE[f(C)] \u2264 1 1\u2212 c \u2211 a\u2208A qa m f({a})+e\u2212 \u2211 a\u2208A qa \u00b7f(\u2205). (7)\nWe show the main claim by contradiction. Assume that\nEP [f(C)] > EQ[f({a})] + e\u2212 \u2211 a\u2208A qaf(\u2205).\nIf EQ[f({a})] = 0, the contradiction follows directly from (7). Otherwise, EQ[f({a})] > 0 implies that there exists an > 0 such that\nEP [f(C)] > (1 + )EQ[f({a})] + e\u2212 \u2211 a\u2208A qaf(\u2205). (8)\nBy definition, we have\nc = \u220f a\u2208A ( 1\u2212 qa m ) = 1\u2212 \u2211 a\u2208A qa m + o ( 1 m ) .\nThus, there exists a m \u2208 N sufficiently large such that\nc = 1\u2212 \u2211 a\u2208A qa m + o ( 1 m ) \u2264 1\u2212 1 1 + \u2211 a\u2208A qa m .\nTogether with (7), this implies\nE[f(C)] \u2264 1 + \u2211 a\u2208A qa m \u2211 a\u2208A qa m f({a}) + e\u2212 \u2211 a\u2208A qa \u00b7 f(\u2205)\n= (1 + )EQ[f({a})] + e\u2212 \u2211 a\u2208A qaf(\u2205).\nwhich is a contradiction to (8) and thus proves the claim.\nLemma 2 extends Lemma 1 to k-means\u2016-style sampling probabilities of the form qa = min (1, `pa).\nLemma 2. Let ` \u2265 1. LetA be a finite set and let f : 2A \u2192 R be a set function that is non-negative and monotonically decreasing, i.e., f(V ) \u2265 f(U) \u2265 0, for all V \u2286 U . For each a \u2208 A, let pa \u2265 0 and \u2211 a\u2208A pa \u2264 1.\nLet P be the probability distribution where, for each a \u2208 A, Ea denotes an independent event that occurs with probability qa = min (1, `pa). Let C be the set of elements a \u2208 A for which the event Ea occurs.\nLet Q be the probability distribution on A where a single a \u2208 A is sampled with probability pa/ \u2211 a\u2208A pa.\nThen, with \u2205 denoting the empty set, we have that\nEP [f(C)] \u2264 2EQ[f({a})] + e\u2212` \u2211 a\u2208A paf(\u2205).\nProof. LetA1 be the set of elements a \u2208 A such that `pa \u2264 1 and A2 the set of elements a \u2208 A such that `pa > 1. By definition, every element in A2 is sampled almost surely, i.e., A2 \u2286 C. This implies that almost surely\nf(C) = f (A2 \u222a (C \u2229A1)) . (9)\nIf |A1|= 0, the result follows trivially since\nEP [f(C)] = f(A2) = EQ[f({a})].\nSimilarly, if |A2|= 0, the result follows directly from Lemma 1 with qa = `pa. For the remainder of the proof, we may thus assume that both A1 and A2 are non-empty.\nFor a \u2208 A1, let qa = `pa and define the non-negative and monotonically decreasing function\ng(C) = f (A2 \u222a C) .\nLet p1 = \u2211 a\u2208A1 pa and p2 = \u2211 a\u2208A2 pa. Lemma 1 applied to A1, qa and g implies that\nEP [f(C)] = E[g(C)] \u2264 \u2211 a\u2208A1 pa p1 g({a}) + e\u2212`p1g(\u2205).\n(10)\nLet d = ( 1\u2212 e\u2212`p2 ) e\u2212`p1\nand define \u03b1 =\np2 p1 + p2 \u2212 p1 p1 + p2 d.\nBy design, \u03b1 \u2264 1. Furthermore\n`p1 \u2265 log `p1.\nSinceA2 is nonempty and pa \u2265 1` for all a \u2208 A2, it follows that p2 \u2265 1` . This implies\ne`p1 \u2265 `p1 \u2265 p1 p2 .\nSince 0 \u2264 ( 1\u2212 e\u2212`p2 ) \u2264 1, we have\np2 \u2265 p1e\u2212`p1 \u2265 p1 ( 1\u2212 e\u2212`p2 ) e\u2212`p1 = p1d.\nHence,\n\u03b1 = p2 p1 + p2 \u2212 p1 p1 + p2\n( 1\u2212 e\u2212`p2 ) e\u2212`p1 \u2265 0.\nSince \u03b1 \u2208 [0, 1] and g({a}) \u2264 g(\u2205) for any a \u2208 A1, we may write (10), i.e., EP [f(C)] \u2264 (1\u2212 \u03b1) \u2211 a\u2208A1 pa p1 g({a}) + ( \u03b1+ e\u2212`p1 ) g(\u2205).\n(11)\nBy definition, we have\n1\u2212 \u03b1 = 1\u2212 p2 p1 + p2 + p1 p1 + p2 d = p1 p1 + p2 (1 + d).\nSince g({a}) \u2264 f({a}), we thus have\n(1\u2212 \u03b1) \u2211 a\u2208A1 pa p1 g({a}) \u2264 (1 + d) \u2211 a\u2208A1 pa p1 + p2 f({a}).\n(12)\nSimilarly, we have\n\u03b1+ e\u2212`p1 = p2 p1 + p2 \u2212 p1 p1 + p2 d+ e\u2212`p1\n= p2\np1 + p2 + d p2 p1 + p2 \u2212 d+ e\u2212`p1\n= (1 + d) p2\np1 + p2 + e\u2212`(p1+p2).\nSince g(\u2205) \u2264 f(\u2205), it follows that( \u03b1+ e\u2212`p1 ) g(\u2205) \u2264 (1+d) p2\np1 + p2 g(\u2205)+e\u2212`(p1+p2)f(\u2205).\n(13) Since g(\u2205) = f (A2) and thus g(\u2205) \u2264 f({a}) for all a \u2208 A2, we have\np2g(\u2205) = \u2211 a\u2208A2 pag(\u2205) \u2264 \u2211 a\u2208A2 paf({a}). (14)\nCombining (11), (12), (13), and (14) leads to\nEP [f(C)] \u2264 (1 + d)EQ[f({a})] + e\u2212` \u2211 a\u2208A paf(\u2205).\nSince p1 \u2265 0, we have 1 + d = 1 + ( 1\u2212 e\u2212`p2 ) e\u2212`p1 \u2264 2\nwhich proves the main claim.\nLemma 3 bounds the solution quality after each iteration of Algorithm 2 based on the solution before the iteration.\nLemma 3. Let k \u2208 N and ` \u2265 1. Let X be a data set in Rd and denote by \u03c6OPT(X ) the optimal k-Means clustering cost. LetC denote the set of cluster centers at the beginning of an iteration in Algorithm 2 and C \u2032 the random set added in the iteration. Then, it holds that\nE[\u03c6X (C \u222a C \u2032)] \u2264 ( k\ne`\n) \u03c6X (C) + 16\u03c6OPT(X ).\nProof. The proof relies on applying Lemma 2 to each cluster of the optimal solution. Let OPT denote any clustering achieving the minimal cost \u03c6OPT(X ) on X . We assign all the points x \u2208 X to their closest cluster center in OPT with ties broken arbitrarily but consistently. For c \u2208 OPT we denote by Xc the subset of X assigned to c. For each c \u2208 OPT, let\nC \u2032c = C \u2032 \u2229 Xc.\nBy definition, a \u2208 Xc is included in C \u2032c with probability\nqa = min\n( 1,\n`d(a,C)2\u2211 a\u2032\u2208X d(a \u2032, C)2\n) .\nFor each c \u2208 OPT, we define the monotonically decreasing function fc : 2Xc \u2192 R\u22650 to be\nfc(C \u2032 c) = \u03c6Xc(C \u222a C \u2032c).\nFor each c \u2208 OPT, Lemma 2 applied to Xc, C \u2032c and fc implies\nE[fc(C \u2032c)] \u22642 \u2211 a\u2208Xc d(a,C)2\u2211 a\u2032\u2208Xc d(a \u2032, C)2 fc({a})\n+ e \u2212`\n\u2211 a\u2208Xc d(a,C)\n2\u2211 a\u2032\u2208X d(a\n\u2032,C)2 fc(\u2205).\n(15)\nSince fc({a}) = \u03c6Xc(C \u222a {a}), the first term is equivalent to sampling a single element from Xc using D2 sampling. Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we have for all c \u2208 OPT\u2211\na\u2208Xc\nd(a,C)2\u2211 a\u2032\u2208Xc d(a \u2032, C)2 fc({a}). \u2264 8\u03c6OPT(Xc). (16)\nFor each c \u2208 OPT, we further have\ne \u2212`\n\u2211 a\u2208Xc d(a,C)\n2\u2211 a\u2032\u2208X d(a\n\u2032,C)2 fc(\u2205) = e\u2212`ucuc\u03c6X (C).\nwhere\nuc =\n\u2211 a\u2208Xc d(a,C)\n2\u2211 a\u2032\u2208X d(a \u2032, C)2 = \u03c6Xc(C) \u03c6X (C) .\nWe have that\nlog `uc \u2264 `uc \u2212 1 \u21d0\u21d2 `uc \u2264 e`uc\ne\nwhich implies\ne\u2212`ucuc\u03c6X (C) \u2264 1\ne` \u03c6X (C). (17)\nCombining (15), (16) and (17), we obtain\nE[fc(C \u2032c)] \u226416\u03c6OPT(Xc) + 1\ne` \u03c6X (C). (18)\nSince E[\u03c6X (C \u222a C \u2032)] \u2264 \u2211 c\u2208OPT E[fc(C \u2032c)]\nand \u03c6X (OPT) = \u2211 c\u2208OPT \u03c6Xc(OPT),\nwe thus have E[\u03c6X (C \u222a C \u2032)] \u2264 ( k\ne`\n) \u03c6X (C) + 16\u03c6OPT(X )\nwhich concludes the proof.\nAn iterated application of Lemma 3 allows us to bound the solution quality of Algorithm 2 in Lemma 4.\nLemma 4. Let k \u2208 N, t \u2208 N and ` \u2265 k. Let X be a data set in Rd and C be the random set returned by Algorithm 2. Then,\nE[\u03c6X (C)] \u2264 2 ( k\ne`\n)t Var(X ) + 26\u03c6OPT(X ).\nProof. The algorithm starts with a uniformly sampled initial cluster center c1. We iteratively apply Lemma 3 for each of the t rounds to obtain\nE[\u03c6X (C)] \u2264 ( k\ne`\n)t E[\u03c6X ({c1})] + 16st\u03c6OPT(X ) (19)\nwhere\nst = t\u2211 i=1 ( k e` )i\u22121 .\nFor ` \u2265 k, we have 0 \u2264 ke` \u2264 1/e and hence\nst \u2264 t\u2211 i=1 1 ei\u22121 \u2264 \u221e\u2211 i=0 1 ei =\n1\n1\u2212 1/e . (20)\nBy Lemma 3.2 of Arthur & Vassilvitskii (2007), we have that E[\u03c6X ({c1})] \u2264 2 Var(X ). Together with (19), (20) and 16/(1\u2212 1/e) \u2248 25.31 < 26, this implies the required result.\nWith Lemma 4, we are further able to bound the solution quality of Algorithm 3 and prove Theorem 1.\nProof of Theorem 1. Let B be the set returned by Algorithm 2. For any x \u2208 X , let bx denote its closest point in B with ties broken arbitrarily. By the triangle inequality and since (|a|+|b|)2 \u2264 2a2 + 2b2, for any x \u2208 X\nd(x,C)2 \u2264 2 d(x, bx)2 + 2 d(bx, C)2\nand hence\nE[\u03c6X (C)] = \u2211 x\u2208X d(x,C)2\n\u2264 2 \u2211 x\u2208X d(x, bx) 2 + 2 \u2211 x\u2208X d(bx, C) 2\n= 2\u03c6X (B) + 2 \u2211 x\u2208B wx d(x,C) 2.\n(21)\nLet OPTX be the optimal k-Means clustering solution on X and OPT(B,w) the optimal solution on the weighted set (B,w). By Theorem 1.1 of Arthur & Vassilvitskii (2007),\nk-means++ produces an \u03b1 = 8(log2 k + 2) approximation to the optimal solution. This implies that\u2211\nx\u2208B wx d(x,C) 2 \u2264 \u03b1 \u2211 x\u2208B wx d ( x,OPT(B,w) ) 2\n\u2264 \u03b1 \u2211 x\u2208B wx d(x,OPTX ) 2\n= \u03b1 \u2211 x\u2208X d(bx,OPTX ) 2.\n(22)\nBy the triangle inequality and since (|a|+|b|)2 \u2264 2a2+2b2, it holds for any x \u2208 X that\nd(bx,OPTX ) 2 \u2264 2 d(x, bx)2 + 2 d(x,OPTX )2\nand hence\u2211 x\u2208X d(bx,OPTX ) 2 \u2264 2\u03c6X (B) + 2\u03c6OPT(X ). (23)\nCombining (21), (22) and (23), we obtain\nE[\u03c6X (C)] \u2264 2(1 + \u03b1)\u03c6X (B) + 2\u03b1\u03c6OPT(X ).\nFinally, by Lemma 4, we have E[\u03c6X (C)] \u2264(32 log2 k + 68) ( k\ne`\n)t Var(X )\n+ (432 log2 k + 916)\u03c6OPT(X ).\nProof of Theorem 2. For this proof, we explicitly construct a data set: Let \u03b2\u2032 > 0 and consider points in onedimensional Euclidean space. For i = 1, 2, . . . , t, set\nxi =\n\u221a \u03b2\u2032(4`t) 1\u2212i \u2212 \u03b2\u2032(4`t)\u2212i\nas well as xt+1 = \u221a \u03b2\u2032(4`t) \u2212t .\nLet the data setX consist of the t+1 points {xi}t=1,2,...,t+1 as well as t + 1 points at the origin. Since t < k \u2212 1, the optimal k-Means clustering solution consists of t + 2 points placed at each of the {xi}i=1,2,...t+1 and at 0. By design, this solution has a quantization error of zero and the variance is nonzero, i.e., \u03c6OPT(X ) = 0 and Var(X ) > 0 as claimed.\nChoose \u03b2\u2032 = \u03b22(t+1) . The maximal distance \u2206 between any two points in X is bounded by \u2206 = d(0, x1)2 \u2264 \u03b2\u2032. Since n = 2(t+ 1), this implies \u03c8 \u2264 n\u22062 \u2264 \u03b2 as claimed.\nFor i = 1, 2, . . . , t, let Ci consist of 0 and all xj with j < i. By design, we have d(0, Ci)2 = 0 as well as d(xj , Ci)2 = 0 for j < i. For j \u2265 i, we have d(xj , Ci)2 = d(xj , 0)2. For any i = 1, 2, . . . , t+ 1, we thus have\u2211\nj\u2265i\nd(xj , 0) 2 = \u03b2\u2032(4`t) 1\u2212i .\nConsider a single iteration of Algorithm 2 where C = Ci. In this case, all points in Xj with j < i are added to C \u2032 with probability zero and for j > i each point xj is added to C \u2032 with probability\nmin ( 1, `d(xj , 0) 2\u2211\nj\u2032\u2265i d(xj\u2032 , 0) 2\n) = `d(xj , 0) 2\n\u03b2\u2032(4`t) 1\u2212i .\nBy the union bound, the probability that any of the points in \u22c3 j>i{xj} are sampled is bounded by\u2211\nj>i\n`d(xj , 0) 2 \u03b2\u2032(4`t) 1\u2212i = 1 4t .\nThe point xi is not sampled with probability at most\n1\u2212min ( 1, ` d(xi, 0) 2\u2211\nj\u2032\u2265i d(xj\u2032 , 0) 2\n) = 1\u2212min ( 1, `\u2212 1\n4t ) \u2264 1\n4t .\nBy the union bound, a single iteration of Algorithm 2 with C = Ci hence samples exactly the set C \u2032 = {xi} with probability at least ( 1\u2212 12t ) .\nIn Algorithm 2, the first center is sampled uniformly at random from X . Since half of the elements in X are placed at 0, with probability at least 12 , the first center is at 0 or equivalently C = C1. With probability ( 1\u2212 12t\n)t \u2265 12 , we then sample exactly the points x1, x2, . . . , xt in the t subsequent iterations. Hence, with probability at least 14 , the solution produced by Algorithm 2 consists of 0 and all xi except xt+1. Since xt+1 is closest to 0, this implies\nE[\u03c6X (C)] \u2265 1\n4 d(xt+1, 0)\n2 = 1\n4 \u03b2\u2032(4`t) \u2212t . (24)\nThe variance of X is bounded by a single point at 0, i.e., Var(X ) \u2264 \u03c6X ({0}) = \u2211 j\u22651 d(xj , 0) 2 = \u03b2\u2032.\nTogether with (24), we have that\nE[\u03c6X (C)] \u2265 1\n4 (4`t)\n\u2212t Var(X ).\nThe same result extends to the output of Algorithm 3 as it always picks a subset of the output of Algorithm 2."}, {"heading": "Acknowledgements", "text": "This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing."}], "year": 2017, "references": [{"title": "StreamKM++: A clustering algorithm for data streams", "authors": ["Ackermann", "Marcel R", "M\u00e4rtens", "Marcus", "Raupach", "Christoph", "Swierkot", "Kamil", "Lammersen", "Christiane", "Sohler", "Christian"], "venue": "Journal of Experimental Algorithmics (JEA),", "year": 2012}, {"title": "Adaptive sampling for k-means clustering. In Approximation, Randomization, and Combinatorial Optimization", "authors": ["Aggarwal", "Ankit", "Deshpande", "Amit", "Kannan", "Ravi"], "venue": "Algorithms and Techniques,", "year": 2009}, {"title": "Streaming k-means approximation", "authors": ["Ailon", "Nir", "Jaiswal", "Ragesh", "Monteleoni", "Claire"], "venue": "In Advances in Neural Information Processing Systems, pp", "year": 2009}, {"title": "k-means++: The advantages of careful seeding", "authors": ["Arthur", "David", "Vassilvitskii", "Sergei"], "venue": "In Symposium on Discrete Algorithms (SODA),", "year": 2007}, {"title": "Coresets for nonparametric estimation - the case of DPmeans", "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Krause", "Andreas"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Fast and provably good seedings for k-means", "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "Hamed", "Krause", "Andreas"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Approximate k-means++ in sublinear time", "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "S. Hamed", "Krause", "Andreas"], "venue": "In Conference on Artificial Intelligence (AAAI),", "year": 2016}, {"title": "Uniform deviation bounds for kmeans clustering", "authors": ["Bachem", "Olivier", "Lucic", "Mario", "Hassani", "S. Hamed", "Krause", "Andreas"], "venue": "In To appear in International Conference on Machine Learning (ICML),", "year": 2017}, {"title": "Scalable KMeans++", "authors": ["Bahmani", "Bahman", "Moseley", "Benjamin", "Vattani", "Andrea", "Kumar", "Ravi", "Vassilvitskii", "Sergei"], "venue": "Very Large Data Bases (VLDB),", "year": 2012}, {"title": "A bad instance for kmeans++", "authors": ["Brunsch", "Tobias", "R\u00f6glin", "Heiko"], "venue": "In International Conference on Theory and Applications of Models of Computation,", "year": 2011}, {"title": "A comparative study of efficient initialization methods for the k-means clustering algorithm", "authors": ["Celebi", "M Emre", "Kingravi", "Hassan A", "Vela", "Patricio A"], "venue": "Expert Systems with Applications,", "year": 2013}, {"title": "Bico: Birch meets coresets for k-means clustering", "authors": ["Fichtenberger", "Hendrik", "Gill\u00e9", "Marc", "Schmidt", "Melanie", "Schwiegelshohn", "Chris", "Sohler", "Christian"], "venue": "In European Symposium on Algorithms,", "year": 2013}, {"title": "A simple D2-sampling based PTAS for k-means and other clustering problems", "authors": ["Jaiswal", "Ragesh", "Kumar", "Amit", "Sen", "Sandeep"], "year": 2014}, {"title": "Improved analysis of D2-sampling based PTAS for k-means and other clustering problems", "authors": ["Jaiswal", "Ragesh", "Kumar", "Mehul", "Yadav", "Pulkit"], "venue": "Information Processing Letters,", "year": 2015}, {"title": "Least squares quantization in PCM", "authors": ["Lloyd", "Stuart"], "venue": "IEEE Transactions on Information Theory,", "year": 1982}, {"title": "Strong coresets for hard and soft Bregman clustering with applications to exponential family mixtures", "authors": ["Lucic", "Mario", "Bachem", "Olivier", "Krause", "Andreas"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2016}, {"title": "Training mixture models at scale via coresets", "authors": ["Lucic", "Mario", "Faulkner", "Matthew", "Krause", "Andreas", "Feldman", "Dan"], "venue": "To appear in Journal of Machine Learning Research (JMLR),", "year": 2017}, {"title": "Mllib: Machine learning in Apache Spark", "authors": ["Meng", "Xiangrui", "Bradley", "Joseph", "B Yuvaz", "Sparks", "Evan", "Venkataraman", "Shivaram", "Liu", "Davies", "Freeman", "Jeremy", "D Tsai", "Amde", "Manish", "Owen", "Sean"], "venue": "Journal of Machine Learning Research (JMLR),", "year": 2016}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "authors": ["Ostrovsky", "Rafail", "Rabani", "Yuval", "Schulman", "Leonard J", "Swamy", "Chaitanya"], "venue": "In Symposium on Foundations of Computer Science (FOCS),", "year": 2006}], "id": "SP:6821cf9d1b2e4e802787b5b509c768a3c0b983b4", "authors": [{"name": "Olivier Bachem", "affiliations": []}, {"name": "Mario Lucic", "affiliations": []}, {"name": "Andreas Krause", "affiliations": []}], "abstractText": "The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation. However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets. The standard remedy is to use the k-means\u2016 algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting. In this paper, we provide a novel analysis of the k-means\u2016 algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice. In particular, we show that k-means\u2016 provides provably good clusterings even for a small, constant number of iterations. This theoretical finding explains the common observation that k-means\u2016 performs extremely well in practice even if the number of rounds is low. We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k\u22121 rounds are employed.", "title": "Distributed and Provably Good Seedings for k-Means in Constant Rounds"}