1	25	For example, to understand the relation between sentences “Mary walked to a restaurant” and “She ordered some foods”, we need commonsense knowledge such as “Mary is a girl”, “restaurant sells food”, etc.
2	39	The task of understanding natural language with commonsense knowledge is usually referred as commonsense machine comprehension, which has been a hot topic in recent years (Richardson et al., 2013; Weston et al., 2015; Zhang et al., 2016).
3	17	Recently, RocStories (Mostafazadeh et al., 2016a), a commonsense machine comprehension task, has attached many researchers’ attention due to its significant difference from previous machine comprehension tasks.
4	39	RocStories focuses on reasoning with implicit commonsense knowledge, rather than matching with explicit information in given contexts.
5	24	In this task, a system requires choosing a sentence, namely hypothesis, to complete a given commonsense story, called as premise document.
12	63	Besides, people can further confirm their judgement based on the sentimental coherence between “finish super early” and “job well done”.
13	56	Furthermore, in the second example, even both hypothesises are consistent with the premise document in both event and sentimental facets, we can still infer the right answer easily using the commonsense knowledge that “puppy” is a dog, meanwhile “kitten” is a cat.
15	16	However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016).
24	16	These heterogeneous knowledge are encoded into a uniform representation – inference rules between elements under different kinds of relations, with an inference cost for each rule.
32	54	Specifically, we mine three types of commonly used commonsense knowledge, including: 1)Event narrative knowledge, which captures temporal and causal relations between events; 2)Entity semantic knowledge, which captures semantic relations between entities; 3)Sentiment coherent knowledge, which captures sentimental coherence between elements.
50	21	Entities, often serving as event participants or environment variables, are important components of commonsense stories.
52	38	For example, if a premise document contains “Starbucks”, then “coffeehouse” and “latte” will be reasonable entities in hypothesis since “Starbucks” is a possible coreference of “coffeehouse” and it is semantically related to “latte”.
54	40	In stories, besides to pronouns, an entity is often referred using its hypernyms, e.g, the second example in Table 1 uses “dog” to refer to “puppy”.
59	25	Specifically, given two entities e1 and e2, we compute the semantic distance dist(e1, e2) between them as: dist(e1, e2) = log(max(|E1|, |E2|)− log(|E1⋂E2|)) log(|W |)− log(min(|E1|, |E2|)) (3) where E1 and E2 are the sets of all entities that link to these two entities in Wikipedia respectively, and W is the entire Wikipedia.
68	21	However, because we extract them from different sources and estimate their costs using different measurements, the cost metrics of these rules may not be consistent with each other.
80	22	This section describes how to leverage acquired knowledge for commonsense machine comprehension.
82	17	Then we model how to choose inference rules for a specific reasoning context.
84	56	Given a premise document D = {d1, d2, ..., dm} containing m elements, a hypothesis H = {h1, h2, ..., hn} containing n elements, a valid inference R from D to H is a set of inference rules that all elements in H can be inferred from one element inD using one and only one rule inR.
93	27	In Figure 1, inference (a) should have a higher probability than inference (b) because it is more reasonable to infer “foods” from “a restaurant” with associative relation, rather than from “walked to” with narrative relation.
98	47	Motivated by above observations, we endow each inference a probability P (R|D,H), indicating the possibility thatR is chosen to infer hypothesis H from premise document D. For simplicity, we assume that each element in hypothesis is independently inferred using individual inference rule, then P (R|D,H) can be written as: P (R|D,H) = n∏ i=1 P (ri|D,H) (5) = n∏ i=1 P (ri|D,hi) (6) = n∏ i=1 m∑ j=1 P (ri, dj |D,hi) (7) Equation (7) clearly shows how an inference rule is selected given the premise document D and the element hi in hypothesis.
100	16	We then refactor the probability P (ri, dj |D,hi) to be: P (ri, dj |D,hi) = { 0 , antecedent(ri) 6= dj g(hi, dj , f(ri);D) , otherwise (8) Here f(r) is the relation type of inference rule r, and g(h, d, f ;D) is defined as: g(h, d, f ;D) = s(h, d)a(h, f)a(d, f)∑ f∈F ∑ d∈D s(h, d)a(h, f)a(d, f) (9) Here F denotes all relation types of inference rules, s(e1, e2) is a matching function between two elements e1 and e2, measuring by cosine similarity based on GoogleNews word2vec (Mikolov et al., 2013).
103	27	Using attention mechanism, our method models the possibility that an inference rule is applied during the inference from a premise document to a hypothesis by considering the relatedness between elements and knowledge category, as well as the relatedness between two elements, which make it able to select the most reasonable inference rules to derive each part of the hypothesis.
118	22	We compared our approach with following three baselines: 1) Narrative Event Chain (Chambers and Jurafsky, 2008), which scores hypothesis using PMI scores between events.
136	26	We conducted experiments by removing one kind of knowledge from our final model at a time, and investigate the change of accuracy.
140	19	We compared our method with following two heuristic settings: 1) Minimum Cost Mechanism, which measures the reasoning distance by only selecting the inference rule with minimum cost for each hypothesis element.
147	30	We can see that removing negation rules will significantly drop the system performance, which confirm the effectiveness of our proposed negation rules.
166	19	This paper proposes a commonsense machine comprehension method, which performs effective commonsense reasoning by taking heterogenous knowledge into consideration.
167	48	Specifically, we mine commonsense knowledge from heterogeneous knowledge sources and simultaneously exploit them by proposing a highly extensible multiknowledge reasoning framework.
168	82	Experiment results shown that our method surpasses baselines by a large margin.
169	205	Currently, there are little labeled training instances for commonsense machine comprehension, for future work we want to address this issue by developing semi-supervised or unsupervised approaches.
