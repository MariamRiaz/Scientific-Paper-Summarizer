1	26	For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word representations learned from large corpora.
6	28	Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data.
7	17	Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.
8	58	In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.
9	38	Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.
16	31	Our multi-task model combines classification and ranking tasks.
22	176	Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution.
24	33	Note that one query can belong to multiple domains.
28	28	For each domain t, we assume supervised data (Q, yt = {0, 1} with yt as binary labels.1 Web Search: Given a search queryQ and a document list L, the model ranks documents in the order of relevance.
33	42	These vectors can then be used to perform query classification or web search.
36	80	The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs.
37	28	Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300.
42	33	We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k).
43	19	For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.
44	57	Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.
50	35	The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z) = 1 1+e−z : P (C1|Q) = g(Wt=C13 ·QC1) (3) Web Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd .
54	79	For query classification, each training sample includes one query and its category label.
59	43	This approximately optimizes the sum of all multi-task objectives.
60	20	For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))} (5) where yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).
62	31	Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J .
79	18	The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997).
83	26	• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM).
92	31	http://www.csie.ntu.edu.tw/ cjlin/liblinear/ • Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA • DSSM: single-task ranking model (Figure 2) • MT-DNN: our multi-task proposal (Figure 1) Again, we observe that MT-DNN performs best.
108	35	To evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗.
109	41	Train MTDNN on the remaining tasks (including Web Search task) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗ We compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure.
115	102	Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se- manticRepresentation is recommended.4 In a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗; (2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗.
116	21	The purpose is to see whether shared semantic representation is useful even under a DNN architecture.
119	29	Otherwise, one is better off using a shared semantic representation trained by multitask objectives.
140	84	Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.
141	28	A comprehensive exploration will be pursued as future work.
