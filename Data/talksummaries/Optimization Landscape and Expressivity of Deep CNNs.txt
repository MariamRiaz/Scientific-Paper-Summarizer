23	1	All proofs are moved to the appendix due to limited space.
24	1	We first introduce our notation and definition of CNNs.
38	1	Furthermore, bk ∈ Rnk denotes the bias vector and σk : R → R the activation function for each layer.
46	1	(2) For some results we also allow max-pooling layers.
57	1	, fk(xN )] T ∈ RN×nk , Gk = [gk(x1), .
63	1	In this section, we show that a class of standard CNN architectures with convolutional layers, fully connected layers and max-pooling layers plus standard activation functions like ReLU, sigmoid, softplus, etc are able to learn linearly independent features at every wide hidden layer if it has more neurons than the number of training samples.
64	1	Our assumption on training data is the following.
66	1	Assumption 3.1 is quite weak, especially if the size of the input patches is large.
104	1	While Theorem 3.5 does not hold for the ReLU activation function as it is not an analytic function, we note again that one can approximate the ReLU function arbitrarily well using the softplus function (see 5), which is analytic function for any α > 0 and thus Theorem 3.5 applies.
111	1	We use the MNIST dataset with N = 60000 training and 10000 test samples.
128	1	Then for every target y ∈ RN , there exists { λ, (Wl, bl) L−1 l=1 } so that it holds fL(xi) = yi for every i ∈ [N ].
129	1	The expressivity of neural networks has been well-studied, in particular in the universal approximation theorems for one hidden layer networks (Cybenko, 1989; Hornik et al., 1989).
134	1	For fully connected networks, universal finite sample ex- pressivity has been studied by (Zhang et al., 2017; Nguyen & Hein, 2017; Hardt & Ma, 2017).
135	1	It is shown that a single hidden layer fully connected network with N hidden units can express any training set of size N .
141	1	By using shared weights and sparsity structure, CNNs seem to implicitly regularize the model to achieve good generalization performance.
155	1	In the following, we examine conditions for global optimality in Sk.
170	1	Then it holds∥∥∇Uk+1Φ∥∥F ≥σmin(Fk) ( L−1∏ l=k+1 σmin(Ul+1) ‖σ′l(Gl)‖min ) ‖FL − Y ‖F and∥∥∇Uk+1Φ∥∥F ≤σmax(Fk) ( L−1∏ l=k+1 σmax(Ul+1) ‖σ′l(Gl)‖max ) ‖FL − Y ‖F .
181	1	Thus it holds σmin(Fk) > 0 and σmin(Ul) > 0 for every l ∈ [k + 2, L].
195	1	Obviously, some of these critical points can also be global minima, but we conjecture that they cannot be suboptimal local minima due to the following reasons.
199	1	Second, a similar argument applies to the case where one has a critical point outside Sk such that the features are not linearly independent.
200	1	In particular, any neighborhood of such a critical point contains points which have linearly independent features at layer k, from which it is easy to reach zero loss if one fixes the parameters of the first k layers and optimizes the loss w.r.t.
202	1	In summary, if there are critical points lying outside the set Sk, then it is very “unlikely” that these are suboptimal local minima but rather also global minima, saddle points or local maxima.
203	1	It remains an interesting open problem if the result of Theorem 4.5 can be transferred to the case where layer k + 1 is also convolutional.
204	1	In any case whether layer k + 1 is fully connected or not, one might assume that a solution with zero training error still exists as it is usually the case for practical over-parameterized networks.
205	1	However, note that Theorem 4.4 shows that at those points where the loss is zero, the gradient of Φ w.r.t.
206	3	An interesting special case of Theorem 4.5 is when the network is fully connected in which case all the results of Theorem 4.5 hold without any modifications.
207	14	This can be seen as a formal proof for the implicit assumption used in the recent work (Nguyen & Hein, 2017) that there exists a global minimum with absolute zero training error for the class of fully connected, deep and wide networks.
208	26	We have analyzed the expressiveness and loss surface of CNNs in realistic and practically relevant settings.
209	132	As stateof-the-art networks fulfill exactly or approximately the condition to have a sufficiently wide convolutional layer, we think that our results help to understand why current CNNs can be trained so effectively.
210	126	It would be interesting to discuss the loss surface for cross-entropy loss, which currently does not fit into our analysis as the global minimum does not exist when the data is linearly separable.
