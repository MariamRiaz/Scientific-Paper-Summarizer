22	14	In Section 4, the experiments are performed on TimeBank-Dense and we compare our model to the baseline and two state-of-the-art systems.
23	18	The final conclusion is made in Section 5.
38	28	We follow a similar experiment setting to the other two systems (Mirza and Tonelli, 2016; Chambers et al., 2014) with the same 9 documents as test data and the others as training data (15% of training data is split as validation data for early stopping).
39	12	Intuitively, the dependency path based idea can be introduced into the temporal relation classification task.
41	12	A crucial obstacle is how to represent the dependency path of a cross-sentence link.
42	6	In this work, we make a naive assumption that two neighboring sentences share a “common root”.
51	12	The forward and backward outputs of both source and target branches are all concatenated, and fed into a fully connected hidden units layer.
52	83	The final Softmax layer generates multi-class predictions.
53	6	Since an E-D link contains single event SDP branch, our system applies a similar architecture, but with single branch Bi-LSTM with outputs fed into the penultimate hidden layer, as shown in Figure 3b.
54	71	In this work, we use word2vec5 (Mikolov et al.,  2013a,b) to train 200-dimensions word embeddings on English Gigaword 4th edition with skipgram model and other default settings.
56	40	The grid search exploring a full hyper-parameter space takes time for three classifiers (E-E, E-T and E-D).
57	11	Empirically, we set each single LSTM output with the same dimensions (equal to 300) as the concatenation of word, POS, DEP embeddings.
59	52	Our system adopts dependency paths as input, which means that the entities in the same sentences contain highly covered word sequence input.
