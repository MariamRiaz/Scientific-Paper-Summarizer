0,1,label2,summary_sentences
"Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013).",1 Introduction,[0],[0]
"Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.",1 Introduction,[0],[0]
"For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.",1 Introduction,[0],[0]
"Without heuristics, these parsers take minutes to process a single sentence.
",1 Introduction,[0],[0]
"To speed up inference, we will learn a pruning policy.",1 Introduction,[0],[0]
"During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference process.
",1 Introduction,[0],[0]
"Our approach searches for a policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy−λ · runtime.",1 Introduction,[0],[0]
"The parameter λ ≥ 0
specifies the relative importance of runtime and accuracy.",1 Introduction,[0],[0]
"By adjusting λ, we obtain policies with different speed-accuracy tradeoffs.
",1 Introduction,[0],[0]
"For learning, we use Locally Optimal Learning to Search (LOLS) (Chang et al., 2015b), an algorithm for learning sequential decision-making policies, which accounts for the end-to-end performance of the entire decision sequence jointly.",1 Introduction,[0],[0]
"Unfortunately, executing LOLS naively in our setting is prohibitive because it would run inference from scratch millions of times under different policies, training examples, and variations of the decision sequence.",1 Introduction,[0],[0]
"Thus, this paper presents efficient algorithms for repeated inference, which are applicable to a wide variety of NLP tasks, including parsing, machine translation and sequence tagging.",1 Introduction,[0],[0]
"These algorithms, based on change propagation and dynamic programming, dramatically reduce time spent evaluating similar decision sequences by leveraging problem structure and sharing work among evaluations.
",1 Introduction,[0],[0]
We evaluate our approach by learning pruning heuristics for constituency parsing.,1 Introduction,[0],[0]
"In this setting, our approach is the first to account for end-to-end performance of the pruning policy, without making independence assumptions about the reward function, as in prior work (Bodenstab et al., 2011).",1 Introduction,[0],[0]
"In the larger context of learning-to-search for structured prediction, our work is unusual in that it learns to control a dynamic programming algorithm (i.e., graphbased parsing) rather than a greedy algorithm (e.g., transition-based parsing).",1 Introduction,[0],[0]
Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.,1 Introduction,[0],[0]
"A simple yet effective approach to speeding up parsing was proposed by Bodenstab et al. (2011), who trained a pruning policy π to classify whether or not spans of the input sentence w1 · · ·wn form plausible
263
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Weighted CKY with pruning,[0],[0]
"263–278, 2017.",2 Weighted CKY with pruning,[0],[0]
Action Editor: Marco Kuhlmann.,2 Weighted CKY with pruning,[0],[0]
"Submission batch: 5/2016; Revision batch: 9/2016; Published 8/2017.
",2 Weighted CKY with pruning,[0],[0]
c©2017 Association for Computational Linguistics.,2 Weighted CKY with pruning,[0],[0]
"Distributed under a CC-BY 4.0 license.
constituents based on features of the input sentence.",2 Weighted CKY with pruning,[0],[0]
"These predictions enable a parsing algorithm, such as CKY, to skip expensive steps during its execution: unlikely constituents are pruned.",2 Weighted CKY with pruning,[0],[0]
"Only plausible constituents are kept, and the parser assembles the highest-scoring parse from the available constituents.
",2 Weighted CKY with pruning,[0],[0]
Alg. 1 provides pseudocode for weighted CKY with pruning.,2 Weighted CKY with pruning,[0],[0]
"Weighted CKY aims to find the highestscoring derivation (parse tree) of a given sentence, where a given grammar specifies a non-negative score for each derivation rule and a derivation’s score is the product of the scores of the rules it uses.1 CKY uses a dynamic programming strategy to fill in a three-dimensional array β, known as the chart.",2 Weighted CKY with pruning,[0],[0]
The score βikx is the score of the highest-scoring subderivation with fringe wi+1 . . .,2 Weighted CKY with pruning,[0],[0]
wk and root,2 Weighted CKY with pruning,[0],[0]
x.,2 Weighted CKY with pruning,[0],[0]
This value is computed by looping over the possible ways to assemble such a subderivation from smaller subderivations with scores βijy and βjkz (lines 17–22).,2 Weighted CKY with pruning,[0],[0]
"Additionally, we track a witness (backpointer) for each βikx, so that we can easily reconstruct the corresponding subderivation at line 23.",2 Weighted CKY with pruning,[0],[0]
"The chart is initialized with lexical grammar rules (lines 3–9), which derive words from grammar symbols.
",2 Weighted CKY with pruning,[0],[0]
"The key difference between pruned and unpruned CKY is an additional “if” statement (line 14), which queries the pruning policy π to decide whether to compute the several values βikx associated with a span (i, k).",2 Weighted CKY with pruning,[0],[0]
Note that width-1 and width-n spans are always kept because all valid parses require them.,2 Weighted CKY with pruning,[0],[0]
Bodenstab et al. (2011) train their pruning policy as a supervised classifier of spans.,3 End-to-end training,[0],[0]
"They derive direct supervision as follows: try to keep a span if it appears in the gold-standard parse, and prune it otherwise.",3 End-to-end training,[0],[0]
They found that using an asymmetric weighting scheme helped find the right balance between false positives and false negatives.,3 End-to-end training,[0],[0]
"Intuitively, failing to prune is only a slight slowdown, whereas pruning a good item can ruin the accuracy of the parse.
",3 End-to-end training,[0],[0]
"1As is common practice, we assume the grammar has been binarized.",3 End-to-end training,[0],[0]
"We focus on pre-trained grammars, leaving coadaptation of the grammar and pruning policy to future work.",3 End-to-end training,[0],[0]
"As indicated at lines 6 and 19, a rule’s score may be made to depend on the context in which that rule is applied (Finkel et al., 2008), although the pre-trained grammars in our present experiments are ordinary PCFGs for which this is not the case.
",3 End-to-end training,[0],[0]
"Algorithm 1 PARSE: Weighted CKY with pruning 1: Input: grammar G, sentence w, policy π
Output: completed chart β, derivation d 2: .",3 End-to-end training,[0],[0]
"Initialize chart 3: β := 0 4: for k := 1 to n : 5: for x such that (x→ wk) ∈ rules(G) : 6: s := G(x→ wk | w, k) 7: if s > βk−1,k,x : 8: βk−1,k,x := s 9: witness(k−1, k, x) := (k−1, k, wk)
10: for width := 2 to n : 11: for i := 0 to n− width : 12: k := i+ width .",3 End-to-end training,[0],[0]
"Current span is (i, k) 13: .",3 End-to-end training,[0],[0]
"Policy determines whether to fill in this span 14: if π(w, i, k) = prune : 15: continue 16: .",3 End-to-end training,[0],[0]
Fill in span by considering each split point j 17: for j := i+ 1 to k,3 End-to-end training,[0],[0]
"− 1 : 18: for (x→ y z) ∈ rules(G) : 19: s := βijy ·βjkz ·G(x→ y z | w, i, j, k) 20: if s > βikx : 21: βikx := s 22: witness(i, k, x) := (j, y, z) 23: d̂ := follow backpointers from (0, n,ROOT) 24: return (β, d̂)
",3 End-to-end training,[0],[0]
"Our end-to-end training approach improves upon asymmetric weighting by jointly evaluating the sequence of pruning decisions, measuring its effect on the test-time evaluation metric by actually running pruned CKY (Alg. 1).",3 End-to-end training,[0],[0]
"To estimate the value of a pruning policy π, we call PARSE(G,w(i), π) on each training sentence w(i), and apply the reward function, r = accuracy−λ · runtime.",3 End-to-end training,[0],[0]
"The empirical value of a policy is its average reward on the training set:
R(π) = 1 m
m∑
i=1
E",3 End-to-end training,[0],[0]
"[ r(PARSE(G,w(i), π)) ]",3 End-to-end training,[0],[0]
"(1)
The expectation in the definition may be dropped if PARSE, π, and r are all deterministic, as in our setting.2 Our definition of r depends on the user parameter λ ≥ 0, which specifies the amount of accuracy the user would sacrifice to save one unit of
2Parsers may break ties randomly or use Monte Carlo methods.",3 End-to-end training,[0],[0]
"The reward function r can be nondeterministic when it involves wallclock time or human judgments.
runtime.",3 End-to-end training,[0],[0]
"Training under a range of values for λ gives rise to policies covering a number of operating points along the Pareto frontier of accuracy and runtime.
",3 End-to-end training,[0],[0]
End-to-end training gives us a principled way to decide what to prune.,3 End-to-end training,[0],[0]
"Rather than artificially labeling each pruning decision as inherently good or bad, we evaluate its effect in the context of the particular sentence and the other pruning decisions.",3 End-to-end training,[0],[0]
"Actions that prune a gold constituent are not equally bad—some cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse.",3 End-to-end training,[0],[0]
"Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse.
",3 End-to-end training,[0],[0]
"More generally, the gold vs. non-gold distinction is not even available in NLP tasks where one is pruning potential elements of a latent structure, such as an alignment (Xu et al., 2013) or a finer-grained parse (Matsuzaki et al., 2005).",3 End-to-end training,[0],[0]
"Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves.
",3 End-to-end training,[0],[0]
Past work on optimizing end-to-end performance is discussed in §8.,3 End-to-end training,[0],[0]
"One might try to scale these techniques to learning to prune, but in this work we take a different approach.",3 End-to-end training,[0],[0]
"Given a policy, we can easily find small ways to improve it on specific sentences by varying individual pruning actions (e.g., if π currently prunes a span then try keeping it instead).",3 End-to-end training,[0],[0]
"Given a batch of improved action sequences (trajectories), the remaining step is to search for a policy which produces the improved trajectories.",3 End-to-end training,[0],[0]
"Conveniently, this can be reduced to a classification problem, much like the asymmetric weighting approach, except that the supervised labels and misclassification costs are not fixed across iterations, but rather are derived from interaction with the environment (i.e., PARSE and the reward function).",3 End-to-end training,[0],[0]
"This idea is formalized as a learning algorithm called Locally Optimal Learning to Search (Chang et al., 2015b), described in §4.
",3 End-to-end training,[0],[0]
The counterfactual interventions we require— evaluating how reward would change if we changed one action—can be computed more efficiently using our novel algorithms (§5) than by the default strategy of running the parser repeatedly from scratch.,3 End-to-end training,[0],[0]
"The key is to reuse work among evaluations, which is
possible because LOLS only makes tiny changes.",3 End-to-end training,[0],[0]
Pruned inference is a sequential decision process.,4 Learning algorithm,[0],[0]
The process begins in an initial state s0.,4 Learning algorithm,[0],[0]
"In pruned CKY, s0 specifies the state of Alg.",4 Learning algorithm,[0],[0]
"1 at line 10, after the chart has been initialized from some selected sentence.",4 Learning algorithm,[0],[0]
"Next, the policy is invoked to choose action a0 = π(s0)—in",4 Learning algorithm,[0],[0]
our case at line 14—which affects what the parser does next.,4 Learning algorithm,[0],[0]
"Eventually the parser reaches some state s1 from which it calls the policy to choose action a1 = π(s1), and so on.",4 Learning algorithm,[0],[0]
"When the policy is invoked at state st, it selects action at based on features extracted from the current state st—a snapshot of the input sentence, grammar and parse chart at time t.3",4 Learning algorithm,[0],[0]
"We call the state-action sequence s0 a0 s1 a1 · · · sT a trajectory, where T is the trajectory length.",4 Learning algorithm,[0],[0]
"At the final state, the reward function is evaluated, r(sT ).
",4 Learning algorithm,[0],[0]
The LOLS algorithm for learning a policy is given in Alg.,4 Learning algorithm,[0],[0]
"2,4 with a graphical illustration in Fig. 1.",4 Learning algorithm,[0],[0]
"At a high level, LOLS alternates between evaluating and improving the current policy πi.
",4 Learning algorithm,[0],[0]
"The evaluation phase first samples a trajectory from πi, called a roll-in: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi).",4 Learning algorithm,[0],[0]
"In our setting, s0 is derived from a randomly sampled training sentence, but the rest of the trajectory is then deterministically computed by πi given s0.",4 Learning algorithm,[0],[0]
"Then we revisit each state s in the roll-in (line 7), and try each available action ā∈A(s)",4 Learning algorithm,[0],[0]
"(line 9), executing πi thereafter—a rollout—to measure the resulting reward r̂[ā] (line 10).",4 Learning algorithm,[0],[0]
"Our parser is deterministic, so a single rollout is an unbiased, 0-variance estimate of the expected reward.",4 Learning algorithm,[0],[0]
"This process is repeated many times, yielding a large list Q̂i of pairs 〈s, r̂〉, where s is a state that was encountered in some roll-in and r̂ maps the possible actions A(s) in that state to their measured rewards.
",4 Learning algorithm,[0],[0]
"The improvement phase now trains a new policy πi+1 to try to choose high-reward actions, seeking a policy that will “on average” get high rewards r[πi+1(s)].",4 Learning algorithm,[0],[0]
"Good generalization is important: the policy must select high-reward actions even in states s that are not represented in Q̂i, in case they are
3Our experiments do not make use of the current state of the chart.",4 Learning algorithm,[0],[0]
"We discuss this decision in §8.
4Alg.",4 Learning algorithm,[0],[0]
"2 is simpler than in Chang et al. (2015b) because it omits oracle rollouts, which we do not use in our experiments.
",4 Learning algorithm,[0],[0]
Algorithm 2 LOLS algorithm for learning to prune.,4 Learning algorithm,[0],[0]
1: π1 := INITIALIZEPOLICY(. . . ),4 Learning algorithm,[0],[0]
2: for i := 1 to number of iterations : 3: .,4 Learning algorithm,[0],[0]
Evaluate: Collect dataset for πi 4: Q̂i := ∅ 5: for j := 1 to minibatch size : 6: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi) .,4 Learning algorithm,[0],[0]
Sample 7: for t := 0 to T−1 : 8: .,4 Learning algorithm,[0],[0]
Intervene: Evaluate each action at st 9: for āt ∈ A(st) : .,4 Learning algorithm,[0],[0]
"Possible actions
10: r̂t[āt] ∼ ROLLOUT(πi, st, āt) 11: Q̂i.append(〈st, r̂t 〉) 12: .",4 Learning algorithm,[0],[0]
"Improve: Train with dataset aggregation
13: πi+1 ← TRAIN",4 Learning algorithm,[0],[0]
(,4 Learning algorithm,[0],[0]
"⋃i k=1 Q̂k )
14: .",4 Learning algorithm,[0],[0]
Finalize: Pick the best policy over all iterations 15: return argmaxi′ R(πi′) encountered when running the new policy πi+1 (or when parsing test sentences).,4 Learning algorithm,[0],[0]
"Thus, beyond just regularizing the training objective, we apply dataset aggregation (Ross et al., 2011): we take the training set to include not just Q̂i but also the examples from previous iterations (line 13).",4 Learning algorithm,[0],[0]
"This also ensures that the sequence of policies π1, π2, . .",4 Learning algorithm,[0],[0]
".will be “stable” (Ross and Bagnell, 2011) and will eventually converge.
",4 Learning algorithm,[0],[0]
"So line 13 seeks to find a good classifier πi+1 using a training set: a possible classifier π would receive from each training example 〈s, r̂〉 a reward of r̂[π(s)].",4 Learning algorithm,[0],[0]
"In our case, where A(s) = {keep, prune}, this cost-sensitive classification problem is equivalent to training an ordinary binary classifier, after converting each training example 〈s, r̂〉 to 〈s, argmaxa",4 Learning algorithm,[0],[0]
"r̂[a]〉 and giving this example a weight of |r̂t,keep− r̂t,prune|.",4 Learning algorithm,[0],[0]
"Our specific classifier is described in §6.
",4 Learning algorithm,[0],[0]
"In summary, the evaluation phase of LOLS collects training data for a cost-sensitive classifier, where the
inputs (states), outputs (actions), and costs are obtained by interacting with the environment.",4 Learning algorithm,[0.9515898937319115],"['The second step, similarly to Pentina & Lampert (2014), bounds the transfer-risk at the task-environment level (i.e, the error caused by observing only a finite number of tasks) by the average expected error in the observed tasks plus the environment-complexity term.']"
"LOLS concocts a training set and repeatedly revises it, similar to the well-known Expectation-Maximization algorithm.",4 Learning algorithm,[0],[0]
This enables end-to-end training of systems with discrete decisions and nondecomposable reward functions.,4 Learning algorithm,[0],[0]
LOLS gives us a principled framework for deriving (nonstationary) “supervision” even in tricky cases such as latent-variable inference (mentioned in §3).,4 Learning algorithm,[0],[0]
"LOLS has strong theoretical guarantees, though in pathological cases, it may take exponential time to converge (Chang et al., 2015b).
",4 Learning algorithm,[0],[0]
"The inner loop of the evaluation phase performs roll-ins, interventions and rollouts.",4 Learning algorithm,[0],[0]
Roll-ins ensure that the policy is (eventually) trained under the distribution of states it tends to encounter at test time.,4 Learning algorithm,[0],[0]
Interventions and rollouts force πi to explore the effect of currently disfavored actions.,4 Learning algorithm,[0],[0]
"Unlike most applications of LOLS and related algorithms, such as SEARN (Daumé III, 2006) and DAGGER (Ross et al., 2011), executing the policy is a major bottleneck in training.",5 Efficient rollouts,[0],[0]
"Because our dynamic programming parser explores many possibilities (unlike a greedy, transition-based decoder) its trajectories are quite long.",5 Efficient rollouts,[0],[0]
"This not only slows down each rollout: it means we must do more rollouts.
",5 Efficient rollouts,[0],[0]
"In our case, the trajectory has length T = n·(n+1)
2",5 Efficient rollouts,[0],[0]
− 1− n,5 Efficient rollouts,[0],[0]
"for a sentence of length n, where T is also the number of pruning decisions: one for each span other than the root and width-1 spans.",5 Efficient rollouts,[0],[0]
LOLS must then perform T rollouts on this example.,5 Efficient rollouts,[0],[0]
"This means that to evaluate policy πi, we must parse each sentence in the minibatch hundreds of times (e.g., 189 for n=20, 434 for n=30, and 779 for n=40).
",5 Efficient rollouts,[0],[0]
"We can regard each policy π as defining a pruning
mask m, an array that maps each of the T spans (i, k) to a decision mik (1 = keep, 0 = prune).",5 Efficient rollouts,[0],[0]
"Each rollout tries flipping a different bit in this mask.
",5 Efficient rollouts,[0],[0]
We could spend less time on each sentence by sampling only some of its T rollouts (see §6).,5 Efficient rollouts,[0],[0]
"Regardless, the rollouts we do on a given sentence are related: in this section we show how to get further speedups by sharing work among them.",5 Efficient rollouts,[0],[0]
"In §5.2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision).",5 Efficient rollouts,[0],[0]
"In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later).",5 Efficient rollouts,[0],[0]
We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2).,5 Efficient rollouts,[0],[0]
"It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007).",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph describes the information flow among related quantities in a dynamic programming algorithm.,5.1 Background: Parsing as hypergraphs,[0],[0]
"Many computational tricks apply generically to hypergraphs.
",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs.,5.1 Background: Parsing as hypergraphs,[0],[0]
"We regard each quantity βikx,mik, or G(. . .)",5.1 Background: Parsing as hypergraphs,[0],[0]
"in Alg. 1 as the value of a corresponding hypergraph vertex β̇ikx, ṁik, or Ġ(. . .).",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus, value(v̇) = v for any vertex v̇. Each ṁik’s value is computed by the policy π or chosen by a rollout intervention.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Each Ġ’s value is given by the grammar.
",5.1 Background: Parsing as hypergraphs,[0],[0]
"Values of β̇ikx, by contrast, are computed at line 19 if k − i > 1.",5.1 Background: Parsing as hypergraphs,[0],[0]
"To record the dependence of βikx on other quantities, our hypergraph includes the hyperedge β̇ikx ≺",5.1 Background: Parsing as hypergraphs,[0],[0]
"(β̇ijy, β̇jkz, ṁik, ġ) for each 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
i < j < k ≤ n,5.1 Background: Parsing as hypergraphs,[0],[0]
"and (x→ y z) ∈ rules(G), where ġ denotes the vertex Ġ(x→ y z | w, i, j, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"If k − i = 1, then values of βikx are instead computed at line 6, which does not access any other β values or the pruning mask.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus our hypergraph includes the hyperedge vikx ≺(ġ) whenever i = k−1, 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
"i < k ≤ n, and (x→ wk) ∈ rules(G), with ġ = Ġ(x→ wk | w, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"With this setup, the value βikx is the maximum score of any derivation of vertex β̇ikx (a tree rooted at β̇ikx, representing a subderivation), where the score
of a derivation is the product of its leaf values.",5.1 Background: Parsing as hypergraphs,[0],[0]
Alg. 1 computes it by considering hyperedges β̇ikx ≺ T and the previously computed values of the vertices in the tail T .,5.1 Background: Parsing as hypergraphs,[0],[0]
"For a vertex v̇, we write In(v̇) and Out(v̇) for its sets of incoming and outgoing hyperedges.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Our algorithms follow these hyperedges implicitly, without the overhead of materializing or storing them.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Change propagation is an efficient method for incrementally re-evaluating a computation under a change to its inputs (Acar and Ley-Wild, 2008; Filardo and Eisner, 2012).",5.2 Change propagation (CP),[0],[0]
"In our setting, each roll-in at Alg. 2 line 6 evaluates the reward r(PARSE(G, xi, π)) from (1), which involves computing an entire parse chart via Alg. 1.",5.2 Change propagation (CP),[0],[0]
"The inner loop at line 10 performs T interventions per roll-in, which ask how reward would have changed if one bit in the pruning maskm had been different.",5.2 Change propagation (CP),[0],[0]
"Rather than reparsing from scratch (T times) to determine this, we can simply adjust the initial roll-in computation (T times).
",5.2 Change propagation (CP),[0],[0]
CP is efficient when only a small fraction of the computation needs to be adjusted.,5.2 Change propagation (CP),[0],[0]
"In principle, flipping a single pruning bit can change up to 50% of the chart, so one might expect the bookkeeping overhead of CP to outweigh the gains.",5.2 Change propagation (CP),[0],[0]
"In practice, however, 90% of the interventions change < 10% of the β values in the chart.",5.2 Change propagation (CP),[0],[0]
"The reason is that βikx is a maximum over many quantities, only one of which “wins.”",5.2 Change propagation (CP),[0],[0]
"Changing a given βijy rarely affects this maximum, and so changes are unlikely to propagate from vertex β̇ijy to β̇ikx.",5.2 Change propagation (CP),[0],[0]
"Since changes are not very contagious, the “epidemic of changes” does not spread far.
",5.2 Change propagation (CP),[0],[0]
Alg. 3 provides pseudocode for updating the highest-scoring derivation found by Alg. 1.,5.2 Change propagation (CP),[0],[0]
"We remark that the RECOMPUTE is called only when we flip a bit from keep to prune, which removes hyperedges and potentially decreases vertex values.",5.2 Change propagation (CP),[0],[0]
"The reverse flip only adds hyperedges, which increases vertex values via a running max (lines 12–14).
",5.2 Change propagation (CP),[0],[0]
"After determining the effect of flipping a bit, we must restore the original chart before trying a different bit (the next rollout).",5.2 Change propagation (CP),[0],[0]
The simplest approach is to call Alg. 3 again to flip the bit,5.2 Change propagation (CP),[0],[0]
"back.5
5Our implementation uses a slightly faster method which accumulates an “undo list” of changes that it makes to the chart to quickly revert the modified chart to the original roll-in state.
",5.2 Change propagation (CP),[0.9529539243921006],"['The meta-learner might learn a prior which fixes the lower layers of the network to extract generic image features, but allows variation in the higher layers to adapt to new classes.']"
Algorithm 3 Change propagation algorithm 1: Global: Alg.,5.2 Change propagation (CP),[0],[0]
"1’s vertex values/witnesses (roll-in) 2: procedure CHANGE(v̇, v) 3: .",5.2 Change propagation (CP),[0],[0]
Change the value of a leaf vertex v̇ to v 4: value(v̇) := v ; witness(v̇) = LEAF 5: Q := ∅; Q.push(v̇) .,5.2 Change propagation (CP),[0],[0]
Work queue (“agenda”) 6: while Q 6= ∅ : .,5.2 Change propagation (CP),[0],[0]
Propagate until convergence 7: u̇,5.2 Change propagation (CP),[0],[0]
:= Q.pop() .,5.2 Change propagation (CP),[0],[0]
Narrower constituents first 8: if witness(u̇) = NULL : .,5.2 Change propagation (CP),[0],[0]
Value is unknown 9: RECOMPUTE(u̇) .,5.2 Change propagation (CP),[0],[0]
"Get value & witness
10: for e ∈ Out(u̇) : .",5.2 Change propagation (CP),[0],[0]
Propagate new value of u̇ 11: ṡ := e.head; s := ∏ u̇′∈e.,5.2 Change propagation (CP),[0],[0]
"Tail value(u̇
′) 12: if s > value(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Increase value 13: value(ṡ) := s; witness(ṡ) := e 14: Q.push(ṡ) 15: else if witness(ṡ) = e and s < value(ṡ): 16: witness(ṡ) := NULL .Value,5.2 Change propagation (CP),[0],[0]
may decrease 17: Q.push(ṡ) .,5.2 Change propagation (CP),[0],[0]
"so, recompute upon pop 18: procedure RECOMPUTE(ṡ) 19: for e ∈ In(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Max over incoming hyperedges 20: s := ∏ u̇∈e.,5.2 Change propagation (CP),[0],[0]
Tail value(u̇) 21: if s > value(ṡ) : 22: value(ṡ) = s; witness(ṡ) =,5.2 Change propagation (CP),[0],[0]
e,5.2 Change propagation (CP),[0],[0]
The naive rollout algorithm runs the parser T times— once for each variation of the pruning mask.,5.3 Dynamic programming (DP),[0],[0]
"The reader may be reminded of the finite difference approximation to the gradient of a function, which also measures the effects from perturbing each input value individually.",5.3 Dynamic programming (DP),[0],[0]
"In fact, for certain reward functions, the naive algorithm can be precisely regarded as computing a gradient—and thus we can use a more efficient algorithm, back-propagation, which finds the entire gradient vector of reward as fast (in the big-O sense) as computing the reward once.",5.3 Dynamic programming (DP),[0],[0]
"The overall algorithm is O(|E| + T ) where |E| is the total number of hyperedges, whereas the naive algorithm is O(|E′|·T ) where |E′| ≤ |E| is the maximum number of hyperedges actually visited on any rollout.
",5.3 Dynamic programming (DP),[0],[0]
What accuracy measure must we use?,5.3 Dynamic programming (DP),[0],[0]
Let r(d) denote the recall of a derivation d—the fraction of gold constituents that appear as vertices in the derivation.,5.3 Dynamic programming (DP),[0],[0]
"A simple accuracy metric would be 1-best recall, the recall r(d̂) of the highest-scoring derivation d̂ that was not pruned.",5.3 Dynamic programming (DP),[0],[0]
"In this section, we relax that to ex-
pected recall,6 r̄= ∑
d p(d)r(d).",5.3 Dynamic programming (DP),[0],[0]
"Here we interpret the pruned hypergraph’s values as an unnormalized probability distribution over derivations, where the probability p(d) =",5.3 Dynamic programming (DP),[0],[0]
p̃(d)/Z of a derivation is proportional to its score p̃(d) =,5.3 Dynamic programming (DP),[0],[0]
"∏ u̇∈leaves(d) value(u̇).
",5.3 Dynamic programming (DP),[0],[0]
"Though r̄ is not quite our evaluation metric, it captures more information about the parse forest, and so may offer some regularizing effect when used in a training criterion (see §7.1).",5.3 Dynamic programming (DP),[0],[0]
"In any case, r̄ is close to r(d̂) when probability mass is concentrated on a few derivations, which is common with heavy pruning.
",5.3 Dynamic programming (DP),[0],[0]
"We can re-express r̄ as r̃/Z, where
r̃ = ∑
d
p̃(d)r(d) Z = ∑
d
p̃(d) (2)
These can be efficiently computed by dynamic programming (DP), specifically by a variant of the inside algorithm (Li and Eisner, 2009).",5.3 Dynamic programming (DP),[0],[0]
"Since p̃(d) is a product of rule weights and pruning mask bits at d’s leaves (§5.1), each appearing at most once, both r̃ and Z vary linearly in any one of these inputs provided that all other inputs are held constant.",5.3 Dynamic programming (DP),[0],[0]
"Thus, the exact effect on r̃ or Z of changing an input mik can be found from the partial derivatives with respect to it.",5.3 Dynamic programming (DP),[0],[0]
"In particular, if we increased mik by ∆ ∈ {−1, 1} (to flip this bit), the new value of r̄ would be exactly
r̃ + ∆ · ∂r̃/∂mik",5.3 Dynamic programming (DP),[0],[0]
"Z + ∆ · ∂Z/∂mik
(3)
",5.3 Dynamic programming (DP),[0],[0]
It remains to compute these partial derivatives.,5.3 Dynamic programming (DP),[0],[0]
"All partials can be jointly computed by back-propagation, which equivalent to another dynamic program known as the outside algorithm (Eisner, 2016).
",5.3 Dynamic programming (DP),[0],[0]
"The inside algorithm only needs to visit the |E′| unpruned edges, but the outside algorithm must also visit some pruned edges, to determine the effect of “unpruning” them (changing their mik input from 0 to 1) by finding ∂r̃/∂mik and ∂Z/∂mik.",5.3 Dynamic programming (DP),[0],[0]
"On the other hand, these partials are 0 when some other input to the hyperedge is 0.",5.3 Dynamic programming (DP),[0],[0]
"This case is common when the hypergraph is heavily pruned (|E′| |E|), and means that back-propagation need not descend further through that hyperedge.
",5.3 Dynamic programming (DP),[0],[0]
"6In theory, we could anneal from expected to 1-best recall (Smith and Eisner, 2006).",5.3 Dynamic programming (DP),[0],[0]
"We experimented extensively with annealing but found it to be too numerically unstable for our purposes, even with high-precision arithmetic libraries.
",5.3 Dynamic programming (DP),[0],[0]
Note that the DP method computes only the accuracies of rollouts—not the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7).",5.3 Dynamic programming (DP),[0],[0]
An alternative would be to use CP to roll out the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"This is very efficient: to measure just runtime, CP only needs to update the record of which constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly.
6 Parser details7
Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40.",5.3 Dynamic programming (DP),[0],[0]
We do not restrict the length of test sentences.,5.3 Dynamic programming (DP),[0],[0]
"We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5).",5.3 Dynamic programming (DP),[0],[0]
The parsing algorithms used during training are described in §5.,5.3 Dynamic programming (DP),[0],[0]
"Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010).",5.3 Dynamic programming (DP),[0],[0]
All algorithms allow unary rules (though not chains).,5.3 Dynamic programming (DP),[0],[0]
"We evaluate accuracy at test time with the F1 score from the official EVALB script (Sekine and Collins, 1997).
",5.3 Dynamic programming (DP),[0],[0]
Training:,5.3 Dynamic programming (DP),[0],[0]
Note that we never retrain the grammar weights—we train only the pruning policy.,5.3 Dynamic programming (DP),[0],[0]
"To TRAIN our classifiers (Alg. 2 line 13), we use L2-regularized logistic regression, trained with L-BFGS optimization.",5.3 Dynamic programming (DP),[0],[0]
"We always rescale the example weights in the training set to sum to 1 (otherwise as LOLS proceeds, dataset aggregation overwhelms the regularizer).",5.3 Dynamic programming (DP),[0],[0]
"For the baseline (defined in next section), we determine the regularization coefficient by sweeping {2−11, 2−12, 2−13, 2−14, 2−15} and picking the best value (2−13) based on the dev frontier.",5.3 Dynamic programming (DP),[0],[0]
We re-used this regularization parameter for LOLS.,5.3 Dynamic programming (DP),[0],[0]
"The number of LOLS iterations is determined by a 6-day training-time limit9 (meaning some jobs run many
7Code for experiments is available at http://github.",5.3 Dynamic programming (DP),[0],[0]
"com/timvieira/learning-to-prune.
",5.3 Dynamic programming (DP),[0],[0]
8Data train/dev/test split (by section) 2–21 / 22 / 23.,5.3 Dynamic programming (DP),[0],[0]
"Normalization operations: Remove function tags, traces, spurious unary edges (X → X), and empty subtrees left by other operations.",5.3 Dynamic programming (DP),[0],[0]
"Relabel ADVP and PRT|ADVP tags to PRT.
",5.3 Dynamic programming (DP),[0],[0]
"9On the 7th day, LOLS rested and performance was good.
",5.3 Dynamic programming (DP),[0],[0]
fewer iterations than others).,5.3 Dynamic programming (DP),[0],[0]
For LOLS minibatch size we use 10K on the coarse grammar and 5K on the fine grammar.,5.3 Dynamic programming (DP),[0],[0]
"At line 15 of Alg. 2, we return the policy that maximized reward on development data, using the reward function from training.
",5.3 Dynamic programming (DP),[0],[0]
"Features: We use similar features to Bodenstab et al. (2011), but we have removed features that depend on part-of-speech tags.",5.3 Dynamic programming (DP),[0],[0]
"We use the following 16 feature templates for span (i, k) with 1 < k−i < N : bias, sentence length, boundary words, conjunctions of boundary words, conjunctions of word shapes, span shape, width bucket.",5.3 Dynamic programming (DP),[0],[0]
"Shape features map a word or phrase into a string of character classes (uppercase, lowercase, numeric, spaces); we truncate substrings of identical classes to length two; punctuation chars are never modified in any way.",5.3 Dynamic programming (DP),[0],[0]
"Width buckets use the following partition: 2, 3, 4, 5, [6, 10], [11, 20], [21,∞).",5.3 Dynamic programming (DP),[0],[0]
"We use feature hashing (Weinberger et al., 2009) with MurmurHash3 (Appleby, 2008) and project to 222 features.",5.3 Dynamic programming (DP),[0],[0]
"Conjunctions are taken at positions (i−1, i), (k, k+1), (i−1, k+1) and (i, k).",5.3 Dynamic programming (DP),[0],[0]
"We use special begin and end symbols when a template accesses positions beyond the sentence boundary.
",5.3 Dynamic programming (DP),[0],[0]
Hall et al. (2014) give examples motivating our feature templates and show experimentally that they are effective in multiple languages.,5.3 Dynamic programming (DP),[0],[0]
Boundary words are strong surface cues for phrase boundaries.,5.3 Dynamic programming (DP),[0],[0]
Span shape features are also useful as they (minimally) check for matched parentheses and quotation marks.,5.3 Dynamic programming (DP),[0],[0]
Reward functions and surrogates: Each user has a personal reward function.,7 Experimental design and results,[0],[0]
"In this paper, we choose to specify our true reward as accuracy − λ · runtime, where accuracy is given by labeled F1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines 6 and 19 of Alg. 1, which is in practice proportional to seconds per sentence (correlation > 0.95) and is more replicable.",7 Experimental design and results,[0],[0]
We evaluate accordingly (on test data)—but during LOLS training we approximate these metrics.,7 Experimental design and results,[0],[0]
"We compare:
• rCP (fast): Use change propagation (§5.2) to compute accuracy on a sentence as F1 of just that sentence, and to approximate runtime as ||β||0,
the number of constituents that were built.10
• rDP (faster): Use dynamic programming (§5.3) to approximate accuracy on a sentence as expected recall.11 This time we approximate runtime more crudely as ||m||0, the number of nonzeros in the pruning mask for the sentence (i.e., the number of spans whose constituents the policy would be willing to keep if they were built).
",7 Experimental design and results,[0],[0]
We use these surrogates because they admit efficient rollout algorithms.,7 Experimental design and results,[0],[0]
"Less important, they preserve the training objective (1) as an average over sentences.",7 Experimental design and results,[0],[0]
"(Our true F1 metric on a corpus cannot be computed in this way, though it could reasonably be estimated by averaging over minibatches of sentences in (1).)
",7 Experimental design and results,[0],[0]
"Controlled experimental design: Our baseline system is an adaptation of Bodenstab et al. (2011) to learning-to-prune, as described in §3 and §6.",7 Experimental design and results,[0],[0]
Our goal is to determine whether such systems can be improved by LOLS training.,7 Experimental design and results,[0],[0]
"We repeat the following design for both reward surrogates (rCP and rDP) and for both grammars (coarse and fine).
",7 Experimental design and results,[0],[0]
¬ We start by training a number of baseline models by sweeping the asymmetric weighting parameter.,7 Experimental design and results,[0],[0]
"For the coarse grammar we train 8 such models, and for the fine grammar 12.
 ",7 Experimental design and results,[0],[0]
"For each baseline policy, we estimate a value of λ for which that policy is optimal (among baseline policies) according to surrogate reward.12
10When using rCP, we speed up LOLS by doing≤ 2n rollouts per sentence of length n. We sample these uniformly without replacement from the T possible rollouts (§5), and compensate by upweighting the resulting training examples by T/(2n).
",7 Experimental design and results,[0],[0]
"11Considering all nodes in the binarized tree, except for the root, width-1 constituents, and children of unary rules.
",7 Experimental design and results,[0],[0]
"12We estimate λ by first fitting a parametric model yi = h(xi) , ymax · sigmoid(a · log(xi + c) + b) to the baseline runtime-accuracy measurements on dev data (shown in green in Fig. 2) by minimizing mean squared error.",7 Experimental design and results,[0],[0]
"We then use the fitted curve’s slope h′ to estimate each λi = h′(xi), where xi is the runtime of baseline i. The resulting choice of reward function y−λi",7 Experimental design and results,[0],[0]
"·x increases along the green arrow in Fig. 2, and is indeed maximized (subject to y ≤ h(x), and in the region where h is concave) at x = xi.",7 Experimental design and results,[0],[0]
"As a sanity check, notice since λi is a derivative of the function y = h(x), its units are in units of y (accuracy) per unit of x (runtime), as appropriate for use in the expression",7 Experimental design and results,[0],[0]
y,7 Experimental design and results,[0],[0]
− λi · x.,7 Experimental design and results,[0],[0]
"Indeed, this procedure will construct the same reward function regardless of the units we use to express x.",7 Experimental design and results,[0],[0]
"Our specific parametric model h is a sigmoidal curve, with
® For each baseline policy, we run LOLS with the same surrogate reward function (defined by λ) for which that baseline policy was optimal.",7 Experimental design and results,[0],[0]
We initialize LOLS by setting π0 to the baseline policy.,7 Experimental design and results,[0],[0]
"Furthermore, we include the baseline policy’s weighted training set Q̂0 in the ⋃ at line 13.
",7 Experimental design and results,[0],[0]
"Fig. 2 shows that LOLS learns to improve on the baseline, as evaluated on development data.
¯",7 Experimental design and results,[0],[0]
But do these surrogate reward improvements also improve our true reward?,7 Experimental design and results,[0],[0]
"For each baseline policy, we use dev data to estimate a value of λ for which that policy is optimal according to our true reward function.",7 Experimental design and results,[0],[0]
"We use blind test data to compare the baseline policy to its corresponding LOLS policy on this true reward function, testing significance with a paired permutation test.",7 Experimental design and results,[0],[0]
"The improvements hold up, as shown in Fig. 3.
",7 Experimental design and results,[0],[0]
"The rationale behind this design is that a user who actually wishes to maximize accuracy−λ·runtime, for some specific λ, could reasonably start by choosing the best baseline policy for this reward function, and then try to improve that baseline by running LOLS with the same reward function.",7 Experimental design and results,[0],[0]
"Our experiments show this procedure works for a range of λ values.
",7 Experimental design and results,[0],[0]
"In the real world, a user’s true objective might instead be some nonlinear function of runtime and accuracy.",7 Experimental design and results,[0],[0]
"For example, when accuracy is “good enough,” it may be more important to improve runtime, and vice-versa.",7 Experimental design and results,[0],[0]
LOLS could be used with such a nonlinear reward function as well.,7 Experimental design and results,[0],[0]
"In fact, a user does not even have to quantify their global preferences by writing down such a function.",7 Experimental design and results,[0],[0]
"Rather, they could select manually among the baseline policies, choosing one with an attractive speed-accuracy tradeoff, and then specify λ to indicate a local direction of desired improvement (like the green arrows in Fig. 2), modifying this direction periodically as LOLS runs.",7 Experimental design and results,[0],[0]
"As previous work has shown, learning to prune gives us excellent parsers with less than < 2% overhead
accuracy → ymax asymptotically as runtime → ∞. It obtains an excellent fit by placing accuracy and runtime on the loglogit scale—that is, log(xi + c) and logit(yi/ymax) transforms are used to convert our bounded random variables xi and yi to unbounded ones—and then assuming they are linearly related.
for deciding what to prune (i.e., pruning feature extraction and span classification).",7.1 Discussion,[0],[0]
"Even the baseline pruner has access to features unavailable to the grammar, and so it learns to override the grammar, improving an unpruned coarse parser’s accuracy from 61.1 to as high as 70.1% F1 on test data (i.e., beneficial search error).",7.1 Discussion,[0],[0]
"It is also 8.1x faster!13 LOLS simply does a better job at figuring out where to prune, raising accuracy 2.1 points to 72.2 (while maintaining a 7.4x speedup).",7.1 Discussion,[0],[0]
"Where pruning is more aggressive,
13We measure runtime as best of 10 runs (recommended by Dunlop (2014)).",7.1 Discussion,[0],[0]
"All parser timing experiments were performed on a Linux laptop with the following specs: Intel® Core™ i5-2540M 2.60GHz CPU, 8GB memory, 32K/256K/3072K L1/L2/L3 cache.",7.1 Discussion,[0],[0]
"Code is written in the Cython language.
",7.1 Discussion,[0],[0]
"LOLS has even more impact on accuracy.
",7.1 Discussion,[0],[0]
"Even on the fine grammar, where there is less room to improve accuracy, the most accurate LOLS system improves an unpruned parser by +0.16% F1 with a 8.6x speedup.",7.1 Discussion,[0],[0]
"For comparison, the most accurate baseline drops −0.03% F1 with a 9.7x speedup.
",7.1 Discussion,[0],[0]
"With the fine grammar, we do not see much improvement over the baseline in the accuracy > 85 regions.",7.1 Discussion,[0],[0]
This is because the supervision specified by asymmetric weighting is similar to what LOLS surmises via rollouts.,7.1 Discussion,[0],[0]
"However, in lower-accuracy regions we see that LOLS can significantly improve reward over its baseline policy.",7.1 Discussion,[0],[0]
"This is because the baseline supervision does not teach which plausible
constituents are “safest” to prune, nor can it learn strategies such as “skip all long sentences.”",7.1 Discussion,[0],[0]
"We discuss why LOLS does not help as much in the high accuracy regions further in §7.3.
",7.1 Discussion,[0],[0]
"In a few cases in Fig. 2, LOLS finds no policy that improves surrogate reward on dev data.",7.1 Discussion,[0],[0]
"In these cases, surrogate reward does improve slightly on training data (not shown), but early stopping just keeps the initial (baseline) policy since it is just as good on dev data.",7.1 Discussion,[0],[0]
"Adding a bit of additional random exploration might help break out of this initialization.
",7.1 Discussion,[0],[0]
"Interestingly, the rDP LOLS policies find higheraccuracy policies than the corresponding rCP policies, despite a greater mismatch in surrogate accuracy definitions.",7.1 Discussion,[0],[0]
"We suspect that rDP’s approach of trying to improve expected accuracy may provide a useful regularizing effect, which smooths out the reward signal and provides a useful bias (§5.3).
",7.1 Discussion,[0],[0]
"The most pronounced qualitative difference due to LOLS training is substantially lower rates of parse failure in the mid- to high- λ-range on both grammars
(not shown).",7.1 Discussion,[0],[0]
"Since LOLS does end-to-end training, it can advise the learner that a certain pruning decision catastrophically results in no parse being found.",7.1 Discussion,[0],[0]
Part of the contribution of this paper is faster algorithms for performing LOLS rollouts during training (§5).,7.2 Training speed and convergence,[0],[0]
"Compared to the naive strategy of running the parser from scratch T times, rCP achieves speedups of 4.9–6.6x on the coarse grammar and 1.9–2.4x on the fine grammar.",7.2 Training speed and convergence,[0],[0]
"rDP is even faster, 10.4–11.9x on coarse and 10.5–13.8x on fine.",7.2 Training speed and convergence,[0],[0]
"Most of the speedup comes from longer sentences, which take up most of the runtime for all methods.",7.2 Training speed and convergence,[0],[0]
Our new algorithms enable us to train on fairly long sentences (≤ 40).,7.2 Training speed and convergence,[0],[0]
"We note that our implementations of rCP and rDP are not as highly optimized as our test-time parser, so there may be room for improvement.
",7.2 Training speed and convergence,[0],[0]
Orthogonal to the cost per rollout is the number of training iterations.,7.2 Training speed and convergence,[0],[0]
"LOLS may take many steps to converge if trajectories are long (i.e., T is large)
because each iteration of LOLS training attempts to improve the current policy by a single action.",7.2 Training speed and convergence,[0],[0]
"In our setting, T is quite large (discussed extensively in §5), but we are able to circumvent slow convergence by initializing the policy (via the baseline method).",7.2 Training speed and convergence,[0],[0]
This means that LOLS can focus on fine-tuning a policy which is already quite good.,7.2 Training speed and convergence,[0],[0]
"In fact, in 4 cases, LOLS did not improve from its initial policy.
",7.2 Training speed and convergence,[0],[0]
We find that when λ is large—the cases where we get meaningful improvements because the initial policy is far from locally optimal—LOLS steadily and smoothly improves the surrogate reward on both training and development data.,7.2 Training speed and convergence,[0],[0]
"Because these are fast parsers, LOLS was able to run on the order of 10 (fine grammar) or 100 (coarse grammar) epochs within our 6-day limit; usually it was still improving when we terminated it.",7.2 Training speed and convergence,[0],[0]
"By contrast, for the slower and more accurate small-λ parsers (which completed fewer training epochs), LOLS still improves surrogate reward on training data, but without systematically improving on development data—often the reward on development fluctuates, and early stopping simply picks the best of this small set of “random” variants.",7.2 Training speed and convergence,[0],[0]
"In §3, we argued that LOLS gives a more appropriate training signal for pruning than the baseline method of consulting the gold parse, because it uses rollouts to measure the full effect of each pruning decision in the context of the other decisions made by the policy.
",7.3 Understanding the LOLS training signal,[0],[0]
"To better understand the results of our previous experiments, we analyze how often a rollout does determine that the baseline supervision for a span is suboptimal, and how suboptimal it is in those cases.
",7.3 Understanding the LOLS training signal,[0],[0]
We specifically consider LOLS rollouts that evaluate the rCP surrogate (because rDP is a cruder approximation to true reward).,7.3 Understanding the LOLS training signal,[0],[0]
"These rollouts Q̂i tell us what actions LOLS is trying to improve in its current policy πi for a given λ, although there is no guarantee that the learner in §4 will succeed at classifying Q̂i correctly (due to limited features, regularization, and the effects of dataset aggregation).
",7.3 Understanding the LOLS training signal,[0],[0]
We define regret of the baseline oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"Let best(s) , argmaxaROLLOUT(π, s, a) and regret(s) , (ROLLOUT(π, s, best(s) − ROLLOUT(π, s, gold(s)))).",7.3 Understanding the LOLS training signal,[0],[0]
"Note that regret(s)≥0 for all s, and let diff(s) be the event that regret(s) > 0 strictly.",7.3 Understanding the LOLS training signal,[0],[0]
"We are interested in analyzing the expected regret over all gold and
non-gold spans, which we break down as
E[regret] = p(diff) (4) · ( p(gold | diff) · E[regret | gold, diff] + p(¬ gold | diff) · E[regret | ¬ gold, diff] )
where expectations are taken over s ∼ ROLL-IN(π).",7.3 Understanding the LOLS training signal,[0],[0]
"Empirical analysis of regret: To show where the benefit of the LOLS oracle comes from, Fig. 4 graphs the various quantities that enter into the definition (4) of baseline regret, for different π, λ, and grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"The LOLS oracle evolves along with the policy π, since it identifies the best action given π.",7.3 Understanding the LOLS training signal,[0],[0]
"We thus evaluate the oracle baseline against two LOLS oracles: the one used at the start of LOLS training (derived from the initial policy π1 that was trained on baseline supervision), and the one obtained at the end (derived from the LOLS-trained policy π∗ selected by early stopping).",7.3 Understanding the LOLS training signal,[0],[0]
"These comparisons are shown by solid and dashed lines respectively.
",7.3 Understanding the LOLS training signal,[0],[0]
"Class imbalance (black curves): In all graphs, the aggregate curves primarily reflect the non-gold spans, since only 8% of spans are gold.
",7.3 Understanding the LOLS training signal,[0],[0]
"Gold spans (gold curves): The top graphs show that a substantial fraction of the gold spans should be pruned (whereas the baseline tries to keep them all), although the middle row shows that the benefit of pruning them is small.",7.3 Understanding the LOLS training signal,[0],[0]
"In most of these cases, pruning a gold span improves speed but leaves accuracy unchanged—because that gold span was missed anyway by the highest-scoring parse.",7.3 Understanding the LOLS training signal,[0],[0]
Such cases become both more frequent and more beneficial as λ increases and we prune more heavily.,7.3 Understanding the LOLS training signal,[0],[0]
"In a minority of cases, however, pruning a gold span also improves accuracy (through beneficial search error).
",7.3 Understanding the LOLS training signal,[0],[0]
"Non-gold spans (purple curves): Conversely, the top graphs show that a few non-gold spans should be kept (whereas the baseline tries to prune them all), and the middle row shows a large benefit from keeping them.",7.3 Understanding the LOLS training signal,[0],[0]
"They are needed to recover from catastrophic errors and get a mostly-correct parse.
",7.3 Understanding the LOLS training signal,[0],[0]
Coarse vs. fine (left vs. right):,7.3 Understanding the LOLS training signal,[0],[0]
"The two grammars differ mainly for small λ, and this difference comes especially from the top row.",7.3 Understanding the LOLS training signal,[0],[0]
"With a fine grammar and small λ, the baseline parses are more accurate, so LOLS has less room for improvement: fewer
gold spans go unused, and fewer non-gold spans are needed for recovery.
",7.3 Understanding the LOLS training signal,[0],[0]
"Effect of λ: Aggressive pruning (large λ) reduces accuracy, so its effect on the top row is similar to that of using a coarse grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"Aggressive pruning also has an effect on the middle row: there is more benefit to be derived from pruning unused gold spans (surprisingly), and especially from keeping those non-gold spans that are helpful (presumably they enable recovery from more severe parse errors).",7.3 Understanding the LOLS training signal,[0],[0]
"These effects are considerably sharper with rDP reward (not shown here), which more smoothly evaluates the entire weighted pruned parse forest rather than trying to coordinate actions to ensure a good single 1-best tree; the baseline oracle is excellent at choosing the action that gets the better forest when the forest is mostly present (small λ) but not when it is mostly pruned (large λ).
",7.3 Understanding the LOLS training signal,[0],[0]
Effect on retraining the policy: The black lines in the bottom graphs show the overall regret (on training data) if we were to perfectly follow the baseline oracle rather than the LOLS oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"In practice, retraining the policy to match the oracle will not match it perfectly in either case.",7.3 Understanding the LOLS training signal,[0],[0]
"Thus the baseline method has a further disadvantage: when it trains a policy, its training objective weights all gold or all non-gold examples equally, whereas LOLS invests greater effort in matching the oracle on those states where doing so would give greater downstream reward.",7.3 Understanding the LOLS training signal,[0],[0]
Our experiments have focused on using LOLS to improve a reasonable baseline.,8 Related work,[0],[0]
Fig. 5 shows that our resulting parser fits reasonably among state-of-the-art constituency parsers trained and tested on the Penn Treebank.,8 Related work,[0],[0]
These parsers include a variety of techniques that improve speed or accuracy.,8 Related work,[0],[0]
"Many are quite orthogonal to our work here—e.g., the SpMV method (which is necessary for Bodenstab’s parser to beat ours) is a set of cache-efficient optimizations (Dunlop, 2014) that could be added to our parser (just as it was added to Bodenstab’s), while Hall et al. (2014) and Fernández-González and Martins (2015) replace the grammar with faster scoring models that have more conditional independence.",8 Related work,[0],[0]
"Overall, other fast parsers could also be trained using LOLS, so that
they quickly find parses that are accurate, or at least helpful to the accuracy of some downstream task.
",8 Related work,[0],[0]
"Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011).",8 Related work,[0],[0]
"Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998).
",8 Related work,[0],[0]
This paper focuses on learning pruning heuristics that have trainable parameters.,8 Related work,[0],[0]
"In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient).",8 Related work,[0],[0]
"In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here.",8 Related work,[0],[0]
"Some previous “coarse-to-fine” work does not optimize heuris-
14We focus here on parsing, but pruning is generally useful in structured prediction.",8 Related work,[0],[0]
"E.g., Xu et al. (2013) train a classifier to prune (latent) alignments in a machine translation system.
tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model.",8 Related work,[0],[0]
"Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015).
",8 Related work,[0],[0]
Dynamic features are ones that depend on previous actions.,8 Related work,[0],[0]
"In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14.",8 Related work,[0],[0]
"While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently.",8 Related work,[0],[0]
"It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013).",8 Related work,[0],[0]
"LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k).",8 Related work,[0],[0]
This permits our fast CP and DP rollout algorithms.,8 Related work,[0],[0]
"It also reduces the time and space cost of dataset aggregation.15
LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure.",8 Related work,[0],[0]
"In NLP, sequential decisions are more commonly trained with step-by-step supervision
15LOLS repeatedly evaluates actions given (w, i, k).",8 Related work,[0],[0]
"We consolidate the resulting training examples by summing their reward vectors r̂, so the aggregated dataset does not grow over time.
",8 Related work,[0],[0]
"(Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004).",8 Related work,[0],[0]
LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions.,8 Related work,[0],[0]
"It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all.",8 Related work,[0],[0]
"LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration.
",8 Related work,[0],[0]
"We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate.",8 Related work,[0],[0]
"This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16",8 Related work,[0],[0]
"However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks.",8 Related work,[0],[0]
"Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing.",8 Related work,[0],[0]
"Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in the presence of incomplete supervision, latent structure, or indirect feedback.",8 Related work,[0],[0]
"Our method could also be used immediately to speed up dynamic programming methods for MT, synchronous parsing, parsing with non-context-free grammar formalisms, and other structured prediction problems for which transition systems have not (yet) been designed.",8 Related work,[0],[0]
We presented an approach to learning pruning policies that optimizes end-to-end performance on a userspecified speed-accuracy tradeoff.,9 Conclusions,[0],[0]
We developed two novel algorithms for efficiently measuring how varying policy actions affects reward.,9 Conclusions,[0],[0]
"In the case of parsing, given a performance criterion and a good baseline policy for that criterion, the learner consistently manages to find a higher-reward policy.",9 Conclusions,[0.9578881034119702],"['As a motivational example, consider the case in which a meta-learner observes many image classification tasks of natural images, and uses a CNN to learn each task.']"
"We hope this work inspires a new generation of fast and accurate structured prediction models with tunable runtimes.
16Of course, LOLS can also train transition-based parsers (Chang et al., 2015a), or even vary their beam width dynamically.",9 Conclusions,[0],[0]
This material is based in part on research sponsored by the National Science Foundation under Grant No. 0964681 and DARPA under agreement number FA8750-13-2-0017 (DEFT program).,Acknowledgments,[0],[0]
"We’d like to thank Nathaniel Wesley Filardo, Adam Teichert, Matt Gormley and Hal Daumé III for helpful discussions.",Acknowledgments,[0],[0]
"Finally, we thank TACL action editor Marco Kuhlmann and the anonymous reviewers and copy editor for suggestions that improved this paper.",Acknowledgments,[0],[0]
Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing.,abstractText,[0],[0]
"Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy.",abstractText,[0],[0]
"This poses a difficult machine learning problem, which we tackle with the LOLS algorithm.",abstractText,[0],[0]
"LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms.",abstractText,[0],[0]
"We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.",abstractText,[0],[0]
Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing,title,[0],[0]
Deep neural networks (DNNs) have been widely used for machine learning applications due to their powerful capacity for modeling complex input patterns.,1. Introduction,[0],[0]
"Despite their success, it has been shown that DNNs are prone to training set biases, i.e. the training set is drawn from a joint distribution p(x, y) that is different from the distribution p(xv, yv) of the evaluation set.",1. Introduction,[0],[0]
"This distribution mismatch could have many
1Uber Advanced Technologies Group, Toronto ON, CANADA 2Department of Computer Science, University of Toronto, Toronto ON, CANADA.",1. Introduction,[0],[0]
"Correspondence to: Mengye Ren <mren3@uber.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
different forms.",1. Introduction,[0],[0]
Class imbalance in the training set is a very common example.,1. Introduction,[0],[0]
"In applications such as object detection in the context of autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy.",1. Introduction,[0],[0]
"This will sometime lead to biased training models that do not perform well in practice.
",1. Introduction,[0],[0]
Another popular type of training set bias is label noise.,1. Introduction,[0],[0]
"To train a reasonable supervised deep model, we ideally need a large dataset with high-quality labels, which require many passes of expensive human quality assurance (QA).",1. Introduction,[0],[0]
"Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance, e.g. Zhang et al. (2017) has shown that a standard CNN can fit any ratio of label flipping noise in the training set and eventually leads to poor generalization performance.
",1. Introduction,[0],[0]
"Training set biases and misspecification can sometimes be addressed with dataset resampling (Chawla et al., 2002), i.e. choosing the correct proportion of labels to train a network on, or more generally by assigning a weight to each example and minimizing a weighted training loss.",1. Introduction,[0],[0]
"The example weights are typically calculated based on the training loss, as in many classical algorithms such as AdaBoost (Freund & Schapire, 1997), hard negative mining (Malisiewicz et al., 2011), self-paced learning (Kumar et al., 2010), and other more recent work (Chang et al., 2017; Jiang et al., 2017).
",1. Introduction,[0],[0]
"However, there exist two contradicting ideas in training loss based approaches.",1. Introduction,[0],[0]
"In noisy label problems, we prefer examples with smaller training losses as they are more likely to be clean images; yet in class imbalance problems, algorithms such as hard negative mining (Malisiewicz et al., 2011) prioritize examples with higher training loss since they are more likely to be the minority class.",1. Introduction,[0],[0]
"In cases when the training set is both imbalanced and noisy, these existing methods would have the wrong model assumptions.",1. Introduction,[0],[0]
"In fact, without a proper definition of an unbiased test set, solving the training set bias problem is inherently ill-defined.",1. Introduction,[0],[0]
"As the model cannot distinguish the right from the wrong, stronger regularization can usually work surprisingly well in certain synthetic noise settings.",1. Introduction,[0],[0]
"Here we argue that in order to learn general forms of training set biases, it is necessary to have a small unbiased validation to guide training.",1. Introduction,[0],[0]
"It is actually
not uncommon to construct a dataset with two parts - one relatively small but very accurately labeled, and another massive but coarsely labeled.",1. Introduction,[0],[0]
"Coarse labels can come from inexpensive crowdsourcing services or weakly supervised data (Cordts et al., 2016; Russakovsky et al., 2015; Chen & Gupta, 2015).
",1. Introduction,[0],[0]
"Different from existing training loss based approaches, we follow a meta-learning paradigm and model the most basic assumption instead: the best example weighting should minimize the loss of a set of unbiased clean validation examples that are consistent with the evaluation procedure.",1. Introduction,[0],[0]
"Traditionally, validation is performed at the end of training, which can be prohibitively expensive if we treat the example weights as some hyperparameters to optimize; to circumvent this, we perform validation at every training iteration to dynamically determine the example weights of the current batch.",1. Introduction,[0],[0]
"Towards this goal, we propose an online reweighting method that leverages an additional small validation set and adaptively assigns importance weights to examples in every iteration.",1. Introduction,[0],[0]
We experiment with both class imbalance and corrupted label problems and find that our approach significantly increases the robustness to training set biases.,1. Introduction,[0],[0]
The idea of weighting each training example has been well studied in the literature.,2. Related Work,[0],[0]
"Importance sampling (Kahn & Marshall, 1953), a classical method in statistics, assigns weights to samples in order to match one distribution to another.",2. Related Work,[0],[0]
"Boosting algorithms such as AdaBoost (Freund & Schapire, 1997), select harder examples to train subsequent classifiers.",2. Related Work,[0],[0]
"Similarly, hard example mining (Malisiewicz et al., 2011), downsamples the majority class and exploits the most difficult examples.",2. Related Work,[0],[0]
"Focal loss (Lin et al., 2017) adds a soft weighting scheme that emphasizes harder examples.
",2. Related Work,[0],[0]
Hard examples are not always preferred in the presence of outliers and noise processes.,2. Related Work,[0],[0]
Robust loss estimators typically downweigh examples with high loss.,2. Related Work,[0],[0]
"In selfpaced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss encouraging learning easier examples first.",2. Related Work,[0],[0]
"In each step, the learning algorithm jointly solves a mixed integer program that iterates optimizing over model parameters and binary example weights.",2. Related Work,[0],[0]
"Various regularization terms on the example weights have since been proposed to prevent overfitting and trivial solutions of assigning weights to be all zeros (Kumar et al., 2010; Ma et al., 2017; Jiang et al., 2015).",2. Related Work,[0],[0]
Wang et al. (2017) proposed a Bayesian method that infers the example weights as latent variables.,2. Related Work,[0],[0]
"More recently, Jiang et al. (2017) proposed to use a meta-learning LSTM to output the weights of the examples based on the training loss.",2. Related Work,[0],[0]
"Reweighting examples is also related to curriculum learning (Bengio et al., 2009), where the model reweights
among many available tasks.",2. Related Work,[0],[0]
"Similar to self-paced learning, typically it is beneficial to start with easier examples.
",2. Related Work,[0],[0]
One crucial advantage of reweighting examples is robustness against training set bias.,2. Related Work,[0],[0]
"There has also been a multitude of prior studies on class imbalance problems, including using dataset resampling (Chawla et al., 2002; Dong et al., 2017), cost-sensitive weighting (Ting, 2000; Khan et al., 2015), and structured margin based objectives (Huang et al., 2016).",2. Related Work,[0],[0]
"Meanwhile, the noisy label problem has been thoroughly studied by the learning theory community (Natarajan et al., 2013; Angluin & Laird, 1988) and practical methods have also been proposed (Reed et al., 2014; Sukhbaatar & Fergus, 2014; Xiao et al., 2015; Azadi et al., 2016; Goldberger & Ben-Reuven, 2017; Li et al., 2017; Jiang et al., 2017; Vahdat, 2017; Hendrycks et al., 2018).",2. Related Work,[0],[0]
"In addition to corrupted data, Koh & Liang (2017); Muñoz-González et al. (2017) demonstrate the possibility of a dataset adversarial attack (i.e. dataset poisoning).
",2. Related Work,[0],[0]
"Our method improves the training objective through a weighted loss rather than an average loss and is an instantiation of meta-learning (Thrun & Pratt, 1998; Lake et al., 2017; Andrychowicz et al., 2016), i.e. learning to learn better.",2. Related Work,[0],[0]
"Using validation loss as the meta-objective has been explored in recent meta-learning literature for few-shot learning (Ravi & Larochelle, 2017; Ren et al., 2018; Lorraine & Duvenaud, 2018), where only a handful of examples are available for each class.",2. Related Work,[0],[0]
"Our algorithm also resembles MAML (Finn et al., 2017) by taking one gradient descent step on the meta-objective for each iteration.",2. Related Work,[0],[0]
"However, different from these meta-learning approaches, our reweighting method does not have any additional hyperparameters and circumvents an expensive offline training stage.",2. Related Work,[0],[0]
"Hence, our method can work in an online fashion during regular training.",2. Related Work,[0],[0]
"In this section, we derive our model from a meta-learning objective towards an online approximation that can fit into any regular supervised training.",3. Learning to Reweight Examples,[0],[0]
We give a practical implementation suitable for any deep network type and provide theoretical guarantees under mild conditions that our algorithm has a convergence rate of O(1/ 2).,3. Learning to Reweight Examples,[0],[0]
Note that this is the same as that of stochastic gradient descent (SGD).,3. Learning to Reweight Examples,[0],[0]
"Let (x, y) be an input-target pair, and {(xi, yi), 1 ≤",3.1. From a meta-learning objective to an online approximation,[0],[0]
i ≤ N} be the training set.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"We assume that there is a small unbiased and clean validation set {(xvi , yvi ), 1 ≤ i ≤M}, and M N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Hereafter, we will use superscript v to denote validation set and subscript i to denote the ith data.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We also assume
that the training set contains the validation set; otherwise, we can always add this small validation set into the training set and leverage more information during training.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let Φ(x, θ) be our neural network model, and θ be the model parameters.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We consider a loss function C(ŷ, y) to minimize during training, where ŷ = Φ(x, θ).
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In standard training, we aim to minimize the expected loss for the training set: 1N ∑N i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"C(ŷi, yi) = 1 N ∑N i=1 fi(θ), where each input example is weighted equally, and fi(θ) stands for the loss function associating with data xi.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Here we aim to learn a reweighting of the inputs, where we minimize a weighted loss:
θ∗(w) = arg min θ N∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"wifi(θ), (1)
with wi unknown upon beginning.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Note that {wi}Ni=1 can be understood as training hyperparameters, and the optimal selection of w is based on its validation performance:
w∗ = arg min w,w≥0
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi,3.1. From a meta-learning objective to an online approximation,[0],[0]
(θ ∗(w)).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(2)
It is necessary that wi ≥ 0 for all i, since minimizing the negative training loss can usually result in unstable behavior.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Online approximation Calculating the optimal wi requires two nested loops of optimization, and every single loop can be very expensive.",3.1. From a meta-learning objective to an online approximation,[0],[0]
The motivation of our approach is to adapt online w through a single optimization loop.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"For each training iteration, we inspect the descent direction of some training examples locally on the training loss surface and reweight them according to their similarity to the descent direction of the validation loss surface.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"For most training of deep neural networks, SGD or its variants are used to optimize such loss functions.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"At every step t of training, a mini-batch of training examples {(xi, yi), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size, n N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
Then the parameters are adjusted according to the descent direction of the expected loss on the mini-batch.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let’s consider vanilla SGD:
θt+1 = θt − α∇
( 1
n n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi(θt)
) , (3)
where α is the step size.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
We want to understand what would be the impact of training example,3.1. From a meta-learning objective to an online approximation,[0],[0]
"i towards the performance of the validation set at training step t. Following a similar analysis to Koh & Liang (2017), we consider perturbing the weighting by i for each
training example in the mini- batch,
fi, (θ) = ifi(θ), (4)
θ̂t+1( ) = θt − α∇ n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi, (θ) ∣∣∣ θ=θt .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(5)
We can then look for the optimal ∗ that minimizes the validation loss fv locally at step t:
∗t = arg min
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi (θt+1( )).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(6)
Unfortunately, this can still be quite time-consuming.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To get a cheap estimate of wi at step t, we take a single gradient descent step on a mini-batch of validation samples wrt. t, and then rectify the output to get a non-negative weighting:
ui,t = −η ∂
∂",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t
1
m m∑ j=1 fvj (θt+1( )) ∣∣∣",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t=0 , (7)
w̃i,t = max(ui,t, 0).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(8)
where η is the descent step size on .
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To match the original training step size, in practice, we can consider normalizing the weights of all examples in a training batch so that they sum up to one.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In other words, we choose to have a hard constraint within the set {w : ‖w‖1 = 1} ∪ {0}.
wi,t = w̃i,t ( ∑ j w̃j,t) + δ",3.1. From a meta-learning objective to an online approximation,[0],[0]
"( ∑ j w̃j,t) , (9)
where δ(·) is to prevent the degenerate case when all wi’s in a mini-batch are zeros, i.e. δ(a) = 1 if a = 0, and equals to 0 otherwise.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Without the batch-normalization step, it is possible that the algorithm modifies its effective learning rate of the training progress, and our one-step look ahead may be too conservative in terms of the choice of learning rate (Wu et al., 2018).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Moreover, with batch normalization, we effectively cancel the meta learning rate parameter η.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In this section, we study how to compute wi,t in a multilayer perceptron (MLP) network.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
One of the core steps is to compute the gradients of the validation loss wrt.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"the local perturbation , We can consider a multi-layered network where we have parameters for each layer θ = {θl}Ll=1, and at every layer, we first compute zl the pre-activation, a weighted sum of inputs to the layer, and afterwards we apply a non-linear activation function σ to obtain z̃l the post-activation:
zl = θ > l z̃l−1, (10)
z̃l = σ(zl).",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(11)
During backpropagation, let gl be the gradients of loss wrt. zl, and the gradients wrt.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θl is given by z̃l−1g>l .,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"We can further express the gradients towards as a sum of local dot products.
∂ ∂",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t E [ fv(θt+1( )) ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t=0 ] ∝− 1
m m∑ j=1 ∂fvj (θ) ∂θ ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
>,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θ=θt ∂fi(θ) ∂θ ∣∣∣ θ,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"=θt
=− 1 m m∑ j=1 L∑ l=1",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(z̃vj,l−1 >z̃i,l−1)(g v j,l >gi,l).
",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(12)
Detailed derivations can be found in Supplementary Materials.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
Eq. 12 suggests that the meta-gradient on is composed of the sum of the products of two terms: z>zv and g>gv.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"The first dot product computes the similarity between the training and validation inputs to the layer, while the second computes the similarity between the training and validation gradient directions.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In other words, suppose that a pair of training and validation examples are very similar, and they also provide similar gradient directions, then this training example is helpful and should be up-weighted, and conversely, if they provide opposite gradient directions, this training example is harmful and should be downweighed.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In an MLP and a CNN, the unnormalized weights can be calculated based on the sum of the correlations of layerwise activation gradients and input activations.",3.3. Implementation using automatic differentiation,[0],[0]
"In more general networks, we can leverage automatic differentiation techniques to compute the gradient of the validation loss wrt.",3.3. Implementation using automatic differentiation,[0],[0]
the example weights of the current batch.,3.3. Implementation using automatic differentiation,[0],[0]
"As shown in Figure 1, to get the gradients of the example weights, one needs to first unroll the gradient graph of the training batch, and then use backward-on-backward automatic differentiation to take a second order gradient
pass (see Step 5 in Figure 1).",3.3. Implementation using automatic differentiation,[0],[0]
We list detailed step-bystep pseudo-code in Algorithm 1.,3.3. Implementation using automatic differentiation,[0],[0]
"This implementation can be generalized to any deep learning architectures and can be very easily implemented using popular deep learning frameworks such as TensorFlow (Abadi et al., 2016).
",3.3. Implementation using automatic differentiation,[0],[0]
"Algorithm 1 Learning to Reweight Examples using Automatic Differentiation Require: θ0, Df , Dg , n, m Ensure: θT
1: for t = 0 ...",3.3. Implementation using automatic differentiation,[0],[0]
T,3.3. Implementation using automatic differentiation,[0],[0]
"− 1 do 2: {Xf , yf} ← SampleMiniBatch(Df , n) 3: {Xg, yg} ← SampleMiniBatch(Dg , m) 4: ŷf ← Forward(Xf , yf , θt) 5: ← 0; lf ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"iC(yf,i, ŷf,i) 6: ∇θt ← BackwardAD(lf , θt) 7: θ̂t ← θt − α∇θt 8: ŷg",3.3. Implementation using automatic differentiation,[0],[0]
"← Forward(Xg, yg, θ̂t) 9: lg ← 1m ∑m i=1",3.3. Implementation using automatic differentiation,[0],[0]
"C(yg,i, ŷg,i)
10: ∇ ← BackwardAD(lg, ) 11:",3.3. Implementation using automatic differentiation,[0],[0]
"w̃ ← max(−∇ , 0); w ← w̃∑
j w̃+δ( ∑ j w̃)
12: l̂f ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"wiC(yi, ŷf,i) 13: ∇θt",3.3. Implementation using automatic differentiation,[0],[0]
"← BackwardAD(l̂f , θt) 14: θt+1 ← OptimizerStep(θt,∇θt) 15: end for
Training time Our automatic reweighting method will introduce a constant factor of overhead.",3.3. Implementation using automatic differentiation,[0],[0]
"First, it requires two full forward and backward passes of the network on training and validation respectively, and then another backward on backward pass (Step 5 in Figure 1), to get the gradients to the example weights, and finally a backward pass to minimize the reweighted objective.",3.3. Implementation using automatic differentiation,[0],[0]
"In modern networks, a backwardon-backward pass usually takes about the same time as a forward pass, and therefore compared to regular training, our method needs approximately 3× training time; it is also possible to reduce the batch size of the validation pass for speedup.",3.3. Implementation using automatic differentiation,[0],[0]
"We expect that it is worthwhile to spend the extra time to avoid the irritation of choosing early stopping, finetuning schedules, and other hyperparameters.",3.3. Implementation using automatic differentiation,[0],[0]
"Convergence results of SGD based optimization methods are well-known (Reddi et al., 2016).",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However it is still meaningful to establish a convergence result about our method since it involves optimization of two-level objectives (Eq. 1, 2) rather than one, and we further make some firstorder approximation by introducing Eq. 7.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Here, we show theoretically that our method converges to the critical point of the validation loss function under some mild conditions, and we also give its convergence rate.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"More detailed proofs can be found in the Supplementary Materials.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Definition 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"A function f(x) : Rd → R is said to be Lipschitz-smooth with constant L if
‖∇f(x)−∇f(y)‖ ≤",3.4. Analysis: convergence of the reweighted training,[0],[0]
"L‖x− y‖,∀x, y ∈ Rd.
Definition 2. f(x) has σ-bounded gradients if ‖∇f(x)‖ ≤ σ for all x ∈ Rd.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
"In most real-world cases, the high-quality validation set is really small, and thus we could set the mini-batch size m to be the same as the size of the validation set M .",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Under this condition, the following lemma shows that our algorithm always converges to a critical point of the validation loss.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However, our method is not equivalent to training a model only on this small validation set.",3.4. Analysis: convergence of the reweighted training,[0],[0]
Because directly training a model on a small validation set will lead to severe overfitting issues.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"On the contrary, our method can leverage useful information from a larger training set, and still converge to an appropriate distribution favored by this clean and balanced validation dataset.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"This helps both generalization and robustness to biases in the training set, which will be shown in our experiments.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Lemma 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose the validation loss function is Lipschitzsmooth with constant L, and the train loss function fi of training data xi have σ-bounded gradients.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Let the learning rate αt satisfies αt ≤ 2nLσ2 , where n is the training batch size.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Then, following our algorithm, the validation loss always monotonically decreases for any sequence of training batches, namely,
G(θt+1) ≤ G(θt), (13)
where G(θ) is the total validation loss
G(θ) = 1
M M∑ i=1",3.4. Analysis: convergence of the reweighted training,[0],[0]
fvi (θt+1( )).,3.4. Analysis: convergence of the reweighted training,[0],[0]
"(14)
Furthermore, in expectation, the equality in Eq. 13 holds only when the gradient of validation loss becomes 0 at some time step t, namely Et [G(θt+1)]",3.4. Analysis: convergence of the reweighted training,[0],[0]
"= G(θt) if and only if ∇G(θt) = 0, where the expectation is taking over possible training batches at time step t.
Moreover, we can prove the convergence rate of our method to be O(1/ 2).
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Theorem 2.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose G, fi and αt satisfy the aforementioned conditions, then Algorithm 1 achieves E",3.4. Analysis: convergence of the reweighted training,[0],[0]
[ ‖∇G(θt)‖2 ] ≤ in O(1/ 2) steps.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"More specifically,
min 0<t<T
E [ ‖∇G(θt)‖2 ] ≤ C√
T , (15)
where C is some constant independent of the convergence process.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"To test the effectiveness of our reweighting algorithm, we designed both class imbalance and noisy label settings, and a combination of both, on standard MNIST and CIFAR benchmarks for image classification using deep CNNs.",4. Experiments,[0],[0]
We use the standard MNIST handwritten digit classification dataset and subsample the dataset to generate a class imbalance binary classification task.,4.1. MNIST data imbalance experiments,[0],[0]
"We select a total of 5,000 images of size 28×28 on class 4 and 9, where 9 dominates the training data distribution.",4.1. MNIST data imbalance experiments,[0],[0]
We train a standard LeNet on this task and we compare our method with a suite of commonly used tricks for class imbalance: 1) PROPORTION weights each example by the inverse frequency 2),4.1. MNIST data imbalance experiments,[0],[0]
"RESAMPLE samples a class-balanced minibatch for each iteration 3) HARD MINING selects the highest loss examples from the majority class and 4) RANDOM is a random example weight baseline that assigns weights based on a rectified Gaussian distribution:
wrndi = max(zi, 0)∑",4.1. MNIST data imbalance experiments,[0],[0]
"i max(zi, 0) , where zi ∼ N (0, 1).",4.1. MNIST data imbalance experiments,[0],[0]
"(16)
To make sure that our method does not have the privilege of training on more data, we split the balanced validation set of 10 images directly from the training set.",4.1. MNIST data imbalance experiments,[0],[0]
"The network is trained with SGD with a learning rate of 1e-3 and mini-batch size of 100 for a total of 8,000 steps.
",4.1. MNIST data imbalance experiments,[0],[0]
Figure 2 plots the test error rate across various imbalance ratios averaged from 10 runs with random splits.,4.1. MNIST data imbalance experiments,[0],[0]
Note that our method significantly outperforms all the baselines.,4.1. MNIST data imbalance experiments,[0],[0]
"With class imbalance ratio of 200:1, our method only reports a small increase of error rate around 2%, whereas other methods suffer terribly under this setting.",4.1. MNIST data imbalance experiments,[0],[0]
"Compared with resampling and hard negative mining baselines, our approach does not throw away samples based on its class or training loss - as long as a sample is helpful towards the validation loss, it will be included as a part of the training loss.",4.1. MNIST data imbalance experiments,[0],[0]
Reweighting algorithm can also be useful on datasets where the labels are noisy.,4.2. CIFAR noisy label experiments,[0],[0]
"We study two settings of label noise here:
• UNIFORMFLIP: All label classes can uniformly flip to any other label classes, which is the most studied in the literature.",4.2. CIFAR noisy label experiments,[0],[0]
• BACKGROUNDFLIP:,4.2. CIFAR noisy label experiments,[0],[0]
All label classes can flip to a single background class.,4.2. CIFAR noisy label experiments,[0],[0]
This noise setting is very realistic.,4.2. CIFAR noisy label experiments,[0],[0]
"For instance, human annotators may not have recognized all the positive instances, while the
rest remain in the background class.",4.2. CIFAR noisy label experiments,[0],[0]
"This is also a combination of label imbalance and label noise since the background class usually dominates the label distribution.
",4.2. CIFAR noisy label experiments,[0],[0]
"We compare our method with prior work on the noisy label problem.
",4.2. CIFAR noisy label experiments,[0],[0]
"• REED, proposed by Reed et al. (2014), is a bootstrapping technique where the training target is a convex combination of the model prediction and the label.
",4.2. CIFAR noisy label experiments,[0],[0]
"• S-MODEL, proposed by Goldberger & Ben-Reuven (2017), adds a fully connected softmax layer after the regular classification output layer to model the noise transition matrix.
",4.2. CIFAR noisy label experiments,[0],[0]
"• MENTORNET, proposed by Jiang et al. (2017), is an RNN-based meta-learning model that takes in a sequence of loss values and outputs the example weights.",4.2. CIFAR noisy label experiments,[0],[0]
"We compare numbers reported in their paper with a base model that achieves similar test accuracy under 0% noise.
",4.2. CIFAR noisy label experiments,[0],[0]
"In addition, we propose two simple baselines: 1) RANDOM, which assigns weights according to a rectified Gaussian (see Eq. 16); 2) WEIGHTED, designed for BACKGROUNDFLIP, where the model knows the oracle noise ratio for each class and reweights the training loss proportional to the percentage of clean images of that label class.
",4.2. CIFAR noisy label experiments,[0],[0]
"Clean validation set For UNIFORMFLIP, we use 1,000 clean images in the validation set; for BACKGROUNDFLIP, we use 10 clean images per label class.",4.2. CIFAR noisy label experiments,[0],[0]
"Since our method uses information from the clean validation, for a fair comparison, we conduct an additional finetuning on the clean data based on the pre-trained baselines.",4.2. CIFAR noisy label experiments,[0],[0]
"We also study the effect on the size of the clean validation set in an ablation study.
",4.2. CIFAR noisy label experiments,[0],[0]
"Hyper-validation set For monitoring training progress and tuning baseline hyperparameters, we split out another
5,000 hyper-validation set from the 50,000 training images.",4.2. CIFAR noisy label experiments,[0],[0]
"We also corrupt the hyper-validation set with the same noise type.
",4.2. CIFAR noisy label experiments,[0],[0]
"Experimental details For REED model, we use the best β reported in Reed et al. (2014) (β = 0.8 for hard bootstrapping and β = 0.95 for soft bootstrapping).",4.2. CIFAR noisy label experiments,[0],[0]
"For the S-MODEL, we explore two versions to initialize the transition weights: 1) a smoothed identity matrix; 2) in background flip experiments we consider initializing the transition matrix with the confusion matrix of a pre-trained baseline model (S-MODEL +CONF).",4.2. CIFAR noisy label experiments,[0],[0]
"We find baselines can easily overfit the training noise, and therefore we also study early stopped versions of the baselines to provide a stronger comparison.",4.2. CIFAR noisy label experiments,[0],[0]
"In contrast, we find early stopping not necessary for our method.
",4.2. CIFAR noisy label experiments,[0],[0]
"To make our results comparable with the ones reported in MENTORNET and to save computation time, we exchange their Wide ResNet-101-10 with a Wide ResNet28-10 (WRN-28-10) (Zagoruyko & Komodakis, 2016) with dropout 0.3 as our base model in the UNIFORMFLIP experiments.",4.2. CIFAR noisy label experiments,[0],[0]
We find that test accuracy differences between the two base models are within 0.5% on CIFAR datasets under 0% noise.,4.2. CIFAR noisy label experiments,[0],[0]
"In the BACKGROUNDFLIP experiments, we use a ResNet-32 (He et al., 2016) as our base model.
",4.2. CIFAR noisy label experiments,[0],[0]
"We train the models with SGD with momentum, at an initial learning rate 0.1 and a momentum 0.9 with mini-batch size 100.",4.2. CIFAR noisy label experiments,[0],[0]
"For ResNet-32 models, the learning rate decays×0.1 at 40K and 60K steps, for a total of 80K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"For WRN and early stopped versions of ResNet-32 models, the learning rate decays at 40K and 50K steps, for a total of 60K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"Under regular 0% noise settings, our base ResNet-32 gets 92.5% and 68.1% classification accuracy on CIFAR-10 and 100, and the WRN-28-10 gets 95.5% and 78.2%.",4.2. CIFAR noisy label experiments,[0],[0]
"For the finetuning stage, we run extra 5K steps of training on the
CLEAN ONLY 15.90 ± 3.32 8.06 ± 0.76 BASELINE +FT 82.82 ± 0.93 54.23 ± 1.75 BASELINE +ES +FT 85.19 ± 0.46 55.22 ± 1.40 WEIGHTED +FT 85.98 ± 0.47 53.99 ± 1.62 S-MODEL +CONF +FT 81.90 ± 0.85 53.11 ± 1.33 S-MODEL +CONF +ES +FT 85.86 ± 0.63 55.75 ± 1.26
OURS 86.73 ± 0.48 59.30 ± 0.60
limited clean data.
",4.2. CIFAR noisy label experiments,[0],[0]
"We report the average test accuracy for 5 different random splits of clean and noisy labels, with 95% confidence interval in Table 1 and 2.",4.2. CIFAR noisy label experiments,[0],[0]
"The background classes for the 5 trials are [0, 1, 3, 5, 7] (CIFAR-10) and [7, 12, 41, 62, 85] (CIFAR-100).",4.2. CIFAR noisy label experiments,[0],[0]
"The first result that draws our attention is that “Random” performs surprisingly well on the UNIFORMFLIP benchmark, outperforming all historical methods that we compared.",4.3. Results and Discussion,[0],[0]
"Given that its performance is comparable with Baseline on BACKGROUNDFLIP and MNIST class imbalance, we hypothesize that random example weights act as a strong regularizer and under which the learning objective on UNIFORMFLIP is still consistent.
",4.3. Results and Discussion,[0],[0]
"Regardless of the strong baseline, our method ranks the top on both UNIFORMFLIP and BACKGROUNDFLIP, showing our method is less affected by the changes in the noise type.",4.3. Results and Discussion,[0],[0]
"On CIFAR-100, our method wins more than 3% compared to the state-of-the-art method.
",4.3. Results and Discussion,[0],[0]
Understanding the reweighting mechanism It is beneficial to understand how our reweighting algorithm contributes to learning more robust models during training.,4.3. Results and Discussion,[0],[0]
"First, we use a pre-trained model (trained at half of the total iterations without learning rate decay) and measure the example weight distribution of a randomly sampled batch of validation images, which the model has never seen.",4.3. Results and Discussion,[0],[0]
"As shown in the left figure of Figure 3, our model correctly
pushes most noisy images to zero weights.",4.3. Results and Discussion,[0],[0]
"Secondly, we conditioned the input mini-batch to be a single nonbackground class and randomly flip 40% of the images to the background, and we would like to see how well our model can distinguish clean and noisy images.",4.3. Results and Discussion,[0],[0]
"As shown in Figure 3 right, the model is able to reliably detect images that are flipped to the background class.
",4.3. Results and Discussion,[0],[0]
"Robustness to overfitting noise Throughout experimentation, we find baseline models can easily overfit to the noise in the training set.",4.3. Results and Discussion,[0],[0]
"For example, shown in Table 2, applying early stopping (“ES”) helps the classification performance of “S-Model” by over 10% on CIFAR-10.",4.3. Results and Discussion,[0],[0]
"Figure 6 compares the final confusion matrices of the baseline and the proposed algorithm, where a large proportion of noise transition probability is cleared in the final prediction.",4.3. Results and Discussion,[0],[0]
Figure 7 shows training curves on the BACKGROUNDFLIP experiments.,4.3. Results and Discussion,[0],[0]
"After the first learning rate decay, both “Baseline” and “SModel” quickly degrade their validation performance due to overfitting, while our model remains the same validation accuracy until termination.",4.3. Results and Discussion,[0],[0]
"Note that here “S-Model” knows the oracle noise ratio in each class, and this information is
not available in our method.
",4.3. Results and Discussion,[0],[0]
Impact of the noise level We would like to investigate how strongly our method can perform on a variety of noise levels.,4.3. Results and Discussion,[0],[0]
"Shown in Figure 5, our method only drops 6% accuracy when the noise ratio increased from 0% to 50%;
whereas the baseline has dropped more than 40%.",4.3. Results and Discussion,[0],[0]
"At 0% noise, our method only slightly underperforms baseline.",4.3. Results and Discussion,[0],[0]
"This is reasonable since we are optimizing on the validation set, which is strictly a subset of the full training set, and therefore suffers from its own subsample bias.
",4.3. Results and Discussion,[0],[0]
"Size of the clean validation set When the size of the clean validation set grows larger, fine-tuning on the validation set will be a reasonble approach.",4.3. Results and Discussion,[0],[0]
"Here, we make an attempt to explore the tradeoff and understand when fine-tuning becomes beneficial.",4.3. Results and Discussion,[0],[0]
Figure 4 plots the classification performance when we varied the size of the clean validation on BACKGROUNDFLIP.,4.3. Results and Discussion,[0],[0]
"Surprisingly, using 15 validation images for all classes only results in a 2% drop in performance, and the overall classification performance does not grow after having more than 100 validation images.",4.3. Results and Discussion,[0],[0]
"In comparison, we observe a significant drop in performance when only fine-tuning on these 15 validation images for the baselines, and the performance catches up around using 1,000 validation images (100 per class).",4.3. Results and Discussion,[0],[0]
"This phenomenon suggests that in our method the clean validation acts more like a regularizer rather than a data source for parameter finetuning, and potentially our method can be complementary with fine-tuning based method when the size of the clean set grows larger.",4.3. Results and Discussion,[0],[0]
"In this work, we propose an online meta-learning algorithm for reweighting training examples and training more robust deep learning models.",5. Conclusion,[0],[0]
"While various types of training set biases exist and manually designed reweighting objectives have their own bias, our automatic reweighting algorithm shows superior performance dealing with class imbalance, noisy labels, and both.",5. Conclusion,[0],[0]
Our method can be directly applied to any deep learning architecture and is expected to train end-to-end without any additional hyperparameter search.,5. Conclusion,[0],[0]
"Validating on every training step is a novel setting and we show that it has links with model regularization, which can be a fruitful future research direction.",5. Conclusion,[0],[0]
Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns.,abstractText,[0],[0]
"However, they can also easily overfit to training set biases and label noises.",abstractText,[0],[0]
"In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters.",abstractText,[0],[0]
"In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions.",abstractText,[0],[0]
"To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set.",abstractText,[0],[0]
"Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",abstractText,[0],[0]
Learning to Reweight Examples for Robust Deep Learning,title,[0],[0]
"Many natural language processing (NLP) and computer vision problems necessitate predicting structured outputs such as labeled sequences, trees or general graphs (Smith, 2010; Nowozin & Lampert, 2011).",1. Introduction,[0],[0]
Such tasks require modeling both input-output relationships and the interactions between predicted outputs to capture correlations.,1. Introduction,[0],[0]
"Across the various structured prediction formulations (Lafferty et al., 2001; Taskar et al., 2003; Chang et al., 2012), prediction requires solving inference problems by searching for scoremaximizing output structures.",1. Introduction,[0],[0]
"The search space for inference is typically large (e.g., all parse trees), and grows with input size.",1. Introduction,[0],[0]
"Exhaustive search can be prohibitive and standard alternatives are either: (a) perform exact inference with a large computational cost or, (b) approximate inference to sacrifice accuracy in favor of time.
",1. Introduction,[0],[0]
"1School of Computing, University of Utah, Salt Lake City, Utah, USA.",1. Introduction,[0],[0]
"Correspondence to: Xingyuan Pan <xpan@cs.utah.edu>, Vivek Srikumar <svivek@cs.utah.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we focus on the computational cost of inference.",1. Introduction,[0],[0]
"We argue that naturally occurring problems have remarkable regularities across both inputs and outputs, and traditional formulations of inference ignore them.",1. Introduction,[0],[0]
"For example, parsing an n-word sentence will cost a standard head-driven lexical parser O(n5) time.",1. Introduction,[0],[0]
Current practice in NLP is to treat each new sentence as a fresh discrete optimization problem and pay the computational price each time.,1. Introduction,[0],[0]
"However, this practice is not only expensive, but also wasteful!",1. Introduction,[0],[0]
"We ignore the fact that slight changes to inputs often do not change the output, or even the sequence of steps taken to produce it.",1. Introduction,[0],[0]
"Moreover, not all outputs are linguistically meaningful structures; as we make more predictions, we should be able to learn to prune the output space.
",1. Introduction,[0],[0]
The motivating question that drives our work is: Can we design inference schemes that learn to make a trained structured predictor faster without sacrificing output quality?,1. Introduction,[0],[0]
"After training, the structured classifier can be thought as a black-box.",1. Introduction,[0],[0]
"Typically, once deployed, it is never modified over its lifetime of classifying new examples.",1. Introduction,[0],[0]
"Subsequently, we can view each prediction of the black-box classifier as an opportunity to learn how to navigate the output space more efficiently.",1. Introduction,[0],[0]
"Thus, if the classifier sees a previously encountered situation, it could make some decisions without needless computations.
",1. Introduction,[0],[0]
We formalize this intuition by considering the trained models as solving arbitrary integer linear programs (ILPs) for combinatorial inference.,1. Introduction,[0],[0]
"We train a second, inexpensive speedup classifier which acts as a heuristic for a searchbased inference algorithm that mimics the more expensive black-box classifier.",1. Introduction,[0],[0]
The speedup heuristic is a function that learns regularities among predicted structures.,1. Introduction,[0],[0]
"We present a mistake bound algorithm that, over the classifier’s lifetime, learns to navigate the feasible regions of the ILPs.",1. Introduction,[0],[0]
"By doing so, we can achieve a reduction in inference time.
",1. Introduction,[0],[0]
We further identify inference situations where the learned speedup heuristic alone can correctly label parts of the outputs without computing the corresponding input features.,1. Introduction,[0],[0]
"In such situations, the search algorithm can safely ignore parts of inputs if the corresponding outputs can be decided based on the sub-structures constructed so far.",1. Introduction,[0],[0]
"Seen this way, the speedup classifier can be seen as a statistical cache of past decisions made by the black-box classifier.
",1. Introduction,[0],[0]
We instantiate our strategy to the task of predicting entities and relations from sentences.,1. Introduction,[0],[0]
"Using an ILP based black-box classifier, we show that the trained speedup classifier mimics the reference inference algorithm to obtain improvements in running time, and also recovers its accuracy.",1. Introduction,[0],[0]
"Indeed, by learning to ignore input components when they will not change the prediction, we show that learned search strategy outperforms even greedy search in terms of speed.
",1. Introduction,[0],[0]
"To summarize, the main contribution of this paper is the formalization of the problem of learning to make structured output classifiers faster without sacrificing accuracy.",1. Introduction,[0],[0]
We develop a learning-to-search framework to train a speedup classifier with a mistake-bound guarantee and a sufficient condition to safely avoid computing input-based features.,1. Introduction,[0],[0]
"We show empirically on an entity-relation extraction task that we can learn a speedup classifier that is (a) faster than both the state-of-the-art Gurobi optimizer and greedy search, and (b) does not incur a loss in output quality.",1. Introduction,[0],[0]
"First, we will define the notation used in this paper with a running example that requires of identifying entity types and their relationships in text.",2. Notation and Preliminaries,[0],[0]
"The input to the problem consists of sentences such as:
Colin went back home in Ordon Village.
",2. Notation and Preliminaries,[0],[0]
"These inputs are typically preprocessed — here, we are given spans of text (underlined) corresponding to entities.",2. Notation and Preliminaries,[0],[0]
"We will denote such preprocessed inputs to the structured prediction problem as x.
We seek to produce a structure y ∈",2. Notation and Preliminaries,[0],[0]
"Yx (e.g., labeled trees, graphs) associated with these inputs.",2. Notation and Preliminaries,[0],[0]
"Here, Yx is the set of all possible output structures for the input x.",2. Notation and Preliminaries,[0],[0]
"In the example problem, our goal is to assign types to the entities and also label the relationships between them.",2. Notation and Preliminaries,[0],[0]
"Suppose our task has three types of entities: person, location and organization.",2. Notation and Preliminaries,[0],[0]
"A pair of entities can participate in one of five possible directed relations: Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",2. Notation and Preliminaries,[0],[0]
"Additionally, there is a special entity label NoEnt meaning a text span is not an entity, and a special relation label NoRel indicating that two spans are unrelated.",2. Notation and Preliminaries,[0],[0]
"Figure 1 shows a plausible structure for the example sentence as per this scheme.
",2. Notation and Preliminaries,[0],[0]
A standard way to model the prediction problem requires learning a model that scores all structures in Yx and searching for the score-maximizing structure.,2. Notation and Preliminaries,[0],[0]
"Linear models are commonly used as scoring functions, and require a feature vector characterizing input-output relationships Φ (x,y).",2. Notation and Preliminaries,[0],[0]
We will represent the model by a weight vector α.,2. Notation and Preliminaries,[0],[0]
"Every structure y associated with an input x is scored as the dot product α · Φ (x,y).",2. Notation and Preliminaries,[0],[0]
"The goal of prediction is to find the
structure y∗ that maximizes this score.",2. Notation and Preliminaries,[0],[0]
"That is,
y∗ = arg max y∈Yx α ·",2. Notation and Preliminaries,[0],[0]
"Φ (x,y) .",2. Notation and Preliminaries,[0],[0]
"(1)
Learning involves using training data to find the best weight vector α.
",2. Notation and Preliminaries,[0],[0]
"In general, the output structure y is a set of K categorical inference variables {y1, y2, · · · , yK} , each of which can take a value from a predefined set of n labels.",2. Notation and Preliminaries,[0],[0]
"That is, each yk ∈ y takes a value from {l1, l2, · · · , ln}.1 In our running example, the inference variables correspond to the four decisions that define the structure: the labels for the two entities, and the relations in each direction.",2. Notation and Preliminaries,[0],[0]
"The feature function Φ decomposes into a sum of features over each yk, each denoted by Φk, giving us the inference problem:
y∗ = arg max y∈Yx K∑",2. Notation and Preliminaries,[0],[0]
k=1 α ·,2. Notation and Preliminaries,[0],[0]
"Φk ( x, yk ) .",2. Notation and Preliminaries,[0],[0]
"(2)
The dependencies between the yk’s specify the nature of the output space.",2. Notation and Preliminaries,[0],[0]
Determining each yk in isolation greedily does not typically represent a viable inference strategy because constraints connecting the variables are ignored.,2. Notation and Preliminaries,[0],[0]
"In this spirit, the problem of finding the best structure can be viewed as a combinatorial optimization problem.
",2. Notation and Preliminaries,[0],[0]
"In this paper, we consider the scenario in which we have already trained a model α.",2. Notation and Preliminaries,[0],[0]
"We focus on solving the inference problem (i.e.,Eq.",2. Notation and Preliminaries,[0],[0]
(2)) efficiently.,2. Notation and Preliminaries,[0],[0]
We conjecture that it should be possible to observe a black-box inference algorithm over its lifetime to learn to predict faster without losing accuracy.,2. Notation and Preliminaries,[0],[0]
One common way to solve inference is by designing efficient dynamic programming algorithms that exploit problem structure.,2.1. Black-box Inference Mechanisms,[0],[0]
"While effective, this approach is limited to special cases where the problem admits efficient decoding, thus placing restrictions on factorization and feature design.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this paper, we seek to reason about the problem of predicting structures in the general case.",2.1. Black-box Inference Mechanisms,[0],[0]
"Since inference is essentially a combinatorial optimization problem, without loss
1We make this choice for simplicity of notation.",2.1. Black-box Inference Mechanisms,[0],[0]
"In general, K depends on the size of the input x, and categorical variables may take values from different label sets.
of generality, we can represent any inference problem as an integer linear programming (ILP) instance (Schrijver, 1998).",2.1. Black-box Inference Mechanisms,[0],[0]
To represent the inference task in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
"(2) as an ILP instance, we will define indicator variables of the form zki ∈ {0, 1}, which stands for the decision that the categorical variable yk is assigned the ith label among the n labels.",2.1. Black-box Inference Mechanisms,[0],[0]
"That is, zki = 1 if yk = li, and 0 otherwise.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using this notation, we can write the cost of any structure y in terms of the indicators as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
cki,2.1. Black-box Inference Mechanisms,[0],[0]
z,2.1. Black-box Inference Mechanisms,[0],[0]
k,2.1. Black-box Inference Mechanisms,[0],[0]
i .,2.1. Black-box Inference Mechanisms,[0],[0]
"(3)
Here, cki is a stand in for −α ·",2.1. Black-box Inference Mechanisms,[0],[0]
"Φk (x, li), namely the cost (negative score) associated with this decision.2",2.1. Black-box Inference Mechanisms,[0],[0]
"In our example, suppose the first categorical variable y1 corresponds to the entity Colin, and it has possible labels {person,location, . . .",2.1. Black-box Inference Mechanisms,[0],[0]
}.,2.1. Black-box Inference Mechanisms,[0],[0]
"Then, assigning person to Colin would correspond to setting z11 = 1, and z 1 i = 0 for all i 6= 1.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using the labels enumerated in §2, there will be 20 indicators for the four categorical decisions.
",2.1. Black-box Inference Mechanisms,[0],[0]
"Of course, arbitrary assignments to the indicators is not allowed.",2.1. Black-box Inference Mechanisms,[0],[0]
We can define the set of feasible structures using linear constraints.,2.1. Black-box Inference Mechanisms,[0],[0]
"Clearly, each categorical variable can take exactly one label, which can be expressed via:
n∑ i=1 zki = 1, for all k. (4)
",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, we can define the set of valid structures Yx using a collection of m linear constraints, the jth one of which can be written as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
"Akjiz k i = bj , for all j. (5)
",2.1. Black-box Inference Mechanisms,[0],[0]
These structural constraints characterize the interactions between the categorical variables.,2.1. Black-box Inference Mechanisms,[0],[0]
"For example, if a directed edge in our running example is labeled as LiveIn, then, its source and target must be a person and a location respectively.",2.1. Black-box Inference Mechanisms,[0],[0]
"While Eq.(5) only shows equality constraints, in practice, inequality constraints can also be included.
",2.1. Black-box Inference Mechanisms,[0],[0]
The inference problem in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(2) is equivalent to the problem of minimizing the objective in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(3) over the 0-1 indicator variables subject to the constraints in Eqs.,2.1. Black-box Inference Mechanisms,[0],[0]
"(4) and (5).
",2.1. Black-box Inference Mechanisms,[0],[0]
We should note the difference between the ability to write an inference problem as an ILP instance and actually solving it as one.,2.1. Black-box Inference Mechanisms,[0],[0]
"The former gives us the ability to reason about inference in general, and perhaps using other methods (such as Lagrangian relaxation (Lemaréchal, 2001)) for inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"However, solving problems with industrial strength ILP
2The negation defines an equivalent minimization problem and makes subsequent description of the search framework easier.
solvers such as the Gurobi solver3 is competitive with other approaches in terms of inference time, even though they may not directly exploit problem structure.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this work, we use the general structure of the ILP inference formulation to develop the theory for speeding up inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, because of its general applicability and fast inference speed, we use the Gurobi ILP solver as our black-box classifier, and learn a speedup heuristic to make even faster inference.",2.1. Black-box Inference Mechanisms,[0],[0]
Directly applying the black-box solver for the large output spaces may be impractical.,2.2. Inference as Search,[0],[0]
An alternative general purpose strategy for inference involves framing the maximization in Eq.,2.2. Inference as Search,[0],[0]
"(2) as a graph search problem.
",2.2. Inference as Search,[0],[0]
"Following Russell & Norvig (2003); Xu et al. (2009), a general graph search problem requires defining an initial search node I , a successor function s(·), and a goal test.",2.2. Inference as Search,[0],[0]
The successor function s(·) maps a search node to its successors.,2.2. Inference as Search,[0],[0]
The goal test determines whether a node is a goal node.,2.2. Inference as Search,[0],[0]
"Usually, each search step is associated with a cost function, and we seek to find a goal node with the least total cost.
",2.2. Inference as Search,[0],[0]
We can define the search problem corresponding to inference as follows.,2.2. Inference as Search,[0],[0]
"We will denote a generic search node in the graph as v, which corresponds to a set of partially assigned categorical variables.",2.2. Inference as Search,[0],[0]
"Specifically, we will define the search node v as a set of pairs {(k, i)}, each element of which specifies that the variable yk is assigned the ith label.",2.2. Inference as Search,[0],[0]
The initial search node I is the empty set since none of the variables has been assigned when the search begins.,2.2. Inference as Search,[0],[0]
"For a node v, its successors s(v) is a set of nodes, each containing one more assigned variable than v. A node is a goal node if all variables yk’s have been assigned.",2.2. Inference as Search,[0],[0]
"The size of any goal node is K, the number of categorical variables.
",2.2. Inference as Search,[0],[0]
"In our running example, at the start of search, we may choose to assign the first label l1 (person) to the variable y1 – the entity Colin – leading us to the successor {(1, 1)}.",2.2. Inference as Search,[0],[0]
Every search node specifies a partial or a full assignment to all the entities and relations.,2.2. Inference as Search,[0],[0]
"The goal test simply checks if we arrive at a full assignment, i.e., all the entity and relation candidates have been assigned a label.
",2.2. Inference as Search,[0],[0]
"Note that goal test does not test the quality of the node, it simply tests whether the search process is finished.",2.2. Inference as Search,[0],[0]
"The quality of the goal node is determined by the path cost from the initial node to the goal node, which is the accumulated cost of each step along the way.",2.2. Inference as Search,[0],[0]
The step cost for assigning label li to a variable yk is the same cki we defined for the ILP objective in Eq.,2.2. Inference as Search,[0],[0]
(3).,2.2. Inference as Search,[0],[0]
"Finding a shortest path in such a search space is equivalent to the original ILP problem
3http://www.gurobi.com
without the structural constraints in Eq.",2.2. Inference as Search,[0],[0]
(5).,2.2. Inference as Search,[0],[0]
The uniquelabel constraints in Eq.,2.2. Inference as Search,[0],[0]
"(4) are automatically satisfied by our formulation of the search process.
",2.2. Inference as Search,[0],[0]
"Indeed, solving inference without the constraints in Eq.(5) is trivial.",2.2. Inference as Search,[0],[0]
"For each categorical variable yk, we can pick the label li that has the lowest value of cki .",2.2. Inference as Search,[0],[0]
"This gives us two possible options for solving inference as search: We can (a) ignore the constraints that make inference slow to greedily predict all the labels, or, (b) enforce constraints at each step of the search, and only consider search nodes that satisfy all constraints.",2.2. Inference as Search,[0],[0]
"The first option is fast, but can give us outputs that are invalid.",2.2. Inference as Search,[0],[0]
"For example, we might get a structure that mandates that the person Colin lives in a person called Ordon Village.",2.2. Inference as Search,[0],[0]
"The second option will give us structurally valid outputs, but can be prohibitively slow.
",2.2. Inference as Search,[0],[0]
Various graph search algorithms can be used for performing inference.,2.2. Inference as Search,[0],[0]
"For efficiency, we can use beam search with a fixed beam width b.",2.2. Inference as Search,[0],[0]
When search begins the beam B0 contains only the initial node B0 =,2.2. Inference as Search,[0],[0]
[I].,2.2. Inference as Search,[0],[0]
"Following Collins & Roark (2004); Xu et al. (2009), we define the function BreadthExpand which takes the beam Bt at step t and generates the candidates Ct+1 for the next beam:
Ct+1 = BreadthExpand(Bt) = ∪v∈Bts(v)
The next beam is given by Bt+1 = Filter(Ct+1), where Filter takes top b nodes according to some priority function p(v).",2.2. Inference as Search,[0],[0]
"In the simplest case, the priority of a node v is the total path cost of reaching that node.",2.2. Inference as Search,[0],[0]
"More generally, the priority function can be informed not only by the path cost, but also by a heuristic function as in the popular A∗ algorithm.",2.2. Inference as Search,[0],[0]
"In the previous section, we saw that using a black-box ILP solver may be slower than greedy search which ignores constraints, but produces valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"However, over its lifetime, a trained classifier predicts structures for a large number of inputs.",3. Speeding up Structured Prediction,[0],[0]
"While the number of unique inputs (e.g. sentences) may be large, the number of unique structures that actually occur among the predictions is not only finite, but also small.",3. Speeding up Structured Prediction,[0],[0]
"This observation was exploited by Srikumar et al. (2012); Kundu et al. (2013) for amortizing inference costs.
",3. Speeding up Structured Prediction,[0],[0]
"In this paper, we are driven by the need for an inference algorithm that learns regularities across outputs to become faster at producing structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"In order to do so, we will develop an inference-as-search scheme that inherits the speed of greedy search, but learns to produce structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"Before developing the algorithmic aspects of such an inference scheme, let us first see a proofof-concept for such a scheme.",3. Speeding up Structured Prediction,[0],[0]
Our goal is to incorporate the structural constraints from Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(5) as a heuristic for greedy or beam search.,3.1. Heuristics for Structural Validity,[0],[0]
"To do so, at each step during search, we need to estimate how likely an assignment can lead to a constraint violation.",3.1. Heuristics for Structural Validity,[0],[0]
"This information can be characterized by using a heuristic function h(v), which will be used to evaluated a node v during search.
",3.1. Heuristics for Structural Validity,[0],[0]
The dual form the ILP in Eqs.,3.1. Heuristics for Structural Validity,[0],[0]
(3) to (5) help justify the idea of capturing constraint information using a heuristic function.,3.1. Heuristics for Structural Validity,[0],[0]
We treat the unique label constraints in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
"(4) as defining the domain in which each 0-1 variable zki lives, and the only real constraints are given by Eq. (5).
Let uj represent the dual variable for the jth constraint.",3.1. Heuristics for Structural Validity,[0],[0]
"Thus, we obtain the Lagrangian4
L(z, u) =",3.1. Heuristics for Structural Validity,[0],[0]
K∑,3.1. Heuristics for Structural Validity,[0],[0]
k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
cki z,3.1. Heuristics for Structural Validity,[0],[0]
k i,3.1. Heuristics for Structural Validity,[0],[0]
− m∑ j=1 uj (,3.1. Heuristics for Structural Validity,[0],[0]
K∑ k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
Akjiz k i,3.1. Heuristics for Structural Validity,[0],[0]
"− bj )
",3.1. Heuristics for Structural Validity,[0],[0]
"= ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j ujA k ji  zki +∑ j bjuj
",3.1. Heuristics for Structural Validity,[0],[0]
"The dual function θ(u) = minz L(z, u), where the minimization is over the domain of the z variables.
",3.1. Heuristics for Structural Validity,[0],[0]
Denote u∗ = arg max θ(u) as the solution to the dual problem.,3.1. Heuristics for Structural Validity,[0],[0]
"In the case of zero duality gap, the theory of Lagrangian relaxation (Lemaréchal, 2001) tells us that solving the following relaxed minimization problem will solve the original ILP:
min ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j u∗jA k ji  zki (6)∑ i zki = 1, for all k (7) zki ∈ {0, 1}, for all k, i (8)
This new optimization problem does not have any structural constraints and can be solved greedily for each k if we know the optimal dual variables u∗.
To formulate the minimization in Eqs (6) to (8) as a search problem, we define the priority function p(v) for ranking the nodes as p(v) = g(v) + h∗(v), where the path cost g(v) and heuristic function h∗(v) are given by
g(v) = ∑
(k,i)∈v
cki , (9)
h∗(v) =",3.1. Heuristics for Structural Validity,[0],[0]
"− ∑
(k,i)∈v ∑ j Akjiu ∗ j (x).",3.1. Heuristics for Structural Validity,[0],[0]
"(10)
Since Eq. (6) is a minimization problem, smaller priority value p(v) means higher ranking during search.",3.1. Heuristics for Structural Validity,[0],[0]
"Note that
4We omit the ranges of the summation indices",3.1. Heuristics for Structural Validity,[0],[0]
"i, j, k hereafter.
even though heuristic function defined in this way is not always admissible, greedy search with ranking function p(v) will lead to the exact solution of Eqs.",3.1. Heuristics for Structural Validity,[0],[0]
(6) to (8).,3.1. Heuristics for Structural Validity,[0],[0]
"In practice, however, we do not have the optimal values for the dual variables u∗.",3.1. Heuristics for Structural Validity,[0],[0]
"Indeed, when Lagrangian relaxation is used for inference, the optmial dual variables are computed using subgradient optimization for each example because their value depends on the original input via the c’s.
",3.1. Heuristics for Structural Validity,[0],[0]
"Instead of performing expensive gradient based optimization for every input instance, we will approximate the heuristic function as a classifier that learns to prioritize structurally valid outputs.",3.1. Heuristics for Structural Validity,[0],[0]
"In this paper, we use a linear model based on a weight vector w to approximate the heuristic as
h(v) = −w · φ(v) (11)
",3.1. Heuristics for Structural Validity,[0],[0]
"For an appropriate choice of node features φ(v), the heuristic h(v) in Eq.(10) is indeed a linear function.5",3.1. Heuristics for Structural Validity,[0],[0]
"In other words, there exists a linear heuristic function that can guide graph search towards creating structurally valid outputs.
",3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the priority function p(v) for each node is determined by two components: the path cost g(v) from the initial node to the current node, and the learned heuristic cost h(v), which is an estimate of how good the current node is.",3.1. Heuristics for Structural Validity,[0],[0]
"Because the purpose of the heuristic is to help improve inference speed, we call φ(v) speedup features.",3.1. Heuristics for Structural Validity,[0],[0]
The speedup features can be different from the original model features in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(2).,3.1. Heuristics for Structural Validity,[0],[0]
In particular it can includes features for partial assignments made so far which were not available in the original model features.,3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the goal of speedup learning is to find suitable weight vector w over the black-box classifier’s lifetime.",3.1. Heuristics for Structural Validity,[0],[0]
"In this section, we will describe a mistake-bound algorithm to learn the weight vector w of the speedup classifier.",4. Learning the Speedup Classifier,[0],[0]
"The design of this algorithm is influenced by learning to search algorithms such as LaSO (Daumé III & Marcu, 2005; Xu et al., 2009).",4. Learning the Speedup Classifier,[0],[0]
"We assume that we have access to a trained black-box ILP solver called Solve, which can solve the structured prediction problems, and we have a large set of examples {xi}Ni=1.",4. Learning the Speedup Classifier,[0],[0]
Our goal is to use this set to train a speedup classifier to mimic the ILP solver while predicting structures for this set of examples.,4. Learning the Speedup Classifier,[0],[0]
"Subsequently, we can use the less expensive speedup influenced search procedure to replace the ILP solver.
",4. Learning the Speedup Classifier,[0],[0]
"To define the algorithm, we will need additional terminology.",4. Learning the Speedup Classifier,[0],[0]
"Given a reference solution y, we define a node v to be ygood, if it can possibly lead to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"If a node v is y-good, then the already assigned variables have the same labels as in the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"We define a
5See supplementary material for an elaboration.
",4. Learning the Speedup Classifier,[0],[0]
"Algorithm 1 Learning a speedup classifier using examples {xi}Ni=1, and a black-box Solver Solve.
1: Initialize the speedup weight vector w← 0 2: for epoch = 1 . .",4. Learning the Speedup Classifier,[0],[0]
.M,4. Learning the Speedup Classifier,[0],[0]
do 3: for i = 1 . . .,4. Learning the Speedup Classifier,[0],[0]
N,4. Learning the Speedup Classifier,[0],[0]
do 4: y← Solve(xi) 5: Initialize the beam B ←,4. Learning the Speedup Classifier,[0],[0]
"[I] 6: while B is y-good and v̂ is not goal do 7: B ← Filter(BreadthExpand(B)) 8: end while 9: if B is not y-good then
10: v∗ ← SetGood(v̂) 11: w←",4. Learning the Speedup Classifier,[0],[0]
w + φ(v∗)− 1|B| ∑ v∈B φ(v) 12: else if v̂ is not y-good then 13: v∗ ← SetGood(v̂) 14: w←,4. Learning the Speedup Classifier,[0],[0]
"w + φ(v∗)− φ(v̂) 15: end if 16: end for 17: end for
beam B is y",4. Learning the Speedup Classifier,[0],[0]
-good if it contains at least one y-good node to represent the notion that search is still viable.,4. Learning the Speedup Classifier,[0],[0]
"We denote the first element (the highest ranked) in a beam by v̂. Finally, we define an operator SetGood, which takes a node that is not y-good, and return its corresponding y-good node by fixing the incorrect assignments according to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"The unassigned variables are still left unassigned by the SetGood operator.
",4. Learning the Speedup Classifier,[0],[0]
The speedup-learning algorithm is listed as Algorithm 1.,4. Learning the Speedup Classifier,[0],[0]
It begins by initializing the weight w to the zero vector.,4. Learning the Speedup Classifier,[0],[0]
We iterate over the examples for M epochs.,4. Learning the Speedup Classifier,[0],[0]
"For each example xi, we first solve inference using the ILP solver to obtain the reference structure y (line 4).",4. Learning the Speedup Classifier,[0],[0]
Next a breadth-expand search is performed (lines 5-8).,4. Learning the Speedup Classifier,[0],[0]
"Every time the beam B is updated, we check if the beam contains at least one ygood node that can possibly lead to the reference solution y. Search terminates if the beam is not y-good, or if the highest ranking node v̂ is a goal.",4. Learning the Speedup Classifier,[0],[0]
"If the beam is not y-good, we compute the corresponding y-good node v∗ from v̂, and perform a perceptron style update to the speedup weights (line 9-11).",4. Learning the Speedup Classifier,[0],[0]
"In other words, we update the weight vector by adding feature vector of φ(v∗), and subtracting the average feature vector of all the nodes in the beam.",4. Learning the Speedup Classifier,[0],[0]
Otherwise v̂ must be a goal node.,4. Learning the Speedup Classifier,[0],[0]
We then check if v̂ agrees with the reference solution (lines 12-15).,4. Learning the Speedup Classifier,[0],[0]
"If not, we perform a similar weight update, by adding the feature vector of φ(v∗), and subtracting φ(v̂).
",4. Learning the Speedup Classifier,[0],[0]
"Mistake bound Next, we show that the Algorithm 1 has a mistake bound.",4. Learning the Speedup Classifier,[0],[0]
"Let Rφ be a positive constant such that for every pair of nodes (v, v′), we have ‖φ(v)− φ(v′)‖ ≤ Rφ.",4. Learning the Speedup Classifier,[0],[0]
"Let Rg be a positive constant such that for every pair of
search nodes (v, v′), we have |g(v)− g(v′)| ≤",4. Learning the Speedup Classifier,[0],[0]
Rg .,4. Learning the Speedup Classifier,[0],[0]
"Finally we define the level margin of a weight vector w for a training set as
γ = min",4. Learning the Speedup Classifier,[0],[0]
"{(v,v′)}
",4. Learning the Speedup Classifier,[0],[0]
"w · ( φ(v)− φ(v′) ) (12)
",4. Learning the Speedup Classifier,[0],[0]
"Here, the set {(v, v′)} contains any pair such that v is ygood, v′ is not y-good, and v and v′ are at the same search level.",4. Learning the Speedup Classifier,[0],[0]
"The level margin denotes the minimum score gap between a y-good and a y-bad node at the same search level.
",4. Learning the Speedup Classifier,[0],[0]
The priority function used to rank the search nodes is defined as pw(v) = g(v)−w,4. Learning the Speedup Classifier,[0],[0]
·φ(v).,4. Learning the Speedup Classifier,[0],[0]
Smaller priority function value ranks higher during search.,4. Learning the Speedup Classifier,[0],[0]
With these definitions we have the following theorem: Theorem 1 (Speedup mistake bound).,4. Learning the Speedup Classifier,[0],[0]
"Given a training set such that there exists a weight vector w with level margin γ > 0 and ‖w‖ = 1, the speedup learning algorithm (Algorithm 1) will converge with a consistent weight vector after making no more than R2φ+2Rg
γ2 weight updates.
",4. Learning the Speedup Classifier,[0],[0]
Proof.,4. Learning the Speedup Classifier,[0],[0]
The complete proof is in the supplementary material of the paper.,4. Learning the Speedup Classifier,[0],[0]
"So far, we have shown that a structured prediction problem can be converted to a beam search problem.",4.1. Avoiding Computing the Input Features,[0],[0]
The priority function for ranking search nodes is determined by p(v) = g(v) + h(v).,4.1. Avoiding Computing the Input Features,[0],[0]
We have seen how the h function be trained to enforce structural constraints.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, there are other opportunities for speeding up as well.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Computing the path cost g(v) involves calculating the corresponding ILP coefficients, which in turn requires feature extraction using the original trained model.",4.1. Avoiding Computing the Input Features,[0],[0]
"This is usually a time-consuming step (Srikumar, 2017), thus motivating the question of whether we can avoid calculating them without losing accuracy.",4.1. Avoiding Computing the Input Features,[0],[0]
"If a search node is strongly preferred by the heuristic function, the path cost is unlikely to reverse the heuristic function’s decision.",4.1. Avoiding Computing the Input Features,[0],[0]
"In this case, we can rank the candidate search nodes with heuristic function only.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Formally, given a fixed beam size b and the beam candidates Ct at step t from which we need to select the beam Bt, we can rank the nodes in Ct from smallest to largest according to the heuristic function value h(v).",4.1. Avoiding Computing the Input Features,[0],[0]
"Denote the bth smallest node as vb and the (b+1)th smallest node as vb+1, we define the heuristic gap ∆t as
∆t = h(vb+1)− h(vb).",4.1. Avoiding Computing the Input Features,[0],[0]
"(13)
If the beam Bt is selected from Ct only according to heuristic function, then ∆t is the gap between the last node in the beam and the first node outside the beam.",4.1. Avoiding Computing the Input Features,[0],[0]
"Next we define the path-cost gap δt as
δt = max v,v′∈Ct
(v − v′) (14)
",4.1. Avoiding Computing the Input Features,[0],[0]
With these definitions we immediately have the following theorem: Theorem 2.,4.1. Avoiding Computing the Input Features,[0],[0]
"Given the beam candidates Ct with heuristic gap ∆t and path-cost gap δt, if ∆t > δt, then using only heuristic function to select the beam Bt will have the same set of nodes selected as using the full priority function up to their ordering in the beam.
",4.1. Avoiding Computing the Input Features,[0],[0]
"If the condition of Theorem 2 holds, then we can rank the candidates using only heuristic function without calculating the path cost.",4.1. Avoiding Computing the Input Features,[0],[0]
This will further save computation time.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, without actually calculating the path cost there is no way to determine the path-cost gap δt at each step.",4.1. Avoiding Computing the Input Features,[0],[0]
"In practice we can treat δt as an empirical parameter θ and define the following priority function
pθ(v) = { h(v), if ∆t > θ, g(v) + h(v), otherwise.
",4.1. Avoiding Computing the Input Features,[0],[0]
(15),4.1. Avoiding Computing the Input Features,[0],[0]
We empirically evaluate the speedup based inference scheme described in Section 4 on the problem of predicting entities and relations (i.e. our running example).,5. Experiments,[0],[0]
"In this task, we are asked to label each entity, and the relation between each pair of the entities.",5. Experiments,[0],[0]
"We assume the entity candidates are given, either from human annotators or from a preprocessing step.",5. Experiments,[0],[0]
"The goal of inference is to determine the types of the entity spans, and the relations between them, as opposed to identify entity candidates.",5. Experiments,[0],[0]
"The research questions we seek to resolve empirically are:
1.",5. Experiments,[0],[0]
Does using a learned speedup heuristic recover structurally valid outputs without paying the inference cost of the integer linear program solver?,5. Experiments,[0],[0]
2.,5. Experiments,[0],[0]
"Can we construct accurate outputs without always computing input features and using only the learned heuristic to guide search?
",5. Experiments,[0],[0]
The dataset we used is from the previous work by Roth & Yih (2004).,5. Experiments,[0],[0]
It contains 1441 sentences.,5. Experiments,[0],[0]
"Each sentence contains several entities with labels, and the labeled relations between every pair of entity.",5. Experiments,[0],[0]
"There are three types of entities, person, location and organization, and five types of relations, Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",5. Experiments,[0],[0]
"There are two constraints associated with each relation type, specifying the allowed source and target arguments.",5. Experiments,[0],[0]
"For example, if the relation label is LiveIn, the source entity must be person and the target entity must be location.",5. Experiments,[0],[0]
"There is also another kind of constraint which says for every pair of entities, they can not have a relation label in both directions between them, i.e., one of the direction must be labeled as NoRel.
",5. Experiments,[0],[0]
"We re-implemented the model from the original work using the same set of features as for the entity and relation scoring
functions.",5. Experiments,[0],[0]
"We used 70% of the labeled data to train an ILPbased inference scheme, which will become our black-box solver for learning the speedup classifier.",5. Experiments,[0],[0]
"The remaining 30% labeled data are held out for evaluations.
",5. Experiments,[0],[0]
"We use 29950 sentences from the Gigaword corpus (Graff et al., 2003) to train the speedup classifier.",5. Experiments,[0],[0]
"The entity candidates are extracted using the Stanford Named Entity Recognizer (Manning et al., 2014).",5. Experiments,[0],[0]
"We ignore the entity labels, however, since our task requires determining the type of the entities and relations.",5. Experiments,[0],[0]
"The features we use for the speedup classifiers are counts of the pairs of labels of the form (source label, relation label), (relation label, target label), and counts of the triples of labels of the form (source label, relation label, target label).",5. Experiments,[0],[0]
"We run Algorithm 1 over this unlabeled dataset, and evaluate the resulting speedup classifier on the held out test set.",5. Experiments,[0],[0]
"In all of our speedup search implementations, we first assign labels to the entities from left to right, then the relations among them.
",5. Experiments,[0],[0]
We evaluate the learned speedup classifier in terms of both accuracy and speed.,5. Experiments,[0],[0]
"The accuracy of the speedup classifier can be evaluated using three kinds of metrics: F-1 scores against gold labels, F-1 scores against the ILP solver’s prediction, and the validity ratio, which is the percentage of the predicted examples agreeing with all constraints.6",5. Experiments,[0],[0]
Our first set of experiments evaluates the impact of Algorithm 1.,5.1. Evaluation of Algorithm 1,[0],[0]
These results are shown in Table 1.,5.1. Evaluation of Algorithm 1,[0],[0]
We see the ILP solver achieves perfect entity and relation F-1 when compared with ILP model itself.,5.1. Evaluation of Algorithm 1,[0],[0]
It guarantees all constraints are satisfied.,5.1. Evaluation of Algorithm 1,[0],[0]
Its accuracy against gold label and its prediction time becomes the baselines of our speedup classifiers.,5.1. Evaluation of Algorithm 1,[0],[0]
We also provide two search baselines.,5.1. Evaluation of Algorithm 1,[0],[0]
The first search baseline just uses greedy search without any constraint considerations.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this setting each label is assigned independently, since the step cost of assigning a label to an entity or a relation variable depends only on the corresponding coefficients in the ILP objectives.",5.1. Evaluation of Algorithm 1,[0],[0]
"In this case, a structured prediction problem becomes several independent multi-class classification problems.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is faster than ILP but the validity ratio is rather low (0.29).,5.1. Evaluation of Algorithm 1,[0],[0]
The second search baseline is greedy search with constraint satisfaction.,5.1. Evaluation of Algorithm 1,[0],[0]
The constraints are guaranteed to be satisfied by using the standard arc-consistency search.,5.1. Evaluation of Algorithm 1,[0],[0]
"The prediction takes much longer than the ILP solver (844 ms vs. 239 ms.).
",5.1. Evaluation of Algorithm 1,[0],[0]
We trained a speedup classifier with two different beam sizes.,5.1. Evaluation of Algorithm 1,[0],[0]
"Even with beam width b = 1, we are able to obtain > 95% validity ratio, and the prediction time is much faster
6All our experiments were conducted on a server with eight Intel i7 3.40 GHz cores and 16G memory.",5.1. Evaluation of Algorithm 1,[0],[0]
"We disabled multithreaded execution in all cases for a fair comparison.
than the ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"Furthermore, we see that the F-1 score evaluated against gold labels is only slightly worse than ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"With beam width b = 2, we recover the ILP model accuracy when evaluated against gold labels.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is still much less than the ILP solver.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this section, we empirically verify the idea that we do not always need to compute the path cost, if the heuristic gap ∆t is large.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
We use the evaluation function pθ(v) in Eq.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
(15) with different values of θ to rank the search nodes.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"The results are given in Table 2.
",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"For both beam widths, θ = 0 is the case in which the original model is completely ignored.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
All the nodes are ranked using the speedup heuristic function only.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"Even though it has perfect validity ratio, the result is rather poor when evaluated on F-1 scores.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"When θ increases, the entity and relation F-1 scores quickly jump up, essentially getting back the same accuracy as the speedup classifiers in Table 1.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
But the prediction time is lowered compared to the results from Table 1.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
The idea of learning memo functions to make computation more efficient goes back to Michie (1968).,6. Discussion and Related Work,[0],[0]
"Speedup learning has been studied since the eighties in the context of general problem solving, where the goal is to learn a problem solver that becomes faster as opposed to becoming more accurate as it sees more data.",6. Discussion and Related Work,[0],[0]
Fern (2011) gives a broad survey of this area.,6. Discussion and Related Work,[0],[0]
"In this paper, we presented a variant of this idea that is more concretely applied to structured output prediction.
",6. Discussion and Related Work,[0],[0]
Efficient inference is a central topic in structured prediction.,6. Discussion and Related Work,[0],[0]
"In order to achieve efficiency, various strategies are adopted in the literature.",6. Discussion and Related Work,[0],[0]
Search based strategies are commonly used for this purpose and several variants abound.,6. Discussion and Related Work,[0],[0]
"The idea of framing a structured prediction problem as a search problem has been explored by several previous works (Collins & Roark, 2004; Daumé III & Marcu, 2005; Daumé III et al., 2009; Huang et al., 2012; Doppa et al., 2014).",6. Discussion and Related Work,[0],[0]
"It usually admits incorporating arbitrary features more easily than fully global structured prediction models like conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), and structured support vector machines (Taskar et al., 2003; Tsochantaridis et al., 2004).",6. Discussion and Related Work,[0],[0]
"In such cases too, inference can be solved approximately using heuristic search.",6. Discussion and Related Work,[0],[0]
"Either a fixed beam size (Xu et al., 2009), or a dynamicallysized beam (Bodenstab et al., 2011) can be used.",6. Discussion and Related Work,[0],[0]
In our work we fix the beam size.,6. Discussion and Related Work,[0],[0]
The key difference from previous work is that our ranking function combines information from the trained model with the heuristic function which characterizes constraint information.,6. Discussion and Related Work,[0],[0]
"Closely related to the
work described in this paper are approaches that learn to prune the search space (He et al., 2014; Vieira & Eisner, 2016) and learn to select features (He et al., 2013).
",6. Discussion and Related Work,[0],[0]
Another line of recent related work focuses on discovering problem level regularities across the inference space.,6. Discussion and Related Work,[0],[0]
"These amortized inference schemes are designed using deterministic rules for discovering when a new inference problem can re-use previously computed solutions (Srikumar et al., 2012; Kundu et al., 2013) or in the context of a Bayesian network by learning a stochastic inverse network that generates outputs (Stuhlmüller et al., 2013).
",6. Discussion and Related Work,[0],[0]
"Our work is also related to the idea of imitation learning (Daumé III et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014; Chang et al., 2015).",6. Discussion and Related Work,[0],[0]
"In this setting, we are given a reference policy, which may or may not be a good policy.",6. Discussion and Related Work,[0],[0]
"The goal of learning is to learn another policy to imitate the given policy, or even learn a better one.",6. Discussion and Related Work,[0],[0]
Learning usually proceeds in an online fashion.,6. Discussion and Related Work,[0],[0]
"However, imitation learning requires learning a new policy which is independent of the given reference policy, since during test time the reference policy is no longer available.",6. Discussion and Related Work,[0],[0]
"In our case, we can think of the black-box solver as a reference policy.",6. Discussion and Related Work,[0],[0]
"During prediction we always have this solver at our disposal, what we want is avoiding unnecessary calls to the solver.",6. Discussion and Related Work,[0],[0]
"Following recent successes in imitation learning, we expect that we can replace the linear heuristic function with a deep network to avoid feature design.
",6. Discussion and Related Work,[0],[0]
"Also related is the idea of knowledge distillation (Bucilă et al., 2006; Hinton et al., 2015; Kim & Rush, 2016), that seeks to train a student classifier (usually a neural network) to compress and mimic a larger teacher network, thus improve prediction speed.",6. Discussion and Related Work,[0],[0]
The primary difference with the speedup idea of this paper is that our goal is to be more efficient at constructing internally self-consistent structures without explicitly searching over the combinatorially large output space with complex constraints.,6. Discussion and Related Work,[0],[0]
"In this paper, we asked whether we can learn to make inference faster over the lifetime of a structured output classifier.",7. Conclusions,[0],[0]
"To address this question, we developed a search-based strategy that learns to mimic a black-box inference engine but is substantially faster.",7. Conclusions,[0],[0]
We further extended this strategy by identifying cases where the learned search algorithm can avoid expensive input feature extraction to further improve speed without losing accuracy.,7. Conclusions,[0],[0]
We empirically evaluated our proposed algorithms on the problem of extracting entities and relations from text.,7. Conclusions,[0],[0]
"Despite using an object-heavy JVM-based implementation of search, we showed that by exploiting regularities across the output space, we can outperform the industrial strength Gurobi integer linear program solver in terms of speed, while matching its accuracy.
",7. Conclusions,[0],[0]
Acknowledgments We thank the Utah NLP group members and the anonymous reviewers for their valuable feedback.,7. Conclusions,[0],[0]
Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces.,abstractText,[0],[0]
"In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy.",abstractText,[0],[0]
"To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach.",abstractText,[0],[0]
"As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space.",abstractText,[0],[0]
We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features.,abstractText,[0],[0]
We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.,abstractText,[0],[0]
Learning to Speed Up Structured Output Prediction,title,[0],[0]
"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.",text,[0],[0]
Dynamical systems modelling is a cornerstone of experimental sciences.,1. Introduction,[0],[0]
"In biology, as well as in physics and chemistry, modelers attempt to capture the dynamical behavior of a given system or a phenomenon in order to improve its understanding and make predictions about its future state.",1. Introduction,[0],[0]
Systems of coupled ordinary differential equations (ODEs) are undoubtedly the most widely used models in science.,1. Introduction,[0],[0]
"Even simple ODE functions can describe complex dynamical behaviours (Hirsch et al., 2004).",1. Introduction,[0],[0]
"Typically, the dynamics are firmly grounded in physics with only a few parameters to be estimated from data.",1. Introduction,[0],[0]
"However, equally ubiquitous are the cases where the governing dynamics are partially or completely unknown.
",1. Introduction,[0],[0]
"We consider the dynamics of a system governed by multi-
*Equal contribution 1Aalto University, Finland 2Helsinki Institute of Information Technology HIIT, Finland.",1. Introduction,[0],[0]
Correspondence to: Markus Heinonen,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"markus.o.heinonen@aalto.fi>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"variate ordinary differential functions:
ẋ(t) = dx(t)
dt = f(x(t)) (1)
where x(t) ∈ X",1. Introduction,[0],[0]
"= RD is the state vector of a Ddimensional dynamical system at time t, and the ẋ(t) ∈",1. Introduction,[0],[0]
Ẋ =,1. Introduction,[0],[0]
"RD is the first order time derivative of x(t) that drives the state x(t) forward, and where f :",1. Introduction,[0],[0]
RD → RD is the vector-valued derivative function.,1. Introduction,[0],[0]
"The ODE solution is determined by
x(t) = x0 + ∫",1. Introduction,[0],[0]
t 0,1. Introduction,[0],[0]
"f(x(τ))dτ, (2)
where we integrate the system state from an initial state x(0) = x0 for time t forward.",1. Introduction,[0],[0]
We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y =,1. Introduction,[0],[0]
"(y1, . . .",1. Introduction,[0],[0]
",yN )",1. Introduction,[0],[0]
"T ∈ RN×D obtained from an additive noisy observation model at observation time points T = (t1, . . .",1. Introduction,[0],[0]
", tN ) ∈ RN ,
y(t)",1. Introduction,[0],[0]
= x(t) +,1. Introduction,[0],[0]
"εt, (3)
where εt ∼ N (0,Ω) follows a stationary zero-mean multivariate Gaussian distribution with diagonal noise variances Ω = diag(ω21 , . . .",1. Introduction,[0],[0]
", ω 2 D).",1. Introduction,[0],[0]
The observation time points do not need to be equally spaced.,1. Introduction,[0],[0]
"Our task is to learn the differential function f(·) given observations Y , with no prior knowledge of the ODE system.
",1. Introduction,[0],[0]
"There is a vast literature on conventional ODEs (Butcher, 2016) where a parametric form for function f(x;θ, t) is assumed to be known, and its parameters θ are subsequently optimised with least squares or Bayesian approach, where the expensive forward solution xθ(ti)",1. Introduction,[0],[0]
"=∫ ti 0
f(x(τ);θ, t)dτ is required to evaluate the system responses xθ(ti) from parameters θ against observations y(ti).",1. Introduction,[0],[0]
"To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi)",1. Introduction,[0],[0]
"≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.",1. Introduction,[0],[0]
"Recently several authors have proposed embedding a parametric differential function within a Bayesian or Gaussian process (GP) framework (Graepel, 2003; Calderhead et al., 2008;
Dondelinger et al., 2013; Wang and Barber, 2014; Macdonald, 2017) (see Macdonald et al. (2015) for a review).",1. Introduction,[0],[0]
"GPs have been successfully applied to model linear differential equations as they are analytically tractable (Gao et al., 2008; Raissi et al., 2017).
",1. Introduction,[0],[0]
"However, conventional ODE modelling can only proceed if a parametric form of the driving function f(·) is known.",1. Introduction,[0],[0]
"Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.",1. Introduction,[0],[0]
"Early works include spline-based smoothing and additive functions ∑D j fj(xj) to infer gene regulatory networks (De Hoon et al., 2002; Henderson and Michailidis, 2014).",1. Introduction,[0],[0]
"Äijö and Lähdesmäki (2009) proposed estimating the unknown nonlinear function with GPs using either finite time differences, or analytically solving the derivative function as a function of only time, ẋ(t) = f(t) (Äijö et al., 2013).",1. Introduction,[0],[0]
"In a seminal technical report of Heinonen and d’Alche Buc (2014) a full vector-valued kernel model f(x) was proposed, however using a gradient matching approximation.",1. Introduction,[0],[0]
"To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).
",1. Introduction,[0],[0]
"In this work we propose NPODE1: the first ODE model for learning arbitrary, and a priori completely unknown nonparametric, non-linear differential functions f : X → Ẋ from data in a Bayesian way.",1. Introduction,[0],[0]
"We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.",1. Introduction,[0],[0]
"We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.",1. Introduction,[0],[0]
"Our model can forecast continuous-time systems arbitrary amounts to future, and we demonstrate the state-of-the-art performance in human motion datasets.",1. Introduction,[0],[0]
"The differential function f(x) to be learned defines a vector field2 f , that is, an assignment of a gradient vector f(x) ∈ RD to every state x ∈ RD.",2. Nonparametric ODE Model,[0],[0]
"We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006)
f(x) ∼ GP(0,K(x,x′)), (4)
which defines a priori distribution over function values f(x) whose mean and covariances are
E[f(x)]",2. Nonparametric ODE Model,[0],[0]
"= 0 (5) cov[f(x), f(x′)] = K(x,x′), (6)
1The implementation is publicly available in http://www. github.com/cagatayyildiz/npode
2We use vector field and differential function interchangeably.
and where the kernel K(x,x′) ∈ RD×D is matrixvalued.",2. Nonparametric ODE Model,[0],[0]
A GP prior defines that for any collection of states X =,2. Nonparametric ODE Model,[0],[0]
"(x1, . . .",2. Nonparametric ODE Model,[0],[0]
",xN )",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D, the function values F = (f(x1), . . .",2. Nonparametric ODE Model,[0],[0]
", f(xN ))",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D follow a matrixvalued normal distribution,
p(F ) = N (vec(F )|0,K(X,X)), (7)
where K(X,X) =",2. Nonparametric ODE Model,[0],[0]
"(K(xi,xj))Ni,j=1 ∈",2. Nonparametric ODE Model,[0],[0]
"RND×ND is a block matrix of matrix-valued kernels K(xi,xj).",2. Nonparametric ODE Model,[0],[0]
"The key property of Gaussian processes is that they encode functions where similar states x,x′ induce similar differentials f(x), f(x′), and where the state similarity is defined by the kernel K(x,x′).
",2. Nonparametric ODE Model,[0],[0]
"In standard GP regression we would obtain the posterior of the vector field by conditioning the GP prior with the data (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
In ODE models the conditional f(x)|Y of a vector field is intractable due to the integral mapping (2) between observed states y(ti) and differentials f(x).,2. Nonparametric ODE Model,[0],[0]
"Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈",2. Nonparametric ODE Model,[0],[0]
"Ẋ , such that f(z) = u (Quiñonero-Candela and
Rasmussen, 2005).",2. Nonparametric ODE Model,[0],[0]
"We choose to interpolate the differential function between the inducing points as (See Figure 1)
f(x) , Kθ(x, Z)Kθ(Z,Z) −1vec(U), (8)
which supports the function f(x) with inducing locations Z = (z1, . . .",2. Nonparametric ODE Model,[0],[0]
", zM ), inducing vectors U = (u1, . . .",2. Nonparametric ODE Model,[0],[0]
",uM ), and θ are the kernel parameters.",2. Nonparametric ODE Model,[0],[0]
"The function above corresponds to a vector-valued kernel function (Alvarez et al., 2012), or to a multi-task Gaussian process conditional mean without the variance term (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
This definition is then compatible with the deterministic nature of the ODE formalism.,2. Nonparametric ODE Model,[0],[0]
"Due to universality of several kernels and kernel functions (Shawe-Taylor and Cristianini, 2004), we can represent arbitrary vector fields with appropriate inducing point and kernel choices.",2. Nonparametric ODE Model,[0],[0]
"The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).",2.1. Operator-valued Kernels,[0],[0]
"Most straightforward operator-valued kernel is the identity decomposable kernel Kdec(z, z′) = k(z, z′) ·",2.1. Operator-valued Kernels,[0],[0]
"ID, where the scalar Gaussian kernel
Kθ(z, z ′) =",2.1. Operator-valued Kernels,[0],[0]
σ2f exp −1 2 D∑ j=1 (zj − z′j)2,2.1. Operator-valued Kernels,[0],[0]
"`2j  (9) with differential variance σ2f and dimension-specific lengthscales ` = (`1, . . .",2.1. Operator-valued Kernels,[0],[0]
", `D) are expanded into a diagonal matrix of size D × D. We collect the kernel parameters as θ = (σf , `).
",2.1. Operator-valued Kernels,[0],[0]
We note that more complex kernels can also be considered given prior information of the underlying system characteristics.,2.1. Operator-valued Kernels,[0],[0]
"The divergence-free matrix-valued kernel induces vector fields that have zero divergence (Wahlström et al., 2013; Solin et al., 2015).",2.1. Operator-valued Kernels,[0],[0]
"Intuitively, these vector fields do not have sinks or sources, and every state always finally returns to itself after sufficient amount of time.",2.1. Operator-valued Kernels,[0],[0]
"Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.",2.1. Operator-valued Kernels,[0],[0]
"For theoretical treatment of vector field kernels, see (Narcowich and Ward, 1994; Bhatia et al., 2013; Fuselier and Wright, 2017).",2.1. Operator-valued Kernels,[0],[0]
"Non-stationary vector fields can be modeled with input-dependent lengthscales (Heinonen et al., 2016), while spectral kernels can represent stationary (Wilson et al., 2013) or non-stationary (Remes et al., 2017) recurring patterns in the differential function.",2.1. Operator-valued Kernels,[0],[0]
"We assume a Gaussian likelihood over the observations yi and the corresponding simulated responses x(ti) of Equation (2),
p(Y |x0, U, Z,ω) = N∏ i=1 N",2.2. Joint Model,[0],[0]
"(yi|x(ti),Ω), (10)
where x(ti) are forward simulated responses using the integral Equation (2) and differential Equation (8), and Ω = diag(ω21 . .",2.2. Joint Model,[0],[0]
.,2.2. Joint Model,[0],[0]
", ω 2 D) collects the dimension-specific noise variances.
",2.2. Joint Model,[0],[0]
"The inducing vectors have a Gaussian process prior
p(U |Z,θ) = N",2.2. Joint Model,[0],[0]
"(vec(U)|0,Kθ(Z,Z)).",2.2. Joint Model,[0],[0]
"(11)
",2.2. Joint Model,[0],[0]
"The model posterior is then
p(U,x0,θ,ω|Y ) ∝",2.2. Joint Model,[0],[0]
"p(Y |x0, U,ω)p(U |θ) = L, (12)
where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.
",2.2. Joint Model,[0],[0]
"The model parameters are the initial state x03, the inducing vectors U , the noise standard deviations ω = (ω1, . . .",2.2. Joint Model,[0],[0]
", ωD), and the kernel hyperparameters θ = (σf , `1, . . .",2.2. Joint Model,[0],[0]
", `D).",2.2. Joint Model,[0],[0]
"We apply a latent parameterisation using Cholesky decomposition LθLTθ = Kθ(Z,Z), which maps the inducing vectors to whitened domain (Kuss and Rasmussen, 2005)
",2.3. Noncentral Parameterisation,[0],[0]
"U = LθŨ , Ũ = L −1 θ U. (13)
",2.3. Noncentral Parameterisation,[0],[0]
The latent variables Ũ are projected on the kernel manifold Lθ to obtain the inducing vectors U .,2.3. Noncentral Parameterisation,[0],[0]
"This non-centered parameterisation (NCP) transforms the hierarchical posterior L of Equation (12) into a reparameterised form
p(x0, Ũ ,θ,ω|Y ) ∝",2.3. Noncentral Parameterisation,[0],[0]
"p(Y |x0, Ũ ,ω,θ)p(Ũ), (14)
where all variables to be optimised are decoupled, with the latent inducing vectors having a standard normal prior Ũ ∼ N (0, I).",2.3. Noncentral Parameterisation,[0],[0]
Optimizing Ũ and θ is now more efficient since they have independent contributions to the vector field via U = LθŨ .,2.3. Noncentral Parameterisation,[0],[0]
"The gradients of the whitened posterior can be retrieved analytically as (Heinonen et al., 2016)
",2.3. Noncentral Parameterisation,[0],[0]
∇Ũ logL = L T,2.3. Noncentral Parameterisation,[0],[0]
"θ∇U logL. (15)
3In case of multiple time-series, we will use one initial state for each time-series.
",2.3. Noncentral Parameterisation,[0],[0]
"Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent,
x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω
logL, (16)
while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).",2.3. Noncentral Parameterisation,[0],[0]
"The partial derivatives of the posterior with respect to noise parameters ω can be found analytically, while the derivative with respect to σf is approximated with finite differences.",2.3. Noncentral Parameterisation,[0],[0]
We select the optimal lengthscales ` by cross-validation.,2.3. Noncentral Parameterisation,[0],[0]
"The key term to carry out the MAP gradient ascent optimization is the likelihood
log p(Y |x0, Ũ ,ω)
that requires forward integration and computing the partial derivatives with respect to the whitened inducing vectors Ũ .",3. Sensitivity Equations,[0],[0]
Given Equation (15) we only need to compute the gradients with respect to the inducing vectors u = vec(U) ∈,3. Sensitivity Equations,[0],[0]
"RMD,
d log p(Y |x0,u,ω)",3. Sensitivity Equations,[0],[0]
"du
= N∑ s=1 d logN (ys|x(ts,u),Ω)",3. Sensitivity Equations,[0],[0]
"dx dx(ts,u) du .",3. Sensitivity Equations,[0],[0]
"(17)
",3. Sensitivity Equations,[0],[0]
"This requires computing the derivatives of the simulated system response x(t,u) against the vector field parameters u,
dx(t,u)
du ≡ S(t) ∈ RD×MD, (18)
which we denote by Sij(t)",3. Sensitivity Equations,[0],[0]
"= ∂x(t,u)i
∂uj , and expand the no-
tation to make the dependency of x on u explicit.",3. Sensitivity Equations,[0],[0]
"Approximating these with finite differences is possible in principle, but is highly inefficient and has been reported to cause unstability (Raue et al., 2013).",3. Sensitivity Equations,[0],[0]
"We instead turn to sensitivity equations for u and x0 that provide computationally efficient, analytical gradients S(t) (Kokotovic and Heller, 1967; Fröhlich et al., 2017).
",3. Sensitivity Equations,[0],[0]
"The solution for dx(t,u)du can be derived by differentiating the full nonparametric ODE system with respect to u by
d
du
dx(t,u)
dt =
d
du f(x(t,u)).",3. Sensitivity Equations,[0],[0]
"(19)
The sensitivity equation for the given system can be obtained by changing the order of differentiation on the left hand side and carrying out the differentiation on the right hand side.
",3. Sensitivity Equations,[0],[0]
"The resulting sensitivity equation can then be expressed in the form
Ṡ(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"d
dt
dx(t,u)
du =
J(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂x
S(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"dx(t,u)
",3. Sensitivity Equations,[0],[0]
"du +
R(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂u ,
(20)
",3. Sensitivity Equations,[0],[0]
"where J(t) ∈ RD×D, R(t), Ṡ(t) ∈ RD×MD (See Supplements for detailed specification).",3. Sensitivity Equations,[0],[0]
"For our nonparametric ODE system the sensitivity equation is fully determined by
J(t) = ∂K(x, Z)
∂x K(Z,Z)−1u",3. Sensitivity Equations,[0],[0]
"(21)
R(t) = K(x, Z)K(Z,Z)−1. (22)
",3. Sensitivity Equations,[0],[0]
The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).,3. Sensitivity Equations,[0],[0]
"The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).",3. Sensitivity Equations,[0],[0]
The initial sensitivities are computed as S(0) = dx0du .,3. Sensitivity Equations,[0],[0]
"In our implementation, we merge x0 with u for sensitivity analysis to obtain the partial derivatives with respect to the initial state which is estimated along with the other parameters.",3. Sensitivity Equations,[0],[0]
"We use the CVODES solver from the SUNDIALS package (Hindmarsh et al., 2005) to solve the nonparametric ODE models and the corresponding gradients numerically.",3. Sensitivity Equations,[0],[0]
"The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.",3. Sensitivity Equations,[0],[0]
"As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form
VDP : ẋ1 = x2 ẋ2 =",4. Simple Simulated Dynamics,[0],[0]
(1− x21)x2,4. Simple Simulated Dynamics,[0],[0]
− x1 FHN :,4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 3(x1 − x31 3 + x2),4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = 0.2− 3x1 − 0.2x2
3 LV :",4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 1.5x1,4. Simple Simulated Dynamics,[0],[0]
− x1x2,4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = −3x2 + x1x2.
",4. Simple Simulated Dynamics,[0],[0]
"In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).",4. Simple Simulated Dynamics,[0],[0]
"Our main goal is to infer unknown dynamics, that is, when these equations are unavailable and we instead represent the dynamics with a nonparametric
vector field of Equation (8).",4. Simple Simulated Dynamics,[0],[0]
"We use these simulated models to only illustrate our model behavior against the true dynamics.
",4. Simple Simulated Dynamics,[0],[0]
"We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1
2.",4. Simple Simulated Dynamics,[0],[0]
"We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.",4. Simple Simulated Dynamics,[0],[0]
Training takes approximately 100 seconds per oscillator.,4. Simple Simulated Dynamics,[0],[0]
"Figure 2 (bottom) shows the training datasets (grey regions), initial states, true trajectories (black lines) and the forecasted trajectory likelihoods (colored regions).",4. Simple Simulated Dynamics,[0],[0]
"The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.
",4. Simple Simulated Dynamics,[0],[0]
Figure 2 (top) shows the corresponding true vector field (black arrows) and the estimated vector field (grey arrows).,4. Simple Simulated Dynamics,[0],[0]
"The vector field is a continuous function, which is plotted on a 8x8 grid for visualisation.",4. Simple Simulated Dynamics,[0],[0]
"In general the most difficult part of the system is learning the middle of the loop (as seen in the FHN model), and learning the most outermost regions (bottom left in the LV model).",4. Simple Simulated Dynamics,[0],[0]
"The model learns the
underlying differential f(x) accurately close to observed points, while making only few errors in the border regions with no data.",4. Simple Simulated Dynamics,[0],[0]
"Next, we illustrate how the model estimates realistic, unknown dynamics from noisy observations y(t1), . . .",5. Unknown System Estimation,[0],[0]
",y(tN ).",5. Unknown System Estimation,[0],[0]
"As in Section 4, we make no assumptions on the structure or form of the underlying system, and capture the underlying dynamics with the nonparameteric system alone.",5. Unknown System Estimation,[0],[0]
"We employ no subjective priors, and assume no inputs, controls or other sources of information.",5. Unknown System Estimation,[0],[0]
"The task is to infer the underlying dynamics f(x), and interpolate or extrapolate the state trajectory outside the observed data.
",5. Unknown System Estimation,[0],[0]
We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.,5. Unknown System Estimation,[0],[0]
"Our dataset contains 50-dimensional pose measurements y(ti) from humans walking, where each pose dimension records a measurement in different parts of the body during movement (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
We apply the preprocessing of Wang et al. (2008) by downsampling the datasets by a factor of four and centering the data.,5. Unknown System Estimation,[0],[0]
"This resulted in a total of 4303 datapoints spread across 43 trajec-
tories with on average 100 frames per trajectory.",5. Unknown System Estimation,[0],[0]
"In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).",5. Unknown System Estimation,[0],[0]
"We place M = 53 inducing vectors on a fixed grid, and optimize our model starting from 100 different initial values, which we set by perturbing the projected empirical differences y(ti)−y(ti−1) to the inducing vectors.",5. Unknown System Estimation,[0],[0]
We use an L-BFGS optimizer in Matlab.,5. Unknown System Estimation,[0],[0]
"The whole inference takes approximately few minutes per trajectory.
",5. Unknown System Estimation,[0],[0]
We evaluate the method with two types of experiments: imputing missing values and forecasting future cycles.,5. Unknown System Estimation,[0],[0]
"For the forecasting the first half of the trajectory is reserved for model training, and the second half is to be forecasted.",5. Unknown System Estimation,[0],[0]
"For imputation we remove roughly 20% of the frames from the middle of the trajectory, which are to be filled by the models.",5. Unknown System Estimation,[0],[0]
We perform model selection for lengthscales ` with crossvalidation split of 80/20.,5. Unknown System Estimation,[0],[0]
"We record the root mean square error (RMSE) over test points in the original feature space in both cases, where we reconstruct the original dimensions from the latent space trajectories.
",5. Unknown System Estimation,[0],[0]
"Due to the current lack of ODE methods suitable for this nonparametric inference task, we instead compare our method to the state-of-the-art state-space models where such problems have been previously considered (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.,5. Unknown System Estimation,[0],[0]
"With sufficiently high sampling rate, such models can estimate and forecast finite approximations of smooth dynamics.",5. Unknown System Estimation,[0],[0]
"In Gaussian process dynamical model (Wang et al., 2006; Frigola et al., 2014; Svensson et al., 2016)",5. Unknown System Estimation,[0],[0]
"a GP transition function is inferred in a latent space, which can be inferred with a standard GPLVM (Lawrence, 2004) or with a dependent GPLVM (Zhao and Sun, 2016).",5. Unknown System Estimation,[0],[0]
"In dynamical systems the transition function is replaced by a GP interpolation (Damianou et al., 2011).",5. Unknown System Estimation,[0],[0]
"The discrete time state-space models emphasize inference of a low-dimensional manifold as an explanation of the high-dimensional measurement trajectories.
",5. Unknown System Estimation,[0],[0]
"We compare our method to the dynamical model GPDM of Wang et al. (2006) and to the dynamical system VGPLVM of Damianou et al. (2011), where we directly apply the implementations provided by the authors at inverseprobability.com/vargplvm and dgp.",5. Unknown System Estimation,[0],[0]
toronto.edu/˜jmwang/gpdm.,5. Unknown System Estimation,[0],[0]
"Both methods optimize their latent spaces separately, and they are thus not directly comparable.",5. Unknown System Estimation,[0],[0]
"In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.",5.1. Forecasting,[0],[0]
"The models are trained and forecasted
within a low-dimensional space, and subsequently projected back into the original space via inverting the PCA or with GPLVM mean predictions.",5.1. Forecasting,[0],[0]
"As all methods optimize their latent spaces separately, they are not directly comparable.",5.1. Forecasting,[0],[0]
"Thus, the mean errors are computed in the original highdimensional space.",5.1. Forecasting,[0],[0]
"Note that the low-dimensional representation necessarily causes some reconstruction errors.
",5.1. Forecasting,[0],[0]
Figure 3 illustrates the models on one of the trajectories 35 12.amc.,5.1. Forecasting,[0],[0]
"The top part (a) shows the training data in the PCA space for npODE, and optimized training data representation for GPDM and VGPLVM (black points).",5.1. Forecasting,[0],[0]
"The colored lines (npODE) and points (GPDM, VGPLVM) indicate the future forecast.",5.1. Forecasting,[0],[0]
The bottom part (b) shows the first 9 reconstructed original pose dimensions reconstructed from the latent forecasted trajectories.,5.1. Forecasting,[0],[0]
"The training data is shown in grey background, while test data is shown with circles.
",5.1. Forecasting,[0],[0]
"The VGPLVM has most trouble forecasting future points, and reverts quickly after training data to a value close to zero, failing to predict future points.",5.1. Forecasting,[0],[0]
"The GPDM model produces more realistic trajectories, but fails to predict any of the poses accurately.",5.1. Forecasting,[0],[0]
"Finally, npODE can accurately predict five poses, and still retains adequate performance on remaining poses, except for pose 2.
",5.1. Forecasting,[0],[0]
"Furthermore, Table 1 indicates that npODE is also best performing method on average over the whole dataset in the forecasting.",5.1. Forecasting,[0],[0]
In the imputation task we remove approximately 20% of the training data from the middle of the trajectory.,5.2. Imputation,[0],[0]
The goals are to learn a model with the remaining data and to forecast the missing values.,5.2. Imputation,[0],[0]
Figure 4 highlights the performance of the three models on the trajectory 07 07.amc.,5.2. Imputation,[0],[0]
"The top part (a) shows the training data (black points) in the PCA space (npODE) or optimized training locations in the latent space (GPDM, VGPLVM).",5.2. Imputation,[0],[0]
The middle part imputation is shown with colored points or lines.,5.2. Imputation,[0],[0]
"Interestingly both npODE and GPDM operate on cyclic representations, while VGPLVM is not cyclic.
",5.2. Imputation,[0],[0]
"The bottom panel (b) shows the first 9 reconstructed pose
dimensions from the three models.",5.2. Imputation,[0],[0]
"The missing values are shown in circles, while training points are shown with black dots.",5.2. Imputation,[0],[0]
"All models can accurately reproduce the overall trends, while npODE seems to fit slightly worse than the other methods.",5.2. Imputation,[0],[0]
The PCA projection causes the seemingly perfect fit of the npODE prediction (at the top) to lead to slightly warped reconstructions (at the bottom).,5.2. Imputation,[0],[0]
All methods mostly fit the missing parts as well.,5.2. Imputation,[0],[0]
Table 1 shows that on average the npODE and VGPLVM have approximately equal top performance on the imputing missing values task.,5.2. Imputation,[0],[0]
"We proposed the framework of nonparametric ODE model that can accurately learn arbitrary, nonlinear continuos-time dynamics from purely observational data without making assumptions of the underlying system dynamics.",6. Discussion,[0],[0]
We demonstrated that the model excels at learning dynamics that can be forecasted into the future.,6. Discussion,[0],[0]
"We consider this work as the
first in a line of studies of nonparametric ODE systems, and foresee several aspects as future work.",6. Discussion,[0],[0]
"Currently we do not handle non-stationary vector fields, that is time-dependent differentials ft(x).",6. Discussion,[0],[0]
"Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).",6. Discussion,[0],[0]
"Finally, including inputs or controls to the system would allow precise modelling in interactive settings, such as robotics.
",6. Discussion,[0],[0]
"The proposed nonparametric ODE model operates along a continuous-time trajectory, while dynamic models such as hidden Markov models or state-space models are restricted to discrete time steps.",6. Discussion,[0],[0]
"These models are unable to consider system state at arbitrary times, for instance, between two successive timepoints.
",6. Discussion,[0],[0]
"Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic
system drift and diffusion processes separately leading to a distribution of trajectories p(x(t))",6. Discussion,[0],[0]
"(Archambeau et al., 2007; Garcı́a et al., 2017).",6. Discussion,[0],[0]
"As future work we will consider stochastic extensions of our nonparametric ODE model, as well as MCMC sampling of the inducing point posterior p(U |Y ), leading to trajectory distribution as well.
Acknowledgements.",6. Discussion,[0],[0]
The data used in this project was obtained from mocap.cs.cmu.edu.,6. Discussion,[0],[0]
The database was created with funding from NSF EIA-0196217.,6. Discussion,[0],[0]
"This work has been supported by the Academy of Finland Center of Excellence in Systems Immunology and Physiology, the Academy of Finland grants no. 284597, 311584, 313271, 299915.",6. Discussion,[0],[0]
In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated.,abstractText,[0],[0]
"However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics.",abstractText,[0],[0]
"In these settings, parametric ODE model cannot be formulated.",abstractText,[0],[0]
"Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge.",abstractText,[0],[0]
"We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism.",abstractText,[0],[0]
We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.,abstractText,[0],[0]
Learning unknown ODE models with Gaussian processes,title,[0],[0]
"Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation (Lample et al., 2017), multilingual word embeddings (Klementiev et al., 2012), and knowledge transfer to low resource languages (Guo et al., 2016).",1 Introduction,[0],[0]
"Research here has a long history under the guise of decipherment (Knight et al., 2006).",1 Introduction,[0],[0]
"Current contemporary methods have achieve effective word translation through theme-aligned corpora (Gouws et al., 2015), or seed dictionaries (Mikolov et al., 2013).
",1 Introduction,[0],[0]
"Mikolov et al. (2013) showed that monolingual word embeddings exhibit isomorphism across languages, and can be aligned with a simple linear transformation.",1 Introduction,[0],[0]
"Given two sets word vectors learned independently from monolingual corpora, and a dictionary of seed pairs to learn a linear transformation for alignment; they were able to
estimate a complete bilingual lexicon.",1 Introduction,[0],[0]
"Many studies have since followed this approach, proposing various improvements such as orthogonal mappings (Artetxe et al., 2016) and improved objectives (Lazaridou et al., 2015).
",1 Introduction,[0],[0]
Obtaining aligned corpora or bilingual seed dictionaries is nevertheless not straightforward for all language pairs.,1 Introduction,[0],[0]
"This has motivated a wave of very recent research into unsupervised word translation: inducing bilingual dictionaries given only monolingual word embeddings (Conneau et al., 2018; Zhang et al., 2017b,a; Artetxe et al., 2017).",1 Introduction,[0],[0]
"The most successful have leveraged ideas from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",1 Introduction,[0],[0]
"In this approach the generator provides the cross-modal mapping, taking embeddings of dictionary words in one language and ‘generating’ their translation in another.",1 Introduction,[0],[0]
The discriminator tries to distinguish between this ‘fake’ set of translations and the true dictionary of embeddings in the target language.,1 Introduction,[0],[0]
"The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal mapping should be capable of inducing a complete dictionary, as per Mikolov et al. (2013).
",1 Introduction,[0],[0]
"Despite these successes, such adversarial methods have a number of well-known drawbacks (Arjovsky et al., 2017):",1 Introduction,[0],[0]
"Due to the nature of their min-max game, adversarial training is very unstable, and they are prone to divergence.",1 Introduction,[0],[0]
"It is extremely hyper-parameter sensitive, requiring problem-specific tuning.",1 Introduction,[0],[0]
"Convergence is also hard to diagnose and does not correspond well to efficacy of the generator in downstream tasks (Hoshen and Wolf, 2018).
",1 Introduction,[0],[0]
"In this paper, we propose an alternative statistical dependency-based approach to unsupervised word translation.",1 Introduction,[0],[0]
"Specifically, we propose to search for the cross-lingual word pairing that maximizes statistical dependency in terms of squared
loss mutual information (SMI) (Yamada et al., 2015; Suzuki and Sugiyama, 2010).",1 Introduction,[0],[0]
"Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) (Quadrianto et al., 2009) we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimising cross-modal pairing with representation learning within each view.",1 Introduction,[0],[0]
"In contrast to prior work that uses a fixed representation, by non-linearly projecting monolingual world vectors before matching, we learn a new embedding where statistical dependency is easier to establish.",1 Introduction,[0],[0]
"Our method: (i) achieves similar unsupervised translation performance to recent adversarial methods, while being significantly easier to train and (ii) clearly outperforms prior non-adversarial methods.",1 Introduction,[0],[0]
"Let dataset D contain two sets of unpaired monolingual word embeddings from two languages D = ({xi}ni=1, {yj}nj=1) where x,y ∈ Rd.",2.1 Deep Distribution Matching,[0],[0]
"Let π be a permutation function over {1, 2, . . .",2.1 Deep Distribution Matching,[0],[0]
",",2.1 Deep Distribution Matching,[0],[0]
"n}, and Π the corresponding permutation indicator matrix: Π ∈ {0, 1}n×n,Π1n",2.1 Deep Distribution Matching,[0],[0]
"= 1n, and Π>1n = 1n.",2.1 Deep Distribution Matching,[0],[0]
Where 1n is the n-dimensional vector with all ones.,2.1 Deep Distribution Matching,[0],[0]
"We aim to optimize for both the permutation Π (bilingual dictionary), and non-linear transformations gx(·) and gy(·) of the respective wordvectors, that maximize statistical dependency between the views.",2.1 Deep Distribution Matching,[0],[0]
While regularising by requiring the original word embedding information is preserved through reconstruction using decoders fx(·) and fy(·).,2.1 Deep Distribution Matching,[0],[0]
"Our overall loss function is:
min Θx,Θy ,Π Ω(D; Θx,Θy)︸ ︷︷ ︸ Regularizer −λDΠ(D; Θx,Θy)︸ ︷︷ ︸ Dependency ,
DΠ(D; Θx,Θy) = DΠ({gx(xi), gy(yπ(i))}ni=1),
Ω(D; Θx,Θy) = n∑ i=1",2.1 Deep Distribution Matching,[0],[0]
"‖xi − fx(gx(xi))‖22
+ ‖yi",2.1 Deep Distribution Matching,[0],[0]
− fy(gy(yi))‖22 +R(Θx),2.1 Deep Distribution Matching,[0],[0]
"+R(Θy).
(1)
where Θs parameterize the encoding and reconstruction transformations, R(·) is a regularizer (e.g., `2-norm and `1-norm), and DΠ(·, ·) is a statistical dependency measure.",2.1 Deep Distribution Matching,[0],[0]
"Crucially compared to prior methods such as matching CCA (Haghighi
et al., 2008), dependency measures such as SMI do not need comparable representations to get started, making the bootstrapping problem less severe.",2.1 Deep Distribution Matching,[0],[0]
Squared-Loss Mutual Information (SMI),2.2 Dependence Estimation,[0],[0]
"The squared loss mutual information between two random variables x and y is defined as (Suzuki and Sugiyama, 2010):
SMI = ∫∫ ( p(x,y) p(x)p(y)",2.2 Dependence Estimation,[0],[0]
"− 1 )2 p(x)p(y)dxdy,
which is the Pearson divergence (Pearson, 1900) from p(x,y) to p(x)p(y).",2.2 Dependence Estimation,[0],[0]
"The SMI is an f - divergence (Ali and Silvey, 1966).",2.2 Dependence Estimation,[0],[0]
"That is, it is a non-negative measure and is zero only if the random variables are independent.
",2.2 Dependence Estimation,[0],[0]
"To measure SMI from a set of samples we take a direct density ratio estimation approach (Suzuki and Sugiyama, 2010), which leads (Yamada et al., 2015) to the estimator:
ŜMI({(xi,yi)}ni=1)",2.2 Dependence Estimation,[0],[0]
"= 1 2n tr (diag (α̂)KL)− 1 2 ,
where K ∈ Rn×n and L ∈ Rn×n are the gram matricies for x and y respectively, and
Ĥ = 1
n2 (KK>) ◦",2.2 Dependence Estimation,[0],[0]
"(LL>),
ĥ = 1
n",2.2 Dependence Estimation,[0],[0]
"(K ◦L)1n, α̂ =
( Ĥ + λIn )−1 ĥ,
λ > 0 is a regularizer and In ∈ Rn×n is the identity matrix.
",2.2 Dependence Estimation,[0],[0]
"SMI for Matching SMI computes the dependency between two sets of variables, under an assumption of known correspondence.",2.2 Dependence Estimation,[0],[0]
In our application this corresponds to a measure of dependency between two aligned sets of monolingual wordvectors.,2.2 Dependence Estimation,[0],[0]
"To exploit SMI for matching, we introduce a permutation variable Π by replacing L→ Π>LΠ in the estimator:
ŜMI({(xi,yπ(i))}n1 )",2.2 Dependence Estimation,[0],[0]
= 1 2n tr ( diag (α̂Π)KΠ >LΠ ),2.2 Dependence Estimation,[0],[0]
"− 1 2 ,
that will enable optimizing Π to maximize SMI.",2.2 Dependence Estimation,[0],[0]
"To initialize Θx and Θy, we first independently estimate them using autoencoders.",2.3 Optimization of parameters,[0],[0]
Then we employ an alternative optimization on Eq.,2.3 Optimization of parameters,[0],[0]
"(1) for
(Θx,Θy) and Π until convergence.",2.3 Optimization of parameters,[0],[0]
We use 3 layer MLP neural networks for both f and g. Algorithm 1 summarises the steps.,2.3 Optimization of parameters,[0],[0]
"Optimization for Θx and Θy With fixed permutation matrix Π (or π), the objective function
min Θx,Θy
Ω(D; Θx,Θy)− λDΠ(D; Θx,Θy) (2)
is an autoencoder optimization with regularizer DΠ(·), and can be solved with backpropagation.",2.3 Optimization of parameters,[0],[0]
"Optimization for Π To find the permutation (word matching) Π that maximizes SMI given fixed encoding parameters Θx,Θy, we only need to optimize the dependency term DΠ in Eq.",2.3 Optimization of parameters,[0],[0]
(1).,2.3 Optimization of parameters,[0],[0]
"We employ the LSOM algorithm (Yamada et al., 2015).",2.3 Optimization of parameters,[0],[0]
"The estimator of SMI for samples {gx(xi), gy(yπ(i))}ni=1 encoded with gx, gy is:
ŜMI = 1 2n tr ( diag (α̂Θ,Π)KΘxΠ >LΘyΠ )",2.3 Optimization of parameters,[0],[0]
"− 1 2 .
",2.3 Optimization of parameters,[0],[0]
"Which leads to the optimization problem:
max Π∈{0,1}n×n
tr (
diag (α̂Θ,Π)KΘxΠ >LΘyΠ ) s.t. Π1n",2.3 Optimization of parameters,[0],[0]
"= 1n,Π>1n = 1n.",2.3 Optimization of parameters,[0],[0]
"(3)
Since the optimization problem is NP-hard, we iteratively solve the relaxed problem (Yamada et al., 2015):
",2.3 Optimization of parameters,[0],[0]
"Πnew = (1− η)Πold+
η argmax Π
tr ( diag ( α̂Θ,Πold ) KΘxΠ >LΘyΠ old ) ,
where 0 <",2.3 Optimization of parameters,[0],[0]
η ≤ 1 is a step size.,2.3 Optimization of parameters,[0],[0]
The optimization problem is a linear assignment problem (LAP).,2.3 Optimization of parameters,[0],[0]
"Thus, we can efficiently solve the algorithm by using the Hungarian method (Kuhn, 1955).",2.3 Optimization of parameters,[0],[0]
"To get discrete Π, we solve the last step by setting η = 1.
",2.3 Optimization of parameters,[0],[0]
"Intuitively, this can be seen as searching for the permutation Π for which the data in the two (initially unsorted views) have a matching withinview affinity (gram) matrix, where matching is defined by maximum SMI.",2.3 Optimization of parameters,[0],[0]
"In this section, we evaluate the efficacy of our proposed method against various state of the art methods for word translation.",3 Experiments,[0],[0]
Implementation Details,3 Experiments,[0],[0]
Our autoencoder consists of two layers with dropout and a tanh nonlinearity.,3 Experiments,[0],[0]
"We use polynomial kernel to compute
Algorithm 1 SMI-based unsupervised word translation Input: Unpaired word embeddings D = ({xi}ni=1, {yj}nj=1).
1: Init: weights Θx, Θy, permutation matrix Π. 2: while not converged do 3: Update Θx,Θy given Π: Backprop (2).",3 Experiments,[0],[0]
"4: Update Π given Θx,Θy: LSOM (3).",3 Experiments,[0],[0]
"5: end while
Output: Permutation Matrix Π. Params Θx, Θy.
the gram matrices K and L.",3 Experiments,[0],[0]
"For all pairs of languages, we fix the number of training epochs to 20.",3 Experiments,[0],[0]
All the word vectors are `2 unit normalized.,3 Experiments,[0],[0]
For CSLS we set the number of neighbors to 10.,3 Experiments,[0],[0]
"For optimizing Π at each epoch, we set the step size η = 0.75 and use 20 iterations.",3 Experiments,[0],[0]
"For the regularization R(Θ), we use the sum of the Frobenius norms of weight matrices.",3 Experiments,[0],[0]
"We train Θ using full batch gradient-descent, with learning rate 0.05.",3 Experiments,[0],[0]
"Datasets We performed experiments on the publicly available English-Italian, EnglishSpanish and English-Chinese datasets released by (Dinu and Baroni, 2015; Zhang et al., 2017b; Vulic and Moens, 2013).",3 Experiments,[0],[0]
We name this collective set of benchmarks BLI.,3 Experiments,[0],[0]
"We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018)1.",3 Experiments,[0],[0]
"Setting and Metrics We evaluate all methods in terms of Precision@1, following standard practice.",3 Experiments,[0],[0]
"We note that while various methods in the literature were initially presented as fully supervised (Mikolov et al., 2013), semi-supervised (using a seed dictionary)",3 Experiments,[0],[0]
"(Haghighi et al., 2008), or unsupervised (Zhang et al., 2017b), most of them can be straightforwardly adapted to run in any of these settings.",3 Experiments,[0],[0]
"Therefore we evaluate all methods both in the unsupervised setting in which we are primarily interested, and also the commonly evaluated semi-supervised setting with 500 seed pairs.",3 Experiments,[0],[0]
"Competitors: Non-Adversarial In terms of competitors that, like us, do not make use of GANs, we evaluate: Translation Matrix (Mikolov et al., 2013), which alternates between estimating a linear transformation by least squares and matching by nearest neighbour (NN).",3 Experiments,[0],[0]
"Multilingual Correlation (Faruqui and Dyer, 2014), and Matching CCA (Haghighi et al., 2008), which alternates between matching and estimat-
1https://github.com/facebookresearch/MUSE/
ing a joint linear subspace.",3 Experiments,[0],[0]
"Kernelized Sorting (Quadrianto et al., 2009), which directly uses HSIC-based statistical dependency to match heterogeneous data points.",3 Experiments,[0],[0]
"Self Training (Artetxe et al., 2017)",3 Experiments,[0],[0]
"A recent state of the art method that alternate between estimating an orthonormal transformation, and NN matching.
",3 Experiments,[0],[0]
"Competitors: Adversarial In terms of competitors that do make use of adversarial training, we compare: W-GAN and EMDOT (Zhang et al., 2017b) make use of adversarial learning using Wasserstein GAN and Earth Movers Distance respectively.",3 Experiments,[0],[0]
"GAN-NN (Conneau et al., 2018) uses adversarial learning to train an orthogonal transformation, along with some refinement steps and an improvement to the conventional NN matching procedure called ‘cross-domain similarity lo-
cal scaling’ (CSLS).",3 Experiments,[0],[0]
"Since this is a distinct step, we also evaluate our method with CSLS.
",3 Experiments,[0],[0]
"We use the provided code for GAN-NN and Self-Train, while re-implementing EDOT/WGAN to avoid dependency on theano.",3 Experiments,[0],[0]
Fully Unsupervised Table 1 presents comparative results for unsupervised word translation on BLI and MUSE.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
Our method (bottom) is consistently and significantly better than non-adversarial alternatives (top).,3.1 Results,[0],[0]
(ii),3.1 Results,[0],[0]
"Compared to adversarial alternatives Deep-SMI performs comparably.
",3.1 Results,[0],[0]
All methods generally perform better on the MUSE dataset than BLI.,3.1 Results,[0],[0]
"These differences are due to a few factors: MUSE is a significantly
larger dataset than BLI, benefitting methods that can exploit a large amount of training data.",3.1 Results,[0],[0]
"In the ground-truth annotation, BLI contains 1-1 translations while MUSE contains more realistic 1-many translations (if any correct translation is picked, a success is counted), making it easier to reach a higher score.
",3.1 Results,[0],[0]
Semi-supervised Results using a 500-word bilingual seed dictionary are presented in Table 2.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
"The conventional methods’ performances (top) jump up, showing that they are more competitive if at least some sparse data is available.",3.1 Results,[0],[0]
"(ii) Deep-SMI performance also improves, and still outperforms the classic methods significantly overall.",3.1 Results,[0],[0]
"(iii) Again, we perform comparably to the GAN methods.",3.1 Results,[0],[0]
Figure 1 shows the convergence process of DeepSMI.,3.2 Discussion,[0],[0]
"From this we see that: (i) Unlike the adversarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess.",3.2 Discussion,[0],[0]
"(ii) Unlike the adversarial methods, our accuracy generally mirrors the model’s loss.",3.2 Discussion,[0],[0]
"In contrast, the various losses of the adversarial approaches do not well reflect translation accuracy, making model selection or early stopping a challenge in itself.",3.2 Discussion,[0],[0]
"Please compare our Figure 1 with Fig 3 in Zhang et al. (2017b), and Fig 2 in Conneau et al. (2018).
",3.2 Discussion,[0],[0]
There are two steps in our optimization: matching permutation Π and representation weights Θ.,3.2 Discussion,[0],[0]
"Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing latent variables (Π) and parameters (Θ).",3.2 Discussion,[0],[0]
"While local minima are a risk, every optimisation step for either variable reduces our objective Eq.",3.2 Discussion,[0],[0]
"(1).
",3.2 Discussion,[0],[0]
"There is no min-max game, so no risk of divergence as in the case of adversarial GAN-type methods.
",3.2 Discussion,[0],[0]
Our method can also be understood as providing an unsupervised Deep-CCA type model for relating heterogeneous data across two views.,3.2 Discussion,[0],[0]
"This is in contrast to the recently proposed unsupervised shallow CCA (Hoshen and Wolf, 2018), and conventional supervised Deep-CCA (Chang et al., 2018) that requires paired data for training; and using SMI rather than correlation as the optimisation objective.",3.2 Discussion,[0],[0]
We have presented an effective approach to unsupervised word translation that performs comparably to adversarial approaches while being significantly easier to train and diagnose; as well as outperforming prior non-adversarial approaches.,4 Conclusion,[0],[0]
"Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks.",abstractText,[0],[0]
"Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora.",abstractText,[0],[0]
"However, state of the art methods for unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyperparameter sensitivity.",abstractText,[0],[0]
We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised – no seed dictionary or parallel corpora required; and introduces no adversary – therefore being much easier to train.,abstractText,[0],[0]
Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.,abstractText,[0],[0]
Learning Unsupervised Word Translations Without Adversaries,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1024–1034 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Learning word representations has become a fundamental problem in processing natural languages.,1 Introduction,[0],[0]
"These semantic representations, which map a word into a point in a linear space, have been widely applied in downstream applications, including named entity recognition (Guo et al., 2014), document ranking (Nalisnick et al., 2016), sentiment analysis (Irsoy and Cardie, 2014), question answering (Antol et al., 2015), and image captioning (Karpathy and Fei-Fei, 2015).
",1 Introduction,[0],[0]
"Over the past few years, various approaches have been proposed to learn word vectors (e.g., (Pennington et al., 2014; Mikolov et al., 2013a; Levy and Goldberg, 2014b; Ji et al., 2015)) based on co-occurrence information between words observed on the training corpus.",1 Introduction,[0],[0]
"The intuition behind this is to represent words with similar vectors if
they have similar contexts.",1 Introduction,[0],[0]
"To learn a good word embedding, most approaches assume a large collection of text is freely available, such that the estimation of word co-occurrences is accurate.",1 Introduction,[0],[0]
"For example, the Google Word2Vec model (Mikolov et al., 2013a) is trained on the Google News dataset, which contains around 100 billion tokens, and the GloVe embedding (Pennington et al., 2014) is trained on a crawled corpus that contains 840 billion tokens in total.",1 Introduction,[0],[0]
"However, such an assumption may not hold for low-resource languages such as Inuit or Sindhi, which are not spoken by many people or have not been put into a digital format.",1 Introduction,[0],[0]
"For those languages, usually, only a limited size corpus is available.",1 Introduction,[0],[0]
"Training word vectors under such a setting is a challenging problem.
",1 Introduction,[0],[0]
One key restriction of the existing approaches is that they often mainly rely on the word pairs that are observed to co-occur on the training data.,1 Introduction,[0],[0]
"When the size of the text corpus is small, most word pairs are unobserved, resulting in an extremely sparse co-occurrence matrix (i.e., most entries are zero)1.",1 Introduction,[0],[0]
"For example, the text82 corpus has about 17,000,000 tokens and 71,000 distinct words.",1 Introduction,[0],[0]
"The corresponding co-occurrence matrix has more than five billion entries, but only about 45,000,000 are non-zeros (observed on the training corpus).",1 Introduction,[0],[0]
"Most existing approaches, such as Glove and Skip-gram, cannot handle a vast number of zero terms in the co-occurrence matrix; therefore, they only sub-sample a small subset of zero entries during the training.
",1 Introduction,[0],[0]
"In contrast, we argue that the unobserved word pairs can provide valuable information for training a word embedding model, especially when the co-occurrence matrix is very sparse.",1 Introduction,[0],[0]
"Inspired
1Note that the zero term can mean either the pairs of words cannot co-occur or the co-occurrence is not observed in the training corpus.
2http://mattmahoney.net/dc/text8.zip
1024
by the success of Positive-Unlabeled Learning (PU-Learning) in collaborative filtering applications (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015), we design an algorithm to effectively learn word embeddings from both positive (observed terms) and unlabeled (unobserved/zero terms) examples.",1 Introduction,[0],[0]
"Essentially, by using the square loss to model the unobserved terms and designing an efficient update rule based on linear algebra operations, the proposed PULearning framework can be trained efficiently and effectively.
",1 Introduction,[0],[0]
We evaluate the performance of the proposed approach in English3 and other three resourcescarce languages.,1 Introduction,[0],[0]
"We collected unlabeled language corpora from Wikipedia and compared the proposed approach with popular approaches, the Glove and the Skip-gram models, for training word embeddings.",1 Introduction,[0],[0]
"The experimental results show that our approach significantly outperforms the baseline models, especially when the size of the training corpus is small.
",1 Introduction,[0],[0]
"Our key contributions are summarized below.
",1 Introduction,[0],[0]
"• We propose a PU-Learning framework for learning word embedding.
",1 Introduction,[0],[0]
"• We tailor the coordinate descent algorithm (Yu et al., 2017b) for solving the corresponding optimization problem.
",1 Introduction,[0],[0]
• Our experimental results show that PULearning improves the word embedding training in the low-resource setting.,1 Introduction,[0],[0]
Learning word vectors.,2 Related work,[0],[0]
"The idea of learning word representations can be traced back to Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), where word vectors are generated by factorizing a worddocument and word-word co-occurrence matrix, respectively.",2 Related work,[0],[0]
"Similar approaches can also be extended to learn other types of relations between words (Yih et al., 2012; Chang et al., 2013) or entities (Chang et al., 2014).",2 Related work,[0],[0]
"However, due to the limitation of the use of principal component analysis,
3Although English is not a resource-scarce language, we simulate the low-resource setting in an English corpus.",2 Related work,[0],[0]
"In this way, we leverage the existing evaluation methods to evaluate the proposed approach.
",2 Related work,[0],[0]
these approaches are often less flexible.,2 Related work,[0],[0]
"Besides, directly factorizing the co-occurrence matrix may cause the frequent words dominating the training objective.
",2 Related work,[0],[0]
"In the past decade, various approaches have been proposed to improve the training of word embeddings.",2 Related work,[0],[0]
"For example, instead of factorizing the co-occurrence count matrix, Bullinaria and Levy (2007); Levy and Goldberg (2014b) proposed to factorize point-wise mutual information (PMI) and positive PMI (PPMI) matrices as these metrics scale the co-occurrence counts (Bullinaria and Levy, 2007; Levy and Goldberg, 2014b).",2 Related work,[0],[0]
"Skipgram model with negative-sampling (SGNS) and Continuous Bag-of-Words models (Mikolov et al., 2013b) were proposed for training word vectors on a large scale without consuming a large amount of memory.",2 Related work,[0],[0]
"GloVe (Pennington et al., 2014) is proposed as an alternative to decompose a weighted log co-occurrence matrix with a bias term added to each word.",2 Related work,[0],[0]
"Very recently, WordRank model (Ji et al., 2015) has been proposed to minimize a ranking loss which naturally fits the tasks requiring ranking based evaluation metrics.",2 Related work,[0],[0]
Stratos et al. (2015) also proposed CCA (canonical correlation analysis)-based word embedding which shows competitive performance.,2 Related work,[0],[0]
"All these approaches focus on the situations where a large text corpus is available.
",2 Related work,[0],[0]
"Positive and Unlabeled (PU) Learning: Positive and Unlabeled (PU) learning (Li and Liu, 2005) is proposed for training a model when the positive instances are partially labeled and the unlabeled instances are mostly negative.",2 Related work,[0],[0]
"Recently, PU learning has been used in many classification and collaborative filtering applications due to the nature of “implicit feedback” in many recommendation systems—users usually only provide positive feedback (e.g., purchases, clicks) and it is very hard to collect negative feedback.
",2 Related work,[0],[0]
"To resolve this problem, a series of PU matrix completion algorithms have been proposed (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015; Yu et al., 2017b).",2 Related work,[0],[0]
The main idea is to assign a small uniform weight to all the missing or zero entries and factorize the corresponding matrix.,2 Related work,[0],[0]
"Among them, Yu et al. (2017b) proposed an efficient algorithm for matrix factorization with PU-learning, such that the weighted matrix is constructed implicitly.",2 Related work,[0],[0]
"In this paper, we
W, C vocabulary of central and context words m,n vocabulary sizes k dimension of word vectors W,H m× k and n×",2 Related work,[0],[0]
"k latent matrices Cij weight for the (i, j) entry Aij value of the PPMI matrix Qij value of the co-occurrence matrix wi,hj i-th row of W and j-th row of H b, b̂ bias term λi, λj regularization parameters | · | the size of a set Ω Set of possible word-context pairs Ω+ Set of observed word-context pairs Ω− Set of unobserved word-context pairs
Table 1: Notations.
design a new approach for training word vectors by leveraging the PU-Learning framework and existing word embedding techniques.",2 Related work,[0],[0]
"To the best of our knowledge, this is the first work to train word embedding models using the PU-learning framework.",2 Related work,[0],[0]
"Similar to GloVe and other word embedding learning algorithms, the proposed approach consists of three steps.",3 PU-Learning for Word Embedding,[0],[0]
The first step is to construct a cooccurrence matrix.,3 PU-Learning for Word Embedding,[0],[0]
"Follow the literature (Levy and Goldberg, 2014a), we use the PPMI metric to measure the co-occurrence between words.",3 PU-Learning for Word Embedding,[0],[0]
"Then, in the second step, a PU-Learning approach is applied to factorize the co-occurrence matrix and generate word vectors and context vectors.",3 PU-Learning for Word Embedding,[0],[0]
"Finally, a post-processing step generates the final embedding vector for each word by combining the word vector and the context vector.
",3 PU-Learning for Word Embedding,[0],[0]
We summarize the notations used in this paper in Table 1 and describe the details of each step in the remainder of this section.,3 PU-Learning for Word Embedding,[0],[0]
Various metrics can be used for estimating the co-occurrence between words in a corpus.,3.1 Building the Co-Occurrence Matrix,[0],[0]
"PPMI metric stems from point-wise mutual information (PMI) which has been widely used as a measure of word association in NLP for various tasks (Church and Hanks, 1990).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In our case, each entry PMI(w, c) represents the relevant measure between a word w and a context word c by calculating the ratio between their joint probability (the
chance they appear together in a local context window) and their marginal probabilities (the chance they appear independently) (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"More specifically, each entry of PMI matrix can be defined by
PMI(w, c) = log P̂ (w, c)
P̂ (w) ·",3.1 Building the Co-Occurrence Matrix,[0],[0]
"P̂ (c) , (1)
where P̂ (w), P̂ (c) and P̂ (w, c) are the the frequency of word w, word c, and word pairs (w, c), respectively.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"The PMI matrix can be computed based on the co-occurrence counts of word pairs, and it is an information-theoretic association measure which effectively eliminates the big differences in magnitude among entries in the cooccurrence matrix.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Extending from the PMI metric, the PPMI metric replaces all the negative entries in PMI matrix by 0:
PPMI(w, c) = max(PMI(w, c), 0).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"(2)
The intuition behind this is that people usually perceive positive associations between words (e.g. “ice” and “snow”).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In contrast, the negative association is hard to define (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Therefore, it is reasonable to replace the negative entries in the PMI matrix by 0, such that the negative association is treated as “uninformative”.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Empirically, several existing works (Levy et al., 2015; Bullinaria and Levy, 2007) showed that the PPMI metric achieves good performance on various semantic similarity tasks.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In practice, we follow the pipeline described in Levy et al. (2015) to build the PPMI matrix and apply several useful tricks to improve its quality.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"First, we apply a context distribution smoothing mechanism to enlarge the probability of sampling a rare context.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In particular, all context counts are scaled to the power of α.4:
PPMIα(w, c) = max
( log P̂ (w, c)
P̂ (w)P̂α(c) , 0
)
P̂α(c) = #(c)α∑ c̄ #(c̄) α ,
where #(w) denotes the number of times word w appears.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"This smoothing mechanism effectively
4Empirically, α = 0.75 works well (Mikolov et al., 2013b).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"alleviates PPMI’s bias towards rare words (Levy et al., 2015).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Next, previous studies show that words that occur too frequent often dominate the training objective (Levy et al., 2015) and degrade the performance of word embedding.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"To avoid this issue, we follow Levy et al. (2015) to sub-sample words with frequency more than a threshold twith a probability p defined as:
p = 1− √ t
P̂ (w) .",3.1 Building the Co-Occurrence Matrix,[0],[0]
We proposed a matrix factorization based word embedding model which aims to minimize the reconstruction error on the PPMI matrix.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"The lowrank embeddings are obtained by solving the following optimization problem:
min W,H
∑
i,j∈Ω",3.2 PU-Learning for Matrix Factorization,[0],[0]
Cij(Aij −wTi hj,3.2 PU-Learning for Matrix Factorization,[0],[0]
"− bi − b̂j)2 + ∑
i
λi‖wi‖2 + ∑
j
λj‖hj‖2, (3)
where W and H are m× k and n× k latent matrices, representing words and context words, respectively.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The first term in Eq.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(3) aims for minimizing reconstruction error, and the second and third terms are regularization terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
λi and λj are weights of regularization term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"They are hyperparameters that need to be tuned.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The zero entries in co-occurrence matrix denote that two words never appear together in the current corpus, which also refers to unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The unobserved term can be either real zero (two words shouldn’t be co-occurred even when we use very large corpus) or just missing in the small corpus.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In contrast to SGNS sub-sampling a small set of zero entries as negative samples, our model will try to use the information from all zeros.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The set Ω includes all the |W| × |C| entries— both positive and zero entries:
Ω = Ω+ ∪ Ω−. (4)
Note that we define the positive samples Ω+ to be all the (w, c) pairs that appear at least one time in the corpus, and negative samples Ω− are word pairs that never appear in the corpus.
Weighting function.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Eq (3) is very similar to the one used in previous matrix factorization approaches such as GloVe, but we propose a new way to set the weights Cij .",3.2 PU-Learning for Matrix Factorization,[0],[0]
"If we set equal weights for all the entries, then Cij = constant, and the model is very similar to conducting SVD for the PPMI matrix.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Previous work has shown that this approach often suffers from poor performance (Pennington et al., 2014).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"More advanced methods, such as GloVe, set non-uniform weights for observed entries to reflect their confidence.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"However, the time complexity of their algorithm is proportional to number of nonzero weights (|(i, j) | Cij 6= 0|), thus they have to set zero weights for all the unobserved entries (Cij = 0 for Ω−), or try to incorporate a small set of unobserved entries by negative sampling.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We propose to set the weights for Ω+ and Ω−
differently using the following scheme:
Cij =   (Qij/xmax)",3.2 PU-Learning for Matrix Factorization,[0],[0]
"α, if Qij ≤ xmax, and (i, j) ∈ Ω+
1,",3.2 PU-Learning for Matrix Factorization,[0],[0]
"if Qij > xmax, and (i, j) ∈ Ω+",3.2 PU-Learning for Matrix Factorization,[0],[0]
"ρ, (i, j) ∈ Ω−
(5)
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Here xmax and α are re-weighting parameters, and ρ is the unified weight for unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We will discuss them later.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω+, we set the non-uniform weights as in GloVe (Pennington et al., 2014), which assigns larger weights to context word that appears more often with the given word, but also avoids overwhelming the other terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω−, instead of setting their weights to be 0, we assign a small constant weight ρ.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The main idea is from the literature of PU-learning (Hu et al., 2008; Hsieh et al., 2015): although missing entries are highly uncertain, they are still likely to be true 0, so we should incorporate them in the learning process but multiplying with a smaller weight according to the uncertainty.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Therefore, ρ in (5) reflects how confident we are to the zero entries.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In our experiments, we set xmax = 10, α = 3/4 according to (Pennington et al., 2014), and let ρ be a parameter to tune.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Experiments show that adding weighting function obviously improves the performance especially on analogy tasks.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
Bias term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"Unlike previous work on PU matrix completion (Yu et al., 2017b; Hsieh et al., 2015), we add the bias terms for word and context word
vectors.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Instead of directly using w>i hj to approximate Aij , we use
Aij ≈ w>i hj + bi + b̂j .
Yu et al. (2017b) design an efficient columnwise coordinate descent algorithm for solving the PU matrix factorization problem; however, they do not consider the bias term in their implementations.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"To incorporate the bias term in (3), we propose the following training algorithm based on the coordinate descent approach.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Our algorithm does not introduce much overhead compared to that in (Yu et al., 2017b).
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We augment each wi,hj ∈ Rk into the following (k + 2) dimensional vectors:
w′i =   wi1 ...",3.2 PU-Learning for Matrix Factorization,[0],[0]
"wik 1 bi   h′j =   hj1 ... hjk b̂j 1  
Therefore, for each word and context vector, we have the following equality
〈w′i,h′j〉 = 〈wi,hj〉+ bi + b̂j ,
which means the loss function in (3) can be written as
∑
i,j∈Ω Cij(Aij −w′>i h′j)2.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Also, we denote W ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[w′1,w ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",w ′ n] > and H ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[h′1,h ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",h ′ n]",3.2 PU-Learning for Matrix Factorization,[0],[0]
>.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the column-wise coordinate descent method, at each iteration we pick a t ∈ {1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", (k+2)}, and update the t-th column of W ′ and H ′. The updates can be derived for the following two cases:
a. When t ≤ k, the elements in the t-th column is w1t, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", wnt and we can directly use the update rule derived in Yu et al. (2017b) to update them.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
b.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k + 1, we do not update the corresponding column of W ′ since the elements are all 1, and we use the similar coordinate descent update to update the k+ 1-th column of H ′ (corresponding to b̂1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", b̂n).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k+2, we do not update the corresponding column of H ′",3.2 PU-Learning for Matrix Factorization,[0],[0]
(they are all 1) and we update the k+ 2-th column of W ′,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(corresponding to b1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", bn) using coordinate descent.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"With some further derivations, we can show that the algorithm only requires O(nnz(A) + nk) time to update each column,5 so the overall complexity is O(nnz(A)k + nk2) time per epoch, which is only proportional to number of nonzero terms in A. Therefore, with the same time complexity as GloVe, we can utilize the information from all the zero entries in A instead of only sub-sampling a small set of zero entries.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the PU-Learning formulation, ρ represents the unified weight that assigned to the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"Intuitively, ρ reflects the confidence on unobserved entries—larger ρmeans that we are quite certain about the zeroes, while small ρ indicates the many of unobserved pairs are not truly zero.",3.3 Interpretation of Parameters,[0],[0]
"When ρ = 0, the PU-Learning approach reduces to a model similar to GloVe, which discards all the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"In practice, ρ is an important parameter to tune, and we find that ρ = 0.0625 achieves the best results in general.",3.3 Interpretation of Parameters,[0],[0]
"Regarding the other parameter, λ is the regularization term for preventing the embedding model from overfitting.",3.3 Interpretation of Parameters,[0],[0]
"In practice, we found the performance is not very sensitive to λ as long as it is resonably small.",3.3 Interpretation of Parameters,[0],[0]
"More discussion about the parameter setting can be found in Section 5.
",3.3 Interpretation of Parameters,[0],[0]
Post-processing of Word/Context Vectors The PU-Learning framework factorizes the PPMI matrix and generates two vectors for each word,3.3 Interpretation of Parameters,[0],[0]
"i, wi ∈ Rk and hi ∈ Rk.",3.3 Interpretation of Parameters,[0],[0]
The former represents the word when it is the central word and the latter represents the word when it is in context.,3.3 Interpretation of Parameters,[0],[0]
Levy et al. (2015) shows that averaging these two vectors (uavgi = wi + hi) leads to consistently better performance.,3.3 Interpretation of Parameters,[0],[0]
The same trick of constructing word vectors is also used in GloVe.,3.3 Interpretation of Parameters,[0],[0]
"Therefore, in the experiments, we evaluate all models with uavg.",3.3 Interpretation of Parameters,[0],[0]
Our goal in this paper is to train word embedding models for low-resource languages.,4 Experimental Setup,[0],[0]
"In this section, we describe the experimental designs to evaluate the proposed PU-learning approach.",4 Experimental Setup,[0],[0]
We first describe the data sets and the evaluation metrics.,4 Experimental Setup,[0],[0]
"Then, we provide details of parameter tuning.
",4 Experimental Setup,[0],[0]
5Here we assume m = n for the sake of simplicity.,4 Experimental Setup,[0],[0]
"And, nnz(A) denotes the number of nonzero terms in the matrix A.",4 Experimental Setup,[0],[0]
"We consider two widely used tasks for evaluating word embeddings, the word similarity task and the word analogy task.",4.1 Evaluation tasks,[0],[0]
"In the word similarity task, each question contains a word pairs and an annotated similarity score.",4.1 Evaluation tasks,[0],[0]
The goal is to predict the similarity score between two words based on the inner product between the corresponding word vectors.,4.1 Evaluation tasks,[0],[0]
"The performance is then measured by the Spearmans rank correlation coefficient, which estimates the correlation between the model predictions and human annotations.",4.1 Evaluation tasks,[0],[0]
"Following the settings in literature, the experiments are conducted on five data sets, WordSim353 (Finkelstein et al., 2001), WordSim Similarity (Zesch et al., 2008), WordSim Relatedness (Agirre et al., 2009), Mechanical Turk (Radinsky et al., 2011) and MEN (Bruni et al., 2012).
",4.1 Evaluation tasks,[0],[0]
"In the word analogy task, we aim at solving analogy puzzles like “man is to woman as king is to ?”, where the expected answer is “queen.”",4.1 Evaluation tasks,[0],[0]
"We consider two approaches for generating answers to the puzzles, namely 3CosAdd and 3CosMul (see (Levy and Goldberg, 2014a) for details).",4.1 Evaluation tasks,[0],[0]
"We evaluate the performances on Google analogy dataset (Mikolov et al., 2013a) which contains 8,860 semantic and 10,675 syntactic questions.",4.1 Evaluation tasks,[0],[0]
"For the analogy task, only the answer that exactly matches the annotated answer is counted as correct.",4.1 Evaluation tasks,[0],[0]
"As a result, the analogy task is more difficult than the similarity task because the evalu-
ation metric is stricter and it requires algorithms to differentiate words with similar meaning and find the right answer.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performances of models in the low-resource setting, we train word embedding models on Dutch, Danish, Czech and, English data sets collected from Wikipedia.",4.1 Evaluation tasks,[0],[0]
"The original Wikipedia corpora in Dutch, Danish, Czech and English contain 216 million, 47 million, 92 million, and 1.8 billion tokens, respectively.",4.1 Evaluation tasks,[0],[0]
"To simulate the low-resource setting, we sub-sample the Wikipedia corpora and create a subset of 64 million tokens for Dutch and Czech and a subset of 32 million tokens for English.",4.1 Evaluation tasks,[0],[0]
"We will demonstrate how the size of the corpus affects the performance of embedding models in the experiments.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performance of word embeddings in Czech, Danish, and Dutch, we translate the English similarity and analogy test sets to the other languages by using Google Cloud Translation API6.",4.1 Evaluation tasks,[0],[0]
"However, an English word may be translated to multiple words in another language (e.g., compound nouns).",4.1 Evaluation tasks,[0],[0]
We discard questions containing such words (see Table 3 for details).,4.1 Evaluation tasks,[0],[0]
"Because all approaches are compared on the same test set for each language, the comparisons are fair.",4.1 Evaluation tasks,[0],[0]
"We compare the proposed approach with two baseline methods, GloVe and SGNS.",4.2 Implementation and Parameter Setting,[0],[0]
"The imple-
6https://cloud.google.com/translate
mentations of Glove7 and SGNS8 and provided by the original authors, and we apply the default settings when appropriate.",4.2 Implementation and Parameter Setting,[0],[0]
The proposed PULearning framework is implemented based on Yu et al. (2017a).,4.2 Implementation and Parameter Setting,[0],[0]
"With the implementation of efficient update rules, our model requires less than 500 seconds to perform one iteration over the entire text8 corpus, which consists of 17 million tokens 9.",4.2 Implementation and Parameter Setting,[0],[0]
"All the models are implemented in C++.
",4.2 Implementation and Parameter Setting,[0],[0]
"We follow Levy et al. (2015)10 to set windows size as 15, minimal count as 5, and dimension of word vectors as 300 in the experiments.",4.2 Implementation and Parameter Setting,[0],[0]
Training word embedding models involves selecting several hyper-parameters.,4.2 Implementation and Parameter Setting,[0],[0]
"However, as the word embeddings are usually evaluated in an unsupervised setting (i.e., the evaluation data sets are not seen during the training), the parameters should not be tuned on each dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"To conduct a fair comparison, we tune hyper-parameters on the text8 dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"For GloVe model, we tune the discount parameters xmax and find that xmax = 10 per-
7https://nlp.stanford.edu/projects/glove 8https://code.google.com/archive/p/word2vec/ 9http://mattmahoney.net/dc/text8.zip
10https://bitbucket.org/omerlevy/hyperwords
forms the best.",4.2 Implementation and Parameter Setting,[0],[0]
SGNS has a natural parameter k which denotes the number of negative samples.,4.2 Implementation and Parameter Setting,[0],[0]
"Same as Levy et al. (2015), we found that setting k to 5 leads to the best performance.",4.2 Implementation and Parameter Setting,[0],[0]
"For the PU-learning model, ρ and λ are two important parameters that denote the unified weight of zero entries and the weight of regularization terms, respectively.",4.2 Implementation and Parameter Setting,[0],[0]
We tune ρ in a range from 2−1 to 2−14 and λ in a range from 20 to 2−10.,4.2 Implementation and Parameter Setting,[0],[0]
We analyze the sensitivity of the model to these hyper-parameters in the experimental result section.,4.2 Implementation and Parameter Setting,[0],[0]
The best performance of each model on the text8 dataset is shown in the Table 2.,4.2 Implementation and Parameter Setting,[0],[0]
It shows that PU-learning model outperforms two baseline models.,4.2 Implementation and Parameter Setting,[0],[0]
"We compared the proposed PU-Learning framework with two popular word embedding models – SGNS (Mikolov et al., 2013b) and Glove (Pennington et al., 2014) on English and three other languages.",5 Experimental Results,[0],[0]
The experimental results are reported in Table 4.,5 Experimental Results,[0],[0]
The results show that the proposed PULearning framework outperforms the two baseline approaches significantly in most datasets.,5 Experimental Results,[0],[0]
"This re-
sults confirm that the unobserved word pairs carry important information and the PU-Learning model leverages such information and achieves better performance.",5 Experimental Results,[0],[0]
"To better understand the model, we conduct detailed analysis as follows.
",5 Experimental Results,[0],[0]
"Performance v.s. Corpus size We investigate the performance of our algorithm with respect to different corpus size, and plot the results in Figure 1.",5 Experimental Results,[0],[0]
"The results in analogy task are obtained by 3CosMul method (Levy and Goldberg, 2014a).",5 Experimental Results,[0],[0]
"As the corpus size grows, the performance of all models improves, and the PU-learning model consistently outperforms other methods in all the tasks.",5 Experimental Results,[0],[0]
"However, with the size of the corpus increases, the difference becomes smaller.",5 Experimental Results,[0],[0]
"This is reasonable as when the corpus size increases the number of nonzero terms becomes smaller and the PU-learning approach is resemblance to Glove.
",5 Experimental Results,[0],[0]
Impacts of ρ and λ,5 Experimental Results,[0],[0]
"We investigate how sensitive the model is to the hyper-parameters, ρ and λ.",5 Experimental Results,[0],[0]
"Figure 2 shows the performance along with various values of λ and ρ when training on the text8 corpus, respectively.",5 Experimental Results,[0],[0]
Note that the x-axis is in log scale.,5 Experimental Results,[0],[0]
"When ρ is fixed, a big λ degrades the performance of the model significantly.",5 Experimental Results,[0],[0]
This is because when λ is too big the model suffers from underfitting.,5 Experimental Results,[0],[0]
"The model is less sensitive when λ is small and in general, λ = 2−11 achieves consistently good performance.
",5 Experimental Results,[0],[0]
"When λ is fixed, we observe that large ρ (e.g., ρ ≈ 2−4) leads to better performance.",5 Experimental Results,[0],[0]
"As ρ represents the weight assigned to the unobserved term, this result confirms that the model benefits from using the zero terms in the co-occurrences matrix.",5 Experimental Results,[0],[0]
"In this paper, we presented a PU-Learning framework for learning word embeddings of lowresource languages.",6 Conclusion,[0],[0]
"We evaluated the proposed approach on English and other three languages and showed that the proposed approach outperforms other baselines by effectively leveraging the information from unobserved word pairs.
",6 Conclusion,[0],[0]
"In the future, we would like to conduct experiments on other languages where available text corpora are relatively hard to obtain.",6 Conclusion,[0],[0]
"We are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small.",6 Conclusion,[0],[0]
"Besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.
",6 Conclusion,[0],[0]
"Acknowledge
This work was supported in part by National Science Foundation Grant IIS-1760523, IIS-1719097 and an NVIDIA Hardware Grant.",6 Conclusion,[0],[0]
Word embedding is a key component in many downstream applications in processing natural languages.,abstractText,[0],[0]
Existing approaches often assume the existence of a large collection of text for learning effective word embedding.,abstractText,[0],[0]
"However, such a corpus may not be available for some low-resource languages.",abstractText,[0],[0]
"In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens.",abstractText,[0],[0]
"In such a situation, the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unobserved.",abstractText,[0],[0]
"In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information.",abstractText,[0],[0]
We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.,abstractText,[0],[0]
Learning Word Embeddings for Low-resource Languages by PU Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4829–4833 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4829",text,[0],[0]
Co-reference resolution requires models to cluster mentions that refer to the same physical entities.,1 Introduction,[0],[0]
The models based on neural networks typically require different levels of semantic representations of input sentences.,1 Introduction,[0],[0]
"The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and wordlevel embeddings (Turian et al., 2010; Pennington et al., 2014) before predicting antecedents.",1 Introduction,[0],[0]
"The mention-level embeddings are used to make coreference decisions, typically by scoring mention pairs and making links (Lee et al., 2017; Clark and Manning, 2016a; Wiseman et al., 2016).",1 Introduction,[0],[0]
"Long short-term memories (LSTMs) are often used to encode the syntactic and semantic information of input sentences.
",1 Introduction,[0],[0]
Articles and conversations include more than one sentences.,1 Introduction,[0],[0]
"Considering the accuracy and efficiency of co-reference resolution models, the encoder LSTM usually processes input sentences separately as a batch (Lee et al., 2017).",1 Introduction,[0],[0]
"The disadvantage of this method is that the models do not consider the dependency among words from different sentences, which plays a significant role in word representation learning and co-reference predicting.",1 Introduction,[0],[0]
"For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency information.",1 Introduction,[0],[0]
"As a result, a word representation model cannot learn an informative embedding of a pronoun without considering cross-sentence dependency in this case.
",1 Introduction,[0],[0]
It is also problematic if we encode the input document considering cross-sentence dependency and treat the entire document as one sentence.,1 Introduction,[0],[0]
An input article or conversation can be too long for a single LSTM cell to memorize.,1 Introduction,[0],[0]
"If the LSTM updates itself for too many steps, gradients will vanish or explode (Pascanu et al., 2013), and the coreference resolution model will be very difficult to optimize.",1 Introduction,[0],[0]
"Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model.
",1 Introduction,[0],[0]
"To solve the problem that traditional LSTM encoders, which treat the input sentences as a batch, lack an ability to capture cross-sentence dependency, and to avoid the time complexity and difficulties of training the model concatenating all input sentences, we propose a cross-sentence encoder for end-to-end co-reference (E2E-CR).",1 Introduction,[0],[0]
"Borrowing the idea of an external memory module from Sukhbaatar et al. (2015), an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model.",1 Introduction,[0],[0]
"With this context memory block, the proposed model is able to encode
input sentences as a batch, and also calculate the representations of input words by taking both target sentences and context sentences into consideration.",1 Introduction,[0],[0]
Experiments showed that this approach improved the performance of co-reference resolution models.,1 Introduction,[0],[0]
"A popular method of co-reference resolution is mention ranking (Durrett and Klein, 2013).",2.1 Co-reference Resolution,[0],[0]
"Reading each mention, the model calculates coreference scores for all antecedent mentions, and picks the mention with the highest positive score to be its co-reference.",2.1 Co-reference Resolution,[0],[0]
Many recent works are based on this approach.,2.1 Co-reference Resolution,[0],[0]
Durrett and Klein (2013) designed a set of feature templates to improve the mention-ranking model.,2.1 Co-reference Resolution,[0],[0]
Peng et al. (2015) proposed a mention-ranking model by jointly learning mention heads and co-references.,2.1 Co-reference Resolution,[0],[0]
Clark and Manning (2016a) proposed a reinforcement learning framework for the mention ranking approach.,2.1 Co-reference Resolution,[0],[0]
"Based on similar ideas but without using parsing features, the authors of Lee et al. (2017) proposed the current state-of-the-art model which uses neural networks to embed mentions and calculate mention and antecedent scores.",2.1 Co-reference Resolution,[0],[0]
"Lee et al. (2018) applied ELMo embeddings (Peters et al., 2018) to improve within-sentence dependency modeling and word representation learning.",2.1 Co-reference Resolution,[0],[0]
Wiseman et al. (2016) and Clark and Manning (2016b) proposed models using global entity-level features.,2.1 Co-reference Resolution,[0],[0]
"Distributed word embeddings has been used as the basic unit of language representation for over a decade (Bengio et al., 2003).",2.2 Language Representation Learning,[0],[0]
"Pre-trained word embeddings, for example GloVe (Pennington et al., 2014) and Skip-Gram (Mikolov et al., 2013) are widely used as the input of natural language processing models.
",2.2 Language Representation Learning,[0],[0]
"Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are widely used for sentence modeling.",2.2 Language Representation Learning,[0],[0]
"A single-layer LSTM network was applied in the previous state-of-theart co-reference model (Lee et al., 2017) to generate word and mention representations.",2.2 Language Representation Learning,[0],[0]
"To capture dependency of longer distances, Campos et al. (2017) proposed a recurrent model that outputs hidden states by skipping input tokens.
",2.2 Language Representation Learning,[0],[0]
"Recently, memory networks (Sukhbaatar et al.,
2015) have been applied in language modeling (Cheng et al., 2016; Tran et al., 2016).",2.2 Language Representation Learning,[0],[0]
"Applying an attention mechanism on memory cells, memory networks allow the model to focus on significant words or segments for classification and generation tasks.",2.2 Language Representation Learning,[0],[0]
"Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction (Yogatama et al., 2018).",2.2 Language Representation Learning,[0],[0]
"To improve the word representation learning model for better co-reference resolution performance, we propose two word representation models that learn cross-sentence dependency.",3 Learning Cross-Sentence dependency,[0],[0]
"Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM, the most direct way to consider cross-sentence dependency is to initialize LSTM states with the encodings of adjacent sentences.",3.1 Linear Sentence Linking,[0],[0]
"We name this method linear sentence linking (LSL).
",3.1 Linear Sentence Linking,[0],[0]
"In LSL, we encode input sentences with a 2- layer bidirectional LSTM.",3.1 Linear Sentence Linking,[0],[0]
"Give input sentences [s1, s2 . . .",3.1 Linear Sentence Linking,[0],[0]
"sn], the outputs of the first layer are [[−→s 1;←−s 1],",3.1 Linear Sentence Linking,[0],[0]
"[−→s 2;←−s 2], . . .",3.1 Linear Sentence Linking,[0],[0]
[−→s n;←−s n]].,3.1 Linear Sentence Linking,[0],[0]
"In the second LSTM layer, the initial state of the forward LSTM of si is initialized as
−→ S i =",3.1 Linear Sentence Linking,[0],[0]
"[ −→c 20; [−→s i−1;←−s i−1]]
while the backward state is initialized as
←−",3.1 Linear Sentence Linking,[0],[0]
"S i = [ ←−c 20; [−→s i−1;←−s i−1]]
where ci0 stands for the initial cell of the ith layer, and x stands for the final output of the LSTMs in first layer.",3.1 Linear Sentence Linking,[0],[0]
We then concatenate the outputs of the forward and backward LSTMs in the second layer as the word representations for coreference prediction.,3.1 Linear Sentence Linking,[0],[0]
It is difficult for LSTMs to embed enough information about a long sentence into a lowdimensional distributed vector.,3.2 Attentional Sentence Linking,[0],[0]
"To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.
",3.2 Attentional Sentence Linking,[0],[0]
"To describe the architecture of the proposed model, we focus on adjacent input sentences si−1
and si.",3.2 Attentional Sentence Linking,[0],[0]
"We present the input embeddings of the j-th word in the i-th sentence with xi,j .",3.2 Attentional Sentence Linking,[0],[0]
"To solve the traditional recurrent neural networks, Hochreiter and Schmidhuber (1997) proposed the LSTM architecture.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
"The detail of recurrent state updating in LSTMs ht = flstm(xt, ht−1, ct−1) is shown in following equations.
",3.2.1 Long Short-Term Memory RNNs,[0],[0]
it = σ(Wxixt +Whiht−1 + bi) ft = σ(Wxfxt,3.2.1 Long Short-Term Memory RNNs,[0],[0]
+Whfht−1 + bf ),3.2.1 Long Short-Term Memory RNNs,[0],[0]
ct = ft ct−1 + it tanh(Wxcxt +Whcht−1 + bc) ot = σ(Wxoxt +Whoht−1 + bo),3.2.1 Long Short-Term Memory RNNs,[0],[0]
"ht = ot tanh(ct)
where xt is the input embedding and ht is the output representation of the t-th word.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
We name this method attentional sentence linking (ASL).,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"Considering input word xi,t in the ith sentence and all words from the previous sentence Xi−1 =",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"[xi−1,1, xi−1,2, . . .",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
", xi−1,m], we regard the matrix Xi−1 as an external memory module and calculate an attention on its cells, where each cell contains a word embedding.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"αj = ecj∑ k e ck (1)
ck = fc([xi,t;ht−1;xi−1,k] T ) (2)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"With the attention distribution α, we can get a vector summarizing related information from si−1,
vi−1 = ∑ j αj · xi−1,j (3)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The model decides if it needs to pay more attention on the current input or cross-sentence information with a context gate.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"gt = σ(fg([xi,t;ht−1; vi−1] T )) (4)
x̂i,t = gt · xi,t + (1− gt) · vi−1 (5)
σ",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
(·) stands for the Sigmoid function.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The word representation of the target word is calculated as
hi,t = flstm(x̂i,t, hi,t−1, ci,t−1) (6)
where flstm stands for standard LSTM update described in section 3.2.1.",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"In this work, we apply the mention-ranking endto-end co-reference resolution (E2E-CR) model proposed by Lee et al. (2017) for co-reference prediction.",3.3 Co-reference Prediction,[0],[0]
The word representations applied in E2ECR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs.,3.3 Co-reference Prediction,[0],[0]
"In our work, we represent words by concatenating pre-trained word embeddings and the outputs of LSL- and ASL-LSTMs.",3.3 Co-reference Prediction,[0],[0]
"We train and evaluate our model on the English corpus of the CoNLL-2012 shared task (Pradhan et al., 2012).",4 Experiments,[0],[0]
"We implement our model based on the published implementation of the baseline E2ECR model (Lee et al., 2017) 1.",4 Experiments,[0],[0]
Our implementation is also available online for reproducing the results reported in this paper 2.,4 Experiments,[0],[0]
"In this section, we first describe our hyperparameter setup, and then show the experimental results of previous work and our proposed models.",4 Experiments,[0],[0]
"In practice, the LSTM modules applied in our model have 200 output units.",4.1 Model and Hyperparameter Setup,[0],[0]
"In ASL, we calculate cross-sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units.",4.1 Model and Hyperparameter Setup,[0],[0]
The initial learning rate is set as 0.001 and decays 0.001% every 100 steps.,4.1 Model and Hyperparameter Setup,[0],[0]
"The model is optimized with the Adam algorithm (Kingma and Ba, 2014).",4.1 Model and Hyperparameter Setup,[0],[0]
We randomly select up to 40 continuous sentences for training if the input is too long.,4.1 Model and Hyperparameter Setup,[0],[0]
"In co-reference prediction, we select 250 candidate antecedents as our baseline model.",4.1 Model and Hyperparameter Setup,[0],[0]
We evaluate our model on the test set of the CoNLL-2012 shared task.,4.2 Experiment Results and Discussion,[0],[0]
The performance of previous work and our model are shown in Table 1.,4.2 Experiment Results and Discussion,[0],[0]
"We mainly focus on the average F1 score of MUC, B3, and CEAF metrics.",4.2 Experiment Results and Discussion,[0],[0]
"Comparing with the baseline model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1.",4.2 Experiment Results and Discussion,[0],[0]
"Experiments
1https://github.com/kentonl/e2e-coref 2https://github.com/luohongyin/
coatt-coref
show that the models that consider cross-sentence dependency significantly outperform the baseline model, which encodes each sentence from the input document separately.
",4.2 Experiment Results and Discussion,[0],[0]
"Experiments also indicated that the ASL model has better performance than the LSL model, since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence-level embeddings.",4.2 Experiment Results and Discussion,[0],[0]
"This gives the model a better ability to model cross-sentence dependency.
",4.2 Experiment Results and Discussion,[0],[0]
Examples for comparing the performance of the ASL model and the baseline are shown in Table 2.,4.2 Experiment Results and Discussion,[0],[0]
Each example contains two continuous sentences with co-references distritubed in different sentences.,4.2 Experiment Results and Discussion,[0],[0]
Underlined spans in bold are target mentions and annotated co-references.,4.2 Experiment Results and Discussion,[0],[0]
"Spans in
green are ASL predictions, and spans in red are baseline predictions.",4.2 Experiment Results and Discussion,[0],[0]
"A prediction on “-” means that no mention is predicted as a co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"Table 2 shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence.",4.2 Experiment Results and Discussion,[0],[0]
The pretrained embeddings of pronouns are not informative enough.,4.2 Experiment Results and Discussion,[0],[0]
"In the first example, “it” is not semantically similar with “SMS” in GloVe without any context, and in this case, “it” and “SMS” are in different sentences.",4.2 Experiment Results and Discussion,[0],[0]
"As a result, if reading this two sentences separately, it is hard for the encoder to represent “it” with the semantics of “SMS”.",4.2 Experiment Results and Discussion,[0],[0]
"This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention, or cannot find any co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"However, with ASL, the model learns the semantics of pronouns with an attention to words in other sentences.",4.2 Experiment Results and Discussion,[0],[0]
"With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough.",4.2 Experiment Results and Discussion,[0],[0]
"Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions.",4.2 Experiment Results and Discussion,[0],[0]
We proposed linear and attentional sentence linking models for learning word representations that captures cross-sentence dependency.,5 Conclusion and Future Work,[0],[0]
"Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state-of-the-art co-reference resolution model, indicating that cross-sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences.",5 Conclusion and Future Work,[0],[0]
"It worth exploring if our model can improve the performance of other natural language processing
applications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis.",5 Conclusion and Future Work,[0],[0]
"In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR).",abstractText,[0],[0]
"While the traditional E2ECR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency.",abstractText,[0],[0]
Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word.,abstractText,[0],[0]
"With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document.",abstractText,[0],[0]
"Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.",abstractText,[0],[0]
Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 506–517 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1047",text,[0],[0]
"Automatically discovering words and other elements of linguistic structure from continuous speech has been a longstanding goal in computational linguists, cognitive science, and other speech processing fields.",1.1 Problem Statement and Motivation,[0],[0]
"Practically all humans acquire language at a very early age, but this task has proven to be an incredibly difficult problem for computers.",1.1 Problem Statement and Motivation,[0],[0]
"While conventional automatic speech recognition (ASR) systems have a long history and have recently made great strides thanks to the revival of deep neural networks (DNNs), their reliance on highly supervised training paradigms has essentially restricted their application to the major languages of the world, accounting for a small fraction of the more than 7,000 human languages spoken worldwide (Lewis et al., 2016).",1.1 Problem Statement and Motivation,[0.953287688831357],"['While in the standard approach to learning, described in the previous paragraph, one usually selects a single classifier (e.g., the one minimizing the empirical error), the PAC-Bayes framework, first formulated by McAllester (1999), considers the construction of a complete probability distribution overH, and the selection of a single hypothesis h ∈ H based on this distribution.']"
"The main
reason for this limitation is the fact that these supervised approaches require enormous amounts of very expensive human transcripts.",1.1 Problem Statement and Motivation,[0],[0]
"Moreover, the use of the written word is a convenient but limiting convention, since there are many oral languages which do not even employ a writing system.",1.1 Problem Statement and Motivation,[0],[0]
"In constrast, infants learn to communicate verbally before they are capable of reading and writing - so there is no inherent reason why spoken language systems need to be inseparably tied to text.
",1.1 Problem Statement and Motivation,[0],[0]
The key contribution of this paper has two facets.,1.1 Problem Statement and Motivation,[0],[0]
"First, we introduce a methodology capable of not only discovering word-like units from continuous speech at the waveform level with no additional text transcriptions or conventional speech recognition apparatus.",1.1 Problem Statement and Motivation,[0],[0]
"Instead, we jointly learn the semantics of those units via visual associations.",1.1 Problem Statement and Motivation,[0],[0]
"Although we evaluate our algorithm on an English corpus, it could conceivably run on any language without requiring any text or associated ASR capability.",1.1 Problem Statement and Motivation,[0],[0]
"Second, from a computational perspective, our method of speech pattern discovery runs in linear time.",1.1 Problem Statement and Motivation,[0],[0]
"Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park and Glass, 2008; Jansen et al., 2010; Jansen and Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n2) complexity, since they do an exhaustive comparison of the data against itself.",1.1 Problem Statement and Motivation,[0],[0]
Our method leverages correlated information from a second modality - the visual domain - to guide the discovery of words and phrases.,1.1 Problem Statement and Motivation,[0],[0]
"This enables our method to run in O(n) time, and we demonstrate it scalability by discovering acoustic patterns in over 522 hours of audio.",1.1 Problem Statement and Motivation,[0],[0]
"A sub-field within speech processing that has garnered much attention recently is unsupervised
506
speech pattern discovery.",1.2 Previous Work,[0],[0]
"Segmental Dynamic Time Warping (S-DTW) was introduced by Park and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data.",1.2 Previous Work,[0],[0]
"Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009).",1.2 Previous Work,[0],[0]
"Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).
",1.2 Previous Work,[0],[0]
"While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weaklysupervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).",1.2 Previous Work,[0],[0]
"Although the focus of this work is discovering acoustic patterns, in the process we jointly associate the acoustic patterns with clusters of image crops, which we demonstrate capture visual patterns as well.
",1.2 Previous Work,[0],[0]
The computer vision and NLP communities have begun to leverage deep learning to create multimodal models of images and text.,1.2 Previous Work,[0],[0]
"Many works have focused on generating annotations or text captions for images (Socher and Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy and Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).",1.2 Previous Work,[0],[0]
"One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos and Chrupaa (2016), who uses images to segment words within captions at the phoneme string level.",1.2 Previous Work,[0],[0]
"Other work has taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Roy, 2003; Harwath and Glass, 2015; Harwath et al., 2016).",1.2 Previous Work,[0],[0]
"The work of (Harwath et al., 2016) is the most similar to ours, in which the authors learned embeddings at the entire image and entire spoken caption level and then used the embeddings to perform bidirectional retrieval.",1.2 Previous Work,[0],[0]
"In this work, we go further by automatically segmenting and clustering the spoken captions into individual word-like units, as well as the images into object-like categories.",1.2 Previous Work,[0],[0]
"We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.",2 Experimental Data,[0],[0]
"The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.",2 Experimental Data,[0],[0]
"The data collection scheme is described in detail in Harwath et al. (2016), but the experiments in this paper leverage nearly twice the amount of data.",2 Experimental Data,[0],[0]
"For training our multimodal neural network as well as the pattern discovery experiments, we use a subset of 214,585 image/caption pairs, and we hold out a set of 1,000 pairs for evaluating the multimodal network’s retrieval ability.",2 Experimental Data,[0],[0]
"Because we lack ground truth text transcripts for the data, we used Google’s Speech Recognition public API to generate proxy transcripts which we use when analyzing our system.",2 Experimental Data,[0],[0]
"Note that the ASR was only used for analysis of the results, and was not involved in any of the learning.",2 Experimental Data,[0],[0]
"We first train a deep multimodal embedding network similar in spirit to the one described in Harwath et al. (2016), but with a more sophisticated architecture.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is trained to map entire image frames and entire spoken captions into a shared embedding space; however, as we will show, the trained network can then be used to localize patterns corresponding to words and phrases within the spectrogram, as well as visual objects within the image by applying it to small sub-regions of the image and spectrogram.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is comprised of two branches, one which takes as input images, and the other which takes as input spectrograms.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The image network is formed by taking the off-the-shelf VGG 16 layer network (Simonyan and Zisserman, 2014) and replacing the softmax classification layer with a linear transform which maps the 4096-dimensional activations of the second fully connected layer into our 1024-dimensional multimodal embedding space.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"In our experiments, the weights of this projection layer are trained, but the layers taken from the VGG network below it are kept fixed.",3 Audio-Visual Embedding Neural Networks,[0],[0]
The second branch of our network analyzes speech spectrograms as if they were black and white images.,3 Audio-Visual Embedding Neural Networks,[0],[0]
"Our spectrograms are computed using 40 log Mel
filterbanks with a 25ms Hamming window and a 10ms shift.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The input to this branch always has 1 color channel and is always 40 pixels high (corresponding to the 40 Mel filterbanks), but the width of the spectrogram varies depending upon the duration of the spoken caption, with each pixel corresponding to approximately 10 milliseconds worth of audio.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The architecture we use is entirely convolutional and shown below, where C denotes the number of convolutional channels, W is filter width, H is filter height, and S is pooling stride.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
1.,3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=128, W=1, H=40, ReLU 2.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=256, W=11, H=1, ReLU 3.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 4.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 5.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 6.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 7.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 8.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=1024, W=17, H=1, ReLU 9.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Meanpool over entire caption
10.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"L2 normalization In practice during training, we restrict the caption spectrograms to all be 1024 frames wide (i.e., 10sec of speech) by applying truncation or zero padding.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Additionally, both the images and spectrograms are mean normalized before training.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The overall multimodal network is formed by tying together the image and audio branches with a layer which takes both of their output vectors and computes an inner product between them, representing the similarity score between a given image/caption pair.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"We train the network to assign high scores to matching image/caption pairs, and lower scores to mismatched pairs.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Within a minibatch of B image/caption pairs, let Spj , j = 1, . . .",3 Audio-Visual Embedding Neural Networks,[0],[0]
", B denote the similarity score of the jth image/caption pair as output by the neural network.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Next, for each pair we randomly sample one impostor caption and one impostor image from the same minibatch.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Let Sij denote the similarity score between the jth caption and its impostor image, and Scj be the similarity score between the jth image and its impostor caption.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The total loss for the entire minibatch is then computed as
L(θ) = B∑
j=1
[max(0, Scj − Spj + 1)
+ max(0,",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Sij − Spj + 1)] (1)
We train the neural network with 50 epochs of stochastic gradient descent using a batch size B =
128, a momentum of 0.9, and a learning rate of 1e5 which is set to geometrically decay by a factor between 2 and 5 every 5 to 10 epochs.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Although we have trained our multimodal network to compute embeddings at the granularity of entire images and entire caption spectrograms, we can easily apply it in a more localized fashion.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the case of images, we can simply take any arbitrary crop of an original image and resize it to 224x224 pixels.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The audio network is even more trivial to apply locally, because it is entirely convolutional and the final mean pooling layer ensures that the output will be a 1024-dim vector no matter the extent of the input.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The bigger question is where to locally apply the networks in order to discover meaningful acoustic and visual patterns.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Given an image and its corresponding spoken audio caption, we use the term grounding to refer to extracting meaningful segments from the caption and associating them with an appropriate subregion of the image.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"For example, if an image depicted a person eating ice cream and its caption contained the spoken words “A person is enjoying some ice cream,” an ideal set of groundings would entail the acoustic segment containing the word “person” linked to a bounding box around the person, and the segment containing the word “ice cream” linked to a box around the ice cream.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use a constrained brute force ranking scheme to evaluate all possible groundings (with a restricted granularity) between an image and its caption.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Specifically, we divide the image into a grid, and extract all of the image crops whose boundaries sit on the grid lines.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Because we are mainly interested in extracting regions of interest and not high precision object detection boxes, to keep the number of proposal regions under control we impose several restrictions.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"First, we use a 10x10 grid on each image regardless of its original size.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Second, we define minimum and maximum aspect ratios as 2:3 and 3:2 so as not to introduce too much distortion and also to reduce the number of proposal boxes.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Third, we define a minimum bounding width as 30% of the original image width, and similarly a minimum height as 30% of the original image height.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In practice, this results in a few thousand proposal regions per image.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To extract proposal segments from the audio
caption spectrogram, we similarly define a 1-dim grid along the time axis, and consider all possible start/end points at 10 frame (pixel) intervals.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We impose minimum and maximum segment length constraints at 50 and 100 frames (pixels), implying that our discovered acoustic patterns are restricted to fall between 0.5 and 1 second in duration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The number of proposal segments will vary depending on the caption length, and typically number in the several thousands.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Note that when learning groundings we consider the entire audio sequence, and do not incorporate the 10sec duration constraint imposed during training.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Once we have extracted a set of proposed visual bounding boxes and acoustic segments for a given image/caption pair, we use our multimodal network to compute a similarity score between each unique image crop/acoustic segment pair.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Each triplet of an image crop, acoustic segment, and similarity score constitutes a proposed grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"A naive approach would be to simply keep the top N groundings from this list, but in practice we ran into two problems with this strategy.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"First, many proposed acoustic segments capture mostly silence due to pauses present in natural speech.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We solve this issue by using a simple voice activity detector (VAD) which was trained on the TIMIT corpus(Garofolo et al., 1993).",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the VAD estimates that 40% or more of any proposed acoustic segment is silence, we discard that entire grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
The second problem we ran into is the fact that the top of the sorted grounding list is dominated by highly overlapping acoustic segments.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"This makes sense, because highly informative content words will show up in many different groundings with slightly perturbed start or end times.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To alleviate this issue, when evaluating a grounding from the top of the proposal list we compare the interval intersection over union (IOU) of its acoustic segment against all acoustic segments already accepted for further consideration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the IOU exceeds a threshold of 0.1, we discard the new grounding and continue moving down the list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We stop accumulating groundings once the scores fall to below 50% of the top score in the “keep” list, or when 10 groundings have been added to the “keep” list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Figure 1 displays a pictorial example of our grounding procedure.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Once we have completed the grounding procedure, we are left with a small set of regions of interest in each image and caption spectrogram.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use the respective branches of our multimodal network to compute embedding vectors for each grounding’s image crop and acoustic segment.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We then employ k-means clustering separately on the collection of image embedding vectors as well as the collection of acoustic embedding vectors.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The last step is to establish an affinity score between each image cluster I and each acoustic cluster A; we do so using the equation
Affinity(I,A) = ∑
i∈I
∑ a∈A",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"i>a · Pair(i,a) (2)
where i is an image crop embedding vector, a is an acoustic segment embedding vector, and Pair(i,a) is equal to 1",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"when i and a belong to the same grounding pair, and 0 otherwise.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"After clustering, we are left with a set of acoustic pattern clusters, a set of visual pattern clusters, and a set of linkages describing which acoustic clusters are associated with which image clusters.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the next section, we investigate these clusters in more detail.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy and Li (2015).",5 Experiments and Analysis,[0],[0]
"The image annotation and search recall scores on a 1,000 image/caption pair held-out test set are shown in Table 1.",5 Experiments and Analysis,[0],[0]
"Also shown in this table are the scores
achieved by a model which uses the ASR text transcriptions for each caption instead of the speech audio.",5 Experiments and Analysis,[0],[0]
"The text captions were truncated/padded to 20 words, and the audio branch of the network was replaced with a branch with the following architecture:
1.",5 Experiments and Analysis,[0],[0]
"Word embedding layer of dimension 200
2.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=512, W=3, ReLU 3.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=1024, W=3 4.",5 Experiments and Analysis,[0],[0]
Meanpool over entire caption 5.,5 Experiments and Analysis,[0],[0]
"L2 normalization
One would expect that access to ASR hypotheses should improve the recall scores, but the performance gap is not enormous.",5 Experiments and Analysis,[0],[0]
"Access to the ASR hypotheses provides a relative improvement of approximately 21.8% for image search R@10 and 12.5% for annotation R@10 compared to using no transcriptions or ASR whatsoever.
",5 Experiments and Analysis,[0],[0]
"We performed the grounding and pattern clustering steps on the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.",5 Experiments and Analysis,[0],[0]
"For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.",5 Experiments and Analysis,[0],[0]
"Our strategy is to forcealign the Google recognition hypothesis text to the audio, and then assign a label string to each acoustic segment based upon which words it overlaps in time.",5 Experiments and Analysis,[0],[0]
"The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer
Table 3:",5 Experiments and Analysis,[0],[0]
Top 50 clusters with k = 500 sorted by increasing variance.,5 Experiments and Analysis,[0],[0]
"Legend: |Cc| is acoustic cluster size, |Ci| is associated image cluster size, Pur. is acoustic cluster purity, σ2 is acoustic cluster variance, and Cov. is acoustic cluster coverage.",5 Experiments and Analysis,[0],[0]
"A dash (-) indicates a cluster whose majority label is silence.
",5 Experiments and Analysis,[0],[0]
"Trans |Cc| |Ci| Pur. σ2 Cov. Trans |Cc| |Ci| Pur. σ2 Cov. - 1059 3480 0.70 0.26 - snow 4331 3480 0.85 0.26 0.45
desert 1936 2896 0.82 0.27 0.67 kitchen 3200 2990 0.88 0.28 0.76 restaurant 1921 2536 0.89 0.29 0.71 mountain 4571 2768 0.86 0.30 0.38
black 4369 2387 0.64 0.30 0.17 skyscraper 843 3205 0.84 0.30 0.84 bridge 1654 2025 0.84 0.30 0.25 tree 5303 3758 0.90 0.30 0.16 castle 1298 2887 0.72 0.31 0.74 bridge 2779 2025 0.81 0.32 0.41
- 2349 2165 0.31 0.33 - ocean 2913 3505 0.87 0.33 0.71 table 3765 2165 0.94 0.33 0.23 windmill 1458 3752 0.71 0.33 0.76 window 1890 2795 0.85 0.34 0.21 river 2643 3204 0.76 0.35 0.62 water 5868 3204 0.90 0.35 0.27 beach 1897 2964 0.79 0.35 0.64 flower 3906 2587 0.92 0.35 0.67 wall 3158 3636 0.84 0.35 0.23
sky 4306 6055 0.76 0.36 0.34 street 2602 2385 0.86 0.36 0.49 golf course 1678 3864 0.44 0.36 0.63 field 3896 3261 0.74 0.36 0.37
tree 4098 3758 0.89 0.36 0.13 lighthouse 1254 1518 0.61 0.36 0.83 forest 1752 3431 0.80 0.37 0.56 church 2503 3140 0.86 0.37 0.72 people 3624 2275 0.91 0.37 0.14 baseball 2777 1929 0.66 0.37 0.86 field 2603 3922 0.74 0.37 0.25 car 3442 2118 0.79 0.38 0.27
people 4074 2286 0.92 0.38 0.17 shower 1271 2206 0.74 0.38 0.82 people walking 918 2224 0.63 0.38 0.25 wooden 3095 2723 0.63 0.38 0.28
mountain 3464 3239 0.88 0.38 0.29 tree 3676 2393 0.89 0.39 0.11 - 1976 3158 0.28 0.39 - snow 2521 3480 0.79 0.39 0.24
water 3102 2948 0.90 0.39 0.14 rock 2897 2967 0.76 0.39 0.26 - 2918 3459 0.08 0.39 - night 3027 3185 0.44 0.39 0.59
station 2063 2083 0.85 0.39 0.62 chair 2589 2288 0.89 0.39 0.22 building 6791 3450 0.89 0.40 0.21 city 2951 3190 0.67 0.40 0.50
Figure 2:",5 Experiments and Analysis,[0],[0]
"Scatter plot of audio cluster purity weighted by log cluster size vs variance for k = 500 (least-squares line superimposed).
based on the standard WSJ recipe and trained using the Google ASR hypothesis as a proxy for the transcriptions.",5 Experiments and Analysis,[0],[0]
Any word whose duration is overlapped 30% or more by the acoustic segment is included in the label string for the segment.,5 Experiments and Analysis,[0],[0]
We then employ a majority vote scheme to derive the overall cluster labels.,5 Experiments and Analysis,[0],[0]
"When computing the purity of a
cluster, we count a cluster member as matching the cluster label as long as the overall cluster label appears in the member’s label string.",5 Experiments and Analysis,[0],[0]
"In other words, an acoustic segment overlapping the words “the lighthouse” would receive credit for matching the overall cluster label “lighthouse”.",5 Experiments and Analysis,[0],[0]
A breakdown of the segments captured by two clusters is shown in Table 2.,5 Experiments and Analysis,[0],[0]
"We investigated some simple schemes for predicting highly pure clusters, and found that the empirical variance of the cluster members (average squared distance to the cluster centroid) was a good indicator.",5 Experiments and Analysis,[0],[0]
Figure 2 displays a scatter plot of cluster purity weighted by the natural log of the cluster size against the empirical variance.,5 Experiments and Analysis,[0],[0]
"Large, pure clusters are easily predicted by their low empirical variance, while a high variance is indicative of a garbage cluster.
",5 Experiments and Analysis,[0],[0]
"Ranking a set of k = 500 acoustic clusters by their variance, Table 3 displays some statistics for the 50 lowest-variance clusters.",5 Experiments and Analysis,[0],[0]
"We see that most of the clusters are very large and highly pure, and their labels reflect interesting object categories being identified by the neural network.",5 Experiments and Analysis,[0],[0]
"We additionally compute the coverage of each cluster by counting the total number of instances of the clus-
ter label anywhere in the training data, and then compute what fraction of those instances were captured by the cluster.",5 Experiments and Analysis,[0],[0]
"There are many examples of high coverage clusters, e.g. the “skyscraper” cluster captures 84% of all occurrences of the word “skyscraper”, while the “baseball” cluster captures 86% of all occurrences of the word “baseball”.",5 Experiments and Analysis,[0],[0]
"This is quite impressive given the fact that no conventional speech recognition was employed, and neither the multimodal neural network nor the grounding algorithm had access to the text transcripts of the captions.
",5 Experiments and Analysis,[0],[0]
"To get an idea of the impact of the k parameter as well as a variance-based cluster pruning threshold based on Figure 2, we swept k from 250 to 2000 and computed a set of statistics shown in Table 4.",5 Experiments and Analysis,[0],[0]
We compute the standard overall cluster purity evaluation metric in addition to the average coverage across clusters.,5 Experiments and Analysis,[0],[0]
"The table shows the natural tradeoff between cluster purity and redun-
dancy (indicated by the average cluster coverage) as k is increased.",5 Experiments and Analysis,[0],[0]
"In all cases, the variance-based cluster pruning greatly increases both the overall purity and average cluster coverage metrics.",5 Experiments and Analysis,[0],[0]
"We also notice that more unique cluster labels are discovered with a larger k.
Next, we examine the image clusters.",5 Experiments and Analysis,[0],[0]
"Figure 3 displays the 9 most central image crops for a set of 10 different image clusters, along with the majority-vote label of each image cluster’s associated audio cluster.",5 Experiments and Analysis,[0],[0]
"In all cases, we see that the image crops are highly relevant to their audio cluster label.",5 Experiments and Analysis,[0],[0]
"We include many more example image clusters in Appendix A.
In order to examine the semantic embedding space in more depth, we took the top 150 clusters from the same k = 500 clustering run described in Table 3 and performed t-SNE (van der Maaten and Hinton, 2008) analysis on the cluster centroid vectors.",5 Experiments and Analysis,[0],[0]
"We projected each centroid down to 2 di-
mensions and plotted their majority-vote labels in Figure 4.",5 Experiments and Analysis,[0],[0]
"Immediately we see that different clusters which capture the same label closely neighbor one another, indicating that distances in the embedding space do indeed carry information discriminative across word types (and suggesting that a more sophisticated clustering algorithm than kmeans would perform better).",5 Experiments and Analysis,[0],[0]
"More interestingly, we see that semantic information is also reflected in these distances.",5 Experiments and Analysis,[0],[0]
"The cluster centroids for “lake,” “river,” “body,” “water,” “waterfall,” “pond,” and “pool” all form a tight meta-cluster, as do “restaurant,” “store,” “shop,” and “shelves,” as well as “children,” “girl,” “woman,” and “man.”",5 Experiments and Analysis,[0],[0]
"Many other semantic meta-clusters can be seen in Figure 4, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.
",5 Experiments and Analysis,[0],[0]
"Because our experiments revolve around the discovery of word and object categories, a key question to address is the extent to which the supervision used to train the VGG network constrains or influences the kinds of objects learned.",5 Experiments and Analysis,[0],[0]
"Because the 1,000 object classes from the ILSVRC2012 task (Russakovsky et al., 2015) used to train the VGG network were derived from WordNet synsets (Fellbaum, 1998), we can measure the semantic similarity between the words
learned by our network and the ILSVRC2012 class labels by using synset similarity measures within WordNet.",5 Experiments and Analysis,[0],[0]
"We do this by first building a list of the 1,000 WordNet synsets associated with the ILSVRC2012 classes.",5 Experiments and Analysis,[0],[0]
"We then take the set of unique majority-vote labels associated with the discovered word clusters for k = 500, filtered by setting a threshold on their variance (σ2 ≤ 0.65) so as to get rid of garbage clusters, leaving us with 197 unique acoustic cluster labels.",5 Experiments and Analysis,[0],[0]
"We then look up each cluster label in WordNet, and compare all noun senses of the label to every ILSVRC2012 class synset according to the path similarity measure.",5 Experiments and Analysis,[0],[0]
"This measure describes the distance between two synsets in a hyponym/hypernym hierarchy, where a score of 1 represents identity and lower scores indicate less similarity.",5 Experiments and Analysis,[0],[0]
We retain the highest score between any sense of the cluster label and any ILSVRC2012 synset.,5 Experiments and Analysis,[0],[0]
"Of the 197 unique cluster labels, only 16 had a distance of 1 from any ILSVRC12 class, which would indicate an exact match.",5 Experiments and Analysis,[0],[0]
"A path similarity of 0.5 indicates one degree of separation in the hyponym/hypernym hierarchy - for example, the similarity between “desk” and “table” is 0.5.",5 Experiments and Analysis,[0],[0]
"47 cluster labels were found to have a similarity of 0.5 to some ILSVRC12 class, leaving 134 cluster labels whose highest similarity to any ILSVRC12 class was less than 0.5.",5 Experiments and Analysis,[0],[0]
"In
other words, more than two thirds of the highly pure pattern clusters learned by our network were dissimilar to all of the 1,000 ILSVRC12 classes used to pretrain the VGG network, indicating that our model is able to generalize far beyond the set of classes found in the ILSVRC12 data.",5 Experiments and Analysis,[0],[0]
We display the labels of the 40 lowest variance acoustic clusters labels along with the name and similarity score of their closest ILSVRC12 synset in Table 5.,5 Experiments and Analysis,[0],[0]
"In this paper, we have demonstrated that a neural network trained to associate images with the waveforms representing their spoken audio captions can successfully be applied to discover and
cluster acoustic patterns representing words or short phrases in untranscribed audio data.",6 Conclusions and Future Work,[0],[0]
"An analogous procedure can be applied to visual images to discover visual patterns, and then the two modalities can be linked, allowing the network to learn, for example, that spoken instances of the word “train” are associated with image regions containing trains.",6 Conclusions and Future Work,[0],[0]
"This is done without the use of a conventional automatic speech recognition system and zero text transcriptions, and therefore is completely agnostic to the language in which the captions are spoken.",6 Conclusions and Future Work,[0],[0]
"Further, this is done in O(n) time with respect to the number of image/caption pairs, whereas previous stateof-the-art acoustic pattern discovery algorithms which leveraged acoustic data alone run in O(n2) time.",6 Conclusions and Future Work,[0],[0]
"We demonstrate the success of our methodology on a large-scale dataset of over 214,000 image/caption pairs comprising over 522 hours of spoken audio data, which is to our knowledge the largest scale acoustic pattern discovery experiment ever performed.",6 Conclusions and Future Work,[0],[0]
"We have shown that the shared multimodal embedding space learned by our model is discriminative not only across visual object categories, but also acoustically and semantically across spoken words.
",6 Conclusions and Future Work,[0],[0]
The future directions in which this research could be taken are incredibly fertile.,6 Conclusions and Future Work,[0],[0]
"Because our method creates a segmentation as well as an alignment between images and their spoken captions, a generative model could be trained using these alignments.",6 Conclusions and Future Work,[0],[0]
"The model could provide a spoken caption for an arbitrary image, or even synthesize an image given a spoken description.",6 Conclusions and Future Work,[0],[0]
"Modeling improvements are also possible, aimed at the goal of incorporating both visual and acoustic localization into the neural network itself.",6 Conclusions and Future Work,[0],[0]
"The same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.",6 Conclusions and Future Work,[0],[0]
"Additionally, by collecting a second dataset of captions for our images in a different language, such as Spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.",6 Conclusions and Future Work,[0],[0]
"This paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations.",6 Conclusions and Future Work,[0],[0]
"beach cliff pool desert field
chair table staircase statue stone
church forest mountain skyscraper trees
waterfall windmills window city bridge
flowers man wall archway baseball
boat shelves cockpit girl children
building rock kitchen plant hallway",A Additional Cluster Visualizations,[0],[0]
"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions.",abstractText,[0],[0]
"For example, our model is able to detect spoken instances of the words “lighthouse” within an utterance and associate them with image regions containing lighthouses.",abstractText,[0],[0]
"We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations.",abstractText,[0],[0]
"Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.",abstractText,[0],[0]
Learning word-like units from joint audio-visual analysis,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1817–1827, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems.,1 Introduction,[0],[0]
A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality.,1 Introduction,[0],[0]
"This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000).
∗",1 Introduction,[0],[0]
"The author now is affiliated with Google, Japan.
",1 Introduction,[0],[0]
The EM algorithm for WA has a great influence in SMT.,1 Introduction,[0],[0]
"Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm.",1 Introduction,[0],[0]
GIZA++,1 Introduction,[0],[0]
"in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013).
",1 Introduction,[0],[0]
"However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.”",1 Introduction,[0],[0]
"Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Graça et al., 2010).",1 Introduction,[0],[0]
"Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment.",1 Introduction,[0],[0]
"The Chinese word “HE ZHANG,” denoted wr, which means river custodian, only occurs once in the whole corpus.",1 Introduction,[0],[0]
"We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sentence pairs from the NIST Open Machine Translation (OpenMT) 2006 evaluation2.",1 Introduction,[0],[0]
The resulting alignment is shown in Figure 1(b).,1 Introduction,[0],[0]
"It can be seen that wr is erroneously aligned to multiple English words.
",1 Introduction,[0],[0]
"To find the cause of this, we checked the alignments in each iteration i of s, denoted ais.",1 Introduction,[0],[0]
"We found that in a1s , wr together with the other source-side words were aligned with uniform probability to all the target-side words since the alignment models provided no prior information.",1 Introduction,[0],[0]
"However, in a2s , wr became erroneously aligned,
1Released by Linguistic Data Consortium, catalog number LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05.
2http://www.itl.nist.gov/iad/mig/ tests/mt/2006/
1817
because the alignment distribution3 of wr was only learned from a1s , thus consisted of non-zero values only for generating the target-side words in s. Therefore, the alignment probabilities from the rare word wr to the unaligned words in s were extraordinarily high, since almost all of the probability mass was distributed among them.",1 Introduction,[0],[0]
"In other words, the story behind these garbage collector effects is that erroneous alignments are able to provide support for themselves; the probability distribution learned only from s is re-applied to s. In this way, these “garbage collector effects” are a form of over-fitting.
",1 Introduction,[0],[0]
"Motivated by this observation, we propose a leave-one-out EM algorithm for WA in this paper.",1 Introduction,[0],[0]
"Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed.",1 Introduction,[0],[0]
Figure 1(c) shows the effect of using our technique on the example.,1 Introduction,[0],[0]
"The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation.",1 Introduction,[0],[0]
"The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012).",2 Related Work,[0],[0]
"The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment.",2 Related Work,[0],[0]
"As word alignment is a foundation of most MT systems, our method have a wider application.
",2 Related Work,[0],[0]
"Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012).",2 Related Work,[0],[0]
"Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach.
",2 Related Work,[0],[0]
"Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al.,
3The probability distribution of generating target language words from wr .",2 Related Work,[0],[0]
"The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.
",2 Related Work,[0],[0]
"(a)
2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014).",2 Related Work,[0],[0]
"This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation.",3 Methodology,[0],[0]
The main notation used in this section is shown in Table 1.,3 Methodology,[0],[0]
"To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data.","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney, 2003),
Expectation Step (E step): calculating the conditional probability of alignments for each sentence pair, P (a|s, θ) = ∏Jj=1 θali(aj |aj−1, I)θlex(fj |eaj ),(1) where θali(i|i′, I) is the alignment probability and θlex(f |e) is the translation probability.","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Note that
(1) is a general form for IBM model 1, model 2 and the HMM model.
","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Maximization step (M step): re-estimating the probability models,
θali(i|i′, I)","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑
s Ni|i′,I(s)∑ s Ni′,I(s)
(2) θlex(f |e)","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑
s Nf |e(s)∑ s ne(s)
(3)
where Ni′,I(s) is the marginal number of times ei′ is aligned to some foreign word if the length of e is I , or 0 otherwise; Ni|i′,I(s) is the marginal number of times the next alignment position after i′ is i in a if the length of e is I , or 0 otherwise; ne(s) is the count of e in e; Nf |e(s,a) is the marginal number of times e is aligned to f .","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
Leave-one-out EM for WA differs from standard EM in the way the alignment and translation probabilities are calculated.,"3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Each sentence pair will
have its own alignment and translation probability models calculated by excluding the sentence pair itself.","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"More formally, leave-one-out EM for WA are formulated as follows,
Leave-one-out E step: employing leave-oneout models for each s to calculate the conditional probability of alignments
P (a|s, θs̄) = ∏J j=1 θ s̄ ali(aj |aj−1, I)θs̄lex(fj |eaj ),(4)
where θs̄ali(i|i′, I) and θs̄lex(fj |eaj ) are the leaveone-out alignment probability and translation probability, respectively.
","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Leave-one-out M step: re-estimating leaveone-out probability models,
θs̄ali(i|i′, I)","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑ s′ 6=s Ni|i′,I(s ′)∑
s′ 6=s Ni′,I(s′) (5) θs̄lex(f |e)","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑ s′ 6=s Nf |e(s ′)∑
s′ 6=s ne(s′) .","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
(6),"3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"The framework of the standard EM for IBM Model 4 is similar with the one for IBM Models 1, 2 and HMM Model, but the calculation of alignment probability is more complicated.
",3.3 Standard EM for IBM Model 4,[0],[0]
"E step: calculating the conditional probability through the reverted alignment (Och and Ney, 2003),
P (a|s, θ) = P (B0|B1, . .",3.3 Standard EM for IBM Model 4,[0],[0]
.,3.3 Standard EM for IBM Model 4,[0],[0]
", BI)·",3.3 Standard EM for IBM Model 4,[0],[0]
"I∏
i=1
P (Bi|Bi−1, ei) · I∏
i=1",3.3 Standard EM for IBM Model 4,[0],[0]
∏,3.3 Standard EM for IBM Model 4,[0],[0]
"j∈Bi θlex(fj |ei), (7)
where B0 means the set of foreign words aligned with the empty word; P (B0|B1, . .",3.3 Standard EM for IBM Model 4,[0],[0]
.,3.3 Standard EM for IBM Model 4,[0],[0]
", BI) is assumed to be a binomial distribution for the size of B0 (Brown et al., 1993b) or an modified distribution to relieve deficiency (Och and Ney, 2003).
",3.3 Standard EM for IBM Model 4,[0],[0]
"The distribution P (Bi|Bi−1, ei) is decomposed as
P (Bi|Bi−1, ei) = θfer(φi|ei)· θhea(Bi,1 −Bρi |Eρi) · φi∏
k=2
θoth(Bi,k −Bi,k−1),
(8)
where θfer is a fertility model; θhea is a probability model for the head (first) aligned foreign word; θoth is a probability model for the other aligned foreign words.",3.3 Standard EM for IBM Model 4,[0],[0]
"θhea is assumed to be conditioned
on the word class Eρi , following the paper of (Och and Ney, 2003) and the implementation of GIZA++ and CICADA.
",3.3 Standard EM for IBM Model 4,[0],[0]
"M step: re-estimating the probability models, θfer(φ|e) ← ∑
s Nφ|e(s)∑ s ∑ φ′ Nφ′|e(s)
(9)
θhea(∆i|E)",3.3 Standard EM for IBM Model 4,[0],[0]
"← ∑ s N hea ∆i|E(s)∑
s ∑ ∆i′ N hea ∆i′|E(s)
(10)
θoth(∆i) ← ∑ s N oth ∆i (s)∑
s ∑ ∆i′ N oth ∆i′(s) , (11)
where ∆i is a difference of the indexes of two foreign words.",3.3 Standard EM for IBM Model 4,[0],[0]
"The leave-one-out treatment were applied to the three component probability models θfer, θhea and θoth of IBM model 4.
",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"Leave-one-out E step: calculating the conditional probability through leave-one-out probability models
P (a|s, θs̄) = P (B0|B1, . .",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
.,3.4 Leave-one-out EM for IBM Model 4,[0],[0]
", BI)·",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"I∏
i=1
P s̄(Bi|Bi−1, ei) · I∏
i=1",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
∏,3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"j∈Bi θs̄lex(fj |ei), (12)
P s̄(Bi|Bi−1, ei) = θs̄fer(φi|ei)·
θs̄hea(Bi,1 −Bρi |Eρi) · φi∏
k=2
θs̄oth(Bi,k −Bi,k−1).
",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"(13)
Leave-one-out M step: re-estimating the leaveone-out probability models,
θs̄fer(φ|e) ← ∑ s′ 6=s Nφ|e(s ′)∑
s′ 6=s ∑ φ′ Nφ′|e(s′) (14)
θs̄hea(∆i|E)",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"← ∑ s′ 6=s N hea ∆i|E(s
′)∑ s′ 6=s ∑ ∆i′ N hea ∆i′|E(s ′) (15)
θs̄oth(∆i) ← ∑ s′ 6=s N oth ∆i (s
′)∑ s′ 6=s ∑ ∆i′ N oth ∆i′(s ′) .",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
(16),3.4 Leave-one-out EM for IBM Model 4,[0],[0]
Singletons are the words that occur only once in corpora.,3.5 Handling Singletons,[0],[0]
Singletons cause problems when applying leave-one-out to lexicalized models such as the translation model θs̄lex and the fertility model θ s̄ fer.,3.5 Handling Singletons,[0],[0]
"When calculating (6) and (14) for singletons, the
denominators become zero, thus the probabilities are undefined.
",3.5 Handling Singletons,[0],[0]
"For singletons, there is no prior information to guide their alignment, so we back off to uniform distributions.",3.5 Handling Singletons,[0],[0]
"In that case, the alignments are primarily determined by the rest of the sentence.
",3.5 Handling Singletons,[0],[0]
"In addition, singletons can be in the target side of the translation model θs̄lex.",3.5 Handling Singletons,[0],[0]
"In that case, the probabilities become zero.",3.5 Handling Singletons,[0],[0]
"This is handled by setting a minimum probability value of 1.0× 10−12, which was decided by pilot experiments.",3.5 Handling Singletons,[0],[0]
"To alleviate memory requirements and increase speed, our implementation did not build or store the local alignment models explicitly for each sentence pair.",3.6 Implementation Details,[0],[0]
"The following formula was used to efficiently calculate (5), (6) and (14–16) to build temporary probability models,∑
s′ 6=s Nx(s′) =",3.6 Implementation Details,[0],[0]
"( ∑ s′ Nx(s′))−Nx(s), (17)
where x is a alignment event.",3.6 Implementation Details,[0],[0]
"Our implementation maintained global counts of all alignment events ∑ s′ Nx(s
′), and (considerably smaller) local counts Nx(s) from each sentence pair s.
Take the translation model θs̄lex for example.",3.6 Implementation Details,[0],[0]
For a sentence pair s = (f1 . . .,3.6 Implementation Details,[0],[0]
"fJ , e1 . . .",3.6 Implementation Details,[0],[0]
"eI), it is cauclulated as,
θs̄lex(fj |ei) =",3.6 Implementation Details,[0],[0]
"( ∑ s′ N(fj |ei)(s ′))−N(fj |ei)(s) ( ∑ s′ nei(s′))− nei(s) .
",3.6 Implementation Details,[0],[0]
"(18)
",3.6 Implementation Details,[0],[0]
"The global counts to be maintained are∑ s′ N(fj |ei)(s
′) and nei(s′), and the local counts are ∑ s N(fj |ei)(s) and nei(s).",3.6 Implementation Details,[0],[0]
"Therefore the memory cost is,
|E| · (|F|+ 1) + ∑ s Is(Js + 1), (19)
where |E| is the size of English vocabulary, |F| is the size of foreign language vocabulary, Is is the length of the English sentence of s, and Js is the length of the foreign sentence of s.
",3.6 Implementation Details,[0],[0]
"The calculation of the leave-one-out translation model is performed for each English word and foreign word in s. Therefore, the time cost is,∑
s
Is(Js + 1).",3.6 Implementation Details,[0],[0]
"(20)
In addition, because the local counts N(fj |ei)(s) and nei(s) are read in order, storing them in a external memory such as a hard disk will not slow down the running speed much.",3.6 Implementation Details,[0],[0]
"This will reduce the memory cost to
|E| · (|F|+ 1).",3.6 Implementation Details,[0],[0]
"(21) This cost is independent to the number of sentence pairs4.
",3.6 Implementation Details,[0],[0]
The speed of the proposed method can be boosted through parallelism.,3.6 Implementation Details,[0],[0]
These calculations on each sentence pair can be performed independently.,3.6 Implementation Details,[0],[0]
"We found empirically that when our implementation of the proposed method is run on a 16-core computer, it finishes the task earlier than GIZA++5.",3.6 Implementation Details,[0],[0]
The proposed WA method was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2).,4 Experiments,[0],[0]
"Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks.",4 Experiments,[0],[0]
GIZA++ and our own implementation of standard EM were used as baselines.,4 Experiments,[0],[0]
The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus.,4.1 Experimental Settings,[0],[0]
"They are from the same domain, both contain newswire texts and web blogs.",4.1 Experimental Settings,[0],[0]
"The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set.",4.1 Experimental Settings,[0],[0]
"The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6.",4.1 Experimental Settings,[0],[0]
"The corpus contains a set of 1,235 sentence pairs that are manually word aligned.
",4.1 Experimental Settings,[0],[0]
The corpora were processed using a standard procedure for machine translation.,4.1 Experimental Settings,[0],[0]
"The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7; the Japanese texts
4We found the memory of our server is large enough, so we did not implement it
5We plan to make our code public available.",4.1 Experimental Settings,[0],[0]
"6http://www.phontron.com/kftt/ 7http://nlp.stanford.edu/software/
segmenter.shtml
were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8).",4.1 Experimental Settings,[0],[0]
"Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.
",4.1 Experimental Settings,[0],[0]
"GIZA++ was run with the default Moses settings (Koehn et al., 2007).",4.1 Experimental Settings,[0],[0]
"The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations.",4.1 Experimental Settings,[0],[0]
"We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4.",4.1 Experimental Settings,[0],[0]
"In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings.",4.1 Experimental Settings,[0],[0]
"They were run with 5, 5 and 6 iterations.
",4.1 Experimental Settings,[0],[0]
"The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details.",4.1 Experimental Settings,[0],[0]
"Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9.",4.1 Experimental Settings,[0],[0]
"We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS.
",4.1 Experimental Settings,[0],[0]
"In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages.",4.1 Experimental Settings,[0],[0]
"Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003).",4.1 Experimental Settings,[0],[0]
"Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003).",4.2 Word Alignment Accuracy,[0],[0]
The proposed method gave rise to higher quality alignments in all our experiments.,4.2 Word Alignment Accuracy,[0],[0]
"The improvement in F1, precision and recall based on IBM Model 4 is in the range 8.3% to 9.1% compared with the GIZA++ baseline, and in the range 5.0% to 17.2% compared with our own baseline.
",4.2 Word Alignment Accuracy,[0],[0]
"The most meaningful result comes from the comparison of the models trained using standard EM log-likelihood training, and the proposed EM leave-one-out log-likelihood training.",4.2 Word Alignment Accuracy,[0],[0]
These models are identical except for way in which the model likelihood is calculated.,4.2 Word Alignment Accuracy,[0],[0]
In all our experiments the proposed method gave rise to higher quality alignments.,4.2 Word Alignment Accuracy,[0],[0]
"The standard EM implementation achieved
8http://www.phontron.com/kytea/ 9http://www2.nict.go.jp/univ-com/multi trans/cicada/
alignment performance approximately comparable to GIZA++, whereas the proposed method exceeded the performance of both implementations.",4.2 Word Alignment Accuracy,[0],[0]
"BLEU scores achieved by the phrase-based and hierachical SMT systems10 which were trained from different alignment results, are shown in Table 4.",4.3 End-to-end Translation Quality,[0],[0]
Each experiment was conducted three times to mitigate the variance in the results due to MERT.,4.3 End-to-end Translation Quality,[0],[0]
The results show that the proposed alignment method achieved the highest BLEU score in all experiments.,4.3 End-to-end Translation Quality,[0],[0]
"The improvement over the baseline is in range 0.03 to 1.03 for phrase-based systems, and ranged from 0.43 to 1.30 for hierarchical systems.
",4.3 End-to-end Translation Quality,[0],[0]
Hierarchical systems benifit more from the proposed method than phrase-based systems.,4.3 End-to-end Translation Quality,[0],[0]
We think this is because that hierarchical systems are more sensitive to word alignment quality than phrase-based systems.,4.3 End-to-end Translation Quality,[0],[0]
"Phrase-based systems only
10from the Moses toolkit
take contiguous parallel phrase pairs as translation rules, while hierarchical systems also use patterns made by subtracting (inner) short parallel phrases from (outer) longer parallel phrases.",4.3 End-to-end Translation Quality,[0],[0]
Both the outer and inner phrases typically need to be noisefree in order to produce high quality rules.,4.3 End-to-end Translation Quality,[0],[0]
This puts a high demand on the alignment quality.,4.3 End-to-end Translation Quality,[0],[0]
"Training corpora of different sizes were employed to perform unsupervised WA experiments and MT experiments (see Tables 5 and 6).
",4.4 Effect of Training Corpus Size,[0],[0]
The training corpora were randomly sampled from the Chinese-English manual WA corpora and the parallel training corpus.,4.4 Effect of Training Corpus Size,[0],[0]
"The manual WA corpus has a priority for being sampled so that the gold WA annotation is available for MT experi-
ments.",4.4 Effect of Training Corpus Size,[0],[0]
The settings of the unsupervised WA experiments and the MT experiments are the same with the previous experiments.,4.4 Effect of Training Corpus Size,[0],[0]
"In the WA experiments, GIZA++, our implemented standard EM and the proposed leave-one-out EM are applied to training corpora with the same parameter settings as the previous.",4.4 Effect of Training Corpus Size,[0],[0]
"In the MT experiments, the WA results of different methods and the gold WA (if available) are employed to extract translation rules; the rest settings including language models, development and test corpus, and parameters are the same as the previous.
",4.4 Effect of Training Corpus Size,[0],[0]
"On word alignment accuracy, the proposed method achieved improvements of F1 from 0.041 to 0.090 under the different training corpora (Table 5.",4.4 Effect of Training Corpus Size,[0],[0]
"The maximum improvement compared with GIZA++ is 0.069 when the training corpus has 4,000 sentence pairs.",4.4 Effect of Training Corpus Size,[0],[0]
"The maximum improvement compared with our own implement is 0.090 when the training corpus has 64,000 sentence pairs.
",4.4 Effect of Training Corpus Size,[0],[0]
"Figure 2 shows that the extent of improvements slightly changes under different training corpora, but they are all quite stable and obvious.
",4.4 Effect of Training Corpus Size,[0],[0]
"On translation quality, the proposed method achieved improvements of BLEU under the different training corpora.",4.4 Effect of Training Corpus Size,[0],[0]
The improvements ranged from 0.19 to 1.72 for phrase-based MT and ranged from 0.25 to 3.02 (see Table 5).,4.4 Effect of Training Corpus Size,[0],[0]
"The improvements are larger under smaller training corpora (see Figure 3).
",4.4 Effect of Training Corpus Size,[0],[0]
"In addition, the BLEUs achieved by the proposed method is close to the ones achieved by gold WA annotations.",4.4 Effect of Training Corpus Size,[0],[0]
"The proposed method slightly outperforms the gold WA annotations when using the full manual WA corpus of 18,057 sentence pairs.
",4.4 Effect of Training Corpus Size,[0],[0]
"4.5 Comparison to l0-Normalization and Kneser-Ney Smoothing Methods
The proposed leave-one-word word alignment method was empirically compared to l0-normalized GIZA++",4.4 Effect of Training Corpus Size,[0],[0]
"(Vaswani et al., 2012)11 and Kneser-Ney smoothed GIZA++",4.4 Effect of Training Corpus Size,[0],[0]
"(Zhang and Chiang, 2014)12.",4.4 Effect of Training Corpus Size,[0],[0]
l0-normalization and KneserNey smoothing methods are established methods to overcome the sparse problem.,4.4 Effect of Training Corpus Size,[0],[0]
This enables the probability distributions on rare words to be estimated more effectively.,4.4 Effect of Training Corpus Size,[0],[0]
"In this way, these two GIZA++ variants are related to the proposed method.
",4.4 Effect of Training Corpus Size,[0],[0]
"l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES.",4.4 Effect of Training Corpus Size,[0],[0]
For the settings of l0-normalized GIZA++ that are not in common with GIZA++ were the default settings.,4.4 Effect of Training Corpus Size,[0],[0]
"As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 – 4 and HMM model
11http://www.isi.edu/˜avaswani/ giza-pp-l0.html
12https://github.com/hznlp/giza-kn
were turned on.
",4.4 Effect of Training Corpus Size,[0],[0]
The experimental results are presented in Table 7.,4.4 Effect of Training Corpus Size,[0],[0]
The experiments were run on the ChineseEnglish language pair.,4.4 Effect of Training Corpus Size,[0],[0]
The word alignment quality was evaluated separately for all words and for various levels of rare words.,4.4 Effect of Training Corpus Size,[0],[0]
"The leave-one-out method outperformed related methods in terms of precision, recall and F1 when evaluated on all words.
",4.4 Effect of Training Corpus Size,[0],[0]
Rare words were categorized based on the number of occurences in the source-language text of the training data.,4.4 Effect of Training Corpus Size,[0],[0]
The evaluations were carried out on the subset of alignment links that had a rare word on the source side.,4.4 Effect of Training Corpus Size,[0],[0]
"Table 7 presents the results for thresholds 1, 2, 5 and 10.",4.4 Effect of Training Corpus Size,[0],[0]
"The proposed method achieved much higher precision on rare words than the other methods, but performed poorly on recall.",4.4 Effect of Training Corpus Size,[0],[0]
The Kneser-Ney Smoothed GIZA++ had higher recall.,4.4 Effect of Training Corpus Size,[0],[0]
"The explanation might be that the leave-one-out method punishes rare words more than the Kneser-Ney smoothing method, by totally removing the derived expected counts of current sentence pair from the alignment models.",4.4 Effect of Training Corpus Size,[0],[0]
This leads to rare words being passively aligned.,4.4 Effect of Training Corpus Size,[0],[0]
"In other words, the leave-one-out method would align rare words unless the confidence is high.",4.4 Effect of Training Corpus Size,[0],[0]
"Therefore, we plan to seek a method to integrate Kneser-Ney smoothing into the proposed leave-one-out method in the future work.
",4.4 Effect of Training Corpus Size,[0],[0]
The BLEU scores achieved by phrase-based SMT and hierarchical SMT for different alignment methods are presented in Table 7.,4.4 Effect of Training Corpus Size,[0],[0]
The proposed method outperforms the other methods.,4.4 Effect of Training Corpus Size,[0],[0]
The Kneser-Ney Smoothed GIZA++ performed the second best.,4.4 Effect of Training Corpus Size,[0],[0]
"We tried to further analyze the relation between word alignment and BLEU, but found the analysis was obscured by the many processing stages.",4.4 Effect of Training Corpus Size,[0],[0]
"These stages include paral-
lel phrase extraction (or translation rule extraction from hierarchical SMT), log-linear model, MERT tuning and practical decoding where a lot of pruning happened.",4.4 Effect of Training Corpus Size,[0],[0]
This paper proposes a leave-one-out EM algorithm for WA to overcome the over-fitting problem that occurs when using standard EM for WA.,5 Conclusion,[0],[0]
"The experimental results on Chinese-English and Japanese-English corpora show that both the WA accuracy and the end-to-end translation are improved.
",5 Conclusion,[0],[0]
"In addition, we have a interesting finding about the effect of manual WA annotations on training MT systems.",5 Conclusion,[0],[0]
"In a Chinese-English parallel training corpus of 18,057 sentence pairs, the manual WA annotation outperformed the unsupervised WA results produced by standard EM algorithms.",5 Conclusion,[0],[0]
"However, the unsupervised WA results produced by proposed leave-one-out EM algorithm outperformed the manual WA annotation.
",5 Conclusion,[0],[0]
Our future work will focus on increasing the gains in end-to-end translation quality through the proposed leave-one-out aligner.,5 Conclusion,[0],[0]
It is a interesting question why GIZA++ achieved competitive BLEU scores though its alignment accuracy measured by F1 was substantially lower.,5 Conclusion,[0],[0]
The answer to this question which may reveal essence of good word alignment for MT and eventually help to improve MT.,5 Conclusion,[0],[0]
"In addition, we plan to improve the proposed method by integrating Kneser-Ney smoothing.",5 Conclusion,[0],[0]
We appreciated the valuable comments from the reviewers.,Acknowledgments,[0],[0]
"Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the field of unsupervised word alignment.",abstractText,[0],[0]
"However, these algorithms have a problem of over-fitting, leading to “garbage collector effects,” where rare words tend to be erroneously aligned to untranslated words.",abstractText,[0],[0]
This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem.,abstractText,[0],[0]
The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it.,abstractText,[0],[0]
This prevents erroneous alignments within a sentence pair from supporting themselves.,abstractText,[0],[0]
"Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% – 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 – 1.30.",abstractText,[0],[0]
The proposed method also outperformed l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++.,abstractText,[0],[0]
Leave-one-out Word Alignment without Garbage Collector Effects,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1090–1100 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.",text,[0.9556997915611534],['In Section 5 we will empirically evaluate the different bounds as meta-learning objectives and show that the improved tightness is critical for performance.']
"Topic models provide a high-level view of the main themes of a document collection (Boyd-Graber et al., 2017).",1 Introduction,[0],[0]
"Document collections, however, are often not in a single language, driving the development of multilingual topic models.",1 Introduction,[0],[0]
"These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis (Vulić et al., 2015), such as detecting cultural differences (Gutiérrez et al., 2016) and bilingual dictionary extraction (Liu et al., 2015).
",1 Introduction,[0],[0]
"Monolingual topic models can be evaluated through likelihood (Wallach et al., 2009b) or coherence (Newman et al., 2010), but topic model evaluation is not well understood in multilingual settings.",1 Introduction,[0],[0]
Our contributions are two-fold.,1 Introduction,[0],[0]
"We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2).",1 Introduction,[0],[0]
We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications.,1 Introduction,[0],[0]
"This metric
correlates well with human judgments and crosslingual classification results (Sections 5 and 6).
",1 Introduction,[0],[0]
"We also focus on evaluation in low-resource languages, which lack large parallel corpora, dictionaries, and other tools that are often used in learning and evaluating topic models.",1 Introduction,[0],[0]
"To adapt CNPMI to these settings, we create a coherence estimator (Section 3) that extrapolates statistics derived from antiquated, specialized texts like the Bible: often the only resource available for many languages.",1 Introduction,[0],[0]
A multilingual topic contains one topic for each language.,2 Evaluating Multilingual Coherence,[0],[0]
"For a multilingual topic to be meaningful to humans (Figure 1), the meanings should be consistent across the languages, in addition to coherent within each language (i.e., all words in a topic are related).
",2 Evaluating Multilingual Coherence,[0],[0]
This section describes our approach to evaluating the quality of multilingual topics.,2 Evaluating Multilingual Coherence,[0],[0]
"After defining the multilingual topic model, we describe topic model evaluation extending standard monolingual approaches to multilingual settings.",2 Evaluating Multilingual Coherence,[0],[0]
"Probabilistic topic models associate each document in a corpus with a distribution over latent topics, while each topic is associated with a distribution over words in the vocabulary.",2.1 Multilingual Topic Modeling,[0],[0]
"The most widely used topic model, latent Dirichlet allocation (Blei et al., 2003, LDA), can be extended to connect languages.",2.1 Multilingual Topic Modeling,[0],[0]
"These extensions require additional knowledge to link languages together.
",2.1 Multilingual Topic Modeling,[0],[0]
"One common encoding of multilingual knowledge is document links (indicators that documents are parallel or comparable), used in polylingual topic models (Mimno et al., 2009; Ni et al., 2009).",2.1 Multilingual Topic Modeling,[0],[0]
"In these models, each document d indexes a tuple of parallel/comparable language-specific documents,
1090
d(`), and the language-specific “views” of a document share the document-topic distribution θd.",2.1 Multilingual Topic Modeling,[0],[0]
"The generative story for the document-links model is:
1 for each topic k and each language ` do 2 Draw a distribution over words φ`k ∼ Dirichlet(β); 3 for each document tuple d = ( d(1), . . .",2.1 Multilingual Topic Modeling,[0],[0]
", d(L) )",2.1 Multilingual Topic Modeling,[0],[0]
"do 4 Draw a distribution over topics θd ∼ Dirichlet(α); 5 for each language ` = 1, . . .",2.1 Multilingual Topic Modeling,[0],[0]
",L do 6 for each token t ∈ d(`) do 7 Draw a topic zn ∼ θd; 8 Draw a word wn ∼ φ`z;
Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges.",2.1 Multilingual Topic Modeling,[0],[0]
"Since the polylingual topic model is the most common approach for building multilingual topic models (Vulić et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model.",2.1 Multilingual Topic Modeling,[0],[0]
"Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010).",2.2 Monolingual Evaluation,[0],[0]
"The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI).",2.2 Monolingual Evaluation,[0],[0]
"NPMI compares the joint probability of words appearing together Pr(wi,wj) to their probability assuming independence Pr(wi) Pr(wj), normalized by the joint probability:
NPMI(wi,wj) = log
Pr(wi,wj) Pr(wi) Pr(wj)
log Pr(wi,wj) .",2.2 Monolingual Evaluation,[0],[0]
"(1)
The word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset.
",2.2 Monolingual Evaluation,[0],[0]
"The quality of topic k is the average NPMI of all word pairs (wi,wj) in the topic:
NPMIk = −1",2.2 Monolingual Evaluation,[0],[0]
"( C 2 ) ∑ i∈W(k,C) ∑ j 6=i NPMI(wi,wj), (2)
where W(k,C) are the C most probable words in the topic-word distribution φk (the number of words is the topic’s cardinality).",2.2 Monolingual Evaluation,[0],[0]
Higher NPMIk means the topic’s top words are more coupled.,2.2 Monolingual Evaluation,[0],[0]
"While automatic evaluation has been well-studied for monolingual topic models, there are no robust evaluations for multilingual topic models.",2.3 Existing Multilingual Evaluations,[0],[0]
"We first consider two straightforward metrics that could be used for multilingual evaluation, both with limitations.",2.3 Existing Multilingual Evaluations,[0],[0]
"We then propose an extension of NPMI that addresses these limitations.
",2.3 Existing Multilingual Evaluations,[0],[0]
Internal Coherence.,2.3 Existing Multilingual Evaluations,[0],[0]
A simple adaptation of NPMI is to calculate the monolingual NPMI score for each language independently and take the average.,2.3 Existing Multilingual Evaluations,[0],[0]
We refer this as internal NPMI (INPMI) as it evaluates coherence within a language.,2.3 Existing Multilingual Evaluations,[0],[0]
"However, this metric does not consider whether the topic is coherent across languages—that is, whether a language-specific word distribution φ`1k is related to the corresponding distribution in another language, φ`2k.
Crosslingual Consistency.",2.3 Existing Multilingual Evaluations,[0],[0]
"Another straightforward measurement is Matching Translation Accuracy (Boyd-Graber and Blei, 2009, MTA), which counts the number of word translations in a topic between two languages using a bilingual dictionary.",2.3 Existing Multilingual Evaluations,[0],[0]
"This metric can measure whether a topic is well-aligned across languages literally, but cannot capture non-literal more holistic similarities across languages.",2.3 Existing Multilingual Evaluations,[0],[0]
"We extend NPMI to multilingual models, with a metric we call crosslingual normalized pointwise mutual information (CNPMI).",2.4 New Metric: Crosslingual NPMI,[0],[0]
"This metric will be the focus of our experiments.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"A multilingually coherent topic means that if wi,`1 in language `1 and wj,`2 in language `2 are in the same topic, they should appear in similar contexts in comparable or parallel corporaR(`1,`2).
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Our adaptation of NPMI is based on the same principles as the monolingual version, but focuses on the co-occurrences of bilingual word pairs.",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Given a bilingual word pair (wi,`1 ,wj,`2) the co-occurrence of this word pair is the event where word wi,`1 appears in a document in language `1 and the word wj,`2 appears in a comparable or parallel document in language `2.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"The co-occurrence probability of each bilingual word pair is:
Pr (wi,`1 ,wj,`2) , ∣∣{d : wi,`1 ∈ d(`1),wj,`2 ∈ d(`2) }∣∣ ∣∣R(`1,`2) ∣∣ , (3)
where d = ( d(`1), d(`2) ) is a pair of parallel/comparable documents in the reference corpus R(`1,`2).",2.4 New Metric: Crosslingual NPMI,[0],[0]
"When one or both words in a bilingual pair do not appear in the reference corpus, the cooccurrence score is zero.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Similar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C2 bilingual word pairs,
CNPMI(`1, `2, k) =
∑C",2.4 New Metric: Crosslingual NPMI,[0],[0]
"i,j NPMI (wi,`1 ,wj,`2)
C2 .",2.4 New Metric: Crosslingual NPMI,[0],[0]
"(4)
It is straightforward to generalize CNPMI from a language pair to multiple languages by averaging CNPMI(`i, `j , k) over all language pairs (`i, `j).",2.4 New Metric: Crosslingual NPMI,[0],[0]
CNPMI needs a reference corpus for co-occurrence statistics.,3 Adapting to Low-Resource Languages,[0],[0]
"Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016).",3 Adapting to Low-Resource Languages,[0],[0]
"Unfortunately, Wikipedia is often unavailable or not large enough for lowresource languages.",3 Adapting to Low-Resource Languages,[0],[0]
"It only covers 282 languages,1 and only 249 languages have more than 1,000 pages: many of pages are short or unlinked to
1 https://meta.wikimedia.org/wiki/List_of_Wikipedias
a high-resource language.",3 Adapting to Low-Resource Languages,[0],[0]
"Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents.
",3 Adapting to Low-Resource Languages,[0],[0]
"Another option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic.",3 Adapting to Low-Resource Languages,[0],[0]
"It is good at evaluating topics such as family and religion, but not “modern” topics like biology and Internet.",3 Adapting to Low-Resource Languages,[0],[0]
"Without reference co-occurrence statistics relevant to these topics, CNPMI will fail to judge topic coherence—it must give the ambiguous answer of zero.",3 Adapting to Low-Resource Languages,[0],[0]
"Such a score could mean a totally incoherent topic where each word pair never appears together (Topics 6 in Figure 1), or an unjudgeable topic (Topic 5).
",3 Adapting to Low-Resource Languages,[0],[0]
Our goal is to obtain a reliable estimation of topic coherence for low-resource languages when the Bible is the only reference.,3 Adapting to Low-Resource Languages,[0],[0]
We propose a model that can correct the drawbacks of a Bible-derived CNPMI.,3 Adapting to Low-Resource Languages,[0],[0]
"While we assume bilingual topics paired with English, our approach can be applied to any high-resource/low-resource language pair.
",3 Adapting to Low-Resource Languages,[0],[0]
We take Wikipedia’s CNPMI from high-resource languages as accurate estimations.,3 Adapting to Low-Resource Languages,[0],[0]
"We then build a coherence estimator on topics from high-resource languages, with the Wikipedia CNPMI as the target output.",3 Adapting to Low-Resource Languages,[0],[0]
We use linear regression using the below features.,3 Adapting to Low-Resource Languages,[0],[0]
"Given a topic in low-resource language, the estimator produces an estimated coherence (Figure 2).",3 Adapting to Low-Resource Languages,[0],[0]
The key to the estimator is to find features that capture whether we should trust the Bible.,3.1 Estimator Features,[0],[0]
"For generality, we focus on features independent of the available resources other than the Bible.",3.1 Estimator Features,[0],[0]
"This section describes the features, which we split into four groups.
",3.1 Estimator Features,[0],[0]
Base Features (BASE),3.1 Estimator Features,[0],[0]
"Our base features include information we can collect from the Bible and the topic model: cardinality C, CNPMI and INPMI, MTA, and topic word coverage (TWC), which counts the percentage of topic words in a topic that appear in a reference corpus.
Crosslingual Gap (GAP) A low CNPMI score could indicate a topic pair where each language has a monolingually coherent topic but that are not about the same theme (Topic 6 in Figure 1).",3.1 Estimator Features,[0],[0]
"Thus, we add two features to capture this information
2The Bible is available in 2,530 languages.
using the Bible: mismatch coefficients (MC) and internal comparison coefficients (ICC):
MC(`1; `2, k) = CNPMI(`1, `2, k)
INPMI(`1, k) + α , (5)
",3.1 Estimator Features,[0],[0]
"ICC(`1, `2, k) = INPMI(`1, k) + α
INPMI(`2, k) + α , (6)
where α is a smoothing factor (α = 0.001 in our experiments).",3.1 Estimator Features,[0],[0]
"MC recognizes the gap between crosslingual and monolingual coherence, so a higher MC score indicates a gap between coherence within and across languages.",3.1 Estimator Features,[0],[0]
"Similarly, ICC compares monolingual coherence to tell if both languages are coherent: the closer to 1 the ICC is, the more comparable internal coherence both languages have.
",3.1 Estimator Features,[0],[0]
"Word Era (ERA) Because the Bible’s vocabulary is unable to evaluate modern topics, we must tell the model what the modern words are.",3.1 Estimator Features,[0],[0]
The word era features are the earliest usage year 3 for each word in a topic.,3.1 Estimator Features,[0],[0]
"We use both the mean and standard deviation as features.
",3.1 Estimator Features,[0],[0]
Meaning Drift (DRIFT).,3.1 Estimator Features,[0],[0]
The meaning of a word can expand and drift over time.,3.1 Estimator Features,[0],[0]
"For example, in the Bible, “web” appears in Isaiah 59:5:
They hatch cockatrice’ eggs, and weave the spider’s web.
3 https://oxforddictionaries.com/
The word “web” could be evaluated correctly in an animal topic.",3.1 Estimator Features,[0],[0]
"For modern topics, however, Bible fails to capture modern meanings of “web”, as in Topic 5 (Figure 1).
",3.1 Estimator Features,[0],[0]
"To address this meaning drift, we use a method similar to Hamilton et al. (2016).",3.1 Estimator Features,[0],[0]
"For each English word, we calculate the context vector from Bible and from Wikipedia with a window size of five and calculate the cosine similarity between them as word similarity.",3.1 Estimator Features,[0],[0]
Similar context vectors mean that the usage in the Bible is consistent with Wikipedia.,3.1 Estimator Features,[0],[0]
We calculate word similarities for all the English topic words in a topic and use the average and standard deviation as features.,3.1 Estimator Features,[0],[0]
"In Figure 3, Topic 1 is coherent while Topic 8 is not.",3.2 Example,[0],[0]
"From left to right, we incrementally add new feature sets, and show how the estimated topic coherence scores (dashed lines) approach the ideal CNPMI (dotted lines).",3.2 Example,[0],[0]
"When only using the BASE features, the estimator gives a higher prediction to Topic 8 than to Topic 1.",3.2 Example,[0],[0]
Their low MTA and TWC prevent accurate evaluations.,3.2 Example,[0],[0]
Adding GAP does not help much.,3.2 Example,[0],[0]
"However, ICC(EN, AM, k = 1) is much smaller, which might indicate a large gap of internal coherence between the two languages.
",3.2 Example,[0],[0]
Adding ERA makes the estimated scores flip between the two topics.,3.2 Example,[0],[0]
"Topic 1 has word era of 1823, much older than Topic 8’s word era of 1923, in-
dicating that Topic 8 includes modern words the Bible lacks (e.g., “computer”).",3.2 Example,[0],[0]
"Using all the features, the estimator gives more accurate topic coherence evaluations.",3.2 Example,[0],[0]
"We experiment on six languages (Table 1) from three corpora: Romanian (RO) and Swedish (SV) from EuroParl as representative of well-studied and rich-resource languages (Koehn, 2005); Amharic (AM) and Tagalog (TL) from collected news, as lowresource languages (Huang et al., 2002a,b); and Chinese (ZH) and Turkish (TR) from TED Talks 2013 (Tiedemann, 2012), adding language variety to our experiments.",4 Experiments: Bible to Wikipedia,[0],[0]
"Each language is paired with English as a bilingual corpus.
",4 Experiments: Bible to Wikipedia,[0],[0]
"Typical preprocessing methods (stemming, stop word removal, etc.) are often unavailable for lowresource languages.",4 Experiments: Bible to Wikipedia,[0],[0]
"For a meaningful comparison across languages, we do not apply any stemming or lemmatization strategies, including English, except removing digit numbers and symbols.",4 Experiments: Bible to Wikipedia,[0],[0]
"However, we remove words that appear in more than 30% of documents for each language.
",4 Experiments: Bible to Wikipedia,[0],[0]
"Each language pair is separately trained using the MALLET (McCallum, 2002) implementation of the polylingual topic model.",4 Experiments: Bible to Wikipedia,[0],[0]
"Each experiment runs five Gibbs sampling chains with 1,000 iterations per chain with twenty topics.",4 Experiments: Bible to Wikipedia,[0],[0]
"The hyperparameters are set to the default values (α = 0.1, β = 0.01), and are optimized every 50 iterations in MALLET using slice sampling (Wallach et al., 2009a).",4 Experiments: Bible to Wikipedia,[0],[0]
We use Wikipedia and the Bible as reference corpora for calculating co-occurrence statistics.,4.1 Evaluating Multilingual Topics,[0],[0]
"Different numbers of Wikipedia articles are available for each language pair (Table 1), while the Bible contains a complete set of 1,189 chapters for all of its translations (Christodoulopoulos and Steed-
Are these two groups of words talking about the same thing?
man, 2015).",4.1 Evaluating Multilingual Topics,[0],[0]
We use Wiktionary as the dictionary to calculate MTA.,4.1 Evaluating Multilingual Topics,[0],[0]
"In addition to experimenting on Wikipedia-based CNPMI, we also re-evaluate the topics’ Bible coherence using our estimator.",4.2 Training the Estimator,[0],[0]
"In the following experiments, we use an AdaBoost regressor with linear regression as the coherence estimator (Friedman, 2002; Collins et al., 2000).",4.2 Training the Estimator,[0],[0]
"The estimator takes a topic and low-quality CNPMI score as input and outputs (hopefully) an improved CNPMI score.
",4.2 Training the Estimator,[0],[0]
"To make our testing scenario more realistic, we treat one language as our estimator’s test language and train on multilingual topics from the other languages.",4.2 Training the Estimator,[0],[0]
"We use three-fold cross-validation over languages to select the best hyperparameters, including the learning rate and loss function in AdaBoost.",4.2 Training the Estimator,[0],[0]
"R2 (Drucker, 1997).",4.2 Training the Estimator,[0],[0]
We first study CNPMI at the topic level: does a particular topic make sense?,5 Topic-Level Evaluation,[0],[0]
"An effective evaluation should be consistent with human judgment of the topics (Chang et al., 2009).",5 Topic-Level Evaluation,[0],[0]
"In this section, we measure gold-standard human interpretability of multilingual topics to establish which automatic measures of topic interpretability work best.",5 Topic-Level Evaluation,[0],[0]
"Following monolingual coherence evaluations (Lau et al., 2014), we present topic pairs to bilingual CrowdFlower users.",5.1 Task Design,[0],[0]
Each task is a topic pair with the top ten topic words (C = 10) for each language.,5.1 Task Design,[0],[0]
"We ask if both languages’ top words in a multilingual topic are talking about the same concept (Figure 4), and make a judgment on a three-point scale—coherent (2 points), somewhat coherent (1 point), and incoherent (0 points).",5.1 Task Design,[0],[0]
"To ensure the users have adequate language competency, we insert several topics that are easily identifiable as incoherent as a qualification test.
",5.1 Task Design,[0],[0]
"We randomly select sixty topics from each language pair (360 topics total), and each topic is judged by five users.",5.1 Task Design,[0],[0]
We take the average of the judgment points and calculate Pearson correlations with the proposed evaluation metrics (Table 2).,5.1 Task Design,[0],[0]
NPMI-based scores are separately calculated from each reference corpus.,5.1 Task Design,[0],[0]
"CNPMI (the extended metric) has higher correlations with human judgments than INPMI (the naive adaptation of monolingual NPMI), while MTA (matching translation accuracy) correlations are comparable to CNPMI.
",5.2 Agreement with Human Judgments,[0],[0]
"Unsurprisingly, when using Wikipedia as the reference, the correlations are usually higher than when using the Bible.",5.2 Agreement with Human Judgments,[0],[0]
"The Bible’s archaic content limits its ability to estimate human judgments in modern corpora (Section 3).
",5.2 Agreement with Human Judgments,[0],[0]
"Next, we compare CNPMI to two baselines: INPMI and MTA.",5.2 Agreement with Human Judgments,[0],[0]
"As expected, CNPMI outperforms INPMI regardless of reference corpus overall, because INPMI only considers monolingual coherence.",5.2 Agreement with Human Judgments,[0],[0]
"MTA has higher correlations than CNPMI
scores from the Bible, because the Bible fails to give accurate estimates due to limited topic coverage.",5.2 Agreement with Human Judgments,[0],[0]
"MTA, on the other hand, only depends on dictionaries, which are more comprehensive than the Bible.",5.2 Agreement with Human Judgments,[0],[0]
"It is also possible that users are judging coherence based on translations across a topic pair, rather than the overall coherence, which would closely correlate with MTA.",5.2 Agreement with Human Judgments,[0],[0]
The Bible—by itself—produces CNPMI values that do not correlate well with human judgments (Table 2).,5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"After training an estimator (Section 4.2), we calculate Pearson’s correlation between Wikipedia’s CNPMI and the estimated topic coherence score (Table 3).",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"A higher correlation with Wikipedia’s CNPMI means more accurate coherence.
",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"As a baseline, the correlation of Bible-based CNPMI without adaptation has negative and nearzero correlations with Wikipedia;4 it does not capture coherence.",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"After training the estimator, the correlations become stronger, indicating the estimated scores are closer to Wikipedia’s CNPMI.",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"We analyze MTA from two aspects—the inability to capture semantically-related non-translation topic words, and insensitivity to cardinality—to show why MTA is not an ideal measurement, even though it correlates well with human judgments.
",5.4 When MTA Falls Short,[0],[0]
Semantics We take two examples with EN-ZH (Topic 1) and EN-TL (Topic 2) in Figure 5.,5.4 When MTA Falls Short,[0],[0]
"Topic 1 has fewer translation pairs than Topic 2, which leads to a lower MTA score for Topic 1.",5.4 When MTA Falls Short,[0],[0]
"However, all words in Topic 1 talk about art, while it is hard to interpret Topic 2.",5.4 When MTA Falls Short,[0],[0]
"Wikipedia CNPMI scores reveals
4Normally one would not estimate CNPMI on rich-resource languages using low-resource languages.",5.4 When MTA Falls Short,[0],[0]
"For completeness, however, we also include these situations.
",5.4 When MTA Falls Short,[0],[0]
Topic 1 is more coherent.,5.4 When MTA Falls Short,[0],[0]
"Because our experiments are on datasets with little divergence between the themes discussed across languages, this is uncommon for us but could appear in noisier datasets.
",5.4 When MTA Falls Short,[0],[0]
"Cardinality Increasing cardinality diminishes a topic’s coherence (Lau and Baldwin, 2016).",5.4 When MTA Falls Short,[0],[0]
We vary the cardinality of topics from ten to fifty at intervals of ten (Figure 6).,5.4 When MTA Falls Short,[0],[0]
"As cardinality increases, more low-probability and irrelevant words appear the topic, which lowers CNPMI scores.",5.4 When MTA Falls Short,[0],[0]
"However, MTA stays stable or increases with increasing cardinality.",5.4 When MTA Falls Short,[0],[0]
"Thus, MTA fails to fulfill a critical property of topic model evaluation.
",5.4 When MTA Falls Short,[0],[0]
"Finally, MTA requires a comprehensive multilingual dictionary, which may be unavailable for lowresource languages.",5.4 When MTA Falls Short,[0],[0]
"Additionally, most languages often only have one dictionary, which makes it problematic to use the same resource (a language’s single multilingual dictionary) for training and evaluating models that use a dictionary to build multilingual topics (Hu et al., 2014).",5.4 When MTA Falls Short,[0],[0]
"Given these concerns, we continue the paper’s focus on CNPMI as a data-driven alternative to MTA.",5.4 When MTA Falls Short,[0],[0]
"However, for many applications MTA may suffice as a simple, adequate evaluation metric.",5.4 When MTA Falls Short,[0],[0]
"While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model’s constituent topics.",6 Model-Level Evaluation,[0],[0]
"Adding more knowledge to multilingual topic models improves topics (Hu et al., 2014), so an effective evaluation should reflect this improvement as knowlege is added to the model.",6.1 Training Knowledge,[0],[0]
"For polylingual topic models, this knowledge takes the form of the number of linked documents.
",6.1 Training Knowledge,[0],[0]
We start by experimenting with no multilingual knowledge: no document pairs share a topic distribution,6.1 Training Knowledge,[0],[0]
θd (but the documents are in the collection as unlinked documents).,6.1 Training Knowledge,[0],[0]
We then increase the number of document pairs that share θd from 20% of the corpus to 100%.,6.1 Training Knowledge,[0],[0]
"Fixing the topic cardinality at ten, CNPMI captures the improvements in models (Figure 7) through a higher coherence score.",6.1 Training Knowledge,[0],[0]
"Topic models are often used as a feature extraction technique for downstream machine learning
applications, and topic model evaluations should reflect whether these features are useful (Ramage et al., 2009).",6.2 Agreement with Machines,[0],[0]
"For each model, we apply a document classifier trained on the model parameters to test whether CNPMI is consistent with classification accuracy.
",6.2 Agreement with Machines,[0],[0]
"Specifically, we want our classifier to transfer information from training on one language to testing on another (Smet et al., 2011; Heyman et al., 2016).",6.2 Agreement with Machines,[0],[0]
"We train a classifier on one language’s documents, where each document’s feature vector is the document-topic distribution θd.",6.2 Agreement with Machines,[0],[0]
"We apply this to TED Talks, where each document is labeled with multiple categories.",6.2 Agreement with Machines,[0],[0]
"We choose the most frequent seven categories across the corpus as labels,5 and only have labeled documents in one side of a bilingual topic model.",6.2 Agreement with Machines,[0],[0]
"CNPMI has very strong correlations with classification results, though using the Bible as the reference corpus gives slightly lower correlation—with higher variance— than Wikipedia (Figure 8).",6.2 Agreement with Machines,[0],[0]
"In Section 5.3, we improve Bible-based CNPMI scores for individual topics.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"Here, we show the estimator also improves model-level coherence.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"We apply the estimator on the models created in Section 6.2 and calculate the correlation between estimated scores and Wikipedia’s CNPMI (Table 4).
",6.3 Re-Estimating Model-Level Coherence,[0],[0]
The coherence estimator substantially improves scores except for Turkish: the correlation is better before applying the estimator (0.911).,6.3 Re-Estimating Model-Level Coherence,[0],[0]
"We suspect a lack of overlap between topics between Turkish and languages other than Chinese is to blame (Figure 9); the features used by the estimator do not generalize well to other kinds of features; training on many languages pairs would hopefully solve this
5design, global issues, art, science, technology, business, and culture
issue.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"Turkish is also morphologically rich, and our preprocessing completely ignores morphology.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"One challenge with low-resource languages is that even if Wikipedia is available, it may have too few documents to accurately calculate coherence.",6.4 Reference Size,[0],[0]
"As a final analysis, we examine how the reliability of CNPMI degrades with a smaller reference corpus.
",6.4 Reference Size,[0],[0]
"We randomly sample 20% to 100% of document pairs from the reference corpora and evaluate the polylingual topic model with all document links (Figure 10), again fixing the cardinality as 10.
",6.4 Reference Size,[0],[0]
"CNPMI is stable across different amounts of ref-
erence documents, as long as the number of reference documents is sufficiently large.",6.4 Reference Size,[0],[0]
"If there are too few reference documents (for example, 20% of Amharic Wikipedia is only 316 documents), then CNPMI degrades.",6.4 Reference Size,[0],[0]
Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI.,7 Related Work,[0],[0]
"Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments.",7 Related Work,[0],[0]
"NPMI has been the current gold standard for evaluation and improvements of monolingual topic models (Pecina, 2010; Newman et al., 2011).
",7 Related Work,[0],[0]
"External Tasks Another approach is to use a model for predictive tasks: the better the results are on external tasks, the better a topic model is assumed to be.",7 Related Work,[0],[0]
"A common task is held-out likelihood (Wallach et al., 2009b; Jagarlamudi and Daumé III, 2010; Fukumasu et al., 2012), but as Chang et al. (2009) show, this does not always reflect human interpretability.",7 Related Work,[0],[0]
"Other specific tasks have also been used, such as bilingual dictionary extraction (Liu et al., 2015; Ma and Nasukawa, 2017), cultural difference deteciton (Gutiérrez et al., 2016), and crosslingual document clustering (Vulić et al., 2015).
",7 Related Work,[0],[0]
"Representation Learning Topic models are one example of a broad class of techniques of learning representations of documents (Bengio et al., 2013).",7 Related Work,[0],[0]
"Other approaches learn respresentations at the word (Klementiev et al., 2012; Vyas and Carpuat, 2016), paragraph (Mogadala and Rettinger, 2016), or corpus level (Søgaard et al., 2015).",7 Related Work,[0],[0]
"However, neural representation learning approaches are often data hungry and not adaptable to low-resource languages.",7 Related Work,[0],[0]
"The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015).",7 Related Work,[0],[0]
"We have provided a comprehensive analysis of topic model evaluation in multilingual settings, including for low-resource languages.",8 Conclusion,[0],[0]
"While evaluation is an important area of topic model research, no previous work has studied evaluation of multilingual topic models.",8 Conclusion,[0],[0]
"Our work provided two primary contributions to this area, including a new intrinsic evaluation metric, CNPMI, as well as a model for adapting this metric to low-resource languages without large reference corpora.
",8 Conclusion,[0],[0]
"As the first study on evaluation for multilingual topic models, there is still room for improvement and further applications.",8 Conclusion,[0],[0]
"For example, human judgment is more difficult to measure than in monolingual settings, and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments.",8 Conclusion,[0],[0]
"As a measurement of multilingual coherence, we plan to extend CNPMI to high-dimensional representations, e.g., multilingual word embeddings, particularly in low-resource languages (Ruder et al., 2017).",8 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful and constructive comments.,Acknowledgement,[0],[0]
"Hao has been supported under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113.",Acknowledgement,[0],[0]
Boyd-Graber and Paul were supported by NSF grant IIS-1564275.,Acknowledgement,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.",Acknowledgement,[0],[0]
Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data.,abstractText,[0],[0]
"However, there is no standard and effective metric to evaluate the quality of multilingual topics.",abstractText,[0],[0]
We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications.,abstractText,[0],[0]
"Importantly, we also study evaluation for low-resource languages.",abstractText,[0],[0]
"Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.",abstractText,[0],[0]
Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 741–752 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1069",text,[0],[0]
The importance of understanding political discourse on social media platforms is becoming increasingly clear.,1 Introduction,[0],[0]
"In recent U.S. presidential elections, Twitter was widely used by all candidates to promote their agenda, interact with supporters, and attack their opponents.",1 Introduction,[0],[0]
Social interactions on such platforms allow politicians to quickly react to current events and gauge interest in and support for their actions.,1 Introduction,[0],[0]
These dynamic settings emphasize the importance of constructing automated tools for analyzing this content.,1 Introduction,[0],[0]
"However, these same dynamics make constructing such tools difficult, as the language used to discuss new events and political agendas continuously changes.",1 Introduction,[0],[0]
"Consequently, the rich social interactions on Twitter can be leveraged to help support such analysis by providing alternatives to direct supervision.
",1 Introduction,[0],[0]
"In this paper we focus on political framing, a very nuanced political discourse analysis task, on
a variety of issues frequently discussed on Twitter.",1 Introduction,[0],[0]
"Framing (Entman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue.",1 Introduction,[0],[0]
"For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue.",1 Introduction,[0],[0]
"While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase.",1 Introduction,[0],[0]
"Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles.",1 Introduction,[0],[0]
"Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.).",1 Introduction,[0],[0]
"These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter.
",1 Introduction,[0],[0]
Twitter issue framing is a challenging multilabel prediction task.,1 Introduction,[0],[0]
"Each tweet can be labeled as using one or more frames, out of 17 possibilities, while only providing 140 characters as input to the classifier.",1 Introduction,[0],[0]
The main contribution of this work is to evaluate whether the social and behavioral information available on Twitter is sufficient for constructing a reliable classifier for this task.,1 Introduction,[0],[0]
"We approach this framing prediction task using a weakly supervised collective classification approach which leverages the dependencies between tweet frame predictions based on the interactions between their authors.
",1 Introduction,[0],[0]
These dependencies are modeled by connecting Twitter users who have social connections or behavioral similarities.,1 Introduction,[0],[0]
"Social connections are di-
741
rected dependencies that represent the followers of each user as well as retweeting behavior (i.e., user A retweets user B’s content).",1 Introduction,[0],[0]
"Interestingly, such social connections capture the flow of influence within political parties; however, the number of connections that cross party lines is extremely low.",1 Introduction,[0],[0]
"Instead, we rely on capturing behavioral similarity between users to provide this information.",1 Introduction,[0],[0]
"For example, users whose Twitter activity peaks at similar times tend to discuss issues in similar ways, providing indicators of their frame usage for those issues.",1 Introduction,[0],[0]
"In addition to using social and behavioral information, our approach also incorporates each politician’s party affiliation and the frequent phrases (e.g., bigrams and trigrams) used by politicians on Twitter.
",1 Introduction,[0],[0]
"These lexical, social, and behavioral features are extracted from tweets via weakly supervised models and then declaratively compiled into a graphical model using Probabilistic Soft Logic (PSL), a recently introduced probabilistic modeling framework.1 As described in Section 4, PSL specifies high level rules over a relational representation of these features.",1 Introduction,[0],[0]
"These rules are then compiled into a graphical model called a hingeloss Markov random field (Bach et al., 2013), which is used to make the frame prediction.",1 Introduction,[0],[0]
"Instead of direct supervision we take a bootstrapping approach by providing a small seed set of keywords adapted from Boydstun et al. (2014), for each frame.
",1 Introduction,[0],[0]
"Our experiments show that modeling social and behavioral connections improves F1 prediction scores in both supervised and unsupervised settings, with double the increase in the latter.",1 Introduction,[0],[0]
We apply our unsupervised model to our entire dataset of tweets to analyze framing patterns over time by both party and individual politicians.,1 Introduction,[0],[0]
"Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party.",1 Introduction,[0],[0]
"Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004).",2 Related Work,[0],[0]
"Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card
1http://psl.cs.umd.edu
et al., 2015; Baumer et al., 2015).",2 Related Work,[0],[0]
"Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter.
",2 Related Work,[0],[0]
In recent years there has been growing interest in analyzing political discourse.,2 Related Work,[0],[0]
"Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009).",2 Related Work,[0],[0]
"Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter.",2 Related Work,[0],[0]
"Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016).",2 Related Work,[0],[0]
"Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue.",2 Related Work,[0],[0]
"Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012).
",2 Related Work,[0],[0]
"Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014).",2 Related Work,[0],[0]
"Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach.",2 Related Work,[0],[0]
"Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013).
",2 Related Work,[0],[0]
"Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011).",2 Related Work,[0],[0]
"Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls
2http://alt.qcri.org/semeval2016/ task6/
based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012).
",2 Related Work,[0],[0]
"Several works from political and social science research have studied the role of Twitter and framing in shaping public opinion of certain events, e.g. the Vancouver riots (Burch et al., 2015) and the Egyptian protests (Harlow and Johnson, 2011; Meraz and Papacharissi, 2013).",2 Related Work,[0],[0]
"Others have covered framing and sentiment analysis of opponents (Groshek and Al-Rawi, 2013) and network agenda modeling (Vargo et al., 2014) in the 2012 U.S. presidential election.",2 Related Work,[0],[0]
Jang and Hart (2015) studied frames used by the general population specific to global warming.,2 Related Work,[0],[0]
"In contrast to these works, we predict the issue-independent general frames of tweets, by U.S. politicians, which discuss six different policy issues.",2 Related Work,[0],[0]
"Data Collection and Preprocessing: We collected 184,914 of the most recent tweets of members of the U.S. Congress (both the House of Representatives and Senate).",3 Data Collection and Annotation,[0],[0]
"Using an average of ten keywords per issue, we filtered out tweets not related to the following six issues of interest: (1) limiting or gaining access to abortion, (2) debates concerning the Affordable Care Act (i.e., ACA or Obamacare), (3) the issue of gun rights versus gun control, (4) effects of immigration policies, (5) acts of terrorism, and (6) issues concerning the LGBTQ community.",3 Data Collection and Annotation,[0],[0]
"Forty politicians (10 Republicans and 10 Democrats, from both the House and Senate), were chosen randomly for annotation.",3 Data Collection and Annotation,[0],[0]
"Table 1 presents the statistics of our congressional tweets dataset, which is available for the community.3 Appendix A contains more details of our dataset and preprocessing steps.
",3 Data Collection and Annotation,[0],[0]
Data Annotation: Two graduate students were trained in the use of the Policy Frames Codebook developed by Boydstun et al. (2014) for annotating each tweet with a frame.,3 Data Collection and Annotation,[0],[0]
The general aspects of each frame are shown in Table 2.,3 Data Collection and Annotation,[0],[0]
Frames are designed to generalize across issues and overlap of multiple frames is possible.,3 Data Collection and Annotation,[0],[0]
"Additionally, the Codebook is typically applied to newspaper ar-
3The dataset and PSL scripts are available at: http://purduenlp.cs.purdue.edu/projects/ twitterframing.
",3 Data Collection and Annotation,[0],[0]
ticles where discussion of policy can encompass other frames in the text.,3 Data Collection and Annotation,[0],[0]
"Consequently, annotators using the Codebook are advised to be careful when assigning Frame 13 to a text.
",3 Data Collection and Annotation,[0],[0]
"Based on this guidance and the difficulty of labeling tweets (as discussed in Card et al. (2015)), annotators were instructed to use the following procedure: (1) attempt to assign a primary frame to the tweet if possible, (2) if not possible, assign two frames to the tweet where the first frame is chosen as the more accurate of the two frames, (3) when assigning frames 12 through 17, double check that the tweet cannot be assigned to any other frames.",3 Data Collection and Annotation,[0],[0]
Annotators spent one month labeling the randomly chosen tweets.,3 Data Collection and Annotation,[0],[0]
"For all tweets with more than one frame, annotators met to come to a consensus on whether the tweet should have one frame or both.",3 Data Collection and Annotation,[0],[0]
"The labeled dataset has an inter-annotator agreement, calculated using Cohen’s Kappa statistic, of 73.4%.
",3 Data Collection and Annotation,[0],[0]
Extensions of the Codebook for Twitter Use: The first 14 frames outlined in Table 2 are directly applicable to the tweets of U.S. politicians.,3 Data Collection and Annotation,[0],[0]
"In our labeled set, Frame 15 (Other) was never used.",3 Data Collection and Annotation,[0],[0]
"Therefore, we drop its analysis from this paper.",3 Data Collection and Annotation,[0],[0]
"From our observations, we propose the addition of the 3 frames at the bottom of Table 2 for Twitter analysis: Factual, (Self) Promotion, and Personal Sympathy and Support.",3 Data Collection and Annotation,[0],[0]
"Tweets that present a fact, with no detectable political spin or twists, are labeled as having the Factual frame (15).",3 Data Collection and Annotation,[0],[0]
"Tweets that discuss a politician’s appearances, speeches, statements, or refer to political friends are considered to have the (Self) Promotion frame.",3 Data Collection and Annotation,[0],[0]
"Finally, tweets where a politician offers their “thoughts and prayers”, condolences, or stands in support of others, are considered to have the Personal frame.
",3 Data Collection and Annotation,[0],[0]
"We find that for many tweets, one frame is not enough.",3 Data Collection and Annotation,[0],[0]
"This is caused by the compound nature of many tweets, e.g., some tweets are two separate sentences, with each sentence having a different frame or tweets begin with one frame and end with another.",3 Data Collection and Annotation,[0],[0]
"A final problem, that may also be relevant to longer text articles, is that of subframes within a larger frame.",3 Data Collection and Annotation,[0],[0]
"For example, the tweet “We must bolster the security of our borders and craft an immigration policy that grows our economy.”",3 Data Collection and Annotation,[0],[0]
has two frames: Security & Defense and Economic.,3 Data Collection and Annotation,[0],[0]
"However, both frames could fall under Frame 13 (Policy), if this tweet as a whole was a rebuttal point about an immigration policy.",3 Data Collection and Annotation,[0],[0]
"The lack of
available context for short tweets can make it difficult to determine if a tweet should have one primary frame or is more accurately represented by multiple frames.",3 Data Collection and Annotation,[0],[0]
"Due to the dynamic nature of political discourse on Twitter, our approach is designed to require as little supervision as possible.",4 Global Models of Twitter Language and Activity,[0],[0]
We implement 6 weakly supervised models which are datadependent and used to extract and format information from tweets into input for PSL predicates.,4 Global Models of Twitter Language and Activity,[0],[0]
These predicates are then combined into the probabilistic rules of each model as shown in Table 3.,4 Global Models of Twitter Language and Activity,[0],[0]
"The only sources of supervision these models require includes: unigrams related to the issues, unigrams adapted from the Boydstun et al. (2014) Codebook for frames, and political party of the author of the tweets.",4 Global Models of Twitter Language and Activity,[0],[0]
"PSL is a declarative modeling language which can be used to specify weighted, first-order logic rules.",4.1 Global Modeling Using PSL,[0],[0]
"These rules are compiled into a hinge-loss Markov random field which defines a probability distribution over possible continuous value assignments to the random variables of the model (Bach et al.,
2015).4 This probability density function is represented as:
P (Y | X) = 1 Z exp
MX
r=1
r r(Y , X)
!
where Z is a normalization constant, is the weight vector, and
r(Y, X) =",4.1 Global Modeling Using PSL,[0],[0]
"(max{lr(Y, X), 0})⇢r
is the hinge-loss potential specified by a linear function lr.",4.1 Global Modeling Using PSL,[0],[0]
"The exponent ⇢r 2 1, 2 is optional.",4.1 Global Modeling Using PSL,[0],[0]
"Each potential represents the instantiation of a rule, which takes the following form:
1 : P1(x) ^",4.1 Global Modeling Using PSL,[0],[0]
"P2(x, y) !",4.1 Global Modeling Using PSL,[0],[0]
P3(y) 2 : P1(x) ^,4.1 Global Modeling Using PSL,[0],[0]
"P4(x, y) !",4.1 Global Modeling Using PSL,[0],[0]
"¬P3(y)
P1, P2, P3, and P4 are predicates (e.g., political party, issue, frame, and presence of n-grams) and x, y are variables.",4.1 Global Modeling Using PSL,[0],[0]
Each rule has a weight which reflects that rule’s importance and is learned using the Expectation-Maximization algorithm in our unsupervised experiments.,4.1 Global Modeling Using PSL,[0],[0]
"Using concrete constants a, b (e.g., tweets and words) which instantiate the variables x, y, model atoms are mapped
4Unlike other probabilistic logical models, e.g. MLNs, in which the model’s random variables are strictly true or false.
",4.1 Global Modeling Using PSL,[0],[0]
"to continuous [0,1] assignments.",4.1 Global Modeling Using PSL,[0],[0]
"More important rules (i.e., those with larger weights) are given preference by the model.",4.1 Global Modeling Using PSL,[0],[0]
"Unigrams: Using the guidelines provided in the Policy Frames Codebook (Boydstun et al., 2014), we adapted a list of expected unigrams for each frame.",4.2 Language Based Models,[0],[0]
"For example, unigrams that should be related to Frame 12 (Political Factors & Implications) include: filibuster, lobby, Democrats, Republicans.",4.2 Language Based Models,[0],[0]
"We expect that if a tweet and frame contain a matching unigram, then that frame is likely present in that tweet.",4.2 Language Based Models,[0],[0]
"The information that tweet T has expected unigram U of frame F is represented with the PSL predicate: UNIGRAMF (T, U).",4.2 Language Based Models,[0],[0]
"This knowledge is then used as input to PSL Model 1 via the rule: UNIGRAMF (T, U) !",4.2 Language Based Models,[0],[0]
"FRAME(T, F) (shown in line 1 of Table 3).
",4.2 Language Based Models,[0],[0]
"However, not every tweet will have a unigram that matches those in this list.",4.2 Language Based Models,[0],[0]
"Under the intuition that at least one unigram in a tweet should be similar to a unigram in the list, we designed the following MaxSim metric to compute the maximum similarity between a word in a tweet and a word from the list of unigrams.
MAXSIM(T, F) = arg max u2F,w2T SIMILARITY(W,U)
(1) T is a tweet, W is each word in T, and U is each unigram in the list of expected unigrams (per frame).",4.2 Language Based Models,[0],[0]
SIMILARITY is the computed word2vec similarity (using pretrained embeddings) of each word in the tweet with every unigram in the list of unigrams for each frame.,4.2 Language Based Models,[0],[0]
"The frame F of the maximum scoring unigram is input to the PSL predicate: MAXSIMF (T, F), which indicates that tweet T has the highest similarity to frame F.
Bigrams and Trigrams:",4.2 Language Based Models,[0],[0]
"In addition to unigrams, we also explored the effects of political party slogans on frame prediction.",4.2 Language Based Models,[0],[0]
Slogans are common catch phrases or sayings that people typically associate with different U.S. political parties.,4.2 Language Based Models,[0],[0]
"For example, Republicans are known for using the phrase “repeal and replace” when they discuss the ACA.",4.2 Language Based Models,[0],[0]
"Similarly, in the 2016 U.S. presidential election, Secretary Hillary Clinton’s campaign slogan became “Love Trumps Hate”.",4.2 Language Based Models,[0],[0]
"To visualize slogan usage by parties for different issues, we used the entire tweets dataset, including all unlabeled tweets, to extract the top bigrams
and trigrams per party for each issue.",4.2 Language Based Models,[0],[0]
The histograms in Figure 1 show these distributions for the top 100 bigrams and trigrams.,4.2 Language Based Models,[0],[0]
"Based on these results, we use the top 20 bigrams (e.g., women’s healthcare and immigration reform) and trigrams (e.g. prevent gun violence) as input to PSL predicates BIGRAMIP (T, B) and TRIGRAMIP (T, TG).",4.2 Language Based Models,[0],[0]
These rules represent that tweet T has bigram B or trigram TG from the respective issue I phrase lists of either party P.,4.2 Language Based Models,[0],[0]
"In addition to language based features of tweets, we also exploit the behavioral and social features of Twitter including similarities between temporal activity and network relationships.
",4.3 Twitter Behavior Based Models,[0],[0]
Temporal Similarity: We construct a temporal histogram for each politician which captures their Twitter activity over time.,4.3 Twitter Behavior Based Models,[0],[0]
When an event happens politicians are most likely to tweet about that event within hours of its occurrence.,4.3 Twitter Behavior Based Models,[0],[0]
"Similarly, most politicians tweet about the event most frequently the day of the event and this frequency decreases over time.",4.3 Twitter Behavior Based Models,[0],[0]
"From these temporal histograms, we observed that the frames used the day of an event were similar and gradually changed over time.",4.3 Twitter Behavior Based Models,[0],[0]
"For example, once the public is notified of a shooting, politicians respond with Frame 17 to offer sympathy to the victims and their families.",4.3 Twitter Behavior Based Models,[0],[0]
"Over the next days or weeks, both parties slowly transition to using additional frames, e.g. Democrats use Frame 7 to argue for gun control legislation.",4.3 Twitter Behavior Based Models,[0],[0]
"To capture this behavior we use the PSL predicate SAMETIME(T1, T2).",4.3 Twitter Behavior Based Models,[0],[0]
"This indicates that tweet T1 occurs around the same time as tweet
T2.5 This information is used in Model 4 via rules such as: SAMETIME(T1, T2) & FRAME(T1, F) !",4.3 Twitter Behavior Based Models,[0],[0]
"FRAME(T2, F), as shown in line 4 of Table 3.
",4.3 Twitter Behavior Based Models,[0],[0]
"Network Similarity: Finally, we expect that politicians who share ideologies, and thus are likely to frame issues similarly, will retweet and/or follow each other on Twitter.",4.3 Twitter Behavior Based Models,[0],[0]
"Due to the compound nature of tweets, retweeting with additional comments can add more frames to the original tweet.",4.3 Twitter Behavior Based Models,[0],[0]
"Additionally, politicians on Twitter are more likely to follow members of their own party or similar non-political entities than those of the opposing party.",4.3 Twitter Behavior Based Models,[0],[0]
"To capture this network-based behavior we use two PSL predicates: RETWEETS(T1, T2) and FOLLOWS(T1, T2).",4.3 Twitter Behavior Based Models,[0],[0]
"These predicates indicate that the content of tweet T1 includes a retweet of tweet T2 and that the author of T1 follows the author of T2 on Twitter, respectively.",4.3 Twitter Behavior Based Models,[0],[0]
The last two lines of Table 3 show examples of how network similarity is incorporated into PSL rules.,4.3 Twitter Behavior Based Models,[0],[0]
"Evaluation Metrics: Since each tweet can have more than one frame, our prediction task is a multilabel classification task.",5 Experiments,[0],[0]
"The precision of a multilabel model is the ratio of how many predicted labels are correct:
Precision = 1
T
TX
t=1
|Yt \ h(xt)|",5 Experiments,[0],[0]
"|h(xt)|
(2)
",5 Experiments,[0],[0]
"The recall of this model is the ratio of how many of the actual labels were predicted:
Recall = 1
T
TX
t=1
|Yt \ h(xt)|",5 Experiments,[0],[0]
"|Yt|
(3)
5We conducted experiments with different hour and day limits and found that using a time frame of one hour results in the best accuracy while limiting noise.
",5 Experiments,[0],[0]
"In both formulas, T is the number of tweets, Yt is the true label for tweet t, xt is a tweet example, and h(xt) are the predicted labels for that tweet.",5 Experiments,[0],[0]
The F1 score is computed as the harmonic mean of the precision and recall.,5 Experiments,[0],[0]
"Additionally, in Tables 4, 5, and 6 the reported average is the micro-weighted average F1 scores over all frames.
",5 Experiments,[0],[0]
Experimental Settings: We provide an analysis of our PSL models under both supervised and unsupervised settings.,5 Experiments,[0],[0]
"In the PSL supervised experiments, we used five-fold cross validation with randomly chosen splits.
",5 Experiments,[0],[0]
"Previous works typically use an SVM, with bagof-words features, which is not used in a multilabel prediction, i.e., each frame is predicted individually.",5 Experiments,[0],[0]
The results of this approach on our dataset are shown in column 2 of Table 4.,5 Experiments,[0],[0]
"In this scenario, the SVM tends to prefer the majority class, which results in many incorrect labels.",5 Experiments,[0],[0]
Column 3 shows the results of using an SVM with bag-of-words features to perform multilabel classification.,5 Experiments,[0],[0]
This approach decreases the F1 score for a majority of frames.,5 Experiments,[0],[0]
"Both SVMs also result in F1 scores of 0 for some frames, further lowering the overall performance.",5 Experiments,[0],[0]
"Finally, columns 4 and 5 show the results of using our worst and best PSL models, respectively.",5 Experiments,[0],[0]
"PSL Model 1, which uses our adapted unigram features instead of the bag-of-words features for multilabel classification, serves as our baseline to improve upon.",5 Experiments,[0],[0]
"Additionally, Model 6 of the supervised, collective network setting represents the best results we can achieve.
",5 Experiments,[0],[0]
We also explore the results of our PSL models in an unsupervised setting because the highly dynamic nature of political discourse on Twitter makes it unrealistic to expect annotated data to generalize to future discussions.,5 Experiments,[0],[0]
The only source of supervision comes from the initial unigrams lists and party information as described in Section 4.,5 Experiments,[0],[0]
The labeled tweets are used for evaluation only.,5 Experiments,[0],[0]
"As seen in Table 4, we are able to improve
the best unsupervised model to within an F1 score of 7.36 points of the unigram baseline of 66.02, and 19.13 points of the best supervised score of 77.79.
",5 Experiments,[0],[0]
Analysis of Supervised Experiments: Table 5 shows the results of our supervised experiments.,5 Experiments,[0],[0]
"Here we can see that by adding Twitter behavior (beginning with Model 4), our behaviorbased models achieve the best F1 scores across all frames.",5 Experiments,[0],[0]
"Model 4 achieves the highest results on two frames, suggesting retweeting and network follower information do not help improve the prediction score for these frames.",5 Experiments,[0],[0]
"Similarly, Model 5 achieves the highest prediction for 5 of the frames, suggesting network follower information cannot further improve the score for these frames.",5 Experiments,[0],[0]
"Overall, the Twitter behavior based models are able to outperform language based models alone, including the best performing language model (Model 3) which combines unigrams, bigrams, and trigrams together to collectively infer the correct frames.
",5 Experiments,[0],[0]
"Analysis of Unsupervised Experiments: In the unsupervised setting, Model 6, the combination of language and Twitter behavior features achieves the best results on 16 of the 17 issues, as shown in Table 6.",5 Experiments,[0],[0]
There are a few interesting aspects of the unsupervised setting which differ from the supervised setting.,5 Experiments,[0],[0]
"Six of the frame predictions do worse in Model 2, which is double that of the supervised version.",5 Experiments,[0],[0]
"This is likely due to the presence of overlapping bigrams across frames and issues, e.g., “women’s healthcare” could appear in both Frames 4 and 8 and the issues of ACA and abortion.",5 Experiments,[0],[0]
"However, all six are able to improve with the addition of trigrams (Model 3), whereas only 1 of 3 frames improves in the supervised setting.",5 Experiments,[0],[0]
This suggests that bigrams may not be as useful as trigrams in an unsupervised setting.,5 Experiments,[0],[0]
"Finally, in Model 5, which adds retweet behaviors, we notice that 5 of the frames decrease in F1 score and 11
of the frames have the same score as the previous model.",5 Experiments,[0],[0]
These results suggest that retweet behaviors are not as useful as the follower network relationships in an unsupervised setting.,5 Experiments,[0],[0]
"To explore the usefulness of frame identification in political discourse analysis, we apply our best performing model (Model 6) on the unlabeled dataset to determine framing patterns over time, both by party and individual.",6 Qualitative Analysis,[0],[0]
Figure 2 shows the results of our frame analysis for both parties over time for two issues: ACA and terrorism.6 We compiled the predicted frames for tweets from 2014 to 2016 for each party.,6 Qualitative Analysis,[0],[0]
"Figure 3 presents the results of frame prediction for 2015 tweets of aisle-crossing individual politicians for these two issues.
",6 Qualitative Analysis,[0],[0]
"Party Frames: From Figure 2(a) we can see that Democrats mainly use Frames 1, 4, 8, 9, and 15 to discuss ACA, while Figure 2(c) shows that Republicans predominantly use Frames 1, 8, 9, 12, and 13.",6 Qualitative Analysis,[0],[0]
"Though the parties use similar frames, they are used to express different agendas.",6 Qualitative Analysis,[0],[0]
"For example, Democrats use Frame 8 to indicate the positive effect that the ACA has had in granting more Americans health care access.",6 Qualitative Analysis,[0],[0]
"Republicans, however, use Frame 8 (and Frame 13) to indicate their party’s agenda to replace the ACA with access to different options for health care.",6 Qualitative Analysis,[0],[0]
"Additionally, Democrats use the Fairness & Equality Frame (Frame 4) to convey that the ACA gives minority groups a better chance at accessing health care.
6Due to space, we omit the other 4 issues.",6 Qualitative Analysis,[0],[0]
"These 2 were chosen because they are among the most frequently discussed issues in our dataset.
",6 Qualitative Analysis,[0],[0]
They also use Frame 15 to express statistics about enrollment of Americans under the ACA.,6 Qualitative Analysis,[0],[0]
"Finally, Republicans use Frames 12 and 13 to bring attention to their own party’s actions to “repeal and replace” the ACA with different policies.
",6 Qualitative Analysis,[0],[0]
Figures 2(b) and 2(d) show the party-based framing patterns over time for terrorism related tweets.,6 Qualitative Analysis,[0],[0]
"For this issue both parties use similar frames: 3, 7, 10, 14, 16, and 17, but to express different views.",6 Qualitative Analysis,[0],[0]
"For example, Democrats use Frame 3 to indicate a moral responsibility to fight ISIS.",6 Qualitative Analysis,[0],[0]
Republicans use Frame 3 to frame terrorists or their attacks as a result of “radical Islam”.,6 Qualitative Analysis,[0],[0]
An interesting pattern to note is seen in Frames 10 and 14 for both parties.,6 Qualitative Analysis,[0],[0]
"In 2015 there is a large in-
crease in the usage of this frame.",6 Qualitative Analysis,[0],[0]
"This seems to indicate that parties possibly adopt new frames simultaneously or in response to the opposing party, perhaps in an effort to be in control of the way the message is delivered through that frame.
",6 Qualitative Analysis,[0],[0]
"Individual Frames: In addition to entire party analysis, we were interested in seeing if frames could shed light on the behavior of aisle-crossing politicians.",6 Qualitative Analysis,[0],[0]
"These are politicians who do not vote the same as the majority vote of their party (i.e., they vote the same as the opposing party).",6 Qualitative Analysis,[0],[0]
"Identifying such politicians can be useful in governments which are heavily split by party, i.e., governments such as the recent U.S. Congress (2015 to 2017), where politicians tend to vote the same
as the rest of their party members.",6 Qualitative Analysis,[0],[0]
"For this analysis, we collected five 2015 votes from the House of Representatives on both issues and compiled a list of the politicians who voted opposite to their party.",6 Qualitative Analysis,[0],[0]
The most important descriptor we noticed was that all aisle-crossing politicians tweet less frequently on the issue than their fellow party members.,6 Qualitative Analysis,[0],[0]
This is true for both parties.,6 Qualitative Analysis,[0],[0]
"This behavior could indicate lack of desire to draw attention to one’s stance on the particular issue.
",6 Qualitative Analysis,[0],[0]
Figure 3(a) shows the framing patterns of aislecrossing Republicans on ACA votes from 2015.,6 Qualitative Analysis,[0],[0]
"Recall from Figure 2 that Democrats mostly use Frames 1, 4, 8, 9, and 15, while Republicans mainly use Frames 1, 8, and 9.",6 Qualitative Analysis,[0],[0]
"In this example, these Republicans are considered aislecrossing votes because they have voted the same as Democrats on this issue.",6 Qualitative Analysis,[0],[0]
"The most interesting pattern to note here is that these Republicans use the same framing patterns as the Republicans (Frames 1, 8, and 9), but they also use the frames that are unique to Democrats: Frames 4 and 15.",6 Qualitative Analysis,[0],[0]
These latter two frames appear significantly less in the Republican tweets of our entire dataset as well.,6 Qualitative Analysis,[0],[0]
"These results suggest that to predict aisle-crossing
Republicans it would be useful to check for usage of typically Democrat-associated frames, especially if those frames are infrequently used by Republicans.
",6 Qualitative Analysis,[0],[0]
Figure 3(b) shows the predicted frames for aisle-crossing Democrats on terrorism-related votes.,6 Qualitative Analysis,[0],[0]
"We see here that there are very few tweets from these Democrats on this issue and that overall they use the same framing patterns as seen previously: Frames 3, 7, 10, 14, 16, and 17.",6 Qualitative Analysis,[0],[0]
"However, given the small scale of these tweets, we can also consider Frames 12 and 13 to show peaks for this example.",6 Qualitative Analysis,[0],[0]
This suggests that for aisle-crossing Democrats the use of additional frames not often used by their party for discussing an issue might indicate potentially different voting behaviors.,6 Qualitative Analysis,[0],[0]
In this paper we present the task of collective classification of Twitter data for framing prediction.,7 Conclusion,[0],[0]
"We show that by incorporating Twitter behaviors such as similar activity times and similar networks, we can increase F1 score prediction.",7 Conclusion,[0],[0]
"We provide an analysis of our approach in both supervised and unsupervised settings, as well as a real world analysis of framing patterns over time.",7 Conclusion,[0],[0]
"Finally, our global PSL models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their thoughtful comments and suggestions.,Acknowledgments,[0],[0]
"In this section we provide additional information about our congressional tweets dataset, as well as the lists of keywords and phrases used to filter tweets by issue and the unigrams used to extract information used for the Unigram and MaxSim PSL predicates.",A Supplementary Material,[0],[0]
"It is important to note that during preprocessing capitalization, stop words, URLs, and punctuation have been removed from tweets in our dataset.",A Supplementary Material,[0],[0]
"Additional word lists along with our PSL scripts and dataset are available at: http://purduenlp.cs.purdue.edu/ projects/twitterframing.
",A Supplementary Material,[0],[0]
Dataset Statistics:,A Supplementary Material,[0],[0]
Figure 4 shows the coverage of the labeled frames by party.,A Supplementary Material,[0],[0]
"From this, general patterns can be observed.",A Supplementary Material,[0],[0]
"For example, Republicans use Frames 12 and 17 more frequently than Democrats, while Democrats tend to use Frames 4, 9, 10, and 11.",A Supplementary Material,[0],[0]
"Table 7 shows the count of each type of frame that appears in each issue in our labeled dataset.
",A Supplementary Material,[0],[0]
Word Lists: Table 8 lists the keywords or phrases used to filter the entire dataset to only tweets related to the six issues studied in this paper.,A Supplementary Material,[0],[0]
"Table 9 lists the unigrams that were designed based on the descriptions for Frames 1 through 14
provided in the Policy Frames Codebook (Boydstun et al., 2014).",A Supplementary Material,[0],[0]
These unigrams provide the initial supervision for our models as described in Section 4.,A Supplementary Material,[0],[0]
Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues.,abstractText,[0],[0]
"Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches.",abstractText,[0],[0]
"We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter.",abstractText,[0],[0]
"Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.",abstractText,[0],[0]
Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345–350 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
345",text,[0],[0]
"Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (Baldwin and Kim, 2010).",1 Introduction,[0],[0]
"Verb–noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (Fazly et al., 2009).",1 Introduction,[0],[0]
"Many VNCs are ambiguous between MWEs and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage (i.e., an MWE), while 2 is a literal combination.1
1.",1 Introduction,[0],[0]
"Hereford United were seeing stars at Gillingham after letting in 2 early goals
2.",1 Introduction,[0],[0]
"Look into the night sky to see the stars 1These examples, and idiomaticity judgements, are taken
from the VNC-Tokens dataset (Cook et al., 2008).
",1 Introduction,[0],[0]
"MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010), and must be able to make such distinctions.",1 Introduction,[0],[0]
"This is particularly important for applications such as machine translation (Sag et al., 2002), where the appropriate meaning of word combinations in context must be preserved for accurate translation.
",1 Introduction,[0],[0]
"In this paper, following prior work (e.g., Salton et al., 2016), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal.",1 Introduction,[0],[0]
"We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013), word embeddings tailored to representing sentences (Kenter et al., 2016), and skip-thoughts sentence embeddings (Kiros et al., 2015).",1 Introduction,[0],[0]
We then train a support vector machine (SVM) on these representations to classify unseen VNC instances.,1 Introduction,[0],[0]
"Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by Salton et al. (2016).
VNCs exhibit lexico-syntactic fixedness.",1 Introduction,[0],[0]
"For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars.",1 Introduction,[0],[0]
"Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation.
",1 Introduction,[0],[0]
In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs — automatically acquired from corpora using the method of Fazly et al. (2009) — into our various embedding-based approaches.,1 Introduction,[0],[0]
"Our experimental results show that this leads to substantial improve-
ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations.",1 Introduction,[0],[0]
"Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017).
",2 Related work,[0],[0]
"Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016).
",2 Related work,[0],[0]
"In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013) skip-gram model.",2 Related work,[0],[0]
"During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.",2 Related work,[0],[0]
"Salton et al. then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier.",2 Related work,[0],[0]
"We treat this skip-thoughts based approach as a strong baseline to compare against.
",2 Related work,[0],[0]
"Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural).",2 Related work,[0],[0]
"They then determine the canonical form, C(v, n), for a given VNC as follows:2
C(v, n)",2 Related work,[0],[0]
=,2 Related work,[0],[0]
"{ptk ∈ P |z(v, n, ptk) >",2 Related work,[0],[0]
"Tz} (1) where P is the set of patterns, Tz is a predetermined threshold, which is set to 1, and z(v, n, ptk) is calculated as follows:
z(v, n, ptk) = f(v, n,",2 Related work,[0],[0]
"ptk)− f
s (2)
2In a small number of cases a VNC is found to have a small number of canonical forms, as opposed to just one.
",2 Related work,[0],[0]
"where f(·) is the frequency of a VNC occurring in a given pattern in a corpus,3 and f and s are the mean and standard deviations for all patterns for the given VNC, respectively.
",2 Related work,[0],[0]
"Fazly et al. (2009) showed that idiomatic usages of a VNC tend to occur in that expression’s canonical form, while literal usages do not.",2 Related work,[0],[0]
"This approach provides a strong, linguistically-informed, unsupervised baseline, referred to as CForm, for predicting whether VNC instances are idiomatic or literal.",2 Related work,[0],[0]
"In this paper we incorporate knowledge of canonical forms into embedding-based approaches to VNC token classification, and show that this linguistic knowledge can be leveraged to improve such approaches.",2 Related work,[0],[0]
We describe the models used to represent VNC token instances below.,3 Models,[0],[0]
"For each model, a linear SVM classifier is trained on these representations.",3 Models,[0],[0]
"We trained word2vec’s skip-gram model (Mikolov et al., 2013) on a snapshot of Wikipedia from September 2015, which consists of approximately 2.6 billion tokens.",3.1 Word2vec,[0],[0]
We used a window size of ±8 and 300 dimensions.,3.1 Word2vec,[0],[0]
"We ignore all words that occur less than fifteen times in the training corpus, and did not set a maximum vocabulary size.",3.1 Word2vec,[0],[0]
We perform negative sampling and set the number of training epochs to five.,3.1 Word2vec,[0],[0]
"We used batch processing with approximately 10k words in each batch.
",3.1 Word2vec,[0],[0]
"To embed a given a sentence containing a VNC token instance, we average the word embeddings for each word in the sentence, including stopwords.4 Prior to averaging, we normalize each embedding to have unit length.",3.1 Word2vec,[0],[0]
"The Siamese CBOW model (Kenter et al., 2016) learns word embeddings that are better able to represent a sentence through averaging than conventional word embeddings such as skip-gram or CBOW.",3.2 Siamese CBOW,[0],[0]
"We use a Siamese CBOW model that was pretrained on a snapshot of Wikipedia from November 2012 using randomly initialized word
3Fazly et al. (2009) used the British National Corpus (Burnard, 2000).
",3.2 Siamese CBOW,[0],[0]
"4Preliminary experiments showed that models performed better when stopword removal was not applied.
embeddings.5 Similarly to the word2vec model, to embed a given sentence containing a VNC instance, we average the word embeddings for each word in the sentence.",3.2 Siamese CBOW,[0],[0]
"We use a publicly-available skip-thoughts model, that was pre-trained on a corpus of books.6 We represent a given sentence containing a VNC instance using the skip-thoughts encoder.",3.3 Skip-thoughts,[0],[0]
"Note that this approach is our re-implementation of the skipthoughts based method of Salton et al. (2016), and we use it as a strong baseline for comparison.",3.3 Skip-thoughts,[0],[0]
"In this section, we discuss the dataset used in our experiments, and the evaluation of our models.",4 Data and evaluation,[0],[0]
"We use the VNC-Tokens dataset (Cook et al., 2008) — the same dataset used by Fazly et al. (2009) and Salton et al. (2016) — to train and evaluate our models.",4.1 Dataset,[0],[0]
"This dataset consists of sentences containing VNC usages drawn from the British National Corpus (Burnard, 2000),7 along with a label indicating whether the VNC is an idiomatic or literal usage (or whether this cannot be determined, in which case it is labelled “unknown”).
",4.1 Dataset,[0],[0]
VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.,4.1 Dataset,[0],[0]
"Following Salton et al. (2016), we use DEV and TEST, and ignore all token instances annotated as “unknown”.
",4.1 Dataset,[0],[0]
Fazly et al. (2009) and Salton et al. (2016) structured their experiments differently.,4.1 Dataset,[0],[0]
Fazly et al. report results over DEV and TEST separately.,4.1 Dataset,[0],[0]
In this setup TEST consists of expressions that were not seen during model development (done on DEV).,4.1 Dataset,[0],[0]
"Salton et al., on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.
",4.1 Dataset,[0],[0]
We borrowed ideas from both of these approaches in structuring our experiments.,4.1 Dataset,[0],[0]
"We retain
5https://bitbucket.org/TomKenter/ siamese-cbow
6https://github.com/ryankiros/ skip-thoughts
7http://www.natcorp.ox.ac.uk
the type-level division of Fazly et al. (2009) into DEV and TEST.",4.1 Dataset,[0],[0]
"We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as Salton et al. (2016).",4.1 Dataset,[0],[0]
"This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST.",4.1 Dataset,[0],[0]
The proportion of idiomatic usages in the testing portions of both DEV and TEST is 63%.,4.2 Evaluation,[0],[0]
We therefore use accuracy to evaluate our models following Fazly et al. (2009) because the classes are roughly balanced.,4.2 Evaluation,[0],[0]
"We randomly divide both DEV and TEST into training and testing portions ten times, following Salton et al. (2016).",4.2 Evaluation,[0],[0]
"For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.",4.2 Evaluation,[0],[0]
We then report the average accuracy over the ten runs.,4.2 Evaluation,[0],[0]
"In this section we first consider the effect of tuning the cost parameter of the SVM for each model on DEV, and then report results on DEV and TEST using the tuned models.",5 Experimental results,[0],[0]
"We tune the SVM for each model on DEV by carrying out a linear search for the penalty cost from 0.01–100, increasing by a factor of ten each time.",5.1 Parameter tuning,[0],[0]
Results for this parameter tuning are shown in Table 1.,5.1 Parameter tuning,[0],[0]
These results highlight the importance of choosing an appropriate setting for the penalty cost.,5.1 Parameter tuning,[0],[0]
"For example, the accuracy of the word2vec model ranges from 0.619–0.830 depending on the cost setting.",5.1 Parameter tuning,[0],[0]
"In subsequent experiments, for each
model, we use the penalty cost that achieves the highest accuracy in Table 1.",5.1 Parameter tuning,[0],[0]
"In Table 2 we report results on DEV and TEST for each model, as well as the unsupervised CForm model of Fazly et al. (2009), which simply labels a VNC as idiomatic if it occurs in its canonical form, and as literal otherwise.",5.2 DEV and TEST results,[0],[0]
We further consider each model (other than CForm) in two setups.,5.2 DEV and TEST results,[0],[0]
−CF corresponds to the models as described in Section 3.,5.2 DEV and TEST results,[0],[0]
"+CF further incorporates lexico-syntactic knowledge of canonical forms into each model by concatenating the embedding representing each VNC token instance with a one-dimensional vector which is one if the VNC occurs in its canonical form, and zero otherwise.
",5.2 DEV and TEST results,[0],[0]
We first consider results for the −CF setup.,5.2 DEV and TEST results,[0],[0]
"On both DEV and TEST, the accuracy achieved by each supervised model is higher than that of the unsupervised CForm approach, except for Siamese CBOW on TEST.",5.2 DEV and TEST results,[0],[0]
"The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively.",5.2 DEV and TEST results,[0],[0]
"The difference between the word2vec model and the next-best model, skip-thoughts, is significant using a bootstrap test (Berg-Kirkpatrick et al., 2012) with 10k repetitions for DEV (p = 0.006), but not for TEST (p = 0.051).",5.2 DEV and TEST results,[0],[0]
"Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by Salton et al. (2016).8
8The word2vec and skip-thoughts models were trained on different corpora, which could contribute to the differences in results for these models.",5.2 DEV and TEST results,[0],[0]
"We therefore carried out an additional experiment in which we trained word2vec on BookCorpus, the corpus on which skip-thoughts was trained.",5.2 DEV and TEST results,[0],[0]
"This new word2vec model achieved accuracies of 0.825 and 0.809, on DEV and TEST, respectively, which are also higher accu-
Turning to the +CF setup, we observe that, for both DEV and TEST, each model achieves higher accuracy than in the −CF setup.9",5.2 DEV and TEST results,[0],[0]
All of these differences are significant using a bootstrap test (p < 0.002 in each case).,5.2 DEV and TEST results,[0],[0]
"In addition, each method outperforms the unsupervised CForm approach on both DEV and TEST.",5.2 DEV and TEST results,[0],[0]
"These findings demonstrate that the linguistically-motivated, lexico-syntactic knowledge encoded by the canonical form feature is complementary to the information from a wide range of types of distributed representations.",5.2 DEV and TEST results,[0],[0]
"In the +CF setup, the word2vec model again achieves the highest accuracy on both DEV and TEST of 0.854 and 0.852, respectively.10 The difference between the word2vec model and the next-best model, again skip-thoughts, is significant for both DEV and TEST using a bootstrap test (p < 0.05 in each case).
",5.2 DEV and TEST results,[0],[0]
"To better understand the impact of the canonical form feature when combined with the word2vec model, we compute the average precision, recall, and F1 score for each MWE for both the positive (idiomatic) and negative (literal) classes, for each run on TEST.11 For a given run, we then compute the average precision, recall, and F1 score across all MWEs, and then the average over all ten runs.",5.2 DEV and TEST results,[0],[0]
"We do this using CForm, and the word2vec model with and without the canonical form feature.",5.2 DEV and TEST results,[0],[0]
Results are shown in Table 3.,5.2 DEV and TEST results,[0],[0]
"In line with the findings of Fazly et al. (2009), CForm achieves higher precision and recall on idiomatic usages than literal ones.",5.2 DEV and TEST results,[0],[0]
"In particular, the relatively low recall for the literal class indicates that many literal usages occur in a canonical form.",5.2 DEV and TEST results,[0],[0]
"Comparing the word2vec model with and without the canonical form feature, we see that, when this feature is used, there is a relatively larger increase in precision and recall (and F1 score) for the literal class, than for the idiomatic class.",5.2 DEV and TEST results,[0],[0]
"This indicates that, although the
racies than those obtained by the skip-thoughts model.",5.2 DEV and TEST results,[0],[0]
"9In order to determine that this improvement is due to the information about canonical forms carried by the additional feature in the +CF setup, and not due to the increase in number of dimensions, we performed additional experiments in which we concatenated the embedding representations with a random binary feature, and with a randomly chosen value between 0 and 1.",5.2 DEV and TEST results,[0],[0]
"For each model, neither of these approaches outperformed that model using the +CF setup.
",5.2 DEV and TEST results,[0],[0]
"10In the +CF setup, the word2vec model using embeddings that were trained on the same corpus as skip-thoughts achieved accuracies of 0.846 and 0.851, on DEV and TEST, respectively.",5.2 DEV and TEST results,[0],[0]
"These are again higher accuracies than the corresponding setup for the skip-thoughts model.
",5.2 DEV and TEST results,[0],[0]
11We carried out the same analysis on DEV.,5.2 DEV and TEST results,[0],[0]
"The findings were similar.
canonical form feature itself performs relatively poorly on literal usages, it provides information that enables the word2vec model to better identify literal usages.",5.2 DEV and TEST results,[0],[0]
"Determining whether a usage of a VNC is idiomatic or literal is important for applications such as machine translation, where it is vital to preserve the meanings of word combinations.",6 Conclusions,[0],[0]
In this paper we proposed two approaches to the task of classifying VNC token instances as idiomatic or literal based on word2vec embeddings and Siamese CBOW.,6 Conclusions,[0],[0]
"We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (Salton et al., 2016).",6 Conclusions,[0],[0]
"Our experimental results show that a comparatively simple approach based on averaging word embeddings performs at least as well as, or better than, the approach based on skip-thoughts.",6 Conclusions,[0],[0]
"We further proposed methods to combine linguistic knowledge of the lexico-syntactic fixedness of VNCs — socalled “canonical forms”, which can be automatically acquired from corpora via statistical methods — with the embedding based approaches.",6 Conclusions,[0],[0]
"Our findings indicate that this rich linguistic knowledge is complementary to that available in distributed representations.
",6 Conclusions,[0],[0]
"Alternative approaches to embedding sentences containing VNC instances could also be considered, for example, FastSent (Hill et al., 2016).",6 Conclusions,[0],[0]
"However, all of the models we used represent the context of a VNC by the sentence in which it occurs.",6 Conclusions,[0],[0]
"In future work we therefore also intend to consider approaches such as context2vec (Melamud et al., 2016) which explicitly encode the context in which a token occurs.",6 Conclusions,[0],[0]
"Finally, one known challenge of VNC token classification is to develop models that are able to generalize to VNC types that were not seen during training (Gharbieh et al., 2016).",6 Conclusions,[0],[0]
"In future work we plan to explore
this experimental setup.",6 Conclusions,[0],[0]
"Verb–noun combinations (VNCs) — e.g., blow the whistle, hit the roof, and see stars — are a common type of English idiom that are ambiguous with literal usages.",abstractText,[0],[0]
"In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations.",abstractText,[0],[0]
"Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts.",abstractText,[0],[0]
Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness.,abstractText,[0],[0]
"We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations.",abstractText,[0],[0]
Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations,title,[0],[0]
"Relational learning from network data, particularly with probabilistic methods, has gained a wide range of applications such as social network analysis (Xiang et al., 2010), recommender systems (Gopalan et al., 2014b), knowledge graph completion (Hu et al., 2016b), and bioinformatics (Huopaniemi et al., 2010).",1. Introduction,[0],[0]
"Generally speaking, the goal of relational learning is to discover and analyse latent clusters of entities (i.e., community detection), and predict missing links (i.e., link prediction).
",1. Introduction,[0],[0]
The standard approach for modelling relational data is latent factor analysis via matrix factorisation and its variations.,1. Introduction,[0],[0]
"Among the existing approaches, Non-negative Matrix Factorisation (NMF) and the Stochastic Block Model (SBM) are prominent foundational methods.",1. Introduction,[0],[0]
"NMF is usually used to model relationships between two sets of entities such as users and movies in collaborative filtering (Mnih & Salakhutdinov, 2008).",1. Introduction,[0],[0]
"While developed independently, SBM (Wang & Wong, 1987; Nowicki & Snijders, 2001) can be viewed as an extension of NMF that introduces
1Faculty of Information Technology, Monash University, Australia.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"He Zhao <he.zhao@monash.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
a block matrix to capture the interactions between latent factors.,1. Introduction,[0],[0]
"There have been many Bayesian extensions of these two methods, relaxing the assumptions and/or introducing extra components, such as the Infinite Relational Model (IRM) (Kemp et al., 2006), the mixture membership stochastic block model (MMSB) (Airoldi et al., 2008), and the non-parametric latent feature models (NLFM) (Miller et al., 2009).",1. Introduction,[0],[0]
"Poisson Factorisation (PF) (Dunson & Herring, 2005; Zhou et al., 2012), is a popular version of NMF which models count data with convenient statistical properties (Gopalan et al., 2014b; 2015).",1. Introduction,[0],[0]
"Combining the ideas of PF and SBM, the infinite Edge Partition Model (EPM) (Zhou, 2015) and its extensions (Hu et al., 2016b) have proven successful for relational networks.
",1. Introduction,[0],[0]
"When a network has less data, relational learning becomes more difficult.",1. Introduction,[0],[0]
"One extreme case is the cold-start problem (Lin et al., 2013; Sedhain et al., 2014; Zhang & Wang, 2015), where a node has no observed links, making suggestion of links for that node even more challenging.",1. Introduction,[0],[0]
"In such cases, it is natural to appeal to side information such as node attributes or features.",1. Introduction,[0],[0]
"For instance, papers in citation networks are often associated with categories and authors, and users in Facebook or Twitter are often asked to provide information such as age, gender and interests.",1. Introduction,[0],[0]
"It is reasonable to assume that nodes having similar attributes are more likely to relate to each other (i.e., homophily, Nickel et al., 2016).",1. Introduction,[0],[0]
"Thus, node attributes serve as important complementary information to relational data.
",1. Introduction,[0],[0]
There are few Bayesian probabilistic relational models that are able to leverage side information.,1. Introduction,[0],[0]
"For example, NLFM uses a linear regression model to transform the features of each node into a single number, which contributes to link probabilities.",1. Introduction,[0],[0]
"However, side information in NLFM cannot directly influence the latent factors, which gives little support for community detection.",1. Introduction,[0],[0]
"As an extension of MMSB, the Non-parametric Meta-data Dependent Relational (NMDR) model (Kim et al., 2012) incorporates attributes into the mixed-membership distribution of each node with the logistic-normal transform, which results in non-conjugacy for inference.",1. Introduction,[0],[0]
"Fan et al. (2016) further developed this idea in the Node information Involved Mixture Membership model (niMM), where side information is integrated in a conjugate way.",1. Introduction,[0],[0]
"Although these models demonstrate improvement using side information, they
scale quadratically in the number of nodes and the incorporation of side information is often complicated.
",1. Introduction,[0],[0]
"Several recent methods (Gopalan et al., 2014a; Acharya et al., 2015; Hu et al., 2016a) extend PF with side information using the additivity of the Poisson and gamma distributions/processes.",1. Introduction,[0],[0]
"With improved scalability, the Structural Side Information Poisson Factorisation (SSI-PF) (Hu et al., 2016a) models directed unweighted networks with node labels, such as citation networks with papers labelled with one of several categories.",1. Introduction,[0],[0]
"However, its performance remains untested when a node has multiple attributes.",1. Introduction,[0],[0]
"Moreover, undirected networks are not handled by SSI-PF.
",1. Introduction,[0],[0]
"In this paper we present the Node Attribute Relational Model (NARM)1, a fully Bayesian approach that models large, sparse, and unweighted relational networks with arbitrary node attributes encoded in binary form.",1. Introduction,[0],[0]
It works with Poisson gamma relational models to incorporate side information.,1. Introduction,[0],[0]
"Specifically, we propose the Symmetric NARM (Sym-NARM) for undirected networks, an extension of EPM (Zhou, 2015) and the Asymmetric NARM (Asym-NARM) for directed networks, an extension of PF (Zhou et al., 2012).",1. Introduction,[0],[0]
"The proposed models have several key properties: (1) Effectively modelling node attributes: the proposed models are able to achieve improved link prediction performance, especially where training data are limited.",1. Introduction,[0],[0]
"(2) Fully Bayesian and conjugate: the inference is done by efficient, closed-form Gibbs sampling which scales linearly in the number of observed links and takes advantage of the sparsity of node attributes.",1. Introduction,[0],[0]
It makes our models scalable for large but sparse relational networks with large sets of node attributes.,1. Introduction,[0],[0]
(3) Flexibility: the proposed models work on directed and undirected relational networks with flat and hierarchical node attributes.,1. Introduction,[0],[0]
"Here we focus on modelling unweighted networks that can be either directed (i.e., the relationship is asymmetric) or undirected.",2. The Node Attribute Relational Model,[0],[0]
"Assume a relational network with N nodes is stored in a binary adjacency matrix Y ∈ {0, 1}N×N where yi,j = 1 indicates the presence of a link between nodes i and j. If the relationship described in the network is symmetric, then yi,j = yj,i, and if asymmetric, possibly yi,j 6= yj,i. Node attributes are encoded in a binary matrix F ∈ {0, 1}N×L, where L is the total number of attributes.",2. The Node Attribute Relational Model,[0],[0]
"Attribute fi,l = 1 indicates attribute l is active with node i and vice versa.",2. The Node Attribute Relational Model,[0],[0]
"Although our models incorporate binary attributes, categorical attributes and real-valued attributes can be converted into binary values with proper transformations (Kim et al., 2012; Fan et al., 2016; Hu et al., 2016a).
",2. The Node Attribute Relational Model,[0],[0]
1Code available at https://github.com/ ethanhezhao/NARM/,2. The Node Attribute Relational Model,[0],[0]
Sym-NARM works with undirected networks.,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
Its generative process is shown in Figure 1.,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Instead of modelling the binary matrix Y directly, it applies the Bernoulli-Poisson link (BPL) function (Zhou, 2015) using an underlying latent count matrix X. One first draws a latent count xi,j from the Poisson distribution and then thresholds it at 1 to generate a binary value yi,j .",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
This is shown in Eqs.,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
(1)- (3).,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Analysed in (Zhou, 2015; Hu et al., 2016b;a), BPL has the appealing property that if yi,j = 0, then xi,j = 0 with probability one.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Thus, only non-zeros in Y need to be sampled, giving huge computational savings for large sparse networks, illustrated in Section 3 and Section 5.4.
",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
The latent matrix X is further factorised into K latent factors with a non-negative bilinear model: X ∼ Poi(ΦΛΦT ) where Φ ∈ RN×K+ and Λ ∈ RK×K+ .,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Φ is referred to as the node factor loading matrix where φi,k models the strength of the connection between node i and latent factor k.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"As in SBM, the correlations of the latent factors are modelled in a symmetric matrix Λ, referred to as the block matrix.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Following (Zhou, 2015), we draw Λ from a hierarchical relational gamma process (implemented with truncation as a vector of gamma variables) , shown in Eqs.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"(8) and (9).
",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"One appealing aspect of our model is the incorporation of node attributes on the prior of φi,k (i.e., gi,k).",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
Shown in Eq.,2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"(5), gi,k is constructed with a log linear combination of fi,l. hl,k is referred to as the kth attribute factor loading of attribute l, which influences gi,k iff attribute l is active with node i (i.e., fi,l = 1).",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"bk acts as an attribute-free bias for each latent factor k. hl,k and bk are gamma distributed with mean 1, hence if attribute l does not contribute to latent factor k or is less useful, hl,k is expected to be near 1 and to have little influence on gi,k.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"The hyper-parameter µ0 controls the variation of hl,k.
The intuition of our model is: if two nodes have more common attributes, their gamma shape parameters will be more similar, with similar node factor loadings, resulting in a larger probability that they relate to each other.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Moreover, instead of incorporating the node attributes directly into the node factor loadings, Sym-NARM uses them as the prior information using Eq.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"(4), which results in a principled way of balancing the side information and the network data.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"In addition, different attributes can contribute differently to the latent factors.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"For example, the gender of an author may be much less important to co-authorship with others than the research fields.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"This is controlled by the attribute factor loading hl,k in our model.",2.1. The Symmetric Node Attribute Relational Model,[0],[0]
"Extending the Beta Gamma Gamma Poisson factorisation (BGGPF) (Zhou et al., 2012), Asym-NARM works on di-
rected relational networks with node attributes incorporated in a similar way to Sym-NARM.",2.2. The Asymmetric Node Attribute Relational Model,[0],[0]
Figure 2 shows its generative process.,2.2. The Asymmetric Node Attribute Relational Model,[0],[0]
"Here the latent count matrix X is factorised as X ∼ Poi(ΦΘ), where Φ ∈ RN×K+ and Θ ∈ RK×N+ are referred to as the factor loading matrix and the factor score matrix respectively.",2.2. The Asymmetric Node Attribute Relational Model,[0],[0]
"Similar to SSI-PF, the node attributes are incorporated on the prior of Φ.",2.2. The Asymmetric Node Attribute Relational Model,[0],[0]
"Relational networks can be associated with hierarchical side information (Hu et al., 2016a).",2.3. Incorporating Hierarchical Node Attributes,[0],[0]
"For example, in a patent citation network, patents can be labelled with the International Patent Classification (IPC) code, which is a hierarchy of patent categories and sub-categories.",2.3. Incorporating Hierarchical Node Attributes,[0],[0]
"Suppose the second level attributes are stored in a binary matrix F′ ∈ {0, 1}L×M whereM is the number of attributes in the second level.",2.3. Incorporating Hierarchical Node Attributes,[0],[0]
"Our models can be used to incorporate hierarchical node attributes via a straightforward extension: re-
place hyper-parameter µ0 in Eq.",2.3. Incorporating Hierarchical Node Attributes,[0],[0]
"(6) with µl,k = ∏M m δ f ′l,m m,k .",2.3. Incorporating Hierarchical Node Attributes,[0],[0]
This extension mirrors what is done for first level attributes.,2.3. Incorporating Hierarchical Node Attributes,[0],[0]
Both Sym-NARM and Asym-NARM enjoy local conjugacy so the inference of all latent variables can be done by closed-form Gibbs sampling.,3. Inference with Gibbs Sampling,[0],[0]
"Moreover, the inference only needs to be conducted on the non-zero entries in Y and F. This section focuses on the sampling of hl,k (bk), the key variable in the proposed incorporation of node attributes.",3. Inference with Gibbs Sampling,[0],[0]
"The sampling of the other latent variables is similar to those in EPM and BGGPF, detailed in (Zhou, 2015;
Zhou et al., 2012).",3. Inference with Gibbs Sampling,[0],[0]
"As the sampling for hl,k is analogous in Sym-NARM and Asym-NARM, our introduction will be based on Asym-NARM alone.
",3. Inference with Gibbs Sampling,[0],[0]
"With the Poisson gamma conjugacy, the likelihood for gi,k with φi,k marginalised out is:
p(gi,k | xi,·,k) ∝",3. Inference with Gibbs Sampling,[0],[0]
"(1− qk)gi,k Γ(gi,k + xi,·,k)
Γ(gi,k) (19) where xi,·,k = ∑
j xi,j,k and xi,j,k is the latent count.",3. Inference with Gibbs Sampling,[0],[0]
The gamma ratio in Eq.,3. Inference with Gibbs Sampling,[0],[0]
"(19), i.e., the Pochhammer symbol for a rising factorial, can be augmented with an auxiliary variable ti,k:
Γ(gi,k+xi,·,k) Γ(gi,k)
",3. Inference with Gibbs Sampling,[0],[0]
"= ∑xi,·,k
ti,k=0 S xi,·,k",3. Inference with Gibbs Sampling,[0],[0]
"ti,k g ti,k",3. Inference with Gibbs Sampling,[0],[0]
"i,k where S x t in-
dicates an unsigned Stirling number of the first kind (Chen et al., 2011; Teh et al., 2012; Zhou & Carin, 2015).
TakingO(xi,·,k), ti,k can be directly sampled by a Chinese Restaurant Process with gi,k as the concentration and xi,·,k",3. Inference with Gibbs Sampling,[0],[0]
"as the number of customers:
ti,k ← ti,k + Bern (
gi,k gi,k + i′
) for i′ = 1 : xi,·,k (20)
where Bern(·) is the Bernoulli distribution.",3. Inference with Gibbs Sampling,[0],[0]
"Alternatively, for large xi,·,k, because the standard deviation of ti,k is O( √
log xi,·,k) (Buntine & Hutter, 2012), one can sample ti,k in a small window around the current value (Du et al., 2010).
",3. Inference with Gibbs Sampling,[0],[0]
With the above augmentation and Eq.,3. Inference with Gibbs Sampling,[0],[0]
"(15), we get: p(G,H | x:,·,:,T,F) ∝",3. Inference with Gibbs Sampling,[0],[0]
(21) N∏ i=1,3. Inference with Gibbs Sampling,[0],[0]
K∏,3. Inference with Gibbs Sampling,[0],[0]
"k=1 S xi,·,k",3. Inference with Gibbs Sampling,[0],[0]
"ti,k e",3. Inference with Gibbs Sampling,[0],[0]
− log ( 1 1−qk ),3. Inference with Gibbs Sampling,[0],[0]
"gi,k · L∏ l=1 K∏",3. Inference with Gibbs Sampling,[0],[0]
"k=1 h ∑N i=1 fi,lti,k l,k
Recall that all the attributes are binary and hl,k influences gi,k only when fi,l = 1.",3. Inference with Gibbs Sampling,[0],[0]
"Extracting all the terms related to
hl,k in Eq.",3. Inference with Gibbs Sampling,[0],[0]
"(21), we get the likelihood of hl,k:
p ( hl,k ∣∣∣∣ gi,khl,k , t:,k, f:,l ) ∝",3. Inference with Gibbs Sampling,[0],[0]
"(22)
e −hl,k log
( 1
1−qk )",3. Inference with Gibbs Sampling,[0],[0]
∑N i=1,3. Inference with Gibbs Sampling,[0],[0]
:,3. Inference with Gibbs Sampling,[0],[0]
"fi,l=1 gi,k hl,k h ∑N i=1 fi,lti,k
l,k
where gi,khl,k is the value of gi,k with hl,k removed when fi,l = 1.",3. Inference with Gibbs Sampling,[0],[0]
The likelihood function above is in a form that is conjugate to the gamma prior.,3. Inference with Gibbs Sampling,[0],[0]
"Therefore, it is straightforward to yield the following sampling strategy for hl,k:
hl,k ∼ Ga(µ′, 1/ν′) (23)
µ′ = µ0 + N∑ i=1",3. Inference with Gibbs Sampling,[0],[0]
":fi,l=1 ti,k (24)
ν′ = 1/µ0",3. Inference with Gibbs Sampling,[0],[0]
"− log (1− qk) N∑
i=1:fi,l=1
gi,k hl,k
(25)
Precomputed with Eq. (15), gi,k can be updated with Eq.",3. Inference with Gibbs Sampling,[0],[0]
"(26), after hl,k is sampled.
",3. Inference with Gibbs Sampling,[0],[0]
"gi,k ← gi,kh
′",3. Inference with Gibbs Sampling,[0],[0]
"l,k
hl,k for i = 1 : N and fi,l = 1 (26)
where h′i,k is the newly sampled value of hi,k.
To compute Eqs.",3. Inference with Gibbs Sampling,[0],[0]
"(24)-(26), we only need to iterate over the nodes that attribute l is active with (i.e., fi,l = 1).",3. Inference with Gibbs Sampling,[0],[0]
"Thus, the sampling for H takes O(D′KL) where D′ is the average number of nodes that an attribute is active with.",3. Inference with Gibbs Sampling,[0],[0]
This demonstrates how the sparsity of node attributes is leveraged.,3. Inference with Gibbs Sampling,[0],[0]
"As the mean of xi,·,k is D/K, sampling the tables T ∈ NN×K takes O(ND) which can be accelerated with the window sampling technique explained above.
",3. Inference with Gibbs Sampling,[0],[0]
We show the computational complexity of our and related models in Table 1.,3. Inference with Gibbs Sampling,[0],[0]
The empirical comparison of running speed is in Section 5.4.,3. Inference with Gibbs Sampling,[0],[0]
"By taking advantage of both network sparsity and node attribute sparsity, our models are more efficient than the competitors, especially on large sparse networks with large sets of attributes.",3. Inference with Gibbs Sampling,[0],[0]
"Compared with the node-attribute models such as NMDR and niMM whose methods result in complicated inference, our Sym-NARM is much more efficient on large sparse networks, illustrated in Table 1.
",4. Related work,[0],[0]
"The most closely related model to our Asym-NARM, also extending the BGGPF algorithm, is SSI-PF.",4. Related work,[0],[0]
But it uses the gamma additivity to construct the prior of node factor loadings with the sum of attribute factor loadings.,4. Related work,[0],[0]
Our model has several advantages over SSI-PF: (1) The derivation of Gibbs sampling of SSI-PF requires that each column of Θ is normalised (Eq. (18)).,4. Related work,[0],[0]
This limits the application of SSI-PF to other models such as EPM which is an unnormalised model.,4. Related work,[0],[0]
"(2) Shown in Table 1, Asym-NARM enjoys more efficient computational complexity.",4. Related work,[0],[0]
"(3) Shown
in Section 5, our model is more effective especially when a node has multiple attributes.
",4. Related work,[0],[0]
"There are also models that extend PF and collective matrix factorisation (Singh & Gordon, 2008) to jointly factorise relational networks and document-word matrices such as (Gopalan et al., 2014a; Zhang & Wang, 2015; Acharya et al., 2015).",4. Related work,[0],[0]
"Our NARM models incorporate general node attributes (not only texts) as the priors of the factor loading matrix in a supervised manner, rather than jointly modelling the side information in an unsupervised manner.
",4. Related work,[0],[0]
"Another related area is supervised topic models such as (Mcauliffe & Blei, 2008; Ramage et al., 2009; Lim & Buntine, 2016).",4. Related work,[0],[0]
"The Dirichlet Multinomial Regression (DMR) model (Mimno & McCallum, 2012) is the most related one to ours.",4. Related work,[0],[0]
It models document attributes on the priors of the topic proportions with the logistic-normal transform.,4. Related work,[0],[0]
"For comparison, we propose DMR-MMSB, extending MMSB with the DMR technique to incorporate side information on the mixed-membership distribution of each node.",4. Related work,[0],[0]
In this section we evaluate Sym-NARM and Asym-NARM with a set of the link prediction tasks on 10 real-world relational datasets with different sizes and various kinds of node attributes.,5. Experiments,[0],[0]
"We compare our models with the stateof-the-art relational models, demonstrating that our models outperform the competitors on those datasets in terms of link prediction performance and per-iteration running time.",5. Experiments,[0],[0]
We report the average area under the curve of both the receiver operating characteristic (AUC-ROC) and precision recall (AUC-PR) for quantitatively analysing the models.,5. Experiments,[0],[0]
"Moreover, we perform qualitative analysis by comparing the link probabilities estimated by the compared models.",5. Experiments,[0],[0]
"For the link prediction task on undirected network data, we compared our Sym-NARM with two models that do
not consider node attributes, EPM (Zhou, 2015), a stateof-the-art relational model, and iMMM (Koutsourelakis & Eliassi-Rad, 2008), a non-parametric version of MMSB,
and two node attribute models, niMM (Fan et al., 2016), a non-parametric relational model which has been demonstrated to outperform NMDR (Kim et al., 2012), and DMRMMSB, our extension to MMSB using the Dirichlet Multinomial Regression (Mimno & McCallum, 2012).",5.1. Link Prediction on Undirected Networks,[0],[0]
SymNAMR was implemented in MATLAB on top of the EPM code and we used the code released by the original authors for EPM and niMM.,5.1. Link Prediction on Undirected Networks,[0],[0]
"iMMM was implemented by Fan et al. (2016) as a variant of niMM.
",5.1. Link Prediction on Undirected Networks,[0],[0]
"The description of the four datasets used is given below:
• Lazega-cowork:",5.1. Link Prediction on Undirected Networks,[0],[0]
"This dataset (Lazega, 2001) contains 378 links of the co-work relationship among 71 attorneys.",5.1. Link Prediction on Undirected Networks,[0],[0]
"Each attorney is associated with attributes such as gender, office location, and age.",5.1. Link Prediction on Undirected Networks,[0],[0]
"After discretisation and binarisation, we derived a 71× 18 binary node attribute matrix with 497 non-zero entries.",5.1. Link Prediction on Undirected Networks,[0],[0]
• NIPS234:,5.1. Link Prediction on Undirected Networks,[0],[0]
"This is a co-author network of the 234 authors with 598 links extracted from NIPS 1-17 conferences (Zhou, 2015).",5.1. Link Prediction on Undirected Networks,[0],[0]
"We merged all the papers written by the same author as a document, and then trained a LDA model with 100 topics.",5.1. Link Prediction on Undirected Networks,[0],[0]
"The 5 most frequent topics were used as the attributes, which gives us a 234 × 100 attribute matrix with 1170 non-zero entries.",5.1. Link Prediction on Undirected Networks,[0],[0]
"• Facebook-ego: The original dataset (McAuley & Leskovec, 2012) was collected from survey participants of Facebook users.",5.1. Link Prediction on Undirected Networks,[0],[0]
"Out of the 10 circles (i.e., friend lists), we used the first circle that contains 347 users with 2519 links.",5.1. Link Prediction on Undirected Networks,[0],[0]
"Each user is associated with 227 binary attributes, encoding side information such as age, gender, and education.",5.1. Link Prediction on Undirected Networks,[0],[0]
We got a 347×227 binary node attribute matrix with 3318 non-zero entries.,5.1. Link Prediction on Undirected Networks,[0],[0]
• NIPS12:,5.1. Link Prediction on Undirected Networks,[0],[0]
NIPS12 was collected from NIPS papers in vols 0-12.,5.1. Link Prediction on Undirected Networks,[0],[0]
It is a median-size co-author network with 2037 authors and 3134 links.,5.1. Link Prediction on Undirected Networks,[0],[0]
"Similar to NIPS234, we used the 5 most frequent topics as the attributes for each author.",5.1. Link Prediction on Undirected Networks,[0],[0]
We got a 2037×100 binary node attribute matrix with 10185 non-zero entries.,5.1. Link Prediction on Undirected Networks,[0],[0]
"For each dataset, we varied the training data from 10% to 90% and used the remaining in testing.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"For each proportion, to generate five random splits, we used the code in the EPM package (Zhou, 2015) which splits a network in terms of its nodes.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
The reported AUC-ROC/PR scores were averaged over the five splits.,5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"We used the default hyper-parameter settings enclosed in the released code for EPM, niMM and iMMM.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"For our Sym-NARM, we set µ0 = 1 and all the other hyper-parameters the same as those in EPM.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
Note that the models in comparison except DMR-MMSB are non-parametric models.,5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"For Sym-NARM and EPM, we set the truncation level large enough for each dataset: Kmax = 50, 100, 256 for Lazega-
cowork, Facebook-ego and NIPS234, NIPS12 respectively.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"For DMR-MMSB, we varied K in {5, 10, 25, 50} and reported the best one.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
"Following (Zhou, 2015), we used 3000 MCMC iterations and computed AUC-ROC/PR with the average probability over the last 1500.",5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
The performance of iMMM and niMM on NIPS12 and DMR-MMSB on Facebook-ego and NIPS12 are not reported as the datasets are too large for them given our computational resources.,5.1.1. EXPERIMENTAL SETTINGS,[0],[0]
The AUC-ROC/PR scores are reported in Figure 3.,5.1.2. RESULTS,[0],[0]
"Overall, our Sym-NARM model performs significantly better than niMM, iMMM, and DMR-MMSB on all the datasets, and EPM on 3 datasets (except Facebook-ego with large training proportions).",5.1.2. RESULTS,[0],[0]
It is interesting that the performance of EPM on Facebook-ego gradually approaches ours when more than 30% training data were used.,5.1.2. RESULTS,[0],[0]
"Note that Facebook-ego is much denser than the others, which means the network information itself could be rich enough for EPM to reconstruct the network and the node attributes contribute less.",5.1.2. RESULTS,[0],[0]
"However in general, when relational data are highly incomplete (with less training data), our model is able to achieve improved link prediction performance.
",5.1.2. RESULTS,[0],[0]
"To illustrate how side information helps, we qualitatively compared our model with EPM and niMM by estimating the link probabilities on NIPS234, shown in Figure 4.",5.1.2. RESULTS,[0],[0]
"With 20% training data, EPM does not give a meaningful reconstruction of the original network, but it starts to with more data presented.",5.1.2. RESULTS,[0],[0]
"The similarity of the authors’ topics in Figure 4e matches the original network, demonstrating the usefulness of the topics, but with some error.",5.1.2. RESULTS,[0],[0]
"Using the topics as the authors’ attributes, our Sym-NARM achieves reasonably good reconstruction of the network with only 20% training data, further improving with 80% training data.",5.1.2. RESULTS,[0],[0]
"Although niMM uses the same node attributes, its performance is not as good and is even outperformed by EPM with 80% training data.",5.1.2. RESULTS,[0],[0]
"Here we compared our Asym-NARM (implemented in MATLAB on top of the BGGPF code) with two models that do not consider node attributes, BGGPF (Zhou et al., 2012) and iMMM, and three node-attribute models, niMM, SSIPF (Hu et al., 2016a) and DMR-MMSB.",5.2. Link Prediction on Directed Networks,[0],[0]
"We used the following four datasets:
• Lazega-advice: This dataset is a directed network with 892 links of the advice relation among the attorneys.",5.2. Link Prediction on Directed Networks,[0],[0]
The node attributes are the same as in Lazega-cowork.,5.2. Link Prediction on Directed Networks,[0],[0]
•,5.2. Link Prediction on Directed Networks,[0],[0]
Citeseer:,5.2. Link Prediction on Directed Networks,[0],[0]
"This dataset2 contains a citation network with 2http://linqs.umiacs.umd.edu/projects/ /projects/lbc/index.html
4591 links of 3312 papers, labelled with one of 6 categories.",5.2. Link Prediction on Directed Networks,[0],[0]
"For each paper, we used both the category label and the presence/absence of 500 most frequent words as two separate attribute sets.",5.2. Link Prediction on Directed Networks,[0],[0]
We got a 3312 × 500 word attribute matrix with 65674 non-zero entries.,5.2. Link Prediction on Directed Networks,[0],[0]
•,5.2. Link Prediction on Directed Networks,[0],[0]
Cora:,5.2. Link Prediction on Directed Networks,[0],[0]
"This dataset2 contains a citation network with 5429 links of 2708 papers in machine learning, labelled with one of 7 categories.",5.2. Link Prediction on Directed Networks,[0],[0]
"Similar to Citeseer, we used both the category label and the 500 most frequent words as two separate attribute sets.",5.2. Link Prediction on Directed Networks,[0],[0]
We got a 2708×500 word attribute matrix with 39268 non-zero entries.,5.2. Link Prediction on Directed Networks,[0],[0]
•,5.2. Link Prediction on Directed Networks,[0],[0]
"Aminer: The Aminer dataset (Tang et al., 2009) contains a citation network with 2555 papers labelled with 10 categories and 5967 links.",5.2. Link Prediction on Directed Networks,[0],[0]
"We further collected information of each paper via the Aminer’s API, including the authors’ names (2597 unique authors), abstract, venue, year, and number of citations.",5.2. Link Prediction on Directed Networks,[0],[0]
"For the abstract, we extract the 5 most frequent topics for each paper in a similar way to NIPS234.",5.2. Link Prediction on Directed Networks,[0],[0]
"In total, we prepared two sets of attributes: the labels and the others formed with the combination of all collected information.",5.2. Link Prediction on Directed Networks,[0],[0]
"For fair comparison, we generated training/testing data with the code in the SSI-PF package, which splits a network in terms of its links.",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"We used the default hyper-parameter settings of BGGPF, SSI-PF, and niMM, provided by the original authors.",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"Kmax was set to 50 on Lazega-advice and 200 (same as (Hu et al., 2016a))",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
on all the other three datasets.,5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"For our Asym-NARM, we set µ0 = 1 and the
other hyper-parameters the same as those used in (Zhou et al., 2012; Hu et al., 2016a).",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"Following the suggestion of Hu et al. (2016a), we used 1500 MCMC iterations in total and the last 500 samples to compute the AUC-ROC/PR scores.",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"Since Citeseer, Cora, and Aminer are already too large for niMM, iMMM, and DMR-MMSB to produce results in reasonable time given our computational resources, we reported their performance only on Lazega-advice.",5.2.1. EXPERIMENTAL SETTINGS,[0],[0]
"Shown in Figure 5a, Asym-NARM gains better results in terms of AUC-ROC/PR on Lazega-advice in most of the training proportions.",5.2.2. RESULTS,[0],[0]
"Overall, the node-attribute models perform better than the models that do not consider node attributes, showing the usefulness of node attributes.",5.2.2. RESULTS,[0],[0]
"On the other three datasets, we used different sets of attributes to study how different attributes influence the performance of Asym-NARM and SSI-PF.
",5.2.2. RESULTS,[0],[0]
"In general, Asym-NARM performs better than SSI-PF regardless of which set of attributes is used.",5.2.2. RESULTS,[0],[0]
The performance of SSI-PF approaches ours in Citeseer with the labels as attributes (indicated by “-l”).,5.2.2. RESULTS,[0],[0]
But the gap between SSI-PF and our model becomes larger when the words are used as attributes (indicated by “-w”).,5.2.2. RESULTS,[0],[0]
"In Cora, SSI-PF with the words does not perform as well as its non-node-attribute counterpart, BGGPF, indicating it may not be as robust as our model with large sets of attributes.",5.2.2. RESULTS,[0],[0]
"To investigate this, we varied the number of the most frequent words from 10 to 500 for Asym-NARM and SSI-PF on Citeseer and Cora.",5.2.2. RESULTS,[0],[0]
"With more words, the AUC-ROC/PR score of SSI-PF de-
grades increasingly.",5.2.2. RESULTS,[0],[0]
"We further checked the prior of the node factor loadings in SSI-PF (the variable that incorporates node attributes and corresponds to gi,k in our model) and found that the coefficient of variation of each node’s prior drops dramatically, indicating with more words, SSIPF is failing to use the supervised information in the words.",5.2.2. RESULTS,[0],[0]
Here we used two datasets with hierarchical node attributes: (1) Cora-hier: a citation network with 1712 papers and 6308 links extracted from the original Cora dataset3.,5.3. Link Prediction with Hierarchical Node Attributes,[0],[0]
"The papers are labelled with one of 63 sub-areas (first level) and each sub-area belongs to one of 10 primary areas (second level), such as “machine learning in artificial intelligence” and “memory management in operating systems”; (2) Patent-hier: a citation network with 1461 patents and 2141 links from the National Bureau of Economic Research where the hierarchical International Patent Classification (IPC) code of a patent is used as attributes.
",5.3. Link Prediction with Hierarchical Node Attributes,[0],[0]
"The AUC-ROC/PR scores in Figure 6 show that our AsymNARM with hierarchical attributes outperforms the others, which demonstrates leveraging hierarchical side information is beneficial to link prediction.",5.3. Link Prediction with Hierarchical Node Attributes,[0],[0]
"Although SSI-PF also models the hierarchical attributes, its performance in these two datasets is not comparable with our model’s.",5.3. Link Prediction with Hierarchical Node Attributes,[0],[0]
"In this section, we compare the running time of the models for directed networks (all implemented in MATLAB and running on a desktop with 3.40 GHz CPU and 16GB RAM).",5.4. Running Time,[0],[0]
"Using 80% data for training, the running time for Asym-NARM, SSI-PF, and niMM on Aminer with different sets of node attributes is reported in Table 2.",5.4. Running Time,[0],[0]
Note DMR-MMSB did not complete with “Authors” and “All” due to our computational resources.,5.4. Running Time,[0],[0]
"Asym-NARM is about 10 times faster than SSI-PF with all the attributes and about
3https://people.cs.umass.edu/˜mccallum/ data.html
2 times faster with the labels.",5.4. Running Time,[0],[0]
"Thus Asym-NARM is more efficient, especially with large sets of attributes, supporting the complexity analysis in Table 1.",5.4. Running Time,[0],[0]
"As a summary of the experiments, Asym/Sym-NARM achieved better link prediction performance with faster inference.",6. Conclusion,[0],[0]
"While EPM, a non-node-attribute model, performed well on nearly complete networks, it degraded with less training data.",6. Conclusion,[0],[0]
"niMM and DMR-MMSB, extensions to MMSB with the logistic-normal transform, had similar results to Sym-NARM but scaled inefficiently.",6. Conclusion,[0],[0]
"SSI-PF’s performance and scalability were not as good as Asym-NARM in the presented cases with flat and hierarchical attributes and it was less effective with larger numbers of attributes.
",6. Conclusion,[0],[0]
"Thus NARM is a comparatively simple yet effective and efficient way of incorporating node attributes, including hierarchical attributes, for relational models with Poisson likelihood.",6. Conclusion,[0],[0]
This leads to improved link prediction and matrix completion for less complete relational data of both directed and undirected networks.,6. Conclusion,[0],[0]
"With the efficient inference, our models can be used to model large sparse relational networks with node attributes.
",6. Conclusion,[0],[0]
"NARM can easily be extended to multi-relational networks such as (Hu et al., 2016b) and topic models with document and word attributes, which is left for our future work.",6. Conclusion,[0],[0]
"Relational data are usually highly incomplete in practice, which inspires us to leverage side information to improve the performance of community detection and link prediction.",abstractText,[0],[0]
This paper presents a Bayesian probabilistic approach that incorporates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood.,abstractText,[0],[0]
Our method works flexibly with both directed and undirected relational networks.,abstractText,[0],[0]
The inference can be done by efficient Gibbs sampling which leverages sparsity of both networks and node attributes.,abstractText,[0],[0]
"Extensive experiments show that our models achieve the stateof-the-art link prediction results, especially with highly incomplete relational data.",abstractText,[0],[0]
Leveraging Node Attributes for Incomplete Relational Data,title,[0],[0]
"The union of subspaces (UoS) model, in which data vectors lie near one of several subspaces, has been used actively in the computer vision community on datasets ranging from images of objects under various lighting conditions (Basri & Jacobs, 2003) to visual surveillance tasks (Oliver et al., 2000).",1. Introduction,[0],[0]
"The recent textbook (Vidal et al., 2016) includes a number of useful applications for this model, including lossy image compression, clustering of face images under different lighting conditions, and video segmentation.",1. Introduction,[0],[0]
"Subspace clustering algorithms utilize the UoS model to cluster data
1Department of Electrical and Computer Engineering, University Michigan, Ann Arbor, MI, USA.",1. Introduction,[0],[0]
"Correspondence to: John Lipor <lipor@umich.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
vectors and estimate the underlying subspaces, achieving excellent performance on a variety of real datasets.",1. Introduction,[0],[0]
"However, as we will show in Section 4, even oracle UoS classifiers do not achieve perfect clustering on these datasets.",1. Introduction,[0],[0]
"While current algorithms for subspace clustering are unsupervised, in many cases a human could provide relevant information in the form of pairwise constraints between points, e.g., answering whether two images are of the same person or whether two objects are the same.
",1. Introduction,[0],[0]
The incorporation of pairwise constraints into clustering algorithms is known as pairwise-constrained clustering (PCC).,1. Introduction,[0],[0]
PCC algorithms use supervision in the form of must-link and cannot-link constraints by ensuring that points with must-link constraints are clustered together and points with cannot-link constraints are clustered apart.,1. Introduction,[0],[0]
"In (Davidson et al., 2006), the authors investigate the phenomenon that incorporating poorly-chosen constraints can lead to an increase in clustering error, rather than a decrease as one would expect from additional label information.",1. Introduction,[0],[0]
This is because points constrained to be in the same cluster that are otherwise dissimilar can confound the constrained clustering algorithm.,1. Introduction,[0],[0]
"For this reason, researchers have turned to active query selection methods, in which constraints are intelligently selected based on a number of heuristics.",1. Introduction,[0],[0]
These algorithms perform well across a number of datasets but do not take advantage of any known structure in the data.,1. Introduction,[0],[0]
"In the case where data lie on a union of subspaces, one would hope that knowledge of the underlying geometry could give hints as to which points are likely to be clustered incorrectly.
",1. Introduction,[0],[0]
Let X = { xi ∈,1. Introduction,[0],[0]
"RD }N i=1
be a set of data points lying near a union of K linear subspaces of the ambient space.",1. Introduction,[0],[0]
"We denote the subspaces by {Sk}Kk=1, each having dimension dk.",1. Introduction,[0],[0]
"An example union of subspaces is shown in Fig. 1, where d1 = 2, d2 = d3 = 1.",1. Introduction,[0],[0]
The goal of subspace clustering algorithms has traditionally been to cluster the points in X according to their nearest subspace without any supervised input.,1. Introduction,[0],[0]
"We turn this around and ask whether this model is useful for active clustering, where we request a very small number of intelligently selected labels.",1. Introduction,[0],[0]
A key observation when considering data well-modeled by a union of subspaces is that uncertain points will be ones lying equally distant to multiple subspaces.,1. Introduction,[0],[0]
"Using a novel definition of margin tailored for the union of subspaces model, we incorporate this observation into an active subspace clustering
algorithm.
",1. Introduction,[0],[0]
Our contributions are as follows.,1. Introduction,[0],[0]
We introduce a novel algorithm for pairwise constrained clustering that leverages UoS structure in the data.,1. Introduction,[0],[0]
"A key step in our algorithm is choosing points of minimum margin, i.e., those lying near a decision boundary between subspaces.",1. Introduction,[0],[0]
We define a notion of margin for the UoS model and provide theoretical insight as to why points of minimum margin are likely to be misclustered by unsupervised algorithms.,1. Introduction,[0],[0]
"We show through extensive experimental results that when the data lie near a union of subspaces, our method drastically outperforms existing PCC algorithms, requiring far fewer queries to achieve perfect clustering.",1. Introduction,[0],[0]
"Our datasets range in dimension from 256-2016, number of data points from 320-9298, and number of subspaces from 5-100.",1. Introduction,[0],[0]
"On ten MNIST digits with a modest number of queries, we get 5% classification error with only 500 pairwise queries compared to about 20% error for current state-of-the-art PCC algorithms and 35% for unsupervised algorithms.",1. Introduction,[0],[0]
"We also achieve 0% classification error on the full Yale, COIL, and USPS datasets with a small fraction of the number of queries needed by competing algorithms.",1. Introduction,[0],[0]
"In datasets where we do not expect subspace structure, our algorithm still achieves competitive performance.",1. Introduction,[0],[0]
"Further, our algorithm is agnostic to the input subspace clustering algorithm and can therefore take advantage of any future algorithmic advances for subspace clustering.",1. Introduction,[0],[0]
"A survey of recently developed subspace clustering algorithms can be found in (Vidal, 2011) and the textbook (Vidal et al., 2016).",2. Related Work,[0],[0]
"In these and more recent work, clustering algorithms that employ spectral methods achieve the best performance on most datasets.",2. Related Work,[0],[0]
"Notable examples of such algorithms include Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2013) and its extensions (You et al., 2016b;a), Low-Rank Representation (LRR) (Liu et al., 2010), Thresholded Subspace Clustering (TSC) (Heckel & Bölcskei, 2015), and Greedy Subspace Clustering (GSC)
(Park et al., 2014).",2. Related Work,[0],[0]
"Many recent algorithms exist with both strong theoretical guarantees and empirical performance, and a full review of all approaches is beyond the scope of this work.",2. Related Work,[0],[0]
"However, the core element of all recent algorithms lies in the formation of the affinity matrix, after which spectral clustering is performed to obtain label estimates.",2. Related Work,[0],[0]
"In SSC, the affinity matrix is formed via a series of `1-penalized regressions.",2. Related Work,[0],[0]
LRR uses a similar cost function but penalizes the nuclear norm instead of the `1.,2. Related Work,[0],[0]
"TSC thresholds the spherical distance between points, and GSC works by successively (greedily) building subspaces from points likely to lie in the same subspace.",2. Related Work,[0],[0]
"Of these methods, variants of SSC achieve the best overall performance on benchmark datasets and has the strongest theoretical guarantees, which were introduced in (Elhamifar & Vidal, 2013) and strengthened in numerous recent works (Soltanolkotabi & Candes, 2012; 2014; Wang & Xu, 2013; Wang et al., 2016).",2. Related Work,[0],[0]
"While the development of efficient algorithms with stronger guarantees has received a great deal of attention, very little attention has been paid to the question of what to do about data that cannot be correctly clustered.",2. Related Work,[0],[0]
"Thus, when reducing clustering error to zero (or near zero) is a priority, users must look beyond unsupervised subspace clustering algorithms to alternative methods.",2. Related Work,[0],[0]
"One such method is to request some supervised input in the form of pairwise constraints, leading to the study of pairwise-constrained clustering (PCC).
",2. Related Work,[0],[0]
"PCC algorithms work by incorporating must-link and cannot-link constraints between points, where points with must-link constraints are forced (or encouraged in the case of spectral clustering) to be clustered together, and points with cannot-link constraints are forced to be in separate clusters.",2. Related Work,[0],[0]
"In many cases, these constraints can be provided by a human labeler.",2. Related Work,[0],[0]
"For example, in (Biswas & Jacobs, 2014), the authors perform experiments where comparisons between human faces are provided by users of Amazon Mechanical Turk with an error rate of 1.2%.",2. Related Work,[0],[0]
"Similarly, for subspace clustering datasets such as Yale B and MNIST, a human could easily answer questions such as, “Are these two faces the same person?” and “Are these two images the same number?”",2. Related Work,[0],[0]
"An early example of PCC is found in (Wagstaff et al., 2001), where the authors modify the K-means cost function to incorporate such constraints.",2. Related Work,[0],[0]
"In (Basu et al., 2004), the authors utilize active methods to initialize K-means in an intelligent “EXPLORE” phase, during which neighborhoods of must-linked points are built up.",2. Related Work,[0],[0]
"After this phase, new points are queried against representatives from each neighborhood until a must-link is obtained.",2. Related Work,[0],[0]
"A similar explore phase is used in (Mallapragada et al., 2008), after which a min-max approach is used to select the most uncertain sample.",2. Related Work,[0],[0]
"Early work on constrained spectral clustering appears in (Xu et al., 2005; Wang & Davidson, 2010), in which spectral clustering is improved by examining the
eigenvectors of the affinity matrix in order to determine the most informative points.",2. Related Work,[0],[0]
"However, these methods are limited to the case of two clusters and therefore impractical in many cases.
",2. Related Work,[0],[0]
"More recently, the authors in (Xiong et al., 2016; Biswas & Jacobs, 2014) improve constrained clustering by modeling which points will be most informative given the current clustering, with state-of-the-art results achieved on numerous datasets by the algorithm in (Xiong et al., 2016), referred to as Uncertainty Reducing Active Spectral Clustering (URASC).",2. Related Work,[0],[0]
"URASC works by maintaining a set of certain sets, whereby points in the same certain set are mustlinked and points in different certain sets are cannot-linked.",2. Related Work,[0],[0]
"A test point xT is selected via an uncertainty-reduction model motivated by matrix perturbation theory, after which queries are presented in an intelligent manner until xT is either matched with an existing certain set or placed in its own new certain set.",2. Related Work,[0],[0]
"In practice (Xiong, 2016), the certain sets are initialized using the EXPLORE algorithm of (Basu et al., 2004).
",2. Related Work,[0],[0]
"While we are certainly not the first to consider actively selecting labels to improve clustering performance, to the best of our knowledge we are the first to do so with structured clusters.",2. Related Work,[0],[0]
"Structure within and between data clusters is often leveraged for unsupervised clustering (Wright et al., 2009), and that structure is also leveraged for adaptive sampling of the structured signals themselves (e.g., see previous work on sparse (Haupt et al., 2011; Indyk et al., 2011), structured sparse (Soni & Haupt, 2014), and low rank signals (Krishnamurthy & Singh, 2013)).",2. Related Work,[0],[0]
"This paper emphasizes the power of that structure for reducing the number of required labels in an active learning algorithm as opposed to reducing the number of samples of the signal itself, and points to exciting open questions regarding the tradeoff between signal measurements and query requirements in semi-supervised clustering.",2. Related Work,[0],[0]
Recall that X = { xi ∈,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"RD }N i=1
is a set of data points lying on a union ofK subspaces {Sk}Kk=1, each having dimension d.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"In this work, we assume all subspaces have the same dimension, but it is possible to extend our algorithm to deal with non-uniform dimensions.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"The goal is to cluster the data points according to this generative model, i.e., assigning each data point to its (unknown) subspace.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"In this section we describe our algorithm, which actively selects pairwise constraints in order to improve clustering accuracy.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"The key step is choosing an informative query test point, which we do using a novel notion of minimum subspace margin.
",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
Denote the true clustering of a point x ∈ X by C(x).,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Let
the output of a clustering algorithm (such as SSC) be an affinity/similarity matrix A and a set of label estimates{ Ĉ(xi) }N i=1
.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
These are the inputs to our algorithm.,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
The high-level operation of our algorithm is as follows.,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"To initialize, we build a set of certain sets Z using an EXPLORE-like algorithm similar to that of (Basu et al., 2004).",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
Certain sets are in some sense equivalent to labels in that points within a certain set belong to the same cluster and points across certain sets belong to different clusters.,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Following this, the following steps are repeated until a maximum number of queries has been made:
1.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
Spectral Clustering:,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Obtain label estimates via spectral clustering.
2.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"PCA on each cluster: Obtain a low-dimensional subspace estimate from points currently sharing the same estimated cluster label.
3.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Select Test Point: Obtain a test point xT using subspace margin with respect to the just estimated subspaces.
",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
4.,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Assign xT to Certain Set: Query the human to compare the test point with representatives from certain sets until a must-link is found or all certain sets have been queried, in which case the test point becomes its own certain set.
5.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
Impute Label Information:,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Certain sets are used to impute must-link and cannot-link values in the affinity matrix.
",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
We refer to our algorithm as SUPERPAC (SUbsPace clustERing with Pairwise Active Constraints).,3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"A diagram of the algorithm is given in Fig. 2, and we outline each of these steps below and provide pseudocode in Algorithm 1.",3. UoS-Based Pairwise-Constrained Clustering,[0],[0]
"Min-margin points have been studied extensively in active learning; intuitively, these are points that lie near the decision boundary of the current classifier.",3.1. Sample Selection via Margin,[0],[0]
"In (Settles, 2012), the author notes that actively querying points of minimum margin (as opposed to maximum entropy or minimum confidence) is an appropriate choice for reducing classification error.",3.1. Sample Selection via Margin,[0],[0]
"In (Wang & Singh, 2016), the authors present a margin-based binary classification algorithm that achieves an optimal rate of convergence (within a logarithmic factor).
",3.1. Sample Selection via Margin,[0],[0]
"In this section, we define a novel notion of margin for the UoS model and provide theoretical insight as to why points of minimum margin are likely to be misclustered.",3.1. Sample Selection via Margin,[0],[0]
"For a subspace Sk with orthonormal basis Uk, let the distance of a point to that subspace be dist(x,Sk) =",3.1. Sample Selection via Margin,[0],[0]
miny∈Sk ‖x,3.1. Sample Selection via Margin,[0],[0]
"− y‖2 =
∥∥x− UkUTk x∥∥2 .",3.1. Sample Selection via Margin,[0],[0]
"Let k∗ = arg mink∈[K] dist(x,Sk) be the index of the closest subspace, where [K] = {1, 2, · · · ,K}.",3.1. Sample Selection via Margin,[0],[0]
"Then the subspace
margin of a point x ∈ X is the ratio of closest and second closest subspaces, defined as
µ̂(x) = 1− max j 6=k∗,j∈[K]
dist(x, Sk∗)
",3.1. Sample Selection via Margin,[0],[0]
"dist(x, Sj) .",3.1. Sample Selection via Margin,[0],[0]
"(1)
The point of minimum margin is then defined as arg minx∈X µ̂(x).",3.1. Sample Selection via Margin,[0],[0]
"Note that the fraction is a value in [0, 1], where the a value of 0 implies that the point x is equidistant to its two closest subspaces.",3.1. Sample Selection via Margin,[0],[0]
"This notion is illustrated in Figure 3, where the yellow-green color shows the region within some margin of the decision boundary.
",3.1. Sample Selection via Margin,[0],[0]
"In the following theorem, we show that points lying near the intersection of subspaces are included among those of minimum margin with high probability.",3.1. Sample Selection via Margin,[0],[0]
This method of point selection is then motivated by the fact that the difficult points to cluster are those lying near the intersection of subspaces [12].,3.1. Sample Selection via Margin,[0],[0]
"Further, theory for SSC ([11],[15]) shows that problematic points are those having large inner product with some or all directions in other subspaces.",3.1. Sample Selection via Margin,[0],[0]
Subspace margin captures exactly this phenomenon.,3.1. Sample Selection via Margin,[0],[0]
Theorem 1.,3.1. Sample Selection via Margin,[0],[0]
Consider two d-dimensional subspaces S1 and S2.,3.1. Sample Selection via Margin,[0],[0]
"Let y = x + n, where x ∈ S1 and n ∼ N (0, σ2ID).",3.1. Sample Selection via Margin,[0],[0]
"Define
µ(y) = 1− dist(y,S1) dist(y,S2) .
",3.1. Sample Selection via Margin,[0],[0]
"Then 1− (1 + ε) √ σ2(D − d)
(1− ε) √ σ2(D − d) +",3.1. Sample Selection via Margin,[0],[0]
"dist(x,S2)2 ≤ µ(y)
and µ(y) ≤ 1− (1− ε) √",3.1. Sample Selection via Margin,[0],[0]
"σ2(D − d)
(1 + ε) √",3.1. Sample Selection via Margin,[0],[0]
σ2(D − d) +,3.1. Sample Selection via Margin,[0],[0]
"dist(x,S2)2 ,
with probability at least 1 − 4e−cε2(D−d), where c is an absolute constant.
",3.1. Sample Selection via Margin,[0],[0]
The proof is given in the supplementary material.,3.1. Sample Selection via Margin,[0],[0]
"Note that if dist(y,S1) ≤",3.1. Sample Selection via Margin,[0],[0]
"dist(y,S2), then µ(y) = µ̂(y).",3.1. Sample Selection via Margin,[0],[0]
"In this case, Thm. 1 states that under the given noise model, points with small residual to the incorrect subspace (i.e., points near the intersection of subspaces) will have small margin.",3.1. Sample Selection via Margin,[0],[0]
"These are exactly the points for which supervised label information will be most beneficial.
",3.1. Sample Selection via Margin,[0],[0]
The statement of Thm. 1 allows us to quantify exactly how near a point must be to the intersection of two subspaces to be considered a point of minimum margin.,3.1. Sample Selection via Margin,[0],[0]
Let φ1 ≤ φ2 ≤ · · · ≤ φd be the d principal angles1 between S1 and S2.,3.1. Sample Selection via Margin,[0],[0]
"If the subspaces are very far apart, 1d ∑d i=1",3.1. Sample Selection via Margin,[0],[0]
"sin
2(φi) is near 1, and if they are very close 1d ∑d i=1",3.1. Sample Selection via Margin,[0],[0]
"sin
2(φi) is near zero.",3.1. Sample Selection via Margin,[0],[0]
"Note that, for any x ∈ S1,
sin2(φ1) ≤",3.1. Sample Selection via Margin,[0],[0]
"dist(x,S2)2 ≤ sin2(φd) ;
that is, there are bounds on dist(x,S2) depending on the relationship of the two subspaces.",3.1. Sample Selection via Margin,[0],[0]
"We also know that if x is drawn using isotropic Gaussian weights from a basis for S1, then
E [ dist(x,S2)2 ]",3.1. Sample Selection via Margin,[0],[0]
"= 1
d d∑ i=1",3.1. Sample Selection via Margin,[0],[0]
"sin2(φi) .
",3.1. Sample Selection via Margin,[0],[0]
"Given this, we might imagine that margin of the noisy points is a useful indicator of points near the intersection in a scenario where sin2(φ1) is small but 1d ∑d i=1 sin 2(φi) is not,
1See (Golub & Loan, 2012) for a definition of principal angles.
",3.1. Sample Selection via Margin,[0],[0]
"e.g., when the subspaces have an intersection but are distant in other directions.",3.1. Sample Selection via Margin,[0],[0]
"With this in mind we state the following corollary, whose proof can be found in the supplementary material.
",3.1. Sample Selection via Margin,[0],[0]
Corollary 1.,3.1. Sample Selection via Margin,[0],[0]
"Suppose x1 ∈ S1 is such that
dist(x1,S2)2 = sin2(φ1) + δ",3.1. Sample Selection via Margin,[0],[0]
"( 1
d d∑ i=1",3.1. Sample Selection via Margin,[0],[0]
"sin2(φi)
) (2)
for some small δ ≥ 0; that is, x1 is close to the intersection of S1 and S2.",3.1. Sample Selection via Margin,[0],[0]
"Let x2 be a random point in S1 generated as x2 = U1w where U1 is a basis for S1 and w ∼ N (0, 1dId).",3.1. Sample Selection via Margin,[0],[0]
"We observe yi = xi + ni, where ni ∼ N (0, σ2), i = 1, 2.",3.1. Sample Selection via Margin,[0],[0]
If there exists τ,3.1. Sample Selection via Margin,[0],[0]
"> 1 such that
δ < 5 7",3.1. Sample Selection via Margin,[0],[0]
"− 1 τ
and
τ ( sin2(φ1) + 1
6 σ2 (D − d)
)",3.1. Sample Selection via Margin,[0],[0]
"< 1
d d∑ i=1",3.1. Sample Selection via Margin,[0],[0]
"sin2(φi) , (3)
that is, the average angle is sufficiently larger than the smallest angle, then
P {µ(y1) <",3.1. Sample Selection via Margin,[0],[0]
µ(y2)},3.1. Sample Selection via Margin,[0],[0]
"≥ 1− e−c( 7 100 ) 2 ds − 4e−c( 150 ) 2 (D−d)
where µ(y) is defined as in Thm. 1, c is an absolute constant, and s = 1d ∑d i=1 sin 2(φi).
",3.1. Sample Selection via Margin,[0],[0]
We make some remarks first to connect our results to other subspace distances that are often used.,3.1. Sample Selection via Margin,[0],[0]
Perhaps the most intuitive form of subspace distance between that spanned by U1 and U2 is 1d‖(I,3.1. Sample Selection via Margin,[0],[0]
"− U1U1)TU2‖2F ; if the two subspaces are the same, the projection onto the orthogonal complement is zero; if they are orthogonal, we get the norm of U2 alone, giving a distance of 1.",3.1. Sample Selection via Margin,[0],[0]
"This is equal to the more visually symmetric 1− 1d‖UT1 U2‖2F , another common distance.",3.1. Sample Selection via Margin,[0],[0]
"Further we note that, by the definition of principal angles (Golub & Loan, 2012),
1− 1 d ‖UT1 U2‖2F = 1− 1 d d∑ i=1",3.1. Sample Selection via Margin,[0],[0]
cos2(φi),3.1. Sample Selection via Margin,[0],[0]
= 1 d d∑ i=1,3.1. Sample Selection via Margin,[0],[0]
"sin2(φi) .
",3.1. Sample Selection via Margin,[0],[0]
"From Equation (2), we see that the size of δ determines how close x1 ∈ S1 is to S2; if δ = 0, x1 is as close to S2 as possible.",3.1. Sample Selection via Margin,[0],[0]
"For example, if φ1 = 0, the two subspaces intersect, and δ = 0 implies that x1 ∈ S1 ∩ S2.",3.1. Sample Selection via Margin,[0],[0]
Equation (3) captures the gap between average principal angle and the smallest principal angle.,3.1. Sample Selection via Margin,[0],[0]
"We conclude that if this gap is large enough and δ is small enough so that x1 is close to S2, then the observed y1 will have smaller margin than the average point in S1, even when observed with noise.
",3.1. Sample Selection via Margin,[0],[0]
"Algorithm 1 SUPERPAC Input: X = {x1, x2, . . .",3.1. Sample Selection via Margin,[0],[0]
", xN}: data, K: number of clusters, d: subspace dimension, A: affinity matrix, maxQueries: maximum number of pairwise comparisons Estimate Labels: Ĉ ← SPECTRALCLUSTERING(A,K) Initialize Certain Sets: Initialize Z = {Z1, · · · , Znc} and numQueries via UOS-EXPLORE in supplementary material.",3.1. Sample Selection via Margin,[0],[0]
"while numQueries < maxQueries do
PCA on Each Cluster: Solve
Sk = min U∈RD×d ∑ i:Ĉ(xi)=k ‖xi − UU ′xi‖2 .
",3.1. Sample Selection via Margin,[0],[0]
Obtain Test Point: select xT,3.1. Sample Selection via Margin,[0],[0]
"← arg minx∈X µ̂(x) Assign xT to Certain Set:
Sort {Z1, · · · , Znc} in order of most likely mustlink (via subspace residual for xT ), query xT against representatives from Zk until must-link constraint is found or k = nc.",3.1. Sample Selection via Margin,[0],[0]
"If no must-link constraint is found, set Z ← {Z1, · · · , Znc , {xT }} and increment nc.",3.1. Sample Selection via Margin,[0],[0]
"Impute Constraints: Set Aij = Aji = 1 for (xi, xj) in the same certain set and Aij = Aji = 0 for (xi, xj) in different certain sets (do not impute for points absent from certain sets).",3.1. Sample Selection via Margin,[0],[0]
"Estimate Labels: Ĉ ← SPECTRALCLUSTERING(A,K)
end while
For another perspective, consider that in the noiseless case, for x1, x2 ∈ S1, the condition dist(x1,S2) < dist(x2,S2) is enough to guarantee that x1 lies nearer to S2.",3.1. Sample Selection via Margin,[0],[0]
"Under the given additive noise model (yi = xi + ni for i = 1, 2) the gap between dist(x1,S2) and dist(x2,S2) must be larger by some factor depending on the noise level.",3.1. Sample Selection via Margin,[0],[0]
"After two applications of Thm. 1 and rearranging terms, we have that µ(y1) < µ(y2) with high probability if
βdist(x2,S2)2−dist(x1,S2)2 > (1−β)σ2(D−d).",3.1. Sample Selection via Margin,[0],[0]
"(4)
where β = ((1− ε)/(1 + ε))4, a value near 1 for small ε.",3.1. Sample Selection via Margin,[0],[0]
"Equation (4) shows that the gap dist(x2,S2)2 − dist(x1,S2)2 must grow (approximately linearly) with the noise level σ2.",3.1. Sample Selection via Margin,[0],[0]
The relationship of this gap to the subspace distances is quantified by Corollary 1; plugging sin2(φ1) from Equation (2) into Equation (3) and rearranging yields a statement of the form in Equation (4).,3.1. Sample Selection via Margin,[0],[0]
"We now describe SUPERPAC in more detail, our algorithm for PCC when data lie near a union of subspaces, given in Algorithm 1.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"The algorithm begins by initializing a set of disjoint certain sets, an optional process described in the
supplementary material due to space constraints.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
Next our algorithm assigns the points most likely to be misclassified to certain sets by presenting a series of pairwise comparisons.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Finally, we impute values onto the affinity matrix for all points in the certain sets and perform spectral clustering.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"The process is then repeated until the maximum number of pairwise comparisons has been reached.
",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
Let xT be the test point chosen as the min-margin point.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
Our goal is to assign xT to a certain set using as the fewest number of queries possible.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"For each certain set Zk, the representative xk is chosen as the maximum-margin point within the set.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Next, for each k, we let Uk be the ddimensional PCA estimate of the matrix whose columns are the points { x ∈ X",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
:,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
Ĉ(x) = Ĉ(xk) } .,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"We then query
our test point xT against the representatives xk in order of residual ∥∥xT − UkUTk xT∥∥2",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
(smallest first).,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"If a must-link constraint is found, we place xT in the corresponding certain set.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Otherwise, we place xT in its own certain set and update the number of certain sets.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
Pseudocode for the complete algorithm is given in Algorithm 1.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"As a technical note, we first normalize the input affinity matrix A so that the maximum value is 2.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"For must-link constraints, we impute a value of 1 in the affinity matrix, while for cannot-link constraints we impute a 0.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
The approach of imputing values in the affinity matrix is common in the literature but does not strictly enforce the constraints.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Further, we found in our experiments that imputing the maximum value in the affinity matrix resulted in unstable results.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Thus, users must be careful to not only choose the correct constraints as noted in (Basu et al., 2004), but to incorporate these constraints in a way that allows for robust clustering.
",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"SUPERPAC can be thought of as an extension of ideas from PCC literature (Basu et al., 2004; Biswas & Jacobs, 2014; Xiong et al., 2016) to leverage prior knowledge about the underlying geometry of the data.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"For datasets such as Yale B and MNIST, the strong subspace structure makes Euclidean distance a poor proxy for similarity between points in the same cluster, leading to the superior performance of our algorithm demonstrated in the following sections.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"This
structure does not exist in all datasets, in which case we do not expect our algorithm to outperform current PCC algorithms.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"The reader will note we made a choice to order the certain sets according to the UoS model; this is similar to the choice in (Xiong et al., 2016) to query according to similarity, where our notion of similarity here is based on subspace distances.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"We found this resulted in significant performance benefits, matching our intuition that points are clustered based on their nearest subspace.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"In contrast to (Biswas & Jacobs, 2014; Xiong et al., 2016), where the test point is chosen according to a global improvement metric, we choose test points according to their classification margin.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"In our experiments, we found subspace margin to be a strong indicator of which points are misclassified, meaning that our algorithm rapidly corrects the errors that occur as a result of unsupervised subspace clustering.
",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"Finally, note that the use of certain sets relies on the assumption that the pairwise queries are answered correctly—an assumption that is common in the literature (Basu et al., 2004; Mallapragada et al., 2008; Xiong et al., 2016).",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
"However, in (Xiong et al., 2016), the authors demonstrate that an algorithm based on certain sets still yields significant improvements under a small error rate.",3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
The study of robustly incorporating noisy pairwise comparisons is an interesting topic for further study.,3.2. Pairwise Constrained Clustering with SUPERPAC,[0],[0]
We compare the performance of our method and the nonparametric version of the URASC algorithm (URASC-N)2 over a variety of datasets.,4. Experimental Results,[0],[0]
"Note that while numerous PCC algorithms exist, URASC achieves both the best empirical results and computational complexity on a variety of datasets.",4. Experimental Results,[0],[0]
"We also compared with the methods from (Basu et al., 2004) and (Biswas & Jacobs, 2014) but found both to perform significanly worse than URASC on all datasets considered, with a far greater computational cost in the case
2In our experiments, the parametric version of URASC was found to be numerically unstable and did not have significantly different performance from URASC-N in the best cases.
of (Biswas & Jacobs, 2014).",4. Experimental Results,[0],[0]
We use a maximum query budget of 2K for UOS-EXPLORE and EXPLORE.,4. Experimental Results,[0],[0]
"For completeness, we also compare to random constraints, in which queries are chosen uniformly at random from the set of unqueried pairs.
",4. Experimental Results,[0],[0]
"Finally, we compare against the oracle PCA classifier, which we now define.",4. Experimental Results,[0],[0]
Let Uk be the d-dimensional PCA estimate of the points whose true label C(x) =,4. Experimental Results,[0],[0]
"k. Then the oracle label is Ĉo(x) = arg mink∈[K]
∥∥x− UkUTk",4. Experimental Results,[0],[0]
x∥∥2.,4. Experimental Results,[0],[0]
"This allows us to quantitatively capture the idea that, because the true classes are not perfectly low-rank, some points would not be clustered with the low-rank approximation of their own true cluster.",4. Experimental Results,[0],[0]
"In our experiments, we also compared with oracle robust PCA (Candes et al., 2011) implemented via the augmented Lagrange multiplier method (Lin et al., 2011) but did not find any improvement in classification error.
",4. Experimental Results,[0],[0]
"Datasets We consider five datasets commonly used as benchmarks in the subspace clustering literature3, with a summary of the datasets and their relevant parameters are given in Table 1.",4. Experimental Results,[0],[0]
The Yale B dataset consists of 64 images of size 192 × 168 of each of 38 different subjects under a variety of lighting conditions.,4. Experimental Results,[0],[0]
"For values of K less than 38, we follow the methodology of (Zhang et al., 2012) and perform clustering on 100 randomly selected subsets of size K. We choose d = 9 as is common in the literature (Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2015).",4. Experimental Results,[0],[0]
"The MNIST handwritten digit database test dataset consists of 10,000 centered 28 × 28 pixel images of handwritten digits 0-9.",4. Experimental Results,[0],[0]
"We follow a similar methodology to the previous section and select 100 random subsets of size K, using subspace dimension d = 3 as in (Heckel & Bölcskei, 2015).",4. Experimental Results,[0],[0]
"The COIL-20 dataset (Nene et al., 1996b) consists of 72 images
3The validity of the UoS assumption for two of these datasets is investigated in (Elhamifar & Vidal, 2013; Heckel & Bölcskei, 2015).
of size 32× 32 of each of 20 objects.",4. Experimental Results,[0],[0]
"The COIL-100 dataset (Nene et al., 1996a) contains 100 objects (distinct from the COIL-20 objects) of the same size and with the same number of images of each object.",4. Experimental Results,[0],[0]
"For both datasets, we use subspace dimension d = 9.",4. Experimental Results,[0],[0]
"Finally, we apply our algorithm to the USPS dataset provided by (Cai et al., 2011), which contains 9,298 total images of handwritten digits 0-9 of size 16 × 16 with roughly even label distribution.",4. Experimental Results,[0],[0]
"We again use subspace dimension d = 9.
",4. Experimental Results,[0],[0]
Input Subspace Clustering Algorithms,4. Experimental Results,[0],[0]
A major strength of our algorithm is that it is agnostic to the initial subspace clustering algorithm used to generate the input affinity matrix.,4. Experimental Results,[0],[0]
"To demonstrate this fact, we apply our algorithm with an input affinity matrix obtained from a variety of subspace clustering methods, summarized in Table 1.",4. Experimental Results,[0],[0]
Note that some recent algorithms are not included in the simulations here.,4. Experimental Results,[0],[0]
"However, the simulations show that our algorithm works well with any initial clustering, and hence we expect similar results as new algorithms are developed.
",4. Experimental Results,[0],[0]
Experimental Results Fig. 4 shows the clustering error versus the number of pairwise comparisons for the Yale and MNIST datasets.,4. Experimental Results,[0],[0]
The input affinity matrix is obtained by running SSC for the Yale datset and by running TSC for the MNIST dataset.,4. Experimental Results,[0],[0]
"The figure clearly demonstrates the benefits of leveraging UoS structure in constrained clustering—in all cases, SUPERPAC requires roughly half the number of queries needed by URASC to achieve perfect clustering.",4. Experimental Results,[0],[0]
"For the Yale dataset with K = 5, roughly 2Kd queries are required to surpass oracle performance, and for K = 10 roughly 3Kd queries are required.",4. Experimental Results,[0],[0]
"Note that for the Yale dataset, the clustering error increases using URASC.",4. Experimental Results,[0],[0]
This is due to the previously mentioned fact that imputing the wrong constraints can lead to worse clustering performance.,4. Experimental Results,[0],[0]
"For sufficiently many queries, the error decreases as expected.",4. Experimental Results,[0],[0]
"Fig. 5 shows the misclassification rate versus number of points for allK = 38 subjects of the Yale databse, with the input affinity matrix taken from SSC-OMP (You et al., 2016b).",4. Experimental Results,[0],[0]
We space out the markers for clearer plots.,4. Experimental Results,[0],[0]
"In this case, URASC performs roughly the same as random query selection, while SUPERPAC performs significantly
better.
",4. Experimental Results,[0],[0]
Fig. 6 demonstrates the continued superiority of our algorithm in the case where UoS structure exists.,4. Experimental Results,[0],[0]
"In the case of COIL-20, the clustering is sometimes unstable, alternating between roughly 0% and 7% clustering error for both active algorithms.",4. Experimental Results,[0],[0]
This further demonstrates the observed phenomenon that spectral clustering is sensitive to small perturbations.,4. Experimental Results,[0],[0]
"To avoid this issue, we kept track of the K-subspaces cost function (see (Bradley & Mangasarian, 2000)) and ensured the cost decreased at every iteration.",4. Experimental Results,[0],[0]
We refer to this added heuristic as SUPERPAC-S in the figure.,4. Experimental Results,[0],[0]
"The incorporation of this heuristic into our algorithm is a topic for further study.
",4. Experimental Results,[0],[0]
"Fig. 7 shows the resulting error on the USPS dataset, again indicating the superiority of our method.",4. Experimental Results,[0],[0]
"Note that N is large for this dataset, making spectral clustering computationally burdensome.",4. Experimental Results,[0],[0]
"Further, the computational complexity of URASC is dependent on N .",4. Experimental Results,[0],[0]
"As a result, URASC did not complete 2000 queries in 48 hours of run time when using 10 cores, so we compare to the result after completing only 1000 queries.",4. Experimental Results,[0],[0]
"Finally, in Fig. 8, we demonstrate that even on data without natural subspace structure, SUPERPAC performs competitively with URASC.",4. Experimental Results,[0],[0]
We have presented a method of selecting and incorporating pairwise constraints into subspace clustering that considers the underlying geometric structure of the problem.,5. Conclusion,[0],[0]
The union of subspaces model is often used in computer vision applications where it is possible to request input from human labelers in the form of pairwise constraints.,5. Conclusion,[0],[0]
"We showed that labeling is often necessary for subspace classifiers to achieve a clustering error near zero; additionally, these constraints can be chosen intelligently to improve the clustering procedure overall and allow for perfect clustering with a modest number of requests for human input.
",5. Conclusion,[0],[0]
Developing techniques for handling noisy query responses will allow extension to undersampled or compressed data.,5. Conclusion,[0],[0]
"One may assume that compressed data would be harder to distinguish, leading to noisier query responses.",5. Conclusion,[0],[0]
"Finally, we saw that for datasets with different types of cluster structure, the structure assumptions of each algorithm had direct impact on performance; in the future we plan to additionally develop techniques for learning from unlabeled data whether the union of subspace model or a standard clustering approach is more appropriate.",5. Conclusion,[0],[0]
This work was supported by NSF F031543-071159-GRFP and US ARO Grant W911NF1410634.,Acknowledgements,[0],[0]
"Many clustering problems in computer vision and other contexts are also classification problems, where each cluster shares a meaningful label.",abstractText,[0],[0]
"Subspace clustering algorithms in particular are often applied to problems that fit this description, for example with face images or handwritten digits.",abstractText,[0],[0]
"While it is straightforward to request human input on these datasets, our goal is to reduce this input as much as possible.",abstractText,[0],[0]
We present a pairwiseconstrained clustering algorithm that actively selects queries based on the union-of-subspaces model.,abstractText,[0],[0]
"The central step of the algorithm is in querying points of minimum margin between estimated subspaces; analogous to classifier margin, these lie near the decision boundary.",abstractText,[0],[0]
We prove that points lying near the intersection of subspaces are points with low margin.,abstractText,[0],[0]
Our procedure can be used after any subspace clustering algorithm that outputs an affinity matrix.,abstractText,[0],[0]
We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the stateof-the-art active query algorithms on datasets with subspace structure and is competitive on other datasets.,abstractText,[0],[0]
Leveraging Union of Subspace Structure to Improve Constrained Clustering,title,[0],[0]
"Analyzing high dimensional, high volume data can be timeconsuming and resource intensive.",1. Introduction,[0],[0]
"Core data analysis, such as robust instances of regression, involve convex optimization tasks over large matrices, and do not naturally distribute or parallelize.",1. Introduction,[0],[0]
"In response to this, approximation algorithms have been proposed which follow a “sketch and solve” paradigm: produce a reduced size representation of the data, and solve a version of the problem on this summary (Woodruff, 2014).",1. Introduction,[0],[0]
It is then argued that the solution on the reduced data provides an approximation to the original problem on the original data.,1. Introduction,[0],[0]
"This paradigm is particularly attractive when the summarization can be computed efficiently on partial views of the full data—for example, when it can be computed incrementally as the data arrives (streaming model) or assembled from summarizations of disjoint partitions of the data (distributed model) (Woodruff, 2014; Agarwal et al., 2012; Feldman et al., 2006).",1. Introduction,[0],[0]
"This
*Equal contribution 1Department of Computer Science, University of Warwick, Coventry, UK 2School of Computer Science, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA.",1. Introduction,[0],[0]
"Correspondence to: Charlie Dickens <c.dickens@warwick.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"template has been instantiated for a number of fundamental tasks in high dimensional linear algebra such as matrix multiplication, low rank approximation, and regression.
",1. Introduction,[0],[0]
"Our understanding is well-established in the common case of the Euclidean norm, i.e., when distances are measured under the Minkowski p-norm for p = 2.",1. Introduction,[0],[0]
"Here, it suffices to choose a sketching matrix independent of the data—where each entry is i.i.d.",1. Introduction,[0],[0]
"Gaussian, Rademacher, or more efficient variants of these.",1. Introduction,[0],[0]
"For other p values, less is known, but these are often needed to handle limitations of the 2-norm.",1. Introduction,[0],[0]
"For instance, p = 1 is widely used as it is extremely robust with respect to the presence of outliers while p > 2 can be used to detect outlying observations.
",1. Introduction,[0],[0]
We continue the study of algorithms for `p norms on streaming and distributed data.,1. Introduction,[0],[0]
"A particular novelty of our results is that unlike previous distributed and streaming algorithms, they can all be implemented deterministically, i.e., our algorithms make no random choices.",1. Introduction,[0],[0]
"While in a number of settings randomized algorithms are highly beneficial, leading to massive computational savings, there are other applications which require extremely high reliability, for which one needs to obtain guaranteed performance across a large number of inputs.",1. Introduction,[0],[0]
"If one were to use a randomized algorithm, then it would need vanishingly small error probability; however, many celebrated algorithms in numerical linear algebra succeed with only constant probability.",1. Introduction,[0],[0]
"Another limitation of randomized algorithms was shown in (Hardt & Woodruff, 2013): if the input to a randomized sketch depends on the output of a preceding algorithm using the same sketch, then the randomized sketch can give an arbitrarily bad answer.",1. Introduction,[0],[0]
"Hence, such methods cannot handle adaptively chosen inputs.",1. Introduction,[0],[0]
"Thus, while randomized algorithms certainly have their place, the issues of high reliability and adaptivity motivate the development of deterministic methods for a number of other settings, for which algorithms are scarce.
",1. Introduction,[0],[0]
"Our techniques can be viewed as a conceptual generalization of Liberty’s Frequent Directions (in the 2-norm) (Liberty, 2013), which progressively computes an SVD on subsequent blocks of the input.",1. Introduction,[0],[0]
"This line of work (Liberty, 2013; Ghashami & Phillips, 2014; Ghashami et al., 2016; Ghashami et al., 2016) is the notable exception in numerical linear algebra, as it provides deterministic methods,
although all such methods are specific to the 2-norm.",1. Introduction,[0],[0]
"Our core algorithm is similar in nature, but we require a very different technical analysis to argue that the basis transformation computed preserves the shape in the target p-norm.
",1. Introduction,[0],[0]
Our main application is to show how high dimensional regression and low rank approximation problems can be solved approximately and deterministically in the sketch and solve paradigm.,1. Introduction,[0],[0]
The core of the summary is to find rows of the original matrix which have high leverage scores.,1. Introduction,[0],[0]
"That is, they contain a lot of information about the shape of the data.",1. Introduction,[0],[0]
"In the Euclidean norm, leverage scores correspond directly to row norms of an orthonormal basis.",1. Introduction,[0],[0]
"This is less straightforward for other `p norms, where the scores correspond to the row norms of so-called `p-well-conditioned bases.",1. Introduction,[0],[0]
"Moreover, while leverage scores are often used for sampling in randomized algorithms, we use them here in the context of fully deterministic algorithms.
",1. Introduction,[0],[0]
"We show how a superset of rows with high leverage scores can be found for arbitrary `p norms, based on only local information.",1. Introduction,[0],[0]
"This leads to efficient algorithms which identify rows with high (local) leverage scores within subsets of the data, and proceed hierarchically to collect a sufficient set of rows.",1. Introduction,[0],[0]
"These rows then allow us to solve regression problems: essentially, we solve the regression problem corresponding to just the retained input rows.",1. Introduction,[0],[0]
We apply this technique to `p-regression and entrywise `p-low rank approximation.,1. Introduction,[0],[0]
"In particular, we use it to solve the `∞-regression problem with additive error in a stream.",1. Introduction,[0],[0]
"Note that the `∞ problem reduces to finding a ball of minimum radius which covers the data, and global solutions are slow due to the need to solve a linear program.",1. Introduction,[0],[0]
"Instead, we show that only a subset of the data needs to be retained in the streaming model to compute accurate approximations.",1. Introduction,[0],[0]
"Given the relationship between the streaming model and the distributed model that we later define, this could be seen in the context of having data stored over multiple machines who could send ‘important’ rows of their data to a central coordinator in order to compute the approximation.
",1. Introduction,[0],[0]
Summary of Results.,1. Introduction,[0],[0]
"All our algorithms are deterministic polynomial time, and use significantly sublinear memory or communication in streaming and distributed models, respectively.",1. Introduction,[0],[0]
We consider tall and thin n× d matrices A for overconstrained regression so one should think of n,1. Introduction,[0],[0]
d. We implement both deterministic and randomized variants of our algorithms.,1. Introduction,[0],[0]
Section 3 presents an algorithm which returns rows of high ‘importance’ in a data matrix with additive error.,1. Introduction,[0],[0]
This follows by storing a polynomial number (in d) of rows and using these to compute a well-conditioned basis.,1. Introduction,[0],[0]
"The key insight here is that rows of high norm in the full wellconditioned basis cannot have their norm decrease too much in a well-conditioned basis associated with a subblock; in
fact they remain large up to a multiplicative poly(d) factor.",1. Introduction,[0],[0]
Section 4 gives a method for computing a so-called `psubspace embedding of a data matrix in polynomial time.,1. Introduction,[0],[0]
"The space is nγ to obtain dO(1/γ) distortion, for γ ∈ (0, 1) a small constant.",1. Introduction,[0],[0]
This result is then applied to `p-regression which is shown to have a poly(d) approximation factor with the same amount of space.,1. Introduction,[0],[0]
Section 5 describes a deterministic algorithm which gives a poly(k)-approximation to the optimal low rank approximation problem in entrywise `1-norm.,1. Introduction,[0],[0]
It runs in polynomial time for constant k.,1. Introduction,[0],[0]
"This method builds on prior work by derandomizing a subroutine from (Song et al., 2017).",1. Introduction,[0],[0]
"Section 6 describes an algorithm for computing an additiveerror solution to the `∞-regression problem, and shows a corresponding lower bound, showing that relative error solutions in this norm are not possible in sublinear space, even for randomized algorithms.",1. Introduction,[0],[0]
Section 7 concludes with an empirical evaluation.,1. Introduction,[0],[0]
"More experiments, intermediate results, and formal proofs can be found in the Supplementary Material, as can results on approximate matrix multiplication.
",1. Introduction,[0],[0]
Comparison to Related Work.,1. Introduction,[0],[0]
There is a rich literature on algorithms for numerical linear algebra in general p-norms; most of which are randomized with the notable exception of Frequent Directions.,1. Introduction,[0],[0]
"The key contributions of our work for each of the problems considered and its relation to prior work is as follows:
Finding high leverage rows: our algorithm is a single pass streaming algorithm and uses small space.",1. Introduction,[0],[0]
We show that the global property of `p-leverage scores can be understood by considering only local statistics.,1. Introduction,[0],[0]
Frequent Directions is the only comparable result to ours and outputs a summary of the rows only in the `2-norm.,1. Introduction,[0],[0]
"However, our method covers all p ≥ 1.",1. Introduction,[0],[0]
"Theorem 3.3 is the key result and is later used to prove Theorem 6.1 and approximate the `∞-regression problem.
",1. Introduction,[0],[0]
"Subspace embedding, regression and `1 low-rank approximation: various approaches using row-sampling (Cohen & Peng, 2015; Dasgupta et al., 2008), and data oblivious methods such as low-distortion embeddings can solve regression in time proportional to the sparsity of the input matrix (Clarkson et al., 2013; Meng & Mahoney, 2013; Song et al., 2017; Woodruff & Zhang, 2013).",1. Introduction,[0],[0]
"However, despite the attractive running times and error guarantees of these works, they are all randomized and do not necessarily translate well to the streaming model of computation.",1. Introduction,[0],[0]
Our contribution here is a fully deterministic algorithm that works for all p ≥ 1 in both streaming and distributed models.,1. Introduction,[0],[0]
"Randomized methods for `1 low-rank approximation have also been developed in (Song et al., 2017) and our result exploits a derandomized subroutine from this work to obtain a deterministic result which applies in both models.",1. Introduction,[0],[0]
"We consider computing `p-leverage scores of a matrix, lowrank approximation, regression, and matrix multiplication.",2. Preliminaries and Notation,[0],[0]
We assume the input is a matrix A ∈ Rn×d and,2. Preliminaries and Notation,[0],[0]
n d so rank(A) ≤ d and the regression problems are overconstrained.,2. Preliminaries and Notation,[0],[0]
Without loss of generality we may assume that the columns of the input matrix are linearly independent so that rank(A) =,2. Preliminaries and Notation,[0],[0]
"d. Throughout this paper we rely heavily on the notion of a well-conditioned basis for the column space of an input matrix, in the context of the entrywise p-norm which is ‖A‖p = ( ∑ i,j |Aij |p)1/",2. Preliminaries and Notation,[0],[0]
p. Definition 2.1 (Well-conditioned basis).,2. Preliminaries and Notation,[0],[0]
Let A ∈ Rn×d have rank d. For p ∈,2. Preliminaries and Notation,[0],[0]
"[1,∞) let q = pp−1 be its dual norm.",2. Preliminaries and Notation,[0],[0]
"An n× d matrix U is an (α, β, p)-well-conditioned basis for A if the column span of U is equal to that of A, ‖U‖p",2. Preliminaries and Notation,[0],[0]
"≤ α, for all z ∈ Rd, ‖z‖q ≤ β‖Uz‖p , and α, β, dO(1) are independent of n",2. Preliminaries and Notation,[0],[0]
"(Dasgupta et al., 2008).
",2. Preliminaries and Notation,[0],[0]
We focus on the cases p < 2 and p > 2 because the deterministic p = 2 case is relatively straightforward.,2. Preliminaries and Notation,[0],[0]
"Indeed, for p = 2, ATA can be maintained incrementally as rows are added, allowing xTATAx to be computed for any vector x.",2. Preliminaries and Notation,[0],[0]
So it is possible to find an exact `2 subspace embedding using O(d2) space in a stream and O(ndω−1) time (ω is the matrix multiplication constant).,2. Preliminaries and Notation,[0],[0]
"We adopt the convention that when p = 1 we take q =∞. Theorem 2.2 ((Dasgupta et al., 2008)).",2. Preliminaries and Notation,[0],[0]
"Let A be an n× d matrix of rank d, let p ∈",2. Preliminaries and Notation,[0],[0]
"[1,∞)",2. Preliminaries and Notation,[0],[0]
and let q be its dual norm.,2. Preliminaries and Notation,[0],[0]
"There exists an (α, β, p)-well-conditioned basis U for the column space of A such that:
1.",2. Preliminaries and Notation,[0],[0]
"if p < 2 then α = d 1 p+ 1 2 and β = 1, 2.",2. Preliminaries and Notation,[0],[0]
"if p = 2 then α = √ d and β = 1, and 3.",2. Preliminaries and Notation,[0],[0]
if p > 2,2. Preliminaries and Notation,[0],[0]
"then α = d 1 p+ 1 2 and β = d 1 p− 12 .
",2. Preliminaries and Notation,[0],[0]
"Moreover,U can be computed in deterministic timeO(nd2+ nd5 log n) for p 6= 2 and O(nd2) if p = 2.
",2. Preliminaries and Notation,[0],[0]
"We freely use the fact that a well-conditioned basis U = AR can be efficiently computed for the given data matrix A. Details for the computation can be found in (Dasgupta et al., 2008) but this is done by computing a change of basis R such that U = AR is well-conditioned.",2. Preliminaries and Notation,[0],[0]
"Similarly, as R can be inverted we have the relation that UR−1 = A.",2. Preliminaries and Notation,[0],[0]
Both methods are used so we adopt the convention that U = AR when writing a well-conditioned basis in terms of the input and US = A for the input in terms of the basis.,2. Preliminaries and Notation,[0],[0]
Our algorithms operate under the streaming and distributed models of computation.,2.1. Computation Models,[0],[0]
In both settings an algorithm receives as input a matrix A ∈ Rn×d.,2.1. Computation Models,[0],[0]
"For a problem P, the
algorithm must keep a subset of the rows of A and, upon reading the full input, may use a black-box solver to compute an approximate solution to P with only the subset of rows stored.",2.1. Computation Models,[0.9534069384944787],['A hyper distribution would correspond in this case to a distribution over the mean and covariance of P .']
"In both models we measure the summary size (storage), the update time which is the time taken to find the local summary, and the query time which is the time taken to compute an approximation to P using the summary.
",2.1. Computation Models,[0],[0]
The Streaming Model: The rows of A are given to the (centralized) algorithm one-by-one.,2.1. Computation Models,[0],[0]
Let b be the maximum number of rows that can be stored under the constraint that b is sublinear in n.,2.1. Computation Models,[0],[0]
The stored subset is used to compute local statistics which determine those rows to be kept or discarded from the stored set.,2.1. Computation Models,[0],[0]
Further rows are then appended and the process is repeated until the full matrix has been read.,2.1. Computation Models,[0],[0]
"An approximation to the problem is then computed by solving P on the reduced subset of rows.
",2.1. Computation Models,[0],[0]
The Distributed Summary Model:,2.1. Computation Models,[0],[0]
"Given a small constant γ ∈ (0, 1), the input in the form of matrix A ∈ Rn×d is partitioned into blocks among distributed compute nodes so that no block exceeds nγ rows.",2.1. Computation Models,[0],[0]
The computation then follows a tree structure: the initial blocks of the matrix form n1−γ leaves of the compute tree.,2.1. Computation Models,[0],[0]
Each internal node merges and reduces its input from its child nodes.,2.1. Computation Models,[0],[0]
"The first phase is for the leaf nodes l1, . . .",2.1. Computation Models,[0],[0]
", lm of the tree to reduce their input by computing a local summary on the block they receive as input.",2.1. Computation Models,[0],[0]
"This is then sent to parent nodes p1, . . .",2.1. Computation Models,[0],[0]
", pm which merge and reduce the received rows until the space bound is reached.",2.1. Computation Models,[0],[0]
"The resulting summaries are passed up the tree until we reach the root where a single summary of bounded size is obtained which can be used to compute an approximation to P. In total, there are O(1/γ) levels in the tree.",2.1. Computation Models,[0],[0]
"As the methods require only light synchronization (compute summary and return to coordinator), we do not model implementation issues relating to synchronization.",2.1. Computation Models,[0],[0]
Remark 2.3.,2.1. Computation Models,[0],[0]
"The two models are quite close: the streaming model can be seen as a special case of the distributed model with only one participant who individually computes a summary, appends rows to the stored set, and reduces the new summary.",2.1. Computation Models,[0],[0]
"This is represented as a deep binary tree, where each internal node has one leaf child.",2.1. Computation Models,[0],[0]
"Likewise, the Distributed Summary Model can be implemented in a full streaming fashion over the entire binary tree.",2.1. Computation Models,[0],[0]
The experiments in Section 7 perform one round of merge-and-reduce in the distributed model to simulate the streaming approach.,2.1. Computation Models,[0],[0]
This section is concerned with finding rows of high leverage from a matrix with respect to various p-norms.,3. Finding Rows of High Leverage,[0],[0]
We conclude the section with an algorithm that returns rows of high leverage up to polynomial additive error.,3. Finding Rows of High Leverage,[0],[0]
Definition 3.1.,3. Finding Rows of High Leverage,[0],[0]
"Let R be a change of basis matrix such that AR is a well-conditioned basis for the column space of A.
The (full) `p-leverage scores are defined as wi = ‖eTi AR‖pp.
Note that wi depends both on A and the choice of R, but we suppress this dependence in our notation.",3. Finding Rows of High Leverage,[0],[0]
Next we present some basic facts about the `p leverage scores.,3. Finding Rows of High Leverage,[0],[0]
Fact 1.,3. Finding Rows of High Leverage,[0],[0]
"By Definition 2.1 we have ∑ i wi =∑
i ‖(AR)i‖pp ≤ αp.",3. Finding Rows of High Leverage,[0],[0]
Theorem 2.2 shows α = poly(d).,3. Finding Rows of High Leverage,[0],[0]
Define I,3. Finding Rows of High Leverage,[0],[0]
= {i ∈,3. Finding Rows of High Leverage,[0],[0]
"[n] : wi > τ‖AR‖pp} to be the index set of all rows whose `p leverage exceeds a τ fraction of ‖AR‖pp, then: αp ≥ ∑ i wi ≥ ∑ i∈I wi ≥ |I| · τ‖AR‖pp.",3. Finding Rows of High Leverage,[0],[0]
"Hence, |I| ≤ αp/τ‖AR‖pp = poly(d)/τ .",3. Finding Rows of High Leverage,[0],[0]
So there are at most poly(d)/τ rows i for which wi ≥ τ‖AR‖pp.,3. Finding Rows of High Leverage,[0],[0]
Fact 2.,3. Finding Rows of High Leverage,[0],[0]
Definition 2.1 and Hölder’s inequality show that for any vector x we have |(ARx)i|p ≤ β‖eTi,3. Finding Rows of High Leverage,[0],[0]
AR‖pp · ‖ARx‖pp.,3. Finding Rows of High Leverage,[0],[0]
Then τ ≤ |eTi ARx|p/‖ARx‖pp ≤ βwi.,3. Finding Rows of High Leverage,[0],[0]
From this we deduce that if a row contributes at least a τ fraction of ‖ARx‖pp then τ ≤ wiβ.,3. Finding Rows of High Leverage,[0],[0]
"That is, τ ≤ wi for p ∈",3. Finding Rows of High Leverage,[0],[0]
"[1, 2] and τ ≤ d1/2wi for p ∈ (2,∞) by using Theorem 2.2.",3. Finding Rows of High Leverage,[0],[0]
Definition 3.2.,3. Finding Rows of High Leverage,[0],[0]
Let X be a matrix and Y be a subset of the rows of X .,3. Finding Rows of High Leverage,[0],[0]
"Define the local `p-leverage scores of Y with respect to X to be the leverage scores of rows Y found by computing a well-conditioned basis for Y rather than the whole matrix X .
",3. Finding Rows of High Leverage,[0],[0]
A key technical insight to proving Theorem 3.3 below is that rows of high leverage globally can be found by repeatedly finding rows of local high leverage.,3. Finding Rows of High Leverage,[0],[0]
"While relative `p row norms of a submatrix are at least as large as the full relative `p norms, it is not guaranteed that this property holds for leverage scores.",3. Finding Rows of High Leverage,[0],[0]
This is because leverage scores are calculated from a well-conditioned basis for a matrix which need not be a well-conditioned basis for a block.,3. Finding Rows of High Leverage,[0],[0]
"However, we show that local `p leverage scores restricted to a coordinate subspace of a matrix basis do not decrease too much when compared to leverage scores in the original space.",3. Finding Rows of High Leverage,[0],[0]
Let i be a row in A with local leverage score ŵi and global leverage score wi.,3. Finding Rows of High Leverage,[0],[0]
Then ŵi ≥ wi/ poly(d).,3. Finding Rows of High Leverage,[0],[0]
"The proof relies heavily on properties of the well-conditioned basis and details are given in the Supplementary Material, Lemma A.1.",3. Finding Rows of High Leverage,[0],[0]
"This lemma shows that local leverage scores can potentially drop in arbitrary `p norm, contrasting the behavior in `2.",3. Finding Rows of High Leverage,[0],[0]
"However, it is possible to find all rows exceeding a threshold globally by altering the local threshold.",3. Finding Rows of High Leverage,[0],[0]
"That is, to find all wi > τ",3. Finding Rows of High Leverage,[0],[0]
globally we can find all local leverage scores exceeding an adjusted threshold ŵi > τ/poly(d) to obtain a superset of all rows which exceed the global threshold.,3. Finding Rows of High Leverage,[0],[0]
"The price to pay for this is a poly(d) increase in space cost which, importantly, remains sublinear in n.",3. Finding Rows of High Leverage,[0],[0]
"Hence, we can gradually prune out rows of small leverage and keep only the most important rows of a matrix.",3. Finding Rows of High Leverage,[0],[0]
"Combining Lemmas A.1 and A.2 we can present the main theorem of the section.
",3. Finding Rows of High Leverage,[0],[0]
"We prove Theorem 3.3 by arguing the correctness of Algorithm 1 which reads A once only, row by row, and so
operates in the streaming model of computation as follows.",3. Finding Rows of High Leverage,[0],[0]
Let A′ be the submatrix of A induced by the b block of poly(d)/τ rows.,3. Finding Rows of High Leverage,[0],[0]
"Upon storing A′, we compute U , a local well-conditioned basis for A′ and the local leverage scores with respect to U , ŵi(U) are calculated.",3. Finding Rows of High Leverage,[0],[0]
"Now, the local and global leverage scores can be related by Lemma A.1 as wi/poly(d) ≤",3. Finding Rows of High Leverage,[0],[0]
ŵi so we can decide which rows to keep using an adjusted threshold.,3. Finding Rows of High Leverage,[0],[0]
Any i for which the local leverage exceeds the adjusted threshold is kept in the sample and all other rows are deleted.,3. Finding Rows of High Leverage,[0],[0]
The sample cannot be too large by properties of the well-conditioned basis and leverage scores so these kept rows can be appended to the next block which is read in before computing another well-conditioned basis and repeating in the same fashion.,3. Finding Rows of High Leverage,[0],[0]
The proof of Theorem 3.3 is deferred to Appendix A. Theorem 3.3.,3. Finding Rows of High Leverage,[0],[0]
Let τ > 0 be a fixed constant and let b denote a bound on the available space.,3. Finding Rows of High Leverage,[0],[0]
"There exists a deterministic algorithm, namely, Algorithm 1, which computes the `p-leverage scores of a matrix A ∈ Rn×d with O(bd2 + bd5 log b) update time, poly(d) space, and returns all rows of A with `p leverage score satisfying wi ≥ τ/ poly(d).
4.",3. Finding Rows of High Leverage,[0],[0]
`p-Subspace Embeddings,3. Finding Rows of High Leverage,[0],[0]
Under the assumptions of the Distributed Summary Model we present an algorithm which computes an `p-subspace embedding.,3. Finding Rows of High Leverage,[0],[0]
"By extension, this applies to both the distributed and streaming models of computation as described in Section 2.1.",3. Finding Rows of High Leverage,[0],[0]
Two operations are needed for this model of computation: the merge and reduce steps.,3. Finding Rows of High Leverage,[0],[0]
To reduce the input at each level a summary is computed by taking a block of input B (corresponding to a leaf node or a node higher up the tree) and computing a well-conditioned basis B = US.,3. Finding Rows of High Leverage,[0],[0]
"In particular, the summary is now the matrix S with U and B deleted.",3. Finding Rows of High Leverage,[0],[0]
"For the merge step, successive matrices S are concatenated until the space requirement is met.",3. Finding Rows of High Leverage,[0],[0]
A further reduce step takes as input this concatenated matrix and the process is repeated.,3. Finding Rows of High Leverage,[0],[0]
"Further details, pseudocode, and proofs for this section are given in Appendix B. Definition 4.1.",3. Finding Rows of High Leverage,[0],[0]
"A matrix T is a relative error (c1, c2)-`p subspace embedding for the column space of a matrix A ∈ Rn×d",3. Finding Rows of High Leverage,[0],[0]
"if there are constants c1, c2 > 0",3. Finding Rows of High Leverage,[0],[0]
"so that for all x ∈ Rd, c1‖Ax‖p ≤ ‖Tx‖p ≤ c2‖Ax‖p.",3. Finding Rows of High Leverage,[0],[0]
Theorem 4.2.,3. Finding Rows of High Leverage,[0],[0]
"Let A ∈ Rn×d, p 6= 2,∞ be fixed and fix a constant γ ∈ (0, 1).",3. Finding Rows of High Leverage,[0],[0]
"Then there exists a one-pass deterministic algorithm which constructs a (1/dO(1/γ), 1) relative error `p-subspace embedding in with O(nγd2 +nγd5 log nγ) update time and O(nγd) space in the streaming and distributed models of computation.
",3. Finding Rows of High Leverage,[0],[0]
"The algorithm is used in a tree structure as follows: split inputA ∈ Rn×d into n1−γ blocks of size nγ , these form the leaves of the tree.",3. Finding Rows of High Leverage,[0],[0]
"For each block, a well-conditioned basis is
computed and the change of basis matrix S ∈ Rd×d is stored and passed to the next level of the tree.",3. Finding Rows of High Leverage,[0],[0]
This is repeated until the concatenation of all the S matrices would exceed nγ .,3. Finding Rows of High Leverage,[0],[0]
"At this point, the concatenated S matrices form the parent node of the leaves in the tree and the process is repeated upon this node: this is the merge and reduce step of the algorithm.",3. Finding Rows of High Leverage,[0],[0]
"At every iteration of the merge-and-reduce steps it can be shown that a distortion of 1/d is introduced by using the summaries S. However, this can be controlled across all of the O(1/γ) levels in the tree to give a deterministic relative error `p subspace embedding which requires only sublinear space and little communication.",3. Finding Rows of High Leverage,[0],[0]
"In addition, the subspace embedding can be used to achieve a deterministic relativeerror approximate regression result.",3. Finding Rows of High Leverage,[0],[0]
"The proof relies upon analyzing the merge-and-reduce behaviour across all nodes of the tree.
",3. Finding Rows of High Leverage,[0],[0]
`p-Regression Problem:,3. Finding Rows of High Leverage,[0],[0]
"Given matrix A ∈ Rn×d and target vector b ∈ Rn, find x̂ = argminx ‖Ax− b‖p.",3. Finding Rows of High Leverage,[0],[0]
Theorem 4.3.,3. Finding Rows of High Leverage,[0],[0]
"Let A ∈ Rn×d, b ∈ Rn, fix p 6= 2,∞ and a constant γ > 0.",3. Finding Rows of High Leverage,[0],[0]
The `p-regression problem can be solved deterministically in the streaming and distributed models with a (d + 1)O(1/γ) = poly(d) relative error approximation factor.,3. Finding Rows of High Leverage,[0],[0]
The update time is poly(nγ(d + 1)) and O((1/γ)nγ(d + 1)) storage.,3. Finding Rows of High Leverage,[0],[0]
The query time is poly(nγ) for the cost of convex optimization.,3. Finding Rows of High Leverage,[0],[0]
`1-Low-Rank Approximation Problem:,5. Low-Rank Approximation,[0],[0]
"Given matrix A ∈ Rn×d output a matrix B of rank k s.t., for constant k:
‖A−B‖1 ≤ poly(k) min A′:rankk ‖A−A′‖1.",5. Low-Rank Approximation,[0],[0]
"(1)
Theorem 5.1.",5. Low-Rank Approximation,[0],[0]
Let A ∈ Rn×d be the given data matrix and k be the (constant) target rank.,5. Low-Rank Approximation,[0],[0]
Let γ > 0 be an arbitrary (small) constant.,5. Low-Rank Approximation,[0],[0]
"Then there exists a deterministic distributed and streaming algorithm (namely Algorithm 5 in Appendix C) which can output a solution to
the `1-Low Rank Approximation Problem with relative error poly(k) approximation factor, update time poly(n, d), space bounded by nγpoly(d), and query time poly(n, d).
",5. Low-Rank Approximation,[0],[0]
The key technique is similar to that of the previous section by using a tree structure with merge-and-reduce operations.,5. Low-Rank Approximation,[0],[0]
For input A ∈ Rn×d and constant γ > 0 partition A into n1−γ groups of rows which form the leaves of the tree.,5. Low-Rank Approximation,[0],[0]
"The tree is defined as previously with the same ‘merge’ operation, but the ‘reduce’ step to summarize the data exploits a derandomization (subroutine Algorithm 4) of (Song et al., 2017) to compute an approximation to the optimal `1-lowrank approximation.",5. Low-Rank Approximation,[0],[0]
"Once this is computed, k of the rows in the summary are kept for later merge steps.
",5. Low-Rank Approximation,[0],[0]
This process is continued with the successive k rows from nγ rows being ‘merged’ or added to the matrix until it has nγ rows.,5. Low-Rank Approximation,[0],[0]
"The process is repeated across all of the groups in the level and again on the successive levels on the tree from which it can be shown that the error does not propagate too much over the tree, thus giving the desired result.",5. Low-Rank Approximation,[0],[0]
Here we present a method for solving `∞-regression in a streaming fashion.,6. Application: `∞-Regression,[0],[0]
"Given input A and a target vector b, it is possible to achieve additive approximation error of the form ε‖b‖p for arbitrarily large p.",6. Application: `∞-Regression,[0],[0]
This contrasts with both Theorems 4.2 and 4.3 which achieve a relative error poly(d) approximation.,6. Application: `∞-Regression,[0],[0]
Both of these theorems require that p is constant and not equal to the ∞-norm.,6. Application: `∞-Regression,[0],[0]
This restriction is due to a lower bound for `∞- regression showing that it cannot be approximated with relative error in sublinear space.,6. Application: `∞-Regression,[0],[0]
"The key to proving Theorem 6.1 below is using Theorem 3.3 to find high leverage rows and arguing that these are sufficient to give the claimed error guarantee.
",6. Application: `∞-Regression,[0],[0]
The `∞-regression problem has been previously studied in the overdetermined case and can naturally be applied to curve-fitting under this norm.,6. Application: `∞-Regression,[0],[0]
"`∞-regression can be solved
by linear programming (Sposito, 1976) and such a transformation allows the identification of outliers in the data.",6. Application: `∞-Regression,[0],[0]
"Also, if the errors are known to be distributed uniformly across an interval then `∞-regression estimator is the maximumlikelihood parameter choice (Hand, 1978).",6. Application: `∞-Regression,[0],[0]
The same work argues that such uniform distributions on the errors often arise as round-off errors in industrial applications whereby the error is controlled or is small relative to the signal.,6. Application: `∞-Regression,[0],[0]
"There are further applications such as using `∞-regression to remove outliers prior to `2 regression in order to make the problem more robust (Shen et al., 2014).",6. Application: `∞-Regression,[0],[0]
By applying `∞ regression on subsets of the data an approximation to the Least Median of Squares (another robust form of regression) can be found.,6. Application: `∞-Regression,[0],[0]
"We now define the problem and proceed to show that it is possible to compute an approximate solution with additive error in `p-norm for arbitrarily large p.
Approximate `∞-Regression problem:",6. Application: `∞-Regression,[0],[0]
"Given data A ∈ Rn×d, target vector b ∈ Rn, and error parameter ε > 0, compute an additive ε‖b‖p error solution to:
min x∈Rd ‖Ax− b‖∞ = min x∈Rd",6. Application: `∞-Regression,[0],[0]
"[ max i |(Ax)i − bi| ] .
",6. Application: `∞-Regression,[0],[0]
Theorem 6.1.,6. Application: `∞-Regression,[0],[0]
"Let A ∈ Rn×d, b ∈ Rn and fix constants p ≥ 1, ε > 0 with p 6=∞. There exists a one-pass deterministic streaming algorithm which solves the `∞-regression problem up to an additive ε‖b‖p error in dO(p)/εO(1) space, O(md5 + md2 logm) update time and Tsolve(m, d) query time.
",6. Application: `∞-Regression,[0],[0]
"Note that Tsolve(m, d) query time is the time taken to solve the linear program associated with the above problem on a reduced instance size.",6. Application: `∞-Regression,[0],[0]
"Also, observe that Theorem 6.1 requires p < ∞.",6. Application: `∞-Regression,[0],[0]
This restriction is necessary to forbid relative error with respect to the infinity norm.,6. Application: `∞-Regression,[0],[0]
"Indeed, p can be an arbitrarily large constant, but for p = ∞ we can look for rows above an ε/poly(d) threshold in the case when A is an all-ones column n-vector (so an n× 1 matrix).",6. Application: `∞-Regression,[0],[0]
Then ‖Ax‖∞ = ‖x‖∞ since x is a scalar.,6. Application: `∞-Regression,[0],[0]
"Also, A is a wellconditioned basis for its own column span but the number of rows of leverage exceeding ε/poly(d) = ε is n for a small constant ε.",6. Application: `∞-Regression,[0],[0]
"This intuition allows us to prove the following theorem.
",6. Application: `∞-Regression,[0],[0]
Theorem 6.2.,6. Application: `∞-Regression,[0],[0]
"Any algorithm which outputs an ε‖b‖∞ relative error solution to the `∞-regression problem requires min { n, 2Ω(d) } space.",6. Application: `∞-Regression,[0],[0]
"To validate our approach, we evaluate the use of high `p-leverage rows in order to approximate `∞-regression1, focusing particularly on the cases using `1 and `2 well-
1Code available at https://github.com/c-dickens/ stream-summaries-high-lev-rows
conditioned bases.",7. Experimental Evaluation,[0],[0]
It is straightforward to model `∞- regression as a linear program in the offline setting.,7. Experimental Evaluation,[0],[0]
We use this to measure the accuracy of our algorithm.,7. Experimental Evaluation,[0],[0]
"The implementation is carried out in the single pass streaming model with a fixed space constraint, m, and threshold, αp/m for both conditioning methods to ensure the number of rows kept in the summary did not exceed m. Recall from Remark 2.3 that the single-pass streaming implementation is equivalent to the distributed model with only one participant applying merge-and-reduce, so this experiment can also be seen as a distributed computation with the merge step being the appending of new rows and the reduce step being the thresholding in the new well-conditioned basis.
",7. Experimental Evaluation,[0],[0]
Methods.,7. Experimental Evaluation,[0],[0]
We analyze two instantiations of our methods based on how we find a well-conditioned basis and repeat over 5 independent trials with random permutations of the data.,7. Experimental Evaluation,[0],[0]
"The methods are as follows:
SPC3: We use an algorithm of Yang et al. (2013) to compute an `1-wcb.",7. Experimental Evaluation,[0],[0]
"This method is randomized as it employs the Sparse Cauchy Transform and is only an `1-wellconditioned basis with constant probability We also implemented a check condition which showed that almost always, roughly 99% of the time, the randomized construction SPC3 would return a (d2.5, 1, 1)-well-conditioned basis.",7. Experimental Evaluation,[0],[0]
"Thus, we bypassed this check in our experiment to ensure quick update times.
",7. Experimental Evaluation,[0],[0]
Orth:,7. Experimental Evaluation,[0],[0]
"In addition, we also used an orthonormal basis using the QR decomposition which is an `2-wcb.",7. Experimental Evaluation,[0],[0]
"This method is fully deterministic and outputs a ( √ d, 1, 2)-well- conditioned basis.
",7. Experimental Evaluation,[0],[0]
"Sample: A sample of the data is chosen uniformly at random and the retained summary has size exactly m.
Identity: No conditioning is performed.",7. Experimental Evaluation,[0],[0]
"For a block B of the input, the surrogate scores wi(B) =",7. Experimental Evaluation,[0],[0]
‖eTi B‖22/‖B‖2F are used to determine which rows to keep.,7. Experimental Evaluation,[0],[0]
"As the sum of these wi(B) is 1, we keep all rows which have wi(B) > 2/m. Since no more than m/2 of the rows can satisfy wi(B)",7. Experimental Evaluation,[0],[0]
"> 2/m, the size of the stored subset of rows can be controlled and cannot grow too large.
",7. Experimental Evaluation,[0],[0]
Remark 7.1.,7. Experimental Evaluation,[0],[0]
The Identity method keeps only the rows with high norm which contrasts our conditioning approach: if most of the mass of the block is concentrated on a few rows then these will appear heavy locally despite the possibility that they may correspond to previously seen or unimportant directions.,7. Experimental Evaluation,[0],[0]
"In particular, if these heavy rows significantly outweigh the weight of some sparse directions in the data it is likely that the sparse directions will not be found at all.",7. Experimental Evaluation,[0],[0]
"For instance, consider data X ∈ Rn×d which is then augmented by appending the identity (and zeros) so that these are the only vectors in the new directions.",7. Experimental Evaluation,[0],[0]
"That is, set X ′ =",7. Experimental Evaluation,[0],[0]
"[X,0n×k;0k×d, Ik×k] and then
permute the rows of X ′. The appended sparse vectors from Ik×k will have leverage of 1 so will be detected by the wellconditioned basis methods.",7. Experimental Evaluation,[0],[0]
However there is no guarantee that the Identity method will identify these directions if the entries in X significantly outweigh those in Ik×k.,7. Experimental Evaluation,[0],[0]
"In addition, there is also no guarantee that using uniform sampling will identify these points, particularly when k is small compared to n and d.",7. Experimental Evaluation,[0],[0]
"So while choosing to do no conditioning seems attractive, this example shows that doing so may not give any meaningful guarantees and hence we prefer the approach in Section 3.",7. Experimental Evaluation,[0],[0]
"We compare only to these baselines as we are not aware of any other competing methods in the small memory regime for the `∞-regression problem.
Datasets.",7. Experimental Evaluation,[0],[0]
"We tested the methods on a subset of the US Census Data containing 5 million rows and 11 columns2 and YearPredictionMSD3 which has roughly 500,000 rows and 90 columns (although we focus on a fixed 50,000 row sample so that the LP for regression is tractable: see Figure 4c in the Supplementary Material, Appendix F).",7. Experimental Evaluation,[0],[0]
"For the census dataset, space constraints between 50,000 and 500,000 rows were tested and for the YearPredictionsMSD data space budgets were tested between 2,500 and 25,000.",7. Experimental Evaluation,[0],[0]
"The general behavior is roughly the same for both datasets so for brevity we primarily show the results for US Census Data, and defer corresponding plots for YearPredictionsMSD to Appendix F.
Results on approximation error compared to storage Let f∗ denote the minimal value of the full regression obtained by x∗ and let x′ be the output of the reduced problem.",7. Experimental Evaluation,[0],[0]
The approximate solution to the full problem is then f̂ = ‖Ax′,7. Experimental Evaluation,[0],[0]
− b‖∞ and approximation error is measured as f̂/f∗ − 1 (note that f̂ ≥ f∗).,7. Experimental Evaluation,[0],[0]
An error closer to 0 demonstrates that f̂ is roughly the same as f∗ so the optimal value is well-approximated.,7. Experimental Evaluation,[0],[0]
Figures 2a and 2b show that on both datasets the Identity method consistently performs poorly while Sample achieves comparable accuracy to the conditioning methods.,7. Experimental Evaluation,[0],[0]
"Despite the simplicity of uniform sampling to keep a summary, the succeeding sections discuss the increased time and space costs of using such a sample and show that doing so is not favourable.",7. Experimental Evaluation,[0],[0]
"Thus, neither of the baseline methods output a summary which can be used to approximate the regression problem both accurately and quickly, hence justifying our use of leverage scores.",7. Experimental Evaluation,[0],[0]
"Our conditioning methods perform particularly well in the US Census Data data (Figure 2a) with Orth appearing to give the most accurate summary and SPC3 performing comparably well but with slightly more fluctuation: similar behaviour is observed in the YearPredictionMSD
2http://www.census.gov/census2000/PUMS5.",7. Experimental Evaluation,[0],[0]
"html
3https://archive.ics.uci.edu/ml/datasets/ yearpredictionmsd
(Figure 2b) data too.",7. Experimental Evaluation,[0],[0]
"The conditioning methods are also seen to be robust to the storage constraint, give accurate performance across both datasets using significantly less storage than sampling, and give a better estimate in general than doing no conditioning.
",7. Experimental Evaluation,[0],[0]
Results on Space Complexity.,7. Experimental Evaluation,[0],[0]
"Recall that the space constraint is m rows and throughout the stream, after a local computation, the merge step concatenates more rows to the existing summary until the bound m is met, prior to computing the next reduction.",7. Experimental Evaluation,[0],[0]
"During the initialization of the block A′ by Algorithm 1, the number of stored rows is exactly m.",7. Experimental Evaluation,[0],[0]
"However, we measure the maximum number of rows kept in a summary after every reduction step to understand how large the returned summary can grow.",7. Experimental Evaluation,[0],[0]
"As seen in Figure 2c, Identity keeps the smallest summary but there is no reason to expect it has kept the most important rows.",7. Experimental Evaluation,[0],[0]
"In contrast, if m is the bound on the summary size, then uniform sampling always returns a summary of size exactly m. However, we see that this is not optimal as both conditioning methods can return a set of rows which are pruned at every iteration to roughly half the size and contains only the most important rows in that block.",7. Experimental Evaluation,[0],[0]
Both conditioning methods exhibit similar behavior and are bounded between both Sample and Identity methods.,7. Experimental Evaluation,[0],[0]
"Therefore, both of the conditioning methods respect the theoretical bound and, crucially, return a summary which is sublinear in the space constraint and hence a significantly smaller fraction of the input size.
Results on Time Complexity.",7. Experimental Evaluation,[0],[0]
There are three time costs measured.,7. Experimental Evaluation,[0],[0]
The first is the update time taken to compute the local well-conditioned basis which is theoretically O(md2 +md5 logm) by Theorem 2.2.,7. Experimental Evaluation,[0],[0]
"However, the two bases that we test are an orthonormal basis, computable in time O(md2) and the SPC3 transform which takes time O(nnz(B) logm) for a block B with m rows and nnz(B) non-zero entries.",7. Experimental Evaluation,[0],[0]
Figure 3a demonstrates that SPC3 is faster than Orth on this data in practice but this small absolute difference becomes negligible over the entirety of the stream as seen in Figure 3c.,7. Experimental Evaluation,[0],[0]
The query time in Figure 3b is roughly proportional to the summary size in all instances but here the conditioning methods perform noticeably better due to the smaller summary size that is returned as discussed in the previous section.,7. Experimental Evaluation,[0],[0]
"However, as seen in Figure 4c, (Supplementary Material, Appendix F ) this disparity becomes hugely significant on higher dimensionality data due to the increased size summary retained by sampling, further justifying our approach of pruning rows at every stage.",7. Experimental Evaluation,[0],[0]
"While Identity appears to have fast query time, this is due to the summary being smaller.",7. Experimental Evaluation,[0],[0]
"Although it may seem that for smaller summaries more local bases need to be computed and this time could prohibitively increase over the stream, Figure 3c demonstrates that even using small blocks does not cause the overall time (to process the stream and pro-
duce an approximate query) to increase too much.",7. Experimental Evaluation,[0],[0]
"Hence, an approximation can be obtained which is highly accurate, and in total time faster than the brute force solver.
",7. Experimental Evaluation,[0],[0]
Experimental Summary.,7. Experimental Evaluation,[0],[0]
"While it might seem attractive not to perform any conditioning on the matrix and just pick heavy rows, our experiments show that this strategy is not effective in practice, and delivers poor accuracy.",7. Experimental Evaluation,[0],[0]
"Although a simple sample of randomly chosen rows can be easily maintained, this appears less useful due to the increased time costs associated with larger summaries when conditioning methods output a similar estimate in less time over the entire stream.",7. Experimental Evaluation,[0],[0]
As the `∞-regression problems depend only on a few rows of the data there are cases when uniform sampling can perform well: if many of the critical rows look similar then there is a chance that uniform sampling will select some examples.,7. Experimental Evaluation,[0],[0]
"In this case, the leverage of the important direction is divided across the repetitions, and so it is harder to ensure that desired direction is identified.",7. Experimental Evaluation,[0],[0]
Despite this potential drawback we have shown that both Orth and SPC3 can be used to find accurate summaries which perform robustly across each of the measures we have tested.,7. Experimental Evaluation,[0],[0]
"It appears that SPC3 performs comparably
to Orth; both are relatively quick to compute and admit accurate summaries in similar space.",7. Experimental Evaluation,[0],[0]
"In particular, both conditioning methods return summaries which are a fraction of the space budget and hence highly sublinear in the input size, which give accurate approximations and are robust to the concatenation of new rows.",7. Experimental Evaluation,[0],[0]
"All of these factors make the conditioning method fast in practice to both find the important rows in the data and then compute the reduced regression problem with high accuracy.
",7. Experimental Evaluation,[0],[0]
"Due to the problems in constructing summaries which can be used to solve regression quickly and accurately when using random sampling or no transformation, our methods are shown to be efficient and accurate alternatives.",7. Experimental Evaluation,[0],[0]
Our approach is vindicated both theoretically and practically: this is most clear in the U.S. Census dataset where small error can be achieved using a summary roughly 2% the size of the data.,7. Experimental Evaluation,[0],[0]
This also results in an overall speedup as solving the optimization on the reduced set is much faster than solving on the full problem.,7. Experimental Evaluation,[0],[0]
Such significant savings show that this general approach can be useful in large-scale applications.,7. Experimental Evaluation,[0],[0]
The work of G. Cormode and C. Dickens is supported by European Research Council grant ERC-2014-CoG 647557 and The Alan Turing Institute under the EPSRC grant EP/N510129/1.,Acknowledgements,[0],[0]
D. Woodruff would like to acknowledge the support by the National Science Foundation under Grant No.,Acknowledgements,[0],[0]
CCF-1815840.,Acknowledgements,[0],[0]
"Work on approximate linear algebra has led to efficient distributed and streaming algorithms for problems such as approximate matrix multiplication, low rank approximation, and regression, primarily for the Euclidean norm `2.",abstractText,[0],[0]
"We study other `p norms, which are more robust for p < 2, and can be used to find outliers for p > 2.",abstractText,[0],[0]
"Unlike previous algorithms for such norms, we give algorithms that are (1) deterministic, (2) work simultaneously for every p ≥ 1, including p =∞, and (3) can be implemented in both distributed and streaming environments.",abstractText,[0],[0]
"We apply our results to `p-regression, entrywise `1-low rank approximation, and approximate matrix multiplication.",abstractText,[0],[0]
Leveraging Well-Conditioned Bases: Streaming and Distributed Summaries in Minkowski p-Norms,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 50–59, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Much attention has recently been paid to non-task-oriented dialogue systems —or chatoriented dialogue systems— both in research (Higashinaka et al., 2014; Yu et al., 2016) and in industry.",1 Introduction,[0],[0]
"In addition to pure chat-oriented systems, some task-oriented dialogue systems can engage in chat-oriented dialogues (Lee et al., 2009; Dingli and Scerri, 2013; Kobori et al., 2016; Papaioannou and Lemon, 2017) because such dialogues are expected to build rapport (Bickmore and Picard, 2005) between users and systems.",1 Introduction,[0],[0]
"For simplicity, we will call any system that can engage in chat-oriented dialogue a chat-
bot.",1 Introduction,[0],[0]
"Since an open-domain chatbot that always generates appropriate utterances is still difficult to build (Higashinaka et al., 2015), we think it is worth building a closed-domain chatbot, which tries to continue dialogues in a specific domain.
",1 Introduction,[0],[0]
"One problem in building closed-domain chatbots is that, although they should preferably have comprehensive lexical knowledge in their domains, all the knowledge cannot realistically be prepared in advance.",1 Introduction,[0],[0]
"Therefore, we must consider the case where a user uses terms outside of the system’s vocabulary1, i.e. terms that have ontological categories the system does not know.",1 Introduction,[0],[0]
"If the system can acquire the term’s category during dialogues, it will be able to interact with users more naturally and the cost of expanding its knowledge base will be reduced.
",1 Introduction,[0],[0]
We call the problem of acquiring the category of an unknown term lexical acquisition.,1 Introduction,[0],[0]
"If the system can predict the category of an unknown term, it can ask the user if it is correct (Otsuka et al., 2013; Komatani et al., 2016).",1 Introduction,[0],[0]
"However, repeating such explicit confirmation requests can degrade the user experience in chat-oriented dialogues2.",1 Introduction,[0],[0]
We therefore need to find a way to enable chatbots to: (1) interact with the user naturally and (2) acquire lexical information.,1 Introduction,[0],[0]
"To solve this dilemma, we proposed an approach using implicit confirmation (Ono et al., 2016), where the system makes a confirmation request about the predicted category and uses the user’s response to decide if the category is correct or not.",1 Introduction,[0],[0]
"However, whether such an approach is really possible or not has not been well studied.
",1 Introduction,[0],[0]
"This paper proposes a method that utilizes im-
1Here, we use term to mean an expression denoting an entity that can be in the knowledge base.",1 Introduction,[0],[0]
"A term may consist of multiple words.
",1 Introduction,[0],[0]
2Some typical examples will be shown in Section 2.,1 Introduction,[0],[0]
"We will verify this intuition by conducting a user study.
50
plicit confirmation dialogues from multiple users to increase the accuracy for determining if the predicted category is correct or not3.",1 Introduction,[0],[0]
The system estimates the confidence score that the category prediction is correct from the responses of multiple users to the same implicit confirmation requests (Figure 1: right).,1 Introduction,[0],[0]
Our proposed method has the goal of improving the confidence score estimation by using implicit confirmation sub-dialogues with multiple users.,1 Introduction,[0],[0]
Then the system can determine if it should add the lexical information to the system’s knowledge.,1 Introduction,[0],[0]
"For a sub-task, we consider the problem of estimating how likely the predicted category is to be correct from implicit confirmation sub-dialogues with one user (Figure 1: left).
",1 Introduction,[0],[0]
It is reasonable to assume that the system can make confirmation requests about the same unknown term with different users because chatbots typically run on servers so they can share interaction logs for different users.,1 Introduction,[0],[0]
"Furthermore, it is difficult to ask a single user to respond to confirmation requests with the same predicted category many times, so collecting responses from multiple users is desirable.
",1 Introduction,[0],[0]
This paper is organized as follows.,1 Introduction,[0],[0]
The problem settings and related work are discussed in the next two sections.,1 Introduction,[0],[0]
Section 4 describes the proposed method to determine correct categories in implicit confirmation requests on the basis of multiple implicit confirmation sub-dialogues with different users.,1 Introduction,[0],[0]
"Sections 5 and 6 show the data collection by crowdsourcing and several results as preparation for the main experimental evaluation of the proposed method, which is detailed in Section 7.",1 Introduction,[0],[0]
"Section 8 concludes this paper and discusses future work.
",1 Introduction,[0],[0]
"3We do not deal with multi-party dialogues but utilize the interaction logs of two-party dialogues with different users.
",1 Introduction,[0],[0]
"(a) explicit, correct",1 Introduction,[0],[0]
This section describes the problem we address in this paper in detail.,2 Problem Setting,[0],[0]
"We are building a closeddomain Japanese language chatbot targeting the food and restaurant domain, so we use examples in this domain throughout this paper.",2 Problem Setting,[0],[0]
"In this domain, the problem is to acquire the categories of foods that the system does not know.",2 Problem Setting,[0],[0]
"We assume that the system can identify a food name in the user’s input even if it is not in the system’s vocabulary by using methods such as named entity recognition (Mesnil et al., 2015).",2 Problem Setting,[0],[0]
"Note that in this paper we also assume the category of an unknown term is predicted with an existing method (Otsuka et al., 2013; Ono et al., 2016).",2 Problem Setting,[0],[0]
"We do not assume any ontological structure of foods.
",2 Problem Setting,[0],[0]
This paper focuses on deciding if the predicted category of unknown terms is correct or not in dialogues.,2 Problem Setting,[0],[0]
"To this end, methods for generating explicit confirmation have been proposed.",2 Problem Setting,[0],[0]
Otsuka et al. (2013) proposed lexical acquisition methods that explicitly ask the user questions on the basis of category prediction results.,2 Problem Setting,[0],[0]
"For example, if the system does not know nasi goreng in the user input (denote as U1) in Figure 2 (a), the system predicts its category as Indonesian food and asks the user “Is nasi goreng Indonesian?”4 Komatani et al. (2016) also proposed a utilitybased method for selecting appropriate questions
4Note that Figures 2 through 4 show artificial examples, rather than those excerpted from the experimental data described in Section 5 because the experimental data are in Japanese and their direct translations are not natural.
",2 Problem Setting,[0],[0]
"(a) implicit, correct",2 Problem Setting,[0],[0]
on the basis of the results of category prediction.,U1: Philly cheesesteaks have a lot of,[0],[0]
"However, such explicit confirmation requests can degrade the user experience in chat-oriented dialogues, especially when the predicted category is incorrect as in Figure 2 (b), or the category of the unknown term is obvious as in Figure 2 (c).
",U1: Philly cheesesteaks have a lot of,[0],[0]
"We have proposed using implicit confirmation (Ono et al., 2016).",U1: Philly cheesesteaks have a lot of,[0],[0]
"For example, S1 in Figure 3 (a) does not explicitly ask the user if the category of tempura soba is Japanese, but from U2, it is possible to determine the category is correct.",U1: Philly cheesesteaks have a lot of,[0],[0]
"As another example, in Figure 3 (b), the system can determine the predicted category is incorrect from U2.
",U1: Philly cheesesteaks have a lot of,[0],[0]
"Determining if the predicted category is correct or not in implicit confirmation, however, is not always easy.",U1: Philly cheesesteaks have a lot of,[0],[0]
"Since user responses to implicit confirmation requests can come in various forms, looking at just the linguistic expressions of the user responses is not enough.",U1: Philly cheesesteaks have a lot of,[0],[0]
"For example, in Figure 4, the system incorrectly predicts the category Japanese food for Pandoro mentioned in U1 although it is Italian and generates an implicit confirmation request, S1.",U1: Philly cheesesteaks have a lot of,[0],[0]
The user then talks about Japanese food to continue the dialogue (U2).,U1: Philly cheesesteaks have a lot of,[0],[0]
"In
such cases, it is not simple to determine if the category is incorrect.",U1: Philly cheesesteaks have a lot of,[0],[0]
"If the system’s determination is wrong, it might add incorrect information to its database.",U1: Philly cheesesteaks have a lot of,[0],[0]
"Thus, we need to find a way to accurately determine the correctness of the predicted categories through implicit confirmation.",U1: Philly cheesesteaks have a lot of,[0],[0]
"So far, several studies have addressed lexical acquisition in dialogues.",3 Related Work,[0],[0]
Meng et al. (2004) and Takahashi et al. (2002) proposed methods for predicting the categories of unknown terms.,3 Related Work,[0],[0]
"They acquire coarse categories for unknown terms, which roughly correspond to named entity categories.",3 Related Work,[0],[0]
Those categories can be acquired more easily than the more specific categories that we are trying to acquire.,3 Related Work,[0],[0]
Holzapfel et al. (2008) proposed a method for a robot to acquire fine-grained categories for unknown terms by iteratively asking questions.,3 Related Work,[0],[0]
We do not think this method is suitable for chatbots as it repeats explicit questions.,3 Related Work,[0],[0]
"Whereas a previous study tried to acquire relationships among domain-dependent entities in dialogues (Pappu and Rudnicky, 2014), here we focus on acquiring lexical information, which is required before such relations are obtained.
",3 Related Work,[0],[0]
We address the problem of deciding if the content of an implicit confirmation request is correct or not.,3 Related Work,[0],[0]
Some studies related to this problem have tried to classify affirmative and negative sentences by using rules or statistical methods.,3 Related Work,[0],[0]
"For example, de Marneffe et al. (2009) built rules for judging if a response to a yes/no question is affirmative or negative when it is not a simple “yes” or “no.” Gokcen and de Marneffe (2015)",3 Related Work,[0],[0]
investigated features for detecting disagreement in the corpus of arguments on the Web.,3 Related Work,[0],[0]
"In contrast, in this paper, we do not try to classify user responses into affirmative and negative ones but try to determine whether a category in an implicit confirmation request is correct or not.",3 Related Work,[0],[0]
"Furthermore, we utilize multiple sub-dialogues with different users.
",3 Related Work,[0],[0]
"Our method can be considered as an instance of implicitly supervised learning (Banerjee and Rudnicky, 2007; Komatani and Rudnicky, 2009) in that user responses to implicit confirmation requests are used as indicators for acquisition, though the target knowledge is different from those works.",3 Related Work,[0],[0]
The purpose of our method is to prevent the system from learning incorrect categories for an unknown term by using multiple implicit confirmation subdialogues with different users.,4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
This is possible because our system is designed as a server-based dialogue system and can give implicit confirmation requests with the same predicted category to different users.,4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
"The proposed method determines more accurately whether or not the predicted category in the implicit confirmation request is correct by exploiting multiple responses to them.
",4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
"Let pi(w, c) be the probability that a predicted category c of an unknown term w is correct after a single implicit confirmation request.",4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
"The category can be predicted using surface information of the unknown term such as character n-gram and character types in Japanese (Otsuka et al., 2013).",4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
The index i denotes the i-th response to implicit confirmation requests.,4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
"Our goal here is to obtain a confidence score Conf (w , c) representing how likely category c of the unknown term w is to be correct on the basis of replies to implicit confirmation requests from n different users.",4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
"We can then determine whether or not the system can add the pair of the unknown term w and category c into the system knowledge by setting a threshold for Conf (w , c).",4 Determining Correct Categories Using Responses from Multiple Users,[0],[0]
Figure 5 gives an overview of the proposed method.,4.1 Procedure,[0],[0]
"The steps below initially start with i = 1.
1.",4.1 Procedure,[0],[0]
"Generate an implicit confirmation request containing a predicted category c for user i after an unknown term w appears.
",4.1 Procedure,[0],[0]
2.,4.1 Procedure,[0],[0]
"Obtain the probability pi(w, c) from the implicit confirmation sub-dialogue with user i.",4.1 Procedure,[0],[0]
"The probability can be obtained by machine learning that has features based on expressions from the user response and its context.
3.",4.1 Procedure,[0],[0]
"Extract features from p1(w, c), ..., pi(w, c) and calculate the confidence score Conf (w , c) that represents how likely the category c of the unknown term w is to be correct.
4.",4.1 Procedure,[0],[0]
"If Conf (w , c) exceeds a predetermined threshold, c is regarded as correct and is acquired as knowledge.",4.1 Procedure,[0],[0]
"Otherwise, increment i, go to Step 1, and generate one more implicit confirmation with c to another user after the unknown term w appears.",4.1 Procedure,[0],[0]
"The problem of obtaining the confidence score Conf (w , c) can be formulated as a regression using probabilities of n user responses {p1(w, c), ..., pn(w, c)} as its input.",4.2 Obtaining Confidence Scores for Correct Categories,[0],[0]
"Intuitively, the category c can be regarded as more likely to be correct when pi(w, c) with higher values are obtained more times.
",4.2 Obtaining Confidence Scores for Correct Categories,[0],[0]
"Table 1 lists the features used in this regression for when probabilities pi(w, c) are obtained n times.",4.2 Obtaining Confidence Scores for Correct Categories,[0],[0]
"To use the same regression function when
n increases, we design features that consist of a constant number even when n varies and that are derived from n responses to implicit confirmation requests with category c.",4.2 Obtaining Confidence Scores for Correct Categories,[0],[0]
We conducted experiments to verify if our method is effective.,5 Data Collection via Crowdsourcing,[0],[0]
"Although it would have been desirable to collect experimental data by incorporating our method into the chatbot we are developing and having it used by many people without giving any instructions, this would have required a huge amount of interactions to collect enough data to verify our method.",5 Data Collection via Crowdsourcing,[0],[0]
We therefore collected user responses to implicit confirmation requests from 100 workers via crowdsourcing5.,5 Data Collection via Crowdsourcing,[0],[0]
"The data collection procedure consists of three steps: (1) a worker inputs an utterance containing a term specified on the interface at the crowdsourcing site, (2) the system generates an implicit confirmation request about the term, and (3) the worker fills in the response to the confirmation request.",5 Data Collection via Crowdsourcing,[0],[0]
"This procedure was repeated for 20 specified terms per worker.
",5 Data Collection via Crowdsourcing,[0],[0]
Figure 6 shows a schematic diagram of the graphical user interface (GUI) used in the crowdsourcing.,5 Data Collection via Crowdsourcing,[0],[0]
Note that it was actually in Japanese.,5 Data Collection via Crowdsourcing,[0],[0]
"The lines starting with “YOU” and “SYSTEM” denote the worker’s and the system’s utterances, respectively.",5 Data Collection via Crowdsourcing,[0],[0]
"At Step (1), the worker was asked to input an utterance that contains a term specified in
5 We used a crowdsourcing platform provided by Crowdworks, Inc. https://crowdworks.co.jp/
the uppermost part in Figure 6.",5 Data Collection via Crowdsourcing,[0],[0]
The worker was able to check the Wikipedia page for the specified term by following a link on the GUI.,5 Data Collection via Crowdsourcing,[0],[0]
"This was to prevent them from talking without understanding the term.
",5 Data Collection via Crowdsourcing,[0],[0]
We prepared 20 terms and their corresponding implicit confirmation requests used at Step (2): 10 had correct categories and the other 10 had incorrect categories.,5 Data Collection via Crowdsourcing,[0],[0]
"For example, for “shurasuko” (the Japanese rendering of churrasco), an implicit confirmation request with its correct category “meat dish6” is “Eating meat is fun, isn’t it?”",5 Data Collection via Crowdsourcing,[0],[0]
"On the other hand, for “sangria,” an implicit confirmation request with an incorrect category “yogashi7” is “Yogashi have a rich taste, don’t they?”",5 Data Collection via Crowdsourcing,[0],[0]
"Furthermore, expressions of the implicit confirmation request were altered to make the confirmation request more natural when a worker’s input was interrogative or negative.
",5 Data Collection via Crowdsourcing,[0],[0]
"We obtained 1,956 responses from 98 workers, half of which were responses to implicit confirmation requests with correct categories, and the other half were responses to those with incorrect ones.",5 Data Collection via Crowdsourcing,[0],[0]
We removed data from two workers who just input only specified words or repeated the same sentences.,5 Data Collection via Crowdsourcing,[0],[0]
We also removed four invalid inputs consisting of only spaces.,5 Data Collection via Crowdsourcing,[0],[0]
"Table 2 lists the features for estimating how likely the categories in system confirmations are to be
6Food category hierarchies usually used in Japan are different from those used in other countries.
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"7Yogashi means western sweets in Japanese.
correct.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"Here, U1，S1，and U2 respectively denote a user input, the implicit confirmation request by the system after U1, and the user response to the request.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"All feature values are binary; if the sentence for a feature is true, its value is 1, otherwise it is 0.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"These features were designed to represent differences in expressions of user responses to implicit confirmation requests with either a correct or incorrect category.
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
We briefly explain some important features by using the examples below.,6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
A user often uses affirmative expressions when responding to an implicit confirmation request with a correct category.,6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"This is represented by Feature g1, for which 15 affirmative expressions in Japanese were used such as “Yes” and “That’s right.”
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"When a category in an implicit confirmation request is correct, a user tends to continue with the same topic in U2 as in U1.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"In the example in Figure 3 (a), the user continues with the same topic and uses the same term tempura soba in U1 and U2.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"This is represented by Feature g4.
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"When the system makes an implicit confirmation request on the basis of an incorrect category, users tend to feel the system has suddenly changed the topic.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"In this case, the user tries to return the topic in U2 to the original one in U1.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"An example is as follows.
U1: I like sangria with its fruity taste.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
S1:,6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"Yogashi have a rich taste, don’t they?",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"U2: I am talking about the alcoholic bev-
erage.
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"In this example, the system generates an implicit confirmation with the incorrect category “yogashi” in S1 although the correct category of sangria is “alcoholic beverage.”",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
Then the user says that the topic is an alcoholic beverage and tries to return to the original topic.,6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"Here, another category name not used in S1 is included in U2.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"This is represented as Feature g6.
",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"For Feature g2, 17 negative expressions were used such as “is not [category name used in S1]” and “No.”",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"For Feature g3, six expressions such as “It is [category name not used in S1]” that tries to correct the system’s previous confirmation request were used.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
"Our system has 20 categories, and five more names such as “cheese” and “pasta” were used as category names for Features g6 and g9.",6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
Eighteen expressions including interrogatives were used for Feature g10.,6.1 Features for Obtaining Probabilities with Single User Responses,[0],[0]
We conducted a preliminary experiment to classify responses to implicit confirmation requests with correct and incorrect categories.,6.2 Classification Performance with Single User Responses,[0],[0]
"The data consists of the 1,956 responses and their contexts obtained by crowdsourcing as described in Section 5.",6.2 Classification Performance with Single User Responses,[0],[0]
We applied logistic regression to them with the features listed in Table 2.,6.2 Classification Performance with Single User Responses,[0],[0]
"We used the module in Weka (version 3.8.1) (Hall et al., 2009) as its implementation.",6.2 Classification Performance with Single User Responses,[0],[0]
The parameters were the default values.,6.2 Classification Performance with Single User Responses,[0],[0]
"The classification was performed by setting a threshold to the obtained probability pi(w, c).",6.2 Classification Performance with Single User Responses,[0],[0]
"The threshold was 0.5, which is also the default value of Weka.",6.2 Classification Performance with Single User Responses,[0],[0]
"Evaluation was conducted with a 10-fold cross validation.
",6.2 Classification Performance with Single User Responses,[0],[0]
We compared two feature sets: one consists of all 11 features listed in Table 2 and the other consists of Features g1 and g2 only.,6.2 Classification Performance with Single User Responses,[0],[0]
"The latter corresponds to a baseline condition that only considers affirmative and negative expressions of U2 and does not consider any relationship with S1 and U1.
",6.2 Classification Performance with Single User Responses,[0],[0]
The results are shown in Tables 3 and 4.,6.2 Classification Performance with Single User Responses,[0],[0]
Table 3 shows confusion matrices of the raw outputs for the two feature sets.,6.2 Classification Performance with Single User Responses,[0],[0]
Table 4 summarizes the results as precision and recall rates and F-measures of the two categories (correct and incorrect) also for the two feature sets.,6.2 Classification Performance with Single User Responses,[0],[0]
"The average-F scores, i.e. the arithmetic means of F-measures for the two categories, were 0.719 and 0.528 when all features and only g1 and g2 were used, respectively.
",6.2 Classification Performance with Single User Responses,[0],[0]
"This indicates that using the features representing context improves the classification more than using only the features obtained from U2.
",6.2 Classification Performance with Single User Responses,[0],[0]
We also performed feature selection to analyze which features were effective for the classification.,6.2 Classification Performance with Single User Responses,[0],[0]
"More specifically, we performed the same experiments with all combinations of the 11 features, i.e., 2047(= 211 − 1) feature sets, and calculated their average-F scores.",6.2 Classification Performance with Single User Responses,[0],[0]
Table 5 lists top-10 feature sets sorted by the scores.,6.2 Classification Performance with Single User Responses,[0],[0]
“None” denotes the case when all the 11 features were used.,6.2 Classification Performance with Single User Responses,[0],[0]
"First, the “None” condition was ranked second in the table, which shows that almost all features were effective for the classification.",6.2 Classification Performance with Single User Responses,[0],[0]
"Next, when Feature g10 was removed, the F-value for the Incorrect category slightly improved and thus the average-F score also improved, as shown in the table.",6.2 Classification Performance with Single User Responses,[0],[0]
"Because Feature g10 also appears in the table several times, Feature g10 was implied to be less helpful in this classification.",6.2 Classification Performance with Single User Responses,[0],[0]
"On the other hand, the weight value for Feature g8 of the logistic regression function had the largest and positive value when Feature g10 was removed.",6.2 Classification Performance with Single User Responses,[0],[0]
"This shows Feature g8 gave strong evidence and resulting pi(w, c) tended to be higher when Feature g8 was 1.",6.2 Classification Performance with Single User Responses,[0],[0]
"This means that, when the common category name is included both in U1 and S1, the category included in S1 tended to be correct because the topic is not changed abruptly.
",6.2 Classification Performance with Single User Responses,[0],[0]
The results shown above indicate the classification performance was about 70% precision and recall rates on the basis of the user response and its context.,6.2 Classification Performance with Single User Responses,[0],[0]
"However, we need higher precision because pairs of an unknown term and its predicted category will be added to the system knowledge, which must not contain errors.",6.2 Classification Performance with Single User Responses,[0],[0]
"Thus, we have proposed a method using multiple user responses as described in Section 4, the effectiveness of which
is verified in the following section.",6.2 Classification Performance with Single User Responses,[0],[0]
"In this section, we explain how to prepare data for training and evaluating the regression function to obtain Conf (w , c).",7.1 Data Preparation,[0],[0]
We performed the experiment in a perfectly open manner: no data were shared in training and test phases from the viewpoint of either workers or questions.,7.1 Data Preparation,[0],[0]
"More specifically, we had 98 (or 97) responses to implicit confirmation requests with 10 correct and 10 incorrect categories for making implicit confirmation requests, as explained in Section 5.",7.1 Data Preparation,[0],[0]
"Thus, we divided them into four disjointed groups, i.e., one group consists of 49 (or 48) workers with five correct and five incorrect categories.
",7.1 Data Preparation,[0],[0]
The data were generated using responses collected from multiple users.,7.1 Data Preparation,[0],[0]
"The responses are mutually independent because they are obtained by a server-based dialogue system, so they can be combined in an arbitrary order.",7.1 Data Preparation,[0],[0]
"Thus, when we have N responses to single implicit confirmation requests, we can generate ( N n ) patterns.",7.1 Data Preparation,[0],[0]
"In our experiment, N was 49 (or 48) in each group.",7.1 Data Preparation,[0],[0]
"Since the values of ( N n ) become very large, we set a cut-off value when generating the combination randomly.",7.1 Data Preparation,[0],[0]
"The value was set to 1, 000 when ( N n ) exceeds 1, 000.
",7.1 Data Preparation,[0],[0]
"From this data combination, we obtained feature values listed in Table 1 with the reference values for every case.",7.1 Data Preparation,[0],[0]
"The reference value was set to either 1 or 0 depending on whether the category used in the implicit confirmation request was correct or not, respectively.
",7.1 Data Preparation,[0],[0]
We then trained the regression function with each set of divided data of the four groups.,7.1 Data Preparation,[0],[0]
"We selected test data sets to be completely disjointed
from each of the four data sets from the viewpoint of both workers and questions.",7.1 Data Preparation,[0],[0]
"We also used the logistic regression, which was implemented in Weka (version 3.8.1) (Hall et al., 2009), with its default parameters.",7.1 Data Preparation,[0],[0]
The results by the regression for the four test sets are used together and analyzed hereafter.,7.1 Data Preparation,[0],[0]
We first investigated if the performance was better when the system used multiple responses from users.,7.2 Performance of Regression with Multiple Responses,[0],[0]
"The precision and recall rates were calculated by setting various thresholds to Conf (w , c) representing how likely a category c is to be correct for an unknown term w.
Figure 7 depicts the precision and recall curves for n up to 8.",7.2 Performance of Regression with Multiple Responses,[0],[0]
"It also shows a line indicating the breakeven points (BEPs), meaning the value where the two rates are equal.",7.2 Performance of Regression with Multiple Responses,[0],[0]
The BEP is used as a single point representing a precision and recall curve and to show how good the estimated confidence score is when n changes.,7.2 Performance of Regression with Multiple Responses,[0],[0]
"Note that n = 1 corresponds to the case when only single responses were used for the regression.
",7.2 Performance of Regression with Multiple Responses,[0],[0]
The performance represented by the BEP values became better as n became larger.,7.2 Performance of Regression with Multiple Responses,[0],[0]
"In particular, the BEP values of n ≥ 2 were larger than that of n = 1.",7.2 Performance of Regression with Multiple Responses,[0],[0]
"This proves that the proposed method using multiple user responses more accurately determines whether the predicted category is correct or not.
",7.2 Performance of Regression with Multiple Responses,[0],[0]
We also performed feature selection by removing arbitrary features listed in Table 1.,7.2 Performance of Regression with Multiple Responses,[0],[0]
"The performance of the regression function was measured by the summation of BEP values for each n (1 ≤
n ≤ 48).",7.2 Performance of Regression with Multiple Responses,[0],[0]
The result revealed the best performance in the case was obtained when we used only Features f3 and f4.,7.2 Performance of Regression with Multiple Responses,[0],[0]
One reason for this result was that the correlations among the features might be high.,7.2 Performance of Regression with Multiple Responses,[0],[0]
"We still need to further investigate feature sets to obtain better Conf(w, c), which is future work.",7.2 Performance of Regression with Multiple Responses,[0],[0]
We discuss the relationship between the values of n and the performance of the regression function in more detail.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
Figure 7 shows that the performance represented by the BEP improved when n increased.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
"On the other hand, cost will need to be incurred for increasing n, i.e., collecting responses from more human users.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"Thus, we investigate how much the performance of the regression function changed when n increased.
",7.3 Discussion on Reasonable Number of Responses,[0],[0]
We first investigated how the BEP values increased in accordance with n values.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
Figure 8 depicts the increases in the BEP values when n was incremented by 1.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
It shows the increases were large while n ≤ 5.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
"This result indicates that it is worthwhile to ask more users implicit confirmation requests with predicted category c especially while n is small, to more accurately determine whether or not the category is correct.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"The figure also shows that the improvement mostly diminished, especially when n ≥ 10.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"This indicates that the effect by asking implicit confirmation requests to more human users shows diminishing returns as n increases from the viewpoint of the performance represented by the BEP.
",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"We furthermore investigated recall rates when thresholds were set to Conf (w , c) so as to keep precision rates high.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"In our problem setting, high precision rates rather than high recall rates are re-
quired to avoid incorrect information being mistakenly added to the system knowledge.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"Figure 7 also shows the precision rate approached 1 for n ≥ 5 by setting very large thresholds to Conf (w , c).",7.3 Discussion on Reasonable Number of Responses,[0],[0]
These cases indicate that the system can be almost perfectly confident that the predicted category c is correct.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
The recall rates were low for such cases because the precision and recall rates are in a trade-off relationship.,7.3 Discussion on Reasonable Number of Responses,[0],[0]
"We investigated the recall rates for such cases when n increased.
",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"Figure 9 depicts the recall rates when we set very high threshold values for Conf (w , c) so that the precision rates become almost one, i.e., 1 − ϵ. Here, we set ϵ = 0.0058.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"First, the graph shows that the precision rate existed when n was 5 or more.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"For example, the recall rate for n = 5 was 0.175.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
"This recall rate was rather low, but we think high precision rates should be prioritized over recall rates, even if some correct information is discarded at the current n. Second, the graph also shows that the recall rates increased with n. This means that, if the system asks more implicit confirmation requests with category c, more unknown terms the categories of which are c will be acquired with a sufficiently high precision rate.",7.3 Discussion on Reasonable Number of Responses,[0],[0]
We have proposed a method to determine if the ontological category of an unknown term included in an implicit confirmation request is correct or not.,8 Concluding Remarks,[0],[0]
"Although responses to implicit confirmation requests seem to be insufficient for determining this, our method makes it effective by using the information on the context of the responses and exploiting responses from multiple users.",8 Concluding Remarks,[0],[0]
"Exper-
",8 Concluding Remarks,[0],[0]
8,8 Concluding Remarks,[0],[0]
The margin ϵ,8 Concluding Remarks,[0],[0]
is required because the confidence score obtained by the logistic regression function cannot be 1 theoretically (the score can only converge to 1).,8 Concluding Remarks,[0],[0]
"Therefore, we selected the smallest ϵ with which we can calculate reasonable recall values.
",8 Concluding Remarks,[0],[0]
imental results revealed that the proposed method exhibited higher performance than when only single user responses were used.,8 Concluding Remarks,[0],[0]
"We hope the performance will be improved with further feature engineering.
",8 Concluding Remarks,[0],[0]
"The proposed method is expected to enable a chatbot to acquire knowledge through dialogues without annoying users with repetitive simple explicit confirmation requests, while it can avoid acquiring wrong knowledge by achieving a high precision rate for determining the correctness of the knowledge.
",8 Concluding Remarks,[0],[0]
We are planning to address several issues before deploying this method in a chatbot.,8 Concluding Remarks,[0],[0]
"Although we intuitively think implicit confirmation requests do not degrade users’ impressions compared with repetitive explicit confirmation requests, we need to experimentally verify this by a user study.",8 Concluding Remarks,[0],[0]
"On the basis of its results, we will define a strategy of when to make implicit confirmation requests and when to make explicit confirmation requests.",8 Concluding Remarks,[0],[0]
"Despite these remaining issues, we believe that the experimental results presented in this paper are valuable in that they show the possibility of lexical acquisition through implicit confirmation.",8 Concluding Remarks,[0],[0]
This work was partly supported by JSPS KAKENHI Grant Number JP16H02869.,Acknowledgments,[0],[0]
We address the problem of acquiring the ontological categories of unknown terms through implicit confirmation in dialogues.,abstractText,[0],[0]
We develop an approach that makes implicit confirmation requests with an unknown term’s predicted category.,abstractText,[0],[0]
"Our approach does not degrade user experience with repetitive explicit confirmations, but the system has difficulty determining if information in the confirmation request can be correctly acquired.",abstractText,[0],[0]
"To overcome this challenge, we propose a method for determining whether or not the predicted category is correct, which is included in an implicit confirmation request.",abstractText,[0],[0]
Our method exploits multiple user responses to implicit confirmation requests containing the same ontological category.,abstractText,[0],[0]
Experimental results revealed that the proposed method exhibited a higher precision rate for determining the correctly predicted categories than when only single user responses were considered.,abstractText,[0],[0]
Lexical Acquisition through Implicit Confirmations over Multiple Dialogues,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events.",1 Introduction,[0],[0]
"For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly.",1 Introduction,[0],[0]
"Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010).",1 Introduction,[0],[0]
"Lexical temporal knowledge is further important for model-
ing grammatical phenomena such as tense and aspect (Steedman, 2002).
",1 Introduction,[0],[0]
"In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments.",1 Introduction,[0],[0]
"Concretely, the task is to predict the order of an unordered set of predicate-argument structures.",1 Introduction,[0],[0]
"Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets.",1 Introduction,[0],[0]
"A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences.
",1 Introduction,[0],[0]
"Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for.",1 Introduction,[0],[0]
"Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations.",1 Introduction,[0],[0]
"However, due to incurred costs, annotated corpora are too small for most lexical tasks.",1 Introduction,[0],[0]
"For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013).
",1 Introduction,[0],[0]
"Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012).",1 Introduction,[0],[0]
"However, despite their appeal, these methods have mostly fo-
1161
cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features).",1 Introduction,[0],[0]
"Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often precisionbased or extrinsic.
",1 Introduction,[0],[0]
"We take a graph-based structured prediction approach to the task, motivated by the flexibility it allows in incorporating various feature sets and constraints.",1 Introduction,[0],[0]
"We use an edge-factored model, which decomposes over the edges in the graph of events comprising the recipe (§4).",1 Introduction,[0],[0]
We estimate the model using the structured perceptron algorithm.,1 Introduction,[0],[0]
"We compare the structured perceptron approach to an approximate greedy baseline and to a locally normalized model reminiscent of common approaches for order learning, obtaining superior results (§8).",1 Introduction,[0],[0]
"The learning algorithm is of potential use in other ordering tasks such as machine translation reordering (Tromble and Eisner, 2009).
",1 Introduction,[0],[0]
We focus on domains in which the order of events in the text is aligned with their temporal order.,1 Introduction,[0],[0]
"By doing so we avoid the costly and error-prone manual annotation of temporal relations by using the textual order of recipes to approximate their temporal order.1 Specifically, we address the cooking recipes domain, which we motivate in §2.
",1 Introduction,[0],[0]
"In summary, the contribution of this paper is three-fold: (1) we explore the task of lexical event ordering and means for its evaluation; (2) we present an edge-factored model for the task, and show it can be used to predict the order of events well (77.7% according to standard measures for ordering evaluation); (3) we present a method for extracting events and create a dataset of ordered events using recipes extracted from the web.",1 Introduction,[0],[0]
Temporal semantics is receiving increasing attention in recent years.,2 Related Work,[0],[0]
Lexical features are in frequent use and rely in most part on external resources which are either manually compiled or automatically induced.,2 Related Work,[0],[0]
"The line of work most closely related to ours focuses on inducing lexical relations between
1See Cassidy et al. (2014) for a discussion of inter-annotator agreement in TimeML-based schemes.
event types.",2 Related Work,[0],[0]
"Most work has been unsupervised, often using pattern-based approaches relying on manually crafted (Chklovski and Pantel, 2004) or induced patterns (Davidov et al., 2007), that correlate with temporal relations (e.g., temporal discourse connectives).",2 Related Work,[0],[0]
Talukdar et al. (2012) uses the textual order of events in Wikipedia biographical articles to induce lexical information.,2 Related Work,[0],[0]
"We use both textual order and discourse connectives to define our feature set, and explore a setting which allows for the straightforward incorporation of additional features.
Chambers and Jurafsky (2008b; 2009) addressed the unsupervised induction of partially ordered event chains (or schema) in the news domain, centered around a common protagonist.",2 Related Work,[0],[0]
"One of their evaluation scenarios tackles a binary classification related to event ordering, and seeks to distinguish ordered sets of events from randomly permuted ones, yielding an accuracy of 75%.",2 Related Work,[0],[0]
Manshadi et al. (2008) used language models to learn event sequences and conducted a similar evaluation on weblogs with about 65% accuracy.,2 Related Work,[0],[0]
"The classification task we explore here is considerably more complex (see §8).
",2 Related Work,[0],[0]
The task of script knowledge induction has been frequently addressed in recent years.,2 Related Work,[0],[0]
Balasubramanian et al. (2013) and Pichotta and Mooney (2014) extended Chambers and Jurafsky’s model to include events that have multiple arguments.,2 Related Work,[0],[0]
"Jans et al. (2012) use skip-grams to capture event-event relations between not necessarily consecutive events.
",2 Related Work,[0],[0]
Regneri et al. (2010) constructed a temporal lexical knowledge base through crowd-sourcing.,2 Related Work,[0],[0]
Their approach is appealing as it greatly reduces the costs incurred by manual annotation and can potentially be used in conjunction with lexical information obtained from raw text.,2 Related Work,[0],[0]
"Modi and Titov (2014) jointly learns the stereotypical order of events and their distributional representation, in order to capture paraphrased instances of the same event type.",2 Related Work,[0],[0]
Frermann et al. (2014) models the joint task of inducing event paraphrases and their order using a Bayesian framework.,2 Related Work,[0],[0]
"All latter three works evaluated their induced temporal ordering knowledge on a binary prediction of whether a temporal relation between a pair of (not necessarily related) events holds, and not on the prediction of a complete permutation given an unordered event set as in this work.",2 Related Work,[0],[0]
"Their evaluation was conducted on 30 event pairs manually an-
notated through crowd-sourcing, where Modi and Titov (2014) further included an evaluation on a large set of pairs automatically extracted from the Gigaword corpus.
",2 Related Work,[0],[0]
"The appeal of the cooking domain for studying various semantic phenomena has been recognized by several studies in NLP and AI (Tasse and Smith, 2008; Bollini et al., 2013; Cimiano et al., 2013; Regneri et al., 2013; Malmaud et al., 2014).",2 Related Work,[0],[0]
The domain is here motivated by several considerations.,2 Related Work,[0],[0]
"First, recipes mostly describe concrete actions, rather than abstract relations, which are less relevant to temporal ordering.",2 Related Work,[0],[0]
"Second, from a practical point of view, many recipes are available online in computerreadable format.",2 Related Work,[0],[0]
"Third, the restrictiveness of the cooking domain can also be seen as an advantage, as it can reveal major conceptual challenges raised by the task, without introducing additional confounds.",2 Related Work,[0],[0]
We formalize our task as follows.,3 Temporally Ordering Lexical Events,[0],[0]
"Let U be a set of event types, namely actions or states (represented as predicates) and objects which these actions operate on (represented as arguments to the predicates; mostly ingredients or kitchenware).",3 Temporally Ordering Lexical Events,[0],[0]
"Formally, each e ∈ U is a tuple 〈a, c1, . . .",3 Temporally Ordering Lexical Events,[0],[0]
", cn〉 where a is the main verb or predicate describing the event (such as “stir” or “mix”) and c1, . . .",3 Temporally Ordering Lexical Events,[0],[0]
", cn is a list of arguments that the predicate takes (e.g., “salt” or “spoon”).",3 Temporally Ordering Lexical Events,[0],[0]
"Two additional marked events, s and f , correspond to “start” and “finish” events.",3 Temporally Ordering Lexical Events,[0],[0]
"A recipe is a sequence of events in U , starting at s and ending at f .
",3 Temporally Ordering Lexical Events,[0],[0]
"Given a recipe R = 〈e1, ..., em〉, we wish to predict the order of the events just from the (multi)set {ei}mi=1.",3 Temporally Ordering Lexical Events,[0],[0]
"In this work we use the textual order of events to approximate their temporal order (see, e.g., Talukdar et al. (2012) for a similar assumption).",3 Temporally Ordering Lexical Events,[0],[0]
"The validity of this assumption for cooking recipes is supported in §6.
",3 Temporally Ordering Lexical Events,[0],[0]
Figure 1 gives an example of a set of events extracted from our dataset for the dish “Apple Crisp Ala [sic] Brigitte.”,3 Temporally Ordering Lexical Events,[0],[0]
Lexical information places quite a few limitations on the order of this recipe.,3 Temporally Ordering Lexical Events,[0],[0]
"For instance, in most cases serving is carried out at the end while putting the ingredients in is done prior to baking them.",3 Temporally Ordering Lexical Events,[0],[0]
"However, lexical knowledge in itself is unlikely to predict the exact ordering of the events
as given in the recipe (e.g., spreading butter might be done before or after baking).
",3 Temporally Ordering Lexical Events,[0],[0]
One of the major obstacles in tackling planning problems in AI is the knowledge bottleneck.,3 Temporally Ordering Lexical Events,[0],[0]
Lexical event ordering is therefore a step towards more ambitious goals in planning.,3 Temporally Ordering Lexical Events,[0],[0]
"For instance, temporal relations may be used to induce planning operators (Mourão et al., 2012), which can in turn be used to generate a plan (recipe) given a specified goal and an initial set of ingredients.",3 Temporally Ordering Lexical Events,[0],[0]
In this section we describe the main learning components that compose our approach to event ordering.,"4 Model, Inference and Learning",[0],[0]
We hereby detail the linear model we use for ordering events.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Let S = {e1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", em} ⊆ U be a set of events as mentioned in §3.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Let G(S) =,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(S ∪ {s, f}, E(S)) be an almost-complete directed graph withE(S)",4.1 Edge-Factored Model for Event Ordering,[0],[0]
=,4.1 Edge-Factored Model for Event Ordering,[0],[0]
(S∪{s})×(S∪{f}) ⊆ U×U .,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Every Hamiltonian path2 inG(S) that starts in s and ends in f defines an ordering of the events in S. The edge (ei, ej) in such a path denotes that ei is the event that comes before ej .
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
The modeling problem is to score Hamiltonian paths in a given directed graph G(S).,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Here, we use an edge-factored model.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Let φ : (U × U) → Rd be a feature function for pairs of events, represented as directed edges.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"In addition, let θ ∈ Rd be a weight vector.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"We define the score of a Hamiltonian path h = (h1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", hm+1)",4.1 Edge-Factored Model for Event Ordering,[0],[0]
(hi ∈ E(S)),4.1 Edge-Factored Model for Event Ordering,[0],[0]
"as:
score(h|S) = m+1∑ i=1",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"θ>φ(hi) (1)
Given a weight vector θ and a set of events S, inference is carried out by computing the highest scoring Hamiltonian path in G(S):
h∗ = arg max h∈H(S) score(h|S) (2)
where H(S) is the set of Hamiltonian paths inG(S) that start with s and end with f .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The path h∗ is the best temporal ordering of the set of events S according to the model in Eq. 1 with weight vector θ.
2A path in a graph that visits all nodes exactly once.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(a) e1 = 〈butter, dish〉 e2 = 〈put, apples,water, ...",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"flour, cinnamon, it〉 e3 = 〈mix,with spoon, 〉 e4 = 〈spread, butter, salt, ... over mix〉 e5 = 〈bake,F〉 e6 = 〈serve, cream, cream〉
(b) Butter a deep baking dish, put apples, water, flour, sugar and cin-
namon in it.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Mix with spoon and
spread butter and salt over the ap-
ple mix.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Bake at 350 degrees F until
the apples are tender and the crust
brown, about 30 minutes.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Serve
with cream or whipped cream.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(c)
(a) e 1
= hbutter, dishi e 2
= hput, apples,water, ... flour, cinnamon, iti
e 3 = hmix, spoon, i e 4
= hspread, butter, salt,mixi e 5
= hbake,Fi e 6 = hserve, cream, creami
(b) Butter a deep baking dish, put apples, water, flour, sugar and cinnamon in it.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Mix with spoon and spread butter and salt over the apple mix.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Bake at 350 degrees F until the apples are tender and the crust brown, about 30 minutes.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Serve with cream or whipped cream.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"s
e 1
e 2
e 3
e 4
e 5
e 6
f
(c) (a) e1 = hmix, ✏, tarragon, vinegari e2 = hblend, ✏,mustardi e3 = hmix, ✏, salt, pepperi e4 = hblend, ✏,mayonnaise,",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"sour creami e5 = hcover, ✏i e6 = hchill, ✏i (b) you mix the tarragon and vinegar together and blend in the mustard.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"you mix in the salt and pepper, blending well.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
you blend in the mayonnaise and then the sour cream.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
you cover and chill.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"s
e1 e2
e3 e4
e5
e6
e
Figure 1: (a) Example of events describing a recipe for the dish “.”",4.1 Edge-Factored Model for Event Ordering,[0],[0]
(b) The actual recipe for this dish.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
(c) A complete graph over the set of events with start and end states.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Each internal node in the graph is one of the events ei for i 2 {1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", 5}.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The path in bold denotes the correct Hamiltonian path describing the set of actions that need to be taken to follow the recipe.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"3 Model, Inference and Learning
In this section we describe the main learning components that compose our approach to event ordering.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"3.1 Edge-Factored Model for Event Ordering
We now turn to explain the linear model we use for ordering events in time.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Let S = {v1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", vm} ✓ U be a set of events as mentioned in section 2.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Let G(S) =,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(S [ {s, e}, E(S)) be an almost-complete directed graph with E(S)",4.1 Edge-Factored Model for Event Ordering,[0],[0]
= (S[{s})⇥(S[{e}),4.1 Edge-Factored Model for Event Ordering,[0],[0]
✓ (U ⇥ U).,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Every Hamiltonian path5 in G(S) that starts in s and ends in e can be thought of as an ordering of the events in S. The edge (vi, vj) in such a path denotes that vi is the event that comes before vj .
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The modeling problem, therefore, is to score Hamiltonian paths in a given directed graph G(S) such as the above.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Here, we use an edge-factored model.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Let : (U ⇥ U) !,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Rd be a feature vector for pairs of events, represented as directed edges.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"In addition, let ✓ 2 Rd be a weight vector.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Then, we define the score of an Hamiltonian path h = (h1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", hm+1) (where hi 2 E(S) for i 2 {1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
",m+ 1}) as:
5An Hamiltonian path in a graph is a path that visits all nodes exactly once.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"score(h|S) = m+1X
i=1
✓> (hi) (1)
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Given a weight vector w and a set of events S, inference is carried out by computing the highest scoring Hamiltonian path in G(S):
h⇤ = arg max h2H(S) score(h|S) (2)
where H(S) is the set of Hamiltonian paths in G(S) that start with s and end with e. h⇤ is the best temporal ordering of the set of events S according to the structured model in Equation 1 with weight vector w.
3.2 Inference As mentioned above, inference with the edgefactored model we presented would have to solve the maximization problem in Eq. 2.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"This corresponds to finding an Hamiltonian path in a complete graph, which is generally an NP-hard problem6.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"In the general case there is no reasonable approximation algorithm to solve the maximization algorithm, although
6The NP complete problem of finding a Hamiltonian cycle in an undirected graph can be trivially reduced to finding the maximal Hamiltonian cycle in a directed graph.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Figure 1: (a) Example of events describing a recipe for the dish “Apple Crisp Ala [sic] Brigitte.”,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"For brevity, arguments are represented as headwords and their syntactic type is omitted.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
(b) The actual recipe for this dish.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
(c) A complete graph over the set of events with start and end states.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Each internal node in the graph is one of the events ei for i 2 {1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", 6}.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered in the recipe.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
Red edges denote edges from the start state a d to the end state.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The edges, in practice, are weighted.
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
ILP formulation yields superior performance to the other evaluated systems (§8).,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"ILP has been proven to be a practical and flexible tool in various structured prediction tasks in NLP (Roth and tau Yih, 2007; Talukdar et al., 2012; Scaria et al., 2013).",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Our ILP formulation is given in Appendix A.
We experiment with an additional greedy inference algorithm, similar to the one described by Lapata (2003) for sentence ordering.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The algorithm iteratively selects an outgoing edge (starting from the node s) that has the largest weight to a node that has not been visited so far, until all vertices are covered, at which point the path terminates by travelling to f .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"4.3 Learning The learning problem takes as input a dataset consisting of unordered sets of events, paired with a target ordering.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
We consider two types of learning algorithms for the edge-factored model in the previous section.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The first learns in a global training setting using the averaged structured perceptron (Collins, 2002), with the decoding algorithm being either the one based on ILP (henceforth, GLOBAL-PRC), or the greedy one (GREEDY-PRC).
",4.1 Edge-Factored Model for Event Ordering,[0],[0]
The second learning algorithm we try is based on factored training.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"This algorithm maximizes the likelihood of a conditional log-linear model p:
p(e2|e1, ✓, S) = exp
✓> (e1, e2)
Z(✓, S, e1)
Z(✓, S, e1) =",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"X
e : (e1,e)2E(S)
exp ✓> (e1, e)
where e 1 , e 2 2 S",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"[ {s, f}.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"This is a locally normalized log-linear model that gives the probability of transitioning to node e
2 from node e 1 .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Maximizing the score in Eq. 1 has an interpretation of finding the highest scoring path according to an edge-factored Markovian model, such that:
p(h|✓, S) = m+1Y
i=2
p(ei|ei 1, ✓, S),
where h = (h 1 , . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", h m+1 ) is a Hamiltonian path with h
i =",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(e i 1, ei) being a directed edge in
the path.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Initial experimentation suggested that greedy inference (henceforth, GREEDY-LOGLIN) works better in practice than the ILP formulation for the locally-normalized model.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
We therefore do not report results on global inference with this loglinear model.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"GREEDY-LOGLIN closely resembles the learning model of Lapata (2003), except that it is a discriminative log-linear model, rather of a generative Markovian model.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
5,4.1 Edge-Factored Model for Event Ordering,[0],[0]
The Feature Set Table 1 presents all the complete set of features used for defining the feature function .,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects;
Figure 1: (a) The sequence of events representing the recipe for the dish “Apple Crisp Ala [sic] Brigitte.”",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"(b) The actual recipe
for this dish.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
(c) A complete graph over the set of events with start and finish states.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"Each internal node in the graph is one of the
events ei for i ∈ {1, . . .",4.1 Edge-Factored Model for Event Ordering,[0],[0]
", 6}.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The path in blue bold denotes the correct Hamiltonian path describing the set of actions as ordered in
the recipe.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
Red edges denot edges from the start state and to the end state.,4.1 Edge-Factored Model for Event Ordering,[0],[0]
"The edges, in practice, are weighted.",4.1 Edge-Factored Model for Event Ordering,[0],[0]
"As mentioned above, inference with the edgefactored model requires solving he maximization problem in Eq. 2.",4.2 Inference,[0],[0]
"This corresponds to finding a Hamiltonian path in a complete graph, which is generally an NP-hard problem.",4.2 Inference,[0],[0]
Reasonable approximations for this problem are also NP-hard.,4.2 Inference,[0],[0]
"Still tec - niques are developed for specialized cases, due to the problem’s importance in discrete optimization.
",4.2 Inference,[0],[0]
"Despite its the retical NP-hardness, this maximization problem can be repr sented as an Integer Linear Program (ILP), and then solved using generic techniques for ILP optimizatio .",4.2 Inference,[0],[0]
"Due to the relatively short length of recipes (13.8 vents on average in our corpus), th probl m can be effectively solved in most cases.
",4.2 Inference,[0],[0]
The proposed algorithmic setting is appealing for its flexibility.,4.2 Inference,[0],[0]
"The linear score formulation allows us to use rich features, while using ILP allows to easily incorporate structural constraints.",4.2 Inference,[0],[0]
"Indeed, ILP has been proven valuable in various NLP tasks (Roth and Yih, 2007; Talukdar et al., 2012; Scaria al., 2013).",4.2 Inference,[0],[0]
"See Appendix A for our ILP formulation.
",4.2 Inference,[0],[0]
"As a baseline, we experiment with an additional greedy inference algorithm, similar to the one described by Lapata (2003) for sentence ordering.",4.2 Inference,[0],[0]
"he algorithm iteratively selects an outgoing edge (starting from the node s) that has the largest weight to a node that has not been visited so far, until all vertices are covered, at which point the path terminates by traveling to f .",4.2 Inference,[0],[0]
"The learning problem takes as input a dataset con-
sisting of unordered sets of events, paired with a target ordering.",4.3 Learning,[0],[0]
We c nsider two types of learning algorithms for the edge-factored model in the previous section.,4.3 Learning,[0],[0]
"The first learns in a global training setting using the averaged structured perceptron (Collins, 200 ), with the decoding algorithm being either the one based on ILP (henceforth, GLOBAL-PRC), or the greedy one (GREEDY-PRC).",4.3 Learning,[0],[0]
"Given a training instance S and its correct label hc, the structured perceptron calls the inference procedure as a subroutine and updates the weight vector θ according to the difference between the value of the feature function on the predicted path ( ∑ h∗ φ(hi)) and on the correct
path ( ∑
hc φ(hi)).",4.3 Learning,[0],[0]
The second learning algorithm we try is based on factored training.,4.3 Learning,[0],[0]
"This algorithm maximizes the likelihood of a conditional log-linear model p:
p(e2|e1, θ, S) = exp
( θ>φ(e1, e2) )",4.3 Learning,[0],[0]
"Z(θ, S, e1)
Z(θ, S, e1)",4.3 Learning,[0],[0]
"= ∑
e : (e1,e)∈E(S) exp
( θ>φ(e1, e) ) where e1, e2 ∈ S ∪ {s, f}.",4.3 Learning,[0],[0]
This is a locally normalized log-linear model that gives the probability of transitioning to node e2 from node e1.,4.3 Learning,[0],[0]
"Maximizing the score in Eq. 1 has an interpretation of finding the highest scoring path according to an edge-factored Markovian model, such that:
p(h|θ, S) = m+1∏ i=1",4.3 Learning,[0],[0]
"p(ei|ei−1, θ, S),
where h = (h1, . . .",4.3 Learning,[0],[0]
", hm+1) is a Hamiltonian path with hi = (ei−1, ei) being a directed edge in the path.",4.3 Learning,[0],[0]
"Initial experimentation suggested that greedy inference (henceforth, GREEDY-LOGLIN) works better in practice than the ILP formulation for the locally-normalized model.",4.3 Learning,[0],[0]
We therefore do not report results on global inference with this log-linear model.,4.3 Learning,[0],[0]
"We suspect that greedy inference works better with the log-linear model because it is trained locally, while the perceptron algorithm includes a global inference step in its training, and therefore better matches global decoding.
",4.3 Learning,[0],[0]
"GREEDY-LOGLIN closely resembles the learning model of Lapata (2003), as both are firstorder Markovian and use the same (greedy) inference procedure.",4.3 Learning,[0],[0]
"Lapata’s model differs from GREEDY-LOGLIN in being a generative model, where each event is a tuple of features, and the transition probability between events is defined as the product of transition probabilities between feature pairs.",4.3 Learning,[0],[0]
"GREEDY-LOGLIN is discriminative, so to be maximally comparable to the presented model.",4.3 Learning,[0],[0]
Table 1 presents the complete set of features.,5 The Feature Set,[0],[0]
"We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects; Brown uses Brown clusters (Brown et al., 1992) to encode similar information, but allows generalization between distributionally similar words; and Frequency encodes the empirical distribution of temporally-related phenomena.
",5 The Feature Set,[0],[0]
The feature definitions make use of several functions.,5 The Feature Set,[0],[0]
"For brevity, we sometimes say that an event e is (a, c1) if e’s predicate is a and its first argument is c1, disregarding its other arguments.",5 The Feature Set,[0],[0]
Let C be a reference corpus of recipes for collecting statistics.,5 The Feature Set,[0],[0]
"The function B(w) gives the Brown cluster of a word w, as determined by clustering C into 50 clusters {1, . . .",5 The Feature Set,[0],[0]
", 50}.",5 The Feature Set,[0],[0]
"The function ORD(a, c) returns the mean ordinal number of an (a, c) event in C. The ordinal number of the event ei in a recipe (e1, ..., em) is defined as i− m2 .
",5 The Feature Set,[0],[0]
"We further encode the tendency of two events to appear with temporal discourse connectives, such as “before” or “until.”",5 The Feature Set,[0],[0]
"We define a linkage between two events as a triplet (e1, e2, `) ∈",5 The Feature Set,[0],[0]
"(U × U × L), where L is the set of linkage types, defined according to their marker’s written form.",5 The Feature Set,[0],[0]
§6 details the extraction process of linkages from recipes.,5 The Feature Set,[0],[0]
"We further include a special linkage type linear based on the order of events in the text, and consider every pair of events e1 and e2 that follow one another in a recipe as linked under this linkage type.
",5 The Feature Set,[0],[0]
"For each linkage type ` ∈ L, we define an empirical probability distribution P`((a, c1), (a′, c′1))",5 The Feature Set,[0],[0]
"= P ((a, c1), (a′, c′1)|`), based on simple counting.",5 The Feature Set,[0],[0]
"The function PMI gives the point-wise mutual information of two events and is defined as:
PMI`((a, c), (a′, c′))",5 The Feature Set,[0],[0]
"= log ( P`((a, c1), (a′, c′1))",5 The Feature Set,[0],[0]
"P`(a, c1) ·",5 The Feature Set,[0],[0]
"P`(a′, c′1) )
",5 The Feature Set,[0],[0]
Frequency-based features encode the empirical estimate of the probabilities that various pairs of features would occur one after the other or linked with a discourse marker.,5 The Feature Set,[0],[0]
"They are equivalent to using probabilities extracted from maximum likelihood estima-
tion according to a bigram model in the discriminative learning.",5 The Feature Set,[0],[0]
"While some of this information is implicitly found in the lexical features, collecting frequency counts from a large training set is much quicker than running costly structured optimization.",5 The Feature Set,[0],[0]
Rather the discriminative training can weigh the different empirical probabilities according to their discriminative power.,5 The Feature Set,[0],[0]
Indeed we find that these features are important in practice and can result in high accuracy even after training on a small training set.,5 The Feature Set,[0],[0]
Data and Preprocessing.,6 The Recipe Dataset,[0],[0]
The data is extracted from a recipe repository found on the web.3,6 The Recipe Dataset,[0],[0]
The recipes are given as free text.,6 The Recipe Dataset,[0],[0]
"To extract event types we run the Stanford CoreNLP4 pipeline of a tokenizer, POS tagger, a lexical constituency parser (the englishPCFG parsing model) and extract typed Stanford dependencies (de Marneffe and Manning, 2008).",6 The Recipe Dataset,[0],[0]
"As is common with web extractions, the recipes contain occasional spelling, grammatical and formatting errors.",6 The Recipe Dataset,[0],[0]
"The corpus consists of 139 files, 73484 recipes, 1.02M events (13.8 events per recipe on average) and 11.05M words.5
Event Extraction.",6 The Recipe Dataset,[0],[0]
"We focus on verbal events and do not extract nominal and adjectival argument structures, which are not as well supported by current parsing technology.",6 The Recipe Dataset,[0],[0]
"Any verb is taken to define an event, aside from modal verbs, auxiliaries and secondary verbs.",6 The Recipe Dataset,[0],[0]
"A secondary verb (e.g., “let,” “begin”) does not describe an action in its own right, but rather modifies an event introduced by another verb.",6 The Recipe Dataset,[0],[0]
"We identify these verbs heuristically using a list given in Dixon (2005, p. 490–491) and a few simple rules defined over parse trees.",6 The Recipe Dataset,[0],[0]
"E.g., from the sentence “you should begin to chop the onion,” we extract a single event with a predicate “chop.”",6 The Recipe Dataset,[0],[0]
Arguments are taken to be the immediate dependents of the predicate that have an argument dependency type (such as direct or indirect objects) according to the extracted Stanford dependencies.,6 The Recipe Dataset,[0],[0]
"For prepositional phrases, we include the preposition as part of
3 http://www.ffts.com/recipes.htm 4 http://nlp.stanford.edu/software/corenlp.shtml
5Links to the original recipes, the preprocessed recipes and all extracted events can be found in http://homepages. inf.ed.ac.uk/oabend/event_order.html.
",6 The Recipe Dataset,[0],[0]
the argument.,6 The Recipe Dataset,[0],[0]
Argument indices are determined by their order in the text.,6 The Recipe Dataset,[0],[0]
"The order of events is taken to be the order of their verbs in the text.
",6 The Recipe Dataset,[0],[0]
Linkage Extraction.,6 The Recipe Dataset,[0],[0]
"We focus on a subset of linkage relations, which are relevant for temporal relations.",6 The Recipe Dataset,[0],[0]
"We use Pitler and Nenkova’s (2009) explicit discourse connectives classifier to identify temporal discourse linkers, discarding all other discourse linkers.",6 The Recipe Dataset,[0],[0]
"Once a discourse linker has been detected, we heuristically extract its arguments (namely the pair of verbs it links) according to a deterministic extraction rule defined over the parse tree.",6 The Recipe Dataset,[0],[0]
"We find 28 distinct connectives in our training set, where the 5 most common linkers “until,” “then,” “before,” “when” and “as” cover over 95% of the instances.",6 The Recipe Dataset,[0],[0]
"We extract 36756 such linkages from the corpus, 0.5 linkages per recipe on average.
",6 The Recipe Dataset,[0],[0]
Temporal and Textual Ordering.,6 The Recipe Dataset,[0],[0]
"In order to confirm that temporal and textual order of recipes are generally in agreement, we manually examine the first 20 recipes in our development set.",6 The Recipe Dataset,[0],[0]
"One recipe was excluded as noise6, resulting in 19 recipes and 353 events.",6 The Recipe Dataset,[0],[0]
We identify the sources of misalignment between the linear order and the temporal order of the events.7 13 events (3.7%) did not have any clear temporal orderings.,6 The Recipe Dataset,[0],[0]
"These consisted of mostly negations and modalities (e.g., “do not overbrown!”), sub-section headings (e.g., “Preparation”) or other general statements that do not constitute actions or states.",6 The Recipe Dataset,[0],[0]
"For the remaining 340 events, we compare their linear and the temporal orderings.
",6 The Recipe Dataset,[0],[0]
We estimate the frequency of sub-sequences that contradict the temporal order and confirm that they occur only infrequently.,6 The Recipe Dataset,[0],[0]
"We find that most disagreements fall into these two categories: (1) disjunctions between several events, only one of which will actually take place (e.g., “roll Springerle pin over dough, or press mold into top”); (2) a pair, or less commonly a triplet, of events are expressed in reverse order.",6 The Recipe Dataset,[0],[0]
"For instance, “place on greased and floured cookie sheet,” where greasing and flouring should occur before the placing action.",6 The Recipe Dataset,[0],[0]
"We note that assuming the alignment of the temporal and textual order
6This did not result from an extraction problem, but rather from the recipe text itself being too noisy to interpret.
7Events are parsed manually so to avoid confounding the results with the parser’s performance.
of recipes does not suggest that the textual order is the only order of events that would yield the same outcome.
",6 The Recipe Dataset,[0],[0]
"We compute the Kendall’s Tau correlation, a standard measure for information ordering (Lapata, 2006), between the temporal and linear orderings for each recipe.",6 The Recipe Dataset,[0],[0]
"In cases of several events that happen simultaneously (including disjunctions), we take their ordinals to be equal.",6 The Recipe Dataset,[0],[0]
"For instance, for three events where the last two happen at the same time, we take their ordering to be (1,2,2) in our analysis.",6 The Recipe Dataset,[0],[0]
"We find that indeed temporal and textual orderings are in very high agreement, with 6 recipes of the 19 perfectly aligned.",6 The Recipe Dataset,[0],[0]
The average Kendall’s Tau between the temporal ordering and the linear one is 0.924.,6 The Recipe Dataset,[0],[0]
Evaluation.,7 Experimental Setup,[0],[0]
We compute the accuracy of our algorithms by comparing the predicted order to the one in which the events are written.,7 Experimental Setup,[0],[0]
"We first compute the number of exact matches, denoted with EXACT, namely the percentage of recipes in which the predicted and the textual orders are the same.
",7 Experimental Setup,[0],[0]
"For a more detailed analysis of imperfect predictions, we compute the agreement between subsequences of the orderings.",7 Experimental Setup,[0],[0]
We borrow the notion of a “concordant pair” from the definition of Kendall’s Tau and generalize it to capture agreement of longer sub-sequences.,7 Experimental Setup,[0],[0]
"Two k-tuples of integers (x1, ..., xk) and (y1, ..., yk) are said to “agree in order” if for every 1 ≤",7 Experimental Setup,[0],[0]
"i < j ≤ k, xi < xj iff yi < yj .",7 Experimental Setup,[0],[0]
"Given two orderings of the same recipe O1 = (eτ(1), ..., eτ(m)) and O2 = (eσ(1), ..., eσ(m)) (where τ and σ are permutations over [m] = {1, . . .",7 Experimental Setup,[0],[0]
",m})",7 Experimental Setup,[0],[0]
"and given a sequence of k monotonically increasing indices t = (i1, ..., ik), t is said to be a “concordant k-tuple” of O1 andO2 if (τ(i1), ..., τ(ik))",7 Experimental Setup,[0],[0]
"and (σ(i1), ..., σ(ik)) agree in order, as defined above.
",7 Experimental Setup,[0],[0]
"Denote the unordered recipes of the test data as {Ri}Ni=1, where Ri = {ei1, ..., eimi} ⊂ U for all i, and their target orderings Σ = {σi}Ni=1, where σi is a permutation over [mi].",7 Experimental Setup,[0],[0]
"Assume we wish to evaluate a set of predicted orderings for this test data T = {τi}Ni=1, where again τi is a permutation over [mi].",7 Experimental Setup,[0],[0]
"Denote the number of concordant k-tuples of σi and τi as conc(σi, τi).",7 Experimental Setup,[0],[0]
"The total number of of
monotonically increasing k-tuples of indices is ( mi k ) .",7 Experimental Setup,[0],[0]
"The k-wise (micro-averaged) accuracy of T with respect to Σ is:
acck(Σ,T) = ∑N
i=1",7 Experimental Setup,[0],[0]
"conc(σi, τi)∑N i=1",7 Experimental Setup,[0],[0]
( mi k ),7 Experimental Setup,[0],[0]
"Any k-tuples containing the start node s or the end node f are excluded, as their ordering is trivial.",7 Experimental Setup,[0],[0]
Recipes of length less than k are discarded when computing acck.,7 Experimental Setup,[0],[0]
A micro-averaged accuracy measure is used so as not to disproportionately weigh short recipes.,7 Experimental Setup,[0],[0]
"However, in order to allow comparison to mean Kendall’s Tau, commonly used in works on order learning, we further report a macroaveraged acc2 by computing acc2 for each recipe separately, and taking the average of resulting accuracy levels.",7 Experimental Setup,[0],[0]
"Average Kendall’s Tau can now be computed by 2acc2−1 for the macro-averaged acc2 score.
",7 Experimental Setup,[0],[0]
Data.,7 Experimental Setup,[0],[0]
"We randomly partition the text into training, test and development sets, taking an 80-10-10 percent split.",7 Experimental Setup,[0],[0]
We do not partition the individual files so as to avoid statistical artifacts introduced by recipe duplications or near-duplications.,7 Experimental Setup,[0],[0]
"The training, development and test sets contain 58038, 7667 and 7779 recipes respectively.",7 Experimental Setup,[0],[0]
"The total number of feature template instantiations in the training data is 8.94M.
Baselines and Algorithms.",7 Experimental Setup,[0],[0]
We compare three learning algorithms.,7 Experimental Setup,[0],[0]
GLOBAL-PRC is the structured perceptron algorithm that uses ILP inference.,7 Experimental Setup,[0],[0]
GREEDY-PRC is a structured perceptron in which inference is done greedily.,7 Experimental Setup,[0],[0]
GREEDY-LOGLIN is the locally normalized log-linear model with greedy inference.,7 Experimental Setup,[0],[0]
"RANDOM randomly (uniformly) selects a permutation of the recipe’s events.
",7 Experimental Setup,[0],[0]
Experimental Settings.,7 Experimental Setup,[0],[0]
"The structured perceptron algorithms, GLOBAL-PRC and GREEDY-PRC, are run with a learning rate of 0.1 for 3 iterations.",7 Experimental Setup,[0],[0]
"To avoid exceedingly long runs, we set a time limit in seconds β on the running time of each ILP inference stage used in GLOBAL-PRC.",7 Experimental Setup,[0],[0]
"We consider two training scenarios: 4K, which trains on the first 4K recipes of the training set, and 58K, which trains on the full training data of 58K recipes.",7 Experimental Setup,[0],[0]
In GLOBAL-PRC we set β to be 30 seconds for the 4K,7 Experimental Setup,[0],[0]
"scenario, and 5 seconds in the 58K scenario.",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
The number of threads was limited to 3.,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"Where the time limit is reached before an optimal solution is found, the highest scoring Hamiltonian path found up to that point is returned by the ILP solver.",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"In the infrequent samples where no feasible solution is found during training, the sample is skipped over, while at test time, we perform greedy inference instead.
",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
We define the following feature sets.,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"Fr includes only features of class Frequency, while Fr + Lex includes features from both the Frequency and Lexical categories.",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
Full includes all feature sets.,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"All above feature sets take C, the reference corpus for computing FREQUENCY features, to be the entire 58K training samples in both scenarios.",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"In the 4K scenario, we also experiment with FrLim, which includes all features, but takes C to contain only the 4K samples of the training data.
",K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
We use the Gurobi package for ILP.8 Brown clusters are extracted from the 58K samples of the training data using Liang’s implementation.9,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
The convex log-likelihood function of GREEDY-LOGLIN is optimized using LBFGS.,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
All features are selected and all parameters are tuned using the development set.,K GLOBAL-PRC 68.9 76.4 41.3 24.8 34.4,[0],[0]
"Table 2 presents the results of the three major algorithms in the two main scenarios 58K and 4K.
8 http://www.gurobi.com 9 https://github.com/percyliang/brown-cluster
We find that the structured perceptron algorithm, GLOBAL-PRC, obtains the best results in both cases and under all evaluation measures.",8 Results,[0],[0]
"The importance of global optimization was also stressed in other works on event ordering (Chambers and Jurafsky, 2008a; Talukdar et al., 2012).
",8 Results,[0],[0]
"In order to assess the contribution of the different components of the model of the best scoring model, GLOBAL-PRC, we compare the performance of the different feature sets and settings of β on the development set in 4K (Table 3).",8 Results,[0],[0]
Results reveal the strong impact of the Frequency feature set on the results.,8 Results,[0],[0]
"Using this category set alone (Fr) yields slightly lower results than using the full feature set, while estimating the Frequency features on a small corpus (FrLim) lowers results dramatically.",8 Results,[0],[0]
"Adding Lexical and Brown features yields a small improvement over using Frequency alone.
",8 Results,[0],[0]
"While Table 3 demonstrates the importance of β in the performance of GLOBAL-PRC, it also shows that on a limited time budget, a small training set and few features (4K, Fr) and a reasonably small β (5) can yield competitive results.",8 Results,[0],[0]
Increasing β from 5 to 30 generally improves results by 2 to 3 percent absolute.,8 Results,[0],[0]
"The importance of β is further demonstrated in Table 2, where performance with 4K training instances and β = 30 is better than with 58K training instances and β = 5.",8 Results,[0],[0]
"Preliminary experiments conducted on the development data with higher values of β of 60 and 120 suggest that further increasing β
yields no further improvement.",8 Results,[0],[0]
Previous studies evaluated their models on the related problem of distinguishing randomly permuted and correctly ordered chains of events (§2).,8 Results,[0],[0]
In this paper we generalize this task to complete event ordering.,8 Results,[0],[0]
"In order to demonstrate the relative difficulty of the tasks, we apply our highest scoring model (4K, Fr + Le) to the binary task (without re-training it).",8 Results,[0],[0]
We do so by computing the percentage of cases in which the correct ordering obtains a higher score than an average ordering.,8 Results,[0],[0]
"The high resulting accuracy of 93%, as opposed to considerably lower accuracies obtained under ordering evaluation measures, reflects the relative difficulty of the tasks.
",8 Results,[0],[0]
"The proposed edge-factored model can easily capture pair-wise ordering relations between events, but is more limited in accounting for relations between larger sets of events.",8 Results,[0],[0]
A simple way of doing so is by adding the feature ∑ e P (ei|e)P (e|ej) between events ei and ej (in addition to the regular transition probabilities P (ei|ej)).,8 Results,[0],[0]
"However, preliminary experimentation with this technique did not yield improved performance.",8 Results,[0],[0]
"Future work will address higher-order models that straightforwardly account for such long-distance dependencies.
",8 Results,[0],[0]
"To qualitatively assess what generalizations are learned by the model, we apply GLOBAL-PRC to the development data and look at what event pairs obtained either particularly high or particularly low results.",8 Results,[0],[0]
"For each pair of predicates and their first arguments (a1,c11), (a
2,c21), we compute the average weight of an edge connecting events of these types, discarding pairs of frequency less than 20.
",8 Results,[0],[0]
"The 20 highest scoring edges contain pairs such as (“add,” “mixing after addition”), (“beat whites,” “fold into mixture”) and (“cover for minutes,” “cook”), in addition to a few noisy pairs resulting from parser errors.",8 Results,[0],[0]
The 20 lowest scoring edges contain event pairs that are likely to appear in the opposite order.,8 Results,[0],[0]
"11 of the cases include as a first argument the predicates “serve,” “cool” or “chill,” which are likely to occur at the end of a recipe.",8 Results,[0],[0]
"3 other edges linked duplications (e.g., (“reduce heat,” “reduce heat”)), which are indeed unlikely to immediately follow one another.",8 Results,[0],[0]
"These findings suggest the importance of detecting both lexical pairs that are unlikely to follow one another, in addition to those that are likely to.",8 Results,[0],[0]
"We addressed the problem of lexical event ordering, and developed an edge-factored model for tackling it.",9 Conclusion,[0],[0]
"We rely on temporally aligned texts, using a new dataset of cooking recipes as a test case, thereby avoiding the need for costly and error-prone manual annotation.",9 Conclusion,[0],[0]
"We present results of a pair-wise accuracy of over 70% using a basic set of features, and show the utility of the structured perceptron algorithm over simpler greedy and local approaches.",9 Conclusion,[0],[0]
"The setup we explore, which uses a discriminative model and an ILP formulation, is easy to extend both in terms of features and in terms of more complex formal constraints and edge dependencies, as was done in graph-based dependency parsing (McDonald et al., 2005).",9 Conclusion,[0],[0]
"Future work will address the extension of the feature set and model, and the application of this model to temporal semantics and planning tasks.",9 Conclusion,[0],[0]
"We will further address the application of semi-supervised variants of the proposed techniques (e.g., self-training) to other domains, where no sizable corpora of temporally aligned data can be found.",9 Conclusion,[0],[0]
"We would like to thank Nathan Schneider, Roy Schwartz, Bonnie Webber and the members of the Probmodels group at the University of Edinburgh for helpful comments.",Acknowledgments,[0],[0]
This work was supported by ERC Advanced Fellowship 249520 GRAMPLUS.,Acknowledgments,[0],[0]
Let G(S),Appendix A: Maximal Hamiltonian Path,[0],[0]
=,Appendix A: Maximal Hamiltonian Path,[0],[0]
"(S ∪ {s, f}, E(S)) be an almostcomplete directed graph with E = E(S)",Appendix A: Maximal Hamiltonian Path,[0],[0]
= (S ∪ {s}) × (S ∪ {f}).,Appendix A: Maximal Hamiltonian Path,[0],[0]
"Let cij ∈ R be weights for its edges ((i, j) ∈ E).",Appendix A: Maximal Hamiltonian Path,[0],[0]
"A Hamiltonian path between s, f ∈ V can be found by solving the following program, returning P = {(i, j)|xij = 1}.
max xij∈{0,1} : (i,j)∈E
ui∈Z : i∈V
n∑ i6=j cijxij
s.t. n∑
i=0,i6=j xij = 1 ∀j 6= s; n∑ j=0,j 6",Appendix A: Maximal Hamiltonian Path,[0],[0]
=i xij = 1 ∀i 6=,Appendix A: Maximal Hamiltonian Path,[0],[0]
"e;
ui − uj + |V |xij ≤ |V",Appendix A: Maximal Hamiltonian Path,[0],[0]
"| − 1 ∀(i, j) ∈ E",Appendix A: Maximal Hamiltonian Path,[0],[0]
Extensive lexical knowledge is necessary for temporal analysis and planning tasks.,abstractText,[0],[0]
We address in this paper a lexical setting that allows for the straightforward incorporation of rich features and structural constraints.,abstractText,[0],[0]
"We explore a lexical event ordering task, namely determining the likely temporal order of events based solely on the identity of their predicates and arguments.",abstractText,[0],[0]
We propose an “edgefactored” model for the task that decomposes over the edges of the event graph.,abstractText,[0],[0]
We learn it using the structured perceptron.,abstractText,[0],[0]
"As lexical tasks require large amounts of text, we do not attempt manual annotation and instead use the textual order of events in a domain where this order is aligned with their temporal order, namely cooking recipes.",abstractText,[0],[0]
Lexical Event Ordering with an Edge-Factored Model,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 14–19 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2003",text,[0],[0]
"Similar to many other tasks, lexical features are a major source of information in current coreference resolvers.",1 Introduction,[0],[0]
Coreference resolution is a set partitioning problem in which each resulting partition refers to an entity.,1 Introduction,[0],[0]
"As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity.",1 Introduction,[0],[0]
"However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains.
",1 Introduction,[0],[0]
"The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system
by Lee et al. (2013).",1 Introduction,[0],[0]
"However, this substantial improvement does not seem to be visible in downstream tasks.",1 Introduction,[0],[0]
"Worse, the difference between stateof-the-art coreference resolvers and the rule-based system drops significantly when they are applied on a new dataset, even with consistent definitions of mentions and coreference relations (Ghaddar and Langlais, 2016a).
",1 Introduction,[0],[0]
"In this paper, we show that if we mainly rely on lexical features, as it is the case in state-of-theart coreference resolvers, overfitting become more sever.",1 Introduction,[0],[0]
Overfitting to the training dataset is a problem that cannot be completely avoided.,1 Introduction,[0],[0]
"However, there is a notable overlap between the CoNLL training, development and test sets that encourages overfitting.",1 Introduction,[0],[0]
"Therefore, the current coreference evaluation scheme is flawed by only evaluating on this overlapped validation set.",1 Introduction,[0],[0]
"To ensure meaningful improvements in coreference resolution, we believe an out-of-domain evaluation is a must in the coreference literature.",1 Introduction,[0],[0]
"The large difference in performance between coreference resolvers that use lexical features and ones which do not, implies the importance of lexical features.",2 Lexical Features,[0],[0]
"Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.g. definiteness and syntactic roles, which were previously modeled by heuristic features.",2 Lexical Features,[0],[0]
Durrett and Klein (2013) use exact surface forms as lexical features.,2 Lexical Features,[0],[0]
"However, when word embeddings are used instead of surface forms, the use of lexical features is even more beneficial.",2 Lexical Features,[0],[0]
Word embeddings are an efficient way of capturing semantic relatedness.,2 Lexical Features,[0],[0]
"Especially, they provide an efficient way for describing the context of mentions.
",2 Lexical Features,[0],[0]
"Durrett and Klein (2013) show that the addition of some heuristic features like gender, num-
14
ber, person and animacy agreements and syntactic roles on top of their lexical features does not result in a significant improvement.
",2 Lexical Features,[0],[0]
"deep-coref, the state-of-the-art coreference resolver, follows the same approach.",2 Lexical Features,[0],[0]
"Clark and Manning (2016b) capture the required information for resolving coreference relations by using a large number of lexical features and a small set of nonlexical features including string match, distance, mention type, speaker and genre features.",2 Lexical Features,[0],[0]
"The main difference is that Clark and Manning (2016b) use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013).
",2 Lexical Features,[0],[0]
"Based on the error analysis by cort (Martschat and Strube, 2014), in comparison to systems that do not use word embeddings, deep-coref has fewer recall and precision errors especially for pronouns.",2 Lexical Features,[0],[0]
"For example, deep-coref correctly recognizes around 83 percent of non-anaphoric “it” in the CoNLL development set.",2 Lexical Features,[0],[0]
This could be a direct result of a better context representation by word embeddings.,2 Lexical Features,[0],[0]
"Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains.",3 Out-of-Domain Evaluation,[0],[0]
"As reported by Ghaddar and Langlais (2016b), state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (Lee et al., 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.",3 Out-of-Domain Evaluation,[0],[0]
"The results of some of recent coreference resolvers on this dataset are listed in Table 1.
",3 Out-of-Domain Evaluation,[0],[0]
"The results are reported using MUC (Vilain
et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016).
",3 Out-of-Domain Evaluation,[0],[0]
"berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features.
",3 Out-of-Domain Evaluation,[0],[0]
cort is the mention-ranking model of Martschat and Strube (2015).,3 Out-of-Domain Evaluation,[0],[0]
"cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the head word, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features.",3 Out-of-Domain Evaluation,[0],[0]
"berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a).
",3 Out-of-Domain Evaluation,[0],[0]
deep-coref is the mention-ranking model of Clark and Manning (2016b).,3 Out-of-Domain Evaluation,[0],[0]
"deep-coref incorporates a large set of embeddings, i.e. embeddings of the head, first, last, two previous/following words, and the dependency governor of a mention in addition to the averaged embeddings of the five previous/following words, all words of the mention, sentence words, and document words.",3 Out-of-Domain Evaluation,[0],[0]
"deep-coref also incorporates type, length, and position of a mention, whether the mention is nested in any other mention, distance of two mentions, speaker features and a small set of string match features.
",3 Out-of-Domain Evaluation,[0],[0]
For deep-coref [conll] the averaged CoNLL score is used to select the best trained model on the development set.,3 Out-of-Domain Evaluation,[0],[0]
"deep-coref [lea] uses the LEA
metric (Moosavi and Strube, 2016) for choosing the best model.",3 Out-of-Domain Evaluation,[0],[0]
It is worth noting that the results of deep-coref ’s ranking model may be slightly different at various experiments.,3 Out-of-Domain Evaluation,[0],[0]
"However, the performance of deep-coref [lea] is always higher than that of deep-coref [conll].
",3 Out-of-Domain Evaluation,[0],[0]
We add WikiCoref’s words to deep-coref ’s dictionary for both deep-coref [conll] and deep-coref [lea].,3 Out-of-Domain Evaluation,[0],[0]
deep-coref− reports the performance of deep-coref [lea] in which WikiCoref’s words are not incorporated into the dictionary.,3 Out-of-Domain Evaluation,[0],[0]
"Therefore, for deep-coref−, WikiCoref’s words that do not exist in CoNLL will be initialized randomly instead of using pre-trained word2vec word embeddings.",3 Out-of-Domain Evaluation,[0],[0]
The performance gain of deep-coref [lea] in comparison to deep-coref− indicates the benefit of using pre-trained word embeddings and word embeddings in general.,3 Out-of-Domain Evaluation,[0],[0]
"Henceforth, we refer to deep-coref [lea] as deep-coref.",3 Out-of-Domain Evaluation,[0],[0]
"In this section, we investigate how much lexical features contribute to the fact that current improvements in coreference resolution do not properly apply to a new domain.
",4 Why do Improvements Fade Away?,[0],[0]
Table 2 shows the ratio of non-pronominal coreferent mentions in the CoNLL test set that also appear as coreferent mentions in the training data.,4 Why do Improvements Fade Away?,[0],[0]
"These high ratios indicate a high degree of overlap between the mentions of the CoNLL datasets.
",4 Why do Improvements Fade Away?,[0],[0]
The highest overlap between the training and test sets exists in genre pt (Bible).,4 Why do Improvements Fade Away?,[0],[0]
The tc (telephone conversation) genre has the lowest overlap for non-pronominal mentions.,4 Why do Improvements Fade Away?,[0],[0]
"However, this genre includes a large number of pronouns.",4 Why do Improvements Fade Away?,[0],[0]
"We choose wb (weblog) and pt for our analysis as two genres with low and high degree of overlap.
",4 Why do Improvements Fade Away?,[0],[0]
"Table 3 shows the results of the examined coreference resolvers when the test set only includes one genre, i.e. pt or wb, in two different settings: (1) the training set includes all genres (in-domain
evaluation), and (2) the corresponding genre of the test set is excluded from the training and development sets (out-of-domain evaluation).
",4 Why do Improvements Fade Away?,[0],[0]
berkeley-final is the coreference resolver of Durrett and Klein (2013) with the FINAL feature set explained in Section 3.,4 Why do Improvements Fade Away?,[0],[0]
"berkeley-surface is the same coreference resolver with only surface features, i.e. ancestry, gender, number, same speaker and nested features are excluded from the FINAL feature set.
",4 Why do Improvements Fade Away?,[0],[0]
"cort−lexical is a version of cort in which no lexical feature is used, i.e. the head, first, last, governor, preceding and following words of a mention are excluded.
",4 Why do Improvements Fade Away?,[0],[0]
"For in-domain evaluations we train deep-coref ’s ranking model for 100 iterations, i.e. the setting used by Clark and Manning (2016a).",4 Why do Improvements Fade Away?,[0],[0]
"However, based on the performance on the development set, we only train the model for 50 iterations in out-ofdomain evaluations.
",4 Why do Improvements Fade Away?,[0],[0]
"The results of the pt genre show that when there is a high overlap between the training and test datasets, the performance of all learning-based classifiers significantly improves.",4 Why do Improvements Fade Away?,[0],[0]
deep-coref has the largest gain from including pt in the training data that is more than 13% based on the LEA score.,4 Why do Improvements Fade Away?,[0],[0]
cort uses both lexical and a relatively large number of non-lexical features while berkeley-surface is a pure lexicalized system.,4 Why do Improvements Fade Away?,[0],[0]
"However, the difference between the berkeley-surface’s performances when pt is included or excluded from the training data is lower than that of cort.",4 Why do Improvements Fade Away?,[0],[0]
berkeley uses feature-value pruning so lexical features that occur fewer than 20 times are pruned from the training data.,4 Why do Improvements Fade Away?,[0],[0]
"Maybe, this is the reason that berkeley’s performance difference is less than other lexicalized systems in highly overlapping datasets.
",4 Why do Improvements Fade Away?,[0],[0]
"For a less overlapping genre, i.e. wb, the performance gain of including the genre in the training data is significantly lower for all lexicalized systems.",4 Why do Improvements Fade Away?,[0],[0]
"Interestingly, the performance of berkeleyfinal, cort and cort−lexical increases for the wb genre when this genre is excluded from the training set.",4 Why do Improvements Fade Away?,[0],[0]
"deep-coref, which uses a complex deep neural network and mainly lexical features, has the highest gain from the redundancy in the training and test datasets.",4 Why do Improvements Fade Away?,[0],[0]
"As we use more complex neural networks, there is more capacity for brute-force memorization of the training dataset.
",4 Why do Improvements Fade Away?,[0],[0]
"It is also worth noting that the performance gains and drops in out-of-domain evaluations are
not entirely because of lexical features, as the performance of cort−lexical also drops significantly in pt out-of-domain evaluation.",4 Why do Improvements Fade Away?,[0],[0]
The classifier may also memorize other properties of the seen mentions in the training data.,4 Why do Improvements Fade Away?,[0],[0]
"However, in comparison to features like gender and number agreement or syntactic roles, lexical features have the highest potential for overfitting.
",4 Why do Improvements Fade Away?,[0],[0]
We further analyze the output of deep-coref on the development set.,4 Why do Improvements Fade Away?,[0],[0]
The all rows in Table 4 show the number of pairwise links that are created by deep-coref on the development set for different mention types.,4 Why do Improvements Fade Away?,[0],[0]
"The seen rows show the ratio of each category of links for which the (antecedent head, anaphor head) pair is seen in the training set.",4 Why do Improvements Fade Away?,[0],[0]
All ratios are surprisingly high.,4 Why do Improvements Fade Away?,[0],[0]
"The most worrisome cases are those in which both mentions are either a proper name or a common noun.
",4 Why do Improvements Fade Away?,[0],[0]
Table 5 further divides the links of Table 4 based on whether they are correct coreferent links.,4 Why do Improvements Fade Away?,[0],[0]
"The results of Table 5 show that most of the incorrect links are also made between the mentions that are both seen in the training data.
",4 Why do Improvements Fade Away?,[0],[0]
"The high ratios indicate that (1) there is a high
overlap between the mention pairs of the training and development sets, and (2) even though that deep-coref uses generalized word embeddings instead of exact surface forms, it is strongly biased towards the seen mentions.
",4 Why do Improvements Fade Away?,[0],[0]
We analyze the links that are created by Stanford’s rule-based system and compute the ratio of the links that exist in the training set.,4 Why do Improvements Fade Away?,[0],[0]
All corresponding ratios are lower than those of deep-coref in Table 5.,4 Why do Improvements Fade Away?,[0],[0]
"However, the ratios are surprisingly high for a system that does not use the training data.",4 Why do Improvements Fade Away?,[0],[0]
This analysis emphasizes the overlap in the CoNLL datasets.,4 Why do Improvements Fade Away?,[0],[0]
"Because of this high overlap, it is not easy to assess the generalizability of a coreference resolver to unseen mentions on the CoNLL dataset given its official split.
",4 Why do Improvements Fade Away?,[0],[0]
"We also compute the ratios of Table 5 for the missing links that are associated with the recall er-
rors of deep-coref.",4 Why do Improvements Fade Away?,[0],[0]
"We compute the recall errors by cort error analysis tool (Martschat and Strube, 2014).",4 Why do Improvements Fade Away?,[0],[0]
Table 6 shows the corresponding ratios for recall errors.,4 Why do Improvements Fade Away?,[0],[0]
"The lower ratios of Table 6 in comparison to those of Table 4 emphasize the bias of deep-coref towards the seen mentions.
",4 Why do Improvements Fade Away?,[0],[0]
"For example, the deep-coref links include 31 cases in which both mentions are either proper names or common nouns and the head of one of the mentions is “country”.",4 Why do Improvements Fade Away?,[0],[0]
"For all these links, “country” is linked to a mention that is seen in the training data.",4 Why do Improvements Fade Away?,[0],[0]
"Therefore, this raises the question how the classifier would perform on a text about countries not mentioned in the training data.
",4 Why do Improvements Fade Away?,[0],[0]
Memorizing the pairs in which one of them is a common noun could help the classifier to capture world knowledge to some extent.,4 Why do Improvements Fade Away?,[0],[0]
"From the seen pairs like (Haiti, his country), and (Guangzhou, the city) the classifier could learn that “Haiti” is a country and “Guangzhou” is a city.",4 Why do Improvements Fade Away?,[0],[0]
"However, it is questionable how useful word knowledge is if it is mainly based on the training data.
",4 Why do Improvements Fade Away?,[0],[0]
The coreference relation of two nominal noun phrases with no head match can be very hard to resolve.,4 Why do Improvements Fade Away?,[0],[0]
"The resolution of such pairs has been referred to as capturing semantic similarity (Clark and Manning, 2016b).",4 Why do Improvements Fade Away?,[0],[0]
deep-coref links 49 such pairs on the development set.,4 Why do Improvements Fade Away?,[0],[0]
"Among all these links, only 5 pairs are unseen on the training set and all of them are incorrect links.
",4 Why do Improvements Fade Away?,[0],[0]
The effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment.,4 Why do Improvements Fade Away?,[0],[0]
They show that state-of-the-art classifiers memorize words from the training data.,4 Why do Improvements Fade Away?,[0],[0]
The classifiers benefit from this lexical memorization when there are common words between the training and test sets.,4 Why do Improvements Fade Away?,[0],[0]
"We show the extensive use of lexical features biases coreference resolvers towards seen mentions.
",5 Discussion,[0],[0]
This bias holds us back from developing more robust and generalizable coreference resolvers.,5 Discussion,[0],[0]
"After all, while coreference resolution is an important step for text understanding, it is not an endtask.",5 Discussion,[0],[0]
Coreference resolvers are going to be used in tasks and domains for which coreference annotated corpora may not be available.,5 Discussion,[0],[0]
"Therefore, generalizability should be brought into attention in developing coreference resolvers.
",5 Discussion,[0],[0]
"Moreover, we show that there is a significant overlap between the training and validation sets in the CoNLL dataset.",5 Discussion,[0],[0]
"The LEA metric (Moosavi and Strube, 2016) is introduced as an attempt to make coreference evaluations more reliable.",5 Discussion,[0],[0]
"However, in order to ensure valid developments on coreference resolution, it is not enough to have reliable evaluation metrics.",5 Discussion,[0],[0]
The validation set on which the evaluations are performed also needs to be reliable.,5 Discussion,[0],[0]
"A dataset is reliable for evaluations if a considerable improvement on this dataset indicates a better solution for the coreference problem instead of a better exploitation of the dataset itself.
",5 Discussion,[0],[0]
This paper is not intended to argue against the use of lexical features.,5 Discussion,[0],[0]
"Especially, when word embeddings are used as lexical features.",5 Discussion,[0],[0]
The incorporation of word embeddings is an efficient way for capturing semantic relatedness.,5 Discussion,[0],[0]
Maybe we should use them more for describing the context and less for describing the mentions themselves.,5 Discussion,[0],[0]
"Pruning rare lexical features plus incorporating more generalizable features could also help to prevent overfitting.
",5 Discussion,[0],[0]
"To ensure more meaningful improvements, we ask to incorporate out-of-domain evaluations in the current coreference evaluation scheme.",5 Discussion,[0],[0]
"Outof-domain evaluations could be performed by using either the existing genres of the CoNLL dataset or by using other existing coreference annotated datasets like WikiCoref, MUC or ACE.",5 Discussion,[0],[0]
The authors would like to thank Kevin Clark for answering all of our questions regarding deepcoref.,Acknowledgments,[0],[0]
We would also like to thank the three anonymous reviewers for their thoughtful comments.,Acknowledgments,[0],[0]
"This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany.",Acknowledgments,[0],[0]
The first author has been supported by a Heidelberg Institute for Theoretical Studies PhD. scholarship.,Acknowledgments,[0],[0]
Lexical features are a major source of information in state-of-the-art coreference resolvers.,abstractText,[0],[0]
Lexical features implicitly model some of the linguistic phenomena at a fine granularity level.,abstractText,[0],[0]
They are especially useful for representing the context of mentions.,abstractText,[0],[0]
In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers.,abstractText,[0],[0]
"We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains.",abstractText,[0],[0]
"Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.",abstractText,[0],[0]
Lexical Features in Coreference Resolution: To be Used With Caution,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4717–4724 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4717",text,[0],[0]
"The formal semantics literature has long been concerned with the complex array of inferences that different open class lexical items trigger (Kiparsky and Kiparsky, 1970; Karttunen, 1971a,b; Horn, 1972; Karttunen and Peters, 1979; Heim, 1992; Simons, 2001, 2007; Simons et al., 2010; Abusch, 2002, 2010; Gajewski, 2007; Anand and Hacquard, 2013, 2014).",1 Introduction,[0],[0]
"For example, why does (1a) give rise to the inference (2a), while the structurally identical (1b) triggers the inference (2b)?",1 Introduction,[0],[0]
"(1) a. Jo doesn’t believe that Bo left.
",1 Introduction,[0],[0]
b. Jo doesn’t know that Bo left.,1 Introduction,[0],[0]
"(2) a. Jo believes that Bo didn’t leave.
",1 Introduction,[0],[0]
b. Bo left.,1 Introduction,[0],[0]
"c. Bo didn’t leave.
",1 Introduction,[0],[0]
A major finding of this literature is that lexically triggered inferences are conditioned by surprising aspects of the syntactic context that a word occurs in.,1 Introduction,[0],[0]
"For example, while (3a), (3b), and (4a) trigger the inference (2b), (4b) triggers the inference (2c).",1 Introduction,[0],[0]
"(3) a. Jo remembered that Bo left.
",1 Introduction,[0],[0]
b. Jo didn’t remember that Bo left.,1 Introduction,[0],[0]
"(4) a. Bo remembered to leave.
",1 Introduction,[0],[0]
"b. Bo didn’t remember to leave.
",1 Introduction,[0],[0]
"Accurately capturing such interactions – e.g. between clause-embedding verbs, negation, and embedded clause type – is important for any system that aims to do general natural language inference (MacCartney et al. 2008",1 Introduction,[0],[0]
"et seq; cf. Dagan et al. 2006) or event extraction (see Grishman and Sundheim 1996 et seq), and it seems unlikely to be a trivial phenomenon to capture, given the complexity and variability of the inferences involved (see, e.g., Karttunen, 2012, 2013; Karttunen et al., 2014; van Leusen, 2012; White, 2014; Baglini and Francez, 2016; Nadathur, 2016, on implicatives).
",1 Introduction,[0],[0]
"In this paper, we investigate how well current state-of-the-art neural systems for a subtask of general event extraction – event factuality prediction (EFP; Nairn et al., 2006; Saurı́ and Pustejovsky, 2009, 2012; de Marneffe et al., 2012; Lee et al., 2015; Stanovsky et al., 2017; Rudinger et al., 2018) – capture inferential interactions between lexical items and syntactic context – lexicosyntactic inferences – when trained on current event factuality datasets.",1 Introduction,[0],[0]
"Probing these particular systems is useful for understanding neural systems’ behavior more generally because (i) the best performing neural models for EFP (Rudinger et al., 2018) are simple instances of common baseline models; and (ii) the task itself is relatively constrained.
",1 Introduction,[0],[0]
"To do this, we substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018) to cover all English clause-embedding verbs in a variety of the syntactic contexts covered by recent psycholinguistic work (White and Rawlins, 2016), and we use the resulting dataset – MegaVeridicality2 – to probe these models’ behavior.",1 Introduction,[0],[0]
"We focus on clause-embedding verbs because they show effectively every possible patterning of lexicosyntactic inference (Karttunen, 2012).
",1 Introduction,[0],[0]
We discuss three findings: (i) Tree biLSTMs,1 Introduction,[0],[0]
"(TbiLSTMs) are better able to correctly predict lexicosyntactic inferences than linear-chain biLSTMs
(L-biLSTMs); (ii) L-biLSTMs and T-biLSTMs capture different lexicosyntactic inferences, and thus ensembling their predictions can reliably improve performance; and (iii) even when ensembled, these models show systematic errors – e.g. performing well when the polarity of the matrix clause matches the polarity of the true inference, but poorly when these polarities mismatch.
",1 Introduction,[0],[0]
We furthermore release MegaVeridicality2 at MegaAttitude.io as a benchmark for probing the ability of neural systems – whether for factuality prediction or for general natural language inference – to capture lexicosyntactic inference.,1 Introduction,[0],[0]
"We substantially extend the MegaVeridicality1 dataset (White and Rawlins, 2018), which contains factuality judgments for all English clauseembedding verbs that take tensed subordinate clauses.",2 Data collection,[0],[0]
"In White and Rawlins’s annotation protocol, all verbs that are grammatical with such subordinate clauses – based on the MegaAttitude dataset (White and Rawlins, 2016) – are slotted into contexts either like (5a) or (5b), depending on whether they take a direct object or not.",2 Data collection,[0],[0]
"(5) a. Someone {knew, didn’t know} that a par-
ticular thing happened.",2 Data collection,[0],[0]
"b. Someone {was, wasn’t} told that a particu-
lar thing happened.",2 Data collection,[0],[0]
"For each sentence generated in this way, 10 different annotators are asked to answer the question did that thing happen?:",2 Data collection,[0],[0]
"yes, maybe or maybe not, no.
",2 Data collection,[0],[0]
There are two important aspects of these contexts to note.,2 Data collection,[0],[0]
"First, all lexical items besides the embedding verbs are semantically bleached to ensure that the measured lexicosyntactic inferences are only due to interactions between the embedding predicate – e.g. know or tell – and the syntactic context.",2 Data collection,[0],[0]
"Second, the matrix polarity – i.e. the presence or absence of not as a direct dependent of the embedding verb – is manipulated to create two sentences for each verb-context pair.
",2 Data collection,[0],[0]
"Our extension, MegaVeridicality2, includes judgments for a variety of infinitival subordinate clause types, exemplified in (6).1 We investigate infinitival clauses because they can give rise to dif-
1We also explicitly manipulate two aspects of the subordinate clause in our extension of the MegaVeridicality dataset: (i) how NP embedded subjects are introduced; and (ii) whether the embedded clause contains an eventive predicate (do, happen) or a stative predicate (have).",2 Data collection,[0],[0]
"See Appendix A for details on the reasoning behind these manipulations.
",2 Data collection,[0],[0]
ferent lexicosyntactic inferences than finite subordinate clauses – e.g. compare (3) and (4).,2 Data collection,[0],[0]
"(6) a. Someone {needed, didn’t need} for a par-
ticular thing to happen.",2 Data collection,[0],[0]
"b. Someone {wanted, didn’t want} a particu-
lar person to do, have a particular thing.",2 Data collection,[0],[0]
"c. Someone {wanted, didn’t want} a particu-
lar person to have a particular thing.",2 Data collection,[0],[0]
d.,2 Data collection,[0],[0]
"A particular person {was, wasn’t} over-
joyed to do a particular thing.",2 Data collection,[0],[0]
e.,2 Data collection,[0],[0]
"A particular person {was, wasn’t} over-
joyed to have a particular thing.",2 Data collection,[0],[0]
f.,2 Data collection,[0],[0]
"A particular person {managed, didn’t man-
age} to do a particular thing.",2 Data collection,[0],[0]
"g. A particular person {managed, didn’t man-
age} to have a particular thing.",2 Data collection,[0],[0]
"For each sentence, we also collect judgments from 10 different annotators, using the same question as White and Rawlins for context (6a) and modified questions for contexts (6b)-(6g): did that person do that thing?",2 Data collection,[0],[0]
"for (6b), (6d), and (6f); and did that person have that thing?",2 Data collection,[0],[0]
"for for (6c), (6e), and (6g).",2 Data collection,[0],[0]
Table 1 shows the number of verb types for each syntactic context.,2 Data collection,[0],[0]
"With the polarity manipulation, this yields a total of 3,938 sentences.
",2 Data collection,[0],[0]
"To build a factuality prediction test set from these sentences, we combine MegaVeridicality1 with our dataset and replace each instance of a particular person or a particular thing with someone or something (respectively).",2 Data collection,[0],[0]
"Then, following White and Rawlins, we normalize the 10 responses for each sentence to a single real value using an ordinal mixed model-based procedure.",2 Data collection,[0],[0]
We refer to the resulting dataset as MegaVeridicality2.,2 Data collection,[0],[0]
"We use MegaVeridicality2 to evaluate the performance of three state-of-the-art neural models of
event factuality (Rudinger et al., 2018): a linearchain biLSTM (L-biLSTM), a dependency tree biLSTM",3 Model and evaluation,[0],[0]
"(T-biLSTM), and a hybrid biLSTM (HbiLSTM) that ensembles the two.",3 Model and evaluation,[0],[0]
"To predict the factuality of the event referred to by a particular predicate, these models pass the output state of the biLSTM at that predicate through a two-layer regression.",3 Model and evaluation,[0],[0]
"In the case of the H-biLSTM, the output state of both the L- and T-biLSTMs are simply concatenated and passed through the regression.2
Following the multi-task training regime described by Rudinger et al. (2018), we train these models on four standard factuality datasets – FactBank (Saurı́ and Pustejovsky, 2009, 2012), UW (Lee et al., 2015), MEANTIME (Minard et al., 2016), and UDS (White et al., 2016; Rudinger et al., 2018) – with tied biLSTM weights but regression parameters specific to each dataset.",3 Model and evaluation,[0],[0]
"We then use these trained models to predict the factuality of the embedded predicate in our dataset.
",3 Model and evaluation,[0],[0]
"To understand how much of these models’ performance on our dataset is really due to a correct computation of lexicosyntactic inferences, we also generate predictions for the sentences in our dataset with the embedding verbs UNKed.",3 Model and evaluation,[0],[0]
"In this case, the model can rely only on the syntactic context surrounding the predicate to make its inferences.",3 Model and evaluation,[0],[0]
"We refer to the models with lexical information as the LEX models and the ones without lexical information as the UNK models.
",3 Model and evaluation,[0],[0]
"Each model produces four predictions, corresponding to the four different datasets it was trained on.",3 Model and evaluation,[0],[0]
"We consider three different ways of ensembling these predictions using a cross-validated ridge regression: (i) ensembling the four predictions for each specific model (LEX or UNK); (ii) ensembling the predictions for the LEX version of a particular model with the UNK version of that same model (LEX+UNK); and (iii) ensembling the predictions across all models (LEX, UNK, or LEX+UNK).",3 Model and evaluation,[0],[0]
"Each ensemble is evaluated in a 10- fold/10-fold nested cross-validation (see Cawley and Talbot, 2010).",3 Model and evaluation,[0],[0]
"In each iteration of the outer cross-validation, a 10% test set is split off, and a 10-fold cross-validation to tune the regularization is conducted on the remaining 90%.",3 Model and evaluation,[0],[0]
"Figure 1 shows the mean correlation between model predictions and true factuality on the outer
2See Appendix B for further details.
",4 Results,[0],[0]
fold test sets of the nested cross-validation described in §3.,4 Results,[0],[0]
"We note three aspects of this plot.
",4 Results,[0],[0]
"First, among the LEX models, the T-biLSTM performs best, followed by the L-biLSTM, then the H-biLSTM.",4 Results,[0],[0]
"This is somewhat surprising, since Rudinger et al. find the opposite pattern of performance: the L- and H-biLSTMs vie for dominance, both outperforming the T-biLSTM.",4 Results,[0],[0]
"This indicates that T-biLSTMs are better able to represent the lexicosyntactic inferences relevant to this dataset, even though they underperform on more general datasets.",4 Results,[0],[0]
"This possibility is bolstered by the fact that, in contrast to the L- and H-biLSTMs, the LEX version of the T-biLSTMs performs significantly better than the UNK version, suggesting that the T-biLSTM is potentially more reliant on the lexical information than the other two.
",4 Results,[0],[0]
"Second, when the LEX and UNK version of each model is ensembled (LEX+UNK), we find comparable performance for all three biLSTMs – each outperforming the LEX version of the TbiLSTM.",4 Results,[0],[0]
"This indicates that each model captures similar amounts of information about lexicosyntactic inference, but this information is captured in the models’ parameterizations in different ways.
",4 Results,[0],[0]
"Finally, when all three models are ensembled, we find that both the LEX and UNK version perform significantly better than any specific LEX+UNK model.",4 Results,[0],[0]
"This may indicate two things: (i) the models that only have access to syntax can perform just as well as ones that have access to both lexical information and syntax; but (ii) these models appear to capture different aspects of inference, since an ensemble of all models (AllLEX+UNK) performs significantly better than ei-
ther the All-LEX or All-UNK ensembles alone.",4 Results,[0],[0]
"Interestingly, however, even this ensemble performs more than 10 points worse than each model alone on FactBank, UW, and UDS.",4 Results,[0],[0]
This raises the question of which lexicosyntactic inferences these models are missing – investigated below.,4 Results,[0],[0]
We investigate two questions: (i) which inferences do all models do poorly on?; and (ii) what drives the differing strengths of each model?,5 Analysis,[0],[0]
Where do all models fail?,5 Analysis,[0],[0]
Table 2 shows the 20 sentences with the highest prediction errors under the All-LEX+UNK ensemble.,5 Analysis,[0],[0]
There are two interesting things to note about these sentences.,5 Analysis,[0],[0]
"First, most of them involve negative lexicosyntactic inferences that the model predicts to be either positive or near zero.",5 Analysis,[0],[0]
"Second, when the true inference is not positive, the matrix polarity of the original sentence is negative.",5 Analysis,[0],[0]
"This suggests that the models are not able to capture inferences whose polarity mismatches the matrix clause polarity.
",5 Analysis,[0],[0]
One question that arises here is whether this inability affects all contexts equally.,5 Analysis,[0],[0]
"To answer this, we regress the absolute error of the predictions from this same ensemble (logged and standardized) against true factuality, matrix polarity, and context (as well as all of their two- and three-way interactions).3",5 Analysis,[0],[0]
"We find that the three-way interactions in this regression are reliable ( 2(8)=27.97, p < 0.001) – suggesting that there are nontrivial differences in these state-of-the-art factuality systems’ ability to capture inferential interactions across verbs and syntactic contexts.",5 Analysis,[0],[0]
"The differences can be verified visually in Figure 2, which
3See Appendix C for further details, including a summary of the regression on which the above discussion is based.",5 Analysis,[0],[0]
"plots the factuality predicted by this ensemble against the true factuality from MegaVeridicality2.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"To elaborate, the ensemble does best overall on contexts like (7a) and (7b), and worst overall on contexts like (7c).",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"The contrast between (7b) and (7c) is particularly interesting because (i) (7c) is just the passivized form of (7b); and (ii) we do not observe similar behavior for contexts (7d) and (7e), which are analogous to (7b) and (7c), but replace the stative have with the eventive do.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"(7) Someone...
a. { ed, didn’t } for something to happen.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"b. { ed, didn’t } someone to have something.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"c. {was ed, wasn’t ed} to have something.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"d. { ed, didn’t } someone to do something.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"e. {was ed, wasn’t ed} to do something.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"f. { ed, didn’t } that something happened.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"An additional nuance is that the ensemble does reliably better on the negative matrix polarity version of (7b) than on the positive, with the opposite true for (7e).",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"This suggests these models do not capture an important inferential interaction between passivization and eventivity.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"This suggestion is further bolstered by the fact that the ensemble’s ability to predict cases where the matrix polarity mismatches the true factuality are reliably poorer in context (7c) but not in its minimal pairs (7e) and (7b), where the ensemble performs reliably poorer when the two match.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"Indeed, it is contexts (7c) and (7f) that drive the polarity mismatch effect evident in Table 2.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
What drives differences between models?,NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"In §4, we noted two ways that the biLSTMs we in-
vestigate differ: (i) the T-biLSTM appears to be more reliant on lexical information than L- and HbiLSTMs; and (ii) each model appears to encode information about lexicosyntactic inference in its parameterizations in different ways.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"We hypothesize that these two differences are related – specifically, that the T-biLSTM’s heavier reliance on lexical information comes about as a consequence of stronger entanglement between lexical and syntactic information in its hidden states.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"To probe this, we ask to what extent the embedding verb’s embedding can be recovered from the embedded verb’s hidden state using linear functions.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"If the lexical information is more strongly entangled with the syntactic information, it should be more difficult to construct a homomorphic (linear) function to decode the embedding verb’s embedding from the embedded verb’s hidden state.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"To measure this, we conduct a Canonical Correlation Analysis (CCA; Hotelling, 1936) between these two vector space representations for every sentence in our dataset.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"Given two matrices X (the embedding verb embeddings column stacked) and Y (the embedded verb hidden states column stacked), CCA constructs matrices A and B, such that ai,bi = arga0,b0max corr(a0X,b0Y) and corr(aiX,ajX) = corr(biY,bjY) = 0, 8i <",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"j. This guarantees that the canonical correlation at component i, corr(aiX,biY), is nonincreasing in i, and thus the linearly decodable information about Y in X can be assessed using this function.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
Figure 3 plots the canonical correlations for the first 50 components for each of the biLSTMs we investigated.,NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
We find that the canonical correlations associated with the T-biLSTM are substantially lower than those associated with the Land H-biLSTMs across these first 50 components.,NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"This suggests that the T-biLSTM more strongly entangles lexical and syntactic information, per-
haps explaining its apparently heavier reliance on lexical information, observed in §4.
",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"Of note here is that the pattern seen in Figure 3 is probably at least partly a consequence of the different nonlinearities used for the L-biLSTM (tanh) and T-biLSTM (ReLU), and not the architectures themselves.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"But whether or not this pattern is due to the architectures, nonlinearities, or both, the entanglement hypothesis may still help explain the pattern of results discussed in §4.",NP _ed that S NP was _ed that S NP _ed for NP to VP,[0],[0]
"This work is inspired by recent work in recasting various semantic annotations into natural language inference (NLI) datasets (White et al., 2017; Poliak et al., 2018a,b; Wang et al., 2018) to gain a better understanding of which phenomena standard neural NLI models (Bowman et al., 2015; Conneau et al., 2017) can capture – a line of work with deep roots (Cooper et al., 1996).",6 Related work,[0],[0]
"The experimental setup – specifically, the idea of UNKing the embedding verb – was inspired by recent work that uses hypothesis-only baselines for a similar purpose (Gururangan et al., 2018; Poliak et al., 2018c; Tsuchiya, 2018).",6 Related work,[0],[0]
"This work is also related to the broader investigation of sentence representations – particularly, tasks aimed at probing these representations’ content (Pavlick and Callison-Burch, 2016; Adi et al., 2016; Conneau et al., 2018; Conneau and Kiela, 2018; Dasgupta et al., 2018).",6 Related work,[0],[0]
"We investigated neural models’ ability to capture lexicosyntactic inference, taking the task of event factuality prediction (EFP) as a case study.",7 Conclusion,[0],[0]
We built a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts and used this dataset to probe current stateof-the-art EFP systems.,7 Conclusion,[0],[0]
We showed that these systems make certain systematic errors that are clearly visible through the lens of factuality.,7 Conclusion,[0],[0]
"This research was supported by the JHU HLTCOE, DARPA LORELEI and AIDA, NSF-BCS (1748969/1749025), and NSF-GRFP (1232825).",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.,Acknowledgments,[0],[0]
The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.,Acknowledgments,[0],[0]
We investigate neural models’ ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information.,abstractText,[0],[0]
We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts.,abstractText,[0],[0]
"We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.",abstractText,[0],[0]
Lexicosyntactic Inference in Neural Models,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 148–154 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2023",text,[0],[0]
"Aspect extraction is a key task of opinion mining (Liu, 2012).",1 Introduction,[0],[0]
It extracts opinion targets from opinion text.,1 Introduction,[0],[0]
"For example, from the sentence “The screen is great”, it aims to extract “screen”, which is a product feature, also called an aspect.
",1 Introduction,[0],[0]
Aspect extraction is commonly done using a supervised or an unsupervised approach.,1 Introduction,[0],[0]
"The unsupervised approach includes methods such as frequent pattern mining (Hu and Liu, 2004; Popescu and Etzioni, 2005; Zhu et al., 2009), syntactic rules-based extraction (Zhuang et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Zhang et al., 2010; Qiu et al., 2011; Poria et al., 2014), topic modeling (Mei et al., 2007; Titov and McDonald, 2008; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Moghaddam and Ester, 2011; Mukherjee and Liu, 2012; Lin and He, 2009; Zhao et al., 2010; Jo and Oh, 2011; Fang and Huang, 2012; Wang et al., 2016), word alignment (Liu et al., 2013), label propagation (Zhou et al., 2013; Shu et al., 2016), and others (Zhao et al., 2015).
",1 Introduction,[0],[0]
"This paper focuses on the supervised approach (Jakob and Gurevych, 2010; Choi and Cardie,
2010; Mitchell et al., 2013) using Conditional Random Fields (CRF) (Lafferty et al., 2001).",1 Introduction,[0],[0]
"It shows that the results of CRF can be significantly improved by leveraging some prior knowledge automatically mined from the extraction results of previous domains, including domains without labeled data.",1 Introduction,[0],[0]
"The improvement is possible because although every product (domain) is different, there is a fair amount of aspects sharing across domains (Chen and Liu, 2014).",1 Introduction,[0],[0]
"For example, every review domain has the aspect price and reviews of many products have the aspect battery life or screen.",1 Introduction,[0],[0]
Those shared aspects may not appear in the training data but appear in unlabeled data and the test data.,1 Introduction,[0],[0]
"We can exploit such sharing to help CRF perform much better.
",1 Introduction,[0],[0]
"Due to leveraging the knowledge gained from the past to help the new domain extraction, we are using the idea of lifelong machine learning (LML) (Chen and Liu, 2016; Thrun, 1998; Silver et al., 2013), which is a continuous learning paradigm that retains the knowledge learned in the past and uses it to help future learning and problem solving with possible adaptations.
",1 Introduction,[0],[0]
The setting of the proposed approach L-CRF (Lifelong CRF) is as follows: A CRF model M has been trained with a labeled training review dataset.,1 Introduction,[0],[0]
"At a particular point in time, M has extracted aspects from data in n previous domains D1, . . .",1 Introduction,[0],[0]
", Dn (which are unlabeled) and the extracted sets of aspects are A1, . . .",1 Introduction,[0],[0]
", An.",1 Introduction,[0],[0]
"Now, the system is faced with a new domain data Dn+1.",1 Introduction,[0],[0]
"M can leverage some reliable prior knowledge in A1, . . .",1 Introduction,[0],[0]
", An to make a better extraction fromDn+1 than without leveraging this prior knowledge.
",1 Introduction,[0],[0]
"The key innovation of L-CRF is that even after supervised training, the model can still improve its extraction in testing or its applications with experiences.",1 Introduction,[0],[0]
"Note that L-CRF is different from semisupervised learning (Zhu, 2005) as the n previous
148
(unlabeled) domain data used in extraction are not used or not available during model training.
",1 Introduction,[0],[0]
"There are prior LML works for aspect extraction (Chen et al., 2014; Liu et al., 2016), but they were all unsupervised methods.",1 Introduction,[0],[0]
"Supervised LML methods exist (Chen et al., 2015; Ruvolo and Eaton, 2013), but they are for classification rather than for sequence learning or labeling like CRF.",1 Introduction,[0],[0]
"A semi-supervised LML method is used in NELL (Mitchell et al., 2015), but it is heuristic patternbased.",1 Introduction,[0],[0]
It doesn’t use sequence learning and is not for aspect extraction.,1 Introduction,[0],[0]
"LML is related to transfer learning and multi-task learning (Pan and Yang, 2010), but they are also quite different (see (Chen and Liu, 2016) for details).
",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first paper that uses LML to help a supervised extraction method to markedly improve its results.",1 Introduction,[0],[0]
"CRF learns from an observation sequence x to estimate a label sequence y: p(y|x;θ), where θ is a set of weights.",2 Conditional Random Fields,[0],[0]
Let l be the l-th position in the sequence.,2 Conditional Random Fields,[0],[0]
"The core parts of CRF are a set of feature functions F = {fh(yl, yl−1,xl)}Hh=1 and their corresponding weights θ = {θh}Hh=1.",2 Conditional Random Fields,[0],[0]
Feature Functions: We use two types of feature functions (FF).,2 Conditional Random Fields,[0],[0]
"One is Label-Label (LL) FF:
fLLij (yl, yl−1)",2 Conditional Random Fields,[0],[0]
"= 1{yl = i}1{yl−1 = j},∀i, j ∈ Y, (1)
where Y is the set of labels, and 1{·} an indicator function.",2 Conditional Random Fields,[0],[0]
The other is Label-Word (LW),2 Conditional Random Fields,[0],[0]
"FF:
fLWiv (yl,xl) = 1{yl = i}1{xl = v}, ∀i ∈ Y,∀v ∈ V, (2)
where V is the vocabulary.",2 Conditional Random Fields,[0],[0]
This FF returns 1 when the l-th word is v and the l-th label is v’s specific label i; otherwise 0.,2 Conditional Random Fields,[0],[0]
"xl is the current word, and is represented as a multi-dimensional vector.",2 Conditional Random Fields,[0],[0]
"Each dimension in the vector is a feature of xl.
",2 Conditional Random Fields,[0],[0]
"Following the previous work in (Jakob and Gurevych, 2010), we use the feature set {W, -1W, +1W, P, -1P, +1P, G}, where W is the word and P is its POS-tag, -1W is the previous word, -1P is its POS-tag, +1W is the next word, +1P is its POStag, and G is the generalized dependency feature.
",2 Conditional Random Fields,[0],[0]
"Under the Label-Word FF type, we have two sub-types of FF: Label-dimension FF and Label-G FF.",2 Conditional Random Fields,[0],[0]
"Label-dimension FF is for the first 6 features, and Label-G is for the G feature.
",2 Conditional Random Fields,[0],[0]
"The Label-dimension (Ld) FF is defined as
fLd ivd (yl,xl) = 1{yl = i}1{xdl = vd},∀i ∈ Y, ∀vd ∈",2 Conditional Random Fields,[0],[0]
"Vd, (3)
where Vd is the set of observed values in feature d ∈ {W,−1W,+1W,P,−1P,+1P}",2 Conditional Random Fields,[0],[0]
and we call Vd feature d’s feature values.,2 Conditional Random Fields,[0],[0]
Eq. (3) is a FF that returns 1 when xl’s feature d equals to the feature value vd and the variable yl (lth label) equals to the label value i; otherwise,2 Conditional Random Fields,[0],[0]
"0.
We describe G and its feature function next, which also holds the key to the proposed L-CRF.",2 Conditional Random Fields,[0],[0]
Feature G uses generalized dependency relations.,3 General Dependency Feature (G),[0],[0]
What is interesting about this feature is that it enables L-CRF to use past knowledge in its sequence prediction at the test time in order to perform much better.,3 General Dependency Feature (G),[0],[0]
This will become clear shortly.,3 General Dependency Feature (G),[0],[0]
"This feature takes a dependency pattern as its value, which is generalized from dependency relations.
",3 General Dependency Feature (G),[0],[0]
The general dependency feature (G) of the variable xl takes a set of feature values VG.,3 General Dependency Feature (G),[0],[0]
Each feature value vG is a dependency pattern.,3 General Dependency Feature (G),[0],[0]
"The LabelG (LG) FF is defined as:
fLG ivG (yl,xl) =",3 General Dependency Feature (G),[0],[0]
"1{yl = i}1{xGl = vG},∀i ∈ Y, ∀vG ∈ VG.",3 General Dependency Feature (G),[0],[0]
"(4)
Such a FF returns 1 when the dependency feature of the variable xl equals to a dependency pattern vG and the variable yl equals to the label value i.",3 General Dependency Feature (G),[0],[0]
"Dependency relations have been shown useful in many sentiment analysis applications (Johansson and Moschitti, 2010; Jakob and Gurevych, 2010).",3.1 Dependency Relation,[0],[0]
"A dependency relation 1 is a quintuple-tuple: (type, gov, govpos, dep, deppos), where type is the type of the dependency relation, gov is the governor word, govpos is the POS tag of the governor word, dep is the dependent word, and deppos is the POS tag of the dependent word.",3.1 Dependency Relation,[0],[0]
The l-th word can either be the governor word or the dependent word in a dependency relation.,3.1 Dependency Relation,[0],[0]
"We generalize dependency relations into dependency patterns using the following steps:
1.",3.2 Dependency Pattern,[0],[0]
"For each dependency relation, replace the current word (governor word or dependent word) and its POS tag with a wildcard since we already have the word (W) and the POS tag (P) features.
",3.2 Dependency Pattern,[0],[0]
"1We obtain dependency relations using Stanford CoreNLP: http://stanfordnlp.github.io/CoreNLP/.
2.",3.2 Dependency Pattern,[0],[0]
Replace the context word (the word other than the l-th word) in each dependency relation with a knowledge label to form a more general dependency pattern.,3.2 Dependency Pattern,[0],[0]
Let the set of aspects annotated in the training data be Kt.,3.2 Dependency Pattern,[0],[0]
"If the context word in the dependency relation appears in Kt, we replace it with a knowledge label ‘A’ (aspect); otherwise ‘O’ (other).
",3.2 Dependency Pattern,[0],[0]
"For example, we work on the sentence “The battery of this camera is great.”",3.2 Dependency Pattern,[0],[0]
The dependency relations are given in Table 1.,3.2 Dependency Pattern,[0],[0]
"Assume the current word is “battery,” and “camera” is annotated as an aspect.",3.2 Dependency Pattern,[0],[0]
"The original dependency relation between “camera” and “battery” produced by a parser is (nmod, battery, NN, camera, NN).",3.2 Dependency Pattern,[0],[0]
Note that we do not use the word positions in the relations in Table 1.,3.2 Dependency Pattern,[0],[0]
"Since the current word’s information (the word itself and its POS-tag) in the dependency relation is redundant, we replace it with a wild-card.",3.2 Dependency Pattern,[0],[0]
"The relation becomes (nmod, *, camera, NN).",3.2 Dependency Pattern,[0],[0]
"Secondly, since “camera” is in Kt, we replace “camera” with a general label ‘A’.",3.2 Dependency Pattern,[0],[0]
"The final dependency pattern becomes (nmod,*, A, NN).
",3.2 Dependency Pattern,[0],[0]
We now explain why dependency patterns can enable a CRF model to leverage the past knowledge.,3.2 Dependency Pattern,[0],[0]
"The key is the knowledge label ‘A’ above, which indicates a likely aspect.",3.2 Dependency Pattern,[0],[0]
"Recall that our problem setting is that when we need to extract from the new domain Dn+1 using a trained CRF model M , we have already extracted from many previous domains D1, . . .",3.2 Dependency Pattern,[0],[0]
", Dn and retained their extracted sets of aspects A1, . . .",3.2 Dependency Pattern,[0],[0]
", An.",3.2 Dependency Pattern,[0],[0]
"Then, we can mine reliable aspects from A1, . . .",3.2 Dependency Pattern,[0],[0]
", An and add them in Kt, which enables many knowledge labels in the dependency patterns of the new data An+1 due to sharing of aspects across domains.",3.2 Dependency Pattern,[0],[0]
"This enriches the dependency pattern features, which consequently allows more aspects to be extracted from the new domain Dn+1.",3.2 Dependency Pattern,[0],[0]
We now present the L-CRF algorithm.,4 The Proposed L-CRF Algorithm,[0],[0]
"As the dependency patterns for the general dependency fea-
Algorithm 1 Lifelong Extraction of L-CRF 1: Kp ← ∅ 2: loop 3: F ← FeatureGeneration(Dn+1,K) 4: An+1 ← Apply-CRF-Model(M,F ) 5: S ← S ∪ {An+1} 6: Kn+1 ← Frequent-Aspects-Mining(S, λ) 7: if Kp = Kn+1 then 8: break 9: else 10: K ← Kt ∪Kn+1 11:",4 The Proposed L-CRF Algorithm,[0],[0]
"Kp ← Kn+1 12: S ← S − {An+1} 13: end if 14: end loop
ture do not use any actual words and they can also use the prior knowledge, they are quite powerful for cross-domain extraction (the test domain is not used in training).
",4 The Proposed L-CRF Algorithm,[0],[0]
Let K be a set of reliable aspects mined from the aspects extracted in past domain datasets using the CRF model M .,4 The Proposed L-CRF Algorithm,[0],[0]
Note that we assume that M has already been trained using some labeled training data Dt.,4 The Proposed L-CRF Algorithm,[0],[0]
"Initially, K is Kt (the set of all annotated aspects in the training data Dt).",4 The Proposed L-CRF Algorithm,[0],[0]
"The more domainsM has worked on, the more aspects it extracts, and the larger the set K gets.",4 The Proposed L-CRF Algorithm,[0],[0]
"When faced with a new domain Dn+1, K allows the general dependency feature to generate more dependency patterns related to aspects due to more knowledge labels ‘A’ as we explained in the previous section.",4 The Proposed L-CRF Algorithm,[0],[0]
"Consequently, CRF has more informed features to produce better extraction results.
",4 The Proposed L-CRF Algorithm,[0],[0]
L-CRF works in two phases: training phase and lifelong extraction phase.,4 The Proposed L-CRF Algorithm,[0],[0]
"The training phase trains a CRF model M using the training data Dt, which is the same as normal CRF training, and will not be discussed further.",4 The Proposed L-CRF Algorithm,[0],[0]
"In the lifelong extraction phase, M is used to extract aspects from coming domains (M does not change and the domain data are unlabeled).",4 The Proposed L-CRF Algorithm,[0],[0]
"All the results from the domains are retained in past aspect store S. At
a particular time, it is assumed M has been applied to n past domains, and is now faced with the n + 1 domain.",4 The Proposed L-CRF Algorithm,[0],[0]
L-CRF uses M and reliable aspects (denoted Kn+1) mined from S and Kt (K = Kt ∪ Kn+1) to extract from Dn+1.,4 The Proposed L-CRF Algorithm,[0],[0]
"Note that aspects Kt from the training data are considered always reliable as they are manually labeled, thus a subset ofK. We cannot use all extracted aspects from past domains as reliable aspects due to many extraction errors.",4 The Proposed L-CRF Algorithm,[0],[0]
But those aspects that appear in multiple past domains are more likely to be correct.,4 The Proposed L-CRF Algorithm,[0],[0]
"ThusK contains those frequent aspects in S. The lifelong extraction phase is in Algorithm 1.
",4 The Proposed L-CRF Algorithm,[0],[0]
"Lifelong Extraction Phase: Algorithm 1 performs extraction on Dn+1 iteratively.
1.",4 The Proposed L-CRF Algorithm,[0],[0]
"It generates features (F ) on the data Dn+1 (line 3), and applies the CRF model M on F to produce a set of aspects An+1",4 The Proposed L-CRF Algorithm,[0],[0]
"(line 4).
2. An+1 is added to S, the past aspect store.",4 The Proposed L-CRF Algorithm,[0],[0]
"From S, we mine a set of frequent aspects Kn+1.",4 The Proposed L-CRF Algorithm,[0],[0]
"The frequency threshold is λ.
3.",4 The Proposed L-CRF Algorithm,[0],[0]
"If Kn+1 is the same as Kp from the previous iteration, the algorithm exits as no new aspects can be found.",4 The Proposed L-CRF Algorithm,[0],[0]
"We use an iterative process because each extraction gives new results, which may increase the size of K, the reliable past aspects or past knowledge.",4 The Proposed L-CRF Algorithm,[0],[0]
"The increased K may produce more dependency patterns, which can enable more extractions.
4.",4 The Proposed L-CRF Algorithm,[0],[0]
Else: some additional reliable aspects are found.,4 The Proposed L-CRF Algorithm,[0],[0]
M may extract additional aspects in the next iteration.,4 The Proposed L-CRF Algorithm,[0],[0]
Lines 10 and 11 update the two sets for the next iteration.,4 The Proposed L-CRF Algorithm,[0],[0]
We now evaluate the proposed L-CRF method and compare with baselines.,5 Experiments,[0],[0]
We use two types of data for our experiments.,5.1 Evaluation Datasets,[0],[0]
The first type consists of seven (7) annotated benchmark review datasets from 7 domains (types of products).,5.1 Evaluation Datasets,[0],[0]
"Since they are annotated, they are used in training and testing.",5.1 Evaluation Datasets,[0],[0]
"The first 4 datasets are from (Hu and Liu, 2004), which actually has 5 datasets from 4 domains.",5.1 Evaluation Datasets,[0],[0]
"Since we are mainly interested in results at the domain level, we did not use one of the domain-repeated datasets.",5.1 Evaluation Datasets,[0],[0]
"The last 3 datasets of three domains (products) are from (Liu et al., 2016).",5.1 Evaluation Datasets,[0],[0]
These datasets are used to make up our CRF training data Dt and test data Dn+1.,5.1 Evaluation Datasets,[0],[0]
"The annotation details are given in Table 2.
",5.1 Evaluation Datasets,[0],[0]
"The second type has 50 unlabeled review datasets from 50 domains or types of products (Chen and Liu, 2014).",5.1 Evaluation Datasets,[0],[0]
Each dataset has 1000 reviews.,5.1 Evaluation Datasets,[0],[0]
"They are used as the past domain data, i.e., D1, . . .",5.1 Evaluation Datasets,[0],[0]
", Dn (n = 50).",5.1 Evaluation Datasets,[0],[0]
"Since they are not labeled, they cannot be used for training or testing.",5.1 Evaluation Datasets,[0],[0]
We compare L-CRF with CRF.,5.2 Baseline Methods,[0],[0]
"We will not compare with unsupervised methods, which have been shown improvable by lifelong learning (Chen et al., 2014; Liu et al., 2016).",5.2 Baseline Methods,[0],[0]
"The frequency threshold λ in Algorithm 1 used in our experiment to judge which extracted aspects are considered reliable is empirically set to 2.
",5.2 Baseline Methods,[0],[0]
CRF:,5.2 Baseline Methods,[0],[0]
We use the linear chain CRF from 2.,5.2 Baseline Methods,[0],[0]
"Note that CRF uses all features including dependency features as the proposed L-CRF but does not employ the 50 domains unlabeled data used for lifelong learning
CRF+R:",5.2 Baseline Methods,[0],[0]
It treats the reliable aspect set K as a dictionary.,5.2 Baseline Methods,[0],[0]
It adds those reliable aspects in K that are not extracted by CRF but are in the test data to the final results.,5.2 Baseline Methods,[0],[0]
"We want to see whether incorporating K into the CRF extraction through dependency patterns in L-CRF is actually needed.
",5.2 Baseline Methods,[0],[0]
We do not compare with domain adaptation or transfer learning because domain adaption basically uses the source domain labeled data to help learning in the target domain with few or no labeled data.,5.2 Baseline Methods,[0],[0]
Our 50 domains used in lifelong learning have no labels.,5.2 Baseline Methods,[0],[0]
So they cannot help in transfer learning.,5.2 Baseline Methods,[0],[0]
"Although in transfer learning, the target domain usually has a large quantity of unlabeled data, but the 50 domains are not used as the target domains in our experiments.
",5.2 Baseline Methods,[0],[0]
2https://github.com/huangzhengsjtu/pcrf/,5.2 Baseline Methods,[0],[0]
"To compare the systems using the same training and test data, for each dataset we use 200 sentences for training and 200 sentences for testing to avoid bias towards any dataset or domain because we will combine multiple domain datasets for CRF training.",5.3 Experiment Setting,[0],[0]
We conducted both cross-domain and in-domain tests.,5.3 Experiment Setting,[0],[0]
Our problem setting is crossdomain.,5.3 Experiment Setting,[0],[0]
In-domain is used for completeness.,5.3 Experiment Setting,[0],[0]
"In both cases, we assume that extraction has been done for the 50 domains.
Cross-domain experiments: We combine 6 labeled domain datasets for training (1200 sentences) and test on the 7th domain (not used in training).",5.3 Experiment Setting,[0],[0]
This gives us 7 cross-domain results.,5.3 Experiment Setting,[0],[0]
"This set of tests is particularly interesting as it is desirable to have the trained model used in crossdomain situations to save manual labeling effort.
",5.3 Experiment Setting,[0],[0]
In-domain experiments: We train and test on the same 6 domains (1200 sentences for training and 1200 sentences for testing).,5.3 Experiment Setting,[0],[0]
"This also gives us 7 in-domain results.
",5.3 Experiment Setting,[0],[0]
"Evaluating Measures: We use the popular precision P , recallR, and F1-score.",5.3 Experiment Setting,[0],[0]
All the experiment results are given in Table 3.,5.4 Results and Analysis,[0],[0]
Cross-domain: Each −X in column 1 means that domain X is not used in training.,5.4 Results and Analysis,[0],[0]
"X in col-
umn 2 means that domain X is used in testing.",5.4 Results and Analysis,[0],[0]
We can see that L-CRF is markedly better than CRF and CRF+R in F1.,5.4 Results and Analysis,[0],[0]
"CRF+R is very poor due to poor precisions, which shows treating the reliable aspects set K as a dictionary isn’t a good idea.
",5.4 Results and Analysis,[0],[0]
In-domain: −X in training and test columns means that the other 6 domains are used in both training and testing (thus in-domain).,5.4 Results and Analysis,[0],[0]
We again see that L-CRF is consistently better than CRF and CRF+R in F1.,5.4 Results and Analysis,[0],[0]
The amount of gain is smaller.,5.4 Results and Analysis,[0],[0]
This is expected because most aspects appeared in training probably also appear in the test data as they are reviews from the same 6 products.,5.4 Results and Analysis,[0],[0]
This paper proposed a lifelong learning method to enable CRF to leverage the knowledge gained from extraction results of previous domains (unlabeled) to improve its extraction.,6 Conclusion,[0],[0]
Experimental results showed the effectiveness of L-CRF.,6 Conclusion,[0],[0]
The current approach does not change the CRF model itself.,6 Conclusion,[0],[0]
"In our future work, we plan to modify CRF so that it can consider previous extraction results as well as the knowledge in previous CRF models.",6 Conclusion,[0],[0]
This work was supported in part by grants from National Science Foundation (NSF) under grant no.,Acknowledgments,[0],[0]
IIS-1407927 and IIS-1650900.,Acknowledgments,[0],[0]
This paper makes a focused contribution to supervised aspect extraction.,abstractText,[0],[0]
"It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge.",abstractText,[0],[0]
"The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications.",abstractText,[0],[0]
Lifelong Learning CRF for Supervised Aspect Extraction,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 225–235, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"A core problem of opinion mining or sentiment analysis is to identify each opinion/sentiment target and to classify the opinion/sentiment polarity on the target (Liu, 2012).",1 Introduction,[0],[0]
"For example, in a review sentence for a car, one wrote “Although the engine is slightly weak, this car is great.”",1 Introduction,[0],[0]
"The person is positive (opinion polarity) about the car (opinion target) as a whole, but slightly negative (opinion polarity) about the car’s engine (opinion target).
",1 Introduction,[0],[0]
"Past research has proposed many techniques to extract opinion targets (we will just call them targets
hereafter for simplicity) and also to classify sentiment polarities on the targets.",1 Introduction,[0],[0]
"However, a target can be an entity or an aspect (part or attribute) of an entity.",1 Introduction,[0],[0]
"“Engine” in the above sentence is just one aspect of the car, while “this car” refers to the whole car.",1 Introduction,[0],[0]
"Note that in (Liu, 2012), an entity is called a general aspect.",1 Introduction,[0],[0]
"For effective opinion mining, we need to classify whether a target is an entity or an aspect because they refer to very different things.",1 Introduction,[0],[0]
"One can be positive about the whole entity (car) but negative about some aspects of it (e.g., engine) and vice versa.",1 Introduction,[0],[0]
"This paper aims to perform the target classification task, which, to our knowledge, has not been attempted before.",1 Introduction,[0],[0]
"Although in supervised extraction one can annotate entities and aspects with separate labels in the training data to build a model to extract them separately, in this paper our goal is to help unsupervised target extraction methods to classify targets.",1 Introduction,[0],[0]
"Unsupervised target extraction methods are often preferred because they save the time-consuming data labeling or annotation step for each domain.
",1 Introduction,[0],[0]
Problem Statement:,1 Introduction,[0],[0]
"Given a set of opinion targets T = {t1, . . .",1 Introduction,[0],[0]
", tn} extracted from an opinion corpus d, we want to classify each target ti ∈ T into one of the three classes, entity, aspect, or NIL, which are called class labels.",1 Introduction,[0],[0]
"NIL means that the target is neither an entity nor an aspect and is used because target extraction algorithms can make mistakes.
",1 Introduction,[0],[0]
This paper does not propose a new target extraction algorithm.,1 Introduction,[0],[0]
"We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al., 2011), for extraction.",1 Introduction,[0],[0]
We only focus on target classification after the targets have been extracted.,1 Introduction,[0],[0]
"Note that an entity here can be a named entity, a prod-
225
uct category, or an abstract product (e.g., “this machine” and “this product”).",1 Introduction,[0],[0]
"An named entity can be the name of a brand, a model, or a manufacturer.",1 Introduction,[0],[0]
"An aspect is a part or attribute of an entity, e.g., “battery” and “price” of the entity “camera”.
",1 Introduction,[0],[0]
"Since our entities not just include the traditional named entities (e.g., “Microsoft” and “Google”) but also other expressions that refer to such entities, traditional named entity recognition algorithms are not sufficient.",1 Introduction,[0],[0]
"Pronouns such as “it,” “they,” etc., are not considered in this paper as co-reference resolution is out of the scope of this work.
",1 Introduction,[0],[0]
We solve this problem in an unsupervised manner so that there is no need for labor-intensive manual labeling of the training data.,1 Introduction,[0],[0]
"One key observation of the problem is that although entities and aspects are different, they are closely related because aspects are parts or attributes of entities and they often have syntactic relationships in a sentence, e.g., “This phone’s screen is super.”",1 Introduction,[0],[0]
Thus it is natural to solve the problem using a relational learning method.,1 Introduction,[0],[0]
"We employ the graph labeling algorithm, Relaxation Labeling (RL) (Hummel and Zucker, 1983), which performs unsupervised belief propagation on a graph.",1 Introduction,[0],[0]
"In our case, each target extracted from the given corpus d forms a graph node and each relation identified in d between two targets forms an edge.",1 Introduction,[0],[0]
"With some initial probability assignments, RL can assign each target node the most probable class label.",1 Introduction,[0],[0]
"Although some other graph labeling methods can be applied as well, the key issue here is that just using a propagation method in isolation is far from sufficient due to lack of information from the given corpus, which we detail in Section 5.",1 Introduction,[0],[0]
"We then employ Lifelong Machine Learning (LML) (Thrun, 1998; Chen and Liu, 2014b) to make a major improvement.
",1 Introduction,[0],[0]
LML works as follows: The learner has performed a number learning tasks in the past and has retained the knowledge gained so far.,1 Introduction,[0],[0]
"In the new/current task, it makes use of the past knowledge to help current learning and problem solving.",1 Introduction,[0],[0]
"Since RL is unsupervised, we can assume that the system has performed the same task on reviews of a large number of products/domains (or corpora).",1 Introduction,[0],[0]
It has also saved all the graphs and classification results from those past domains in a Knowledge Base (KB).,1 Introduction,[0],[0]
It then exploits this past knowledge to help classification in the current task/domain.,1 Introduction,[0],[0]
"We call this
combined approach of relaxation labeling and LML Lifelong-RL.",1 Introduction,[0],[0]
"The approach is effective because there is a significant amount of sharing of targets and target relations across domains.
",1 Introduction,[0],[0]
LML is different from the classic learning paradigm (supervised or unsupervised) because classic learning has no memory.,1 Introduction,[0],[0]
"It basically runs a learning algorithm on a given data in isolation without considering any past learned knowledge (Silver et al., 2013).",1 Introduction,[0],[0]
"LML aims to mimic human learning, which always retains the learned knowledge from the past and uses it to help future learning.
",1 Introduction,[0],[0]
Our experimental results show that the proposed Lifelong-RL system is highly promising.,1 Introduction,[0],[0]
The paradigm of LML helps improve the classification results greatly.,1 Introduction,[0],[0]
"Although many target extraction methods exist (Hu and Liu, 2004; Zhuang et al., 2006; Ku et al., 2006; Wang and Wang, 2008; Wu et al., 2009; Lin and He, 2009; Zhang et al., 2010; Mei et al., 2007; Li et al., 2010; Brody and Elhadad, 2010; Wang et al., 2010; Mukherjee and Liu, 2012; Fang and Huang, 2012; Zhou et al., 2013; Liu et al., 2013; Poria et al., 2014), we are not aware of any attempt to solve the proposed problem.",2 Related Work,[0],[0]
"As mentioned in the introduction, although in supervised target extraction, one can annotate entities and aspects with different labels, supervised methods need manually labeled training data, which is time-consuming and laborintensive to produce (Jakob and Gurevych, 2010; Choi and Cardie, 2010; Mitchell et al., 2013).",2 Related Work,[0],[0]
"Note that relaxation labeling was used for sentiment classification in (Popescu and Etzioni, 2007), but not for target classification.",2 Related Work,[0],[0]
"More details of opinion mining can be found in (Liu, 2012; Pang and Lee, 2008).
",2 Related Work,[0],[0]
"Our work is related to transfer learning (Pan and Yang, 2010), which uses the source domain labeled data to help target domain learning, which has little or no labeled data.",2 Related Work,[0],[0]
Our work is not just using a source domain to help a target domain.,2 Related Work,[0],[0]
It is a continuous and cumulative learning process.,2 Related Work,[0],[0]
Each new task can make use of the knowledge learned from all past tasks.,2 Related Work,[0],[0]
Knowledge learned from the new task can also help improve learning of any past task.,2 Related Work,[0],[0]
"Transfer learning is not continuous, does not
accumulate knowledge over time and cannot improve learning in the source domain.",2 Related Work,[0],[0]
"Our work is also related to multi-task learning (Caruana, 1997), which jointly optimizes a set of related learning tasks.",2 Related Work,[0],[0]
"Clearly, multi-task learning is different as we learn and save information which is more realistic when a large number of tasks are involved.
",2 Related Work,[0],[0]
Our work is most related to Lifelong Machine Learning (LML).,2 Related Work,[0],[0]
"Traditional LML focuses on supervised learning (Thrun, 1998; Ruvolo and Eaton, 2013; Chen et al., 2015).",2 Related Work,[0],[0]
"Recent work used LML in topic modeling (Chen and Liu, 2014a), which is unsupervised.",2 Related Work,[0],[0]
"Basically, they used topics generated from past domains to help current domain model inference.",2 Related Work,[0],[0]
"However, they are just for aspect extraction.",2 Related Work,[0],[0]
"So is the method in (Liu et al., 2016).",2 Related Work,[0],[0]
They do not solve our problem.,2 Related Work,[0],[0]
Their LML methods are also different from ours as we use a graph and results obtained in the past domains to augment the current task/domain graph to solve the problem.,2 Related Work,[0],[0]
"In this section, we present the proposed general framework of lifelong relaxation labeling (LifelongRL).",3 Lifelong-RL: The General Framework,[0],[0]
"We first give an overview of the relaxation labeling algorithm, which forms the base.",3 Lifelong-RL: The General Framework,[0],[0]
We then incorporate it with the LML capability.,3 Lifelong-RL: The General Framework,[0],[0]
The next two sections detail how this general framework is applied to our proposed task of separating entities and aspects in opinion targets.,3 Lifelong-RL: The General Framework,[0],[0]
Relaxation Labeling (RL) is an unsupervised graphbased label propagation algorithm that works iteratively.,3.1 Relaxation Labeling,[0],[0]
The graph consists of nodes and edges.,3.1 Relaxation Labeling,[0],[0]
Each edge represents a binary relationship between two nodes.,3.1 Relaxation Labeling,[0],[0]
Each node ti in the graph is associated with a multinomial distribution P (L(ti)),3.1 Relaxation Labeling,[0],[0]
(L(ti) being the label of ti) on a label set Y .,3.1 Relaxation Labeling,[0],[0]
"Each edge is associated with two conditional probability distributions P (L(ti)|L(tj)) and P (L(tj)|L(ti)), where P (L(ti)|L(tj)) represents how the label L(tj) influences the label L(ti) and vice versa.",3.1 Relaxation Labeling,[0],[0]
"The neighbors Ne(ti) of a node ti are associated with a weight distribution w(tj |ti) with ∑ tj∈Ne(ti)w(tj |ti) = 1.
",3.1 Relaxation Labeling,[0],[0]
"Given the initial values of these quantities as inputs, RL iteratively updates the label distribution
of each node until convergence.",3.1 Relaxation Labeling,[0],[0]
"Initially, we have P 0(L(ti)).",3.1 Relaxation Labeling,[0],[0]
Let ∆P r+1(L(ti)),3.1 Relaxation Labeling,[0],[0]
be the change of P (L(ti)) at iteration r+ 1.,3.1 Relaxation Labeling,[0],[0]
"Given P r(L(ti)) at iteration r, ∆P r+1(L(ti)) is computed by:
∆P r+1(L(ti))",3.1 Relaxation Labeling,[0],[0]
"= ∑
tj∈Ne(ti)(w(tj |ti) ·∑y∈Y (P (L(ti)|L(tj) = y)P r(L(tj) = y)))",3.1 Relaxation Labeling,[0],[0]
"(1) Then, the updated label distribution for iteration
r + 1, P r+1(L(ti)), is computed as follows:
P r+1(L(ti))",3.1 Relaxation Labeling,[0],[0]
"= P r(L(ti))(1+∆P
r+1(L(ti)))∑ y∈Y P r(L(ti)=y)(1+∆P r+1(L(ti)=y))
(2)
",3.1 Relaxation Labeling,[0],[0]
"Once RL ends, the final label of node ti is its highest probable label: L(ti) =",3.1 Relaxation Labeling,[0],[0]
"argmax
y∈Y (P (L(ti) = y)).
",3.1 Relaxation Labeling,[0],[0]
Note that P (L(ti)|L(tj)) and w(tj |ti) are not updated in each RL iteration but only P (L(ti)) is.,3.1 Relaxation Labeling,[0],[0]
"P (L(ti)|L(tj)), w(tj |ti) and P 0(L(ti)) are provided by the user or computed based on the application context.",3.1 Relaxation Labeling,[0],[0]
RL uses these values as input and iteratively updates P (L(ti)) based on Equations (1) and (2) until convergence.,3.1 Relaxation Labeling,[0],[0]
Next we discuss how to incorporate LML in RL.,3.1 Relaxation Labeling,[0],[0]
"For LML, it is assumed that at any time step, the system has worked on u past domain corpora D = {d1, . . .",3.2 Lifelong Relaxation Labeling,[0],[0]
", du}.",3.2 Lifelong Relaxation Labeling,[0],[0]
"For each past domain corpus d ∈ D, the same Lifelong-RL algorithm was applied and its results were saved in the Knowledge Base (KB).",3.2 Lifelong Relaxation Labeling,[0],[0]
Then the algorithm can borrow some useful prior/past knowledge in the KB to help RL in the new/current domain du+1.,3.2 Lifelong Relaxation Labeling,[0],[0]
"Once the results of the current domain are produced, they are also added to the KB for future use.
",3.2 Lifelong Relaxation Labeling,[0],[0]
"We now detail the specific types of information or knowledge that can be obtained from the past domains to help RL in the future, which should thus be stored in the KB.
1.",3.2 Lifelong Relaxation Labeling,[0],[0]
"Prior edges: In many applications, the graph is not given.",3.2 Lifelong Relaxation Labeling,[0],[0]
"Instead, it has to be constructed based on the data from the new task/domain data du+1.",3.2 Lifelong Relaxation Labeling,[0],[0]
"However, due to the limited data in du+1, some edges between nodes that should be present are not extracted from the data.",3.2 Lifelong Relaxation Labeling,[0],[0]
"But such edges between the nodes may exist in
some past domains.",3.2 Lifelong Relaxation Labeling,[0],[0]
"Then, those edges and their associated probabilities can be borrowed.
2.",3.2 Lifelong Relaxation Labeling,[0],[0]
Prior labels: Some nodes in the current new domain may also exist in some past domains.,3.2 Lifelong Relaxation Labeling,[0],[0]
Their labels in the past domains are very likely to be the same as those in the current domain.,3.2 Lifelong Relaxation Labeling,[0],[0]
"Then, those prior labels can give us a better idea about the initial label probability distributions of the nodes in the current domain du+1.
",3.2 Lifelong Relaxation Labeling,[0],[0]
"To leverage those edges and labels from the past domains, the system needs to ensure that they are likely to be correct and applicable to the current task domain.",3.2 Lifelong Relaxation Labeling,[0],[0]
This is a challenge problem.,3.2 Lifelong Relaxation Labeling,[0],[0]
"In the next two sections, we detail how to ensure these to a large extent in our application context along with how to compute those initial probabilities.",3.2 Lifelong Relaxation Labeling,[0],[0]
We now discuss how the proposed Lifelong-RL general framework is applied to solve our problem.,4 Initialization of Relaxation Labeling,[0],[0]
"In our case, each node in the graph is an extracted target ti ∈ T , and each edge represents a binary relationship between two targets.",4 Initialization of Relaxation Labeling,[0],[0]
"T is the given set of all opinion targets extracted by an extraction algorithm from a review dataset/corpus d. The label set for each target is Y = {entity, aspect,NIL}.",4 Initialization of Relaxation Labeling,[0],[0]
"In this section, we describe how to use text clues in the corpus d to compute P (L(ti)|L(tj)), w(tj |ti) and P 0(L(ti)).",4 Initialization of Relaxation Labeling,[0],[0]
"In the next section, we present how these quantities are improved using prior knowledge from the past domains in the LML fashion.",4 Initialization of Relaxation Labeling,[0],[0]
"We use two kinds of text clues, called type modifiers M(t) and relation modifiers MR to compute the initial label distribution P (L(ti)) and conditional label distribution P (L(ti)|L(tj)) respectively.
",4.1 Text Clues for Initialization,[0],[0]
Type Modifier:,4.1 Text Clues for Initialization,[0],[0]
"This has two kinds MT = {mE ,mA}, where mE and mA represent entity modifier and aspect modifier respectively.",4.1 Text Clues for Initialization,[0],[0]
"For example, the word “this” as in “this camera is great” indicates that “camera” is probably an entity.",4.1 Text Clues for Initialization,[0],[0]
"Thus, “this” is a type modifier indicating M(camera) = mE .",4.1 Text Clues for Initialization,[0],[0]
“These” is also a type modifier.,4.1 Text Clues for Initialization,[0],[0]
"Aspect modifier is implicitly assumed when the number of appearances of entity modifiers is less than or equal to a threshold (see Section 4.2).
",4.1 Text Clues for Initialization,[0],[0]
Relation Modifier:,4.1 Text Clues for Initialization,[0],[0]
"Given two targets, ti and tj , we use Mtj (ti) to denote the relation modifier that the label of target ti is influenced by the label of target tj .",4.1 Text Clues for Initialization,[0],[0]
"Relation modifiers are further divided into 3 kinds: MR = {mc,mA|E ,mE|A}.
",4.1 Text Clues for Initialization,[0],[0]
Conjunction modifier mc: Conjoined items are usually of the same type.,4.1 Text Clues for Initialization,[0],[0]
"For example, in “price and service”, “and service” indicates a conjunction modifier for “price” and vice versa.
",4.1 Text Clues for Initialization,[0],[0]
Entity-aspect modifier mA|E : A possessive expression indicates an entity and an aspect relation.,4.1 Text Clues for Initialization,[0],[0]
"For example, in “the camera’s battery”, “camera” indicates an entity-aspect modifier for “battery”.
",4.1 Text Clues for Initialization,[0],[0]
"Aspect-entity modifier mE|A: Same as above except that “battery” indicates an aspect-entity modifier for “camera”.
",4.1 Text Clues for Initialization,[0],[0]
Modifier Extraction: These modifiers are identified from the corpus d using three syntactic rules.,4.1 Text Clues for Initialization,[0],[0]
“This” and “these” are used to extract type modifier M(t) = mE .,4.1 Text Clues for Initialization,[0],[0]
"CmE (t) is the occurrence count of that modifier on target t, which is used in determining the initial label distribution in Section 4.2.
",4.1 Text Clues for Initialization,[0],[0]
"Relation modifiers are identified by dependency relations conj(ti, tj) and poss(ti, tj) using the Stanford Parser (Klein and Manning, 2003).",4.1 Text Clues for Initialization,[0],[0]
Each occurrence of a relation rule contributes one count of Mtj (ti) for ti and one count of Mti(tj) for tj .,4.1 Text Clues for Initialization,[0],[0]
"We use Cmc,tj (ti), CmA|E ,tj (ti) and CmE|A,tj (ti) to denote the count of tj modifying ti with conjunction, entity-aspect and aspect-entity modifiers respectively.",4.1 Text Clues for Initialization,[0],[0]
"For example, “price and service” will contribute one count to Cmc,price(service) and one count to Cmc,service(price).",4.1 Text Clues for Initialization,[0],[0]
"Similarly, “camera’s battery” will contribute one count to CmA|E ,camera(battery) and one count to CmE|A,battery(camera).",4.1 Text Clues for Initialization,[0],[0]
"The initial label probability distribution of target t is computed based on CmE (t), i.e.,
P 0(L(t))",4.2 Computing Initial Probabilities,[0],[0]
= { PmE (L(t)) if CmE (t) >,4.2 Computing Initial Probabilities,[0],[0]
α PmA(L(t)),4.2 Computing Initial Probabilities,[0],[0]
"if CmE (t) ≤ α
(3) Here, we have two pre-defined distributions: PmE and PmA , which have a higher probability on entity and aspect respectively.",4.2 Computing Initial Probabilities,[0],[0]
"The parameter α is a threshold indicating that if the entity modifier rarely occurs, the target is more likely to be an aspect.",4.2 Computing Initial Probabilities,[0],[0]
"These
values are set empirically (see Section 6).",4.2 Computing Initial Probabilities,[0],[0]
"Let term q(Mtj (ti) = m) be the normalized weight on the count for each kind of relation modifier m ∈MR:
q(Mtj (ti) = m)",4.2 Computing Initial Probabilities,[0],[0]
=,4.2 Computing Initial Probabilities,[0],[0]
"Cm,tj (ti)
Ctj (ti) (4)
where Ctj (ti) = ∑
m∈MR",4.2 Computing Initial Probabilities,[0],[0]
"Cm,tj (ti).",4.2 Computing Initial Probabilities,[0],[0]
"The conditional label distribution P (L(ti)|L(tj)) of ti given the label of tj is the weighted sum over the three kinds of relation modifiers:
P (L(ti)|L(tj))",4.2 Computing Initial Probabilities,[0],[0]
= q(Mtj (ti) = mc) · Pmc(L(ti)|L(tj)),4.2 Computing Initial Probabilities,[0],[0]
+q(Mtj (ti) = mA|E) · PmA|E (L(ti)|L(tj)),4.2 Computing Initial Probabilities,[0],[0]
"+q(Mtj (ti) = mE|A) · PmE|A(L(ti)|L(tj)) (5)
where Pmc , PmA|E , and PmE|A are pre-defined conditional distributions.",4.2 Computing Initial Probabilities,[0],[0]
"They are filled with values to model the label influence from neighbors and can be found in Section 6.
",4.2 Computing Initial Probabilities,[0],[0]
"Finally, target ti’s neighbor weight for target tj , i.e., w(tj |ti), is the ratio of the count of relation modifiers Ctj (ti) over the total of all ti’s neighbors:
w(tj |ti) = Ctj (ti)∑
tj′∈Ne(ti)Ctj′ (ti) (6)
If Ctj (ti) = 0, ti and tj has no edge between them.",4.2 Computing Initial Probabilities,[0],[0]
"Due to the fact that the review corpus du+1 in the current task domain may not be very large and that we use high quality syntactic rules to extract relations to build the graph to ensure precision, the number of relations extracted can be small and insufficient to produce a graph that is information rich with accurate initial probabilities.",5 Using Past Knowledge in Lifelong-RL,[0],[0]
We thus apply LML to help using knowledge learned in the past.,5 Using Past Knowledge in Lifelong-RL,[0],[0]
"The proposed LML process in Lifelong-RL for our task is shown in Figure 1.
",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"Our prior knowledge includes type modifiers, relation modifiers and labels of targets obtained from past domains in D. Each record in the KB is stored as a 9-tuple: (d, ti, tj ,M d(ti),M d(tj), C d m,tj (ti), C d m,ti(tj), L d(ti), L d(tj)) where d ∈ D is a past domain; ti and tj are two targets; Md(ti), Md(tj) are their type
modifiers, Cdm,tj (ti) and C d m,ti(tj) are counts for relation modifiers; Ld(ti) and Ld(tj) are labels decided by RL.",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"For example, the sentence “This camera’s battery is good” forms: (d, camera, battery,mE ,mA, CmE|A,battery(camera)",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"= 1, CmA|E ,camera(battery) = 1, entity, aspect) .",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"It means that in the past domain d, “camera” and “battery” are extracted targets.",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"Since “camera” is followed by “this”, its type modifier is mE .",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"Since “battery” is not identified by an entity modifier, it is mA.",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"The pattern “camera’s battery” contributes one count for both relation modifiers CmE|A,battery(camera) and CmA|E ,camera(battery).",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"RL has labeled “camera” as entity and “battery” as aspect in d.
The next two subsections present how to use the knowledge in the KB to improve the initial assignments for the label distributions, conditional label distributions and neighborhood weight distributions in order to achieve better final labeling/classification results for the current/new domain du+1.",5 Using Past Knowledge in Lifelong-RL,[0],[0]
"If two targets in the current domain corpus have no edge, we can check whether relation modifiers of the same two targets exist in some past domains.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"If so, we may be able to borrow them.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"But to ensure suitability, two consistency checks are performed.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Label Consistency Check: Since RL makes mistakes, we need to ensure that relation modifiers in a record in the KB are consistent with target labels in that past domain.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"For example, “camera’s battery” is confirmed by “camera” being labeled as entity and “battery” being labeled as aspect in a past domain d ∈ D. Without this consistency, the record may not be reliable and should be discarded from the KB.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"We define an indicator variable Idm,tj (ti) to ensure that the record r’s relation modifier is consistent
with the labels of its two targets:
IdmA|E ,tj (ti) =    1 if CdmA|E ,tj (ti) > 0 and Ld(ti) = aspect and Ld(tj) = entity
0 otherwise
(7)
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"For example, if “camera” is labeled as entity and “battery” is labeled as aspect in the past domain d, we have IdmA|E ,camera(battery) = 1 and IdmE|A,battery(camera)",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"= 1.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Type Consistency Check: Here we ensure the type modifiers for two targets in the current domain du+1 are consistent with these type modifiers in the past domain d ∈ D.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
This is because an item can be an aspect in one domain but an entity in another.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"For example, if the current domain is “Cellphone”, borrowing the relation “camera’s battery” from domain “Camera” can introduce an error because “camera” is an aspect in domain “Cellphone”.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Syntactic pattern “this” is a good indicator for this checking.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"In the “Cellphone” domain, “its camera” or “the camera” are often mentioned but not “this camera”.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"In the “Camera” domain, “this camera” is often mentioned.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"The type modifier of “camera” in “Cellphone” is mA, but in “Camera” it is mE .
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Updating Probabilities in Current Domain du+1: Edges for RL are in the forms of conditional label distribution P (L(ti)|L(tj)) and neighborhood weight distribution w(tj |ti).,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"We now discuss how to use the KB to estimate them more accurately.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Updating Conditional Label Distribution: Equation (5) tells that conditional label distribution P (L(ti)|L(tj)) is the weighted sum of relation modifiers’ label distributions Pmc , PmA|E , and PmE|A .",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
These 3 label distributions are pre-defined and given in Table 2.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
They are not changed.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Thus, we update conditional label distribution through updating the three relation modifiers’ weights q(Mtj (ti)) with the knowledge in the KB.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Recall the three relation modifiers are MR = {mc,mA|E ,mE|A}.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"After consistency check, there can be multiple relation modifiers between two targets in similar past domains",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Ds ⊂ D. The number of domains supporting a relation modifier m ∈ MR can tell which kind of relation modifiers is common and likely to be correct.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"For example, given many past domains like “Laptop”, “Tablet”, “Cellphone”, etc., “camera
and battery” appears more than “camera’s battery”, “camera” should be modified by “battery” more with mE|A rather than mc (likely to be an aspect).
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Let Cdu+1m,tj (ti) be the count that target ti modified by target tj on relation m in the current domain du+1 (not in KB).",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
The count C(CL) is for updating the Conditional Label (CL) distributions considering the information in both the current domain du+1 and the KB.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"It is calculated as:
C (CL) m,tj (ti) =
{ C du+1 m,tj (ti) if C du+1 m,tj
(ti) >",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
0∑,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"d∈Ds Idm,tj (ti) if ∑ m∈MR C du+1",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
m,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
",tj (ti)) = 0
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"This equation says that if there is any relation modifier existing between the two targets in the new domain du+1, we do not borrow edges from the KB; Otherwise, the number of similar past domains supporting the relation modifier m is used.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Recall that Idm,tj (ti) is the result calculated by Equation (7) after label consistency check.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"We use count C(CL)m,tj (ti) to update q du+1(Mtj (ti)) using Equation (4) in Section 4.2.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Then the conditional label distribution accommodating relation modifiers in the KB, P (LL1)(L(ti)|L(tj)), is calculated by Equation, (5) using qdu+1(Mtj (ti)).",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"LL1 denotes Lifelong Learning 1.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Updating Neighbor Weight Distribution: Equation (6) says that w(tj |ti) is the importance of target ti’s neighbor tj to ti among all ti’s neighbors.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"When updating conditional label distribution using the KB, the number of domains can decide which kind of relation modifiersm is more common between the two targets ti and tj .",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"But we cannot tell that neighbor tj is more important than another neighbor tj′ to ti.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"For example, given the past domains such as “Laptop”, “Tablet”, “Cellphone”, etc., no matter how many domains believe “camera” is an aspect given “battery” is also an aspect, if the current domain is “All-in-one desktop computer”, we should not consider the strong influences from “battery” in the past domains.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
We should rely more on the weights of “camera”’s neighbors provided by “Allin-one desktop computer”.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"That means “mouse”, “keyboard”, “screen” etc., should have strong influences on “camera” than “battery” because most Allin-one desktops (e.g. iMac) do not have battery.
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"We introduce another indicator variable IDm,tj (ti) =",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"⋃ d∈Ds Idm,tj (ti), to indicate whether target tj modified ti on relation m in past similar domains Ds.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"It only considers the existence of a
relation modifier m among domains Ds.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
The count C(w)tj (ti) for updating the neighbor weight (w) distribution considers both the KB and the current domain du+1.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"It is as follows:
C (w) tj (ti) =
{ ∑ m∈MR C du+1 m,tj (ti) if ∑ m∈MR C du+1 m,tj
(ti) >",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
0∑ m∈MR,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"I Du m,tj (ti) if ∑ m∈MR C du+1 m,tj (ti) = 0
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"This equation tells that if there are relation modifiers existing between the two targets in the new domain du+1, we count the total times that tj modifies ti in the new domain; Otherwise, we count the total kinds of relation modifiers in MR if a relation modifier m ∈ MR existed in past domains.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
Let w(LL1)(tj |ti) be the neighbor weight distribution considering knowledge from the KB and du+1.,5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"It is calculated by Equation (6) using C(w)tj (ti).
",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"The initial label distribution P du+1,0 is calculated by Equation (3) only using type modifiers found in the new domain du+1.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"We use Lifelong-RL-1 to denote the method that employs P (LL1)(L(ti)|L(tj)), w(LL1)(tj |ti) and P du+1,0 as inputs for RL.",5.1 Exploiting Relation Modifiers in the KB,[0],[0]
"Since we have target labels from past domains, we may have a better idea about the initial label probabilities of targets in the current domain du+1.",5.2 Exploiting Target Labels in the KB,[0],[0]
"For example, after labeling domains like “Cellphone”, “Laptop”, “Tablet,” and “E-reader”, we may have a good sense that “camera” is likely to be an aspect.",5.2 Exploiting Target Labels in the KB,[0],[0]
"To use such knowledge, we need to check if the type modifier of target t in the current domain matches those in past domains and only keep those domains that have such a matching type modifier.
",5.2 Exploiting Target Labels in the KB,[0],[0]
Let Ds ⊂ D be the past domains consistent with target t’s type modifier in the current domain du+1.,5.2 Exploiting Target Labels in the KB,[0],[0]
Let CD s (L(t)) be the number of domains in Ds that target t is labeled as L(t).,5.2 Exploiting Target Labels in the KB,[0],[0]
Let λ be the ratio that controls how much we trust knowledge from the KB.,5.2 Exploiting Target Labels in the KB,[0],[0]
"Then the initial label probability distribution P du+1,0 calculated by Equation (3) only using type modifier found in du+1 is replaced by :
P (LL2),0(L(t))",5.2 Exploiting Target Labels in the KB,[0],[0]
"= |D|×P du+1,0(L(t))+λCD
s (L(t))
|D|+λ|D| (8)
Similarly, let Ds ⊂ D be the past domains consistent with both targets ti’s and tj’s type modifiers in du+1.",5.2 Exploiting Target Labels in the KB,[0],[0]
"Let CD s (L(ti), L(tj)) be the number of domains inDs that ti and tj are labeled as L(ti) and
L(tj) respectively.",5.2 Exploiting Target Labels in the KB,[0],[0]
"The conditional label probability distribution accommodating relation modifiers in the KB, P (LL1)(L(ti)|L(tj)), is further updated to P (LL2)(L(ti)|L(tj)) by exploiting the target labels in KB (LL2 denotes Lifelong Learning 2):
P (LL2)(L(ti)|L(tj))",5.2 Exploiting Target Labels in the KB,[0],[0]
= |D|×P,5.2 Exploiting Target Labels in the KB,[0],[0]
"(LL1)(L(ti)|L(tj))+λCD s (L(ti),L(tj))
|D|+λ|D| (9)
",5.2 Exploiting Target Labels in the KB,[0],[0]
"For example, given “this camera”, “battery” in the current domain, we are more likely to consider domains (e.g. “Film Camera”, “DSLR”, but not “Cellphone”) that have entity modifiers on “camera” and aspect modifiers on “battery”.",5.2 Exploiting Target Labels in the KB,[0],[0]
"Then we count the number of those domains that label “camera” as entity and “battery” as aspect: CD s (L(camera) = entity, L(battery) = aspect).",5.2 Exploiting Target Labels in the KB,[0],[0]
"Similarly, we count domains having other types of target labels on “camera” and “battery”.",5.2 Exploiting Target Labels in the KB,[0],[0]
"These counts form an updated conditional label distribution that estimates “camera” as an entity and “battery” as an aspect.
",5.2 Exploiting Target Labels in the KB,[0],[0]
Note that |D,5.2 Exploiting Target Labels in the KB,[0],[0]
"− Ds|, the number of past domains not consistent with targets’ type modifiers, is added to CD s (L(ti) = NIL) and CD s (L(ti) = NIL, L(tj)) for Equations (8) and (9) respectively to make the sum over L(ti) equal to 1.",5.2 Exploiting Target Labels in the KB,[0],[0]
"We use Lifelong-RL to denote this method which uses P (LL2),0(L(t)), P (LL2)(L(ti)|L(tj)) and w(LL1)(tj |ti) as input for RL.",5.2 Exploiting Target Labels in the KB,[0],[0]
We now evaluate the proposed method and compare with baselines.,6 Experiments,[0],[0]
"We use the DP method for target extraction (Qiu et al., 2011).",6 Experiments,[0],[0]
This method uses dependency relations between opinion words and targets to extract targets using seed opinion words.,6 Experiments,[0],[0]
"Since our paper does not focus on extraction, interested readers can refer to (Qiu et al., 2011) for details.",6 Experiments,[0],[0]
Evaluation Datasets: We use two sets of datasets.,6.1 Experiment Settings,[0],[0]
The first set consists of eight (8) annotated review datasets.,6.1 Experiment Settings,[0],[0]
"We use each of them as the new domain data in LML to compute precision, recall, F1 scores.",6.1 Experiment Settings,[0],[0]
"Five of them are from (Hu and Liu, 2004), and the remaining three are from (Liu et al., 2016).",6.1 Experiment Settings,[0],[0]
"They have been used for target extraction, and thus have annotated targets, but no annotation on whether a
target is an entity or aspect.",6.1 Experiment Settings,[0],[0]
"We made this annotation, which is straightforward.",6.1 Experiment Settings,[0],[0]
We used two annotators to annotate the datasets.,6.1 Experiment Settings,[0],[0]
The Cohen’s kappa is 0.84.,6.1 Experiment Settings,[0],[0]
"Through discussion, the annotators got complete agreement.",6.1 Experiment Settings,[0],[0]
Details of the datasets are listed in Table 1.,6.1 Experiment Settings,[0],[0]
Each cell is the number of distinct terms.,6.1 Experiment Settings,[0],[0]
"These datasets are not very large but they are realistic because many products do not have a large number of reviews.
",6.1 Experiment Settings,[0],[0]
The second set consists of unlabeled review datasets from 100 diverse products or domains (Chen and Liu 2014).,6.1 Experiment Settings,[0],[0]
Each domain has 1000 reviews.,6.1 Experiment Settings,[0],[0]
"They are treated as past domain data in LML since they are not annotated and thus cannot be used for computing evaluation measures.
",6.1 Experiment Settings,[0],[0]
"Evaluating Measures: We mainly use precision P , recall R, and F1-score F1 as evaluation measures.",6.1 Experiment Settings,[0],[0]
"We take multiple occurrences of the same target as one count, and only evaluate entities and aspects.",6.1 Experiment Settings,[0],[0]
"We will also give the accuracy results.
",6.1 Experiment Settings,[0],[0]
"Compared Methods: We compare the following methods, including our proposed method, LifelongRL.
NER+TM: NER is Named Entity Recognition.
",6.1 Experiment Settings,[0],[0]
We can regard the extracted terms from a NER system as entities and the rest of the targets as aspects.,6.1 Experiment Settings,[0],[0]
"However, a NER system cannot identify entities such as “this car” from “this car is great.”",6.1 Experiment Settings,[0],[0]
Its result is rather poor.,6.1 Experiment Settings,[0],[0]
"But our type modifier (TM) does that, i.e., if an opinion target appears after “this” or “these” in at least two sentences, TM labels the target as an entity; otherwise an aspect.",6.1 Experiment Settings,[0],[0]
"However, TM cannot extract named entities.",6.1 Experiment Settings,[0],[0]
Its result is also rather poor.,6.1 Experiment Settings,[0],[0]
We thus combine the two methods to give NER+TM as they complement each other very well.,6.1 Experiment Settings,[0],[0]
"To make NER more powerful, we use two NER systems: Stanford-NER 1(Manning et al., 2014) and UIUC-NER2 (Ratinov and Roth, 2009).",6.1 Experiment Settings,[0],[0]
"NER+TM treats the extracted entities by the three systems as entities and the rest of the targets as aspects.
",6.1 Experiment Settings,[0],[0]
"NER+TM+DICT: We run NER+TM on the 100 datasets for LML to get a list of entities, which we call the dictionary (DICT).",6.1 Experiment Settings,[0],[0]
"For a new task, if any target word is in the list, it is treated as an entity; otherwise an aspect.
",6.1 Experiment Settings,[0],[0]
RL:,6.1 Experiment Settings,[0],[0]
This is the base method described in Section 3.,6.1 Experiment Settings,[0],[0]
"It performs relaxation labeling (RL) without the help of LML.
",6.1 Experiment Settings,[0],[0]
Lifelong-RL-1:,6.1 Experiment Settings,[0],[0]
"This performs LML with RL but the current task only uses the relations in the KB from previous tasks (Section 5.1).
",6.1 Experiment Settings,[0],[0]
Lifelong-RL:,6.1 Experiment Settings,[0],[0]
This is our proposed final method.,6.1 Experiment Settings,[0],[0]
"It improves Lifelong-RL-1 by further incorporating target labels in the KB from previous tasks (Section 5.2).
",6.1 Experiment Settings,[0],[0]
"Parameter Settings: RL has 2 initial label distributions PmE and PmA and 3 conditional label distributions Pmc , PmE|A and PmA|E .",6.1 Experiment Settings,[0],[0]
"Like other belief propagation algorithms, these probabilities need to be set empirically, as shown in Table 2.",6.1 Experiment Settings,[0],[0]
The parameter α is set to 1.,6.1 Experiment Settings,[0],[0]
Our LML method has one parameter λ for Lifelong-RL.,6.1 Experiment Settings,[0],[0]
We set it to 0.1.,6.1 Experiment Settings,[0],[0]
"Table 3 shows the test results of all systems in precision, recall and F1-score except NER+TM+DICT.",6.2 Results Analysis,[0],[0]
NER+TM+DICT is not included due to space limitations and because it performed very poorly.,6.2 Results Analysis,[0],[0]
"The reason is that a target can be an entity in one domain
1http://nlp.stanford.edu/software/CRF-NER.shtml 2https://cogcomp.cs.illinois.edu/page/software view/NETagger
but an aspect in another.",6.2 Results Analysis,[0],[0]
"Its average F1-score for entity is only 49.2, and for aspect is only 50.2.
",6.2 Results Analysis,[0],[0]
"Entity Results Comparison: We observe from the table that although NER+TM combines NER and TM, its result for entities is still rather poor.",6.2 Results Analysis,[0],[0]
We notice that phrases like “this price” causes low precision.,6.2 Results Analysis,[0],[0]
"Since it does not use many other relations and NER does not recognize many named entities that are written in lower case letters (e.g., “apple is good”), its recall is also low.
",6.2 Results Analysis,[0],[0]
RL has a higher precision as it considers relation modifiers.,6.2 Results Analysis,[0],[0]
"However, its recall is low because it lacks information in its graph, which causes RL to make many wrong decisions.",6.2 Results Analysis,[0],[0]
Lifelong-RL-1 introduces relation modifiers in KB from past domains into the current task.,6.2 Results Analysis,[0],[0]
"Both precision and recall increase markedly.
",6.2 Results Analysis,[0],[0]
Lifelong-RL improves Lifelong-RL-1 further by considering target labels of past domains.,6.2 Results Analysis,[0],[0]
Their counts improve the initial label probability distributions and conditional label probability distributions.,6.2 Results Analysis,[0],[0]
"For example, “this price” may appear in some domains but “price”’s target label is mostly aspect.",6.2 Results Analysis,[0],[0]
We consider their counts in initial label distributions and thus rectify the initial distribution of “price”.,6.2 Results Analysis,[0],[0]
"This makes “price” easier to be classified as aspect and thus improves the precision for entity.
",6.2 Results Analysis,[0],[0]
Aspect Results,6.2 Results Analysis,[0],[0]
"Comparison: For aspects, the trend is the same but the improvements are not as dramatic as for entity.",6.2 Results Analysis,[0],[0]
This is because the distribution of entity and aspect in the data is highly skewed.,6.2 Results Analysis,[0],[0]
There are many more aspects than entities as we can see from the Table 1.,6.2 Results Analysis,[0],[0]
"When an entity term is wrongly classified as an aspect, it has much less impact on the aspect result than on the entity result.
",6.2 Results Analysis,[0],[0]
"Accuracy Results Comparison: Table 4 gives the classification accuracy results considering all
three classes.",6.2 Results Analysis,[0],[0]
We can see the similar trend.,6.2 Results Analysis,[0],[0]
NER+TM+DICT’s average accuracy is only 45.89 and is not included in the table.,6.2 Results Analysis,[0],[0]
This paper studied the problem of classifying opinion targets into entities and aspects.,7 Conclusion,[0],[0]
"To the best of our knowledge, this problem has not been attempted in the unsupervised opinion target extraction setting.",7 Conclusion,[0],[0]
But this is an important problem because without separating or classifying them one will not know whether an opinion is about an entity as a whole or about a specific aspect of an entity.,7 Conclusion,[0],[0]
This paper proposed a novel method based on relaxation labeling and the paradigm of lifelong machine learning to solve the problem.,7 Conclusion,[0],[0]
Experimental results showed the effectiveness of the proposed method.,7 Conclusion,[0],[0]
"This work was partially supported by National Science Foundation (NSF) grants IIS-1407927 and IIS1650900, and NCI grant R01CA192240.",Acknowledgments,[0],[0]
The content of the paper is solely the responsibility of the authors and does not necessarily represent the official views of the NSF or NCI.,Acknowledgments,[0],[0]
It is well-known that opinions have targets.,abstractText,[0],[0]
"Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use.",abstractText,[0],[0]
So far many algorithms have been proposed to extract opinion targets.,abstractText,[0],[0]
"However, an opinion target can be an entity or an aspect (part or attribute) of an entity.",abstractText,[0],[0]
"An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity.",abstractText,[0],[0]
"Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions.",abstractText,[0],[0]
"This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling.",abstractText,[0],[0]
Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.,abstractText,[0],[0]
Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1389–1399, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016;
Verga and McCallum, 2016).",1 Introduction,[0],[0]
"Although these models are able to learn robust representations from large amounts of data, they often lack commonsense knowledge.",1 Introduction,[0],[0]
"Such knowledge is rarely explicitly stated in texts but can be found in resources like PPDB (Ganitkevitch et al., 2013) or WordNet (Miller, 1995).
",1 Introduction,[0],[0]
"Combining neural methods with symbolic commonsense knowledge, for instance in the form of implication rules, is in the focus of current research (Rocktäschel et al., 2014; Wang et al., 2014; Bowman et al., 2015; Wang et al., 2015; Vendrov et al., 2016; Hu et al., 2016; Rocktäschel and Riedel, 2016; Cohen, 2016).",1 Introduction,[0],[0]
"A recent approach (Rocktäschel et al., 2015) regularizes entity-tuple and relation embeddings via first-order logic rules.",1 Introduction,[0],[0]
"To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule.",1 Introduction,[0],[0]
This approach does not scale beyond only a few entity-tuples and rules.,1 Introduction,[0],[0]
"For example, propositionalizing the rule ∀x : isMan(x)⇒ isMortal(x) would result in a very large number of loss terms on a large database.
",1 Introduction,[0],[0]
"In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts.",1 Introduction,[0],[0]
"This is achieved by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entity pairs.",1 Introduction,[0],[0]
"It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space.",1 Introduction,[0],[0]
"In the example given above, if we require that every component of the
1389
vector representation of isMan is smaller than the corresponding component of relation isMortal, then we can show that the rule holds for any nonnegative representation of an entity-tuple.",1 Introduction,[0],[0]
Hence our method avoids the need for separate loss terms for every ground atom resulting from propositionalizing rules.,1 Introduction,[0],[0]
"In statistical relational learning this type of approach is often referred to as lifted inference or learning (Poole, 2003; Braz, 2007) because it deals with groups of random variables at a first-order level.",1 Introduction,[0],[0]
In this sense our approach is a lifted form of rule injection.,1 Introduction,[0],[0]
This allows for imposing large numbers of rules while learning distributed representations of relations and entity-tuples.,1 Introduction,[0],[0]
"Besides drastically lower computation time, an important advantage of our method over Rocktäschel et al. (2015) is that when these constraints are satisfied, the injected rules always hold, even for unseen but inferred facts.",1 Introduction,[0],[0]
"While the method presented here only deals with implications and not general first-order rules, it does not rely on the assumption of independence between relations, and is hence more generally applicable.
",1 Introduction,[0],[0]
"Our contributions are fourfold: (i) we develop a very efficient way of regularizing relation representations to incorporate first-order logic implications (§3), (ii) we reveal that, against expectation, mapping entity-tuple embeddings to non-negative space does not hurt but instead improves the generalization ability of our model (§5.1) (iii) we show improvements on a knowledge base completion task by injecting mined commonsense rules from WordNet (§5.3), and finally (iv) we give a qualitative analysis of the results, demonstrating that implication constraints are indeed satisfied in an asymmetric way and result in a substantially increased structuring of the relation embedding space (§5.6).",1 Introduction,[0],[0]
In this section we revisit the matrix factorization relation extraction model by Riedel et al. (2013) and introduce the notation used throughout the paper.,2 Background,[0],[0]
"We choose the matrix factorization model for its simplicity as the base on which we develop implication injection.
",2 Background,[0],[0]
"Riedel et al. (2013) represent every relation r ∈ R (selected from Freebase (Bollacker et al., 2008) or extracted as textual surface pattern) by a k-
dimensional latent representation r ∈ Rk.",2 Background,[0],[0]
"A particular relation instance or fact is the combination of a relation r and a tuple t of entities that are engaged in that relation, and is written as 〈r, t〉.",2 Background,[0],[0]
We write O as the set of all such input facts available for training.,2 Background,[0],[0]
"Furthermore, every entity-tuple t ∈ T is represented by a latent vector t ∈ Rk (with T the set of all entity-tuples in O).
",2 Background,[0],[0]
Model F by Riedel et al. (2013) measures the compatibility between a relation r and an entitytuple t using the dot product r>t of their respective vector representations.,2 Background,[0],[0]
"During training, the representations are learned such that valid facts receive high scores, whereas negative ones receive low scores.",2 Background,[0],[0]
"Typically no negative evidence is available at training time, and therefore a Bayesian Personalized Ranking (BPR) objective (Rendle et al., 2009) is used.",2 Background,[0],[0]
"Given a pair of facts fp := 〈rp, tp〉 6∈",2 Background,[0],[0]
"O and fq := 〈rq, tq",2 Background,[0],[0]
"〉 ∈ O, this objective requires that
r>p tp ≤ r>q tq.",2 Background,[0],[0]
"(1)
The embeddings can be trained by minimizing a convex loss function `R that penalizes violations of that requirement when iterating over the training set.",2 Background,[0],[0]
"In practice, each positive training fact 〈r, tq〉 is compared with a randomly sampled unobserved fact 〈r, tp〉 for the same relation.",2 Background,[0],[0]
"The overall loss can hence be written as
LR = ∑
〈r,tq〉∈O tp∈T , 〈r,tp〉6∈O
`R ( r>[tp − tq] ) .",2 Background,[0],[0]
"(2)
and measures how well observed valid facts are ranked above unobserved facts, thus reconstructing the ranking of the training data.",2 Background,[0],[0]
"We will henceforth call LR the reconstruction loss, to make a distinction with the implication loss that we will introduce later.",2 Background,[0],[0]
"Riedel et al. (2013) use the logistic loss `R(s) := − log σ(−s), where σ(s) := (1 + e−x)−1 denotes the sigmoid function.",2 Background,[0],[0]
"In order to avoid overfitting, an L2 regularization term on the r and t embeddings is added to the reconstruction loss.",2 Background,[0],[0]
"The overall objective to minimize hence is
LF = LR + α (∑ r‖r‖22 + ∑ t‖t‖22 )
(3)
where α is the regularization strength.",2 Background,[0],[0]
"In this section, we show how an implication
∀t ∈ T : 〈rp, t〉 ⇒ 〈rq, t〉, (4)
can be imposed independently of the entity-tuples.",3 Lifted Injection of Implications,[0],[0]
"For simplicity, we abbreviate such implications as rp ⇒ rq (e.g., professorAt⇒ employeeAt).",3 Lifted Injection of Implications,[0],[0]
The implication rule can be imposed by requiring that every tuple t ∈ T is at least as compatible with relation rp as with rq.,3.1 Grounded Loss Formulation,[0],[0]
"Written in terms of the latent representations, eq. (4) therefore becomes
∀t ∈ T :",3.1 Grounded Loss Formulation,[0],[0]
"r>p t ≤ r>q t (5)
",3.1 Grounded Loss Formulation,[0],[0]
"If 〈rp, t〉 is a true fact with a high score r>p t, and the fact 〈rq, t〉 has an even higher score, it must also be true, but not vice versa.",3.1 Grounded Loss Formulation,[0],[0]
"We can therefore inject an implication rule by minimizing a loss term with a separate contribution from every t ∈ T , adding up to the total loss if the corresponding inequality is not satisfied.",3.1 Grounded Loss Formulation,[0],[0]
"In order to make the contribution of every tuple t to that loss independent of the magnitude of the tuple embedding, we divide both sides of the above inequality by ‖t‖1.",3.1 Grounded Loss Formulation,[0],[0]
"With t̃ := t/‖t‖1, the implication loss for the rule rp ⇒ rq can be written as
LI = ∑ ∀t∈T `I ( [rp − rq]>t̃ ) (6)
for an appropriate convex loss function `I , similarly to eq.",3.1 Grounded Loss Formulation,[0],[0]
(2).,3.1 Grounded Loss Formulation,[0],[0]
"In practice, the summation can be reduced to those tuples that occur in combination with rp or rq in the training data.",3.1 Grounded Loss Formulation,[0],[0]
"Still, the propositionalization in terms of training facts leads to a heavy computational cost for imposing a single implication, similar to the technique introduced in Rocktäschel et al. (2015).",3.1 Grounded Loss Formulation,[0],[0]
"Moreover, with that simplification there is no guarantee that the implication between both relations would generalize towards inferred facts not seen during training.",3.1 Grounded Loss Formulation,[0],[0]
"The problems mentioned above can be avoided if instead of LI , a tuple-independent upper bound is minimized.",3.2 Lifted Loss Formulation,[0],[0]
"Such a bound can be constructed, provided all components of t are restricted to a nonnegative embedding space, i.e., T ⊆ Rk,+.",3.2 Lifted Loss Formulation,[0],[0]
"If this
holds, Jensen’s inequality allows us to transform eq.",3.2 Lifted Loss Formulation,[0],[0]
"(6) as follows
LI = ∑
∀t∈T `I
( k∑
i=1
t̃i [rp − rq]>1i )
(7)
≤ k∑
i=1
`I ( [rp − rq]>1i )",3.2 Lifted Loss Formulation,[0],[0]
"∑
∀t∈T t̃i (8)
where 1i is the unit vector along dimension i in tuple-space.",3.2 Lifted Loss Formulation,[0],[0]
"This is allowed because the {t̃i}ki=1 form convex coefficients (t̃i > 0, and ∑ i t̃i = 1), and `I is a convex function.",3.2 Lifted Loss Formulation,[0],[0]
"If we define
LUI := k∑
i=1
`I ( [rp − rq]>1i ) (9)
we can write LI ≤ βLUI",3.2 Lifted Loss Formulation,[0],[0]
"(10) in which β is an upper bound on ∑
t t̃i.",3.2 Lifted Loss Formulation,[0],[0]
"One such bound is |T |, but others are conceivable too.",3.2 Lifted Loss Formulation,[0],[0]
In practice we rescale β to a hyper-parameter β̃ that we use to control the impact of the upper bound to the overall loss.,3.2 Lifted Loss Formulation,[0],[0]
"We call LUI the lifted loss, as it no longer depends on any of the entity-tuples; it is grounded over the unit tuples 1i instead.
",3.2 Lifted Loss Formulation,[0],[0]
The implication rp ⇒ rq can thus be imposed by minimizing the lifted loss LUI .,3.2 Lifted Loss Formulation,[0],[0]
"Note that by minimizing LUI , the model is encouraged to satisfy the constraint",3.2 Lifted Loss Formulation,[0],[0]
"rp ≤ rq on the relation embeddings, where ≤ denotes the component-wise comparison.",3.2 Lifted Loss Formulation,[0],[0]
"In fact, a sufficient condition for eq. (5) to hold, is
rp ≤ rq and ∀t ∈ T :",3.2 Lifted Loss Formulation,[0],[0]
"t ≥ 0 (11)
with 0 the k-dimensional null vector.",3.2 Lifted Loss Formulation,[0],[0]
"This corresponds to a single relation-specific loss term, and the general restriction T ⊆ Rk,+ on the tupleembedding space.",3.2 Lifted Loss Formulation,[0],[0]
"In order to impose implications by minimizing a lifted loss LUI , the tuple-embedding space needs to be restricted to Rk,+.",3.3 Approximately Boolean Entity Tuples,[0],[0]
"We have chosen to restrict the tuple space even more than required, namely to the hypercube t ∈",3.3 Approximately Boolean Entity Tuples,[0],[0]
"[0, 1]k, as approximately Boolean embeddings (Kruszewski et al., 2015).",3.3 Approximately Boolean Entity Tuples,[0],[0]
"The tuple
embeddings are constructed from real-valued vectors e, using the component-wise sigmoid function
t = σ(e), e ∈ Rk.",3.3 Approximately Boolean Entity Tuples,[0],[0]
"(12)
",3.3 Approximately Boolean Entity Tuples,[0],[0]
"For minimizing the loss, the gradients are hence computed with respect to e, and the L2 regularization is applied to the components of e instead of t.
Other choices for ensuring the restriction t ≥ 0 in eq.",3.3 Approximately Boolean Entity Tuples,[0],[0]
"(11) are possible, but we found that our approach works better in practice than those (e.g., the exponential transformation proposed by Demeester et al. (2016)).",3.3 Approximately Boolean Entity Tuples,[0],[0]
"It can also be observed that the unit tuples over which the implication loss is grounded, form a special case of approximately Boolean embeddings.
",3.3 Approximately Boolean Entity Tuples,[0],[0]
"In order to investigate the impact of this restriction even when not injecting any rules, we introduce model FS: the original model F, but with sigmoidal entity-tuples:
LFS = ∑
〈r,tq〉∈O tp∈T , 〈r,tp〉6∈O
`R ( r>[σ(ep)− σ(eq)] )
+ α (∑ r‖r‖22 + ∑ e‖e‖22 )
(13)
Here, ep and eq are the real-valued representations as in eq.",3.3 Approximately Boolean Entity Tuples,[0],[0]
"(12), for tuples tp and tq, respectively.
",3.3 Approximately Boolean Entity Tuples,[0],[0]
"With the above choice of a non-negative tupleembedding space we can now state the full lifted rule injection model (FSL):
LFSL = LFS + β̃ ∑
I∈I LUI (14)
LUI denotes a lifted loss term for every rule in a set I of implication rules that we want to inject.",3.3 Approximately Boolean Entity Tuples,[0],[0]
The logistic loss `R (see §2) is not suited for imposing implications because once the inequality in eq.,3.4 Convex Implication Loss,[0],[0]
"(11) is satisfied, the components of rp and rq do not need to be separated any further.",3.4 Convex Implication Loss,[0],[0]
"However, with `R this would continue to happen due to the small non-zero gradient.",3.4 Convex Implication Loss,[0],[0]
In the reconstruction loss LR this is a desirable effect which further separates the scores for positive from negative examples.,3.4 Convex Implication Loss,[0],[0]
"However, if an implication is imposed between two relations that are almost equivalent according to the
training data, we still want to find almost equivalent embedding vectors.",3.4 Convex Implication Loss,[0],[0]
"Hence, we propose to use the loss
`I(s) = max(0, s+ δ) (15)
with δ a small positive margin to ensure that the gradient does not disappear before the inequality is actually satisfied.",3.4 Convex Implication Loss,[0],[0]
"We use δ = 0.01 in all experiments.
",3.4 Convex Implication Loss,[0],[0]
"The main advantage of the presented approach over earlier methods that impose the rules in a grounded way (Rocktäschel et al., 2015; Wang et al., 2015) is the computational efficiency of imposing the lifted loss.",3.4 Convex Implication Loss,[0],[0]
Evaluating LUI or its gradient for one implication rule is comparable to evaluating the reconstruction loss for one pair of training facts.,3.4 Convex Implication Loss,[0],[0]
In typical applications there are much fewer rules than training facts and the extra computation time needed to inject these rules is therefore negligible.,3.4 Convex Implication Loss,[0],[0]
Recent research on combining rules with learned vector representations has been important for new developments in the field of knowledge base completion.,4 Related Work,[0],[0]
Rocktäschel et al. (2014) and Rocktäschel et al. (2015) provided a framework to jointly maximize the probability of observed facts and propositionalized first-order logic rules.,4 Related Work,[0],[0]
Wang et al. (2015) demonstrated how different types of rules can be incorporated using an Integer Linear Programming approach.,4 Related Work,[0],[0]
Wang and Cohen (2016) learned embeddings for facts and first-order logic rules using matrix factorization.,4 Related Work,[0],[0]
"Yet, all of these approaches ground the rules in the training data, limiting their scalability towards large rule sets and KBs with many entities.",4 Related Work,[0],[0]
"As argued in the introduction, this forms an important motivation for the lifted rule injection model put forward in this work, which by construction does not suffer from that limitation.",4 Related Work,[0],[0]
"Wei et al. (2015) proposed an alternative strategy to tackle the scalability problem by reasoning on a filtered subset of grounded facts.
",4 Related Work,[0],[0]
"Wu et al. (2015) proposed to use a path ranking approach for capturing long-range interactions between entities, and to add these as an extra loss term, besides the loss that models pairwise relations.",4 Related Work,[0],[0]
"Our model FSL differs substantially from their approach, in that we consider tuples instead of separate entities, and we inject a given set of rules.",4 Related Work,[0],[0]
"Yet, by cre-
ating a partial ordering in the relation embeddings as a result of injecting implication rules, model FSL can also capture interactions beyond direct relations.",4 Related Work,[0],[0]
"This will be demonstrated in §5.3 by injecting rules between surface patterns only and still measuring an improvement on predictions for structured Freebase relations.
",4 Related Work,[0],[0]
Combining logic and distributed representations is also an active field of research outside of automated knowledge base completion.,4 Related Work,[0],[0]
"Recent advances include the work by Faruqui et al. (2014), who injected ontological knowledge from WordNet into word representations.",4 Related Work,[0],[0]
"Furthermore, Vendrov et al. (2016) proposed to enforce a partial ordering in an embeddings space of images and phrases.",4 Related Work,[0],[0]
Our method is related to such order embeddings since we define a partial ordering on relation embeddings.,4 Related Work,[0],[0]
"However, to ensure that implications hold for all entity-tuples we also need a restriction on the entitytuple embedding space and derive bounds on the loss.",4 Related Work,[0],[0]
"Another important contribution is the recent work by Hu et al. (2016), who proposed a framework for injecting rules into general neural network architectures, by jointly training on the actual targets and on the rule-regularized predictions provided by a teacher network.",4 Related Work,[0],[0]
"Although quite different at first sight, their work could offer a way to use our model in various neural network architectures, by integrating the proposed lifted loss into the teacher network.
",4 Related Work,[0],[0]
"This paper builds upon our previous workshop paper (Demeester et al., 2016).",4 Related Work,[0],[0]
"In that work, we tested different tuple embedding transformations in an ad-hoc manner.",4 Related Work,[0],[0]
"We used approximately Boolean representations of relations instead of entity-tuples, strongly reducing the model’s degrees of freedom.",4 Related Work,[0],[0]
We now derive the FSL model from a carefully considered mathematical transformation of the grounded loss.,4 Related Work,[0],[0]
"The FSL model only restricts the tuple embedding space, whereby relation vectors remain real valued.",4 Related Work,[0],[0]
"Furthermore, previous experiments were performed on small-scale artificial datasets, whereas we now test on a real-world relation extraction benchmark.
",4 Related Work,[0],[0]
"Finally, we explicitly discuss the main differences with respect to the strongly related work from Rocktäschel et al. (2015).",4 Related Work,[0],[0]
"Their method is more general, as they cover a wide range of first-order logic rules, whereas we only discuss implications.",4 Related Work,[0],[0]
"Lifted
rule injection beyond implications will be studied in future research contributions.",4 Related Work,[0],[0]
"However, albeit less general, our model has a number of clear advantages:
Scalability – Our proposed model of lifted rule injection scales according to the number of implication rules, instead of the number of rules times the number of observed facts for every relation present in a rule.
",4 Related Work,[0],[0]
"Generalizability – Injected implications will hold even for facts not seen during training, because their validity only depends on the order relation imposed on the relation representations.",4 Related Work,[0],[0]
"This is not guaranteed when training on rules grounded in training facts by Rocktäschel et al. (2015).
",4 Related Work,[0],[0]
"Training Flexibility – Our method can be trained with various loss functions, including the rank-based loss as used in Riedel et al. (2013).",4 Related Work,[0],[0]
"This was not possible for the model of Rocktäschel et al. (2015) and already leads to an improved accuracy as seen from the zero-shot learning experiment in §5.2.
",4 Related Work,[0],[0]
Independence Assumption –,4 Related Work,[0],[0]
"In Rocktäschel et al. (2015) an implication of the form ap ⇒ aq for two ground atoms ap and aq is modeled by the logical equivalence ¬(ap ∧ ¬aq), and its probability is approximated in terms of the elementary probabilities π(ap) and π(aq) as 1 − π(ap) ( 1 − π(aq) ) .",4 Related Work,[0],[0]
"This assumes the independence of the two atoms ap and aq, which may not hold in practice.",4 Related Work,[0],[0]
Our approach does not rely on that assumption and also works for cases of statistical dependence.,4 Related Work,[0],[0]
"For example, the independence assumption does not hold in the trivial case where the relations rp and rq in the two atoms are equivalent, whereas in our model, the constraints rp ≤ rq and rp ≥ rq would simply reduce to rp = rq.",4 Related Work,[0],[0]
We now present our experimental results.,5 Experiments and Results,[0],[0]
We start by describing the experimental setup and hyperparameters.,5 Experiments and Results,[0],[0]
"Before turning to the injection of rules, we compare model F with model FS, and show that restricting the tuple embedding space has a regularization effect, rather than limiting the expressiveness of the model (§5.1).",5 Experiments and Results,[0],[0]
"We then demonstrate that model FSL is capable of zero-shot learning (§5.2), and show that injecting high-quality WordNet rules
leads to an improved precision (§5.3).",5 Experiments and Results,[0],[0]
"We proceed with a visual illustration of the relation embeddings with and without injected rules (§5.4), provide details on time efficiency of the lifted rule injection method (§5.5), and show that it correctly captures the asymmetry of implication rules (§5.6).
",5 Experiments and Results,[0],[0]
"All models were implemented in TensorFlow (Abadi et al., 2015).",5 Experiments and Results,[0],[0]
"We use the hyperparameters of Riedel et al. (2013), with k = 100 hidden dimensions and a weight of α = 0.01 for the L2 regularization loss.",5 Experiments and Results,[0],[0]
"We use ADAM (Kingma and Ba, 2014) for optimization with an initial learning rate of 0.005 and a mini-batch size of 8192.",5 Experiments and Results,[0],[0]
"The embeddings are initialized by sampling uniformly from [−0.1, 0.1] and we use β̃ = 0.1 for the implication loss throughout our experiments.",5 Experiments and Results,[0],[0]
"Before incorporating external commonsense knowledge into relation representations, we were curious how much we lose by restricting the entity-tuple space to approximately Boolean embeddings.",5.1 Restricted Embedding Space,[0],[0]
We evaluate our models on the New York Times dataset introduced by Riedel et al. (2013).,5.1 Restricted Embedding Space,[0],[0]
"Surprisingly, we find that the expressiveness of the model does not
suffer from this strong restriction.",5.1 Restricted Embedding Space,[0],[0]
"From Table 1 we see that restricting the tuple-embedding space seems to perform slightly better (FS) as opposed to a realvalued tuple-embedding space (F), suggesting that this restriction has a regularization effect that improves generalization.",5.1 Restricted Embedding Space,[0],[0]
We also provide the original results for model F by Riedel et al. (2013) (denoted as R13-F) for comparison.,5.1 Restricted Embedding Space,[0],[0]
"Due to a different implementation and optimization procedure, the results for our model F and R13-F are not identical.
",5.1 Restricted Embedding Space,[0],[0]
Inspecting the top relations for a sampled dimension in the embedding space reveals that the relation space of model FS more closely resembles clusters than that of model F (Table 2).,5.1 Restricted Embedding Space,[0],[0]
"We hypothesize that this might be caused by approximately Boolean entity-tuple representations in model FS, resulting in attribute-like entity-tuple vectors that capture which relation clusters they belong to.",5.1 Restricted Embedding Space,[0],[0]
"The zero-shot learning experiment performed in Rocktäschel et al. (2015) leads to an important finding: when injecting implications with right-hand sides for Freebase relations for which no or very limited training facts are available, the model should be able to infer the validity of Freebase facts for those relations based on rules and correlations between textual surface patterns.
",5.2 Zero-shot Learning,[0],[0]
"We inject the same hand-picked relations as used by Rocktäschel et al. (2015), after removing all Freebase training facts.",5.2 Zero-shot Learning,[0],[0]
"The lifted rule injection (model FSL) reaches a weighted MAP of 0.35, comparable with 0.38 by the Joint model from Rocktäschel et al. (2015) (denoted R15-Joint).",5.2 Zero-shot Learning,[0],[0]
"Note that for this experiment we initialized the Freebase relations implied by the rules with negative random vectors (sampled uniformly from [−7.9,−8.1]).",5.2 Zero-shot Learning,[0],[0]
"The reason is that without any negative training facts for these relations, their components can only go up due to the implication loss, and we do not want to get values that are too high before optimization.
",5.2 Zero-shot Learning,[0],[0]
Figure 1 shows how the relation extraction performance improves when more Freebase relation training facts are added.,5.2 Zero-shot Learning,[0],[0]
"It effictively measures how well the proposed models, matrix factorization (F), propositionalized rule injection (R15-Joint), and our model (FSL), can make use of the provided rules and correlations between textual surface form pat-
terns and increased fractions of Freebase training facts.",5.2 Zero-shot Learning,[0],[0]
"Although FSL starts at a lower performance than R15-Joint when no Freebase training facts are present, it outperforms R15-Joint and a plain matrix factorization model by a substantial margin when provided with more than 7.5% of Freebase training facts.",5.2 Zero-shot Learning,[0],[0]
"This indicates that, in addition to being much faster than R15-Joint, it can make better use of provided rules and few training facts.",5.2 Zero-shot Learning,[0],[0]
We attribute this to the Bayesian personalized ranking loss instead of the logistic loss used in Rocktäschel et al. (2015).,5.2 Zero-shot Learning,[0],[0]
"The former is compatible with our ruleinjection method, but not with the approach of maximizing the expectation of propositional rules used by R15-Joint.",5.2 Zero-shot Learning,[0],[0]
"The main purpose of this work is to be able to incorporate rules from external resources for aid-
ing relation extraction.",5.3 Injecting Knowledge from WordNet,[0],[0]
We use WordNet hypernyms to generate rules for the NYT dataset.,5.3 Injecting Knowledge from WordNet,[0],[0]
To this end we iterate over all surface form patterns in the dataset and attempt to replace words in the pattern by their hypernyms.,5.3 Injecting Knowledge from WordNet,[0],[0]
"If the resulting pattern is contained in the dataset, we generate the corresponding rule.",5.3 Injecting Knowledge from WordNet,[0],[0]
"For instance, we generate a rule appos->diplomat->amod ⇒ appos->official->amod since both patterns are contained in the NYT dataset and we know from WordNet that a diplomat is an official.",5.3 Injecting Knowledge from WordNet,[0],[0]
This leads to 427 rules from WordNet that we subsequently annotate manually to obtain 36 high-quality rules.,5.3 Injecting Knowledge from WordNet,[0],[0]
Note that none of these rules directly imply a Freebase relation.,5.3 Injecting Knowledge from WordNet,[0],[0]
"Although the test relations all originate from Freebase, we still hope to see improvements by transitive effects, i.e., better surface form representations that in turn help to predict Freebase facts.
",5.3 Injecting Knowledge from WordNet,[0],[0]
We show results obtained by injecting these WordNet rules in Table 1 (column FSL).,5.3 Injecting Knowledge from WordNet,[0],[0]
"The weighted MAP measure increases by 2% with respect to model FS, and 4% compared to our reimplementation of the matrix factorization model F. This demonstrates that imposing a partial ordering based on implication rules can be used to incorporate logical commonsense knowledge and increase the quality of information extraction systems.",5.3 Injecting Knowledge from WordNet,[0],[0]
"Note that our evaluation setting guarantees that only indirect effects of the rules are measured, i.e., we do not use any rules directly implying test relations.",5.3 Injecting Knowledge from WordNet,[0],[0]
This shows that injecting such rules influences the relation embedding space beyond only the relations explicitly stated in the rules.,5.3 Injecting Knowledge from WordNet,[0],[0]
"For example, injecting the rule appos<-father->appos ⇒ poss<-parent->appos can contribute to improved predictions for the test relation parent/child.",5.3 Injecting Knowledge from WordNet,[0],[0]
We provide a visual inspection of how the structure of the relation embedding space changes when rules are imposed.,5.4 Visualizing Relation Embeddings,[0],[0]
"We select all relations involved in the WordNet rules, and gather them as columns in a single matrix, sorted by increasing `1 norm (values in the 100 dimensions are similarly sorted).",5.4 Visualizing Relation Embeddings,[0],[0]
Figures 2a and 2b show the difference between model F (without injected rules) and FSL (with rules).,5.4 Visualizing Relation Embeddings,[0],[0]
"The values of the embeddings in model FSL are more polarized, i.e., we observe stronger negative or positive components than for model F. Furthermore, FSL also reveals a clearer difference between the leftmost (mostly negative, more specific) and right-most (predominantly positive, more general) embeddings (i.e., a clearer separation between positive and negative values in the plot), which results from imposing the order relation in eq.",5.4 Visualizing Relation Embeddings,[0],[0]
(11) when injecting implications.,5.4 Visualizing Relation Embeddings,[0],[0]
"In order to get an idea of the time efficiency of injecting rules, we measure the time per epoch when restricting the program execution to a single 2.4GHz CPU core.",5.5 Efficiency of Lifted Injection of Rules,[0],[0]
"We measure on average 6.33s per epoch without rules (model FS), against 6.76s and 6.97s
when injecting the 36 high-quality WordNet rules and the unfiltered 427 rules (model FSL), respectively.",5.5 Efficiency of Lifted Injection of Rules,[0],[0]
"Increasing the amount of injected rules from 36 to 427 leads to an increase of only 3% in computation time, even though in our setup all rule losses are used in every training batch.",5.5 Efficiency of Lifted Injection of Rules,[0],[0]
This confirms the high efficiency of our lifted rule injection method.,5.5 Efficiency of Lifted Injection of Rules,[0],[0]
"In order to demonstrate that injecting implications conserves their asymmetric nature, we perform the following experiment.",5.6 Asymmetric Character of Implications,[0],[0]
"After incorporating highquality Wordnet rules rp ⇒ rq into model FSL we select all of the tuples tp that occur with relation rp in a training fact 〈rp, tp〉.",5.6 Asymmetric Character of Implications,[0],[0]
"Matching these with relation rq should result in high values for the scores r>q tp, if the implication holds.",5.6 Asymmetric Character of Implications,[0],[0]
"If however the tuples tq are selected from the training facts 〈rq, tq〉, and matched with relation rp, the scores r>p tq should be much lower if the inverse implication does not hold (in other words, if rq and rp are not equivalent).",5.6 Asymmetric Character of Implications,[0],[0]
"Table 3 lists the averaged results for 5 example rules, and the average over all relations in WordNet rules, both for the case with injected rules (model FSL), and without rules (model FS).",5.6 Asymmetric Character of Implications,[0],[0]
"For easier comparison, the scores are mapped to the unit interval via the sigmoid function.",5.6 Asymmetric Character of Implications,[0],[0]
"This quantity σ(r>t) is often interpreted as the probability that the corresponding fact holds (Riedel et al., 2013), but because of the BPR-based training, only differences between scores play a role here.",5.6 Asymmetric Character of Implications,[0],[0]
"After injecting rules, the average scores of facts inferred by these rules (i.e., column σ(r>q tp) for model FSL) are always higher than for facts (incorrectly) inferred by the inverse rules (column σ(r>p tq) for model FSL).",5.6 Asymmetric Character of Implications,[0],[0]
"In the fourth example, the inverse rule leads to high scores as well (on average 0.79, vs. 0.98 for the actual rule).",5.6 Asymmetric Character of Implications,[0],[0]
"This is due to the fact that the daily and newspaper relations are more or less equivalent, such that the components of rp are not much below those of rq.",5.6 Asymmetric Character of Implications,[0],[0]
"For the last example (the ambassador ⇒ diplomat rule), the asymmetry in the implication is maintained, although the absolute scores are rather low for these two relations.
",5.6 Asymmetric Character of Implications,[0],[0]
The results for model FS reflect how strongly the implications in either direction are latently present in the training data.,5.6 Asymmetric Character of Implications,[0],[0]
"We can only conclude that model FS manages to capture the similarity be-
tween relations, but not the asymmetric character of implications.",5.6 Asymmetric Character of Implications,[0],[0]
"For example, purely based on the training data, it appears to be more likely that the parent relation implies the father relation, than vice versa.",5.6 Asymmetric Character of Implications,[0],[0]
This again demonstrates the importance and added value of injecting external rules capturing commonsense knowledge.,5.6 Asymmetric Character of Implications,[0],[0]
"We presented a novel, fast approach for incorporating first-order implication rules into distributed representations of relations.",6 Conclusions,[0],[0]
"We termed our approach ‘lifted rule injection’, as it avoids the costly grounding of first-order implication rules and is thus independent of the size of the domain of entities.",6 Conclusions,[0],[0]
"By construction, these rules are satisfied for any observed or unobserved fact.",6 Conclusions,[0],[0]
The presented approach requires a restriction on the entity-tuple embedding space.,6 Conclusions,[0],[0]
"However, experiments on a real-world dataset show that this does not impair the expressiveness of the learned representations.",6 Conclusions,[0],[0]
"On the contrary, it appears to have a beneficial regularization effect.
",6 Conclusions,[0],[0]
"By incorporating rules generated from WordNet hypernyms, our model improved over a matrix factorization baseline for knowledge base completion.",6 Conclusions,[0],[0]
"Especially for domains where annotation is costly and only small amounts of training facts are available, our approach provides a way to leverage external knowledge sources for inferring facts.
",6 Conclusions,[0],[0]
"In future work, we want to extend the proposed ideas beyond implications towards general firstorder logic rules.",6 Conclusions,[0],[0]
"We believe that supporting conjunctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion.",6 Conclusions,[0],[0]
"Furthermore, we want to integrate these ideas into neural methods beyond matrix factorization approaches.",6 Conclusions,[0],[0]
"This work was supported by the Research Foundation - Flanders (FWO), Ghent University - iMinds, Microsoft Research through its PhD Scholarship Programme, an Allen Distinguished Investigator Award, and a Marie Curie Career Integration Award.",Acknowledgments,[0],[0]
Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks.,abstractText,[0],[0]
"Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models.",abstractText,[0],[0]
A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules.,abstractText,[0],[0]
"However, propositionalization does not scale beyond domains with only few entities and rules.",abstractText,[0],[0]
In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction.,abstractText,[0],[0]
We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet.,abstractText,[0],[0]
"Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization.",abstractText,[0],[0]
"By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.",abstractText,[0],[0]
Lifted Rule Injection for Relation Embeddings,title,[0],[0]
"The problem of estimating heterogeneous (individualized) causal effects of a treatment from observational data is central in many application domains, including public health and drug development (Foster et al., 2011), computational
1University of California, Los Angeles, USA 2University of Oxford, Oxford, UK 3Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Ahmed M. Alaa <ahmedmalaa@ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"advertising (Bottou et al., 2013), and social sciences (Xie et al., 2012).",1. Introduction,[0],[0]
"The increasing availability of observational data in all these domains has encouraged the development of various machine learning algorithms tailored for inferring treatment effects using observational data (e.g. (Li & Fu, 2017; Wager & Athey, 2017; Shalit et al., 2017; Alaa & van der Schaar, 2017)).",1. Introduction,[0],[0]
"Due to the peculiarity of the treatment effect estimation problem, these algorithms needed to address various modeling aspects that are foreign to standard supervised learning setups; such aspects include ways to handle sample selection bias (Heckman, 1977), and ways to model treated and untreated data points.",1. Introduction,[0],[0]
"Despite a variety of recent algorithmic approaches, principled guidelines for model design are lacking.
",1. Introduction,[0],[0]
"In this paper, we identify guiding principles for designing practical treatment effect estimation algorithms in the context of Bayesian nonparametric inference, and propose one an algorithm that follows these guidelines.",1. Introduction,[0],[0]
"We set these guidelines by characterizing the fundamental limits of estimating treatment effects, and studying the impact of various common modeling choices on the achievability of those limits.",1. Introduction,[0],[0]
"In what follows, we provide a brief technical background for the treatment effect estimation problem, along with a summary of our contributions.",1. Introduction,[0],[0]
"Our analysis hinges on the Rubin-Neyman potential outcomes model (Rubin, 2005).",1.1. Background and Summary of Contributions,[0],[0]
"That is, we consider an observational dataset with a population of subjects, where each subject i is endowed with a d-dimensional feature Xi ∈ X .",1.1. Background and Summary of Contributions,[0],[0]
We assume that X =,1.1. Background and Summary of Contributions,[0],[0]
"[0, 1]d, but most of our results hold for general compact metric spaces (bounded, closed sets in Rd).",1.1. Background and Summary of Contributions,[0],[0]
"A treatment assignment indicator Wi ∈ {0, 1} is associated with subject i; Wi = 1 if the treatment under study was applied to subject i, and Wi = 0 otherwise.",1.1. Background and Summary of Contributions,[0],[0]
"Subject i’s responses with and without the treatment (the potential outcomes) are denoted as Y (1)i and Y (0)
i , respectively.",1.1. Background and Summary of Contributions,[0],[0]
"Treatments are assigned to subjects according to an underlying policy that depends on the subjects’ features, i.e. Wi ⊥̸⊥ Xi.",1.1. Background and Summary of Contributions,[0],[0]
This dependence is quantified via the conditional distribution p(x) =,1.1. Background and Summary of Contributions,[0],[0]
"P(Wi = 1|Xi = x), also known as the propensity score of subject i (Rosenbaum & Rubin,
1984).",1.1. Background and Summary of Contributions,[0],[0]
"The response Y (Wi)i is the “factual outcome” which we observe in the data, whereas Y (1 − Wi)i is the unrealized “counterfactual outcome” (Bottou et al., 2013).",1.1. Background and Summary of Contributions,[0],[0]
An observational dataset Dn comprises n samples of the form:,1.1. Background and Summary of Contributions,[0],[0]
"Dn = {Xi,Wi, Y (Wi)i }ni=1 (1)",1.1. Background and Summary of Contributions,[0],[0]
"The causal effect of the treatment on subject i with a feature Xi = x is characterized through the conditional average treatment effect (CATE) function T (x), which is defined as the expected difference between the two potential outcomes (Rubin, 2005), i.e. T (x) = E[Y (1)i",1.1. Background and Summary of Contributions,[0],[0]
− Y (0)i |Xi = x ] (2) Our goal is to identify a set of guiding principles for building estimators of the CATE T (x) using samples from Dn.,1.1. Background and Summary of Contributions,[0],[0]
"Throughout the paper, we will assume that the joint density dP(Xi,Wi, Y (0)i , Y (1)
i ) supports the assumptions of unconfoundedness and overlap, which are necessary for causal identifiability and consistency.",1.1. Background and Summary of Contributions,[0],[0]
"Unconfoundedness requires that (Y (0)i , Y (1)
i )",1.1. Background and Summary of Contributions,[0],[0]
"⊥⊥ Wi |Xi, whereas overlap requires that 0 < p(x) < 1",1.1. Background and Summary of Contributions,[0],[0]
"(Rosenbaum & Rubin, 1984).",1.1. Background and Summary of Contributions,[0],[0]
"Selection bias occurs in Dn since the distribution of the treated/control subjects does not match that of the overall population.
",1.1. Background and Summary of Contributions,[0],[0]
"In order to come up with principled guidelines for building estimators of T (x), we characterize the fundamental (information-theoretic) limits of estimating the CATE using samples from Dn, and identify the modeling choices that would allow achieving those limits.",1.1. Background and Summary of Contributions,[0],[0]
"To this end, in Section 3 we tackle the following question: what are the fundamental limits of CATE estimation?",1.1. Background and Summary of Contributions,[0],[0]
We answer this question by deriving the optimal minimax rate for estimating T (x) using Dn.,1.1. Background and Summary of Contributions,[0],[0]
"Interestingly, it turns out that the optimal rate does not depend on selection bias, but rather on the smoothness and sparsity of the more “complex” of the functions E[Y (0)i |Xi = x ] and E[Y (1) i |Xi = x ].",1.1. Background and Summary of Contributions,[0],[0]
"We focus our analysis on Bayesian nonparametric methods, since they have the appealing properties of being robust to misspecification and are accessible for theoretical analysis.
",1.1. Background and Summary of Contributions,[0],[0]
Our analysis reveals that the relative importance of the different modeling aspects vary with the sample size.,1.1. Background and Summary of Contributions,[0],[0]
"In particular, in the large-sample regime, selection bias does not pose a serious problem, and the model’s performance would be mainly determined by its structure, i.e. the way the outcomes Y (0)i and Y (1) i are modeled, and the impact of that on variable selection and hyperparameter tuning.",1.1. Background and Summary of Contributions,[0],[0]
"On the contrary, selection bias can seriously harm a model’s generalization performance in small-sample regimes.",1.1. Background and Summary of Contributions,[0],[0]
"A good model should then be carefully designed so that it operates well in both regimes by possessing the right model structure that would allow learning at a fast rate, and the right model selection (hyperparameter optimization) scheme that would account for selection bias.
",1.1. Background and Summary of Contributions,[0],[0]
"In Section 4, we build a practical CATE estimation algorithm guided by the results of the analyses in Section 3.",1.1. Background and Summary of Contributions,[0],[0]
We model the outcomes Y (0)i and Y (1) i using a Gaussian process with a non-stationary kernel that captures the different relevant variables and different levels of smoothness of the functions E[Y (0)i |Xi = x ] and E[Y (1),1.1. Background and Summary of Contributions,[0],[0]
i |Xi = x ].,1.1. Background and Summary of Contributions,[0],[0]
We prove that this model structure can achieve the optimal rate of CATE estimation when tuned with the right hyperparameters.,1.1. Background and Summary of Contributions,[0],[0]
"We also propose a doubly-robust hyperparameter optimization scheme that accounts for selection bias in smallsample regimes, without hindering the model’s minimaxoptimality in the large sample limit.",1.1. Background and Summary of Contributions,[0],[0]
We show that our algorithm outperforms state-of-the-art methods using a wellknown semi-synthetic simulation setup.,1.1. Background and Summary of Contributions,[0],[0]
"Very few works have attempted to characterize the limits of CATE estimation, or study the impact of different modeling choices on the CATE estimation performance in a principled manner.",1.2. Related Work,[0],[0]
"(Alaa & van der Schaar, 2018) characterized the asymptotic “information rates” for different CATE estimators, but provided no clear guidelines on practical model design or an analysis of the impact of sample selection bias.",1.2. Related Work,[0],[0]
"The study in (Künzel et al., 2017) was rather empirical in nature, comparing the performance of different regression structures for the potential outcomes while ignoring selection bias.",1.2. Related Work,[0],[0]
"A similar study, but focusing only on random forest models, was conducted in (Lu et al., 2017).
",1.2. Related Work,[0],[0]
"Most of the previous works have been algorithmic in nature, focusing mainly on devising algorithms that correct for selection bias (e.g. (Johansson et al., 2016; Yoon et al., 2018; Wager & Athey, 2017; Li & Fu, 2017)).",1.2. Related Work,[0],[0]
"Some of these works cast the selection bias problem as a problem of covariate shift (Sugiyama et al., 2007), and use techniques from representation learning to learn feature maps that balance the biased data (e.g. (Li & Fu, 2017; Shalit et al., 2017; Johansson et al., 2016)).",1.2. Related Work,[0],[0]
"However, those works report much bigger improvements in CATE estimation when changing their model structure (e.g. architecture of a neural network), as compared to the gains attained by only accounting for bias (see the comparisons between the TARnet and BNN models in (Shalit et al., 2017)).",1.2. Related Work,[0],[0]
"Similar observations are reported in (Alaa & van der Schaar, 2017; Atan et al., 2018), where the selection of the model structure seemed to influence the achieved CATE estimation performance even when selection bias is not accounted for.",1.2. Related Work,[0],[0]
"However, none of these works offer a discussion on whether selection bias is actually the main challenge in CATE estimation, or whether the outcomes’ model structure may have a bigger influence on performance.
",1.2. Related Work,[0],[0]
"In contrast to the works above, this paper does not attempt to develop a model by presupposing that particular model-
ing aspects are of greater importance than others, but rather provides a framework for understanding the limits on the achievable performance, and how different modeling aspects influence a model’s chance of achieving those limits.",1.2. Related Work,[0],[0]
"We use our analyses to both reflect on the modeling choices made in the works above, and also devise a novel, principled CATE estimation algorithms that achieves the fundamental performance limits.",1.2. Related Work,[0],[0]
"We consider the following random design regression model for the potential outcomes:
Y (w)i = fw(Xi) +",2.1. Potential Outcomes & Propensity Score,[0],[0]
"εi,w, w ∈ {0, 1}, (3)
where εi,w ∼ N (0, σ2w) is a Gaussian noise variable.",2.1. Potential Outcomes & Propensity Score,[0],[0]
It follows from (2) that the CATE is T (x) = f1(x),2.1. Potential Outcomes & Propensity Score,[0],[0]
− f0(x),2.1. Potential Outcomes & Propensity Score,[0],[0]
.,2.1. Potential Outcomes & Propensity Score,[0],[0]
"The response surfaces f1(x) and f0(x) correspond to the subjects’ responses with and without the treatment.
",2.1. Potential Outcomes & Propensity Score,[0],[0]
We assume that fw(.) :,2.1. Potential Outcomes & Propensity Score,[0],[0]
"X → R, w ∈ {0, 1}, is a totally bounded function that lives in a space of “smooth” or “regular” functions, with an unknown smoothness parameter αw.",2.1. Potential Outcomes & Propensity Score,[0],[0]
"We use Hölder balls for concreteness, although our results extend to other function spaces.",2.1. Potential Outcomes & Propensity Score,[0],[0]
"A function fw(.) lies in the Hölder ball Hαw , with a Hölder exponent",2.1. Potential Outcomes & Propensity Score,[0],[0]
"αw > 0, if and only if it is bounded in sup-norm by a constant C > 0, all its partial derivatives up to order ⌊αw⌋ exist, and all its partial derivatives of order ⌊αw⌋ are Lipschitz with exponent (αw − ⌊αw⌋) and constant C. The Hölder exponents quantify the complexities of f0 and f1, and hence the hardness of estimating T (x) would depend on α0 and α1.",2.1. Potential Outcomes & Propensity Score,[0],[0]
"Nonparametric inference is immune to misspecification of the outcomes’ and propensity models (Kennedy, 2018), and hence we focus on Bayesian nonparametric methods for inferring T (.) on the basis of Dn.",2.2. Bayesian Nonparametric Inference,[0],[0]
"Bayesian inference entails specifying a prior distribution Π over f1(.) and f0(.), i.e.
f0, f1 ∼ Π(φ̄β0 , φ̄β1), (4)
where φ̄βw = {φkβw} ∞ k=1, w ∈ {0, 1}, are complete orthonormal bases (indexed by a parameter βw > 0) with respect to Lebesgue measure in X , fw = ∑ k f̄ k w ·φkβw , and f̄kw = ⟨fw, φkβw⟩.",2.2. Bayesian Nonparametric Inference,[0.9599033475205366],"['Next we define the posteriors Qφi , i = 1, ..., n, and the prior Pθ as factorized Gaussian distributions8, Pθ(w) = d∏ k=1 N ( wk;µP,k, σ 2 P,k ) (9) Qφi(w) = d∏ k=1 N ( wk;µi,k, σ 2 i,k ) (10) where for each task, the posterior parameters vector φi = (µi, ρi) ∈ R2d is composed of the means and log-variances of each weight , µi,k and ρi,k = log σ2P,k, k = 1, ..., d. 9 The shared prior vector θ = (µP , ρP ) ∈ R2d has a similar structure.']"
"Thus, for given bases φ̄β0 and φ̄β1 , Π places a probability distribution on the projections {f̄kw}k.",2.2. Bayesian Nonparametric Inference,[0],[0]
"Potential choices for the basis φ̄βw that would give rise to implementable Bayesian inference algorithms include regular wavelet basis (Zhang, 1997), radial basis for a reproducing kernel Hilbert space (RKHS) (van der Vaart et al., 2008), etc.",2.2. Bayesian Nonparametric Inference,[0],[0]
"In general, the parameter βw would determine the smoothness of the function space spanned by φ̄βw .",2.2. Bayesian Nonparametric Inference,[0],[0]
"To evaluate the predictive accuracy of the Bayesian inference procedure, we analyze the “frequentist” loss of point estimators T̂ (x) induced by the Bayesian posterior dΠn(T (x) | Dn), assuming that Dn is generated based on fixed, true response surfaces f1(x) and f0(x).",2.3. Towards Principled CATE Estimation,[0],[0]
"(This type of analysis is sometimes referred to as the “FrequentistBayes” analysis (Sniekers et al., 2015).)",2.3. Towards Principled CATE Estimation,[0],[0]
"In particular, we quantify the performance of a point estimator T̂ (x) = δ(dΠn(T (x) | Dn)) by its squared-L2(P) error, which was dubbed the precision of estimating heterogeneous effects (PEHE) in (Hill, 2011), and is formally defined as:
ψ(T̂ ) , E ∥",2.3. Towards Principled CATE Estimation,[0],[0]
T̂,2.3. Towards Principled CATE Estimation,[0],[0]
"− T ∥2 L2(P), (5)
where L2(P) is the L2 norm with respect to the feature distribution, i.e. ∥f(x)∥2 L2(P) = ∫ f2(x) dP(X = x).
",2.3. Towards Principled CATE Estimation,[0],[0]
"Not a standard supervised learning problem...
",2.3. Towards Principled CATE Estimation,[0],[0]
"The “fundamental problem of causal inference” is that for every subject i in Dn, we only observe the factual outcome Y (Wi)i , whereas the counterfactual Y (1 − Wi) i remains unknown, which renders empirical evaluation of the PEHE in (5) impossible.",2.3. Towards Principled CATE Estimation,[0],[0]
"Moreover, Dn would generally exhibit sample selection bias (Heckman, 1977), because the treatment assignment mechanism (decided by p(x)) creates a discrepancy between the feature distributions of the treated/control population and the overall population.",2.3. Towards Principled CATE Estimation,[0],[0]
"Thus, standard supervised learning approaches based on empirical risk minimization cannot be used to learn a generalizable model for the CATE from samples in Dn.",2.3. Towards Principled CATE Estimation,[0],[0]
"This gives rise to the following fundamental modeling questions that are peculiar to the CATE estimation problem:
•",2.3. Towards Principled CATE Estimation,[0],[0]
"[Q1]: How should the treatment assignment indicator Wi be incorporated into the learning model?
",2.3. Towards Principled CATE Estimation,[0],[0]
•,2.3. Towards Principled CATE Estimation,[0],[0]
"[Q2]: How should selection bias be handled?
",2.3. Towards Principled CATE Estimation,[0],[0]
"Adequate answers to [Q1] and [Q2] would provide guidelines for selecting the prior Π(φ̄β0 , φ̄β1).",2.3. Towards Principled CATE Estimation,[0],[0]
"Addressing the modeling questions above requires a profound understanding of the fundamental limits of CATE estimation, in addition to an understanding of the impact of different modeling choices on the achievability of such limits.",2.3. Towards Principled CATE Estimation,[0],[0]
"The next Sections provide principled answers to [Q1] and [Q2] by addressing the following, more fundamental questions:
Section 3: What are the limits on the performance that can be achieved by any estimator of the CATE?
Section 4: How can we build practical algorithms that can achieve the performance limits?",2.3. Towards Principled CATE Estimation,[0],[0]
"In this Section, we establish an information-theoretic limit on the performance of any CATE estimator.",3. Fundamental Limits of CATE Estimation,[0],[0]
"In what follows, we use the standard Bachmann-Landau order notation, and write a∨ b = max{a, b}, a∧ b = min{a, b}.",3. Fundamental Limits of CATE Estimation,[0],[0]
The notation a .,3. Fundamental Limits of CATE Estimation,[0],[0]
"b means that a ≤ Cb for a universal constant C, and ≍ denotes asymptotic equivalence.",3. Fundamental Limits of CATE Estimation,[0],[0]
"The “hardness” of a nonparametric estimation problem is typically characterized by its minimax risk (Stone, 1982), i.e. the minimum worst case risk achieved by any estimator when the estimand is known to live in a given function space (Yang et al., 2015).",3.1. Optimal Minimax Rates,[0],[0]
"In the following Theorem, we establish the optimal minimax rate for the PEHE risk in terms of the complexity of the response surfaces f0 and f1.
Theorem 1.",3.1. Optimal Minimax Rates,[0],[0]
Suppose that X =,3.1. Optimal Minimax Rates,[0],[0]
"[0, 1]d, and that fw depends on a subset of dw features with dw ≤ min{n, d} for w ∈ {0, 1}.",3.1. Optimal Minimax Rates,[0],[0]
"If f0 ∈ Hα0 and f1 ∈ Hα1 , then the optimal minimax rate is:
inf T̂ sup f0,f1
ψ(T̂ ) ≍",3.1. Optimal Minimax Rates,[0],[0]
n− ( 1+ 1 2 ( d0 α0 ∨ d1 α1 )),3.1. Optimal Minimax Rates,[0],[0]
"−1︸ ︷︷ ︸ CATE estimation
∨ log ( dd0+d1
d d0 0 d d1 1
) 1 n
.︸",3.1. Optimal Minimax Rates,[0],[0]
︷︷ ︸,3.1. Optimal Minimax Rates,[0],[0]
"Variable selection
The above holds for any p(.)",3.1. Optimal Minimax Rates,[0],[0]
"∈ Hαp , αp > 0.
",3.1. Optimal Minimax Rates,[0],[0]
"In Theorem 1, the supremum is taken over αw-Hölder balls (w ∈ {0, 1}), whereas the infimum is taken over all possible Bayesian estimators.",3.1. Optimal Minimax Rates,[0],[0]
The minimax rate in Theorem 1 corresponds to the fastest rate by which any (Bayesian) estimator T̂ (.) can approximate the CATE function T (.).,3.1. Optimal Minimax Rates,[0],[0]
"The proof of Theorem 1 (provided in the supplement) uses information-theoretic techniques based on Fano’s method to derive algorithm-independent estimation rates (Yang & Barron, 1999).",3.1. Optimal Minimax Rates,[0],[0]
"In the following set of remarks, we revisit [Q1] and [Q2] in the light of the results of Theorem 1.
",3.1. Optimal Minimax Rates,[0],[0]
"How can Theorem 1 help us address [Q1] & [Q2]?
◃",3.1. Optimal Minimax Rates,[0],[0]
"Remark 1 (Smoothness & sparsity)
",3.1. Optimal Minimax Rates,[0],[0]
"Theorem 1 says that estimating CATE is as hard as nonparametric regression for functions with additive sparsity (Raskutti et al., 2009; Yang et al., 2015).",3.1. Optimal Minimax Rates,[0],[0]
"The minimax rate in Theorem 1 decomposes into a term reflecting the complexity of CATE estimation under correct variable selection for f0 and f1, and a term reflecting the complexity of variable selection.",3.1. Optimal Minimax Rates,[0],[0]
"Variable selection complexity remains small as long as log(d) = Θ(nζ), for some ζ ∈ (0, 1), and approaches the parametric rates as ζ → 0.",3.1. Optimal Minimax Rates,[0],[0]
"The minimax rate will generally be dominated by the complexity of CATE estimation, and will approach the parametric rates only for very smooth response surfaces with small number of relevant dimensions, i.e. d0α0 ∨ d1 α1 → 0.
",3.1. Optimal Minimax Rates,[0],[0]
"The main takeaway from Theorem 1 is that the CATE learning rate is determined by the more “complex” of the surfaces f0 and f1, where complexity is quantified by the sparsity-to-smoothness ratio dw/αw for w ∈ {0, 1}.",3.1. Optimal Minimax Rates,[0],[0]
"Thus, a model would achieve the optimal CATE learning rate only if it selects the correct relevant variables for f0 and f1, and tunes its “hyperparameters” (i.e. smoothness of the prior) to cope with a complexity of d0α0 ∨ d1 α1 .",3.1. Optimal Minimax Rates,[0],[0]
"When d0α0 and d1 α1
are very different (e.g. f0 and f1 have different relevant features), rate-optimal estimation is possible only if the model incorporates such differences in Π(φ̄β0 , φ̄β1).
",3.1. Optimal Minimax Rates,[0],[0]
The discussion above provides a concrete answer to [Q1]: the treatment assignment variablew should be incorporated into the model in such a way that it encodes the different relevant dimensions and smoothness levels of f0 and f1 in the bases φ̄β0 and φ̄β1 .,3.1. Optimal Minimax Rates,[0],[0]
(The simplest way to achieve this is to use two separate models for f0 and f1.),3.1. Optimal Minimax Rates,[0],[0]
"This is not fulfilled by many of the previous models that built a single regression function of the from f : X ×{0, 1} → R, and estimated the CATE as T̂ (x) = f(x, 1)−f(x, 0) (Hill, 2011; Johansson et al., 2016; Powers et al., 2017).",3.1. Optimal Minimax Rates,[0],[0]
"This is because such models enforced the smoothness of the prior along all features to be the same for w = 0 and w = 1.
◃",3.1. Optimal Minimax Rates,[0],[0]
"Remark 2 (Selection bias)
Theorem 1 gives a rather surprising answer to [Q2]: the optimal learning rate is oblivious to selection bias.",3.1. Optimal Minimax Rates,[0],[0]
"Such a finding is consistent with previous results on nonparametric kernel density estimation under selection bias (Borrajo et al., 2017), and parametric Bayesian inference under covariate shift (Shimodaira, 2000; Sugiyama & Storkey, 2007).",3.1. Optimal Minimax Rates,[0],[0]
"It shows that many of the recent works have missed the target; the works in (Johansson et al., 2016; Shalit et al., 2017; Alaa & van der Schaar, 2017) cast the problem of CATE estimation as one of covariate shift that results from selection bias.",3.1. Optimal Minimax Rates,[0],[0]
"However, Theorem 1 says that selection bias is not a problem when we have a sufficiently large amount of data.",3.1. Optimal Minimax Rates,[0],[0]
"This is because selection bias is inherently a misspecification problem, and hence its impact on nonparametric inference is washed away in large-sample regimes.
",3.1. Optimal Minimax Rates,[0],[0]
Remarks 1 and 2 posit an explanation for various recurrent (empirical) findings reported in previous literature.,3.1. Optimal Minimax Rates,[0],[0]
"For instance, (Hahn et al., 2017) found that separate modeling of f0 and f1 via Bayesian additive regression trees (BART) outperforms the well-known single-surface BART model developed in (Hill, 2011).",3.1. Optimal Minimax Rates,[0],[0]
"Similar findings were reported for models based on Gaussian processes (Alaa & van der Schaar, 2017), and models based on deep neural networks (Shalit et al., 2017).",3.1. Optimal Minimax Rates,[0],[0]
All such findings can be explained in the light of Remark 1.,3.1. Optimal Minimax Rates,[0],[0]
"On the other hand, Remark 2 may provide an explanation as to why the “TARnet” model in (Shalit et al., 2017), which models f0 and f1 using separate neural networks and does not account for selection
bias, outperformed the “BNN” model in (Johansson et al., 2016), which regularizes for selection bias but fits a singleoutput network for f0 and f1.",3.1. Optimal Minimax Rates,[0],[0]
"Theorem 1 shows that selection bias does not hinder the optimal minimax rates, and that it is only the structural properties of the prior Π(φ̄β0 , φ̄β1) that determine a model’s rate of learning.",3.2. Backing off from “Asymptopia”,[0],[0]
But does the achieved learning rate suffice as a sole criterion for addressing the modeling questions,3.2. Backing off from “Asymptopia”,[0],[0]
[Q1] and [Q2]?,3.2. Backing off from “Asymptopia”,[0],[0]
"The answer is “yes” only if Dn comes from a large observational dataset, in which case the learning rate suffices as a descriptor for the large-sample performance.",3.2. Backing off from “Asymptopia”,[0],[0]
"However, if Dn is small, which is typical in posthoc analyses of clinical trials (Foster et al., 2011), then one should make the design choices that would optimize the small-sample performance.",3.2. Backing off from “Asymptopia”,[0],[0]
"In order to give a more complete picture of the performance in large and small-sample regimes, we derive the following bound on the PEHE:
ψ(T̂ ) ≤ C̄ ·",3.2. Backing off from “Asymptopia”,[0],[0]
exp(D2(Q0 ∥Q)) · ∥f0 − f̂0∥2L2(P0) + C̄,3.2. Backing off from “Asymptopia”,[0],[0]
"· exp(D2(Q1 ∥Q)︸ ︷︷ ︸
Réyni Divergence ) · ∥f1",3.2. Backing off from “Asymptopia”,[0],[0]
"− f̂1∥2L2(P1)︸ ︷︷ ︸ Supervised learning loss , (6)
for some C̄",3.2. Backing off from “Asymptopia”,[0],[0]
"> 0, where L2(Pw), for w ∈ {0, 1}, is the L2 norm with respect to dP(X = x |W = w), Q = dP(X = x), Qw = dP(X = x |W = w), and Dm(p ∥ q) is the mth order Réyni divergence.",3.2. Backing off from “Asymptopia”,[0],[0]
"The bound in (6) holds for all n > 0, and is tight (refer to the supplement); it shows that the PEHE is a weighted linear combination of the mean squared losses for the two underlying supervised problems of learning f0 and f1 with no covariate shift, where the weights are determined by the extent of the mismatch between the distributions of the treated and control populations, quantified by the Réyni divergence measure.",3.2. Backing off from “Asymptopia”,[0],[0]
"If Dn is a dataset obtained from a randomized controlled trial (Q = Q0 = Q1), then we have D2(Q0 ∥Q) = D2(Q1 ∥Q) = 0, and the bound boils down to a sum of two supervised learning losses, i.e. ψ(T̂ ) ≤ C̄ · ∥f0 − f̂0∥2L2(P) + C̄ · ∥f1 − f̂1∥ 2 L2(P).
",3.2. Backing off from “Asymptopia”,[0],[0]
Since the minimax rate for standard nonparametric regression is ∥fw − f̂w∥22 ≍,3.2. Backing off from “Asymptopia”,[0],[0]
"Cw · n −2αw 2αw+dw (Stone, 1982), when d0/α0 >",3.2. Backing off from “Asymptopia”,[0],[0]
"> d1/α1, the first-order Taylor approximation for the logarithm of the PEHE in (6) is given by:
log(ψ(T̂ ))",3.2. Backing off from “Asymptopia”,[0],[0]
"≈D2(Q0∥Q)︸ ︷︷ ︸ Selection
bias
+ log(C0)︸ ︷︷ ︸ Bias
correction
− 2α0 2α0 + d0︸ ︷︷ ︸
Learning rate
log(n)
+O ( n −2α1 2α1+d1 + 2α0 2α0+d0 ) .",3.2. Backing off from “Asymptopia”,[0],[0]
"(7)
That is, when viewed on a log-log scale, the behavior of the PEHE versus the number of samples can be described
as follows. log(PEHE) is a linear function of log(n).",3.2. Backing off from “Asymptopia”,[0],[0]
"Selection bias adds a constant offset to log(PEHE), but does not affect its slope, which harms the performance only in the small-sample regime.",3.2. Backing off from “Asymptopia”,[0],[0]
"In the large-sample regime, the slope of log(PEHE), which depends solely on the smoothness and sparsity of the response surfaces, dominates the performance, and selection bias becomes less of a problem.",3.2. Backing off from “Asymptopia”,[0],[0]
Figure 1 depicts the PEHE in (7) on a log-log scale.,3.2. Backing off from “Asymptopia”,[0],[0]
"In this Section, we build on the analyses conducted in Section 3 to design a practical algorithm for CATE estimation.",4. CATE Estimation using Non-Stationary Gaussian Process Regression,[0],[0]
"We specify the prior Π(φ̄β0 , φ̄β1) as a Gaussian process (GP) over functions of the form g :",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"X × {0, 1} → R, with a kernel Kβ , and a hyperparameter set β as follows:
g ∼ GP (0,Kβ(z, z′)) , (8)
where z = (x,w) ∈ X × {0, 1}, and fw(x) = g(x,w).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"The kernel Kβ specifies the bases φ̄β0 and φ̄β1 through its induced canonical feature map Kβ(., z) (Rasmussen & Williams, 2006; Alvarez et al., 2012).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"As pointed out in remark 1, the treatment assignment variable w should encode the different relevant dimensions and smoothness levels of f0 and f1.",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Thus, we model Kβ as a non-stationary kernel that depends explicitly on w as follows:
Kβ(z, z ′)= Γ(w,w′) ·",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"kTβ (x, x′), kβ(x, x ′)=",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"[kβ0(x, x ′), kβ1(x, x ′), kβ0(x, x ′) +",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"kβ1(x, x ′)],
Γ(w,w′)=",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"[Γ0(w,w ′), Γ1(w,w ′), 1− Γ0(w,w′)− Γ1(w,w′)],
where Γ0(w,w′) = (1− w)(1− w′), Γ1(w,w′) = w · w′, and kβw(x, x ′) is a Matérn kernel with a length-scale parameter
βw, for w ∈ {0, 1}.",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"The kernel defined above ensures that any covariance matrix induced by points in X × {0, 1} is positive definite.",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Variable selection is implemented by using the automatic relevance determination version of the Matérn kernel (Rasmussen & Williams, 2006).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"The nonstationarity of Kβ allows setting different length-scales and relevant variables for the marginal priors on f0 and f1 while sharing data between the two surfaces, i.e.
Kβ((x,w), (x ′, w))= kβw (x, x ′), w ∈ {0, 1}, Kβ((x,w), (x ′, w′))= kβ0(x, x ′) +",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"kβ1(x, x ′), w ̸=",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"w′. (9)
That is, all draws from the prior give Matérn sample paths with different smoothness levels (β0 and β1) for f0 and f1, respectively, and the correlations between the paths are captured via the kernel mixture kβ0(x, x ′) +",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"kβ1(x, x ′).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
Note that draws from a Matérn prior with length-scale β are almost surely β̄-Hölder for all β̄ ≤,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
β,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"(Vaart & Zanten, 2011).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Thus, GP(0,Kβ) specifies a βw-Hölder ball as an a priori regularity class for response surface fw, w ∈ {0, 1}.
",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"In the following Theorem, we show that point estimators induced by the prior GP(0,Kβ) can achieve the optimal minimax rate in Theorem 1.
",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
Theorem 2.,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Suppose that the dw relevant features for fw are known a priori for w ∈ {0, 1}.",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"If f0 ∈ Hα0 , f1 ∈ Hα1 , Π = GP(0,Kβ), and T̂ = EΠ [T | Dn ], then we have that
ψ(T̂ ) .",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"n− 2(α0∧β0) 2β0+d0 ∨ n− 2(α1∧β1) 2β1+d1
whenever min{α0, α1, β0, β1} ≥ d/2.
",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Note that posterior consistency holds for all combinations of (α0, α1, β0, β1) since the support of the Matérn prior is the space of bounded continuous functions1.",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"The bound in Theorem 2 can be shown to be tight using the results in (Castillo, 2008).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Theorem 2 says that the posterior induced by the prior GP(0,Kβ) contracts around the true CATE function at the optimal rate given in Theorem 1 provided that the following matching condition is met:
βv = αv
αv d1−v dv ≤ β1−v ≤",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"α1−v + α1−v ·dv2αv − d1−v 2 , (10)
",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
where v = 1,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"if d1/α1 > d0/α0, and v = 0 otherwise",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
.,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
The condition in (10) implies that achieving the optimal rate (steepest slope in Figure 1) via the non-stationary GP prior in Section 4.1 is only a matter of hyperparameter tuning: the smoothness of the prior needs to match the smoothness of the “more complex” of the two response surfaces.,4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"Note that Theorem 2 implies that we do not need to handle selection bias in order to achieve the optimal rate, which is consistent with the earlier discussion in remark 2.
",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
"1This is because the RKHS associated with the prior lies dense in the space of bounded continuous functions (van der Vaart & van Zanten, 2008; van der Vaart et al., 2008).",4.1. Non-Stationary Gaussian Process Priors,[0],[0]
Theorem 2 says that the optimal minimax rate for CATE estimation can be achieved by satisfying the smoothness matching condition in (10).,4.2. Doubly-Robust Hyperparameters,[0],[0]
"However, in practice, the smoothness levels of the true response functions are unknown and need to be learned from the data.",4.2. Doubly-Robust Hyperparameters,[0],[0]
"Moreover, since selection bias is impactful in small-sample regimes, ignoring it may lead to a poor generalization performance when the size of Dn is small.",4.2. Doubly-Robust Hyperparameters,[0],[0]
"In this Section, we propose a hyperparameter optimization algorithm that accounts for selection bias while ensuring minimax-optimality in the large-sample limit.
",4.2. Doubly-Robust Hyperparameters,[0],[0]
"Previous works tend to adjust for selection bias “mechanically” using variants of importance sampling approaches based on inverse-propensity-weighting (IPW) (Sugiyama et al., 2007; Shimodaira, 2000), and kernel mean matching (Huang et al., 2007), or by learning a “balanced representation” of treated and control populations (Li & Fu, 2017).",4.2. Doubly-Robust Hyperparameters,[0],[0]
"We do not attempt to explicitly adjust for selection bias using ad-hoc approaches, and rather seek the “informationally optimal” estimator of the PEHE.",4.2. Doubly-Robust Hyperparameters,[0],[0]
"That is, we seek the most efficient (unbiased) estimator ψ̂∗(T̂ ) of ψ(T̂ ), which satisfies an analog of the Cramér-Rao bound (information-inequality) in parametric estimation, i.e. Var[ψ̂∗(T̂ )] ≤ Var[ψ̂(T̂ )], for any estimator ψ̂(T̂ ).
",4.2. Doubly-Robust Hyperparameters,[0],[0]
"Classical Cramér-Rao bounds do not apply to estimators of the form ψ̂∗(T̂ ), since such estimators are functionals of nonparametric objects.",4.2. Doubly-Robust Hyperparameters,[0],[0]
"There are, however, analogous information inequalities for nonparametric estimation, including Bhattacharyya’s variance bound (Bhattacharyya, 1946), and its generalization due to Bickel (Bickel et al., 1998).",4.2. Doubly-Robust Hyperparameters,[0],[0]
"We proceed by realizing that the PEHE ψ(T̂ ) is simply a functional that belongs to the doubly-robust class of functionals analyzed by Robins in (Robins et al., 2008).",4.2. Doubly-Robust Hyperparameters,[0],[0]
"Thus, one can construct the “most” efficient estimator of ψ(T̂ ) using the most efficient influence function of ψ(T̂ ) as follows (Robins et al., 2008; Robins, 2004):
ψ̂∗(T̂ ) =",4.2. Doubly-Robust Hyperparameters,[0],[0]
"∑n
i=1
( Y
(Wi) i −(Wi−p(Xi))·T̂ (Xi)
p(Xi)·(1−p(Xi))
)2 .
",4.2. Doubly-Robust Hyperparameters,[0],[0]
"The derivation of the estimator above can be found in Theorem 9 in (Robins, 2004) and Section 5 in (Robins et al., 2008).",4.2. Doubly-Robust Hyperparameters,[0],[0]
"When the propensity function p(.) is known, this estimator approximate the PEHE at its optimal minimax rate.",4.2. Doubly-Robust Hyperparameters,[0],[0]
We estimate p(.) via standard kernel density estimation methods.,4.2. Doubly-Robust Hyperparameters,[0],[0]
"It can be easily shown using the results in (Dudoit & van der Laan, 2005) that when using the estimator above to tune the GP hyperparameters via crossvalidation, then the learned length-scale parameters will satisfy the matching condition for minimax optimality.",4.2. Doubly-Robust Hyperparameters,[0],[0]
"In this Section, we check the validity of our analyses using a synthetic simulation setup (Subsection 5.1), and then evaluate the performance of our proposed model using data from a real-world clinical trial with simulated potential outcomes (Subsection 5.2).",5. Experiments,[0],[0]
We will use the acronym NSGP to refer to the non-stationary GP model proposed in Section 4.,5. Experiments,[0],[0]
Let X =,5.1.1. SYNTHETIC MODEL,[0],[0]
"[0, 1], and define a κ-fold integrated Brownian motion Bκ, κ ∈ N+, on X as follows:
Bκ(x)",5.1.1. SYNTHETIC MODEL,[0],[0]
= ∫ x 0 ∫ xκ 0 · · · ∫,5.1.1. SYNTHETIC MODEL,[0],[0]
x2 0,5.1.1. SYNTHETIC MODEL,[0],[0]
"B0(x1) dx1 dx2 · · · dxxκ ,
where B0(.) is a standard Brownian motion (Wiener process).",5.1.1. SYNTHETIC MODEL,[0],[0]
"Sample paths of B0 are almost surely Hölder regular with exponent 1
2 (Karatzas & Shreve, 2012).",5.1.1. SYNTHETIC MODEL,[0],[0]
"Since
B0(x) is almost surely non-differentiable everywhere in X , then sample paths of Bκ(x) are Hölder with exponent κ+ 1
2 , i.e. Bκ ∈ Hκ+
1 2 with probability 1.",5.1.1. SYNTHETIC MODEL,[0],[0]
"Therefore, when
the true response surfaces are κ-fold integrated Brownian paths, the optimality and achievability results in Theorems 1 and 2 should hold.",5.1.1. SYNTHETIC MODEL,[0],[0]
"To this end, we simulate the true response surfaces f0 ∈ Hα0 and f1 ∈ Hα1 as f0 ∼ Bα0− 12 , and f1 ∼ Bα1− 12 , where we set α0 = 2.5 and α1 = 5.5.
",5.1.1. SYNTHETIC MODEL,[0],[0]
The propensity score is modeled as a parametrized logistic function p(x |η) =,5.1.1. SYNTHETIC MODEL,[0],[0]
"(1 + e−η (x− 12 ))−1, where η ∈ R is a parameter that determines the severity of selection bias.",5.1.1. SYNTHETIC MODEL,[0],[0]
"For a pair of fixed Brownian paths f0 and f1, synthetic observational samples (Xi,Wi, Y (Wi)i )i are generated as follows: Xi ∼ Uniform[0,1], Wi ∼ Bernoulli(p(x |η)), and Y
(Wi) i ∼ fWi +N (0, σ 2), where σ2 = 0.1.",5.1.1. SYNTHETIC MODEL,[0],[0]
"Using the setup in Section 5.1.1, we conducted the following Monte Carlo simulations to verify our theoretical findings and highlight the merits of our NSGP model.
",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"• Verifying Theorems 1 and 2: In order to check the validity of the results of Theorems 1 and 2, we use a NSGP Matérn prior GP(0,Kβ), with length-scale parameters β0 and β1 that are matched exactly with the regularities of the Brownian paths f0 and f1 (i.e. β0 = 2.5 and β1 = 5.5).",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"According to Theorem 1, the optimal rate for estimating the CATE T = f1 − f0 is n −5 6 , and from Theorem 2, the NSGP with β0 = 2.5 and β1 = 5.5 should achieve that rate.
",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
Figure 2a provides a scatter-plot for the PEHE achieved by the NSGP with respect to the number of samples on a loglog scale for different settings of η.,5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
We fit a linear regression model that describes the PEHE behavior in the log-log scale.,5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"We found the slope of the linear fit to be 0.8437, which is very close2 to the slope of 56 ≈ 0.833 predicted by Theorem 1.",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"Moreover, by changing the magnitude of η from 0 to 12 , the PEHE curve did not exhibit any significant change in its slope, and was only moved upwards by a constant offset.",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"On the contrary, Figure 2b shows the PEHE behavior when the NSGP prior is over-smoothed (β0 > α0) for η = 0: as predicted by Theorem 2, learning becomes sluggish (slopes become less steep) as β0 increase since the matching condition in (10) does not hold any more.
",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"• NSGPs do not leave any money on the table: In this experiment, we show that the different components of the NSGP model allow it to perform well in small and large sample regimes.",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"We set a strong selection bias of η = 12 and compare the log(PEHE) characteristic of NSGP with a model that uses the same non-stationary kernel as NSGP, and another model that uses a standard stationary kernel, but both models are tuned using marginal likelihood maximization.",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"As we can see in Figure 2c, the model with the non-stationary kernel achieves the same learning rate as NSGP, but exhibits a large offset as it does not account for selection bias, whereas the stationary model fails to learn the smoothness of the rougher Brownian motion since it assigns the same length-scale to both surfaces, and hence it over-smooths the prior, achieving a suboptimal rate.
",5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
2The minor discrepancy is a result of the residual error in the linear regression fit.,5.1.2. EXPERIMENTS AND RESULTS,[0],[0]
"We evaluated the performance of the NSGP model presented in Section 4.1 using the standard semi-synthetic experimental setup designed by Hill in (Hill, 2011).",5.2. The Infant Health and Development Program,[0],[0]
"We report a state-of-the-art result in this setup, and draw connections between our experimental results and our analyses.",5.2. The Infant Health and Development Program,[0],[0]
"The Infant Health and Development Program (IHDP) is an interventional program intended to enhance the health of premature infants (Hill, 2011).",5.2.1. DATA AND BENCHMARKS,[0],[0]
"(Hill, 2011) extracted features and treatment assignments from a real-world clinical trial, and introduced selection bias to the data artificially by removing a subset of the patients.",5.2.1. DATA AND BENCHMARKS,[0],[0]
"The potential outcomes are simulated according to the standard non-linear ”Response Surface B” setting in (Hill, 2011).",5.2.1. DATA AND BENCHMARKS,[0],[0]
"The dataset comprised 747 subjects, with 25 features for each subject.",5.2.1. DATA AND BENCHMARKS,[0],[0]
"Our experimental setup is identical to (Hill, 2011; Johansson et al., 2016; Shalit et al., 2017; Alaa & van der Schaar, 2017): we run 1000 experiments in which we compute the in-sample and out-of-sample √ PEHE (with 80/20 training/testing splits), and report average results in Table 1.
",5.2.1. DATA AND BENCHMARKS,[0],[0]
We compared the performance of NSGP with a total of 23 CATE estimation benchmarks.,5.2.1. DATA AND BENCHMARKS,[0],[0]
"We considered: tree-based algorithms (BART (Hill, 2011), Causal forests (Wager & Athey, 2017), Bayesian causal forests (Hahn et al., 2017)), methods based on deep learning (CFR Wass., CFR MMD, BNN, TARnet (Shalit et al., 2017)), multivariate additive regression splines (MARS) (Powers et al., 2017), Gaussian processes (CMGP) (Alaa & van der Schaar, 2017), nearest neighbor matching (k-NN), propensity score matching (PSM), and targeted maximum likelihood (TMLE) (Porter et al., 2011).",5.2.1. DATA AND BENCHMARKS,[0],[0]
"We also composed a number of T-learners and S-learners as in (Künzel et al., 2017), using a variety of baseline machine learning algorithms (DNN stands for deep networks and OLS stands for linear regression).",5.2.1. DATA AND BENCHMARKS,[0],[0]
"As we can see in Table 1, the proposed NSGP model significantly outperforms all competing benchmarks.",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
The combined benefit of the two components of an NSGP (nonstationary kernel and doubly-robust hyperparameters) is highlighted by comparing its performance to a vanilla SGP (stationary GP) with marginal likelihood maximization.,5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"The gain with respect to such a model is a 2-fold improvement in the PEHE.
",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"Because the IHDP dataset has a “moderate” sample size, both selection bias and learning rate seem to impact the performance.",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"Thus, our method took advantage of having addressed modeling questions [Q1] and [Q2] appropriately by being both “rate-optimal” and “bias-aware”.
",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
The check marks in columns [Q1] and [Q2] designate methods that address modeling questions,5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
[Q1] and [Q2] “appropriately” in the light of the analysis presented in Section 3.,5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"Methods with [Q1] checked use a regression structure with “outcome-specific” hyperparameters, and methods with [Q2] checked adjust for selection bias.",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
A general observation is that the structure of the regression model seem to matter much more than the strategy for handling selection bias.,5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"This is evident from the fact that the TARnet model (does not handle bias but models outcomes separately) significantly outperforms BNN (handles bias but uses a single-surface model (Shalit et al., 2017)), and that all T-learners (models 2 separate response surfaces) outperformed their S-shaped counterparts (models a single surface).",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"For parametric models, such as OLS, the issue of selecting the right regression structure is even more crucial.
",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"To sum up, the results in Table 1 imply that selecting the right regression structure is crucial for rate-optimality in sufficiently large dataset, whereas handling selection bias provides an extra bonus.",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
"In Table 1, methods that address both [Q1] and [Q2] (NSGP, CMGP, and CFR.",5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
Wass and MMD) displayed a superior performance.,5.2.2. RESULTS AND CONCLUSIONS,[0],[0]
The authors would like to thank the reviewers for their helpful comments.,Acknowledgements,[0],[0]
"The research presented in this paper was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS1462245, ECCS1533983, and ECCS1407712).",Acknowledgements,[0],[0]
Estimating heterogeneous treatment effects from observational data is a central problem in many domains.,abstractText,[0],[0]
"Because counterfactual data is inaccessible, the problem differs fundamentally from supervised learning, and entails a more complex set of modeling choices.",abstractText,[0],[0]
"Despite a variety of recently proposed algorithmic solutions, a principled guideline for building estimators of treatment effects using machine learning algorithms is still lacking.",abstractText,[0],[0]
"In this paper, we provide such guidelines by characterizing the fundamental limits of estimating heterogeneous treatment effects, and establishing conditions under which these limits can be achieved.",abstractText,[0],[0]
Our analysis reveals that the relative importance of the different aspects of observational data vary with the sample size.,abstractText,[0],[0]
"For instance, we show that selection bias matters only in small-sample regimes, whereas with a large sample size, the way an algorithm models the control and treated outcomes is what bottlenecks its performance.",abstractText,[0],[0]
"Guided by our analysis, we build a practical algorithm for estimating treatment effects using a non-stationary Gaussian processes with doubly-robust hyperparameters.",abstractText,[0],[0]
"Using a standard semi-synthetic simulation setup, we show that our algorithm outperforms the state-of-the-art, and that the behavior of existing algorithms conforms with our analysis.",abstractText,[0],[0]
Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm Design,title,[0],[0]
"Phase retrieval refers to the problem of recovering an unknown N -dimensional signal vector x 2 HN , with H being the set of either real (R) or complex (C) numbers, from the following nonlinear measurement process:
y = f(Ax+ e z ) + e y .",1. Introduction,[0],[0]
"(1)
Here, the measurement vector y 2 RM contains M realvalued observations, for example measured through the nonlinear function f(z) = |z|2 that operates element-wise on vectors, A 2 HM⇥N is a given measurement matrix, and the vectors ez 2 HM and ey 2 RN model signal and measurement noises, respectively.",1. Introduction,[0],[0]
"In contrast to the majority of
1School of Electrical and Computer Engineering, Cornell University, Ithaca, NY 2Department of EE, Princeton University 3University of Maryland.",1. Introduction,[0],[0]
"Correspondence to: Ramina Ghods <rg548@cornell.edu>, Christoph Studer <studer@cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"existing results on phase retrieval that assume randomness in the measurement matrix A, we focus on the practical scenario in which the measurement matrix A is deterministic, but the signal vector x to be recovered as well as the two noise sources ez and ey are random.",1. Introduction,[0],[0]
"Phase retrieval has been studied extensively over the last decades (Gerchberg & Saxton, 1972; Fienup, 1982) and finds use in a range of applications, including imaging (Fogel et al., 2016; Yeh et al., 2015; Holloway et al., 2016), microscopy (Kou et al., 2010; Faulkner & Rodenburg, 2004), and X-ray crystallography (Harrison, 1993; Miao et al., 2008; Pfeiffer et al., 2006).",1.1. Phase Retrieval,[0],[0]
"Phase retrieval problems were solved traditionally using alternating projection methods, such as the Gerchberg-Saxton (Gerchberg & Saxton, 1972) and Fienup (Fienup, 1982) algorithms.",1.1. Phase Retrieval,[0],[0]
"More recent results have shown that semidefinite programming enables the design of algorithms with performance guarantees (Candès et al., 2013; Candès & Li, 2014; Candès et al., 2015a; Waldspurger et al., 2015).",1.1. Phase Retrieval,[0],[0]
"These methods lift the problem to a higher dimension, resulting in excessive complexity and memory requirements.",1.1. Phase Retrieval,[0],[0]
"To perform phase retrieval for highdimensional problems with performance guarantees, a range of convex (Bahmani & Romberg, 2017; Goldstein & Studer, 2017; Hand & Voroninski, 2016; Dhifallah et al., 2017; Dhifallah & Lu, 2017; Yuan & Wang, 2017; Salehi et al., 2018) and nonconvex methods (Netrapalli et al., 2013; Schniter & Rangan, 2015; Candès et al., 2015b; Chen & Candès, 2015; Zhang & Liang, 2016; Wang et al., 2017a; Zhang et al., 2016; Wei, 2015; Sun et al., 2016; Zeng & So, 2017; Lu & Li, 2017; Ma et al., 2018) have been proposed recently.",1.1. Phase Retrieval,[0],[0]
All of the above non-lifting-based phase retrieval methods rely on accurate initial estimates of the signal vector to be recovered.,1.2. Spectral Initializers,[0],[0]
"Such estimates are typically obtained by means of so-called spectral initializers put forward in (Netrapalli et al., 2013).",1.2. Spectral Initializers,[0],[0]
"Spectral initializers first compute a Hermitian matrix of the following form:
D =
MX
m=1
T (ym)amaHm, (2)
",1.2. Spectral Initializers,[0],[0]
"where > 0 is a suitably-chosen scaling factor, ym denotes the mth measurement, aHm corresponds to the mth row of the measurement matrix A and T : R !",1.2. Spectral Initializers,[0],[0]
R is a (possibly nonlinear) preprocessing function.,1.2. Spectral Initializers,[0],[0]
"While the identity T (y) = y was used originally in (Netrapalli et al., 2013), recent results revealed that carefully crafted preprocessing functions yield more accurate estimates (Chen & Candès, 2015; Chen et al., 2015; Wang et al., 2017a;b; Lu & Li, 2017; Mondelli & Montanari, 2017).",1.2. Spectral Initializers,[0],[0]
"From the matrix D in (2), one then extracts the (scaled) eigenvector ˆx associated with the largest eigenvalue, which serves as an initial estimate of the solution to the phase retrieval problem.
",1.2. Spectral Initializers,[0],[0]
"As shown in (Netrapalli et al., 2013; Chen & Candès, 2015; Chen et al., 2015; Wang et al., 2017a;b; Lu & Li, 2017; Mondelli & Montanari, 2017), for i.i.d.",1.2. Spectral Initializers,[0],[0]
"Gaussian measurement matrices A, sufficiently large measurement ratios = M/N , and carefully crafted preprocessing functions T , spectral initializers provide accurate initialization vectors.",1.2. Spectral Initializers,[0],[0]
"In fact, the results in (Mondelli & Montanari, 2017) for the large-system limit with fixed and M ! 1 show that spectral initializers in combination with an optimal preprocessing function T achieve the fundamental informationtheoretic limits of phase retrieval.",1.2. Spectral Initializers,[0],[0]
"However, the assumption of having i.i.d.",1.2. Spectral Initializers,[0],[0]
"Gaussian measurement matrices A is impractical—it is more natural to assume that the signal vector x is random and the measurement matrix A is deterministic and structured (Bendory & Eldar, 2017).",1.2. Spectral Initializers,[0],[0]
"We propose a novel class of estimators, called linear spectral estimators (LSPEs), that provide accurate estimates for general nonlinear measurement systems of the form (1) and enable a nonasymptotic mean-squared error (MSE) analysis.",1.3. Contributions,[0],[0]
"We showcase the efficacy of LSPEs by applying them to phase retrieval problems, where we compute initialization vectors for real- and complex-valued systems with deterministic and finite-dimensional measurement matrices.",1.3. Contributions,[0],[0]
"For the proposed LSPEs, we derive nonasymptotic and sharp bounds on the MSE for signal estimation from phaseless measurements.",1.3. Contributions,[0],[0]
We use synthetic and real-world phase retrieval problems to demonstrate that LSPEs are able to significantly outperform existing spectral initializers on systems that acquire structured measurements.,1.3. Contributions,[0],[0]
We furthermore show that preprocessing the phaseless measurements enables LSPEs to generate improved initialization vectors for an even broader class of measurement systems.,1.3. Contributions,[0],[0]
"Lowercase and uppercase boldface letters represent column vectors and matrices, respectively.",1.4. Notation,[0],[0]
"For a matrix A, its transpose and Hermitian conjugate is AT and AH , respectively, and the kth row and `th column entry is [A]k,` = Ak,`.",1.4. Notation,[0],[0]
"For
a vector a, the kth entry is [a]k = ak.",1.4. Notation,[0],[0]
The `2-norm of a is denoted by kak2 and the Frobenius norm of A by kAkF .,1.4. Notation,[0],[0]
"The Kronecker product is ⌦, the Hadamard product is , the Hadamard division is ↵, and the trace operator is tr(·).",1.4. Notation,[0],[0]
"The N ⇥N identity matrix is denoted by IN ; the M ⇥N all-zeros and all-ones matrices are denoted by 0M⇥N and 1M⇥N , respectively.",1.4. Notation,[0],[0]
"For a vector a, diag(a) is a square matrix with a on the main diagonal; for a matrix A, diag(A) is a column vector containing the diagonal elements of A.",1.4. Notation,[0],[0]
"We start by reviewing the essentials of spectral initializers and then, introduce linear spectral estimators (LSPEs) for measurement systems of the form (1) with general nonlinearities f .",2. Linear Spectral Estimators,[0],[0]
"We furthermore provide nonasymptotic expressions for the associated estimation error, and we compare our analytical results to that of conventional spectral initializers in (2).",2. Linear Spectral Estimators,[0],[0]
"In Section 3, we will apply LSPEs to phase retrieval.",2. Linear Spectral Estimators,[0],[0]
"One of the key issues of the phase retrieval problem is the fact that if x is a solution to (1), then ej x for any 2 [0, 2⇡) is also a valid solution (assuming H = C).",2.1. Spectral Estimation and Initializers,[0],[0]
"Put simply, the solution is nonunique up to a global phase shift.",2.1. Spectral Estimation and Initializers,[0],[0]
"One way of combating this issue is to directly recover the outer product xxH instead of x, which is unaffected by phase shifts; this insight is the key underlying lifting-based phase retrieval methods (Candès et al., 2013; Candès & Li, 2014; Candès et al., 2015a; Waldspurger et al., 2015).",2.1. Spectral Estimation and Initializers,[0],[0]
"With this in mind, one could envision the design of an estimator that directly minimizes the conditional MSE:
˙ x = arg min x̃2HN E ⇥kxxH ˜x˜xHk2F | y ⇤ .",2.1. Spectral Estimation and Initializers,[0],[0]
"(3)
Here, expectation is with respect to the signal vector x and the two noise sources ez and ey .",2.1. Spectral Estimation and Initializers,[0],[0]
"This optimization problem resembles that of a posterior mean estimator (PME) which is, in general, difficult to derive, even for simple observation models—for phase retrieval, we have two additional challenges: (i) nonlinear phaseless measurements as in (1) and (ii) the quantity ˜x˜xH has rank-1.
",2.1. Spectral Estimation and Initializers,[0],[0]
Spectral initializers avoid the issues of the estimator in (3) by first replacing the true outer product xxH with a socalled spectral estimator matrix D as in (2) that depends on the measurement vector y.,2.1. Spectral Estimation and Initializers,[0],[0]
"In a second step, one then computes the best rank-1 approximation as follows:
ˆ x = arg min x̃2HN kD ˜x˜xHk2F (4)
from which the estimate ˆx can be extracted.",2.1. Spectral Estimation and Initializers,[0],[0]
"By performing an eigenvalue decomposition D = U⇤UH with U H U = IM and the eigenvalues in the diagonal matrix
⇤ = diag([ 1, . . .",2.1. Spectral Estimation and Initializers,[0],[0]
", M ] T ) are sorted in descending order of their magnitudes, a spectral initializer is given by the scaled leading eigenvector ˆx = p 1u1.",2.1. Spectral Estimation and Initializers,[0],[0]
"In practice, one can use power iterations to efficiently compute ˆx.",2.1. Spectral Estimation and Initializers,[0],[0]
"We now propose a novel class of estimators, which we call linear spectral estimators (LSPEs), that provide accurate estimates for general nonlinear measurement systems of the form (1).",2.2. Linear Spectral Estimators,[0],[0]
"To this end, we borrow ideas from the spectral initializer, the PME in (3), and the linear phase retrieval algorithm put forward in (Ghods et al., 2018).",2.2. Linear Spectral Estimators,[0],[0]
"In the first step, LSPEs apply a linear estimator to the nonlinear observations in T (y) to construct a spectral estimator matrix D
y
for which the spectral MSE (or matrix MSE) defined as
S-MSE = E h D
y xxH 2 F
i (5)
is minimal.",2.2. Linear Spectral Estimators,[0],[0]
"We restrict ourselves to spectral estimator matrices D
y
that are affine in T (y), i.e., are of the form
D
y = W0 +
MX
m=1
T (ym)Wm (6)
with Wm 2 HN⇥N , m = 0, . . .",2.2. Linear Spectral Estimators,[0],[0]
",M .",2.2. Linear Spectral Estimators,[0],[0]
"In the second step, we use the spectral estimator matrix D
y to extract a (scaled) leading eigenvector as in (3), which is the linear spectral estimate of the signal vector x. Intuitively, if we can construct a matrix D
y from the preprocessed measurements in T (y) for which the S-MSE in (5) is minimal, then we expect that computing its best rank-1 approximation would yield an accurate estimate of the signal vector x up to a global phase shift.",2.2. Linear Spectral Estimators,[0],[0]
"We will justify this claim in Section 2.3.
",2.2. Linear Spectral Estimators,[0],[0]
"Mathematically, we wish to compute a matrix D y of the form (6) that is the solution to the following problem:
minimize
f Wm2HN⇥N m=0,...,M
E
2
4 f W0 +
MX
m=1
T (ym)fWm xxH
2
F
3
5 .",2.2. Linear Spectral Estimators,[0],[0]
"(7)
Clearly, the spectral estimator matrix D y will depend on the measurement matrix A, the statistics of the signal to be estimated x and the two noise sources ez and ey, the nonlinearity f , as well as the preprocessing function T .",2.2. Linear Spectral Estimators,[0],[0]
"For this setting, we have the following general result which summarizes the LSPE; the proof is given in Appendix A.
Theorem 1 (Linear Spectral Estimator).",2.2. Linear Spectral Estimators,[0],[0]
Let the measurement vector y be a result of the general measurement model in (1) and select a preprocessing function T .,2.2. Linear Spectral Estimators,[0],[0]
"Define the vector T (y) = E[T (y)] and assume the matrix
T = E ⇥",2.2. Linear Spectral Estimators,[0],[0]
"(T (y) T (y))(T (y) T (y))T ⇤
is full rank.",2.2. Linear Spectral Estimators,[0],[0]
Let t 2 RM satisfy Tt = T (y) T (y) and Vm = E ⇥,2.2. Linear Spectral Estimators,[0],[0]
"(T (ym) T (ym))(xxH Kx) ⇤
for m = 1, . . .",2.2. Linear Spectral Estimators,[0],[0]
",M with K x = E ⇥",2.2. Linear Spectral Estimators,[0],[0]
xx H ⇤ .,2.2. Linear Spectral Estimators,[0],[0]
"Then, the LSPE matrix that minimizes the S-MSE in (5) is given by
D
y
= K
x
+
MX
m=1
tmVm.",2.2. Linear Spectral Estimators,[0],[0]
"(8)
The linear spectral estimate ˆx is then given by the scaled leading eigenvector of the matrix D
y
in (8).
",2.2. Linear Spectral Estimators,[0],[0]
"The vector t is the only quantity in Theorem 1 that depends on the actual (nonlinear) observations contained in the measurement vector y. All other quantities depend only on the first two moments of xxH as well as the considered signal, noise, and measurement models.",2.2. Linear Spectral Estimators,[0],[0]
The key features of the LSPE are as follows: (i) the involved quantities can often be computed in closed form (see Section 3 for two applications to phase retrieval) and (ii) LSPEs enable a nonasymptotic and sharp analysis of the associated estimation error.,2.2. Linear Spectral Estimators,[0],[0]
Remark 1.,2.2. Linear Spectral Estimators,[0],[0]
Theorem 1 requires the matrix T to be invertible.,2.2. Linear Spectral Estimators,[0],[0]
This condition is satisfied in most practical situations with nondegenerate measurement matrices A or in situations with nonzero measurement noise.,2.2. Linear Spectral Estimators,[0],[0]
The remaining piece of the proposed LSPE is to show that the result of this two-step estimation procedure indeed yields a vector that is close to the signal vector x.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
We start with the following result; the proof is given in Appendix B. Theorem 2 (S-MSE of the LSPE).,2.3. Estimation Error Analysis of LSPEs,[0],[0]
Let the assumptions of Theorem 1 hold.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
"Then, the S-MSE in (5) for the LSPE matrix in (8) is given by
S-MSELSPE = C xx
H MX
m=1
MX
m0=1
[T 1 ]m,m0 tr V H mVm0
(9)
with C xx
H = E",2.3. Estimation Error Analysis of LSPEs,[0],[0]
h,2.3. Estimation Error Analysis of LSPEs,[0],[0]
xx,2.3. Estimation Error Analysis of LSPEs,[0],[0]
"H K x 2 F i .
",2.3. Estimation Error Analysis of LSPEs,[0],[0]
"With this result, we are ready to establish a bound on the estimation error of the LSPE.",2.3. Estimation Error Analysis of LSPEs,[0],[0]
The proof of the following result follows from Theorem 2 and is given in Appendix C. Corollary 1 (LSPE Estimation Error).,2.3. Estimation Error Analysis of LSPEs,[0],[0]
Let the assumptions of Theorem 1 hold.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
"Then, the estimation error (EER) of the LSPE satisfies the following inequality:
EERLSPE = E ⇥kˆxˆxH",2.3. Estimation Error Analysis of LSPEs,[0],[0]
xxHk2F,2.3. Estimation Error Analysis of LSPEs,[0],[0]
⇤  4 S-MSELSPE.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
"(10)
",2.3. Estimation Error Analysis of LSPEs,[0],[0]
"This result implies that by minimizing the S-MSE in (5) via (7), we are also reducing the EER of the LSPE.",2.3. Estimation Error Analysis of LSPEs,[0],[0]
"In other words, if the spectral error E = D
y ˆxˆxH is small, then the EER of the LSPE (10) will be small.
",2.3. Estimation Error Analysis of LSPEs,[0],[0]
Remark 2.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
Corollary 1 is nonasymptotic and depends on the instance of measurement matrix A.,2.3. Estimation Error Analysis of LSPEs,[0],[0]
"This result is in stark contrast to existing performance bounds for spectral initializers (Netrapalli et al., 2013; Chen & Candès, 2015; Chen et al., 2015; Wang et al., 2017a;b) that strongly rely on randomness in the measurement matrix.",2.3. Estimation Error Analysis of LSPEs,[0],[0]
"In addition to randomness, the sharp performance guarantees in (Lu & Li, 2017; Mondelli & Montanari, 2017) focus on the asymptotic regime for which = M/N is fixed and M ! 1.",2.3. Estimation Error Analysis of LSPEs,[0],[0]
We can also derive an exact expression for the S-MSE of the conventional spectral initializer in (2).,2.4. S-MSE of Spectral Initializers,[0],[0]
"We assume optimal scaling, i.e., the parameter is set to minimize the S-MSE.",2.4. S-MSE of Spectral Initializers,[0],[0]
The following result characterizes the S-MSE of such a scaled spectral initializer; the proof is given in Appendix D. Proposition 1 (S-MSE of the Spectral Initializer).,2.4. S-MSE of Spectral Initializers,[0],[0]
Let D be the conventional spectral initializer matrix in (2).,2.4. S-MSE of Spectral Initializers,[0],[0]
"Then, the optimally-scaled S-MSE defined as
S-MSESI = min 2H
E ⇥kD xxHk2F",2.4. S-MSE of Spectral Initializers,[0],[0]
"⇤ (11)
is given by
S-MSESI = R xx
H PM m=1 a H m e Vmam 2
PM m=1 PM m0=1",2.4. S-MSE of Spectral Initializers,[0],[0]
"e Tm,m0 |aHmam0 |2 ,
(12)
where R xx H = E ⇥kxxHk2F",2.4. S-MSE of Spectral Initializers,[0],[0]
"⇤ , eVm = E ⇥T (ym)xxH ⇤ , m = 1, . . .",2.4. S-MSE of Spectral Initializers,[0],[0]
",M , and eT = E ⇥T (y)T (y)T ⇤.
",2.4. S-MSE of Spectral Initializers,[0],[0]
"Since the matrix in (2) is a special case of the LSPE matrix in (6), we have the following simple yet important property:
S-MSELSPE  S-MSESI.",2.4. S-MSE of Spectral Initializers,[0],[0]
"In words, the spectral MSE of the LSPE cannot be worse than that of a spectral initializer.",2.4. S-MSE of Spectral Initializers,[0],[0]
"As we will show in Section 4, LSPEs are able to outperform spectral initializers on both synthetic and real-world phase retrieval problems given that the same preprocessing function T is used.",2.4. S-MSE of Spectral Initializers,[0],[0]
The LSPE provides a framework for estimating signal vectors from the general observation model in (1).,3. LSPEs for Phase Retrieval Problems,[0],[0]
"To make the concept of LSPEs explicit and to demonstrate their efficacy in practice, we now show two application examples to phase retrieval in complex-valued systems.",3. LSPEs for Phase Retrieval Problems,[0],[0]
The LSPE for real-valued phase retrieval can be found in Appendix E.,3. LSPEs for Phase Retrieval Problems,[0],[0]
"We first focus on the case where the signal vector x to be estimated and the measurement matrix A are both complex-
valued.",3.1. Phase Retrieval without Preprocessing,[0],[0]
"The phaseless measurements y, however, remain real-valued.",3.1. Phase Retrieval without Preprocessing,[0],[0]
"We need the following assumptions.
",3.1. Phase Retrieval without Preprocessing,[0],[0]
Assumptions 1.,3.1. Phase Retrieval without Preprocessing,[0],[0]
Let H = C. Assume square absolute measurements f(z) = |z|2 and the identity preprocessing function T (y),3.1. Phase Retrieval without Preprocessing,[0],[0]
= y. Assume that the signal vector x 2 CN is i.i.d.,3.1. Phase Retrieval without Preprocessing,[0],[0]
"circularly-symmetric complex Gaussian with covariance matrix C
x
= 2 xIN , i.e., x ⇠ CN (0N⇥1, 2xIN ).",3.1. Phase Retrieval without Preprocessing,[0],[0]
"As-
sume that the signal noise vector ez is circularly-symmetric complex Gaussian with covariance matrix C
e z , i.e., ez ⇠ CN (0M⇥1,Cez ), and the measurement noise vector ey is a real-valued Gaussian vector with mean ¯ey and covariance matrix C
e y , i.e., ey ⇠ N (¯ey,C e y ).",3.1. Phase Retrieval without Preprocessing,[0],[0]
"Furthermore assume
that x, ez , and ey are independent.
",3.1. Phase Retrieval without Preprocessing,[0],[0]
"Under these assumptions, we can derive the following LSPE which we call LSPE-C; the detailed derivations of this spectral estimator are given in Appendix G.
Estimator 1 (LSPE-C).",3.1. Phase Retrieval without Preprocessing,[0],[0]
Let Assumptions 1 hold.,3.1. Phase Retrieval without Preprocessing,[0],[0]
"Then, the spectral estimation matrix is given by
D C y = K x +
MX
m=1
tmVm, (13)
",3.1. Phase Retrieval without Preprocessing,[0],[0]
"where K x = 2 xIN , the vector t 2 RM is given by the solution to the linear system Tt = y y with
y = diag(C
z
)",3.1. Phase Retrieval without Preprocessing,[0],[0]
"+
¯ e
y
C
z
= 2 xAA",3.1. Phase Retrieval without Preprocessing,[0],[0]
"H +C e z
T = C
z C",3.1. Phase Retrieval without Preprocessing,[0],[0]
"⇤ z +C e y
and Vm = 4xamaHm, m = 1, . . .",3.1. Phase Retrieval without Preprocessing,[0],[0]
",M .",3.1. Phase Retrieval without Preprocessing,[0],[0]
"The spectral estimate ˆx is given by the (scaled) leading eigenvector of DC
y
in (13).",3.1. Phase Retrieval without Preprocessing,[0],[0]
"Furthermore, the S-MSE is given by Theorem 2.
",3.1. Phase Retrieval without Preprocessing,[0],[0]
We emphasize that the spectral estimator matrix in (13) resembles that of the conventional spectral initializer matrix (2) with the following key differences.,3.1. Phase Retrieval without Preprocessing,[0],[0]
"First and foremost, each outer product contained in Vm = 4xamaHm in Estimator 1 is weighted by tm, which is a function of all phaseless measurements in y and of the covariance matrix C
x .",3.1. Phase Retrieval without Preprocessing,[0],[0]
"In contrast, each outer product in the conventional spectral initializer matrix in (2) is only weighted by the associated measurement ym.",3.1. Phase Retrieval without Preprocessing,[0],[0]
"This difference enables the LSPE to weight each outer product depending on correlations in the phaseless measurements caused by structure in the matrix A. Second, the spectral estimator matrix includes a mean term K
x , which is absent in the spectral initializer matrix.",3.1. Phase Retrieval without Preprocessing,[0],[0]
"As we will show in Section 4, for the same preprocessing function T , Estimator 1 is able to outperform spectral initializers for systems with structured measurement matrices A.",3.1. Phase Retrieval without Preprocessing,[0],[0]
For large i.i.d.,3.1. Phase Retrieval without Preprocessing,[0],[0]
"Gaussian measurement matrices, there is no particular correlation structure to exploit and LSPEs perform on par with spectral initializers.",3.1. Phase Retrieval without Preprocessing,[0],[0]
"To demonstrate the flexibility and generality of our framework, we now design an LSPE with an exponential preprocessing function for complex-valued phase retrieval.",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
We derive the LSPE under the following assumptions.,3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
Assumptions 2.,3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"Let H = C. Assume square absolute measurements f(z) = |z|2 and the exponential preprocessing function T (y) = exp( y) with > 0, i.e., we consider
T (y) = exp (|z|2 + ey) and z = Ax+",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"ez, where the exponential function is applied element-wise to vectors.",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"The remaining assumptions are the same as in Assumptions 1.
",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
We now derive the following LSPE called LSPE-Exp; the derivation of this spectral estimator is given in Appendix H. Estimator 2 (LSPE-Exp).,3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
Let Assumptions 2 hold.,3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"Then, the spectral estimation matrix is given by
D",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"Exp y = K x +
MX
m=1
tmVm, (14)
",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"where K x = 2 xIN , the vector t 2 RM is given by the solution to the linear system Tt = T (y) T (y) with T (y) = p ↵ q
T =",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"(p p T ) exp( 2Cey )↵(q qT 2Cz C⇤z)
(p pT )",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"↵ (q qT )
",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
Vm = 4 x[p,3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"]m
( [C
z",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"]m,m + 1) 2 ama
H m, m = 1, . . .",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
",M,
where we use the following definitions:
q = diag(Cz) + 1M⇥1 p = exp
¯ey + 2 12 diag(Cey )
C
z
= 2 xAA",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"H +C e z .
",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"The spectral estimate ˆx is given by the (scaled) leading eigenvector of DExp
y in (14).",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"Furthermore, the S-MSE of this estimator is given by Theorem 2.
",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"At first sight, the choice of the exponential preprocessing function used in Estimator 2 seems to be arbitrary.",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"We emphasize, however, that this particular function is inspired by the asymptotically-optimal preprocessing function for properly-normalized Gaussian measurement ensembles proposed in (Mondelli & Montanari, 2017) which is given by
Topt(y) = y 1 y + p 1 .",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"(15)
As it turns out, we can scale, negate, and shift the exponential preprocessing function T (y) = exp( y) to make it
take a similar shape as the function in (15).",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
"More concretely, exponential preprocessing as well as Topt(y) enables one to attenuate the effect of measurements with large magnitude, which is also the idea underlying the class of orthogonal spectral initializers, as proposed in (Chen et al., 2015; Wang et al., 2017a;b), that perform well in practice.",3.2. Phase Retrieval with Exponential Preprocessing,[0],[0]
We now compare the performance of our LSPEs against existing spectral initializers proposed for phase retrieval on synthetic and real image data.,4. Numerical Results,[0],[0]
"All our results use the spectral initializers and experimental setups provided by PhasePack (Chandra et al., 2017).",4. Numerical Results,[0],[0]
"We start by comparing the normalized MSE (N-MSE) defined as (Chandra et al., 2017)
N-MSE = min↵2H kx ↵ˆxk2
kxk2 for a range of spectral initializers on different measurement ensembles.",4.1. Impact of Measurement Ensemble,[0],[0]
"Specifically, we focus on the complex-valued case and consider (i) an i.i.d.",4.1. Impact of Measurement Ensemble,[0],[0]
"Gaussian measurement matrix with signal dimension N = 16, (ii) an i.i.d.",4.1. Impact of Measurement Ensemble,[0],[0]
"Gaussian measurement matrix with N = 256, and (iii) the structured “transmission matrix” used for image recovery through multiple scattering media as detailed in (Metzler et al., 2017).",4.1. Impact of Measurement Ensemble,[0],[0]
"We vary the oversampling ratio = M/N and compare the N-MSE of the proposed complex-valued LSPEs, LSPE-C (Estimator 1) and LSPE-Exp (Estimator 2 with = 0.001), to the following spectral initializers: the original spectral initializer (Netrapalli et al., 2013; Candès et al., 2015a) called “spectral,” truncated spectral initializer (Chen & Candès, 2015) called “truncated,” weighted spectral initializer (Wang et al., 2017b) called “weighted,” amplitude spectral initializer (Wang et al., 2017a) called “amplitude,” orthogonal spectral initializer (Chen et al., 2015)",4.1. Impact of Measurement Ensemble,[0],[0]
"called “orthogonal,” and the asymptotically-optimal spectral initializer (Mondelli & Montanari, 2017) called “optimal.”",4.1. Impact of Measurement Ensemble,[0],[0]
"For the following synthetic experiments, we generate the signals to be recovered according to Assumptions 1 and Assumptions 2 for LSPE-C and LSPE-Exp, respectively.
",4.1. Impact of Measurement Ensemble,[0],[0]
Figure 1a shows that the proposed LSPEs significantly outperform all existing spectral initializers for small problem dimensions with Gaussian measurements; this improvement is even more pronounced for large oversampling ratios.,4.1. Impact of Measurement Ensemble,[0],[0]
"The reason is that since we randomly generate a low-dimensional sensing matrix, the system will exhibit strong correlations among the measurements that can be exploited by LSPEs.",4.1. Impact of Measurement Ensemble,[0],[0]
"For larger dimensions with Gaussian measurements, we see in Figure 1b that the proposed LSPEs do not provide an advantage over other methods.",4.1. Impact of Measurement Ensemble,[0],[0]
"In fact, only LSPE-Exp is
able to perform as well as the orthogonal spectral initializer, which achieves the best performance in this scenario.",4.1. Impact of Measurement Ensemble,[0],[0]
This behavior can be attributed to the facts that (i) for large random matrices there is no particular correlation structure among the measurements to exploit and (ii) ignoring measurements associated to large values in ym is increasingly important.,4.1. Impact of Measurement Ensemble,[0],[0]
"For structured measurements, as it is the case for the transmission matrix from (Metzler et al., 2017), we see in Figure 1c that LSPEs significantly outperform existing methods that are designed for random measurement ensembles.",4.1. Impact of Measurement Ensemble,[0],[0]
"In this scenario, exponential preprocessing does not improve performance since correlations in the transmission matrix are dominating the performance.",4.1. Impact of Measurement Ensemble,[0],[0]
"We now validate our theoretical S-MSE expressions in Theorem 2 and Proposition 1, and confirm the accuracy of the EER bound given in Corollary 1.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
"In the following experiment, we set M = 8N and vary the dimension N from 8 to 64.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
"For each pair (M,N), we randomly generate one instance of an i.i.d.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
"circularly symmetric complex Gaussian measurement matrix and average the different errors (S-MSE and EER) over 10, 000 Monte-Carlo trials.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
"We consider a noiseless setting and assume identity preprocessing, i.e., T (y) = y.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
The signal vectors are generated according to an i.i.d.,4.2. S-MSE Expressions and Approximation Error,[0],[0]
circularly complex Gaussian random vector.,4.2. S-MSE Expressions and Approximation Error,[0],[0]
"From Figure 2, we see that our analytical S-MSE expressions for the LSPE-C and spectral initializers match their empirical values.",4.2. S-MSE Expressions and Approximation Error,[0],[0]
We furthermore see that the empirical EER is only about 6 dB to 10 dB lower than our non-asymptotic upper bound given in Corollary 1.,4.2. S-MSE Expressions and Approximation Error,[0],[0]
We finally illustrate the efficacy of LSPEs in a more realistic scenario.,4.3. Real-World Image Recovery,[0],[0]
"In particular, we show results for a real image reconstruction task by using LSPEs and spectral initializers
only, i.e., we are not using any additional phase retrieval algorithm.",4.3. Real-World Image Recovery,[0],[0]
"Our goal is to recover a 16⇥16-pixel and a 40⇥40- pixel image that was captured through a multiple scattering media using the deterministic and highly-structured transmission matrix as detailed in (Metzler et al., 2017).",4.3. Real-World Image Recovery,[0],[0]
We compare the proposed LSPEs to the same set of spectral initializers as in Section 4.1.,4.3. Real-World Image Recovery,[0],[0]
"The signal priors are as in Assumptions 1 (LSPE-C) and Assumptions 2 (LSPE-Exp).
",4.3. Real-World Image Recovery,[0],[0]
Figures 3 and 4 show the recovered images along with the N-MSE values.,4.3. Real-World Image Recovery,[0],[0]
The proposed LSPEs (often significantly) outperform all spectral initializers in terms of visual quality as well as the N-MSE.,4.3. Real-World Image Recovery,[0.9501666912100888],['(iii) Empirical demonstration of the performance enhancement compared to naive approaches as well as recent methods in this field.']
This result confirms the observations made in Figure 1c that LSPEs outperform existing spectral initializers for structured measurement matrices.,4.3. Real-World Image Recovery,[0],[0]
We note that exponential preprocessing for LSPEs does not noticeably improve the N-MSE (over LSPE-C) in this setting since correlations in the transmission measurement matrix are dominating the recovery performance.,4.3. Real-World Image Recovery,[0],[0]
"We have proposed a novel class of estimators, called linear spectral estimators (LSPEs), which are suitable for the recovery of signals from general nonlinear measurement systems.",5. Conclusions,[0],[0]
"We have developed nonasymptotic and deterministic performance guarantees for LSPEs that provide accurate bounds on the estimation error, especially for structured or low-dimensional measurement systems.",5. Conclusions,[0],[0]
"To demonstrate the efficacy of LSPEs in practice, we have applied them to complex-valued phase retrieval problems, in which LSPEs can be used to compute accurate signal estimates or initialization vectors for other convex or nonconvex phase retrieval algorithms.",5. Conclusions,[0],[0]
We have shown that properly preprocessing the nonlinear measurements can further improve the performance of LSPEs in practical scenarios.,5. Conclusions,[0],[0]
"Our simulations with synthetic and real data have shown that LSPEs are able to significantly outperform existing spectral initializers, especially for low-dimensional problems, for structured measurement matrices, or for large oversampling ratios.
",5. Conclusions,[0],[0]
There are many avenues for future work.,5. Conclusions,[0],[0]
"First, one could derive LSPEs for the asymptotically-optimal preprocessing function in (15) or for other commonly used functions, which may lead to further performance improvements.",5. Conclusions,[0],[0]
"Second, the proposed error analysis could be used to generate improved measurement matrices.",5. Conclusions,[0],[0]
"Third, an exploration of LSPEs for other nonlinearities that arise in machine learning and signal processing applications is left for future work.",5. Conclusions,[0],[0]
"R. Ghods and C. Studer were supported in part by Xilinx, Inc. and by the US National Science Foundation (NSF) under grants ECCS-1408006, EECS-1740286, CCF-1535897, CCF-1652065, and CNS-1717559.",Acknowledgments,[0],[0]
"T. Goldstein was supported by the US NSF under grant CCF-1535902, the US ONR under grant N00014-15-1-2676, the DARPA Lifelong Learning Machines program, and the Sloan Foundation.",Acknowledgments,[0],[0]
"The proof proceeds in two steps detailed as follows.
",A. Proof of Theorem 1,[0],[0]
Mean Matrix We first compute the mean matrix W0.,A. Proof of Theorem 1,[0],[0]
"Since (7) is a quadratic form, we can take the derivative in fWH0 and set it to zero, i.e.,
d
d f W
H 0
E
2
4 f W0 +
MX
m=1
T (ym)fWm xxH
2
F
3
5 = 0.
",A. Proof of Theorem 1,[0],[0]
"Basic matrix calculus yields
f W0 = Kx PM m=1 T (ym)fWm (16)
with T (ym) = E[T (ym)] and Kx = E ⇥",A. Proof of Theorem 1,[0],[0]
"xx H ⇤ .
",A. Proof of Theorem 1,[0],[0]
"Linear Estimation Matrix With (16) and the fact that (7) is a quadratic form in the matrices Wm, m = 1, . . .",A. Proof of Theorem 1,[0],[0]
",M ,
we take the derivatives in WHm and setting them to zero:
d dfWHm
E "" MX
m=1
(T (ym) T (ym))fWm (xxH Kx) 2
F
# =0.
",A. Proof of Theorem 1,[0],[0]
"By interchanging the derivative with expectation and with basic manipulations, we obtain the following set of optimality conditions for Wm for m = 1, . . .",A. Proof of Theorem 1,[0],[0]
",M : PM
m0=1 f Wm0 E ⇥",A. Proof of Theorem 1,[0],[0]
(T (ym) T (ym))(T (ym0) T (ym0)),A. Proof of Theorem 1,[0],[0]
"⇤
= E ⇥",A. Proof of Theorem 1,[0],[0]
(T (ym) T (ym))(xxH Kx) ⇤ .,A. Proof of Theorem 1,[0],[0]
"(17)
In compact matrix form, the above condition reads
(T⌦ IN⇥N )",A. Proof of Theorem 1,[0],[0]
"W = V, (18) where we used the following shortcuts:
T = E ⇥",A. Proof of Theorem 1,[0],[0]
"(T (y) T (y))(T (y) T (y))T ⇤
W =",A. Proof of Theorem 1,[0],[0]
"[ f W T 1 , . . .",A. Proof of Theorem 1,[0],[0]
", f W T m, . . .",A. Proof of Theorem 1,[0],[0]
", f W T M ] T
Vm= E ⇥",A. Proof of Theorem 1,[0],[0]
"(T (ym) T (ym))(xxH Kx) ⇤ ,m = 1, . . .",A. Proof of Theorem 1,[0],[0]
",M
V =",A. Proof of Theorem 1,[0],[0]
"[V T 1 , . . .",A. Proof of Theorem 1,[0],[0]
",V T m, . . .",A. Proof of Theorem 1,[0],[0]
",V T M ] T .
",A. Proof of Theorem 1,[0],[0]
"The condition in (18) can be solved for the estimation matrices in W leading to W = (T 1 ⌦ IN⇥N )V, where we require the matrix T to be full rank.",A. Proof of Theorem 1,[0],[0]
"To obtain the linear spectral estimator matrix, we simplify as
D
y
= K
x + ((T (y) T (y))T ⌦ IN⇥N )",A. Proof of Theorem 1,[0],[0]
"W = K
x
+ PM m=1 tmVm,
where we define the vector t = T 1(T (y) T (y)).",A. Proof of Theorem 1,[0],[0]
"To compute the spectral MSE in (5), we simplify
S-MSE = E  PM m=1 tmVm (xxH Kx) 2
F
.
",B. Proof of Theorem 2,[0],[0]
"We expand this expression into four terms
E  PM m=1 tmVm (xxH Kx) 2
F
= E  PM m=1 tmVm 2
F
(19)
",B. Proof of Theorem 2,[0],[0]
+,B. Proof of Theorem 2,[0],[0]
E h,B. Proof of Theorem 2,[0],[0]
xx,B. Proof of Theorem 2,[0],[0]
"H K x 2 F i
E h tr ⇣ (xx H K x )",B. Proof of Theorem 2,[0],[0]
"H ⇣PM
m=1 tmVm
⌘⌘i (20)
E  tr ✓⇣PM m=1 tmVm ⌘",B. Proof of Theorem 2,[0],[0]
H,B. Proof of Theorem 2,[0],[0]
(,B. Proof of Theorem 2,[0],[0]
"xx H K x ) ◆ (21)
and simplify each expression individually.",B. Proof of Theorem 2,[0],[0]
"We start with (19) and use the fact that
PM m=1 tmVm = ((T (y) T (y))TT 1 ⌦ IN⇥N )V
and rewrite the quantity within expectation as follows:
V H (T 1 (T (y) T (y))⌦ IN⇥N )
⇥",B. Proof of Theorem 2,[0],[0]
"((T (y) T (y))TT 1 ⌦ IN⇥N )V = V H ((T 1 (T (y) T (y))
⇥",B. Proof of Theorem 2,[0],[0]
(T (y) T (y))TT 1)⌦ IN⇥N ),B. Proof of Theorem 2,[0],[0]
"V. We now evaluate the expectation which leads to
E  PM m=1 tmVm 2
F
= tr ⇣ V H (T 1 ⌦ IN⇥N )V ⌘
or, equivalently, to E  PM m=1tmVm 2
F
=
MX
m=1
MX
m0=1
[T 1 ]m,m0 tr V H mVm0 .
",B. Proof of Theorem 2,[0],[0]
We next will simplify (20).,B. Proof of Theorem 2,[0],[0]
"Recall that
tm = PM m0=1[T 1
]m,m0(T (ym0) T (ym0)), which enables us to write (20) as
E h tr ⇣ (xx H K x )",B. Proof of Theorem 2,[0],[0]
"H ⇣PM
m=1 tmVm
⌘⌘i
= PM m=1 PM m0=1[T 1 ]m,m0
⇥ tr E⇥(xxH K x ) H (T (ym0) T (ym0))",B. Proof of Theorem 2,[0],[0]
"⇤ Vm
= PM m=1 PM m0=1[T 1 ]m,m0 tr V H m0Vm .",B. Proof of Theorem 2,[0],[0]
"(22)
Seeing as (21) is the Hermitian conjugate of (20), we have
E  tr ✓⇣PM m=1 tmVm ⌘",B. Proof of Theorem 2,[0],[0]
"H (xx H K x ) ◆
= PM m=1 PM m0=1[T 1 ]",B. Proof of Theorem 2,[0],[0]
"⇤ m,m0 tr V H mVm0 .",B. Proof of Theorem 2,[0],[0]
"(23)
Combining all these terms yield the spectral MSE
S-MSE = E  PM m=1 tmVm (xxH Kx) 2
F
= C
xx
H tr ⇣ V H (T 1 ⌦ IN⇥N )V ⌘ .
with C xx
H = E",B. Proof of Theorem 2,[0],[0]
h xx H K x 2 F i .,B. Proof of Theorem 2,[0],[0]
We bound the estimation error with the spectral MSE of the LSPE as follows.,C. Proof of Corollary 1,[0],[0]
"For a given instance, we have
kˆxˆxH xxHk2F",C. Proof of Corollary 1,[0],[0]
= kˆxˆxH,C. Proof of Corollary 1,[0],[0]
Dy,C. Proof of Corollary 1,[0],[0]
"+Dy xxHk2F (a) 2kˆxˆxH D
y k2F +",C. Proof of Corollary 1,[0],[0]
2kDy xxHk2F,C. Proof of Corollary 1,[0],[0]
(,C. Proof of Corollary 1,[0],[0]
"b) 4kD
y xxHk2F , where (a) follows from the squared triangle inequality and (b) because ˆxˆxH is the best rank-1 approximation of D
y .",C. Proof of Corollary 1,[0],[0]
"Averaging over all instances finally yields
E ⇥kˆxˆxH xxHk2F",C. Proof of Corollary 1,[0],[0]
⇤  4 S-MSELSPE.,C. Proof of Corollary 1,[0],[0]
Phase retrieval refers to the problem of recovering realor complex-valued vectors from magnitude measurements.,abstractText,[0],[0]
The best-known algorithms for this problem are iterative in nature and rely on so-called spectral initializers that provide accurate initialization vectors.,abstractText,[0],[0]
"We propose a novel class of estimators suitable for general nonlinear measurement systems, called linear spectral estimators (LSPEs), which can be used to compute accurate initialization vectors for phase retrieval problems.",abstractText,[0],[0]
"The proposed LSPEs not only provide accurate initialization vectors for noisy phase retrieval systems with structured or random measurement matrices, but also enable the derivation of sharp and nonasymptotic mean-squared error bounds.",abstractText,[0],[0]
"We demonstrate the efficacy of LSPEs on synthetic and real-world phase retrieval problems, and show that our estimators significantly outperform existing methods for structured measurement systems that arise in practice.",abstractText,[0],[0]
Linear Spectral Estimators and an Application to Phase Retrieval,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 477–483 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
477",text,[0],[0]
"Span-based neural constituency parsing (Cross and Huang, 2016; Stern et al., 2017a) has attracted attention due to its high accuracy and extreme simplicity.",1 Introduction,[0],[0]
"Compared with other recent neural constituency parsers (Dyer et al., 2016; Liu and Zhang, 2016; Durrett and Klein, 2015) which use neural networks to model tree structures, the spanbased framework is considerably simpler, only using bidirectional RNNs to model the input sequence and not the output tree.",1 Introduction,[0],[0]
"Because of this factorization, the output space is decomposable
which enables efficient dynamic programming algorithm such as CKY.",1 Introduction,[0],[0]
"But existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser (Cross and Huang, 2016) is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Björkelund et al., 2016).
",1 Introduction,[0],[0]
We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space.,1 Introduction,[0],[0]
"Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n − 1) actions (i.e., steps) for a sentence of nwords.",1 Introduction,[0],[0]
"While previous non-neural work in this area requires sophisticated features (Huang and Sagae, 2010; Mi and Huang, 2015) and thus high time complexity such as O(n11), our states are as simple as ` : (i, j) where ` is the step index and (i, j) is the span, modeled using bidirectional RNNs without any syntactic features.",1 Introduction,[0],[0]
"This gives a running time ofO(n4), with the extraO(n) for step index.",1 Introduction,[0],[0]
We further employ beam search to have a practical runtime of O(nb2) at the cost of exact search where b is the beam size.,1 Introduction,[0],[0]
"However, on the Penn Treebank, most sentences are less than 40 words (n < 40), and even with a small beam size of b = 10, the observed complexity of an O(nb2) parser is not exactly linear in n (see Experiments).",1 Introduction,[0],[0]
"To solve this problem, we apply cube pruning (Chiang, 2007; Huang and Chiang, 2007) to improve the runtime toO(nb log b) which
renders an observed complexity that is linear in n (with minor extra inexactness).
",1 Introduction,[0],[0]
"We make the following contributions:
• We design the first neural parser that is both linear time and capable of searching over exponentially large space.1
•",1 Introduction,[0],[0]
"We are the first to apply cube pruning to incremental parsing, and achieves, for the first time, the complexity of O(nb log b), i.e., linear in sentence length and (almost) linear in beam size.",1 Introduction,[0],[0]
"This leads to an observed complexity strictly linear in sentence length n.
•",1 Introduction,[0],[0]
"We devise a novel loss function which penalizes wrong spans that cross gold-tree spans, and employ max-violation update (Huang et al., 2012) to train this parser with structured SVM and beam search.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with chart parsing baselines, our parser is substantially faster for long sentences on the Penn Treebank, and orders of magnitude faster for end-to-end discourse parsing.",1 Introduction,[0],[0]
"It also achieves the highest F1 score on the Penn Treebank among single model end-to-end systems.
",1 Introduction,[0],[0]
"• We devise a new formulation of graphstructured stack (Tomita, 1991) which requires no extra bookkeeping, proving a new theorem that gives deep insight into GSS.",1 Introduction,[0],[0]
"A span-based shift-reduce constituency parser (Cross and Huang, 2016) maintains a stack of spans (i, j), and progressively adds a new span each time it takes a shift or reduce action.",2.1 Span-Based Shift-Reduce Parsing,[0],[0]
"With (i, j) on top of the stack, the parser can either shift to push the next singleton span (j, j + 1) on the stack, or it can reduce to combine the top two spans, (k, i) and (i, j), forming the larger span (k, j).",2.1 Span-Based Shift-Reduce Parsing,[0],[0]
"After each shift/reduce action, the top-most span is labeled as either a constituent or with a null label ∅, which means that the subsequence is not a subtree in the final decoded parse.",2.1 Span-Based Shift-Reduce Parsing,[0],[0]
"Parsing initializes with an empty stack and continues until (0, n) is formed, representing the entire sentence.
",2.1 Span-Based Shift-Reduce Parsing,[0],[0]
1 https://github.com/junekihong/beam-span-parser,2.1 Span-Based Shift-Reduce Parsing,[0],[0]
"To get the feature representation of a span (i, j), we use the output sequence of a bi-directional LSTM (Cross and Huang, 2016; Stern et al., 2017a).",2.2 Bi-LSTM features,[0],[0]
"The LSTM produces f0, ..., fn forwards and bn, ...,b0 backwards outputs, which we concatenate the differences of (fj−fi) and (bi−bj) as the representation for span (i, j).",2.2 Bi-LSTM features,[0],[0]
"This eliminates the need for complex feature engineering, and can be stored for efficient querying during decoding.",2.2 Bi-LSTM features,[0],[0]
"Like Stern et al. (2017a), we also decompose the score of a tree t to be the sum of the span scores:
s(t) = ∑
(i,j,X)∈t
s(i, j,X) (1)
= ∑
(i,j)∈t
max X
s((fj − fi;bi − bj), X) (2)
Note that X is a nonterminal label, a unary chain (e.g., S-VP), or null label ∅.2 In a shift-reduce setting, there are 2n − 1 steps (n shifts and n − 1 reduces) and after each step we take the best label for the resulting span; therefore there are exactly
2The actual code base of Stern et al. (2017b) forces s(i, j,∅) to be 0, which simplifies their CKY parser and slightly improves their parsing accuracy.",3.1 Score Decomposition,[0],[0]
"However, in our incremental parser, this change favors shift over reduce and degrades accuracy, so our parser keeps a learned score for ∅.
2n−1 such (labeled) spans (i, j,X) in tree t. Also note that the choice of the label for any span (i, j) is only dependent on (i, j) itself (and not depending on any subtree information), thus the max over label X is independent of other spans, which is a nice property of span-based parsing (Cross and Huang, 2016; Stern et al., 2017a).",3.1 Score Decomposition,[0],[0]
We now reformulate this DP parser in the above section as a shift-reduce parser.,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
We maintain a step index ` in order to perform action-synchronous beam search (see below).,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"Figure 1 shows how to represent a parsing stack using only the top span (i, j).",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"If the top span (i, j) shifts, it produces (j, j + 1), but if it reduces, it needs to know the second last span on the stack, (k, i), which is not represented in the current state.",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"This problem can be solved by graph-structure stack (Tomita, 1991; Huang and Sagae, 2010), which maintains, for each state p, a set of predecessor states π(p) that p can combine with on the left.
",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"This is the way our actual code works (π(p) is implemented as a list of pointers, or “left pointers”), but here for simplicity of presentation we devise a novel but easier-to-understand formulation in Fig. 1, where we explicitly represent the set of predecessor states that state ` : (i, j) can combine with as `′ : (k, i) where `′ =",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"`−2(j− i) + 1, i.e., (i, j) at step ` can combine with any (k, i) for any k at step `′. The rationale behind this new formulation is the following theorem:
Theorem 1",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"The predecessor states π(` : (i, j)) are all in the same step `′ =",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
`− 2(j − i) + 1.,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
Proof.,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
"By induction.
",3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
This Theorem bring new and deep insights and suggests an alternative implementation that does not require any extra bookkeeping.,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
The time complexity of this algorithm is O(n4) with the extra O(n) due to step index.3,3.2 Graph-Struct. Stack w/o Bookkeeping,[0],[0]
The incremental nature of our parser allows us to further lower the runtime complexity at the cost of inexact search.,3.3 Action-Synchronous Beam Search,[0],[0]
"At each time step, we maintain the top b parsing states, pruning off the rest.",3.3 Action-Synchronous Beam Search,[0],[0]
"Thus, a candidate parse that made it to the end of decoding had to survive within the top b at every step.
",3.3 Action-Synchronous Beam Search,[0],[0]
"3The word-synchronous alternative does not need the step index ` and enjoys a cubic time complexity, being almost identical to CKY.",3.3 Action-Synchronous Beam Search,[0],[0]
"However, beam search becomes very tricky.
",3.3 Action-Synchronous Beam Search,[0],[0]
With O(n) parsing actions our time complexity becomes linear in the length of the sentence.,3.3 Action-Synchronous Beam Search,[0],[0]
"However, Theorem 1 suggests that a parsing state p can have up to b predecessor states (“left pointers”), i.e., |π(p)| ≤ b because π(p) are all in the same step, a reduce action can produce up to b subsequent new reduced states.",3.4 Cube Pruning,[0],[0]
"With b items on a beam and O(n) actions to take, this gives us an overall complexity of O(nb2).",3.4 Cube Pruning,[0],[0]
"Even though b2 is a constant, even modest values of b can make b2 dominate the length of the sentence.",3.4 Cube Pruning,[0],[0]
"4
To improve this at the cost of additional inexactness, we introduce cube pruning to our beam search, where we put candidate actions into a heap and retrieve the top b states to be considered in the next time-step.",3.4 Cube Pruning,[0],[0]
We heapify the top b shiftmerged states and the top b reduced states.,3.4 Cube Pruning,[0],[0]
"To avoid inserting all b2 reduced states from the previous beam, we only consider each state’s highest scoring left pointer,5 and whenever we pop a reduced state from the heap, we iterate down its left pointers to insert the next non-duplicate reduced state back into the heap.",3.4 Cube Pruning,[0],[0]
This process finishes when we pop b items from the heap.,3.4 Cube Pruning,[0],[0]
"The initialization of the heap takes O(b) and popping b items takes O(b log b), giving us an overall improved runtime of O(nb log b).",3.4 Cube Pruning,[0],[0]
"We use a Structured SVM approach for training (Stern et al., 2017a; Shi et al., 2017).",4 Training,[0],[0]
"We want the model to score the gold tree t∗ higher than any other tree t by at least a margin ∆(t, t∗):
∀t, s(t∗)− s(t) ≥ ∆(t, t∗).
",4 Training,[0],[0]
"Note that ∆(t, t) = 0 for any t and ∆(t, t∗)",4 Training,[0],[0]
> 0,4 Training,[0],[0]
for any t 6= t∗.,4 Training,[0],[0]
"At training time we perform lossaugmented decoding:
t̂ = arg max t s∆(t)",4 Training,[0],[0]
"= arg max t s(t) + ∆(t, t∗).
",4 Training,[0],[0]
4The average length of a sentence in the Penn Treebank training set is about 24.,4 Training,[0],[0]
"Even with a beam size of 10, we already have b2 = 100, which would be a significant factor in our runtime.",4 Training,[0],[0]
"In practice, each parsing state will rarely have the maximum b left pointers so this ends up being a loose upper-bound.",4 Training,[0],[0]
"Nevertheless, the beam search should be performed with the input length in mind, or else as b increases we risk losing a linear runtime.
",4 Training,[0],[0]
"5If each previous beam is sorted, and if the beam search is conducted by going top-to-bottom, then each state’s left pointers will implicitly be kept in sorted order.
",4 Training,[0],[0]
where s∆(·) is the loss-augmented score.,4 Training,[0],[0]
"If t̂ = t∗, then all constraints are satisfied (which implies arg maxt s(t) = t
∗), otherwise we perform an update by backpropagating from s∆(t̂)− s(t∗).",4 Training,[0],[0]
"The baseline loss function from Stern et al. (2017a) counts the incorrect labels (i, j,X) in the predicted tree:
∆base(t, t ∗)",4.1 Cross-Span Loss,[0],[0]
"= ∑ (i,j,X)∈t 1 ( X 6= t∗(i,j) ) .
",4.1 Cross-Span Loss,[0],[0]
"Note that X can be null ∅, and t∗(i,j) denotes the gold label for span (i, j), which could also be ∅.6 However, there are two cases where t∗(i,j) = ∅: a subspan (i, j) due to binarization (e.g., a span combining the first two subtrees in a ternary branching node), or an invalid span in t that crosses a gold span in t∗.",4.1 Cross-Span Loss,[0],[0]
"In the baseline function above, these two cases are treated equivalently; for example, a span (3, 5,∅",4.1 Cross-Span Loss,[0],[0]
") ∈ t is not penalized even if there is a gold span (4, 6,VP)",4.1 Cross-Span Loss,[0],[0]
∈ t∗.,4.1 Cross-Span Loss,[0],[0]
"So we revise our loss function as:
∆new(t, t ∗) =",4.1 Cross-Span Loss,[0],[0]
"∑ (i,j,X)∈t 1 ( X 6= t∗(i,j)
∨ cross(i, j, t∗) )
6Note that the predicted tree t has exactly 2n − 1 spans but t∗ has much fewer spans (only labeled spans without ∅).
",4.1 Cross-Span Loss,[0],[0]
"where cross(i, j, t∗) = ∃ (k, l) ∈ t∗, and i <",4.1 Cross-Span Loss,[0],[0]
k,4.1 Cross-Span Loss,[0],[0]
< j,4.1 Cross-Span Loss,[0],[0]
< l or k,4.1 Cross-Span Loss,[0],[0]
< i,4.1 Cross-Span Loss,[0],[0]
< l < j.,4.1 Cross-Span Loss,[0],[0]
"Given that we maintain loss-augmented scores even for partial trees, we can perform a training update on a given example sentence by choosing to take the loss where it is the greatest along the parse trajectory.",4.2 Max Violation Updates,[0],[0]
"At each parsing time-step `, the violation is the difference between the highest augmented-scoring parse trajectory up to that point and the gold trajectory (Huang et al., 2012; Yu et al., 2013).",4.2 Max Violation Updates,[0],[0]
Note that computing the violation gives us the max-margin loss described above.,4.2 Max Violation Updates,[0],[0]
Taking the largest violation from all time-steps gives us the max-violation loss.,4.2 Max Violation Updates,[0],[0]
"We present experiments on the Penn Treebank (Marcus et al., 1993) and the PTB-RST discourse treebank (Zhao and Huang, 2017).",5 Experiments,[0],[0]
"In both cases, the training set is shuffled before each epoch, and dropout (Hinton et al., 2012) is employed with probability 0.4 to the recurrent outputs for regularization.",5 Experiments,[0],[0]
Updates with minibatches of size 10 and 1 are used for PTB and the PTB-RST respectively.,5 Experiments,[0],[0]
"We use Adam (Kingma and Ba, 2014) with default settings to schedule learning rates for all the weights.",5 Experiments,[0],[0]
"To address unknown words during training, we adopt the strategy described by Kiperwasser and Goldberg (Kiperwasser and Goldberg, 2016); words in the training set are replaced with the unknown word symbol UNK with probability punk = 1 1+f(w) , with f(w) being the number of
occurrences of word w in the training corpus.",5 Experiments,[0],[0]
"Our system is implemented in Python using the DyNet neural network library (Neubig et al., 2017).",5 Experiments,[0],[0]
"We use the Wall Street Journal portion of the Penn Treebank, with the standard split of sections 2-21 for training, 22 for development, and 23 for testing.",5.1 Penn Treebank,[0],[0]
"Tags are provided using the Stanford tagger with 10-way jackknifing.
",5.1 Penn Treebank,[0],[0]
"Table 1 shows our development results and overall speeds, while Table 2 compares our test results.",5.1 Penn Treebank,[0],[0]
We show that a beam size of 20 can be fast while still achieving state-of-the-art performances.,5.1 Penn Treebank,[0],[0]
"To measure the tractability of parsing on longer sequences, we also consider experiments on the
PTB-RST discourse Treebank, a joint discourse and constituency dataset with a combined representation, allowing for parsing at either level (Zhao and Huang, 2017).",5.2 Discourse Parsing,[0],[0]
We compare our runtimes out-of-the-box in Figure 3.,5.2 Discourse Parsing,[0],[0]
"Without any pre-processing, and by treating discourse examples as constituency trees with thousands of words, our trained models represent end-to-end discourse parsing systems.
",5.2 Discourse Parsing,[0],[0]
"For our overall constituency results in Table 3, and for discourse results in Table 4, we adapt the split-point feature described in (Zhao and Huang, 2017) in addition to the base parser.",5.2 Discourse Parsing,[0],[0]
We find that larger beamsizes are required to achieve good discourse scores.,5.2 Discourse Parsing,[0],[0]
"We have developed a new neural parser that maintains linear time, while still searching over an exponentially large space.",6 Conclusions,[0],[0]
We also use cube pruning to further improve the runtime to O(nb log b).,6 Conclusions,[0],[0]
"For training, we introduce a new loss function, and achieve state-of-the-art results among singlemodel end-to-end systems.",6 Conclusions,[0],[0]
We thank Dezhong Deng who contributed greatly to Secs.,Acknowledgments,[0],[0]
"3.2 and 4 (he deserves co-authorship), and Mitchell Stern for releasing his code and and suggestions.",Acknowledgments,[0],[0]
This work was supported in part by NSF IIS-1656051 and DARPA N66001-17-2-4030.,Acknowledgments,[0],[0]
"Recently, span-based constituency parsing has achieved competitive accuracies with extremely simple models by using bidirectional RNNs to model “spans”.",abstractText,[0],[0]
"However, the minimal span parser of Stern et al. (2017a) which holds the current state of the art accuracy is a chart parser running in cubic time, O(n3), which is too slow for longer sentences and for applications beyond sentence boundaries such as end-toend discourse parsing and joint sentence boundary detection and parsing.",abstractText,[0],[0]
"We propose a linear-time constituency parser with RNNs and dynamic programming using graph-structured stack and beam search, which runs in time O(nb2) where b is the beam size.",abstractText,[0],[0]
We further speed this up to O(nb log b) by integrating cube pruning.,abstractText,[0],[0]
"Compared with chart parsing baselines, this linear-time parser is substantially faster for long sentences on the Penn Treebank and orders of magnitude faster for discourse parsing, and achieves the highest F1 accuracy on the Penn Treebank among single model end-to-end systems.",abstractText,[0],[0]
Linear-Time Constituency Parsing with RNNs and Dynamic Programming,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1941–1950 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Deception detection is a critical problem studied by psychologists, criminologists, and computer scientists.",1 Introduction,[0],[0]
In recent years the NLP and speech communities have increased their interest in deception detection.,1 Introduction,[0],[0]
"Language cues are inexpensive and easy to collect, and research examining text-based and speech-based cues to deception has been quite promising.",1 Introduction,[0],[0]
"Prior work has examined deceptive language in several domains, including fake reviews, mock crime scenes, and opinions about topics such as abortion or the death penalty.",1 Introduction,[0],[0]
"In this work we explore the domain of interview dialogues, which are similar to many real-world deception conditions.
",1 Introduction,[0],[0]
"Previous work has presented the results of classification experiments using linguistic features, attempting to identify which features contribute most to classification accuracy.",1 Introduction,[0],[0]
"However, studies often do not include an empirical analysis of features.",1 Introduction,[0],[0]
"We might know that a particular feature
set (e.g. LIWC categories) is useful for deception classification, but we lack insight about the nature of the deceptive and truthful language that makes the feature set useful, and whether the differences in language use are statistically significant.",1 Introduction,[0],[0]
In this work we conduct an empirical analysis of feature sets and report on the different characteristics of truthful and deceptive language.,1 Introduction,[0],[0]
"In addition, previous work has focused on the characteristics of deceptive language, and not on the characteristics of perceived deceptive language.",1 Introduction,[0],[0]
"We are also interested in human perception of deception; that is, what are the characteristics of language that listeners perceive as truthful or deceptive?",1 Introduction,[0],[0]
"We examine a unique dataset that includes information about both the deceiver and the interviewer, along with interviewer judgments of deception.",1 Introduction,[0],[0]
"Along with an analysis of deceptive and truthful speech, we analyze the believed and disbelieved speech, according to reported interviewer judgments.",1 Introduction,[0],[0]
"Finally, previous work has focused on general inferences about deception; here we include analysis of gender and native language, to study their effect on deceptive behavior, and also their effect on perception of deception.",1 Introduction,[0],[0]
"This work contributes to the critical problem of automatic deception detection, and increases our scientific understanding of deception, deception perception, and speaker differences in deceptive behavior.
",1 Introduction,[0],[0]
The paper is organized as follows:,1 Introduction,[0],[0]
In Section 2 we review related work in language-based cues to deception.,1 Introduction,[0],[0]
"Section 3 describes the dataset used for this work, and Section 4 details the different feature sets we employ.",1 Introduction,[0],[0]
"In Section 5, we report on the results of our empirical study of indicators of deception and perceived deception, as well as gender and native language differences.",1 Introduction,[0],[0]
Section 6 presents our machine learning classification results using the deception indicator feature sets.,1 Introduction,[0],[0]
"We conclude in Section 7 with a discussion and ideas
1941
for future work.",1 Introduction,[0],[0]
Language-based cues to deception have been analyzed in many genres.,2 Related Work,[0],[0]
"Ott et al. (2011) compared approaches to automatically detecting deceptive opinion spam, using a crowdsourced dataset of fake hotel reviews.",2 Related Work,[0],[0]
"Several studies use a fake opinion paradigm for collecting data, instructing subjects to write or record deceptive and truthful opinions about controversial topics such as the death penalty or abortion, or about a person that they like/dislike (Newman et al., 2003; Mihalcea and Strapparava, 2009).",2 Related Work,[0],[0]
"Other research has focused on real-world data obtained from court testimonies and depositions (Fornaciari and Poesio, 2013; Bachenko et al., 2008; Pérez-Rosas et al., 2015).",2 Related Work,[0],[0]
"Real-world deceptive situations are highstakes, where there is much to be gained or lost if deception succeeds or fails; it is hypothesized that these conditions are more likely to elicit strong cues to deception.",2 Related Work,[0],[0]
"However, working with such data requires extensive research to annotate each utterance for veracity, so such datasets are often quite small and not always reliable.
",2 Related Work,[0],[0]
"Linguistic features such as n-grams and language complexity have been analyzed as cues to deception (Pérez-Rosas and Mihalcea, 2015; Yancheva and Rudzicz, 2013).",2 Related Work,[0],[0]
"Syntactic features such as part of speech tags have also been found to be useful for structured data (Ott et al., 2011; Feng et al., 2012).",2 Related Work,[0],[0]
"Statement Analysis (Adams, 1996) is a text-based deception detection approach that combines lexical and syntactic features.",2 Related Work,[0],[0]
"An especially useful resource for text-based deception detection is the Linguistic Inquiry and Word Count (LIWC) (Pennebaker and King, 1999), which groups words into psychologically motivated categories.",2 Related Work,[0],[0]
"In addition to lexical features, some studies have examined acousticprosodic cues to deception (Rockwell et al., 1997; Enos, 2009; Mendels et al., 2017).",2 Related Work,[0],[0]
"(Benus et al., 2006) studied pause behavior in deceptive speech.",2 Related Work,[0],[0]
"This work is very promising, but it is more difficult to obtain large, cleanly recorded speech corpora with deception annotations than to obtain text corpora.",2 Related Work,[0],[0]
"An excellent meta-study of verbal cues to deception can be found in (DePaulo et al., 2003).",2 Related Work,[0],[0]
"For this work, we examined the Columbia XCultural Deception (CXD) Corpus (Levitan et al., 2015a) a collection of within-subject deceptive and non-deceptive speech from native speakers of Standard American English (SAE) and Mandarin Chinese (MC), all speaking in English.",3.1 Corpus,[0],[0]
The corpus contains dialogues between 340 subjects.,3.1 Corpus,[0],[0]
A variation of a fake resume paradigm was used to collect the data.,3.1 Corpus,[0],[0]
Previously unacquainted pairs of subjects played a ”lying game” with each other.,3.1 Corpus,[0],[0]
Each subject filled out a 24-item biographical questionnaire and were instructed to create false answers for a random half of the questions.,3.1 Corpus,[0],[0]
"They also reported demographic information including gender and native language, and completed the NEO-FFI personality inventory (Costa and McCrae, 1989).
",3.1 Corpus,[0],[0]
The lying game was recorded in a sound booth.,3.1 Corpus,[0],[0]
"For the first half of the game, one subject assumed the role of the interviewer, while the other answered the biographical questions, lying for half and telling the truth for the other; questions chosen in each category were balanced across the corpus.",3.1 Corpus,[0],[0]
"For the second half of the game, the subjects roles were reversed, and the interviewer became the interviewee.",3.1 Corpus,[0],[0]
"During the game, the interviewer was allowed to ask the 24 questions in any order s/he chose; the interviewer was also encouraged to ask follow-up questions to aid them in determining the truth of the interviewees answers.",3.1 Corpus,[0],[0]
"Interviewers recorded their judgments for each of the 24 questions, providing information about human perception of deception.",3.1 Corpus,[0],[0]
"The entire corpus was orthographically transcribed using the Amazon Mechanical Turk (AMT)1 crowd-sourcing platform, and the speech was segmented into inter-pausal units (IPUs), defined as pause-free segments of speech separated by a minimum pause length of 50 ms.",3.1 Corpus,[0],[0]
"The speech was also segmented into turn units, where a turn is defined as a maximal sequence of IPUs from a single speaker without any interlocutor speech that is not a backchannel.",3.1 Corpus,[0],[0]
There are two forms of deception annotations in the corpus: local and global.,3.1 Corpus,[0],[0]
Interviewees labeled their responses with local annotations by pressing a ”T” or ”F” key for each utterance as they spoke.,3.1 Corpus,[0],[0]
These keypresses were automatically aligned with speaker IPUs and turns.,3.1 Corpus,[0],[0]
"Global la-
1https://www.mturk.com/mturk/
bels were provided by the biographical questionnaire, where each of the 24 questions was labeled as truthful or deceptive.
",3.1 Corpus,[0],[0]
Consider the following dialogue:,3.1 Corpus,[0],[0]
Interviewer: What is your mother’s job?,3.1 Corpus,[0],[0]
Interviewee:,3.1 Corpus,[0],[0]
My mother is a doctor (F).,3.1 Corpus,[0],[0]
"She has always worked very late hours and I felt neglected as a child (T).
",3.1 Corpus,[0],[0]
Is the interviewee response true or false?,3.1 Corpus,[0],[0]
We differentiate between global and local deception.,3.1 Corpus,[0],[0]
"Globally, the response to the question is deceptive.",3.1 Corpus,[0],[0]
"However, it contains local instances of both truth and deception.",3.1 Corpus,[0],[0]
"In this work we focus on dialoguebased deception, using global deception labels.",3.1 Corpus,[0],[0]
"Previous work with the CXD corpus has focused on IPU-level and turn-level analysis and classification of local deception, mostly with acousticprosodic features (Levitan et al., 2015b; Mendels et al., 2017).",3.2 Global Segmentation,[0],[0]
Here we are interested in exploring global deception at the dialogue-level for the first time in this corpus.,3.2 Global Segmentation,[0],[0]
We define response-segments as sets of turns that are related to a single question (of the 24 interview questions).,3.2 Global Segmentation,[0],[0]
"In order to annotate these segments, we first used a question detection and identification system (Maredia et al., 2017) that uses word embeddings to match semantically similar variations of questions to a target question list.",3.2 Global Segmentation,[0],[0]
This was necessary because interviewers asked the 24 questions using different wording from the original list of questions.,3.2 Global Segmentation,[0],[0]
"On this corpus, (Maredia et al., 2017) obtained an F1score of .95%.
",3.2 Global Segmentation,[0],[0]
"After tagging interviewer turns with this system, we labeled the set of interviewee turns between two interviewer questions q1 and q2 as corresponding to question q1.",3.2 Global Segmentation,[0],[0]
"The intuition behind this was that those turns were responses to follow up questions related to q1, and while the question detection and identification system discussed above did not identify follow up questions, we found that most of the follow up questions after an interviewer question q1 would be related to q1 in our hand annotation.",3.2 Global Segmentation,[0],[0]
"We evaluated this global segmentation on a hand-annotated test set of 17 interviews (about 10% of the corpus) consisting of 2,671 interviewee turns, 408 interviewer questions, and 977 follow up questions.",3.2 Global Segmentation,[0],[0]
"Our global segmentation approach resulted in 77.8% accuracy on our hand-labeled test set (errors were mostly
due to turns that were unrelated to any question).",3.2 Global Segmentation,[0],[0]
"We performed our analysis and classification on two segmentations of the data using this tagging method: (1) first turn: we analyzed only the single interviewee turn directly following the original question, and (2) multiple turns we analyzed the entire segment of interviewee turns that were responding to the original interviewer question and subsequent follow-up questions.",3.2 Global Segmentation,[0],[0]
"In our classification experiments, we explore whether a deceptive answer is be better classified by the interviewee’s initial response or by all of the follow-up conversation between interviewer and interviewee.",3.2 Global Segmentation,[0],[0]
"LIWC Previous work has found that deceivers tend to use different word usage patterns when they are lying (Newman et al., 2003).",4 Features,[0],[0]
"We used LIWC (Pennebaker et al., 2001) to extract semantic features from each utterance.",4 Features,[0],[0]
LIWC is a text analysis program that computes features consisting of normalized word counts for 93 semantic classes.,4 Features,[0],[0]
"LIWC dimensions have been used in many studies to predict outcomes including personality (Pennebaker and King, 1999), deception (Newman et al., 2003), and health (Pennebaker et al., 1997).",4 Features,[0],[0]
"We extracted a total of 93 features using LIWC 2015 2, including standard linguistic dimensions (e.g. percentage of words that are pronouns, articles), markers of psychological processes (e.g. affect, social, cognitive), punctuation categories (e.g periods, commas), and formality measures (e.g. fillers, swear words).",4 Features,[0],[0]
"Linguistic We extracted 23 linguistic features 3 which we adopted from previous deception studies such as (Enos, 2009; Bachenko et al., 2008).",4 Features,[0],[0]
"Included in this list are binary and numeric features capturing hedge words, filled pauses, laughter, complexity, contractions, and denials.",4 Features,[0],[0]
"We include Dictionary of Affect Language (DAL) (Whissell et al., 1986) scores that measure the emotional meaning of texts, and a specificity score which measures level of detail (Li and Nenkova, 2015).",4 Features,[0],[0]
"The full list of features is: ’hasAbsolutelyReally’, ’hasContraction’, ’hasI’, ’hasWe’, ’hasYes’, ’hasNAposT’ (turns
2A full description of the features is found here: https: //s3-us-west-2.amazonaws.com/downloads.",4 Features,[0],[0]
"liwc.net/LIWC2015_OperatorManual.pdf
3A detailed explanation of these linguistic features and how they were computed is found here: http://www.cs.",4 Features,[0],[0]
"columbia.edu/speech/cxd/features.html
that contain words with the contraction ”n’t”), ’hasNo’, ’hasNot’, ’isJustYes’, ’isJustNo’, ’noYesOrNo’, ’specificDenial’, ’thirdPersonPronouns’, ’hasFalseStart’, ’hasFilledPause’, ’numFilledPauses’, ’hasCuePhrase’, ’numCuePhrases’, ’hasHedgePhrase’, ’numHedgePhrases’, ’hasLaugh’, ’complexity’, ’numLaugh’, ’DALwc’, ’DAL-pleasant’, ’DAL-activate’, ’DALimagery’, ’specScores’ (specificity score).",4 Features,[0],[0]
"Response Length Previous work has found that response length, in seconds, is shorter in deceptive speech, and that the difference in number of words in a segment of speech is insignificant between deceptive and truthful speech (DePaulo et al., 2003).",4 Features,[0],[0]
"For our question-level analysis, we used four different measures for response length: the total number of seconds of an interviewee responsesegment, the total number of words in an interviewee response-segment, the average response time of a turn in an interviewee response-segment, and the average number of words per turn in an interviewee response-segment.",4 Features,[0],[0]
Individual Traits,4 Features,[0],[0]
We analyzed gender and native language of the speakers to determine if these traits were related to ability to deceive and to detect deception.,4 Features,[0],[0]
"We also analyzed linguistic cues to deception across gender and native language, and used gender and native language information in our classification experiments.",4 Features,[0],[0]
"All speakers were either male or female, and their native language was either Standard American English or Mandarin Chinese.",4 Features,[0],[0]
"In addition, we used the NEO-FFI (5 factor) personality inventory scores as features in classification experiments, but not for the statistical analysis in this paper.",4 Features,[0],[0]
Follow-up Questions Follow-up questions are questions that an interviewer asks after they ask a question from the original prescribed set of questions.,4 Features,[0],[0]
"We hypothesized that if an interviewer asked more follow-up questions, they were more likely to identify deceptive responses, because asking follow-up questions indicated interviewer doubt of the interviewee’s truthfulness.",4 Features,[0],[0]
"For each interviewee response-segment, we counted the number of follow-up questions interviewees were asked by the interviewer.",4 Features,[0],[0]
"In order to analyze the differences between deceptive and truthful speech, we extracted the above features from each question response-segment,
and calculated a series of paired t-tests between the features of truthful speech and deceptive speech.",5 Analysis,[0],[0]
All tests for significance correct for family-wise Type I error by controlling the false discovery rate (FDR) at α = 0.05.,5 Analysis,[0],[0]
The kth smallest p value is considered significant if it is less than k∗α n .,5 Analysis,[0],[0]
Table 1 shows the features that were statistically significant indicators of truth and deception in interviewee response-segments consisting of multiple turns.,5.1 Interviewee Responses,[0],[0]
"Below, we highlight some interesting findings.
",5.1 Interviewee Responses,[0],[0]
"In contrast to (DePaulo et al., 2003), we found that the total duration of an interviewee responsesegment was longer for deceptive speech than for truthful speech.",5.1 Interviewee Responses,[0],[0]
"Additionally, while (DePaulo et al., 2003) showed that the number of words in a segment of speech was not significantly different between deceptive and truthful speech, we found that deceptive response-segments had more words than truthful response-segments.",5.1 Interviewee Responses,[0],[0]
"Furthermore, we found that longer average response time per turn and more words per sentence were significant indicators of deception.",5.1 Interviewee Responses,[0],[0]
"These results show that when interviewees are trying to deceive, not only is their aggregate response longer in duration and number of words, but their individual responses to each follow-up question are also longer.",5.1 Interviewee Responses,[0],[0]
"Consistent with (DePaulo et al., 2003), we found that more filled pauses in an interviewee responsesegment was a significant indicator of deception.",5.1 Interviewee Responses,[0],[0]
"Deceivers are hypothesized to experience an increase in cognitive load (Vrij et al., 1996), and this can result in difficulties in speech planning, which can be signaled by filled pauses.",5.1 Interviewee Responses,[0],[0]
"Although (Benus et al., 2006) found that, in general, the use of pauses correlates more with truthful than with deceptive speech, we found that filled pauses such as ”um” were correlated with deceptive speech.",5.1 Interviewee Responses,[0],[0]
"The LIWC cogproc (cognitive processes) dimension, which includes words such as ”cause”, ”know”, ”ought” was significantly more frequent in truthful speech, also supporting the theory that cognitive load is increased while practicing deception.
",5.1 Interviewee Responses,[0],[0]
"We found that increased DALimagery scores, which compute words often used in speech to create vivid descriptions, were indicators of deception.",5.1 Interviewee Responses,[0],[0]
"We also found that the LIWC language summary variables of authenticity and adjectives
were indicators of deception: in an effort to sound more truthful and authentic, interviewees may have provided a level of detail that is uncharacteristic of truthful speech.",5.1 Interviewee Responses,[0],[0]
"Similarly, the specificity metric was indicative of deception: deceptive responses contained more detailed language.",5.1 Interviewee Responses,[0],[0]
"Words in the LIWC clout category - a category describing words that indicate power of influence - were more prevalent in deceptive responses, suggesting that subjects sounded more confident while lying.",5.1 Interviewee Responses,[0],[0]
Interrogatives were an indicator of deception.,5.1 Interviewee Responses,[0],[0]
"In the context of the interviewerinterviewee paradigm, these are interviewee questions to the interviewer.",5.1 Interviewee Responses,[0],[0]
"Perhaps this was a technique used to stall so that they had more time to
develop an answer (e.g. ”Can you repeat the question?”), or to deflect the interviewer’s attention from their deception and put the interviewer on the spot.",5.1 Interviewee Responses,[0],[0]
"We observed that hedge words and phrases, which speakers use to distance themselves from a proposition, were more frequent in deceptive speech.",5.1 Interviewee Responses,[0],[0]
"This is consistent with Statement Analysis (Adams, 1996), which posits that hedge words are used in deceptive statements to intentionally create vagueness that obscures facts.",5.1 Interviewee Responses,[0],[0]
"Consistent with this finding, certainty in language (words such as ”always” or ”never”) was a strong indicator of truthfulness.
",5.1 Interviewee Responses,[0],[0]
"It is also interesting to note the features that were not significant indicators of truth or decep-
tion.",5.1 Interviewee Responses,[0],[0]
"For example, there was no significant difference in laughter frequency or apostrophes (used for contractions in this corpus) between truthful and deceptive responses.
",5.1 Interviewee Responses,[0],[0]
"When we compared indicators of truth vs. deception across multiple turns to indicators of truth vs. deception in just the first turns of interviewee response-segments, we found that, generally, indicators in first turns are a subset of indicators across multiple turns.",5.1 Interviewee Responses,[0],[0]
In some cases there were interesting differences.,5.1 Interviewee Responses,[0],[0]
"For example, although tone (emotional tone - higher numbers indicate more positive, and lower indicate negative) was not a significant indicator of deception for the entire interviewee response-segment, negative tone was a moderate indicator of deception in first turns.",5.1 Interviewee Responses,[0],[0]
"This suggests that the tone of interviewees, when they have just started their lie, is different from when they are given the opportunity to expand on that lie.",5.1 Interviewee Responses,[0],[0]
The findings from our analysis of first turns suggest that there might be enough information in the first response alone to distinguish between deceptive and truthful speech; we test this in our classification experiments in Section 6.,5.1 Interviewee Responses,[0],[0]
"In addition to analyzing the linguistic differences between truthful and deceptive speech, we were interested in studying the characteristics of speech that is believed or disbelieved.",5.2 Interviewer Judgments of Deception,[0],[0]
"Since the CXD corpus includes interviewer judgments of deception for each question asked, we have the unique opportunity to study human perception of deception on a large scale.",5.2 Interviewer Judgments of Deception,[0],[0]
Table 2 shows the features that were statistically significant indicators of truth and deception in interviewee responses - consisting of multiple turns - that were perceived as true or false by interviewers.,5.2 Interviewer Judgments of Deception,[0],[0]
Here we highlight some interesting findings.,5.2 Interviewer Judgments of Deception,[0],[0]
"There were many features that were prevalent in speech that interviewers perceived as deceptive, which were in fact cues to deception.",5.2 Interviewer Judgments of Deception,[0],[0]
"For example, speech containing more words in a response-segment and more words per sentence was generally perceived as deceptive by interviewers, and indeed, this perception was correct.",5.2 Interviewer Judgments of Deception,[0],[0]
"Disbelieved answers had a greater frequency of filled pauses and hedge words, and greater specificity, all of which were increased in deceptive speech.
",5.2 Interviewer Judgments of Deception,[0],[0]
"There were also several features that were indicators of deception, but were not found in higher rates in statements that were perceived
as false.",5.2 Interviewer Judgments of Deception,[0],[0]
"For example, the LIWC dimensions clout and certain were not significantly different in believed vs. disbelieved interviewee responses, but clout was increased in deceptive speech and certain language was increased in truthful speech.",5.2 Interviewer Judgments of Deception,[0],[0]
"There were also features that were significantly different between believed and disbelieved statements, but were not indicators of deception.",5.2 Interviewer Judgments of Deception,[0],[0]
"For example, statements that were perceived as false by interviewers had a greater proportion of specificDenials (e.g. ”I did not”) than those that were perceived as true; this was not a valid cue to deception.",5.2 Interviewer Judgments of Deception,[0],[0]
Number of turns was increased in dialogue segments where the interviewer did not ultimately believe the interviewee response.,5.2 Interviewer Judgments of Deception,[0],[0]
"That is, more follow up questions were asked when an interviewer did not believe their interlocutor’s response, which is an intuitive behavior.",5.2 Interviewer Judgments of Deception,[0],[0]
"When we compared indicators of speech that was perceived as deceptive across multiple turns to indicators of speech that was perceived as deceptive in just the first turns, we found that, generally, indicators in first turns are a subset of indicators across multiple turns.
",5.2 Interviewer Judgments of Deception,[0],[0]
"On average, human accuracy at judging truth and deception in the CXD corpus was 56.75%, and accuracy at judging deceptive statements only was 47.93%.",5.2 Interviewer Judgments of Deception,[0],[0]
The average F1-score for humans was 46.,5.2 Interviewer Judgments of Deception,[0],[0]
"Thus, although some cues were correctly perceived by interviewers, humans were generally poor at deception perception.",5.2 Interviewer Judgments of Deception,[0],[0]
"Nonetheless, characterizing the nature of speech that is believed or not believed is useful for applications where we would ultimately like to synthesize speech that is trustworthy.",5.2 Interviewer Judgments of Deception,[0],[0]
"Having discovered many differences between deceptive and truthful language across all speakers, we were interested in analyzing differences in deceptive language across groups of speakers.",5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
"Using gender and native language (English or Mandarin Chinese) as group traits, we conducted two types of analysis.",5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
"First, we directly compared deception performance measures (ability to deceive as interviewee, and ability to detect deception as interviewer) between speakers with different traits, to assess the effect of individual characteristics on deception abilities.",5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
"In addition, we compared the features of deceptive and truthful language in sub-
sets of the corpus, considering only people with a particular trait, in order to determine groupspecific patterns of deceptive language.",5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
"As before, tests for significance correct for family-wise Type I error by controlling the false discovery rate (FDR) at α = 0.05.",5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
The kth smallest p value is considered significant if it is less than k∗αn .,5.3 Gender and Native Language Differences in Deception Behavior,[0],[0]
There were no significant differences in deception ability between male and female speakers.,5.3.1 Gender,[0],[0]
"However, there were many differences in language between male and female speakers.",5.3.1 Gender,[0],[0]
"Further, some features were only discriminative between deception and truth for a specific gender.",5.3.1 Gender,[0],[0]
"Table 3 shows linguistic features that were significantly different between truthful and deceptive speech, but only for one gender.",5.3.1 Gender,[0],[0]
"In some cases the feature was found in different proportions in male and females, and in other cases there was no significant difference.",5.3.1 Gender,[0],[0]
"For example, family words were indicative of deception only in female speakers, and these words were also used more frequently by female speakers than male speakers.
",5.3.1 Gender,[0],[0]
"The LIWC category of compare was also indicative of deception for females only, and this feature was generally found more frequently in female speech.",5.3.1 Gender,[0],[0]
"Article usage was only significantly different between truthful and deceptive speech in females (more articles were found in deceptive speech), but articles were used more frequently in male speech.",5.3.1 Gender,[0],[0]
"On the other hand, the LIWC category of posemo (positive emotion) was increased in truthful speech for male speakers only, and there
was no significant difference of posemo frequency across gender.",5.3.1 Gender,[0],[0]
"Interviewees were more successful at deceiving native Chinese speakers than at deceiving native English speakers (t(170) = −2.13, p = 0.033).",5.3.2 Native Language,[0],[0]
"This was true regardless of interviewee gender and native language, and slightly stronger for female interviewers (t(170) = −2.22, p = 0.027).",5.3.2 Native Language,[0],[0]
"When considering only female interviewers, interviewees were more successful at deceiving nonnative speakers than native speakers, but this difference was not significant when considering only male interviewers.",5.3.2 Native Language,[0],[0]
"As with gender, there were several features that were discriminative between deception and truth for only native speakers of English, or only native speakers of Mandarin.",5.3.2 Native Language,[0],[0]
"Table 3 shows LIWC categories and their relation to deception, broken down by native language.",5.3.2 Native Language,[0],[0]
"For example, power words were found more frequently in deception statements, when considering native English speakers only.",5.3.2 Native Language,[0],[0]
"In general, power words were used more by native Mandarin speakers than by native English speakers.",5.3.2 Native Language,[0],[0]
"LIWC categories of compare, relative, and swear were more prevalent in deceptive speech, only for English speakers.",5.3.2 Native Language,[0],[0]
"On the other hand, feel and perception dimensions were only indicators of deception for native Mandarin speakers, although there was no significant difference in the use of these word categories across native language.",5.3.2 Native Language,[0],[0]
"Informal and netspeak word dimensions tended to be more frequent in truthful speech for native Chinese speakers only (approaching significance), and these word categories were generally more frequent in native Mandarin speech.",5.3.2 Native Language,[0],[0]
"Finally, filler words tended to be more frequent in deceptive speech (approaching significance) only for native Mandarin speakers, and these were used more frequently by native Mandarin speakers than native English speakers.
",5.3.2 Native Language,[0],[0]
"Overall, our findings suggest that deceptive behavior in general, and deceptive language in particular, are affected by a person’s individual characteristics, including gender and native language.",5.3.2 Native Language,[0],[0]
"When building a deception classification system, it is important to account for this variation across speaker groups.",5.3.2 Native Language,[0],[0]
"Motivated by our analysis showing many significant differences in the language of truthful and deceptive responses to interview questions, we trained machine learning classifiers to automatically distinguish between truthful and deceptive text, using the feature sets described in section 4.",6 Deception Classification,[0],[0]
We compared classification performance for the two segmentation methods described in section 3.2: first turn and multiple turns.,6 Deception Classification,[0],[0]
This allowed us to explore the role of context in automatic deception detection.,6 Deception Classification,[0],[0]
"When classifying interviewee response-segments, should the immediate response only be used for classification, or is inclusion of surrounding turns helpful?",6 Deception Classification,[0],[0]
"This has implications not only for deception classification, but for practitioners as well.",6 Deception Classification,[0],[0]
"Should human interviewers make use of responses to follow up questions when determining response veracity, or should the initial response receive the most consideration?
",6 Deception Classification,[0],[0]
"We compared the performance of 3 classification algorithms: Random Forest, Logistic Regression, and SVM (sklearn implementation).",6 Deception Classification,[0],[0]
"In total, there were 7,792 question segments for both single turn and multiple turns segmentations.",6 Deception Classification,[0],[0]
"We divided this into 66% train and 33% test, and used the same fixed test set in experiments for both segmentations in order to directly compare results.",6 Deception Classification,[0],[0]
"The random baseline performance is 50, since the dataset is balanced for truthful and deceptive statements.",6 Deception Classification,[0],[0]
"Another baseline is human performance, which is 46.0 F1 in this corpus.",6 Deception Classification,[0],[0]
"The Random For-
est classifier was consistently the best performing, and we only report those results due to space constraints.",6 Deception Classification,[0],[0]
"Table 4 displays the classification performance for each feature set individually, as well as feature combinations, for both single turn and multiple turn segmentations.",6 Deception Classification,[0],[0]
"It also shows the human baseline performance, obtained from the interviewers’ judgments of deception in the corpus, which were made after asking each question along with related follow-up questions (i.e. multiple turn segmentation).
",6 Deception Classification,[0],[0]
The best performance (72.74 F1-score) was obtained using LIWC features extracted from multiple turns.,6 Deception Classification,[0],[0]
"This is a 22.74% absolute increase over the random baseline of 50, and a 26.74% absolute increase over the human baseline of 46.",6 Deception Classification,[0],[0]
"The performance of classifiers trained on multiple turns was consistently better than those trained on single turns, for all feature sets.",6 Deception Classification,[0],[0]
"For multiple turns, LIWC features were better than the lexical feature set, and combining lexical with LIWC features did not improve over the performance of LIWC features alone.",6 Deception Classification,[0],[0]
Adding individual traits information was also not beneficial.,6 Deception Classification,[0],[0]
"However, when considering the first turn only, the best results (70.87 F1-score) were obtained using a combination of LIWC+lexical+individual features.",6 Deception Classification,[0],[0]
"Using the first turns segmentation, lexical features were slightly better than LIWC features, and interestingly, adding individual traits helped both feature sets.",6 Deception Classification,[0],[0]
"A combination of LIWC and lexical features was better than each on its own.
",6 Deception Classification,[0],[0]
"These results suggest that contextual informa-
tion, in the form of follow up questions, is beneficial for deception classification.",6 Deception Classification,[0],[0]
"It seems that individual traits, including gender, native language, and personality scores, are helpful in deception classification under the condition where contextual information is not available.",6 Deception Classification,[0],[0]
"When the contextual information is available, the the additional lexical content is more useful than individual traits.",6 Deception Classification,[0],[0]
In this paper we presented a study of deceptive language in interview dialogues.,7 Conclusions and Future Work,[0],[0]
Our analysis of linguistic characteristics of deceptive and truthful speech provides insight into the nature of deceptive language.,7 Conclusions and Future Work,[0],[0]
"We also analyzed the linguistic characteristics of speech that is perceived as deceptive and truthful, which is important for understanding the nature of trustworthy speech.",7 Conclusions and Future Work,[0],[0]
"We explored variation across gender and native language in linguistic cues to deception, highlighting cues that are specific to particular groups of speakers.",7 Conclusions and Future Work,[0],[0]
We built classifiers that use combinations of linguistic features and individual traits to automatically identify deceptive speech.,7 Conclusions and Future Work,[0],[0]
"We compared the performance of using cues from the single first turn of an interviewee response-segment with using cues from the full context of multiple interviewee turns, achieving performance as high as 72.74% F1-score (about 27% better than human detection performance).
",7 Conclusions and Future Work,[0],[0]
"This work contributes to the critical problem of automatic deception detection, and increases our scientific understanding of deception, deception perception, and individual differences in deceptive behavior.",7 Conclusions and Future Work,[0],[0]
"In future work, we plan to conduct similar analysis in additional deception corpora in other domains, in order to identify consistent domain-independent deception indicators.",7 Conclusions and Future Work,[0],[0]
"In addition, we plan to conduct cross-corpus machine learning experiments, to evaluate the robustness of these and other feature sets in deception detection.",7 Conclusions and Future Work,[0],[0]
"We also would like to explore additional feature combinations, such as adding acoustic-prosodic features.",7 Conclusions and Future Work,[0],[0]
"Finally, we plan to conduct an empirical analysis of deception behavior across personality types.",7 Conclusions and Future Work,[0],[0]
"This work was partially funded by AFOSR FA9550-11-1-0120 and by NSF DGE-11-44155.
",Acknowledgments,[0],[0]
Thank you to Bingyan Hu for her assistance with feature extraction.,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
We explore deception detection in interview dialogues.,abstractText,[0],[0]
We analyze a set of linguistic features in both truthful and deceptive responses to interview questions.,abstractText,[0],[0]
"We also study the perception of deception, identifying characteristics of statements that are perceived as truthful or deceptive by interviewers.",abstractText,[0],[0]
"Our analysis show significant differences between truthful and deceptive question responses, as well as variations in deception patterns across gender and native language.",abstractText,[0],[0]
This analysis motivated our selection of features for machine learning experiments aimed at classifying globally deceptive speech.,abstractText,[0],[0]
"Our best classification performance is 72.74 F1-Score (about 27% better than human performance), which is achieved using a combination of linguistic features and individual traits.",abstractText,[0],[0]
Linguistic Cues to Deception and Perceived Deception in Interview Dialogues,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5027–5038 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
5027",text,[0],[0]
"Semantic role labeling (SRL) extracts a high-level representation of meaning from a sentence, labeling e.g. who did what to whom.",1 Introduction,[0],[0]
"Explicit representations of such semantic information have been
shown to improve results in challenging downstream tasks such as dialog systems (Tur et al., 2005; Chen et al., 2013), machine reading (Berant et al., 2014; Wang et al., 2015) and translation (Liu and Gildea, 2010; Bazrafshan and Gildea, 2013).
",1 Introduction,[0],[0]
"Though syntax was long considered an obvious prerequisite for SRL systems (Levin, 1993; Punyakanok et al., 2008), recently deep neural network architectures have surpassed syntacticallyinformed models (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017; Tan et al., 2018; He et al., 2018), achieving state-of-the art SRL performance with no explicit modeling of syntax.",1 Introduction,[0],[0]
"An additional benefit of these end-to-end models is that they require just raw tokens and (usually) detected predicates as input, whereas richer linguistic features typically require extraction by an auxiliary pipeline of models.
",1 Introduction,[0],[0]
"Still, recent work (Roth and Lapata, 2016; He et al., 2017; Marcheggiani and Titov, 2017) indicates that neural network models could see even higher accuracy gains by leveraging syntactic information rather than ignoring it.",1 Introduction,[0],[0]
"He et al. (2017) indicate that many of the errors made by a syntaxfree neural network on SRL are tied to certain syntactic confusions such as prepositional phrase attachment, and show that while constrained inference using a relatively low-accuracy predicted parse can provide small improvements in SRL accuracy, providing a gold-quality parse leads to substantial gains.",1 Introduction,[0],[0]
"Marcheggiani and Titov (2017) incorporate syntax from a high-quality parser (Kiperwasser and Goldberg, 2016) using graph convolutional neural networks (Kipf and Welling, 2017), but like He et al. (2017) they attain only small increases over a model with no syntactic parse, and even perform worse than a syntax-free model on out-of-domain data.",1 Introduction,[0],[0]
"These works suggest that though syntax has the potential to improve neural network SRL models, we have not
yet designed an architecture which maximizes the benefits of auxiliary syntactic information.
",1 Introduction,[0],[0]
"In response, we propose linguistically-informed self-attention (LISA): a model that combines multi-task learning (Caruana, 1993) with stacked layers of multi-head self-attention (Vaswani et al., 2017); the model is trained to: (1) jointly predict parts of speech and predicates; (2) perform parsing; and (3) attend to syntactic parse parents, while (4) assigning semantic role labels.",1 Introduction,[0],[0]
"Whereas prior work typically requires separate models to provide linguistic analysis, including most syntaxfree neural models which still rely on external predicate detection, our model is truly end-to-end: earlier layers are trained to predict prerequisite parts-of-speech and predicates, the latter of which are supplied to later layers for scoring.",1 Introduction,[0],[0]
"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL, we more efficiently encode each sentence only once, predict its predicates, part-of-speech tags and labeled syntactic parse, then predict the semantic roles for all predicates in the sentence in parallel.",1 Introduction,[0],[0]
"The model is trained such that, as syntactic parsing models improve, providing high-quality parses at test time will improve its performance, allowing the model to leverage updated parsing models without requiring re-training.
",1 Introduction,[0],[0]
In experiments on the CoNLL-2005 and CoNLL-2012 datasets we show that our linguistically-informed models out-perform the syntax-free state-of-the-art.,1 Introduction,[0],[0]
"On CoNLL-2005 with predicted predicates and standard word embeddings, our single model out-performs the previous state-of-the-art model on the WSJ test set by 2.5 F1 points absolute.",1 Introduction,[0],[0]
"On the challenging out-of-domain Brown test set, our model improves substantially over the previous state-of-the-art by more than 3.5 F1, a nearly 10% reduction in error.",1 Introduction,[0],[0]
"On CoNLL-2012, our model gains more than 2.5 F1 absolute over the previous state-of-the-art.",1 Introduction,[0],[0]
"Our models also show improvements when using contextually-encoded word representations (Peters et al., 2018), obtaining nearly 1.0 F1 higher than the state-of-the-art on CoNLL-2005 news and more than 2.0 F1 improvement on out-of-domain text.1
1Our implementation in TensorFlow (Abadi et al., 2015) is available at : http://github.com/strubell/",1 Introduction,[0],[0]
LISA,1 Introduction,[0],[0]
Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform endto-end SRL.,2 Model,[0],[0]
"LISA achieves this by combining: (1) A new technique of supervising neural attention to predict syntactic dependencies with (2) multi-task learning across four related tasks.
",2 Model,[0],[0]
Figure 1 depicts the overall architecture of our model.,2 Model,[0],[0]
"The basis for our model is the Transformer encoder introduced by Vaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked multi-head self-attention and feedforward layers (§2.1).
",2 Model,[0],[0]
"To incorporate syntax, one self-attention head is trained to attend to each token’s syntactic parent, allowing the model to use this attention head as an oracle for syntactic dependencies.",2 Model,[0],[0]
"We introduce this syntactically-informed self-attention (Figure 2) in more detail in §2.2.
",2 Model,[0],[0]
Our model is designed for the more realistic setting in which gold predicates are not provided at test-time.,2 Model,[0],[0]
"Our model predicts predicates and integrates part-of-speech (POS) information into earlier layers by re-purposing representations closer to the input to predict predicate and POS tags us-
ing hard parameter sharing (§2.3).",2 Model,[0],[0]
"We simplify optimization and benefit from shared statistical strength derived from highly correlated POS and predicates by treating tagging and predicate detection as a single task, performing multi-class classification into the joint Cartesian product space of POS and predicate labels.
",2 Model,[0],[0]
"Though typical models, which re-encode the sentence for each predicate, can simplify SRL to token-wise tagging, our joint model requires a different approach to classify roles with respect to each predicate.",2 Model,[0],[0]
"Contextually encoded tokens are projected to distinct predicate and role embeddings (§2.4), and each predicted predicate is scored with the sequence’s role representations using a bilinear model (Eqn. 6), producing per-label scores for BIO-encoded semantic role labels for each token and each semantic frame.
",2 Model,[0],[0]
The model is trained end-to-end by maximum likelihood using stochastic gradient descent (§2.5).,2 Model,[0],[0]
"The basis for our model is a multi-head selfattention token encoder, recently shown to achieve state-of-the-art performance on SRL (Tan et al., 2018), and which provides a natural mechanism
for incorporating syntax, as described in §2.2.",2.1 Self-attention token encoder,[0],[0]
"Our implementation replicates Vaswani et al. (2017).
",2.1 Self-attention token encoder,[0],[0]
The input to the network is a sequence X of T token representations xt.,2.1 Self-attention token encoder,[0],[0]
"In the standard setting these token representations are initialized to pretrained word embeddings, but we also experiment with supplying pre-trained ELMo representations combined with task-specific learned parameters, which have been shown to substantially improve performance of other SRL models (Peters et al., 2018).",2.1 Self-attention token encoder,[0],[0]
"For experiments with gold predicates, we concatenate a predicate indicator embedding pt following previous work (He et al., 2017).
",2.1 Self-attention token encoder,[0],[0]
We project2 these input embeddings to a representation that is the same size as the output of the self-attention layers.,2.1 Self-attention token encoder,[0],[0]
"We then add a positional encoding vector computed as a deterministic sinusoidal function of t, since the self-attention has no innate notion of token position.
",2.1 Self-attention token encoder,[0],[0]
We feed this token representation as input to a series of J residual multi-head self-attention layers with feed-forward connections.,2.1 Self-attention token encoder,[0],[0]
"Denoting the jth self-attention layer as T (j)(·), the output of that layer s(j)t , and LN(·) layer normalization, the following recurrence applied to initial input c(p)t :
s (j) t = LN(s (j 1) t + T (j)(s(j 1)t ))",2.1 Self-attention token encoder,[0],[0]
"(1)
gives our final token representations s(j)t .",2.1 Self-attention token encoder,[0],[0]
Each T (j)(·) consists of: (a) multi-head self-attention and (b) a feed-forward projection.,2.1 Self-attention token encoder,[0],[0]
"The multi-head self attention consists of H attention heads, each of which learns a distinct attention function to attend to all of the tokens in the sequence.",2.1 Self-attention token encoder,[0],[0]
"This self-attention is performed for each token for each head, and the results of the H self-attentions are concatenated to form the final self-attended representation for each token.
",2.1 Self-attention token encoder,[0],[0]
"Specifically, consider the matrix S(j 1) of T token representations at layer j 1.",2.1 Self-attention token encoder,[0],[0]
"For each attention head h, we project this matrix into distinct key, value and query representations K(j)h , V (j) h and Q(j)h of dimensions T⇥dk, T⇥dq, and T⇥dv, respectively.",2.1 Self-attention token encoder,[0],[0]
We can then multiply Q(j)h by K (j) h to obtain a T ⇥ T matrix of attention weights A(j)h between each pair of tokens in the sentence.,2.1 Self-attention token encoder,[0],[0]
"Following Vaswani et al. (2017) we perform scaled dot-product attention: We scale the weights by the inverse square root of their embedding dimension
2All linear projections include bias terms, which we omit in this exposition for the sake of clarity.
and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence:
A (j) h = softmax(d 0.5 k Q (j) h K (j) h
T ) (2)
",2.1 Self-attention token encoder,[0],[0]
"These attention weights are then multiplied by V
(j) h for each token to obtain the self-attended to-
ken representations M (j)h :
M (j) h = A (j) h V (j) h (3)
Row t of M (j)h , the self-attended representation for token t at layer j, is thus the weighted sum with respect to t (with weights given by A(j)h ) over the token representations in V (j)h .
",2.1 Self-attention token encoder,[0],[0]
"The outputs of all attention heads for each token are concatenated, and this representation is passed to the feed-forward layer, which consists of two linear projections each followed by leaky ReLU activations (Maas et al., 2013).",2.1 Self-attention token encoder,[0],[0]
"We add the output of the feed-forward to the initial representation and apply layer normalization to give the final output of self-attention layer j, as in Eqn. 1.",2.1 Self-attention token encoder,[0],[0]
"Typically, neural attention mechanisms are left on their own to learn to attend to relevant inputs.",2.2 Syntactically-informed self-attention,[0],[0]
"Instead, we propose training the self-attention to attend to specific tokens corresponding to the syntactic structure of the sentence as a mechanism for passing linguistic knowledge to later layers.
",2.2 Syntactically-informed self-attention,[0],[0]
"Specifically, we replace one attention head with the deep bi-affine model of Dozat and Manning (2017), trained to predict syntactic dependencies.",2.2 Syntactically-informed self-attention,[0],[0]
"Let Aparse be the parse attention weights, at layer i.",2.2 Syntactically-informed self-attention,[0],[0]
Its input is the matrix of token representations S (i 1).,2.2 Syntactically-informed self-attention,[0],[0]
"As with the other attention heads, we project S(i 1) into key, value and query representations, denoted Kparse, Qparse, Vparse.",2.2 Syntactically-informed self-attention,[0],[0]
"Here the key and query projections correspond to parent and dependent representations of the tokens, and we allow their dimensions to differ from the rest of the attention heads to more closely follow the implementation of Dozat and Manning (2017).",2.2 Syntactically-informed self-attention,[0],[0]
"Unlike the other attention heads which use a dot product to score key-query pairs, we score the compatibility between Kparse and Qparse using a bi-affine operator Uheads to obtain attention weights:
Aparse = softmax(QparseUheadsK T parse) (4)
These attention weights are used to compose a weighted average of the value representations Vparse as in the other attention heads.
",2.2 Syntactically-informed self-attention,[0],[0]
"We apply auxiliary supervision at this attention head to encourage it to attend to each token’s parent in a syntactic dependency tree, and to encode information about the token’s dependency label.",2.2 Syntactically-informed self-attention,[0],[0]
"Denoting the attention weight from token t to a candidate head q as Aparse[t, q], we model the probability of token t having parent q as:
P (q = head(t) | X ) = Aparse[t, q] (5)
using the attention weights Aparse[t] as the distribution over possible heads for token t.",2.2 Syntactically-informed self-attention,[0],[0]
We define the root token as having a self-loop.,2.2 Syntactically-informed self-attention,[0],[0]
"This attention head thus emits a directed graph3 where each token’s parent is the token to which the attention Aparse assigns the highest weight.
",2.2 Syntactically-informed self-attention,[0],[0]
"We also predict dependency labels using perclass bi-affine operations between parent and dependent representations Qparse and Kparse to produce per-label scores, with locally normalized probabilities over dependency labels ydept given by the softmax function.",2.2 Syntactically-informed self-attention,[0],[0]
"We refer the reader to Dozat and Manning (2017) for more details.
",2.2 Syntactically-informed self-attention,[0],[0]
"This attention head now becomes an oracle for syntax, denoted P , providing a dependency parse to downstream layers.",2.2 Syntactically-informed self-attention,[0],[0]
"This model not only predicts its own dependency arcs, but allows for the injection of auxiliary parse information at test time by simply setting Aparse to the parse parents produced by e.g. a state-of-the-art parser.",2.2 Syntactically-informed self-attention,[0],[0]
"In this way, our model can benefit from improved, external parsing models without re-training.",2.2 Syntactically-informed self-attention,[0],[0]
"Unlike typical multi-task models, ours maintains the ability to leverage external syntactic information.",2.2 Syntactically-informed self-attention,[0],[0]
We also share the parameters of lower layers in our model to predict POS tags and predicates.,2.3 Multi-task learning,[0],[0]
"Following He et al. (2017), we focus on the end-toend setting, where predicates must be predicted on-the-fly.",2.3 Multi-task learning,[0],[0]
"Since we also train our model to predict syntactic dependencies, it is beneficial to give the model knowledge of POS information.",2.3 Multi-task learning,[0],[0]
"While much previous work employs a pipelined approach to both POS tagging for dependency parsing and predicate detection for SRL, we take a multi-task learning (MTL) approach (Caruana,
3Usually the head emits a tree, but we do not enforce it here.
1993), sharing the parameters of earlier layers in our SRL model with a joint POS and predicate detection objective.",2.3 Multi-task learning,[0.9554010691051488],"['When encountering a new task, the learner samples a prior from the hyper posterior Q(P ), and then use it for learning.']"
"Since POS is a strong predictor of predicates4 and the complexity of training a multi-task model increases with the number of tasks, we combine POS tagging and predicate detection into a joint label space: For each POS tag TAG which is observed co-occurring with a predicate, we add a label of the form TAG:PREDICATE.
",2.3 Multi-task learning,[0],[0]
"Specifically, we feed the representation s(r)t from a layer r preceding the syntacticallyinformed layer p to a linear classifier to produce per-class scores rt for token t.",2.3 Multi-task learning,[0],[0]
"We compute locally-normalized probabilities using the softmax function: P (yprpt | X ) / exp(rt), where y prp t is a label in the joint space.",2.3 Multi-task learning,[0],[0]
Our final goal is to predict semantic roles for each predicate in the sequence.,2.4 Predicting semantic roles,[0],[0]
"We score each predicate against each token in the sequence using a bilinear operation, producing per-label scores for each token for each predicate, with predicates and syntax determined by oracles V and P .
",2.4 Predicting semantic roles,[0],[0]
"First, we project each token representation s(J)t to a predicate-specific representation spredt and a role-specific representation srolet .",2.4 Predicting semantic roles,[0],[0]
We then provide these representations to a bilinear transformation U for scoring.,2.4 Predicting semantic roles,[0],[0]
"So, the role label scores sft for the token at index t with respect to the predicate at index f (i.e. token t and frame f ) are given by:
sft = (s pred f )",2.4 Predicting semantic roles,[0],[0]
"T Us role t (6)
which can be computed in parallel across all semantic frames in an entire minibatch.",2.4 Predicting semantic roles,[0],[0]
"We calculate a locally normalized distribution over role labels for token t in frame f using the softmax function: P (yroleft | P,V,X ) / exp(sft).
",2.4 Predicting semantic roles,[0],[0]
"At test time, we perform constrained decoding using the Viterbi algorithm to emit valid sequences of BIO tags, using unary scores sft and the transition probabilities given by the training data.",2.4 Predicting semantic roles,[0],[0]
We maximize the sum of the likelihoods of the individual tasks.,2.5 Training,[0],[0]
"In order to maximize our model’s ability to leverage syntax, during training we clamp P to the gold parse (PG) and V to gold predicates VG when passing parse and predicate
4All predicates in CoNLL-2005 are verbs; CoNLL-2012 includes some nominal predicates.
representations to later layers, whereas syntactic head prediction and joint predicate/POS prediction are conditioned only on the input sequence X .",2.5 Training,[0],[0]
"The overall objective is thus:
1
T
TX
t=1
h FX
f=1
log P (yroleft | PG,VG,X )
+ log P",2.5 Training,[0],[0]
(,2.5 Training,[0],[0]
yprpt | X ),2.5 Training,[0],[0]
"+ 1 log P (head(t) | X )
",2.5 Training,[0],[0]
"+ 2 log P (y dep t | PG,X )
",2.5 Training,[0],[0]
"i (7)
where 1 and 2 are penalties on the syntactic attention loss.
",2.5 Training,[0],[0]
"We train the model using Nadam (Dozat, 2016)",2.5 Training,[0],[0]
SGD combined with the learning rate schedule in Vaswani et al. (2017).,2.5 Training,[0],[0]
"In addition to MTL, we regularize our model using dropout (Srivastava et al., 2014).",2.5 Training,[0],[0]
"We use gradient clipping to avoid exploding gradients (Bengio et al., 1994; Pascanu et al., 2013).",2.5 Training,[0],[0]
Additional details on optimization and hyperparameters are included in Appendix A.,2.5 Training,[0],[0]
"Early approaches to SRL (Pradhan et al., 2005; Surdeanu et al., 2007; Johansson and Nugues, 2008; Toutanova et al., 2008) focused on developing rich sets of linguistic features as input to a linear model, often combined with complex constrained inference e.g. with an ILP (Punyakanok et al., 2008). Täckström",3 Related work,[0],[0]
et al. (2015) showed that constraints could be enforced more efficiently using a clever dynamic program for exact inference.,3 Related work,[0],[0]
"Sutton and McCallum (2005) modeled syntactic parsing and SRL jointly, and Lewis et al. (2015) jointly modeled SRL and CCG parsing.
",3 Related work,[0],[0]
"Collobert et al. (2011) were among the first to use a neural network model for SRL, a CNN over word embeddings which failed to out-perform non-neural models.",3 Related work,[0],[0]
FitzGerald et al. (2015) successfully employed neural networks by embedding lexicalized features and providing them as factors in the model of Täckström,3 Related work,[0],[0]
"et al. (2015).
",3 Related work,[0],[0]
More recent neural models are syntax-free.,3 Related work,[0],[0]
"Zhou and Xu (2015), Marcheggiani et al. (2017) and He et al. (2017) all use variants of deep LSTMs with constrained decoding, while Tan et al. (2018) apply self-attention to obtain state-ofthe-art SRL with gold predicates.",3 Related work,[0],[0]
"Like this work, He et al. (2017) present end-to-end experiments, predicting predicates using an LSTM, and He et al.
(2018) jointly predict SRL spans and predicates in a model based on that of Lee et al. (2017), obtaining state-of-the-art predicted predicate SRL.",3 Related work,[0],[0]
"Concurrent to this work, Peters et al. (2018) and He et al. (2018) report significant gains on PropBank SRL by training a wide LSTM language model and using a task-specific transformation of its hidden representations (ELMo) as a deep, and computationally expensive, alternative to typical word embeddings.",3 Related work,[0],[0]
"We find that LISA obtains further accuracy increases when provided with ELMo word representations, especially on out-of-domain data.
",3 Related work,[0],[0]
Some work has incorporated syntax into neural models for SRL.,3 Related work,[0],[0]
"Roth and Lapata (2016) incorporate syntax by embedding dependency paths, and similarly Marcheggiani and Titov (2017) encode syntax using a graph CNN over a predicted syntax tree, out-performing models without syntax on CoNLL-2009.",3 Related work,[0],[0]
These works are limited to incorporating partial dependency paths between tokens whereas our technique incorporates the entire parse.,3 Related work,[0],[0]
"Additionally, Marcheggiani and Titov (2017) report that their model does not out-perform syntax-free models on out-of-domain data, a setting in which our technique excels.
",3 Related work,[0],[0]
"MTL (Caruana, 1993) is popular in NLP, and others have proposed MTL models which incorporate subsets of the tasks we do (Collobert et al., 2011; Zhang and Weiss, 2016; Hashimoto et al., 2017; Peng et al., 2017; Swayamdipta et al., 2017), and we build off work that investigates where and when to combine different tasks to achieve the best results (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017; Alonso and Plank, 2017).",3 Related work,[0],[0]
"Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference.
",3 Related work,[0],[0]
"The question of training on gold versus predicted labels is closely related to learning to search (Daumé III et al., 2009; Ross et al., 2011; Chang et al., 2015) and scheduled sampling (Bengio et al., 2015), with applications in NLP to sequence labeling and transition-based parsing (Choi and Palmer, 2011; Goldberg and Nivre, 2012; Ballesteros et al., 2016).",3 Related work,[0],[0]
"Our approach may be interpreted as an extension of teacher forcing (Williams and Zipser, 1989) to MTL.",3 Related work,[0],[0]
"We leave exploration of more advanced scheduled sampling techniques to
future work.",3 Related work,[0],[0]
"We present results on the CoNLL-2005 shared task (Carreras and Màrquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora.",4 Experimental results,[0],[0]
"We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fine-tuned task-specific parameters (Peters et al., 2018) in order to best compare to prior work.",4 Experimental results,[0],[0]
"Hyperparameters that resulted in the best performance on the validation set were selected via a small grid search, and models were trained for a maximum of 4 days on one TitanX GPU using early stopping on the validation set.",4 Experimental results,[0],[0]
"We convert constituencies to dependencies using the Stanford head rules v3.5 (de Marneffe and Manning, 2008).",4 Experimental results,[0],[0]
"A detailed description of hyperparameter settings and data pre-processing can be found in Appendix A.
We compare our LISA models to four strong baselines:",4 Experimental results,[0],[0]
"For experiments using predicted predicates, we compare to He et al. (2018) and the ensemble model (PoE) from He et al. (2017), as well as a version of our own self-attention model which does not incorporate syntactic information (SA).",4 Experimental results,[0],[0]
"To compare to more prior work, we present additional results on CoNLL-2005 with models given gold predicates at test time.",4 Experimental results,[0],[0]
"In these experiments we also compare to Tan et al. (2018), the previous state-of-the art SRL model using gold predicates and standard embeddings.
",4 Experimental results,[0],[0]
"We demonstrate that our models benefit from injecting state-of-the-art predicted parses at test time (+D&M) by fixing the attention to parses predicted by Dozat and Manning (2017), the winner of the 2017 CoNLL shared task (Zeman et al., 2017) which we re-train using ELMo embeddings.",4 Experimental results,[0],[0]
"In all cases, using these parses at test time improves performance.
",4 Experimental results,[0],[0]
"We also evaluate our model using the gold syntactic parse at test time (+Gold), to provide an upper bound for the benefit that syntax could have for SRL using LISA.",4 Experimental results,[0],[0]
"These experiments show that despite LISA’s strong performance, there remains substantial room for improvement.",4 Experimental results,[0],[0]
In §4.3 we perform further analysis comparing SRL models using gold and predicted parses.,4 Experimental results,[0],[0]
"Table 1 lists precision, recall and F1 on the CoNLL-2005 development and test sets using predicted predicates.",4.1 Semantic role labeling,[0],[0]
"For models using GloVe embeddings, our syntax-free SA model already achieves a new state-of-the-art by jointly predicting predicates, POS and SRL.",4.1 Semantic role labeling,[0],[0]
"LISA with its own parses performs comparably to SA, but when supplied with D&M parses LISA out-performs the previous state-of-the-art by 2.5 F1 points.",4.1 Semantic role labeling,[0],[0]
"On the out-ofdomain Brown test set, LISA also performs comparably to its syntax-free counterpart with its own parses, but with D&M parses LISA performs exceptionally well, more than 3.5 F1 points higher than He et al. (2018).",4.1 Semantic role labeling,[0],[0]
"Incorporating ELMo em-
beddings improves all scores.",4.1 Semantic role labeling,[0],[0]
"The gap in SRL F1 between models using LISA and D&M parses is smaller due to LISA’s improved parsing accuracy (see §4.2), but LISA with D&M parses still achieves the highest F1: nearly 1.0 absolute F1 higher than the previous state-of-the art on WSJ, and more than 2.0 F1 higher on Brown.",4.1 Semantic role labeling,[0],[0]
"In both settings LISA leverages domain-agnostic syntactic information rather than over-fitting to the newswire training data which leads to high performance even on out-of-domain text.
",4.1 Semantic role labeling,[0],[0]
To compare to more prior work we also evaluate our models in the artificial setting where gold predicates are provided at test time.,4.1 Semantic role labeling,[0],[0]
"For fair comparison we use GloVe embeddings, provide predicate indicator embeddings on the input and reencode the sequence relative to each gold predicate.",4.1 Semantic role labeling,[0],[0]
"Here LISA still excels: with D&M parses, LISA out-performs the previous state-of-the-art by more than 2 F1 on both WSJ and Brown.
",4.1 Semantic role labeling,[0],[0]
"Table 3 reports precision, recall and F1 on the CoNLL-2012 test set.",4.1 Semantic role labeling,[0],[0]
We observe performance similar to that observed on ConLL-2005:,4.1 Semantic role labeling,[0],[0]
Using GloVe embeddings our SA baseline already out-performs He et al. (2018) by nearly 1.5 F1.,4.1 Semantic role labeling,[0],[0]
"With its own parses, LISA slightly under-performs our syntax-free model, but when provided with stronger D&M parses LISA outperforms the state-of-the-art by more than 2.5 F1.",4.1 Semantic role labeling,[0],[0]
"Like CoNLL-2005, ELMo representations improve all models and close the F1 gap between models supplied with LISA and D&M parses.",4.1 Semantic role labeling,[0],[0]
"On this dataset ELMo also substantially narrows the
difference between models with- and without syntactic information.",4.1 Semantic role labeling,[0],[0]
"This suggests that for this challenging dataset, ELMo already encodes much of the information available in the D&M parses.",4.1 Semantic role labeling,[0],[0]
"Yet, higher accuracy parses could still yield improvements since providing gold parses increases F1 by 4 points even with ELMo embeddings.",4.1 Semantic role labeling,[0],[0]
"We first report the labeled and unlabeled attachment scores (LAS, UAS) of our parsing models on the CoNLL-2005 and 2012 test sets (Table 4) with GloVe (G) and ELMo (E) embeddings.","4.2 Parsing, POS and predicate detection",[0],[0]
D&M achieves the best scores.,"4.2 Parsing, POS and predicate detection",[0],[0]
"Still, LISA’s GloVe UAS is comparable to popular off-the-shelf dependency parsers such as spaCy,5 and with ELMo
5spaCy reports 94.48 UAS on WSJ using Stanford dependencies v3.3: https://spacy.io/usage/
embeddings comparable to the standalone D&M parser.","4.2 Parsing, POS and predicate detection",[0],[0]
"The difference in parse accuracy between LISAG and D&M likely explains the large increase in SRL performance we see from decoding with D&M parses in that setting.
","4.2 Parsing, POS and predicate detection",[0],[0]
"In Table 5 we present predicate detection precision, recall and F1 on the CoNLL-2005 and 2012 test sets.","4.2 Parsing, POS and predicate detection",[0],[0]
SA and LISA with and without ELMo attain comparable scores so we report only LISA+GloVe.,"4.2 Parsing, POS and predicate detection",[0],[0]
"We compare to He et al. (2017) on CoNLL-2005, the only cited work reporting comparable predicate detection F1.","4.2 Parsing, POS and predicate detection",[0],[0]
"LISA attains high predicate detection scores, above 97 F1, on both in-domain datasets, and out-performs He et al. (2017) by 1.5-2 F1 points even on the out-ofdomain Brown test set, suggesting that multi-task learning works well for SRL predicate detection.","4.2 Parsing, POS and predicate detection",[0],[0]
First we assess SRL F1 on sentences divided by parse accuracy.,4.3 Analysis,[0],[0]
"Table 6 lists average SRL F1 (across sentences) for the four conditions of LISA and D&M parses being correct or not (L±, D±).",4.3 Analysis,[0],[0]
"Both parsers are correct on 26% of sentences.
facts-figures
Here there is little difference between any of the models, with LISA models tending to perform slightly better than SA.",4.3 Analysis,[0],[0]
"Both parsers make mistakes on the majority of sentences (57%), difficult sentences where SA also performs the worst.",4.3 Analysis,[0],[0]
"These examples are likely where gold and D&M parses improve the most over other models in overall F1: Though both parsers fail to correctly parse the entire sentence, the D&M parser is less wrong (87.5 vs. 85.7 average LAS), leading to higher SRL F1 by about 1.5 average F1.
",4.3 Analysis,[0],[0]
"Following He et al. (2017), we next apply a series of corrections to model predictions in order to understand which error types the gold parse resolves: e.g. Fix Labels fixes labels on spans matching gold boundaries, and Merge Spans merges adjacent predicted spans into a gold span.6
In Figure 3 we see that much of the performance gap between the gold and predicted parses is due to span boundary errors (Merge Spans, Split Spans and Fix Span Boundary), which supports the hypothesis proposed by He et al. (2017) that incorporating syntax could be particularly helpful for resolving these errors.",4.3 Analysis,[0],[0]
"He et al. (2017) also point out
6Refer to He et al. (2017) for a detailed explanation of the different error types.
that these errors are due mainly to prepositional phrase (PP) attachment mistakes.",4.3 Analysis,[0],[0]
We also find this to be the case: Figure 4 shows a breakdown of split/merge corrections by phrase type.,4.3 Analysis,[0],[0]
"Though the number of corrections decreases substantially across phrase types, the proportion of corrections attributed to PPs remains the same (approx. 50%) even after providing the correct PP attachment to the model, indicating that PP span boundary mistakes are a fundamental difficulty for SRL.",4.3 Analysis,[0],[0]
We present linguistically-informed self-attention: a multi-task neural network model that effectively incorporates rich linguistic information for semantic role labeling.,5 Conclusion,[0],[0]
"LISA out-performs the state-ofthe-art on two benchmark SRL datasets, including out-of-domain.",5 Conclusion,[0],[0]
"Future work will explore improving LISA’s parsing accuracy, developing better training techniques and adapting to more tasks.",5 Conclusion,[0],[0]
"We are grateful to Luheng He for helpful discussions and code, Timothy Dozat for sharing his code, and to the NLP reading groups at Google and UMass and the anonymous reviewers for feedback on drafts of this work.",Acknowledgments,[0],[0]
"This work was supported in part by an IBM PhD Fellowship Award to E.S., in part by the Center for Intelligent Information Retrieval, and in part by the National Science Foundation under Grant Nos. DMR-1534431 and IIS-1514053.",Acknowledgments,[0],[0]
"Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",Acknowledgments,[0],[0]
Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features.,abstractText,[0],[0]
"However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax.",abstractText,[0],[0]
"In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-ofspeech tagging, predicate detection and SRL.",abstractText,[0],[0]
"Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates.",abstractText,[0],[0]
Syntax is incorporated by training one attention head to attend to syntactic parents for each token.,abstractText,[0],[0]
"Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model.",abstractText,[0],[0]
"In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on outof-domain data, nearly 10% reduction in error.",abstractText,[0],[0]
On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1.,abstractText,[0],[0]
"LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.",abstractText,[0],[0]
Linguistically-Informed Self-Attention for Semantic Role Labeling,title,[0],[0]
"Recent advances in deep reinforcement learning, supported by the ability of generating and processing large amounts of data, allowed impressive achievements such as playing Atari at human level (Mnih et al., 2015) or mastering the game of Go (Silver et al., 2016).",1. Introduction,[0],[0]
"In robotics however, sample complexity is paramount as sample generation on physical systems cannot be sped up and can cause wear and damage to the robot when excessive (Kober et al., 2013).",1. Introduction,[0],[0]
"Relying on a simulator to carry the learning will inevitably result in a reality gap, since mechanical forces such as stiction are hard to accurately model.",1. Introduction,[0],[0]
"However, a policy learned in a simulated environment can still be valuable provided the availability of a sample efficient algorithm to
1CLAS/IAS, TU Darmstadt, Darmstadt, Germany 2Max Planck Institute for Intelligent Systems, Tübingen, Germany 3LCAS, University of Lincoln, Lincoln, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Riad Akrour <riad@robot-learning.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
carry an additional optimization phase on the physical system.
",1. Introduction,[0],[0]
"Bayesian optimization is best known as a black-box global optimizer (Brochu et al., 2010; Shahriari et al., 2016).",1. Introduction,[0],[0]
"It was shown to be efficient for several function landscapes (Jones, 2001), real world scenarios such as the automatic tuning of machine learning algorithms (Bergstra et al., 2011; Snoek et al., 2012; Feurer et al., 2015) or robotics and control (Lizotte et al., 2007; Wilson et al., 2014; Calandra et al., 2016) and several of its variants have convergence guaranties to a global optimum (Vazquez & Bect, 2010; Bull, 2011).",1. Introduction,[0],[0]
Its efficiency stems from two key principles: a probabilistic modeling of the objective function and a sampling procedure that fully exploits this model.,1. Introduction,[0],[0]
"However, as the dimensionality of the task increases, non-stationarity effects of the objective or the noise function (see Shahriari et al. (2016), Sec. V.D. for a discussion of these effects) are exacerbated, rendering the modeling of the objective function challenging.",1. Introduction,[0],[0]
"An additional difficulty stemming from the increase in dimensionality is the tendency of Bayesian optimization to over-explore, which was experimentally observed in e.g. Brochu et al. (2007).",1. Introduction,[0],[0]
Several recent approaches trying to scale Bayesian optimization to higher dimensions assume additional structure of the objective function.,1. Introduction,[0],[0]
"In Snoek et al. (2014), it is assumed that stationarity of the objective function can be recovered through the use of a parametric family of mappings.",1. Introduction,[0],[0]
"While it is assumed that the objective function has a lower intrinsic dimension in Djolonga et al. (2013); Wang et al. (2016), can be decomposable into a sum of lower dimensional functions in Kandasamy et al. (2015) or a combination of both hypothesis in Li et al. (2016).
",1. Introduction,[0],[0]
"In this paper, we assume prior knowledge on the location of the optimum—given by an initial solution and a confidence on the optimality thereof—and leverage Bayesian optimization in a local manner to improve over this solution.",1. Introduction,[0],[0]
"We are especially interested in the application of our algorithm to the optimization of motor skills since i) evaluating the policy return is expensive on physical systems and will likely dominate the computational budget of the optimization process; as such, sample efficient algorithms such as Bayesian optimization are desirable ii) robotics applications are typically high dimensional and global optimization might be prohibitively expensive iii) an initial solution
can often be obtained through the use of imitation learning (Argall et al., 2009) or by a preliminary optimization on a surrogate model such as a simulator.",1. Introduction,[0],[0]
"Our algorithm can be seen as a local stochastic search algorithm akin to Covariance Matrix Adaptation (CMAES) (Hansen & Ostermeier, 2001), cross-entropy (Mannor et al., 2003) or MOdel-based Relative Entropy (MORE) (Abdolmaleki et al., 2015).",2. Related work,[0],[0]
"Local stochastic search algorithms typically maintain a Gaussian search distribution from which samples are generated, the objective function is evaluated and the search distribution is updated.",2. Related work,[0],[0]
"As in Bayesian optimization, they are of particular use when the gradient of the objective function is unknown.",2. Related work,[0],[0]
"Their use as a black-box optimization routine is gaining popularity in the machine learning community, e.g. in reinforcement learning (Thomas et al., 2015) or even for hyperparameter tuning (Bergstra et al., 2011) and the optimization of the acquisition function (Wang et al., 2016) of global Bayesian optimization.
",2. Related work,[0],[0]
Our algorithm shares the same general structure as local stochastic search algorithms and additionally learns a (probabilistic) model of the objective function.,2. Related work,[0],[0]
Modeling the objective function was already explored in the stochastic search literature.,2. Related work,[0],[0]
"A surrogate function is learned in (Loshchilov et al., 2013) using SVM-Rank, and is optimized using CMA-ES for a few iterations, yielding an update of the search distribution without requiring additional function evaluations.",2. Related work,[0],[0]
"While in MORE (Abdolmaleki et al., 2015), a local quadratic approximation of the objective function yields the new mean and covariance of the Gaussian search distribution upon an information-theoretic update.",2. Related work,[0],[0]
"Unlike these algorithms, we do not optimize the learned (probabilistic) model, but derive from it p(x = x?|D), the probability of x being optimal.",2. Related work,[0],[0]
Our search distribution is then updated such as to minimize the KullbackLeibler (KL) divergence to p(x = x?|D).,2. Related work,[0],[0]
"Compared to these surrogate assisted local stochastic search algorithms (Loshchilov et al., 2013; Abdolmaleki et al., 2015), the transformation of the optimization landscape (minimizing the KL-divergence to p(x = x?|D) instead of the objective function) facilitates learning of the surrogate model by lowering the variance in poorly performing regions, as illustrated in Fig. 1.
",2. Related work,[0],[0]
To approximate p(x = x?|D),2. Related work,[0],[0]
we rely on a probabilistic modeling of the objective function and to select the next point to sample we locally optimize an acquisition function.,2. Related work,[0],[0]
"As such, our algorithm can also be seen as Bayesian optimization where the usual box constraint is moved towards a high value area of the objective function to restrict exploration.
",2. Related work,[0],[0]
"Algorithm 1 Local Bayesian Optimization of Motor Skills
Input: Initial policy π0 = N (µ0, σ20I), step-size , entropy reduction rate β Output: Policy πN for n = 1 to N do
Fit: Gaussian p̂n from local samples of p?n (Sec. 3.2)",2. Related work,[0],[0]
"Optimize: (η∗, ω∗) = arg min gn(η, ω) (Sec. 3.1.1)",2. Related work,[0],[0]
"Bayesian Update: (πn+1)
η?+ω?",2. Related work,[0],[0]
∝,2. Related work,[0],[0]
πη?n p?n (Sec. 3.1.1),2. Related work,[0],[0]
Evaluate: xn from local samples of p?n (Sec. 3.3),2. Related work,[0],[0]
"Dn ←− Dn−1 ∪ {(xn, yn)}
end for
In reinforcement learning, probabilistic modeling was used to e.g. learn a transition model (Deisenroth & Rasmussen, 2011) or the policy gradient (Ghavamzadeh et al., 2016) with Gaussian processes.",2. Related work,[0],[0]
"Closer to our work, the use of an adaptive box constraint was explored in Bayesian optimization to ensure a safe optimization of a robot controller (Berkenkamp et al., 2016; Englert & Toussaint, 2016).",2. Related work,[0],[0]
Considering safety is crucial for motor skill learning on physical systems to prevent the evaluation of ’dangerous’ parameters.,2. Related work,[0],[0]
Both approaches restrict exploration to an initial safe region of the parameter space that is incrementally expanded using additional problem assumptions.,2. Related work,[0],[0]
Without such assumptions our algorithm cannot guarantee safety but its local nature is expected to dampen the potential risk of global Bayesian optimization.,2. Related work,[0],[0]
Let f : Rd 7→ R be an objective function.,3. Local Bayesian optimization,[0],[0]
For example f(x) can be the expected reward of a robot controller parameterized by x ∈ Rd.,3. Local Bayesian optimization,[0],[0]
"We assume that the algorithm only has access to noisy evaluations y = f(x) + , where ∼ N (0, σ2s) is Gaussian noise of unknown deviation σs.",3. Local Bayesian optimization,[0],[0]
"The algorithm will produce a sequence {(x1, y1) . . .",3. Local Bayesian optimization,[0],[0]
"(xN , yN )} of parameter-evaluation pairs and the goal is to minimize the cumulative regret 1N ∑ i f(x
?)",3. Local Bayesian optimization,[0],[0]
− yi for some global maximizer x? of f .,3. Local Bayesian optimization,[0],[0]
"The cumulative regret emphasizes the inherent cost in evaluating a bad parameter, potentially causing wear and damage to the robot.
",3. Local Bayesian optimization,[0],[0]
"Prior knowledge on an optimum’s location x? is given to the algorithm by a Gaussian distribution π0 = N (µ0, σ20I).",3. Local Bayesian optimization,[0],[0]
In what follows we will indistinctly refer to π as a search distribution or a policy following the terminology of the stochastic search and reinforcement learning (RL) communities.,3. Local Bayesian optimization,[0],[0]
"In an RL context, an informative prior can often be obtained from human generated data or from a simulator.",3. Local Bayesian optimization,[0],[0]
"Specifically, we assume that the mean µ0 of π0 is obtained by imitation learning if near-optimal demonstrations are available or by a preliminary optimization on a less accurate but inexpensive model of the system dy-
namics.",3. Local Bayesian optimization,[0],[0]
"Whereas σ0 is a hyper-parameter of the algorithm, manually set in our experiments, and expressing the confidence in the optimality of µ0.
",3. Local Bayesian optimization,[0],[0]
The search distribution πn is updated by solving the optimization problem formally defined in Sec.,3. Local Bayesian optimization,[0],[0]
3.1.1.,3. Local Bayesian optimization,[0],[0]
"The objective of the optimization problem is to minimize the KL divergence between πn and p(x = x?|Dn), the probability of x? being optimal according to the data set of parameter-evaluation pairs Dn.",3. Local Bayesian optimization,[0],[0]
"Solving this problem results in a Bayesian update, as shown in Alg. 1, where the prior πn(x) on the optimality of x is weighted by the likelihood p(x = x?|Dn) of x being optimal according to Dn.",3. Local Bayesian optimization,[0],[0]
"Letting the likelihood p(x = x?|Dn) be denoted by p?n(x), the first step of the algorithm is to fit p̂n, a Gaussian approximation of p?n (Sec. 3.2).",3. Local Bayesian optimization,[0],[0]
"Subsequently, a dual function is optimized (Sec. 3.1.1) to make sure that the search distribution moves slowly towards p?",3. Local Bayesian optimization,[0],[0]
as new evaluations are collected.,3. Local Bayesian optimization,[0],[0]
Modulating the Bayesian update with the dual parameters η∗ and ω∗ is important sinceDn is initially empty and p?n not initially informative.,3. Local Bayesian optimization,[0],[0]
"Finally, a new evaluations of f is requested by selecting xn from the previously generated samples of p?n and the process is iterated.
",3. Local Bayesian optimization,[0],[0]
The next subsections give a detailed presentation of both the search distribution update and the sampling procedure from p?n.,3. Local Bayesian optimization,[0],[0]
The search distribution in our algorithm is updated such as to minimize the KL divergence between πn and p?n.,3.1. Search distribution update,[0],[0]
"The resulting optimization problem is closely related to the one solved by the MORE algorithm (Abdolmaleki et al., 2015).",3.1. Search distribution update,[0],[0]
"In the next subsections, we will first formalize our search distribution update before briefly describing the search distribution update of MORE and showing how their deriva-
tions can be used to obtain our search distribution update.",3.1. Search distribution update,[0],[0]
The search distribution is updated such that its KL divergence w.r.t. p?n is minimized.,3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"Since future evaluations of f will be performed around the updated search distribution, it becomes critical to control the change of distribution between iterations by constraining the aforementioned minimization problem.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
These constraints will ensure that the exploration is not reduced too fast or that the mean is not moved too quickly from the initial solution µ0.,3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"The resulting optimization problem is given by
arg min π
KL(π ‖ p?n),
subject to KL(π ‖ πn) ≤ , (1) H(πn)−H(π) ≤",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"β, (2)
where KL(p ‖ q) = ∫ p(x) log p(x)q(x)dx is the KL diver-
gence between p and q andH(p) =",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
− ∫ p(x) log(p(x))dx is the entropy of p.,3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"The hyper-parameters and β respectively bound the change in distribution and the reduction in entropy between successive iterations.
",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"The use of the KL divergence to constrain the update is widespread in the reinforcement learning community (Peters et al., 2010; Schulman et al., 2015).",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"When the search distributions π and πn are of Gaussian form, the KL divergence in Eq.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
(1) is impacted by three factors.,3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"On one side, by the change in entropy between the two distributions— having a direct impact on the exploration rate.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"On the other side, by the displacement of the mean and the rotation of the covariance matrix—not impacting the exploration rate.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"To better control the exploration, we choose to decouple
the reduction in entropy from the KL constraint.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"It was shown in (Abdolmaleki et al., 2015) that the additional entropy constraint can lead to significantly better solutions at the expense of a slower start.
",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
The optimization problem defined in this section is closely related to the one solved by MORE.,3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"In fact, when the inequality (2) is replaced by the equality constraintH(πn)− H(π) = β for both algorithms then the two problems coincide; while only a small modification of the dual function is necessary otherwise.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"For the sake of clarity and to keep the paper self-contained, we will briefly introduce MORE before showing how we can reuse their derivation of the search distribution update in our algorithm.",3.1.1. THE OPTIMIZATION PROBLEM,[0],[0]
"MORE (Abdolmaleki et al., 2015) is a local stochastic search algorithm where the search distribution πn(x) is updated by solving the following constrained problem
arg max π
∫ π(x)f(x)dx
subject to KL(π ‖ πn) ≤ , (3) H(πn)−H(π) ≤",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"β, (4)
",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"An analytic solution of the problem is obtained by locally approximating f with the quadratic model
Rn(x) =",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"− 1
2 xTRnx+ x Trn + rn,
learned by linear regression from the data set Dn.",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"Letting the search distribution πn(x) = N (x|µn,Σn) at iteration n be parameterized by the mean µn and covariance matrix Σn, the aforementioned optimization problem yields the closed form update where the new mean and covariance are given by
Σ−1n+1 = (η ?",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"+ ω?)−1 ( η?Σ−1n +Rn ) , (5) µn+1",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
= (η ? + ω?)−1Σn+1,3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"( η?Σ−1n µn + rn ) , (6)
where η? and ω?",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"are the Lagrange multipliers of the constraints (3) and (4) respectively, and are obtained by minimizing the dual function gn(η, ω) by gradient descent (Abdolmaleki et al., 2015).
",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"As can be seen in Eq. 5, the new covariance matrix is a trade-off between the old covariance and the local curvature of the objective function f—where the trade-off parameters are computed in order to satisfy both constraints of the optimization problem.",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"As such, it is appropriate to use the covariance matrix of the search distribution in the kernel function for the local approximation of f when using GP regression.
",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"We additionally define MORE with equality constraint as a variant of MORE where the inequality constraint in (4)
is replaced with the equality constraint H(πn) − H(π) = β, forcing the reduction in entropy at each iteration to be exactly β.",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"This modification will not change the shape of the update but only the Lagrange multipliers, that can be obtained by simply alleviating the constraint ω",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
≥ 0,3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
"in the minimization of gn(η, ω).",3.1.2. MODEL-BASED RELATIVE ENTROPY SEARCH,[0],[0]
We now show that the optimization problem in our algorithm can be phrased as the optimization problem solved by MORE for the equality entropy constraint; while only a small modification of the dual minimization is required for the inequality entropy constraint.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"The equivalence of the optimization problems will allow us to use Eq. 5 and 6 to update our search distribution.
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
Proposition 1.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
The optimization problem in Sec.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"3.1.1 can be reduced to the optimization problem in 3.1.2 for the objective function f = log p?n when both problems enforce an exact entropy reduction constraint on π.
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
Proof.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
We first rephrase the problem in Sec.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"3.1.1 as the maximization over π of
−KL(π ‖ p?n)",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"+ β −H(πn),
where we switched the sign of the KL divergence term and added the constant term β − H(πn).",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
These modifications will not change the value of the stationary points of the Lagrangian.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"The resulting Lagrangian is
L(π, η, ω) = ∫",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"π(x) log p?n(x)dx+η( −KL(π ‖ πn))
+ (ω + 1)(H(π)−H(πn) + β),
with dual variables η ≥ 0 and ω ∈ R and where we have split the term KL(π ‖ p?n) into the expected log-density of p?n and the entropy H(π) of π.",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"A MORE formulation with similar entropy and KL divergence constraints and where the objective is to maximize the log-density log p?n yields the Lagrangian
L′(π, η, ω) = ∫",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"π(x) log p?n(x)dx+η( −KL(π ‖ πn))
+ ω(H(π)−H(πn) + β).
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"Since we have no constraint on ω, it is easy to see that the dual variable minimizing the dual of the first problem ω?",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
and of the second (MORE) problem ω′∗ are related by ω? = ω′∗,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"− 1 and both problems will result in the same update of π.
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"Intuitively, the minimization of KL(π ‖ p?n) can be reduced to the maximization (in expectation of π) of the log-density
log p?n because the equality constraintH(π) =",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"H(πn)− β annihilates the effect of the additional entropy term H(π) coming from the KL objective.
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"From Proposition 1 and following the derivations in (Abdolmaleki et al., 2015), the search distribution πn+1 solution of the optimization problem in Sec.",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"3.1.1 is given by
πn+1 ∝",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
(πn) η? η?+ω?,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
(p?n) 1 η?+ω?,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
", (7)
where η? and ω?",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"are the Lagrange multipliers related to the KL and entropy constraints respectively and minimizing the dual function gn(η, ω).",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"We refer the reader to Sec. 2.1 in (Abdolmaleki et al., 2015) for the definition of gn(η, ω).
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"When the entropy constraint is the inequality in (2) instead of an equality, the Lagrange multipliers for our update and the MORE update may differ.",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"However, η? and ω? can still be obtained by the minimization of the same gn(η, ω) with the additional constraint ω ≥ 1.
",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
Note that the new search distribution πn+1 as defined in Eq.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
(7) is not necessarily Gaussian because of the multiplication by p?n.,3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"However, by approximating p ? n",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
"by a Gaussian distribution p̂n, log p̂n will be a quadratic model and Eq. 5 and 6 can be used to obtain a Gaussian πn+1.",3.1.3. OPTIMIZATION PROBLEMS’ EQUIVALENCE,[0],[0]
To obtain a closed form update of the Gaussian search distribution in Eq.,3.2. Approximating the argmax distribution,[0],[0]
"(7), we will approximate p?n by fitting a Gaussian p̂n to samples of p?n as shown in Fig. 1e.",3.2. Approximating the argmax distribution,[0],[0]
"To generate samples from p?n, we use Thompson sampling (Chapelle & Li, 2011; Russo & Roy, 2014) from a probabilistic model of the objective function f .
",3.2. Approximating the argmax distribution,[0],[0]
The probabilistic model of f follows from both a Gaussian process (GP) prior and a Gaussian likelihood assumption.,3.2. Approximating the argmax distribution,[0],[0]
"We use in this paper the squared exponential kernel kn(xi,xj) = θ0 exp(−θ1(xi −",3.2. Approximating the argmax distribution,[0],[0]
xj)TΣ−1n (xi − xj)) with hyper-parameters θ0 and θ1 and Σn the covariance matrix of πn.,3.2. Approximating the argmax distribution,[0],[0]
"The resulting model has hyper-parameter vector φ = (θ0, θ1, σs), where σ2s is the noise variance of the likelihood function as previously defined.",3.2. Approximating the argmax distribution,[0],[0]
"Samples from p?n are generated by i) sampling a hyper-parameter vector φ from the posterior distribution p(φ|Dn) using slice sampling (Murray & Adams, 2010), ii) sampling a function from the GP posterior p(f̃ |Dn,φ) and iii) returning the argmax of f̃ .
",3.2. Approximating the argmax distribution,[0],[0]
The computational complexity of evaluating f̃ is cubical in the number of requested evaluations as it involves a matrix inversion.,3.2. Approximating the argmax distribution,[0],[0]
"As such, the exact maximization of f̃ can prove to be challenging.",3.2. Approximating the argmax distribution,[0],[0]
"Prior work in the literature considered approximating f̃ with a linear function (see for example Hernández-Lobato et al. (2014), Sec. 2.1) and globally
maximizing the linear surrogate.",3.2. Approximating the argmax distribution,[0],[0]
"In our local optimization context, we follow a more straightforward approach by generating samples from πn, and returning the sample with maximal value of f̃ .",3.2. Approximating the argmax distribution,[0],[0]
The rational behind searching the argmax of f̃ in the vicinity of πn is that samples from πn are likely to have high f̃ value since πn is updated such that the KL divergence w.r.t. p?n is minimized.,3.2. Approximating the argmax distribution,[0],[0]
"The repeated process of drawing points from πn, drawing their value from the GP posterior and selecting the point with highest value will constitute a data set D?n containing local samples from p?n.
",3.2. Approximating the argmax distribution,[0],[0]
"Once samples from p?n are generated and stored in D?n, we set p̂n = N (µ?n,Σ?n) where µ?n and Σ?n are the sample mean and covariance of the samples in D?n.",3.2. Approximating the argmax distribution,[0],[0]
"Because p̂n is Gaussian, log p̂n is quadratic and the search distribution update in Eq.",3.2. Approximating the argmax distribution,[0],[0]
(7) yields a Gaussian distribution πn+1 with covariance and mean as defined in Eq.,3.2. Approximating the argmax distribution,[0],[0]
(5) and Eq. (6) respectively withRn = Σ?n −1,3.2. Approximating the argmax distribution,[0],[0]
and rn = Σ?n −1µ?n.,3.2. Approximating the argmax distribution,[0],[0]
The function f is initially evaluated at a point x0 drawn from the prior distribution π0.,3.3. Sample generation,[0],[0]
"In subsequent iterations, a point xn is randomly selected from D?n, the set of samples used in the computation of p̂n (Sec. 3.2).
",3.3. Sample generation,[0],[0]
"Experimentally, we noticed that the exploration in our algorithm is heavily influenced by the centering of the values {yi}n1 in Dn.",3.3. Sample generation,[0],[0]
Three variants of our algorithm are initially evaluated with different target values of the GP.,3.3. Sample generation,[0],[0]
The target values are obtained by subtracting from yi either the max the min or the mean of {yi}n1 .,3.3. Sample generation,[0],[0]
"Since the GP modeling of f has a zero mean prior, the extreme case where the max (resp.",3.3. Sample generation,[0],[0]
the min) is subtracted from the data results in an optimistic (resp. pessimistic) exploration strategy considering that the objective function in unexplored areas have values higher (resp. lower) in expectation than the best (resp. worst) evaluation so far.,3.3. Sample generation,[0],[0]
We initially investigate in this section the impact of the target centering (Sec. 3.3) on the exploration-exploitation trade-off of our algorithm.,4. Experiments,[0],[0]
"We then compare our algorithm to two state-of-the-art model based optimizers: the global Bayesian optimizer and the local Model-Based Relative Entropy Search (Abdolmaleki et al., 2015).",4. Experiments,[0],[0]
"The algorithms are compared on several continuous function benchmarks as well as a simulated robotics task.
Benchmarks.",4. Experiments,[0],[0]
Variants of our algorithm are first compared on randomly generated smooth 2 dimensional objective functions.,4. Experiments,[0],[0]
"We then conduct a comparison to the state-ofthe-art on the COmparing COntinuous optimisers (COCO)
testbed on the 20 functions f5 to f24 (we refer the reader to http://coco.gforge.inria.fr/ for an illustration and the mathematical definition of each function).",4. Experiments,[0],[0]
We chose to split the experimentation between the uni-modal and the multi-modal categories of the testbed.,4. Experiments,[0],[0]
The unimodal category is representative of the informed initialization hypothesis that only requires local improvements.,4. Experiments,[0],[0]
While the multi-modal category assesses the robustness of our algorithm to more complex function landscapes— which can be encountered in practice despite the informed initialization if e.g. a too wide variance σ20 is initially set.,4. Experiments,[0],[0]
"We vary the dimension of the COCO functions from 3 to 30 while the robotics task evaluates our algorithm on a 70 dimensional setting.
",4. Experiments,[0],[0]
Algorithms.,4. Experiments,[0],[0]
"In what follows, we will refer to our algorithm as L-BayesOpt.",4. Experiments,[0],[0]
"We rely on the GPStuff library (Vanhatalo et al., 2013) for the GP implementation and the posterior sampling of hyper-parameters.",4. Experiments,[0],[0]
"We use the BayesOpt library (Martinez-Cantin, 2014) for global Bayesian optimization with a similar to L-BayesOpt squared exponential kernel and MCMC sampling of hyper-parameters and an additional Automatic Relevance Determination step executed every 50 samples.",4. Experiments,[0],[0]
"In the experiments we evaluate BayesOpt with both Expected Improvement and Thompson Sampling acquisition functions.
",4. Experiments,[0],[0]
"In all of the experiments, L-BayesOpt and MORE will share the same initial policy, step-size , entropy reduction β and will sample ten points per iteration.",4. Experiments,[0],[0]
We choose to use an equality constraint for the entropy reduction for both algorithms.,4. Experiments,[0],[0]
"As a result, both L-BayesOpt and MORE will have the same entropy at every iteration and any difference in performance will be attributed to a better location of the mean, adaptation of the covariance matrix or sampling procedure rather than a faster reduction in exploration.",4. Experiments,[0],[0]
"In all but the last experiment = β = .05 while for the robotics experiment with an initial solution learned by imitation learning we set a more aggressive step size and entropy reduction = β = 1.
Evaluation criterion.",4. Experiments,[0],[0]
The performance metric in RL is typically given by the average return J(πn) =∫ πn(x)f(x)dx while in Bayesian optimization it is typically determined by the minimal evaluation min1≤i≤n yi reached at iteration n. When the evaluations are noisy the minimum evaluation is not a robust performance metric— nor an appropriate criterion for the algorithm to select the returned optimizer.,4. Experiments,[0],[0]
"In order to have a common evaluation criterion, all the approaches are seen as multi-armed bandit algorithms and we use the cumulative regret 1n ∑ i f(x
?)",4. Experiments,[0],[0]
− yi as the evaluation criterion.,4. Experiments,[0],[0]
The cumulative regret of (global),4. Experiments,[0],[0]
"Bayesian optimizers is expected to be asymptotically lower than that of local optimizers as it always finds the global maximum given sufficiently many evaluations.
",4. Experiments,[0],[0]
"Conversely, trading-off global optimality for fast local improvements might result in a lower regret for local optimizers when the evaluation budget is moderate.",4. Experiments,[0],[0]
"In this first set of experiments, we evaluate the different exploration strategies resulting from three different centering methods of the y values in Dn.",4.1. Exploration variants,[0],[0]
We compare these three variants of L-BayesOpt on 11 randomly generated two dimensional Gaussian mixture objective functions (see Fig.,4.1. Exploration variants,[0],[0]
2a for an illustration).,4.1. Exploration variants,[0],[0]
"We chose these functions as they are cheap to evaluate, easy to approximate by a GP and their multi-modal nature is appropriate for evaluating the exploration-exploitation trade-off of the three variants.
",4.1. Exploration variants,[0],[0]
"As hypothesized in Sec. 3.3, the cumulative regret in Fig.",4.1. Exploration variants,[0],[0]
2b shows that the min variant exhibits the lowest exploration and reduces the regret faster than the other optimizers.,4.1. Exploration variants,[0],[0]
"Yet, when compared to MORE it manages to converge to better local optima in 5 out of the 11 randomly generated objectives while MORE converges to a better optimum in one of the 6 remaining objectives.",4.1. Exploration variants,[0],[0]
Note that MORE manages to decrease the regret faster than our algorithm during the first 100 evaluations.,4.1. Exploration variants,[0],[0]
"However, the sampling scheme relying on the Thompson sampling acquisition function and the convergence to higher modes gives the advantage to the L-BayesOpt variants after the initial 100 evaluations.",4.1. Exploration variants,[0],[0]
In the remainder of the experimental section only the min variant of our algorithm will be considered.,4.1. Exploration variants,[0],[0]
We compare our algorithm to MORE and Bayesian optimization on the COCO testbed.,4.2. State-of-the-art benchmark comparisons,[0],[0]
We form two sets each containing 10 objective functions.,4.2. State-of-the-art benchmark comparisons,[0],[0]
The first one includes unimodal functions (f5 to f14) while the second one includes multi-modal function with an adequate (f15 to f19) and a weak (f20 to f24) global structure.,4.2. State-of-the-art benchmark comparisons,[0],[0]
"Each function has a global optimum in [−5, 5]D, where D is the dimension of the objective function that we vary in the set {3, 10, 30}.
",4.2. State-of-the-art benchmark comparisons,[0],[0]
"The bounding box [−5, 5]D is provided to Bayesian optimization while for the local stochastic search algorithms we set the initial distribution to π0 = N (0, 3I).",4.2. State-of-the-art benchmark comparisons,[0],[0]
"Note that this is not an informed initialization and none of the functions had their optimum on the null vector.
",4.2. State-of-the-art benchmark comparisons,[0],[0]
Fig. 3 shows the performance of the four algorithms on the multi-modal (top row) and uni-modal (bottom row) function sets.,4.2. State-of-the-art benchmark comparisons,[0],[0]
"On the multi-modal set of functions and when D = 3, Bayesian optimization with Thompson sampling proves to be an extremely efficient bandit algorithm for uncovering the highest reward point with a minimal number of evaluations.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"On the contrary, both local stochastic search algorithms struggle to improve over the initial performance.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"Upon closer inspection, this appears to be especially true for functions with weak global structure such as f23.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"We hypothesize that for these highly multi-modal functions, both model based stochastic search algorithms learn poor quadratic models (when either approximating f or p?n).",4.2. State-of-the-art benchmark comparisons,[0],[0]
"The performance gap between Bayesian optimization and our algorithm reduces however as the dimensionality of the problem increases.
",4.2. State-of-the-art benchmark comparisons,[0],[0]
"On the uni-modal functions set, our algorithm reduces significantly faster the regret than Bayesian optimization.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"As
the dimension of the objective function increases from D = 10 to D = 30, more evaluations are required by Bayesian optimization to reach our algorithm.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"Compared to MORE, and since the objectives are uni-modal, the use of an acquisition function is the main driving factor for the faster decrease of the regret.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"Note that even if the functions are uni-modal, both local search algorithms are not necessarily zero if the decrease in entropy is too fast.
",4.2. State-of-the-art benchmark comparisons,[0],[0]
Both L-BayesOpt and BayesOpt/TS rely on the Thompson sampling acquisition function for selecting the next point to evaluate.,4.2. State-of-the-art benchmark comparisons,[0],[0]
"While the acquisition function is maximized on the full support of the objective in the case of Bayesian optimization, it is only optimized in the vicinity of the current search distribution by our algorithm.",4.2. State-of-the-art benchmark comparisons,[0],[0]
"The experiments on the COCO testbed show that when the function landscape enables the learning of an appropriate update direction for moving the search distribution, the adaptive strategy employed by our algorithm can be more efficient than the global search performed by Bayesian optimization.",4.2. State-of-the-art benchmark comparisons,[0],[0]
The task’s objective is for the Barrett robot arm to swing the ball upward and place it in the cup (Fig. 4a).,4.3. Robot ball in the cup,[0],[0]
"The
optimization is performed on the 70 weights of the forcing function of a Dynamical Movement Primitive (DMP, Ijspeert & Schaal 2003) controlling the 7 joints of the robot.",4.3. Robot ball in the cup,[0],[0]
The initial forcing function weights µ0 are learned by linear regression from a single demonstrated trajectory that successfully swings the ball up but where the ball lands at circa 20cm from the cup.,4.3. Robot ball in the cup,[0],[0]
"We compare the performance of MORE and L-BayesOpt in optimizing the initial policy π0 = N (µ0, I) using the same hyper-parameters.",4.3. Robot ball in the cup,[0],[0]
"The challenge of the task, in addition to the dimension of the action space, stems from the two exploration regimes required by the exploration scheme.",4.3. Robot ball in the cup,[0],[0]
"While initially a significant amount of noise needs to be introduced to the parameters to get the ball closer to the cup; successfully getting the ball in the cup requires a more careful tuning of the forcing function.
",4.3. Robot ball in the cup,[0],[0]
Fig.,4.3. Robot ball in the cup,[0],[0]
4b shows the performance of both MORE and LBayesOpt on the robot ball in a cup task.,4.3. Robot ball in the cup,[0],[0]
MORE has a better initial sample efficiency and gets the ball closer to the cup at a faster pace than L-BayesOpt.,4.3. Robot ball in the cup,[0],[0]
"However, the acquisition function based sampling scheme of our algorithm was more efficient for discovering parameters that successfully put the ball in the cup and results in a lower regret (averaged over 5 runs) after 1000 evaluations.",4.3. Robot ball in the cup,[0],[0]
"The experiment shows that for such high dimensional tasks, our algorithm was better at tuning the policy only when the entropy of the search distribution was significantly reduced.",4.3. Robot ball in the cup,[0],[0]
This might be due to the low correlation between Euclidean distance between parameters and difference in reward.,4.3. Robot ball in the cup,[0],[0]
"One promising direction for future work in a reinforcement learning context is to use kernels based on trajectory data distance instead of parameter distance in euclidian space (Wilson et al., 2014).",4.3. Robot ball in the cup,[0],[0]
"The algorithm presented in this paper can be seen as Bayesian optimization where the usual box constraint is rotated, shrunk and moved at each iteration towards the most
promising region of the objective function.",5. Discussion,[0],[0]
The constant reduction of the entropy of the search distribution ensures that the objective function is not modeled and optimized on the entirety of its domain.,5. Discussion,[0],[0]
"Compared to (global) Bayesian optimization, we experimentally demonstrated on several continuous optimization benchmarks that it results in faster improvements over the initial solution, at the expense of global optimality.",5. Discussion,[0],[0]
"This property is especially useful when an initial informative solution is available and only requires to be locally improved.
",5. Discussion,[0],[0]
The computational cost of the search distribution update in our algorithm is significantly higher than most local stochastic search algorithms.,5. Discussion,[0],[0]
This cost mainly arises from the full Bayesian treatment of the modeling of the objective function f .,5. Discussion,[0],[0]
"If the evaluation of f is cheap, a better performance per second is obtained by less expensive stochastic search algorithms where the additional computational budget can be spent in running additional randomized restarts of the algorithms (Auger & Hansen, 2005).",5. Discussion,[0],[0]
"However, if the optimization cost is dominated by the evaluation of f , the probabilistic modeling proved to be more sample efficient on several benchmarks by actively selecting the next point to evaluate.",5. Discussion,[0],[0]
"As a result, when f is expensive to evaluate our algorithm is expected to have better per second performance than state-of-the-art stochastic search algorithms.
",5. Discussion,[0],[0]
The search distribution update proposed in this paper is well founded and results in an interpretable update.,5. Discussion,[0],[0]
"At each iteration the current search distribution is simply weighted by p(x = x?|Dn), the probability of x being optimal according to the current data set.",5. Discussion,[0],[0]
Future work can further improve the sample efficiency of our algorithm in at least three ways.,5. Discussion,[0],[0]
"First, if the objective function is upper bounded and the bound is known, we expect that the integration of an additional constraint f(x) < f(x∗) for all x to lead to a more accurate probabilistic modeling and a better exploration-exploitation trade-off.",5. Discussion,[0],[0]
"Secondly, the search distribution update is phrased as the minimization of the I-projection of p(x = x?|Dn), which has the property of focusing on one mode of the distribution (Bishop, 2006).",5. Discussion,[0],[0]
"However, the Gaussian approximation of p(x = x?|Dn) can average over multiple modes if the GP is unsure about which of them is the highest.",5. Discussion,[0],[0]
We expected that a better update direction can be obtained if a clustering algorithm can detect the highest mode from samples of p(x = x?|Dn).,5. Discussion,[0],[0]
"Finally and perharps most interestingly, we expect our algorithm to be able to scale to significantly higher dimensional policies in an RL setting if a trajectory data kernel is used (Wilson et al., 2014).",5. Discussion,[0],[0]
"Specifically, distance between policies can be measured by the similarity of actions taken in similar states.",5. Discussion,[0],[0]
The local nature of our algorithm will additionally ensure that such similarity is evaluated on states that are likely to be reached by the evaluated policies.,5. Discussion,[0],[0]
The research leading to these results was funded by the DFG Project LearnRobotS under the SPP 1527 Autonomous Learning.,Acknowledgments,[0],[0]
Bayesian optimization is renowned for its sample efficiency but its application to higher dimensional tasks is impeded by its focus on global optimization.,abstractText,[0],[0]
"To scale to higher dimensional problems, we leverage the sample efficiency of Bayesian optimization in a local context.",abstractText,[0],[0]
The optimization of the acquisition function is restricted to the vicinity of a Gaussian search distribution which is moved towards high value areas of the objective.,abstractText,[0],[0]
The proposed informationtheoretic update of the search distribution results in a Bayesian interpretation of local stochastic search: the search distribution encodes prior knowledge on the optimum’s location and is weighted at each iteration by the likelihood of this location’s optimality.,abstractText,[0],[0]
We demonstrate the effectiveness of our algorithm on several benchmark objective functions as well as a continuous robotic task in which an informative prior is obtained by imitation learning.,abstractText,[0],[0]
Local Bayesian Optimization of Motor Skills,title,[0],[0]
"In this work, we study a basic question that arises in the study of high dimensional vector representations: given a dataset D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Such density estimates are important building blocks in non-parametric clustering, determining the popularity of topics, search and recommendation systems, the analysis of the neighborhoods of nodes in social networks, and in outlier detection, where geometric representations of data are frequently used.",1. Introduction,[0],[0]
"Yet for high dimensional datasets, we still lack simple, practical, experimentally verified and theoretically justified solutions to tackle this question.
",1. Introduction,[0],[0]
Our questions have been studied in the context of spherical range counting.,1. Introduction,[0],[0]
"One class of solution methods arising in the computational geometry literature, such as hierarchical splitting via trees, (Arya et al., 2010) have performance guarantees that depend exponentially on dimension.",1. Introduction,[0],[0]
"These are unsuitable for the higher dimensional models that ma-
1Stanford University, USA 2Laserlike Inc, USA.",1. Introduction,[0],[0]
"Correspondence to: Xian Wu <xwu20@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"chine learning methods are increasingly shifting towards e.g. word embeddings (Pennington et al., 2014; Mikolov et al., 2013) and graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover & Leskovec, 2016; Yang et al., 2016; Wang et al., 2017; Hamilton et al., 2017).",1. Introduction,[0],[0]
"Over-parameterized models are oftentimes easier to train (Livni et al., 2014), and perform just as well, if not better (Zhang et al., 2016).",1. Introduction,[0],[0]
"Word embeddings is one example where rigorous evaluation has shown increased performance with higher dimensionality (Melamud et al., 2016) (Lai et al., 2016).
",1. Introduction,[0],[0]
"In this paper, we develop an estimation scheme for high dimensional datasets to count the number of elements around a query that are in a given radius of cosine similarity.",1. Introduction,[0],[0]
"Angular distance, which corresponds to Euclidean distance for data points on the unit sphere is commonly used in applications related to word and document embeddings, and image and video search (Jegou et al., 2011)",1. Introduction,[0],[0]
"(Huang et al., 2012).",1. Introduction,[0],[0]
"Brute force search requires a linear scan over the entire dataset, which is prohibitively expensive.",1. Introduction,[0],[0]
"Our approach uses indexing and search via locality sensitive hashing (LSH) functions in order to estimate the size of the neighborhood in a more efficient manner than retrieving the neighbors within the given radius of similarity.
",1. Introduction,[0],[0]
Recent work has also explored LSH techniques for spherical range counting and related questions around density estimation for high-dimensional models.,1. Introduction,[0],[0]
"For example (Aumüller et al., 2017) generalizes nearest neighbor LSH hash functions to be sensitive to custom distance ranges.",1. Introduction,[0],[0]
"(Ahle et al., 2017) builds many different parameterized versions of the prototypical LSH hash tables and adaptively probes them for spherical range reporting.",1. Introduction,[0],[0]
"The closest works to ours in terms of solution method that we are aware of is that of (Spring & Shrivastava, 2017), giving an LSH based estimator to compute the partition function of a log-linear model, and (Charikar & Siminelakis, 2017), adapting LSH to solve a class of kernel density estimation problems.",1. Introduction,[0],[0]
"Both works produce an unbiased estimator, using LSH to implement a biased sampling scheme that lowers the variance of this estimator.",1. Introduction,[0],[0]
"However their technique leverages only one hash bucket per table, and hence requires a large number of tables for an accurate estimate.",1. Introduction,[0],[0]
"The biggest drawback to these works is the very high storage (hash tables) and query complexities – their techniques, as presented, are impractical for
adoption.
",1. Introduction,[0],[0]
Our approach improves upon the storage and sample complexities of previous methods using a combination of extracting information from multiple buckets per table (hence reducing table complexity) and importance sampling (hence reducing sample complexity).,1. Introduction,[0],[0]
"As we show in our experimental study on GLOVE embeddings, our estimate of the number of elements that are 60 degrees from a query q (which corresponds to synonyms and/or related words to q in the English vocabulary), achieves multiple orders of magnitude improved accuracy over competing methods, subject to reasonable and practical resource constraints.",1. Introduction,[0],[0]
Our theoretical analysis develops a rigorous understanding of our technique and offers practitioners further insight on optimizing our solution method for their particular datasets.,1. Introduction,[0],[0]
"Given a dataset D of vectors v1, . . .",2. Problem Formulation and Approach,[0],[0]
"vn ∈ Rd on the unit sphere, a query q ∈ Rd also on the unit sphere, and a range of angles of interestA, for example 0-60 degrees, how many elements v in D are such that the angle between q and v, denoted θqv, are within range A?",2. Problem Formulation and Approach,[0],[0]
We use Aq to denote the set of data vectors v that are within angle A to q (that have angular distance to query q that is in the range of interest A).,2. Problem Formulation and Approach,[0],[0]
"Our goal is to preprocess D in order to estimate the cardinality of this set, denoted |Aq|, efficiently for any given q.
One final note is that our scheme is conceptualized using bit-wise LSH functions; functions that hash vectors to 0-1 bits, and where the hamming distance between the hash sequences of two data points captures information about their angular distance.",2. Problem Formulation and Approach,[0],[0]
"For their simplicity, easy implementation, and high performance in practice, bit hashes such as hyperplane LSH (Charikar, 2002) are the standard hash functions used in practice for angular distance (Andoni et al., 2015).",2. Problem Formulation and Approach,[0],[0]
"Our technique and results can be extended for other hash functions; however, we will use hamming distance and other implementation details specific to bit-wise LSH functions in this work.",2. Problem Formulation and Approach,[0],[0]
Our overall estimation scheme is an implementation of importance sampling.,2.1. Approach Overview,[0],[0]
"It consists of two steps, a preprocessing step that applies locality sensitive hash functions to our dataset to produce hash tables.",2.1. Approach Overview,[0],[0]
"After this preprocessing step, we sample from our hash tables to produce our final estimate.
",2.1. Approach Overview,[0],[0]
"To help guide the reader through the technical details of our implementation, we first offer an intuitive explanation of our approach.",2.1. Approach Overview,[0],[0]
"Our importance sampling scheme achieves 2 main objectives: we concentrate the elements of interest
in our overall dataset into a few buckets that we can easily sample from, and we sample from these buckets to produce our estimate.",2.1. Approach Overview,[0],[0]
"In order to compensate for the concentrated sampling, we adjust the value of each sample by the inverse of the probability that the sample lands in the target buckets.
",2.1. Approach Overview,[0],[0]
Our technique relies on the key insight that LSH functions can effectively implement both of these objectives.,2.1. Approach Overview,[0],[0]
"Using LSH functions to index our dataset ensures that for a given query q, elements that are close to q in angular distance have a comparative higher probability of hashing to q’s bucket and to buckets that are of small hamming distance to q’s bucket, thereby concentrating the elements of interest into certain buckets that we can selectively sample from.
",2.1. Approach Overview,[0],[0]
"Additionally, the hamming distance collision probabilities for bit-wise LSH functions are well expressed in terms of angular distance.",2.1. Approach Overview,[0],[0]
"Consider random hyperplane LSH (Charikar, 2002), where each hash vector is chosen uniformly at random from the d-dimensional unit sphere.",2.1. Approach Overview,[0],[0]
"Each hash vector r contributes one bit to the hash sequence of a data point v, based on the rule:
hr(v) =",2.1. Approach Overview,[0],[0]
"{ 0 if r · v ≤ 0 1 otherwise.
",2.1. Approach Overview,[0],[0]
"It is well-known that for any particular hamming distance i, and any data point x,
P(dqx = i|θqx) =",2.1. Approach Overview,[0],[0]
"( t
i
)( 1− θqx
π )t−i( θqx π )i where dqx is the hamming distance between the hash for query q and the hash for data vector x, θqx denotes the angle between the 2 vectors, and t is the total number of bits in the hash sequence.
",2.1. Approach Overview,[0],[0]
"Thus, the choice of t affects the sensitivity of the LSH scheme – the correlation between the hamming distances of two hash sequences and the angle between the two underlying data points.",2.1. Approach Overview,[0],[0]
"Moreover, depending on the design choice for t, the set of hamming distances I that contains most of the probability mass for collision with elements of angular distance in range",2.1. Approach Overview,[0],[0]
A is different.,2.1. Approach Overview,[0],[0]
This is also a consideration in our sampling scheme; we want to sample from buckets of hamming distances,2.1. Approach Overview,[0],[0]
"I that have a high probability of containing elements that are within angle A of q.
",2.1. Approach Overview,[0],[0]
"Our sampling scheme picks elements over K hash tables from buckets that are at hamming distance I to the query, where I is tuned to A.",2.1. Approach Overview,[0],[0]
"Given a sample, x, we compute the angular distance θqx = cos−1(q · x).",2.1. Approach Overview,[0],[0]
"Let p(x) = P(dqx ∈ I|θqx), the collision probability that x lands in a bucket that is hamming distance I from q over the random choice of hash functions.
",2.1. Approach Overview,[0],[0]
"We define a random variable Z as a function of sample x as follows:
Z =
{∑K k=1 C k q",2.1. Approach Overview,[0],[0]
"(I)
K·p(x)",2.1. Approach Overview,[0],[0]
if θqx ∈,2.1. Approach Overview,[0],[0]
"A 0 otherwise.
",2.1. Approach Overview,[0],[0]
"(1)
where Ckq (I) is the total number of elements in buckets of hamming distance I from q’s bucket in table",2.1. Approach Overview,[0],[0]
"k.
We take S samples and construct Z1, Z2, . . .",2.1. Approach Overview,[0],[0]
ZS .,2.1. Approach Overview,[0],[0]
We report∑S i=1,2.1. Approach Overview,[0],[0]
"Zi S as our estimate for |Aq|.
",2.1. Approach Overview,[0],[0]
Comparison to Related Work: Note that our problem can be viewed as kernel density estimation problem for a specific kernel function that has value 1 for pairs of points within the required angle range of interest and 0 outside.,2.1. Approach Overview,[0],[0]
"However the analysis of (Charikar & Siminelakis, 2017) does not apply to our setting because they need a scale free hash function (with collision probabilities related to the kernel value) and there is no such function for our 0-1 kernel.",2.1. Approach Overview,[0],[0]
"The work of (Spring & Shrivastava, 2017) does not make such an assumption on the hash function, but they do not give an analysis that gives meaningful bounds in our setting.",2.1. Approach Overview,[0],[0]
"As noted previously, both works only look at a single hash bucket in each hash table, leading to a high storage overhead.",2.1. Approach Overview,[0],[0]
We establish the following theoretical bounds on the storage and sample complexity of our estimator in order to achieve a (1±ε)-approximation to the true count with high probability.,2.2. Main Result,[0],[0]
Theorem 2.1 (Main Result).,2.2. Main Result,[0],[0]
"For a given angular distance range of interest A and a given query q, with probability 1 − δ, our estimator returns a (1 ± ε)approximation to |Aq|, the true number of elements within
angle A to q using O
( 1
ε2 min x∈Aq
p(x) log( 1 δ )
) tables and
O
( E(Cq(I))
ε2|Aq|· min x∈Aq
p(x) log( 1 δ )
) samples.
",2.2. Main Result,[0],[0]
"To help the reader digest this result, we briefly compare this statement to the sample complexity of naive random sampling.",2.2. Main Result,[0],[0]
"It can be shown through a standard BernoulliChernoff argument that the sample complexity for random sampling is O( n|Aq|ε2 ln ( 1 δ ) ), where n|Aq| is the inverse proportion of elements of interest in the overall population.",2.2. Main Result,[0],[0]
"Intuitively this says that you need to take more random samples if |Aq| is very small compared to n.
Our sample complexity replaces the n|Aq| term with E(Cq(I)) |Aq|· min x∈Aq p(x) , where |Aq| · minx∈Aq p(x) is a measure of the expected number of elements from the set of interest Aq that will land in hamming distance I to q, and E(Cq(I)) is
the expected size of the overall sampling pool of elements in hamming distance I.",2.2. Main Result,[0],[0]
This ratio of expectations seems intuitive – one would expect to get such an expression if our scheme took one sample per table.,2.2. Main Result,[0],[0]
"Surprisingly, we achieve this same type of sample complexity bound while sampling from relatively few hash tables.
",2.2. Main Result,[0],[0]
"Just like random sampling, our sample complexity bound is also based on the proportion of elements of interest in hamming distance I to the total number of elements in hamming distance I.",2.2. Main Result,[0],[0]
"However, it is easy to see that applying LSH to our dataset will increase this proportion to yield a smaller sample complexity.",2.2. Main Result,[0],[0]
"We choose I so that min
x∈Aq p(x) is high
(this probability can be high even for a small set of hamming distances I, since p(x) is the cumulative probability mass of I successes in t trials, and binomial distributions in t concentrate in an O( √ t) sized interval around the mean), and E(Cq(I)) to be small (to filter out elements that are not interesting).
",2.2. Main Result,[0],[0]
There are certain tradeoffs to choosing I .,2.2. Main Result,[0],[0]
"If more hamming distances are included in I, then min
x∈Aq p(x) is higher, how-
ever, E(Cq(I)) is also larger.",2.2. Main Result,[0],[0]
The optimal choice for I is to choose the hamming distances that substantially increase min x∈Aq p(x) yet do not substantially increase E(Cq(I)),2.2. Main Result,[0],[0]
"(so not too many uninteresting elements are infiltrating those buckets).
",2.2. Main Result,[0],[0]
"In the following sections, we explain our scheme further and present our experimental results.",2.2. Main Result,[0],[0]
"The preprocessing step contributes 3 key ingredients to the overall estimation scheme:
Hash Tables:",3. Preprocessing,[0],[0]
"Given a family of bit-wise hash functions H, define a function family G = {g : D → {0, 1}t} such that g(v) = (h1(v), . . .",3. Preprocessing,[0],[0]
"ht(v)), where hj ∈ H.",3. Preprocessing,[0],[0]
"To construct K tables, we choose K functions g1, g2, . . .",3. Preprocessing,[0],[0]
gK from G independently and uniformly at random.,3. Preprocessing,[0],[0]
"We store each v ∈ D in bucket gk(v) for k = 1, 2 . .",3. Preprocessing,[0],[0]
.K.,3. Preprocessing,[0],[0]
"This step sets up the hash tables that we will sample from in our scheme.
",3. Preprocessing,[0],[0]
"Counts Vector: We create a counts vector, denoted Cki ∈ Rt+1 for each hash address ik for each table k ∈",3. Preprocessing,[0],[0]
"{1, . . .",3. Preprocessing,[0],[0]
",K}, whereCki (d) is the count of the total number of items in buckets that are at hamming distance d = 0, 1, . . .",3. Preprocessing,[0],[0]
"t away from ik in table k.
Sampler: We create a sampler that given a separate hash address ik for each table k ∈",3. Preprocessing,[0],[0]
"{1, . . .",3. Preprocessing,[0],[0]
",K} and set of hamming distances",3. Preprocessing,[0],[0]
"I, returns a data point uniformly at random from the union of elements that were hashed to buckets of hamming distance I from ik across the K tables.
",3. Preprocessing,[0],[0]
We describe in greater detail the 3 contributions of the preprocessing step.,3. Preprocessing,[0],[0]
"For the rest of this paper, all omitted proofs appear in Appendix C.",3. Preprocessing,[0],[0]
Setting up quality hash tables to enable accurate and efficient importance sampling is vital to our scheme.,3.1. Hash Tables,[0],[0]
"Since we are importance sampling from buckets of hamming distance I acrossK tables, we need to make enough tables to guarantee unbiasedness or near-unbiasedness for our sampling-based estimator; due to the variance of the randomly generated hash functions, if we make too few tables we may not find enough elements of interest contained in those tables within hamming distance I.",3.1. Hash Tables,[0],[0]
"We want to characterize the bias of our importance sampling scheme in relation to the contents of the buckets of our hash tables.
",3.1. Hash Tables,[0],[0]
"We let Bkq (I) denote the set of hash buckets that are at hamming distance I from the hash address of query q for table k. Next, we introduce an intermediate random variable:
W = 1
K K∑ k=1 ∑ x∈Aq 1(x",3.1. Hash Tables,[0],[0]
∈ Bkq (I)),3.1. Hash Tables,[0],[0]
"p(x) .
where p(x) = P(dqx ∈ I|θqx).
",3.1. Hash Tables,[0],[0]
"W is a random variable that represents the sum of the elements of interest |Aq| that are hashed to the buckets of sampling focus Bkq (I), weighted by their probabilities p(x).",3.1. Hash Tables,[0],[0]
"It is clear that once the set of hash functions is fixed, W becomes deterministic.
",3.1. Hash Tables,[0],[0]
"We first show that the random variable Z, as defined in Equation (1), is an unbiased estimator.",3.1. Hash Tables,[0],[0]
Lemma 3.1 (Expectation of Z).,3.1. Hash Tables,[0],[0]
"The expectation of Z over the random choice of hash functions is |Aq|, i.e. E(Z)",3.1. Hash Tables,[0],[0]
= |Aq|.,3.1. Hash Tables,[0],[0]
"The expectation of Z given a specific realization of hash functions, or equivalently, given W , is E(Z|W ) =W .
",3.1. Hash Tables,[0],[0]
"As a consequence, it is immediately clear that E(W )",3.1. Hash Tables,[0],[0]
= |Aq|.,3.1. Hash Tables,[0],[0]
It is important to understand the implications of this lemma.,3.1. Hash Tables,[0],[0]
"In particular, the expression for E(Z|W ) says that in a specific realization of a choice of hash functions (or a set of tables), the estimator Z is biased if W 6= |Aq|.",3.1. Hash Tables,[0],[0]
"Therefore K is essential for helping concentrate the realized value of W around its mean.
",3.1. Hash Tables,[0],[0]
"Since in expectation, our estimator Z gives W , we want to understand how many tables K are required to ensure that W concentrates around its mean, |Aq|.",3.1. Hash Tables,[0],[0]
"This is related to the variance of W .
",3.1. Hash Tables,[0],[0]
"We also introduce a new quantity p(x, y) =",3.1. Hash Tables,[0],[0]
P(dqx ∈,3.1. Hash Tables,[0],[0]
"I ∩ dqy ∈ I|θqx, θqy), the collision probability that x and y both land in buckets that are hamming distance I from q over the random choice of hash functions.
",3.1. Hash Tables,[0],[0]
Lemma 3.2 (Variance of W ).,3.1. Hash Tables,[0],[0]
σ2(W ),3.1. Hash Tables,[0],[0]
"= 1 K ∑ x,y∈Aq ( p(x,y) p(x)p(y)",3.1. Hash Tables,[0],[0]
− 1 ),3.1. Hash Tables,[0],[0]
We want to put these pieces together to make a statement about the number of tables K we should create to guarantee low inherent bias in our estimator.,3.1. Hash Tables,[0],[0]
We use Chebyshev’s Inequality to bound W ’s deviation from its mean as a function of K with a constant failure probability 18 .,3.1. Hash Tables,[0],[0]
"For simplicity, we fix a constant failure probability that we will boost later by average over several sets of estimators.",3.1. Hash Tables,[0],[0]
"This analysis is without loss of generality, as the bounds can be adjusted for any desired failure probability δ.",3.1. Hash Tables,[0],[0]
We will use this piece again when we analyze our overall estimator.,3.1. Hash Tables,[0],[0]
Lemma 3.3 (Bound on Number of Tables).,3.1. Hash Tables,[0],[0]
"It suffices to make K ≥ 8ε2 min
x∈Aq p(x) tables to guarantee that W is within
ε of |Aq| (relatively) with probability 78 .
",3.1. Hash Tables,[0],[0]
Proof.,3.1. Hash Tables,[0],[0]
Chebyshev’s inequality states: P(|W − |Aq|| ≥ ε|Aq|) ≤ σ,3.1. Hash Tables,[0],[0]
2(W ),3.1. Hash Tables,[0],[0]
"ε2|Aq|2 .
",3.1. Hash Tables,[0],[0]
"Therefore, to achieve a constant failure probability δ = 18 , it suffices to create enough tables so that
σ2(W )",3.1. Hash Tables,[0],[0]
"= 1
K ∑ x,y∈Aq ( p(x, y) p(x)p(y)",3.1. Hash Tables,[0],[0]
− 1 ) ≤ ε 2|Aq|2,3.1. Hash Tables,[0],[0]
"8
",3.1. Hash Tables,[0],[0]
"Hence K needs to be large enough so that: K ≥ 8 ∑ x,y∈Aq ( p(x,y) p(x)p(y)",3.1. Hash Tables,[0],[0]
"− 1 ) ε2|Aq|2
Since p(x, y) ≤ min{p(x), p(y)}, we see that it is sufficient for K to satisfy
K ≥ 8|Aq|2
( minx∈Aq 1 p(x)",3.1. Hash Tables,[0],[0]
"− 1 ) ε2|Aq|2
Therefore we conclude with the following bound on K:
K ≥ 8 ε2 min
x∈Aq p(x)
(2)
We emphasize that the joint probability p(x, y) ≤ min{p(x), p(y)} is a very loose worst-case bound assuming high correlation between data points.",3.1. Hash Tables,[0],[0]
"The final bound for K, Equation (2), is also a worst-case bound in the sense that it is possible that a very minuscule fraction of x ∈ Aq have small values for p(x).",3.1. Hash Tables,[0],[0]
"In the experimental section of the paper, we do an empirical analysis of the inherent bias for different values of K and demonstrate that for real datasets the number of tables needed can be far fewer than what is theoretically required in the worst case scenario.",3.1. Hash Tables,[0],[0]
"Query q maps to a bucket ik for each table k = 1, 2 . .",3.2. Counts Vector,[0],[0]
.K.,3.2. Counts Vector,[0],[0]
"The preprocessing step produces an average counts vector corresponding to bucket ik, denoted Ckq , where C k q",3.2. Counts Vector,[0],[0]
"(i) is the count of the total number of items in buckets that are at hamming distance i = 0, 1, . . .",3.2. Counts Vector,[0],[0]
t away from the hash address for q in,3.2. Counts Vector,[0],[0]
table k.,3.2. Counts Vector,[0],[0]
"For the hamming distances of interest I , we let Ckq (I) = ∑ d∈I C k",3.2. Counts Vector,[0],[0]
q,3.2. Counts Vector,[0],[0]
"(d).
",3.2. Counts Vector,[0],[0]
Ckq (I) is an integral part of our weighted importance sampling scheme.,3.2. Counts Vector,[0],[0]
"In Appendix A, we show how to compute these vectors efficiently.
",3.2. Counts Vector,[0],[0]
Theorem 3.1 (Aggregate-Counts).,3.2. Counts Vector,[0],[0]
"Given a set of K hash tables, each with 2t hash buckets with addresses in {0, 1}t, Aggregate-Counts (Algorithm 1) computes, for each hash address i, the number of elements in buckets that are hamming distance 0, 1, . . .",3.2. Counts Vector,[0],[0]
"t away from i, in each of the K tables, in time O(Kt22t).
",3.2. Counts Vector,[0],[0]
"Note that the t in our hashing scheme is the length of the hash sequence; as a general rule of thumb, for bit-wise hash functions, implementers choose t",3.2. Counts Vector,[0],[0]
"≈ log(n), so as to average out to one element per hash bucket.",3.2. Counts Vector,[0],[0]
"Therefore, the preprocessing runtime of a reasonable hashing implementation for Aggregate-Counts (Algorithm 1) is approximately O(nK log2(n)).
",3.2. Counts Vector,[0],[0]
"The key benefit of Aggregate-Counts is that it computes via a message-passing or dynamic programming strategy that is much more efficient than a naive brute-force approach that would take time O(K22t), or O(Kn2) if t ≈ log(n).",3.2. Counts Vector,[0],[0]
"We create a sampler that, given a hash address ik for each table, and a set of hamming distances I that we want to sample from, generates a sample uniformly at random from the union of elements that were hashed to hamming distance I across the K tables.",3.3. Sampler,[0],[0]
"For an implementation and analysis, please consult Appendix B.
Theorem 3.2 (Sampler).",3.3. Sampler,[0],[0]
"Given a set of K hash tables, each with 2t hash buckets with addresses in {0, 1}t, a sampling scheme consisting of a data structure and a sampling algorithm can generate a sample uniformly at random from any fixed hash table k, an element at hamming distance d to hash address i.",3.3. Sampler,[0],[0]
"The data structure is a counts matrix that can be precomputed in preprocessing time O(Kt32t), and the sampling algorithm Hamming-Distance-Sampler (Algorithm 2) generates a sample in time O(t).
",3.3. Sampler,[0],[0]
"Again, if we follow t ≈ log(n), the preprocessing time comes out to roughly O(nK log3(n)).",3.3. Sampler,[0],[0]
"Also we expect the O(t) online sample generation cost to be negligible compared to, say, the inner product computation cost for q · x,
which our method and all competing methods use.",3.3. Sampler,[0],[0]
We describe the importance sampling scheme in the next section.,3.3. Sampler,[0],[0]
We now analyze our sampling algorithm.,4. Sampling,[0],[0]
Recall that our sampling scheme works in the following way.,4. Sampling,[0],[0]
"Given query q, we generate the hash for q in each of our K tables, by solving for ik = gk(q) for k = 1, . .",4. Sampling,[0],[0]
.K.,4. Sampling,[0],[0]
"Given the hash for q in each of our K tables and the set of hamming distances I that we want to sample from, we invoke our sampler to generate a sample from across the K tables.
",4. Sampling,[0],[0]
"Given this sample, x, we compute the angular distance θqx = cos
−1(q · x).",4. Sampling,[0],[0]
"Let p(x) = P(dqx ∈ I|θqx), the collision probability that x lands in a bucket that is hamming distance I from q over the random choice of hash functions; p(x) is an endogenous property of an LSH function.
",4. Sampling,[0],[0]
"We score each sample as in Equation (1).
",4. Sampling,[0],[0]
"We take S samples and construct Z1, Z2, . . .",4. Sampling,[0],[0]
ZS .,4. Sampling,[0],[0]
We report∑S i=1,4. Sampling,[0],[0]
"Zi S as our estimate for |Aq|.
",4. Sampling,[0],[0]
"As an immediate consequence of Lemma 3.1, it is clear that
E",4. Sampling,[0],[0]
[∑S i=1,4. Sampling,[0],[0]
"Zi S ] = |Aq| .
",4. Sampling,[0],[0]
"Now we analyze the variance of our estimator:
Lemma 4.1 (Variance of Estimator).
",4. Sampling,[0],[0]
"E (∑Si=1 Zi S − |Aq| )2 ≤ E[Z2] S + σ2(W )
",4. Sampling,[0],[0]
This decomposition of the variance into the two terms indicates that the variance is coming from two sources.,4. Sampling,[0],[0]
"The first source is the variance of the samples, E[Z
2] S .",4. Sampling,[0],[0]
"If we don’t take
enough samples, we do not get a good estimate.",4. Sampling,[0],[0]
"The second source is the variance from the random variable W , σ2(W ), which corresponds to the contents in the tables.",4. Sampling,[0],[0]
"As we have shown, it is crucial to create enough tables so that W is concentrated around its expectation, |Aq|.",4. Sampling,[0],[0]
"Therefore, this second source of variance of the overall estimator comes from the variance of the hash functions that underlie table creation and composition.
",4. Sampling,[0],[0]
"The σ2(W ) term has already been analyzed in Section 3.1, see Lemma 3.2.",4. Sampling,[0],[0]
"Now we analyze the second moment of Z.
Lemma 4.2 (Variance of Z).
",4. Sampling,[0],[0]
"E[Z2] = ∑ x∈Aq ∑ y∈D [ p(x, y) K · p(x)2 +",4. Sampling,[0],[0]
"( 1− 1 K ) p(y) p(x) ]
Now that we have all the components, we are ready to put together the final sample and storage complexities for our estimator.",4. Sampling,[0],[0]
"We want a final estimate that concentrates with at most error around its mean, |Aq| with probability 1− δ.",4. Sampling,[0],[0]
"To do this, we make several sets 1, 2, . .",4. Sampling,[0],[0]
.M,4. Sampling,[0],[0]
of our estimator (one estimator consists of a set of K tables and S samples).,4. Sampling,[0],[0]
"We choose K and S so that the failure probability of our estimator is a constant, say 14 .",4. Sampling,[0],[0]
"Each estimator produces an estimate, call it Em, for m ∈ {1, . .",4. Sampling,[0],[0]
.M},4. Sampling,[0],[0]
.,4. Sampling,[0],[0]
We report our final estimate as the median of these estimates.,4. Sampling,[0],[0]
"This is the classic Median-of-Means technique.
",4. Sampling,[0],[0]
Let Fm be the indicator variable indicating if the estimator Em fails to concentrate.,4. Sampling,[0],[0]
Clearly E(Fm) ≤ 14 .,4. Sampling,[0],[0]
"Moreover, E(F = ∑M m=1 Fm) ≤",4. Sampling,[0],[0]
M 4 .,4. Sampling,[0],[0]
"The probability that the median estimate is bad, P(median of Emfails) ≤",4. Sampling,[0],[0]
P(half of Em fails) =,4. Sampling,[0],[0]
P(F ≥ M2 ).,4. Sampling,[0],[0]
"By a simple Chernoff bound, we see that: P(F ≥ M2 ) ≤",4. Sampling,[0],[0]
"e
−(2 ln 2−1)M4 ≤ e−M11 .",4. Sampling,[0],[0]
"So to satisfy a desired failure probability δ, it suffices to have e
−M 11 ≤ δ, therefore M ∈ O(log( 1δ )).
",4. Sampling,[0],[0]
"In the rest of the section, we establish bounds on K and S so that one estimator fails with probability at most 14 .",4. Sampling,[0],[0]
"We appeal again to Chebyshev’s Inequality:
P (∣∣∣∣∣ ∑S i=1",4. Sampling,[0],[0]
Zi S,4. Sampling,[0],[0]
− |Aq| ∣∣∣∣∣,4. Sampling,[0],[0]
≥ ε|Aq| ) ≤ σ2( ∑S i=1,4. Sampling,[0],[0]
Zi S ),4. Sampling,[0],[0]
"ε2|Aq|2
In Lemma 4.1, we analyze the variance of our estimator, and show that σ2( ∑S i=1",4. Sampling,[0],[0]
"Zi S ) ≤ E[Z2] S + σ
2(W ).",4. Sampling,[0],[0]
"Therefore, in order so that the failure probability is less than 14 , it suffices
to have σ2( ∑S
i=1",4. Sampling,[0],[0]
Zi S ),4. Sampling,[0],[0]
"≤ ε2|Aq|2 4 , which can be obtained by
letting E[Z 2] S ≤ ε2|Aq|2 8 and σ 2(W ) ≤ ε",4. Sampling,[0],[0]
"2|Aq|2 8 .
",4. Sampling,[0],[0]
"Focusing on the σ2(W ) term, which depends on the number of tables K created, we show in Lemma 3.3 from Section 3.1 that it suffices to take K ≥ 8ε2",4. Sampling,[0],[0]
"min
x∈Aq p(x) .
",4. Sampling,[0],[0]
"Now that we have our table complexity, we can analyze our sampling complexity S to bound E[Z
2] S .
Lemma 4.1.",4. Sampling,[0],[0]
Suppose K ≥ 8ε2 min x∈Aq p(x) .,4. Sampling,[0],[0]
"Then S ∈
O
( E(Cq(I))
ε2|Aq|· min x∈Aq p(x)
) suffices to achieve E[Z
2] S ≤ ε2|Aq|2 8 .
",4. Sampling,[0],[0]
Proof.,4. Sampling,[0],[0]
"By Lemma 4.2 we have:
E[Z2] S = 1 S ∑ x∈Aq ∑ y∈D [ p(x, y) K · p(x)2 +",4. Sampling,[0],[0]
"( 1− 1 K ) p(y) p(x) ]
Substituting for K ≥ 8ε2",4. Sampling,[0],[0]
"min x∈Aq p(x) gives:
E[Z2] S ≤ 1 S ∑ x∈Aq ∑ y∈D ε2p(x, y) minx∈Aq p(x) 8p(x)2 + p(y) p(x)  ≤ 1 S ∑ x∈Aq ∑ y∈D [ ε2p(x, y) 8p(x) + p(y) p(x) ]
≤ 1 S ∑ x∈Aq ∑ y∈D",4. Sampling,[0],[0]
"[ (1 + ε2) p(y) p(x) ]
",4. Sampling,[0],[0]
"In order to guarantee E[Z 2] S ≤ ε2|Aq|2 8 , we need:
S ≥
∑ x∈Aq ∑",4. Sampling,[0],[0]
y∈D,4. Sampling,[0],[0]
[ (1 + ε2) p(y)p(x) ],4. Sampling,[0],[0]
"ε2|Aq|2
= (1 + ε2)
∑ x∈Aq 1 p(x) ∑ y∈D p(y)
",4. Sampling,[0],[0]
"ε2|Aq|2
= (1 + 1
ε2 )
∑ x∈Aq
1 p(x) E(Cq(I))",4. Sampling,[0],[0]
"|Aq|2
Therefore, we conclude that
S ∈ O  E(Cq(I)) ε2|Aq| · min
x∈Aq p(x)  is sufficient.
",4. Sampling,[0],[0]
"Putting together Lemmas 3.3 and 4.1 with the median of means strategy yields our main result, Theorem 2.1.
",4. Sampling,[0],[0]
In the rest of this paper we discuss the results of our experiments on real datasets.,4. Sampling,[0],[0]
We describe our experiments using the GLOVE dataset.,5. Experiments,[0],[0]
"We use the set of 400,000 pre-trained 50-dimensional word embedding vectors trained from Wikipedia 2014 + Gigaword 5, provided by (Pennington et al., 2014).",5. Experiments,[0],[0]
"We normalize the embeddings, as is standard in many word embedding applications (Sugawara et al., 2016)",5. Experiments,[0],[0]
"We choose 3 query words with different neighborhood profiles: “venice”, “cake”, “book”.",5. Experiments,[0],[0]
"Venice has the smallest neighborhood, with 206 elements with angular distance less than 60 degrees, cake has a medium sized neighborhood with about 698 elements, book has the largest neighborhood with 1275 elements.",5. Experiments,[0],[0]
"The histogram for these 3 queries are shown in Figure 1.
",5. Experiments,[0],[0]
"We also choose our angle range of interest, A, to be 0- 60 degrees.",5. Experiments,[0],[0]
"A search through our dataset gave “florence”, “cannes”, “rome” as representative elements that are 40-50
degrees from “venice”, and “renaissance”, “milan”, “tuscany”, “italy” in the 50-60 degree range.",5. Experiments,[0],[0]
"Terms such as “cheesecake”, “desserts”, “ganache”, and “bakes” appear in the 40-50 degree annulus around “cake”, while terms such as “fruitcake”, “cupcake”, “confections”, “poundcake”, and “eggs” appear in the 50-60 degree histogram.",5. Experiments,[0],[0]
"For “book”: “character”, “chronicles”, “paperback”, “authors”, and “text” are in the 40-50 degree range while “bestseller”, “protagonist”, “publishers”, “booklet”, “publishes”, “editing”, “monograph”, and “chapter” are in the 50-60 degree range.",5. Experiments,[0],[0]
"This particular experiment shows that while elements in the 40-50 degree range are extremely related, words in the 50-60 degree range are also relevant, and so we fix A to be 0-60 degrees in all of our experiments.",5. Experiments,[0],[0]
"We also fix t = 20 in all of our experiments, since we have 400,000 embeddings in total and 20 ≈ log2(400, 000).
",5. Experiments,[0],[0]
"As Table 1 illustrates, the biggest challenge for this estimation problem is the fact that the count of the number of elements within 0-60 degrees is dwarfed by the number of elements 60-120 degrees away from the queries.",5. Experiments,[0],[0]
"This issue makes locality sensitive techniques necessary for efficient search and retrieval in high dimensions.
",5. Experiments,[0],[0]
Table 1:,5. Experiments,[0],[0]
"Statistics of Queries
QUERY #",5. Experiments,[0],[0]
"WITHIN 60 DEGREES % OF POPULATION
VENICE 206 .0515",5. Experiments,[0],[0]
CAKE 698 .1745,5. Experiments,[0],[0]
"BOOK 1275 .31875
As we have previously mentioned in section 3.1, the number of tables K theoretically required for (near) unbiased estimation relies on a worst-case variance bound; real-world data do not necessarily exhibit worst-case behavior.",5. Experiments,[0],[0]
"In our studies of our 3 queries see Figures 2, the inherent bias of our estimator decreases as we increase the sampling ham-
ming threshold.",5. Experiments,[0],[0]
"This is as expected, using a larger range of hamming distances helps concentrate the count of the elements of interest Aq that fall into the specified range of hamming distances around the mean, which means that a smaller K is required to achieve small bias.
",5. Experiments,[0],[0]
"Moreover, the empirical bias of our estimator at hamming threshold 5 is around 5% for 20 hash tables, with very little improvement with 40 hash tables.",5. Experiments,[0],[0]
This is consistent with our 3 queries.,5. Experiments,[0],[0]
"With this in mind, we compare our estimator against the benchmark estimator introduced by (Spring & Shrivastava, 2017).",5. Experiments,[0],[0]
"Though their work originally intended to solve a different problem, their technique can solve our problem by adapting the weight function appropriately.",5. Experiments,[0],[0]
"The key differences between their work and ours is that they only probe the 0 hamming distance bucket in each table, similar to the classic LSH literature, and instead of sampling, they simply enumerate the elements in the hamming distance 0 bucket for each table.",5. Experiments,[0],[0]
"For higher values of K, which our experiments demonstrate that their estimator needs in order to get good results, enumeration might not be so efficient.
",5. Experiments,[0],[0]
"In Figure 3, we compare (Spring & Shrivastava, 2017)’s technique of enumerating and importance-weighting hamming distance 0 elements to our technique of importance sampling from different hamming thresholds.",5. Experiments,[0],[0]
"Our experiments use random hyperplane LSH and we report relative error averaged over 25 trials, where in each trial we generate a new set of K tables.",5. Experiments,[0],[0]
"Panel (b) experiments with (Spring & Shrivastava, 2017)’s technique for the 3 queries, with different choices of K, the number of tables.",5. Experiments,[0],[0]
"Our results show that even for K = 40 tables, the relative error of their technique can still be higher than 50%, particularly for queries with small neighborhoods such as “venice”.",5. Experiments,[0],[0]
For “venice” the increase in table allocation from 20 to 40 made a very small difference to the overall estimation error.,5. Experiments,[0],[0]
"“book” and “cake” fared better at 40 tables, however, the error was still around 25 %, while our estimator (panel a) estimated to within about 10% error using only 20 tables.
",5. Experiments,[0],[0]
Panel (a) of Figure 3 shows that utilizing any hamming threshold greater than 0 gives superior estimation performance to staying only within the 0 hamming distance bucket.,5. Experiments,[0],[0]
"In this experiment, we fix our sampling budget to 1000 samples and the table budget to 20 tables.",5. Experiments,[0],[0]
The hamming distance 0 error reported in this figure uses enumeration; all other hamming thresholds use the 1000 sampling budget.,5. Experiments,[0],[0]
"In our experiments for the 3 queries, one can expect about 80 points in total in the hamming distance 0 buckets across 20 tables.",5. Experiments,[0],[0]
"In this experiment, our technique uses 1000 samples vs 80 points, however, this (somewhat negligible in today’s computing infrastructure) sample complexity trades off against a large improvement in precision, as well as a much lower storage cost in the number of tables K.
Finally, we note that panel (a) of Figure 3 shows the smallest
error for “venice” at hamming threshold 3.",5. Experiments,[0],[0]
This is related to the characteristics of this query and the sampling budget.,5. Experiments,[0],[0]
"We see in this example that for “venice”, which is a fairly isolated data point compared to the other 2 queries, going to further hamming distances actually hurts the quality of the estimate because we actually dilute the proportion of interesting elements.",5. Experiments,[0],[0]
"Using higher thresholds typically requires more samples, as shown in Figure 4.",5. Experiments,[0],[0]
"However, higher thresholds typically lowers the inherent bias in the importance sampling scheme, as demonstrated in Figure 2.",5. Experiments,[0],[0]
Implementers should consider this tradeoff in their algorithmic design choices.,5. Experiments,[0],[0]
"Given the case study of our estimator achieving the smallest estimation error for “venice” at hamming threshold 3, whereas for the more popular queries “cake” and “book” performance improves steadily at higher hamming thresholds, it would be interesting to, from the practitioner’s point of view, understand what is the best hamming threshold to sample from, and given a hamming threshold, how many samples should be taken for a quality estimate.",6. Discussion,[0],[0]
"The optimal
sample complexity is data-dependent, and cannot be known without a sense of |Aq|, the very quantity we aim to estimate.",6. Discussion,[0],[0]
"But instead of fixing the sample complexity up-front, is there a way we can iteratively, in an on-line fashion, determine whether we should keep sampling or stop, based on a current belief of |Aq|?",6. Discussion,[0],[0]
"This work was initiated while the authors were visiting Laserlike, Inc.",Acknowledgements,[0],[0]
Xian Wu was supported by a Harold Thomas Hahn Jr. Fellowship from the Department of Management Science and Engineering at Stanford University.,Acknowledgements,[0],[0]
Moses Charikar was supported by NSF grant CCF-1617577 and a Simons Investigator Award.,Acknowledgements,[0],[0]
"An important question that arises in the study of high dimensional vector representations learned from data is: given a set D of vectors and a query q, estimate the number of points within a specified distance threshold of q. Our algorithm uses locality sensitive hashing to preprocess the data to accurately and efficiently estimate the answers to such questions via an unbiased estimator that uses importance sampling.",abstractText,[0],[0]
A key innovation is the ability to maintain a small number of hash tables via preprocessing data structures and algorithms that sample from multiple buckets in each hash table.,abstractText,[0],[0]
"We give bounds on the space requirements and query complexity of our scheme, and demonstrate the effectiveness of our algorithm by experiments on a standard word embedding dataset.",abstractText,[0],[0]
Local Density Estimation in High Dimensions,title,[0],[0]
"Hypothesis testing is a widely applied statistical tool used to test whether given models should be rejected, or not, based on sampled data from a population.",1. Introduction,[0],[0]
"Hypothesis testing was initially developed for scientific and survey data, but today it is also an essential tool to test models over collections of social network, mobile, and crowdsourced data (American Statistical Association, 2014; Hunter et al., 2008; Steele et al., 2017).",1. Introduction,[0],[0]
"Collected data samples may contain highly sensitive information about the subjects, and the privacy of individuals can be compromised when the results of a data analysis are released.",1. Introduction,[0],[0]
A way to address this concern is by developing new techniques to support privacy-preserving data analysis.,1. Introduction,[0],[0]
"Among the different approaches, differential privacy (Dwork et al., 2006b) has emerged as a viable solution: it provides strong privacy guarantees and it allows to release accurate statistics.",1. Introduction,[0],[0]
A standard way to achieve differential privacy is by injecting some statistical noise in the computation of the data analysis.,1. Introduction,[0],[0]
"When the noise is carefully chosen, it helps to protect the individual privacy without compromising the utility of the data analysis.",1. Introduction,[0],[0]
"Several recent works have studied differentially private hypothesis tests
*Equal contribution 1 University at Buffalo, Buffalo, NY, USA 2University of Pennsylvania, Philadelphia, PA, USA.",1. Introduction,[0],[0]
"Correspondence to: Marco Gaboardi <gaboardi@buffalo.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"that can be used in place of the standard, non-private hypothesis tests (Uhler et al., 2013; Yu et al., 2014; Sheffet, 2015; Karwa & Slavković, 2016; Wang et al., 2015; Gaboardi et al., 2016; Kifer & Rogers, 2017; Cai et al., 2017).",1. Introduction,[0],[0]
These tests work in the curator model of differential privacy.,1. Introduction,[0],[0]
"In this model, the data is centrally stored and the curator carefully injects noise in the computation of the data analysis in order to satisfy differential privacy.
",1. Introduction,[0],[0]
"In this work we instead address the local model of privacy, formally introduced by Raskhodnikova et al. (2008).",1. Introduction,[0],[0]
"The first differentially private algorithm called randomized response – in fact it predates the definition of differential privacy by more than 40 years – guarantees differential privacy in the local model (Warner, 1965).",1. Introduction,[0],[0]
"In this model, there is no trusted centralized entity that is responsible for the noise injection.",1. Introduction,[0],[0]
"Instead, each individual adds enough noise to guarantee differential privacy for their own data, which provides a stronger privacy guarantee than the curator model.",1. Introduction,[0],[0]
The data analysis is then run over the collection of the individually sanitized data.,1. Introduction,[0],[0]
"The local model of differential privacy is a convenient model for several applications: for example it is used to collect statistics about the activity of the Google Chrome Web browser users (Erlingsson et al., 2014), and to collect statistics about the typing patterns of Apple’s iPhone users (Apple Press Info, 2016).",1. Introduction,[0],[0]
"Despite these applications, the local model has received far less attention than the centralized curator model.",1. Introduction,[0],[0]
"This is in part due to the more firm requirements imposed by this model, which make the design of effective data analysis harder.
",1. Introduction,[0],[0]
Our main contribution is in designing chi-square hypothesis tests for the local model of differential privacy.,1. Introduction,[0],[0]
Similar to previous works we focus on goodness of fit and independence hypothesis tests.,1. Introduction,[0],[0]
"Most of the private chi-square tests proposed so far are based on mechanisms that add noise in some form to the aggregate data, e.g. the cells of the contingency tables, or the resulting chi-square statistics value.",1. Introduction,[0],[0]
"These approaches cannot be used in the local model, since noise needs to be added at the individual’s data level.",1. Introduction,[0],[0]
"We then consider instead general privatizing techniques in the local model, and we study how to build new hypothesis tests with them.",1. Introduction,[0],[0]
Each test we present is characterized by a specific local model mechanism.,1. Introduction,[0],[0]
"The main technical challenge for designing each test is to create statistics, which incorporate the local model mechanisms, that converge as
we collect more data to a chi-square distribution, as in the classical chi-square tests.",1. Introduction,[0],[0]
"We then use these statistics to find the critical value to correctly bound the Type I error.
",1. Introduction,[0],[0]
"We present three different goodness of fit tests: LocalNoiseGOF presents a statistic that guarantees the convergence to a chi-square distribution under the null hypothesis so that we can use the correct critical values when local (concentrated) differential privacy is guaranteed by adding Laplace or Gaussian noise to the individual data; LocalGenRRGOF also provides a statistic that converges to a chi-square under the null hypothesis when a private value for each individual is selected by using a generalized form of randomized response, which can also be thought of as an instantiation of the exponential mechanism (McSherry & Talwar, 2007); finally, LocalBitFlipGOF introduces a statistic that converges to a chi-square distribution when the data is privatized using a bit flipping algorithm (Bassily & Smith, 2015), which provide better accuracy for higher dimensional data.",1. Introduction,[0],[0]
"Further, we develop corresponding independence tests: LocalNoiseIND (see supplementary file), LocalGenRRIND, and LocalBitFlipIND.",1. Introduction,[0],[0]
For all these tests we study their asymptotic behavior.,1. Introduction,[0],[0]
"A desiderata for private hypothesis tests is to have a guaranteed upper bound on the probability of a false discovery (or Type I error) – rejecting a null hypothesis or model when the data was actually generated from it – and to minimize the probability of a Type II error, which is failing to reject the null hypothesis when the model is indeed false.",1. Introduction,[0],[0]
This latter criteria corresponds to the power of the statistical test.,1. Introduction,[0],[0]
We then present experimental results showing the power of the different tests which demonstrates that no single local differentially private algorithm is best across all data dimensions and privacy parameter regimes.,1. Introduction,[0],[0]
"However, this evaluation also shows a relation between the power of the test and the noncentral parameter of the test statistic that is used.",1. Introduction,[0],[0]
"This suggests that besides looking at the parameters of the test, a data analyst may need also to consider which test statistic results in the largest noncentral parameter.",1. Introduction,[0],[0]
"There have been several works in developing private hypothesis test for categorical data, but all look at the traditional model of (concentrated) differential privacy instead of the local model, which we consider here.",2. Related Works,[0],[0]
"Several works have explored private statistical inference for GWAS data, (Uhler et al., 2013; Yu et al., 2014; Johnson & Shmatikov, 2013).",2. Related Works,[0],[0]
"Following these works, there has also been general work in private chi-square hypothesis tests, where the main tests are for goodness of fit and independence testing, although some do extend to more general tests (Wang et al., 2015; Gaboardi et al., 2016; Kifer & Rogers, 2017; Cai et al., 2017; Kakizaki et al., 2017).",2. Related Works,[0],[0]
"Among these, the works most related to
ours are the ones by Gaboardi et al. (2016); Kifer & Rogers (2017).",2. Related Works,[0],[0]
"One of our mechanisms, LocalNoiseGOF, can be seen as an adaptation of their techniques to the local model.",2. Related Works,[0],[0]
"However, the other mechanisms we introduce differ substantially and require novel asymptotic analyses.",2. Related Works,[0],[0]
"There has also been work in private hypothesis testing for ordinary least squares regression (Sheffet, 2015).
",2. Related Works,[0],[0]
Duchi et al. (2013b;a) focus on controlling disclosure risk in statistical estimation and inference by ensuring the analysis satisfies local differential privacy.,2. Related Works,[0],[0]
"In their work, they show that a generalized version of randomized response gives optimal sample complexity for estimating the multinomial probability vector.",2. Related Works,[0],[0]
We use this idea as the basis for our hypothesis test LocalBitFlipGOF.,2. Related Works,[0],[0]
"Kairouz et al. (2014) also considers hypothesis testing in the local model, although they measure utility in terms of f -divergences and do not give a decision rule, i.e. when to reject a given null hypothesis.",2. Related Works,[0],[0]
"We provide statistics whose distributions asymptotically follow a chi-square distribution, which allows for approximating statistical p-values that can be used in a decision rule.",2. Related Works,[0],[0]
We consider their extremal mechanisms and empirically confirm their result that for small privacy regimes (small ) one mechanism has higher utility than other mechanisms and for large privacy regimes (large ) a different mechanism outperforms the other.,2. Related Works,[0],[0]
"However, we measure utility in terms of the power of a locally private hypothesis test subject to a given Type I error bound.",2. Related Works,[0],[0]
"Other notable works in the local privacy model include Pastore & Gastpar (2016); Kairouz et al. (2016); Ye & Barg (2017)
Independent of this work, another paper (Sheffet, 2018) has addressed local private hypothesis testing.",2. Related Works,[0],[0]
Sheffet (2018) considers finite sample complexity by showing certain test quantities take different values under the null- and alternative-hypothesis.,2. Related Works,[0],[0]
"In this work, we design and analyze asymptotic statistical tests and empirically evaluate the performance of each test for finite samples.",2. Related Works,[0],[0]
"We consider datasets x = (x1, · · · , xn) ∈",3. Preliminaries,[0],[0]
"Xn in some data universe X , typically X = {0, 1}d where d is the dimensionality.",3. Preliminaries,[0],[0]
"We first present the standard definition of differential privacy, as well as its variant concentrated differential privacy.",3. Preliminaries,[0],[0]
"We say that two datasets x,x′ ∈ Xn are neighboring if they differ in at most one element, i.e. ∃i ∈",3. Preliminaries,[0],[0]
[n] such that xi 6= x′i and ∀j 6=,3. Preliminaries,[0],[0]
"i, xj = x′j .",3. Preliminaries,[0],[0]
Definition 3.1 (Dwork et al. (2006b;a)).,3. Preliminaries,[0],[0]
"An algorithm M : Xn → Y is ( , δ)-differentially private (DP)",3. Preliminaries,[0],[0]
"if for all neighboring datasets x,x′ ∈ Xn and for all outcomes S ⊆ Y , we have Pr [M(x) ∈ S] ≤",3. Preliminaries,[0],[0]
e,3. Preliminaries,[0],[0]
Pr [M(x′) ∈ S] + δ.,3. Preliminaries,[0],[0]
Definition 3.2 (Bun & Steinke (2016)).,3. Preliminaries,[0],[0]
"An algorithm M : Xn → Y is ρ-zero-mean concentrated differentially private (zCDP) if for all neighboring datasets
x,",3. Preliminaries,[0],[0]
"x′ ∈ Xn, we have the following bound for all t > 0",3. Preliminaries,[0],[0]
"where the expectation is over outcomes y ∼ M(x), E [ exp ( t ( ln (
Pr[M(x)=y] Pr[M(x′)=y]
)",3. Preliminaries,[0],[0]
− ρ ))],3. Preliminaries,[0],[0]
"≤ et2ρ.
Note that in both of these privacy definitions, it is assumed that all the data is stored in a central location and the algorithm M can access all the data.",3. Preliminaries,[0],[0]
Most of the work in differential privacy has been in this trusted curator model.,3. Preliminaries,[0],[0]
"We then define local differential privacy, formalized by Raskhodnikova et al. (2008) and Dwork & Roth (2014), which does not require the subjects to release their raw data, rather each data entry is perturbed to prevent the true entry from being stored.",3. Preliminaries,[0],[0]
"Thus, local differential privacy ensures one of the strongest privacy guarantees.",3. Preliminaries,[0],[0]
Definition 3.3 (LR Oracle).,3. Preliminaries,[0],[0]
"Given a dataset x, a local randomizer oracle LRx(·, ·) takes as input an index i ∈",3. Preliminaries,[0],[0]
"[n] and an -DP algorithm R, and",3. Preliminaries,[0],[0]
"outputs y ∈ Y chosen according to the distribution of R(xi), i.e. LRx(i, R) = R(xi).
",3. Preliminaries,[0],[0]
Definition 3.4 (Raskhodnikova et al. (2008)).,3. Preliminaries,[0],[0]
"An algorithm M : Xn → Y is ( , δ)-local differentially private (LDP) if it accesses the input database x via the LR oracle LRx with the following restriction: if LR(i, Rj) for j ∈",3. Preliminaries,[0],[0]
"[k] areM’s invocations of LRx on index i, then each Rj for j ∈",3. Preliminaries,[0],[0]
"[k] is ( j , δj)- DP and ∑k j=1 j ≤ , ∑k j=1 δj ≤ δ.
",3. Preliminaries,[0],[0]
"From this we have that a ( , δ)-LDP algorithm is also ( , δ)DP.",3. Preliminaries,[0],[0]
Note that these definitions can be extended to include ρ-local zCDP (LzCDP) where each local randomizer is ρjzCDP and ∑k j=1 ρj ≤ ρ.,3. Preliminaries,[0],[0]
"We point out the following connection between LzCDP and LDP , which follows directly from results in (Bun & Steinke, 2016) Lemma 3.5.",3. Preliminaries,[0],[0]
"If M : Xn → Y is ( , 0)-LDP then it is also 2/2-LzCDP.",3. Preliminaries,[0],[0]
"If M is ρ-LzCDP, then it is also(( ρ+ √ 2ρ ln(2/δ) ) , δ )",3. Preliminaries,[0],[0]
-LDP for any δ > 0.,3. Preliminaries,[0],[0]
"As was studied in (Gaboardi et al., 2016), (Wang et al., 2015), and (Kifer & Rogers, 2017), we will study hypothesis tests with categorical data.",4. Chi-Square Hypothesis Tests,[0],[0]
"A null hypothesis, or model H0 is how we might expect the data to be generated.",4. Chi-Square Hypothesis Tests,[0],[0]
The goal for hypothesis testing is to reject the null hypothesis if the data is not likely to have been generated from the given model.,4. Chi-Square Hypothesis Tests,[0],[0]
"As is common in statistical inference, we want to design hypothesis tests to bound the probability of a false discovery (or Type I error), i.e. rejecting a null hypothesis when the data was actually generated from it, by at most some amount α, such as 5%.",4. Chi-Square Hypothesis Tests,[0],[0]
"However, designing tests that achieve this is easy, because we can just ignore the data and always fail to reject the null hypothesis, i.e. have an inconclusive test.",4. Chi-Square Hypothesis Tests,[0],[0]
"Thus, we want additionally to design our tests so that they can reject H0 if the data was not actually generated from the given model.",4. Chi-Square Hypothesis Tests,[0],[0]
"We then want to minimize
the probability of a Type II error, which is failing to reject H0 when the model is false, subject to a given Type I error.
",4. Chi-Square Hypothesis Tests,[0],[0]
"For goodness of fit testing, we assume that each individual’s dataX i for i ∈",4. Chi-Square Hypothesis Tests,[0],[0]
[n] is sampled i.i.d.,4. Chi-Square Hypothesis Tests,[0],[0]
"from Multinomial(1, p) where p ∈ Rd>0 and pᵀ · 1 = 1.",4. Chi-Square Hypothesis Tests,[0],[0]
The classical chi-square hypothesis test (without privacy) forms the histogramH =,4. Chi-Square Hypothesis Tests,[0],[0]
"(H1, · · · , Hd) = ∑n i=1X i and computes the chi-square
statistic T = ∑d j=1 (Hj−np0j) 2
np0j .",4. Chi-Square Hypothesis Tests,[0],[0]
"The reason for using this
statistic is that it converges in distribution to χ2d−1 as more data is collected, i.e. n → ∞, when H0 : p = p0 holds.",4. Chi-Square Hypothesis Tests,[0],[0]
"Hence, we can ensure the probability of false discovery to be close to α as long as we only reject H0 when T > χ2d−1,1−α where the critical value χ2d−1,1−α is defined as the following
quantity Pr [ χ2d−1 > χ 2 d−1,1−α ] = α.
",4. Chi-Square Hypothesis Tests,[0],[0]
Prior Private Chi-square Tests in the Curator Model.,4. Chi-Square Hypothesis Tests,[0],[0]
"One approach for chi-square private hypothesis tests is to add noise (Gaussian or Laplace) directly to the histogram to ensure privacy and then use the classical test statistic (Gaboardi et al., 2016; Wang et al., 2015) .",4. Chi-Square Hypothesis Tests,[0],[0]
Note that the resulting asymptotic distribution needs to be modified for such changes to the statistic – it is no longer a chi-square random variable.,4. Chi-Square Hypothesis Tests,[0],[0]
"To introduce the different statistics, we will consider goodness of fit testing after adding noise Z from distribution Dn to the histogram of counts H̃ = H + Z , which ensures ρ-zCDP when D = N (0, 1/ρ) and -DP whenD = Lap(2/ ).",4. Chi-Square Hypothesis Tests,[0],[0]
"The chi-square statistic then becomes
T̃ (D) =",4. Chi-Square Hypothesis Tests,[0],[0]
"d∑ i=1
",4. Chi-Square Hypothesis Tests,[0],[0]
( Hi + Zi,4. Chi-Square Hypothesis Tests,[0],[0]
− np0i )2 np0i where Z ∼ Dn.,4. Chi-Square Hypothesis Tests,[0],[0]
"(1)
The previous works then show that this statistic converges in distribution to a linear combination of chi-squared variables, when D ∼ N (0, 1/ρ) and ρ is also decreasing with n.
Kifer & Rogers (2017) showed that modifying the chisquare statistic to account for the additional noise leads to tests with better empirical power.",4. Chi-Square Hypothesis Tests,[0],[0]
The projected statistic from Kifer & Rogers (2017) is the following where we use projection matrix Π,4. Chi-Square Hypothesis Tests,[0],[0]
"defn= ( Id − 1d11 ᵀ ) , middle ma-
trix Mσ = Π ( Diag ( p0 + σ )",4. Chi-Square Hypothesis Tests,[0],[0]
"− p0 ( p0 )ᵀ)−1
Π, and sample noise Z ∼ Dn, with Ĥ = H +Z
T (n) KR (σ;D) = n
( Ĥ
n",4. Chi-Square Hypothesis Tests,[0],[0]
"− p0
)ᵀ",4. Chi-Square Hypothesis Tests,[0],[0]
"Mσ ( Ĥ
n",4. Chi-Square Hypothesis Tests,[0],[0]
"− p0
) (2)
We use D = Lap(2/ ) with σ = 8n 2 for an -DP claim or D = N (0, 1/ρ) with σ = 1nρ for a ρ-zCDP claim.",4. Chi-Square Hypothesis Tests,[0],[0]
"When comparing the power of all our tests, we will be considering the alternate H1 : p = p1n where p1n = p 0 + ∆√ n where 1ᵀ∆ = 0.
",4. Chi-Square Hypothesis Tests,[0],[0]
Theorem 4.1 (Kifer & Rogers (2017)).,4. Chi-Square Hypothesis Tests,[0],[0]
"Under the null hypothesis H0 : p = p0, the statistic T (n) KR ( 1 nρ ; N (0, 1/ρ) ) given in (2) for ρ > 0 converges in distribution to χ2d−1.",4. Chi-Square Hypothesis Tests,[0],[0]
"Further, under the alternate hypothesis H1 : p = p1n, the resulting asymptotic distribution is a noncentral chi-square random variable with d− 1 degrees of freedom and noncentral parameter ∆ᵀ ( Diag(p0)− p0 ( p0 )ᵀ + 1/ρId )−1 ∆
When D = Lap(2/ ), Gaboardi et al. (2016) showed that we can still obtain the null hypothesis distribution using Monte Carlo simulations to estimate the critical value, since the asymptotic distribution will no longer be chi-square.",4. Chi-Square Hypothesis Tests,[0],[0]
"That is, we can obtainm samples from the statistic under the null hypothesis with Laplace noise added to the histogram of counts.",4. Chi-Square Hypothesis Tests,[0],[0]
We can then guarantee that the probability of a false discovery is at most α as long as m > d1/αe.,4. Chi-Square Hypothesis Tests,[0],[0]
We now turn to designing local private goodness of fit tests.,5. Local Private Goodness of Fit,[0],[0]
"We first show how the existing statistics from the previous section can be adapted to the local setting and then develop new tests based on the generalized randomized response mechanism that returns one of d > 1 categories and bit flipping (Bassily & Smith, 2015).",5. Local Private Goodness of Fit,[0],[0]
Each test is locally private because it perturbs each individual’s data through a local randomizer.,5. Local Private Goodness of Fit,[0],[0]
"However, each of them has a different asymptotic behavior and so we need different analyses to identify the different critical values.",5. Local Private Goodness of Fit,[0],[0]
We empirically check the power of each test to see which tests outperform others in different parameter regimes.,5. Local Private Goodness of Fit,[0],[0]
"An interesting result of this analysis is that the power of a test is directly related to the size of the noncentral parameter of the chi-square statistic under the alternate distribution.
",5. Local Private Goodness of Fit,[0],[0]
Testing with Noise Addition.,5. Local Private Goodness of Fit,[0],[0]
"In the local model we can add Z i ∼ N ( 0, 1ρ Id ) independent noise to each individual’s data X i to ensure ρ-LzCDP or Z i i.i.d.∼ Lap ( 2 ) independent noise toX i to ensure -LDP.",5. Local Private Goodness of Fit,[0],[0]
"In either case, the resulting noisy histogram Ĥ = H +Z where Z = ∑ iZ i will have variance that scales with n for fixed privacy parameters , ρ > 0.",5. Local Private Goodness of Fit,[0],[0]
"Consider the case where we add Gaussian noise, which results in the following histogram, Ĥ = H+Z where Z ∼ N ( 0, nρ Id ) .",5. Local Private Goodness of Fit,[0],[0]
"Thus, we can use either statistic T̃ (ρ/n) or T(n)KR (ρ/n), with the latter statistic typically having better empirical power (Kifer & Rogers, 2017).",5. Local Private Goodness of Fit,[0],[0]
"We then give our first local private hypothesis test in Algorithm 1.
Theorem 5.1.",5. Local Private Goodness of Fit,[0],[0]
"LocalNoiseGOF is ρ-LzCDP when D = N (0, 1/ρ) and -LDP when D = Lap(2/ ).
",5. Local Private Goodness of Fit,[0],[0]
"Although we cannot guarantee the probability of a Type I error at most α due to the fact that we use the asymptotic
Algorithm 1 Locally Private GOF Test:LocalNoiseGOF
Input: x = (x1, · · · ,xn), ρ, α, H0 : p = p0. LetH = ∑n `=1 x`
if D = N (0, n/ρ) then Set q = T(n)KR (n/ρ;D) given in (2).",5. Local Private Goodness of Fit,[0],[0]
"if q > χ2d−1,1−α Decision←",5. Local Private Goodness of Fit,[0],[0]
Reject.,5. Local Private Goodness of Fit,[0],[0]
else Decision←,5. Local Private Goodness of Fit,[0],[0]
Fail to Reject.,5. Local Private Goodness of Fit,[0],[0]
end if if D = ∑n i=1 Lap(2/ ),5. Local Private Goodness of Fit,[0],[0]
"then
Set q = T(n)KR ( 8n/ 2;D ) given in (2).",5. Local Private Goodness of Fit,[0],[0]
"Sample m > d1/αe from the distribution of T
(n) KR
( 8n/ 2;D ) assuming H0
Set τ to be the d(m+ 1)(1− α)eth largest sample.",5. Local Private Goodness of Fit,[0],[0]
if q > τ,5. Local Private Goodness of Fit,[0],[0]
Decision←,5. Local Private Goodness of Fit,[0],[0]
Reject.,5. Local Private Goodness of Fit,[0],[0]
else Decision←,5. Local Private Goodness of Fit,[0],[0]
"Fail to Reject.
end if Output: Decision
distribution (as in the tests from prior work and the classical chi-square tests without privacy), we expect the Type I errors to be similar to those from the nonprivate test.",5. Local Private Goodness of Fit,[0],[0]
"Note that the test can be modified to accommodate arbitrary noise distributions, e.g. Laplace to ensure differential privacy.",5. Local Private Goodness of Fit,[0],[0]
"In this case, we can use a Monte Carlo (MC) approach to estimate the critical value τ that ensures the probability of a Type I error is at most α if we reject H0 when the statistic is larger than τ .",5. Local Private Goodness of Fit,[0],[0]
"For the local setting, if each individual perturbs each coordinate by adding Lap (2/ ) then this will ensure our test is -LDP.",5. Local Private Goodness of Fit,[0],[0]
"However, the sum of independent Laplace random variables is not Laplace, so we will need to estimate a sum of n independent Laplace random variables using MC.",5. Local Private Goodness of Fit,[0],[0]
We can do this by sampling m entries from the exact distribution under H0 to find the critical value.,5. Local Private Goodness of Fit,[0],[0]
"In the experiments section we will use this method to compare the power of the other local private tests with the one of the version of LocalNoiseGOF using Laplace noise, which has a better power than the one using Gaussian noise.
",5. Local Private Goodness of Fit,[0],[0]
Testing with Generalized Randomized Response.,5. Local Private Goodness of Fit,[0],[0]
"Rather than having to add noise to each component of the original histogram, we consider applying randomized response to obtain a LDP hypothesis test.",5. Local Private Goodness of Fit,[0],[0]
"We will use a generalized form of randomized response given in Algorithm 2 which takes a single data entry from the set {e1, · · · , ed}, where ej ∈ Rd is the standard basis element with a 1 in the jth coordinate and is zero elsewhere, and reports the original entry with probability slightly more than uniform and otherwise reports a different element with equal probability.",5. Local Private Goodness of Fit,[0],[0]
"Note thatMGenRR is -DP.
",5. Local Private Goodness of Fit,[0],[0]
"We have the following result when we useMGenRR on each data entry to obtain a private histogram.
",5. Local Private Goodness of Fit,[0],[0]
"Algorithm 2 Generalized Randomized Response:MGenRR Input: x ∈ {e1, · · · , ed}, .
",5. Local Private Goodness of Fit,[0],[0]
"Let q(x,z) = 1{x = z} Select x̌ with probability exp[ q(x,x̌)]e −1+d
Output: x̌
Lemma 5.2.",5. Local Private Goodness of Fit,[0],[0]
"If we have histogram H = ∑n i=1X i, where {X i} i.i.d.∼ Multinomial(1, p) and we write Ȟ =∑n
i=1MGenRR(X",5. Local Private Goodness of Fit,[0],[0]
"i, ) for each",5. Local Private Goodness of Fit,[0],[0]
i ∈,5. Local Private Goodness of Fit,[0],[0]
"[n], then Ȟ ∼ Multinomial(n, p̌) where
p̌ = p
( e
e + d− 1
) + (1 − p) ( 1
e + d− 1
) .",5. Local Private Goodness of Fit,[0],[0]
"(3)
Once we have Ȟ , we can create a chi-square statistic by subtracting Ȟ by its expectation and dividing the difference by the expectation.",5. Local Private Goodness of Fit,[0],[0]
"Hence testing H0 : p = p0 after the generalized randomized response mechanism, is equivalent to testing H0 : p = p̌0 with data Ȟ .
",5. Local Private Goodness of Fit,[0],[0]
We can then form a chi-square statistic using the histogram Ȟ which will have the correct asymptotic distribution.,5. Local Private Goodness of Fit,[0],[0]
Theorem 5.3.,5. Local Private Goodness of Fit,[0],[0]
"Let H ∼ Multinomial(n,p) and Ȟ be given in Theorem 5.2 with privacy parameter > 0.",5. Local Private Goodness of Fit,[0],[0]
"Under the null hypothesis H0 : p = p0, we have for p̌0 = 1e",5. Local Private Goodness of Fit,[0],[0]
"+d−1 ( e p0 + (1− p0) ) ,
T (n) GenRR ( ) = d∑ j=1 (Ȟj − np̌0j )2 np̌0j D→ χ2d−1. (4)
",5. Local Private Goodness of Fit,[0],[0]
"Further, with alternate H1 : p = p1n, the resulting asymptotic distribution is a noncentral chi-square distribution with d − 1 degrees of freedom and noncentral parameter,(
e −1",5. Local Private Goodness of Fit,[0],[0]
e +d−1 )2∑d j=1 ∆2j,5. Local Private Goodness of Fit,[0],[0]
"p̌0j .
",5. Local Private Goodness of Fit,[0],[0]
We then base our LDP goodness of fit test on this result to obtain the correct critical value to reject the null hypothesis based on a chi-square distribution.,5. Local Private Goodness of Fit,[0],[0]
The test is presented in Algorithm 3.,5. Local Private Goodness of Fit,[0],[0]
"The following result is immediate from the
Algorithm 3 Local DP GOF Test: LocalGenRRGOF
Input: x = (x1, · · · ,xn), , α, H0 : p = p0.",5. Local Private Goodness of Fit,[0],[0]
Let p̌0 = 1e,5. Local Private Goodness of Fit,[0],[0]
"+d−1 ( e p0 + (1− p0) ) .
",5. Local Private Goodness of Fit,[0],[0]
Let,5. Local Private Goodness of Fit,[0],[0]
"Ȟ = ∑n i=1MGenRR(xi, ).
",5. Local Private Goodness of Fit,[0],[0]
"Set q = ∑d j=1 (Ȟj−np̌0j ) 2
np̌0j
if q > χ2d−1,1−α Decision←",5. Local Private Goodness of Fit,[0],[0]
Reject.,5. Local Private Goodness of Fit,[0],[0]
else Decision←,5. Local Private Goodness of Fit,[0],[0]
"Fail to Reject.
",5. Local Private Goodness of Fit,[0],[0]
"Output: Decision
generalized randomized response mechanism being -DP and the fact that we use it as a local randomizer.
",5. Local Private Goodness of Fit,[0],[0]
Theorem 5.4.,5. Local Private Goodness of Fit,[0],[0]
"LocalGenRRGOF is -LDP.
",5. Local Private Goodness of Fit,[0],[0]
Testing with Bit Flipping.,5. Local Private Goodness of Fit,[0],[0]
"Note that the noncentral parameter in Theorem 5.3 goes to zero as d grows large due
to the coefficient being (
e −1 e +d−1
)2 .",5. Local Private Goodness of Fit,[0],[0]
"Thus, for large dimen-
sional data the generalized randomized response cannot reject a false null hypothesis.",5. Local Private Goodness of Fit,[0],[0]
"We next consider another differentially private algorithmM : {e1, · · · , ed} → {0, 1}d, given in Algorithm 4 used in (Bassily & Smith, 2015) that flips each bit with some biased probability.",5. Local Private Goodness of Fit,[0],[0]
"1
Algorithm 4 Bit Flip Local Randomizer:Mbit Input: x ∈ {e1, · · · , ed}, .
for j ∈",5. Local Private Goodness of Fit,[0],[0]
"[d] do Set zj = xj with probability e /2
e /2+1 , otherwise zj =
(1− xj).",5. Local Private Goodness of Fit,[0],[0]
"end for
Output: z
Theorem 5.5.",5. Local Private Goodness of Fit,[0],[0]
"The algorithmMbit is -DP.
",5. Local Private Goodness of Fit,[0],[0]
"We then want to form a statistic based on the output z ∈ {0, 1}d that is asymptotically distributed as a chi-square under the null hypothesis.",5. Local Private Goodness of Fit,[0],[0]
"We defer the proof to the supplementary material.
",5. Local Private Goodness of Fit,[0],[0]
Lemma 5.6.,5. Local Private Goodness of Fit,[0],[0]
"Consider X i ∼ Multinomial(1, p) for each",5. Local Private Goodness of Fit,[0],[0]
i ∈,5. Local Private Goodness of Fit,[0],[0]
[n].,5. Local Private Goodness of Fit,[0],[0]
We define the following covariance matrix Σ(p) and mean vector p̃ =,5. Local Private Goodness of Fit,[0],[0]
"[(
e /2−1)p+1] e /2+1 , in terms of α = ( e /2−1 e /2+1 ) Σ(p) =α2 [Diag (p)− p (p) ᵀ ]",5. Local Private Goodness of Fit,[0],[0]
"+
e /2( e /2 + 1 )2",5. Local Private Goodness of Fit,[0],[0]
Id (5),5. Local Private Goodness of Fit,[0],[0]
"The histogram H̃ = ∑n i=1Mbit(X i) has the following
asymptotic distribution √ n",5. Local Private Goodness of Fit,[0],[0]
( H̃ n,5. Local Private Goodness of Fit,[0],[0]
"− p̃ ) D→ N (0,Σ(p)) .",5. Local Private Goodness of Fit,[0],[0]
"Further, Σ(p) is invertible for any > 0 and p > 0.
",5. Local Private Goodness of Fit,[0],[0]
"Following a similar analysis in (Kifer & Rogers, 2017), we can form the following statistic for null hypothesis H0 : p = p0 in terms of the histogram H̃ and projection matrix Π = Id − 1d11 ᵀ, as well as the covariance Σ = Σ ( p0 )
and mean p̃0 both given in (5) where we replace p with p0:
T (n)",5. Local Private Goodness of Fit,[0],[0]
"BitFlip ( ) = n
( H̃
n",5. Local Private Goodness of Fit,[0],[0]
"− p̃0
)ᵀ ΠΣ−1Π ( H̃
n",5. Local Private Goodness of Fit,[0],[0]
"− p̃0
) (6)
We can then design a hypothesis test based on the outputs fromMbit in Algorithm 5 Theorem 5.7.",5. Local Private Goodness of Fit,[0],[0]
"LocalBitFlipGOF is -LDP.
",5. Local Private Goodness of Fit,[0],[0]
"1Special thanks to Adam Smith for recommending to use this particular algorithm.
",5. Local Private Goodness of Fit,[0],[0]
"Algorithm 5 Local DP GOF Test: LocalBitFlipGOF
Input: x = (x1, · · · ,xn), , α, H0 : p = p0.",5. Local Private Goodness of Fit,[0],[0]
"Let H̃ = ∑n i=1Mbit(xi, ).
",5. Local Private Goodness of Fit,[0],[0]
"Set q = T(n)BitFlip ( ) if q > χ2d−1,1−α",5. Local Private Goodness of Fit,[0],[0]
Decision← Reject.,5. Local Private Goodness of Fit,[0],[0]
else Decision←,5. Local Private Goodness of Fit,[0],[0]
"Fail to Reject.
",5. Local Private Goodness of Fit,[0],[0]
"Output: Decision
We now show that the statistic in (6) is asymptotically distributed as χ2d−1, with proof in the supplementary file.",5. Local Private Goodness of Fit,[0],[0]
Theorem 5.8.,5. Local Private Goodness of Fit,[0],[0]
"If the null hypothesis H0 : p = p0 holds, then the statistic T(n)BitFlip ( ) is asymptotically distributed as a chisquare, i.e. T(n)BitFlip ( ) D→ χ2d−1.",5. Local Private Goodness of Fit,[0],[0]
"Further, if we consider the alternate H1 : p = p1 then T (n) BitFlip ( ) converges in distribution to a noncentral chi-square with d−1 degrees of freedom and noncentral parameter ( e /2−1 e /2+1 )2 ·∆ᵀΣ(p0)−1∆.
Comparison of Noncentral Parameters.",5. Local Private Goodness of Fit,[0],[0]
"We now compare the noncentral parameters of the three local private tests we presented in Algorithms 1, 3 and 5.",5. Local Private Goodness of Fit,[0],[0]
"We consider the null hypothesis p0 = (1/d, · · · , 1/d) for d > 2, and alternate H1 : p = p
0 + ∆√ n .",5. Local Private Goodness of Fit,[0],[0]
"In this case, we can easily compare the various noncentral parameters for various privacy parameters and dimensions d.",5. Local Private Goodness of Fit,[0],[0]
In Figure 1 we give the coefficient to the term ∆ᵀ∆ in the noncentral parameter of the asymptotic distribution for each local private test presented thus far.,5. Local Private Goodness of Fit,[0],[0]
"The larger this coefficient is, the better the power will be for any alternate ∆ vector.",5. Local Private Goodness of Fit,[0],[0]
"Note that in LocalNoiseGOF, we set ρ = 2/8 which makes the variance the same as for a random variable distributed as Lap(2/ ) for an -DP guarantee – recall that LocalNoiseGOF with Gaussian noise does not satisfy -DP for any > 0.",5. Local Private Goodness of Fit,[0],[0]
"We give results for ∈ {1, 2, 3, 4} which are all in the range of privacy parameters that have been considered in actual locally differentially private algorithms used in practice.2 From
2In (Erlingsson et al., 2014), we know that Google uses = ln(3) in RAPPOR and from Aleksandra Korolova’s Twitter post on Sept. 13, 2016 https://twitter.com/korolova/
the plots, we see how LocalGenRRGOF may outperform LocalBitFlipGOF depending on the privacy parameter and dimension of the data.",5. Local Private Goodness of Fit,[0],[0]
We can use these plots to determine which test to use given and the dimension of data d.,5. Local Private Goodness of Fit,[0],[0]
"When H0 is not uniform, we can use the noncentral parameters given for each test to find the test with the largest noncentral parameter for a particular privacy budget .
",5. Local Private Goodness of Fit,[0],[0]
Empirical Results.,5. Local Private Goodness of Fit,[0],[0]
"We then empirically compare the power between LocalNoiseGOF with Laplace noise in Algorithm 1, LocalGenRRGOF in Algorithm 3, and LocalBitFlipGOF in Algorithm 5.",5. Local Private Goodness of Fit,[0],[0]
Recall that all three of these tests have the same privacy benchmark of local differential privacy.,5. Local Private Goodness of Fit,[0],[0]
"For LocalNoiseGOF with Laplace noise, we will use m = 999 samples in our Monte Carlo simulations.",5. Local Private Goodness of Fit,[0],[0]
"In our experiments we fix α = 0.05 and ∈ {1, 2, 4}.",5. Local Private Goodness of Fit,[0],[0]
"We then consider null hypotheses of the form p0 = (1/d, 1/d, · · · , 1/d) and alternate H1 : p = p0 + η(1,−1, · · · , 1,−1) for some η > 0.",5. Local Private Goodness of Fit,[0],[0]
"In Figure 2, we plot the number of times our tests correctly rejects the null hypothesis in 1000 independent trials for various sample sizes n and privacy parameters .",5. Local Private Goodness of Fit,[0],[0]
"From Figure 2, we can see that the test statistics that have the largest noncentral parameter for a particular dimension d and privacy parameter will have the best empirical power.",5. Local Private Goodness of Fit,[0],[0]
"When d = 4, we see that LocalGenRRGOF performs the best.",5. Local Private Goodness of Fit,[0],[0]
"However, for d = 40 it is not so clear cut.",5. Local Private Goodness of Fit,[0],[0]
"When = 4, we can see that LocalGenRRGOF does the best, but then when = 2, LocalBitFlipGOF does best.",5. Local Private Goodness of Fit,[0],[0]
"Thus, the best Local DP Goodness of Fit test depends on the noncentral parameter, which is a function of , the null hypothesis p0, and alternate p = p0 + ∆. Note that the worst local DP test also depends on the privacy parameter and the dimension d. Based on our empirical results, we see that no single locally private test is best for all data dimensions.",5. Local Private Goodness of Fit,[0],[0]
"However, knowing the corresponding noncentral parameter for a given problem is useful in determining which tests to use.",5. Local Private Goodness of Fit,[0],[0]
"Indeed, the larger the noncentral parameter is the higher the power will be.
",5. Local Private Goodness of Fit,[0],[0]
"status/775801259504734208, Apple uses = 1, 4.",5. Local Private Goodness of Fit,[0],[0]
"Our techniques can be extended to include composite hypothesis tests, where we test whether the data comes from a whole family of probability distributions.",6. Local Private Independence Tests,[0],[0]
"We will focus on independence testing, but much of the theory can be extended to general chi-square tests.",6. Local Private Independence Tests,[0],[0]
"We will closely follow the presentation and notation as in (Kifer & Rogers, 2017).
",6. Local Private Independence Tests,[0],[0]
"We consider two multinomial random variables {U `}n`=1
i.i.d.∼ Multinomial(1,π(1)) for π(1) ∈",6. Local Private Independence Tests,[0],[0]
"Rr, {V `}n`=1
i.i.d.∼ Multinomial(1,π(2)) for π(2) ∈",6. Local Private Independence Tests,[0],[0]
Rc and no component of π(1) or π(2) is zero and each sums to 1.,6. Local Private Independence Tests,[0],[0]
"Without loss of generality, we will consider an individual to be in one of r groups who reports a data record that is in one of c categories.",6. Local Private Independence Tests,[0],[0]
"The collected data consists of n joint outcomes H whose (i, j)th coordinate is Hi,j = ∑n `=1 1{U`,i = 1 & V`,j = 1}.",6. Local Private Independence Tests,[0],[0]
Note that H is then the contingency table over the joint outcomes.,6. Local Private Independence Tests,[0],[0]
"Under the null hypothesis of independence between {U `}n`=1 and {V `}n`=1, for probability vector p(π(1),π(2)) = π(1) ( π(2) )ᵀ , we have
H ∼ Multinomial ( n,p(π(1),π(2)) )",6. Local Private Independence Tests,[0],[0]
"What makes this test difficult is that the analyst does not know the data distribution p(π(1),π(2)) and so cannot simply plug it into the chi-square statistic.",6. Local Private Independence Tests,[0],[0]
"Rather, we use the data to estimate the best guess for the unknown probability distribution that satisfies the null hypothesis.",6. Local Private Independence Tests,[0],[0]
"Note that without privacy, each individual ` ∈",6. Local Private Independence Tests,[0],[0]
[n] is reporting a r × c matrixX ` which would be 1 in exactly one location.,6. Local Private Independence Tests,[0],[0]
Thus we can alternatively write the contingency table as H = ∑n `=1X `.,6. Local Private Independence Tests,[0],[0]
We then use the three local private algorithms we presented earlier to see how we can form a private chi-square statistic for independence testing.,6. Local Private Independence Tests,[0],[0]
We want to be able to ensure the privacy of both the group and the category that each individual belongs to.,6. Local Private Independence Tests,[0],[0]
"Due to space we will only cover private independence tests that use the generalized randomized response mechanism from Algorithm 2 and the
bit flipping local randomizer from Algorithm 4.",6. Local Private Independence Tests,[0],[0]
"We defer our independence test with noise addition in the local setting to the supplementary file.
",6. Local Private Independence Tests,[0],[0]
Testing with Generalized Randomized Response.,6. Local Private Independence Tests,[0],[0]
We want to design an independence test when the data is generated from MGenRR given in Algorithm 2.,6. Local Private Independence Tests,[0],[0]
"In this case our contingency table can be written as Ȟ ∼ Multinomial ( n, p̌(π(1),π(2)) )",6. Local Private Independence Tests,[0],[0]
where β = 1e,6. Local Private Independence Tests,[0],[0]
"+rc−1 and we use (3) to get
p̌(π(1),π(2))",6. Local Private Independence Tests,[0],[0]
= β,6. Local Private Independence Tests,[0],[0]
"( (e − 1)π(1) ( π(2) )ᵀ + 1 ) (7)
We then obtain an estimate for the unknown parameters,
π̌(1) = 1
β (e − 1) ( Ȟi,· n",6. Local Private Independence Tests,[0],[0]
− cβ : i ∈,6. Local Private Independence Tests,[0],[0]
"[r] ) ,
π̌(2) = 1
β (e − 1) ( Ȟ·,j n",6. Local Private Independence Tests,[0],[0]
− rβ :,6. Local Private Independence Tests,[0],[0]
j ∈,6. Local Private Independence Tests,[0],[0]
"[c] )
Ť (n)GenRR ( ) = ∑",6. Local Private Independence Tests,[0],[0]
"i,j
( Ȟi,j − np̌i,j ( π̌(1), π̌(2) ))",6. Local Private Independence Tests,[0],[0]
"2 np̌i,j(π̌ (1), π̌(2)) (8)
We can then prove the following result, where the full proof is in the supplementary file.
",6. Local Private Independence Tests,[0],[0]
Theorem 6.1.,6. Local Private Independence Tests,[0],[0]
"Assuming U and V are independent with true probability vectors π(1),π(2) > 0 respectively, then as n→∞ we have Ť (n)GenRR ( ) D→ χ2(r−1)(c−1).
",6. Local Private Independence Tests,[0],[0]
"We then use this result to design Algorithm 6.
Theorem 6.2.",6. Local Private Independence Tests,[0],[0]
"LocalGenRRIND is -LDP.
",6. Local Private Independence Tests,[0],[0]
Testing with Bit Flipping.,6. Local Private Independence Tests,[0],[0]
"Lastly, we design an independence test when the data is reported via Mbit in Algorithm 4.",6. Local Private Independence Tests,[0],[0]
"Assuming that H = ∑n `=1X ` ∼
Algorithm 6 Local DP IND Test: LocalGenRRIND
Input: x = (x1, · · · ,xn), , α, H0 : p = p0.",6. Local Private Independence Tests,[0],[0]
"Let Ȟ = ∑n i=1MGenRR(xi, ).
",6. Local Private Independence Tests,[0],[0]
"Set q = Ť (n)GenRR ( ) from (8) if q > χ2d−1,1−α, Decision← Reject.",6. Local Private Independence Tests,[0],[0]
else Decision←,6. Local Private Independence Tests,[0],[0]
"Fail to Reject.
",6. Local Private Independence Tests,[0],[0]
"Output: Decision
Multinomial ( n,p(π(1),π(2)) )",6. Local Private Independence Tests,[0],[0]
", then we know that replacing p0 with p(π(1),π(2))",6. Local Private Independence Tests,[0],[0]
"in Section 5 gives us the following asymptotic distribution (treating the contingency table of values as a vector) with covariance matrix Σ(·) given in (5)
",6. Local Private Independence Tests,[0],[0]
√ n H̃n −  ( e /2,6. Local Private Independence Tests,[0],[0]
− 1 e /2,6. Local Private Independence Tests,[0],[0]
+ 1 ) π(1) ( π(2) )ᵀ,6. Local Private Independence Tests,[0],[0]
"+
1
e /2",6. Local Private Independence Tests,[0],[0]
"+ 1︸ ︷︷ ︸ p̃(π(1),π(2))
 
",6. Local Private Independence Tests,[0],[0]
"D→ N ( 0,Σ ( π(1) ( π(2) )ᵀ)) (9)
Similar to analysis for Theorem 6.1, we start with a rough estimate for the unknown parameters which converges in probability to the true estimates, so we use α =",6. Local Private Independence Tests,[0],[0]
"( e /2−1 e /2+1 ) to get
π̃ (1) =
( 1
α )( H̃i,· n",6. Local Private Independence Tests,[0],[0]
− c e /2,6. Local Private Independence Tests,[0],[0]
+ 1 : i ∈,6. Local Private Independence Tests,[0],[0]
"[r] )
π̃ (2) =
( 1
α )( H̃·,j n",6. Local Private Independence Tests,[0],[0]
− r e /2,6. Local Private Independence Tests,[0],[0]
+ 1,6. Local Private Independence Tests,[0],[0]
: j ∈,6. Local Private Independence Tests,[0],[0]
"[c] ) (10)
We then give the resulting statistic, parameterized by the unknown parameters π(`), for ` ∈ {1, 2}.",6. Local Private Independence Tests,[0],[0]
"For middle matrix
M̃ = ΠΣ ( π̃ (1) ( π̃ (2) )",6. Local Private Independence Tests,[0],[0]
"ᵀ)−1 Π, we have
T̃ (n)
",6. Local Private Independence Tests,[0],[0]
"BitFlip
( θ(1), θ(2); )",6. Local Private Independence Tests,[0],[0]
"= 1
n
( H̃ − np̃ ( θ(1), θ(2) ))",6. Local Private Independence Tests,[0],[0]
"ᵀ M̃ ( H̃ − np̃ ( θ(1), θ(2) ))",6. Local Private Independence Tests,[0],[0]
"(11)
Minimizing T̃ (n)
",6. Local Private Independence Tests,[0],[0]
"BitFlip
( θ(1), θ(2); ) over (θ(1), θ(2)) results in
a statistic that is distributed as a chi-square random variable, we defer the full proof to the supplementary file.",6. Local Private Independence Tests,[0],[0]
Theorem 6.3.,6. Local Private Independence Tests,[0],[0]
"Under the null hypothesis where U and V are independent with true probability vectors π(1),π(2) > 0 respectively, then we have as n → ∞,
minθ(1),θ(2)
{ T̃ (n)
BitFlip
( θ(1), θ(2); )}",6. Local Private Independence Tests,[0],[0]
D→ χ2(r−1)(c−1).,6. Local Private Independence Tests,[0],[0]
We present the test in Algorithm 7.,6. Local Private Independence Tests,[0],[0]
The following result follows from same privacy analysis as before.,6. Local Private Independence Tests,[0],[0]
Theorem 6.4.,6. Local Private Independence Tests,[0],[0]
"LocalBitFlipIND is -LDP.
",6. Local Private Independence Tests,[0],[0]
Algorithm 7 Local DP IND Test: LocalBitFlipIND,6. Local Private Independence Tests,[0],[0]
"Input: (x1, · · · ,xn), , α.
Let H̃ = ∑n i=1Mbit(xi, ).
",6. Local Private Independence Tests,[0],[0]
"q = minπ(1),π(2)
{ T̃ (n)
BitFlip
( π(1),π(2); )} from (11).
",6. Local Private Independence Tests,[0],[0]
"if q > χ2(r−1)(c−1),1−α Decision← Reject.",6. Local Private Independence Tests,[0],[0]
else Decision←,6. Local Private Independence Tests,[0],[0]
"Fail to Reject.
",6. Local Private Independence Tests,[0],[0]
"Output: Decision
Empirical Results.",6. Local Private Independence Tests,[0],[0]
"As we did for the goodness of fit tests, we empirically compare the power for our various tests for independence.",6. Local Private Independence Tests,[0],[0]
We consider the null hypothesis that the two sequences of categorical random variables {U `}n`=1 and {V `}n`=1 are independent of one another.,6. Local Private Independence Tests,[0],[0]
"Under an alternate hypothesis, we generate the contingency data according to a non-product distribution.",6. Local Private Independence Tests,[0],[0]
"We fix the distribution p1 for the contingency table to be of the following form, where π(1) ∈",6. Local Private Independence Tests,[0],[0]
"Rr is the unknown distribution for {U `}n`=1, π(2) ∈",6. Local Private Independence Tests,[0],[0]
"Rc is the unknown distribution for {V `}n`=1, and r, c are even
p1 = π(1) ( π(2) )ᵀ + η(1,−1, · · · ,−1, 1)ᵀ(1,−1, · · · ,−1, 1) (12)
Note that the hypothesis test does not know the underlying π(i) for i ∈ {1, 2}, but to generate the data we must fix these distributions.",6. Local Private Independence Tests,[0],[0]
We show power results when the marginal distributions satisfy π(1) =,6. Local Private Independence Tests,[0],[0]
"(1/r, · · · , 1/r) and π(2) =",6. Local Private Independence Tests,[0],[0]
"(1/c, · · · , 1/c).",6. Local Private Independence Tests,[0],[0]
"In Figure 2, we give results for various n and ∈ {1, 2, 4} .",6. Local Private Independence Tests,[0],[0]
"We have designed several hypothesis tests, each depending on different local differentially private algorithms.",7. Conclusion,[0],[0]
We showed that each statistic has a noncentral chi-square distribution when the data is drawn from some alternate hypothesis H1.,7. Conclusion,[0],[0]
"Depending on the form of the alternate probability distribution, the dimension of the data, and the privacy parameter, either LocalGenRRGOF or LocalBitFlipGOF gave the best power.",7. Conclusion,[0],[0]
"This corroborates the results from Kairouz et al. (2014) who showed that in hypothesis testing, different privacy regimes have different optimal local differentially private mechanisms, although utility in their work was in terms of KL divergence.",7. Conclusion,[0],[0]
Our results show that the power of the test is directly related to the noncentral parameter of the test statistic that is used.,7. Conclusion,[0],[0]
"This requires the data analyst to carefully consider alternate hypotheses, as well as the data dimension and privacy parameter for a particular test and then see which test statistic results in the largest noncentral parameter.",7. Conclusion,[0],[0]
Marco Gaboardi has been partially supported by NSF under grant TWC-1565365.,Acknowledgements,[0],[0]
The local model for differential privacy is emerging as the reference model for practical applications of collecting and sharing sensitive information while satisfying strong privacy guarantees.,abstractText,[0],[0]
"In the local model, there is no trusted entity which is allowed to have each individual’s raw data as is assumed in the traditional curator model.",abstractText,[0],[0]
Individuals’ data are usually perturbed before sharing them.,abstractText,[0],[0]
"We explore the design of private hypothesis tests in the local model, where each data entry is perturbed to ensure the privacy of each participant.",abstractText,[0],[0]
"Specifically, we analyze locally private chi-square tests for goodness of fit and independence testing.",abstractText,[0],[0]
Local Private Hypothesis Testing: Chi-Square Tests,title,[0],[0]
"Bayesian networks have been used in classification (Aliferis et al., 2010), feature selection (Gao et al., 2015), latent variable discovery (Lazic et al., 2013; Gao & Ji, 2016a), and knowledge discovery (Spirtes et al., 1999; Gao & Ji, 2015) in various domains (Ott et al., 2004).",1. Introduction,[0],[0]
"However, due to its NP-hard nature (Chickering et al., 2012), exact BN structure learning on directed acyclic graphs (DAG) faces scalability issues.
",1. Introduction,[0],[0]
"In this paper, we consider a local-to-global approach to learn the Bayesian network structure, starting from the local graph structure of one node and then gradually expanding the graph based on already learned structures.
",1. Introduction,[0],[0]
"1IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598 USA.",1. Introduction,[0],[0]
"Correspondence to: Tian Gao <tgao@us.ibm.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"The predominant exact score-based structure learning algorithms adopt the global approach and focus on better scoring criteria (Acid et al., 2005; Brenner & Sontag, 2013) or more efficient search procedures (Chickering, 2002; Koivisto & Sood, 2004; Silander & Myllymaki, 2006; Jaakkola et al., 2010; Cussens, 2011; Yuan & Malone, 2013) to navigate the intractable search space of possible directed acyclic graphs over all the variables present.",1. Introduction,[0],[0]
"Despite such progress, the practical usage of these algorithms is still limited when there are large numbers of variables.",1. Introduction,[0],[0]
"Many approximations (Scanagatta et al., 2015), constraints (de Campos et al., 2009; Chen et al., 2016), and assumptions (Nie et al., 2014) are utilized to alleviate time and memory complexity.
",1. Introduction,[0],[0]
"Instead of searching the entire DAG space for all the variables at the same time, the local-to-global approach limits the size of the space by learning a local structure with only a limited number of variables.",1. Introduction,[0],[0]
"These variables consists of potential candidates for local structures, usually defined by the parent-child (PC) or the Markov Blanket (MB) (Pearl, 1988) set of a target node in a DAG.",1. Introduction,[0],[0]
"Many local learning algorithms (Koller & Sahami, 1996; Tsamardinos et al., 2003; Fu & Desmarais, 2008) iteratively query new variables to update and learn the local structure, either PC, MB set or both, for one specific target variable.",1. Introduction,[0],[0]
"Many constraint-based and score-based local learning algorithms have been proposed and shown to have promising performances in practice (Aliferis et al., 2010).",1. Introduction,[0],[0]
"Using learned local structures, some prior works have proposed to combine the local structures for the global structure.",1. Introduction,[0],[0]
"Several works (Margaritis & Thrun, 1999; Pellet & Ellisseeff, 2008) have proposed algorithms to identify MBs of every node in the graph first, and then connect the MBs in a maximally consistent way to learn the global structure of a BN.",1. Introduction,[0],[0]
"Both constraint-based (Tsamardinos et al., 2006) and score-based local-to-global structure learning methods (Niinimaki & Parviainen, 2012) have been proposed.",1. Introduction,[0],[0]
"This local-to-global approach has the benefit of improving the exact structure learning efficiency, as at each step only a small number of variables are expected to be used, although the accuracy has not been very competitive.
",1. Introduction,[0],[0]
We aim to improve the accuracy of the local-to-global approach and propose a new local-to-global structure learning algorithm.,1. Introduction,[0],[0]
"The algorithm starts the local learning at one
variable first, and then iteratively applies the local learning procedure to its neighbors and so on, gradually expanding the learned graph with minimal repeated learning.",1. Introduction,[0],[0]
"GGSL efficiently grows local graphs to global graphs without considering the traditional the AND-rule, or a consistency check on learned local neighborhoods.",1. Introduction,[0],[0]
"It uses only the necessary variables for each local learning step to learn and resolve any possible conflicts among local structures, hence improving efficiency over global learning algorithms and improving accuracy over existing local-to-global algorithms with the AND-rule.
",1. Introduction,[0],[0]
"Notation: We use capital letters (such asX,Y ) to represent variables, small letters (such as x, y) to represent values of variables, and bold letters (such as V,MB) to represent variable sets.",1. Introduction,[0],[0]
"|V| represents the size of a set V. X ⊥ Y and X ⊥\⊥ Y represent independence and dependence between X and Y , respectively.",1. Introduction,[0],[0]
Let V denote a set of random variables.,2. Technical Preliminaries,[0],[0]
"A Bayesian Network for V is represented by a pair (G, θ).",2. Technical Preliminaries,[0],[0]
"The network structure G is a directed acyclic graph with nodes corresponding to the random variables in V. If a directed edge exists from node X to node Y in G, X is a parent of Y and Y is a child of X .",2. Technical Preliminaries,[0],[0]
The parameters θ indicate the conditional probability distribution of each node X ∈ V given its parents.,2. Technical Preliminaries,[0],[0]
"Moreover, let a path between two nodes X and Y in G be any sequence of nodes between them such that any successive nodes are connected by a directed edge, and no node appears in the sequence twice.",2. Technical Preliminaries,[0],[0]
"A directed path of a DAG is a path with nodes (V1, ..., Vn) such that, for 1 ≤",2. Technical Preliminaries,[0],[0]
"i < n, Vi is a parent of Vi+1.",2. Technical Preliminaries,[0],[0]
"If there is a directed path from X to Y , then X is an ancestor of Y and Y is a descendant of X .",2. Technical Preliminaries,[0],[0]
"If X and Y have a common child and they are not adjacent, X and Y are spouses of each other.",2. Technical Preliminaries,[0],[0]
"Three nodes X , Y , and Z form a V-structure if node Y has two incoming edges from X and Z, forming X → Y ← Z, and X is not adjacent to Z. Y is a collider if Y has two incoming edges from X and Z in a path.",2. Technical Preliminaries,[0],[0]
"A path J from node X to Y is blocked by a set of nodes Z, if any of following holds true: 1)",2. Technical Preliminaries,[0],[0]
There is a non-collider node in J belonging to Z. 2),2. Technical Preliminaries,[0],[0]
"There is a collider node C on J such that neither C nor any of its descendants belong to Z. Otherwise, J from X to Y is unblocked or active.
",2. Technical Preliminaries,[0],[0]
"The Local Markov Condition (Pearl, 1988) states a node in a BN is independent of its non-descendant nodes, given its parents.",2. Technical Preliminaries,[0],[0]
"It enables the recovery of a distribution P (in term of independence relationships) from a known DAG G. A DAG G and a joint distribution P are faithful to each other if all and only the conditional independencies true in P are entailed by G (Pearl, 1988).",2. Technical Preliminaries,[0],[0]
"The faithfulness condition enables us to recover a DAG G from a distribution P
to completely characterize P .
",2. Technical Preliminaries,[0],[0]
"A Markov Blanket of a target variable T , MBT , is the minimal set of nodes conditioned on which all other nodes are independent of T , denoted as X ⊥ T |MBT ,∀X ∈ {V \ T \MBT }.",2. Technical Preliminaries,[0],[0]
Given independently and identically distributed (i.i.d.),2. Technical Preliminaries,[0],[0]
"samples D from an unknown distribution P , represented by a faithful but unknown DAG G0 to P , local structure learning is to find the PC or MB of a target node inG0.",2. Technical Preliminaries,[0],[0]
"To avoid symbol confusion, we useG to represent any learned DAG and use G0 to represent the ground truth DAG.",2. Technical Preliminaries,[0],[0]
"Under the faithfulness assumption between G0 and P , the PC and MB of a target node is uniquely identifiable (Pearl, 1988).",2. Technical Preliminaries,[0],[0]
"For example, in Figure 1a, nodes A and D form PCB .",2. Technical Preliminaries,[0],[0]
"MBB contains its parent node A, its child D, and its spouse C. All other nodes E, F , and H are independent of B, given MBB , due to blocked paths.
",2. Technical Preliminaries,[0],[0]
"Score-based structure learning algorithms rely on some score criteria s to learn a best-fitting DAG G for data D. Score s(G,D) of a BN DAG structure G measures the goodness of fit of G on D. Let G be any BN structure and G′ be the same structure as G but with an edge from a node T to a node X .",2. Technical Preliminaries,[0],[0]
"Let PaGX be the parent set of X in G. Score s is locally consistent if, as the size of the data D goes to infinity, the following two properties hold true: 1) if X ⊥\⊥ T |PaGX , then s(G,D) < s(G′, D), and 2) if X ⊥ T |PaGX , then s(G,D) > s(G′, D).",2. Technical Preliminaries,[0],[0]
"In addition, s is score equivalent if Markov equivalent DAGs have the same score.",2. Technical Preliminaries,[0],[0]
s is decomposable if it is a sum of each node’s individual score that depends on only this node and its parents.,2. Technical Preliminaries,[0],[0]
"Commonly used Bayesian score criteria, such as BDeu, are decomposable, consistent, locally consistent (Chickering, 2002), and score equivalent (Heckerman et al., 1995).",2. Technical Preliminaries,[0],[0]
"We assume the Markov condition, faithfulness condition, and the infinite data size hold in the theoretical analysis part of
the paper.
",2. Technical Preliminaries,[0],[0]
"Lastly, one of the main concepts in the topology-based MB algorithms is the symmetry constraint, or the AND-rule.
",2. Technical Preliminaries,[0],[0]
Lemma 1.,2. Technical Preliminaries,[0],[0]
AND-Rule.,2. Technical Preliminaries,[0],[0]
"For a node X to be adjacent to T in G, both of the following statements hold true: X must be in the PC set of T and T must be in the PC set ofX , i.e.,",2. Technical Preliminaries,[0],[0]
"X ∈ PCGT and T ∈ PC G X .
",2. Technical Preliminaries,[0],[0]
"The local-to-global BN structure learning algorithms generally use the AND-rule to enforce consistency between different learned local structures to obtain a global DAG (Margaritis & Thrun, 1999).",2. Technical Preliminaries,[0],[0]
"Local structure learning algorithms also employ it to guarantee soundness (Niinimaki & Parviainen, 2012).",2. Technical Preliminaries,[0],[0]
"We will first introduce local BN structure learning and provide some new theoretical guarantees, then propose a novel procedure to expand the local graph to the global graph, including some consistency guarantee and the proposed GGSL algorithm.",3. Local-to-Global BN Structure Learning,[0],[0]
"The local-to-global learning approach first uses local structure learning algorithms, either constraint-based (Tsamardinos et al., 2006) or score-based (Niinimaki & Parviainen, 2012), to discover the PC set or the Markov Blanket of the target.",3.1. Local Structure Learning,[0],[0]
"The arguably state-of-art algorithms to find the local structure of Bayesian network use a scorebased framework (Gao & Ji, 2017), shown in Algorithm 1, LocalLearn.
",3.1. Local Structure Learning,[0],[0]
"In Algorithm 1, subroutine BNStructLearn learns an optimal DAG over a set of variables in the data, and can use any exact global BN structure learning algorithm.",3.1. Local Structure Learning,[0],[0]
"Subroutine findPC and findSpouse extract a variable T ’s PC set (by finding parent set P and children set C) and spouse set given the adjacency matrix of a graph G. LocalLearn first sequentially learns the PC set by repeatedly using BNStructLearn on a set of nodes Z containing the target node T , its current PC set PCT , and one new query variable X .",3.1. Local Structure Learning,[0],[0]
Then it uses a similar procedure to learn the spouse set and update the PC set.,3.1. Local Structure Learning,[0],[0]
"PCGT is guaranteed to contain all the true positive PC nodes of T .
",3.1. Local Structure Learning,[0],[0]
Lemma 2.,3.1. Local Structure Learning,[0],[0]
"Preservation of True Positive PCs (Niinimaki & Parviainen, 2012).",3.1. Local Structure Learning,[0],[0]
"Let G0 be the faithful DAG of distribution P over V, and G be the DAG learned by exact BN structure learning algorithms over the subset of variables ZL ⊆ V at the last iteration of Step 1 of Algorithm 1.",3.1. Local Structure Learning,[0],[0]
Let PCGT be the learned PC set of the target T in G and PC 0 T be the PC set of T inG0.,3.1. Local Structure Learning,[0],[0]
"Under the faithfulness and infinite data assumption, PC0T ⊆ PC G T .
",3.1. Local Structure Learning,[0],[0]
"Algorithm 1 LocalLearn Input: dataset D, target node T {step 1: find the PC set } PCT ← ∅, O← V \ {T}; while O is nonempty do
choose X ∈",3.1. Local Structure Learning,[0],[0]
"O, O← O \ {X}; Z← {T,X} ∪PCT ; G← BNStructLearn (Z, DZ); PCT ,PT ,CT ← findPC(G,T ) ;
end while {step 2: remove false PC nodes and find spouses} ST ← ∅, O← V \PCT ; while O is nonempty do
choose X ∈",3.1. Local Structure Learning,[0],[0]
"O, O← O \ {X}; Z← {T,X} ∪PCT ∪ ST ; G← BNStructLearn (Z, DZ); PCT ,PT ,CT ← findPC(G,T ) ; ST ← findSpouse(G,T ) ;
end while Return: MB← PT ∪CT ∪ ST ;
However, PCGT may contain false positive PC nodes (Aliferis et al., 2010), as shown in Figure 1 of (Niinimaki & Parviainen, 2012).",3.1. Local Structure Learning,[0],[0]
"The iterative nature of Algorithm 1 can potentially violate the faithfulness assumption during the learning, due to absent variables.",3.1. Local Structure Learning,[0],[0]
"Previous analysis (Niinimaki & Parviainen, 2012; Gao & Ji, 2017) conjectured the soundness and completeness of LocalLearn.",3.1. Local Structure Learning,[0],[0]
"Here we provide a new theoretical proofs of these results in the LocalLearn .
",3.1. Local Structure Learning,[0],[0]
Lemma 3.,3.1. Local Structure Learning,[0],[0]
Preservation of Dependence Relationships between T and the Learned PC Set.,3.1. Local Structure Learning,[0],[0]
"Let G0 be the global faithful DAG of P for the entire variable set V, and G be the DAG learned by exact BN structure learning algorithms over a subset of variables of V, VG, present at the last iteration of Step 1 of Algorithm 1.",3.1. Local Structure Learning,[0],[0]
"Then inG0 every variable in the learned PC set PCGT is dependent of the target T, conditioned on any subset of the ground truth PC set: i.e., X ⊥\⊥ T |Z,∀X ∈ PCGT ,∀Z ⊆ PC",3.1. Local Structure Learning,[0],[0]
0 T,3.1. Local Structure Learning,[0],[0]
\,3.1. Local Structure Learning,[0],[0]
"{X}.
",3.1. Local Structure Learning,[0],[0]
Proof.,3.1. Local Structure Learning,[0],[0]
"If X ∈ PC0T , then the lemma holds automatically.",3.1. Local Structure Learning,[0],[0]
"Else if X 6∈ PC0T , assuming ∃X ∈ S such that X ⊥ T |S \ X , where S = VG \ {T}.",3.1. Local Structure Learning,[0],[0]
"Then, one of the following two cases must hold in G: T → X or X → T .",3.1. Local Structure Learning,[0],[0]
"If T → X , since each node in ChildrenX ∪ SpousesX is either 1) not saved during the iterative procedure, or 2) saved as a node of PCGT , in which case it forms a fully connected subgraph with X and T and can be changed to a ParentX .",3.1. Local Structure Learning,[0],[0]
"Then, P (X|S\X) = P (X|PaGX) by MB definition.",3.1. Local Structure Learning,[0],[0]
"Since P (X|S\X,T ) = P (X|S\X) by assumption X ⊥ T |S \ X , then T 6∈ PaX since {S \ X} must contain PaGX .",3.1. Local Structure Learning,[0],[0]
"Then by local consistency removing the edge T → X will increase the score, which contradicts the as-
sumptions.",3.1. Local Structure Learning,[0],[0]
"IfX → T , since P (T |X,S\X) = P (T |S\X) by assumption, then using a similar argument, X 6∈ PaGT and removing the edge X → T will increase the score, which contradicts the assumption.",3.1. Local Structure Learning,[0],[0]
"Hence, X ⊥\⊥ T |S \X .",3.1. Local Structure Learning,[0],[0]
"Since Z ⊆ PC0T \ {X} ⊆ S \ {X}, the lemma holds.
",3.1. Local Structure Learning,[0],[0]
"Lemma 3 shows that the false PC nodes consist of only descendents of T , which is the same as Lemma 3 of (Gao & Ji, 2017) but without conjectured results:
Lemma 4.",3.1. Local Structure Learning,[0],[0]
PC False Positive Identity.,3.1. Local Structure Learning,[0],[0]
"Let PCGT be the learned PC set of the target T in the learned graph G from Step 1 of Algorithm 1, and PC0T be the ground truth PC set in G0.",3.1. Local Structure Learning,[0],[0]
"The false positives F in PCGT consist of only descendants of T inG0, denoted as Des0T , i.e., F ⊆ Des 0 T , F = PCGT \PC 0 T .
",3.1. Local Structure Learning,[0],[0]
Proof.,3.1. Local Structure Learning,[0],[0]
"By Lemma 2, PCGT consists of the entire PC 0 T and some false positives F. We show F ⊆ Des0T .",3.1. Local Structure Learning,[0],[0]
"According to Lemma 3, a node X ∈ F is conditionally dependent of T given any Z ⊆ PC0T",3.1. Local Structure Learning,[0],[0]
\ {X} in G0.,3.1. Local Structure Learning,[0],[0]
"In the last iteration of Step 1, PCGT must contain the true positive parents of T Pa0T , as Pa 0 T ⊆ PC 0 T ⊆ PC G T by Lemma 2.",3.1. Local Structure Learning,[0],[0]
"Therefore, X ⊥\⊥ T |Pa0T .",3.1. Local Structure Learning,[0],[0]
"However, by the Markov condition, all the non-descendant nodes X are independent of T given Pa0T inG0.",3.1. Local Structure Learning,[0],[0]
Hence non-descendantsX cannot be in PCGT by the last iteration.,3.1. Local Structure Learning,[0],[0]
"Thus, F ⊆ Des0T .
",3.1. Local Structure Learning,[0],[0]
"For the sake of complete discussion, we include the following property, showing the existence of unblocked paths:
Lemma 5.",3.1. Local Structure Learning,[0],[0]
Coexistence Between Descendants and Spouses in Score-Based PC Search.,3.1. Local Structure Learning,[0],[0]
"In the learnedG from Step 1 of Algorithm 1, the only false positives F in PCGT belong to the descendants of T , Des0T, due to an unblocked path between T and its descendants via a V-structure T → Child← Spouse in G0.
",3.1. Local Structure Learning,[0],[0]
Proof.,3.1. Local Structure Learning,[0],[0]
"Lemma 4 shows the first part of the lemma is true, and we just need to show the second part holds.",3.1. Local Structure Learning,[0],[0]
"Assuming false positive PC nodes F exist, let X ∈ F ⊆ Des0T , then Lemma 4 shows that X ⊥\⊥ T |Z,∀Z ⊆ PC0T .",3.1. Local Structure Learning,[0],[0]
"For F to exist, X ⊥\⊥ T |PC0T must be true.",3.1. Local Structure Learning,[0],[0]
"Since PC 0 T must be present in all paths from T to X in G0, in the last iteration of the score-based PC search the dependence between T and X occurs only if PC0T unblocks some paths from T to X .",3.1. Local Structure Learning,[0],[0]
This can only happen when there is a collider node in PC0T .,3.1. Local Structure Learning,[0],[0]
"Hence, the only wayX can exist in PC G T is through an unblocked path that contains a V-structure T → child← spouse in G0.
",3.1. Local Structure Learning,[0],[0]
"We show the consistency results of LocalLearn:
Theorem 1.",3.1. Local Structure Learning,[0],[0]
"Under the infinite data and faithfulness assumption, LocalLearn finds all and only the Markov Blanket nodes of the target node.
",3.1. Local Structure Learning,[0],[0]
Proof.,3.1. Local Structure Learning,[0],[0]
"Step 1 of Algorithm 1 returns all of the true positive parents, children, and some descendants, if they exist, by Lemma 4 and Lemma 5.",3.1. Local Structure Learning,[0],[0]
"The tasks left are to add the true positive spouses and remove false positive PC nodes, i.e. the non-child descendants, from PCT .
",3.1. Local Structure Learning,[0],[0]
"First, we show LocalLearn will find all of the true positive spouses.",3.1. Local Structure Learning,[0],[0]
"In Step 2, LocalLearn learns a structure with the target, the current PC set, the current spouse set, and one query variable X ∈",3.1. Local Structure Learning,[0],[0]
O = V \ PCT .,3.1. Local Structure Learning,[0],[0]
"At each step, PCT is a set that has been found to be dependent or conditionally dependent of the target.",3.1. Local Structure Learning,[0],[0]
"Since PCT includes and will always include the true positive PC set by Lemma 2, true positive spouses are conditionally dependent of T given PCT .",3.1. Local Structure Learning,[0],[0]
"Following a similar logic as Lemma 2, S0T must directly connect to the true positive children of T .",3.1. Local Structure Learning,[0],[0]
"Because Step 2 of Algorithm 1 queries every variable in V and S0T must be dependent of nonadjacent T given PC0T , to capture the correct independence relationships with the locally consistent score, S0T must be included in ST .
",3.1. Local Structure Learning,[0],[0]
"Secondly, we show false positive PCs and spouses will be removed.",3.1. Local Structure Learning,[0],[0]
"Since all the true positive PC nodes and spouses are present in the last iteration, the false positive PC nodes (the non-MB descendants by Lemma 4), if exist, should be adjacent to the true positive PC nodes and spouses in G. However, the DAG obtained from G by removing the edge from false positives PC to T would score higher by capturing the same independence relationships (i.e., the descendants are dependent of children and spouse nodes of T and independent of T given the true positive MB set) and the dependence relationships between the true positive MB set and T , but with fewer edges.",3.1. Local Structure Learning,[0],[0]
"Therefore, all false positive PC nodes will be removed from PCT .",3.1. Local Structure Learning,[0],[0]
"Similarly, since all the true positive PC nodes and spouses are present in the last iteration, if there exist false positive spouses F ⊆ SGT \ S0T , F would be parents to some true positive children nodes C0T in G and F ⊥\⊥ T |C.",3.1. Local Structure Learning,[0],[0]
"If so, F should be adjacent to S0T as well in G as every path between C0 and F must go through S0T .",3.1. Local Structure Learning,[0],[0]
"However, the DAG obtained from G by removing the edge from C to F would score higher than G by capturing the same independence relationships but with less edges.",3.1. Local Structure Learning,[0],[0]
"Therefore, there will not be any false positive spouse nodes.
",3.1. Local Structure Learning,[0],[0]
"Thus, PCT and S will contain all and only the true PC and spouses.",3.1. Local Structure Learning,[0],[0]
Their union contains all and only the MB nodes.,3.1. Local Structure Learning,[0],[0]
"Therefore, LocalLearn is sound and complete.
",3.1. Local Structure Learning,[0],[0]
"Note that the learned parent set P and children set C from LocalLearn themselves may not be sound or complete, due to Markov equivalence, even though their joint set is sound and complete.",3.1. Local Structure Learning,[0],[0]
"Using the local structures, one alternative approach to global structure learning is to learn the local structures of all nodes and then combine them to construct the global graph structure from the existing local graphs.",3.2. Local-to-Global Learning,[0],[0]
"Many algorithms (Margaritis & Thrun, 1999; Niinimaki & Parviainen, 2012) first use the AND-rule to resolve the potential conflicts among different local structures between adjacent variables, and then use the Meek rules (Meek, 1995) to obtain the final DAG.",3.2. Local-to-Global Learning,[0],[0]
"The AND-rule seems arbitrary in the learning results and can introduce errors if one of the neighbors learns a wrong local graph, hence affecting the accuracy of the final global graph.",3.2. Local-to-Global Learning,[0],[0]
One naive way to solve such conflicts is to re-run the subroutine BNStructLearn on the variable set containing both neighbor sets to redetermine the existence of the edges.,3.2. Local-to-Global Learning,[0],[0]
"However, the procedure is inefficient as it repeats learning for local structures of every edge after repeating for every node, relearning the same parts of the graph multiple times.
",3.2. Local-to-Global Learning,[0],[0]
"We propose to anchor the learning at one target variable T , and then grow the graph by gradually expanding it.",3.2. Local-to-Global Learning,[0],[0]
"It is made possible as the LocalLearn does not require neighbors’ local structure to learn T ’s neighborhood correctly, unlike previous algorithms.",3.2. Local-to-Global Learning,[0],[0]
We only use the necessary variables at each iteration to expand the graph while keeping other parts of the learned graph fixed.,3.2. Local-to-Global Learning,[0],[0]
The proposed graph growing structure learning (GGSL) algorithm is shown in Algorithm 2.,3.2. Local-to-Global Learning,[0],[0]
"Starting from an empty graph, GGSL iteratively updates the global graph by using LocalLearn to learn the local structure of one target variable T at a time.",3.2. Local-to-Global Learning,[0],[0]
Each local learning uses the set consisting of the current local variables of the target variable in the learned graph G and variables that are not in the local structures of any variable.,3.2. Local-to-Global Learning,[0],[0]
Then GGSL updates G with learned results using updateGraph.,3.2. Local-to-Global Learning,[0],[0]
"It runs until all but one variable is not learned and has four main steps:
",3.2. Local-to-Global Learning,[0],[0]
"Step 1: GGSL chooses one target variable T at each iteration, first chosen randomly and then based on query set Q, which contains adjacent nodes of the already queried variables in the graph G. Q can be maintained as a regular queue (first in, first out).",3.2. Local-to-Global Learning,[0],[0]
"Queried variable set A keeps all the learned T s and prevents repeated learning.
",3.2. Local-to-Global Learning,[0],[0]
Step 2:,3.2. Local-to-Global Learning,[0],[0]
"GGSL uses LocalLearn shown in Algorithm 1 to find the local structure of the target variable over the query set Z. This step resolves the potential edge conflicts through efficient learning, avoiding the simple AND-rule used by other local-to-global structure learning algorithms The main difference between each run of LocalLearn is that the previous T ’s will not be considered, unless they are in the local structure of the current target variable.",3.2. Local-to-Global Learning,[0],[0]
"Hence the max possible variable set size decreases over iterations, improving the memory efficiency.
",3.2. Local-to-Global Learning,[0],[0]
"Algorithm 2 Graph Growing Structure Learning Input: data D, size m, variable set V Q← ∅; A← ∅; G← zeros(|V |, |V |) repeat
if Q 6=",3.2. Local-to-Global Learning,[0],[0]
∅ & Q[0] 6∈,3.2. Local-to-Global Learning,[0],[0]
"A then T ← Q.pop(0) else T ← the next unqueried variable in V end if G← GGSL(D,T,G,A,V) add adjacent nodes of T in G to Q A← A ∪ T
until |S| = |V",3.2. Local-to-Global Learning,[0],[0]
| − 1 G←,3.2. Local-to-Global Learning,[0],[0]
"PDAG-to-DAG(G) Return: G
Algorithm 3 GGSL Subroutine Input: data D, target variable T , current DAG G, variable set A, variable set V PC,P,C← findPC(G,T ); Sp← findSpouse(G,T )",3.2. Local-to-Global Learning,[0],[0]
Z←,3.2. Local-to-Global Learning,[0],[0]
"T ∪PC ∪ Sp ∪V \ {T,PC,Sp,A} DZ ← D of the set Z MB,P,C,Sp← LocalLearn(DZ,",3.2. Local-to-Global Learning,[0],[0]
"T ) G← updateGraph(G,T,P,C,Sp) Return: G
Step 3: Subroutine updateGraph, shown in Algorithm 3, performs the following check to enforce graph consistency: First, it checks if any directed parent in the existing G is learned as a child from the local learned children set C.",3.2. Local-to-Global Learning,[0],[0]
"If so, it corrects the children into parents.",3.2. Local-to-Global Learning,[0],[0]
"Secondly, it checks if any directed child in the existing G is wrongly learned as a parent from P. If so, it corrects the parents into children.",3.2. Local-to-Global Learning,[0],[0]
"Lastly, the algorithm checks if any P and C nodes can have a different edge direction without introducing new or destroying existing V-structures; if so, all these P and C are marked as undirected.",3.2. Local-to-Global Learning,[0],[0]
"Otherwise, they are marked as directed edges.",3.2. Local-to-Global Learning,[0],[0]
"There checks are needed to ensure the already-oriented edges remain unchanged, as the earlier runs of local structure learning uses more variables and hence are more likely to be correct.",3.2. Local-to-Global Learning,[0],[0]
"Lastly, to orient the newly learned spouse set Sp is straightforward, due to the definitive nature of V-structures.
",3.2. Local-to-Global Learning,[0],[0]
"Step 4: After repeating the first three steps for all but one variable, the last step of GGSL is to obtain a DAG given all the directed and undirected edges in the graph.",3.2. Local-to-Global Learning,[0],[0]
"Applying the conventional rules (Meek, 1995) to convert a partial directed acyclic graph (PDAG) to completely directed acyclic graph is sufficient, with an option to convert to DAG if desired.
",3.2. Local-to-Global Learning,[0],[0]
Algorithm 4 updateGraph,3.2. Local-to-Global Learning,[0],[0]
"Input: current DAGG, target T , learned parent P, child set C, spouse set",3.2. Local-to-Global Learning,[0],[0]
"Sp {Step 1. update the PC set} for all C in C do
if C conflicts with directed edge C → T in G then add C into P and remove C from C
end if end for for all P in P do
if P conflicts with directed edge T → P in G then add P into C and remove P from P
end if end for if |P| ≥ 2 then
orient P and C accordingly as a directed edge in G else if directed edge T -A, ∀A ∈ {P ∪ C} cannot be reversed without destroying existing or introducing new V-structures then
orient T -A accordingly as a directed edge in G else
orient T -A accordingly as a undirected edge in G end if {Step 2.",3.2. Local-to-Global Learning,[0],[0]
"update the spouse set} Orient Sp and T to their children accordingly as directed edges in G Return: G
Lemma 6.",3.2. Local-to-Global Learning,[0],[0]
Requirement of Post-processing .,3.2. Local-to-Global Learning,[0],[0]
"Subroutine PDAG-to-DAG is required in a local-to-global learning system to correctly orient DAG G to its Markov equivalent class in Algorithm 2.
",3.2. Local-to-Global Learning,[0],[0]
"A simple example, shown in Figure 2 would justify Lemma 6.",3.2. Local-to-Global Learning,[0],[0]
"If the target T is chosen with the following order: F,E,D,C, and A, then the direction of edges C −D, D −E, and E − F can be set in both directions and hence are labeled as undirected edges by Algorithm 2.",3.2. Local-to-Global Learning,[0],[0]
"When node C becomes the target, then edge C − D is known.",3.2. Local-to-Global Learning,[0],[0]
Other edges D−E and E−F have to be checked and corrected.,3.2. Local-to-Global Learning,[0],[0]
"Without subroutine PDAG-to-DAG to propagate the changes back, the learned DAG is not guaranteed to be the correct completely partially directed DAG (CPDAG).
",3.2. Local-to-Global Learning,[0],[0]
We show Algorithm 2 is sound and complete: Theorem 2.,3.2. Local-to-Global Learning,[0],[0]
Soundness and Completeness.,3.2. Local-to-Global Learning,[0],[0]
"Under the infinite data and faithfulness assumption, Algorithm 2 GGSL learns and directs all and only the correct edges in the underlying DAGG0, up to the Markov equivalent class ofG0.
",3.2. Local-to-Global Learning,[0],[0]
Proof.,3.2. Local-to-Global Learning,[0],[0]
"For the first target variable T1, GGSL finds the correct local structure of one target variable by Theorem 1.",3.2. Local-to-Global Learning,[0],[0]
The updated DAG G is sound and complete for T1.,3.2. Local-to-Global Learning,[0],[0]
"Starting from the second iteration i ≥ 2, since the learned graph
G is shown to be correct, if any variable in the target variable Ti’s MB set has been queried or saved, they must exist in the PC and Sp variable, hence in Z of Algorithm 3.",3.2. Local-to-Global Learning,[0],[0]
"With the rest of variables complementing the PC and Sp variables, all the true positive PC set and spouse set of Ti must be included in the set Z of Algorithm 3.",3.2. Local-to-Global Learning,[0],[0]
"By Theorem 1 again, the local structure learned must be sound and complete.",3.2. Local-to-Global Learning,[0],[0]
"Hence, at each iteration of Algorithm 2, the updated DAG G must be sound and complete for all Tis as well.",3.2. Local-to-Global Learning,[0],[0]
"Since the PDAG-to-DAG subroutine is also proven to orient the correct DAG from PDAG (Meek, 1995), the result DAG G at the end of Algorithm 2 must be in the Markov equivalent class of G0.",3.2. Local-to-Global Learning,[0],[0]
"Applying Algorithm 2 to Figure 1a would result in the following procedure, assuming the query order for target T is A ⇒ B ⇒ C ⇒",3.3. Case Study,[0],[0]
D ⇒ E ⇒ F .,3.3. Case Study,[0],[0]
"When T1 = A, LocalLearn finds B and C in the local structure of A with the query set of all variables, with undirected edges between them , and update G. With T2 = B, the query set of variables contains its local structure A from G and the rest of variables.",3.3. Case Study,[0],[0]
"LocalLearn finds D and C as B’s local structure, and updates G. With T3 = C, LocalLearn finds the same A and D and does not make new update of G. With T4 = D, LocalLearn finds E and update G with directed edge D − E in G. Similarly, LocalLearn finds the directed edge E−F and H−F due to the definitive V-structure and update G accordingly, shown in Figure 1b, when Ti = E and F .",3.3. Case Study,[0],[0]
PDAG-to-DAG takes G and produces a possible DAG in Figure 1a.,3.3. Case Study,[0],[0]
"While Algorithm 2 is theoretically sound and complete, in practice further optimizations in the algorithmic procedure can be implemented.",3.4. Implementation Optimization,[0],[0]
"First, inside the iterative LocalLearn procedure, variables in the existing PC and Sp set of Ti should be queried first.",3.4. Implementation Optimization,[0],[0]
These true positive MB set variables could potentially reduce the queried set size to BNStructLearn at each iteration by removing non-MB set of variables early in the process.,3.4. Implementation Optimization,[0],[0]
"Using this procedure is similar to the idea behind an improved version of LocalLearn (Gao & Ji, 2017), which is shown to improve accuracy and reduce computational time.",3.4. Implementation Optimization,[0],[0]
"Secondly, using the same concept, one can also keep track of which variables are removed from Ti’s local structure after adding each queried variable X for target Ti’s local learning procedure, hence forming a separation set, or sepset, of X from T .",3.4. Implementation Optimization,[0],[0]
"Querying variables from the sepset first when X becomes the target could also reduce the potential query set variable size to LocalLearn, as sepset variables are more likely to be adjacent variables of X .",3.4. Implementation Optimization,[0],[0]
"The exact global learning complexity varies depending on the algorithm, but is exponential in the worst case.",3.5. Complexity and Performance Discussion,[0],[0]
"For example, Dynamic programming approaches (Silander & Myllymaki, 2006) cost O(N22N ), where N is the total number of variables present.",3.5. Complexity and Performance Discussion,[0],[0]
"The most expensive step of local-to-global approach is each iteration of local learning using LocalLearn, which costs O(N32N ) using the same dynamic programming approach.",3.5. Complexity and Performance Discussion,[0],[0]
"Repeating for all the variable present, the local-to-global approach takes O(N42N ) in the worst case (Niinimaki & Parviainen, 2012; Gao & Ji, 2017).",3.5. Complexity and Performance Discussion,[0],[0]
"As one can see, in the case where the local structure of one target variable includes all other variable (such as in the Naive Bayesian model), the local-to-global approach would match the complexity of the global approach.",3.5. Complexity and Performance Discussion,[0],[0]
"However, by the iterative nature of the learning and the fact that the number of query variables decreases at later stages of learning, the expected running time is much lower for the local-to-global learning approach.",3.5. Complexity and Performance Discussion,[0],[0]
"If we assume a uniform distribution on the local neighbor size of each node in a network of N nodes, then the expected time complexity of the proposed GGSL approach is O( ∑N i=1",3.5. Complexity and Performance Discussion,[0],[0]
"i
32i) = O(N32N ).",3.5. Complexity and Performance Discussion,[0],[0]
"If we assume l to be the maximum size of local neighbors, then the average complexity would be O(l32l), which can lead to a big performance gain O(2N−l).
",3.5. Complexity and Performance Discussion,[0],[0]
"Our algorithm is a score-based algorithm, as it can use any one of existing score-based optimal learner as the BNStructLearn subroutine.",3.5. Complexity and Performance Discussion,[0],[0]
Theoretical optimality of the proposed algorithm holds only under standard assumptions.,3.5. Complexity and Performance Discussion,[0],[0]
"In real datasets, when the faithfulness assumption
is violated or estimated probabilistic distributions are estimated incorrectly due to insufficient data, the performances of all the algorithms (global and local) are not guaranteed.",3.5. Complexity and Performance Discussion,[0],[0]
"During the learning procedure of the GGSL algorithm, even with a smaller query set of variables, the information about edge existence and orientation with sufficient data does not decrease compared to the global learning methods, if the local variables around each edge are all present.",3.5. Complexity and Performance Discussion,[0],[0]
"Hence, reducing the query set size would not affect performance with sufficient data, although the information loss does happen in practice.",3.5. Complexity and Performance Discussion,[0],[0]
"On the other hand, the estimation performance on the number of data samples is known to be sensitive to the number of parameters.",3.5. Complexity and Performance Discussion,[0],[0]
"The standard error of estimation is σ/ 2 √ N , where σ is the standard deviation.",3.5. Complexity and Performance Discussion,[0],[0]
"Due to the smaller set of variable present, the computation of Bayesian scores could be more accurate in practice when the sample size is limited.",3.5. Complexity and Performance Discussion,[0],[0]
The trade-off between the information loss and estimation error varies among different datasets.,3.5. Complexity and Performance Discussion,[0],[0]
We compare the proposed algorithms with both global and local-to-global learning methods.,4. Experiments,[0],[0]
"Specifically, we compare our results with global methods, Dynamic Programming (DP) structure learning (Silander & Myllymaki, 2006), Constrained Structure Learning (CSL) (de Campos et al., 2009), GOBNILP (Cussens et al., 2016), and local methods Score-based Local Learning (SLL+C) (Niinimaki & Parviainen, 2012) with three different BNStructLearn as above (DP, CSL, and GOBNILP), denoted as SLL+C-DP, SLL+C-CSL, and SLL+C-GOBNILP.",4. Experiments,[0],[0]
"We use the existing implementation of DP (in MATLAB), CSL (in C), and GOBNILP (in C), and implement our algorithms in MATLAB.",4. Experiments,[0],[0]
"We test the algorithms on benchmark BN datasets from the BN repository1, using the datasets provided from existing works(Tsamardinos et al., 2006).",4. Experiments,[0],[0]
"We run the algorithms with 1000 samples of each dataset 10 times, and compare BDeu scores of each algorithm, along with the standard deviation, shown in Table 1.",4. Experiments,[0],[0]
We also compare the algorithms on a synthetic 7-node network for DP as BNStructLearn so algorithms can return results within the time limit.,4. Experiments,[0],[0]
"We report the running time (the entire algorithmic time, including data access, score computation and structure search) of each algorithm2, along with the standard deviation, shown in Table 2, with the maximum running time of 24 hours.",4. Experiments,[0],[0]
The experiments are conducted on a machine with Intel i5-3320M 2.6GHz with 8 GB memory.,4. Experiments,[0],[0]
"Due to memory limitation, the DP method can fail to finish.",4. Experiments,[0],[0]
"CSL and GOBNILP have parameters that can control
1http://www.bnlearn.com/bnrepository/ 2 The time results are different from results on the GOBNILP website, which represent the times of finding the optimal structure given already computed scores.
",4. Experiments,[0],[0]
"DATA SET VARIABLE SIZE DP SLL+C-DP GGSL-DP
7BN 7 -13854.6± 133 -14224.6± 143",4. Experiments,[0],[0]
"-13781.2 ± 133 ALARM 37 OOM -14774.1± 210 -11557.8± 379 CHILDREN 20 OOM -13548.0±172 -12690.0± 106 HAILFINDER 56 OOM -62281.5±213 -54551.6 ± 404 CHILDREN3 60 OOM -38713.2±255 -37271.2± 315
DATA SET VARIABLE SIZE CSL SLL+C-CSL GGSL-CSL
ALARM 37 -10989.8 ± 196",4. Experiments,[0],[0]
"-11437.0.1±268 -11033.4± 382 CHILDREN 20 -12690.0 ± 104 -12811.4±153 -12600.0± 106 HAILFINDER 56 -54375.6 ±111 -58138.1±523 -57794.1± 623 CHILDREN3 60 -37407.5± 228 -38634.8± 249 -37258.3± 340
DATA SET VARIABLE SIZE",4. Experiments,[0],[0]
"GOBNILP SLL+C-GOB GGSL-GOB
ALARM 37 DNF -10337.1± 410 -10575.0±269 CHILDREN 20 -12690.0 ± 104 -12811.4±153 -12600.0± 106",4. Experiments,[0],[0]
"HAILFINDER 56 DNF -54192.5± 781 -53411.7± 844 CHILDREN3 60 DNF -38303.0±402 -36950.1± 382
memory usages.
",4. Experiments,[0],[0]
"As one can see from Table 1, GGSL improves the learning scores by a significant margin over the SLL+C algorithm, with different BNStructLearn in all four datasets tested, except one case in ALARM with GOBNILP.",4. Experiments,[0],[0]
It can even compete with global structure learning approaches in some cases.,4. Experiments,[0],[0]
GGSL outperforms the global learning methods in CHILDREN datasets with CSL and GOBNILP.,4. Experiments,[0],[0]
"Efficiency-wise, from Table 2, using DP, GGSL is more ef-
ficient than SLL+C and can achieve one than one order of speedup.",4. Experiments,[0],[0]
"Using CSL and GOBNILP, GGSL has more than one order of magnitude speed-ups in 3 out of 4 datasets when compared with global learning method.",4. Experiments,[0],[0]
"However, GGSL’s running time is generally slower to SLL+C algorithm using CSL and GOBNILP.",4. Experiments,[0],[0]
"It is faster than SLL+C on HAILFINDER with CSL, and is slightly slower in the other testing cases.",4. Experiments,[0],[0]
"We speculate that the difference in speed gains across different algorithms is mainly the code base of BNStructLearn, where the extra checking Step 3 in GGSL can take proportionally longer time if BNStructLearn is implemented in C.",4. Experiments,[0],[0]
We have proposed a novel graph expanding learning algorithm to learn BN structure.,5. Discussion and Conclusion,[0],[0]
"We strengthen the existing local structure learning analysis, justifying its soundness when the traditional faithfulness condition fails with absent variables, and propose a new local-to-global approach to combine the local structures efficiently.",5. Discussion and Conclusion,[0],[0]
"Experiments have shown that the proposed GGSL improves the accuracy over existing local-to-global algorithms and improves efficiency over existing global algorithms, both by a significant margin.",5. Discussion and Conclusion,[0],[0]
"In addition, GGSL can work with any exact score-based BN learning algorithm and achieve consistent performance gain.",5. Discussion and Conclusion,[0],[0]
"The iterative nature of GGSL can have many applications, such as online BN structure learning with streaming data.",5. Discussion and Conclusion,[0],[0]
"Future work could study how GGSL would work with constraint-based (van Beek & Hoffmann, 2015; Gao & Ji, 2016b) and approximated BN structure learning algorithms as the BNStructLearn routines.",5. Discussion and Conclusion,[0],[0]
We thank Dennis Wei and anonymous reviewers for inspiration and helpful comments.,Acknowledgements,[0],[0]
"We introduce a new local-to-global structure learning algorithm, called graph growing structure learning (GGSL), to learn Bayesian network (BN) structures.",abstractText,[0],[0]
GGSL starts at a (random) node and then gradually expands the learned structure through a series of local learning steps.,abstractText,[0],[0]
"At each local learning step, the proposed algorithm only needs to revisit a subset of the learned nodes, consisting of the local neighborhood of a target, and therefore improves on both memory and time efficiency compared to traditional global structure learning approaches.",abstractText,[0],[0]
"GGSL also improves on the existing local-to-global learning approaches by removing the need for conflictresolving AND-rules, and achieves better learning accuracy.",abstractText,[0],[0]
"We provide theoretical analysis for the local learning step, and show that GGSL outperforms existing algorithms on benchmark datasets.",abstractText,[0],[0]
"Overall, GGSL demonstrates a novel direction to scale up BN structure learning while limiting accuracy loss.",abstractText,[0],[0]
Local-to-Global Bayesian Network Structure Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1380–1390 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1380",text,[0],[0]
Consider the video and natural language query in Figure 1 where we seek to localize the desired moment in the video specified by the query.,1 Introduction,[0],[0]
"Queries like “the girl bends down” require understanding objects and actions, but do not require reasoning about different video moments.",1 Introduction,[0],[0]
"In contrast, queries like “the little girl talks after bending down” require reasoning about the temporal relationship between different actions (“talk” and “bend down”).",1 Introduction,[0],[0]
"Localizing natural language queries in video is an important challenge, recently studied in Hendricks et al. (2017) and Gao et al. (2017) with applications in areas such as video search and retrieval.",1 Introduction,[0],[0]
"We argue that to
∗Work done at Adobe during LAH’s summer internship.
properly localize queries with temporal language, models must understand and reason about intravideo context.
",1 Introduction,[0],[0]
Reasoning about intra-video context is difficult as we do not know a priori which moments should be involved in the contextual reasoning and different queries may require reasoning about different contextual moments.,1 Introduction,[0],[0]
"For example, in “the little girl talks after bending down”, the relevant contextual moment “bending down” occurs just before the target moment “the little girl talks”.",1 Introduction,[0],[0]
This is in contrast to the query “the little girl talks before bending down” where the relevant contextual moment occurs just after.,1 Introduction,[0],[0]
"A limitation of current moment-localization models (Hendricks et al., 2017; Gao et al., 2017) is they consider query-independent video context when localizing moments.",1 Introduction,[0],[0]
"For example, when determining whether a proposed temporal region matches a natural language query, Gao et al. (2017) considers the proposed temporal region, as well as video regions just before and after the proposed region.",1 Introduction,[0],[0]
"Similarly, Hendricks et al. (2017) considers video context in the form of a global-context feature which represents the entire video.",1 Introduction,[0],[0]
"While both may implicitly include the appropriate contextual moment in their context feature, they do not explicitly determine the relevant context for the query.
",1 Introduction,[0],[0]
"To address this difficulty, we propose Moment
Localization with Latent Context (MLLC) which models video context as a latent variable.",1 Introduction,[0],[0]
"The latent variable enables the model to attend to different video contexts conditioned on the specific query/video pair, offering flexibility in the location and length of the contextual moment and overcoming the limitation of query-independent contextual reasoning.",1 Introduction,[0],[0]
We validate the importance of latent context by showing that our model performs well both on simple queries without temporal words and more complex queries requiring temporal reasoning.,1 Introduction,[0],[0]
"Moreover, our formulation is generic and unifies approaches in Hendricks et al. (2017) and Gao et al. (2017), allowing us to ablate model component choices, as well as which kind of video context is best for localizing moments described with temporal language.
",1 Introduction,[0],[0]
"Though datasets used for moment localization in video (Hendricks et al., 2017; Regneri et al., 2013; Sigurdsson et al., 2016) include temporal language, as we will show, there is not enough temporal language to effectively train and evaluate models.",1 Introduction,[0],[0]
"We seek to extensively study this aspect, particularly with respect to temporal prepositions (Pratt-Hartmann, 2004).",1 Introduction,[0],[0]
"Thus, we collect the TEMPOral reasoning in video and language (TEMPO) dataset which builds off the recently collected DiDeMo dataset (Hendricks et al., 2017).",1 Introduction,[0],[0]
"The dataset consists of two parts: a dataset with real videos and sentences created with a template model (TEMPO - Template Language (TL)), and a dataset with real videos and newly collected user-provided temporal annotations (TEMPO - Human Language (HL)).",1 Introduction,[0],[0]
Considering template sentences allows us to create a large dataset of sentences quickly for study of temporal language in a controlled setting.,1 Introduction,[0],[0]
The human language data then allows us to see these trends transfer to more complex human-language queries.,1 Introduction,[0],[0]
"For data collection, we focus on the most common temporal referring words naturally occurring in language-and-video datasets.
",1 Introduction,[0],[0]
Our contributions are twofold.,1 Introduction,[0],[0]
(i),1 Introduction,[0],[0]
We are the first to study models for temporal language in video moment retrieval with natural language queries.,1 Introduction,[0],[0]
"To this end, we introduce TEMPO which includes examples of how humans use temporal language to refer to video moments.",1 Introduction,[0],[0]
(ii) We propose MLLC for moment localization which treats video context as a latent variable and unifies prior approaches for moment localization.,1 Introduction,[0],[0]
"Our
model outperforms prior work on TEMPO-TL and TEMPO-HL as well as the original DiDeMo dataset.",1 Introduction,[0],[0]
Localizing Video Segments with Natural Language.,2 Related Work,[0],[0]
"Prior work has considered aligning natural language with video, e.g., instructional videos with transcribed text (Kiddon et al., 2015; Huang et al., 2017; Malmaud et al., 2014, 2015).",2 Related Work,[0],[0]
"Our work is most related to recent work in video moment retrieval with natural language (Gao et al., 2017; Hendricks et al., 2017).",2 Related Work,[0],[0]
"Both works take a natural language query and candidate video segment as input, and output a score for how well the natural language phrase aligns with the video segment.",2 Related Work,[0],[0]
"Gao et al. (2017) includes an additional loss to regress to start and end-points, whereas Hendricks et al. (2017) simplifies the problem by choosing from a discrete set of video segments.",2 Related Work,[0],[0]
"Importantly, to represent a proposed video segment, both models consider context features around a moment: Hendricks et al. (2017) uses global context by averaging features over an entire input video, and Gao et al. (2017) incorporates features adjacent to the proposed video segment.",2 Related Work,[0],[0]
"We argue that to do proper temporal reasoning, pre-determined, query independent context features may not cover all possible temporal relations.",2 Related Work,[0],[0]
"Thus, we propose to model the context as a latent variable, allowing our method to learn which context moments to consider as a function of the video and importantly, the query.
",2 Related Work,[0],[0]
Both Gao et al. (2017) and Hendricks et al. (2017) collect data to test their models; Gao et al.,2 Related Work,[0],[0]
"(2017) considers the Charades (Sigurdsson et al., 2016) and TACoS (Regneri et al., 2013) datasets.",2 Related Work,[0],[0]
"While TACoS includes localized sentences, Charades only has sentences and activity detection localizations, so a semi-automatic method is used to align action detection annotations to visual descriptions in Charades.",2 Related Work,[0],[0]
"Hendricks et al. (2017) collected the Distinct Describable Moment (DiDeMo) dataset, which consists of Flickr (Thomee et al., 2016) videos with localized referring expressions.",2 Related Work,[0],[0]
"Both Charades and DiDeMo contain a large set of diverse videos (approximately 10,000 videos each).",2 Related Work,[0],[0]
"We chose to base TEMPO on DiDeMo because it contains more clip/sentence pairs (40,000 vs. 13,000), and is focused on general videos which we believe is
an interesting and useful scenario, rather than being restricted to indoor activities.
",2 Related Work,[0],[0]
Temporal Language.,2 Related Work,[0],[0]
"Prior work on temporal language processing has considered building explicit logical frameworks to process temporal prepositions like “during” or “until” (Pratt-Hartmann (2004), Konur (2008)).",2 Related Work,[0],[0]
"We do not derive a particular temporal logic, but rather learn to understand temporal language in a data driven fashion.",2 Related Work,[0],[0]
"Furthermore, we specifically consider how to understand temporal words commonly used when referring to video content.",2 Related Work,[0],[0]
"Other work has modeled dynamics for words which represent a change of state (e.g., “pick up”)",2 Related Work,[0],[0]
"( Siskind (2001), Yu et al. (2015)) in limited environments.",2 Related Work,[0],[0]
"Though we limit the selection of temporal words in our study, the natural language in our data is open-world describing diverse events and how they relate to each other in video.",2 Related Work,[0],[0]
"Interpretation of temporal expressions in text (“The game happened on the 19th”) is a widely studied task (Angeli et al. (2012), Zhong et al. (2017)).",2 Related Work,[0],[0]
"Our work is distinctly different from this line of work as we specifically study temporal prepositions and how they refer to video.
",2 Related Work,[0],[0]
Modeling Visual Relationships.,2 Related Work,[0],[0]
"A variety of papers have considered modeling spatial relationships in natural images (Dai et al., 2017; Hu et al., 2017; Peyre et al., 2017; Plummer et al., 2017).",2 Related Work,[0],[0]
Our approach is analogous to this in the temporal domain; we hope to localize moments in videos.,2 Related Work,[0],[0]
"CLEVR, a synthetic visual question answering (VQA) dataset (Johnson et al., 2016), was created to allow researchers to systematically study the ability of models to perform complex reasoning.",2 Related Work,[0],[0]
Our dataset is partially motivated by the success of CLEVR to enable researchers to study reasoning abilities of different models in a controlled setting.,2 Related Work,[0],[0]
"In contrast to CLEVR we consider a more diverse visual input in the form of real videos.
",2 Related Work,[0],[0]
"In the video domain, the TGIF-QA (Jang et al., 2017) and Mario-QA (Mun et al., 2016) datasets provide opportunities to study temporal reasoning for the task of VQA.",2 Related Work,[0],[0]
"The TGIF-QA dataset considers three types of temporal questions: before/after questions, repetition count, and determining a repeating action.",2 Related Work,[0],[0]
Each question is accompanied by multiple choice answers.,2 Related Work,[0],[0]
Videos we consider are much longer (25-30s as opposed to an average of 3.1s) which makes the use of temporal reasoning much more important.,2 Related Work,[0],[0]
"The MarioQA dataset is an additional VQA dataset de-
signed to gauge temporal reasoning of VQA systems.",2 Related Work,[0],[0]
Both TGIF-QA and MarioQA datasets include template-based natural language queries.,2 Related Work,[0],[0]
"In this paper, we consider synthetic queries similar to TGIF-QA and MarioQA, but also include human language queries.",2 Related Work,[0],[0]
"In addition, unlike the MarioQA dataset, that consists of synthetic data constructed from gameplay videos, our dataset consists of real visual inputs, and includes temporal grounding of natural language phrases.",2 Related Work,[0],[0]
"Finally, neither TGIFQA nor MarioQA include temporal localization.",2 Related Work,[0],[0]
"Given a video v and natural-language query q describing a moment in the video, our goal is to output the moment τ =",3 Moment Localization with Latent Context,[0],[0]
"( τ (s), τ (e) )",3 Moment Localization with Latent Context,[0],[0]
"where τ (s) and τ (e) are temporal start and end points in the video, respectively.",3 Moment Localization with Latent Context,[0],[0]
"In the following, we formulate a generic, unified model which encompasses prior approaches (Hendricks et al., 2017; Gao et al., 2017).",3 Moment Localization with Latent Context,[0],[0]
This allows us to explore and evaluate trade offs for different model components and extensions which then leads to higher performance.,3 Moment Localization with Latent Context,[0],[0]
"Unlike prior work, we consider a latent context variable which enables our model to better reason about temporal language.
",3 Moment Localization with Latent Context,[0],[0]
Let the moment τ corresponding to the text query be the base moment and the set of other video moments Tτ be possible context moments for τ .,3 Moment Localization with Latent Context,[0],[0]
"We define a scoring function between the video moment and natural-language query by maximizing over all possible context moments
τ ′",3 Moment Localization with Latent Context,[0],[0]
"∈ Tτ ,
sφ (v, q, τ) = max τ ′∈Tτ
fS ( fV ( v, τ, τ ′ ) , fL (q) ) ,
(1) where fV and fL are functions computing features over the video and language query, fS is a similarity function, and φ are model parameters.",3 Moment Localization with Latent Context,[0],[0]
This formulation is generic and trivially encompasses the MCN and TALL formulations by letting the set of possible context moments Tτ be their respective single-context moment.,3 Moment Localization with Latent Context,[0],[0]
"Figure 2 shows the generic structure of our model.
",3 Moment Localization with Latent Context,[0],[0]
"With this formulation, we seek to answer the following questions: (i) Which combination of model components performs best for the momentretrieval task?",3 Moment Localization with Latent Context,[0],[0]
"Though our primary goal is localizing moments with temporal language, we believe a good base moment retrieval model is important for localizing moments with temporal language.",3 Moment Localization with Latent Context,[0],[0]
(ii) How best to incorporate context for moment retrieval with temporal language?,3 Moment Localization with Latent Context,[0],[0]
"We first detail the different terms and outline different model design choices, where design choices marked with bolditalic font is ablated in Section 5.",3 Moment Localization with Latent Context,[0],[0]
"Components which are used in our final proposed Moment Localization with Latent Context (MLLC) model and prior models are summarized in Table 3.
Video feature fV .",3 Moment Localization with Latent Context,[0],[0]
"The video feature fV = (g (v, τ) , g (v, τ ′) , fT (τ, τ
′)) is a concatenation of visual features for the base g (v, τ) and context g (v, τ ′) moments and endpoint features fT (τ, τ
′).",3 Moment Localization with Latent Context,[0],[0]
"To compute visual features g for a temporal region τ , per-frame features are averaged over the temporal region.",3 Moment Localization with Latent Context,[0],[0]
"Note that if the context moment consists of more than one contiguous temporal region, then the visual features are computed over each contiguous temporal region and then concatenated (c.f., before/after context in TALL, explained below).",3 Moment Localization with Latent Context,[0],[0]
There are many choices for visual features.,3 Moment Localization with Latent Context,[0],[0]
"TALL (Gao et al., 2017) compares average fc7 features (extracted from (Simonyan and Zisserman, 2014)) to features extracted with C3D (Tran et al., 2015) and LSTM features (Donahue et al., 2015).",3 Moment Localization with Latent Context,[0],[0]
"Surprisingly, C3D features only outperform average fc7 features by a small margin.",3 Moment Localization with Latent Context,[0],[0]
"We use the visual features used in the MCN model (Hendricks et al., 2017), which are similar to the fc7 features from (Gao et al., 2017), but included motion features as well, computed from optical flow (extracted with (Wang et al., 2016)).",3 Moment Localization with Latent Context,[0],[0]
"We then pass the extracted visual
features through a MLP.",3 Moment Localization with Latent Context,[0],[0]
"Note that we learn separate embedding functions for RGB and optical flow inputs and combine scores from different input modalities using a late-fusion approach (Hendricks et al., 2017).
",3 Moment Localization with Latent Context,[0],[0]
Endpoint feature fT .,3 Moment Localization with Latent Context,[0],[0]
Modeling temporal context requires understanding how different temporal segments relate in time.,3 Moment Localization with Latent Context,[0],[0]
"Hendricks et al. (2017) suggest including temporal endpoint features (TEF) fT = ( τ (s), τ (e) ) for the base moment which encode when the moment starts and ends to better localize sentences which include words like “first” and “last”.",3 Moment Localization with Latent Context,[0],[0]
"Note that TALL (Gao et al., 2017) does not incorporate TEFs.",3 Moment Localization with Latent Context,[0],[0]
"In order to understand temporal relationships, it is important that models also include features which indicate when a context moment occurs.",3 Moment Localization with Latent Context,[0],[0]
"In addition to providing TEFs for base moments, we also experiment with concatenating TEFs for context moments (conTEF) fT = ( τ (s), τ (e), τ ′(s), τ ′(e) ) .
",3 Moment Localization with Latent Context,[0],[0]
"Language feature fL. Text queries are transformed into a fixed-length vector with an LSTM (Hochreiter and Schmidhuber, 1997).",3 Moment Localization with Latent Context,[0],[0]
"Before inputting words into the LSTM, they are embedded in the Glove (Pennington et al., 2014) embedding space.",3 Moment Localization with Latent Context,[0],[0]
The final layer of the LSTM is projected into the shared video-language embedding space with a fully connected layer.,3 Moment Localization with Latent Context,[0],[0]
Gao et al. (2017) considers LSTM language features and Skip-thought encoders.,3 Moment Localization with Latent Context,[0],[0]
"Our main goal is to study how context impacts moment localization with temporal language, so we use the LSTM features used on the original DiDeMo dataset.
",3 Moment Localization with Latent Context,[0],[0]
Similarity fS .,3 Moment Localization with Latent Context,[0],[0]
"Given video fV and language fL features, we consider three ways to encode similarity between the features.",3 Moment Localization with Latent Context,[0],[0]
"Like Hendricks et al. (2017), we consider a distance-based similarity fS = ( |fV − fL|2 ) .",3 Moment Localization with Latent Context,[0],[0]
"Second, we consider a fused-feature similarity (mult) where the Hadamard product fV fL between the two features are passed to a MLP.",3 Moment Localization with Latent Context,[0],[0]
We also explore unit normalizing features before the Hadamard product (normalized mult).,3 Moment Localization with Latent Context,[0],[0]
"Finally, we consider the similarity (TALL similarity) which consists of the concatenation (fV , fL, fV fL, fV + fL) and then passed to a MLP.
",3 Moment Localization with Latent Context,[0],[0]
Context moments,3 Moment Localization with Latent Context,[0],[0]
Tτ .,3 Moment Localization with Latent Context,[0],[0]
We consider three sets of context moments.,3 Moment Localization with Latent Context,[0],[0]
"First, we consider the entire video as the context moment (global) following Hendricks et al. (2017).",3 Moment Localization with Latent Context,[0],[0]
"Second, we consider us-
ing the moments just before and after the base moment (before/after).",3 Moment Localization with Latent Context,[0],[0]
"Finally, we consider using the set of all possible moments (latent context) which offers greatest flexibility in contextual reasoning.
",3 Moment Localization with Latent Context,[0],[0]
Training loss.,3 Moment Localization with Latent Context,[0],[0]
We consider two training losses.,3 Moment Localization with Latent Context,[0],[0]
The first loss is the MCN ranking loss which encourages positive moment/query pairs to have a smaller distance in a shared embedding space than negative moment/query pairs.,3 Moment Localization with Latent Context,[0],[0]
"To sample negative moment/sentence pairs, they consider negative moments within a specific video (called intravideo negative moments) and negative moments in different videos (called inter-video negative moments).",3 Moment Localization with Latent Context,[0],[0]
This sampling strategy leads to a small improvement in performance (approximately one point on all metrics) when compared to just using intra-video negative moments.,3 Moment Localization with Latent Context,[0],[0]
"We also consider the alignment loss used in TALL (TALL loss) which is the sum of two log-logistic functions over positive and negative training query/moment pairs (intra-video negatives are used).
",3 Moment Localization with Latent Context,[0],[0]
Supervising context moments.,3 Moment Localization with Latent Context,[0],[0]
"For the temporal sentences in our newly collected dataset (Section 4), we have access to the ground-truth context moment during training.",3 Moment Localization with Latent Context,[0],[0]
"Thus, we can contrast a weakly supervised setting in which we optimize over the unknown latent context moments during learning and inference to a strongly supervised setting.",3 Moment Localization with Latent Context,[0],[0]
Implementation details.,3 Moment Localization with Latent Context,[0],[0]
Candidate base and context moments coincide to the pre-segmented fivesecond segments used when annotating DiDeMo.,3 Moment Localization with Latent Context,[0],[0]
Moments may consist of any contiguous set of five-second segments.,3 Moment Localization with Latent Context,[0],[0]
"For a 30-second video partitioned into six five-second segments, there are 21 possible moments.",3 Moment Localization with Latent Context,[0],[0]
"All models were implemented in Caffe (Jia et al., 2014) and optimized with SGD.",3 Moment Localization with Latent Context,[0],[0]
"Models were trained for ∼ 90 epochs with an initial learning rate of 0.05, which decreases every 30 epochs.",3 Moment Localization with Latent Context,[0],[0]
Code is publicly released∗.,3 Moment Localization with Latent Context,[0],[0]
We collect the TEMPOral reasoning in video and language (TEMPO) dataset based off the recently released DiDeMo dataset.,4 The TEMPO Dataset,[0],[0]
Our dataset consists of two parts: TEMPO - Template Language (TL) and TEMPO - Human Language (HL).,4 The TEMPO Dataset,[0],[0]
"We create TEMPO - TL using language templates to augment the original sentences in DiDeMo with tem∗https://people.eecs.berkeley.edu/ ˜lisa_anne/tempo.html
poral words.",4 The TEMPO Dataset,[0],[0]
The template allows us to generate a large number of sentences with known ground truth base and context moments.,4 The TEMPO Dataset,[0],[0]
"However, template language lacks the complexity of human language, so we then collect an additional fully userconstructed dataset, TEMPO - HL, consisting of sentences that contain specific temporal words.
",4 The TEMPO Dataset,[0],[0]
Temporal Words in Current Datasets.,4 The TEMPO Dataset,[0],[0]
We first analyze temporal words which occur in current natural language moment retrieval datasets.,4 The TEMPO Dataset,[0],[0]
"We consider temporal adjectives, adverbs, and prepositions found both by closely analyzing moment-localization datasets and consulting lists containing words which belong to different parts of speech.",4 The TEMPO Dataset,[0],[0]
"In particular, we rely on the preposition project (Litkowski and Hargraves, 2005)† to scrape relevant temporal words.",4 The TEMPO Dataset,[0],[0]
"Table 2 shows example temporal words and the number of times they occur in each dataset (TACoS (Regneri et al., 2013), Charades (Gao et al., 2017), DiDeMo (Hendricks et al., 2017)).",4 The TEMPO Dataset,[0],[0]
"Though all moment localization datasets use temporal words, they do not contain enough examples to reliably train and evaluate current models.",4 The TEMPO Dataset,[0],[0]
"Additionally, we observe that temporal words which are frequently used when describing video segments are different than those commonly used in text without video grounding.",4 The TEMPO Dataset,[0],[0]
"For example, in PrattHartmann (2004), “during” is a common example, but we observe that “during” is infrequently used when describing video.",4 The TEMPO Dataset,[0],[0]
"Of temporal words, we focus on the four most common words, “before”, “after”, “then”, and “while” when creating our dataset.
",4 The TEMPO Dataset,[0],[0]
TEMPO - Template Language.,4 The TEMPO Dataset,[0],[0]
"To construct sentences in TEMPO-TL, we find adjacent moments in the DiDeMo dataset and fill in template sentences for “before”, “after”, and “then” temporal words.",4 The TEMPO Dataset,[0],[0]
"For “before”, we use two templates: “X before Y ” and “Before Y , X”, where X and Y are sentences from the original DiDeMo dataset.",4 The TEMPO Dataset,[0],[0]
"Likewise for “after”, we consider the templates “X after Y ” and “After Y , X”.",4 The TEMPO Dataset,[0],[0]
"For “then” we only consider one template, “X then Y .”
TEMPO - Human Language.",4 The TEMPO Dataset,[0],[0]
"Though the template dataset is an interesting testbed for understanding temporal language, it is difficult to replicate the interesting complexities in human language.",4 The TEMPO Dataset,[0],[0]
"For example, when writing long sen-
†http://www.clres.com/prepositions.",4 The TEMPO Dataset,[0],[0]
"html
",4 The TEMPO Dataset,[0],[0]
"The girl looks at the camera and waves
The little girl turns and waves at the camera while on her skates.",4 The TEMPO Dataset,[0],[0]
"After the girl waves at the camera she continues to skate.
",4 The TEMPO Dataset,[0],[0]
"tences with temporal prepositions, humans frequently make use of language structure such as coreference to form more cohesive statements.
",4 The TEMPO Dataset,[0],[0]
"To collect annotations, we follow the protocol in Hendricks et al. (2017) and segment videos into 5-second temporal segments.",4 The TEMPO Dataset,[0],[0]
"After collecting descriptions, we ensure descriptions are localizable by asking other workers to localize each moment.",4 The TEMPO Dataset,[0],[0]
"To collect data for “before”, “after”, and “then”, we ask annotators to describe a segment in relation to a “reference” moment from the DiDeMo dataset.",4 The TEMPO Dataset,[0],[0]
"For example, if the DiDeMo dataset includes a localized phrase like “the cat jumps”, annotators write a sentence which refers to the segment “the cat jumps” using a specific temporal word.",4 The TEMPO Dataset,[0],[0]
"We provide both the phrase (“the cat jumps”) and the reference moment to annotators, and the annotators provide a sentence describing a new moment which references the reference moment.
",4 The TEMPO Dataset,[0],[0]
TEMPO-HL includes unique properties which are hard to replicate with template data.,4 The TEMPO Dataset,[0],[0]
"Figure 3
depicts the base moment provided to workers, as well as descriptions from TEMPO-HL.",4 The TEMPO Dataset,[0],[0]
"In Figure 3, the description “The adult hands the little boy the stick then they walk away” includes an example of visual coreference (“they”).",4 The TEMPO Dataset,[0],[0]
"We note that use of pronouns is much more prevalent in TEMPO-HL, with 28.1% of sentences in TEMPOHL including pronouns (“he”, “she”, “it”) in contrast to 10.3% of sentences in the original DiDeMo dataset.",4 The TEMPO Dataset,[0],[0]
"Additionally, annotators will refer to the base moment with different language than originally used in the base moment (e.g., “the girl waves at the camera” versus the base moment “the girl looks at the camera and waves”) in order to make their sentences more fluent.",4 The TEMPO Dataset,[0],[0]
Evaluation Method.,5 Experiments,[0],[0]
"We follow the evaluation protocol defined for the DiDeMo dataset (Hendricks et al., 2017) over all possible combinations of the five-second video segments.",5 Experiments,[0],[0]
"We report rank at one (R@1), rank at five (R@5), and mean intersection over union (mIOU) using their aggregator over three out of the four human annotators.",5 Experiments,[0],[0]
"We compare our models on TEMPOTL, TEMPO-HL, and the DiDeMo dataset.",5 Experiments,[0],[0]
"When training our models, we combine the DiDeMo dataset with TEMPO-TL or TEMPO-HL.",5 Experiments,[0],[0]
"This enables our model to concurrently learn to localize the simpler DiDeMo sentences with more complex TEMPO sentences.
",5 Experiments,[0],[0]
Baselines.,5 Experiments,[0],[0]
"We compare to the two recently proposed approaches for video moment localization: MCN (Hendricks et al., 2017) and TALL (Gao et al., 2017).",5 Experiments,[0],[0]
"We adapt the implementation of TALL (Gao et al., 2017) to the DiDeMo dataset in three ways.",5 Experiments,[0],[0]
"First, we do not include the temporal localization loss required to regress to specific start and end points as DiDeMo, and thus also TEMPO, is pre-segmented, so the model does not need to compute exact start and end points.",5 Experiments,[0],[0]
"Second, the original TALL model uses C3D features.
",5 Experiments,[0],[0]
For a fair comparison we train both models with the same RGB and flow features extracted as was done for the original MCN model.,5 Experiments,[0],[0]
"Finally, the MCN model proposes temporal endpoint features (TEF) to indicate when a proposed moment occurs within a video.",5 Experiments,[0],[0]
"We train TALL with and without the TEF and show that TEF improves performance on the original DiDeMo dataset.
Ablations.",5 Experiments,[0],[0]
"To ablate our proposed latent context, we compare to other models which share the same MLLC base network.",5 Experiments,[0],[0]
We consider the MLLC model with global context and before/after context.,5 Experiments,[0],[0]
We also train a model with weakly supervised (WS) latent context and strongly supervised (SS) latent context.,5 Experiments,[0],[0]
"We also train models both with and without context TEF (conTEF).
",5 Experiments,[0],[0]
The MLLC Base Model.,5 Experiments,[0],[0]
We first ablate our MLLC base model (Table 3).,5 Experiments,[0],[0]
We train our models on TEMPO-TL and DiDeMo and evaluate on the original DiDeMo dataset.,5 Experiments,[0],[0]
All models are trained with global context.,5 Experiments,[0],[0]
We find that the ranking loss is preferable on the DiDeMo dataset (compare lines 1 and 2) and that TALL-similarity performs better than the distance based similarity of the MCN model (compare lines 1 and 5).,5 Experiments,[0],[0]
"A simpler version of the TALL-similarity, in which the concatenated element wise multiplication, element wise sum, and concatenation is replaced by a single normalized elementwise multiplication, increases R@1 by almost one point and increases mIoU by over two points (compare lines 5-7).",5 Experiments,[0],[0]
We call our best model the MLLC-Base model (line 7).,5 Experiments,[0],[0]
"Our MLLC-Base model performs better than previous models (MCN line 1 and TALL line 3).
",5 Experiments,[0],[0]
Results: TEMPO - TL.,5 Experiments,[0],[0]
We first compare different moment localization models on TEMPO - TL (Table 4).,5 Experiments,[0],[0]
"In particular, our model performs well on “before” and “after” words.",5 Experiments,[0],[0]
"Additionally,
our MLLC model with global context outperforms both the MCN model (Hendricks et al., 2017) and the TALL (Gao et al., 2017) model when considering all sentence types, verifying the strength of our base MLLC model.
",5 Experiments,[0],[0]
"Comparing MLLC with global context and MLLC with before/after context (compare row 4 and 5), we note that before/after context is important for localizing “before” and “after” moments.",5 Experiments,[0],[0]
"However, our model with strong supervision (row 9) outperforms the model trained with before and after context, suggesting that learning to reason about which context moment is correct (as opposed to being explicitly provided with the context before and after the moment) is beneficial.",5 Experiments,[0],[0]
"We note that strong supervision (SS) outperforms weak supervision (WS) (compare rows 7 and 9) and that the context TEF is important for best performance (compare rows 8 and 9).
",5 Experiments,[0],[0]
"We note that though the MLLC-global model outperforms our full model for “then” on TEMPOTL, our full model performs better on then for the TEMPO-HL (Table 6).",5 Experiments,[0],[0]
One possibility is that the “then” moments in TEMPO-TL do not require context to properly localize the moment.,5 Experiments,[0],[0]
"Because TEMPO-TL is constructed from DiDeMo sentences, constituent sentence parts are referring.",5 Experiments,[0],[0]
"For example, given an example sentence from TEMPO-TL (e.g., “The cross is seen for the first time then window is first seen in room”), the model does not need to reason about the ordering of “cross seen for the first time” and “window is seen for the first time” because both moments only happen once in the video.",5 Experiments,[0],[0]
"In contrast, when considering the sentence “The adult hands the little boy a stick then they begin to walk” (from Figure 3), “begin to walk” could refer to multiple video moments.",5 Experiments,[0],[0]
"Consequently, our model must reason about the temporal ordering of reference moments to properly localize the video moment.
",5 Experiments,[0],[0]
"On TEMPO - TL, sentences differ from original DiDeMo sentences solely because of the use of temporal words.",5 Experiments,[0],[0]
"Thus, we can do a controlled study of how well models understand temporal words.",5 Experiments,[0],[0]
"If a model has good temporal reasoning, then if it can localize a reference moment “the dog jumps” it should be easier for the model to localize the moment “the dog sits after the dog jumps”.",5 Experiments,[0],[0]
"To test whether models are capable of this, we look at only sentences in TEMPO - TL where the model has correctly localized the cor-
responding context moment in DiDeMo (Table 5).",5 Experiments,[0],[0]
We report the difference in performance when considering only sentences in which temporal context was properly localized and all sentences.,5 Experiments,[0],[0]
"On our model, performance on all three temporal word types increases when the context moment can be properly localized.",5 Experiments,[0],[0]
"When considering global context, performance on “before” and “after” actually decreases, suggesting global context does not understand temporal reasoning well.",5 Experiments,[0],[0]
"Finally, even when the context is correctly localized, there is still ample room for improvement on all three sentence types motivating future work on temporal reasoning for moment retrieval.
",5 Experiments,[0],[0]
Results: TEMPO - HL.,5 Experiments,[0],[0]
Table 6 compares performance on TEMPO - HL.,5 Experiments,[0],[0]
We compare our bestperforming model from training on the TEMPOTL (strongly supervised MLLC and conTEF) to prior work (MCN and TALL) and to MLLC with global and before/after context.,5 Experiments,[0],[0]
"Performance on TEMPO-HL is considerably lower than TEMPOTL suggesting that TEMPO-HL is harder than TEMPO-TL.
",5 Experiments,[0],[0]
"On TEMPO - HL, we observe similar trends as on TEMPO-TL.",5 Experiments,[0],[0]
"When considering all sentence
types, MLLC has the best performance across all metrics.",5 Experiments,[0],[0]
"In particular, our model has the strongest performance for all sentence types considering the mIoU metric.",5 Experiments,[0],[0]
"In addition to performing better on temporal words, our model also performs better on the original DiDeMo dataset.",5 Experiments,[0],[0]
"As was seen in TEMPO-TL, including before/after context performs better than our model trained with global context for both “before” and “after” words.
",5 Experiments,[0],[0]
The final row of Table 6 shows an upper bound in which the ground truth context is used at test time instead of the latent context.,5 Experiments,[0],[0]
"We note that results improve for “before”, “after”, and “then”, suggesting that learning to better localize context will improve results for these sentence types.
",5 Experiments,[0],[0]
Localizing Context Fragments.,5 Experiments,[0],[0]
"TEMPO-HL sentences can be broken into two parts: a basesentence fragment (which refers to the base moment), and a context-sentence fragment (which refers to the context moment).",5 Experiments,[0],[0]
"For example, for the sentence “The girl holds the ball before throwing it,”, “the girl holds the ball” is the base fragment and “throwing it” is the context fragment.",5 Experiments,[0],[0]
"A majority of the “before” and “after” sentences in TEMPO-HL are of the form “X before (or after) Y ”, so we can determine a list of sentence fragments by splitting sentences based on the temporal word.",5 Experiments,[0],[0]
"Given “before” and “after” sentences, we determine the ground truth context fragment by considering which reference moment was given to annotators.",5 Experiments,[0],[0]
We can then measure how well models localize context fragments.,5 Experiments,[0],[0]
"Table 7 compares two approaches to localizing context fragments: inputting just the context fragment into MLLC
TEMPO - Human Language (HL)
Getting up while holding baby.
",5 Experiments,[0],[0]
"Ground truth
The mother sheep leaves the babies, then the babies follow.
",5 Experiments,[0],[0]
"Ground truth
and reporting the context used by MLLC when inputting the entire query into our model.",5 Experiments,[0],[0]
"We find that our model reliably selects the correct context fragments, most likely because it can properly exploit temporal understanding of how the context fragment relates to the base fragment.
",5 Experiments,[0],[0]
Visualizing Context.,5 Experiments,[0],[0]
"In addition to a localized query, we can also visualize which context moment the temporal query refers to.",5 Experiments,[0],[0]
"Figure 4 shows predicted moments and their corresponding con-
text moments.",5 Experiments,[0],[0]
"For the query “The girl with a hat takes a drink before the girl without a hat waves”, the little girl in the hat drinks twice, but our model correctly localizes the time she drinks before the other girl waves.",5 Experiments,[0],[0]
"Likewise, for the moment “After zooming in to the dog, the dog darts across the grass and into the woods”, the dog darts towards the woods twice (at the beginning of the video and at the end).",5 Experiments,[0],[0]
"Our model properly localizes the moment when the dog runs towards the forest the second time as well as the context fragment “zooming in on dog” when localizing the moment.
",5 Experiments,[0],[0]
Discussion.,5 Experiments,[0],[0]
"We show promising results on both TEMPO-TL and TEMPO-HL, but there is potential improvement for building better frameworks for understanding temporal language.",5 Experiments,[0],[0]
"In Table 6, strongly supervising context at test time improves overall results, suggesting that models which can better localize context text will outperform our current model.",5 Experiments,[0],[0]
"Though TEMPO and DiDeMo have over 60,000 sentences combined, visual content is quite diverse.",5 Experiments,[0],[0]
"Integrating outside data sources (e.g., image retrieval and captioning) could possibly improve results on moment localization, both with and without temporal language queries.",5 Experiments,[0],[0]
"Additionally, in Table 5, even when the MLLC model can properly localize context, it does not always properly localize temporal sentences indicating that improved temporal reasoning can also improve our results.",5 Experiments,[0],[0]
"We believe our dataset, analysis, and method are an important step towards better moment retrieval models that effectively reason about temporal language.",5 Experiments,[0],[0]
We thank Anna Rohrbach for helpful feedback.,Acknowledgements,[0],[0]
"Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding.",abstractText,[0],[0]
"Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text.",abstractText,[0],[0]
"We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language.",abstractText,[0],[0]
"To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset.",abstractText,[0],[0]
"Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO Human Language).",abstractText,[0],[0]
Localizing Moments in Video with Temporal Language,title,[0],[0]
Differential privacy is a mathematically rigorous notion of privacy that has become the de-facto gold-standard of privacy preserving data analysis.,1. Introduction,[0],[0]
"Informally, -differential privacy bounds the affect of a single datapoint on any result of the computation by .",1. Introduction,[0],[0]
In recent years the subject of private hypothesis testing has been receiving increasing attention (see Related Work below).,1. Introduction,[0],[0]
"However, by and large, the focus of private hypothesis testing is in the centralized model (or the curated model), where a single trusted entity holds the sensitive details of n users and runs the private hypothesis tester on the actual data.
",1. Introduction,[0],[0]
1Dept.,1. Introduction,[0],[0]
"of Computing Science, University of Alberta..",1. Introduction,[0],[0]
Correspondence to: Or Sheffet,1. Introduction,[0],[0]
"<osheffet@ualberta.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In contrast, the subject of this work is private hypothesis testing in the local-model (or the distributed model), where a -differentially private mechanism is applied independently to each datum.",1. Introduction,[0],[0]
"This model, which alleviates trust (each user can run the mechanism independently on her own and release the noisy signal from the mechanism), has gained much popularity in recent years, especially since it was adopted by Google’s Rappor (Erlingsson et al., 2014) and Apple (Apple, 2017).",1. Introduction,[0],[0]
"And yet, despite its popularity, and the fact that recent works (Bassily & Smith, 2015; Bassily et al., 2017) have shown the space of possible locally-private mechanism is richer than what was originally thought, little is known about private hypothesis testing in the local-model.",1. Introduction,[0],[0]
We view the local differentially private model as a signaling scheme.,1.1. Background: Local Differential Privacy,[0],[0]
Each datum / user has a type x taken from a predefined and publicly known set of possible types X whose size is T = |X |.,1.1. Background: Local Differential Privacy,[0],[0]
"The differentially private mechanism is merely a randomized function M : ([n],X ) → S, mapping each possible type X of the i-th datum to some set of possible signals S, which we assume to be -differentially private: for any index",1.1. Background: Local Differential Privacy,[0],[0]
"i, any pair of types x, x′ ∈ X and any signal s ∈ S",1.1. Background: Local Differential Privacy,[0],[0]
"it holds that Pr[M(i, x) = s] ≤ e Pr[M(i, x′) = s].1 In our most general results (Theorems 1 and 9), we ignore the fact thatM is -differentially private, and just refer to any signaling scheme that transforms one domain (namely, X ) into another (S).",1.1. Background: Local Differential Privacy,[0],[0]
"For example, a surveyer might unify rarely occurring types under the category of “other”, or perhaps users report their types over noisy channels, etc.
",1.1. Background: Local Differential Privacy,[0],[0]
"We differentiate between two types of signaling schemes: the symmetric (or index-oblivious) variety, and the nonsymmetric (index-aware) type.",1.1. Background: Local Differential Privacy,[0],[0]
A local signaling mechanism is called symmetric if it is independent of the index of the datum.,1.1. Background: Local Differential Privacy,[0],[0]
"Namely, if for any i 6=",1.1. Background: Local Differential Privacy,[0],[0]
"j we have that M(i, x) = M(j, x) def= M(x).",1.1. Background: Local Differential Privacy,[0],[0]
"A classic exam-
1For simplicity, we assume S, the set of possible signals, is discrete.",1.1. Background: Local Differential Privacy,[0],[0]
Note that this doesn’t exclude mechanisms such as adding Gaussian/Gamma noise to a point in Rd — such mechanisms require X to be some bounded subset of Rd and use the bound to set the noise appropriately.,1.1. Background: Local Differential Privacy,[0],[0]
"Therefore, the standard approach of discretizing X and projecting the noisy point to the closest point in the grid yields a finite set of signals S.
ple of such a mechanism is randomized-response — that actually dates back to before differential privacy was defined (Warner, 1965) and was first put to use in differential privacy in (Kasiviswanathan et al., 2008) — where each user / datum x draws her own signal from the set S = X skewing the probability ever-so-slightly in favor of the original type.",1.1. Background: Local Differential Privacy,[0],[0]
"I.e. if the user’s type is x then
M(x) =
{ x, w.p. e
T−1+e
x′, for any other x′ w.p. 1T−1+e .
",1.1. Background: Local Differential Privacy,[0],[0]
"The utility of the above-mentioned symmetric mechanism scales polynomially with T (or rather, with |S|), which motivated the question of designing local mechanisms with error scaling logarithmically in T .",1.1. Background: Local Differential Privacy,[0],[0]
"This question was recently answered in the affirmative by the works of Bassily and Smith (2015) and Bassily et al (2017), whose mechanisms are not symmetric.",1.1. Background: Local Differential Privacy,[0],[0]
"In fact, both of them work by presenting each user i with a mapping fi : X → S (the mapping itself is chosen randomly, but it is public, so we treat it as a given), and the user then runs the standard randomized response mechanism on the signals using fi(x) as the more-likely signal.",1.1. Background: Local Differential Privacy,[0],[0]
"(In fact, in both schemes, S = {1,−1}: in (Bassily & Smith, 2015) fi is merely the j-th coordinate of a hashing of the types where j and the hashing function are publicly known, and in (Bassily et al., 2017) fi maps a u.a.r chosen subset of X to 1 and its complementary to −1.2)",1.1. Background: Local Differential Privacy,[0],[0]
"So given fi, the user then tosses her own private random coins to determine what signal she broadcasts.",1.1. Background: Local Differential Privacy,[0],[0]
"Therefore, each user’s mechanism can be summarized in a |S| × |X |-matrix, where Mi(s, x) is the probability a user of type x sends the signal s.",1.1. Background: Local Differential Privacy,[0],[0]
"For example, using the mechanism of (Bassily et al., 2017), each user whose type maps to 1 sends “signal 1” with probability e
1+e and “signal −1” with probability 1 1+e .",1.1. Background: Local Differential Privacy,[0],[0]
"Namely, Mi(fi(x), x) = e 1+e andMi(−fi(x), x) = 1
1+e , where fi is the mapping X → {1,−1} set for user i.",1.1. Background: Local Differential Privacy,[0],[0]
This work initiates (to the best of our knowledge) the theory of differentially private hypothesis testing in the local model.,1.2. Our Contribution and Organization,[0],[0]
First we survey related work and preliminaries.,1.2. Our Contribution and Organization,[0],[0]
"Then, in Section 3, we examine the symmetric case and show that any mechanism (not necessarily a differentially private one) yields a distribution on the signals for which finding a maximum-likelihood hypothesis is feasible, assuming the set of possible hypotheses is convex.",1.2. Our Contribution and Organization,[0],[0]
"Then, focusing on the classic randomized-response mechanism, we show that the problem of maximizing the likelihood of the observed signals is strongly-convex and thus simpler than the original problem.",1.2. Our Contribution and Organization,[0],[0]
"More importantly, in essence
2In both works, much effort is put to first reducing T to the most frequent √ n types, and then run the counting algorithm.",1.2. Our Contribution and Organization,[0],[0]
"Regardless, the end-counts / collection of users’ signals are the ones we care for the sake of hypothesis testing.
",1.2. Our Contribution and Organization,[0],[0]
we give a characterization of hypothesis testing under randomized response: the symmetric locally-private mechanism translates the original null hypothesis,1.2. Our Contribution and Organization,[0],[0]
H0 (and the alternative H1) by a known affine translation into a different set ϕ(H0) (and resp. ϕ(H1)).,1.2. Our Contribution and Organization,[0],[0]
"Hence, hypothesis testing under randomized-response boils to discerning between two different (and considerably closer in total-variation distance) sets, but in the exact same model as in standard hypothesis testing as all signals were drawn from the same hypothesis in ϕ(H0).",1.2. Our Contribution and Organization,[0],[0]
As an immediate corollary we give bounds on identity-testing (Corollary 5) and independencetesting (Theorem 6) under randomized-response.,1.2. Our Contribution and Organization,[0],[0]
(The latter requires some manipulations and far less straightforward than the former.),1.2. Our Contribution and Organization,[0],[0]
"The sample complexity (under certain simplifying assumptions) of both problems is proportional to T 2.5.
",1.2. Our Contribution and Organization,[0],[0]
In Section 4 we move to the non-symmetric local-model.,1.2. Our Contribution and Organization,[0],[0]
"Again, we start with a general result showing that in this case too, finding an hypothesis that maximizes the likelihood of the observed signals is feasible when the hypothesis-set is convex.",1.2. Our Contribution and Organization,[0],[0]
We then focus on the mechanism of Bassily et al (2017) and show that it also makes the problem of finding a maximum-likelihood hypothesis strongly-convex.,1.2. Our Contribution and Organization,[0],[0]
"We then give a simple identity tester under this scheme whose sample complexity is proportional to T 2, and is thus more efficient than any tester under standard randomized-response.",1.2. Our Contribution and Organization,[0],[0]
"Similarly, we also give an independence-tester with a similar sample complexity.",1.2. Our Contribution and Organization,[0],[0]
"In Section 4.2 we empirically investigate alternative identitytesting and independence-testing based on Pearson’s χ2test in this non-symmetric scheme, and identify a couple of open problems in this regime.",1.2. Our Contribution and Organization,[0],[0]
"Several works have looked at the intersection of differential privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013a; Dwork et al., 2015) mostly focusing on robust statistics; but only a handful of works study rigorously the significance and power of hypotheses testing under differential privacy.",1.3. Related Work,[0],[0]
Vu and Slavkovic (2009) looked at the sample size for privately testing the bias of a coin.,1.3. Related Work,[0],[0]
"Johnson and Shmatikov (2013), Uhler et al (2013) and Yu et al (2014) focused on the Pearson χ2-test (the simplest goodness of fit test), showing that the noise added by differential privacy vanishes asymptotically as the number of datapoints goes to infinity, and propose a private χ2-based test which they study empirically.",1.3. Related Work,[0],[0]
"Wang et al (2015) and Gaboardi et al (2016) who have noticed the issues with both of these approaches, have revised the statistical tests themselves to incorporate also the added noise in the private computation.",1.3. Related Work,[0],[0]
"Cai et al (2017) give a private identity tester based on noisy χ2-test over large bins, Sheffet (2017) studies private Ordinary Least Squares using the JL transform, and Karwa and Vadhan (2018) give
matching upper- and lower-bounds on the confidence intervals for the mean of a population.",1.3. Related Work,[0],[0]
"All of these works however deal with the centralized-model of differential privacy.
",1.3. Related Work,[0],[0]
Perhaps the closest to our work are the works of Duchi et al (2013a; 2013b) who give matching upper- and lowerbound on robust estimators in the local model.,1.3. Related Work,[0],[0]
"And while their lower bounds do inform as to the sample complexity’s dependency on −2, they do not ascertain the sample complexity dependency on the size of the domain (T ) we get in Section 3.",1.3. Related Work,[0],[0]
"Moreover, these works disregard independence testing (and in fact (Duchi et al., 2013b) focus on mean estimation so they apply randomized-response to each feature independently generating a product-distribution even when the input isn’t sampled from a product-distribution).",1.3. Related Work,[0],[0]
"And so, to the best of our knowledge, no work has focused on hypothesis testing in the local model, let alone in the (relatively new) non-symmetric local model.",1.3. Related Work,[0],[0]
"Lastly, developed concurrently to our work, Gaboardi and Rogers (2018) study the asymptotic power of a variety chi-squared based hypothesis testing in the local model.",1.3. Related Work,[0],[0]
Notation.,"2. Preliminaries, Notation and Background",[0],[0]
"We user lower-case letters to denote scalars, bold characters to denote vectors and CAPITAL letters to denote matrices.","2. Preliminaries, Notation and Background",[0],[0]
"So 1 denotes the number, 1 denotes the all-1 vector, and 1X×X denotes the all-1 matrix over a domain X .","2. Preliminaries, Notation and Background",[0],[0]
"We use ex to denote the standard basis vector with a single 1 in coordinate corresponding to x. To denote the x-coordinate of a vector v we use v(x), and to denote the (x, x′)-coordinate of a matrix M we use M(x, x′).","2. Preliminaries, Notation and Background",[0],[0]
"For a given vector v , we use diag(v) to denote the matrix whose diagonal entries are the coordinates of v .","2. Preliminaries, Notation and Background",[0],[0]
"For any natural n, we use [n] to denote the set {1, 2, ..., n}.
Distances and norms.","2. Preliminaries, Notation and Background",[0],[0]
"Unless specified otherwise ‖v‖ refers to the L2-norm of v , whereas ‖v‖1 refers to the L1-
norm.","2. Preliminaries, Notation and Background",[0],[0]
"We also denote ‖v‖ 2 3
= (∑
i |vi| 2 3
) 3 2
.","2. Preliminaries, Notation and Background",[0],[0]
"For a matrix, ‖M‖1 denotes (as usual) the maximum absolute column sum.","2. Preliminaries, Notation and Background",[0],[0]
We identify a distribution p over a domain X as a T -dimensional vector with non-negative entries that sum to 1.,"2. Preliminaries, Notation and Background",[0],[0]
"This defines the total variation distance between two distributions: dTV(p,q) = 12‖p","2. Preliminaries, Notation and Background",[0],[0]
− q‖1.,"2. Preliminaries, Notation and Background",[0],[0]
"(On occasion, we will apply dTV to vectors that aren’t distributions, but rather nearby estimations; in those cases we use the same definition: the half of the L1-norm.)","2. Preliminaries, Notation and Background",[0],[0]
It is known that the TV-distance is a metric overs distributions.,"2. Preliminaries, Notation and Background",[0],[0]
"We also use the χ2-divergence to measure difference between two distributions: dχ2(p,q) = ∑ x (p(x)−q(x))2 p(x) =","2. Preliminaries, Notation and Background",[0],[0]
(∑ x (q(x))2 p(x) ),"2. Preliminaries, Notation and Background",[0],[0]
− 1.,"2. Preliminaries, Notation and Background",[0],[0]
"The χ2-divergence is not symmetric and can be infinite, however it is non-negative and zeros only when p = q .","2. Preliminaries, Notation and Background",[0],[0]
"We refer the reader to (Sason & Verdú, 2016) for more properties of the total-variance distance the χ2-divergence.
","2. Preliminaries, Notation and Background",[0],[0]
Differential Privacy.,"2. Preliminaries, Notation and Background",[0],[0]
"An algorithm A is called - differentially private, if for any two datasets D and D′ that differ only on the details of a single user and any set of outputsO, we have that Pr[A(D) ∈","2. Preliminaries, Notation and Background",[0],[0]
O] ≤ e Pr[A(D′) ∈,"2. Preliminaries, Notation and Background",[0],[0]
O].,"2. Preliminaries, Notation and Background",[0],[0]
"The unacquainted reader is referred to the Dwork-Roth monograph (Dwork & Roth, 2014) as an introduction to the rapidly-growing field of differential privacy.
","2. Preliminaries, Notation and Background",[0],[0]
Hypothesis testing.,"2. Preliminaries, Notation and Background",[0],[0]
The problem of hypothesis testing is to test whether a given set of samples was drawn from a distribution satisfying the null-hypothesis or the alternativehypothesis.,"2. Preliminaries, Notation and Background",[0],[0]
"Thus, the null-hypothesis is merely a set of possible distributions H0 and the alternative is disjoint set H1.","2. Preliminaries, Notation and Background",[0],[0]
Hypothesis tests boils down to estimating a teststatistic θ whose distribution has been estimated under the null-hypothesis.,"2. Preliminaries, Notation and Background",[0],[0]
"We can thus reject the null-hypothesis if the value of θ is highly unlikely, or accept the nullhypothesis otherwise.","2. Preliminaries, Notation and Background",[0],[0]
We call an algorithm a tester if the acceptance (in the completeness case) or rejection (in the soundness case) happen with probability ≥ 2/3.,"2. Preliminaries, Notation and Background",[0],[0]
Standard amplification techniques (return the median of independent tests) reduce the error probability from 1/3 to any β > 0,"2. Preliminaries, Notation and Background",[0],[0]
at the expense of increasing the sample complexity by a factor of O(log(1/β)); hence we focus on achieving a constant error probability.,"2. Preliminaries, Notation and Background",[0],[0]
"One of the most prevalent and basic tests is the identity-testing, where the null-hypothesis is composed of a single distribution H0 = {p} and our goal is to accept if the samples are drawn from p and reject if they were drawn from any other α-far (in dTV) distribution.","2. Preliminaries, Notation and Background",[0],[0]
"Another extremely common tester is for independence when X is composed of several features (i.e., X = X 1 ×X 2 × ...×X d) and the null-hypothesis is composed of all product distributions H0 = {p1 × ...","2. Preliminaries, Notation and Background",[0],[0]
"× pd} where each pj is a distribution on the jth feature X j .
Miscellaneous.","2. Preliminaries, Notation and Background",[0],[0]
"We use M 0 to denote that M is a positive semi-definite (PSD) matrix, and M N to denote that (M − N) 0.","2. Preliminaries, Notation and Background",[0],[0]
We use M† to denote M ’s pseudoinverse.,"2. Preliminaries, Notation and Background",[0],[0]
"We emphasize that we made no effort to minimize constants in our proofs, and only strived to obtain asymptotic bounds (O(·),Ω(·)).","2. Preliminaries, Notation and Background",[0],[0]
"Recall, in the symmetric signaling scheme, each user’s type is mapped through a random functionM into a set of signals S.",3. Symmetric Signaling Scheme,[0],[0]
"This mapping is index-oblivious — each user of type x ∈ X , sends the signal s with the same probability Pr[M(x) = s].",3. Symmetric Signaling Scheme,[0],[0]
"We denote the matrixG as the (|S|×|X |)- matrix whose entries are Pr[M(x) = s], and its sth-row by gs.",3. Symmetric Signaling Scheme,[0],[0]
Note that all entries of G are non negative and that for each x we have ‖Gex‖1 = 1.,3. Symmetric Signaling Scheme,[0],[0]
"By garbling each datum i.i.d, we observe the new dataset (y1, y2, ...,",3. Symmetric Signaling Scheme,[0],[0]
"yn) ∈ Sn.
Theorem 1.",3. Symmetric Signaling Scheme,[0],[0]
"For any convex setH of hypotheses, the problem of finding the max-likelihood p ∈ H generating the observed signals (y1, .., yn) is poly-time solvable.
",3. Symmetric Signaling Scheme,[0],[0]
Proof.,3. Symmetric Signaling Scheme,[0],[0]
"Since G(s, x) describes the probability that a user of type x sends the signal s, any distribution p ∈ H over the types in X yields a distribution on S where Pr[user sends s] = ∑ x∈X G(s, x) · p(x) = gTsp.",3. Symmetric Signaling Scheme,[0],[0]
"Therefore, given signals (y1, ..., yn) summarized as a signalshistogram 〈ns〉s∈S , the likelihood of these signals is given by: L(p; y1, ..., yn) = ∏",3. Symmetric Signaling Scheme,[0],[0]
"i g T yip = ∏ s∈S(g T s p) ns =
exp (∑ s ns log(g T sp) ) .",3. Symmetric Signaling Scheme,[0],[0]
"Thus, the gradient of the negative
log-loss function is∇f = − 1n ∑ s∈S ns gTsp ·gs, and its Hes-
sian is given by the matrix 1n ∑ s∈S ns (gTsp) 2gsg T s .",3. Symmetric Signaling Scheme,[0],[0]
"Clearly, as a non-negative sum of rank-1 matrices, the Hessian is a PSD matrix.so our loss-function is convex.",3. Symmetric Signaling Scheme,[0],[0]
"Known polytime algorithms for minimizing a convex function over a convex set (e.g. (Zinkevich, 2003)) conclude the proof.
",3. Symmetric Signaling Scheme,[0],[0]
"Unfortunately, in general the solution to this problem has no closed form (to the best of our knowledge).",3. Symmetric Signaling Scheme,[0],[0]
"However, we can find a close-form solution under the assumption that G isn’t just any linear transformation but rather one that induces probability distribution over S, the assumption that |S| ≤ |X | (in all applications we are aware of use fewer signals than user-types) and one extra-condition.
",3. Symmetric Signaling Scheme,[0],[0]
Corollary 2.,3. Symmetric Signaling Scheme,[0],[0]
Let q∗ be the |S|-dimensional vector given by 〈nsn 〉.,3. Symmetric Signaling Scheme,[0],[0]
"Given that |S| ≤ |X |, that G is a full-rank matrix satisfying ‖G‖1 = 1 and assuming that ( G†q∗+ker(G) ) ∩",3. Symmetric Signaling Scheme,[0],[0]
H 6=,3. Symmetric Signaling Scheme,[0],[0]
"∅, then any vector inH of the form p∗+u where p∗ = G†q∗",3. Symmetric Signaling Scheme,[0],[0]
"and u ∈ ker(G) is an hypothesis that maximizes the likelihood of the given signals (y1, ..., yn).",3. Symmetric Signaling Scheme,[0],[0]
"Proof deferred to the supplementary material, Section B.",3. Symmetric Signaling Scheme,[0],[0]
"We now aim to check the affect of a particular G, the one given by the randomized-response mechanism.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In this case S = X and we denote G as the matrix whose entries are
G(x, x′) = {",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"ρ+ γ , if x′ = x ρ , otherwise where ρ def= 1T−1+e
and γ def= e −1
T−1+e .",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
We get that G = ρ · 1X×X + γI (where 1X×X is the all-1 matrix).,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In particular, all vectors gs = gx, which correspond to the rows of G, are of the form: gx = ρ1 + γex.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
It follows that for any probability distribution p ∈ H we have that Pr[seeing signal x] = gTxp = ρ+ γp(x).,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"We have therefore translated any p ∈ H (over X ) to an hypothesis q over S (which in this case S = X ), using the affine transformation ϕ(p) = ρ1 +γp = TρuX +γp when uX denotes the uniform distribution over X .",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"(Indeed, γ = 1 − Tρ, an identity we will often apply.)",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"At the risk of overburdening notation, we use ϕ to denote the same transformation over scalars, vectors and even sets (applying ϕ to each vector in the set).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Since ϕ is injective, we have therefore discovered the following theorem.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Theorem 3.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Under the classic randomized response mechanism, testing for any hypothesis H0 (or for comparing H0 against the alternative H1) of the original distribution,
translates into testing for hypothesis ϕ(H0) (or ϕ(H0) against ϕ(H1)) for generating the signals y1, ..., yn.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Theorem 3 seems very natural and simple, and yet (to the best of our knowledge)",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"it was never put to words.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Moreover, it is simple to see that under standardrandomized response, our log-loss function is in fact strongly-convex, and therefore finding p∗ becomes drastically more efficient (see, for example (Hazan et al., 2006)).
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Claim 4.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Given signals y1, ..., yn generated using standard randomized response with parameter < 1, we have that our log-loss function is Θ( 2 · minx{nx}n )-strongly convex.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Note that in expectation nx ≥ ρn, hence with overwhelming probability we have minx nx ≥ n/(2T ).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"The proof is fairly straight-forward and is deferred to the supplementary material, Section B.
A variety of corollaries follow from Theorem 3.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In particular, a variety of detailing matching sample complexity upper- and lower-bounds translate automatically into the realm of making such hypothesis-tests over the outcomes of the randomized-response mechanism.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"We focus here on two of the most prevalent tests: identity testing and independence testing.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Identity Testing.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Perhaps the simplest of the all hypothesis testing is to test whether a given sample was generated according to a given distribution or not.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Namely, the null hypothesis is a single hypothesis H0 = {p}, and the alternative is H1 = {q : dTV(p,q) ≥ α} for a given parameter α.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"The seminal work of Valiant and Valiant (2014) discerns that (roughly) Θ(‖p‖ 2
3 /α2)",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"samples are sufficient
and are necessary for correctly rejecting or accepting the null-hypothesis w.p.≥ 2/3.3
Here, the problem of identity testing under standard randomized response reduces to the problem of hypothesis testing between ϕ(H0) = {ρ1 + γp : p ∈",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
H0} and ϕ(H1),3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"= {ϕ(q) : q satisfying dTV(p,q) ≥ α}.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Corollary 5.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In order to do identity testing under standard randomized response with confidence and power ≥ 2/3, it is necessary and sufficient that we get Θ( T 2.5
2α2 ) samples.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"The proof uses the results of (Valiant & Valiant, 2014) as a black-box and is mainly composed of calculations, so it is deferred to supplementary material, Section B.
Independence Testing.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Another prevalent hypothesis testing over a domain X where each type is composed of multiple feature is independence testing.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Denoting X = X 1 × X 2 × ... × X d as a domain with d possible features (hence T = |X | = ∏ j |X j | def = ∏ j T
j), our goal is to discern whether an observed sample is drawn from a product distribution or a distribution α-far from any product distri-
3For the sake of brevity, we ignore pathological examples where by removing α probability mass from p we obtain a vector of significantly smaller 2
3 -norm.
bution.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In particular, the null-hypothesis in this case is a complex one:",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"H0 = {p̄ = p1×p2× ...×pd} and the alternative is H1 = {q : minp̄∈H0 dTV(q, p̄) ≥ α}.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"To the best of our knowledge, the (current) tester with smallest sample complexity is of Acharya et al (2015), which requires Ω ( ( √ T + ∑ j T j)/α2 ) iid samples.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
We now consider the problem of testing for independence under standard randomized response.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Our goal is to prove the following theorem.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Theorem 6.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"There exists an algorithm that takes n =
Ω̃( T 2
α2 2
( d2(max
j {T j})2",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"+
√ T ) )",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"signals generated by
applying standard randomized response (with < 1) on n samples drawn from a distribution p and with probability ≥ 2/3 accepts if p ∈ H0, or rejects if p ∈ H1.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Moreover, no algorithm can achieve such guarantee using n = o(T 5/2/(α2 2)) signals.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Note there are at least two types per feature, so d ≤ log2(T ).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Should all T
js be equal we have (T j)2 ≤ T 2d , making T 2.5/(α2 2) the leading term in the above bound.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Proof.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Theorem 3 implies we are comparing ϕ(H0) = {ρ1X+γ(p1×...×pd)} to ϕ(H1) = {ρ1X+γq : q ∈ H1}.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Note that ϕ(H0) is not a subset of product-distributions over X but rather a convex combination (with publicly known weights) of the uniform distribution and H0; so we cannot run the independence tester of Acharya et al on the signals as a black-box.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Luckily it holds that ϕ(H1) is far from all distributions in ϕ(H0): for each q ∈ H1 and p̄ ∈,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"H0 we have dTV(ϕ(q), ϕ(p̄))",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"≥ γdTV(q, p̄) ≥ γα.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"And so we leverage on the main result of Acharya et al ((2015), Theorem 2): we first find a distribution ρ1 + γz̄ ∈ ϕ(H0) such that if the signals were generated by some ρ1X + γp̄ ∈ ϕ(H0) then dχ2(ϕ(z̄), ϕ(p̄))",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"≤ γ2α2/500, and then test if indeed the signals are likely to be generated by a distribution close to ϕ(z̄) using Acharya et al’s algorithm.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"We now give our procedure for finding the product-distribution z̄ .
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Per feature j, given the jth feature of the signals yj1, ..., y j n where each xj ∈ X j appears nxj times, our procedure for finding zj is as follows.
0.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
(Preprocessing:) Denote τ = α/(10d · T j).,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"We call any type xj where nxjn ≤ 1−γ T j + γτ as small and
otherwise we say type xj is large.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Ignore all small types, and learn zj only over large types.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"(For brevity, we refer to n as the number of signals on large types and T j as the number of large types.)
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"1. Set the distribution z̃j as the “add-1” estimator of Kamath et al (2015) for the signals: z̃j(xj) = 1+nxjT j+n .
2.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Compute zj = 1γ ( I − 1−γT j 1X j ) z̃j .
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Once zj is found for each feature j, set z̄ = z1 × ...",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"× zd run the test of Acharya et al (2015) (Theorem 2) with ϕ(z̄) looking only at the large types from each feature, setting
the distance parameter to αγ2 and confidence 1 9 , to decide whether to accept or reject.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"In order to successfully apply the Acharya et al’s test, a few conditions need to hold.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"First, the provided distribution ϕ(z̄) should be close to ϕ(H0).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"This however hold trivially, as z̄ is a product-distribution.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Secondly, we need that ϕ(z̄) and ϕ(p̄) to be close in χ2-divergence, as we argue next.
",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Lemma 7.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Suppose that n, the number of signals, is at least Ω( d 2
α2γ2 maxj{T j}).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Then the above procedure cre-
ates distributions zj such that the product distribution z̄ = z1×z2× ...×zd satisfies the following property.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"If the signals y1, ..., yn were generated by ϕ(p̄) for some productdistribution p̄ = p1 × ...",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"× pd, then w.p. ≥ 8/9 we have that dχ2(ϕ(z̄), ϕ(p̄)) ≤ γ2α2/1000.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
We table the proof of Lemma 7 to Section B in the supplementary material.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Next, either completeness or soundness must happen: either the signals were taken from randomized-response on a product distribution, or they were generated by a distribution γα/2-far from ϕ(H0).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"If no type of any feature was deemed as “small”, this condition clearly holds; but we need to argue this continues to hold even when we run our tester on a strict subset of X composed only of large types in each feature.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Completeness is straight-forward: since we remove types feature by feature, the types now come from a product distribution p̄large = p 1 large × ...",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
× pdlarge where each pjlarge is a restriction of p j to the large types of feature j. Soundness however is more intricate.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"We partition X into two subsets: AllLarge = {(x1, x2, ..., xd) ∈ X : ∀j, xj is large} and Rest = X \AllLarge; and break q into q = ηqRest + (1− η)qAllLarge, with η = Prq [Rest].",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Claim 8 (proof deferred to the supplementary material) argues that η < α2 .,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Therefore, dTV(q, qAllLarge) ≤ α 2 , implying that dTV(ϕ(qAllLarge), ϕ(H0))",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
> α · γ− αγ2 = αγ 2 .,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Claim 8.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Assume the underlying distribution of the samples is q and that the number of signals is at least n = Ω( d2(maxj T j)2
α2γ2 log(dmaxj T j)).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Then w.p. ≥ 8/9 our
preprocessing step marks certain types each feature as “small” such that the probability (under q) of sampling a type (x1, x2, ..., xd) such that ∃j, xj is small is ≤ α/2.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"So, given that both Lemma 7 and Claim 8 hold, we can use the test of Acharya et al, which requires a sample of size n = Ω( √ T/(αγ)2).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
Recall that < 1,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"so γ = Θ( /T ), and we get that the sample size required for the last test is n = Ω( T 2.5
α2 2 ).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Moreover, for this last part, the lower bound in Acharya et al (2015) still holds (for the same reason it holds in the identity-testing case): the lower bound is derived from the counter example of testing whether the signals were generated from the uniform distribution (which clearly lies in ϕ(H0)) or any distribution from a collection of perturbations which all belong to ϕ(H1) (See (Paninski, 2008) for more details).",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Each of distribution is thus γα-far from ϕ(H0) and so any tester for this particular construc-
tion requires √ T/(αγ)2-many samples.",3.1. Hypothesis Testing under Randomized-Response,[0],[0]
This proves both the upper- and lower-bounds of Theorem 6.,3.1. Hypothesis Testing under Randomized-Response,[0],[0]
"Let us recall the non-symmetric signaling schemes in (Bassily & Smith, 2015; Bassily et al., 2017).",4. Non-Symmetric Signaling Schemes,[0],[0]
"Each user, with true type x ∈ X , is assigned her own mapping (the mapping is broadcast and publicly known) fi : X → S .",4. Non-Symmetric Signaling Schemes,[0],[0]
"This sets her inherent signal to fi(x), and then she runs standard (symmetric) randomized response on the signals, making the probability of sending her true signal fi(x) to be e -times greater than any other signal s 6= fi(x).
",4. Non-Symmetric Signaling Schemes,[0],[0]
"In fact, let us allow an even broader look.",4. Non-Symmetric Signaling Schemes,[0],[0]
"Each user is given a mapping fi : X → S, and denoting (like before) T = |X | and S = |S|, we identify this mapping with a (S × T )-matrix Gi.",4. Non-Symmetric Signaling Schemes,[0],[0]
The column gxi = Giex is the probability distribution that a user of type x is going to use to pick which signal she broadcasts.,4. Non-Symmetric Signaling Schemes,[0],[0]
(And so the guarantee of differential privacy is that for any signal s ∈ S and any two types x 6=,4. Non-Symmetric Signaling Schemes,[0],[0]
x′ we have that gxi (s) ≤,4. Non-Symmetric Signaling Schemes,[0],[0]
"e gx ′
i (s).)",4. Non-Symmetric Signaling Schemes,[0],[0]
"Therefore, all entries in Gi are non-negative and ‖Gi‖1",4. Non-Symmetric Signaling Schemes,[0],[0]
"= 1 for all is.
",4. Non-Symmetric Signaling Schemes,[0],[0]
"Similarly to the symmetric case, we first exhibit the feasibility of finding a maximum-likelihood hypothesis given the signals from the non-symmetric scheme.",4. Non-Symmetric Signaling Schemes,[0],[0]
"Since we view which signal in S was sent, our likelihood mainly depends on the row vectors gsi .",4. Non-Symmetric Signaling Schemes,[0],[0]
"We prove the following theorem, proof deferred to Section C in the supplementary material.
",4. Non-Symmetric Signaling Schemes,[0],[0]
Theorem 9.,4. Non-Symmetric Signaling Schemes,[0],[0]
"For any convex set H , the problem of finding the max-likelihood p ∈ H generating the observed nonsymmetric signals (y1, .., yn) is poly-time solvable.",4. Non-Symmetric Signaling Schemes,[0],[0]
Let us recap the differentially private scheme of Bassily et al (2017).,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"It this scheme, the mechanism uses solely two signals S = {1,−1} (so S = 2).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"For every i the mechanism sets Gi by picking u.a.r for each x ∈ X which of the two signals in S is more likely; the chosen signal gets a probability mass of e
1+e and the other get probability mass of 11+e .",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"We denote η as the constant such that 1 2 + η = e
1+e and 1 2 − η = 1 1+e ; namely η = e −1 2(e +1) =
Θ( )",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
when < 1.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Thus, for every s ∈ {1,−1} the row vector gsi is chosen such that each coordinate is chosen iid and uniformly from { 12 + η, 1 2 − η}.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"(Obviously, there’s dependence between g1i and g −1",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"i , as g 1 i + g −1",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"i = 1, but the distribution of g1i is identical to the one of g −1",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"i .)
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"First we argue that for any distribution p, if n is sufficiently large then w.h.p over the generation of theGis and over the signals we view from each user, then finding p̂ which maximizes the likelihood of the observed signals yields a good approximation to p. To that end, it suffices to argue that the function we optimize is Lipfshitz and strongly-convex.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Lemma 10.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Fix δ > 0 and assume that the number of signals we observe is n = Ω(T 3 log(1/δ)).,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Then w.p.≥ 1− δ,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"it holds that the function f(p) we optimize (as given in
Equation (1)) is ( 3 √ T ) -Lipfshitz",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"and ( η2
2
) -strongly con-
vex over the subspace {x : xT1 = 0} (all vectors orthogonal to the all-1 vector).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"The proof of Lemma 10 — which (in part) is hairy due to the dependency between the matrix Gi and the signal yi — is deferred to Section C in the supplementary material.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Identity Testing.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Designing an Identity Test based solely on the maximum-likelihood is feasible, due to results like Cesa-Binachi et al (2002) which allow us to compare between the risk of the result p̃ of a online gradient descent algorithm to the original distribution p which generated the signals.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Through some manipulations one can (eventually) infer that |f(p),4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− f(p̃)| = O(1/ √ n).,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"However, since strong-convexity refers to the L2-norm squared of ‖p − p̃‖, we derive the resulting bound is ‖p − p̃‖21 ≤",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"T‖p − p̃‖22 = O( 1η2√n ), which leads to a sample complexity bound proportional to T 3/(αη)4.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"This bound is worse than the bounds in Section 3.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"We therefore design a different, simple, identity tester in the local non-symmetric scheme, based on the estimator given in (Bassily et al., 2017).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"The tester itself — which takes as input a given distribution p, a distance parameter α > 0 and the n signals — is quite simple.
1.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Given the n matrices G1, ..., Gn and the n observed signals y1, ..., yn, compute the estimator θ = 1n ∑",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"i 1 η ( gyii − 121 ) .
2.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"If dTV( 12ηθ,p) ≤",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"α 2 then accept, else reject.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Theorem 11.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Assume < 1.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"If we observe n = Ω( ( T α )2 ) signals generated by a distribution q then w.p. ≥ 2/3 over the matrices Gi we generate and the signals we observe, it holds that dTV( 12ηθ,q) ≤ α/2.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"The correctness of the tester now follows from checking for the two cases where either p = q or dTV(p,q) > α.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Proof.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
In the first part of the proof we assume the types of the n users were already drawn and are now fixed.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
We denote xi as the type of user i.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"We denote the frequency vector f = 〈nxn 〉x∈X , generated by counting the number of users of type x and normalizing it by n.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Given f , we examine the estimator θ.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"For each user i we have that 1η (g yi i − 121) ∈ {−1, 1}
T .",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Because xi, the type of user i, is fixed, then for each coordinate x′ 6= xi, the signal yi is independent of the x′-column in Gi (yi depends solely on the entries in the xi-column).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"We thus have that gyii (x
′) is distributed uniformly among { 12 ± η} and so E[ 1 η (g yi i (x
′)",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− 12 )] = 0.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"In contrast, Pr[ 1η (g yi i (xi) − 12 )",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"= 1 ]
= ∑ s∈{−1,1} Pr[ 1 η (g s i (xi) − 12 ) = 1 and yi = s] = 2 · 12 · ( 1 2 + η) = 1 2 +η.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Therefore, E[ 1 η (g yi i (xi)− 12 )] =
( 12 + η)− ( 1 2 − η) = 2η.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
It follows that E[ 1 η (g yi i − 121)],4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"= 2ηexi and so E[θ] = 2ηf .
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Next we examine the variance of θ , and argue the following (proof deferred to supplementary material).
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Proposition 12.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
E[(θ,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− 2ηf ),4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
(θ − 2ηf )T] 1nI,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"So as a result, the expectedL2-difference E[‖θ − 2ηf ‖2] = E[trace((θ − 2ηf )(θ − 2ηf )T)]",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
= trace(E[(θ − 2ηf )(θ − 2ηf )T]) ≤,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Tn .,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Chesbyshev’s inequality assures us that therefore Pr[ 12η‖θ − 2ηf ‖ >,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"√ 6T 2η √ n ] ≤ T/n6T/n = 1 6 .
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"So far we have assumed f is fixed, and only looked at the event that the coin-tosses of the mechanism yielded an estimator far from its expected value.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
We now turn to bounding the distance between f and its expected value q (the distribution that generated the types).,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Indeed, it is clear to see that the expected value of f = 1n ∑ i exi is E[f ]",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
= q .,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Moreover, it isn’t hard (and has been computed before many times, e.g. Agresti (2003)) to see that E[(f − q)(f",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− q)T] = 1n ( diag(q)− qqT ) .,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Thus
E[‖f −q‖2] = trace( 1n ( diag(q)− qqT ) )",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"= 1n (1−‖q‖
2).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Therefore, applying Chebyshev again, we get that w.p. at most 1/6 over the choice of types by q , we have that Pr[‖f − q‖ > √ 6/n]",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
≤ 1/n6/n,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"= 1 6 .
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Combining both results we get that w.p. ≥ 2/3 we have that ‖ 12ηθ,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− q‖1 ≤,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
√ T‖ 12ηθ,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"− q‖ ≤√
T ( ‖ 12ηθ",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− f ‖+ ‖f,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
− q‖ ) ≤,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"√ 6T 2 4η2n+ √ 6T n ≤ α since
we have n = Ω( T 2
η2α2 ).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Recall that η = Θ( ) and that dTV(x,y) = 1 2‖x−y‖1, and the bound of α 2 is proven.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Independence Testing.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Similarly to the identity tester, we propose a similar tester for independence.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Recall that in this case, X is composed of d features, hence X = X 1 × X 2 × ...",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"× X d, with our notation of T j = |X j",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
|,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"for each j. Our tester should accept when the underlying distribution over the types is some product distribution p, and should reject when the underlying distribution over the types is α-far from any product distribution.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"The tester, whose input is the n signals and a distance parameter α > 0, is as follows.
1.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Given the n matrices G1, ..., Gn and the n observed signals y1, ..., yn, compute the estimator θ = 1n ∑",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"i 1 η ( gyii − 121 ) .
2.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"For each feature j compute θj — the jth marginal of 1 2ηθ (namely, for each x
j ∈ X j sum all types whose jth feature is xj).",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Denote θ̄ = θ1 × ...×,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"θd.
3.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"If dTV( 12ηθ, θ̄) ≤",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"α 2 then accept, else reject.
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Theorem 13.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Assume < 1.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Given n = Ω( Tα2 2 ( T + d2 ∑ j T j )
)",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"iid drawn signals from the nonsymmetric locally-private mechanism under a dataset whose types were drawn iid from some distribution q , then
w.p. ≥ 2/3 over the matrices Gi we generate and the types in the dataset we have the following guarantee.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"If q is a product distribution, then dTV( 12ηθ, θ̄) ≤",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"α 2 , and if q is αfar from any product distribution then dTV( 12ηθ, θ̄) >",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
α 2 .,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"(Proof deferred to the supplementary material, Section C.)
",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
Open Problems.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
(1) Is there a tester with a better sample complexity?,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
The experiment in Section 4.2 leads us to conjecture that there exists a tester with sample complexity of T 1.5/(ηα)2.,4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"There could exist better testers, of smaller sample complexity, which leads to the second question.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"(2) Can one derive lower bounds for identity/independence testing in this model, where each sample has its own distribution, related to the original distribution over types?",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"In Section D in the supplementary material we give more details as to possible venues to tackle both problems, relating them to the problem of learning a mixture-model of product distributions.",4.1. Hypothesis Testing under Non-Symmetric Locally-Private Mechanisms,[0],[0]
"Following the derivations in the proof of Theorem 11, we can see that Var(θ) =",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
1n ( I − 4η2diag(f 2) ) .,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"As ever, we assume is a small constant and as a result the variance in 2ηf (which is approximately 4η 2
n diag(p)) is significantly smaller than the variance of θ.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
This allows us to use the handwavey approximation f,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"≈ p,",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
and argue that we have the approximation Var(θ),4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
≈ 1 n,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
( I − 4η2diag(p2) ),4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
def = 1nM .,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Central Limit Theorem thus give that √ nM−1/2(θ − 2ηp) n→∞→ N (0, I).",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Therefore, it stands to reason that the norm of the LHS is distributed like a χ2-distribution, namely,
P (θ) def = n ∑",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
x∈X,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"(θ(x)− 2η · p(x))2 1− 4η2p(x)2 n→∞→ χ2T
Our experiment is aimed at determining whether P (θ) can serve as a test statistic and assessing its sample complexity.
",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Setting and Default Values.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We set a true ground distribution on T possible types, p.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
We then pick a distribution q which is α-far from p using the counter example of Paninski (2008): we pair the types and randomly move 2αT probability mess between each pair of matched types.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We then generate n samples according to q , and apply the nonsymmetric -differentially private mechanism of (Bassily et al., 2017).",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Finally, we aggregate the suitable vectors to obtain our estimator θ and compute P (θ).",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"If we decide to accept/reject we do so based on comparison of P to the 23 -quantile of the χ 2 T -distribution, so that in the limit we reject only w.p. 1/3 under the null-hypothesis.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
We repeat this entire process t times.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We have set the default values T = 10, p = uT (uniform on [T ]), α = 0.2, n = 1000, = 0.25 and therefore η = 12 e −1 e +1 , and t = 10000.
",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Experiment 1: Convergence to the χ2-distribution in the null case.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"First we ask ourself whether our approximation, denoting P (θ) ≈ χ2T is correct when indeed p is
the distribution generating the signals.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"To that end, we set α = 0",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"(so the types are distributed according to p) and plot the t empirical values of P we in our experiment, varying both the sample size n ∈ {10, 100, 1000, 10000} and the domain size T ∈ {10, 25, 50, 100}.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
The results are consistent — P is distributed like a χ2T - distribution.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Indeed, the mean of the t sample points is≈ T (the mean of a χ2T -distribution).",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"The results themselves appear in Figure 2 in the supplementary material, Section D.
Experiment 2:",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Divergence from the χ2-distribution in the alternate case.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Secondly, we asked whether P can serve as a good way to differentiate between the null hypothesis (the distribution over the types is derived from p) and the alternative hypothesis (the distribution over the types if ≥ α-far from p).",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
We therefore ran our experiment while varying α (between 0.25 and 0.05) and increasing n.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Again, the results show that the distribution does shift towards higher values as n increases.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"The results are given in
Figure 3 in the supplementary material, Section D.
Experiment 3: Sample Complexity.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Next, we set to find the required sample complexity for rejection.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We fix the α-far distribution from p, and first do binary search to hone on an interval [nL, nU ] where the empirical rejection probability is between 30% − 35%; then we equipartition this interval and return the n for which the empirical rejection probability is the closest to 33%.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We repeat this experiment multiple times, each time varying just one of the 3 most important parameters, T , α and .",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We maintain two parameters at default values, and vary just one parameter: T ∈ {5, 10, 15, .., 100}, α ∈ {0.05, 0.1, 0.15, ..., 0.5}, ∈ {0.05, 0.1, 0.15, ..., 0.5}.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"The results are shown in Figure 1, where next to each curve we plot the curve of our conjecture in a dotted line.4 We conjecture initially that n ∝ T cT · αcα · c .",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"And so, for any parameter ξ ∈ {T, α, }, if we compare two experiments i, j that differ only on the value of this parameter and resulted in two empirical estimations Ni, Nj of the sample complexity, then we get that cξ ≈ log(Ni/Nj)log(ξi/ξj) .",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"And so for any ξ ∈ {T, α, } we take the median over of all pairs of i and j and we get the empirical estimations of c = −1.900793, cα = −1.930947 and cT",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
= 1.486957.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"This leads us to the conjecture that the actual sample complexity according to this test is T 1.5
α2 2 .
",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Open Problem.,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Perhaps even more interesting, is the experiment we wish we could have run: a χ2-based independence testing.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Assuming the distribution of the type is a product distribution p̄ = p1,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
× ...,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"× pd, the proof of Theorem 13 shows that for each feature j we have Var(θj − pj)",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
≈ 14η2n T T j IX j .,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
Thus 4η 2nT j T ‖θ,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
j,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
− pj‖2 n→∞→ χ2T j .,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"However, the d estimators θj are not independent, so it is
4We plot the dependency on α and on the same plot, as both took the same empirical values.
not true that ∑ j 4η 2nT j T ‖θ",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
j,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
− pj‖2 n→∞→ χ2∑ j T j .,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Moreover, even if the estimators of the marginals were independent,5 we are still unable to determine the asymptotic distribution of ‖θ̄−p̄‖2 (only a bound, scaled byO(maxj Tj), using Proposition 17 in the supplementary material), let alone the asymptotic distribution of ‖ 12ηθ − θ̄‖ 2.
",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"Nonetheless, we did empirically measure the quantity
Q(θ) def = n ∑",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
x,4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
( 1 2η θ(x)−θ̄(x)),4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"2 θ̄(x) under the null (α = 0) and the alternative (α = 0.25) hypothesis with n = 25, 000 samples in each experiment.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"The results (given in Figure 4 in the supplementary material) show that the distribution of Q — albeit not resembling a χ2-distribution — is different under the null- and the alternative-hypothesis, so we suspect that there’s merit to using this quantity as a tester.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"We thus leave the design of a χ2-based statistics for independence in this model as an open problem.
5E.g.",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"by assigning each example i to one of the d estimators, costing only d = log(T ) factor in sample complexity",4.2. Experiment: Proposed χ2-Based Testers,[0],[0]
"This work was supported by the Natural Sciences and Engineering Council of Canada, Grant #2017-06701.",Acknowledgments,[0],[0]
The author is also an unpaid collaborator on NSF grant 1565387.,Acknowledgments,[0],[0]
"The authors thanks the anonymous reviewers for many helpful suggestions and ideas, as well as Marco Gaboardi and Ryan Rogers for helpful discussions illustrating the similarities and differences between our two papers.",Acknowledgments,[0],[0]
"We initiate the study of differentially private hypothesis testing in the local-model, under both the standard (symmetric) randomized-response mechanism (Warner, 1965; Kasiviswanathan et al., 2008) and the newer (non-symmetric) mechanisms (Bassily & Smith, 2015; Bassily et al., 2017).",abstractText,[0],[0]
"First, we study the general framework of mapping each user’s type into a signal and show that the problem of finding the maximum-likelihood distribution over the signals is feasible.",abstractText,[0],[0]
"Then we discuss the randomizedresponse mechanism and show that, in essence, it maps the nulland alternative-hypotheses onto new sets, an affine translation of the original sets.",abstractText,[0],[0]
We then give sample complexity bounds for identity and independence testing under randomizedresponse.,abstractText,[0],[0]
We then move to the newer nonsymmetric mechanisms and show that there too the problem of finding the maximum-likelihood distribution is feasible.,abstractText,[0],[0]
"Under the mechanism of Bassily et al (2017) we give identity and independence testers with better sample complexity than the testers in the symmetric case, and we also propose a χ-based identity tester which we investigate empirically.",abstractText,[0],[0]
Locally Private Hypothesis Testing,title,[0],[0]
Can we efficiently predict which face is in the picture amongst multiple billions of people?,1. Introduction,[0],[0]
"In a translation, can we effectively predict which word should come next amongst 105 possibilities?",1. Introduction,[0],[0]
More generally can we predict one of K classes in polylogarithmic time in K?,1. Introduction,[0],[0]
"This question gives rise to the area of extreme multiclass classification (Bengio et al., 2010; Beygelzimer et al., 2009; Bhatia et al., 2015; Choromanska & Langford, 2015; Morin & Bengio, 2005; Prabhu & Varma, 2014; Weston et al., 2013), in which K is very large.",1. Introduction,[0],[0]
"If efficiency is not a concern, the most common and generally effective representation for multiclass prediction is a one-against-all (OAA) structure.",1. Introduction,[0],[0]
"Here, inference consists of computing a score for each class and returning the class with the maximum score.",1. Introduction,[0],[0]
"If efficiency is a concern, an attractive strategy for picking one of K items is to use a tree; unfortunately, this often comes at the cost of increased error.
",1. Introduction,[0],[0]
"A general replacement for the one-against-all approach must satisfy a difficult set of desiderata.
",1. Introduction,[0],[0]
"• High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong base-
*Equal contribution 1University of Maryland 2Microsoft.",1. Introduction,[0],[0]
"Correspondence to: Paul Mineiro <pmineiro@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
line (Rifkin & Klautau, 2004) which is the standard “output layer” of many learning systems such as winners of the ImageNet contest (He et al., 2015; Simonyan & Zisserman, 2014).",1. Introduction,[0],[0]
"• High speed at training time and test time: A multiclass classifier must spend at least Ω(logK) time (Choromanska & Langford, 2015))",1. Introduction,[0],[0]
so this is a natural benchmark to optimize against.,1. Introduction,[0],[0]
• Online operation: Many learning algorithms use either online updates or mini-batch updates.,1. Introduction,[0],[0]
Approaches satisfying this constraint can be easily composed into an end-to-end learning system for solving complex problems like image recognition.,1. Introduction,[0],[0]
"For algorithms which operate in batch fashion, online components can be easily used.",1. Introduction,[0],[0]
"• Linear space: In order to have a drop-in replacement for OAA, an approach must not take much more space than OAA.",1. Introduction,[0],[0]
"Memory is at a premium when K is very large, especially for models trained on GPUs, or deployed to small devices.
",1. Introduction,[0],[0]
"We use an OAA-like structure to make a final prediction, but instead of scoring every class, we only score a small subset of O(logK) classes.",1. Introduction,[0],[0]
We call this “one-againstsome” (OAS).,1. Introduction,[0],[0]
How can you efficiently determine what classes should be scored?,1. Introduction,[0],[0]
We use a dynamically built tree to efficiently whittle down the set of candidate classes.,1. Introduction,[0],[0]
The goal of the tree is to maximize the recall of the candidate set so we call this approach “The Recall Tree.”,1. Introduction,[0],[0]
"In a traditional tree-based classifier, a traversal of the tree leads to a leaf, and a leaf corresponds to a single label, In the Recall Tree, we loosen the latter requirement and allow a leaf to corresponds to a set of labels of size O(logK).",1. Introduction,[0],[0]
"At test time, when a leaf is reached, scores are computed for this small subset (see Figure 1).
",1. Introduction,[0],[0]
"The Recall Tree achieves good accuracy, improving on previous online approaches (Choromanska & Langford, 2015) and sometimes surpassing the OAA baseline.",1. Introduction,[0],[0]
The algorithm requires only poly(logK) time during training and testing.,1. Introduction,[0],[0]
"In practice, the computational benefits are substantial when K ≥ 1000.1",1. Introduction,[0],[0]
"The Recall Tree constructs a tree and learns parameters in a fully online manner as a reduction, allowing composition with systems trained via online updates.",1. Introduction,[0],[0]
"All of this requires only twice as much space as OAA approaches.
",1. Introduction,[0],[0]
"Our contributions are the following:
• We propose a new online tree construction algorithm which jointly optimizes the construction of the tree, the routers and the underlying OAS regressors (see section 3.1).",1. Introduction,[0],[0]
"• We analyze elements of the algorithm, including a new boosting bound (see section 3.3) on multiclass classification performance and a representational trick which allows the algorithm to perform well if either a tree representation does well or an OAA representation does well as discussed in section 3.2.",1. Introduction,[0],[0]
"• We experiment with the new algorithm, both to analyze its performance relative to baselines and understand the impact of design decisions via ablation experiments.
",1. Introduction,[0],[0]
The net effect is a theoretically motivated algorithm which empirically performs well providing a plausible replacement for the standard one-against-all approach for largeK.,1. Introduction,[0],[0]
"Here we present a concrete description of the Recall Tree and defer all theoretical results that motivate our design de-
1Our implementation of baseline approaches, including OAA, involve vectorized computations that increase throughput by a factor of 10 to 20, making them much more difficult to outpace than naı̈ve implementations.
",2. The Recall Tree Algorithm,[0],[0]
"Algorithm 1 Predict. n.f(x) evaluates the node’s route, scorey(x) evaluates a per-class regressor, r̂ecall(◦) is an empirical bound on the recall of a node (◦)",2. The Recall Tree Algorithm,[0],[0]
"(see Eq (1)), and x+ {(n.id : 1)} indicates the addition of a sparse feature with index n.id and value 1.
1: Input: Example x, Root Node n 2: Output: Predicted class ŷ 3: while n.leaf is false do 4: c← n.f(x) > 0 ?",2. The Recall Tree Algorithm,[0],[0]
n.left :,2. The Recall Tree Algorithm,[0],[0]
n.right 5: if r̂ecall(n) > r̂ecall(c),2. The Recall Tree Algorithm,[0],[0]
then 6: break 7: end if 8: n←,2. The Recall Tree Algorithm,[0],[0]
"c 9: x← x+ {(n.id : 1)}
10: end while 11: ŷ",2. The Recall Tree Algorithm,[0],[0]
"← argmax
y∈n.candidates scorey(x)
cisions to section 3.",2. The Recall Tree Algorithm,[0],[0]
"The Recall Tree data structure (see Figure 2) consists of two components: (1) a binary tree, described below; and (2) a scoring function scorey(x) that will evaluate the quality of a small set of candidates y to make a final prediction.",2.1. Recall Tree at Test Time,[0],[0]
"Each node n in the binary tree maintains:
• a router, denoted f , that maps an example to either a left or right child; routers are implemented as binary classifiers; • a histogram of the labels of all training examples that have been routed to, or through, n.
The primary purpose of the histogram is to generate a candidate set of labels to be scored, taken to be the most frequent labels in that histogram.",2.1. Recall Tree at Test Time,[0],[0]
"Intuitively, the goal of the candidate set is to maintain good recall, while the goal of the score function is to achieve good precision.",2.1. Recall Tree at Test Time,[0],[0]
"Crucially, the leaves of the tree do not partition the set of classes: classes can (and do) have support at multiple leaves.
",2.1. Recall Tree at Test Time,[0],[0]
"At test time, an input x is provided and a recursive computation begins at the root of the tree.",2.1. Recall Tree at Test Time,[0],[0]
The tree is descended according to the binary classification decision made at each internal node.,2.1. Recall Tree at Test Time,[0],[0]
"When the recursion ends (for instance, when a leaf is reached), the top F most frequent labels according to the node’s label counter are used as a candidate set.",2.1. Recall Tree at Test Time,[0],[0]
When F = O(logK) this does not compromise the goal of achieving logarithmic time classification.,2.1. Recall Tree at Test Time,[0],[0]
"Once this candidate set is chosen, each y in that set is scored using the score function, and the largest scoring y is returned.
",2.1. Recall Tree at Test Time,[0],[0]
"It turns out that it is advantageous to allow the recursion to end before hitting a leaf, which is a consequence of how
training happens on tree-structured classifiers.",2.1. Recall Tree at Test Time,[0],[0]
"In particular, the number of labeled examples that the root classifier “sees” is much larger than the number of labeled examples that any leaf sees.",2.1. Recall Tree at Test Time,[0],[0]
This potentially leads to: (1) high variance toward the leaves; and (2) insufficient representation complexity toward the root.,2.1. Recall Tree at Test Time,[0],[0]
"Instead of halting at a leaf, we can halt at an internal node for which the top F most frequent labels contain the true answer with a sufficiently high probability.
",2.1. Recall Tree at Test Time,[0],[0]
Algorithm 1 formalizes the test-time behavior of the Recall Tree.,2.1. Recall Tree at Test Time,[0],[0]
"The primary routing occurs in the first line of the main loop, where c is the child selected by the current node’s router.",2.1. Recall Tree at Test Time,[0],[0]
"On the next line, the recursion considers the possibility of terminating on an internal node if the bounded recall, r̂ecall, of the current node n is greater than the estimated recall of the chosen child c.",2.1. Recall Tree at Test Time,[0],[0]
"If the recursion does not end, a new “path feature” is added to x at the end of the main loop, which records the path taken in the recall tree: the benefit of adding these features is that it increases the representational capacity of the recall tree to ensure competitiveness with OAA (§3.2).",2.1. Recall Tree at Test Time,[0],[0]
"Whichever way the recursion ends, the final node n has a (small) set of candidate labels n.candidates ⊂ Y .",2.1. Recall Tree at Test Time,[0],[0]
"Each is scored according to a one-against-some rule and the label with the largest score is returned.
",2.1. Recall Tree at Test Time,[0],[0]
A natural way to estimate recall at a node n is to consider it’s empirical recall r̂n.,2.1. Recall Tree at Test Time,[0],[0]
This is simply the fraction of the mass consumed by the F most frequent labels in n’s counter.,2.1. Recall Tree at Test Time,[0],[0]
"For example, if the counter saw label 1 two times, label 4 fifty times and label 3 ten times, and if F = 2, then the empirical recall would be 60/62.",2.1. Recall Tree at Test Time,[0],[0]
"However, because, in general, a parent node will see more data than a child node, the quality of this estimate is likely to be much better for the parent than the child due to a missing mass problem (Good, 1953).",2.1. Recall Tree at Test Time,[0],[0]
"To accomodate this, we instead use an empirical Bernstein lower bound (Maurer & Pontil, 2009), which is summarized by the following proposition.",2.1. Recall Tree at Test Time,[0],[0]
Proposition 1.,2.1. Recall Tree at Test Time,[0],[0]
For all multiclass classification problems defined by a distribution D over X ×,2.1. Recall Tree at Test Time,[0],[0]
"[K], and all nodes n in a fixed tree, there exists a constant λ > 0",2.1. Recall Tree at Test Time,[0],[0]
"such that with probability 1− δ:
r̂ecall(n) = r̂n",2.1. Recall Tree at Test Time,[0],[0]
"− √ λr̂n(1− r̂n)
mn − λ mn ≤ rn (1)
where r̂n is the empirical recall of node n computed over mn = n.total items; and rn is the expected value of this recall in the population limit.
",2.1. Recall Tree at Test Time,[0],[0]
"Here, λ is a hyperparameter of the recall tree (in fact, it is the only additional hyperparameter), which controls how aggressively the tree branches.",2.1. Recall Tree at Test Time,[0],[0]
"We show in our experiments that these various design decisions (path features, Bernstein lower bounds, and early termination) are useful in practice.
",2.1. Recall Tree at Test Time,[0],[0]
Algorithm 2 Train.,2.1. Recall Tree at Test Time,[0],[0]
An input labeled example descends the tree as in Algorithm 1.,2.1. Recall Tree at Test Time,[0],[0]
"update candidates updates the set of candidate labels at each node and update regressors updates the one-against-some regressors; and r̂ecall(◦) is a an empirical bound on the recall of a node (◦) (see section 3.1).
",2.1. Recall Tree at Test Time,[0],[0]
"Input: Example (x, y), Root node n Output: Update tree with root at n while n.leaf is false do update router(x, y, n) c← n.f(x) > 0 ?",2.1. Recall Tree at Test Time,[0],[0]
n.left :,2.1. Recall Tree at Test Time,[0],[0]
"n.right update candidates(x, y, c)
if r̂ecall(n) > r̂ecall(c) then break end if n←",2.1. Recall Tree at Test Time,[0],[0]
c x←,2.1. Recall Tree at Test Time,[0],[0]
"x+ {(n.id : 1)}
end while update regressors(x, y, n.candidates)",2.1. Recall Tree at Test Time,[0],[0]
The Recall Tree maintains one regressor for each class and a tree whose purpose is to eliminate regressor from consideration.,2.2. Recall Tree at Training Time,[0],[0]
We refer to the per-class regressor as one-againstsome (OAS) regressors.,2.2. Recall Tree at Training Time,[0],[0]
The tree creates a high recall set of candidate classes and then leverages the OAS regressors to achieve precision.,2.2. Recall Tree at Training Time,[0],[0]
"Algorithm 2 outlines the learning procedures, which we now describe in more detail.
",2.2. Recall Tree at Training Time,[0],[0]
"Learning the regressors for each class In Algorithm 2, update regressors updates the candidate set regressors using the standard OAA strategy restricted to the set of eligible classes.",2.2. Recall Tree at Training Time,[0],[0]
"If the true label is not in the F most frequent classes at this node then no update occurs.
",2.2. Recall Tree at Training Time,[0],[0]
"Learning the set of candidates in each node In Algorithm 2, update candidates updates the count of the true label at this node.",2.2. Recall Tree at Training Time,[0],[0]
"At each node, the most frequent F labels are the candidate set.
",2.2. Recall Tree at Training Time,[0],[0]
"Learning the routers at each node In Algorithm 2, update router updates the router at a node by optimizing the reduction in the entropy of the label distribution (the label entropy) due to routing, as detailed in Algorithm 3.",2.2. Recall Tree at Training Time,[0],[0]
This is in accordance with our theory (Section 3.3).,2.2. Recall Tree at Training Time,[0],[0]
The label entropy for a node is estimated using the empirical counts of each class label entering the node.,2.2. Recall Tree at Training Time,[0],[0]
These counts are reliable as update router is only called for the root or nodes whose true recall bound is better than their children.,2.2. Recall Tree at Training Time,[0],[0]
"The expected label entropy after routing is estimated by averaging the estimated label entropy of each child node, weighted by the fraction of examples routing left or right.",2.2. Recall Tree at Training Time,[0],[0]
"Finally, we compute the advantage of routing left vs. right
Algorithm 3 update router.",2.2. Recall Tree at Training Time,[0],[0]
entropy computes two values: the empirical entropy of labels incident on a node without and with (respectively) an extra label y. Ĥ|left is an estimate of the average entropy if the example is routed left.,2.2. Recall Tree at Training Time,[0],[0]
"Learnn(x,w, y) is an importance-weighted update to the binary classifier f(x) for node n with features x, label y, and weight w.
Input: Example (x, y);",2.2. Recall Tree at Training Time,[0],[0]
Node n Output: Update node n,2.2. Recall Tree at Training Time,[0],[0]
"(Ĥleft, Ĥ ′ left) .",2.2. Recall Tree at Training Time,[0],[0]
"= entropy(n.left, y) (Ĥright, Ĥ ′",2.2. Recall Tree at Training Time,[0],[0]
right) .,2.2. Recall Tree at Training Time,[0],[0]
"= entropy(n.right, y) Ĥ|left .",2.2. Recall Tree at Training Time,[0],[0]
= n.left.totaln.total Ĥ ′,2.2. Recall Tree at Training Time,[0],[0]
left +,2.2. Recall Tree at Training Time,[0],[0]
n.right.total n.total Ĥright Ĥ|right .,2.2. Recall Tree at Training Time,[0],[0]
= n.left.totaln.total Ĥleft +,2.2. Recall Tree at Training Time,[0],[0]
n.right.total n.total Ĥ ′,2.2. Recall Tree at Training Time,[0],[0]
right ∆̂Hpost ← Ĥ|left,2.2. Recall Tree at Training Time,[0],[0]
"− Ĥ|right Learnn(x, |∆̂Hpost|, sign(∆̂Hpost))
",2.2. Recall Tree at Training Time,[0],[0]
"Algorithm 4 update regressors updates the OAS scoring functions for a single example.
",2.2. Recall Tree at Training Time,[0],[0]
"Input: Example (x, y); Candidate set candidates Output: Update scoring functions score if y ∈ candidates then
online update to scorey(x) with label +1 for ŷ ∈ candidates−",2.2. Recall Tree at Training Time,[0],[0]
"{y} do
online update to scoreŷ(x) with label −1 end for
end if
by taking the difference of the expected label entropies for routing left vs. right.",2.2. Recall Tree at Training Time,[0],[0]
"The sign of this difference determines the binary label for updating the router.
",2.2. Recall Tree at Training Time,[0],[0]
"Tree depth control We calculate a lower bound r̂ecall(n) on the true recall of node n (Section 3.1), halting descent as in Algorithm 2.",2.2. Recall Tree at Training Time,[0],[0]
"As we descend the tree, the bound first increases (empirical recall increases) then declines (variance increases).",2.2. Recall Tree at Training Time,[0],[0]
We also limit the maximum depth d of the tree.,2.2. Recall Tree at Training Time,[0],[0]
This parameter is typically not operative but adds an additional safety check and sees some use on datasets where multipasses are employed.,2.2. Recall Tree at Training Time,[0],[0]
Online construction of an optimal logarithmic time regressors for multiclass classification given an arbitrary fixed representation at each node appears deeply intractable.,3. Theoretical Motivation,[0],[0]
A primary difficulty is that decisions have to be hard since we cannot afford to maintain a distribution over all class labels.,3. Theoretical Motivation,[0],[0]
"Choosing a classifier so as to minimize error rate has been considered for cryptographic primitives (Blum et al., 1993)",3. Theoretical Motivation,[0],[0]
so it is plausibly hard on average rather than merely hard in the worst case.,3. Theoretical Motivation,[0],[0]
"Furthermore, the joint optimization of
all regressors does not nicely decompose into independent problems.",3. Theoretical Motivation,[0],[0]
Solving the above problems requires an implausible break-through in complexity theory which we do not achieve here.,3. Theoretical Motivation,[0],[0]
"Instead, we use learning theory to assist the design by analyzing various simplifications of the problem.",3. Theoretical Motivation,[0],[0]
"For binary classification, a simple trick can (in theory) collapse the number of leaves while preserving prediction performance.",3.1. One-Against-Some Recall,[0],[0]
"In particular, branching programs (Mansour & McAllester, 2002) result in exponentially more succinct representations than decision trees (Kearns & Mansour, 1996) by joining nodes to create directed acyclic graphs.",3.1. One-Against-Some Recall,[0],[0]
"The key observation is that nodes in the same level with a similar distribution over class labels can be joined into one node, implying that the number of nodes at one level is only θ(1/γ) where γ is the weak learning parameter rather than exponential in the depth.",3.1. One-Against-Some Recall,[0],[0]
This approach generally fails in the multiclass setting because covering the simplex of multiclass label distributions requires (K − 1)θ(1/γ) nodes.,3.1. One-Against-Some Recall,[0],[0]
One easy special case exists.,3.1. One-Against-Some Recall,[0],[0]
"When the distribution over class labels is skewed so one label is the majority class, learning an entropy minimizing binary classifier predicts whether the class is the majority or not.",3.1. One-Against-Some Recall,[0],[0]
"There are only K possible OAS regressors of this sort so maintaining one for each class label is computationally tractable.
",3.1. One-Against-Some Recall,[0],[0]
Using OAS classifiers creates a limited branching program structure over predictions.,3.1. One-Against-Some Recall,[0],[0]
"Aside from the space savings generated, this also implies that nodes deep in the tree use many more labeled examples than are otherwise available.",3.1. One-Against-Some Recall,[0],[0]
"In finite sample regimes, which are not covered by these boosting analyses, more labeled samples induce a better predictor as per standard sample complexity analysis.
",3.1. One-Against-Some Recall,[0],[0]
"As a result, we use the empirical Bernstein lower bound on recall described in §2.1.",3.1. One-Against-Some Recall,[0],[0]
Reducing the depth of the tree by using this lower bound and joining labeled examples from many leaves in a one-against-some approach both relieves data sparsity problems and allows greater error tolerance by the root node.,3.1. One-Against-Some Recall,[0],[0]
Different multiclass classification schemes give rise to different multiclass hypothesis classes.,3.2. Path Features,[0],[0]
"For example, the set of multiclass decision boundaries realizable under an OAA structure over linear regressors is fundamentally different from that realizable under a tree structure over linear regressors.",3.2. Path Features,[0],[0]
Are OAA types of representations inherently more or less powerful than a tree based representation?,3.2. Path Features,[0],[0]
"Figure 3 shows two learning problems illustrating two extremes assuming a linear representation.
",3.2. Path Features,[0],[0]
Linear OAA:,3.2. Path Features,[0],[0]
"If all the class parameter vectors happen to have the same `2 norm, then OAA classification is equivalent to finding the nearest neighbor amongst a set of vectors (one per class) which partition the space into a Voronoi diagram as in 3 on the left.",3.2. Path Features,[0],[0]
"The general case, with unequal vectors corresponds to a weighted Voronoi diagram where the magnitude of two vectors sharing a border determines the edge of the partition.",3.2. Path Features,[0],[0]
"No weighted Voronoi diagram can account for the partition on the right.
",3.2. Path Features,[0],[0]
"Trees: If the partition of a space can be represented by a sequence of conditional splits, then a tree can represent the solution accurately as in 3 on the right.",3.2. Path Features,[0],[0]
"On the other hand, extra work is generally required to represent a Voronoi diagram as on the left.",3.2. Path Features,[0],[0]
"In general, the number of edges in a multidimensional Voronoi diagram may grow at least quadratically in the number of points implying that the number of nodes required for a tree to faithfully represent a Voronoi diagram is at least Θ(n2).
",3.2. Path Features,[0],[0]
"Based on this, neither tree-based nor OAA style prediction is inherently more powerful, with the best solution being problem dependent.
",3.2. Path Features,[0],[0]
"Since we are interested in starting with a tree-based approach and ending with a OAS classifier, there is a simple representational trick which provides the best of both worlds.",3.2. Path Features,[0],[0]
We can add features which record the path through the tree.,3.2. Path Features,[0],[0]
"To be precise, let T be a tree and pathT (x) be a vector with one dimension per node in T which is set to 1 if x traverses the node and 0 otherwise.",3.2. Path Features,[0],[0]
"The following proposition holds for linear representations, which are special because they are tractably analyzed and because they are the fundamental building blocks around which many more complex representations are built.
Proposition.",3.2. Path Features,[0],[0]
For any distribution D over X ×,3.2. Path Features,[0],[0]
"[K] for which a tree T achieves error rate , a OAA classifier over linear regressors, whose input consists of x ∈ X and the corresponding routing path of x in T (as indicator features) can also achieve error rate .
",3.2. Path Features,[0],[0]
Proof.,3.2. Path Features,[0],[0]
"A linear OAA classifier is defined by a matrix wiy where i ranges over the input and y ranges over the labels.
",3.2. Path Features,[0],[0]
Let wiy = 0 by default and 1 when i corresponds to a leaf for which the tree predicts y.,3.2. Path Features,[0],[0]
"Under this representation, the prediction of OAA(x, pathT (x)) is identical to T (x), and hence achieves the same error rate.",3.2. Path Features,[0],[0]
The Shannon Entropy of class labels is optimized in the router of Algorithm 3.,3.3. Optimization Objective,[0],[0]
"Why?
Since the Recall Tree jointly optimizes over many base learning algorithms, the systemic properties of the joint optimization are important to consider.",3.3. Optimization Objective,[0],[0]
"A theory of decision tree learning as boosting (Kearns & Mansour, 1996) provides a way to understand these joint properties in a population limit (or equivalently on a training set iterated until convergence).",3.3. Optimization Objective,[0],[0]
"In essence, the analysis shows each level of the tree boosts the accuracy of the resulting tree with this conclusion holding for several common objectives.
",3.3. Optimization Objective,[0],[0]
"In boosting for multiclass classification (Choromanska et al., 2016; Choromanska & Langford, 2015; Takimoto & Maruoka, 2003), it is important to achieve a weak dependence on the number of class labels.",3.3. Optimization Objective,[0],[0]
"Shannon Entropy is particularly well-suited to this goal, because it has only a logarithmic dependence on the number of class labels.",3.3. Optimization Objective,[0],[0]
"Let πi|n be the probability that the correct label is i, conditioned on the corresponding example reaching",3.3. Optimization Objective,[0],[0]
node n.,3.3. Optimization Objective,[0],[0]
Then Hn = ∑K i=1,3.3. Optimization Objective,[0],[0]
"πi|n log2 1 πi|n
is the Shannon entropy of class labels reaching node n.
For this section, we consider a simplified algorithm which neglects concerns of finite sample analysis, how optimization is done, and the leaf predictors.",3.3. Optimization Objective,[0],[0]
What’s left is the value of optimizing the router objective.,3.3. Optimization Objective,[0],[0]
We consider an algorithm which recursively splits the leaf with the largest proportion p of all examples starting at the root and reaching the leaf.,3.3. Optimization Objective,[0],[0]
The leaf is split into two new leaves to the left l and right r.,3.3. Optimization Objective,[0],[0]
"If pl and pr are the fraction of examples going left and right (so pl +pr = 1), the split criterion minimizes the expectation over the leaves of the average class entropy, plHl + prHr.",3.3. Optimization Objective,[0],[0]
This might be achieved by update router in Algorithm 2 or by any other means.,3.3. Optimization Objective,[0],[0]
With this criterion we are in a position to directly optimize information boosting.,3.3. Optimization Objective,[0],[0]
Definition 1.,3.3. Optimization Objective,[0],[0]
(γ-Weak Learning Assumption),3.3. Optimization Objective,[0],[0]
"For all distributions D(x, y) a learning algorithm using examples (x, y)∗ IID from D finds a binary classifier c :",3.3. Optimization Objective,[0],[0]
"X → {l, r} satisfying
plHl + prHr ≤",3.3. Optimization Objective,[0],[0]
"Hn − γ .
",3.3. Optimization Objective,[0],[0]
"This approach is similar to previous (Takimoto & Maruoka, 2003) except that we boost in an additive rather than a multiplicative sense.",3.3. Optimization Objective,[0],[0]
"A multiplicative approach suppresses a necessary dependence on K. In particular, for any nontrivial γ there exists a K such that with a uniform distribution U , HU (1 − γ) > 1).",3.3. Optimization Objective,[0],[0]
"As a consequence, theorems proved
with a multiplicative γ are necessarily vacuous for large K while additive approaches do not suffer from this issue.
",3.3. Optimization Objective,[0],[0]
"As long as Weak Learning occurs, we can prove the following theorem.",3.3. Optimization Objective,[0],[0]
Theorem 2.,3.3. Optimization Objective,[0],[0]
"If γ Weak Learning holds for every node in the tree and nodes with the largest fraction of examples are split first, then after t > 2 splits the multiclass error rate of the tree is bounded by:
≤ H1 − γ ln(t+ 1)
where H1 is the entropy of the marginal distribution of class labels.
",3.3. Optimization Objective,[0],[0]
"The proof in appendix A reuses techniques from (Choromanska & Langford, 2015; Kearns & Mansour, 1996) but has a tighter result.
",3.3. Optimization Objective,[0],[0]
"The most important observation from the theorem is that as t (the number of splits) increases, the error rate is increasingly bounded.",3.3. Optimization Objective,[0],[0]
This rate depends on ln t agreeing with the intuition that boosting happens level by level in the tree.,3.3. Optimization Objective,[0],[0]
"The dependence on the initial entropy H1 shows that skewed marginal class distributions are inherently easier to learn than uniform marginal class distributions, as might be expected.",3.3. Optimization Objective,[0],[0]
"These results are similar to previous results (Choromanska et al., 2016; Choromanska & Langford, 2015; Kearns & Mansour, 1996; Takimoto & Maruoka, 2003) with advantages.",3.3. Optimization Objective,[0],[0]
"We handle multiclass rather than binary classification (Kearns & Mansour, 1996), we bound error rates instead of entropy (Choromanska et al., 2016; Choromanska & Langford, 2015), and we use additive rather than multiplicative weak learning (Takimoto & Maruoka, 2003).",3.3. Optimization Objective,[0],[0]
"We study several questions empirically.
1.",4. Empirical Results,[0],[0]
What is the benefit of using one-against-some on a recall set?,4. Empirical Results,[0],[0]
2.,4. Empirical Results,[0],[0]
What is the benefit of path features?,4. Empirical Results,[0],[0]
"3. Is the online nature of the Recall Tree useful on non-
stationary problems?",4. Empirical Results,[0],[0]
4.,4. Empirical Results,[0],[0]
"How does the Recall Tree compare to one-against-all
statistically and computationally?
5.",4. Empirical Results,[0],[0]
"How does the Recall Tree compare to LOMTree statistically and computationally?
",4. Empirical Results,[0],[0]
Throughout this section we conduct experiments using learning with a linear representation.,4. Empirical Results,[0],[0]
Table 1 overviews the data sets used for experimentation.,4.1. Datasets,[0],[0]
"These include the largest datasets where published results are available for LOMTree (Aloi, Imagenet, ODP), plus an additional language modeling data set (LTCB).",4.1. Datasets,[0],[0]
"Implementations of the learning algorithms, and scripts to reproduce the data sets and experimental results, are available on github (Mineiro, 2017).",4.1. Datasets,[0],[0]
Additional details about the datasets can be found in Appendix B.,4.1. Datasets,[0],[0]
"In our first set of experiments, we compare Recall Tree with a strong computational baseline and a strong statistical baseline.",4.2. Comparison with other Algorithms,[0],[0]
"The computational baseline is LOMTree, the only other online logarithmic-time multiclass algorithm of which we are aware.",4.2. Comparison with other Algorithms,[0],[0]
"The statistical baseline is OAA, whose statistical performance we want to match (or even exceed), and whose linear computational dependence on the number of classes we want to avoid.",4.2. Comparison with other Algorithms,[0],[0]
"Details regarding the experimental methodology are in Appendix C. Results are summarized in Figure 4.
",4.2. Comparison with other Algorithms,[0],[0]
Comparison with LOMTree,4.2. Comparison with other Algorithms,[0],[0]
The Recall Tree uses a factor of 32 less state than the LOMTree which makes a dramatic difference in feasibility for large scale applications.,4.2. Comparison with other Algorithms,[0],[0]
"Given this state reduction, the default expectation is worse prediction performance by the Recall Tree.",4.2. Comparison with other Algorithms,[0],[0]
"Instead, we observe superior or onpar statistical performance despite the state constraint.",4.2. Comparison with other Algorithms,[0],[0]
"This typically comes with an additional computational cost since the Recall Tree evaluates a number of per-class regressors.
",4.2. Comparison with other Algorithms,[0],[0]
"Comparison with OAA On one dataset (ALOI) prediction performance is superior to OAA while on the others it is somewhat worse.
",4.2. Comparison with other Algorithms,[0],[0]
Computationally OAA has favorable constant factors since it is highly amenable to vectorization.,4.2. Comparison with other Algorithms,[0],[0]
"Conversely, the
-4
0
4
8
12
ALOI Imagenet
ODP
De lta
T es
t E rr
or (% )",4.2. Comparison with other Algorithms,[0],[0]
"Fr om O
AA Statistical Performance LOMTree Recall Tree
1e-05 0.0001
0.001 0.01
0.1 1
ALOI Imagenet
ODP
In fe
re nc
e Ti
m e
Pe r E
xa m
pl e
(s ec
on ds
)
",4.2. Comparison with other Algorithms,[0],[0]
"Computational Performance
OAA Recall Tree
LOMTree
Figure 4.",4.2. Comparison with other Algorithms,[0],[0]
Empirical comparison of statistical (left) and computational (right) performance of Recall Tree against two strong competitors: OAA (statistically good) and LOMTree (computationally good).,4.2. Comparison with other Algorithms,[0],[0]
"In both graphs, lower is better.",4.2. Comparison with other Algorithms,[0],[0]
"Recall Tree has poly(log) dependence upon number of classes (like LOMTree) but can surpass OAA statistically.
",4.2. Comparison with other Algorithms,[0],[0]
conditional execution pattern of the Recall Tree frustrates vectorization even with example mini-batching.,4.2. Comparison with other Algorithms,[0],[0]
"Thus on ALOI although Recall Tree does on average 50 hyperplane evaluations per example while OAA does 1000, OAA is actually faster: larger numbers of classes are required to experience the asymptotic benefits.",4.2. Comparison with other Algorithms,[0],[0]
"For ODP with ∼ 105 classes, with negative gradient subsampling and using 24 cores in parallel, OAA is about the same wall clock time to train as Recall Tree on a single core.2 Negative gradient sampling does not improve inference times, which are ∼ 300 times slower for OAA than Recall Tree on ODP.",4.2. Comparison with other Algorithms,[0],[0]
In this experiment we leverage the online nature of the algorithm to exploit nonstationarity in the data to improve results.,4.3. Online Operation,[0],[0]
"This is not something that is easily done with batch oriented algorithms, or with algorithms that post-process a trained predictor to accelerate inference.
",4.3. Online Operation,[0],[0]
We consider two versions of LTCB.,4.3. Online Operation,[0],[0]
In both versions the task is to predict the next word given the previous 6 tokens.,4.3. Online Operation,[0],[0]
"The difference is that in one version, the Wikipedia dump is processed in the original order (“in-order”); whereas in the other version the training data is permuted prior to input to the learning algorithm (“permuted”).",4.3. Online Operation,[0],[0]
"We assess progressive validation loss (Blum et al., 1999) on the sequence.",4.3. Online Operation,[0],[0]
"The result in Figure 5a confirms the Recall Tree is able to take advantage of the sequentially revealed data; in particular, the far-right difference in accuracies is significant at a factor P < 0.0001 according to an N −1",4.3. Online Operation,[0],[0]
Chi-squared test.,4.3. Online Operation,[0],[0]
"Two differences between Recall Tree and LOMTree are the use of multiple regressors at each tree node and the aug-
2While not yet implemented, Recall Tree can presumably also leverage multicore for acceleration.
mentation of the example with path features.",4.4. Path Features and Multiple Regressors,[0],[0]
"In this experiment we explore the impact of these design choices using the ALOI dataset.
",4.4. Path Features and Multiple Regressors,[0],[0]
Figure 5b shows the effect of these two aspects on statistical performance.,4.4. Path Features and Multiple Regressors,[0],[0]
"As the candidate set size is increased, test error decreases, but with diminishing returns.",4.4. Path Features and Multiple Regressors,[0],[0]
"Disabling path features degrades performance, and the effect is more pronounced as the candidate set size increases.",4.4. Path Features and Multiple Regressors,[0],[0]
"This is expected, as a larger candidate set size decreases the difficulty of obtaining good recall (i.e., a good tree) but increases the difficulty of obtaining good precision (i.e., good class regressors), and path features are only applicable to the latter.",4.4. Path Features and Multiple Regressors,[0],[0]
"All differences here are significant at a P < 0.0001 according to an N − 1 Chi-squared test, except for when the candidate set size is 2, where there is no significant difference.",4.4. Path Features and Multiple Regressors,[0],[0]
To test this we trained on the LTCB dataset with a multiplier on the bound of either 0 (i.e. just using empirical recall directly) or 1.,4.5. Is the empirical Bernstein bound useful?,[0],[0]
"The results are stark: with a multiplier of 1, the test error was 78% while with a multiplier of 0 the test error was 91%.",4.5. Is the empirical Bernstein bound useful?,[0],[0]
"Clearly, in the few samples per class regime this form of direct regularization is very helpful.",4.5. Is the empirical Bernstein bound useful?,[0],[0]
"The LOMTree (Choromanska et al., 2016; Choromanska & Langford, 2015) is the closest prior work.",5. Related Work,[0],[0]
It misses on space requirements: up to a factor of 64 more space than OAA was used experimentally.,5. Related Work,[0],[0]
Despite working with radically less space we show the Recall Tree typically provides better predictive performance.,5. Related Work,[0],[0]
"The key differences here are algorithmic: a tighter reduction at internal nodes and the one-against-some approach yields generally better performance despite much tighter resource constraints.
",5. Related Work,[0],[0]
"70 75 80 85 90 95
100
10000 100000",5. Related Work,[0],[0]
1e+06,5. Related Work,[0],[0]
1e+07,5. Related Work,[0],[0]
"1e+08
Av er
ag e
Cu m
ul at
iv e
Pr og
re ss
iv e
Ac cu
ra cy
(% )
",5. Related Work,[0],[0]
"Examples
in-order permuted
(a) When the LTCB dataset is presented in the original order, Recall Tree is able to exploit sequential correlations for improved performance.",5. Related Work,[0],[0]
"After all examples are processed, the average progressive accuracy is 73.3% vs. 74.6%.
",5. Related Work,[0],[0]
"10 15 20 25 30 35 40
1 10 100
Te st
E rr
or (%
)
Candidate Set Size
with path features without path features
(b) Test error on ALOI for various candidate set sizes, with or without path features (all other parameters held fixed).",5. Related Work,[0],[0]
"Using multiple regressors per leaf and including path features improves performance.
",5. Related Work,[0],[0]
"Figure 5.
",5. Related Work,[0],[0]
"Our use of entropy optimization is closely related to the foundational work on decision tree learning (Quinlan, 1993), picking single features on which to split based on entropy.",5. Related Work,[0],[0]
"More recently, it is decision tree learning can be thought of as boosting (Kearns & Mansour, 1996) for multiclass learning (Takimoto & Maruoka, 2003), based on on a generalized notion of entropy, which results in low 0/1 loss.",5. Related Work,[0],[0]
Relative to these works we show how to efficiently achieve weak learning by reduction to binary classification making this approach empirically practical.,5. Related Work,[0],[0]
"We also address a structural issue in the multiclass analysis (see section 3.3).
",5. Related Work,[0],[0]
"Other approaches such as hierarchical softmax (HSM) and the the Filter Tree (Beygelzimer et al., 2009) use a fixed tree structure (Morin & Bengio, 2005).",5. Related Work,[0],[0]
"In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously (Bengio et al., 2010; Choromanska & Langford, 2015).
",5. Related Work,[0],[0]
"Most other approaches in extreme classification either do not work online (Mnih & Hinton, 2009; Prabhu & Varma, 2014) or only focus on speeding up either prediction time or training time but not both.",5. Related Work,[0],[0]
Most of the works that enjoy sublinear inference time (but (super)linear training time) are based on tree decomposition approaches.,5. Related Work,[0],[0]
"In (Mnih & Hinton, 2009) the authors try to add tree structure learning to HSM via iteratively clustering the classes.",5. Related Work,[0],[0]
"While the end result is a classifier whose inference time scales logarithmically with the number of classes, the clustering steps are batch and scale poorly with the number of classes.",5. Related Work,[0],[0]
"Similar remarks apply to (Bengio et al., 2010) where the authors propose to learn a tree by solving an eigenvalue problem after (OAA) training.",5. Related Work,[0],[0]
"The work of (Weston et al., 2013) is similar in spirit to ours, as the authors propose to learn
a label filter to reduce the number of candidate classes in an OAA approach.",5. Related Work,[0],[0]
"However they learn the tree after training the underlying OAA regressors while here we learn, and more crucially use, the tree during training of the OAS regressors.",5. Related Work,[0],[0]
"Among the approaches that speed up training time we distinguish exact ones (de Brébisson & Vincent, 2015; Vincent et al., 2015) that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.g. in (Weston et al., 2011).",5. Related Work,[0],[0]
"Though these techniques do not address inference time, separate procedures for speeding up inference (given a trained model) have been proposed (Shrivastava & Li, 2014).",5. Related Work,[0],[0]
"However, such two step procedures can lead to substantially suboptimal results.",5. Related Work,[0],[0]
"In this work we proposed the Recall Tree, a reduction of multiclass to binary classification, which operates online and scales logarithmically with the number of classes.",6. Conclusion,[0],[0]
"Unlike the LOMTree (Choromanska & Langford, 2015), we share classifiers among the nodes of the tree which alleviates data sparsity at deep levels while greatly reducing the required state.",6. Conclusion,[0],[0]
We also use a tighter analysis which is more closely followed in the implementation.,6. Conclusion,[0],[0]
These features allow us to reduce the statistical gap with OAA while still operating many orders of magnitude faster for large K multiclass datasets.,6. Conclusion,[0],[0]
In the future we plan to investigate multiway splits in the tree since O(logK)-way splits does not affect our O(poly logK) running time and they might reduce contention in the root and nodes high in the tree.,6. Conclusion,[0],[0]
We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes.,abstractText,[0],[0]
We show that several simple techniques give rise to an algorithm which is superior to previous logarithmic time classification approaches while competing with one-against-all in space.,abstractText,[0],[0]
"The core construction is based on using a tree to select a small subset of labels with high recall, which are then scored using a one-against-some structure with high precision.",abstractText,[0],[0]
Logarithmic Time One-Against-Some,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 79–89, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al., 2011).",1 Introduction,[0],[0]
"It is expressive enough to represent complex natural language queries on a relational database, yet simple enough to be latently learned from question-answer pairs.",1 Introduction,[0],[0]
"In this paper, we equip DCS with logical inference, which, in one point of view, is “the best way of testing an NLP system’s semantic capacity” (Cooper et al., 1996).
",1 Introduction,[0],[0]
"It should be noted that, however, a framework primarily designed for question answering is not readily suited for logical inference.",1 Introduction,[0],[0]
"Because, answers returned by a query depend on the specific database, but implication is independent of any databases.",1 Introduction,[0],[0]
"For example, answers to the question “What books are read by students?”, should always be a subset of answers to “What books are ever read by anyone?”, no matter how we store the data of students and how many records of books are there in our database.
",1 Introduction,[0],[0]
"Thus, our first step is to fix a notation which abstracts the calculation process of DCS trees, so as to clarify its meaning without the aid of any existing database.",1 Introduction,[0],[0]
"The idea is to borrow a minimal set of operators from relational algebra (Codd, 1970), which is already able to formulate the calculation in DCS and define abstract denotation, which is an abstraction of the computation of denotations guided by DCS trees.",1 Introduction,[0],[0]
Meanings of sentences then can be represented by primary relations among abstract denotations.,1 Introduction,[0],[0]
"This formulation keeps the simpleness and computability of DCS trees mostly unaffected; for example, our semantic calculation for DCS trees is parallel to the denotation computation in original DCS.
",1 Introduction,[0],[0]
An inference engine is built to handle inference on abstract denotations.,1 Introduction,[0],[0]
"Moreover, to compensate the lack of background knowledge in practical inference, we combine our framework with the idea of tree transformation (Bar-Haim et al., 2007), to propose a way of generating knowledge in logical representation from entailment rules (Szpektor et al., 2007), which are by now typically considered as syntactic rewriting rules.
",1 Introduction,[0],[0]
"We test our system on FraCaS (Cooper et al., 1996) and PASCAL RTE datasets (Dagan et al., 2006).",1 Introduction,[0],[0]
The experiments show: (i) a competitive performance on FraCaS dataset; (ii) a big impact of our automatically generated on-the-fly knowledge in achieving high recall for a logicbased RTE system; and (iii) a result that outperforms state-of-the-art RTE system on RTE5 data.,1 Introduction,[0],[0]
Our whole system is publicly released and can be downloaded from http://kmcs.nii.ac.,1 Introduction,[0],[0]
jp/tianran/tifmo/.,1 Introduction,[0],[0]
"In this section we describe the idea of representing natural language semantics by DCS trees, and achieving inference by computing logical relations among the corresponding abstract denotations.
79",2 The Idea,[0],[0]
"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al., 2011) (Figure 1).",2.1 DCS trees,[0],[0]
"For the sentence “students read books”, imagine a database consists of three tables, namely, a set of students, a set of books, and a set of “reading” events (Table 1).",2.1 DCS trees,[0],[0]
"The DCS tree in Figure 1 is interpreted as a command for querying these tables, obtaining “reading” entries whose “SUBJ” field is student and whose “OBJ” field is book.",2.1 DCS trees,[0],[0]
"The result is a set {John reads Ulysses, . . .}, which is called a denotation.
",2.1 DCS trees,[0],[0]
"DCS trees can be extended to represent linguistic phenomena such as quantification and coreference, with additional markers introducing additional operations on tables.",2.1 DCS trees,[0],[0]
"Figure 2 shows an example with a quantifier “every”, which is marked as “⊂” on the edge (love)OBJ-ARG(dog) and interpreted as a division operator qOBJ⊂",2.1 DCS trees,[0],[0]
(§2.2).,2.1 DCS trees,[0],[0]
"Optimistically, we believe DCS can provide a framework of semantic representation with sufficiently wide coverage for real-world texts.
",2.1 DCS trees,[0],[0]
The strict semantics of DCS trees brings us the idea of applying DCS to logical inference.,2.1 DCS trees,[0],[0]
"This is not trivial, however, because DCS works under the assumption that databases are explicitly available.",2.1 DCS trees,[0],[0]
"Obviously this is unrealistic for logical inference on unrestricted texts, because we cannot prepare a database for everything in the world.",2.1 DCS trees,[0],[0]
"This fact fairly restricts the applicable tasks of DCS.
",2.1 DCS trees,[0],[0]
"Our solution is to redefine DCS trees without the aid of any databases, by considering each node of a DCS tree as a content word in a sentence (but may no longer be a table in a specific database), while each edge represents semantic relations between two words.",2.1 DCS trees,[0],[0]
"The labels on both ends of an edge, such as SUBJ (subject) and OBJ (object), are considered as semantic roles of the cor-
responding words1.",2.1 DCS trees,[0],[0]
"To formulate the database querying process defined by a DCS tree, we provide formal semantics to DCS trees by employing relational algebra (Codd, 1970) for representing the query.",2.1 DCS trees,[0],[0]
"As described below, we represent meanings of sentences with abstract denotations, and logical relations among sentences are computed as relations among their abstract denotations.",2.1 DCS trees,[0],[0]
"In this way, we can perform inference over formulas of relational algebra, without computing database entries explicitly.",2.1 DCS trees,[0],[0]
"Abstract denotations are formulas constructed from a minimal set of relational algebra (Codd, 1970) operators, which is already able to formulate the database queries defined by DCS trees.
",2.2 Abstract denotations,[0],[0]
"For example, the semantics of “students read books” is given by the abstract denotation:
F1 = read ∩ (studentSUBJ × bookOBJ),
where read, student and book denote sets represented by these words respectively, and wr represents the set w considered as the domain of the semantic role r (e.g. bookOBJ is the set of books considered as objects).",2.2 Abstract denotations,[0],[0]
"The operators∩ and× represent intersection and Cartesian product respectively, both borrowed from relational algebra.",2.2 Abstract denotations,[0],[0]
"It is not hard to see the abstract denotation denotes the intersection of the “reading” set (as illustrated by the “read” table in Table 1) with the product of “student” set and “book” set, which results in the same denotation as computed by the DCS tree in Figure 1, i.e. {John reads Ulysses, . . .}.",2.2 Abstract denotations,[0],[0]
"However, the point is that F1 itself is an algebraic formula that does not depend on any concrete databases.
",2.2 Abstract denotations,[0],[0]
"Formally, we introduce the following constants:
• W : a universal set containing all entities.",2.2 Abstract denotations,[0],[0]
"1The semantic role ARG is specifically defined for denot-
ing nominal predicate.
example phrase abstract denotation / statement compound noun pet fish pet ∩ fish modification nice day day ∩ (WARG × niceMOD) temporal relation boys study at night study ∩ (boySUBJ × nightTIME) relative clause books that book ∩ πOBJ(read students read ∩(studentSUBJ ×WOBJ))",2.2 Abstract denotations,[0],[0]
quantification all men die man ⊂ πSUBJ(die) hypernym dog ⊂ animal derivation all criminals commit criminal ⊂ πSUBJ(commit∩ a crime (WSUBJ × crimeOBJ)),2.2 Abstract denotations,[0],[0]
"antonym rise ‖ fall negation no dogs are hurt dog ‖ πOBJ(hurt)
",2.2 Abstract denotations,[0],[0]
An abstract denotation is then defined as finite applications of functions on either constants or other abstract denotations.,2.2 Abstract denotations,[0],[0]
"As the semantics of DCS trees is formulated by abstract denotations, the meanings of declarative sentences are represented by statements on abstract denotations.",2.3 Statements,[0],[0]
"Statements are declarations of some relations among abstract denotations, for which we consider the following set relations:
Non-emptiness",2.3 Statements,[0],[0]
A 6= ∅: the set A is not empty.,2.3 Statements,[0],[0]
"Subsumption A ⊂ B: set A is subsumed by B.3 Roughly speaking, the relations correspond to the logical concepts satisfiability and entailment.
",2.3 Statements,[0],[0]
2If,2.3 Statements,[0],[0]
"A and B has the same dimension, q⊂(A,B) is either ∅ or {∗} (0-dimension point set), depending on if A ⊂ B.
3Using division operator, subsumption can be represented by non-emptiness, since for setsA,B of the same dimension, q⊂(A,B) 6= ∅",2.3 Statements,[0],[0]
"⇔ A ⊂ B.
Abstract denotations and statements are convenient for representing semantics of various types of expressions and linguistic knowledge.",2.3 Statements,[0],[0]
Some examples are shown in Table 2.4,2.3 Statements,[0],[0]
"Based on abstract denotations, we briefly describe our process to apply DCS to textual inference.",2.4 Logical inference on DCS,[0],[0]
"To obtain DCS trees from natural language, we use Stanford CoreNLP5 for dependency parsing (Socher et al., 2013), and convert Stanford dependencies to DCS trees by pattern matching on POS tags and dependency labels.6",2.4.1 Natural language to DCS trees,[0],[0]
"Currently we use the following semantic roles: ARG, SUBJ, OBJ, IOBJ, TIME and MOD.",2.4.1 Natural language to DCS trees,[0],[0]
The semantic role MOD is used for any restrictive modifiers.,2.4.1 Natural language to DCS trees,[0],[0]
"Determiners such as “all”, “every” and “each” trigger quantifiers, as shown in Figure 2.",2.4.1 Natural language to DCS trees,[0],[0]
"A DCS tree T = (N , E) is defined as a rooted tree, where each node σ ∈ N is labeled with a content word w(σ) and each edge (σ, σ′) ∈ E ⊂",2.4.2 DCS trees to statements,[0],[0]
N,2.4.2 DCS trees to statements,[0],[0]
"× N is labeled with a pair of semantic roles (r, r′)7.",2.4.2 DCS trees to statements,[0],[0]
Here σ is the node nearer to the root.,2.4.2 DCS trees to statements,[0],[0]
"Furthermore, for each edge (σ, σ′) we can optionally assign a quantification marker.
",2.4.2 DCS trees to statements,[0],[0]
Abstract denotation of a DCS tree can be calculated in a bottom-up manner.,2.4.2 DCS trees to statements,[0],[0]
"For example, the abstract denotation of H in Figure 2 is calculated from the leaf node Mary, and then: Node love (Mary loves): F2 = love ∩ (MarySUBJ ×WOBJ) Node animal (Animal that Mary loves): F3 = animal ∩ πOBJ(F2) Node have (Tom has an animal that Mary loves): F4 = have ∩ (TomSUBJ × (F3)OBJ).",2.4.2 DCS trees to statements,[0],[0]
"Formally, suppose the root σ of a DCS tree T has children τ1, . . .",2.4.2 DCS trees to statements,[0],[0]
", τn, and edges (σ, τ1), . . .",2.4.2 DCS trees to statements,[0],[0]
", (σ, τn) labeled by (r1, r′1), . . .",2.4.2 DCS trees to statements,[0],[0]
", (rn, r′n), respectively.",2.4.2 DCS trees to statements,[0],[0]
"The abstract denotation of T is defined as:
",2.4.2 DCS trees to statements,[0],[0]
[[T ]]=w(σ) ∩ ( n⋂ i=1,2.4.2 DCS trees to statements,[0],[0]
ιri(πr′i([[Tτi,2.4.2 DCS trees to statements,[0],[0]
"]]))×WRσ\ri),
4Negation and disjointness (“‖”) are explained in §2.5.",2.4.2 DCS trees to statements,[0],[0]
"5http://nlp.stanford.edu/software/
corenlp.shtml",2.4.2 DCS trees to statements,[0],[0]
6In,2.4.2 DCS trees to statements,[0],[0]
"(Liang et al., 2011)",2.4.2 DCS trees to statements,[0],[0]
DCS trees are learned from QA pairs and database entries.,2.4.2 DCS trees to statements,[0],[0]
"We obtain DCS trees from dependency trees, to bypass the need of a concrete database.
",2.4.2 DCS trees to statements,[0],[0]
"7The definition differs slightly from the original Liang et al. (2011), mainly for the sake of simplicity and clarity.
where Tτi is the subtree of T rooted at τi, and Rσ is the set of possible semantic roles for content word w(σ)",2.4.2 DCS trees to statements,[0],[0]
"(e.g. Rlove = {SUBJ,OBJ}), and WRσ\ri is the product of W which has dimension Rσ \ ri (e.g. W{SUBJ,OBJ}\SUBJ = WOBJ).
",2.4.2 DCS trees to statements,[0],[0]
"When universal quantifiers are involved, we need to add division operators to the formula.",2.4.2 DCS trees to statements,[0],[0]
"If (σ, τi) is assigned by a quantification marker “⊂”8, then the abstract denotation is9
[[T ]]=qri⊂ (πRσ\{r1,...,ri−1}([[T ′]]), πr′i([[Tτi ]])), where T ′ is the same tree as T except that the edge (σ, τi) is removed.",2.4.2 DCS trees to statements,[0],[0]
"For example, the abstract denotation of the first sentence of T in Figure 2 (Mary loves every dog) is calculated from F2 (Mary loves) as
F5 = qOBJ⊂",2.4.2 DCS trees to statements,[0],[0]
"(πOBJ(F2),dog).
",2.4.2 DCS trees to statements,[0],[0]
"After the abstract denotation [[T ]] is calculated, the statement representing the meaning of the sentence is defined as [[T ]] 6= ∅.",2.4.2 DCS trees to statements,[0],[0]
"For example, the statement of “students read books” is read ∩ (studentSUBJ × bookOBJ) 6=",2.4.2 DCS trees to statements,[0],[0]
"∅, and the statement of “Mary loves every dog” is qOBJ⊂ (πOBJ(F2),dog) 6=",2.4.2 DCS trees to statements,[0],[0]
"∅, which is logically equivalent to dog ⊂ πOBJ(F2).10",2.4.2 DCS trees to statements,[0],[0]
"Since meanings of sentences are represented by statements on abstract denotations, logical inference among sentences is reduced to deriving new relations among abstract denotations.",2.4.3 Logical inference,[0],[0]
"This is done by applying axioms to known statements, and approximately 30 axioms are implemented (Table 3).
8Multiple quantifiers can be processed similarly.",2.4.3 Logical inference,[0],[0]
"9The result of [[T ]] depends on the order of the children τ1, . . .",2.4.3 Logical inference,[0],[0]
", τn.",2.4.3 Logical inference,[0],[0]
"Different orders correspond to readings of different quantifier scopes.
",2.4.3 Logical inference,[0],[0]
"10See Footnote 2,3.
",2.4.3 Logical inference,[0],[0]
"These are algebraic properties of abstract denotations, among which we choose a set of axioms that can be handled efficiently and enable most common types of inference seen in natural language.
",2.4.3 Logical inference,[0],[0]
"For the example in Figure 2, by constructing the following abstract denotations:
Tom has a dog: F6 = have ∩ (TomSUBJ × dogOBJ) Objects that Tom has: F7 = πOBJ(have ∩ (TomSUBJ ×WOBJ)), we can use the lexical knowledge dog ⊂ animal, the statements of T (i.e. dog ⊂ πOBJ(F2) and F6 6= ∅), and the axioms in Table 3,11 to prove the statement of H (i.e. F4 6= ∅) (Figure 3).
",2.4.3 Logical inference,[0],[0]
We built an inference engine to perform logical inference on abstract denotations as above.,2.4.3 Logical inference,[0],[0]
"In this logical system, we treat abstract denotations as terms and statements as atomic sentences, which are far more easier to handle than first order predicate logic (FOL) formulas.",2.4.3 Logical inference,[0],[0]
"Furthermore, all implemented axioms are horn clauses, hence we can employ forward-chaining, which is very efficient.",2.4.3 Logical inference,[0],[0]
"Further extensions of our framework are made to deal with additional linguistic phenomena, as briefly explained below.
",2.5 Extensions,[0],[0]
"Negation To deal with negation in our forwardchaining inference engine, we introduce one more relation on abstract denotations, namely disjointness A ‖ B, meaning that A and B are disjoint sets.",2.5 Extensions,[0],[0]
"Using disjointness we implemented two types of negations: (i) atomic negation, for each content word w we allow negation w̄ of that word, characterized by the property w ‖ w̄; and (ii) root negation, for a DCS tree T and its denotation",2.5 Extensions,[0],[0]
"[[T ]], the negation of T is represented by T ‖ T , meaning that T = ∅ in its effect.",2.5 Extensions,[0],[0]
"Selection Selection operators in relational algebra select a subset from a set to satisfy some spe-
11Algebraic identities, such as πOBJ(F4) = F3 ∩ F7 and πOBJ(F6) = dog ∩ F7, are also axioms.
cific properties.",2.5 Extensions,[0],[0]
This can be employed to represent linguistic phenomena such as downward monotonicity and generalized quantifiers.,2.5 Extensions,[0],[0]
"In the current system, we implement (i) superlatives, e.g. shighest(mountain∩ (WARG×AsiaMOD)) (the highest mountain in Asia) and (ii) numerics, e.g. stwo(pet ∩ fish) (two pet fish), where sf is a selection marker.",2.5 Extensions,[0],[0]
"Selection operators are implemented as markers assigned to abstract denotations, with specially designed axioms.",2.5 Extensions,[0],[0]
For example superlatives satisfy the following property: A ⊂ B & shighest(B) ⊂,2.5 Extensions,[0],[0]
A ⇒ shighest(B) = shighest(A).,2.5 Extensions,[0],[0]
"New rules can be added if necessary.
",2.5 Extensions,[0],[0]
"Coreference We use Stanford CoreNLP to resolve coreferences (Raghunathan et al., 2010), whereas coreference is implemented as a special type of selection.",2.5 Extensions,[0],[0]
"If a node σ in a DCS tree T belongs to a mention cluster m, we take the abstract denotation",2.5 Extensions,[0],[0]
"[[Tσ]] and make a selection sm([[Tσ]]), which is regarded as the abstract denotation of that mention.",2.5 Extensions,[0],[0]
Then all selections of the same mention cluster are declared to be equal.,2.5 Extensions,[0],[0]
"Recognizing textual entailment (RTE) is the task of determining whether a given textual statement H can be inferred by a text passage T. For this, our primary textual inference system operates as:
1.",3 Generating On-the-fly Knowledge,[0],[0]
"For a T-H pair, apply dependency parsing and coreference resolution.
2.",3 Generating On-the-fly Knowledge,[0],[0]
"Perform rule-based conversion from dependency parses to DCS trees, which are translated to statements on abstract denotations.
3.",3 Generating On-the-fly Knowledge,[0],[0]
"Use statements of T and linguistic knowledge as premises, and try to prove statements of H by our inference engine.
",3 Generating On-the-fly Knowledge,[0],[0]
"However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs “no entailment” for almost all pairs (Bos and Markert, 2005).
",3 Generating On-the-fly Knowledge,[0],[0]
The transparent syntax-to-semantics interface of DCS enables us to back off to NLP techniques during inference for catching up the lack of knowledge.,3 Generating On-the-fly Knowledge,[0],[0]
"We extract fragments of DCS trees as paraphrase candidates, translate them back to linguis-
tic expressions, and apply distributional similarity to judge their validity.",3 Generating On-the-fly Knowledge,[0],[0]
"In this way, our framework combines distributional and logical semantics, which is also the main subject of Lewis and Steedman (2013) and Beltagy et al. (2013).
",3 Generating On-the-fly Knowledge,[0],[0]
"As follows, our full system (Figure 4) additionally invokes linguistic knowledge on-the-fly:
4.",3 Generating On-the-fly Knowledge,[0],[0]
"If H is not proven, compare DCS trees of T and H, and generate path alignments.
5.",3 Generating On-the-fly Knowledge,[0],[0]
Aligned paths are evaluated by a similarity score to estimate their likelihood of being paraphrases.,3 Generating On-the-fly Knowledge,[0],[0]
"Path alignments with scores higher than a threshold are accepted.
6.",3 Generating On-the-fly Knowledge,[0],[0]
"Convert accepted path alignments into statements on abstract denotations, use them in logical inference as new knowledge, and try to prove H again.",3 Generating On-the-fly Knowledge,[0],[0]
On-the-fly knowledge is generated by aligning paths in DCS trees.,3.1 Generating path alignments,[0],[0]
"A path is considered as joining two germs in a DCS tree, where a germ is defined as a specific semantic role of a node.",3.1 Generating path alignments,[0],[0]
"For example, Figure 5 shows DCS trees of the following sentences (a simplified pair from RTE2-dev):",3.1 Generating path alignments,[0],[0]
The germ OBJ(blame) and germ ARG(death) in DCS tree of T are joined by the underscored path.,H: A storm has caused loss of life.,[0],[0]
"Two paths are aligned if the joined germs are aligned, and we impose constraints on aligned germs to inhibit meaningless alignments, as described below.",H: A storm has caused loss of life.,[0],[0]
"Two germs are aligned if they are both at leaf nodes (e.g. ARG(death) in T and ARG(life) in H, Figure 5), or they already have part of their meanings in common, by some logical clues.
",3.2 Aligning germs by logical clues,[0],[0]
"To formulate this properly, we define the abstract denotation of a germ, which, intuitively, represents the meaning of the germ in the specific sentence.",3.2 Aligning germs by logical clues,[0],[0]
"The abstract denotation of a germ is defined in a top-down manner: for the root node ρ of a DCS tree T , we define its denotation",3.2 Aligning germs by logical clues,[0],[0]
[[ρ]]T as the denotation of the entire tree,3.2 Aligning germs by logical clues,[0],[0]
"[[T ]]; for a non-root node τ and its parent node σ, let the edge (σ, τ) be labeled by semantic roles (r, r′), then define
[[τ ]]T =",3.2 Aligning germs by logical clues,[0],[0]
[[Tτ ]] ∩ (ιr′(πr([[σ]]T )),3.2 Aligning germs by logical clues,[0],[0]
"×WRτ\r′).
",3.2 Aligning germs by logical clues,[0],[0]
"Now for a germ r(σ), the denotation is defined as the projection of the denotation of node σ onto the specific semantic role r:",3.2 Aligning germs by logical clues,[0],[0]
"[[r(σ)]]T = πr([[σ]]T ).
",3.2 Aligning germs by logical clues,[0],[0]
"For example, the abstract denotation of germ ARG(book) in Figure 1 is defined as πARG(book∩ πOBJ(read∩(studentSUBJ×bookOBJ))), meaning “books read by students”.",3.2 Aligning germs by logical clues,[0],[0]
"Similarly, denotation of germ OBJ(blame) in T of Figure 5 indicates the object of “blame” as in the sentence “Tropical storm Debby is blamed for death”, which is a tropical storm, is Debby, etc.",3.2 Aligning germs by logical clues,[0],[0]
"Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al., 2011) of that variable.
",3.2 Aligning germs by logical clues,[0],[0]
"The logical clue to align germs is: if there exists an abstract denotation, other than W , that is a superset of both abstract denotations of two germs, then the two germs can be aligned.",3.2 Aligning germs by logical clues,[0],[0]
"A simple example is that ARG(storm) in T can be aligned to ARG(storm) in H, because their denotations have a common superset other than W , namely πARG(storm).",3.2 Aligning germs by logical clues,[0],[0]
"A more complicated example is that OBJ(blame) and SUBJ(cause) can be aligned, because inference can induce [[OBJ(blame)]]T = [[ARG(Debby)]]T =",3.2 Aligning germs by logical clues,[0],[0]
"[[ARG(storm)]]T, as well as [[SUBJ(cause)]]H =",3.2 Aligning germs by logical clues,[0],[0]
"[[ARG(storm)]]H, so they also have the common superset πARG(storm).",3.2 Aligning germs by logical clues,[0],[0]
"However, for example, logical clues can avoid aligning ARG(storm) to ARG(loss), which is obviously
meaningless.",3.2 Aligning germs by logical clues,[0],[0]
"Aligned paths are evaluated by a similarity score, for which we use distributional similarity of the words that appear in the paths (§4.1).",3.3 Scoring path alignments by similarity,[0],[0]
Only path alignments with high similarity scores can be accepted.,3.3 Scoring path alignments by similarity,[0],[0]
"Also, we only accept paths of length ≤ 5, to prevent too long paths to be aligned.",3.3 Scoring path alignments by similarity,[0],[0]
"Accepted aligned paths are converted into statements, which are used as new knowledge.",3.4 Applying path alignments,[0],[0]
"The conversion is done by first performing a DCS tree transformation according to the aligned paths, and then declare a subsumption relation between the denotations of aligned germs.",3.4 Applying path alignments,[0],[0]
"For example, to apply the aligned path pair generated in Figure 5, we use it to transform T into a new tree T’ (Figure 6), and then the aligned germs, OBJ(blame) in T and SUBJ(cause) in T’, will generate the on-the-fly knowledge:",3.4 Applying path alignments,[0],[0]
[[OBJ(blame)]]T ⊂,3.4 Applying path alignments,[0],[0]
"[[SUBJ(cause)]]T’.
Similar to the tree transformation based approach to RTE (Bar-Haim et al., 2007), this process can also utilize lexical-syntactic entailment rules (Szpektor et al., 2007).",3.4 Applying path alignments,[0],[0]
"Furthermore, since the on-the-fly knowledge is generated by transformed pairs of DCS trees, all contexts are preserved: in Figure 6, though the tree transformation can be seen as generated from the entailment rule “X is blamed for death→ X causes loss of life”, the generated on-the-fly knowledge, as shown above the trees, only fires with the additional condition that X is a tropical storm and is Debby.",3.4 Applying path alignments,[0],[0]
"Hence, the process can also be used to generate knowledge from context sensitive rules (Melamud et al., 2013), which are known to have higher quality (Pantel et al., 2007; Clark and Harrison, 2009).
",3.4 Applying path alignments,[0],[0]
"However, it should be noted that using on-thefly knowledge in logical inference is not a trivial
task.",3.4 Applying path alignments,[0],[0]
"For example, the FOL formula of the rule “X is blamed for death→ X causes loss of life” is:
∀x; (∃a; blame(x, a) & death(a))→ (∃b, c; cause(x, b) & loss(b, c) & life(c)),
which is not a horn clause.",3.4 Applying path alignments,[0],[0]
The FOL formula for the context-preserved rule in Figure 6 is even more involved.,3.4 Applying path alignments,[0],[0]
"Still, it can be efficiently treated by our inference engine because as a statement, the formula",3.4 Applying path alignments,[0],[0]
"[[OBJ(blame)]]T ⊂ [[SUBJ(cause)]]T’ is an atomic sentence, more than a horn clause.",3.4 Applying path alignments,[0],[0]
"In this section, we evaluate our system on FraCaS (§4.2) and PASCAL RTE datasets (§4.3).",4 Experiments,[0],[0]
"The lexical knowledge we use are synonyms, hypernyms and antonyms extracted from WordNet12.",4.1 Language Resources,[0],[0]
"We also add axioms on named entities, stopwords, numerics and superlatives.",4.1 Language Resources,[0],[0]
"For example, named entities are singletons, so we add axioms such as ∀x; (x ⊂ Tom & x 6= ∅)→",4.1 Language Resources,[0],[0]
"Tom ⊂ x.
To calculate the similarity scores of path alignments, we use the sum of word vectors of the words from each path, and calculate the cosine similarity.",4.1 Language Resources,[0],[0]
"For example, the similarity score of the path alignment “OBJ(blame)IOBJ-ARG(death)",4.1 Language Resources,[0],[0]
≈ SUBJ(cause)OBJ-ARG(loss)MOD-ARG(life)” is calculated as the cosine similarity of vectors blame+death and cause+loss+life.,4.1 Language Resources,[0],[0]
"Other structures in the paths, such as semantic roles, are ignored in the calculation.",4.1 Language Resources,[0],[0]
"The word vectors we use are from Mikolov et al. (2013)13 (Mikolov13), and additional results are also shown using Turian et al. (2010)14 (Turian10).",4.1 Language Resources,[0],[0]
"The threshold for accepted path alignments is set to 0.4, based on preexperiments on RTE development sets.",4.1 Language Resources,[0],[0]
"The FraCaS test suite contains 346 inference problems divided into 9 sections, each focused on a category of semantic phenomena.",4.2 Experiments on FraCaS,[0],[0]
"We use the data by MacCartney and Manning (2007), and experiment on the first section, Quantifiers, following Lewis and Steedman (2013).",4.2 Experiments on FraCaS,[0],[0]
This section has 44 single premise and 30 multi premise problems.,4.2 Experiments on FraCaS,[0],[0]
"Most of
12http://wordnet.princeton.edu/ 13http://code.google.com/p/word2vec/ 14http://metaoptimize.com/projects/
wordreprs/
the problems do not require lexical knowledge, so we use our primary textual inference system without on-the-fly knowledge nor WordNet, to test the performance of the DCS framework as formal semantics.",4.2 Experiments on FraCaS,[0],[0]
"To obtain the three-valued output (i.e. yes, no, and unknown), we output “yes” if H is proven, or try to prove the negation of H if H is not proven.",4.2 Experiments on FraCaS,[0],[0]
"To negate H, we use the root negation as described in §2.5.",4.2 Experiments on FraCaS,[0],[0]
"If the negation of H is proven, we output “no”, otherwise we output “unknown”.
",4.2 Experiments on FraCaS,[0],[0]
The result is shown in Table 4.,4.2 Experiments on FraCaS,[0],[0]
"Since our system uses an off-the-shelf dependency parser, and semantic representations are obtained from simple rule-based conversion from dependency trees, there will be only one (right or wrong) interpretation in face of ambiguous sentences.",4.2 Experiments on FraCaS,[0],[0]
"Still, our system outperforms Lewis and Steedman (2013)’s probabilistic CCG-parser.",4.2 Experiments on FraCaS,[0],[0]
"Compared to MacCartney and Manning (2007) and MacCartney and Manning (2008), our system does not need a pretrained alignment model, and it improves by making multi-sentence inferences.",4.2 Experiments on FraCaS,[0],[0]
"To sum up, the result shows that DCS is good at handling universal quantifiers and negations.
",4.2 Experiments on FraCaS,[0],[0]
Most errors are due to wrongly generated DCS trees (e.g. wrongly assigned semantic roles) or unimplemented quantifier triggers (e.g. “neither”) or generalized quantifiers (e.g. “at least a few”).,4.2 Experiments on FraCaS,[0],[0]
These could be addressed by future work.,4.2 Experiments on FraCaS,[0],[0]
"On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting.",4.3 Experiments on PASCAL RTE datasets,[0],[0]
"We test the effect of on-the-fly knowledge on RTE2, RTE3, RTE4 and RTE5 datasets, and compare our system with other approaches.",4.3 Experiments on PASCAL RTE datasets,[0],[0]
Results on test data are shown in Table 5.,4.3.1 Impact of on-the-fly knowledge,[0],[0]
"When only primary knowledge is used in inference (the first row), recalls are actually very low; After we activate the on-the-fly knowledge, recalls jump to over 50%, with a moderate fall of precision.",4.3.1 Impact of on-the-fly knowledge,[0],[0]
"As a result, accuracies significantly increase.",4.3.1 Impact of on-the-fly knowledge,[0],[0]
A comparison between our system and other RTE systems is shown in Table 6.,4.3.2 Comparison to other RTE systems,[0],[0]
"Bos06 (Bos and Markert, 2006) is a hybrid system combining deep features from a theorem prover and a model builder, together with shallow features such as lexical overlap and text length.",4.3.2 Comparison to other RTE systems,[0],[0]
"MacCartney08 (MacCartney and Manning, 2008) uses natural logic to calculate inference relations between two superficially aligned sentences.",4.3.2 Comparison to other RTE systems,[0],[0]
"Clark08 (Clark and Harrison, 2008) is a logic-based system utilizing various resources including WordNet and DIRT paraphrases (Lin and Pantel, 2001), and is tolerant to partially unproven H sentences in some degree.",4.3.2 Comparison to other RTE systems,[0],[0]
"All of the three systems pursue a logical approach, while combining various techniques to achieve robustness.",4.3.2 Comparison to other RTE systems,[0],[0]
The result shows that our system has comparable performance.,4.3.2 Comparison to other RTE systems,[0],[0]
"On the other hand, Wang10 (Wang and Manning, 2010) learns a treeedit model from training data, and captures entailment relation by tree edit distance.",4.3.2 Comparison to other RTE systems,[0],[0]
"Stern11 (Stern and Dagan, 2011) and Stern12 (Stern et al., 2012) extend this framework to utilize entailment rules as tree transformations.",4.3.2 Comparison to other RTE systems,[0],[0]
These are more tailored systems using machine learning with many handcrafted features.,4.3.2 Comparison to other RTE systems,[0],[0]
"Still, our unsupervised system outperforms the state-of-the-art on RTE5 dataset.",4.3.2 Comparison to other RTE systems,[0],[0]
"Summing up test data from RTE2 to RTE5, Figure 7 shows the proportion of all proven pairs and their precision.",4.3.3 Analysis,[0],[0]
"Less than 5% pairs can be proven primarily, with a precision of 77%.",4.3.3 Analysis,[0],[0]
"Over 40% pairs can be proven by one piece of on-the-fly knowledge, yet pairs do exist in which more than 2 pieces are necessary.",4.3.3 Analysis,[0],[0]
"The precisions of 1 and 2 pieces on-the-fly knowledge application are over
60%, which is fairly high, given our rough estimation of the similarity score.",4.3.3 Analysis,[0],[0]
"As a comparison, Dinu and Wang (2009) studied the proportion of proven pairs and precision by applying DIRT rules to tree skeletons in RTE2 and RTE3 data.",4.3.3 Analysis,[0],[0]
"The proportion is 8% with precision 65% on RTE2, and proportion 6% with precision 72% on RTE3.",4.3.3 Analysis,[0],[0]
"Applied by our logical system, the noisy on-the-fly knowledge can achieve a precision comparable to higher quality resources such as DIRT.
",4.3.3 Analysis,[0],[0]
A major type of error is caused by the ignorance of semantic roles in calculation of similarity scores.,4.3.3 Analysis,[0],[0]
"For example, though “Italy beats Kazakhstan” is not primarily proven from “Italy is defeated by Kazakhstan”, our system does produce the path alignment “SUBJ(beat)OBJ ≈ OBJ(defeat)SUBJ” with a high similarity score.",4.3.3 Analysis,[0],[0]
"The impact of such errors depends on the data making methodology, though.",4.3.3 Analysis,[0],[0]
"It lowers precisions in RTE2 and RTE3 data, particularly in “IE” subtask (where precisions drop under 0.5).",4.3.3 Analysis,[0],[0]
"On the other hand, it occurs less often in “IR” subtask.
",4.3.3 Analysis,[0],[0]
"Finally, to see if we “get lucky” on RTE5 data in the choice of word vectors and thresholds, we change the thresholds from 0.1 to 0.7 and draw the precision-recall curve, using two types of word vectors, Mikolov13 and Turian10.",4.3.3 Analysis,[0],[0]
"As shown in Figure 8, though the precision drops for Turian10, both curves show the pattern that our system keeps gaining recall while maintaining precision to a certain level.",4.3.3 Analysis,[0],[0]
"Not too much “magic” in Mikolov13 actually: for over 80% pairs, every node in DCS tree of H can be covered by a path of length ≤ 5 that
has a corresponding path of length ≤ 5 in T with a similarity score > 0.4.",4.3.3 Analysis,[0],[0]
"We have presented a method of deriving abstract denotation from DCS trees, which enables logical inference on DCS, and we developed a textual inference system based on the framework.",5 Conclusion and Discussion,[0],[0]
"Experimental results have shown the power of the representation that allows both strict inference as on FraCaS data and robust reasoning as on RTE data.
",5 Conclusion and Discussion,[0],[0]
Exploration of an appropriate meaning representation for querying and reasoning on knowledge bases has a long history.,5 Conclusion and Discussion,[0],[0]
"Description logic, being less expressive than FOL but featuring more efficient reasoning, is used as a theory base for Semantic Web (W3C, 2012).",5 Conclusion and Discussion,[0],[0]
"Ideas similar to our framework, including the use of sets in a representation that benefits efficient reasoning, are also found in description logic and knowledge representation community (Baader et al., 2003; Sowa, 2000; Sukkarieh, 2003).",5 Conclusion and Discussion,[0],[0]
"To our knowledge, however, their applications to logical inference beyond the use for database querying have not been much explored in the context of NLP.
",5 Conclusion and Discussion,[0],[0]
The pursue of a logic more suitable for natural language inference is not new.,5 Conclusion and Discussion,[0],[0]
"For instance, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970).",5 Conclusion and Discussion,[0],[0]
"While being computationally efficient, various inference patterns are out of the scope of their system.
",5 Conclusion and Discussion,[0],[0]
"Much work has been done in mapping natural language into database queries (Cai and Yates, 2013; Kwiatkowski et al., 2013; Poon, 2013).",5 Conclusion and Discussion,[0],[0]
"Among these, the (λ-)DCS (Liang et al., 2011; Berant et al., 2013) framework defines algorithms that transparently map a labeled tree to a database querying procedure.",5 Conclusion and Discussion,[0],[0]
"Essentially, this is because DCS trees restrict the querying process to a very limited subset of possible operations.",5 Conclusion and Discussion,[0],[0]
"Our main contribution, the abstract denotation of DCS trees,
can thus be considered as an attempt to characterize a fragment of FOL that is suited for both natural language inference and transparent syntaxsemantics mapping, through the choice of operations and relations on sets.
",5 Conclusion and Discussion,[0],[0]
We have demonstrated the utility of logical inference on DCS through the RTE task.,5 Conclusion and Discussion,[0],[0]
"A wide variety of strategies tackling the RTE task have been investigated (Androutsopoulos and Malakasiotis, 2010), including the comparison of surface strings (Jijkoun and De Rijke, 2005), syntactic and semantic structures (Haghighi et al., 2005; Snow et al., 2006; Zanzotto et al., 2009; Burchardt et al., 2009; Heilman and Smith, 2010; Wang and Manning, 2010), semantic vectors (Erk and Padó, 2009) and logical representations (Bos and Markert, 2005; Raina et al., 2005; Tatu and Moldovan, 2005).",5 Conclusion and Discussion,[0],[0]
"Acquisition of basic knowledge for RTE is also a huge stream of research (Lin and Pantel, 2001; Shinyama et al., 2002; Sudo et al., 2003; Szpektor et al., 2004; Fujita et al., 2012; Weisman et al., 2012; Yan et al., 2013).",5 Conclusion and Discussion,[0],[0]
"These previous works include various techniques for acquiring and incorporating different kinds of linguistic and world knowledge, and further fight against the knowledge bottleneck problem, e.g. by back-off to shallower representations.
",5 Conclusion and Discussion,[0],[0]
Logic-based RTE systems employ various approaches to bridge knowledge gaps.,5 Conclusion and Discussion,[0],[0]
"Bos and Markert (2005) proposes features from a model builder; Raina et al. (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system.
",5 Conclusion and Discussion,[0],[0]
"As such, our current RTE system is at a proofof-concept stage, in that many of the above techniques are yet to be implemented.",5 Conclusion and Discussion,[0],[0]
"Nonetheless, we would like to emphasize that it already shows performance competitive to state-of-the-art systems on one data set (RTE5).",5 Conclusion and Discussion,[0],[0]
Other directions of our future work include further exploitation of the new semantic representation.,5 Conclusion and Discussion,[0],[0]
"For example, since abstract denotations are readily suited for data querying, they can be used to verify newly generated assumptions by fact search in a database.",5 Conclusion and Discussion,[0],[0]
"This may open a way towards a hybrid approach to RTE wherein logical inference is intermingled with large scale database querying.
",5 Conclusion and Discussion,[0],[0]
Acknowledgments This research was supported by the Todai Robot Project at National Institute of Informatics.,5 Conclusion and Discussion,[0],[0]
Dependency-based Compositional Semantics (DCS) is a framework of natural language semantics with easy-to-process structures as well as strict semantics.,abstractText,[0],[0]
"In this paper, we equip the DCS framework with logical inference, by defining abstract denotations as an abstraction of the computing process of denotations in original DCS.",abstractText,[0],[0]
An inference engine is built to achieve inference on abstract denotations.,abstractText,[0],[0]
"Furthermore, we propose a way to generate on-the-fly knowledge in logical inference, by combining our framework with the idea of tree transformation.",abstractText,[0],[0]
Experiments on FraCaS and PASCAL RTE datasets show promising results.,abstractText,[0],[0]
Logical Inference on Dependency-based Compositional Semantics,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–739 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
732",text,[0],[0]
"Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) has become the de-facto recurrent neural network (RNN) for learning representations of sequences in NLP.",1 Introduction,[0],[0]
"Like simple recurrent neural networks (S-RNNs) (Elman, 1990), LSTMs are able to learn non-linear functions of arbitrary-length input sequences.",1 Introduction,[0],[0]
"However, they also introduce an additional memory cell to mitigate the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994).",1 Introduction,[0],[0]
"This memory is controlled by a mechanism of gates, whose additive connections allow long-distance dependencies to be learned more easily during backpropagation.",1 Introduction,[0],[0]
"While this view is mathematically accurate, in this paper we argue that it does not provide a complete picture of why LSTMs work in practice.
",1 Introduction,[0],[0]
"∗The first two authors contributed equally to this paper.
",1 Introduction,[0],[0]
We present an alternate view to explain the success of LSTMs: the gates themselves are powerful recurrent models that provide more representational power than previously realized.,1 Introduction,[0],[0]
"To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN’s outputs over time, which is implicitly computed by the gates.",1 Introduction,[0],[0]
"We hypothesize that, for many practical NLP problems, the weighted sum serves as the main modeling component.",1 Introduction,[0],[0]
"The SRNN, while theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model.",1 Introduction,[0],[0]
"By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing.
",1 Introduction,[0],[0]
"We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al., 2017).",1 Introduction,[0],[0]
"We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation.",1 Introduction,[0],[0]
"Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance.",1 Introduction,[0],[0]
"We also show that, in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum.",1 Introduction,[0],[0]
"Together, these results suggest that the gates’ ability to compute an element-wise weighted sum, rather than the non-linear transition dynamics of S-RNNs, are the driving force behind LSTM’s success.",1 Introduction,[0],[0]
"LSTMs are typically motivated as an augmentation of simple RNNs (S-RNNs), defined as:
ht = tanh(Whhht−1",2 What Do Memory Cells Compute?,[0],[0]
"+Whxxt + bh) (1)
S-RNNs suffer from the vanishing gradient problem (Hochreiter, 1991; Bengio et al., 1994) due to compounding multiplicative updates of the hidden state.",2 What Do Memory Cells Compute?,[0],[0]
"By introducing a memory cell and an output layer controlled by gates, LSTMs enable shortcuts through which gradients can flow when learning with backpropagation.",2 What Do Memory Cells Compute?,[0],[0]
"This mechanism enables learning of long-distance dependencies while preserving the expressive power of recurrent nonlinear transformations provided by S-RNNs.
",2 What Do Memory Cells Compute?,[0],[0]
"Rather than viewing the gates as simply an auxiliary mechanism to address a learning problem, we present an alternate view that emphasizes their modeling strengths.",2 What Do Memory Cells Compute?,[0],[0]
"We argue that the LSTM should be interpreted as a hybrid of two distinct recurrent architectures: (1) the S-RNN which provides multiplicative connections across timesteps, and (2) the memory cell which provides additive connections across timesteps.",2 What Do Memory Cells Compute?,[0],[0]
"On top of these recurrences, an output layer is included that simply squashes and filters the memory cell at each step.
",2 What Do Memory Cells Compute?,[0],[0]
"Throughout this paper, let {x1, . . .",2 What Do Memory Cells Compute?,[0],[0]
",xn} be the sequence of input vectors, {h1, . . .",2 What Do Memory Cells Compute?,[0],[0]
",hn} be the sequence of output vectors, and {c1, . . .",2 What Do Memory Cells Compute?,[0],[0]
", cn} be the memory cell’s states.",2 What Do Memory Cells Compute?,[0],[0]
"Then, given the basic LSTM definition below, we can formally identify three sub-components.
",2 What Do Memory Cells Compute?,[0],[0]
"c̃t = tanh(Wchht−1 +Wcxxt + bc) (2)
it = σ(Wihht−1 +Wixxt + bi) (3)
ft = σ(Wfhht−1 +Wfxxt + bf )",2 What Do Memory Cells Compute?,[0],[0]
(4) ct = it ◦,2 What Do Memory Cells Compute?,[0],[0]
"c̃t + ft ◦ ct−1 (5) ot = σ(Wohht−1 +Woxxt + bo) (6) ht = ot ◦ tanh(ct) (7)
Content Layer (Equation 2)",2 What Do Memory Cells Compute?,[0],[0]
"We refer to c̃t as the content layer, which is the output of an SRNN.",2 What Do Memory Cells Compute?,[0],[0]
Evaluating the need for multiplicative recurrent connections in the content layer is the focus of this work.,2 What Do Memory Cells Compute?,[0],[0]
"The content layer is passed to the memory cell, which decides which parts of it to store.
",2 What Do Memory Cells Compute?,[0],[0]
Memory Cell (Equations 3-5),2 What Do Memory Cells Compute?,[0],[0]
The memory cell ct is controlled by two gates.,2 What Do Memory Cells Compute?,[0],[0]
"The input gate it controls what part of the content (c̃t) is written to the memory, while the forget gate ft controls
what part of the memory is deleted by filtering the previous state of the memory (ct−1).",2 What Do Memory Cells Compute?,[0],[0]
"Writing to the memory is done by adding the filtered content (it ◦ c̃t) to the retained memory (ft ◦ ct−1).
",2 What Do Memory Cells Compute?,[0],[0]
Output Layer (Equations 6-7),2 What Do Memory Cells Compute?,[0],[0]
"The output layer ht passes the memory cell through a tanh activation function and uses an output gate ot to read selectively from the squashed memory cell.
",2 What Do Memory Cells Compute?,[0],[0]
Our goal is to study how much each of these components contribute to the empirical performance of LSTMs.,2 What Do Memory Cells Compute?,[0],[0]
"In particular, it is worth considering the memory cell in more detail to reveal why it could serve as a standalone powerful model of long-distance context.",2 What Do Memory Cells Compute?,[0],[0]
"It is possible to show that it implicitly computes an element-wise weighted sum of all the previous content layers by expanding the recurrence relation in Equation 5:
ct = it ◦",2 What Do Memory Cells Compute?,[0],[0]
"c̃t + ft ◦ ct−1
= t∑ j=0",2 What Do Memory Cells Compute?,[0],[0]
( ij ◦,2 What Do Memory Cells Compute?,[0],[0]
t∏ k=j+1 fk ) ◦,2 What Do Memory Cells Compute?,[0],[0]
"c̃j
= t∑
j=0
wtj ◦ c̃j
(8)
Each weight wtj is a product of the input gate ij (when its respective input c̃j was read) and every subsequent forget gate fk.",2 What Do Memory Cells Compute?,[0],[0]
"An interesting property of these weights is that, like the gates, they are also soft element-wise binary filters.",2 What Do Memory Cells Compute?,[0],[0]
"The restricted space of element-wise weighted sums allows for easier mathematical analysis, visualization, and perhaps even learnability.",3 Standalone Memory Cells are Powerful,[0],[0]
"However, constrained function spaces are also less expressive, and a natural question is whether these models will work well for NLP problems that involve understanding context.",3 Standalone Memory Cells are Powerful,[0],[0]
We hypothesize that the memory cell (which computes weighted sums) can function as a standalone contextualizer.,3 Standalone Memory Cells are Powerful,[0],[0]
"To test this hypothesis, we present several simplifications of the LSTM’s architecture (Section 3.1), and show on a variety of NLP benchmarks that there is a qualitative performance difference between models that contain a memory cell and those that do not (Section 3.2).",3 Standalone Memory Cells are Powerful,[0],[0]
"We conclude that the content and output layers are relatively minor contributors, and that the space of element-wise weighted sums is sufficiently powerful to compete with fully parameterized LSTMs (Section 3.3).",3 Standalone Memory Cells are Powerful,[0],[0]
"The modeling power of LSTMs is commonly assumed to derive from the S-RNN in the content layer, with the rest of the model acting as a learning aid to bypass the vanishing gradient problem.",3.1 Simplified Models,[0],[0]
"We first isolate the S-RNN by ablating the gates (denoted as LSTM – GATES for consistency).
",3.1 Simplified Models,[0],[0]
"To test whether the memory cell has enough modeling power of its own, we take an LSTM and replace the S-RNN in the content layer from Equation 2 with a simple linear transformation (c̃t = Wcxxt) creating the LSTM – S-RNN model.
",3.1 Simplified Models,[0],[0]
"We further simplify the LSTM by removing the output gate from Equation 7 (ht = tanh(ct)), leaving only the activation function in the output layer (LSTM – S-RNN – OUT).",3.1 Simplified Models,[0],[0]
"After removing the S-RNN and the output gate from the LSTM, the entire ablated model can be written in a modular, compact form:
ht = OUTPUT ( t∑
j=0
wtj ◦ CONTENT(xj) )",3.1 Simplified Models,[0],[0]
"(9)
where the content layer CONTENT(·) and the output layer OUTPUT(·) are both context-independent functions, making the entire model highly constrained and mathematically simpler.",3.1 Simplified Models,[0],[0]
The complexity of modeling contextual information is needed only for computing the weights wtj .,3.1 Simplified Models,[0],[0]
"As we will see in Section 3.2, both of these ablations perform on par with LSTMs on several tasks.
",3.1 Simplified Models,[0],[0]
"Finally, we ablate the hidden state from the gates as well, by computing each gate gt via σ(Wgxxt+bg).",3.1 Simplified Models,[0],[0]
"In this model, the only recurrence is the additive connection in the memory cell; it has no multiplicative recurrent connections at all.",3.1 Simplified Models,[0],[0]
"It can be seen as a type of QRNN (Bradbury et al., 2016) or SRU (Lei et al., 2017b), but for consistency we label it as LSTM – S-RNN – HIDDEN.",3.1 Simplified Models,[0],[0]
"We compare model performance on four NLP tasks, with an experimental setup that is lenient towards LSTMs and harsh towards its simplifications.",3.2 Experiments,[0],[0]
"In each case, we use existing implementations and previously reported hyperparameter settings.",3.2 Experiments,[0],[0]
"Since these settings were tuned for LSTMs, any simplification that performs equally to (or better than) LSTMs under these LSTM-friendly settings provides strong evidence that the ablated component is not a contributing factor.",3.2 Experiments,[0],[0]
"For each
task we also report the mean and standard deviation of 5 runs of the LSTM settings to demonstrate the typical variance observed due to training with different random initializations.
",3.2 Experiments,[0],[0]
"Language Modeling We evaluate the models on the Penn Treebank (PTB) (Marcus et al., 1993) language modeling benchmark.",3.2 Experiments,[0],[0]
We use the implementation of Zaremba et al. (2014) from TensorFlow’s tutorial while replacing any invocation of LSTMs with simpler models.,3.2 Experiments,[0],[0]
"We test two of their configurations: medium and large (Table 1).
",3.2 Experiments,[0],[0]
"Question Answering For question answering, we use two different QA systems on the Stanford question answering dataset (SQuAD) (Rajpurkar et al., 2016): the Bidirectional Attention Flow model (BiDAF) (Seo et al., 2016) and DrQA (Chen et al., 2017).",3.2 Experiments,[0],[0]
"BiDAF contains 3 LSTMs, which are referred to as the phrase layer, the modeling layer, and the span end encoder.",3.2 Experiments,[0],[0]
Our experiments replace each of these LSTMs with their simplified counterparts.,3.2 Experiments,[0],[0]
"We directly use the implementation of BiDAF from AllenNLP (Gardner et al., 2017), and all experiments reuse the existing hyperparameters that were tuned for LSTMs.",3.2 Experiments,[0],[0]
"Likewise, we use an open-source implementation of DrQA1 and replace only the LSTMs, while leaving everything else intact.",3.2 Experiments,[0],[0]
"Table 2 shows the results.
",3.2 Experiments,[0],[0]
"Dependency Parsing For dependency parsing, we use the Deep Biaffine Dependency Parser (Dozat and Manning, 2016), which relies on stacked bidirectional LSTMs to learn contextsensitive word embeddings for determining arcs between a pair of words.",3.2 Experiments,[0],[0]
"We directly use their released implementation, which is evaluated on the Universal Dependencies English Web Treebank v1.3 (Silveira et al., 2014).",3.2 Experiments,[0],[0]
"In our experiments, we use the existing hyperparameters and only replace the LSTMs with the simplified architectures.",3.2 Experiments,[0],[0]
"Table 3 shows the results.
",3.2 Experiments,[0],[0]
"Machine Translation For machine translation, we used OpenNMT (Klein et al., 2017) to train English to German translation models on the multimodal benchmarks from WMT 2016 (used in OpenNMT’s readme file).",3.2 Experiments,[0],[0]
"We use OpenNMT’s default model and hyperparameters, replacing the stacked bidirectional LSTM encoder with the sim-
1https://github.com/hitvoice/DrQA
plified architectures.2 Table 4 shows the results.",3.2 Experiments,[0],[0]
We showed four major ablations of the LSTM.,3.3 Discussion,[0],[0]
"In the S-RNN experiments (LSTM – GATES), we ablate the memory cell and the output layer.",3.3 Discussion,[0],[0]
"In the LSTM – S-RNN and LSTM – S-RNN – OUT experiments, we ablate the S-RNN.",3.3 Discussion,[0],[0]
"In the LSTM – SRNN – HIDDEN, we remove not only the S-RNN in the content layer, but also the S-RNNs in the gates, resulting in a model whose sole recurrence is in the memory cell’s additive connection.
",3.3 Discussion,[0],[0]
"As consistent with previous literature, removing the memory cell degrades performance drastically.",3.3 Discussion,[0],[0]
"In contrast, removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP.
",3.3 Discussion,[0],[0]
"Even after removing every multiplicative recurrence from the memory cell itself, the model’s performance remains well above the vanilla S-
2For the S-RNN baseline (LSTM – GATES), we had to tune the learning rate to 0.1 because the default value (1.0) resulted in exploding gradients.",3.3 Discussion,[0],[0]
"This is the only case where hyperparameters were modified in all of our experiments.
",3.3 Discussion,[0],[0]
"RNN’s, and falls within the standard deviation of an LSTM’s on some tasks (see Table 3).",3.3 Discussion,[0],[0]
This latter result indicates that the additive recurrent connection in the memory cell – and not the multiplicative recurrent connections in the content layer or in the gates – is the most important computational element in an LSTM.,3.3 Discussion,[0],[0]
"As a corollary, this result also suggests that a weighted sum of context words, while mathematically simple, is a powerful model of contextual information.",3.3 Discussion,[0],[0]
"Attention mechanisms are widely used in the NLP literature to aggregate over a sequence (Cho et al., 2014; Bahdanau et al., 2015) or contextualize tokens within a sequence (Cheng et al., 2016; Parikh et al., 2016) by explicitly computing weighted sums.",4 LSTM as Self-Attention,[0],[0]
"In the previous sections, we demonstrated that LSTMs implicitly compute weighted sums as well, and that this computation is central to their success.",4 LSTM as Self-Attention,[0],[0]
"How, then, are these two computations related, and in what ways do they differ?
",4 LSTM as Self-Attention,[0],[0]
"After simplifying the content layer and removing the output gate (LSTM – S-RNN – OUT), the model’s computation can be expressed as a weighted sum of context-independent functions of the inputs (Equation 9 in Section 3.1).",4 LSTM as Self-Attention,[0],[0]
"This formula abstracts over both the simplified LSTM and the family of attention mechanisms, and through this lens, the memory cell’s computation can be seen as a “cousin” of self-attention.",4 LSTM as Self-Attention,[0],[0]
"In fact, we can also leverage this abstraction to visualize the
simplified LSTM’s weights as is commonly done with attention (see Appendix A for visualization).
",4 LSTM as Self-Attention,[0],[0]
"However, there are three major differences in how the weights wtj are computed.
",4 LSTM as Self-Attention,[0],[0]
"First, the LSTM’s weights are vectors, while attention typically computes scalar weights; i.e. a separate weighted sum is computed for every dimension of the LSTM’s memory cell.",4 LSTM as Self-Attention,[0],[0]
"Multiheaded self-attention (Vaswani et al., 2017) can be seen as a middle ground between the two approaches, allocating a scalar weight for different subsets of the dimensions.
",4 LSTM as Self-Attention,[0],[0]
"Second, the weighted sum is accumulated with a dynamic program.",4 LSTM as Self-Attention,[0],[0]
"This enables a linear rather than quadratic complexity in comparison to selfattention, but reduces the amount of parallel computation.",4 LSTM as Self-Attention,[0],[0]
"This accumulation also creates an inductive bias of attending to nearby words, since the weights can only decrease over time.
",4 LSTM as Self-Attention,[0],[0]
"Finally, attention has a probabilistic interpretation due to the softmax normalization, while the sum of weights in LSTMs can grow up to the sequence length.",4 LSTM as Self-Attention,[0],[0]
"In variants of the LSTM that tie the input and forget gate, such as coupled-gate LSTMs (Greff et al., 2016) and GRUs (Cho et al., 2014), the memory cell instead computes a weighted average with a probabilistic interpretation.",4 LSTM as Self-Attention,[0],[0]
These variants compute locally normalized distributions via a product of sigmoids rather than globally normalized distributions via a single softmax.,4 LSTM as Self-Attention,[0],[0]
"Many variants of LSTMs (Hochreiter and Schmidhuber, 1997) have been previously explored.",5 Related Work,[0],[0]
"These typically consist of a different parameterization of the gates, such as LSTMs with peephole connections (Gers and Schmidhuber, 2000), or a rewiring of the connections, such as GRUs (Cho et al., 2014).",5 Related Work,[0],[0]
"However, these modifications invariably maintain the recurrent content layer.",5 Related Work,[0],[0]
"Even more systematic explorations (Józefowicz et al., 2015; Greff et al., 2016; Zoph and Le, 2017) do not question the importance of the embedded SRNN.",5 Related Work,[0],[0]
"This is the first study to provide applesto-apples comparisons between LSTMs with and without the recurrent content layer.
",5 Related Work,[0],[0]
"Several other recent works have also reported promising results with recurrent models that are vastly simpler than LSTMs, such as quasirecurrent neural networks (Bradbury et al., 2016), strongly-typed recurrent neural networks (Bal-
duzzi and Ghifary, 2016), recurrent additive networks (Lee et al., 2017), kernel neural networks (Lei et al., 2017a), and simple recurrent units (Lei et al., 2017b), making it increasingly apparent that LSTMs are over-parameterized.",5 Related Work,[0],[0]
"While these works indicate an obvious trend, they do not focus on explaining what LSTMs are learning.",5 Related Work,[0],[0]
"In our carefully controlled ablation studies, we propose and evaluate the minimal changes required to test our hypothesis that LSTMs are powerful because they dynamically compute element-wise weighted sums of content layers.",5 Related Work,[0],[0]
We presented an alternate view of LSTMs: they are a hybrid of S-RNNs and a gated model that dynamically computes weighted sums of the S-RNN outputs.,6 Conclusion,[0],[0]
Our experiments investigated whether the S-RNN is a necessary component of LSTMs.,6 Conclusion,[0],[0]
"In other words, are the gates alone as powerful of a model as an LSTM?",6 Conclusion,[0],[0]
"Results across four major NLP tasks (language modeling, question answering, dependency parsing, and machine translation) indicate that LSTMs suffer little to no performance loss when removing the S-RNN.",6 Conclusion,[0],[0]
This provides evidence that the gating mechanism is doing the heavy lifting in modeling context.,6 Conclusion,[0],[0]
"We further ablate the recurrence in each gate and find that this incurs only a modest drop in performance, indicating that the real modeling power of LSTMs stems from their ability to compute element-wise weighted sums of context-independent functions of their inputs.
",6 Conclusion,[0],[0]
This realization allows us to mathematically relate LSTMs and other gated RNNs to attention-based models.,6 Conclusion,[0],[0]
"Casting an LSTM as a dynamically-computed attention mechanism enables the visualization of how context is used at every timestep, shedding light on the inner workings of the relatively opaque LSTM.",6 Conclusion,[0],[0]
"The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google, Tencent, and Nvidia, and an Allen Distinguished Investigator Award.",Acknowledgements,[0],[0]
"We also thank Yoav Goldberg, Benjamin Heinzerling, Tao Lei, and the UW NLP group for helpful conversations and comments on the work.",Acknowledgements,[0],[0]
"Given the empirical evidence that LSTMs are effectively learning weighted sums of the content layers, it is natural to investigate what weights the model learns in practice.",A Weight Visualization,[0],[0]
"Using the more mathematically transparent simplification of LSTMs, we can visualize the weights wtj that are placed on every input j at every timestep t (see Equation 9).
",A Weight Visualization,[0],[0]
"Unlike attention mechanisms, these weights are vectors rather than scalar values.",A Weight Visualization,[0],[0]
"Therefore, we can only provide a coarse-grained visualization of the weights by rendering their L2-norm, as shown in Table 5.",A Weight Visualization,[0],[0]
"In the visualization, each column indicates the word represented by the weighted sum, and each row indicates the word over which the weighted sum is computed.",A Weight Visualization,[0],[0]
Dark horizontal streaks indicate the duration for which a word was remembered.,A Weight Visualization,[0],[0]
"Unsurprisingly, the weights on the diagonal are always the largest since it indicates the weight of the current word.",A Weight Visualization,[0],[0]
"More interesting task-specific patterns emerge when inspecting the off-diagonals that represent the weight on the context words.
",A Weight Visualization,[0],[0]
The first visualization uses the language model.,A Weight Visualization,[0],[0]
"Due to the language modeling setup, there are only non-zero weights on the current or previous words.",A Weight Visualization,[0],[0]
"We find that the common function words are quickly forgotten, while infrequent words that
signal the topic are remembered over very long distances.
",A Weight Visualization,[0],[0]
The second visualization uses the dependency parser.,A Weight Visualization,[0],[0]
"In this setting, since the recurrent architectures are bidirectional, there are non-zero weights on all words in the sentence.",A Weight Visualization,[0],[0]
"The top-right triangle indicates weights from the forward direction, and the bottom-left triangle indicates from the backward direction.",A Weight Visualization,[0],[0]
"For syntax, we see a significantly different pattern.",A Weight Visualization,[0],[0]
Function words that are useful for determining syntax are more likely to be remembered.,A Weight Visualization,[0],[0]
"Weights on head words are also likely to persist until the end of a constituent.
",A Weight Visualization,[0],[0]
"This illustration provides only a glimpse into what the model is capturing, and perhaps future, more detailed visualizations that take the individual dimensions into account can provide further insight into what LSTMs are learning in practice.
739
Language model weights Dependency parser weights
The hym n was sun g at my first inau gura
l
chu rch serv ice as gov erno
r
The hymn
was
sung
at
my first inaugural
church service
as governor
The hym n was sun g at my first inau gura
l
chu rch serv ice as gov erno
r
The hymn
was
sung
at
my first inaugural
church service
as governor
US troo",A Weight Visualization,[0],[0]
ps ther e clas hed with gue rrilla s in a figh t that left one Iraq,A Weight Visualization,[0],[0]
"i dea d
US troops there clashed with guerrillas in a
fight that left one Iraqi dead
US troo ps ther e clas",A Weight Visualization,[0],[0]
hed with gue rrilla s in a figh t that left one Iraq,A Weight Visualization,[0],[0]
"i dea d
US troops there clashed with guerrillas in a
fight that left one Iraqi dead",A Weight Visualization,[0],[0]
LSTMs were introduced to combat vanishing gradients in simple RNNs by augmenting them with gated additive recurrent connections.,abstractText,[0],[0]
We present an alternative view to explain the success of LSTMs: the gates themselves are versatile recurrent models that provide more representational power than previously appreciated.,abstractText,[0],[0]
"We do this by decoupling the LSTM’s gates from the embedded simple RNN, producing a new class of RNNs where the recurrence computes an element-wise weighted sum of context-independent functions of the input.",abstractText,[0],[0]
"Ablations on a range of problems demonstrate that the gating mechanism alone performs as well as an LSTM in most settings, strongly suggesting that the gates are doing much more in practice than just alleviating vanishing gradients.",abstractText,[0],[0]
Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1197–1206, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Word segmentation is a fundamental task for Chinese language processing.,1 Introduction,[0],[0]
"In recent years, Chinese word segmentation (CWS) has undergone great development.",1 Introduction,[0],[0]
"The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004).",1 Introduction,[0],[0]
"The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional RandomFields (CRF) (Lafferty et al., 2001).",1 Introduction,[0],[0]
"However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus.",1 Introduction,[0],[0]
"Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert
∗Corresponding author.
",1 Introduction,[0],[0]
"et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003).",1 Introduction,[0],[0]
Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English.,1 Introduction,[0],[0]
"Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance.",1 Introduction,[0],[0]
"Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013).",1 Introduction,[0],[0]
Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task.,1 Introduction,[0],[0]
Each neuron in GRNN can be regarded as a different combination of the input characters.,1 Introduction,[0],[0]
"Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods.",1 Introduction,[0],[0]
"Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window.",1 Introduction,[0],[0]
"Intuitively, many words are difficult to segment based on the local information only.",1 Introduction,[0],[0]
"For example, the segmentation of the following sentence needs the information of the long distance collocation.",1 Introduction,[0],[0]
冬天 (winter)，能 (can) 穿 (wear) 多少 (amount) 穿 (wear) 多少 (amount)；夏天 (summer)，能 (can)穿 (wear)多 (more)少 (little)穿 (wear)多 (more)少 (little)。,1 Introduction,[0],[0]
"Without the word “夏天 (summer)” or “冬天 (winter)”, it is difficult to segment the phrase “能 穿多少穿多少”.",1 Introduction,[0],[0]
"Therefore, we usually need utilize the non-local information for more accurate word segmentation.",1 Introduction,[0],[0]
"However, it does not work by simply increasing the context window size.",1 Introduction,[0],[0]
"As reported in (Zheng et al., 2013), the performance drops smoothly when the window size is larger than 3.",1 Introduction,[0],[0]
"The reason is that the number of its parameters is so large that the trained network has
1197
overfitted on training data.",1 Introduction,[0],[0]
"Therefore, it is necessary to capture the potential long-distance dependencies without increasing the size of the context window.",1 Introduction,[0],[0]
"In order to address this problem, we propose a neural model based on Long Short-Term Memory Neural Network (LSTM) (Hochreiter and Schmidhuber, 1997) that explicitly model the previous information by exploiting input, output and forget gates to decide how to utilize and update the memory of pervious information.",1 Introduction,[0],[0]
"Intuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it easily carries this information (the existence of the feature) over a long distance, hence, capturing the potential useful long-distance information.",1 Introduction,[0],[0]
"We evaluate our model on three popular benchmark datasets (PKU, MSRA and CTB6), and the experimental results show that our model achieves the state-of-the-art performance with the smaller context window size (0,2).",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows.
",1 Introduction,[0],[0]
• We first introduce the LSTM neural network for Chinese word segmentation.,1 Introduction,[0],[0]
"The LSTM can capture potential long-distance dependencies and keep the previous useful information in memory, which avoids the limit of the size of context window.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Although there are relatively few researches of applying dropout method to the LSTM, we investigate several dropout strategies and find that dropout is also effective to avoid the overfitting of the LSTM.
",1 Introduction,[0],[0]
"• Despite Chinese word segmentation being a specific case, our model can be easily generalized and applied to the other sequence labeling tasks.",1 Introduction,[0],[0]
Chinese word segmentation is usually regarded as character-based sequence labeling.,2 Neural Model for Chinese Word Segmentation,[0],[0]
"Each character is labeled as one of {B, M, E, S} to indicate the segmentation.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"{B, M, E} represent Begin, Middle, End of a multi-character segmentation respectively, and S represents a Single character segmentation.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"The neural model is usually characterized by three specialized layers: (1) a character embedding
layer; (2) a series of classical neural network layers and (3) tag inference layer.",2 Neural Model for Chinese Word Segmentation,[0],[0]
An illustration is shown in Figure 1.,2 Neural Model for Chinese Word Segmentation,[0],[0]
The most common tagging approach is based on a local window.,2 Neural Model for Chinese Word Segmentation,[0],[0]
The window approach assumes that the tag of a character largely depends on its neighboring characters.,2 Neural Model for Chinese Word Segmentation,[0],[0]
"Given an input sentence c(1:n), a window of size k slides over the sentence from character c(1) to c(n), where n is the length of the sentence.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"As shown in Figure 1, for each character c(t)(1 ≤",2 Neural Model for Chinese Word Segmentation,[0],[0]
"t ≤ n), the context characters (c(t−2),c(t−1),c(t),c(t+1),c(t+2)) are fed into the lookup table layer when the window size k is 5.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"The characters exceeding the sentence boundaries are mapped to one of two special symbols, namely “start” and “end” symbols.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"The character embeddings extracted by the lookup table layer are then concatenated into a single vector x(t) ∈ RH1 , where H1 = k × d is the size of layer 1.",2 Neural Model for Chinese Word Segmentation,[0],[0]
Then x(t) is fed into the next layer which performs linear transformation followed by an element-wise activation function g such as sigmoid function σ(x) =,2 Neural Model for Chinese Word Segmentation,[0],[0]
(1+e−x)−1 and hyperbolic tangent function ϕ(x),2 Neural Model for Chinese Word Segmentation,[0],[0]
"= ex−e−x
ex+e−x here.
",2 Neural Model for Chinese Word Segmentation,[0],[0]
"h(t) = g(W1x(t) + b1), (1)
whereW1 ∈",2 Neural Model for Chinese Word Segmentation,[0],[0]
"RH2×H1 , b1 ∈ RH2 , h(t) ∈ RH2 .",2 Neural Model for Chinese Word Segmentation,[0],[0]
H2 is a hyper-parameter which indicates the number of hidden units in layer 2.,2 Neural Model for Chinese Word Segmentation,[0],[0]
"Given a set of tags T of size |T |, a similar linear transformation is performed except that no non-linear function is followed:
y(t)",2 Neural Model for Chinese Word Segmentation,[0],[0]
"= W2h(t) + b2, (2)
where W2 ∈ R|T |×H2 , b2 ∈ R|T |. y(t) ∈ R|T",2 Neural Model for Chinese Word Segmentation,[0],[0]
| is the score vector for each possible tag.,2 Neural Model for Chinese Word Segmentation,[0],[0]
"In Chinese word segmentation, the most prevalent tag set T j T is {B, M, E, S} as mentioned above.",2 Neural Model for Chinese Word Segmentation,[0],[0]
"To model the tag dependency, a transition score Aij is introduced to measure the probability of jumping from tag i ∈ T to tag j ∈ T (Collobert et al., 2011).",2 Neural Model for Chinese Word Segmentation,[0],[0]
"Although this model works well for Chinese word segmentation and other sequence labeling tasks, it just utilizes the information of context of a limited-length window.",2 Neural Model for Chinese Word Segmentation,[0],[0]
Some useful long distance information is neglected.,2 Neural Model for Chinese Word Segmentation,[0],[0]
"In this section, we introduce the LSTM neural network for Chinese word segmentation.",3 Long Short-Term Memory Neural Network for Chinese Word Segmentation,[0],[0]
"The first step of using neural network to process symbolic data is to represent them into distributed vectors, also called embeddings (Bengio et al., 2003; Collobert and Weston, 2008).",3.1 Character Embeddings,[0],[0]
"Formally, in Chinese word segmentation task, we have a character dictionary C of size |C|.",3.1 Character Embeddings,[0],[0]
"Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere.",3.1 Character Embeddings,[0],[0]
Each character c ∈ C is represented as a real-valued vector (character embedding),3.1 Character Embeddings,[0],[0]
vc ∈ Rd where d is the dimensionality of the vector space.,3.1 Character Embeddings,[0],[0]
The character embeddings are then stacked into an embeddingmatrixM ∈ Rd×|C|.,3.1 Character Embeddings,[0],[0]
"For a character c ∈ C, the corresponding character embedding vc ∈ Rd is retrieved by the lookup table layer.",3.1 Character Embeddings,[0],[0]
And the lookup table layer can be regarded as a simple projection layer where the character embedding for each context character is achieved by table lookup operation according to its index.,3.1 Character Embeddings,[0],[0]
"The long short term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is an extension of the recurrent neural network (RNN).
",3.2 LSTM,[0],[0]
The RNN has recurrent hidden states whose output at each time is dependent on that of the previous time.,3.2 LSTM,[0],[0]
"More formally, given a sequence x(1:n) =",3.2 LSTM,[0],[0]
"(x(1), x(2), . . .",3.2 LSTM,[0],[0]
", x(t), . . .",3.2 LSTM,[0],[0]
", x(n)), the RNN updates its recurrent hidden state h(t) by
h(t) = g(Uh(t−1) +Wx(t) + b), (3)
where g is a nonlinear function as mentioned above.",3.2 LSTM,[0],[0]
"Though RNN has been proven successful on many tasks such as speech recognition (Vinyals et al., 2012), language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011), it can be difficult to train them to learn longterm dynamics, likely due in part to the vanishing and exploding gradient problem (Hochreiter and Schmidhuber, 1997).",3.2 LSTM,[0],[0]
The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information.,3.2 LSTM,[0],[0]
"Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs.",3.2 LSTM,[0],[0]
"In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014).",3.2 LSTM,[0],[0]
The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) .,3.2 LSTM,[0],[0]
"The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o.",3.2 LSTM,[0],[0]
"The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector.",3.2 LSTM,[0],[0]
The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of upper layer of neural network at current time step t.,3.2 LSTM,[0],[0]
"The definitions of the gates, cell update and output are as follows:
i(t) = σ(Wixx(t) + Wihh(t−1) + Wicc(t−1)), (4) f(t) = σ(Wfxx(t) + Wfhh(t−1) + Wfcc(t−1)), (5) c(t) = f(t) ⊙ c(t−1) + i(t) ⊙ ϕ(Wcxx(t) + Wchh(t−1)), (6) o(t) = σ(Woxx(t)",3.2 LSTM,[0],[0]
"+Wohh(t−1) +Wocc(t)), (7)
h(t) = o(t) ⊙ ϕ(c(t)), (8)
whereσ andϕ are the logistic sigmoid function and hyperbolic tangent function respectively; i(t), f(t), o(t) and c(t) are respectively the input gate, forget gate, output gate, and memory cell activation vector at time step t, all of which have the same size as the hidden vector h(t) ∈ RH2 ; the parameter matrices W s with different subscripts are all square matrices; ⊙ denotes the element-wise product of the vectors.",3.2 LSTM,[0],[0]
"Note that Wic, Wfc and Woc are diagonal matrices.",3.2 LSTM,[0],[0]
"To fully utilize the LSTM, we propose four different structures of neural network to select the effective features via memory units.",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"Figure 3 illustrates our proposed architectures.
",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
LSTM-1,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
The LSTM-1 simply replace the hidden neurons in Eq.,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"(1) with LSTM units (See Figure 3a).
",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
The input of the LSTM unit is from a window of context characters.,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"For each character, c(t), (1 ≤ t ≤ n), the input of the LSTM unit x(t),
x(t) = v(t−k1)c ⊕ · · · ⊕ v(t+k2)c , (9)
is concatenated from character embeddings of c(t−k1):(t+k2), where k1 and k2 represent the numbers of characters from left and right contexts respectively.",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"The output of the LSTM unit is used in final inference function (Eq. (11) ) after a linear transformation.
",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
LSTM-2,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"The LSTM-2 can be created by stacking multiple LSTM hidden layers on top of each other, with the output sequence of one layer forming the input sequence for the next (See Figure 3b).",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
Here we use two LSTM layers.,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"Specifically, input of the upper LSTM layer takes h(t) from the lower LSTM layer without any transformation.",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"The input of the first layer is same to LSTM-1, and the output of the second layer is as same operation as LSTM-1.
",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"LSTM-3 The LSTM-3 is a extension of LSTM1, which adopts a local context of LSTM layer as input of the last layer (See Figure 3c).",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"For each time step t, we concatenate the outputs of a window of the LSTM layer into a vector ĥ(t),
ĥ(t) = h(t−m1) ⊕ · · · ⊕ h(t+m2), (",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"10)
wherem1 andm2 represent the lengths of time lags before and after current time step.",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"Finally, ĥ(t) is used in final inference function (Eq. (11) )",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"after a linear transformation.
LSTM-4 The LSTM-4 (see Figure 3d) is a mixture of the LSTM-2 and LSTM-3, which consists of two LSTM layers.",3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
The output sequence of the lower LSTM layer forms the input sequence of the upper LSTM layer.,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
The final layer adopts a local context of upper LSTM layer as input.,3.3 LSTM Architectures for Chinese Word Segmentation,[0],[0]
"To model the tag dependency, previous neural network models (Collobert et al., 2011; Zheng et al., 2013; Pei et al., 2014) introduced the transition score Aij for measuring the probability of jumping from tag i ∈ T to tag j ∈ T .",3.4 Inference at Sentence Level,[0],[0]
"For a input sentence c(1:n) with a tag sequence y(1:n), a sentencelevel score is then given by the sum of tag transition scores and network tagging scores:
s(c(1:n), y(1:n), θ) = n∑ t=1 ( Ay(t−1)y(t)",3.4 Inference at Sentence Level,[0],[0]
"+ y (t) y(t) ) , (11)
where y(t) y(t) indicates the score of tag y(t), and y(t) is computed by the network as in Eq.",3.4 Inference at Sentence Level,[0],[0]
(2).,3.4 Inference at Sentence Level,[0],[0]
"The parameter set of our model θ = {M,A,Wic,Wfc,Woc,Wix,Wfx,Wox,Wih,Wfh, Woh,Wcx,Wch}.",3.4 Inference at Sentence Level,[0],[0]
We use the Max-Margin criterion to train our model.,4.1 Max-Margin criterion,[0],[0]
"Intuitively, the Max-Margin criterion provides an alternative to probabilistic, likelihood based estimation methods by concentrating directly on the robustness of the decision boundary of a model (Taskar et al., 2005).",4.1 Max-Margin criterion,[0],[0]
We use Y (xi) to denote the set of all possible tag sequences for a given sentence xi and the correct tag sequence for xi is yi.,4.1 Max-Margin criterion,[0],[0]
The parameter set of our model is θ.,4.1 Max-Margin criterion,[0],[0]
"We first define a structured margin loss ∆(yi, ŷ) for predicted tag sequence ŷ:
∆(yi, ŷ)",4.1 Max-Margin criterion,[0],[0]
"= n∑ t η1{y(t)i ̸= ŷ(t)}, (12)
where n is the length of sentence xi and η is a discount parameter.",4.1 Max-Margin criterion,[0],[0]
The loss is proportional to the number of characters with incorrect tags in the proposed tag sequence.,4.1 Max-Margin criterion,[0],[0]
"For a given training instance (xi, yi),the predicted tag sequence ŷi ∈ Y (xi) is the one with the highest score:
ŷi = argmax y∈Y (xi) s(xi, y, θ), (13)
where the function s(·) is sentence-level score and defined in equation (11).",4.1 Max-Margin criterion,[0],[0]
"Given a set of training setD, the regularized objective function is the loss function J(θ) including
a l2-norm term:
J(θ)",4.1 Max-Margin criterion,[0],[0]
"= 1 |D| ∑ (xi,yi)∈D li(θ)",4.1 Max-Margin criterion,[0],[0]
"+ λ 2 ∥θ∥22, (14)
where li(θ) = max(0, s(xi, ŷi, θ) + ∆(yi, ŷi)",4.1 Max-Margin criterion,[0],[0]
"− s(xi, yi, θ)).",4.1 Max-Margin criterion,[0],[0]
"To minimize J(θ), we use a generalization of gradient descent called subgradient method (Ratliff et al., 2007) which computes a gradientlike direction.",4.1 Max-Margin criterion,[0],[0]
"Following (Socher et al., 2013), we also use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatchs to minimize the objective.",4.1 Max-Margin criterion,[0],[0]
"The parameter update for the i-th parameter θt,i at time step t is as follows:
θt,i = θt−1,i − α√∑t τ=1 g 2 τ,i gt,i, (15)
where α is the initial learning rate and gτ ∈ R|θi| is the subgradient at time step τ for parameter θi.",4.1 Max-Margin criterion,[0],[0]
"In addition, the process of back propagation is followd Hochreiter and Schmidhuber (1997).",4.1 Max-Margin criterion,[0],[0]
"Dropout is one of prevalent methods to avoid overfitting in neural networks (Srivastava et al., 2014).",4.2 Dropout,[0],[0]
"When dropping a unit out, we temporarily remove it from the network, alongwith all its incoming and outgoing connections.",4.2 Dropout,[0],[0]
"In the simplest case, each unit is omitted with a fixed probability p independent of other units, namely dropout rate, where p is also chosen on development set.",4.2 Dropout,[0],[0]
"We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model.",5.1 Datasets,[0],[0]
"The PKU
and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) (Xue et al., 2005), which is a segmented, part-of-speech tagged and fully bracketed corpus in the constituency formalism.",5.1 Datasets,[0],[0]
These datasets are commonly used by previous state-of-the-art models and neural network models.,5.1 Datasets,[0],[0]
"In addition, we use the first 90% sentences of the training data as training set and the rest 10%
sentences as development set for PKU and MSRA datasets.",5.1 Datasets,[0],[0]
"For CTB6 dataset, we divide the training, development and test sets according to (Yang and Xue, 2012)",5.1 Datasets,[0],[0]
All datasets are preprocessed by replacing the Chinese idioms and the continuous English characters and digits with a unique flag.,5.1 Datasets,[0],[0]
"For evaluation, we use the standard bake-off scoring program to calculate precision, recall, F1score and out-of-vocabulary (OOV) word recall.",5.1 Datasets,[0],[0]
Hyper-parameters of neural model impact the performance of the algorithm significantly.,5.2 Hyper-parameters,[0],[0]
"According to experiment results, we choose the hyperparameters of our model as showing in Figure 1.",5.2 Hyper-parameters,[0],[0]
The minibatch size is set to 20.,5.2 Hyper-parameters,[0],[0]
"Generally, the number of hidden units has a limited impact on the performance as long as it is large enough.",5.2 Hyper-parameters,[0],[0]
We found that 150 is a good trade-off between speed and model performance.,5.2 Hyper-parameters,[0],[0]
The dimensionality of character embedding is set to 100 which achieved the best performance.,5.2 Hyper-parameters,[0],[0]
All these hyperparameters are chosen according to their average performances on three development sets.,5.2 Hyper-parameters,[0],[0]
"For the context lengths (k1, k2) and dropout strategy, we give detailed analysis in next section.",5.2 Hyper-parameters,[0],[0]
"We first investigate the different dropout strategies, including dropout at different layers and with different dropout rate p.",5.3 Dropout and Context Length,[0],[0]
"As a result, we found that it is a good trade-off between speed and model performance to drop the input layer only with dropout rate pinput = 0.2.",5.3 Dropout and Context Length,[0],[0]
"However, it does not show any significant improvement to dropout on hidden LSTM layers.
",5.3 Dropout and Context Length,[0],[0]
"Due to space constraints, we just give the performances of LSTM-1 model on PKU dataset with different context lengths (k1, k2) and dropout rates in Figure 4 and Table 2.",5.3 Dropout and Context Length,[0],[0]
"From Figure 4, we can see that 20% dropout converges slightly slower than the one without dropout, but avoids overfitting.",5.3 Dropout and Context Length,[0],[0]
50% or higher dropout rate seems to be underfitting since its training error is also high.,5.3 Dropout and Context Length,[0],[0]
"Table 2 shows that the LSTM-1 model performs consistently well with the different context length, but the LSTM-1 model with short context length saves computational resource, and gets more efficiency.",5.3 Dropout and Context Length,[0],[0]
"At the meanwhile, the LSTM-1 model with context length (0,2) can receive the same or better performance than that with context length (2,2), which shows that the LSTM model can well model the pervious information, and it is more robust for its insensitivity of window size variation.",5.3 Dropout and Context Length,[0],[0]
"We employ context length (0,2) with the 20% dropout rate in the following experiments to balance the tradeoff between accuracy and efficiency.",5.3 Dropout and Context Length,[0],[0]
We also evaluate the our four proposed models with the hyper-parameter settings in Table 1.,5.4 Model Selection,[0],[0]
"For LSTM-3 and LSTM-4 models, the context window length of top LSTM layer is set to (2,0).",5.4 Model Selection,[0],[0]
"For LSTM-2 and LSTM-4,the number of upper hidden LSTM layer is set to 100.",5.4 Model Selection,[0],[0]
We use PKU dataset to select the best model.,5.4 Model Selection,[0],[0]
Figure 5 shows the results of the fourmodels on PKUdevelopment set from first epoch to 60-th epoch.,5.4 Model Selection,[0],[0]
"We see that the LSTM-1 is the fastest one to converge and achieves the best
performance.",5.4 Model Selection,[0],[0]
"The LSTM-2 (two LSTM layers) get worse, which shows the performance seems not to benefit from deep model.",5.4 Model Selection,[0],[0]
"The LSTM-3 and LSTM-4 models do not converge, which could be caused by the complexity of models.",5.4 Model Selection,[0],[0]
"The results on PKU test set are also shown in Table 3, which again show that the LSTM-1 achieves the best performance.",5.4 Model Selection,[0],[0]
"Therefore, in the rest of the paper we will give more analysis based on the LSTM-1with hyper-parameter settings as showing in Table 1.",5.4 Model Selection,[0],[0]
"In this section, we give comparisons of the LSTM1 with pervious neural models and state-of-the-art methods on the PKU, MSRA and CTB6 datasets.",5.5 Experiment Results,[0],[0]
"We first compare our model with two neural models (Zheng et al., 2013; Pei et al., 2014) on Chinese word segmentation task with random initialized character embeddings.",5.5 Experiment Results,[0],[0]
"As showing in Table 4, the performance is boosted significantly by utilizing LSTM unit.",5.5 Experiment Results,[0],[0]
"And more notably, our window size of the context characters is set to (0,2), while the size of the other models is (2,2).",5.5 Experiment Results,[0],[0]
Previous works found that the performance can be improved by pre-training the character embeddings on large unlabeled data.,5.5 Experiment Results,[0],[0]
"We use word2vec 1 (Mikolov et al., 2013a) toolkit to pre-train the character embeddings on the Chinese Wikipedia corpus.",5.5 Experiment Results,[0],[0]
The obtained embeddings are used to initialize the character lookup table instead of random initialization.,5.5 Experiment Results,[0],[0]
"Inspired by (Pei et al., 2014), we also utilize bigram character embeddings which is simply initialized as the average of embeddings of two consecutive characters.",5.5 Experiment Results,[0],[0]
Table 5 shows the performances with additional pre-trained and bigram character embeddings.,5.5 Experiment Results,[0],[0]
"Again, the performances boost significantly as a result.",5.5 Experiment Results,[0],[0]
"Moreover, when we use bigram embeddings only, which means we do close test without pre-training the embeddings on other extra corpus, our model still perform competitively compared
1http://code.google.com/p/word2vec/
with previous neural models with pre-trained embedding and bigram embeddings.",5.5 Experiment Results,[0],[0]
Table 6 lists the performances of our model as well as previous state-of-the-art systems.,5.5 Experiment Results,[0],[0]
"(Zhang and Clark, 2007) is a word-based segmentation algorithm, which exploit features of complete words, while the rest of the list are character-based word segmenters, whose features are mostly extracted from a window of characters.",5.5 Experiment Results,[0],[0]
"Moreover, some systems (such as Sun and Xu (2011) and Zhang et al. (2013)) also exploit kinds of extra information such as unlabeled data or other knowledge.",5.5 Experiment Results,[0],[0]
"Despite our model only uses simple bigram features, it outperforms previous state-of-the-art models which use more complex features.",5.5 Experiment Results,[0],[0]
"Since that we do not focus on the speed of the algorithm in this paper, we do not optimize the speed
a lot.",5.5 Experiment Results,[0],[0]
"On PKU dataset, it takes about 3 days to train themodel (last row of Table 5) usingCPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only.",5.5 Experiment Results,[0],[0]
All implementation is based on Python.,5.5 Experiment Results,[0],[0]
Chinese word segmentation has been studied with considerable efforts in the NLP community.,6 Related Work,[0],[0]
"The most popular word segmentation methods is based on sequence labeling (Xue, 2003).",6 Related Work,[0],[0]
"Recently, researchers have tended to explore neural network based approaches (Collobert et al., 2011) to reduce efforts of feature engineering (Zheng et al., 2013; Pei et al., 2014; Qi et al., 2014; Chen et al., 2015).",6 Related Work,[0],[0]
The features of all these methods are extracted from a local context and neglect the long distance information.,6 Related Work,[0],[0]
"However, previous information is also crucial for word segmentation.",6 Related Work,[0],[0]
Our model adopts the LSTM to keep the previous important information in memory and avoids the limitation of ambiguity caused by limit of the size of context window.,6 Related Work,[0],[0]
"In this paper, we use LSTM to explicitly model the previous information for Chinese word segmentation, which can well model the potential long-
distance features.",7 Conclusion,[0],[0]
"Though our model use smaller context window size (0,2), it still outperforms the previous neural models with context window size (2,2).",7 Conclusion,[0],[0]
"Besides, our model can also be easily generalized and applied to other sequence labeling tasks.",7 Conclusion,[0],[0]
"Although our model achieves state-of-the-art performance, it only makes use of previous context.",7 Conclusion,[0],[0]
The future context is also useful for Chinese word segmentation.,7 Conclusion,[0],[0]
"In future work, wewould like to adopt the bidirectional recurrent neural network (Schuster and Paliwal, 1997) to process the sequence in both directions.",7 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200).",Acknowledgments,[0],[0]
"Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features aremostly extracted from a local context.",abstractText,[0],[0]
Thesemethods cannot utilize the long distance information which is also crucial for word segmentation.,abstractText,[0],[0]
"In this paper, we propose a novel neural network model for Chinese word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information inmemory cell and avoids the limit of window size of local context.",abstractText,[0],[0]
"Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods.",abstractText,[0],[0]
Long Short-Term Memory Neural Networks for Chinese Word Segmentation,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 297–302 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Entrainment, also called accommodation or alignment, is the tendency of human interlocutors to adapt their behavior to each other to become more similar.",1 Introduction,[0],[0]
"This affects many linguistic features such as referring expressions (Brennan and Clark, 1996), phonetics (Pardo, 2006), syntax (Reitter et al., 2006), linguistic style (Niederhoffer and Pennebaker, 2002), turn-taking (Levitan et al., 2011), and prosody (Levitan and Hirschberg, 2011) as well as non-linguistic behavior (Chartrand and Bargh, 1999).",1 Introduction,[0],[0]
"It has also been linked to external aspects of the conversation such as task success (Reitter and Moore, 2007; Nenkova et al., 2008) and social factors (Ireland et al., 2011; Levitan et al., 2012).
",1 Introduction,[0],[0]
"The study of entrainment thus far has been fragmented, with researchers considering numerous individual features and measuring similarity in various ways, but few searching for correlations or other structure.",1 Introduction,[0],[0]
"For instance, both Ward and Litman (2007) and Fusaroli and Tylén (2016) measured lexical as well as acoustic-prosodic entrainment but neither paper investigated correla-
tions between these measures.",1 Introduction,[0],[0]
There are two recent exceptions to this overall pattern.,1 Introduction,[0],[0]
Mukherjee et al. (2017) found a correlation between speakers’ prosodies becoming more similar over time and their fundamental frequencies varying in synchrony.,1 Introduction,[0],[0]
"Rahimi et al. (2017) also showed correlations, between lexical and acoustic-prosodic entrainment in group conversations.",1 Introduction,[0],[0]
"However, neither considered more complex structure and Rahimi et al., while including lexical features, focus on high-frequency and topic words alone.
",1 Introduction,[0],[0]
"We take a broad view of entrainment, analyzing 18 sets of measurements in four different ways on two corpora to uncover structure, hoping to find higher-level behaviors that explain observed variability between speakers.",1 Introduction,[0],[0]
This is motivated by several cognitive theories that purport to explain linguistic entrainment.,1 Introduction,[0],[0]
"Pickering and Garrod (2004), for instance, claim that it serves dialog success and that “alignment at one level leads to alignment at other levels”.",1 Introduction,[0],[0]
"According to Chartrand and Bargh (1999), entrainment is based on a link between perception and behavior and correlates with “greater perceptual activity directed at the other person”.",1 Introduction,[0],[0]
"Giles et al. (1991), lastly, argue that adaptive behavior is meant to increase or decrease “interpersonal differences” of the interlocutors.",1 Introduction,[0],[0]
All these theories implicitly postulate that entrainment can be considered a single latent behavior or a structured collection of behaviors.,1 Introduction,[0],[0]
"Here, we look for evidence that entrainment behaviors can be explained by an underlying structure, particularly one that spans multiple features.",1 Introduction,[0],[0]
"Practically, it would be useful for downstream analysis to need to consider only a small set of higher-level behaviors rather than each basic entrainment measure in the search for interactions with quality metrics.
",1 Introduction,[0],[0]
Our analysis is based on two corpora of dyadic conversation.,1 Introduction,[0],[0]
"The first is the Objects Games por-
297
tion of the Columbia Games Corpus (Gravano and Hirschberg, 2011), CGC, which comprises 12 sessions with 14 identical tasks each, a total of about four hours of speech.",1 Introduction,[0],[0]
"Second, we use the Switchboard Corpus (Godfrey and Holliman, 1993), SBC, which contains over 2000 free conversations about given topics with a total of more than 200 hours of speech.",1 Introduction,[0],[0]
"Both corpora are fully orthographically transcribed and acoustic-prosodic features were extracted using Praat (Boersma and Weenink, 2001).",1 Introduction,[0],[0]
"We consider three acoustic-prosodic features: pitch (fundamental frequency in Hz), intensity (loudness in dB), and speech rate (in syllables per second).",2.1 Acoustic-prosodic entrainment,[0],[0]
"The arithmetic mean for each feature is determined at the level of an interpausal unit (IPU), a maximal segment of speech by a single speaker without a pause of 50ms or more.",2.1 Acoustic-prosodic entrainment,[0],[0]
"A maximal sequence of IPUs by one speaker, without interruption by the other, is called a turn.
",2.1 Acoustic-prosodic entrainment,[0],[0]
The measures of acoustic-prosodic entrainment we use were defined by Levitan and Hirschberg (2011).,2.1 Acoustic-prosodic entrainment,[0],[0]
Two speakers exhibit local similarity if their feature values differ little at turn exchanges and local convergence if that difference decreases over time.,2.1 Acoustic-prosodic entrainment,[0],[0]
"Global similarity is defined by a small difference in mean feature values over an entire task or session while global convergence is a decreasing difference in means from the first to the
second half of a session.",2.1 Acoustic-prosodic entrainment,[0],[0]
"Synchrony, lastly, exists if both speakers’ feature values rise and fall together at turn exchanges.",2.1 Acoustic-prosodic entrainment,[0],[0]
Figure 1 illustrates these different types of entrainment.,2.1 Acoustic-prosodic entrainment,[0],[0]
Each allows us to numerically quantify a type of likeness of the speakers’ prosodies.,2.1 Acoustic-prosodic entrainment,[0],[0]
"Those numeric values are then normalized and finally correlated, treated as coordinates in a feature space, etc.",2.1 Acoustic-prosodic entrainment,[0],[0]
"We apply three different measures of similarity based on the lemmata, i.e., canonical forms, of the words each speaker used throughout a session.",2.2 Lexical entrainment,[0],[0]
"The first two measures were used by Gravano et al. (2014) to compare ToBI annotations of CGC but, to our knowledge, have not been used before in the context of lexical entrainment.",2.2 Lexical entrainment,[0],[0]
"The third was defined by Nenkova et al. (2008) and shown to correlate with task success in CGC and perceived naturalness in SBC.
",2.2 Lexical entrainment,[0],[0]
"For the perplexity measure, PPL, we use SRILM (Stolcke, 2002) to build a trigram language model for each speaker, predict their partner’s utterances with it, and compute the negated perplexity.",2.2 Lexical entrainment,[0],[0]
"For the second measure, KLD, we compute the negated Kullback-Leibler divergence between pairs of unigram distributions of partners’ words.",2.2 Lexical entrainment,[0],[0]
"Lastly, for the high-frequency words measure, HFW, we compute, for each word w out of the 25 most frequent words in the respective overall corpus, the fraction of each speaker’s words which are w. The sum of the negated absolute differences for the 25 pairs of fractions is our
third measure of similarity for a pair of speakers.",2.2 Lexical entrainment,[0],[0]
Table 1 gives an overview of all our entrainment measures.,2.2 Lexical entrainment,[0],[0]
We apply z-score normalization by gender to our acoustic-prosodic features.,2.3 Normalization,[0],[0]
"That is, for each feature value we subtract the gender mean and then divide by gender standard deviation.
",2.3 Normalization,[0],[0]
"We normalize local similarity at each turn exchange using similarity of either IPU at the exchange with 10 randomly chosen, non-adjacent IPUs from the same session as a baseline.",2.3 Normalization,[0],[0]
"Similarly, global similarity and the lexical measures are normalized using similarity with non-partner speech as a baseline.",2.3 Normalization,[0],[0]
"For each speaker A we compare their similarity with partner B with the similarity with all non-partners C with whom A was never paired and who had the same role (CGC) or talked about the same topic (SBC) as B.
To control for the effect of complexity of speech on the lexical measures, we weight the non-partner similarities by how closely the entropy of the nonpartner’s language model matches that of the actual partner.",2.3 Normalization,[0],[0]
The main purpose of our analysis is to look for structure in an array of entrainment measures.,2.4 Analysis,[0],[0]
"However, we first check whether similarity is significantly greater for partners than non-partners for our lexical measures since PPL and KLD have not previously been used for lexical entrainment and Nenkova et al. (2008) did not report a significance test for HFW.
",2.4 Analysis,[0],[0]
We look for structure in our entrainment measures in four different ways.,2.4 Analysis,[0],[0]
"At the simplest level, we check for pairwise linear correlations by computing Pearson’s correlation coefficient between each pair of entrainment behaviors.",2.4 Analysis,[0],[0]
"Second, we treat each entrainment behavior as binary (present if the speaker is more similar to the partner than to the baseline), and use χ2 tests to investigate whether certain behaviors are disproportionately likely to co-occur.",2.4 Analysis,[0],[0]
"Third, we represent each speaker as a point in a continuous space defined by our entrainment measures and attempt to cluster these points to identify common complex entrainment behaviors.",2.4 Analysis,[0],[0]
"Fourth, we apply principal component analysis (PCA).",2.4 Analysis,[0],[0]
"For each of our lexical entrainment measures, we use t-tests to check whether partner similarities are significantly greater than non-partner similarities, which we consider to be evidence of entrainment.",3.1 Lexical entrainment significance,[0],[0]
"For CGC, we find significance for PPL (p < .001) and KLD (p < .01) but not for HFW (p > .25) while for SBC we find all three to be highly significant (p < 10−6).",3.1 Lexical entrainment significance,[0],[0]
"It is worth mentioning that the greater significance for SBC is attributable to the size of the corpus alone, as the average differences in similarities are comparable in both corpora.",3.1 Lexical entrainment significance,[0],[0]
"That is, even though conversations in SBC are less restricted than in CGC, the partner vs. nonpartner comparison is still “fair”.",3.1 Lexical entrainment significance,[0],[0]
"To check for simple linear correlations, we compute Pearson’s r for each pair of entrainment measures.",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"Due to the large number of correlation tests, we control for false discovery rate (FDR) (Benjamini and Hochberg, 1995) at .05 to reduce the probability of Type I error.
",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
In both corpora we find strong correlations between local similarity and synchrony for each acoustic-prosodic feature (r between +0.64 and +0.95).,3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
This simply results from the measures’ definitions: close feature values at turn exchanges throughout a session imply synchronous variation.,3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"In CGC, we find no other significant correlations.
",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"In SBC, more results are significant due to the greater number of samples.",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"Most correlations, however, are very weak, with only a few reaching |r| > 0.1, all between pairs of measurements on the same feature.",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"Specifically, we find correlations between local and global convergence for each prosodic feature (+0.14 ≤ r ≤ +0.47) and local and global similarity on pitch (r = +0.16) and intensity (r = +0.26).",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"We also find our lexical measures to be correlated with each other (+0.16 ≤ r ≤ +0.58).
",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"We conclude that, contrary to our expectations, entrainment does not correlate across features and even within features this simplest kind of structure is barely present.",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"We note that Rahimi et al. (2017), controlling less strictly for Type I error, did find correlations between lexical and acousticprosodic measures.",3.2 Pearson correlation coefficients between entrainment measures,[0],[0]
"To check for co-occurrence of different entrainment behaviors, we note, for each conversation: whether local and global partner similarity are greater than the respective non-partner similarity; whether the Pearson r defining synchrony and convergence is positive or not; whether global similarity is greater in the second half than in the first; and whether each of the lexical similarity measures between partners is greater than between non-partners.",3.3 χ2 tests,[0],[0]
"Then we use χ2 tests to check whether some behaviors are disproportionately likely to co-occur.
",3.3 χ2 tests,[0],[0]
"For SBC, we consider all of our entrainment measures at the session level.",3.3 χ2 tests,[0],[0]
"For CGC, we analyze conversations at the task level as only this gives us a sufficient number of samples (149 usable tasks after excluding 19 with too little speech by at least one speaker).",3.3 χ2 tests,[0],[0]
"We also do not analyze local or global convergence for this corpus since they are not meaningful at the task level and do not consider the lexical measures because there are too few utterances per task to make use of them.
",3.3 χ2 tests,[0],[0]
We find significant deviations from expected frequencies only for those few pairs of measurements which we found to be correlated according to Pearson’s r in Section 3.2.,3.3 χ2 tests,[0],[0]
We conclude that there is no significant co-occurrence of entrainment across features.,3.3 χ2 tests,[0],[0]
"Next, we attempt to find structure in entrainment behavior through clustering of measurements.",3.4 Clustering of entrainment measures,[0],[0]
"We analyze the same measurements as in Section 3.3,
treating each task/session as a point in a continuous 9D/18D space, respectively, and use k-means clustering to group points in this space.",3.4 Clustering of entrainment measures,[0],[0]
"In addition to the normalization described in Section 2.3, we apply z-score normalization per measure before clustering, which is a best practice.
",3.4 Clustering of entrainment measures,[0],[0]
Figure 2a shows the silhouette scores for various numbers of clusters k (solid line) for SBC.,3.4 Clustering of entrainment measures,[0],[0]
"This score, which ranges from -1 to +1, compares the similarity of points in the same cluster with those in other clusters, with higher values for greater similarity within than across clusters.",3.4 Clustering of entrainment measures,[0],[0]
"For comparison, we compute clusters after shuffling within columns of our data to remove correlations and cluster dummy data randomly sampled from standard normal distributions, the same distribution as our real data after normalization.",3.4 Clustering of entrainment measures,[0],[0]
The silhouette score is low for all values of k but for low values of k the scores achieved for the real data are greater than for the control data.,3.4 Clustering of entrainment measures,[0],[0]
"The same pattern is present in CGC, with a maximum score for k = 2 of .165 versus .13 for the shuffled data.
",3.4 Clustering of entrainment measures,[0],[0]
"For k = 2, we find that the clusters significantly separate gender pairs, for both corpora, according to χ2 analysis.",3.4 Clustering of entrainment measures,[0],[0]
"However, the same can be achieved with many randomly chosen cluster centroids.",3.4 Clustering of entrainment measures,[0],[0]
"Because of this and the low silhouette scores, we conclude that the entrainment behaviors explored here cannot be meaningfully grouped into clusters.",3.4 Clustering of entrainment measures,[0],[0]
"Lastly, we use PCA on the same data as in Section 3.4.",3.5 Principal component analysis,[0],[0]
"We find that all nine dimensions are needed to retain 99% of the variance in CGC, seven to retain 95% and six to retain 90%.",3.5 Principal component analysis,[0],[0]
"For SBC, we find
that all 18 dimensions are needed to retain 99% of variance, 15 for 95% and 13 for 90%.",3.5 Principal component analysis,[0],[0]
These reductions can mostly be attributed to the correlations between local similarity and synchrony per feature and between the lexical measures.,3.5 Principal component analysis,[0],[0]
"Thus, the analysis again confirms a lack of correlation across features since more significant dimensionality reduction would otherwise be possible.",3.5 Principal component analysis,[0],[0]
"A plot of our SBC data in 3D, shown in Figure 2b, retains 31% of the variance and visually confirms our finding of a lack of clusters.",3.5 Principal component analysis,[0],[0]
We present a corpus analysis using four different approaches to discover an underlying structure or collection of latent behaviors in 18 measures of acoustic-prosodic and lexical entrainment across two corpora.,4 Discussion and Conclusion,[0],[0]
"We find virtually no evidence of links between entrainment on different features, whether in the form of correlations or other common, complex behaviors.
",4 Discussion and Conclusion,[0],[0]
"While it is difficult to prove a negative, our results are strong enough to rule out at least the existence of any clear and strong structure.",4 Discussion and Conclusion,[0],[0]
This is contrary to the expectations we had based on cognitive theory.,4 Discussion and Conclusion,[0],[0]
"It appears that entrainment, rather than a single behavior or a structured collection of behaviors, is a set of behaviors which are only loosely linked and perhaps independently explained by the competing theories.",4 Discussion and Conclusion,[0],[0]
"Practically, we had hoped to simplify and motivate downstream uses of entrainment measures, but our findings suggest that they must be considered separately.
",4 Discussion and Conclusion,[0],[0]
"Although we expected to find complex behavior, at least the absence of entrainment across all features simultaneously can be explained with past research.",4 Discussion and Conclusion,[0],[0]
"As far as entrainment is based on “attention”, as Chartrand and Bargh (1999) suggest, this attention seems to be targeted and does not appear to result in entrainment on several features together.",4 Discussion and Conclusion,[0],[0]
"Alternatively, the absence of correlations may be explained by the fact that not all perception necessarily leads to a change in production, as Kraljic et al. (2008) found.",4 Discussion and Conclusion,[0],[0]
"Moreover, it has long been known that “too much” entrainment can be perceived negatively as mocking or patronizing (Giles and Smith, 1979).",4 Discussion and Conclusion,[0],[0]
"Furthermore, entrainment may be constrained by the need to achieve the communicative goal.",4 Discussion and Conclusion,[0],[0]
"Fusaroli and Tylén (2016), for instance, speculate based on their findings that “interpersonal synergies such
as procedural scripts and routines [. . .",4 Discussion and Conclusion,[0],[0]
] guide and constrain other central linguistic processes such as alignment”.,4 Discussion and Conclusion,[0],[0]
"Lastly, there might be cognitive and physiological limits to speakers’ ability to vary each feature individually or all at the same time.
",4 Discussion and Conclusion,[0],[0]
"Nonetheless, it remains surprising that we find a more general lack of structure, so the potential reasons warrant discussion.",4 Discussion and Conclusion,[0],[0]
"Entrainment is measured in various ways, even with regard to the same features.",4 Discussion and Conclusion,[0],[0]
"Therefore, it would be possible to continue our search using different entrainment measures on our features.",4 Discussion and Conclusion,[0],[0]
"However, all our measures meaningfully and diversely capture entrainment.",4 Discussion and Conclusion,[0],[0]
"Thus, it seems unlikely that alternative measures would yield fundamentally different outcomes, such as strong correlations across features.",4 Discussion and Conclusion,[0],[0]
"Similarly, we believe the analytical tools we employ are wellsuited and further analysis of the same features and measures would not produce disparate results.",4 Discussion and Conclusion,[0],[0]
"Since we only considered low-level features, it is, however, conceivable that more latent structure might yet be found for entrainment at higher levels, such as emotional coloring and linguistic style.
",4 Discussion and Conclusion,[0],[0]
"Despite the fact that our result is negative, we consider it a starting point of inquiry, not an end.",4 Discussion and Conclusion,[0],[0]
We intend to investigate higher-level features and perhaps additional corpora to confirm or qualify our findings.,4 Discussion and Conclusion,[0],[0]
"Beyond that, our result raises the question which principles govern the emergence of entrainment on one feature over another in a given conversation.",4 Discussion and Conclusion,[0],[0]
"As a first attempt to find an answer, we plan to use asymmetrical, speakerspecific measures of entrainment and analyze the consistency of each individual’s entrainment behavior across sessions.",4 Discussion and Conclusion,[0],[0]
This material is based upon work supported in part by the PSC-CUNY Research Award Program under Grant No. 60604-00 48.,Acknowledgments,[0],[0]
"We would also like to thank Julia Hirschberg, Štefan Beňuš, and Agustı́n Gravano for their helpful suggestions and Alyssa Caputo for her help with the project.",Acknowledgments,[0],[0]
Entrainment has been shown to occur for various linguistic features individually.,abstractText,[0],[0]
"Motivated by cognitive theories regarding linguistic entrainment, we analyze speakers’ overall entrainment behaviors and search for an underlying structure.",abstractText,[0],[0]
"We consider various measures of both acoustic-prosodic and lexical entrainment, measuring the latter with a novel application of two previously introduced methods in addition to a standard high-frequency word measure.",abstractText,[0],[0]
"We present a negative result of our search, finding no meaningful correlations, clusters, or principal components in various entrainment measures, and discuss practical and theoretical implications.",abstractText,[0],[0]
Looking for structure in lexical and acoustic-prosodic entrainment behaviors,title,[0],[0]
"Large output spaces are ubiquitous in several machine learning problems today: for example, extreme multiclass or multilabel classification problems with many classes, language modeling with big vocabularies, or metric learning with a large number of pairwise distance constraints.",1. Introduction,[0],[0]
"In all such problems, a key bottleneck in training models is evaluation of the loss function and its gradient.",1. Introduction,[0],[0]
"The loss functions used for such problems typically require an enumeration of all the possible outputs, and thus, naı̈vely, necessitate a linear running time in the number of outputs for
1Carnegie Mellon University, Pittsburgh, USA 2Google, New York, USA.",1. Introduction,[0],[0]
Correspondence to: Ian E.H. Yen,1. Introduction,[0],[0]
"<eyan@cs.cmu.edu>, Satyen Kale <satyenkale@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
evaluation.",1. Introduction,[0],[0]
"This can be a significant bottleneck in iterative methods such as gradient descent used to train the model, since each step now requires a huge number of operations.
",1. Introduction,[0],[0]
Many approaches have been proposed to mitigate this issue.,1. Introduction,[0],[0]
"One body of work imposes structure over the output space, such as low-rank (Yu et al., 2014), treestructure (Prabhu & Varma, 2014), locally low-rank (Bhatia et al., 2015), or hierarchical factorization (Morin & Bengio, 2005; Mnih & Hinton, 2009).",1. Introduction,[0],[0]
"However, structural assumptions can be violated in many situations.",1. Introduction,[0],[0]
"For example, while the low-rank structure is typically reasonable in a recommendation problem, it is usually not true in multiclass classification as for each instance there is exactly one correct answer (i.e. classes may not be correlated with each other).",1. Introduction,[0],[0]
"Additionally, even for valid structural assumptions, constructing the correct structure from data is hard, and in practice heuristics or human annotation are required (Morin & Bengio, 2005; Mnih & Hinton, 2009).
",1. Introduction,[0],[0]
"Another approach is sampling approximation (Mikolov et al., 2013; Gutmann & Hyvärinen, 2012; Jean et al., 2014), which computes an estimate of the gradient based on the scores of only a small fraction of the negative output classes and also a small set of classes labeled as positive.",1. Introduction,[0],[0]
"The approximation, however, has large variance when the loss has a skewed distribution over classes.",1. Introduction,[0],[0]
"For example, in extreme multiclass or multilabel classification, the loss typically only concentrates on a few confusing classes, which have small probabilities of being sampled.",1. Introduction,[0],[0]
"The variance in gradient estimation often leads to slow progress of the learning algorithm.
",1. Introduction,[0],[0]
"In this paper, we consider problems with large output spaces, but with each example having only a relatively small set of correct outputs.",1. Introduction,[0],[0]
"The learning objective for such tasks typically has its gradient concentrated on a relatively small number of classes, and therefore an efficient way to learn is to search for classes of significant gradient magnitude.",1. Introduction,[0],[0]
"For example, (Yen et al., 2016; 2017) proposed a method to search classes efficiently by maintaining a sparse model during training.",1. Introduction,[0],[0]
"However, this method applies only in problems of high input dimension.",1. Introduction,[0],[0]
"Another strategy that has received a lot of attention recently is to utilize data structures to find classes efficiently through Maximum Inner-Product Search (MIPS) or Nearest-Neighbor
Search (NNS) (Yen et al., 2013; Vijayanarasimhan et al., 2014; Mussmann & Ermon, 2016; Mussmann et al., 2017; Spring & Shrivastava, 2017b;a; Wu et al., 2017; Guo et al., 2016).",1. Introduction,[0],[0]
"The main challenge here is that as dimension grows, it becomes difficult to perform MIPS or NNS with both high recall and high precision, and therefore gradient approximation through MIPS or NNS often sacrifices accuracy to achieve efficiency.
",1. Introduction,[0],[0]
"In this work, we propose an algorithm based on an application of dual decomposition (Boyd et al., 2011) to the convex-conjugate representation of the loss function.",1. Introduction,[0],[0]
This can be viewed as a complementary technique for applying search data structures to a learning problem.,1. Introduction,[0],[0]
"Essentially, the algorithm replaces the high dimensional search problem with several lower dimensional searches by decoupling the dimensions via dual decomposition.",1. Introduction,[0],[0]
"Lower dimensional search can be done much more efficiently, and the different searches are then coupled together via a greedy message passing algorithm.",1. Introduction,[0],[0]
We prove that this greedy message passing technique is guaranteed to converge and thus we can obtain good approximations to the loss and its gradient.,1. Introduction,[0],[0]
"We term our overall approach LDGS for Loss Decomposition Guided Search.
",1. Introduction,[0],[0]
"Our experiments on large-scale face recognition, document tagging and word embedding show that the proposed approach significantly improves the accuracy of the searchbased gradient approximation method and is orders of magnitude faster than other strategies of gradient approximation such as sampling.",1. Introduction,[0],[0]
"Let X denote the input space and Y the output space, and let K := |Y|.",2. Problem Setup,[0],[0]
"In this paper we focus on the situation where K is extremely large, on the order of hundreds of thousands or larger.",2. Problem Setup,[0],[0]
We are interested in learning a scoring function f :,2. Problem Setup,[0],[0]
"X → RK for a large output space Y from a given class of such functions, F .",2. Problem Setup,[0],[0]
"Labeled samples are pairs (x,P) with x ∈ X and P ⊆ Y which denotes the set of correct labels for the input point x.",2. Problem Setup,[0],[0]
We use the notation N := Y \ P to denote the set of negative labels for the example.,2. Problem Setup,[0],[0]
"Given a collection of training samples {(xi,Pi)}Ni=1, the learning objective takes the following form:
min f∈F
1
N N∑ i=1",2. Problem Setup,[0],[0]
"L(f(xi),Pi).
",2. Problem Setup,[0],[0]
"where L : RK × 2Y → R is a loss function such that L(z,P) penalizes the discrepancy between the score vector z ∈ RK and a set of positive labels P ⊆ Y .",2. Problem Setup,[0],[0]
"The evaluation of the loss function and its gradient with respect to the score vector, ∇zL(z,P), typically has cost growing linearly with the size of the output space K, and thus is
expensive for problems with huge output spaces.
",2. Problem Setup,[0],[0]
"The key to our method for reducing the complexity of loss and gradient evaluation will be the following linear structural assumption on the class of scoring functions F : there is an embedding dimension parameter D ∈ N such that for every f ∈ F , we can associate a weight matrix W ∈ RK×D and feature map φ :",2. Problem Setup,[0],[0]
"X → RD so that for all x ∈ X ,
f(x) = Wφ(x).",2. Problem Setup,[0],[0]
"(1)
We will assume that D K, say on the order of a few hundreds or thousands, so that we can explicitly evaluate φ(x).
",2. Problem Setup,[0],[0]
"The problem we consider is the following: given f and a batch of samples {xi,Pi}Ni=1, compute an approximation to the empirical loss 1N ∑N i=1",2. Problem Setup,[0],[0]
"L(f(xi),Pi)",2. Problem Setup,[0],[0]
and its gradient.,2. Problem Setup,[0],[0]
"This is an important subroutine that naturally arises in either full batch gradient descent or minibatch stochastic gradient descent.
",2. Problem Setup,[0],[0]
"The main challenge here is to construct data structures that preprocess the matrix W so that good approximations to the loss f(xi,Pi) and its gradient can be computed without computing the vector f(x) entirely: i.e. we desire sublinear (in K) time computation of such approximations given access to an appropriate data structure.
",2. Problem Setup,[0],[0]
"Before proceeding to our dual decomposition based search technique, we give a few examples of problems with large output space that fit in our framework:
1.",2. Problem Setup,[0],[0]
Extreme Classification.,2. Problem Setup,[0],[0]
"In extreme classification problems, popular classification loss functions include Cross-Entropy Loss
L(z,P) := ∑ k∈P log (∑K j=1 exp(zj) )",2. Problem Setup,[0],[0]
"− zk (2)
and Max-Margin Loss L(z,P) := [
max k∈P,j∈N zj − zk + 1 ] + .",2. Problem Setup,[0],[0]
"(3)
For multiclass problems, |P| = 1, while for multilabel problems we usually have |P| K. A typical scoring function takes the form
f(x) := Wφ(x).",2. Problem Setup,[0],[0]
"(4)
Here, φ(x) is a feature map constructed either from the domain knowledge or via learning (e.g., a neural network).",2. Problem Setup,[0],[0]
"Both of them fit the structural assumption (1).
2.",2. Problem Setup,[0],[0]
Metric Learning.,2. Problem Setup,[0],[0]
"In Metric Learning problems, during training we learn a function
f(x) =",2. Problem Setup,[0],[0]
"[−d(x,y)]y∈Y , (5)
that denotes the dissimilarities of the point x to a collection of points y ∈ Y .",2. Problem Setup,[0],[0]
"Common choices of the dissimilarity function include the squared Euclidean distance d(x,y) = ‖ψ(x)−ψ(y)‖22 parameterized by a nonlinear transformation ψ",2. Problem Setup,[0],[0]
": X → Rd for some d ∈ N, and, more generally, the squared Mahalanobis distance d(x,y) = (ψ(x)−ψ(y))>M(ψ(x)−ψ(y))",2. Problem Setup,[0],[0]
parameterized by ψ and a positive definite matrixM .,2. Problem Setup,[0],[0]
"The candidate set Y could be the whole set of training samples {xi}Ni=1, or a collection of latent proxies {yk}Kk=1 as suggested by a recent state-of-the-art method (Movshovitz-Attias et al., 2017).",2. Problem Setup,[0],[0]
"For each sample (x,P), the goal is to learn a distance function s.t.",2. Problem Setup,[0],[0]
the positive candidates P are closer to x than the negative ones.,2. Problem Setup,[0],[0]
"Common loss functions for the task are Neighborhood Component Analysis (NCA) loss (Goldberger et al., 2005)
",2. Problem Setup,[0],[0]
"L(z,P) := ∑ k∈P log (∑K j=1 exp(zj) )",2. Problem Setup,[0],[0]
"− zk (6)
and the Triplet loss (Weinberger & Saul, 2009)",2. Problem Setup,[0],[0]
"L(z,P) = ∑ k∈P ∑ j∈N",2. Problem Setup,[0],[0]
[zj − zk + 1]+.,2. Problem Setup,[0],[0]
"(7)
It is easy to see that such scoring functions satisfy the structural assumption (1): for the scoring function f given by the squared Mahalanobis distance parameterized by ψ and M , the matrix W consists of the rows 〈−ψ(y)>Mψ(y), 2ψ(y)>M ,−1〉 for each y ∈ Y , and φ(x) = 〈1,ψ(x)>,ψ(x)>Mψ(x)〉>.",2. Problem Setup,[0],[0]
"Thus the embedding dimension D = d+ 2.
3.",2. Problem Setup,[0],[0]
Word Embeddings.,2. Problem Setup,[0],[0]
"In the standard word2vec training (Mikolov et al., 2013), the input space X is the vocabulary set, and the output space Y = X ; thus K is the vocabulary size.",2. Problem Setup,[0],[0]
"The Skip-gram objective learns a scoring function f of the following form:
f(x) = 〈φ(y)>φ(x)〉y∈X , (8)
where φ(·) is a latent word embedding.",2. Problem Setup,[0],[0]
This clearly fits the structural assumption (1): the rows of the matrixW are the embeddings φ(y) for all y ∈ X .,2. Problem Setup,[0],[0]
"Then given a text corpus D, the loss function1 for a sample (x,P) where P is the set of words in the corpus appearing within a certain size window around the input word x, is given by
L(z,P) = qx ∑ y∈P qy|x·[log (∑ y′∈X exp(zy′) )",2. Problem Setup,[0],[0]
"−zy] (9) where qx is the empirical unigram frequency of x and qy|x is the empirical frequency of observing y within a window of x in the corpus D.
1This is a more compact reformulation of the loss function in (Mikolov et al., 2013).
",2. Problem Setup,[0],[0]
"Algorithm 1 Loss and Gradient Approximation via Search
input A sample (x,P), accuracy parameter τ > 0, and access to a MIPS data structure T for the rows ofW .",2. Problem Setup,[0],[0]
"output Approximations to L(f(x),P),∇L(f(x),P).",2. Problem Setup,[0],[0]
1: Query T with φ(x) and threshold τ to find S := {k | |[f(x)]k| >,2. Problem Setup,[0],[0]
"τ}.
2: Construct a sparse approximation z̃ for f(x) by setting z̃k = f(x)k for k ∈ S ∪P , and z̃k = 0 for k 6∈ S ∪P .",2. Problem Setup,[0],[0]
"3: Return L(z̃,P) and ∇L(z̃,P).",2. Problem Setup,[0],[0]
All the loss functions we considered in the applications mentioned share a key feature: their value can be well approximated by the scores of the positive labels and the largest scores of the negative labels.,2.1. Loss and Gradient Approximation via Search,[0],[0]
"Similarly, their gradients are dominated by the coordinates corresponding to the positive labels and the negative labels with the largest scores.",2.1. Loss and Gradient Approximation via Search,[0],[0]
"For example, the Max-Margin loss (3) is completely determined by the largest score of the negative labels and the lowest scores of the positive labels, and its gradient is non-zero only on the negative label with largest score and the positive label with lowest score.",2.1. Loss and Gradient Approximation via Search,[0],[0]
"Similarly, for the Cross-Entropy loss (2), the coordinates of the gradient corresponding to the negative classes are dominated by the ones with the highest score; the gradient coordinates decrease exponentially as the scores decrease.
",2.1. Loss and Gradient Approximation via Search,[0],[0]
"This key property suggests the following natural idea for approximating these losses and their gradients: since the score function f satisfies the linear structural property (1), we can compute the largest scores efficiently via a Maximum Inner Product Search (MIPS) data structure (Shrivastava & Li, 2014).",2.1. Loss and Gradient Approximation via Search,[0],[0]
"This data structure stores a large data set of vectors v1,v2, . . .",2.1. Loss and Gradient Approximation via Search,[0],[0]
",vK ∈ RD and supports queries of the following form: given a target vector u ∈ RD and a threshold τ , it returns the vectors vi stored in it that satisfy |v",2.1. Loss and Gradient Approximation via Search,[0],[0]
>i u| ≥ τ,2.1. Loss and Gradient Approximation via Search,[0],[0]
in time that is typically sublinear in K.,2.1. Loss and Gradient Approximation via Search,[0],[0]
"Thus, we can preprocess W by storing the rows of W in an efficient MIPS data structure.",2.1. Loss and Gradient Approximation via Search,[0],[0]
"Then for each sample x, we can compute the highest scores by querying this data structure with the target vector φ(x) and some reasonable threshold τ , computing approximations to the loss and gradient from the returned vectors (and treating all other scores as 0).",2.1. Loss and Gradient Approximation via Search,[0],[0]
"This method is depicted in Algorithm 1.
",2.1. Loss and Gradient Approximation via Search,[0],[0]
"The error in this approximation is naturally bounded by τ times the `∞ Lipschitz constant of L(·,P).",2.1. Loss and Gradient Approximation via Search,[0],[0]
"For most loss functions considered in this paper, the `∞ Lipschitz constant is reasonably small: 2 for Max-Margin loss, O(Pmax log(K)) for Cross-Entropy loss (here, Pmax is the maximum number of positive labels for any example), etc.
",2.1. Loss and Gradient Approximation via Search,[0],[0]
"The main difficulty in applying this approach in practice
is the curse of dimensionality: the dependence on D is exponential for exact methods, and even for approximate methods, such as Locality-Sensitive Hashing, the cost still implicitly depends on the dimension as points become far apart when the intrinsic dimensionality is high (Li & Malik, 2017).
",2.1. Loss and Gradient Approximation via Search,[0],[0]
"To deal with the curse of dimensionality, we introduce a novel search technique based on dual decomposition.",2.1. Loss and Gradient Approximation via Search,[0],[0]
"This method, and its analysis, are given in the following section.
",2.1. Loss and Gradient Approximation via Search,[0],[0]
"In order to apply and analyze the technique, we need the loss functions to be smooth (i.e. have Lipschitz continuous gradients).",2.1. Loss and Gradient Approximation via Search,[0],[0]
"For non-smooth losses like Max-Margin loss (3), we apply Nesterov’s smoothing technique (Nesterov, 2005), which constructs a surrogate loss function with guaranteed approximation quality by adding a strongly convex term to the Fenchel conjugate of the loss:
Lµ(z)",2.1. Loss and Gradient Approximation via Search,[0],[0]
":= max α 〈z,α〉 −
( L∗(α) + µ
2 ‖α‖2
) .",2.1. Loss and Gradient Approximation via Search,[0],[0]
"(10)
Here, µ is a smoothing parameter that ensures that the surrogate loss has 1µ Lipschitz continuous gradients while approximating the original loss function to withinO(µ).",2.1. Loss and Gradient Approximation via Search,[0],[0]
"This Smoothed Max-Margin loss has gradient
∇L(z) := projC(z+1Nµ ) (11)
where 1N denotes a vector containing 0 for indices k ∈ P and 1 for k ∈ N , and projC(.) denotes the projection onto the bi-simplex C = {α | ∑ k∈N αk = ∑ k∈P −αk ≤ 1, αN ≥ 0, αP ≤ 0}.",2.1. Loss and Gradient Approximation via Search,[0],[0]
The Smoothed Max-Margin loss and its gradient can again be computed using the largest few scores.,2.1. Loss and Gradient Approximation via Search,[0],[0]
We now describe our loss decomposition method.,3. Loss Decomposition,[0],[0]
Recall the linear structural assumption (1): f(x) = Wφ(x) for all x ∈ X .,3. Loss Decomposition,[0],[0]
"In this section, we will keep (x,P) fixed, and we will drop the dependence on P in L for convenience and simply use the notation L(f(x)) and∇L(f(x)).
",3. Loss Decomposition,[0],[0]
"While MIPS over the D-dimensional rows of W can be computationally expensive, we can exploit the linear structure of f by decomposing it: chunking the D coordinates of the vectors in RD into B blocks, each of size D/B. Here B ∈ N is an integer; larger B leads to easier MIPS problems but reduces accuracy of approximations produced.",3. Loss Decomposition,[0],[0]
"Let W (1),W (2), . . .",3. Loss Decomposition,[0],[0]
",W (B) be the corresponding block partitioning of W obtained by grouping together the columns corresponding to the coordinates in each block.",3. Loss Decomposition,[0],[0]
"Similarly, let φ(1)(x),φ(2)(x), . . .",3. Loss Decomposition,[0],[0]
",φ(B)(x) be the conformal partitioning of the coordinates of φ(x).
",3. Loss Decomposition,[0],[0]
"Now define the overall score vector z := f(x) = Wφ(x), and per-chunk score vectors zj = W (j)φ(j)(x), for j ∈
[B].",3. Loss Decomposition,[0],[0]
"Then we have z = ∑B j=1 zj , in other words, we have a decomposition of the score vector.",3. Loss Decomposition,[0],[0]
The following theorem states that the loss of a decomposable score vector can itself be decomposed into several parts connected through a set of message variables.,3. Loss Decomposition,[0],[0]
This theorem is key to decoupling the variables into lower dimensional chunks that can be optimized separately via an efficient MIPS data structure.,3. Loss Decomposition,[0],[0]
"While this theorem can be derived by applying dual decomposition to the convex conjugate of the loss function, here we provide a simpler direct proof by construction.",3. Loss Decomposition,[0],[0]
Theorem 1.,3. Loss Decomposition,[0],[0]
"Let L : RK → R be a convex function, and let z ∈ RK be decomposed as a sum of B vectors as follows: z = ∑B j=1 zj .",3. Loss Decomposition,[0],[0]
"Then L(z) is equal to the optimum value of the following convex minimization problem:
min λj∈RK , j∈[B]
1
B B∑ j=1 L(B(zj + λj))",3. Loss Decomposition,[0],[0]
"s.t. B∑ j=1 λj = 0.
(12)
Proof.",3. Loss Decomposition,[0],[0]
"First, for any λ1,λ2, . . .",3. Loss Decomposition,[0],[0]
",λB ∈ RK such that∑B j=1 λj = 0, by Jensen’s inequality applied to the con-
vex function L, we have L(z) ≤ 1B ∑B j=1 L(B(zj +λj)).",3. Loss Decomposition,[0],[0]
"On the other hand, if we set λj = 1Bz− zj for all j ∈",3. Loss Decomposition,[0],[0]
"[B], we have L(z) = 1B ∑B j=1 L(B(zj +",3. Loss Decomposition,[0],[0]
λj)).,3. Loss Decomposition,[0],[0]
Theorem (1) is the basis for our algorithm for computing approximations to the loss and its gradient.,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"This approximation is computed by approximately solving the convex minimization problem (12) without computing the whole score vector z, using a form of descent method on the λj variables (which we refer to as “message passing”).",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
The gradient computations required for each step can be (approximately) done using an efficient MIPS data structure storing the D/B dimensional rows ofW,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
(j).,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
The details of the algorithm are given in Algorithm 2.,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"It can be viewed as running a version of the Frank-Wolfe algorithm on an appropriate convex function.
",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"A sublinear in K time implementation of step 5 in the algorithm relies on the fact that both z̃j and λj are sparse vectors, which in turn relies on the fact that gradients of the loss functions of interest are either sparse or concentrated on a few coordinates.",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"Step 9 in the algorithm moves the current solution towards the optimal solution λ∗j that we have a closed form formula for, thanks to the constructive proof of Theorem (1).",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"This movement is only done for the set of coordinates of the gradients of high magnitude identified in step 5 of the algorithm, thus ensuring that only a few coordinates are updated.",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
Thus essentially the algorithm is performing a greedy descent towards the optimal solution.,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"For more details on how the data structures are maintained in the algorithm, refer to Section 4.
",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"Algorithm 2 Greedy Message Passing
input a sample x, threshold parameters τ1, τ2 > 0, and access to B MIPS data structures Tj storing the rows ofW",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"(j), for j ∈",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"[B] output Approximation to ∇L(f(x)).
",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
1:,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
Query Tj with φ(j)(x) and threshold τ to find Sj := {k | |[zj ]k| > τ1}.,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
2: Construct a sparse approximation z̃j for zj by setting [z̃j ]k =,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"[zj ]k for k ∈ Sj ∪ P , and [z̃j ]k = 0 for k 6∈ S ∪ P .",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"3: for t = 1, 2, . . .",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"(until converged) do 4: Compute the set
A := ⋃ j∈[B] {k | |[∇L(B(z̃j +",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"λj))]k| > τ2}.
5: Compute [λ∗j ]k = 1 B [z̃]k",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
−,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
[z̃j ]k for all k ∈ A and all j ∈,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
[B].,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
6: Compute the step size η = 2t+2 .,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
7: For all k ∈ A and all j ∈,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
"[B], update
[λj ]k ← η[λ∗j ]k + (1− η)[λj ]k.
8: end for 9:",3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
Output 1B ∑B,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
j=1∇L(B(z̃j,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
+ λj)).,3.1. Loss Decomposition Guided Search (LDGS),[0],[0]
Define z̃ = ∑B j=1 z̃j .,3.2. Error Analysis,[0],[0]
Note that ‖z,3.2. Error Analysis,[0],[0]
"− z̃‖∞ ≤ Bτ1, so the error in approximating L(z) by L(z̃) is at most Bτ1 times the `∞ Lipschitz constant of L, which is typically small as explained earlier.",3.2. Error Analysis,[0],[0]
The algorithm essentially runs a FrankWolfe type method to converge to L(z̃).,3.2. Error Analysis,[0],[0]
"In the following, we analyze the convergence rate of the greedy message passing algorithm (Algorithm 2) to L(z̃).",3.2. Error Analysis,[0],[0]
The analysis relies on smoothness of the loss function.,3.2. Error Analysis,[0],[0]
"A function is said to be 1/µ-smooth if its gradients are Lipschitz continuous with constant 1/µ. For the Cross-Entropy loss (2) we have µ = 1, and for the smoothed max-margin loss (10), µ is a tunable parameter, and we found setting µ ∈",3.2. Error Analysis,[0],[0]
"[1, 5] works well in our experiments.
",3.2. Error Analysis,[0],[0]
"To analyze the algorithm, denote by Λ the BK dimensional vector 〈λ1,λ2, . . .",3.2. Error Analysis,[0],[0]
",λB〉 in any given step in the loop of the algorithm.",3.2. Error Analysis,[0],[0]
Similarly let Λ∗ denote the BK dimensional vector composed of λ∗j .,3.2. Error Analysis,[0],[0]
"Define G(Λ) = 1 B ∑B j=1 L(B(z̃j+λj)), i.e. the objective function in (12).
",3.2. Error Analysis,[0],[0]
Theorem 2 (Greedy Message Passing).,3.2. Error Analysis,[0],[0]
Suppose the loss function L is 1/µ-smooth.,3.2. Error Analysis,[0],[0]
"Then the suboptimality gap of Λ in the t-th step of the loop can be bounded as follows:
G(Λ)−G(Λ∗) ≤ 2B‖Λ",3.2. Error Analysis,[0],[0]
"∗‖2
µ(t+ 2) + 2τ2",3.2. Error Analysis,[0],[0]
"ln(t)‖Λ∗‖1
Proof.",3.2. Error Analysis,[0],[0]
"Since the loss function L is 1/µ-smooth, it is easy to check that G is B/µ-smooth.",3.2. Error Analysis,[0],[0]
"Thus, if ∆Λ is the change in Λ in a given step of the loop in the algorithm, then
G(Λ + ∆Λ)−G(Λ) ≤ η〈∇G(Λ),∆Λ〉+ η 2B
2µ ‖∆Λ‖2.
",3.2. Error Analysis,[0],[0]
Note that ∆Λ equals Λ∗−Λ in all coordinates except those corresponding to k /∈,3.2. Error Analysis,[0],[0]
A for all j ∈,3.2. Error Analysis,[0],[0]
"[B], and the magnitude of the gradient in those coordinates is at most τ2.",3.2. Error Analysis,[0],[0]
"Thus we have 〈∇G(Λ),∆Λ〉 ≤ 〈∇G(Λ),Λ∗−Λ〉+ τ2‖Λ∗‖1.",3.2. Error Analysis,[0],[0]
"Here, we used the fact that each coordinate of Λ lies between 0 and the corresponding coordinate of Λ∗. Next, by the convexity of G, we have 〈∇G(Λ),Λ∗ − Λ〉 ≤ G(Λ∗)",3.2. Error Analysis,[0],[0]
− G(Λ).,3.2. Error Analysis,[0],[0]
"Putting all the bounds together and following some algebraic manipulations, we have
G(Λ + ∆Λ)−G(Λ∗)
≤ (1− η)(G(Λ)−G(Λ∗))",3.2. Error Analysis,[0],[0]
+ ητ2‖Λ∗‖1,3.2. Error Analysis,[0],[0]
"+ η2B
2µ ‖Λ∗‖2.
(13)
Here, we used the fact that each coordinate of Λ lies between 0 and the corresponding coordinate of Λ∗ to get the bound ‖∆Λ‖2 ≤",3.2. Error Analysis,[0],[0]
"‖Λ∗‖2.
",3.2. Error Analysis,[0],[0]
"Now, using the fact that η = 2t+2 in iteration t, a simple induction on t implies the claimed bound on G(Λ)−G(Λ∗).
",3.2. Error Analysis,[0],[0]
"Thus, to ensure that the suboptimality gap is at most , it suffices to run the greedy procedure for T =",3.2. Error Analysis,[0],[0]
"B‖Λ
∗‖2 4µ steps
with τ2 = 4 ln(T )‖Λ∗‖1 .",3.2. Error Analysis,[0],[0]
"While this theorem provides a proof of convergence for the algorithm to any desired error level, the bound it provides is quite weak.",3.2. Error Analysis,[0],[0]
"In practice, we found that running just one step of the loop suffices to improve performance over direct search-based methods.
",3.2. Error Analysis,[0],[0]
"If, in addition to being smooth, the loss function is also strongly convex (which can be achieved by adding some `22 regularization, for instance) then we can also show convergence of the gradients.",3.2. Error Analysis,[0],[0]
This is because for strongly convex functions the convergence of gradients can be bounded in terms of the convergence of the loss value.,3.2. Error Analysis,[0],[0]
"This is a very standard analysis and we omit it for the sake of clarity.
",3.2. Error Analysis,[0],[0]
Cost Analysis.,3.2. Error Analysis,[0],[0]
Exact gradient evaluation for a single sample can be computed in O(DK) time.,3.2. Error Analysis,[0],[0]
"Directly applying a search-based gradient approximation (Algorithm 1) has a cost ofO(DQD(K)),whereQD(K) is the number of classes retrieved in the MIPS data structure in order to find all classes of significant gradients.",3.2. Error Analysis,[0],[0]
"The query cost QD(K) has a strong dependency on the dimension D. Exact MIPS has a cost QD(K) exponential in D (Shrivastava & Li, 2014; Li & Malik, 2017).",3.2. Error Analysis,[0],[0]
"For approximate search methods,
such as Locality Sensitive Hashing (LSH), the costQD(K) typically only implicitly depends on the dimension.",3.2. Error Analysis,[0],[0]
Our method (Algorithm 2) dividesD,3.2. Error Analysis,[0],[0]
"intoB subproblems of dimension D/B with a cost per message passing iteration of O(DQD/B(K)+DB|A|), whereA is the set computed in step 4 of Algorithm 2.",3.2. Error Analysis,[0],[0]
Note QD/B(K) decreases with B rapidly (exponentially in the exact case) and therefore one can select B such that QD/B(K) QD(K) and balance two terms s.t. (DQD/B(K),3.2. Error Analysis,[0],[0]
+DB|A|) DK.,3.2. Error Analysis,[0],[0]
MIPS queries.,4. Practical Considerations,[0],[0]
"In practice when using the MIPS data strcuctures, instead of retrieving all classes with scores more than the threshold τ1, it is more efficient to retrieve the top Q classes with the highest scores.",4. Practical Considerations,[0],[0]
"In our implementation, we use Spherical Clustering (Auvolat et al., 2015) as the MIPS data structure, where the number of clusters C is selected such that K/C ≤ Q and C ≤ Q.",4. Practical Considerations,[0],[0]
"Note this requires Q ≥ √ K, leading to a speedup bounded by√
K. Similarly, for computing the active set A in step 4 of Algorithm 2, we can compute an appropriate threshold τ2 using the properties of the loss function.",4. Practical Considerations,[0.9518369071537289],"['In the common setting for learning, a set of independent samples, S = {zi}mi=1, from a space of examples Z , is given, each sample drawn from an unknown probability distribution D, namely zi ∼ D. We will use the notation S ∼ Dm to denote the distribution over the full sample.']"
"In the case of margin-based losses, (3) and (7), and their smoothed versions (10), the gradient is sparse so τ2 can be set to 0 or some very small value (τ2 = 10−3 works well in our experiments).",4. Practical Considerations,[0],[0]
"Loss functions like (2), (6) typically have exponentially decayed gradient magnitudes over the nonconfusing negative classes.",4. Practical Considerations,[0],[0]
"For these losses, classes can be retrieved in decreasing order of gradient magnitude, using a lower bound on the partition function Z = ∑ k exp zk summing over only the subset of retrieved classes in order to decide whether more classes need to be retrieved or not.
",4. Practical Considerations,[0],[0]
Updates of data structures.,4. Practical Considerations,[0],[0]
"During training the model parameters determining f will change, and the data structures Tj need to be updated.",4. Practical Considerations,[0],[0]
These data structures stores rows of W and treats φ(x) as query.,4. Practical Considerations,[0],[0]
"For loss functions with a sparse gradient, such as (3) and (7), and their smoothed versions (10), the number of updated rows ofW , kr, is much smaller than K and Q (the number of classes retrieved for a query).",4. Practical Considerations,[0],[0]
"Thus the cost for re-indexing rows ofW is krC(D/B)B = krCD, where C is the number of inner products required to index each row, which is much smaller than the costs of query and updates.",4. Practical Considerations,[0],[0]
"For tasks with large number of updated rows (kr ≈ Q), the method is still effective with a larger mini-batch size Nb.",4. Practical Considerations,[0],[0]
"As the costs of query and updates grow with Nb while the number of rows to re-index is bounded by K, the cost of maintaining data structure becomes insignificant.
",4. Practical Considerations,[0],[0]
Sampling for initialization.,4. Practical Considerations,[0],[0]
"For a randomly initialized model, the early iterates of learning have gradients evenly distributed over the classes, as the scores of all classes are
close to each other.",4. Practical Considerations,[0],[0]
"Therefore, it is unnecessary to search candidates of significant gradient magnitude in the early stage.",4. Practical Considerations,[0],[0]
"In practice, one can switch from a sampling-based gradient approximation to a search-based gradient approximation after a number of mini-batch updates.",4. Practical Considerations,[0],[0]
"In our experiments of unsupervised learning of word embeddings, we initialize the algorithm with a single epoch of SGD with sampling gradient approximation.",4. Practical Considerations,[0],[0]
"In this section, we conduct experiments on three types of problems: (i) multiclass classification (face recognition), (ii) multilabel classification (document tagging), and (iii) Unsupervised Word Embedding (Skip-gram objective (9)).",5. Experiments,[0],[0]
"For multiclass and multilabel classification, we employ a Stochastic Gradient Descent (SGD) optimization algorithm, with an initial step size chosen from {1, 0.1, 0.01} for the best performance of each method, with a 1/(1 + t) cooling scheme where t is the iteration counter.",5. Experiments,[0],[0]
"The minibatch size is 10 and all methods are parallelized with 10 CPU cores in a shared-memory architecture, running on a dedicated machine.",5. Experiments,[0],[0]
All the implementation are in C++.,5. Experiments,[0],[0]
"The following loss functions and gradient evaluation methods are compared for the experiments on multiclass and multilabel classification:
• Softmax: exact gradient evaluation of the crossentropy loss (2).",5. Experiments,[0],[0]
"For multiclass, we have |P| = 1 and for multilabel, |P| K.
• Sampled-Softmax: the sampling strategy in (Jean et al., 2014; Chen et al., 2015), which includes all positive classes of the instances and uniformly subsamples from the remaining negative classes.",5. Experiments,[0],[0]
"Here we choose sample size as K/100.
",5. Experiments,[0],[0]
•,5. Experiments,[0],[0]
"Margin: exact gradient evaluation of the smoothed max-margin loss (10), where we choose µ = 1 for the case of multiclass, and µ = 5 for the case of multilabel.",5. Experiments,[0],[0]
"The bi-simplex projection (11) is computed in O(K logK) using the procedure described in (Yen et al., 2016).",5. Experiments,[0],[0]
"Note the gradient update for this loss is faster than that for cross-entropy, as the loss gradient is very sparse, making the backward pass much faster.
",5. Experiments,[0],[0]
• MIPS: search-based gradient evaluation (Algorithm 1) with smoothed max-margin loss (same setting to Margin).,5. Experiments,[0],[0]
"We use Spherical Clustering (Auvolat et al., 2015) with 100 centroids as the MIPS data structure, and a batch query of size K/100.
",5. Experiments,[0],[0]
"• Decomp-MIPS: gradient evaluation via decomposed search (Algorithm 2, T = 1 iteration).",5. Experiments,[0],[0]
We divide the inner product into B = 8 factors in the multiclass experiment and B = 4 in the multilabel case.,5. Experiments,[0],[0]
The settings for MIPS data structure are the same as above.,5. Experiments,[0],[0]
"For multiclass classification we conduct experiments on the largest publicly available facial recognition dataset MegaFace (Challenge 2)2, where each identity is considered a class, and each sample is an image cropped by a face detector.",5.1. Multiclass Classificatoin,[0],[0]
"The data set statistics are shown in Table 1.
",5.1. Multiclass Classificatoin,[0],[0]
"We employ the FaceNet architecture (Schroff et al., 2015)3 pre-trained on the MS-Celeb-1M dataset, and fine-tune its last layer on the MegaFace dataset.",5.1. Multiclass Classificatoin,[0],[0]
"The input of the last layer is an embedding of size 128, which is divided into B = 8 factors, each of dimension 16, in the Decomp-MIPS method.
",5.1. Multiclass Classificatoin,[0],[0]
"The result is shown in Figure 1, where all methods are run for more than one day.",5.1. Multiclass Classificatoin,[0],[0]
"Firstly, comparing methods
2http://megaface.cs.washington.edu/. 3github.com/davidsandberg/facenet
that optimize the (smoothed) max-margin loss (DecompMIPS, MIPS and Margin) shows that both Decomp-MIPS, MIPS speed up the iterates by 1 ∼ 2 orders of magnitude.",5.1. Multiclass Classificatoin,[0],[0]
"However, MIPS converges at an accuracy much lower than Decomp-MIPS and the gap gets bigger when running for more iterations.",5.1. Multiclass Classificatoin,[0],[0]
Note the time and epochs are in log scale.,5.1. Multiclass Classificatoin,[0],[0]
"Secondly, Softmax has a much slower progress compared to Margin.",5.1. Multiclass Classificatoin,[0],[0]
"Note both of them do not even finish one epoch (4.7M samples) after one day, while the progress of Margin is much better, presumably because its focus on the confusing identities.",5.1. Multiclass Classificatoin,[0],[0]
"Sampled-Softmax has much faster iterates, but the progress per iterate is small, leading to slower overall progress compared to the MIPS-based approaches.",5.1. Multiclass Classificatoin,[0],[0]
"For multilabel classification, we conduct experiments on WikiLSHTC (Partalas et al., 2015), a benchmark data set in the Extreme Classification Repository4, where each class is a catalog tag in the Wikipedia, and each sample is a document with bag of words representation.",5.2. Multilabel Classification,[0],[0]
"The data statistics
4manikvarma.org/downloads/XC/ XMLRepository.html
are shown in Table 2.
",5.2. Multilabel Classification,[0],[0]
We train a one-hidden-layer fully-connected feedforward network for the multilabel classification task.,5.2. Multilabel Classification,[0],[0]
The first layer has input dimension equal to the vocabulary size (1.6M) and an output of dimension 100.,5.2. Multilabel Classification,[0],[0]
"The second layer has output size equal to the number of classes (325K), with different loss functions and approximations for different methods in comparison.",5.2. Multilabel Classification,[0],[0]
The training result also produces document and work embedding as by-products.,5.2. Multilabel Classification,[0],[0]
"For Decomp-MIPS, the input of the last layer is divided into B = 4 factors, each of dimension 25.
",5.2. Multilabel Classification,[0],[0]
We run all the compared methods for more than one day and the result is shown in Figure 2.,5.2. Multilabel Classification,[0],[0]
"First, for this multilabel task, Softmax has very good per-iteration progress, significantly more than that from the other three approaches based on the smoothed max-margin loss (Margin, MIPS, Decomp-MIPS).",5.2. Multilabel Classification,[0],[0]
"However, the iterates of Softmax are much slower than the others as it has a dense loss gradient and thus a slower backpropagation, so that when comparing training time, Softmax performs similarly to Margin.",5.2. Multilabel Classification,[0],[0]
"On the other hand, when comparing Margin Decomp-MIPS, and MIPS in progress per epoch, the updates of DecompMIPS achieve almost the same progress as the exact gradient calculation of Margin, while MIPS has a significant drop in its training accuracy compared with Margin and Decomp-MIPS, since it runs for more iterations.",5.2. Multilabel Classification,[0],[0]
"Overall, the MIPS-based methods lead to an order of magnitude speedup, while Decomp-MIPS retains the accuracy of the exact method.",5.2. Multilabel Classification,[0],[0]
"On the other hand, Sampled-Softmax has an extremely slow per-iteration progress despite its fast iterates, and could not reach a comparable accuracy to other methods even after one day.",5.2. Multilabel Classification,[0],[0]
"In this section, we evaluate the proposed gradient approximation technique on the word embedding task with the Skip-gram learning objective (9) and compare it with two widely-used gradient approximation methods — Hierarchical Softmax (Word2vec-HS) and Negative Sampling (Word2vec-Neg) (Mikolov et al., 2013) implemented in the
word2vec5 package released by the authors.",5.3. Unsupervised Word Embedding,[0],[0]
"The sample size for Word2vec-Neg is selected from {5, 10, 15, 20, 25}.
",5.3. Unsupervised Word Embedding,[0],[0]
We use the benchmark data set BillonW6 of almost a half million vocabulary size.,5.3. Unsupervised Word Embedding,[0],[0]
The data statistics are provided in Table 3.,5.3. Unsupervised Word Embedding,[0],[0]
"Following (Mikolov et al., 2013), we use a window of size 8 and subsample frequent words in the corpus.",5.3. Unsupervised Word Embedding,[0],[0]
"Each word w is dropped with probability max{1 − √ t fw , 0} where fw is the relative frequency of the word in the corpus, and t = 10−4 is a threshold parameter.
",5.3. Unsupervised Word Embedding,[0],[0]
"Note that the Skip-gram objective (9) is presented in a collapsed form equivalent to the one in (Mikolov et al., 2013).",5.3. Unsupervised Word Embedding,[0],[0]
"Here, all terms of the same input-output pairs are grouped together and weighted by the frequency.",5.3. Unsupervised Word Embedding,[0],[0]
"We compute gradients from the positive outputs by summing over the empirical input-output distribution qx, qy|x in (9).",5.3. Unsupervised Word Embedding,[0],[0]
Then we perform gradient descent (GD) updates on the parameters of input words {φ(x)}x∈X and output words {φ(y)}y∈X alternately.,5.3. Unsupervised Word Embedding,[0],[0]
"We use GD, GD-MIPS and GD-Decomp-MIPS to denote the algorithm with different strategies of loss approximations.",5.3. Unsupervised Word Embedding,[0],[0]
"As mentioned in Section 4, since in the early iterates the model has quite evenly distributed gradient over candidates, we use 1 epoch of Word2vec-Neg to initialize GD, GD-MIPS and GD-Decomp-MIPS.",5.3. Unsupervised Word Embedding,[0],[0]
"For this task, we have many more negative classes of significant gradient magnitude than in the multilabel and multiclass experiments.",5.3. Unsupervised Word Embedding,[0],[0]
So we use a batch query of size K/20 instead of K/100 to the MIPS structure.,5.3. Unsupervised Word Embedding,[0],[0]
"All the compared methods are parallelized with 24 CPU cores.
",5.3. Unsupervised Word Embedding,[0],[0]
The results are shown in Figure 3.,5.3. Unsupervised Word Embedding,[0],[0]
"After the first epoch, methods based on alternating gradient descent (GD) (with the collapsed objective (9)) have faster convergence per epoch, and the iterations of GD-Deomp-MIPS are 5 times faster than those of GD while having a significantly better objective value than GD-MIPS for the same training time.
",5.3. Unsupervised Word Embedding,[0],[0]
5code.google.com/archive/p/word2vec/ 6www.statmt.org/lm-benchmark/,5.3. Unsupervised Word Embedding,[0],[0]
I.Y. and P.R. acknowledge the support of NSF via IIS1149803.,Acknowledgements,[0],[0]
"For problems with large output spaces, evaluation of the loss function and its gradient are expensive, typically taking linear time in the size of the output space.",abstractText,[0],[0]
"Recently, methods have been developed to speed up learning via efficient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS).",abstractText,[0],[0]
"However, the performance of such data structures typically degrades in high dimensions.",abstractText,[0],[0]
"In this work, we propose a novel technique to reduce the intractable high dimensional search problem to several much more tractable lower dimensional ones via dual decomposition of the loss function.",abstractText,[0],[0]
"At the same time, we demonstrate guaranteed convergence to the original loss via a greedy message passing procedure.",abstractText,[0],[0]
"In our experiments on multiclass and multilabel classification with hundreds of thousands of classes, as well as training skip-gram word embeddings with a vocabulary size of half a million, our technique consistently improves the accuracy of search-based gradient approximation methods and outperforms sampling-based gradient approximation methods by a large margin.",abstractText,[0],[0]
Loss Decomposition for Fast Learning in Large Output Spaces,title,[0],[0]
"discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",text,[0],[0]
In this work we are concerned with the fundamental problem of sampling from a discrete probability distribution and evaluating its normalizing constant.,1. Introduction,[0],[0]
A probability distribution p on a discrete sample space X is provided in terms of its potential function : X !,1. Introduction,[0],[0]
"[ 1,1), corresponding to log-unnormalized probabilities via p(x) = e (x)/Z, where the normalizing constant Z is the partition function.",1. Introduction,[0],[0]
"In this context, p is the Gibbs distribution on X associated with the potential function .",1. Introduction,[0],[0]
"The challenges of sampling from such a discrete probability distribution and estimating the partition function are fundamental problems with ubiq-
1
University of Cambridge, UK
2
MPI-IS, T¨ubingen, Germany
3
UC Berkeley, USA
4
Uber AI Labs, USA
5
Alan Turing Institute,
UK.",1. Introduction,[0],[0]
Correspondence to: Matej Balog <first.last@gmail.com>.,1. Introduction,[0],[0]
"Code: https://github.com/matejbalog/gumbel-relatives.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
uitous applications in machine learning, classical statistics and statistical physics (see, e.g., Lauritzen, 1996).",1. Introduction,[0],[0]
"Perturb-and-MAP methods (Papandreou & Yuille, 2010) constitute a class of randomized algorithms for estimating partition functions and sampling from Gibbs distributions, which operate by randomly perturbing the corresponding potential functions and employing maximum a posteriori (MAP) solvers on the perturbed models to find a maximum probability configuration.",1. Introduction,[0],[0]
"This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009).",1. Introduction,[0],[0]
"Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems.",1. Introduction,[0],[0]
"The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model).
",1. Introduction,[0],[0]
"The Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration’s potential (x).",1. Introduction,[0],[0]
We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways – in particular using the Weibull and Fr´echet distributions alongside the Gumbel.,1. Introduction,[0],[0]
"We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function.
",1. Introduction,[0],[0]
Evaluating or perturbing each configuration’s potential with i.i.d.,1. Introduction,[0],[0]
Gumbel noise can be computationally expensive.,1. Introduction,[0],[0]
"One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016).",1. Introduction,[0],[0]
"Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d.",1. Introduction,[0],[0]
Gumbel noise with a “low-rank” approximation.,1. Introduction,[0],[0]
"Hazan & Jaakkola (2012); Hazan et al. (2013) showed that from such an approximation, upper and lower bounds on the partition function and a sequential sampler for the Gibbs distribution can still be recovered.",1. Introduction,[0],[0]
"We show that a subfamily of our new methods, consisting of Fr´echet, Exponential and Weibull tricks, can also be used with low-
rank perturbations, and use these tricks to derive new upper and lower bounds on the partition function, and to construct new sequential samplers for the Gibbs distribution.
",1. Introduction,[0],[0]
"Our main contributions are as follows:
1.",1. Introduction,[0],[0]
"A family of tricks that can be implemented by simply
averaging Gumbel perturbations in different spaces, and which can lead to more accurate or more sample effi-
cient estimators of Z (Section 2).",1. Introduction,[0],[0]
2.,1. Introduction,[0],[0]
"New upper and lower bounds on the partition function of
a discrete graphical model computable using low-rank perturbations, and a corresponding family of sequential samplers for the Gibbs distribution (Section 3).",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"Discussion of advantages of the simpler analytical form
of the Gumbel trick including new links between the errors of estimating Z, sampling, and entropy estimation using low-rank Gumbel perturbations (Section 4).
",1. Introduction,[0],[0]
"Background and Related work The idea of perturbing the potential function of a discrete graphical model in order to sample from its associated Gibbs distribution was introduced by Papandreou & Yuille (2011), inspired by their previous work on reducing the sampling problem for Gaussian Markov random fields to the problem of finding the mean, using independent local perturbations of each Gaussian factor (Papandreou & Yuille, 2010).",1. Introduction,[0],[0]
"Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems.",1. Introduction,[0],[0]
"Hazan & Jaakkola (2012) pointed out that MAP perturbations are useful not only for sampling the Gibbs distribution (considering the argmax of the perturbed model), but also for bounding and approximating the partition function (by considering the value of the max).
",1. Introduction,[0],[0]
"Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities.",1. Introduction,[0],[0]
"Due to the low-rank approximation, this algorithm has the option to reject a sample.",1. Introduction,[0],[0]
Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability.,1. Introduction,[0],[0]
"Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation.
",1. Introduction,[0],[0]
"Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling.",1. Introduction,[0],[0]
"Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races.",1. Introduction,[0],[0]
"This recent view generally brings together perturb-
and-MAP and accept-reject samplers, exploiting the connection between the Gumbel distribution and competing exponential clocks that we also discuss in Section 2.1.
",1. Introduction,[0],[0]
"Inspired by A* sampling, Kim et al. (2016) proposed an exact sampler for discrete graphical models based on lazilyinstantiated random perturbations, which uses linear programming relaxations to prune the optimization space.",1. Introduction,[0],[0]
"Further recent applications of perturb-and-MAP include structured prediction in computer vision (Bertasius et al., 2017) and turning the discrete sampling problem into an optimization task that can be cast as a multi-armed bandit problem (Chen & Ghahramani, 2016), see Section 5.2 below.
",1. Introduction,[0],[0]
"In addition to perturb-and-MAP methods, we are aware of three other approaches to estimate the partition function of a discrete graphical model via MAP solver calls.",1. Introduction,[0],[0]
"The WISH method (weighted-integrals-and-sums-by-hashing, Ermon et al., 2013) relies on repeated MAP inference calls applied to the model after subjecting it to random hash constraints.",1. Introduction,[0],[0]
"The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015).",1. Introduction,[0],[0]
"Weller & Jebara (2014a) instead use just one MAP call over a discretized mesh of marginals to approximate the Bethe partition function, which itself is an estimate (which often performs well) of the true partition function.",1. Introduction,[0],[0]
"In this section, we review the Gumbel trick and state the mechanism by which it can be generalized into an entire family of tricks.",2. Relatives of the Gumbel Trick,[0],[0]
"We show how these tricks can equivalently be viewed as averaging standard Gumbel perturbations in different spaces, instantiate several examples, and compare the various tricks’ properties.
",2. Relatives of the Gumbel Trick,[0],[0]
"Notation Throughout this paper, let X be a finite sample space of size N := |X |.",2. Relatives of the Gumbel Trick,[0],[0]
Let p̃ : X !,2. Relatives of the Gumbel Trick,[0],[0]
"[0,1) be an unnormalized mass function over X and let Z := P
x2X p̃(x) be its normalizing partition function.",2. Relatives of the Gumbel Trick,[0],[0]
"Write p(x) := p̃(x)/Z for the normalized version of p̃, and (x) := ln p̃(x) for the log-unnormalized probabilities, i.e. the potential function.
",2. Relatives of the Gumbel Trick,[0],[0]
We write Exp( ) for the exponential distribution with rate (inverse mean) and Gumbel(µ) for the Gumbel distribution with location µ and scale 1.,2. Relatives of the Gumbel Trick,[0],[0]
"The latter has mean µ+ c, where c ⇡ 0.5772 is the Euler-Mascheroni constant.",2. Relatives of the Gumbel Trick,[0],[0]
"Similarly to the connection between the Gumbel trick and the Poisson process established by Maddison (2016), we introduce the Gumbel trick for discrete probability distributions using a simple and elegant construction via competing exponential clocks.",2.1. The Gumbel Trick,[0],[0]
"Consider N independent clocks,
started simultaneously, such that the j-th clock rings after a random time T
j ⇠ Exp( j ).",2.1. The Gumbel Trick,[0],[0]
"Then it is easy to show that
(1) the time until some clock rings has Exp(
P
N j=1 j ) dis-
tribution, and (2) the probability of the j-th clock ringing first is proportional to its rate
j
.",2.1. The Gumbel Trick,[0],[0]
"These properties are also
widely used in survival analysis (Cox & Oakes, 1984).",2.1. The Gumbel Trick,[0],[0]
"Consider N competing exponential clocks {T x } x2X , indexed by elements of X , with respective rates x
= p̃(x).",2.1. The Gumbel Trick,[0],[0]
"Property (1) of competing exponential clocks tells us that
min x2X {T x } ⇠ Exp(Z).",2.1. The Gumbel Trick,[0],[0]
"(1)
Property (2) says that the random variable argmin
x
T x , tak-
ing values in X , is distributed according to p:
argmin x2X {T x } ⇠ p. (2)
",2.1. The Gumbel Trick,[0],[0]
The Gumbel trick is obtained by applying the function g(x) =,2.1. The Gumbel Trick,[0],[0]
lnx c to the equalities in distribution (1) and (2).,2.1. The Gumbel Trick,[0],[0]
"When g is applied to an Exp( ) random variable, the result follows the Gumbel( c + ln ) distribution, which can also be represented as ln + , where
⇠ Gumbel( c).",2.1. The Gumbel Trick,[0],[0]
Defining { (x)} x2X i.i.d.⇠,2.1. The Gumbel Trick,[0],[0]
"Gumbel( c) and noting that g is strictly decreasing, applying the function g to equalities in distribution (1) and (2), we obtain:
max x2X { (x) +",2.1. The Gumbel Trick,[0],[0]
"(x)} ⇠ Gumbel( c+ lnZ), (1’)
argmax x2X { (x) + (x)} ⇠ p, (2’)
where we have recalled that (x) =",2.1. The Gumbel Trick,[0],[0]
ln x = ln p̃(x).,2.1. The Gumbel Trick,[0],[0]
"The distribution Gumbel( c + lnZ) has mean lnZ, and thus the log partition function can be estimated by averaging samples (Hazan & Jaakkola, 2012).",2.1. The Gumbel Trick,[0],[0]
"Given the equality in distribution (1), we can treat the problem of estimating the partition function Z as a parameter estimation problem for the exponential distribution.",2.2. Constructing New Tricks,[0],[0]
Applying the function g(x) =,2.2. Constructing New Tricks,[0],[0]
lnx c,2.2. Constructing New Tricks,[0],[0]
"as in the Gumbel trick to obtain a Gumbel( c+ lnZ) random variable, and
estimating its mean to obtain an unbiased estimator of lnZ, is just one way of inferring information about Z.
We consider applying different functions g to (1); particularly those functions g that transform the exponential distribution to another distribution with known mean.",2.2. Constructing New Tricks,[0],[0]
"As the original exponential distribution has rate Z, the transformed distribution will have mean f(Z), where f will in general no longer be the logarithm function.",2.2. Constructing New Tricks,[0],[0]
"Since we often are interested in estimating various transformations f(Z) of Z, this provides us a with a collection of unbiased estimators from which to choose.",2.2. Constructing New Tricks,[0],[0]
"Moreover, further transforming these estimators yields a collection of (biased) estimators for other transformations of Z, including Z itself.",2.2. Constructing New Tricks,[0],[0]
Example 1 (Weibull tricks).,2.2. Constructing New Tricks,[0],[0]
"For any ↵ > 0, applying the function g(x) = x↵ to an Exp( ) random variable yields a random variable with the Weibull( ↵,↵ 1) distribution with scale ↵ and shape ↵ 1, which has mean ↵ (1 + ↵) and can be also represented as ↵W , where W ⇠ Weibull(1,↵ 1).",2.2. Constructing New Tricks,[0],[0]
"Defining {W (x)} x2X
i.i.d.⇠ Weibull(1,↵ 1) and noting that g is increasing, applying g to the equality in distribution (1) gives
min x2X {p̃ ↵W (x)} ⇠Weibull(Z ↵,↵ 1).",2.2. Constructing New Tricks,[0],[0]
"(1”)
Estimating the mean of Weibull(Z ↵,↵ 1) yields an unbiased estimator of Z ↵ (1 + ↵).",2.2. Constructing New Tricks,[0],[0]
"The special case ↵ = 1 corresponds to the identity function g(x) = x; we call the resulting trick the Exponential trick.
",2.2. Constructing New Tricks,[0],[0]
Table 1 lists several examples of tricks derived this way.,2.2. Constructing New Tricks,[0],[0]
"As Example 1 shows, these tricks may not involve additive perturbation of the potential function (x); the Weibull tricks multiplicatively perturb exponentiated unnormalized probabilities",2.2. Constructing New Tricks,[0],[0]
p̃ ↵ with Weibull noise.,2.2. Constructing New Tricks,[0],[0]
"As models of interest are often specified in terms of potential functions, to be able to reuse existing MAP solvers in a black-box manner with the new tricks, we seek an equivalent formulation in terms of the potential function.",2.2. Constructing New Tricks,[0],[0]
"The following Proposition shows that by not passing the function g through the minimization in equation (1), the new tricks can be equivalently formulated as averaging additive Gumbel perturbations of the potential function in different spaces.
",2.2. Constructing New Tricks,[0],[0]
Proposition 2.,2.2. Constructing New Tricks,[0],[0]
"For any function g : [0,1)!",2.2. Constructing New Tricks,[0],[0]
"R such that f(Z) = E
T⇠Exp(Z)[g(T )] exists, we have
f(Z) =",2.2. Constructing New Tricks,[0],[0]
"E

g
✓
e c exp
✓
max x2X
{ (x) +",2.2. Constructing New Tricks,[0],[0]
"(x)} ◆◆ ,
where { (x)} x2X i.i.d.⇠",2.2. Constructing New Tricks,[0],[0]
"Gumbel( c).
",2.2. Constructing New Tricks,[0],[0]
Proof.,2.2. Constructing New Tricks,[0],[0]
As max x { (x) +,2.2. Constructing New Tricks,[0],[0]
"(x)} ⇠ Gumbel( c + lnZ), we have e c exp(max
x { (x)+ (x)}) ⇠ Exp(Z) and the result follows by the assumption relating f and g.
Proposition 2 shows that the new tricks can be implemented by solving the same MAP problems max
x { (x)+ (x)} as in the Gumbel trick, and then merely passing the solutions through the function x 7!",2.2. Constructing New Tricks,[0],[0]
g(e c,2.2. Constructing New Tricks,[0],[0]
exp(x)),2.2. Constructing New Tricks,[0],[0]
before averaging them to approximate the expectation.,2.2. Constructing New Tricks,[0],[0]
"The Delta method (Casella & Berger, 2002) is a simple technique for assessing the asymptotic variance of estimators that are obtained by a differentiable transformation of an estimator with known variance.",2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
"The last column in Table 1 lists asymptotic variances of corresponding tricks when unbiased estimators of f(Z) are passed through the function f 1 to yield (biased, but consistent and non-negative) estimators of Z itself.",2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
It is interesting to examine the constants that multiply Z2 in some of the obtained asymptotic variance expressions for the different tricks.,2.3.1. ASYMPTOTIC EFFICIENCY,[0.9558820404993953],"['Since the bound holds uniformly for all Q, it is ensured to hold also for the minimizer of the objective Q∗.']"
"For example, it can be shown using Gurland’s ratio (Gurland, 1956) that this constant is at least 1 for the Weibull and Fr´echet tricks, which is precisely the value achieved by the Exponential trick (which corresponds to ↵ = 1).",2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
"Moreover, the Gumbel trick constant ⇡2/6 can be shown to be the limit as ↵ ! 0",2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
of the Weibull and Fr´echet trick constants,2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
.,2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
"In particular, the constant of the Exponential trick is strictly better than that of the standard Gumbel trick: 1 < ⇡2/6 ⇡ 1.65.",2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
This motivates us to compare the Gumbel and Exponential tricks in more detail.,2.3.1. ASYMPTOTIC EFFICIENCY,[0],[0]
"For estimators Y , their MSE(Y ) =",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
var(Y ),2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
+ bias(Y )2 is a commonly used comparison metric.,2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"When the Gumbel or Exponential tricks are used to estimate either Z or lnZ, the biases, variances, and MSEs of the estimators can be computed analytically using standard methods (Appendix A).
",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"For example, the unbiased estimator of lnZ from the Gumbel trick can be turned into a consistent non-negative estimator of Z by exponentiation: Y = exp( 1 M P M m=1 X m ), where X 1 , . . .",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
", X M
i.i.d.⇠",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
Gumbel( c + lnZ) are obtained using equation (1’).,2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"The bias and variance of Y can be computed using independence and the moment generating functions of the X m ’s, see Appendix A for details.
",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"Perhaps surprisingly, all estimator properties only depend on the true value of Z and not on the structure of the model (distribution p), since the estimators rely only on i.i.d. samples of a Gumbel( c + lnZ) random variable.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
Figure 1 shows the analytically computed estimator variances and MSEs.,2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"For estimating Z itself (left), the Exponential trick outperforms the Gumbel trick in terms of MSE for all sample sizes M 3 (for M 2 {1, 2}, both estimators have infinite variance and MSE).",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"The ratio of MSEs quickly approaches ⇡2/6, and in this regime the Exponential trick requires 1 6/⇡2 ⇡ 39% fewer samples than the Gumbel trick to reach the same MSE.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"Also, for estimating lnZ, (Figure 1, right), the Exponential trick provides a lower MSE estimator for sample sizes M 2; only for M = 1 the Gumbel trick provides a better estimator.
",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"Note that as biases are available analytically, the estimators can be easily debiased (by subtracting their bias).",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"One then obtain estimators with MSEs equal to the variances of the original estimators, shown dashed in Figure 1.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"The Exponential trick would then always outperform the Gumbel trick when estimating lnZ, even with sample size M = 1.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"For Weibull tricks with ↵ 6= 1 and Fr´echet tricks, we estimated the biases and variances of estimators of Z and lnZ by constructing K = 100, 000 estimators in each case and evaluating their bias and variance.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
Figure 2 shows the results for varying ↵ and several sample sizes M .,2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"We plot the
analytically computed value for the Gumbel trick at ↵ = 0, as we observe that the Weibull trick interpolates between the Gumbel trick and the Exponential trick as ↵ increases from 0 to 1.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"We note that the minimum MSE estimator is obtained by choosing a value of ↵ that is close to 1, i.e. the Exponential trick.",2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
This agrees with the finding from Section 2.3.1 that ↵ = 1 is optimal as M !1.,2.3.2. MEAN SQUARED ERROR (MSE),[0],[0]
"A Bayesian approach exposes two choices when constructing estimators of Z, or of its transformations f(Z):
1.",2.4. Bayesian Perspective,[0],[0]
"A choice of prior distribution p 0 (Z), encoding prior beliefs about the value of Z before any observations.",2.4. Bayesian Perspective,[0],[0]
2.,2.4. Bayesian Perspective,[0],[0]
"A choice of how to summarize the posterior distribu-
tion p M (Z|X 1 , . . .",2.4. Bayesian Perspective,[0],[0]
", X M ) given M samples.
",2.4. Bayesian Perspective,[0],[0]
Taking the Jeffrey’s prior p 0,2.4. Bayesian Perspective,[0],[0]
"(Z) / Z 1, an improper prior that it is invariant under reparametrization, observing M samples X 1 , . . .",2.4. Bayesian Perspective,[0],[0]
", X M i.i.d.⇠ Exp(Z) yields the posterior:
p M (Z|X 1 , . . .",2.4. Bayesian Perspective,[0],[0]
", X M ) /",2.4. Bayesian Perspective,[0],[0]
"ZM 1e Z P M m=1 Xm .
",2.4. Bayesian Perspective,[0],[0]
"Recognizing the density of a Gamma(M, P M
m=1
X m ) ran-
dom variable, the posterior mean is
E[Z|X 1 , . . .",2.4. Bayesian Perspective,[0],[0]
", X M ] = M P
M m=1 X m
=
1
M
M
X
m=1
X m
!",2.4. Bayesian Perspective,[0],[0]
"1
,
coinciding with the Exponential trick estimator of Z.",2.4. Bayesian Perspective,[0],[0]
One way of exploiting perturb-and-MAP to yield computational savings is to replace independent perturbations of each configuration’s potential with an approximation.,3. Low-rank Perturbations,[0],[0]
"Such approximations are available e.g. in discrete graphical models, where the sampling space X has a product space structure X = X
1 ⇥ · · · ⇥",3. Low-rank Perturbations,[0],[0]
"X n , with X i the state space of
the i-th variable.",3. Low-rank Perturbations,[0],[0]
"Definition 3 ( (Hazan & Jaakkola, 2012)).",3. Low-rank Perturbations,[0],[0]
"The sum-unary perturbation MAP value is the random variable
U := max x2X
n",3. Low-rank Perturbations,[0],[0]
(x) + n,3. Low-rank Perturbations,[0],[0]
"X
i=1
i (x i )
o
,
where { i (x i )",3. Low-rank Perturbations,[0],[0]
"| x i 2 X i , 1  ",3. Low-rank Perturbations,[0],[0]
"i  n} i.i.d⇠ Gumbel( c).
",3. Low-rank Perturbations,[0],[0]
This definition involves |X 1 |+ · · ·+ |X n,3. Low-rank Perturbations,[0],[0]
| i.i.d.,3. Low-rank Perturbations,[0],[0]
"Gumbel random variables, rather than |X |.",3. Low-rank Perturbations,[0],[0]
(With n = 1 this coincides with full-rank perturbations and U ⇠,3. Low-rank Perturbations,[0],[0]
Gumbel( c+lnZ).),3. Low-rank Perturbations,[0],[0]
For n > 2 the distribution of U is not available analytically.,3. Low-rank Perturbations,[0],[0]
"One can similarly define the pairwise (or higher-order) perturbations, where independent Gumbel noise is added to each pairwise (or higher-order) potential.
",3. Low-rank Perturbations,[0],[0]
"Unary perturbations provide the upper bound lnZ  E[U ] on the log partition function (Hazan & Jaakkola, 2012), can be used to construct a sequential sampler for the Gibbs distribution (Hazan et al., 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al., 2013).",3. Low-rank Perturbations,[0],[0]
"In this section we show that a subfamily of tricks introduced in Section 2, consisting of Fr´echet and Weibull (and Exponential) tricks, is applicable in the low-rank perturbation setting and use them to derive new families of upper and lower bounds on lnZ and sequential samplers for the Gibbs distribution.",3. Low-rank Perturbations,[0],[0]
Please note full proofs are deferred to Appendix B and C.,3. Low-rank Perturbations,[0],[0]
The following family of upper bounds on lnZ can be derived from the Fr´echet and Weibull tricks.,3.1. Upper Bounds on the Partition Function,[0],[0]
Proposition 4.,3.1. Upper Bounds on the Partition Function,[0],[0]
"For any ↵ 2 ( 1, 0)",3.1. Upper Bounds on the Partition Function,[0],[0]
"[ (0,1), the upper bound lnZ  U(↵) holds with
U(↵) := n ln (1 + ↵) ↵ + nc 1 ↵ lnE ⇥",3.1. Upper Bounds on the Partition Function,[0],[0]
"e ↵U ⇤ .
",3.1. Upper Bounds on the Partition Function,[0],[0]
Proof.,3.1. Upper Bounds on the Partition Function,[0],[0]
(Sketch.),3.1. Upper Bounds on the Partition Function,[0],[0]
"By induction on n, with the induction step provided by our Clamping Lemma (Lemma 7) below.
",3.1. Upper Bounds on the Partition Function,[0],[0]
"To evaluate these bounds in practice, E[e ↵U ] is estimated using samples of U .",3.1. Upper Bounds on the Partition Function,[0],[0]
"Corollary 9 of Hazan et al. (2016) can be used to show that var(e ↵U ) is finite for ↵ > 1
2 p n ,
and so then the estimation is well-behaved.
",3.1. Upper Bounds on the Partition Function,[0],[0]
A natural question is how these new bounds relate to the Gumbel trick upper bound lnZ  E[U ] by Hazan & Jaakkola (2012).,3.1. Upper Bounds on the Partition Function,[0],[0]
The following result aims to answers this: Proposition 5.,3.1. Upper Bounds on the Partition Function,[0],[0]
"The limit of U(↵) as ↵ ! 0 exists and equals U(0) := E[U ], i.e. the Gumbel trick upper bound.
",3.1. Upper Bounds on the Partition Function,[0],[0]
The question remains: When is it advantageous to use a value ↵ 6= 0 to obtain a tighter bound on lnZ than the Gumbel trick bound?,3.1. Upper Bounds on the Partition Function,[0],[0]
The next result can provide guidance: Proposition 6.,3.1. Upper Bounds on the Partition Function,[0],[0]
"The function U(↵) is differentiable at ↵ = 0 and the derivative equals
d d↵ U(↵)
↵=0
=
1
2
✓
n ⇡2
6
var(U) ◆ .
",3.1. Upper Bounds on the Partition Function,[0],[0]
"While the variance of U is generally not tractable, in practice one obtains samples from U to estimate the expectation in U(↵) and these samples can be reused to assess var(U).",3.1. Upper Bounds on the Partition Function,[0],[0]
"Interestingly, var(U) equals n⇡2/6 for both the uniform distribution and the distribution concentrated on a single configuration, and in our empirical investigations always var(U)  n⇡2/6.",3.1. Upper Bounds on the Partition Function,[0],[0]
Then the derivative at 0 is non-negative and Fr´echet tricks provide tighter bounds on lnZ.,3.1. Upper Bounds on the Partition Function,[0],[0]
"However, as U(↵) is estimated with samples, the question of
estimator variance arises.",3.1. Upper Bounds on the Partition Function,[0],[0]
We investigate the trade-off between tightness of the bound lnZ  U(↵) and the variance incurred in estimating U(↵) empirically in Section 5.3.,3.1. Upper Bounds on the Partition Function,[0],[0]
"Consider the partial sum-unary perturbation MAP values, where the values of the first j 1 variables have been fixed, and only the rest are perturbed:
U j (x 1 , . . .",3.2. Clamping,[0],[0]
", x j 1) := max
x
j
,...,x
n
8 <
:
(x) + n",3.2. Clamping,[0],[0]
"X
i=j
i",3.2. Clamping,[0],[0]
"(x i )
9 =
;
.
",3.2. Clamping,[0],[0]
"The following lemma involving the U j ’s serves three purposes: (I.) it provides the induction step for Proposition 4, (II.)",3.2. Clamping,[0],[0]
"it shows that clamping never hurts partition function estimation with Fr´echet and Weibull tricks, and (III.)",3.2. Clamping,[0],[0]
it will be used to show that a sequential sampler constructed in Section 3.3 below is well-defined.,3.2. Clamping,[0],[0]
Lemma 7 (Clamping Lemma).,3.2. Clamping,[0],[0]
"For any j 2 {1, . . .",3.2. Clamping,[0],[0]
", n} and (x
1 , . . .",3.2. Clamping,[0],[0]
", x j 1) 2 X1 ⇥ · · · ⇥",3.2. Clamping,[0],[0]
"Xj 1, the following in-
equality holds with any ↵ 2 ( 1, 0)",3.2. Clamping,[0],[0]
"[ (0,1):
X
x
j 2X j
E
h e (n j) ln (1+↵) ↵(n j)c)e ↵Uj+1 i 1/↵
 E
h e (n (j 1))",3.2. Clamping,[0],[0]
"ln (1+↵) ↵(n (j 1))c)e ↵Uj i 1/↵
Proof.",3.2. Clamping,[0],[0]
"This follows directly from the Fr´echet trick (↵ 2 ( 1, 0)) or the Weibull trick (↵ > 0) and representing the Fr´echet resp.",3.2. Clamping,[0],[0]
Weibull random variables in terms of Gumbel random variables.,3.2. Clamping,[0],[0]
"See Appendix B.1 for more details.
",3.2. Clamping,[0],[0]
Corollary 8.,3.2. Clamping,[0],[0]
"Clamping never hurts lnZ estimation using any of the Fréchet or Weibull upper bounds U(↵).
",3.2. Clamping,[0],[0]
Proof.,3.2. Clamping,[0],[0]
"Applying the function x 7! ln(x) to both sides of the Clamping Lemma 7 with j = 1, the right-hand side equals U(↵), while the left-hand side is the estimate of lnZ after clamping variable x
1
.
",3.2. Clamping,[0],[0]
"This was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016).",3.2. Clamping,[0],[0]
"Similar results showing that clamping improves partition function estimation have been obtained for the mean field and TRW approximations (Weller & Domke, 2016), and in certain settings for the Bethe approximation (Weller & Jebara, 2014b) and LFIELD (Zhao et al., 2016).",3.2. Clamping,[0],[0]
Hazan et al. (2013) derived a sequential sampling procedure for the Gibbs distribution by exploiting the U(0) Gumbel trick upper bound on lnZ.,3.3. Sequential Sampling,[0],[0]
"In the same spirit, one
can derive sequential sampling procedures from the Fr´echet and Weibull tricks, leading to the following algorithm.
",3.3. Sequential Sampling,[0],[0]
"Algorithm 1 Sequential sampler for Gibbs distribution Input: ↵ 2 ( 1, 0)",3.3. Sequential Sampling,[0],[0]
"[ (0,1), potential function on X Output: a sample x from the Gibbs distribution / e (x)
1: for j = 1 to n do 2: for x
j 2 X j do
3: p j (x j
) e c
(1+↵)
1/↵
E
[
e
↵U j+1(x1,...,xj)
]
1/↵
E
[
e
↵U j (x1,...,x j 1)
]
1/↵
4: p j
(reject) 1 P
x
j 2X j
p j",3.3. Sequential Sampling,[0],[0]
"(x j )
5: x j
sample according to p j
6: if x j == reject then 7: RESTART (goto 1)
",3.3. Sequential Sampling,[0],[0]
"This algorithm is well-defined if p j (reject) 0 for all j, which can be shown by canceling terms in the Clamping Lemma 7.",3.3. Sequential Sampling,[0],[0]
We discuss correctness in Appendix B.2.,3.3. Sequential Sampling,[0],[0]
"As for the Gumbel sequential sampler of Hazan et al. (2013), the expected number of restarts (and hence the running time) only depend on the quality of the upper bound (U(↵) lnZ), and not on the ordering of variables.",3.3. Sequential Sampling,[0],[0]
"Similarly as in the Gumbel trick case (Hazan et al., 2013), one can derive lower bounds on lnZ by perturbing an arbitrary subset S of variables.",3.4. Lower Bounds on the Partition Function,[0],[0]
Proposition 9.,3.4. Lower Bounds on the Partition Function,[0],[0]
"Let X = X 1 ⇥ · · · X n
be a product space and a potential function on X .",3.4. Lower Bounds on the Partition Function,[0],[0]
"Let ↵ 2 ( 1, 0)[ (0,1).",3.4. Lower Bounds on the Partition Function,[0],[0]
"For any subset S ✓ {1, . . .",3.4. Lower Bounds on the Partition Function,[0],[0]
", n} of the variables x
1 , . . .",3.4. Lower Bounds on the Partition Function,[0],[0]
", x n
we have lnZ
c+ ln (1 + ↵)
↵ 1 ↵ lnE
h e ↵maxx{ (x)+ S(xS)} i ,
where x S := {x i :",3.4. Lower Bounds on the Partition Function,[0],[0]
i 2 S} and S (x S ) ⇠,3.4. Lower Bounds on the Partition Function,[0],[0]
"Gumbel( c) independently for each setting of x
S
.
",3.4. Lower Bounds on the Partition Function,[0],[0]
"By averaging n such lower bounds corresponding to singleton sets S = {i} together, we obtain a lower bound on lnZ that involves the average-unary perturbation MAP value
L := max x2X
(
(x) + 1
n
n
X
i=1
i (x i )
)
.
",3.4. Lower Bounds on the Partition Function,[0],[0]
Corollary 10.,3.4. Lower Bounds on the Partition Function,[0],[0]
"For any ↵ 2 ( 1, 0)",3.4. Lower Bounds on the Partition Function,[0],[0]
"[ (0,1), we have the lower bound lnZ L(↵), where
L(↵) := c+ ln (1 + ↵) ↵ 1 n↵ lnE",3.4. Lower Bounds on the Partition Function,[0],[0]
"[exp ( n↵L)] .
",3.4. Lower Bounds on the Partition Function,[0],[0]
"Again, L(0) := E[L] can be defined by continuity, where E[L]  lnZ is the Gumbel trick lower bound by Hazan et al. (2013).",3.4. Lower Bounds on the Partition Function,[0],[0]
"We have seen how the Gumbel trick can be embedded into a continuous family of tricks, consisting of Fr´echet, Exponential, and Weibull tricks.",4. Advantages of the Gumbel Trick,[0],[0]
"We showed that the new tricks can provide more efficient estimators of the partition function in the full-rank perturbation setting (Section 2), and in the low-rank perturbation setting lead to sequential samplers and new bounds on lnZ, which can be also more efficient, as we investigate in Section 5.3.",4. Advantages of the Gumbel Trick,[0],[0]
"To balance the discussion of merits of different tricks, in this section we briefly highlight advantages of the Gumbel trick that stem from its simpler analytical form.
",4. Advantages of the Gumbel Trick,[0],[0]
"First, by consulting Table 1 we see that the function g(x) =",4. Advantages of the Gumbel Trick,[0],[0]
"lnx c has the property that the variance of the resulting estimator (of lnZ) does not depend on the value of Z; the function g is a variance stabilizing transformation for the Exponential distribution.
",4. Advantages of the Gumbel Trick,[0],[0]
"Second, exploiting the fact that the logarithm function leads to additive perturbations, Maji et al. (2014) showed that the entropy of x⇤, the configuration with maximum potential after sum-unary perturbation in the sense of Definition 3, can be bounded as H(x⇤)  ",4. Advantages of the Gumbel Trick,[0],[0]
"B(p) := P n
i=1
",4. Advantages of the Gumbel Trick,[0],[0]
"E
i
[ i (x⇤ i )].
",4. Advantages of the Gumbel Trick,[0],[0]
"We extend this result to show how the errors of bounding lnZ, sampling, and entropy estimation are related: Proposition 11.",4. Advantages of the Gumbel Trick,[0],[0]
"Writing p for the Gibbs distribution and B(p) := E
i
[ i (x⇤ i )] for the entropy bound, we have
(U(0) lnZ)",4. Advantages of the Gumbel Trick,[0],[0]
| {z } error in lnZ bound +KL(x⇤ k p),4. Advantages of the Gumbel Trick,[0],[0]
"| {z } sampling error = B(p) H(x⇤) | {z } error in entropy estimation .
",4. Advantages of the Gumbel Trick,[0],[0]
"Third, the additive character of the Gumbel perturbations can also be used to derive a new result relating the error of the lower bound L(0) and of sampling x⇤⇤ as the configuration achieving the maximum average-unary perturbation value L, instead of sampling from the Gibbs distribution p: Proposition 12.",4. Advantages of the Gumbel Trick,[0],[0]
"Writing p for the Gibbs distribution,
lnZ L(0)",4. Advantages of the Gumbel Trick,[0],[0]
"| {z }
error in lnZ bound
KL(x⇤⇤ k p) | {z }
sampling error
0.
",4. Advantages of the Gumbel Trick,[0],[0]
Remark.,4. Advantages of the Gumbel Trick,[0],[0]
"While we knew from Hazan et al. (2013) that lnZ L(0) 0, this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the approximate sampling distribution of x⇤⇤ and the Gibbs distribution p.
",4. Advantages of the Gumbel Trick,[0],[0]
"Proofs of the new results appear in Appendix B.3 and C.2.
",4. Advantages of the Gumbel Trick,[0],[0]
"Fourth, viewed as a function of the Gumbel perturbations
, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016).",4. Advantages of the Gumbel Trick,[0],[0]
Proving similar measure concentration results for the expectations E[e ↵U ] appearing in U(↵) for ↵ 6= 0,4. Advantages of the Gumbel Trick,[0],[0]
may be more challenging.,4. Advantages of the Gumbel Trick,[0],[0]
"We conducted experiments with the following aims:
1.",5. Experiments,[0],[0]
"To show that the higher efficiency of the Exponential
trick in the full-rank perturbation setting is useful in practice, we compared it to the Gumbel trick in A* sampling (Maddison et al., 2014) (Section 5.1) and in the large-scale discrete sampling setting of Chen & Ghahramani (2016) (Section 5.2).",5. Experiments,[0],[0]
2.,5. Experiments,[0],[0]
"To show that non-zero values of ↵ can lead to better estimators of lnZ in the low-rank perturbation setting as well, we compare the Fr´echet and Weibull trick
bounds U(↵) to the Gumbel trick bound U(0) on a common discrete graphical model with different coupling strengths; see Section 5.3.",5. Experiments,[0],[0]
"A* sampling (Maddison et al., 2014) is a sampling algorithm for continuous distributions that perturbs the logunnormalized density with a continuous generalization of the Gumbel trick, called the Gumbel process, and uses a variant of A* search to find the location of the maximum of the perturbed .",5.1. A* Sampling,[0],[0]
"Returning the location yields an exact sample from the original distribution, as in the discrete Gumbel trick.",5.1. A* Sampling,[0],[0]
"Moreover, the corresponding maximum value also has the Gumbel( c + lnZ) distribution (Maddison et al., 2014).",5.1. A* Sampling,[0],[0]
Our analysis in Section 2.3 tells us that the Exponential trick yields an estimator with lower MSE than the Gumbel trick; we briefly verified this on the Robust Bayesian Regression experiment of Maddison et al. (2014).,5.1. A* Sampling,[0],[0]
"We constructed estimators of lnZ from the Gumbel and Exponential tricks (debiased version, see Section 2.3.2), and assessed their variances by constructing each estimator K = 1000 times and looking at the sample variance.",5.1. A* Sampling,[0],[0]
Figure 3a shows that the Exponential trick requires up to 40% fewer samples to reach a given MSE.,5.1. A* Sampling,[0],[0]
Chen & Ghahramani (2016) considered sampling from a discrete distribution of the form p(x) / f 0,5.2. Scalable Partition Function Estimation,[0],[0]
"(x) Q S
s=1
f s (x) when the number of factors S is large relative to the sample space size |X |.",5.2. Scalable Partition Function Estimation,[0],[0]
Computing i.i.d.,5.2. Scalable Partition Function Estimation,[0],[0]
"Gumbel perturbations (x) for each x 2 X is then relatively cheap compared to evaluating all potentials (x) = f
0
(x) + P S
s=1
ln f s (x).",5.2. Scalable Partition Function Estimation,[0],[0]
"Chen & Ghahramani (2016) observed that each (perturbed) potential can be estimated by subsampling the factors, and potentials that appear unlikely to yield the MAP value can be pruned off from the search early on.",5.2. Scalable Partition Function Estimation,[0],[0]
"The authors formalized the problem as a Multi-armed bandit problem with a finite reward population and derived approximate algorithms for efficiently finding the maximum perturbed potential with a probabilistic guarantee.
",5.2. Scalable Partition Function Estimation,[0],[0]
"(a)
(b)
While Chen & Ghahramani (2016) considered sampling, by modifying their procedure to return the value of the maximum perturbed potential rather than the argmax (cf equations (1) and (2)), we can estimate the partition function instead.",5.2. Scalable Partition Function Estimation,[0],[0]
"However, the approximate algorithm only guarantees to find the MAP configuration with a probability 1 .",5.2. Scalable Partition Function Estimation,[0],[0]
Figure 3b shows the results of running the Racing-Normal algorithm of Chen & Ghahramani (2016) on the synthetic dataset considered by the authors with the “very hard” noise setting = 0.1.,5.2. Scalable Partition Function Estimation,[0],[0]
"For low error bounds the Exponential trick remained close to optimal, but for a larger error bound the Weibull trick interpolation between the Gumbel and Exponential tricks proved useful to provide an estimator with lower MSE.
5.3.",5.2. Scalable Partition Function Estimation,[0],[0]
"Low-rank Perturbation Bounds on lnZ
Hazan & Jaakkola (2012) evaluated tightness of the Gumbel trick upper bound U(0) lnZ on 10⇥ 10 binary spin glass models.",5.2. Scalable Partition Function Estimation,[0],[0]
We show one can obtain more accurate estimates of lnZ on such models by choosing ↵ 6= 0.,5.2. Scalable Partition Function Estimation,[0],[0]
"To account for the fact that in practice an expectation in U(↵) is replaced with a sample average, we treat U(↵) as an estimator of lnZ with asymptotic bias equal to the bound gap (U(↵) lnZ), and estimate its MSE.",5.2. Scalable Partition Function Estimation,[0],[0]
"Figure 4 shows the MSEs of U(↵) as estimators of lnZ on 10⇥ 10 (n = 100) binary pairwise grid models with unary potentials sampled uniformly from [ 1, 1] and pairwise potentials from [0, C] (attractive models) or from [ C,C] (mixed models), for varying coupling strengths C. We replaced the expectations in U(↵)’s with sample averages of size M = 100, using libDAI (Mooij, 2010) to solve the MAP problems yielding these samples.",5.2. Scalable Partition Function Estimation,[0],[0]
We constructed each estimator 1000 times to assess its variance.,5.2. Scalable Partition Function Estimation,[0],[0]
"By casting partition function evaluation as a parameter estimation problem for the exponential distribution, we derived a family of methods of which the Gumbel trick is a special case.",6. Discussion,[0],[0]
"These methods can be equivalently seen as (1) perturbing models using different distributions, or as (2) averaging standard Gumbel perturbations in different spaces, allowing implementations with little additional cost.
",6. Discussion,[0],[0]
"We showed that in the full-rank perturbation setting, the new Exponential trick provides an estimator with lower MSE, or instead allows using up to 40% fewer samples than the Gumbel trick estimator to reach the same MSE.
",6. Discussion,[0],[0]
"In the low-rank perturbation setting, we used our Fr´echet, Exponential and Weibull tricks to derive new bounds on lnZ and sequential samplers for the Gibbs distribution, and showed that these can also behave better than the corresponding Gumbel trick results.",6. Discussion,[0],[0]
"However, the optimal trick to use (as specified by ↵) depends on the model, sample size, and MAP solver used (if approximate).",6. Discussion,[0],[0]
"Since in practice the dominant computational cost is carried by solving repeated instances of the MAP problem, one can try and assess different values of ↵ on the problem at hand.",6. Discussion,[0],[0]
"That said, we believe that investigating when different tricks yield better results is an interesting avenue for future work.
",6. Discussion,[0],[0]
"Finally, we balanced the discussion by pointing out that the Gumbel trick has a simpler analytical form which can be exploited to derive more interesting theoretical statements in the low-rank perturbation setting.",6. Discussion,[0],[0]
"Beyond existing results, we derived new connections between errors of different procedures using low-rank Gumbel perturbations.",6. Discussion,[0],[0]
"The authors thank Tamir Hazan for helpful discussions, and Mark Rowland, Maria Lomeli, and the anonymous reviewers for helpful comments.",Acknowledgements,[0],[0]
"AW acknowledges support by the Alan Turing Institute under EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.",Acknowledgements,[0],[0]
"The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function.",abstractText,[0],[0]
"The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration.",abstractText,[0],[0]
"We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost.",abstractText,[0],[0]
"In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations.",abstractText,[0],[0]
"We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution.",abstractText,[0],[0]
"Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.",abstractText,[0],[0]
Lost Relatives of the Gumbel Trick,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1426–1436 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1426",text,[0],[0]
Recurrent neural networks (RNNs) are remarkably effective models of sequential data.,1 Introduction,[0],[0]
"Recent years have witnessed the widespread adoption of recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997) in various NLP tasks, with state of the art results in language modeling (Melis et al., 2018) and conditional generation tasks like machine translation (Bahdanau et al., 2015) and text summarization (See et al., 2017).
",1 Introduction,[0],[0]
"Here we revisit the question asked by Linzen et al. (2016): as RNNs model word sequences without explicit notions of hierarchical structure,
to what extent are these models able to learn non-local syntactic dependencies in natural language?",1 Introduction,[0],[0]
"Identifying number agreement between subjects and verbs—especially in the presence of attractors—can be understood as a cognitivelymotivated probe that seeks to distinguish hierarchical theories from sequential ones, as models that rely on sequential cues like the most recent noun would favor the incorrect verb form.",1 Introduction,[0],[0]
"We provide an example of this task in Fig. 1, where the plural form of the verb have agrees with the distant subject parts, rather than the adjacent attractors (underlined) of the singular form.
",1 Introduction,[0],[0]
"Contrary to the findings of Linzen et al. (2016), our experiments suggest that sequential LSTMs are able to capture structural dependencies to a large extent, even for cases with multiple attractors (§2).",1 Introduction,[0],[0]
Our finding suggests that network capacity plays a crucial role in capturing structural dependencies with multiple attractors.,1 Introduction,[0],[0]
"Nevertheless, we find that a strong character LSTM language model—which lacks explicit word representation and has to capture much longer sequential dependencies in order to learn non-local structural dependencies effectively—performs much worse in the number agreement task.
",1 Introduction,[0],[0]
"Given the strong performance of word-based LSTM language models, are there are any substantial benefits, in terms of number agreement accuracy, to explicitly modeling hierarchical structures as an inductive bias?",1 Introduction,[0],[0]
"We discover that a
certain class of LSTM language models that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3).",1 Introduction,[0],[0]
We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations.,1 Introduction,[0],[0]
"Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016).
",1 Introduction,[0],[0]
"Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English.",1 Introduction,[0],[0]
"As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4).
",1 Introduction,[0],[0]
"Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017).",1 Introduction,[0],[0]
"Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural dependencies in English, as indicated by number agreement accuracy?",1 Introduction,[0],[0]
"Our key finding is that the top-down generation outperforms left-corner and bottom-up variants for difficult cases with multiple attractors.
",1 Introduction,[0],[0]
"In theory, the three traversal strategies approximate the same chain rule that decompose the joint probability of words and phrase-structure trees, denoted as p(x,y), differently and as such will impose different biases on the learner.",1 Introduction,[0],[0]
"In §4.3, we show that the three variants achieve similar perplexities on a held-out validation set.",1 Introduction,[0],[0]
"As we observe different patterns in number agreement, this demonstrates that while perplexity can be a useful diagnostic tool, it may not be sensitive enough for comparing models in terms of how well they capture grammatical intuitions.",1 Introduction,[0],[0]
"We revisit the number agreement task with LSTMs trained on language modeling objectives, as proposed by Linzen et al. (2016).
",2 Number Agreement with LSTM Language Models,[0],[0]
Experimental Settings.,2 Number Agreement with LSTM Language Models,[0],[0]
"We use the same parsed Wikipedia corpus, verb inflectors, preprocessing steps, and dataset split as Linzen et al. (2016).1 Word types beyond the most frequent 10,000 are converted to their respective POS tags.",2 Number Agreement with LSTM Language Models,[0],[0]
"We summarize the corpus statistics of the dataset, along with the test set distribution of the number of attractors, in Table 1.",2 Number Agreement with LSTM Language Models,[0],[0]
"Similar to Linzen et al. (2016), we only include test cases where all intervening nouns are of the opposite number forms than the subject noun.",2 Number Agreement with LSTM Language Models,[0],[0]
"All models are implemented using the DyNet library (Neubig et al., 2017).
",2 Number Agreement with LSTM Language Models,[0],[0]
Training was done using a language modeling objective that predicts the next word given the prefix; at test time we compute agreement error rates by comparing the probability of the correct verb form with the incorrect one.,2 Number Agreement with LSTM Language Models,[0],[0]
"We report performance of a few different LSTM hidden layer configurations, while other hyper-parameters are selected based on a grid search.2 Following Linzen
1The dataset and scripts are obtained from https:// github.com/TalLinzen/rnn_agreement.
",2 Number Agreement with LSTM Language Models,[0],[0]
"2Based on the grid search results, we used the following hyper-parameters that work well across different hidden layer sizes: 1-layer LSTM, SGD optimizers with an initial learning rate of 0.2, a learning rate decay of 0.10 after 10 epochs, LSTM dropout rates of 0.2, an input embedding dimension of 50, and a batch size of 10 sentences.",2 Number Agreement with LSTM Language Models,[0],[0]
"Our use of singlelayer LSTMs and 50-dimensional word embedding (learned from scratch) as one of the baselines is consistent with the experimental settings of Linzen et al. (2016).
",2 Number Agreement with LSTM Language Models,[0],[0]
"et al. (2016), we include the results of our replication3 of the large-scale language model of Jozefowicz et al. (2016) that was trained on the One Billion Word Benchmark.4 Hyper-parameter tuning is based on validation set perplexity.
Discussion.",2 Number Agreement with LSTM Language Models,[0],[0]
"Table 2 indicates that, given enough capacity, LSTM language models without explicit syntactic supervision are able to perform well in number agreement.",2 Number Agreement with LSTM Language Models,[0],[0]
"For cases with multiple attractors, we observe that the LSTM language model with 50 hidden units trails behind its larger counterparts by a substantial margin despite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps.5 Our experiment independently derives the
3When evaluating the large-scale language model, the primary difference is that we do not map infrequent word types to their POS tags and that we subsample to obtain 500 test instances of each number of attractor due to computation cost; both preprocessing were also done by Linzen et al. (2016).
4The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/ tree/master/research/lm_1b.
",2 Number Agreement with LSTM Language Models,[0],[0]
5This trend is also observed by comparing results with H=150 and H=250.,2 Number Agreement with LSTM Language Models,[0],[0]
"While both models achieve near-identical performance for zero attractor, the model with H=250 per-
same finding as the recent work of Gulordava et al. (2018), who also find that LSTMs trained with language modeling objectives are able to learn number agreement well; here we additionally identify model capacity as one of the reasons for the discrepancy with the Linzen et al. (2016) results.
",2 Number Agreement with LSTM Language Models,[0],[0]
"While the pretrained large-scale language model of Jozefowicz et al. (2016) has certain advantages in terms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set.
",2 Number Agreement with LSTM Language Models,[0],[0]
"Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017).",2 Number Agreement with LSTM Language Models,[0],[0]
"In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1).",2 Number Agreement with LSTM Language Models,[0],[0]
"Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject.
",2 Number Agreement with LSTM Language Models,[0],[0]
"If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is more distant in the sequences, compared to cases where the structural dependency is more adjacent.",2 Number Agreement with LSTM Language Models,[0],[0]
"We empirically test this conjecture by running a strong character-based LSTM language model of Melis et al. (2018) that achieved state of the art results on EnWiki8 from the Hutter Prize dataset (Hutter, 2012), with 1,800 hidden units and 10 million parameters.",2 Number Agreement with LSTM Language Models,[0],[0]
"The character LSTM is trained, validated, and tested6 on the same split of the Linzen et al. (2016) number agreement dataset.
",2 Number Agreement with LSTM Language Models,[0],[0]
"A priori, we expect that number agreement is harder for character LSTMs for two reasons.",2 Number Agreement with LSTM Language Models,[0],[0]
"First, character LSTMs lack explicit word representa-
forms much better for cases with multiple attractors.",2 Number Agreement with LSTM Language Models,[0],[0]
"6For testing, we similarly evaluate number agreement accuracy by comparing the probability of the correct and incorrect verb form given the prefix, as represented by the respective character sequences.
tions, thus succeeding in this task requires identifying structural dependencies between two sequences of character tokens, while word-based LSTMs only need to resolve dependencies between word tokens.",2 Number Agreement with LSTM Language Models,[0],[0]
"Second, by nature of modeling characters, non-local structural dependencies are sequentially further apart than in the wordbased language model.",2 Number Agreement with LSTM Language Models,[0],[0]
"On the other hand, character LSTMs have the ability to exploit and share informative morphological cues, such as the fact that plural nouns in English tend to end with ‘s’.
",2 Number Agreement with LSTM Language Models,[0],[0]
"As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts.",2 Number Agreement with LSTM Language Models,[0],[0]
"This finding is consistent with that of Sennrich (2017), who find that character-level decoders in neural machine translation perform worse than subword models in capturing morphosyntactic agreement.",2 Number Agreement with LSTM Language Models,[0],[0]
"To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to implicitly acquire a lexicon (Le Godais et al., 2017).",2 Number Agreement with LSTM Language Models,[0],[0]
"Given the strong performance of sequential LSTMs in number agreement, is there any further benefit to explicitly modeling hierarchical structures?",3 Number Agreement with RNNGs,[0],[0]
"We focus on recurrent neural network grammars (Dyer et al., 2016, RNNGs), which jointly model the probability of phrase-structure trees and strings, p(x,y), through structurebuilding actions and explicit compositions for representing completed constituents.
",3 Number Agreement with RNNGs,[0],[0]
"Our choice of RNNGs is motivated by the findings of Kuncoro et al. (2017), who find evidence for syntactic headedness in RNNG phrasal representations.",3 Number Agreement with RNNGs,[0],[0]
"Intuitively, the ability to learn heads is beneficial for this task, as the representation for the noun phrase “The flowers in the vase” would be similar to the syntactic head flowers rather than vase.",3 Number Agreement with RNNGs,[0],[0]
"In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGs’ representation.",3 Number Agreement with RNNGs,[0],[0]
"RNNGs (Dyer et al., 2016) are language models that estimate the joint probability of string terminals and phrase-structure tree nonterminals.",3.1 Recurrent Neural Network Grammars,[0],[0]
"Here we use stack-only RNNGs that achieve better perplexity and parsing performance (Kuncoro et al., 2017).",3.1 Recurrent Neural Network Grammars,[0],[0]
"Given the current stack configuration, the objective function of RNNGs is to predict the correct structure-building operation according to a top-down, left-to-right traversal of the phrasestructure tree; a partial traversal for the input sentence “The flowers in the vase are blooming” is illustrated in Fig.",3.1 Recurrent Neural Network Grammars,[0],[0]
"3(a).7
The structural inductive bias of RNNGs derives from an explicit composition operator that represents completed constituents; for instance, the constituent (NP The flowers) is represented by a single composite element on the stack, rather than as four separate symbols.",3.1 Recurrent Neural Network Grammars,[0],[0]
"During each REDUCE action, the topmost stack elements that belong to the new constituent are popped from the stack and then composed by the composition function; the composed symbol is then pushed back into the stack.",3.1 Recurrent Neural Network Grammars,[0],[0]
The model is trained in an end-to-end manner by minimizing the cross-entropy loss relative to a sample of gold trees.,3.1 Recurrent Neural Network Grammars,[0],[0]
"Here we summarize the experimental settings of running RNNGs on the number agreement dataset and discuss the empirical findings.
",3.2 Experiments,[0],[0]
Experimental settings.,3.2 Experiments,[0],[0]
"We obtain phrasestructure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model8 trained on the Penn Treebank (Marcus et al., 1993).",3.2 Experiments,[0],[0]
"At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.9",3.2 Experiments,[0],[0]
"At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols.",3.2 Experiments,[0],[0]
An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a).,3.2 Experiments,[0],[0]
"We similarly run a grid search over the same hyper-parameter range as the sequential
7For a complete example of action sequences, we refer the reader to the example provided by Dyer et al. (2016).
",3.2 Experiments,[0],[0]
"8https://github.com/clab/rnng 9Earlier work on RNNGs (Dyer et al., 2016; Kuncoro et al., 2017) train the model on gold phrase-structure trees on the Penn Treebank, while here we train the RNNG on the number agreement dataset based on predicted trees from another parser.
LSTM and compare the results with the strongest sequential LSTM baseline from §2.
Discussion.",3.2 Experiments,[0],[0]
"Fig. 2 shows that RNNGs (rightmost) achieve much better number agreement accuracy compared to LSTM language models (leftmost) for difficult cases with four and five attractors, with around 30% error rate reductions, along with a 13% error rate reduction (from 9% to 7.8%) for three attractors.",3.2 Experiments,[0],[0]
"We attribute the slightly worse performance of RNNGs on cases with zero and one attractor to the presence of intervening structure-building actions that separate the subject and the verb, such as a REDUCE (step 6 in Fig. 3(a)) action to complete the noun phrase and at least one action to predict a verb phrase (step 15 in Fig. 3(a))",3.2 Experiments,[0],[0]
"before the verb itself is introduced, while LSTM language models benefit from shorter dependencies for zero and one attractor cases.
",3.2 Experiments,[0],[0]
The performance gain of RNNGs might arise from two potential causes.,3.2 Experiments,[0],[0]
"First, RNNGs have access to predicted syntactic annotations, while LSTM language models operate solely on word sequences.",3.2 Experiments,[0],[0]
"Second, RNNGs incorporate explicit compositions, which encourage hierarhical representations and potentially the discovery of syntactic (rather than sequential) dependencies.
",3.2 Experiments,[0],[0]
"Would LSTMs that have access to syntactic annotations, but without the explicit composition function, benefit from the same performance gain as RNNGs?",3.2 Experiments,[0],[0]
"To answer this question, we run sequential LSTMs over the same phrase-structure trees (Choe and Charniak, 2016), similarly estimating the joint probability of phrase-structure nonterminals and string terminals but without an explicit composition operator.",3.2 Experiments,[0],[0]
"Taking the example in Fig. 3(a), the sequential syntactic LSTM would
have fifteen10 symbols on the LSTM when predicting the verb, as opposed to three symbols in the case of RNNGs’ stack LSTM.",3.2 Experiments,[0],[0]
"In theory, the sequential LSTM over the phrase-structure trees (Choe and Charniak, 2016) may be able to incorporate a similar, albeit implicit, composition process as RNNGs and consequently derive similarly syntactic heads, although there is no inductive bias that explicitly encourages such process.
",3.2 Experiments,[0],[0]
"Fig. 2 suggests that the sequential syntactic LSTMs (center) perform comparably with sequential LSTMs without syntax for multiple attractor cases, and worse than RNNGs for nearly all attractors; the gap is highest for multiple attractors.",3.2 Experiments,[0],[0]
"This result showcases the importance of an explicit composition operator and hierarchical representations in identifying structural dependencies, as indicated by number agreement accuracy.",3.2 Experiments,[0],[0]
"Our finding is consistent with the recent work of Yogatama et al. (2018), who find that introducing elements of hierarchical modeling through a stackstructured memory is beneficial for number agreement, outperforming LSTM language models and attention-augmented variants by increasing margins as the number of attractor grows.",3.2 Experiments,[0],[0]
"In order to better interpret the results, we conduct further analysis into the perplexities of each model, followed by a discussion on the effect of incrementality constraints on the RNNG when predicting number agreement.
Perplexity.",3.3 Further Analysis,[0],[0]
To what extent does the success of RNNGs in the number agreement task with multiple attractors correlate with better performance under the perplexity metric?,3.3 Further Analysis,[0],[0]
"We answer this question by using an importance sampling marginalization procedure (Dyer et al., 2016) to obtain an estimate of p(x) under both RNNGs and the sequential syntactic LSTM model.",3.3 Further Analysis,[0],[0]
"Following Dyer et al. (2016), for each sentence on the validation set we sample 100 candidate trees from a discriminative model11 as our proposal distribution.",3.3 Further Analysis,[0],[0]
"As demonstrated in Table 3, the LSTM language model has the lowest validation set perplexity despite substantially worse performance than RNNGs in number agreement with multiple attractors, suggesting that lower perplexity is not neces-
10In the model of Choe and Charniak (2016), each nonterminal, terminal, and closed parenthesis symbol is represented as an element on the LSTM sequence.
",3.3 Further Analysis,[0],[0]
"11https://github.com/clab/rnng
sarily correlated with number agreement success.
",3.3 Further Analysis,[0],[0]
Incrementality constraints.,3.3 Further Analysis,[0],[0]
"As the syntactic prefix was derived from a discriminative model that has access to unprocessed words, one potential concern is that this prefix might violate the incrementality constraints and benefit the RNNG over the LSTM language model.",3.3 Further Analysis,[0],[0]
"To address this concern, we remark that the empirical evidence from Fig. 2 and Table 3 indicates that the LSTM language model without syntactic annotation outperforms the sequential LSTM with syntactic annotation in terms of both perplexity and number agreement throughout nearly all attractor settings, suggesting that the predicted syntactic prefix does not give any unfair advantage to the syntactic models.
",3.3 Further Analysis,[0],[0]
"Furthermore, we run an experiment where the syntactic prefix is instead derived from an incremental beam search procedure of Fried et al. (2017).12 To this end, we take the highest scoring beam entry at the time that the verb is generated to be the syntactic prefix; this procedure is applied to both the correct and incorrect verb forms.13 We then similarly compare the probabilities of the correct and incorrect verb form given each respective syntactic prefix to obtain number agreement accuracy.",3.3 Further Analysis,[0],[0]
"Our finding suggests that using the fully incremental tree prefix leads to even better RNNG number agreement performance for four and five attractors, achieving 7.1% and 8.2% error rates, respectively, compared to 9.4% and 12% for the RNNG error rates in Fig. 2.",3.3 Further Analysis,[0],[0]
"In this section, we propose two new variants of RNNGs that construct trees using a different con-
12As the beam search procedure is time-consuming, we randomly sample 500 cases for each attractor and compute the number agreement accuracy on these samples.
","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"13Consequently, the correct and incorrect forms of the sentence might have different partial trees, as the highest scoring beam entries may be different for each alternative.
struction order than the top-down, left-to-right order used above.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"These are a bottom-up construction order (§4.1) and a left-corner construction order (§4.2), analogous to the well-known parsing strategies (e.g. Hale, 2014, chapter 3).","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"They differ from these classic strategies insofar as they do not announce the phrase-structural content of an entire branch at the same time, adopting instead a node-by-node enumeration reminescent of Markov Grammars (Charniak, 1997).","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"This stepby-step arrangement extends to the derived string as well; since all variants generate words from left to right, the models can be compared using number agreement as a diagnostic.14
Here we state our hypothesis on why the build order matters.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"The three generation strategies represent different chain rule decompositions of the joint probability of strings and phrase-structure trees, thereby imposing different biases on the learner.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"Earlier work in parsing has characterized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of human sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions (Johnson-Laird, 1983; Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992, inter alia), along with neurophysiological evidence in human sentence processing (Nelson et al., 2017).","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"Here we cast the three strategies as models of language generation (Manning and Carpenter, 1997), and focus on the empirical question: which generation order has the most appropriate bias in modeling non-local structural dependencies in English?
","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
These alternative orders organize the learning problem so as to yield intermediate states in generation that condition on different aspects of the grammatical structure.,"4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"In number agreement, this amounts to making an agreement controller, such as the word flowers in Fig. 3, more or less salient.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"If it is more salient, the model should be better-able to inflect the main verb in agreement with this controller, without getting distracted by the attractors.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"The three proposed build orders are compared in Fig. 3, showing the respective configurations (i.e. the prefix) when generating the main verb in a sentence with a single attractor.15 In ad-
14Only the order in which these models build the nonterminal symbols is different, while the terminal symbols are still generated in a left-to-right manner in all variants.
","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"15Although the stack configuration at the time of verb generation varies only slightly, the configurations encountered
dition, we show concrete action sequences for a simpler sentence in each section.","4 Top-Down, Left-Corner, and Bottom-Up Traversals",[0],[0]
"In bottom-up traversals, phrases are recursively constructed and labeled with the nonterminal type once all their daughters have been built, as illustrated in Fig. 4.",4.1 Bottom-Up Traversal,[0],[0]
Bottom-up traversals benefit from shorter stack depths compared to top-down due to the lack of incomplete nonterminals.,4.1 Bottom-Up Traversal,[0],[0]
"As the commitment to label the nonterminal type of a phrase is delayed until its constituents are complete, this means that the generation of a child node cannot condition on the label of its parent node.
",4.1 Bottom-Up Traversal,[0],[0]
"In n-ary branching trees, bottom-up completion of constituents requires a procedure for determining how many of the most recent elements on the stack should be daughters of the node that is being constructed.16 Conceptually, rather than having a single REDUCE operation as we have before, we have a complex REDUCE(X, n) operation that must determine the type of the constituent (i.e., X) as well as the number of daughters (i.e., n).
",4.1 Bottom-Up Traversal,[0],[0]
"In step 5 of Fig. 4, the newly formed NP constituent only covers the terminal worms, and neither the unattached terminal eats nor the constituent (NP The fox) is part of the new noun phrase.",4.1 Bottom-Up Traversal,[0],[0]
"We implement this extent decision using a stick-breaking construction—using the stack LSTM encoding, a single-layer feedforward network, and a logistic output layer—which decides whether the top element on the stack should be the leftmost child of the new constituent (i.e. whether or not the new constituent is complete after popping the current topmost stack element), as illustrated in Fig. 5.",4.1 Bottom-Up Traversal,[0],[0]
"If not, the process is then repeated after the topmost stack element is popped.",4.1 Bottom-Up Traversal,[0],[0]
"Once the extent of the new nonterminal has been decided, we parameterize the decision of the nonterminal label type; in Fig. 5",4.1 Bottom-Up Traversal,[0],[0]
this is an NP.,4.1 Bottom-Up Traversal,[0],[0]
"A second difference to top-down generation is that when a single constituent remains on the stack, the sentence is not necessarily complete (see step 3 of Fig. 4 for examples where this happens).",4.1 Bottom-Up Traversal,[0],[0]
"We thus introduce an explicit STOP action (step 8, Fig. 4), indicating the tree is complete, which is only assigned non-zero probability when the stack has a
during the history of the full generation process vary considerably in the invariances and the kinds of actions they predict.
",4.1 Bottom-Up Traversal,[0],[0]
"16This mechanism is not necessary with strictly binary branching trees, since each new nonterminal always consists of the two children at the top of the stack.
single complete constituent.",4.1 Bottom-Up Traversal,[0],[0]
Left-corner traversals combine some aspects of top-down and bottom-up processing.,4.2 Left-Corner Traversal,[0],[0]
"As illustrated in Fig. 6, this works by first generating the leftmost terminal of the tree, The (step 0), before proceeding bottom-up to predict its parent NP (step 1) and then top-down to predict the rest of its children (step 2).",4.2 Left-Corner Traversal,[0],[0]
A REDUCE action similarly calls the composition operator once the phrase is complete (e.g. step 3).,4.2 Left-Corner Traversal,[0],[0]
"The complete constituent (NP The fox) is the leftmost child of its parent node, thus an NT SW(S) action is done next (step 4).
",4.2 Left-Corner Traversal,[0],[0]
"The NT SW(X) action is similar to the NT(X) from the top-down generator, in that it introduces an open nonterminal node and must be matched later by a corresponding REDUCE operation, but, in addition, swaps the two topmost elements at the top of the stack.",4.2 Left-Corner Traversal,[0],[0]
This is necessary because the parent nonterminal node is not built until after its left-most child has been constructed.,4.2 Left-Corner Traversal,[0],[0]
"In step 1 of Fig. 6, with a single element The on the stack, the action NT SW(NP) adds the open nonterminal symbol NP to become the topmost stack element, but after applying the swap operator the stack now contains (NP | The (step 2).",4.2 Left-Corner Traversal,[0],[0]
We optimize the hyper-parameters of each RNNG variant using grid searches based on validation set perplexity.,4.3 Experiments,[0],[0]
Table 4 summarizes average stack depths and perplexities17 on the Linzen et al. (2016) validation set.,4.3 Experiments,[0],[0]
"We evaluate each of the variants in terms of number agreement accuracy as an evidence of its suitability to model structural dependencies in English, presented in Table 5.",4.3 Experiments,[0],[0]
"To account for randomness in training, we report the error rate summary statistics of ten different runs.
",4.3 Experiments,[0],[0]
"17Here we measure perplexity over p(x,y), where y is the presumptive gold tree on the Linzen et al. (2016) dataset.",4.3 Experiments,[0],[0]
"Dyer et al. (2016) instead used an importance sampling procedure to marginalize and obtain an estimate of p(x).
",4.3 Experiments,[0],[0]
Discussion.,4.3 Experiments,[0],[0]
"In Table 5, we focus on empirical results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors.",4.3 Experiments,[0],[0]
All three RNNG variants outperform the sequential LSTM language model baseline for these cases.,4.3 Experiments,[0],[0]
"Nevertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English.",4.3 Experiments,[0],[0]
"We run an approximate randomization test by stratifying the output and permuting within each stratum (Yeh, 2000) and find that, for four attractors, the performance difference between the top-down RNNG and the other variants is statistically significant at p < 0.05.
",4.3 Experiments,[0],[0]
The success of the top-down traversal in the domain of number-agreement prediction is consistent with a classical view in parsing that argues top-down parsing is the most human-like parsing strategy since it is the most anticipatory.,4.3 Experiments,[0],[0]
"Only
anticipatory representations, it is said, could explain the rapid, incremental processing that humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others.",4.3 Experiments,[0],[0]
"While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling.",4.3 Experiments,[0],[0]
"We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010).",4.3 Experiments,[0],[0]
"Given enough capacity, LSTMs trained on language modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multiple attractors.",5 Conclusion,[0],[0]
"Despite this strong performance, we discover explicit modeling of structure does improve the model’s ability to discover non-local structural dependencies when determining the distribution over subsequent word generation.",5 Conclusion,[0],[0]
"Recurrent neural network grammars (RNNGs), which jointly model phrase-structure trees and strings and employ an explicit composition operator, substantially outperform LSTM language models and syntactic language models without explicit compositions; this highlights the importance of a hierarchical inductive bias in capturing structural dependencies.",5 Conclusion,[0],[0]
We explore the possibility that how the structure is built affects number agreement performance.,5 Conclusion,[0],[0]
"Through novel extensions to RNNGs that enable the use of left-corner and bottom-up generation strategies, we discover that this is indeed the case: the three RNNG variants have different generalization properties for number agreement, with the top-down traversal strategy performing best for cases with multiple attractors.",5 Conclusion,[0],[0]
We would like to thank Tal Linzen for his help in data preparation and answering various questions.,Acknowledgments,[0],[0]
"We also thank Laura Rimell, Nando de Freitas, and the three anonymous reviewers for their helpful comments and suggestions.",Acknowledgments,[0],[0]
"Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-ofthe-art language models, LSTMs, fail to learn long-range syntax-sensitive dependencies.",abstractText,[0],[0]
"Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies—provided they have enough capacity.",abstractText,[0],[0]
"We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance.",abstractText,[0],[0]
"We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved.",abstractText,[0],[0]
"Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms leftcorner and bottom-up variants in capturing long-distance structural dependencies.",abstractText,[0],[0]
"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",title,[0],[0]
"Probabilistic inference in complex models generally requires the evaluation of intractable, high-dimensional integrals.",1. Introduction,[0],[0]
One powerful and generic approach to inference is to use Markov chain Monte Carlo (MCMC) methods to generate asymptotically exact (but correlated) samples from a posterior distribution for inference and learning.,1. Introduction,[0],[0]
"Hamiltonian Monte Carlo (HMC) (Duane et al., 1987; Neal, 2011) is a state-of-the-art MCMC method which uses gradient information from an absolutely continuous target density to encourage efficient sampling and exploration.",1. Introduction,[0],[0]
"Crucially, HMC utilizes proposals inspired by Hamiltonian dynamics (corresponding to the classical mechanics of a point particle) which can traverse long distances in parameter space.",1. Introduction,[0],[0]
"HMC, and variants like NUTS (which eliminates the need to hand-tune the algorithm’s hyperparameters), have been successfully applied to a large class of probabilistic inference problems where they are often the gold standard for
1UC Berkeley, USA 2University of Cambridge, UK 3Uber AI Labs, USA.",1. Introduction,[0],[0]
"Correspondence to: Nilesh Tripuraneni <nileshtrip@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(asymptotically) exact inference (Neal, 1996; Hoffman & Gelman, 2014; Carpenter et al., 2016).
",1. Introduction,[0],[0]
"In this paper, we first review important properties of Hamiltonian dynamics, namely energy-preservation, symplecticity, and time-reversibility, and derive a more general class of dynamics with these properties which we refer to as noncanonical Hamiltonian dynamics.",1. Introduction,[0],[0]
We then discuss the relationship of non-canonical Hamiltonian dynamics to wellknown variants of HMC and propose a novel extension of HMC.,1. Introduction,[0],[0]
We refer to this method as magnetic HMC (see Algorithm 1) since it corresponds to a particular subset of the non-canonical dynamics that in 3 dimensions map onto to the mechanics of a charged particle coupled to a magnetic field – see Figure 1 for an example of these dynamics.,1. Introduction,[0],[0]
"Furthermore, we construct an explicit, symplectic, leapfroglike integrator for magnetic HMC which allows for an efficient numerical integration scheme comparable to that of ordinary HMC.",1. Introduction,[0],[0]
"Finally, we evaluate the performance of magnetic HMC on several sampling problems where we show how its non-canonical dynamics can lead to improved mixing.",1. Introduction,[0],[0]
The proofs of all results in this paper are presented in the corresponding sections of the Appendix.,1. Introduction,[0],[0]
"Given an unnormalized target density ρ(θ) defined on Rd, an MCMC algorithm constructs an ergodic Markov chain (Θn)n∈N such that the distribution of Θn converges to ρ (e.g. in total variation) (Robert & Casella, 2004).",2. Markov chain Monte Carlo,[0],[0]
"Often, the transition kernel of such a Markov chain is specified by the Metropolis-Hastings (MH) algorithm which (i) given the
ar X
iv :1
60 7.
02 73
8v 2
[ st
at .M
L ]
1 9
A ug
2 01
7
current state Θn = θ, proposes a new state θ̃ by sampling from a proposal distribution Q(·|θ), and (ii) sets Θn+1 =",2. Markov chain Monte Carlo,[0],[0]
"θ̃ with probability min ( 1, ρ(θ̃)Q(θ|θ̃)
ρ(θ)Q(θ̃|θ)
) and Θn+1",2. Markov chain Monte Carlo,[0],[0]
"= θ oth-
erwise.",2. Markov chain Monte Carlo,[0],[0]
"The role of the acceptance step is to enforce reversibility (or detailed balance) of the Markov chain with respect to ρ – which implies ρ is a stationary distribution of the transition kernel.
",2. Markov chain Monte Carlo,[0],[0]
"Heuristically, a good MH algorithm should have low intersample correlation while maintaining a high acceptance ratio.",2. Markov chain Monte Carlo,[0],[0]
"Hamiltonian Monte Carlo provides an elegant mechanism to do this by simulating a particle moving along the contour lines of a dynamical system, constructed from the target density, to use as a MCMC proposal.",2. Markov chain Monte Carlo,[0],[0]
"In Hamiltonian Monte Carlo, the target distribution is augmented with “momentum” variables p which are independent of the θ variables but of equal dimension.",2.1. Hamiltonian Monte Carlo,[0],[0]
"For the remainder of the paper, we take the distribution over the momentum variables to be Gaussian, as is common in the literature (indeed, there is evidence that in many cases, the choice of a Gaussian distribution may be optimal (Betancourt, 2017)).",2.1. Hamiltonian Monte Carlo,[0],[0]
"The joint target distribution is therefore:
ρ(θ,p) ∝",2.1. Hamiltonian Monte Carlo,[0],[0]
"e−U(θ)−p>p/2 ≡ e−H(θ,p).",2.1. Hamiltonian Monte Carlo,[0],[0]
"(1)
Crucially, this augmentation allows Hamiltonian dynamics to be used as a proposal for an MCMC algorithm over the space (θ,p), where we interpret θ (resp., p) as position (resp., momentum) coordinates of a physical particle with total energy H(θ,p), given by the sum of its potential energy U(θ) and kinetic energy p>p/2.",2.1. Hamiltonian Monte Carlo,[0],[0]
"We briefly review the Markov chain construction below; see (Neal, 2011) or (Duane et al., 1987) for a more detailed description.",2.1. Hamiltonian Monte Carlo,[0],[0]
"Given the Markov chain state (θn,pn) at time n, the new state for time n + 1 is obtained by first resampling momentum pn ∼ N (0, I), and then proposing a new state according to the following steps: (i) Simulate the deterministic Hamiltonian flow defined by the differential equation
d
dt [ θ(t) p(t) ]",2.1. Hamiltonian Monte Carlo,[0],[0]
"= [ 0 I −I 0 ] ︸ ︷︷ ︸
A
[ ∇θH(p(t), θ(t))",2.1. Hamiltonian Monte Carlo,[0],[0]
"∇pH(p(t), θ(t))",2.1. Hamiltonian Monte Carlo,[0],[0]
"]
≡",2.1. Hamiltonian Monte Carlo,[0],[0]
"[
p(t) −∇θU(θ(t))
] .",2.1. Hamiltonian Monte Carlo,[0],[0]
"(2)
for time τ , with initial condition (θn,pn), to obtain (θ′n,p ′ n) =",2.1. Hamiltonian Monte Carlo,[0],[0]
"Φτ,H(θn,pn)
1; (ii) Flip the resulting momentum component with the map Φp(θ,p) =",2.1. Hamiltonian Monte Carlo,[0],[0]
"(θ,−p) to ob-
1Throughout this paper, we use Φτ,H to denote the map that takes a given position-momentum pair as initial conditions for the
tain (θ̃n+1, p̃n+1) = Φp(θ′n,p ′ n) = Φ̃τ,H(θn,pn); (iii) Apply a MH-type accept/reject step to enforce detailed balance with respect to the target distribution; (iv) Flip the momentum again with Φp so it points in the original direction.
",2.1. Hamiltonian Monte Carlo,[0],[0]
"Note that because the map Φτ,H is time-reversible (in the sense that if the path (θ(t),p(t)) is a solution to (2) then the path with negated momentum traversed in reverse (θ(−t),−p(−t)) is also a solution), the map Φ̃τ,H is selfinverse.",2.1. Hamiltonian Monte Carlo,[0],[0]
"From this, the acceptance ratio in step (iii) enforcing detailed balance can be shown (see e.g. (Green, 1995))",2.1. Hamiltonian Monte Carlo,[0],[0]
"to have the form:
min ( 1,
exp(−H(θ̃n+1, p̃n+1))",2.1. Hamiltonian Monte Carlo,[0],[0]
"exp(−H(θn,pn)) ∣∣∣det∇θ,pΦ̃τ,H(θn,pn)∣∣∣) .",2.1. Hamiltonian Monte Carlo,[0],[0]
"(3) Note that the Hamiltonian flow & momentum flip operator Φ̃τ,H is volume-preserving2, which immediately yields that the Jacobian term in the acceptance ratio (3) is simply 1.",2.1. Hamiltonian Monte Carlo,[0],[0]
"The acceptance probability therefore reduces to min(1, exp(H(θn,pn)",2.1. Hamiltonian Monte Carlo,[0],[0]
"− H(θ̃n+1, p̃n+1)))",2.1. Hamiltonian Monte Carlo,[0],[0]
.,2.1. Hamiltonian Monte Carlo,[0],[0]
"Furthermore, since the Hamiltonian flow defined in (2) is energypreserving (i.e. H(θ̃n+1, p̃n+1) = H(θn,pn)) – the acceptance ratio is identically 1.",2.1. Hamiltonian Monte Carlo,[0],[0]
"Moreover, the momentum resampling in (i) and momentum flip in (iv) both leave the joint distribution invariant.
",2.1. Hamiltonian Monte Carlo,[0],[0]
"While the momentum resampling ensures the Markov chain explores the joint (θ,p) space, the proposals inspired by Hamiltonian dynamics can traverse long distances in parameter space θ, reducing the random-walk behavior of MH that often results in highly correlated samples (Neal, 2011).",2.1. Hamiltonian Monte Carlo,[0],[0]
"Unfortunately, it is rarely possible to integrate the flow defined in (2) analytically; instead an efficient numerical integration scheme must be used to generate a proposal for the MH-type accept/reject test.",2.2. Symplectic Numerical Integration,[0],[0]
"Typically, the leapfrog (Störmer-Verlet) integrator is used since it is an explicit method that is both symplectic and time-reversible (Neal, 2011).",2.2. Symplectic Numerical Integration,[0],[0]
"One elegant way to motivate this integrator is by decomposing the Hamiltonian into a symmetric splitting:
H(θ,p) = U(θ)/2︸ ︷︷ ︸ H1(θ) +",2.2. Symplectic Numerical Integration,[0],[0]
"p>p/2︸ ︷︷ ︸ H2(p) +U(θ)/2︸ ︷︷ ︸ H1(θ)
(4)
and then defining Φ ,H1(θ) and Φ ,H2(p) to be the exactlyintegrated flows for the sub-Hamiltonians H1(θ) and
Hamiltonian flow associated withH for time τ .",2.2. Symplectic Numerical Integration,[0],[0]
"In addition, Φ̃τ,H denotes the composition of Φτ,H with the momentum flip map Φp.
2In fact the Hamiltonian flow satisfies the stronger condition of symplecticity with respect to the A matrix ([∇θ,pΦτ,H(θ,p)]>A−1[∇θ,pΦτ,H(θ,p)] = A−1) which immediately implies it is volume-preserving by taking determinants of this relation.
H2(p), respectively.",2.2. Symplectic Numerical Integration,[0],[0]
"These updates (which are equivalent to Euler translations) can be written:
Φ ,H1(θ)",2.2. Symplectic Numerical Integration,[0],[0]
[ θ p ] =,2.2. Symplectic Numerical Integration,[0],[0]
"[ θ
p− 2∇θU(θ) ]",2.2. Symplectic Numerical Integration,[0],[0]
"Φ ,H2(p)",2.2. Symplectic Numerical Integration,[0],[0]
[ θ p ] =,2.2. Symplectic Numerical Integration,[0],[0]
"[ θ + p p ] (5)
since the Hamilton equations (2) for the sub-Hamiltonians H1(θ) and H2(p) are linear, and hence analytically integrable.",2.2. Symplectic Numerical Integration,[0],[0]
"One leapfrog step is then defined as:
Φfrog ,H(θ,p) =",2.2. Symplectic Numerical Integration,[0],[0]
"Φ ,H1(θ) ◦",2.2. Symplectic Numerical Integration,[0],[0]
"Φ ,H2(p) ◦",2.2. Symplectic Numerical Integration,[0],[0]
"Φ ,H1(θ) (6)
with the overall proposal given by L leapfrog steps, followed by the momentum flip operator Φp as before:
Φ̃frogL, ,H = Φp ◦ ( Φfrog ,H(θ,p) )L .
",2.2. Symplectic Numerical Integration,[0],[0]
"As each of the flows Φ ,H1(θ), Φ ,H2(p) exactly integrates a sub-Hamiltonian, they inherit the symplecticity, volumepreservation, and time-reversibility of the exact dynamics.",2.2. Symplectic Numerical Integration,[0],[0]
"Moreover, since the composition of symplectic flows is also symplectic and the splitting scheme is symmetric (implying the composition of time-reversible flows is also timereversible), the Jacobian term in the acceptance probability (3) is exactly 1 as in the case of perfect simulation.
",2.2. Symplectic Numerical Integration,[0],[0]
"The leapfrog scheme will not exactly preserve the Hamiltonian H , so the remaining acceptance ratio exp(H(θn,pn) − H(θ̃n+1, p̃n+1)) must be calculated.",2.2. Symplectic Numerical Integration,[0],[0]
"However, the leapfrog integrator has error O( 3) in one leapfrog step (Hairer et al., 2006).",2.2. Symplectic Numerical Integration,[0],[0]
"This error scaling will lead to good energy conservation properties (and thus high acceptance rates in the MH step), even when simulating over long trajectories.",2.2. Symplectic Numerical Integration,[0],[0]
"In Section 2, we noted the role time-reversibility, volumepreservation, and energy conservation of canonical Hamiltonian dynamics play in making them useful candidates for MCMC.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"In this section, we develop the properties of a general class of flows we refer to as non-canonical Hamiltonian systems that parallel these properties, we use to construct our method magnetic HMC (see Algorithm 1):
Lemma 1.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"The map ΦAτ,H(θ,p) defined by integrating the non-canonical Hamiltonian system
d
dt [ θ(t) p(t)",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
],3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"= A∇θ,pH(θ(t),p(t))",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"(7)
with initial conditions (θ,p) for time τ , where A ∈ M2n×2n is any invertible, antisymmetric matrix induces a flow on the coordinates (θ,p) that is still energyconserving (∂τH(ΦAτ,H(θ,p))",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"= 0) and symplectic with
respect to A ([∇θ,pΦτ,H(θ,p)]>A−1[∇θ,pΦτ,H(θ,p)] = A−1) which also implies volume-preservation of the flow.
",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"Within the formal construction of classical mechanics, it is known that any Hamiltonian flow defined on the cotangent bundle (θ,p) of a configuration manifold, which is equipped with an arbitrary symplectic 2-form, will preserve its symplectic structure and admit the corresponding Hamiltonian as a first integral invariant (Arnold, 1989).",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
The statement of Lemma 1 is simply a restatement of this fact grounded in a coordinate system.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"Similar arbitrary, antisymmetric terms have also appeared in the study of MCMC algorithms based on diffusion processes; such samplers often do not enforce detailed balance with respect to the target density and are often implemented as discretizations of stochastic differential equations (Rey-Bellet & Spiliopoulos, 2015; Ma et al., 2015), in contrast to the approach taken here.
",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
Our second observation is that the dynamics in (17) are not time-reversible in the traditional sense.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"Instead, if we consider the parametrization of A as:
A = [ E F −F> G ] (8)
where E, G are antisymmetric and F is taken to be general such that A is invertible, then the non-canonical dynamics have a (pseudo) time-reversibility symmetry:
Lemma 2.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"If (θ(t),p(t)) is a solution to the non-canonical dynamics:
d
dt [ θ(t) p(t) ]",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"= [ E F −F> G ] ︸ ︷︷ ︸
A
[ ∇θH(θ(t),p(t)) ∇pH(θ(t),p(t)) ]",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"(9)
then (θ̃(t), p̃(t))",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
=,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"(θ(−t),−p(−t)) is a solution to the modified non-canonical dynamics:
d
dt [ θ̃(t) p̃(t) ]",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"= [ −E F −F> −G ] ︸ ︷︷ ︸
Ã
[ ∇θ̃H(θ̃(t),p(t)) ∇p̃H(θ̃(t), p̃(t)) ]",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"(10)
if H(θ,p) = H(θ,−p).",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"In particular if E = G = 0 then A = Ã, which reduces to the traditional time-reversal symmetry of canonical Hamiltonian dynamics.
",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
Lemma 1 suggests a generalization of HMC that can utilize an arbitrary invertible antisymmetric A matrix in its dynamics; however Lemma 2 indicates the non-canonical dynamics lack a traditional time-reversibility symmetry which poses a potential difficulty to satisfying detailed balance.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"In particular, we cannot compose Φp with an exact/approximate simulation of ΦAτ,H to make Φ̃ A τ,H = Φp ◦ΦAτ,H self-inverse.
",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
Our solution to obtaining a time-reversible proposal is simply to flip the elements of the E and G matrices just as ordinary HMC flips the auxiliary variable p i) at the end of Hamiltonian flow in the proposal and ii) once again after the MH acceptance step to return p to its original direction.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"In this vein, we view the parameters E and G as auxiliary variables in the state space, and simultaneously flip p, E, and G after having simulated the dynamics, rendering the proposal time-reversible according to Lemma 2 – see Section 2 in the Appendix for full details of this construction.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
This ensures that detailed balance is satisfied for this entire proposal.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"To avoid “random walk” behaviour in the resulting Markov chain, we can apply a sign flip to E and G, in addition to p, to return them to their original directions after the MH acceptance step.
",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
The validity of this construction relies on equipping E and G with symmetric auxiliary distributions.,3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"For the remainder of this paper, we further restrict to binary symmetric auxiliary distributions supported on a given antisymmetric matrix V0 and its sign flip −V0 – see Appendix 1.1 for full details.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"This restriction is not necessary, but gives rise to a simple and interpretable class of algorithms, which is in spirit closest to using fixed parameters E and G, whilst ensuring the proposal satisfies detailed balance.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"This construction is also reminiscent of lifting constructions prevalent in the discrete Markov chain literature (Chen et al., 1999); heuristically, the signed variables E and G favour proposals in opposing directions.",3. Non-Canonical Hamiltonian Monte Carlo,[0],[0]
"As with standard HMC, exactly simulating the flow ΦAτ,H is rarely tractable, and a numerical integrator is required to approximate the flow.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"It is not absolutely necessary to use an explicit, symplectic integration scheme; indeed implicit integrators are used in Riemannian HMC to maintain symplecticity of the proposal which comes at a greater complexity and computational cost (Girolami et al., 2009).",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"However explicit, symplectic integrators are simple, have good energy-conservation properties, and are volumepreserving/time-reversible (Hairer et al., 2006), so for the present discussion we restrict our attention to investigating leapfrog-like schemes.
",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"We begin, as in Section 2.2, by considering the symmetric splitting (34), yielding the sub-Hamiltonians H1(θ) = U(θ)/2, H2(p) = p>p/2.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"The corresponding noncanonical dynamics for the sub-Hamiltonians H1(θ) and H2(p) are:
d
dt",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
[ θ p ] =,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"[ E F −F> G ] ︸ ︷︷ ︸
A
[ ∇θU(θ)/2
0
] =",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"[ E∇θU(θ)/2 −F>∇θU(θ)/2 ]
and:
d
dt [ θ p ] =",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"[ E F −F> G ] ︸ ︷︷ ︸
A
[ 0 p ] =",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"[ Fp Gp ] .
",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"We denote the corresponding flows by ΦA ,H1(θ) and ΦA ,H2(p) respectively.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"The flow Φ A ,H1(θ)
is generally not explicitly tractable unless we take E = 0 – in which case it is solved by an Euler translation as before.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Crucially, the flow in ΦA ,H2(p) is a linear differential equation and hence analytically integrable.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"If G is invertible (and F = I) then:
Φ ,H2(p)",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
[ θ p ] =,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
[ θ + G−1(exp(G ),3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
− I)p exp(G )p ] .,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"(11)
See the Appendix for a detailed derivation which also handles the general case where G is not invertible.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Thus when E = 0, the flows ΦA ,H1(θ) and Φ A ,H2(p)
are analytically tractable and will inherit the generalized symplecticity and (pseudo) time-reversibility of the exact dynamics in (17).",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Therefore if we use the symmetric splitting (34) to construct a leapfrog-like step:
Φfrog,A ,H(θ,p) =",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Φ A ,H1(θ) ◦ΦA ,H2(p) ◦",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Φ A ,H1(θ)
(12)
we can construct a total proposal that consists of several leapfrog steps, followed by a flip of the momentum and G, Φp ◦ΦG, which will be a volume-preserving, self-inverse map:
Φ̃ frog,A ,H(θ,p) =",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
Φp ◦ΦG ◦,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"(ΦA ,H1(θ) ◦",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Φ A ,H2(p) ◦ΦA ,H1(θ))",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
L .,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"(13)
Henceforth we will always take E = 0",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"when we use Φfrog,A ,H(θ,p) to generate leapfrog proposals, which interestingly corresponds to a magnetic dynamics as discussed in Lemma 4.",3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
A full description of the magnetic HMC algorithm using this numerical integrator is described in Section 5.,3.1. Symplectic Numerical Integration for Non-Canonical Dynamics,[0],[0]
"Here, we describe several tractable subcases of the general formulation of non-canonical Hamiltonian dynamics since these they have interesting physical interpretations.",4. Special Cases,[0],[0]
"One simple variant of HMC is preconditioned HMC where p ∼ N (0,M)",4.1. Mass Preconditioned Dynamics,[0],[0]
"(Neal, 2011), and can be implemented nearly identically to ordinary HMC.",4.1. Mass Preconditioned Dynamics,[0],[0]
"We note that preconditioning can be recovered within our framework using a simple form for the non-canonical A matrix:
Lemma 3.",4.1. Mass Preconditioned Dynamics,[0],[0]
"i) Preconditioned HMC with momentum variable p ∼ N (0,M) in the (θ,p) coordinates is exactly equivalent to simulating non-canonical HMC with p′ = M−1/2p ∼ N",4.1. Mass Preconditioned Dynamics,[0],[0]
"(0, I) and the non-canonical matrix A =
Magnetic Hamiltonian Monte Carlo[ 0 M1/2
−(M1/2)",4.1. Mass Preconditioned Dynamics,[0],[0]
"> 0
] , and then transforming back to
(θ,p) coordinates using p = M1/2p′. Here M1/2 is a Cholesky factor for M.
ii)",4.1. Mass Preconditioned Dynamics,[0],[0]
"On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates θ′ = F−1θ, simulate HMC in the (θ′,p) coordinates, and transform back to the original basis using F, this is exactly equivalent to
non-canonical HMC with A = [
0 F −F> 0
] .
",4.1. Mass Preconditioned Dynamics,[0],[0]
"Lemma 3 illustrates a fact alluded to in (Neal, 2011); using a mass preconditioning matrix M and a change of basis given by matrix −(M−1/2)",4.1. Mass Preconditioned Dynamics,[0],[0]
> acting on θ leaves the HMC algorithm invariant.,4.1. Mass Preconditioned Dynamics,[0],[0]
"The primary focus of this paper is to investigate the subcase of the dynamics where:
A = ( 0 I −I G ) (14)
for two important reasons3.",4.2. Magnetic Dynamics,[0],[0]
"Firstly for this choice of A we can construct an explicit, symplectic, leapfrog-like integration scheme which is important for developing an efficient HMC sampler as discussed in Section 3.1.",4.2. Magnetic Dynamics,[0.9519112556152234],"['To formulate the single-task learning as an optimization problem, we choose a parametric form for the posterior of each task Qφi , φi ∈ RNQ (see section 4.3 for an explicit example).']"
"Secondly, the dynamics have an interesting physical interpretation quite distinct from mass preconditioning and other HMC variants like Riemannian HMC (Girolami et al., 2009):
Lemma 4.",4.2. Magnetic Dynamics,[0],[0]
"In 3 dimensions the non-canonical Hamiltonian dynamics corresponding to the Hamiltonian H(θ,p) = U(θ) + 12p
>p and A matrix as in (14) are equivalent to the Newtonian mechanics of a charged particle (with unit mass and charge) coupled to a magnetic field ~B (given by a particular function of G - see Appendix): d
2θ dt2 =
−∇θU(θ) + dθdt × ~B.
",4.2. Magnetic Dynamics,[0],[0]
"This interpretation is perhaps surprising since Hamiltonian formulations of classical magnetism are uncommon, although the quantum mechanical treatment naturally incorporates a Hamiltonian framework.",4.2. Magnetic Dynamics,[0],[0]
"However, in light of Lemma 3 we might wonder if by a clever rewriting of the Hamiltonian we can reproduce this system of ODEs using the canonical A matrix (i.e. E = G = 0, F = I).",4.2. Magnetic Dynamics,[0],[0]
"This is not the case:
Lemma 5.",4.2. Magnetic Dynamics,[0],[0]
"The non-canonical Hamiltonian dynamics with magnetic A and Hamiltonian H(θ,p) = U(θ) + 12p
>p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian.",4.2. Magnetic Dynamics,[0],[0]
"(See Appendix).
3Note that the effect of a non-identity F matrix can be achieved by simply composing these magnetic dynamics with a coordinate-transformation as suggested in Lemma 3.",4.2. Magnetic Dynamics,[0],[0]
"Using the results discussed in Section 3 and Section 3.1 we can now propose Magnetic HMC – see Algorithm 1.
",5. The Magnetic HMC (MHMC) Algorithm,[0],[0]
Algorithm 1 Magnetic HMC (MHMC),5. The Magnetic HMC (MHMC) Algorithm,[0],[0]
"Initialize (θ0,p0), and set G0 ← G for n = 1, . . .","Input: H , G, L,",[0],[0]
", N do
Resample pn−1 ∼ N(0, I)","Input: H , G, L,",[0],[0]
"Set (θ̃n, p̃n) ← LF(H,L, , (θn−1,pn−1,Gn−1)) with ΦA ,H2(p) as in (11) Flip momentum (θ̃n, p̃n) ← (θ̃n,−p̃n) and set G̃n ← −Gn−1 if Unif([0, 1]) < min(1, exp(H(θn−1,pn−1)","Input: H , G, L,",[0],[0]
"− H(θ̃n, p̃n)))","Input: H , G, L,",[0],[0]
"then
Set (θn,pn,Gn)← (θ̃n, p̃n, G̃n) else
Set (θn,pn,Gn)← (θn−1,pn−1,Gn−1) end if Flip momentum pn ← −pn and flip Gn ← −Gn
end for Output: (θn)Nn=0
One further remark is that by construction the integrator for magnetic HMC is expected to have similarly good energy conservation properties to the integrator of standard HMC:
Lemma 6.","Input: H , G, L,",[0],[0]
"The symplectic leapfrog-like integrator for magnetic HMC will have the same local (∼ O( 3)) and global (∼ O( 2)) error scaling (over τ ∼ L steps), as the canonical leapfrog integrator of standard HMC if the Hamiltonian is separable.","Input: H , G, L,",[0],[0]
(See Appendix).,"Input: H , G, L,",[0],[0]
It is worthwhile to contrast the algorithmic differences between magnetic HMC and ordinary HMC.,"Input: H , G, L,",[0],[0]
"Intuitively, the role of the flow ΦA ,H2(p) – which reduces to the standard Euler translation update of ordinary HMC when G = 0 – is to introduce a rotation into the momentum space of the flow.","Input: H , G, L,",[0],[0]
"In particular, a non-zero element Gij will allow momentum to periodically flow between pi and pj .","Input: H , G, L,",[0],[0]
"If we regard G as an element in the Lie algebra of antisymmetric matrices, which can be thought of as infinitesimal rotations, then the exponential map exp(G ) will project this transformation into the Lie group of real orthogonal linear maps.
","Input: H , G, L,",[0],[0]
"With respect to computational cost, although magnetic HMC requires matrix exponentiation/diagonalization to simulate ΦA ,H2(p), this only needs to be computed once upfront for ±G and cached; moreover, as ±G is diagonalizable, the exact exponential can be calculated inO(d3) time.","Input: H , G, L,",[0],[0]
"Additionally, there is an O(d2) cost for the matrix-vector products needed to implement the flow ΦA ,H2(p) as with preconditioning.","Input: H , G, L,",[0],[0]
"However, it is possible to design sparsi-
fied matrix representations of A which will translate into sparsified rotations if we only wish to ”curl” in a specific subspace of dimension d0 – which will translate into a computational cost of O(d30) and O(d20) respectively.","Input: H , G, L,",[0],[0]
"An important problem to address is the selection of the G matrix, which affords a great deal of flexibility to MHMC relative to HMC; this point is further discussed in the Experiments section, where we argue that in certain cases intuitive heuristics can be used to select the G matrix.","Input: H , G, L,",[0],[0]
Here we investigate the performance of magnetic HMC against standard HMC in several examples; in each case commenting on our choice of the magnetic field term G. Step sizes ( ) and number of leapfrog steps (L) were tuned to achieve an acceptance rate between .7,6. Experiments,[0],[0]
"− .8, after which the norm of the non-zero elements in G was set to∼ .1−.2",6. Experiments,[0],[0]
"which was found to work well.
",6. Experiments,[0],[0]
In the Appendix we also display illustrations of different MHMC proposals across several targets in order to provide more intuition for MHMC’s dynamics.,6. Experiments,[0],[0]
Further experimental details and an additional experiment on a Gaussian funnel target are also provided in the Appendix.,6. Experiments,[0],[0]
"We consider two highly ill-conditioned Gaussians similar to as in (Sohl-Dickstein et al., 2014) to illustrate a heuristic for G matrix selection and demonstrate properties of the magnetic dynamics.",6.1. Multiscale Gaussians,[0],[0]
"In particular we consider a centered, uncorrelated 2D Gaussian with covariance eigenvalues of 106 and 1 as well as a centered, uncorrelated 10D Gaussian with two large covariance eigenvalues of 106 and remaining eigenvalues of 1.",6.1. Multiscale Gaussians,[0],[0]
"We denote their coordinates as x = (x1, x2) ∈ R2 and x = (x1, . . .",6.1. Multiscale Gaussians,[0],[0]
", x10) ∈ R10 respectively.",6.1. Multiscale Gaussians,[0],[0]
"HMC will have difficulty exploring the directions of
0 5× 104 Lag
0
1
A u to co rr e la ti o n
HMC
MHMC
0 5× 104 Lag
0
1
A u to co rr e la ti o n
HMC
MHMC
Figure 2.",6.1. Multiscale Gaussians,[0],[0]
"Averaged Autocorrelation of HMC vs MHMC on a 10D ill-conditioned Gaussian (left) and Averaged Autocorrelation of HMC vs MHMC on a 2D ill-conditioned Gaussian.
",6.1. Multiscale Gaussians,[0],[0]
large marginal variance since its exploration will often be limited by the smaller variance directions.,6.1. Multiscale Gaussians,[0],[0]
"Accordingly, in order to induce a periodic momentum flow between the directions of small and large variance, we introduce nonzero components Gij into the subspaces spanned directly between the large and small eigenvalues.",6.1. Multiscale Gaussians,[0],[0]
"Indeed, we find that magnetic G term is encouraging more efficient exploration as we can see from the averaged autocorrelation of samples generated from the HMC/MHMC chains – see Figure 2.",6.1. Multiscale Gaussians,[0],[0]
"Further, by running the 50 parallel chains for 107 timesteps, we computed both the bias and Monte Carlo standard errors (MCSE) of the estimators of the target moments as shown in Table 1 and Table 2.",6.1. Multiscale Gaussians,[0],[0]
"We compare MHMC vs. HMC on a simple, but interesting, 2D density over x = (x, y) ∈ R2 comprised of an evenly weighted mixture of isotropic Gaussians: p(x) = 12N (x;µ,Σ)",6.2. Mixture of Gaussians,[0],[0]
"+ 12N (x;−µ,Σ) for σ2x = σ2y = 1, ρxy = 0 and µ = (2.5,−2.5).",6.2. Mixture of Gaussians,[0],[0]
This problem is challenging for HMC because the gradients in canonical Hamiltonian dynamics force it to one of the two modes.,6.2. Mixture of Gaussians,[0],[0]
"We tuned HMC to achieve an acceptance rate of ∼ .75 and used the same , L for MHMC, generating 15000 samples from both HMC and MHMC with these settings.",6.2. Mixture of Gaussians,[0],[0]
The addition of the magnetic field term G – which has one degree of freedom in this case – introduces an asymmetric “curl” into the dynamics that pushes the sampler across the saddlepoint to the other mode allowing it to efficiently mix around both modes and between them – see Figure 3.,6.2. Mixture of Gaussians,[0],[0]
"The maximum mean discrepancy between exact samples generated from the target density and samples generated from both HMC and MHMC chains was also estimated for various magnitudes of G, using a quadratic ker-
nel k(x,x′) =",6.2. Mixture of Gaussians,[0],[0]
"(1 + 〈x,x′〉)2 and averaged over 100 runs of the Markov chains (Borgwardt et al., 2006).",6.2. Mixture of Gaussians,[0],[0]
"Here, we clearly see that for various values of the nonzero component of G, denoted g, the samples generated by MHMC more faithfully reflect the structure of the posterior.",6.2. Mixture of Gaussians,[0],[0]
"As before, we ran 50 parallel chains for 107 timesteps to compute both the bias and Monte Carlo standard errors (MCSE) of the estimators of the target moments as shown in Table 3.",6.2. Mixture of Gaussians,[0],[0]
"Additional experiments over a range of , L (and corre-
MHMC .00239",6.2. Mixture of Gaussians,[0],[0]
"± .012 0.000596 ± 0.00365
sponding acceptance rates) and details are included in the Appendix for this example, demonstrating similar behavior.",6.2. Mixture of Gaussians,[0],[0]
"Finally, we consider the problem of Bayesian inference over the parameters of the FitzHugh-Nagumo model (a set of nonlinear ordinary differential equations, originally developed to model the behavior of axial spiking potentials in neurons) as in (Ramsay et al., 2007; Girolami & Calderhead, 2011).",6.3. FitzHugh-Nagumo model,[0],[0]
"The FitzHugh-Nagumo model is a dynamical system (V (t), R(t)) defined by the following coupled differential equations:
V̇ (t) = c(V (t)− V (t)3/3 +R(t)) Ṙ(t)",6.3. FitzHugh-Nagumo model,[0],[0]
= −(V,6.3. FitzHugh-Nagumo model,[0],[0]
"(t)− a+ bR(t))/c (15)
We consider the problem where the initial conditions
(V (0), R(0)) of the system (15) are known, and a set of noise-corrupted observations (Ṽ (tn), R̃(tn))Nn=0 = (Va,b,c(tn)+ε V n , Ra,b,c(tn)+ε R n ) N",6.3. FitzHugh-Nagumo model,[0],[0]
"n=0 at discrete time points 0 = t0 < t1 < · · · < tN , are available - note that we illustrate dependence of the trajectories on the model parameters explicitly via subscripts.",6.3. FitzHugh-Nagumo model,[0],[0]
"It is not possible to recover the true parameter values of the model from these observations, but we can obtain a posterior distribution over them by specifying a model for the observation noise and a prior distribution over the model parameters.
",6.3. FitzHugh-Nagumo model,[0],[0]
"Similar to (Ramsay et al., 2007; Girolami & Calderhead, 2011), we assume that the observation noise variables (εVn ) N",6.3. FitzHugh-Nagumo model,[0],[0]
n=0 and (ε V n ) N,6.3. FitzHugh-Nagumo model,[0],[0]
"n=0 are iid N (0, 0.12), and take an independent N (0, 1) prior over each parameter a, b, and c. This yields a posterior distribution of the form
p(a, b, c) ∝",6.3. FitzHugh-Nagumo model,[0],[0]
"N (a; 0, 1)N (b; 0, 1)N (c; 0, 1) × N∏ n=0 N",6.3. FitzHugh-Nagumo model,[0],[0]
"(Ṽ (tn);Va,b,c(tn), 0.12) (16)
",6.3. FitzHugh-Nagumo model,[0],[0]
"Importantly, the highly non-linear dependence of the trajectory on the parameters a, b and c yields a complex posterior distribution - see Figure 4.",6.3. FitzHugh-Nagumo model,[0],[0]
"Full details of the model set-up can be found in (Ramsay et al., 2007; Girolami & Calderhead, 2011).
",6.3. FitzHugh-Nagumo model,[0],[0]
"For our experiments, we used fixed parameter settings of a = 0.2, b = 0.2, c = 3.0 to generate 200 evenlyspaced noise-corrupted observations over the time interval t =",6.3. FitzHugh-Nagumo model,[0],[0]
"[0, 20] (as in (Ramsay et al., 2007; Girolami & Calderhead, 2011)).",6.3. FitzHugh-Nagumo model,[0],[0]
"We performed inference over the posterior distribution of parameters (a, b, c) with this set of observations using both the HMC and MHMC algorithms, which was perturbed with a magnetic field in each of the 3 axial planes of parameters – along the ab, ac, and bc axes with magnitude g = 0.1.",6.3. FitzHugh-Nagumo model,[0],[0]
"The chains were run to gener-
ate 1000 samples over 100 repetitions with settings of = 0.015, L = 10, which resulted in an average acceptance rate of∼ .8.",6.3. FitzHugh-Nagumo model,[0],[0]
"The effective sample size of each of the chains normalized per unit time was then computed for each chain.
",6.3. FitzHugh-Nagumo model,[0],[0]
"Since each query to the posterior log-likelihood or posterior gradient log-likelihood requires solving an augmented set of differential equations as in (15), the computation time (∼ 238s) of all the methods was nearly identical.",6.3. FitzHugh-Nagumo model,[0],[0]
"Moreover,
note that all methods achieved nearly perfect mixing over the first coordinate so their effective sample size were truncated at 1000 for the a coordinate.",6.3. FitzHugh-Nagumo model,[0],[0]
"In this example, we can see that all magnetic perturbations slightly increase the mixing rate of the sampler over each of the (b, c) coordinates with the ab perturbation performing best.",6.3. FitzHugh-Nagumo model,[0],[0]
"We have investigated a framework for MCMC algorithms based on non-canonical Hamiltonian dynamics and have given a construction for an explicit, symplectic integrator that is used to implement a generalization of HMC we refer to as magnetic HMC.",7. Discussion and Conclusion,[0],[0]
We have also shown several examples where the non-canonical dynamics of MHMC can improve upon the sampling performance of standard HMC.,7. Discussion and Conclusion,[0],[0]
"Important directions for further research include finding more automated, adaptive mechanisms to set the matrix G, as well as investigating positionally-dependent magnetic field components, similar to how Riemannian HMC corresponds to local preconditioning.",7. Discussion and Conclusion,[0],[0]
We believe that exploiting more general deterministic flows (such as also maintaining a non-zero E in the top left-block of a general A matrix) could form a fruitful area for further research on MCMC methods.,7. Discussion and Conclusion,[0],[0]
"The authors thank John Aston, Adrian Weller, Maria Lomeli, Yarin Gal and the anonymous reviewers for helpful comments.",Acknowledgements,[0],[0]
"MR acknowledges support by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, the Cambridge Centre for Analysis.",Acknowledgements,[0],[0]
RET thanks EPSRC grants EP/M0269571 and EP/L000776/1 as well as Google for funding.,Acknowledgements,[0],[0]
Here we provide proofs for results discussed in Section 3 of the main text regarding non-canonical dynamics.,A. Section 3 and 4 Proofs,[0],[0]
Lemma 1.,A. Section 3 and 4 Proofs,[0],[0]
"The map ΦAτ,H(θ,p) defined by integrating the non-canonical Hamiltonian system
d
dt [ θ(t) p(t)",A. Section 3 and 4 Proofs,[0],[0]
],A. Section 3 and 4 Proofs,[0],[0]
"= A∇θ,pH(θ(t),p(t))",A. Section 3 and 4 Proofs,[0],[0]
"(17)
with initial conditions (θ,p) for time τ , where A ∈ M2n×2n is any invertible, antisymmetric matrix induces a flow on the coordinates (θ,p) that is still energy-conserving (∂τH(ΦAτ,H(θ,p))",A. Section 3 and 4 Proofs,[0],[0]
"= 0) and symplectic with respect to A ([∇θ,pΦτ,H(θ,p)]>A−1[∇θ,pΦτ,H(θ,p)] = A−1) which also implies volume-preservation of the flow.
",A. Section 3 and 4 Proofs,[0],[0]
Proof.,A. Section 3 and 4 Proofs,[0],[0]
"The proofs of both results simply uses the antisymmetry of A.
Energy-Conservation – Simply, we have that:
∂τH(Φτ,H(θ,p))",A. Section 3 and 4 Proofs,[0],[0]
"= ∇θ,pH(Φτ,H(θ,p))∂τΦτ,H(θ,p) =",A. Section 3 and 4 Proofs,[0],[0]
"(18) ∇θ,pH(Φτ,H(θ,p))>A∇θ,pH(Φτ,H(θ,p))",A. Section 3 and 4 Proofs,[0],[0]
"= 0 (19)
using the antisymmetry of A and symmetry of∇θ,pH(Φτ,H(θ,p))∇θ,pH(Φτ,H(θ,p))",A. Section 3 and 4 Proofs,[0],[0]
>.,A. Section 3 and 4 Proofs,[0],[0]
"Symplecticness – We must show that the Jacobian of the flow generated by the dynamics preserves the non-canonical structure matrix A, which amounts to showing:
",A. Section 3 and 4 Proofs,[0],[0]
"[∇θ,pΦτ,H(θ,p)]>︸ ︷︷ ︸ F (τ)>",A. Section 3 and 4 Proofs,[0],[0]
A−1,A. Section 3 and 4 Proofs,[0],[0]
"[∇θ,pΦτ,H(θ,p)]︸ ︷︷ ︸ F (τ) =",A. Section 3 and 4 Proofs,[0],[0]
"A−1 (20)
where we define F (τ) = ∇θ,pΦτ,H(θ,p) as the time-evolving Jacobian of the flow.",A. Section 3 and 4 Proofs,[0],[0]
"First, note that F (τ) can be equivalently described as the solution to the differential equation:
d
dτ F (τ) = A∇θ,p∇θ,pH(Φτ,H(θ,p))F (τ) (21)
with the initial condition F (0) =",A. Section 3 and 4 Proofs,[0],[0]
I2d (the Jacobian for the identity map at t = 0),A. Section 3 and 4 Proofs,[0],[0]
.,A. Section 3 and 4 Proofs,[0],[0]
"Trivially, we have:
F (0)A−1F (0) =",A. Section 3 and 4 Proofs,[0],[0]
"A−1 (22)
Then note that:
d
dτ (F (τ)>A−1F (τ))",A. Section 3 and 4 Proofs,[0],[0]
"= F (τ)>A−1A∇θ,p∇θ,pH(Φτ,H(θ,p))F (τ) + F (τ)>∇θ,p∇θ,pH(Φτ,H(θ,p))A>A−1F (τ) = F (τ)>∇θ,p∇θ,pH(Φτ,H(θ,p))F (τ)− F (τ)>∇θ,p∇θ,pH(Φτ,H(θ,p))F (τ) = 0
as desired by simply using A> = −A.
Time-Reversibility – However, crucially it is not the case that the Hamilton equations are time-reversible in the traditional sense of canonical Hamiltonian dynamics.",A. Section 3 and 4 Proofs,[0],[0]
Lemma 2.,A. Section 3 and 4 Proofs,[0],[0]
"If (θ(t),p(t)) is a solution to the non-canonical dynamics:
d
dt [ θ(t) p(t) ]",A. Section 3 and 4 Proofs,[0],[0]
"= [ E F −F> G ] ︸ ︷︷ ︸
A
[ ∇θH(θ(t),p(t)) ∇pH(θ(t),p(t)) ]",A. Section 3 and 4 Proofs,[0],[0]
"(23)
then (θ̃(t), p̃(t))",A. Section 3 and 4 Proofs,[0],[0]
"= (θ(−t),−p(−t)) is a solution to the modified non-canonical dynamics:
d
dt [ θ̃(t) p̃(t) ] =",A. Section 3 and 4 Proofs,[0],[0]
"[ −E F −F> −G ] ︸ ︷︷ ︸
Ã
[ ∇θ̃H(θ̃(t),p(t)) ∇p̃H(θ̃(t), p̃(t))",A. Section 3 and 4 Proofs,[0],[0]
"] (24)
ifH(θ,p) = H(θ,−p).",A. Section 3 and 4 Proofs,[0],[0]
"In particular if E = G = 0 then A = Ã, which reduces to the traditional time-reversal symmetry of canonical Hamiltonian dynamics.
",A. Section 3 and 4 Proofs,[0],[0]
Proof.,A. Section 3 and 4 Proofs,[0],[0]
"A direct calculation yields
d
dt [ θ̃(t) p̃(t) ] =",A. Section 3 and 4 Proofs,[0],[0]
"[ − ddtθ(−t)
d dtp(−t)
] = [ −E∇θH(θ(−t))− F∇pH(θ(−t))",A. Section 3 and 4 Proofs,[0],[0]
−F>∇θH(θ(−t)),A. Section 3 and 4 Proofs,[0],[0]
+ G∇pH(θ(−t)) ],A. Section 3 and 4 Proofs,[0],[0]
"=
[ −E∇θ̃H(θ̃(t)) + F∇p̃H(θ̃(t)) −F>∇θ̃H(θ̃(t))−G∇p̃H(θ̃(t))",A. Section 3 and 4 Proofs,[0],[0]
] =,A. Section 3 and 4 Proofs,[0],[0]
"[ −E F −F> −G ] ︸ ︷︷ ︸
Ã
[ ∇θ̃H(θ̃(t)) ∇p̃H(θ̃(t))",A. Section 3 and 4 Proofs,[0],[0]
],A. Section 3 and 4 Proofs,[0],[0]
As remarked in the main text it is necessary to flip the E and G matrices at the end of a deterministic simulation of the Hamiltonian dynamics in order to render the proposal time-reversible which is in turn necessary to satisfy detailed balance.,A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"This is crucial for the correctness of the algorithm especially when an approximate simulation of the dynamics is used (as is always often the case).
",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"In particular, say that we wish to use ΦAτ,H(θ,p) as a transition kernel with fixed, non-zero values of E = E0 and G = G0.",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"We first augment the state-space by placing a symmetric, binary distribution independently over E and G such that p(E = E0) = p(E = −E0) = 1/2 and p(G = G0) = p(G = −G0) = 1/2, independently of θ,p:
ρ(θ,p,E,G) ∝",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"e−H(θ,p)p(E)p(G).",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"(25)
Importantly, this augmentation leaves the distribution over θ,p intact when E and G are marginalized out.",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"Just as applying the momentum flip operator, Φp : (θ,p,E,G)→ (θ,−p,E,G), is a deterministic, energy-preserving, volume-preserving transformation, the E, G flip operators, ΦE : (θ,p,E,G)",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"→ (θ,p,−E,G) and ΦG : (θ,p,E,G) → (θ,p,E,−G), are also deterministic, energy-preserving, volume-preserving transformations that leave (25) invariant for this particular augmentation with p(E) and p(G).",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"We can now build a self-inverse operator Φ̃Aτ,H(θ,p), composed of simulating the Hamiltonian flow as ΦAτ,H(θ,p) plus ΦE ◦ΦG ◦Φp, a flip of p, E, G, as:
Φ̃Aτ,H(θ,p) = ΦE ◦ΦG ◦Φp ◦ΦAτ,H(θ,p) (26)
Now we have constructed a deterministic, self-inverse map Φ̃Aτ,H(θ,p).",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"Φ̃ A τ,H(θ,p) can now be used as the proposal for a reversible MCMC algorithm.
",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"An important point to note is that our choice of variable augmentation strategy, namely augmenting with binary distribution, is certainly not unique.",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"However, it is perhaps the most natural and simplest choice which avoids the repetitive computation of matrix exponentials/diagonalizations since the approximate flow detailed in Section B.2 will only need to compute matrix exponentials once upfront for ±G.",A.1. Non-Canonical Dynamics Variable Augmentation,[0],[0]
"A common variation on standard HMC dynamics is to set the kinetic energy term in the HamiltonianH(θ,p) to 12p >M−1p for some symmetric positive-definite matrix M, and sample the initial momentum variable p from the corresponding distribution N (0,M).",A.2. Mass Preconditioning Proofs,[0],[0]
"However, we can contextualize preconditioning using a non-canonical A matrix in the following manner:
Lemma 3.",A.2. Mass Preconditioning Proofs,[0],[0]
"i) Preconditioned HMC with momentum variable p ∼ N (0,M) in the (θ,p) coordinates, is exactly equivalent to simulating non-canonical HMC with p′ = M−1/2p ∼ N (0, I) and the non-canonical matrix:
A =
[ 0 M1/2
−(M1/2)",A.2. Mass Preconditioning Proofs,[0],[0]
> 0,A.2. Mass Preconditioning Proofs,[0],[0]
] and,A.2. Mass Preconditioning Proofs,[0],[0]
"then transforming back to (θ,p) coordinates using p = M1/2p′. Here M1/2 is a Cholesky factor for M.
ii)",A.2. Mass Preconditioning Proofs,[0],[0]
"On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates θ′ = F−1θ, simulate HMC in the (θ′,p) coordinates, and transform back to the original basis using F, this is exactly equivalent to noncanonical HMC with
A = [ 0 F −F> 0",A.2. Mass Preconditioning Proofs,[0],[0]
],A.2. Mass Preconditioning Proofs,[0],[0]
Proof.,A.2. Mass Preconditioning Proofs,[0],[0]
We first prove the equivalence regarding the change of basis in momentum space.,A.2. Mass Preconditioning Proofs,[0],[0]
"Under the M mass matrix variant of HMC, p is drawn from a N (0,M) distribution, and the dynamics of θ,p are then given by
d
dt",A.2. Mass Preconditioning Proofs,[0],[0]
[ θ p ] = [ M−1p −∇θU(θ) ],A.2. Mass Preconditioning Proofs,[0],[0]
"Denoting the upper-triangular Cholesky factor of M−1 by M−1/2, and introducing a new variable p′ = M−1/2p, we obtain the following dynamics for the joint variable (θ,p′):
d
dt",A.2. Mass Preconditioning Proofs,[0],[0]
[ θ p′ ] = d dt [ θ M−1/2p ] =,A.2. Mass Preconditioning Proofs,[0],[0]
[ (M−1/2)>p′ −M−1/2∇θU(θ) ],A.2. Mass Preconditioning Proofs,[0],[0]
= [ 0 (M−1/2)> −M−1/2 0 ],A.2. Mass Preconditioning Proofs,[0],[0]
[ ∇θU(θ) p′ ],A.2. Mass Preconditioning Proofs,[0],[0]
"Further, note that if the marginal distribution of p is N (0,M), then under this change of variables p′ has the marginal distributionN (0, I).",A.2. Mass Preconditioning Proofs,[0],[0]
"Thus, simulating canonical HMC with a non-identity mass matrix is equivalent to making a change of basis in momentum space, simulating non-canonical HMC with a particular choice of non-canonical A matrix, and finally reverting back to the original basis.
",A.2. Mass Preconditioning Proofs,[0],[0]
"We now prove the equivalence regarding the change of basis in θ space, which follows similarly.",A.2. Mass Preconditioning Proofs,[0],[0]
"Consider non-canonical HMC on the state-momentum pair (θ,p), with the antisymmetric matrix A taking the particular form
A = ( 0 F −F> 0 )",A.2. Mass Preconditioning Proofs,[0],[0]
"The states θ,p obtained from this algorithm are equal to those obtained by first changing basis to θ′",A.2. Mass Preconditioning Proofs,[0],[0]
"= F−1θ, then simulating standard HMC dynamics for the pair (θ′,p) with respect to the Hamiltonian
H ′(θ′,p)",A.2. Mass Preconditioning Proofs,[0],[0]
"= U ′(θ′) + 1
2 p>p
= U(Fθ) + 1
2 p>p
and then reverting to the original basis as θ = Fθ′. To see this, first note that if we denote the distribution on the coordinates θ corresponding to the potential U by π(θ) = e−U(θ), then the corresponding distribution on the coordinates θ′ is given by π′, where
π′(θ′) = det(F)π(Fθ′)
",A.2. Mass Preconditioning Proofs,[0],[0]
"The corresponding potential U ′ is therefore given by
U ′(θ′) = U(Fθ′)
",A.2. Mass Preconditioning Proofs,[0],[0]
Running canonical HMC dynamics targeting the Hamiltonian H ′,A.2. Mass Preconditioning Proofs,[0],[0]
"yields the dynamics:
d
dt
[ θ′
p
] =",A.2. Mass Preconditioning Proofs,[0],[0]
"[ p
−∇θ′U(θ′) ]",A.2. Mass Preconditioning Proofs,[0],[0]
"But note then that the dynamics of the original coordinates are then given by:
d
dt [ θ p ] = d dt [ Fθ′ p ] =",A.2. Mass Preconditioning Proofs,[0],[0]
[ Fp −∇θ′U(θ′) ] =,A.2. Mass Preconditioning Proofs,[0],[0]
[ Fp −∇θ′U(Fθ) ] =,A.2. Mass Preconditioning Proofs,[0],[0]
[ Fp −(F>)∇θU(θ) ] = [ 0 F −F> 0 ],A.2. Mass Preconditioning Proofs,[0],[0]
"[ ∇θU(θ) p
] which are exactly a special case of non-canonical HMC dynamics described above.",A.2. Mass Preconditioning Proofs,[0],[0]
Here we provide proofs related to the dynamics of magnetic HMC and it’s symplectic integration scheme.,B. Magnetic HMC (MHMC),[0],[0]
"We first establish the connection between the particular subcase of non-canonical Hamiltonian dynamics where
A = [ 0 I −I G ]
and Newton’s law for a charged particle coupled to a magnetic field.
",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Lemma 4.,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"In 3-dimensions the non-canonical Hamiltonian dynamics, with Hamiltonian H(θ,p) = U(θ) + 12p >p, correspond to simulating the differential equations:
d
dt",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ θ p ] = [ 0 I −I G ] ︸ ︷︷ ︸
A
[ ∇θH ∇pH ] ≡",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ 0 I −I G ] ︸ ︷︷ ︸
A
[ ∇θU(θ)
p
] (27)
where
G =  0 −b3 b2b3 0 −b1 −b2 b1 0  are equivalent to the Newtonian mechanics of a charged particle (with unit mass and charge) coupled to a magnetic field
~B = b1b2 b3  which take the form: d2θ
dt2 = −∇θU(θ)",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"+
dθ dt × ~B (28)
where θ is simply a 3-dimensional vector and × the cross-product.
",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Proof.,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"If we let θ and p denote our position and momentum coordinates in 3 dimensions then Newton’s law for a charged particle in a magnetic field (with m = q = 1) is:
d2θ dt2 = −∇θU(θ) +",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
dθ dt × ~B,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"(29)
Defining momentum canonically as dθdt = p we have:
d
dt [ θ p ] =",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ p
−∇θU(θ) +",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"dθdt × ~B
] =",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ p
−∇θU(θ)",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
+,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"Gp
] ≡",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[
0 I −I G ] ︸ ︷︷ ︸
A
[ ∇θU(θ)
p
] (30)
We now show that the dynamics used in magnetic HMC cannot be reproduced by simply choosing a different smooth Hamiltonian, H ′(θ,p) and using the canonical A matrix:
A = [ 0 I −I 0 ] to generate the dynamics.
",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Lemma 5.,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"The non-canonical Hamiltonian dynamics with magnetic A and HamiltonianH(θ,p) =",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"U(θ)+ 12p >p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian.
",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Proof.,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"Consider the ODEs corresponding to non-canonical dynamics with magnetic A and H(θ,p) = U(θ) + 12p",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
">p:
d
dt",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
[ θ p ] = [ 0 I −I G ],B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
[ ∇θU(θ) p ] =,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
[ p −∇θU(θ) +,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Gp ] .,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"(31)
Assume, to obtain a contradiction, that these canonical Hamiltonian dynamics can be reproduced for some choice of smooth H ′(θ,p) and canonical A matrix (i.e. E = G = 0, F = I):
d
dt",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
[ θ p ] = [ 0 I −I 0 ],B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ ∇θH ′(θ,p) ∇pH ′(θ,p) ]",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
= [ p −∇θU(θ) +,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
Gp ] .,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"(32)
",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"This implies: [ ∇pH ′(θ,p) ∇θH ′(θ,p) ] =",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
[ p ∇θU(θ)−Gp ] =⇒,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"[ ∇θ∇pH ′(θ,p) ∇p∇θH ′(θ,p) ]",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
= [ 0 −G ] .,B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"(33)
However, as long as the 2nd-order mixed partial derivatives are continuous they must be equal; so the conclusion follows.",B.1. Non-Canonical Dynamics and Magnetism,[0],[0]
"We begin by considering the symmetric splitting:
H(θ,p) = U(θ)/2︸ ︷︷ ︸ H1(θ) + pTp/2︸ ︷︷ ︸ H2(p) +U(θ)/2︸ ︷︷ ︸ H1(θ)
(34)
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The corresponding non-canonical dynamics for the sub-Hamiltonians H1(θ) and H2(p) are:
d
dt",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ θ p ] =,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[ E F −F> G ] ︸ ︷︷ ︸
A
[ ∇θU(θ)/2
0
] =",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ E∇θU(θ)/2 −F>∇θU(θ)/2 ],B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"(35)
and
d
dt [ θ p ] =",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[ E F −F> G ] ︸ ︷︷ ︸
A
[ 0 p ] =",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ Fp Gp ] .,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"(36)
We denote the corresponding flows by ΦA ,H1(θ) and Φ A ,H2(p) respectively.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The flow in (35) is generally not explicitly tractable unless we take E = 0 – in which case it is solved by an Euler translation as for standard Hamiltonian dynamics.
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Crucially, the flow in (36) is a linear differential equation and hence analytically integrable.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Without loss of generality, we restrict ourselves to the case F = I (the case for general F follows similarly).",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The dynamics associated with the flow H2(p) introduced in Lemma 4 become
d
dt [ θ(t) p(t) ]",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"= [ p(t) Gp(t) ] with initial condition (θ0,p0).",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Using the power series representation of the matrix exponential, the second differential equation for p may be integrated analytically to yield the following flow in p-space:
p(t)",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"= exp(Gt)p0
Substituting this result into the differential equation for θ yields
dθ dt = exp(Gt)p0
If G is invertible then once again using the power series representation of the matrix exponential and rearranging yields the solution
Φ ,H2(p)",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ θ p ] =,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ θ + G−1(exp(G ),B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"− I)p exp(G )p ]
If G is not invertible, then slightly more care must be taken to first diagonalize G and separate its invertible/singular components.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
Since G is strictly antisymmetric it can be written as iH where H is a Hermitian matrix.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Thus it can be diagonalized over C as:
G = [ UΛ U0 ]",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[Λ 0 0 0 ],B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[ U>Λ U>0 ]
where Λ is a diagonal submatrix consisting of the nonzero eigenvalues of G. [ UΛ U0 ] and [ U>Λ U>0 ] are unitary matrices where the columns of UΛ are the eigenvectors of G corresponding to its nonzero eigenvalues while the columns of U0 are the eigenvectors of G corresponding to its zero eigenvalues.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Even if G is not invertible we still have:
p(t)",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"= exp(Gt)p0
However it is more convenient to represent the matrix exponential as:
exp(Gt) =",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[ UΛ U0 ],B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[exp(Λt) 0 0 I ],B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[ U>Λ U>0 ] Substituting this result into the differential equation for θ, this representation of exp(Gt) implies the non-identity block can be handled as in the invertible case while the I block follows trivially to give:
θ(t) = θ0 + [ UΛ U0 ]",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
[Λ−1(exp(Λt)− I) 0 0 tI ],B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[ U>Λ U>0 ] p0
Note that if G = 0 then the flow map will simply reduce to an Euler translation as in ordinary HMC.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"We can also combine the ideas of Section A.2 to obtain a preconditioned, magnetic HMC algorithm corresponding to a general A-matrix of the form (
0 F −F> G
)
Dealing with a non-zero E becomes more subtle, since the corresponding sub-Hamiltonian is no longer exactly integrable under the splitting construction.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"In order to exactly integrate this sub-block a more costly implicit integrator is needed.
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
B.3.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Integration Error of Magnetic HMC
Since we are using a symmetric, leapfrog splitting scheme for magnetic HMC that exactly integrates each sub-Hamiltonian we obtain identical error scaling to the leapfrog integrator applied to canonical HMC.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Indeed, symplectic integrators are well-known to have many nice error properties in general and so perhaps this result is not so surprising (Hairer et al., 2006).
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
Lemma 6.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The symplectic leapfrog-like integrator for magnetic HMC will have the same local (∼ O( 3)) and global (∼ O( 2)) error scaling (over τ ∼ L steps), as the canonical leapfrog integrator of standard HMC if the Hamiltonian is separable.
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
Proof.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
Note that for the parametrization of A corresponding to magnetic HMC the Hamiltonian vector field ~H = ∇pH∇θ + (−∇θ + G∇pH)∇p ≡ ~A,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
+ ~B will generate the exact flow corresponding to exactly simulating the dynamics.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
We obtain an O( 3) local error by simply exploiting the separability of the Hamiltonian.,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The leapfrog integration scheme splits the Hamiltonian as: H(θ,p) = H1(θ) +H2(p) +H1(θ) and exactly integrates each sub-Hamiltonian so:
Φfrog ,H = Φ ,H1(θ) ◦",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Φ ,H2(p) ◦",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Φ ,H1(θ) = exp ( 2 ~B ) ◦ exp ( ~A ) ◦",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"exp ( 2 ~B )
(37)
(38)
",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"Via repeated applications of the Baker-Campbell-Hausdorff formula (Hairer et al., 2006) obtain:
exp ( 2 ~B ) ◦",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"exp ( ~A + 2 ~B + 2 2 [~A, ~B] )",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
+O( 3) =,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"(39)
exp
(
2 ~B + ~A",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"+ 2 ~B +
2
2",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"[~A, ~B] +
1 2 [ 2 ~B, ~A + 2 ~B +
2
2 [~A, ~B]]
)",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
+O( 3) =,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"(40)
exp ( ~H + 2
4 [~A, ~B] +
2
4 [~B, ~A] +
2
8 [~B, ~B]
)",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
+O( 3) = exp ( ~H ),B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"+O( 3) (41)
where we have used the antisymmetry of the commutator.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"The global error scaling, for an integration time of τ = L follows straightforwardly:
Φfrogτ,H = ( exp ( 2 ~B ) ◦",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
exp ( ~A ) ◦,B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
exp ( 2 ~B )),B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"L
(42) = ( exp ( ~H )",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
+O( 3) ),B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"L (43)
= exp ( L~H ) + L O( 2) (44)
= exp ( τ ~H )",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"+ τO( 2) (45)
= exp ( τ ~H )",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
"+O( 2) (46)
as desired.",B.2. Symplectic Integrator for Magnetic Dynamics,[0],[0]
Here we provide relevant experimental details for some of the Experiments presented in the main text.,C. Section 6 Experimental Details,[0],[0]
"In both experiments the reported autocorrelation measures are averaged over all coordinates as well as over 100 independent runs of the HMC/MHMC chains.
",C.1. Gaussians,[0],[0]
C.1.1.,C.1. Gaussians,[0],[0]
"2D GAUSSIAN
",C.1. Gaussians,[0],[0]
"For the uncorrelated, ill-conditioned 2D Gaussian experiment presented in the main text the magnetic G component only has one non-zero parameter which was set to g = .2.
",C.1. Gaussians,[0],[0]
C.1.2. 10D,C.1. Gaussians,[0],[0]
"GAUSSIAN
For the uncorrelated, ill-conditioned 10D Gaussian experiment presented in the main text, the G matrix was set to encourage the flow of momentum between the directions of large marginal variance with covariance eigenvalues 106 and the remaining 8 directions of directions of small marginal variance with covariance eigenvalues of 1.",C.1. Gaussians,[0],[0]
"We denote the directions of large marginal variance as x1, x2, and the other 8 directions of directions of small marginal variance as xi. G was set such that G1i = G2i = g, Gi1 = Gi2 = −g and G12 = G21 = 0 for g = .2.",C.1. Gaussians,[0],[0]
"The superior mixing of MHMC relative to HMC in this example holds true for a wide range of ( , L) settings as we can see by looking at the maximum mean discrepancy as a function of the number of samples in both Figures 5 and 6.
",C.2. Mixture of Gaussians,[0],[0]
"We found that tuning the parameters ( , L) via Bayesian optimization often resulted in worse performance for ordinary HMC since the values found for ( , L) were too conservative to encourage exploration between both modes.",C.2. Mixture of Gaussians,[0],[0]
"Moreover, more aggressive choices for ( , L) for ordinary HMC led to a sharp drop in acceptance rate and significantly worse performance.",C.2. Mixture of Gaussians,[0],[0]
"In this additional experiment, we consider the Gaussian funnel of (Neal, 2003) with density
p(x, v) = Πni=1N (xi|0, e−v)N (v|0, 32)
in 10+1 dimensions (i.e. n = 10).",C.3. Gaussian Funnel,[0],[0]
"This density illustrates the pathological correlation present in many hierarchical models between x, a vector of low-level parameters, and v, a hyperparameter controlling their variability.",C.3. Gaussian Funnel,[0],[0]
"As noted in (Betancourt & Girolami, 2015; Zhang & Sutton, 2014), Riemannian HMC methods, which incorporate local curvature information of the target, are well-suited to this problem as they help the dynamics traverse the energy surface which rapidly changes as a v varies.",C.3. Gaussian Funnel,[0],[0]
"HMC (as well as MHMC) do not exploit curvature information and will have more difficulty exploring the v direction due to the rapid variation in density – see (Betancourt & Girolami, 2015) for a detailed discussion.",C.3. Gaussian Funnel,[0],[0]
"Despite this difficulty, we might intuitively expect that introducing a “curl” term into the entries of G which couple each xi and v could increase exploration of the dynamics since these variables are nonlinearly correlated.",C.3. Gaussian Funnel,[0],[0]
"In order to encourage the periodic flow of momentum between the marginal direction v and the coordinates xi the G matrix was set such that Gvi = g, Giv = −g, Gij = 0 with g = .2.",C.3. Gaussian Funnel,[0],[0]
"To investigate this, we generated 10000 samples from both HMC and MHMC, discarding 1000 burn-in samples and computed the minimum effective sample size across x and v and bias in the moments of the v parameter similar to the set-up in (Zhang & Sutton, 2014) for various , L (see Table 5).",C.3. Gaussian Funnel,[0],[0]
"We report results averaged over 100 different runs of the Markov chains.
",C.3. Gaussian Funnel,[0],[0]
"We find that adding the magnetic field component decreases the bias in the moments and marginally increases the ESS,
although both samplers struggle to explore the full target density, as the relatively low ESS figures indicate.",C.3. Gaussian Funnel,[0],[0]
"Further details and experiments are provided in the Appendix.
",C.3. Gaussian Funnel,[0],[0]
"Recall the density of the Gaussian funnel p(x, v) = Πni=1N",C.3. Gaussian Funnel,[0],[0]
"(xi|0, e−v)N (v|0, 32).",C.3. Gaussian Funnel,[0],[0]
"In order to encourage the periodic flow of momentum between the marginal direction v and the coordinates xi the G matrix was set such that Gvi = g, Giv = −g, Gij = 0 with g = .2.",C.3. Gaussian Funnel,[0],[0]
Moreover the reported results were averaged over 100 different runs of the Markov chains.,C.3. Gaussian Funnel,[0],[0]
"In this section, we provide illustrations of the proposal distributions of MHMC in simple low-dimensional settings, to aid intuition and demonstrate the divergence of its behaviour from standard HMC.",D. MHMC Proposals and Dynamics,[0],[0]
"We first consider the case of an isotropic Gaussian target, and illustrate the proposal distribution of standard HMC, as well as MHMC with a variety of settings for the skew-symmetric matrix A = (
E F F G
) - see Figure 7.",D.1. Gaussian Densities,[0],[0]
"As in previous sections,
we denote the off-diagonal element of the G matrix by g. We also provide proposal plots for an anisotropic Gaussian target distribution - see Figure 8.",D.1. Gaussian Densities,[0],[0]
"We also provide proposal illustrations for the banana density of (Haario et al., 1999), as shown in Figure 9.",D.2. Banana Density,[0],[0]
Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC).,abstractText,[0],[0]
"In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics.",abstractText,[0],[0]
"We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field.",abstractText,[0],[0]
"We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC.",abstractText,[0],[0]
"Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.",abstractText,[0],[0]
Magnetic Hamiltonian Monte Carlo,title,[0],[0]
"√ T .
It is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings. In a COLT 2017 open problem (Agarwal et al., 2017), Agarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit problem. In the present paper, we resolve this open problem by presenting a new strategy based on augmenting the policy space.1",text,[0],[0]
The contextual bandit problem is an influential extension of the classical multi-armed bandit.,1 Introduction,[0],[0]
It can be described as follows.,1 Introduction,[0],[0]
"Let K be the number of actions, E a set of experts (or “policies”), T the time horizon, and denote ∆K = {x ∈",1 Introduction,[0],[0]
"[0, 1]K : ∑K i=1 x(i) = 1}.",1 Introduction,[0],[0]
"At each time step t = 1, . . .",1 Introduction,[0],[0]
", T ,
• The player receives from each expert e ∈ E an “advice” ξet ∈ ∆K .
",1 Introduction,[0],[0]
"• Using advices and previous feedbacks, the player selects a probability distribution pt ∈ ∆K .
",1 Introduction,[0],[0]
*Equal contribution 1Microsoft Research AI 2Princeton University.,1 Introduction,[0],[0]
"Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Sébastien Bubeck <sebubeck@microsoft.com>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1 Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1 Introduction,[0],[0]
1The full version of this paper can be found at https:// arxiv.org/abs/1802.03386.,1 Introduction,[0],[0]
"The work was done when Yuanzhi Li was a summer intern at Microsoft Research in 2017.
",1 Introduction,[0],[0]
• The adversary selects a loss function `t :,1 Introduction,[0],[0]
"[K]→ [0, 1].",1 Introduction,[0],[0]
• The player plays an action at ∈,1 Introduction,[0],[0]
"[K] at random from pt
(and independently of the past).
",1 Introduction,[0],[0]
• The player’s suffered loss is `t(at) ∈,1 Introduction,[0],[0]
"[0, 1], which is also the only feedback the player receives about the loss function `t.
The player’s performance at the end of the T rounds is measured through the regret with respect to the best expert:
",1 Introduction,[0],[0]
"RT def = max
e∈E { E",1 Introduction,[0],[0]
"[ T∑ t=1 `t(at)− 〈ξet , `t〉 ]}
= max e∈E { E",1 Introduction,[0],[0]
"[ T∑ t=1 〈pt − ξet , `t〉 ]} .",1 Introduction,[0],[0]
"(1.1)
",1 Introduction,[0],[0]
A landmark result by Auer et al. (2002) is that a regret of order O( √ TK log(|E|)) is achievable in this setting.,1 Introduction,[0],[0]
The general intuition captured by regret bounds is that the player’s performance is equal to the best expert’s performance up to a term of lower order.,1 Introduction,[0],[0]
However the aforementioned bound might fail to capture this intuition if T L∗T def,1 Introduction,[0],[0]
"= mine∈E E ∑T t=1〈ξet , `t〉.",1 Introduction,[0],[0]
It is thus natural to ask whether one could obtain a stronger guarantee where T is essentially replaced by L∗T .,1 Introduction,[0],[0]
"This question was posed as a COLT 2017 open problem (Agarwal et al., 2017).",1 Introduction,[0],[0]
"Such bounds are called first-order regret bounds, and they are known to be possible with full information (Auer et al., 2002), as well as in the multi-armed bandit setting (Allenberg et al., 2006) (see also (Foster et al., 2016) for a different proof) and the semi-bandit framework (Neu, 2015; Lykouris et al., 2017).",1 Introduction,[0],[0]
"Our main contribution is a new algorithm for contextual bandit, which we call MYGA (see Section 2), and for which we prove the following first-order regret bound, thus resolving the open problem.",1 Introduction,[0],[0]
Theorem 1.1.,1 Introduction,[0],[0]
"For any loss sequence such that mine∈E E ∑T t=1〈ξet , `t〉 ≤ L∗ one has that MYGA
with γ = Θ(η) and η = Θ ( min { 1 K , √ log(|E|+T ) KL∗ })",1 Introduction,[0],[0]
"satisfies
RT",1 Introduction,[0],[0]
≤ O (√ K log(|E|+ T )L∗,1 Introduction,[0],[0]
+K log(|E|+ T ) ) .,1 Introduction,[0],[0]
In this section we describe the MYGA algorithm.,2 Algorithm Description,[0],[0]
We introduce a truncation operator T ks that takes as input an index k ∈,2.1 Truncation,[0],[0]
[K] and a threshold s ∈,2.1 Truncation,[0],[0]
"[0, 12 ].",2.1 Truncation,[0],[0]
"Then, treating the first k arms as “majority arms” and the last K − k arms as “minority arms,” T ks redistributes “multiplicatively” the probability mass of all minority arms below threshold s to the majority arms.
",2.1 Truncation,[0],[0]
Definition 2.1.,2.1 Truncation,[0],[0]
For k ∈,2.1 Truncation,[0],[0]
"[K] and s ∈ (0, 12 ], the truncation operator T ks : ∆K → ∆K is defined as follows.",2.1 Truncation,[0],[0]
"Given any q ∈ ∆K , then we set T ks q(i) = 0, i > k and q(i) ≤ s; q(i), i > k and q(i) > s; q(i) · ( 1 + ∑ j:j>k∧ q(j)≤s q(j)∑
j≤k q(j)
) , i ≤ k.
Equivalently one can define T ks q(i) for the majority arms",2.1 Truncation,[0],[0]
i ≤ k,2.1 Truncation,[0],[0]
"with the following implicit formula:
T ks q(i) = q(i)∑ j≤k q(j) ∑ j≤k T ks q(j) .",2.1 Truncation,[0],[0]
"(2.1)
To see this it suffices to note that the amount of mass in the majority arms is given by∑ j≤k T ks q(j) = 1− ∑ j>k T ks q(j) = 1− ∑ j:j>k∧ q(j)>s q(j)
= ∑ j≤k q(j) + ∑ j:j>k∧ q(j)≤s q(j) .
",2.1 Truncation,[0],[0]
"If K = 2, then T 1s q simply adds q(2) into q(1)",2.1 Truncation,[0],[0]
if q(2) ≤,2.1 Truncation,[0],[0]
s.,2.1 Truncation,[0],[0]
"For an example with K = 11, see Figure 1.",2.1 Truncation,[0],[0]
"MYGA is parameterized by two parameters: a classical learning rate η > 0, and a thresholding parameter γ ∈ 12T N = { 12T , 2 2T , 3 2T , . . .",2.2 Informal description,[0],[0]
}.,2.2 Informal description,[0],[0]
"Also let S = (γ, 1/2] ∩ 1 2T N =",2.2 Informal description,[0],[0]
"(γ, 1/2] ∩ { 12T , 2 2T , 3 2T , . . . }",2.2 Informal description,[0],[0]
"At a high level, a key feature of MYGA is to introduce a set of auxiliary experts, one for each s ∈ S. More precisely, in each round t, after receiving expert advices {ξet }e∈E ,
MYGA calculates a distribution ξst ∈ ∆K for each s ∈ S.",2.2 Informal description,[0],[0]
"Then, MYGA uses the standard exponential weight updates on E′ = E ∪ S with learning rate η > 0, to calculate a weight functionwt ∈ RE∪S+ —see (2.3).",2.2 Informal description,[0],[0]
"Then, it computes • ζt ∈ ∆K , the weighted average of expert advices in E:
ζt = 1∑
e∈E wt(e) ∑ e∈E wt(e) · ξet .
",2.2 Informal description,[0],[0]
"• qt ∈ ∆K , the weighted average of expert advices in E′:
qt = 1
‖wt‖1 ∑ e∈E′ wt(e) · ξet .
",2.2 Informal description,[0],[0]
"Using these information, MYGA calculates the probability distribution pt ∈ ∆K from which the arm is played at round t.
Let us now explain how pt and ξst , s ∈ S are defined.",2.2 Informal description,[0],[0]
"First we remark that in the contextual bandit setting, the arm index has no real meaning since in each round t we can permute the arms by some πt : [K] → [K] and permute the expert’s advices and the loss vector by the same πt.",2.2 Informal description,[0],[0]
"For this reason, throughout this paper, we shall assume
∀t ∈",2.2 Informal description,[0],[0]
[T ] : ζt(1) ≥ ζt(2) ≥ · · · ζt(K) .,2.2 Informal description,[0],[0]
Let us define the “pivot” index kt = min{i ∈,2.2 Informal description,[0],[0]
[K] :∑ j≤i ζt(j) ≥ 1/2}.,2.2 Informal description,[0],[0]
"Then, in order to perform truncation, MYGA views the first kt arms as “majority arms” and the last K − kt arms as “minority arms” of the current round t. At a high level we will have:
• the distribution to play from is pt = T ktγ qt.",2.2 Informal description,[0],[0]
"• each auxiliary expert s ∈ S is defined by ξst = T kts qt.
",2.2 Informal description,[0],[0]
We now give a more precise description in Algorithm 1.,2.2 Informal description,[0],[0]
Definition 3.1.,3 Preliminaries,[0],[0]
"For analysis purpose, let us define the truncated loss ¯̀t(i) def = `t(i)1{pt(i) > 0}, so that
Eat [ 〈˜̀t, pt〉] = 〈¯̀t, pt〉 = 〈`t, pt〉 .
",3 Preliminaries,[0],[0]
"We next derive two lemmas that will prove useful to isolate
Algorithm 1 MYGA (Make the minoritY Great Again)
",3 Preliminaries,[0],[0]
"Input: learning rate η > 0, threshold parameter γ ∈ 12T N 1: S ← (γ, 1/2] ∩ 12T N and w1 ← (1, . . .",3 Preliminaries,[0],[0]
", 1) ∈ R E∪S
2: for t = 1 to T do 3: receive advices ξet ∈ ∆K from each expert e ∈ E 4: weighted average ζt ← ∑ e∈E wt(e)ξ e t∑
e∈E wt(e) ∈ ∆K
5: assume ζt(1) ≥ ζt(2) ≥ · · · ζt(K) wlog.",3 Preliminaries,[0],[0]
by permuting the arms 6: kt ← min{i ∈,3 Preliminaries,[0],[0]
"[K] : ∑ j≤i ζt(j) ≥ 1/2} the first kt arms are majority arms 7: find qt ∈ ∆K such that qt can be found in time O(K|S|) = O(KT ), see Lemma 6.1
qt = 1∑ e∈E wt(e)+ ∑ s∈S wt(s) (∑ e∈E wt(e)ξ e t + ∑ s∈S wt(s)T kts qt ) .",3 Preliminaries,[0],[0]
"(2.2)
8: ξst ← T kts qt for every s ∈ S and pt ← T ktγ",3 Preliminaries,[0],[0]
qt 9: draw an arm at ∈,3 Preliminaries,[0],[0]
"[K] from probability distribution pt and receive feedback `t(at)
10: compute loss estimator ˜̀t ∈ RK+ as ˜̀t(i)",3 Preliminaries,[0],[0]
=,3 Preliminaries,[0],[0]
"`t(i)pt(i)1i=at 11: update the exponential weights for any e ∈ E ∪ S:
wt+1(e) =",3 Preliminaries,[0],[0]
"exp ( − η ∑t r=1〈ξer , ˜̀r〉) .",3 Preliminaries,[0],[0]
"(2.3)
12: end for
the properties of the truncation operator T ks that are needed to obtain a first-order regret bound.",3 Preliminaries,[0],[0]
Lemma 3.2.,3 Preliminaries,[0],[0]
Let γ ∈,3 Preliminaries,[0],[0]
"[0, 1] and assume",3 Preliminaries,[0],[0]
that for all i ∈,3 Preliminaries,[0],[0]
"[K], (1− cKγ)pt(i) ≤ qt(i) for some universal constant c > 0, and that pt(i) 6=",3 Preliminaries,[0],[0]
0⇒ pt(i) ≥ qt(i).,3 Preliminaries,[0],[0]
"Then one has
(1− cKγ)LT −L∗T ≤ log(|E′|) η + η 2 E T∑ t=1 ‖¯̀t‖22 .",3 Preliminaries,[0],[0]
"(3.1)
Proof.",3 Preliminaries,[0],[0]
"Using 〈pt, `t〉 = 〈pt, ¯̀t〉, 〈−ξet , `t〉 ≤ 〈−ξet , ¯̀t〉, and (1− cKγ)pt(i) ≤ qt(i), we have
(1− cKγ)LT",3 Preliminaries,[0],[0]
− L∗T ≤ max e∈E′ E T∑ t=1,3 Preliminaries,[0],[0]
"〈(1− cKγ)pt − ξet , ¯̀t〉
≤ max e∈E′ E T∑ t=1 〈qt − ξet , ¯̀t〉 .
",3 Preliminaries,[0],[0]
"The rest of the proof follows from standard argument to bound the regret of Exp4, see e.g., (Bubeck & CesaBianchi, 2012, Theorem 4.2) (with the minor modification that the assumption on pt implies that ˜̀t(i) ≤",3 Preliminaries,[0],[0]
"`t(i)qt(i)1{i = at}).
",3 Preliminaries,[0],[0]
The next lemma is straightforward.,3 Preliminaries,[0],[0]
Lemma 3.3.,3 Preliminaries,[0],[0]
"In addition to the assumptions in Lemma 3.2, assume that there exists some numerical constants c′, c′′ ≥ 0 such that
γ E T∑ t=1 ‖¯̀t‖22 ≤ 2 c′ (η + γ) K LT + 2 c′′ log(|E′|) η .
(3.2)
",3 Preliminaries,[0],[0]
"Then one has( 1− cKγ − ( η + η2
γ
) c′K) )",3 Preliminaries,[0],[0]
"(LT − L∗T )
≤",3 Preliminaries,[0],[0]
"( 1
η + c′′ γ
) log(|E′|)",3 Preliminaries,[0],[0]
"+ ( cKγ + ( η + η2
γ
) c′K )",3 Preliminaries,[0],[0]
"L∗T .
",3 Preliminaries,[0],[0]
"We now see that it suffices to show that MYGA satisfies the assumptions of Lemma 3.2 and Lemma 3.3 for γ ' η, and η ' min { 1 K , √ log(|E′|) KL∗T } (assume that L∗T is
known), in which case one obtains a bound of order√ K log(|E′|)L∗T",3 Preliminaries,[0],[0]
"+K log(|E′|).
",3 Preliminaries,[0],[0]
"In fact the assumption of Lemma 3.2 will be easily verified, and the real difficulty will be to prove (3.2).",3 Preliminaries,[0],[0]
"We observe that the standard trick of thresholding the arms with probability below γ would yield (3.2) with the right hand side replaced by LT , and in turn this leads to a regret of order (L∗T ) 2/3.",3 Preliminaries,[0],[0]
"Our goal is to improve over this naive argument.
",3 Preliminaries,[0],[0]
4 Proof of the 2-Armed Case The goal of this section is to explain how our MYGA algorithm arises naturally.,3 Preliminaries,[0],[0]
To focus on the main ideas we restrict to the case K = 2.,3 Preliminaries,[0],[0]
"The complete formal proof of Theorem 1.1 is given in Section 5.
",3 Preliminaries,[0],[0]
Recall we have assumed without loss of generality that ζt(1),3 Preliminaries,[0],[0]
≥ ζt(2) for each round t ∈,3 Preliminaries,[0],[0]
[T ].,3 Preliminaries,[0],[0]
This implies kt = 1 because ζt(1) ≥ 12 .,3 Preliminaries,[0],[0]
"In this simple case, for s ∈",3 Preliminaries,[0],[0]
"[0, 1/2], we abbreviate our truncation operator T kts as Ts, and it acts as
follows.",3 Preliminaries,[0],[0]
Given q ∈ ∆2 if q(2) ≤,3 Preliminaries,[0],[0]
s,3 Preliminaries,[0],[0]
"we have Tsq = (1, 0); and if q(2) > s we have Tsq = q.
",3 Preliminaries,[0],[0]
"In particular, we have qt(1) ≥ qt(2) and pt(1) ≥ pt(2) for all t ∈",3 Preliminaries,[0],[0]
[T ].,3 Preliminaries,[0],[0]
We refer to arm 1 as the majority arm and arm 2 as the minority arm.,3 Preliminaries,[0],[0]
"We denote M = E ∑T t=1 ¯̀ t(1) as
the loss of the majority arm and m = E ∑T t=1 ¯̀ t(2) as the loss of the minority arm.",3 Preliminaries,[0],[0]
Since `t ∈,3 Preliminaries,[0],[0]
"[0, 1]K and K = 2, we have E ∑T t=1",3 Preliminaries,[0],[0]
‖¯̀t‖22 ≤,3 Preliminaries,[0],[0]
E ∑T t=1 ¯̀ t(1) +,3 Preliminaries,[0],[0]
¯̀t(2),3 Preliminaries,[0],[0]
= M +m .,3 Preliminaries,[0],[0]
"(4.1) Observe also that one always has LT ≥ 12M (indeed pt(1) ≥ qt(1) ≥ 1/2), and thus the whole game to prove (3.2) is to upper bound the minority’s loss m.",3 Preliminaries,[0],[0]
Assume that m ≤ (c′ − 1)M for some constant c′ > 0.,4.1 When the minority suffers small loss,[0],[0]
"Then, because M ≤ 2LT , one can directly obtain (3.2) from (4.1) with c′′ = 0.",4.1 When the minority suffers small loss,[0],[0]
"In words, when the minority arm has a total loss comparable to the majority arm, simply playing from ζt would satisfy a first-order regret bound.
",4.1 When the minority suffers small loss,[0],[0]
Our main idea is to somehow enforce this relation m .M,4.1 When the minority suffers small loss,[0],[0]
"between the minority and majority losses, by “truncating” probabilities appropriately.",4.1 When the minority suffers small loss,[0],[0]
"Indeed, recall that if after some truncation we have pt(2) = 0, then it satisfies ¯̀t(2) = 0",4.1 When the minority suffers small loss,[0],[0]
so the minority loss m can be improved.,4.1 When the minority suffers small loss,[0],[0]
"Our key new insight is captured by the following lemma which is proved using an integral averaging argument.
",4.2 Make the minority great again,[0],[0]
Definition 4.1.,4.2 Make the minority great again,[0],[0]
"For each s ≥ γ, let Lst def = E ∑T t=1〈Tsqt, `t〉 be the expected loss if the truncated strategy Tsqt ∈ ∆K is played at each round.",4.2 Make the minority great again,[0],[0]
Lemma 4.2.,4.2 Make the minority great again,[0],[0]
"As long as m−M > 0,
∃s ∈ (γ, 1/2] : m−M ≤ LT − L s T
γ .
",4.2 Make the minority great again,[0],[0]
"In words, ifm is large, then smust be a much better threshold compared to γ, that is LT − LsT is large.
",4.2 Make the minority great again,[0],[0]
Proof of Lemma 4.2.,4.2 Make the minority great again,[0],[0]
"For any s ≥ γ, define the function f(s) def = E ∑T t=1 1{qt(2) ≤ s}(¯̀t(1)− ¯̀t(2)) .",4.2 Make the minority great again,[0],[0]
Let us pick s ∈,4.2 Make the minority great again,[0],[0]
"[γ, 1/2] to minimize f(s), and breaking ties by choosing the smaller value of s. We make several observations:
• f(γ) ≥ 0",4.2 Make the minority great again,[0],[0]
"because for any t with qt(2) ≤ γ we must have ¯̀t(2) = 0.
• f(1/2)",4.2 Make the minority great again,[0],[0]
= M −m < 0.,4.2 Make the minority great again,[0],[0]
• s > γ because f(s) ≤ f(1/2),4.2 Make the minority great again,[0],[0]
"< 0.
Let us define the points s0 def = γ and
{s1 < . . .",4.2 Make the minority great again,[0],[0]
"< sm} def = (γ, s] ∩ {q1(2), . . .",4.2 Make the minority great again,[0],[0]
", qT (2)}.
Note that the tie-breaking rule for the choice of s ensures sm = s (if sm < s then it must satisfy f(sm) = f(s) giving a contradiction).",4.2 Make the minority great again,[0],[0]
"Using the identity T∑ t=1 〈Tsqt − qt, ¯̀t〉 = 1{qt(2) ≤ s}qt(2)(¯̀t(1)− ¯̀t(2)) , (4.2) we calculate that
LT − LsT
= E T∑ t=1 〈Tγqt − Tsqt, `t〉 = E T∑ t=1 〈Tγqt − Tsqt, ¯̀t〉
= E T∑ t=1",4.2 Make the minority great again,[0],[0]
(1{qt(2) ≤,4.2 Make the minority great again,[0],[0]
"γ} − 1{qt(2) ≤ s})
",4.2 Make the minority great again,[0],[0]
"× qt(2)(¯̀t(1)− ¯̀t(2))
= E T∑ t=1",4.2 Make the minority great again,[0],[0]
m∑ i=1 −si1{qt(2) =,4.2 Make the minority great again,[0],[0]
"si}(¯̀t(1)− ¯̀t(2))
",4.2 Make the minority great again,[0],[0]
= m∑ i=1,4.2 Make the minority great again,[0],[0]
"si(f(si−1)− f(si))
",4.2 Make the minority great again,[0],[0]
= m−1∑ i=1,4.2 Make the minority great again,[0],[0]
"(si+1 − si)f(si) + s1f(s0)− smf(sm) .
",4.2 Make the minority great again,[0],[0]
"Since f(s0) ≥ 0, f(si) ≥ f(s) and s = sm, we conclude that
LT − LsT ≥ (sm − s1)f(sm)− smf(sm) = −s1f(sm) ≥ γ(m−M) .
",4.2 Make the minority great again,[0],[0]
"Given Lemma 4.2, a very intuitive strategy start to emerge.",4.2 Make the minority great again,[0],[0]
"Suppose we can somehow get an upper bound of the form
LT − LsT ≤",4.2 Make the minority great again,[0],[0]
O,4.2 Make the minority great again,[0],[0]
( log(|E′|) η + η(m+M) + γLT ) .,4.2 Make the minority great again,[0],[0]
"(4.3) Then, putting this into Lemma 4.2 and using M ≤ 2LT , we have for any γ ≥ 2η,
γm ≤",4.2 Make the minority great again,[0],[0]
"O ( log(|E′|) η + γLT ) .
",4.2 Make the minority great again,[0],[0]
"In words, the minority arm also suffers from a small loss (and thus is great again!)",4.2 Make the minority great again,[0],[0]
"Putting this into (4.1), we immediately get (3.2) as desired and finish the proof of Theorem 1.1 in the case K = 2.
",4.2 Make the minority great again,[0],[0]
"Thus, we are left with showing (4.3).",4.2 Make the minority great again,[0],[0]
The main idea is to add the truncated strategy Tsqt as an additional auxiliary expert.,4.2 Make the minority great again,[0],[0]
"If we can achieve this, then (4.3) can be obtained from the regret formula in Lemma 3.2.",4.2 Make the minority great again,[0],[0]
Assume for a moment that we somehow expand the set of experts into E′ ⊃,4.3 Expanding the set of experts,[0],[0]
E so that: ∀s ∈,4.3 Expanding the set of experts,[0],[0]
"(γ, 1/2],∃e ∈ E′ such that for all t ∈",4.3 Expanding the set of experts,[0],[0]
"[T ], ξet = Tsqt .",4.3 Expanding the set of experts,[0],[0]
"(4.4) Then clearly (4.3) would be satisfied using Lemma 3.2, (4.1) and",4.3 Expanding the set of experts,[0],[0]
"L∗T ≤ LsT (the loss of an expert should be no
better than the loss of the best expert L∗T ).
",4.3 Expanding the set of experts,[0],[0]
"There are two issues with condition (4.4): first, it selfreferential, in the sense that it assumes {ξet }e∈E′ satisfies a certain form depending on qt while qt is defined via {ξet }e∈E′ (recall (2.2)); and second, it potentially requires to have an infinite number of experts (one for each s ∈ (γ, 1/2]).",4.3 Expanding the set of experts,[0],[0]
"Let us first deal with the second issue via discretization.
",4.3 Expanding the set of experts,[0],[0]
Lemma 4.3.,4.3 Expanding the set of experts,[0],[0]
"In the same setting as Lemma 4.2, there exists s ∈ S def=",4.3 Expanding the set of experts,[0],[0]
"(γ, 1/2] ∩ 12T N such that
m−M ≤ 1 + LT − L s T
γ .
",4.3 Expanding the set of experts,[0],[0]
Proof.,4.3 Expanding the set of experts,[0],[0]
"For x ∈ R let x be the smallest element in [x,+∞) ∩ 12T N.",4.3 Expanding the set of experts,[0],[0]
For any s ∈ S we can rewrite (4.2) as (note that x ≤,4.3 Expanding the set of experts,[0],[0]
"s⇔ x ≤ s) 〈Tsqt− qt, ¯̀t〉 = 1{qt(2) ≤ s}qt(2)(¯̀t(1)− ¯̀t(2))",4.3 Expanding the set of experts,[0],[0]
"+ εt,s , where |εt,s| ≤ 1/2T .",4.3 Expanding the set of experts,[0],[0]
"Using the same proof of Lemma 4.2, and redefining
f(s) def = E ∑T t=1 1{qt(2) ≤ s}(¯̀t(1)− ¯̀t(2)) .
",4.3 Expanding the set of experts,[0],[0]
"we get that there exists s1, . . .",4.3 Expanding the set of experts,[0],[0]
", sm ∈ S def =",4.3 Expanding the set of experts,[0],[0]
"(γ, 12 ] ∩ 1 2T N and ε ∈",4.3 Expanding the set of experts,[0],[0]
"[−1, 1] such that
LT − LsT = ε+ m∑ i=1 si(f(si−1)− f(si)) .
",4.3 Expanding the set of experts,[0],[0]
"The rest of the proof now follows from the same proof of Lemma 4.2, except that we minimize f(s) over s ∈ S instead of s ∈",4.3 Expanding the set of experts,[0],[0]
"[γ, 12 ].
Thus, instead of (4.4), we only need to require
∀s ∈ S, ∃e ∈ E′ such that for all t ∈",4.3 Expanding the set of experts,[0],[0]
"[T ], ξet = Tsqt .",4.3 Expanding the set of experts,[0],[0]
"(4.5)
We now resolve the self-referentiality of (4.5) by defining simultaneously qt and ξet , e ∈ S as follows.",4.3 Expanding the set of experts,[0],[0]
"Consider the map Ft : [0, 1/2]→",4.3 Expanding the set of experts,[0],[0]
"[0, 1/2] defined by:
Ft(x)",4.3 Expanding the set of experts,[0],[0]
"= 1∑ e∈E wt(e) + ∑ s∈S wt(s)
× (∑ e∈E wt(e)ξ e t (2) + ∑ s∈S",4.3 Expanding the set of experts,[0],[0]
"wt(s)x1{x > s} ) .
",4.3 Expanding the set of experts,[0],[0]
"It suffices to find a fixed point x = Ft(x): indeed, setting qt def = (1− x, x) and
ξst (2) def = x1{x > s} = Tsqt for s ∈ S, we have both (4.5) holds and qt = 1‖wt‖1 ∑ e∈E′ wt(e) · ξet is the correct weighted average of expert advices in E′ = E ∪ S Finally, Ft has a fixed point since it is a nondecreasing function from a closed interval to itself.",4.3 Expanding the set of experts,[0],[0]
"It is also not hard to find such a point algorithmically.
",4.3 Expanding the set of experts,[0],[0]
This concludes the (slightly informal) proof forK = 2.,4.3 Expanding the set of experts,[0],[0]
We give the complete proof for arbitrary K in the next section.,4.3 Expanding the set of experts,[0],[0]
"In this section, we assume qt ∈ ∆K satisfies (2.2) and we defer the constructive proof of finding qt to Section 6.",5 Proof of Theorem 1.1,[0],[0]
"Recall the arm index has no real meaning so without loss of generality we have permuted the arms so that
ζt(1) ≥ ζt(2) ≤ . . .",5 Proof of Theorem 1.1,[0],[0]
≥ ζt(K),5 Proof of Theorem 1.1,[0],[0]
"for each t = 1, 2, . . .",5 Proof of Theorem 1.1,[0],[0]
", T .",5 Proof of Theorem 1.1,[0],[0]
"We refer to {1, 2, . . .",5 Proof of Theorem 1.1,[0],[0]
", kt} the set of majority arms and {kt+1, . . .",5 Proof of Theorem 1.1,[0],[0]
",K} the set of minority arms at round t.2",5 Proof of Theorem 1.1,[0],[0]
We let M def = ∑T t=1,5 Proof of Theorem 1.1,[0],[0]
E ∑ i≤kt ¯̀ t(i) and m def = ∑T t=1,5 Proof of Theorem 1.1,[0],[0]
E ∑,5 Proof of Theorem 1.1,[0],[0]
i>kt ¯̀ t(i) respectively be the total loss of the majority and minority arms.,5 Proof of Theorem 1.1,[0],[0]
"We again have
E ∑T t=1 ‖¯̀t‖22 ≤",5 Proof of Theorem 1.1,[0],[0]
E ∑T t=1 ∑ i∈[K] ¯̀ t(i) = M +m .,5 Proof of Theorem 1.1,[0],[0]
"(5.1)
",5 Proof of Theorem 1.1,[0],[0]
"Thus, the whole game to prove (3.2) is to upper bound M and m.",5 Proof of Theorem 1.1,[0],[0]
"We state a few properties about qt and its truncations.
",5.1 Useful properties,[0],[0]
Lemma 5.1.,5.1 Useful properties,[0],[0]
"In each round t = 1, 2, . . .",5.1 Useful properties,[0],[0]
", T , if qt satisfies (2.2), then for every s ∈ S and i ≤ kt:
ξst (i) = ζt(i)∑",5.1 Useful properties,[0],[0]
"j≤k ζt(j)
· ( 1− ∑ j>k ξst (j) )
",5.1 Useful properties,[0],[0]
Proof.,5.1 Useful properties,[0],[0]
Let i ≤ kt,5.1 Useful properties,[0],[0]
and,5.1 Useful properties,[0],[0]
s ∈ S.,5.1 Useful properties,[0],[0]
"By (2.1) and since ξst = T kts qt one has
ξst (i) = qt(i)∑ j≤k qt(j) ∑ j≤k ξst (j) .
",5.1 Useful properties,[0],[0]
"Moreover qt is a mixture of ζt and truncated versions of ζt so similarly using (2.1) one has
qt(i) = ζt(i)∑ j≤k ζt(j) ∑ j≤k qt(j) .
",5.1 Useful properties,[0],[0]
"Putting the two above displays together concludes the proof.
",5.1 Useful properties,[0],[0]
Lemma 5.2.,5.1 Useful properties,[0],[0]
"In each round t = 1, 2, . . .",5.1 Useful properties,[0],[0]
", T , if qt satisfies (2.2), then
• for every i > kt",5.1 Useful properties,[0],[0]
"it satisfies qt(i) ≤ ζt(i), and • for every i ≤",5.1 Useful properties,[0],[0]
kt,5.1 Useful properties,[0],[0]
"it satisfies qt(i) ≥ ζt(i) ≥ 12K .
",5.1 Useful properties,[0],[0]
Proof.,5.1 Useful properties,[0],[0]
For sake of notation we drop the index t in this proof.,5.1 Useful properties,[0],[0]
Recall q = ∑ e∈E∪S w(e) ‖w‖1 · ξ,5.1 Useful properties,[0],[0]
"e.
• For every minority arm i > k, every s ∈ S, we have ξs(i) =",5.1 Useful properties,[0],[0]
( T ks q ) (i) ≤ q(i),5.1 Useful properties,[0],[0]
"according to Definition 2.1.
2We stress that in the K-arm setting, although kt is the minimum index such that ζt(1) + · · ·+ ζt(kt) ≥ 12 , it may not be the minimum index so that qt(1) + · · ·+ qt(kt) ≥ 12 .
",5.1 Useful properties,[0],[0]
"Therefore, we must have q(i) = ∑ e∈E∪S w(e) ‖w‖1 ·
ξe(i) ≤ ∑ e∈E w(e)ξ
e(i)∑ e∈E w(e) = ζ(i).
",5.1 Useful properties,[0],[0]
• For every majority arm i ≤,5.1 Useful properties,[0],[0]
"k, we have (using Lemma 5.1)
ξe(i) = ζ(i)∑",5.1 Useful properties,[0],[0]
j≤k ζ(j) ·,5.1 Useful properties,[0],[0]
"(1− ∑ j>k ξs(j))
",5.1 Useful properties,[0],[0]
"≥ ζ(i)∑ j≤k ζ(j) · (1− ∑ j>k ζ(j)) = ζ(i)
",5.1 Useful properties,[0],[0]
From the definition of k = min{i ∈,5.1 Useful properties,[0],[0]
"[K] : ∑ j≤i ζ(j) ≥ 1 2}, we can also conclude ζ(i) ≥ ζ(k)",5.1 Useful properties,[0],[0]
≥ 12K .,5.1 Useful properties,[0],[0]
This is because 1 2 ≤ ∑,5.1 Useful properties,[0],[0]
"j>k ζ(j) ≤ Kζ(k).
",5.1 Useful properties,[0],[0]
The next lemma shows that setting pt = T ktγ,5.1 Useful properties,[0],[0]
"qt satisfies the assumption of Lemma 3.2.
",5.1 Useful properties,[0],[0]
Lemma 5.3.,5.1 Useful properties,[0],[0]
"If qt satisfies (2.2), γ ∈ (0, 12 ] and pt = T ktγ qt, then for every arm i ∈",5.1 Useful properties,[0],[0]
[K]: (1−2Kγ)pt(i) ≤ qt(i) and pt(i) 6=,5.1 Useful properties,[0],[0]
"0⇒ pt(i) ≥ qt(i) .
",5.1 Useful properties,[0],[0]
Proof.,5.1 Useful properties,[0],[0]
"For sake of notation we drop the index t in this proof.
",5.1 Useful properties,[0],[0]
"By Definition 2.1 and Lemma 5.2, we have for every i ∈",5.1 Useful properties,[0],[0]
"[K]:
p(i) ≤",5.1 Useful properties,[0],[0]
q(i),5.1 Useful properties,[0],[0]
"( 1 + ∑ j:j>k∧ q(j)≤γ q(j)∑
j≤k q(j) ) ≤",5.1 Useful properties,[0],[0]
"q(i) ( 1 +
∑ j:q(j)≤γ q(j)∑",5.1 Useful properties,[0],[0]
"j≤k ζ(j) ) ≤ q(i)(1 + 2Kγ) .
",5.1 Useful properties,[0],[0]
"The other statement follows because whenever p(i) 6= 0, Definition 2.1 says it must satisfy p(i) ≥ q(i).",5.1 Useful properties,[0],[0]
"We first upper bound M and then upper bound m.
Lemma 5.4.",5.2 Bounding m and M,[0],[0]
"If qt satisfies (2.2), then M ≤ 2KLT .
",5.2 Bounding m and M,[0],[0]
Proof.,5.2 Bounding m and M,[0],[0]
Using Lemma 5.2 we have qt(i) ≥ 12K for any i ≤ kt.,5.2 Bounding m and M,[0],[0]
"Also, pt(i) ≥ qt(i) for every i satisfying ¯̀t(i) > 0",5.2 Bounding m and M,[0],[0]
(owing to Definition 3.1 and Lemma 5.3).,5.2 Bounding m and M,[0],[0]
"Therefore,
M = T∑ t=1 E ∑ i≤kt ¯̀ t(i) ≤ 2K T∑ t=1",5.2 Bounding m and M,[0],[0]
"E ∑ i≤kt qt(i) · ¯̀t(i)
≤ 2K T∑ t=1 E ∑ i≤kt pt(i) · ¯̀t(i) ≤",5.2 Bounding m and M,[0],[0]
2K T∑ t=1,5.2 Bounding m and M,[0],[0]
"E〈pt, ¯̀t〉
= 2K T∑ t=1",5.2 Bounding m and M,[0],[0]
"E〈pt, `t〉 = 2KLT .
",5.2 Bounding m and M,[0],[0]
Lemma 5.5.,5.2 Bounding m and M,[0],[0]
"Suppose qt satisfies (2.2), and denote by Lst def = E ∑T",5.2 Bounding m and M,[0],[0]
t=1〈T,5.2 Bounding m and M,[0],[0]
"kts qt, `t〉 = E ∑T",5.2 Bounding m and M,[0],[0]
"t=1〈ξst , `t〉 the total
expected loss of qt truncated to s. Then, as long as m− 2KLT > 0,
∃s ∈ (γ, 1/2] ∩ 1 2T N : m− 2KLT ≤ 1 +",5.2 Bounding m and M,[0],[0]
"LT − LsT γ .
",5.2 Bounding m and M,[0],[0]
Proof.,5.2 Bounding m and M,[0],[0]
The proof is a careful generalization of the proof of Lemma 4.3 (which in turn is just a discretization of the proof of Lemma 4.2).,5.2 Bounding m and M,[0],[0]
"Recall the notation x for the smallest element in [x,+∞) ∩ 12T N, and observe that for s ∈ 1 2T N, x ≤",5.2 Bounding m and M,[0],[0]
"s⇔ x ≤ s. Denote by
`majt def = ∑ i≤kt qt(i)∑ j≤kt qt(j) ¯̀ t(i) .
",5.2 Bounding m and M,[0],[0]
the weighted loss of the majority arms at round t. We have∑T t=1 ` maj t ≤ 2LT because ∑ j≤kt qt(j) ≥ ∑ j≤kt ζt(j) ≥ 1 2 and qt(i) ≤ pt(i) whenever ¯̀t(i) > 0,5.2 Bounding m and M,[0],[0]
"(owing to Definition 3.1 and Lemma 5.3).
",5.2 Bounding m and M,[0],[0]
"Now, for any s ≥ γ, define the function
f(s) def = E ∑T t=1",5.2 Bounding m and M,[0],[0]
∑,5.2 Bounding m and M,[0],[0]
i>kt 1{qt(i) ≤ s}(`majt,5.2 Bounding m and M,[0],[0]
"− ¯̀t(i)) .
",5.2 Bounding m and M,[0],[0]
Let us pick s ∈,5.2 Bounding m and M,[0],[0]
"[γ, 1/2] ∩ 12T N to minimize f(s), and breaking ties by choosing the smaller value of s. We make several observations:
• f(γ) ≥ 0",5.2 Bounding m and M,[0],[0]
because for any t and i > kt with qt(i) ≤ γ we must have pt(i),5.2 Bounding m and M,[0],[0]
= (T ktγ qt)(i) = 0 and thus ¯̀t(i),5.2 Bounding m and M,[0],[0]
= 0,5.2 Bounding m and M,[0],[0]
"by the definition of ¯̀t in Definition 3.1.
• f(1/2) =",5.2 Bounding m and M,[0],[0]
∑T t=1(K − kt)` maj t −m ≤,5.2 Bounding m and M,[0],[0]
"2KLT −m < 0.
",5.2 Bounding m and M,[0],[0]
• s > γ because f(s) ≤ f(1/2),5.2 Bounding m and M,[0],[0]
"< 0.
Let us define the points s0 def = γ and
{s1 < . . .",5.2 Bounding m and M,[0],[0]
"< sm} def = (γ, s] ∩ ⋃ i∈[K] {q1(i), . . .",5.2 Bounding m and M,[0],[0]
", qT (i)}.
Note that the tie-breaking rule for the choice of s ensures sm = s (if sm < s then it must satisfy f(sm) = f(s) giving a contradiction).
",5.2 Bounding m and M,[0],[0]
"Observe that by definition of the truncation operator, one has
〈T kts qt − qt, ¯̀t〉 = ∑ i>kt 1{qt(i) ≤ s}qt(i)(`majt − ¯̀t(i))
",5.2 Bounding m and M,[0],[0]
"In fact, after rounding, one can rewrite the above for some εs,t ∈",5.2 Bounding m and M,[0],[0]
"[− 12T , 1 2T ] as
〈T kts qt−qt, ¯̀t〉 = εs,t+ ∑ i>kt 1{qt(i) ≤ s}qt(i)(`majt −¯̀t(i))
",5.2 Bounding m and M,[0],[0]
"Then, for some ε ∈",5.2 Bounding m and M,[0],[0]
"[−1, 1], one has
LT − LsT = E T∑ t=1 〈T ktγ",5.2 Bounding m and M,[0],[0]
qt,5.2 Bounding m and M,[0],[0]
"− T kts qt, `t〉
= E T∑ t=1",5.2 Bounding m and M,[0],[0]
〈T ktγ,5.2 Bounding m and M,[0],[0]
qt,5.2 Bounding m and M,[0],[0]
"− T kts qt, ¯̀t〉
= ε+",5.2 Bounding m and M,[0],[0]
E T∑ t=1,5.2 Bounding m and M,[0],[0]
∑ i>kt (1{qt(i) ≤,5.2 Bounding m and M,[0],[0]
γ} − 1{qt(i) ≤,5.2 Bounding m and M,[0],[0]
"s})qt(i)(`majt − ¯̀t(i))
",5.2 Bounding m and M,[0],[0]
=,5.2 Bounding m and M,[0],[0]
ε+ E m∑ j=1 T∑ t=1,5.2 Bounding m and M,[0],[0]
∑ i>kt −sj1{qt(i) = sj}(`majt,5.2 Bounding m and M,[0],[0]
"− ¯̀t(i))
= ε+ m∑ j=1 sj(f(sj−1)− f(sj))
",5.2 Bounding m and M,[0],[0]
"= ε+ m−1∑ j=1 (sj+1 − sj)f(sj) + s1f(s0)− smf(sm) .
",5.2 Bounding m and M,[0],[0]
"Since f(s0) = f(γ) ≥ 0, f(si) ≥ f(s) and s = sm, we conclude that
LT − LsT ≥ ε+ (sm − s1)f(sm)− smf(sm) = ε− s1f(sm) ≥ γ(m− 2KLT ) .",5.2 Bounding m and M,[0],[0]
"Finally, using Lemma 3.2 (which applies thanks to Lemma 5.3), (5.1) and L∗T ≤ LsT (the loss of an expert is no better than the loss of the best expert L∗T ), we have
LT − LsT ≤",5.3 Putting all together,[0],[0]
O ( log(|E′|) η + η(m+M) + γKLT ) .,5.3 Putting all together,[0],[0]
"(5.2) Putting this into Lemma 5.5 and then using M ≤ 2KLT from Lemma 5.4, we have for any γ ≥ 2η,
γ(m+M) ≤",5.3 Putting all together,[0],[0]
"O ( log(|E′|) η + γKLT ) .
",5.3 Putting all together,[0],[0]
"Putting this into (5.1), we immediately get (3.2) as desired.",5.3 Putting all together,[0],[0]
This finishes the proof of Theorem 1.1.,5.3 Putting all together,[0],[0]
It only remains to ensure that qt verifying (2.2) indeed exists.,5.3 Putting all together,[0],[0]
We provide an algorithm for this in Section 6.,5.3 Putting all together,[0],[0]
"In this section, we answer the question of how to algorithmically find qt satisfying the implicitly definition (2.2).",6 Algorithmic Process to Find qt,[0],[0]
"We recall (2.2):
qt = 1∑ e∈E wt(e) + ∑ s∈S wt(s)
× (∑
e∈E wt(e)ξ e t + ∑ s∈S wt(s)T kts qt ) .",6 Algorithmic Process to Find qt,[0],[0]
"(2.2)
We show the following general lemma: Lemma 6.1.",6 Algorithmic Process to Find qt,[0],[0]
Given k ∈,6 Algorithmic Process to Find qt,[0],[0]
"[K], a finite subset S ⊂",6 Algorithmic Process to Find qt,[0],[0]
"[ 0, 12 ] , ζ ∈ ∆K with ζ(1) ≥ · · · ≥ ζ(K), and W ∈ ∆1+|S|, Algorithm 2 finds some q ∈ ∆K such that
q = W (1)ζ + ∑ s∈SW (s)T ks q .
",6 Algorithmic Process to Find qt,[0],[0]
"Furthermore, Algorithm 2 runs in time O(K · |S|).
",6 Algorithmic Process to Find qt,[0],[0]
"We observe that by setting k = kt, ζ = ζt = ∑ e∈E wt(e)·ξ e t∑
e∈E wt(e) , W (1) =
∑ e∈E wt(e)
",6 Algorithmic Process to Find qt,[0],[0]
"‖wt‖1
and ∀s ∈ S : W (s) = wt(s)‖wt‖1 in Lemma 6.1, we immediately obtain a vector q ∈ ∆K that we can use as qt.
Intuition for Lemma 6.1.",6 Algorithmic Process to Find qt,[0],[0]
We only search for q that is monotonically non-increasing for minority arms.,6 Algorithmic Process to Find qt,[0],[0]
This implies T ks q is also non-increasing for minority arms.,6 Algorithmic Process to Find qt,[0],[0]
"In symbols: q(k + 1) ≥ · · · ≥ q(K) and
(T ks q)(k + 1) ≥ · · · ≥ (T ks q)(K) .",6 Algorithmic Process to Find qt,[0],[0]
"Due to such monotonicity, when computing T ks q for each s ∈ S, there must exist some index πs ∈ {k + 1, k + 2, . . .",6 Algorithmic Process to Find qt,[0],[0]
",K + 1} such that the entry q(i) gets zeroed out for all i ≥ πs or in symbols, (T ks q)(i) = 0",6 Algorithmic Process to Find qt,[0],[0]
for all i ≥ πs.,6 Algorithmic Process to Find qt,[0],[0]
"Now, the main idea of Algorithm 2 is to search for such non-increasing function π : S →",6 Algorithmic Process to Find qt,[0],[0]
[K+1].,6 Algorithmic Process to Find qt,[0],[0]
"It initializes itself with πs = k + 1 for all s ∈ S, and then tries to increase π coordinate by coordinate.
",6 Algorithmic Process to Find qt,[0],[0]
"For each choice of π, Algorithm 2 computes a candidate distribution qπ ∈ ∆K which satisfies
qπ = W (1)ζ + ∑ s∈S W (s)us (6.1)
where each us is qπ but truncated so that its probabilities after πs are redistributed to the first k arms, or in symbols,
us(i) =  0, i ≥ πs; qπ(i),",6 Algorithmic Process to Find qt,[0],[0]
"πs > i > k; qπ(i) · ( 1 + ∑ j:j≥πs qπ(j)∑ j≤k qπ(j) ) , i ≤ k.
One can verify that the distribution qπ ∈ ∆K defined in Line 3 of Algorithm 2 is an explicit solution to (6.1).",6 Algorithmic Process to Find qt,[0],[0]
"Unfortunately, each us may not satisfy T ks qπ = us.",6 Algorithmic Process to Find qt,[0],[0]
"In particular, there may exist
some s ∈ S and i > k such that qπ(i) > s but us(i) = 0.",6 Algorithmic Process to Find qt,[0],[0]
"This means, we may have truncated too much for expert s in defining us, and we must increase πs.
",6 Algorithmic Process to Find qt,[0],[0]
"Perhaps not very surprisingly, if each iteration we only increase one πs by exactly 1, then we never overshoot and there exists a moment when q = qπ exactly satisfies
q",6 Algorithmic Process to Find qt,[0],[0]
"= W (1)ζ + ∑ s∈SW (s)T ks q .
",6 Algorithmic Process to Find qt,[0],[0]
We now give a formal proof of Lemma 6.1.,6 Algorithmic Process to Find qt,[0],[0]
Claim 6.2.,6.1 Proof details,[0],[0]
We claim some properties about Algorithm 2 (a) The process finishes after at most K · |S| iterations.,6.1 Proof details,[0],[0]
(b) We always have qπ(k + 1) ≥ · · · ≥ qπ(K).,6.1 Proof details,[0],[0]
(c),6.1 Proof details,[0],[0]
"As π changes, for each minority arm i > k, qπ(i) never
decreases.
",6.1 Proof details,[0],[0]
"(d) When the while loop ends, for each i > k and s ∈ S, we have qπ(i)",6.1 Proof details,[0],[0]
"> s⇐⇒ πs > i.
",6.1 Proof details,[0],[0]
"The proof of Claim 6.2 can be found in the full version.
",6.1 Proof details,[0],[0]
Proof of Lemma 6.1.,6.1 Proof details,[0],[0]
Suppose in the end of Algorithm 2 we obtain q = qπ for some π : S → [K+1].,6.1 Proof details,[0],[0]
"Let ξs = T ks q
Algorithm 2",6.1 Proof details,[0],[0]
Input: k ∈,6.1 Proof details,[0],[0]
"[K], a finite set S ⊆ [ 0, 12 ] , ζ ∈ ∆K with ζ(1) ≥ · · · ≥ ζ(K), and W ∈ ∆1+|S|
Output: q ∈ ∆K such that q",6.1 Proof details,[0],[0]
= W (1)ζ + ∑ s∈SW (,6.1 Proof details,[0],[0]
"s)T ks q.
1: initialize π : S →",6.1 Proof details,[0],[0]
"[K + 1] as πs = k + 1; will ensure πs ∈ {k + 1, k + 2, . .",6.1 Proof details,[0],[0]
.,6.1 Proof details,[0],[0]
",K + 1} 2: while true do
3: qπ(i)←  W (1) 1− ∑ s∈S∧πs>iW",6.1 Proof details,[0],[0]
"(s)
· ζ(i), if i > k; ζ(i)∑ j≤k ζ(j) · (1− ∑ j>k qπ(j)), if i ≤ k.
qπ ∈ ∆K
4: Pick any s ∈ S with πs ≤ K such that qπ(πs) >",6.1 Proof details,[0],[0]
"s. 5: if s is not found then break 6: else πs ← πs + 1. 7: end while 8: return qπ .
for each s ∈ S and q′",6.1 Proof details,[0],[0]
= W (1)ζ + ∑ s∈SW (,6.1 Proof details,[0],[0]
s)T ks q.,6.1 Proof details,[0],[0]
We need to show q = q′.,6.1 Proof details,[0],[0]
"For every minority arm i > k:
q′(i) ¬ = W (1) · ζ(i) + ∑ s∈S W (s) · ξs(i)
 ",6.1 Proof details,[0],[0]
"= W (1) · ζ(i) + ( ∑ s∈S∧q(i)>s W (s) ) · q(i)
® = W (1) · ζ(i) + ( ∑ s∈S∧πs>i W (s) ) · q(i) ¯= q(i) .
",6.1 Proof details,[0],[0]
"Above, equality ¬ is by the definition of q′, equality  is by the definition of ξs = T",6.1 Proof details,[0],[0]
"ks q, equality ® follows from Claim 6.2.d, and equality ¯ is by definition of q(i) = qπ(i)",6.1 Proof details,[0],[0]
= W (1) 1− ∑ s∈S∧πs>iW,6.1 Proof details,[0],[0]
(s) · ζ(i).,6.1 Proof details,[0],[0]
"For every majority arm i ≤ k, q′(i)
ζ(i)
¬ = W (1) · ζ(i)ζ(i) + ∑ s∈SW (s) · ξs(i) ζ(i)
 = W (1) + ∑ s∈SW (s) · ∑ j≤k ξ s(j)∑ j≤k ζ(j)
(6.2)
where equality ¬ is by the definition of q′ and equality  is because for every i ≤ k",6.1 Proof details,[0],[0]
"it satisfies ξ
s(i) q(i) =
∑ j≤k ξ
s(j)∑ j≤k q(j)
(using definition of ξs = T ks q) and for every i ≤",6.1 Proof details,[0],[0]
k,6.1 Proof details,[0],[0]
"it satisfies ζ(i)q(i) = ∑ j≤k ζ(j)∑ j≤k q(j)
(using definition of q = qπ Line 3 of Algorithm 2).
",6.1 Proof details,[0],[0]
"Now, the right hand side of (6.2) is independent of i. Therefore, we can write q′(i) = C1 · ζ(i) for each i ≤ k with some constant C1 > 0.",6.1 Proof details,[0],[0]
Our definition of q = qπ,6.1 Proof details,[0],[0]
(see Line 3 of Algorithm 2) ensures that we can also write q(i) = C2 · ζ(i) for each i ≤ k with some constant C2 > 0.,6.1 Proof details,[0],[0]
"Therefore, since for every i > k we have already shown q′(i) = q(i), it must satisfy C1 = C2 and therefore q′(i) = q(i) for all i ∈",6.1 Proof details,[0],[0]
[K].,6.1 Proof details,[0],[0]
"After proving q′ = q, we only need to argue about the running time.
",6.1 Proof details,[0],[0]
"If Algorithm 2 is implemented naively, then the total running time is O((K · |S|)2) because there are at most K · |S|
iterations (see Claim 6.2.a) and in each iteration we can compute qπ in time O(K · |S|).",6.1 Proof details,[0],[0]
In fact it is rather easy to find implicit update rules to make each iteration of Algorithm 2 run in O(1) time.,6.1 Proof details,[0],[0]
"We give some hints below.
",6.1 Proof details,[0],[0]
"Indeed, if in an iteration some πs is changed from i to i+ 1 (recalling i > k), then we can update qπ(i) in O(1) time.",6.1 Proof details,[0],[0]
For each j > k where j 6=,6.1 Proof details,[0],[0]
"i, we have qπ(j) is unchanged.",6.1 Proof details,[0],[0]
"The values of qπ(j) for j ≤ k all need to be changed, but they are only changed altogether by the same multiplicative factor (which can again be calculated in O(1) time).
",6.1 Proof details,[0],[0]
"Finally, to search for s ∈ S with πs ≤ K and qπ(πs) > s, we do not need to go through all s ∈ S. Instead, for each",6.1 Proof details,[0],[0]
"i > k, we maintain “the smallest si ∈ S so that qπ(i) > si.”",6.1 Proof details,[0],[0]
"Then, whenever πsi ≤",6.1 Proof details,[0],[0]
"i, that means we can pick s = si because qπ(πs) = qπ(πsi) ≥ qπ(i)",6.1 Proof details,[0],[0]
> si = s.,6.1 Proof details,[0],[0]
"For such reason, one can maintain a first-in-first-out list to store all values of i where qπ(i) > si.",6.1 Proof details,[0],[0]
In each iteration of Algorithm 2 we simply pick the first element in list and perform the update.,6.1 Proof details,[0],[0]
"This changes exactly one qπ(j) for j > k, and thus may additionally insert one element to list.",6.1 Proof details,[0],[0]
"Therefore, in each iteration we only needO(1) time to find some πs to increase.",6.1 Proof details,[0],[0]
"Regret bounds in online learning compare the player’s performance to L∗, the optimal performance in hindsight with a fixed strategy.",abstractText,[0],[0]
Typically such bounds scale with the square root of the time horizon T .,abstractText,[0],[0]
"The more refined concept of first-order regret bound replaces this with a scaling √ L∗, which may be much smaller than √ T .",abstractText,[0],[0]
It is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings.,abstractText,[0],[0]
"In a COLT 2017 open problem (Agarwal et al., 2017), Agarwal, Krishnamurthy, Langford, Luo, and Schapire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit problem.",abstractText,[0],[0]
"In the present paper, we resolve this open problem by presenting a new strategy based on augmenting the policy space.1",abstractText,[0],[0]
Make the Minority Great Again:  First-Order Regret Bound for Contextual Bandits,title,[0],[0]
"Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war. Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc. In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions. We then present a framework for incorporating such declarative knowledge into word problem solving. Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression. This provides a way to handle multiple concepts in the same problem while, at the same time, support interpretability of the answer expression. Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations. Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.",text,[0],[0]
"Many natural language understanding situations require reasoning with respect to numbers or quanti-
∗Most of the work was done when the authors were at the University of Illinois, Urbana Champaign.
ties – understanding financial news, sports results, or the number of casualties in a bombing.",1 Introduction,[0],[0]
Math word problems form a natural abstraction to a lot of these quantitative reasoning problems.,1 Introduction,[0],[0]
"Consequently, there has been a growing interest in developing automated methods to solve math word problems (Kushman et al., 2014; Hosseini et al., 2014; Roy and Roth, 2015).
",1 Introduction,[0],[0]
"Understanding and solving math word problems involves interpreting natural language description of mathematical concepts, as well as understanding their interaction with the physical world.",1 Introduction,[0],[0]
Consider the elementary school level arithmetic word problem shown in Fig 1.,1 Introduction,[0],[0]
"To solve the problem, one needs to understand that “apple pies” and “pecan pies” are kinds of “pies”, and hence, the number of
ar X
iv :1
71 2.
09 39
1v 1
[ cs
.C L
] 2
6 D
ec 2
apple pies and pecan pies needs to be summed up to get the total number of pies.",1 Introduction,[0],[0]
"Similarly, detecting that “5” represents “the number of pies per row” and applying dimensional analysis or unit compatibility knowledge, helps us infer that the total number of pies needs to be divided by 5 to get the answer.",1 Introduction,[0],[0]
"Besides part-whole relationship and dimensional analysis, there are several other concepts that are needed to support reasoning in math word problems.",1 Introduction,[0],[0]
"Some of these involve understanding comparisons, transactions, and the application of math or physics formulas.",1 Introduction,[0],[0]
"Most of this knowledge can be encoded as declarative rules, as illustrated in this paper.
",1 Introduction,[0],[0]
This paper introduces a framework for incorporating this “declarative knowledge” into word problem solving.,1 Introduction,[0],[0]
"We focus on arithmetic word problems, whose solution can be obtained by combining the numbers in the problem with basic operations (addition, subtraction, multiplication or division).",1 Introduction,[0],[0]
"For combining a pair of numbers or math subexpressions, our method first predicts the math concept that is needed for it (e.g., subset relationship, dimensional analysis, etc.), and then predicts a declarative rule under that concept to infer the mathematical operation.",1 Introduction,[0],[0]
"We model the selection of declarative rules as a latent variable, which removes the need for expensive annotations for the intermediate steps.
",1 Introduction,[0],[0]
The proposed approach has some clear advantages compared to existing work on word problem solving.,1 Introduction,[0],[0]
"First, it provides interpretability of the solution, without expensive annotations.",1 Introduction,[0],[0]
Our method selects a declarative knowledge based inference rule for each operation needed in the solution.,1 Introduction,[0],[0]
These rules provide an explanation for the operations performed.,1 Introduction,[0],[0]
"In particular, it learns to select relevant rules without explicit annotations for them.",1 Introduction,[0],[0]
"Second, each individual operation in the solution expression can be generated independently by a separate mathematical concept.",1 Introduction,[0],[0]
"This allows our method to handle multiple concepts in the same problem.
",1 Introduction,[0],[0]
"We show that existing datasets of arithmetic word problems suffer from significant vocabulary biases and, consequently, existing solvers do not do well on conceptually similar problems that are not biased in the same way.",1 Introduction,[0],[0]
"Our method, on the other hand, learns the right abstractions even in the presence of biases in the data.",1 Introduction,[0],[0]
"We also introduce a novel approach to gather word problems without these biases, creating
a new dataset of 1492 problems.",1 Introduction,[0],[0]
The next section discusses related work.,1 Introduction,[0],[0]
"We next introduce the mathematical concepts required for arithmetic word problems, as well as the declarative rules for each concept.",1 Introduction,[0],[0]
Section 4 describes our model – how we predict answers using declarative knowledge – and provides the details of our training paradigm.,1 Introduction,[0],[0]
"Finally, we provide experimental evaluation of our proposed method in Section 6, and then conclude with a discussion of future work.",1 Introduction,[0],[0]
"Our work is primarily related to three major strands of research - automatic word problem solving, semantic parsing, as well as approaches incorporating background knowledge in learning.",2 Related Work,[0],[0]
"There has been a growing interest in automatically solving math word problems, with various systems focusing on particular types of problems.",2.1 Automatic Word Problem Solving,[0],[0]
These can be broadly categorized into two types: arithmetic and algebra.,2.1 Automatic Word Problem Solving,[0],[0]
"Arithmetic Word Problems Arithmetic problems involve combining numbers with basic operations (addition, subtraction, multiplication and division), and are generally directed towards elementary school students.",2.1 Automatic Word Problem Solving,[0],[0]
"Roy and Roth (2015), Roy and Roth (2017) as well as this work focus on this class of word problems.",2.1 Automatic Word Problem Solving,[0],[0]
The works of Hosseini et al. (2014) and Mitra and Baral (2016) focus on arithmetic problems involving only addition and subtraction.,2.1 Automatic Word Problem Solving,[0],[0]
Some of these approaches also try to incorporate some form of declarative or domain knowledge.,2.1 Automatic Word Problem Solving,[0],[0]
Hosseini et al. (2014) incorporates the transfer phenomenon by classifying verbs; Mitra and Baral (2016) maps problems to a set of formulas.,2.1 Automatic Word Problem Solving,[0],[0]
"Both require extensive annotations for intermediate steps (verb classification for Hosseini et al. (2014), alignment of numbers to formulas for Mitra and Baral (2016), etc).",2.1 Automatic Word Problem Solving,[0],[0]
"In contrast, our method can handle a more general class of problems, while training only requires problem-equation pairs coupled with rate component annotations.",2.1 Automatic Word Problem Solving,[0],[0]
"Roy and Roth (2017) focuses only on using dimensional analysis knowledge, and handles the same class of problems as we do.",2.1 Automatic Word Problem Solving,[0],[0]
"In contrast, our method provides a framework
for including any form of declarative knowledge, exemplified here by incorporating common concepts required for arithmetic problems.",2.1 Automatic Word Problem Solving,[0],[0]
Algebra Word Problems Algebra word problems are characterized by the use of (one or more) variables in contructing (one or more) equations.,2.1 Automatic Word Problem Solving,[0],[0]
These are typically middle or high school problems.,2.1 Automatic Word Problem Solving,[0],[0]
"Koncel-Kedziorski et al. (2015) looks at single equation problems, and Shi et al. (2015) focuses on number word problems.",2.1 Automatic Word Problem Solving,[0],[0]
"Kushman et al. (2014) introduces a template based approach to handle general algebra word problems and several works have later proposed improvements over this approach (Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017).",2.1 Automatic Word Problem Solving,[0],[0]
"There has also been work on generating rationale for word problem solving (Ling et al., 2017).",2.1 Automatic Word Problem Solving,[0],[0]
"More recently, some focus turned to pre-university exam questions (Matsuzaki et al., 2017; Hopkins et al., 2017), which requires handling a wider range of problems and often more complex semantics.",2.1 Automatic Word Problem Solving,[0],[0]
"Our work is also related to learning semantic parsers from indirect supervision (Clarke et al., 2010; Liang et al., 2011).",2.2 Semantic Parsing,[0],[0]
"The general approach here is to learn a mapping of sentences to logical forms, with the only supervision being the response of executing the logical form on a knowledge base.",2.2 Semantic Parsing,[0],[0]
"Similarly, we learn to select declarative rules from supervision that only includes the final operation (and not which rule generated it).",2.2 Semantic Parsing,[0],[0]
"However, in contrast to the semantic parsing work, in our case the selection of each declarative rule usually requires reasoning across multiple sentences.",2.2 Semantic Parsing,[0],[0]
"Also, we do not require an explicit grounding of words or phrases to logical variables.",2.2 Semantic Parsing,[0],[0]
"Approaches to incorporate knowledge in learning started with Explanation based Learning (EBL) (DeJong, 1993; DeJong, 2014).",2.3 Background Knowledge in Learning,[0],[0]
"EBL uses domain knowledge based on observable predicates, whereas we learn to map text to predicates of our declarative knowledge.",2.3 Background Knowledge in Learning,[0],[0]
"More recent approaches tried to incorporate knowledge in the form of constraints or expectations from the output (Roth and tau Yih, 2004; wei Chang et al., 2007; Chang et al., 2012; Ganchev et al., 2010; Smith and Eisner, 2006; Naseem et al., 2010; Bisk and Hockenmaier, 2012; Gimpel and
Bansal, 2014).",2.3 Background Knowledge in Learning,[0],[0]
"Finally, we note that there has been some work in the context of Question Answering on perturbing questions or answers as a way to test or assure the robustness of the approach or lack of (Khashabi et al., 2016; Jia and Liang, 2017).",2.3 Background Knowledge in Learning,[0],[0]
We make used of similar ideas in order to generate an unbiased test set for Math word problems (Sec. 6).,2.3 Background Knowledge in Learning,[0],[0]
We introduce here our representation of domain knowledge.,3 Knowledge Representation,[0],[0]
We organize the knowledge hierarchically in two levels – concepts and declarative rules.,3 Knowledge Representation,[0],[0]
A math concept is a phenomenon which needs to be understood to apply reasoning over quantities.,3 Knowledge Representation,[0],[0]
"Examples of concepts include part-whole relations, dimensional analysis, etc.",3 Knowledge Representation,[0],[0]
"Under each concept, there are a few declarative rules, which dictate which operation is needed in a particular context.",3 Knowledge Representation,[0],[0]
"An example of a declarative rule under part-whole concept can be that “if two numbers quantify “parts” of a larger quantity, the operation between them must be addition”.",3 Knowledge Representation,[0],[0]
"These rules use concept specific predicates, which we exemplify in the following subsections.
",3 Knowledge Representation,[0],[0]
"Since this work focuses on arithmetic word problems, we consider 4 math concepts which are most common in these problems, as follows:
1.",3 Knowledge Representation,[0],[0]
Transfer:,3 Knowledge Representation,[0],[0]
This involves understanding the transfer of objects from one person to another.,3 Knowledge Representation,[0],[0]
"For example, the action described by the sentence “Tim gave 5 apples to Jim”, results in Tim losing “5 apples” and Jim gaining “5 apples”.
2.",3 Knowledge Representation,[0],[0]
Dimensional Analysis:,3 Knowledge Representation,[0],[0]
This involves understanding compatibility of units or dimensions.,3 Knowledge Representation,[0],[0]
"For example, “30 pies” can be divided by “5 pies per row” to get the number of rows.
3.",3 Knowledge Representation,[0],[0]
Part-Whole Relation:,3 Knowledge Representation,[0],[0]
"This includes asserting that if two numbers quantify parts of a larger quantity, they are to be added.",3 Knowledge Representation,[0],[0]
"For example, the problem in Section 1 involves understanding “pecan pies” and “apple pies” are parts of “pies”, and hence must be added.
4.",3 Knowledge Representation,[0],[0]
Explicit Math:,3 Knowledge Representation,[0],[0]
Word problems often mention explicit math relationships among quantities or entities in the problem.,3 Knowledge Representation,[0],[0]
"For example, “Jim is 5
inches taller than Tim”.",3 Knowledge Representation,[0],[0]
"This concept captures the reasoning needed for such relationships.
",3 Knowledge Representation,[0],[0]
Each of these concepts comprises a small number of declarative rules which determine the math operations; we describe them below.,3 Knowledge Representation,[0],[0]
Consider the following excerpt of a word problem exhibiting a transfer phenomenon: “Stephen owns 5 books.,3.1 Transfer,[0],[0]
Daniel gave him 4 books.,3.1 Transfer,[0],[0]
"The goal of the declarative rules is to determine which operation is required between 5 and 4, given that we know that a transfer is taking place.",3.1 Transfer,[0],[0]
"We note that a transfer usually involves two entities, which occur as subject and indirect object in a sentence.",3.1 Transfer,[0],[0]
"Also, the direction of transfer is determined by the verbs associated with the entities.",3.1 Transfer,[0],[0]
"We define a set of variables to denote these properties; we define as Subj1, Verb1, IObj1 the subject, verb and indirect object associated with the first number, and as Subj2, Verb2, IObj2 the subject, verb and indirect object related to the second number.",3.1 Transfer,[0],[0]
"For the above example, the assignment of the variables are shown below:
[Stephen]Subj1 [owns]V erb1 5 books.",3.1 Transfer,[0],[0]
"[Daniel]Subj2 [gave]V erb2 [him]IObj2 4 books.
",3.1 Transfer,[0],[0]
"In order to determine the direction of transfer, we require some classification of verbs.",3.1 Transfer,[0],[0]
"In particular, we classify each verb into one of five classes: HAVE, GET, GIVE, CONSTRUCT and DESTROY.",3.1 Transfer,[0],[0]
"The HAVE class consists of all verbs which signify the state of an entity, such as “have”, “own”, etc.",3.1 Transfer,[0],[0]
The GET class contains verbs which indicate the gaining of things for the subject.,3.1 Transfer,[0],[0]
"Examples of such verbs are “acquire”, “borrow”, etc.",3.1 Transfer,[0],[0]
The GIVE class contains verbs which indicate the loss of things for the subject.,3.1 Transfer,[0],[0]
"Verbs like “lend”, “give” belong to this class.",3.1 Transfer,[0],[0]
"Finally CONSTRUCT class constitutes verbs indicating construction or creation, like “build”, “fill”, etc., while DESTROY verbs indicate destruction related verbs like “destroy”, “eat”, “use”, etc.",3.1 Transfer,[0],[0]
"This verb classification is largely based on the work of (Hosseini et al., 2014).
",3.1 Transfer,[0],[0]
"Finally, the declarative rules for this concept have the following form:
[Verb1 ∈ HAVE] ∧",3.1 Transfer,[0],[0]
[Verb2 ∈ GIVE] ∧,3.1 Transfer,[0],[0]
"[Coref(Subj1, IObj2)]⇒ Addition
where Coref(A,B) is true when A and B represent the same entity or are coreferent, and is false otherwise.",3.1 Transfer,[0],[0]
"In the examples above, Verb1 is “own” and hence [Verb1 ∈ HAVE] is true.",3.1 Transfer,[0],[0]
Verb2 is “give” and hence [Verb2 ∈ GIVE] is true.,3.1 Transfer,[0],[0]
"Finally, Subj1 and IObj2 both refer to Stephen, so [Coref(Subj1, IObj2)] returns true.",3.1 Transfer,[0],[0]
"As a result, the above declarative rule dictates that addition should be performed between 5 and 4.
",3.1 Transfer,[0],[0]
"We have 18 such inference rules for transfer, covering all combinations of verb classes and Coref() values.",3.1 Transfer,[0],[0]
All these rules generate addition or subtraction operations.,3.1 Transfer,[0],[0]
We now look at the use of dimensional analysis knowledge in word problem solving.,3.2 Dimensional Analysis,[0],[0]
"To use dimensional analysis, one needs to extract the units of numbers as well as the relations between the units.",3.2 Dimensional Analysis,[0],[0]
Consider the following excerpt of a word problem: “Stephen has 5 bags.,3.2 Dimensional Analysis,[0],[0]
Each bag has 4 apples.,3.2 Dimensional Analysis,[0],[0]
"Knowing that the unit of 5 is “bag” and the effective unit of 4 is “apples per bag”, allows us to infer that the numbers can be multiplied to obtain the total number of apples.
",3.2 Dimensional Analysis,[0],[0]
"To capture these dependencies, we first introduce a few terms.",3.2 Dimensional Analysis,[0],[0]
"Whenever a number has a unit of the form “A per B”, we refer to “A” as the unit of the number, and refer to “B” as the rate component of the number.",3.2 Dimensional Analysis,[0],[0]
"In our example, the unit of 4 is “apple”, and the rate component of 4 is “bag”.",3.2 Dimensional Analysis,[0],[0]
We define variables Unit1 and Rate1 to denote the unit and the rate component of the first number respectively.,3.2 Dimensional Analysis,[0],[0]
We similarly define Unit2 and Rate2.,3.2 Dimensional Analysis,[0],[0]
"For the above example, the assignment of variables are shown below:
Stephen has 5 [bags]Unit1.",3.2 Dimensional Analysis,[0],[0]
"Each [bag]Rate2 has 4 [apples]Unit2.
",3.2 Dimensional Analysis,[0],[0]
"Finally, the declarative rule applicable for our example has the following form:
",3.2 Dimensional Analysis,[0],[0]
"[Coref(Unit1,Rate2)]⇒ Multiplication
We only have 3 rules for dimensional analysis.",3.2 Dimensional Analysis,[0],[0]
They generate multiplication or division operations.,3.2 Dimensional Analysis,[0],[0]
"In this subsection, we want to capture the reasoning behind explicit math relationships expressed in word problems such as the one described in: “Stephen has 5 apples.",3.3 Explicit Math,[0],[0]
Daniel has 4 more apples than Stephen”.,3.3 Explicit Math,[0],[0]
We define by Math1 and Math2 any explicit math term associated with the first and second numbers respectively.,3.3 Explicit Math,[0],[0]
"As was the case for transfers, we also define Subj1, IObj1, Subj2, and IObj2 to denote the entities participating in the math relationship.",3.3 Explicit Math,[0],[0]
"The assignment of these variables in our example is:
[Stephen]Subj1 has 5 apples.",3.3 Explicit Math,[0],[0]
[Daniel]Subj2 has 4 [more apples than]Math2,3.3 Explicit Math,[0],[0]
"[Stephen]IObj2.
",3.3 Explicit Math,[0],[0]
"We classify explicit math terms into one of three classes - ADD, SUB and MUL.",3.3 Explicit Math,[0],[0]
"ADD comprises terms for addition, like “more than”, “taller than” and “heavier than”.",3.3 Explicit Math,[0],[0]
"SUB consists of terms for subtraction like“less than”, “shorter than”, etc., and MUL contains terms indicating multiplication, like “times”, “twice” and “thrice”.",3.3 Explicit Math,[0],[0]
"Finally, the declarative rule that applies for our example is:
[Coref(Subj1, IObj2)]",3.3 Explicit Math,[0],[0]
∧ [Math2 ∈ ADD] ⇒,3.3 Explicit Math,[0],[0]
"Addition
We have only 7 rules for explicit math.",3.3 Explicit Math,[0],[0]
"Understanding part-whole relationship entails understanding whether two quantities are hyponym, hypernym or siblings (that is, co-hyponym, or parts of the same quantity).",3.4 Part-Whole Relation,[0],[0]
"For example, in the excerpt “Mrs. Hilt has 5 pecan pies and 4 apple pies”, determining that pecan pies and apple pies are parts of all pies, helps inferring that addition is needed.",3.4 Part-Whole Relation,[0],[0]
"We have 3 simple rules which directly map from Hyponym, Hypernym or Sibling detection to the corresponding math operation.",3.4 Part-Whole Relation,[0],[0]
"For the above example, the applicable declarative rule is:
[Sibling(Number1,Number2)]⇒ Addition
The rules for part-whole concept can generate addition and subtraction operations.",3.4 Part-Whole Relation,[0],[0]
Table 1 gives a list of all the declarative rules.,3.4 Part-Whole Relation,[0],[0]
Note that all the declarative rules are designed to determine an operation between two numbers only.,3.4 Part-Whole Relation,[0],[0]
"We introduce a strategy in Section 4, which facilitates combining subexpressions with these rules.",3.4 Part-Whole Relation,[0],[0]
"Given an input arithmetic word problem x, the goal is to predict the math expression y, which generates the correct answer.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In order to derive the expression y from the word problem x, we leverage math concepts and declarative rules that we introduced in Section 3.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In order to combine two numbers mentioned in x, we first predict a concept k, and then we choose a declarative knowledge rule r from k.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
The rule r generates the math operation needed to combine the two numbers.,4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
Consider the first example in Table 2.,4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"To combine 6 and 9, we first decide on the transfer concept, and then choose an appropriate rule under transfer to generate the operation.
",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
Next we need to combine the sub-expression (6+ 9) with the number 3.,4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"However, our inference rules were designed for the combination of two numbers only.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In order to combine a sub-expression, we choose a representative number from the subexpression, and use that number to determine the operation.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In our example, we choose the number 6 as the representative number for (6 + 9), and decide the operation between 6 and 3, following a similar procedure as before.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"This operation is now used to combine (6 + 9) and 3.
",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
The representative number for a sub-expression is chosen such that it preserves the reasoning needed for the combination of this sub-expression with other numbers.,4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"We follow a heuristic to choose a representative number from a sub-expression:
1.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"For transfers and part-whole relationship, we choose the representative number of the left subtree.
2.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In case of rate relationship, we choose the number which does not have a rate component.
3.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"In case of explicit math, we choose the number which is not directly associated with the explicit math expression.",4 Mapping of Word Problems to Declarative Knowledge,[0],[0]
"Given the input word problem x, the solution math expression y is constructed by combining numbers in x with operations.",4.1 Scoring Answer Derivations,[0],[0]
We refer to the set of operations used in an expression y as (y).,4.1 Scoring Answer Derivations,[0],[0]
"Each operation o in (y) is generated by first choosing a concept ko, and then selecting a declarative rule ro from that concept.
",4.1 Scoring Answer Derivations,[0],[0]
"In order to discriminate between multiple candidate solution expressions of a word problem x, we
score them using a linear model over features extracted from the derivation of the solution.",4.1 Scoring Answer Derivations,[0],[0]
"Our scoring function has the following form:
SCORE(x, y) = ∑
o∈ (y)
wkφk(x, k o) + wrφr(x, r o)
where φk(x, ko) and φr(x, ro) are feature vectors related to concept ko, and declarative rule ro, respectively, and wk and wr are the corresponding weight vectors.",4.1 Scoring Answer Derivations,[0],[0]
"The term wkφk(x, ko) is the score for the selection of ko, and the termwrφr(x, ro) is the score for the selection of ro.",4.1 Scoring Answer Derivations,[0],[0]
"Finally, the total score is the sum of the scores of all concepts and rule choices, over all operations of y.",4.1 Scoring Answer Derivations,[0],[0]
"We wish to estimate the parameters of the weight vectors wk and wr, such that our scoring function assigns a higher score to the correct math expression, and a lower score to other competing math expressions.",4.2 Learning,[0],[0]
"For learning the parameters, we assume access to word problems paired with the correct math expression.",4.2 Learning,[0],[0]
We show in Section 5 that certain simple heuristics and rate component annotations can be used to create somewhat noisy annotations for the concepts needed for individual operations.,4.2 Learning,[0],[0]
"Hence, we will assume for our formulation access to concept supervision as well.",4.2 Learning,[0],[0]
"We thus assume access to m examples of the following form: {(x1, y1, {ko}o∈ (y1)), (x2, y2, {ko}o∈ (y2)), . . .",4.2 Learning,[0],[0]
", (xm, ym, {ko}o∈ (ym))}.
",4.2 Learning,[0],[0]
"We do not have any supervision for declarative rule selection, which we model as a latent variable.",4.2 Learning,[0],[0]
Two Stage Learning: A straightforward solution for our learning problem could be to jointly learn wk and wr using latent structured SVM.,4.2 Learning,[0],[0]
"However, we found that this model does not perform well.",4.2 Learning,[0],[0]
"Instead, we chose a two stage learning protocol.",4.2 Learning,[0],[0]
"At the first stage, we only learn wr, the weight vector for
scoring the declarative rule choice.",4.2 Learning,[0],[0]
"Once learned, we fix the parameters for wr, and then learn the parameters for wk.
",4.2 Learning,[0],[0]
"In order to learn the parameters for wr, we solve:
min wr
1 2 ||wr||2 + C m∑ i=1",4.2 Learning,[0],[0]
∑,4.2 Learning,[0],[0]
"o∈ (yi) [ max r̂∈ko,r̂⇒ô wr · φr(x, r̂)+
∆(ô, o) ]",4.2 Learning,[0],[0]
"− max
r̂∈ko,r̂⇒o wr · φr(x, r̂),
where r̂ ∈ ko implies that r̂ is a declarative rule for concept ko, r̂ ⇒",4.2 Learning,[0],[0]
o,4.2 Learning,[0],[0]
"signify that the declarative rule r̂ generates operation o, and ∆(ô, o) represents a measure of dissimilarity between operations o and ô. The above objective is similar to that of latent structured SVM.",4.2 Learning,[0],[0]
"For each operation o in the solution expression yi, the objective tries to minimize the difference between the highest scoring rule from its concept ko, and highest scoring rule from ko which explains or generates the operation o.
Next we fix the parameters of wr, and solve:
min wk
1 2 ||wk||2 + C m∑ i=1
max y∈Y
[SCORE(xi, y) + ∆(y, yi)]− SCORE(xi, yi).
",4.2 Learning,[0],[0]
This is equivalent to a standard structured SVM objective.,4.2 Learning,[0],[0]
"We use a 0 − 1 loss for ∆(ô, o).",4.2 Learning,[0],[0]
"Note that fixing the parameters of wr determines the scores for rule selection, removing the need for any latent variables at this stage.",4.2 Learning,[0],[0]
"Given an input word problem x, inferring the best math expression involves computing arg maxy∈Y SCORE(x, y), where Y is the set of all math expressions that can be created by combining the numbers in x with basic math operations.
",4.3 Inference,[0],[0]
"The size of Y is exponential in the number of quantities mentioned in x. As a result, we perform approximate inference using beam search.",4.3 Inference,[0],[0]
We initialize the beam with the set E of all numbers mentioned in the problem x.,4.3 Inference,[0],[0]
"At each step of the beam search, we choose two numbers (or sub-expressions) e1 and e2 fromE, and then select a math concept and a declarative rule to infer an operation o.",4.3 Inference,[0],[0]
We create a new sub-expression e3 by combining the subexpressions e1 and e2 with operation o.,4.3 Inference,[0],[0]
"We finally create a new set E′ from E, by removing e1 and e2 from it, and adding e3 to it.",4.3 Inference,[0],[0]
"We remove E from the beam, and add all such modified sets E′ to the beam.",4.3 Inference,[0],[0]
We continue this process until all sets in the beam have only one element in them.,4.3 Inference,[0],[0]
We choose the highest scoring expression among these elements as the solution expression.,4.3 Inference,[0],[0]
"Each word problem in our dataset is annotated with the solution math expression, along with alignment of numbers from the problem to the solution expression.",5.1 Supervision,[0],[0]
"In addition, we also have annotations for the numbers which possess a rate component.",5.1 Supervision,[0],[0]
An example is shown in Fig 2.,5.1 Supervision,[0],[0]
"This is the same level of supervision used in (Roy and Roth, 2017).",5.1 Supervision,[0],[0]
Many of the annotations can be extracted semi-automatically.,5.1 Supervision,[0],[0]
"The number list is extracted automatically by a number detector, the alignments require human supervision only when the same numeric value is mentioned multiple times in the problem.",5.1 Supervision,[0],[0]
"Most of the rate component annotations can also be extracted automatically, see (Roy and Roth, 2017) for details.
",5.1 Supervision,[0],[0]
"We apply a few heuristics to obtain noisy anno-
tations for the math concepts for operations.",5.1 Supervision,[0],[0]
"Consider the case for combining two numbers num1 and num2, by operation o.",5.1 Supervision,[0],[0]
"We apply the following rules:
1.",5.1 Supervision,[0],[0]
"If we detect an explicit math pattern in the neighborhood of num1 or num2, we assign concept ko to be Explicit Math.
2.",5.1 Supervision,[0],[0]
"If o is multiplication or division, and one of num1 or num2 has a rate component, we assign ko to be Dimensional Analysis.
3.",5.1 Supervision,[0],[0]
"If o is addition or subtraction, we check if the dependent verb of both numbers are identical.",5.1 Supervision,[0],[0]
"If they are, we assign ko to be Part-Whole relationship, otherwise, we assign it to be Transfer.",5.1 Supervision,[0],[0]
"We extract the dependent verb using the Stanford dependency parser (Chen and Manning, 2014).
",5.1 Supervision,[0],[0]
The annotations obtained via these rules are of course not perfect.,5.1 Supervision,[0],[0]
"We could not detect certain uncommon rate patterns like “dividing the cost 4 ways”, and “I read same number of books 4 days running”.",5.1 Supervision,[0],[0]
"There were part-whole relationships exhibited with complementary verbs, as in “I won 4 games, and lost 3.”.",5.1 Supervision,[0],[0]
"Both these cases lead to noisy math concept annotations.
",5.1 Supervision,[0],[0]
"However, we tested a small sample of these annotations, and found less than 5% of them to be wrong.",5.1 Supervision,[0],[0]
"As a result, we assume these annotations to be correct in our problem formulation.",5.1 Supervision,[0],[0]
"We use dependency parse labels and a small set of rules to extract subject, indirect object, dependent verb, unit and rate component of each number
mentioned in the problem.",5.2 Features,[0],[0]
Details of these extractions can be found in the released codebase.,5.2 Features,[0],[0]
"Using these extractions, we define two feature functions φk(x, ko) and φr(x, ro), where x is the input word problem, and ko and ro are the concept and the declarative rule for operation o respectively.",5.2 Features,[0],[0]
"φr(x, r o) constitutes the following features:
1.",5.2 Features,[0],[0]
"If ro contains Coref(·) function, we add features related to similarity of the arguments of Coref(·) (jaccard similarity score and presence of pronoun in one of the arguments).
2.",5.2 Features,[0],[0]
"For part-whole relationships, we add indicators for a list of words like “remaining”, “rest”, “either”, “overall”, “total”, conjoined with the part-whole function in ro (Hyponymy, Hypernymy, Sibling).
3.",5.2 Features,[0],[0]
"Unigrams from the neighborhood of numbers being combined.
",5.2 Features,[0],[0]
"Finally, φk(x, ko) generates the following features:
1.",5.2 Features,[0],[0]
"If ko is related to dimensional analysis, we add features indicating the presence of a rate component in the combining numbers.
2.",5.2 Features,[0],[0]
"If ko is part-whole, we add features indicating whether the verbs of combining numbers are identical.
",5.2 Features,[0],[0]
"Note that these features capture several interpretable functions like coreference, hyponymy, etc.
",5.2 Features,[0],[0]
"We do not learn three components of our system – verb classification for transfer knowledge, categorization of explicit math terms, and irrelevant number detection.",5.2 Features,[0],[0]
"For verb classification, we use a seed list of around 10 verbs for each category.",5.2 Features,[0],[0]
"Given a new verb v, we choose the most similar verb v′ from the seed lists according to Glove vector (Pennington et al., 2014) based similarity .",5.2 Features,[0],[0]
We assign v the category of v′.,5.2 Features,[0],[0]
"This can be replaced by a learned component (Hosseini et al., 2014).",5.2 Features,[0],[0]
However we found seed list based categorization to work well in most cases.,5.2 Features,[0],[0]
"For explicit math, we check for a small list of patterns to detect and categorize math terms.",5.2 Features,[0],[0]
"Note that for both the cases above, we still have to learn Coref(·) function to determine the final operation.",5.2 Features,[0],[0]
"Finally, to detect irrelevant numbers (numbers which
are not used in the solution), we use a set of rules based on the units of numbers.",5.2 Features,[0],[0]
"Again, this can be replaced by a learned model (Roy and Roth, 2015).",5.2 Features,[0],[0]
"We first evaluate our approach on the existing datasets of AllArith, AllArithLex, and AllArithTmpl (Roy and Roth, 2017).",6.1 Results on Existing Dataset,[0],[0]
"AllArithLex and AllArithTmpl are subsets of the AllArith dataset, created to test the robustness to new vocabulary, and new equation forms respectively.",6.1 Results on Existing Dataset,[0],[0]
We compare to the top performing systems for arithmetic word problems.,6.1 Results on Existing Dataset,[0],[0]
"They are as follows:
1.",6.1 Results on Existing Dataset,[0],[0]
TEMPLATE :,6.1 Results on Existing Dataset,[0],[0]
"Template based algebra word problem solver of (Kushman et al., 2014).
",6.1 Results on Existing Dataset,[0],[0]
2.,6.1 Results on Existing Dataset,[0],[0]
"LCA++ : System of (Roy and Roth, 2015) based on lowest common ancestors of math expression trees.
3.",6.1 Results on Existing Dataset,[0],[0]
UNITDEP:,6.1 Results on Existing Dataset,[0],[0]
"Unit dependency graph based solver of (Roy and Roth, 2017).
",6.1 Results on Existing Dataset,[0],[0]
We refer to our approach as KNOWLEDGE.,6.1 Results on Existing Dataset,[0],[0]
"For all solvers, we use the system released by the respective authors.",6.1 Results on Existing Dataset,[0],[0]
"The system of TEMPLATE expects an equation as the answer, whereas our dataset contains only math expressions.",6.1 Results on Existing Dataset,[0],[0]
"We converted expressions to equations by introducing a single variable, and assigning the math expression to it.",6.1 Results on Existing Dataset,[0],[0]
"For example, an expression “(2 + 3)” gets converted to “X = (2 + 3)”.
",6.1 Results on Existing Dataset,[0],[0]
The first few columns of Table 3 shows the performance of the systems on the aforementioned datasets1.,6.1 Results on Existing Dataset,[0],[0]
The performance of KNOWLEDGE is on par or lower than some of the existing systems.,6.1 Results on Existing Dataset,[0],[0]
"We analyzed the systems, and found most of them to be not robust to perturbations of the problem text; Table 4 shows a few examples.",6.1 Results on Existing Dataset,[0],[0]
"We further analyzed the datasets, and identified several biases in the problems (in both train and test).",6.1 Results on Existing Dataset,[0],[0]
Systems which remember these biases get an undue advantage in evaluation.,6.1 Results on Existing Dataset,[0],[0]
"For example, the verb “give” only appears with subtraction, and hence the models are
1Results on the AllArith datasets are slightly different from (Roy and Roth, 2017), since we fixed several ungrammatical sentences in the dataset
learning an erroneous correlation of “give” with subtraction.",6.1 Results on Existing Dataset,[0],[0]
"Since the test also exhibit the same bias, these systems get all the “give”-related questions correct.",6.1 Results on Existing Dataset,[0],[0]
"However, they fail to solve the problem in Table 4, where “give” results in addition.",6.1 Results on Existing Dataset,[0],[0]
"We also tested KNOWLEDGE on the addition subtraction problems dataset released by (Hosseini et al., 2014).",6.1 Results on Existing Dataset,[0],[0]
"It achieved a cross validation accuracy of 77.19%, which is competitive with the state of the art accuracy of 78% achieved with the same level of supervision.",6.1 Results on Existing Dataset,[0],[0]
"The system of (Mitra and Baral, 2016) achieved 86.07% accuracy on this dataset, but requires rich annotations for formulas and alignment of numbers to formulas.",6.1 Results on Existing Dataset,[0],[0]
"In order to remove the aforementioned biases from the dataset, we augment it with new word problems collected via a crowdsourcing platform.",6.2 New Dataset Creation,[0],[0]
"These new word problems are created by perturbing the original problems minimally, such that the answer is different from the original problem.",6.2 New Dataset Creation,[0],[0]
"For each word problem p with an answer expression a in our original dataset AllArith, we replace one operation in a to create a new math expression a′.",6.2 New Dataset Creation,[0],[0]
"We ask annotators to modify problem p minimally, such that a′ is now the solution to the modified word problem.
",6.2 New Dataset Creation,[0],[0]
"We create a′ from a either by replacing an addition with subtraction or vice versa, or by replacing multiplication with division or vice versa.",6.2 New Dataset Creation,[0],[0]
"We do not replace addition and subtraction with multiplication or division, since there might not be an easy perturbation that supports this conversion.",6.2 New Dataset Creation,[0],[0]
We only allowed perturbed expressions which evaluate to values greater than 1.,6.2 New Dataset Creation,[0],[0]
"For example, we generate the expression “(3+2)” from “(3-2)”, we generated expressions “(10+2)/4” and “(10-2)*4” for the expression “(10-2)/4”.",6.2 New Dataset Creation,[0],[0]
"We generate all possible perturbed expressions for a given answer expression, and ask for problem text modification for each one of them.
",6.2 New Dataset Creation,[0],[0]
We show the annotators the original problem text p paired with a perturbed answer a′.,6.2 New Dataset Creation,[0],[0]
"The instructions advised them to copy over the given problem text, and modify it as little as possible so that the given math expression is now the solution to this modified problem.",6.2 New Dataset Creation,[0],[0]
They were also instructed to not add or delete the numbers mentioned in the problem.,6.2 New Dataset Creation,[0],[0]
"If the original problem mentions two “3”s and one “2”, the
modified problem should also contain two “3”s and one “2”.
",6.2 New Dataset Creation,[0],[0]
"We manually pruned problems which did not yield the desired solution a′, or were too different from the input problem p.",6.2 New Dataset Creation,[0],[0]
"This procedure gave us a set of 661 new word problems, which we refer to as Perturb.",6.2 New Dataset Creation,[0],[0]
"Finally we augment AllArith with the problems of Perturb, and call this new dataset Aggregate.",6.2 New Dataset Creation,[0],[0]
"Aggregate has a total of 1492 problems.
",6.2 New Dataset Creation,[0],[0]
The addition of the Perturb problems ensures that the dataset now has problems with similar lexical items generating different answers.,6.2 New Dataset Creation,[0],[0]
This minimizes the bias that we discussed in subsection 6.1.,6.2 New Dataset Creation,[0],[0]
"To quantify this, consider the probability distribution over operations for a quantity q, given that word w is present in the neighborhood of q. For an unbiased dataset, you will expect the entropy of this distribution to be high, since the presence of a single word in a number neighborhood will seldom be completely informative for the operation.",6.2 New Dataset Creation,[0],[0]
We compute the average of this entropy value over all numbers and neighborhood words in our dataset.,6.2 New Dataset Creation,[0],[0]
"AllArith and Perturb have an average entropy of 0.34 and 0.32 respectively, whereas Aggregate’s average entropy is 0.54, indicating that, indeed, the complete data set is significantly less biased.",6.2 New Dataset Creation,[0],[0]
"First, we evaluate the ability of systems to generalize from biased datasets.",6.3 Generalization from Biased Dataset,[0],[0]
"We train all systems on AllArith, and test them on Perturb (which was created by perturbing AllArith problems).",6.3 Generalization from Biased Dataset,[0],[0]
The last column of Table 3 shows the performance of systems in this setting.,6.3 Generalization from Biased Dataset,[0],[0]
KNOWLEDGE outperforms all other systems in this setting with around 19% absolute improvement over UNITDEP.,6.3 Generalization from Biased Dataset,[0],[0]
"This shows that declarative knowledge allows the system to learn the correct abstractions, even from biased datasets.",6.3 Generalization from Biased Dataset,[0],[0]
"Finally, we evaluate the systems on the Aggregate dataset.",6.4 Results on the New Dataset,[0],[0]
"Following previous work (Roy and Roth, 2017), we compute two subsets of Aggregate comprising 756 problems each, using the MAWPS (Koncel-Kedziorski et al., 2016) system.",6.4 Results on the New Dataset,[0],[0]
"The first, called AggregateLex, is one with low lexical repetitions, and the second called AggregateTmpl is one with low repetitions of equation forms.",6.4 Results on the New Dataset,[0],[0]
"We
also evaluate on these two subsets on a 5-fold crossvaliation.",6.4 Results on the New Dataset,[0],[0]
Columns 4-6 of Table 3 show the performance of systems on this setting.,6.4 Results on the New Dataset,[0],[0]
"KNOWLEDGE significantly outperforms other systems on Aggregate and AggregateLex, and is similar to UNITDEP on AggregateTmpl.",6.4 Results on the New Dataset,[0],[0]
"There is a 9% absolute improvement on AggregateLex, showing that KNOWLEDGE is significantly more robust to low lexical overlap between train and test.",6.4 Results on the New Dataset,[0],[0]
"The last column of Table 4 also shows that the other systems do not learn the right abstraction, even when trained on Aggregate.",6.4 Results on the New Dataset,[0],[0]
Coverage of the Declarative Rules We chose math concepts and declarative rules based on their prevalance in arithmetic word problems.,6.5 Analysis,[0],[0]
We found that the four concepts introduced in this paper cover almost all the problems in our dataset; only missing 4 problems involving application of area formulas.,6.5 Analysis,[0],[0]
"We also checked earlier arithmetic problem datasets from the works of (Hosseini et al., 2014; Roy and Roth, 2015), and found that the math concepts and declarative rules introduced in this paper cover all their problems.
",6.5 Analysis,[0],[0]
A major challenge in applying these concepts and rules to algebra word problems is the use of variables in constructing equations.,6.5 Analysis,[0],[0]
"Variables are often implicitly described, and it is difficult to extract units, dependent verbs, associated subjects and objects for the variables.",6.5 Analysis,[0],[0]
"However, we need these extractions in order to apply our declarative rules to combine variables.",6.5 Analysis,[0],[0]
"There has been some work to extract meaning of variables (Roy et al., 2016) in algebra word problems; an extension of this can possibly support the application of rules in algebra word problems.",6.5 Analysis,[0],[0]
"We leave this exploration to future work.
",6.5 Analysis,[0],[0]
"Higher standard word problems often require application of math formulas like ones related to area, interest, probability, etc.",6.5 Analysis,[0],[0]
"Extending our approach to handle such problems will involve encoding math formulas in terms of concepts and rules, as well as adding concept specific features to the learned predictors.",6.5 Analysis,[0],[0]
"The declarative rules under the Explicit Math category currently handles simple cases, this set needs to be augmented to handle complex number word problems found in algebra datasets.
",6.5 Analysis,[0],[0]
"Gains achieved by Declarative Rules Table 5 shows examples of problems which KNOWLEDGE
gets right, but UNITDEP does not.",6.5 Analysis,[0],[0]
The gains can be attributed to the injection of declarative knowledge.,6.5 Analysis,[0],[0]
Earlier systems like UNITDEP try to learn the reasoning required for these problems from the data alone.,6.5 Analysis,[0],[0]
"This is often difficult in the presence of limited data, and noisy output from NLP tools.",6.5 Analysis,[0],[0]
"In contrast, we learn probabilistic models for interpretable functions like coreference, hyponymy, etc., and then use declarative knowledge involving these functions to perform reasoning.",6.5 Analysis,[0],[0]
"This considerably reduces the complexity of the target function to be learnt, and hence we end up with a more robust model.",6.5 Analysis,[0],[0]
Effect of Beam Size We used a beam size of 1000 in all our experiments.,6.5 Analysis,[0],[0]
"However, we found that varying the beam size does not effect the performance significantly.",6.5 Analysis,[0],[0]
Even lowering the beam size to 100 reduced performance by only 1%.,6.5 Analysis,[0],[0]
Weakness of Approach A weakness of our method is the requirement to have all relevant declarative knowledge during training.,6.5 Analysis,[0],[0]
Many of the component functions (like coreference) are learnt through latent alignments with no explicit annotations.,6.5 Analysis,[0],[0]
"If too many problems are not explained by the knowledge, the model will learn noisy alignments for the component functions.
",6.5 Analysis,[0],[0]
Table 6 shows the major categories of errors with examples.,6.5 Analysis,[0],[0]
26% of the errors are due to extraneous number detection.,6.5 Analysis,[0],[0]
"We use a set of rules based on units of numbers, to detect such irrelevant numbers.",6.5 Analysis,[0],[0]
"As a result, we fail to detect numbers which are irrelevant due to other factors, like associated entities, or associated verb.",6.5 Analysis,[0],[0]
"We can potentially expand our rule based system to detect those, or replace it by a learned module like (Roy and Roth, 2015).",6.5 Analysis,[0],[0]
"Another major source of errors is parsing of rate components,
that is, understanding “earns $46 cleaning a home” should be normalized to “46$ per home”.",6.5 Analysis,[0],[0]
"Although we learn a model for coreference function, we make several mistakes related to coreference.",6.5 Analysis,[0],[0]
"For the example in Table 6, we fail to detect the coreference between “team member” and “people”.",6.5 Analysis,[0],[0]
"In this paper, we introduce a framework for incorporating declarative knowledge in word problem solving.",7 Conclusion,[0],[0]
"Our knowledge based approach outperforms all other systems, and also learns better abstractions from biased datasets.",7 Conclusion,[0],[0]
"Given that the variability in text is much larger than the number of declarative rules that governs Math word problems, we believe that this is a good way to introduce Math knowledge to a natural language understanding system.",7 Conclusion,[0],[0]
"Consequently, future work will involve extending our approach to handle a wider range of word problems, possibly by supporting better grounding of implicit variables and including a larger number of math concepts and declarative rules.",7 Conclusion,[0],[0]
"An orthogonal exploration direction is to apply these techniques to generate summaries of financial or sports news, or generate statistics of war or gun violence deaths from news corpora.",7 Conclusion,[0],[0]
"A straightforward approach can be to augment news documents with a question asking for the required information, and treating this augmented news document as a math word problem.
",7 Conclusion,[0],[0]
Code and dataset are available at https:// github.com/CogComp/arithmetic.,7 Conclusion,[0],[0]
"This work is funded by DARPA under agreement number FA8750-13-2-0008, and a grant from the Allen Institute for Artificial Intelligence (allenai.org).",Acknowledgments,[0],[0]
"Math word problems form a natural abstraction to a range of quantitative reasoning problems, such as understanding financial news, sports results, and casualties of war.",abstractText,[0],[0]
"Solving such problems requires the understanding of several mathematical concepts such as dimensional analysis, subset relationships, etc.",abstractText,[0],[0]
"In this paper, we develop declarative rules which govern the translation of natural language description of these concepts to math expressions.",abstractText,[0],[0]
We then present a framework for incorporating such declarative knowledge into word problem solving.,abstractText,[0],[0]
"Our method learns to map arithmetic word problem text to math expressions, by learning to select the relevant declarative knowledge for each operation of the solution expression.",abstractText,[0],[0]
"This provides a way to handle multiple concepts in the same problem while, at the same time, support interpretability of the answer expression.",abstractText,[0],[0]
"Our method models the mapping to declarative knowledge as a latent variable, thus removing the need for expensive annotations.",abstractText,[0],[0]
"Experimental evaluation suggests that our domain knowledge based solver outperforms all other systems, and that it generalizes better in the realistic case where the training data it is exposed to is biased in a different way than the test data.",abstractText,[0],[0]
Mapping to Declarative Knowledge for Word Problem Solving,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2824–2829 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2824",text,[0],[0]
"Identifying entities in text is a vital component in language understanding, facilitating knowledge base construction (Riedel et al., 2013), question answering (Bordes et al., 2015), and search.",1 Introduction,[0],[0]
Identifying these entities are particularly important in biomedical data.,1 Introduction,[0],[0]
"While large scale Named Entity Recognition (NER) datasets exist in news and web data (Tjong Kim Sang and De Meulder, 2003; Hovy et al., 2006), biomedical NER datasets are typically smaller and contain only one or two types per dataset.",1 Introduction,[0],[0]
"Ultimately, we would like to identify all entity types present across the union of the label sets during inference while leveraging all the available annotations to train our models.
",1 Introduction,[0],[0]
"While one may train a single model across the union of all the datasets available, this training
procedure assumes that all labels (from the union of the tag set) are correctly annotated in every training instance – which is incorrect.",1 Introduction,[0],[0]
"On the other hand, training separate models on each available dataset does not take advantage of shared statistical strength from the multiple sources of information, and requires resolution of the conflicting predictions output by the different models.
",1 Introduction,[0],[0]
"To remedy these problems, we propose methods to train a joint model across the multiple tag-sets of the different datasets, sharing statistical strength by using a single feature encoder across datasets while respecting the incompleteness of the labels during training.",1 Introduction,[0],[0]
"Thus, our single model can take full advantage of all the available annotated resources and predict the full set of relevant types given a piece of text.
",1 Introduction,[0],[0]
"In experiments on three datasets, we show our methods outperform models that do not consider the incomplete annotations.",1 Introduction,[0],[0]
We also show that jointly training on multiple datasets improves performance further and achieves state-of-the-art performance on the Biocreative V CDR dataset.,1 Introduction,[0],[0]
"Our models build on state-of-the-art NER systems (Lample et al., 2016) based on bi-directional Long Short Term Memory (BiLSTM) feature extractors fed into a conditional random field (CRF).
",2 Model,[0],[0]
"The data consists of input sequence of tokens x = {x1, . . .",2 Model,[0],[0]
", xT } where each token is a sequence of characters xt",2 Model,[0],[0]
"= {c1, . . .",2 Model,[0],[0]
", cKt}.",2 Model,[0],[0]
"The output consists of labels for each token in the sequence y = {y1, . . .",2 Model,[0],[0]
", yT }.",2 Model,[0],[0]
"Labeling is done using the BILOU tagging scheme, following previous observations that it outperforms the BIO tagging scheme (Ratinov and Roth, 2009).",2 Model,[0],[0]
We have D such datasets of input tokens and output labels.,2 Model,[0],[0]
Our model takes a sequence of tokens from a single abstract as input.,2.1 Feature Encoder BiLSTM,[0],[0]
"Tokens are generated using byte-pair encodings (BPE) (Gage, 1994; Sennrich et al., 2016), which have recently been shown to be effective for tokenization of biological texts by addressing the issue of rare or out-of-vocabulary tokens (Verga et al., 2018).",2.1 Feature Encoder BiLSTM,[0],[0]
BPE starts from white space tokenization and breaks down the tokens further.,2.1 Feature Encoder BiLSTM,[0],[0]
"Because all of the evaluations are on the span level rather than the token level, the use of BPE does not impact any numerical performance.",2.1 Feature Encoder BiLSTM,[0],[0]
"Each token t produced from BPE is mapped to a d dimensional word embedding w.
Character level features have been shown to improve NER accuracy (Lafferty et al., 2001; Lample et al., 2016; Passos et al., 2014).",2.1 Feature Encoder BiLSTM,[0],[0]
"We encode characters in a word using another BiLSTM, similar to Lample et al. (2016), and obtain a character based embedding for every word by concatenating the last hidden state of the forward and backward character LSTM.",2.1 Feature Encoder BiLSTM,[0],[0]
We concatenate this character based embedding with the d-dimensional word embedding and input it to the word-level BiLSTM.,2.1 Feature Encoder BiLSTM,[0],[0]
"This feature representation is then projected to the label dimension L using a linear layer, giving a matrix of scores [fil] where fil is the score for predicting label l ∈",2.1 Feature Encoder BiLSTM,[0],[0]
[L] for token,2.1 Feature Encoder BiLSTM,[0],[0]
i ∈,2.1 Feature Encoder BiLSTM,[0],[0]
[T ].,2.1 Feature Encoder BiLSTM,[0],[0]
"BiLSTM-CRF models used for named entity recognition add a CRF layer (Lafferty et al., 2001)
on the output representations from the BiLSTM model described.",2.2 Conditional Random Field (CRF),[0],[0]
The CRF layer scores all possible labelings to give a probability of the correct label sequence under the model.,2.2 Conditional Random Field (CRF),[0],[0]
"Given an input sequence of tokens x = {x1, . . .",2.2 Conditional Random Field (CRF),[0],[0]
", xT }",2.2 Conditional Random Field (CRF),[0],[0]
"and the output matrix of scores [fil], the score for an output labeling y = {y1, . . .",2.2 Conditional Random Field (CRF),[0],[0]
", yT } is given by: s(x,y) = ∑T t=1",2.2 Conditional Random Field (CRF),[0],[0]
"( Ayt−1,yt + ft,yt ) , where A is an L × L matrix of parameters for transitioning between output labels.",2.2 Conditional Random Field (CRF),[0],[0]
"The CRF then generates the likelihood for the correct labeling by normalizing this score over all possible output labelings:
logP (y|x) = s(x,y)− logsumexp y′
s(x,y′) (1)
",2.2 Conditional Random Field (CRF),[0],[0]
"The log normalization term here is: logsumexp
y′ s(x,y′) = log
∑ y′ exp s(x,y ′)
where the sum goes over all possible labelings y′ of the sequence and is computed efficiently using dynamic programming (Lafferty et al., 2001).",2.2 Conditional Random Field (CRF),[0],[0]
One way to tag multiple datasets is to concatenate all the datasets with all the output labels and train a single BiLSTM-CRF model.,2.3 Tagging Multiple Datasets,[0],[0]
"However, this assumes that each text snippet is completely annotated across the label sets, which is not true.",2.3 Tagging Multiple Datasets,[0],[0]
We now discuss two models which do not make this assumption.,2.3 Tagging Multiple Datasets,[0],[0]
We first propose one simple method to get around the assumption of complete annotation – train separate CRFs for the label set of each dataset.,2.3.1 Multiple CRFs,[0],[0]
"In particular, to share statistical strengths on the input tokens, we share the BiLSTM feature encoder across the datasets but use separate CRF layers for each of the datasets.",2.3.1 Multiple CRFs,[0],[0]
"This is a multi-task learning model (Caruana, 1998) and is expected to perform better than the naive model as it no longer makes the strict assumption of complete annotation (by using separate CRFs), and shares statistical strength across datasets.",2.3.1 Multiple CRFs,[0],[0]
"However, given a new abstract to tag, this model will generate multiple possible labelings from the different CRFs.",2.3.1 Multiple CRFs,[0],[0]
"Moreover, the labelings output by the different CRFs may be inconsistent, and how to combine these multiple labelings is not obvious.",2.3.1 Multiple CRFs,[0],[0]
We propose and evaluate a simple heuristic procedure for merging the outputs of the different CRF predictions.,2.3.1 Multiple CRFs,[0],[0]
"Whenever the different CRF predictions disagree on a span
of tokens, we choose the prediction from the CRF that has higher marginal probability of predicting that span of tokens (Alg. 1 in supplementary).",2.3.1 Multiple CRFs,[0],[0]
We also propose an alternative principled approach that does not require a heuristic merging process.,2.3.2 EM Marginal CRF,[0],[0]
"In order to label D datasets with some disjoint labels, we only consider the probability of the “observed labels” and allow the “unobserved” tokens to be free.",2.3.2 EM Marginal CRF,[0],[0]
"Thus, when tagging dataset",2.3.2 EM Marginal CRF,[0],[0]
i ∈,2.3.2 EM Marginal CRF,[0],[0]
"[D], we treat the non-entity tokens as potentially taking any entity type label from any of the other datasets as well as the ‘O’ label.
",2.3.2 EM Marginal CRF,[0],[0]
For a particular input x of length T from a dataset i ∈,2.3.2 EM Marginal CRF,[0],[0]
"[D] with label set Si, let y be the gold output label.",2.3.2 EM Marginal CRF,[0],[0]
Let E ⊂,2.3.2 EM Marginal CRF,[0],[0]
[T ] be the index of tokens with any entity type label in Si and N ⊂,2.3.2 EM Marginal CRF,[0],[0]
"[T ] be the index of tokens with ‘O’ label, and let yE be the output sequence corresponding to indices in E, and similarly yN be the output sequence for indices in N .",2.3.2 EM Marginal CRF,[0],[0]
"Then, from (1), we get the likelihood Pi(yE ∪ yN|x), and a naive CRF trained on the concatenation of all the data will maximize this probability.",2.3.2 EM Marginal CRF,[0],[0]
"However, since we cannot make the complete annotation assumption, we should instead maximize only the marginal probability of the observed entities on the dataset",2.3.2 EM Marginal CRF,[0],[0]
"i, Pi(yE|x), allowing yN to take any values from the labels of the other datasets: ∪Dj 6=iSj .",2.3.2 EM Marginal CRF,[0],[0]
"Thus,
logPi(yE|x) = log ∑
yN∈∪j 6=iSj
Pi(yE,yN|x)
logPi(yE|x) = logsumexp yN∈∪j 6=iSj s(x,yE ,yN )",2.3.2 EM Marginal CRF,[0],[0]
"− logZ
where logZ is the log normalization term which is the same as in (1).",2.3.2 EM Marginal CRF,[0],[0]
"Note that since the normalization term is the same here as for a standard CRF, we can still use the same dynamic programming algorithm as for a regular CRF to compute this logZ.",2.3.2 EM Marginal CRF,[0],[0]
"Now, in order to compute the first term, we note that it is similar to the computation required to compute logZ – whereas logZ is obtained by summing over all possible output sequences, this term is obtained by summing over all possible output sequences which have indices in E fixed to the correct label and indices in N taking values from ∪j 6=iSj .",2.3.2 EM Marginal CRF,[0],[0]
"Thus, this can be computed using the same dynamic programming algorithm (Tsuboi et al., 2008), and the implementation of training this model is compatible with modern automatic differentiation libraries.",2.3.2 EM Marginal CRF,[0],[0]
"We perform experiments on two benchmark Biocreative datasets as well as the recently introduced MedMentions data (Murty et al., 2018).",3 Experimental Results,[0],[0]
Our experiments consider three types of models.,3 Experimental Results,[0],[0]
"The single CRF model naively concatenates all training datasets together and assumes complete labeling, multi CRF has a single Bi-LSTM feature encoder with a separate CRF for each dataset (Section 2.3.1), and EM CRF has a single feature encoder and a single CRF trained with EM marginalization (Section 2.3.2).",3 Experimental Results,[0],[0]
For full dataset statistics and specific implementation details see supplementary material.,3 Experimental Results,[0],[0]
"Biocreative V Chemical Disease Relation (CDR): consists of 1,500 titles and abstracts from PubMed, human annotated with chemical and disease mentions (Li et al., 2016), and has been used in previous NER evluations (Fries et al., 2017; Leaman and Lu, 2016).",3.1 Biocreative V / VI,[0],[0]
"Biocreative VI ChemProt (CP): consists of 2,432 PubMed titles and abstracts, and contains human annotated mentions of both chemicals and proteins (Krallinger et al., 2017)1.
",3.1 Biocreative V / VI,[0],[0]
Our results are shown in Table 1.,3.1 Biocreative V / VI,[0],[0]
"The top portion of the table shows models trained on single datasets, and the bottom portion shows models trained on both CDR and CP.",3.1 Biocreative V / VI,[0],[0]
"Comparing the top and bottom portions of the table, we can see that models trained on both CP and CDR outperform training on either in isolation.",3.1 Biocreative V / VI,[0],[0]
"Further, we see in the bottom section that our EM CRF outperforms the single CRF model and is generally better than the multi CRF model.",3.1 Biocreative V / VI,[0],[0]
"Weakly Labeled data The addition of weakly labeled data has been used recently to improve the performance of relation extraction systems (Peng et al., 2016; Verga et al., 2018).",3.2 Adding Additional Data,[0],[0]
"In these approaches, titles and abstracts from PubMed are annotated using Pubtator, a state of the art entity tagging and linking/normalization system (Wei et al., 2013).",3.2 Adding Additional Data,[0],[0]
"We use the same weakly labeled data from Verga et al. (2018).
Results when adding in the additional weakly labeled data is shown in Table 2.",3.2 Adding Additional Data,[0],[0]
"Our models
1To the best of our knowledge, there is no benchmark result for this dataset
improve further, outperforming the state-of-the-art TaggerOne model (Leaman and Lu, 2016).",3.2 Adding Additional Data,[0],[0]
"MedMentions (Murty et al., 2018) is a recently introduced large dataset of PubMed abstracts containing entity linked mentions of many different semantic types.",3.3 MedMentions,[0],[0]
We used this data to create an artificially extreme example where two training sets contain 9 and 10 entity types each.,3.3 MedMentions,[0],[0]
"The two type sets are fully disjoint (further details in supplementary).
",3.3 MedMentions,[0],[0]
"In Table 3, we see that the single CRF model performs very poorly in this extreme setting due to the large amount of missing annotations.",3.3 MedMentions,[0],[0]
"The multi CRF and EM CRF both perform well and come close to the performance of a single CRF trained on the full data, which is approximately twice as much annotated data.",3.3 MedMentions,[0],[0]
"Until recently, feature engineered machine learning models were the highest performing approaches to NER (Ratinov and Roth, 2009; Passos et al., 2014).",4 Related Work,[0],[0]
"More recently, neural network based approaches have become state-of-the-art (Lample et al., 2016; Strubell et al., 2017; Peters et al., 2017).",4 Related Work,[0],[0]
"In BioNLP, many highest performing systems still use engineered features fed into a CRF (Wei et al., 2015; Leaman et al., 2015; Leaman and Lu, 2016).",4 Related Work,[0],[0]
"In addition to the two datasets we explored in this work, there are several other popular bio NER datasets for chemicals (Krallinger et al., 2015), species (Wang et al., 2010), diseases
(Doğan et al., 2014), and genes (Tanabe et al., 2005).
",4 Related Work,[0],[0]
"In concurrent work, Wang et al. (2018) train a model very similar to our multi-CRF model on multiple biological NER datasets with non-fully overlapping labels.",4 Related Work,[0],[0]
"Additionally, they experiment with different ways of sharing the parameters of the BiLSTM encoder.",4 Related Work,[0],[0]
"We believe this work is complementary to ours, and in many ways deals with a simpler subset of the tasks we address.",4 Related Work,[0],[0]
"Wang et al. assumes complete labeling in each of their datasets, and does not attempt to merge the final results of the multiple CRFS.",4 Related Work,[0],[0]
"On the other hand, we focus on the problem of cohesively labeling a dataset with the joint set of the different label sets, either directly through the EM model or by the merging process of the multi-CRF model.
",4 Related Work,[0],[0]
"Our method of training via marginal likelihood is the same as Tsuboi et al. (2008), who trained CRF models for Japanese word segmentation and POS tagging where only partial annotations of sentences are available.",4 Related Work,[0],[0]
"In comparison, we use the marginal likelihood training in conjunction with state-of-the art deep learning models for NER and use it to tag across multiple disjoint labels sets.",4 Related Work,[0],[0]
We’ve introduced a method for training NER models on multiple datasets containing disjoint label sets.,5 Conclusions and Future Work,[0],[0]
"We show experimentally that this joint training improves performance and that our EM CRF methods outperform models using a single CRF.
",5 Conclusions and Future Work,[0],[0]
One interesting problem that our models do not account for is the existence of overlapping and non-continuous entity spans.,5 Conclusions and Future Work,[0],[0]
"Particularly when annotating using disjoint label sets, a token could belong to multiple entity spans from different label sets.",5 Conclusions and Future Work,[0],[0]
We are interested in investigating this problem in future work.,5 Conclusions and Future Work,[0],[0]
"This work was supported in part by the Center for Intelligent Information Retrieval and the Center for Data Science, in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction., and in part by the National Science Foundation under Grant No. IIS1514053.",Acknowledgments,[0],[0]
"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",Acknowledgments,[0],[0]
Extracting typed entity mentions from text is a fundamental component to language understanding and reasoning.,abstractText,[0],[0]
"While there exist substantial labeled text datasets for multiple subsets of biomedical entity types—such as genes and proteins, or chemicals and diseases— it is rare to find large labeled datasets containing labels for all desired entity types together.",abstractText,[0],[0]
This paper presents a method for training a single CRF extractor from multiple datasets with disjoint or partially overlapping sets of entity types.,abstractText,[0],[0]
"Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in “missing labels”.",abstractText,[0],[0]
This allows us to leverage all the available data within a single model.,abstractText,[0],[0]
"In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.",abstractText,[0],[0]
Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets,title,[0],[0]
Accurately modeling the underlying generative process of complex events is an important problem in statistical machine leaning and many related areas.,1. Introduction,[0],[0]
"Although events could be spatial and/or high-dimensional, in this paper we exclusively focus on event modeling in the temporal setup due to its dominance in real-world applications.",1. Introduction,[0],[0]
"The Poisson process is a de facto standard for its simplicity in mathematical analysis and flexibility in representing the intensity function (i.e., the event occurring rate) λ(t).",1. Introduction,[0],[0]
"Unlike traditional treatments via adopting a fixed parametric form of λ(t) (e.g., piecewise constant or the Weibull), several extensions have been introduced.",1. Introduction,[0],[0]
"The nonparametric modeling of λ(t) (e.g., the recent RKHS formulation (Flaxman et al., 2017)) can
1Seoul National University of Science & Technology, Korea 2Rutgers University, Piscataway, NJ, USA.",1. Introduction,[0],[0]
"Correspondence to: Minyoung Kim <mikim21@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
reduce the burden of deciding an appropriate form of λ(t).,1. Introduction,[0],[0]
"Another is to regard λ(t) as a random process, known as the Cox process (Cox, 1955), which is useful for accounting for uncertainty in the intensity function.
",1. Introduction,[0],[0]
"In this paper, we are particularly interested in the Cox process where two most popular ones are: the Markov modulated Poisson process (MMPP) and the Gaussian process modulated Cox process (GPCox).",1. Introduction,[0],[0]
"Popular in statistics, the MMPP considers λ(t) as a random sample (trajectory) from a continuous-time Markov chain.",1. Introduction,[0],[0]
The model has a finite set of intensity levels where the latent state at each time determines which intensity level is used at that moment.,1. Introduction,[0],[0]
The GPCox is a nonparametric Bayesian model formed by placing a Gaussian process (GP) prior on λ(t).,1. Introduction,[0],[0]
"The GPCox has received significant attention in the machine learning community for the last decade for its flexible nonparametric modeling with principled uncertainty treatment.
",1. Introduction,[0],[0]
The MMPP is good at modeling highly different intensity phases: bursty events for some intervals and rare events for others.,1. Introduction,[0],[0]
"However, there can be abrupt intensity changes between these regimes which may be unnatural in certain situations.",1. Introduction,[0],[0]
"Furthermore, for the interval under a given latent state, the model follows a constant intensity (i.e., a homogeneous process), which may limit its representational capacity.",1. Introduction,[0],[0]
"On the other hand, the GPCox encourages smooth intensity changes over time.",1. Introduction,[0],[0]
"However, the disadvantage is that the drastic intensity changes are not properly dealt with unless a highly non-smooth kernel is adopted, which can usually happen when a large amount of data is available.
",1. Introduction,[0],[0]
So the main idea in this paper is to devise a novel model that takes benefits from both models.,1. Introduction,[0],[0]
"Similar to the MMPP, we consider an underlying continuous-time Markov chain (CTMC) that generates a latent state trajectory (taking say, r different states).",1. Introduction,[0],[0]
"We incorporate r latent functions with their own GP priors, each of which serves as the intensity responsible for each of the r states.",1. Introduction,[0],[0]
"This model is thus able to model major intensity regime (possibly abrupt) changes via the CTMC dynamics, and at the same time, it also enjoys the GP’s smooth intensity modeling, non-constant within the interval under a given latent state.",1. Introduction,[0],[0]
"Our model, referred to as the Markov modulated Gaussian Cox Process (MMGCP), has richer representational power than previous two models.
",1. Introduction,[0],[0]
"Indeed, it subsumes both models as special cases: i) if the GP priors put all their masses to constant functions, then we end up with the MMPP, and ii) if r = 1 (single-state), then the model reduces to the GPCox.
",1. Introduction,[0],[0]
"In terms of time-stationarity1, the previous two models exhibit extreme characteristics.",1. Introduction,[0],[0]
"The MMPP makes λ(t) fully stationary (i.e., time independent) since the CTMC is stationary and the intensity under a given state is a constant, invariant of t.",1. Introduction,[0],[0]
"On the other hand, the GPCox builds a fully non-stationary (time-variant) λ(t) on top of the kernel function defined over t. Our MMGCP somehow aims to model a so-called semi-stationary intensity function in that the macro-scale intensity regime change is governed by the stationary CTMC dynamics, while within each regime, the intensity is modeled as a smooth time-variant function.",1. Introduction,[0],[0]
"In this sense, an ideal scenario for our model is as follows: there are r underlying candidate intensity functions {λi(t)}ri=1 where at a given time t, which of these candidates is active is determined by the stationary r-state Markov process X(t), that is, λX(t)(t).",1. Introduction,[0],[0]
Our model further imposes the GP prior on these candidate functions to account for uncertainty and grant more modeling flexibility.,1. Introduction,[0],[0]
"In the evaluations, we not only implement this scenario as a synthetic setup, but we also demonstrate on some real datasets that our MMGCP significantly outperforms the previous models.
",1. Introduction,[0],[0]
"We provide an efficient variational inference for the model which is also analytic by adopting the squared link function for the intensity, similar to that of (Lloyd et al., 2015).",1. Introduction,[0],[0]
"However, the posterior expectation over the latent state trajectory required in the variational inference, is carefully analyzed within our model to derive closed-form formulas.",1. Introduction,[0],[0]
The paper is organized as follows.,1. Introduction,[0],[0]
"After briefly discussing some background and reviewing previous two models in Sec. 2, our model is introduced in Sec. 3 with the variational inference fully described in Sec. 4.",1. Introduction,[0],[0]
The empirical evaluation on some synthetic and real-world event datasets follows in Sec. 5.,1. Introduction,[0],[0]
"We are interested in modeling events that can occur over the fixed time horizon [0, T ].",2. Background,[0],[0]
"We basically assume that the events are generated by the (inhomogeneous) Poisson process, which is fully specified by the non-negative intensity function λ :",2. Background,[0],[0]
"[0, T ]→ R+.",2. Background,[0],[0]
"It defines the event occurring rate (i.e., the probability of event occurring during the infinitesimal interval [t, t+dt) is λ(t)dt).",2. Background,[0],[0]
"Then the log-likelihood of observing the event data D = {t1, . . .",2. Background,[0],[0]
", tN} (⊂ [0, T ]) can
1For clarity, the term stationarity is used in the following sense: if there is no time dependency in the data generating model (eg, MMPP), we say that it is stationary; if the data generation process is solely dependent on the time index (eg, a non-constant deterministic intensity function), then it is non-stationary.",2. Background,[0],[0]
"As our model contains both components, we call it semi-stationary.
be written as:
logP (D|λ(·))",2. Background,[0],[0]
"= N∑
n=1
log λ(tn)− ∫ T
0
λ(t) dt. (1)
",2. Background,[0],[0]
"It is common in statistics to assume a specific parametric form for λ(t), then estimate it by the maximum likelihood criterion with (1).",2. Background,[0],[0]
"Instead, the Cox process further regards λ(t) as a random process.",2. Background,[0],[0]
Two most popular ones are briefly described below.,2. Background,[0],[0]
This model basically forms piecewise constant λ(t).,2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"Specifically there are r constant intensity levels {λ1, . . .",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
", λr}, but which level is used at a given moment is determined by the latent Markov process X :",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"[0, T ] → {1, . . .",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
", r} governed by a continuous-time Markov chain (CTMC).",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"An r-state CTMC is specified by the initial state probability πi = P (X(0) = i) for i = 1, . . .",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
", r, and the transition rate matrix Q",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"whose off-diagonal Qij (i 6= j) defines the probability rate of state change from i to j, namely
Qij = lim ∆t→0 P (X(t+ ∆t) = j|X(t) = i) ∆t .",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"(2)
Defining diagonal entries as Qii := − ∑
j 6=iQij lets the probability of staying at state i for duration h be ehQii .",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"Note that the model has no time-variant component, thus adequate for modeling stationary event data.",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"There are well-known EM learning algorithms (Asmussen et al., 1996; Ryden, 1996) for estimating the parameters of the model.",2.1. Markov Modulated Poisson Process (MMPP),[0],[0]
"The GPCox model has a latent function f(t) distributed by a Gaussian process a priori, which determines the intensity function as λ(t) = ρ(f(t)) where ρ(·) is a non-negative link function, for instance, sigmoid, exponential or square function.",2.2. Gaussian Cox Process (GPCox),[0],[0]
The posterior inference P (f(·)|D) is challenging mainly due to the integration in the likelihood function (1).,2.2. Gaussian Cox Process (GPCox),[0],[0]
"Let alone the computational overhead of evaluating the integral, one has to deal with latent function values at all inputs t ∈",2.2. Gaussian Cox Process (GPCox),[0],[0]
"[0, T ], not just those tn’s in the data D as in most conventional GP models (Rasmussen & Williams, 2006).",2.2. Gaussian Cox Process (GPCox),[0],[0]
"Accordingly some previous approaches had to resort to discretizing the time domain (Rathbun & Cressie, 1994; Møller et al., 1998; Cunningham et al., 2008).
",2.2. Gaussian Cox Process (GPCox),[0],[0]
"Recently, several sophisticated inference methods have been proposed to address this difficulty.",2.2. Gaussian Cox Process (GPCox),[0],[0]
"(Adams et al., 2009) formed a tractable MCMC dynamics by exploiting the idea of thinning-based sampling in the Poisson process.",2.2. Gaussian Cox Process (GPCox),[0],[0]
"However, its time complexity is cubic in the data size, which is often prohibitive for large-scale problems.",2.2. Gaussian Cox Process (GPCox),[0],[0]
"To deal with the scalability, (Gunter et al., 2014) proposed an alternative
thinning strategy by sampling from a non-uniform intensity process, while (Samo & Roberts, 2015) introduced inducing points within the MCMC sampler.",2.2. Gaussian Cox Process (GPCox),[0],[0]
"(Lasko, 2014) used a positively transformed intensity function for direct numerical integration and interpolation.",2.2. Gaussian Cox Process (GPCox),[0],[0]
"In parallel, (Lloyd et al., 2015) derived analytic formulation for the scalable variational inference using the square link function and the pseudo input treatment (Titsias, 2009; Dezfouli & Bonilla, 2015).",2.2. Gaussian Cox Process (GPCox),[0],[0]
In this section we describe our model that can take benefits from previous models in Sec. 2.,3. Markov Modulated Gaussian Cox Process,[0],[0]
We consider that there are r underlying latent functions devoted for representing different characteristics of the intensity function.,3. Markov Modulated Gaussian Cox Process,[0],[0]
"Denoted by f(·) := {f i(·)}ri=1, they are assumed to be independently GP distributed a priori.",3. Markov Modulated Gaussian Cox Process,[0],[0]
"That is,
P (f(·))",3. Markov Modulated Gaussian Cox Process,[0],[0]
"= P (f1(·), . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", fr(·))",3. Markov Modulated Gaussian Cox Process,[0],[0]
"= r∏
i=1
P (f i(·)) (3)
where f i(·) ∼ GP ( mi(·), ki(·, ·) ) , i = 1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", r.
To determine which of these r functions is responsible for the intensity at each time, we introduce a latent Markov process X(t), similarly as the MMPP, generated from a r-state CTMC (Q, π).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"The intensity at time t is then determined by fX(t), and we use the square link function similarly as (Lloyd et al., 2015), which leads to:
λ(t)",3. Markov Modulated Gaussian Cox Process,[0],[0]
"| f(·), X(·) =",3. Markov Modulated Gaussian Cox Process,[0],[0]
(fX(t))2.,3. Markov Modulated Gaussian Cox Process,[0],[0]
"(4)
The full joint distribution of the model can be written as:
P (D, X(·), f(·)|Θ,Ω) = (5) P (f(·)|Θ)× P (X(·)|Ω)× P (D|X(·), f(·)),
where Θ = {θm,θk} is the parameters of the mean and covariance functions of the prior GP (e.g., θk = {θik}ri=1 with θik denoting the parameters of the covariance function ki(·, ·) for f i(·)).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"The CTMC parameters are denoted by Ω = {Q, π}.",3. Markov Modulated Gaussian Cox Process,[0],[0]
Thus Θ and Ω constitute the model parameters of the MMGCP.,3. Markov Modulated Gaussian Cox Process,[0],[0]
The last two terms in the RHS of (5) correspond to the likelihood of the state trajectory under the CTMC and the data likelihood given the state trajectory and the latent functions.,3. Markov Modulated Gaussian Cox Process,[0],[0]
"To formally derive these likelihoods, it is convenient to partition the horizon [0, T ] according to a realized state trajectory X(·).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Suppose that a realization X(·) undergoes (L − 1) state changes during [0, T ].",3. Markov Modulated Gaussian Cox Process,[0],[0]
"We denote by ul the time epoch when the l-th state change occurs (l = 1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", L − 1) with u0 = 0 and uL = T for convenience.",3. Markov Modulated Gaussian Cox Process,[0],[0]
"We let sl ∈ {1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", r} be the state during the interval",3. Markov Modulated Gaussian Cox Process,[0],[0]
"[ul−1, ul), and ∆ul be the length of the interval (i.e., ∆ul = ul − ul−1).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Note that these variables {ul, sl,∆ul}l are determined by the realization X(·), and vice versa, in a one-to-one manner.
",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Looking into the likelihood of X restricted to each interval (ul−1, ul], it is composed of two steps: i) no state change during (ul−1, ul) and ii) state change from sl to sl+1",3. Markov Modulated Gaussian Cox Process,[0],[0]
right at the moment ul.,3. Markov Modulated Gaussian Cox Process,[0],[0]
"For the last interval (l = L), it only involves the step i).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"From the well-known theorems of the CTMC2, the probability of the first step is exp(∆ulQslsl), while the likelihood of the second step is Qslsl+1 .",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Combining these over l = 1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", L and including the initial state probability P (X(0) = s1) = πs1 , we have the likelihood of the state trajectory as follows.
",3. Markov Modulated Gaussian Cox Process,[0],[0]
P (X(·)|Ω) =,3. Markov Modulated Gaussian Cox Process,[0],[0]
"πs1 × L∏
l=1 e∆ulQslsl × L−1∏ l=1 Qslsl+1 .",3. Markov Modulated Gaussian Cox Process,[0],[0]
"(6)
To derive the likelihood of observingD given X(·) and f(·), we let {tl1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", tlkl} be the event times in D that fall into the interval",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Il := [ul−1, ul).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Within Il, the intensity is fixed as λsl(t) := (fsl(t))2, and applying the Poisson process likelihood gives: λsl(tl1) · · ·λsl(tlkl) exp(− ∫ Il λ
sl(t)dt).",3. Markov Modulated Gaussian Cox Process,[0],[0]
"Multiplying them over l = 1, . . .",3. Markov Modulated Gaussian Cox Process,[0],[0]
", L yields:
P (D|X, f) = ∏
1≤l≤L, n:tn∈Il
(fsl(tn))",3. Markov Modulated Gaussian Cox Process,[0],[0]
"2 × e−
∫ Il (fsl (t))2dt .",3. Markov Modulated Gaussian Cox Process,[0],[0]
(7),3. Markov Modulated Gaussian Cox Process,[0],[0]
"In this section we provide inference for the posterior distribution in our MMGCP model, specifically
P (X(·), f(·)|D,Θ,Ω).",4. Variational Inference and Learning,[0],[0]
"(8)
This inference is analytically intractable, however, we do it approximately using the recent scalable variational inference technique3",4. Variational Inference and Learning,[0],[0]
"(Titsias, 2009; Dezfouli & Bonilla, 2015; Lloyd et al., 2015; Matthews et al., 2016).",4. Variational Inference and Learning,[0],[0]
It is based on the pseudo inputs which especially plays a crucial role of making inference tractable even if one has to deal with function values for all inputs t ∈,4. Variational Inference and Learning,[0],[0]
"[0, T ].",4. Variational Inference and Learning,[0],[0]
"So we begin with the introduction of our GP notations regarding pseudo inputs.
",4. Variational Inference and Learning,[0],[0]
"We often use the superscript for indicating a specific function among the r latent functions, while the subscript for a specific time epoch or a set of time epochs at which the functions are evaluated.",4. Variational Inference and Learning,[0],[0]
"For instance, for a set of p inputs T = {t̃1, . . .",4. Variational Inference and Learning,[0],[0]
", t̃p} ⊂",4. Variational Inference and Learning,[0],[0]
"[0, T ], we denote by f iT =",4. Variational Inference and Learning,[0],[0]
"[f i(t̃1), . . .",4. Variational Inference and Learning,[0],[0]
", f i(t̃p)]",4. Variational Inference and Learning,[0],[0]
">, the p-dim vector of the i-th function
2The full derivation can be found in standard textbooks on Markov chains or stochastic ODEs such as (Anderson, 2011).",4. Variational Inference and Learning,[0],[0]
"We also provide some brief derivations in Appendix A in the supplemental materials.
",4. Variational Inference and Learning,[0],[0]
3We specifically follow the variational free energy approach.,4. Variational Inference and Learning,[0],[0]
"But we would like to note that there exist other approximation techniques where the readers are encouraged to refer to the recent work on comparison of different approaches (Bauer et al., 2016) and some unified view (Bui et al., 2017).
values.",4. Variational Inference and Learning,[0],[0]
"The boldfaced fT indicates the collection of the function values for all r functions, that is, fT = {f1T , . . .",4. Variational Inference and Learning,[0],[0]
", frT }.",4. Variational Inference and Learning,[0],[0]
"For the GP prior mean and covariance functions (3), we follow the similar convention: miT",4. Variational Inference and Learning,[0],[0]
=,4. Variational Inference and Learning,[0],[0]
"[m i(t̃1), . . .",4. Variational Inference and Learning,[0],[0]
",m i(t̃p)]",4. Variational Inference and Learning,[0],[0]
> is the p-dim vector of the i-th mean function values on T .,4. Variational Inference and Learning,[0],[0]
"For two input sets T and S, KiT ,S denotes the (|T | × |S|) kernel matrix by applying ki(·, ·) on (T × S).
",4. Variational Inference and Learning,[0],[0]
"For each i-th GP (i = 1, . . .",4. Variational Inference and Learning,[0],[0]
", r), we assume that there areMi ( N ) pseudo inputs denoted by Zi = {zi1, . . .",4. Variational Inference and Learning,[0],[0]
", ziMi} ⊂",4. Variational Inference and Learning,[0],[0]
"[0, T ].",4. Variational Inference and Learning,[0],[0]
We also let Z = ⋃r i=1Zi.,4. Variational Inference and Learning,[0],[0]
These pseudo inputs can be thought of as representative points in that knowing the function values at Z has significant impacts on inferring function values at the other input points.,4. Variational Inference and Learning,[0],[0]
"But further insights can be found in the nice survey (Quiñonero-Candela & Rasmussen, 2005).",4. Variational Inference and Learning,[0],[0]
"The pseudo inputs can also be learned from data along with the model parameters, but for the time being we assume that they are fixed4.
",4. Variational Inference and Learning,[0],[0]
"We denote the whole state trajectory and the function values of all r latent functions for the entire set [0, T ] as (infinite dimensional) X and f , respectively.",4. Variational Inference and Learning,[0],[0]
"We define a tractable form of the variational density q(X, f), and optimize it to approximate the true posterior (8) as much as possible.",4. Variational Inference and Learning,[0],[0]
"In defining q(·), we impose independence betweenX and f for computational tractability.",4. Variational Inference and Learning,[0],[0]
"First, we let the posterior distribution of X follows a CTMC, which allows analytic derivations feasible as will be shown below.",4. Variational Inference and Learning,[0],[0]
"Also the posterior of the latent functions f are assumed to be Gaussians factorized over i = 1, . . .",4. Variational Inference and Learning,[0],[0]
", r. Furthermore, we force the conditional density q(f |fZ) to coincide with the prior P (f |fZ) exactly, which is crucial to have some difficult terms canceled out in the KL divergence objective, making the inference scalable (Titsias, 2009; Lloyd et al., 2015).",4. Variational Inference and Learning,[0],[0]
"In summary, our variational density is defined as:
q(X, f) = q(X;C,α)× ∫ q(fZ)P (f |fZ) dfZ (9)
where C is the (r × r) transition rate matrix and α is the (1× r) initial state probabilities for the CTMC q(X).",4. Variational Inference and Learning,[0],[0]
"Also,
q(fZ) =",4. Variational Inference and Learning,[0],[0]
r∏ i=1,4. Variational Inference and Learning,[0],[0]
N,4. Variational Inference and Learning,[0],[0]
"(f iZi ;µ i,Σi).",4. Variational Inference and Learning,[0],[0]
"(10)
Here µi is theMi-dim mean vector and Σi is the (Mi×Mi) covariance matrix.",4. Variational Inference and Learning,[0],[0]
"The variational parameters are denoted as Λ := (C,α,µ := {µi}ri=1,Σ := {Σi}ri=1).
",4. Variational Inference and Learning,[0],[0]
"We aim to minimize the KL divergence between q(·) and the posterior (8), which can be written as:
KL ( q(X, f)||P (X, f |D) )",4. Variational Inference and Learning,[0],[0]
"= logP (D)− ELBO(Θ,Ω,Λ), (11) where the ELBO (evidence lower-bound) is defined as:
ELBO(Θ,Ω,Λ) =",4. Variational Inference and Learning,[0],[0]
"Eq(X,f) [ logP (D|X, f) ]",4. Variational Inference and Learning,[0],[0]
"−
KL ( q(X)||P (X) )",4. Variational Inference and Learning,[0],[0]
− KL ( q(fZ)||P (fZ) ) .,4. Variational Inference and Learning,[0],[0]
"(12)
4We often use the uniformly sampled points from [0, T ].
",4. Variational Inference and Learning,[0],[0]
"From (11) and the fact that KL divergence is non-negative, the ELBO is the lower bound of the log-evidence, namely
logP (D|Θ,Ω) ≥ ELBO(Θ,Ω,Λ).",4. Variational Inference and Learning,[0],[0]
"(13)
Note that the bounding gap in (13) is exactly the KL divergence between q(·) and the posterior.",4. Variational Inference and Learning,[0],[0]
"Thus increasing ELBO(Θ,Ω,Λ) wrt Λ leads to a better variational density (closer to the posterior), whereas increasing it wrt the model parameters (Θ,Ω) can potentially5 improve the data evidence score of the model.",4. Variational Inference and Learning,[0],[0]
"Hence, maximizing the ELBO wrt all the parameters can achieve both variational inference (i.e., q(·) optimization) and model selection (i.e, learning prior model parameters) simultaneously.
",4. Variational Inference and Learning,[0],[0]
"In what follows, we provide full derivations for evaluating each term comprising (12).",4. Variational Inference and Learning,[0],[0]
"The gradients are also required for the optimization of the ELBO, and can be found in Appendix C in the supplemental material.",4. Variational Inference and Learning,[0],[0]
4.1.,4. Variational Inference and Learning,[0],[0]
"KL ( q(fZ)||P (fZ)
)",4. Variational Inference and Learning,[0],[0]
"It is not difficult to see that due to the fully factorized q(fZ) and P (fZ) over individual latent functions i = 1, . . .",4. Variational Inference and Learning,[0],[0]
", r, the KL divergence is the sum of the individual Gaussian KL divergences.",4. Variational Inference and Learning,[0],[0]
"More specifically,
KL ( q(fZ)||P (fZ) )",4. Variational Inference and Learning,[0],[0]
"=
r∑ i=1",4. Variational Inference and Learning,[0],[0]
"1 2
[ log ∣∣KiZi,Zi∣∣∣∣Σi∣∣ −Mi + Tr((KiZi,Zi)−1Σi) +",4. Variational Inference and Learning,[0],[0]
(µi −miZi) >,4. Variational Inference and Learning,[0],[0]
"(KiZi,Zi)−1(µi −miZi) ] (14)
The gradients with respect to the related parameters are derived in Appendix C.1.",4. Variational Inference and Learning,[0],[0]
4.2.,4. Variational Inference and Learning,[0],[0]
"KL ( q(X)||P (X)
)",4. Variational Inference and Learning,[0],[0]
This term involves computing the expectations of the loglikelihoods of the CTMC models (both the prior logP (X) and the variational posterior log q(X)) with respect to q(X).,4. Variational Inference and Learning,[0],[0]
We describe how to compute Eq(X)[P (X)] analytically (Eq(X)[q(X)] done similarly).,4. Variational Inference and Learning,[0],[0]
"For this purpose, we rephrase the CTMC likelihood in (6) using some total statistics from the realization X(·).",4. Variational Inference and Learning,[0],[0]
"With X(·) fixed, let nij be the number of transitions from state i to j where j 6=",4. Variational Inference and Learning,[0],[0]
"i, and ∆i be the sojourn time at state",4. Variational Inference and Learning,[0],[0]
"i, that is, ∆i = ∑",4. Variational Inference and Learning,[0],[0]
"l:sl=i
∆ul.",4. Variational Inference and Learning,[0],[0]
"Note that (nij ,∆i) are the functions ofX(·).",4. Variational Inference and Learning,[0],[0]
"Then we have:
logP (X) = (15) r∑
i=1
( I{X(0)=i} log πi + ∆iQii + ∑ j 6=i nij logQij ) ,
5However, this does not guarantee to improve the evidence logP (D) since the inequality (13) is not tight.
where I{p} is 1 (0) if the predicate p is true (false).
",4. Variational Inference and Learning,[0],[0]
Thus the expectation of (15) requires: Eq[nij ] and Eq[∆i].,4. Variational Inference and Learning,[0],[0]
"For the latter, we first note that ∆i = ∫ T 0 I{X(t)=i}dt.",4. Variational Inference and Learning,[0],[0]
Using q(X(t) = i) =,4. Variational Inference and Learning,[0],[0]
"[αetC ]i from the CTMC theorems (see (4) in Appendix A), we have:
Eq(X)[∆i] =",4. Variational Inference and Learning,[0],[0]
"[αJC ]i, (16) where JC = ∫ T 0 etCdt, is the (r × r) matrix by integrating the matrix exponential over [0, T ], and [v]i indicates the i-th element of the vector",4. Variational Inference and Learning,[0],[0]
v. As the number of transitions nij = ∫ T 0,4. Variational Inference and Learning,[0],[0]
"I{X(t)=i AND X(t+dt)=j}, and using q(X(t) = i,X(t+ dt) = j) =",4. Variational Inference and Learning,[0],[0]
"[αetC ]iCijdt ((5) in Appendix A),
Eq(X)[nij ] =",4. Variational Inference and Learning,[0],[0]
[αJC ]iCij .,4. Variational Inference and Learning,[0],[0]
"(17)
",4. Variational Inference and Learning,[0],[0]
"By applying these to (15), we finally have: KL ( q(X)||P (X) )",4. Variational Inference and Learning,[0],[0]
"= r∑ i=1
{ αi log
αi πi + (18)
",4. Variational Inference and Learning,[0],[0]
"[αJC ]i ( Cii −Qii + ∑ j 6=i Cij log Cij Qij )} .
",4. Variational Inference and Learning,[0],[0]
The remaining thing is how to compute JC .,4. Variational Inference and Learning,[0],[0]
It can be done analytically once the matrix C is diagonalized.,4. Variational Inference and Learning,[0],[0]
The details are found in Appendix B of the supplemental material.,4. Variational Inference and Learning,[0],[0]
"Since r is usually small (e.g., 2 or 3), diagonalization must not incur any computational or numerical issues.",4. Variational Inference and Learning,[0],[0]
"When we compute the gradients of (18) with respect to C, special techniques of taking derivatives of matrix exponentials such as (Kalbfleisch & Lawless, 1985) can be used.",4. Variational Inference and Learning,[0],[0]
The technical details are described in Appendix B and Appendix C.2.,4. Variational Inference and Learning,[0],[0]
4.3.,4. Variational Inference and Learning,[0],[0]
"Eq(X,f) [ logP (D|X, f) ]
",4. Variational Inference and Learning,[0],[0]
"This is the conditional log-likelihood given the state trajectory and the latent functions, expected with respect to the variational posterior q(X, f).",4. Variational Inference and Learning,[0],[0]
"From (7), after slight rephrasing, the conditional log-likelihood can be written as:
logP (D|X, f) = r∑
i=1",4. Variational Inference and Learning,[0],[0]
N∑ n=1 I{X(tn)=i} log (f i(tn)),4. Variational Inference and Learning,[0],[0]
"2
− r∑
i=1
∫ T 0 I{X(t)=i}(f i(t))2dt.",4. Variational Inference and Learning,[0],[0]
"(19)
Exploiting the factorization q(X, f) = q(X)q(f), we take the expectation of (19) wrt q(X) first, followed by q(f).",4. Variational Inference and Learning,[0],[0]
Using q(X(t) = i) =,4. Variational Inference and Learning,[0],[0]
"[αetC ]i from the previous section, Eq(X,f) [ logP (D|X, f) ] =
r∑ i=1",4. Variational Inference and Learning,[0],[0]
"( ELLi − ENOi ) where (20)
ELLi = N∑
n=1
[αetnC ]iEq(fi(tn))",4. Variational Inference and Learning,[0],[0]
"[ log (f i(tn)) 2 ] , (21)
ENOi = ∫ T
0
[αetC ]iEq(fi(t))",4. Variational Inference and Learning,[0],[0]
"[ (f i(t))2 ] dt. (22)
Note that (21) and (22) are very similar to those in the variational inference of the GPCox model proposed in (Lloyd et al., 2015).",4. Variational Inference and Learning,[0],[0]
"However, we have the weighted expected log-likelihood by the weights [αetC ]i over i = 1, . . .",4. Variational Inference and Learning,[0],[0]
", r, determined by the latent state posterior probabilities q(X(t)).",4. Variational Inference and Learning,[0],[0]
Computing these weights for each t can be done analytically when we have a diagonalization ofC,4. Variational Inference and Learning,[0],[0]
(See Appendix B in the supplemental material).,4. Variational Inference and Learning,[0],[0]
"The expectations in (21) and (22) are with respect to Gaussians, more specifically, q(f i(t)))",4. Variational Inference and Learning,[0],[0]
"=∫ q(f iZi)P (f i(t)|f iZi)df i Zi = N (µ̃i(t), σ̃ 2 i (t)) where
µ̃i(t) = m i(t) +Kit,Zi(K i Zi,Zi) −1(µi −miZi), (23) σ̃2i (t) =",4. Variational Inference and Learning,[0],[0]
"K i t,t −Kit,Zi(K i Zi,Zi) −1KiZi,t +
Kit,Zi(K i Zi,Zi) −1Σi(KiZi,Zi) −1KiZi,t. (24)
",4. Variational Inference and Learning,[0],[0]
"Then the expectation in (22) equals (µ̃i(t))2 + σ̃2i (t), which allows us to compute the integral analytically for a certain kernel form (e.g., squared exponential or polynomial kernel) as shown in (Lloyd et al., 2015).",4. Variational Inference and Learning,[0],[0]
"With the additional weight term [αetC ]i multiplied to the integrand in our model, by rewriting the weight as a sum of scalar exponentials after diagonalization of C, we can still derive a closed-form expression for ENOi.",4. Variational Inference and Learning,[0],[0]
"However, it is highly complicated, which becomes even worse when evaluating its gradients (e.g., wrt C).",4. Variational Inference and Learning,[0],[0]
"For the expectation of the log-squared term of ELLi in (21), some confluent hyper-geometric function was adopted in (Lloyd et al., 2015), however, it is either numerically unstable or based on certain interpolation.
",4. Variational Inference and Learning,[0],[0]
"Instead, we employ fairly straightforward strategies for computing ELLi and ENOi.",4. Variational Inference and Learning,[0],[0]
"First, the expectation of the logsquared term in (21) is done by the Monte-Carlo estimation.",4. Variational Inference and Learning,[0],[0]
This must not incur much computational overhead since it is univariate sampling.,4. Variational Inference and Learning,[0],[0]
"Considering that we have to take derivatives of ELLi wrt the parameters related to q(f i(tn)), we also adopt the re-parametrized Gaussian sampling technique as suggested in (Kingma & Welling, 2014).",4. Variational Inference and Learning,[0],[0]
"The idea is to express the random samples from q(f i(tn)), denoted by f in (s) for s = 1, . . .",4. Variational Inference and Learning,[0],[0]
", S, as:
f in (s) = µ̃i(tn)+(σ̃ 2 i (tn)) 1/2 (s) in ,
(s) in ∼ N (0, 1).",4. Variational Inference and Learning,[0],[0]
"(25)
After sampling (s)in , we fix them, and ELLi is estimated as:
N∑ n=1",4. Variational Inference and Learning,[0],[0]
[αetnC ],4. Variational Inference and Learning,[0],[0]
i S S∑ s=1 log ( µ̃i(tn) + (σ̃ 2,4. Variational Inference and Learning,[0],[0]
i (tn)) 1/2 (s) in )2 .,4. Variational Inference and Learning,[0],[0]
"(26)
",4. Variational Inference and Learning,[0],[0]
"As it separates randomness ( (s)in ) from the parameters, the gradient of (26) can be computed straightforwardly while yielding an unbiased estimate of the gradient of the original (21).",4. Variational Inference and Learning,[0],[0]
See Appendix C.3 for the full derivations.,4. Variational Inference and Learning,[0],[0]
"Furthermore, to reduce the variance of the estimate, one can use the Rao-Blackwellization technique (Casella & Robert, 1996).
",4. Variational Inference and Learning,[0],[0]
"For the integration in (22), we do this numerically by uniform grid sampling.",4. Variational Inference and Learning,[0],[0]
"Specifically, by having G uniform grid points {t̃g}Gg=1 over [0, T ] with ∆t = t̃g+1 − t̃g , we define the following statistics:
Ψi0 = G∑",4. Variational Inference and Learning,[0],[0]
"g=1 wigK i Zi,t̃gK i t̃g,Zi , Ψ i 1 = G∑ g=1 wigm i(t̃g)K",4. Variational Inference and Learning,[0],[0]
"i t̃g,Zi ,
Ψi2 = G∑ g=1 wig(m i(t̃g)) 2, Ψi3 = G∑",4. Variational Inference and Learning,[0],[0]
g=1 wigK,4. Variational Inference and Learning,[0],[0]
"i t̃g,t̃g , (27)
where wig =",4. Variational Inference and Learning,[0],[0]
[αe t̃gC ]i∆t.,4. Variational Inference and Learning,[0],[0]
"We then numerically compute ENOi as:
(µi −miZi) >(KiZi,Zi) −1Ψi0(K i Zi,Zi) −1(µi −miZi) + + 2Ψi1(K i Zi,Zi) −1(µi −miZi)−",4. Variational Inference and Learning,[0],[0]
"Tr ( (KiZi,Zi) −1Ψi0 )
+",4. Variational Inference and Learning,[0],[0]
"Tr ( (KiZi,Zi) −1Σi(KiZi,Zi) −1Ψi0 ) + Ψi2 + Ψ",4. Variational Inference and Learning,[0],[0]
i 3.,4. Variational Inference and Learning,[0],[0]
"(28)
",4. Variational Inference and Learning,[0],[0]
"In this way the gradient of ENOi can be derived fairly easily, which is summarized in Appendix C.3.
",4. Variational Inference and Learning,[0],[0]
"When the optimization is done by first-order gradient methods, the computational complexity of the variational inference for our model is no more than r (the number of latent GP functions) times that of the variational inference of the GPCox as in (Lloyd et al., 2015), which can be seen as a special MMGCP model with r = 1.",4. Variational Inference and Learning,[0],[0]
"We discuss how to determine the optimal value of r. The ELBO objective, the lower bound of the data log-likelihood, tends to increase as we increase r since models with higher r naturally subsume those with lower.",4.4. Model Selection and Test Prediction,[0],[0]
"However, it would incur higher chance of overfitting and worse generalization on unseen test data.",4.4. Model Selection and Test Prediction,[0],[0]
"We need to trade off between the model complexity and the goodness of data fitting, and along this line one can employ certain information criteria such as the Bayesian criterion (Schwarz, 1978).",4.4. Model Selection and Test Prediction,[0],[0]
"When specifying the model complexity, we take into consideration all related parameters as well as the inducing points.",4.4. Model Selection and Test Prediction,[0],[0]
"Alternatively, we can choose r by cross validation, measuring performance on a validation set, randomly held-out portion of the training data.",4.4. Model Selection and Test Prediction,[0],[0]
"Once the model and the variational parameters are learned, we can estimate the predictive likelihood for an unseen test dataD∗.",4.4. Model Selection and Test Prediction,[0],[0]
"We see that the predictive log-likelihood, logP (D∗|D,Θ,Ω) is lower-bounded by Eq(X,f)",4.4. Model Selection and Test Prediction,[0],[0]
"[ logP (D∗|X, f ,Θ,Ω) ] , which can be computed by the exactly same procedures as in Sec. 4.3 with D∗.",4.4. Model Selection and Test Prediction,[0],[0]
In this section we demonstrate the performance of the proposed MMGCP model.,5. Evaluations,[0],[0]
"We mainly compare our model with two existing extreme stationarity models, MMPP and
GPCox, since our model is motivated from both.",5. Evaluations,[0],[0]
"For the GPCox, among several inference strategies, we opt for the latest variational inference method proposed in (Lloyd et al., 2015), which exhibits comparable generalization performance to other approaches while being significantly faster than sampling-based methods such as (Adams et al., 2009).",5. Evaluations,[0],[0]
"As a baseline, we also consider: i) the classical kernel smoothing (KS) approach (Diggle, 1985), specifically the Gaussian kernel density estimator, and ii) the log Gaussian Cox process (LGCP) (Rathbun & Cressie, 1994; Møller et al., 1998), which approximates the problem as a standard GP inference with Poisson-likelihood iid data via event counting through discretization of the time horizon.",5. Evaluations,[0],[0]
"To demonstrate the effectiveness of the proposed MMGCP model, we devise three different synthetic data setups that exhibit highly different aspects in terms of time stationarity.
",5.1. Synthetic data,[0],[0]
"The first setup simulates a fully stationary scenario (denoted by Full-Stn), where we generate data from a r = 3- state MMPP model with highly different intensity levels {1.0, 4.0, 8.0}.",5.1. Synthetic data,[0],[0]
"Within the time horizon T = 50, we generate 10 event sequences from the model (Fig. 1 for two exemplar sequences), from which we randomly take 5 sequences as training data while the rest as a test set.",5.1. Synthetic data,[0],[0]
"For the MMPP and our MMGCP models, we choose the model order by cross validation, which both correctly recovered r = 3 hidden states.",5.1. Synthetic data,[0],[0]
"For the GPCox and our model, we use the squared exponential kernels, and the variational inference in both models uses the same M = 10 pseudo inputs (also the same across i = 1, . . .",5.1. Synthetic data,[0],[0]
", r for the MMGCP).
",5.1. Synthetic data,[0],[0]
The average test log-likelihoods are shown in Table 1(A).,5.1. Synthetic data,[0],[0]
"As expected, the MMPP model attains the best performance since the model structure exactly matches that of the data generating one.",5.1. Synthetic data,[0],[0]
"Our MMGCP, although a generalization of MMPP, performs worse than the MMPP due to the use of smooth kernels.",5.1. Synthetic data,[0],[0]
"However, the MMGCP significantly outperforms the GPCox and other non-stationary models.",5.1. Synthetic data,[0],[0]
The figures in the parentheses indicate the p-values from the paired sample t-test for the competing models against our MMGCP.,5.1. Synthetic data,[0],[0]
"Thus the differences between existing models and ours are all statistically significant (p-values less than 0.05).
",5.1. Synthetic data,[0],[0]
The second synthetic dataset represents fully non-stationary intensity setup (denoted by Non-Stn).,5.1. Synthetic data,[0],[0]
"From (Adams et al., 2009)",5.1. Synthetic data,[0],[0]
we take λ(t) = 2 exp(−t/15) + exp(−((t,5.1. Synthetic data,[0],[0]
− 25)/10)2),5.1. Synthetic data,[0],[0]
"as the true (deterministic) intensity function over [0, 50], and generate data from the inhomogeneous Poisson process.",5.1. Synthetic data,[0],[0]
See Fig. 2 for the true intensity function and a sample event sequence.,5.1. Synthetic data,[0],[0]
"With the similar experimental setups as the first dataset, we run the five models and report the test scores in Table 1(B).",5.1. Synthetic data,[0],[0]
"The MMPP, with r = 3 hidden states chosen, now underperforms the non-stationary time-dependent intensity modeling methods with statistical significance.",5.1. Synthetic data,[0],[0]
The GPCox and our MMGCP perform comparably well.,5.1. Synthetic data,[0],[0]
"The MMGCP selects r = 2 hidden states by the cross validation although r = 1 (i.e., GPCox) yields a slightly smaller but very close validation score than that of r = 2.",5.1. Synthetic data,[0],[0]
"This implies that the smooth intensity change is properly represented by the covariance functions of the Gaussian processes.
",5.1. Synthetic data,[0],[0]
"In this Non-Stn dataset, since we have the true intensity function available, we can measure the distance between
the estimated (expected) intensity functions and the true one.",5.1. Synthetic data,[0],[0]
"We use the L2 error defined as ∫ T 0
(λtrue(t)−λ(t))2 dt, where λ(t) = E[λ(t)|D] is the posterior-expected intensity function.",5.1. Synthetic data,[0],[0]
"In our MMGCP model, as we have the posterior approximation q(X, f), using λ(t)",5.1. Synthetic data,[0],[0]
"≈ Eq[(fX(t))2] we have:
λ(t)",5.1. Synthetic data,[0],[0]
"≈ r∑
i=1
",5.1. Synthetic data,[0],[0]
[αetC ]i ( (µ̃i(t)) 2 + σ̃2i (t) ) .,5.1. Synthetic data,[0],[0]
"(29)
",5.1. Synthetic data,[0],[0]
The L2 errors of the competing methods are reported in Table 2.,5.1. Synthetic data,[0],[0]
See also Fig. 2 for the estimated intensity functions.,5.1. Synthetic data,[0],[0]
"Our MMGCP and the GPCox exhibit the best performance.
",5.1. Synthetic data,[0],[0]
"For the last synthetic setup, we aim to simulate the semistationary scenario (denoted by Semi-Stn).",5.1. Synthetic data,[0],[0]
"We consider two underlying candidate intensity functions as follows:
λ1(t) = 2e−t/30 +G25(t) + 2G50(t) + 3.5G85(t) + 4
3 ,
λ2(t) = 1
3 (1.8 sin(0.005t2) + 2), (30)
where Ga(t) = e−((t−a)/10) 2
.",5.1. Synthetic data,[0],[0]
"As shown in Fig. 3, they exhibit highly different patterns and levels from each other.",5.1. Synthetic data,[0],[0]
We also incorporate a 2-state CTMC so that which of the two candidate functions is active at each moment is determined stochastically by the latent Markov process.,5.1. Synthetic data,[0],[0]
"We generate event sequences over the horizon T = 100 from the model.
",5.1. Synthetic data,[0],[0]
We follow the experimental setup similar to the previous two datasets.,5.1. Synthetic data,[0],[0]
"The test log-likelihood scores of the compet-
ing approaches are summarized in Table 1(C).",5.1. Synthetic data,[0],[0]
"In this case, our MMGCP is outstanding for this semi-stationary data.",5.1. Synthetic data,[0],[0]
"The superiority of the MMGCP to competing methods is statistically significant whereas the fully stationary MMPP and the time-dependent inhomogeneous models like GPCox and kernel smoother suffer from the heterogeneity of data: globally undergoing stationary regime switching but being time-dependent within each regime.
",5.1. Synthetic data,[0],[0]
"Overall, our MMGCP is viable consistently across all different time stationarity setups, ranging from fully stationary to non-stationary as well as semi-stationary in between.",5.1. Synthetic data,[0],[0]
"In the following sections, we also demonstrate the effectiveness of our model on some real-world event datasets.",5.1. Synthetic data,[0],[0]
We test on the football events dataset6 from the Kaggle open data platform.,5.2. Football Data,[0],[0]
There are 9074 football games as a whole collected from major European leagues for 5 years (from 2011/12 season to 2016/17).,5.2. Football Data,[0],[0]
"For each game, the major events (e.g., shot attempts, goals, corners, fouls, etc.) are marked in the minute scale.",5.2. Football Data,[0],[0]
"The types and times of the events are obtained from various sources, mainly text commentary and web scraping.
",5.2. Football Data,[0],[0]
"From the dataset, we consider the events of shot attempts only, and focus on those games which contain 30 or more events, which comprise about 2000 games.",5.2. Football Data,[0],[0]
"Each game is represented as a sequence of events, and we regard each sequence as an iid sample from an unknown process within the horizon [0, T ] with T = 90+αminutes where α amounts to the random extra time which is usually less than 5 (minutes).",5.2. Football Data,[0],[0]
"The average number of events per sequence is 33.4 with standard deviation 3.4.
",5.2. Football Data,[0],[0]
"Among these sequences, we randomly select 500 sequences for training and 100 as a test set.",5.2. Football Data,[0],[0]
The test likelihood scores are summarized in Table 3(A).,5.2. Football Data,[0],[0]
"It shows that the proposed MMGCP outperforms the competing models with statistical significance (the p-values with regard to our MMGCP are all
6https://www.kaggle.com/secareanualin/football-events
less than 10−4).",5.2. Football Data,[0],[0]
"The improvement achieved by the proposed approach can be attributed to the semi-stationary nature of the data in some sense: the event rates can be time dependent in certain regimes (e.g., there are often more active attack attempts during the beginning/end of the game or the half time than in the middle of the game), but overall intensities tend to be stationary, exhibiting highly different aspects from game to game.",5.2. Football Data,[0],[0]
We next demonstrate the performance of the proposed approach on the daily earthquake data publicly available from the Kaggle open data platform.,5.3. Italy’s Earthquakes Data,[0],[0]
"The dataset7 is obtained by real-time collections of the earthquake events from the Italian Earthquakes National Center, which contains earthquake records of various magnitudes that hit the center of Italy for three months, from August to November in 2016.",5.3. Italy’s Earthquakes Data,[0],[0]
"As we are interested in the daily patterns, we group them on a daily basis, and regard the events for each day as an iid sequence sample.",5.3. Italy’s Earthquakes Data,[0],[0]
"There are 99 (daily) event sequences for which we split them randomly into 60/39 training/test sets.
",5.3. Italy’s Earthquakes Data,[0],[0]
"We consider all the events with the Richter magnitude no less than 2.0, where the magnitude 2.0 corresponds to earthquakes that are minor, but felt by some people.",5.3. Italy’s Earthquakes Data,[0],[0]
The number of events per sequence is highly varying across sequences: the mean is 81.7 and the standard deviation 107.5.,5.3. Italy’s Earthquakes Data,[0],[0]
"We also scale the event times from the original data down to [0, 100].",5.3. Italy’s Earthquakes Data,[0],[0]
The test results are shown in Table 3(B).,5.3. Italy’s Earthquakes Data,[0],[0]
The MMGCP again exhibits significantly better generalization capability than models based on the extreme time stationarity assumptions.,5.3. Italy’s Earthquakes Data,[0],[0]
"Considering the complexity of the underlying event generating process for this data (e.g., time-sensitive factors as well as stationary changes of states), it signifies that the MMGCP’s added flexibility attained by combining inhomogeneous Poisson process with the latent regime switching to account for major trend changes, can be highly effective for representing a complex event process.",5.3. Italy’s Earthquakes Data,[0],[0]
In this paper we have proposed a novel Markov modulated Gaussian Cox process model that incorporates both the GP-based smooth intensity changes along with major regime switches through a hidden Markov process.,6. Conclusion,[0],[0]
"While subsuming existing stationary and non-stationary Cox process models as special cases, the proposed model is especially suitable for representing semi-stationary event data.",6. Conclusion,[0],[0]
"Through empirical evaluations on both synthetic and realworld datasets, we have demonstrated that the model is promising, yielding better generalization for complex event data modeling than existing approaches.
",6. Conclusion,[0],[0]
7https://www.kaggle.com/blackecho/italy-earthquakes/data,6. Conclusion,[0],[0]
This research is supported by National Research Foundation of Korea (NRF-2016R1A1A1A05921948).,Acknowledgements,[0],[0]
The author thanks Mark Schmidt for helpful discussions where part of this work was conducted when the author was at UBC.,Acknowledgements,[0],[0]
The Cox process is a flexible event model that can account for uncertainty of the intensity function in the Poisson process.,abstractText,[0],[0]
"However, previous approaches make strong assumptions in terms of time stationarity, potentially failing to generalize when the data do not conform to the assumed stationarity conditions.",abstractText,[0],[0]
"In this paper we bring up two most popular Cox models representing two extremes, and propose a novel semi-stationary Cox process model that can take benefits from both models.",abstractText,[0],[0]
Our model has a set of Gaussian process latent functions governed by a latent stationary Markov process where we provide analytic derivations for the variational inference.,abstractText,[0],[0]
"Empirical evaluations on several synthetic and real-world events data including the football shot attempts and daily earthquakes, demonstrate that the proposed model is promising, can yield improved generalization performance over existing approaches.",abstractText,[0],[0]
Markov Modulated Gaussian Cox Processes forSemi-Stationary Intensity Modeling of Events Data,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2083–2093 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2083",text,[0],[0]
"Regular expressions (REs) are widely used in various natural language processing (NLP) tasks like pattern matching, sentence classification, sequence labeling, etc.",1 Introduction,[0],[0]
"(Chang and Manning, 2014).",1 Introduction,[0],[0]
"As a technique based on human-crafted rules, it is concise, interpretable, tunable, and does not rely on much training data to generate.",1 Introduction,[0],[0]
"As such, it is commonly used in industry, especially when the available training examples are limited – a problem known as few-shot learning (GC et al., 2015).
",1 Introduction,[0],[0]
"While powerful, REs have a poor generalization ability because all synonyms and variations in a RE must be explicitly specified.",1 Introduction,[0],[0]
"As a result, REs are often ensembled with data-driven methods, such as neural network (NN) based techniques, where a set of carefully-written REs are
used to handle certain cases with high precision, leaving the rest for data-driven methods.
",1 Introduction,[0],[0]
We believe the use of REs can go beyond simple pattern matching.,1 Introduction,[0],[0]
"In addition to being a separate classifier to be ensembled, a RE also encodes a developer’s knowledge for the problem domain.",1 Introduction,[0],[0]
"The knowledge could be, for example, the informative words (clue words) within a RE’s surface form.",1 Introduction,[0],[0]
"We argue that such information can be utilized by data-driven methods to achieve better prediction results, especially in few-shot learning.
",1 Introduction,[0],[0]
"This work investigates the use of REs to improve NNs – a learning framework that is widely used in many NLP tasks (Goldberg, 2017).",1 Introduction,[0],[0]
The combination of REs and a NN allows us to exploit the conciseness and effectiveness of REs and the strong generalization ability of NNs.,1 Introduction,[0],[0]
"This also provides us an opportunity to learn from various kinds of REs, since NNs are known to be good at tolerating noises (Xie et al., 2016).
",1 Introduction,[0],[0]
This paper presents novel approaches to combine REswith a NN at different levels.,1 Introduction,[0],[0]
"At the input layer, we propose to use the evaluation outcome of REs as the input features of a NN (Sec.3.2).",1 Introduction,[0],[0]
"At the network module level, we show how to exploit the knowledge encoded in REs to guide the attention mechanism of a NN (Sec. 3.3).",1 Introduction,[0],[0]
"At the output layer, we combine the evaluation outcome of a RE with the NN output in a learnable manner (Sec. 3.4).
",1 Introduction,[0],[0]
"We evaluate our approach by applying it to two spoken language understanding (SLU) tasks, namely intent detection and slot filling, which respectively correspond to two fundamental NLP tasks: sentence classification and sequence labeling.",1 Introduction,[0],[0]
"To demonstrate the usefulness of REs in realworld scenarios where the available number of annotated data can vary, we explore both the fewshot learning setting and the one with full training data.",1 Introduction,[0],[0]
"Experimental results show that our approach is highly effective in utilizing the available
annotated data, yielding significantly better learning performance over the RE-unaware method.
",1 Introduction,[0],[0]
Our contributions are as follows.,1 Introduction,[0],[0]
(1) We present the first work to systematically investigate methods for combining REs with NNs.,1 Introduction,[0],[0]
(2) The proposed methods are shown to clearly improve the NN performance in both the few-shot learning and the full annotation settings.,1 Introduction,[0],[0]
(3) We provide a set of guidance on how to combine REswith NNs and RE annotation.,1 Introduction,[0],[0]
"In this paper, we use italic for emphasis like intent detection, the Courier typeface for abbreviations like RE, bold italic for the first appearance of a concept like clue words, Courier surrounded by / for regular expressions like /list( the)? AIRLINE/, and underlined italic for words of sentences in our dataset like Boston.",2.1 Typesetting,[0],[0]
Our work targets two SLU tasks: intent detection and slot filling.,2.2 Problem Definition,[0],[0]
"The former is a sentence classification task where we learn a function to map an input sentence of n words, x =",2.2 Problem Definition,[0],[0]
"[x1, ..., xn], to a corresponding intent label, c. The latter is a sequence labeling task for which we learn a function to take in an input query sentence of n words, x =",2.2 Problem Definition,[0],[0]
"[x1, ..., xn], to produce a corresponding labeling sequence, y =",2.2 Problem Definition,[0],[0]
"[y1, ..., yn], where yi is the slot label of the corresponding word, xi.
Take the sentence in Fig. 1 as an example.",2.2 Problem Definition,[0],[0]
"A successful intent detector would suggest the intent of the sentence as flight, i.e., querying about flight-related information.",2.2 Problem Definition,[0],[0]
"A slot filler, on the other hand, should identify the slots fromloc.city and toloc.city by labeling Boston and Miami, respectively, using the begin-inside-outside (BIO) scheme.",2.2 Problem Definition,[0],[0]
"In this work, a RE defines a mapping from a text pattern to several REtags which are the same as
or related to the target labels (i.e., intent and slot labels).",2.3 The Use of Regular Expressions,[0],[0]
"A search function takes in a RE, applies it to all sentences, and returns any texts that match the pattern.",2.3 The Use of Regular Expressions,[0],[0]
"We then assign the REtag (s) (that are associated with the matching RE) to either the matched sentence (for intent detection) or some matched phrases (for slot filling).
",2.3 The Use of Regular Expressions,[0],[0]
"Specifically, our REtags for intent detection are the same as the intent labels.",2.3 The Use of Regular Expressions,[0],[0]
"For example, in Fig. 1, we get a REtag of flight that is the same as the intent label flight.
",2.3 The Use of Regular Expressions,[0],[0]
"For slot filling, we use two different sets of REs.",2.3 The Use of Regular Expressions,[0],[0]
"Given the group functionality of RE, we can assign REtags to our interested RE groups (i.e., the expressions defined inside parentheses).",2.3 The Use of Regular Expressions,[0],[0]
The translation from REtags to slot labels depends on how the corresponding REs are used.,2.3 The Use of Regular Expressions,[0],[0]
"(1) When REs are used at the network module level (Sec. 3.3), the corresponding REtags are the same as the target slot labels.",2.3 The Use of Regular Expressions,[0],[0]
"For instance, the slot RE in Fig. 1 will assign fromloc.city to the first RE group and toloc.city to the second one.",2.3 The Use of Regular Expressions,[0],[0]
"Here, CITY is a list of city names, which can be replaced with a RE string like /Boston|Miami|LA|.../.",2.3 The Use of Regular Expressions,[0],[0]
"(2) If REs are used in the input (Sec. 3.2) and the output layers (Sec. 3.4) of a NN, the corresponding REtag would be different from the target slot labels.",2.3 The Use of Regular Expressions,[0],[0]
"In this context, the two RE groups in Fig. 1 would be simply tagged as city to capture the commonality of three related target slot labels: fromloc.city, toloc.city, stoploc.city.",2.3 The Use of Regular Expressions,[0],[0]
Note that we could use the target slot labels as REtags for all the settings.,2.3 The Use of Regular Expressions,[0],[0]
The purpose of abstracting REtags to a simplified version of the target slot labels here is to show that REs can still be useful when their evaluation outcome does not exactly match our learning objective.,2.3 The Use of Regular Expressions,[0],[0]
"Further, as shown in Sec. 4.2, using simplified REtags can also make the development of REs easier in our tasks.
",2.3 The Use of Regular Expressions,[0],[0]
"Intuitively, complicated REs can lead to better performance but require more efforts to generate.",2.3 The Use of Regular Expressions,[0],[0]
"Generally, there are two aspects affecting RE complexity most: the number of RE groups1 and or clauses (i.e., expressions separated by the disjunction operator |) in a RE group.",2.3 The Use of Regular Expressions,[0],[0]
"Having a larger number of RE groups often leads to better
1 When discussing complexity, we consider each semantically independent consecutive word sequence as a RE group (excluding clauses, such as \w+, that can match any word).",2.3 The Use of Regular Expressions,[0],[0]
"For instance, the RE: /how long( \w+){1,2}?",2.3 The Use of Regular Expressions,[0],[0]
"(it take|flight)/ has two RE groups: (how long) and (it take|flight).
precision but lower coverage on pattern matching, while a larger number of or clauses usually gives a higher coverage but slightly lower precision.",2.3 The Use of Regular Expressions,[0],[0]
"As depicted in Fig. 2, we propose to combine NNs and REs from three different angles.",3 Our Approach,[0],[0]
"We use the Bi-directional LSTM (BLSTM) as our base NNmodel because it is effective in both intent detection and slot filling (Liu and Lane, 2016).
",3.1 Base Models,[0],[0]
Intent Detection.,3.1 Base Models,[0],[0]
"As shown in Fig. 2, the BLSTM takes as input the word embeddings",3.1 Base Models,[0],[0]
"[x1, ..., xn] of a n-word sentence, and produces a vector hi for each word i.",3.1 Base Models,[0],[0]
"A self-attention layer then takes in the vectors produced by the BLSTM to compute the sentence embedding s:
s = ∑ i αihi, αi = exp(hᵀi Wc)∑",3.1 Base Models,[0],[0]
"i exp(h ᵀ i Wc)
(1)
where αi is the attention for word",3.1 Base Models,[0],[0]
"i, c is a randomly initialized trainable vector used to select informative words for classification, and W is a weight matrix.",3.1 Base Models,[0],[0]
"Finally, s is fed to a softmax classifier for intent classification.
",3.1 Base Models,[0],[0]
Slot Filling.,3.1 Base Models,[0],[0]
The model for slot filling is straightforward – the slot label prediction is generated by a softmax classier which takes in the BLSTM’s output hi and produces the slot label of word i. Note that attention aggregation in Fig. 2 is only employed by the network module level method presented in Sec. 3.3.,3.1 Base Models,[0],[0]
"At the input level, we use the evaluation outcomes of REs as features which are fed to NN models.
",3.2 Using REs at the Input Level,[0],[0]
Intent Detection.,3.2 Using REs at the Input Level,[0],[0]
Our REtag for intent detection is the same as our target intent label.,3.2 Using REs at the Input Level,[0],[0]
"Because real-world REs are unlikely to be perfect, one sentence may be matched by more than one RE.",3.2 Using REs at the Input Level,[0],[0]
This may result in several REtags that are conflict with each other.,3.2 Using REs at the Input Level,[0],[0]
"For instance, the sentence list the Delta airlines flights to Miami can match a RE: /list( the)?",3.2 Using REs at the Input Level,[0],[0]
AIRLINE/,3.2 Using REs at the Input Level,[0],[0]
"that outputs tag airline, and another RE: /list( \w+){0,3} flights?/ that outputs tag flight.
",3.2 Using REs at the Input Level,[0],[0]
"To resolve the conflicting situations illustrated above, we average the randomly initialized trainable tag embeddings to form an aggregated embedding as the NN input.",3.2 Using REs at the Input Level,[0],[0]
"There are two ways to
use the aggregated embedding.",3.2 Using REs at the Input Level,[0],[0]
"We can append the aggregated embedding to either the embedding of every input word, or the input of the softmax classifier (see 1 in Fig. 2(a)).",3.2 Using REs at the Input Level,[0],[0]
"To determine which strategy works best, we perform a pilot study.",3.2 Using REs at the Input Level,[0],[0]
"We found that the first method causes the tag embedding to be copied many times; consequently, the NN tends to heavily rely on the REtags, and the resulting performance is similar to the one given by using REs alone in few-shot settings.",3.2 Using REs at the Input Level,[0],[0]
"Thus, we adopt the second approach.
",3.2 Using REs at the Input Level,[0],[0]
Slot Filling.,3.2 Using REs at the Input Level,[0],[0]
"Since the evaluation outcomes of slot REs are word-level tags, we can simply embed and average the REtags into a vector fi for each word, and append it to the corresponding word embedding wi (as shown in 1 in Fig. 2(b)).",3.2 Using REs at the Input Level,[0],[0]
"Note that we also extend the slot REtags into the BIO format, e.g., the REtags of phrase New York are B-city and I-city if its original tag is city.",3.2 Using REs at the Input Level,[0],[0]
"At the network module level, we explore ways to utilize the clue words in the surface form of a RE (bold blue arrows and words in 2 of Fig. 2) to guide the attention module in NNs.
",3.3 Using REs at the Network Module Level,[0],[0]
Intent Detection.,3.3 Using REs at the Network Module Level,[0],[0]
"Taking the sentence in Fig. 1 for example, the RE: /ˆflights?",3.3 Using REs at the Network Module Level,[0],[0]
from/ that leads to intent flight means that flights from are the key words to decide the intent flight.,3.3 Using REs at the Network Module Level,[0],[0]
"Therefore, the attention module in NNs should leverage these two words to get the correct prediction.",3.3 Using REs at the Network Module Level,[0],[0]
"To this end, we extend the base intent model by making two changes to incorporate the guidance from REs.
",3.3 Using REs at the Network Module Level,[0],[0]
"First, since each intent has its own clue words, using a single sentence embedding for all intent labels would make the attention less focused.",3.3 Using REs at the Network Module Level,[0],[0]
"Therefore, we let each intent label k use different attention ak, which is then used to generate the sentence embedding sk for that intent:
sk = ∑ i αkihi, αki = exp(hᵀi Wack)∑ i exp(h ᵀ i Wack) (2)
where ck is a trainable vector for intent k which is used to compute attention ak, hi is the BLSTM output for word",3.3 Using REs at the Network Module Level,[0],[0]
"i, and Wa is a weight matrix.
",3.3 Using REs at the Network Module Level,[0],[0]
"The probability pk that the input sentence expresses intent k is computed by:
pk = exp(logitk)∑ k exp(logitk)
, logitk = wksk + bk (3)
where wk, logitk, bk are weight vector, logit, and bias for intent k, respectively.
",3.3 Using REs at the Network Module Level,[0],[0]
"Second, apart from indicating a sentence for intent k (positive REs), a RE can also indicate that a sentence does not express intent k (negative REs).",3.3 Using REs at the Network Module Level,[0],[0]
"We thus use a new set of attention (negative attentions, in contrast to positive attentions), to compute another set of logits for each intent with Eqs. 2 and 3.",3.3 Using REs at the Network Module Level,[0],[0]
"We denote the logits computed by positive attentions as logitpk, and those by negative attentions as logitnk, the final logit for intent k can then be calculated as:
logitk = logitpk − logitnk (4)
To use REs to guide attention, we add an attention loss to the final loss:
lossatt = ∑",3.3 Using REs at the Network Module Level,[0],[0]
"k ∑ i tki log(αki) (5)
where tki is set to 0 when none of the matched REs (that leads to intent k) marks word i as a clue word – otherwise tki is set to 1/lk, where lk is the number of clue words for intent",3.3 Using REs at the Network Module Level,[0],[0]
k,3.3 Using REs at the Network Module Level,[0],[0]
"(if no matched RE leads to intent k, then tk∗ = 0).",3.3 Using REs at the Network Module Level,[0],[0]
"We use Eq. 5 to compute the positive attention loss, lossatt p, for positive REs and negative attention loss, lossatt n, for negative ones.",3.3 Using REs at the Network Module Level,[0],[0]
"The final loss is computed as:
loss = lossc + βplossatt p + βnlossatt n",3.3 Using REs at the Network Module Level,[0],[0]
"(6)
where lossc is the original classification loss, βp and βn are weights for the two attention losses.
",3.3 Using REs at the Network Module Level,[0],[0]
Slot Filling.,3.3 Using REs at the Network Module Level,[0],[0]
The two-side attention (positive and negative attention) mechanism introduced for intent prediction is unsuitable for slot filling.,3.3 Using REs at the Network Module Level,[0],[0]
"Because for slot filling, we need to compute attention for each word, which demands more compu-
tational and memory resources than doing that for intent detection2.
",3.3 Using REs at the Network Module Level,[0],[0]
"Because of the aforementioned reason, we use a simplified version of the two-side attention, where all the slot labels share the same set of positive and negative attention.",3.3 Using REs at the Network Module Level,[0],[0]
"Specifically, to predict the slot label of word i, we use the following equations, which are similar to Eq. 1, to generate a sentence embedding spi with regard to word i from positive attention:
spi = ∑ j αpijhj , αpij = exp(hᵀjWsphi)∑ j exp(h ᵀ jWsphi)
(7) where hi and hj are the BLSTM outputs for word i and j respectively, Wsp is a weight matrix, and αpij is the positive attention value for word j with respect to word i. Further, by replacing Wsp with Wsn, we use Eq. 7 again to compute negative attention and generate the corresponding sentence embedding sni.
",3.3 Using REs at the Network Module Level,[0],[0]
"Finally, the prediction pi for word i can be calculated as:
pi = softmax((Wp[spi;hi]",3.3 Using REs at the Network Module Level,[0],[0]
"+ bp) −(Wn[sni;hi] + bn))
(8)
where Wp, Wn, bp, bn are weight matrices and bias vectors for positive and negative attention, respectively.",3.3 Using REs at the Network Module Level,[0],[0]
Here we append the BLSTM output hi to spi and sni because the word i itself also plays a crucial part in identifying its slot label.,3.3 Using REs at the Network Module Level,[0],[0]
"At the output level, REs are used to amend the output of NNs.",3.4 Using REs at the Output Level,[0],[0]
"At this level, we take the same
2Since we need to assign a label to each word, if we still compute attention for each slot label, we will have to compute 2× L× n2 attention values for one sentence.",3.4 Using REs at the Output Level,[0],[0]
"Here, L is the number of tags and n is the sentence length.",3.4 Using REs at the Output Level,[0],[0]
"The BIO tagging format will further double the number of tags.
",3.4 Using REs at the Output Level,[0],[0]
"approach used for intent detection and slot filling (see 3 in Fig. 2).
",3.4 Using REs at the Output Level,[0],[0]
"As mentioned in Sec. 2.3, the slot REs used in the output level only produce a simplified version of target slot labels, for which we can further annotate their corresponding target slot labels.",3.4 Using REs at the Output Level,[0],[0]
"For instance, a RE that outputs city can lead to three slot labels: fromloc.city, toloc.city, stoploc.city.
",3.4 Using REs at the Output Level,[0],[0]
"Let zk be a 0-1 indicator of whether there is at least one matched RE that leads to target label k (intent or slot label), the final logits of label k for a sentence (or a specific word for slot filling) is:
logitk = logit ′",3.4 Using REs at the Output Level,[0],[0]
"k + wkzk (9)
where logit′k is the logit produced by the original NN, and wk is a trainable weight indicating the overall confidence for REs that lead",3.4 Using REs at the Output Level,[0],[0]
to target label k.,3.4 Using REs at the Output Level,[0],[0]
"Here we do not assign a trainable weight for each RE because it is often that only a few sentences match a RE.
",3.4 Using REs at the Output Level,[0],[0]
"We modify the logit instead of the final probability because a logit is an unconstrained real value, which matches the property of wkzk better than probability.",3.4 Using REs at the Output Level,[0],[0]
"Actually, when performing model ensemble, ensembling with logits is often empirically better than with the final probability3.",3.4 Using REs at the Output Level,[0],[0]
This is also the reason why we choose to operate on logits in Sec. 3.3.,3.4 Using REs at the Output Level,[0],[0]
Our experiments aim to answer three questions: Q1: Does the use of REs enhance the learning quality when the number of annotated instances is small?,4 Evaluation Methodology,[0],[0]
Q2: Does the use of REs still help when using the full training data?,4 Evaluation Methodology,[0],[0]
Q3: How can we choose from different combination methods?,4 Evaluation Methodology,[0],[0]
"We use the ATIS dataset (Hemphill et al., 1990) to evaluate our approach.",4.1 Datasets,[0],[0]
This dataset is widely used in SLU research.,4.1 Datasets,[0],[0]
"It includes queries of flights, meal, etc.",4.1 Datasets,[0],[0]
"We follow the setup of Liu and Lane (2016) by using 4,978 queries for training and 893 for testing, with 18 intent labels and 127 slot labels.",4.1 Datasets,[0],[0]
We also split words like Miami’s into Miami ’s during the tokenization phase to reduce the number of words that do not have a pre-trained word embedding.,4.1 Datasets,[0],[0]
"This strategy is useful for fewshot learning.
",4.1 Datasets,[0],[0]
"3 An example can be found in the ensemble version that Juan et al. (2016) used in the Avazu Kaggle competition.
",4.1 Datasets,[0],[0]
"To answer Q1 , we also exploit the full few-shot learning setting.",4.1 Datasets,[0],[0]
"Specifically, for intent detection, we randomly select 5, 10, 20 training instances for each intent to form the few-shot training set; and for slot filling, we also explore 5, 10, 20 shots settings.",4.1 Datasets,[0],[0]
"However, since a sentence typically contains multiple slots, the number of mentions of frequent slot labels may inevitably exceeds the target shot count.",4.1 Datasets,[0],[0]
"To better approximate the target shot count, we select sentences for each slot label in ascending order of label frequencies.",4.1 Datasets,[0],[0]
That is k1-shot dataset will contain k2-shot dataset if k1 > k2.,4.1 Datasets,[0],[0]
"All settings use the original test set.
",4.1 Datasets,[0],[0]
"Since most existing few-shot learning methods require either many few-shot classes or some classes with enough data for training, we also explore the partial few-shot learning setting for intent detection to provide a fair comparison for existing few-shot learning methods.",4.1 Datasets,[0],[0]
"Specifically, we let the 3 most frequent intents have 300 training instances, and the rest remains untouched.",4.1 Datasets,[0],[0]
"This is also a common scenario in real world, where we often have several frequent classes and many classes with limited data.",4.1 Datasets,[0],[0]
"As for slot filling, however, since the number of mentions of frequent slot labels already exceeds the target shot count, the original slot filling few-shot dataset can be directly used to train existing few-shot learning methods.",4.1 Datasets,[0],[0]
"Therefore, we do not distinguish full and partial few-shot learning for slot filling.",4.1 Datasets,[0],[0]
We use the syntax of REs in Perl in this work.,4.2 Preparing REs,[0],[0]
Our REs are written by a paid annotator who is familiar with the domain.,4.2 Preparing REs,[0],[0]
"It took the annotator in total less than 10 hours to develop all the REs, while a domain expert can accomplish the task faster.",4.2 Preparing REs,[0],[0]
"We use the 20-shot training data to develop the REs, but word lists like cities are obtained from the full training set.",4.2 Preparing REs,[0],[0]
The development of REs is considered completed when the REs can cover most of the cases in the 20-shot training data with resonable precision.,4.2 Preparing REs,[0],[0]
"After that, the REs are fixed throughout the experiments.
",4.2 Preparing REs,[0],[0]
The majority of the time for writing the REs is proportional to the number of RE groups.,4.2 Preparing REs,[0],[0]
It took about 1.5 hours to write the 54 intent REs with on average 2.2 groups per RE.,4.2 Preparing REs,[0],[0]
"It is straightforward to write the slot REs for the input and output level methods, for which it took around 1 hour to write the 60 REs with 1.7 groups on average.",4.2 Preparing REs,[0],[0]
"By con-
trast, writing slot REs to guide attention requires more efforts as the annotator needs to carefully select clue words and annotate the full slot label.",4.2 Preparing REs,[0],[0]
"As a result, it took about 5.5 hours to generate 115 REswith on average 3.3 groups.",4.2 Preparing REs,[0],[0]
"The performance of the REs can be found in the last line of Table 1.
",4.2 Preparing REs,[0],[0]
"In practice, a positive RE for intent (or slot) k can often be treated as negative REs for other intents (or slots).",4.2 Preparing REs,[0],[0]
"As such, we use the positive REs for intent (or slot) k",4.2 Preparing REs,[0],[0]
as the negative REs for other intents (or slots) in our experiments.,4.2 Preparing REs,[0],[0]
Hyper-parameters.,4.3 Experimental Setup,[0],[0]
Our hyper-parameters for the BLSTM are similar to the ones used by Liu and Lane (2016).,4.3 Experimental Setup,[0],[0]
"Specifically, we use batch size 16, dropout probability 0.5, and BLSTM cell size 100.",4.3 Experimental Setup,[0],[0]
The attention loss weight is 16 (both positive and negative) for full few-shot learning settings and 1 for other settings.,4.3 Experimental Setup,[0],[0]
"We use the 100d GloVe word vectors (Pennington et al., 2014) pre-trained on Wikipedia and Gigaword (Parker et al., 2011), and the Adam optimizer (Kingma and Ba, 2014) with learning rate 0.001.
",4.3 Experimental Setup,[0],[0]
Evaluation Metrics.,4.3 Experimental Setup,[0],[0]
"We report accuracy and macro-F1 for intent detection, and micro/macroF1 for slot filling.",4.3 Experimental Setup,[0],[0]
Micro/macro-F1 are the harmonic mean of micro/macro precision and recall.,4.3 Experimental Setup,[0],[0]
"Macro-precision/recall are calculated by averaging precision/recall of each label, and microprecision/recall are averaged over each prediction.
",4.3 Experimental Setup,[0],[0]
Competitors and Naming Conventions.,4.3 Experimental Setup,[0],[0]
"Here, a bold Courier typeface like BLSTM denotes the notations of the models that we will compare in Sec. 5.
",4.3 Experimental Setup,[0],[0]
"Specifically, we compare our methods with the baseline BLSTMmodel (Sec. 3.1).",4.3 Experimental Setup,[0],[0]
"Since our attention loss method (Sec. 3.3) uses two-side attention, we include the raw two-side attention model without attention loss (+two) for comparison as well.",4.3 Experimental Setup,[0],[0]
"Besides, we also evaluate the RE output (REO), which uses the REtags as prediction directly, to show the quality of the REs that we will use in the experiments.4
As for our methods for combinging REs with NN, +feat refers to using REtag as input features (Sec. 3.2), +posi and +neg refer to using positive and negative attention loss respectively,
4 For slot filling, we evaluate the REs that use the target slot labels as REtags.
",4.3 Experimental Setup,[0],[0]
"+both refers to using both postive and negative attention losses (Sec. 3.3), and +logitmeans using REtag to modify NN output (Sec. 3.4).
",4.3 Experimental Setup,[0],[0]
"Moverover, since the REs can also be formatted as first-order-logic (FOL) rules, we also compare our methods with the teacher-student framework proposed by Hu et al. (2016a), which is a general framework for distilling knowledge from FOL rules into NN (+hu16).",4.3 Experimental Setup,[0],[0]
"Besides, since we consider few-short learning, we also include the memory module proposed by Kaiser et al. (2017), which performs well in various few-shot datasets (+mem)5.",4.3 Experimental Setup,[0],[0]
"Finally, the state-of-art model on the ATIS dataset is also included (L&L16), which jointly models the intent detection and slot filling in a single network (Liu and Lane, 2016).",4.3 Experimental Setup,[0],[0]
"To answer Q1 , we first explore the full few-shot learning scenario.
",5.1 Full Few-Shot Learning,[0],[0]
Intent Detection.,5.1 Full Few-Shot Learning,[0],[0]
"As shown in Table 1, except for 5-shot, all approaches improve the baseline BLSTM.",5.1 Full Few-Shot Learning,[0],[0]
Our network-module-level methods give the best performance because our attention module directly receives signals from the clue words in REs that contain more meaningful information than the REtag itself used by other methods.,5.1 Full Few-Shot Learning,[0],[0]
"We also observe that since negative REs are derived from positive REs with some noises, posi performs better than neg when the amount of available data is limited.",5.1 Full Few-Shot Learning,[0],[0]
"However, neg is slightly better in 20-shot, possibly because negative REs significantly outnumbers the positive ones.",5.1 Full Few-Shot Learning,[0],[0]
"Besides, two alone works better than the BLSTM when there are sufficient data, confirming the advantage of our two-side attention architecture.
",5.1 Full Few-Shot Learning,[0],[0]
"As for other proposed methods, the output level method (logit) works generally better than the input level method (feat), except for the 5-shot case.",5.1 Full Few-Shot Learning,[0],[0]
We believe this is due to the fewer number of RE related parameters and the shorter distance that the gradient needs to travel from the loss to these parameters – both make logit easier to train.,5.1 Full Few-Shot Learning,[0],[0]
"However, since logit directly modifies the output, the final prediction is more sensitive to the insufficiently trained weights in logit, leading to the inferior results in the 5-shot setting.
",5.1 Full Few-Shot Learning,[0],[0]
"5 We tune C and π0 of hu16, and choose (0.1, 0.3) for intent, and (1, 0.3) for slot.",5.1 Full Few-Shot Learning,[0],[0]
"We tune memory-size and k of mem, and choose (1024, 64) for intent, and (2048, 64) for slot.
",5.1 Full Few-Shot Learning,[0],[0]
"To compare with existing methods of combining NN and rules, we also implement the teacherstudent network (Hu et al., 2016a).",5.1 Full Few-Shot Learning,[0],[0]
"This method lets the NN learn from the posterior label distribution produced by FOL rules in a teacher-student framework, but requires considerable amounts of data.",5.1 Full Few-Shot Learning,[0],[0]
"Therefore, although both hu16 and logit operate at the output level, logit still performs better than hu16 in these few-shot settings, since logit is easier to train.
",5.1 Full Few-Shot Learning,[0],[0]
"It can also be seen that starting from 10-shot, two+both significantly outperforms pure REO.",5.1 Full Few-Shot Learning,[0],[0]
"This suggests that by using our attention loss to connect the distributional representation of the NN and the clue words of REs, we can generalize RE patterns within a NN architecture by using a small amount of annotated data.
",5.1 Full Few-Shot Learning,[0],[0]
Slot Filling.,5.1 Full Few-Shot Learning,[0],[0]
"Different from intent detection, as shown in Table 1, our attention loss does not work for slot filling.",5.1 Full Few-Shot Learning,[0],[0]
"The reason is that the slot label of a target word (the word for which we are trying to predict a slot label) is decided mainly by the semantic meaning of the word itself, together with 0- 3 phrases in the context to provide supplementary information.",5.1 Full Few-Shot Learning,[0],[0]
"However, our attention mechanism can only help in recognizing clue words in the context, which is less important than the word itself and have already been captured by the BLSTM, to some extent.",5.1 Full Few-Shot Learning,[0],[0]
"Therefore, the attention loss and the attention related parameters are more of a burden than a benefit.",5.1 Full Few-Shot Learning,[0],[0]
"As is shown in Fig. 1, the model recognizes Boston as fromloc.city mainly because Boston itself is a city, and its context word from may have already been captured by the BLSTM and our attention mechanism does not help much.",5.1 Full Few-Shot Learning,[0],[0]
"By examining the attention values of +two trained on the full dataset, we find that instead of mark-
ing informative context words, the attention tends to concentrate on the target word itself.",5.1 Full Few-Shot Learning,[0],[0]
"This observation further reinforces our hypothesis on the attention loss.
",5.1 Full Few-Shot Learning,[0],[0]
"On the other hand, since the REtags provide extra information, such as type, about words in the sentence, logit and feat generally work better.",5.1 Full Few-Shot Learning,[0],[0]
"However, different from intent detection, feat only outperforms logit by a margin.",5.1 Full Few-Shot Learning,[0],[0]
"This is because feat can use the REtags of all words to generate better context representations through the NN, while logit can only utilize the REtag of the target word before the final output layer.",5.1 Full Few-Shot Learning,[0],[0]
"As a result, feat actually gathers more information from REs and can make better use of them than logit.",5.1 Full Few-Shot Learning,[0],[0]
"Again, hu16 is still outperformed by logit, possibly due to the insufficient data support in this few-shot scenario.",5.1 Full Few-Shot Learning,[0],[0]
"We also see that even the BLSTM outperforms REO in 5-shot, indicating while it is hard to write high-quality RE patterns, using REs to boost NNs is still feasible.
",5.1 Full Few-Shot Learning,[0],[0]
Summary.,5.1 Full Few-Shot Learning,[0],[0]
The amount of extra information that a NN can utilize from the combined REs significantly affects the resulting performance.,5.1 Full Few-Shot Learning,[0],[0]
"Thus, the attention loss methods work best for intent detection and feat works best for slot filling.",5.1 Full Few-Shot Learning,[0],[0]
We also see that the improvements from REs decreases as having more training data.,5.1 Full Few-Shot Learning,[0],[0]
This is not surprising because the implicit knowledge embedded in the REs are likely to have already been captured by a sufficient large annotated dataset and in this scenario using the REs will bring in fewer benefits.,5.1 Full Few-Shot Learning,[0],[0]
"To better understand the relationship between our approach and existing few-shot learning methods, we also implement the memory network method
(Kaiser et al., 2017) which achieves good results in various few-shot datasets.",5.2 Partial Few-Shot Learning,[0],[0]
"We adapt their opensource code, and add their memory module (mem) to our BLSTM model.
",5.2 Partial Few-Shot Learning,[0],[0]
"Since the memory module requires to be trained on either many few-shot classes or several classes with extra data, we expand our full few-shot dataset for intent detection, so that the top 3 intent labels have 300 sentences (partial few-shot).
",5.2 Partial Few-Shot Learning,[0],[0]
"As shown in Table 2, mem works better than BLSTM, and our attention loss can be further combined with the memory module (mem+posi), with even better performance.",5.2 Partial Few-Shot Learning,[0],[0]
"hu16 also works here, but worse than two+both.",5.2 Partial Few-Shot Learning,[0],[0]
"Note that, the memory module requires the input sentence to have only one embedding, thus we only use one set of positive attention for combination.
",5.2 Partial Few-Shot Learning,[0],[0]
"As for slot filling, since we already have extra data for frequent tags in the original few-shot data (see Sec. 4.1), we use them directly to run the memory module.",5.2 Partial Few-Shot Learning,[0],[0]
"As shown in the bottom of Table 1, mem also improves the base BLSTM, and gains further boost when it is combined with feat6.",5.2 Partial Few-Shot Learning,[0],[0]
"To answer Q2, we also evaluate our methods on the full dataset.",5.3 Full Dataset,[0],[0]
"As seen in Table 3, for intent detection, while two+both still works, feat and logit no longer give improvements.",5.3 Full Dataset,[0],[0]
"This shows
6For compactness, we only combine the best method in each task with mem, but others can also be combined.
",5.3 Full Dataset,[0],[0]
"that since both REtag and annotated data provide intent labels for the input sentence, the value of the extra noisy tag from RE become limited as we have more annotated data.",5.3 Full Dataset,[0],[0]
"However, as there is no guidance on attention in the annotations, the clue words from REs are still useful.",5.3 Full Dataset,[0],[0]
"Further, since feat concatenates REtags at the input level, the powerful NN makes it more likely to overfit than logit, therefore feat performs even worse when compared to the BLSTM.
",5.3 Full Dataset,[0],[0]
"As for slot filling, introducing feat and logit can still bring further improvements.",5.3 Full Dataset,[0],[0]
This shows that the word type information contained in the REtags is still hard to be fully learned even when we have more annotated data.,5.3 Full Dataset,[0],[0]
"Moreover, different from few-shot settings, two+both has a better macro-F1 score than the BLSTM for this task, suggesting that better attention is still useful when the base model is properly trained.
",5.3 Full Dataset,[0],[0]
"Again, hu16 outperforms the BLSTM in both tasks, showing that although the REtags are noisy, their teacher-student network can still distill useful information.",5.3 Full Dataset,[0],[0]
"However, hu16 is a general framework to combine FOL rules, which is more indirect in transferring knowledge from rules to NN than our methods.",5.3 Full Dataset,[0],[0]
"Therefore, it is still inferior to attention loss in intent detection and feat in slot filling, which are designed to combine REs.
",5.3 Full Dataset,[0],[0]
"Further, mem generally works in this setting, and can receive further improvement by combining our fusion methods.",5.3 Full Dataset,[0],[0]
"We can also see that two+both works clearly better than the stateof-art method (L&L16) in intent detection, which jointly models the two tasks.",5.3 Full Dataset,[0],[0]
And mem+feat is comparative to L&L16 in slot filling.,5.3 Full Dataset,[0],[0]
We now discuss how the RE complexity affects the performance of the combination.,5.4 Impact of the RE Complexity,[0],[0]
We choose to control the RE complexity by modifying the number of groups.,5.4 Impact of the RE Complexity,[0],[0]
"Specifically, we reduce the number of groups for existing REs to decrease RE complexity.",5.4 Impact of the RE Complexity,[0],[0]
"To mimic the process of writing simple
REs from scratch, we try our best to keep the key RE groups.",5.4 Impact of the RE Complexity,[0],[0]
"For intent detection, all the REs are reduced to at most 2 groups.",5.4 Impact of the RE Complexity,[0],[0]
"As for slot filling, we also reduce the REs to at most 2 groups, and for some simples case, we further reduce them into word-list patterns, e.g., ( CITY).
",5.4 Impact of the RE Complexity,[0],[0]
"As shown in Table 4, the simple REs already deliver clear improvements to the base NN models, which shows the effectiveness of our methods, and indicates that simple REs are quite costefficient since these simple REs only contain 1-2 RE groups and thus very easy to produce.",5.4 Impact of the RE Complexity,[0],[0]
We can also see that using complex REs generally leads to better results compared to using simple REs.,5.4 Impact of the RE Complexity,[0],[0]
"This indicates that when considering using REs to improve a NN model, we can start with simple REs, and gradually increase the RE complexity to improve the performance over time7.",5.4 Impact of the RE Complexity,[0],[0]
"Our work builds upon the following techniques, while qualitatively differing from each
NN with Rules.",6 Related Work,[0],[0]
"On the initialization side, Li et al. (2017) uses important n-grams to initialize the convolution filters.",6 Related Work,[0],[0]
"On the input side, Wang et al. (2017a) uses knowledge base rules to find relevant concepts for short texts to augment input.",6 Related Work,[0],[0]
"On the output side, Hu et al. (2016a; 2016b) and Guo et al. (2017) use FOL rules to rectify the output probability of NN, and then let NN learn from the rectified distribution in a teacher-student framework.",6 Related Work,[0],[0]
"Xiao et al. (2017), on the other hand, modifies the decoding score of NN by multiplying a weight derived from rules.",6 Related Work,[0],[0]
"On the loss function side, people modify the loss function to model the relationship between premise and conclusion (Demeester et al., 2016), and fit both human-annotated and rule-annotated labels (Alashkar et al., 2017).",6 Related Work,[0],[0]
"Since fusing in initialization or in loss function often require special properties of the task, these approaches are not applicable to our problem.",6 Related Work,[0],[0]
"Our work thus offers new ways to exploit RE rules at different levels of a NN.
NNs and REs.",6 Related Work,[0],[0]
"As for NNs and REs, previous work has tried to use RE to speed up the decoding phase of a NN (Strauß et al., 2016) and generating REs from natural language specifications of the
7We do not include results of both for slot filling since its REs are different from feat and logit, and we have already shown that the attention loss method does not work for slot filling.
",6 Related Work,[0],[0]
"RE (Locascio et al., 2016).",6 Related Work,[0],[0]
"By contrast, our work aims to use REs to improve the prediction ability of a NN.
Few-Shot Learning.",6 Related Work,[0],[0]
"Prior work either considers few-shot learning in a metric learning framework (Koch et al., 2015; Vinyals et al., 2016), or stores instances in a memory (Santoro et al., 2016; Kaiser et al., 2017) to match similar instances in the future.",6 Related Work,[0],[0]
Wang et al. (2017b) further uses the semantic meaning of the class name itself to provide extra information for few-shot learning.,6 Related Work,[0],[0]
"Unlike these previous studies, we seek to use the humangenerated REs to provide additional information.
",6 Related Work,[0],[0]
Natural Language Understanding.,6 Related Work,[0],[0]
"Recurrent neural networks are proven to be effective in both intent detection (Ravuri and Stoicke, 2015) and slot filling (Mesnil et al., 2015).",6 Related Work,[0],[0]
"Researchers also find ways to jointly model the two tasks (Liu and Lane, 2016; Zhang and Wang, 2016).",6 Related Work,[0],[0]
"However, no work so far has combined REs and NNs to improve intent detection and slot filling.",6 Related Work,[0],[0]
"In this paper, we investigate different ways to combine NNs and REs for solving typical SLU tasks.",7 Conclusions,[0],[0]
Our experiments demonstrate that the combination clearly improves the NN performance in both the few-shot learning and the full dataset settings.,7 Conclusions,[0],[0]
"We show that by exploiting the implicit knowledge encoded within REs, one can significantly improve the learning performance.",7 Conclusions,[0],[0]
"Specifically, we observe that using REs to guide the attention module works best for intent detection, and using REtags as features is an effective approach for slot filling.",7 Conclusions,[0],[0]
"We provide interesting insights on how REs of various forms can be employed to improve NNs, showing that while simple REs are very cost-effective, complex REs generally yield better results.",7 Conclusions,[0],[0]
"This work is supported by the National High Technology R&D Program of China (Grant No. 2015AA015403), the National Natural Science Foundation of China (Grant Nos. 61672057 and 61672058); the UK Engineering and Physical Sciences Research Council (EPSRC) under grants EP/M01567X/1 (SANDeRs) and EP/M015793/1 (DIVIDEND); and the Royal Society International Collaboration Grant (IE161012).",Acknowledgement,[0],[0]
"For any correspondence, please contact Yansong Feng.",Acknowledgement,[0],[0]
"The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data.",abstractText,[0],[0]
"In this paper, we ask the question: “Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?”.",abstractText,[0],[0]
"In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available.",abstractText,[0],[0]
We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling.,abstractText,[0],[0]
"Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.",abstractText,[0],[0]
Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding,title,[0],[0]
"Single-Linkage Clustering is one of the oldest methods for clustering multi-dimensional vectors based on the nearestneighbor rule and has been studied since 1951, see e.g. (Zahn, 1971).",1.1. Single-linkage clustering,[0],[0]
"It can be used for hierarchical clustering and is one of the cornerstone techniques in data mining (see e.g. Chapter 17 of a classic text on information retrieval by Manning, Raghavan and Schütze (Manning et al., 2008)).",1.1. Single-linkage clustering,[0],[0]
"Applications of Single-Linkage Clustering include reconstruction of semantic relationships from word embeddings such as Word2Vec (Malak & East, 2016), phylogenetic tree reconstruction (Gower & Ross, 1969), etc.
1Department of Computer Science, Indiana University, Bloomington, Indiana, United States.",1.1. Single-linkage clustering,[0],[0]
Correspondence to: Grigory Yaroslavstev,1.1. Single-linkage clustering,[0],[0]
"<grigory@grigory.us>, Adithya Vadapalli <avadapal@iu.edu>.
",1.1. Single-linkage clustering,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1.1. Single-linkage clustering,[0],[0]
"Copyright 2018 by the author(s).
",1.1. Single-linkage clustering,[0],[0]
We consider the problem of constructing a Single-Linkage Clustering for large-scale data.,1.1. Single-linkage clustering,[0],[0]
"Given a dataset consisting of n real-valued d-dimensional vectors v1, . . .",1.1. Single-linkage clustering,[0],[0]
", vn ∈ Rd the goal of Single-Linkage Clustering is to construct a partition of these vectors into k clusters C1, . . .",1.1. Single-linkage clustering,[0],[0]
", Ck such that the smallest distance between two vectors in different clusters is maximized.",1.1. Single-linkage clustering,[0],[0]
"Formally, for i 6=",1.1. Single-linkage clustering,[0],[0]
"j let the single-linkage distance between two clusters Ci and Cj under `p distance be dp(Ci, Cj) = minva∈Ci,vb∈Cj ‖va",1.1. Single-linkage clustering,[0],[0]
− vb‖p where ‖x‖p = ( ∑ i |xi|p)1/p is the standard p-norm.,1.1. Single-linkage clustering,[0],[0]
"Then in the k-Single-Linkage Clustering (k-SLC) problem under `p distance we aim to find a partition into k clusters that maximizes mini 6=j dp(Ci, Cj).",1.1. Single-linkage clustering,[0],[0]
"It is well-known that k-SLC can be constructed from the Minimum Spanning Tree (MST) of the underlying metric by taking as clusters connected components resulting from removal of k−1 longest MST edges (see Figure 1 for an example).
",1.1. Single-linkage clustering,[0],[0]
Note that with this approach once the MST is constructed it can be used to compute k-SLC for any value of k.,1.1. Single-linkage clustering,[0],[0]
"Furthermore, it induces a hierarchical clustering structure that is often desirable in practice.",1.1. Single-linkage clustering,[0],[0]
"According to Manning, Raghavan and Schütze (Manning et al., 2008) the main impediment to this approach in practice that motivates the use of various heuristics is that for large-scale data no practically feasible techniques are currently known for constructing an exact MST.",1.1. Single-linkage clustering,[0],[0]
"Our work overcomes this challenge by leveraging two observations: 1) inexact but close to optimum solutions can suffice in practice due to the fact that real-valued data always contains rounding errors, 2) while exact MST algorithms are very sequential, approximate solutions can be computed in parallel on a distributed cluster.
",1.1. Single-linkage clustering,[0],[0]
"Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under `p Distances m m ac hi ne s
R Rounds
≤ s bits sent/received
Figure 2.",1.1. Single-linkage clustering,[0],[0]
MPC model of computation,1.1. Single-linkage clustering,[0],[0]
We present analysis of performance of our algorithms in the Massively Parallel Computation model (MPC) which is the most commonly used theoretical model of computation on synchronous large-scale data processing platforms such as MapReduce and Spark.,1.2. Massively parallel computation,[0],[0]
As we demonstrate through experiments in Spark this model accurately reflects performance of our algorithms on real data.,1.2. Massively parallel computation,[0],[0]
MPC model has attracted a lot of interest recently.,1.2. Massively parallel computation,[0],[0]
"It has emerged through a sequence of papers (Feldman et al., 2008; Karloff et al., 2010; Goodrich et al., 2011; Beame et al., 2013; Andoni et al., 2014) and has been analyzed extensively (Fish et al., 2015; Roughgarden et al., 2016).",1.2. Massively parallel computation,[0],[0]
"While several variations of this basic model exist here we follow the strictest known version of the model used in (Andoni et al., 2014) and hence our algorithmic results hold in other versions as well.
",1.2. Massively parallel computation,[0],[0]
In the MPC model we are given access to m identical processors with local RAM space s on each.,1.2. Massively parallel computation,[0],[0]
For an input of size n the total space available to all processors is m · s = Õ(n).,1.2. Massively parallel computation,[0],[0]
The computation is performed in synchronous rounds.,1.2. Massively parallel computation,[0],[0]
"In each round each machine: 1) performs a local computation on its data (under its local space restriction of s), 2) sends and receives messages of total length at most s to other machines which are received before the next round begins1 (see Figure 2).",1.2. Massively parallel computation,[0],[0]
"Furthermore, we assume that the most time/space-efficient known algorithm for local subproblems (in our case almost linear-time and space) is used on each machine during the round.
",1.2. Massively parallel computation,[0],[0]
In this setup the key complexity measure of performance in such computation is the number of rounds it takes to complete it as other characteristics such as time and communication depend directly on it.,1.2. Massively parallel computation,[0],[0]
"The parameter s is set to nα for some fixed constant α < 1, see (Karloff et al., 2010; Andoni et al., 2014) for more details.",1.2. Massively parallel computation,[0],[0]
"In
1Note that restriction of s on the total length of received messages follows from the local space constraint assuming there is no computation performed on the fly on incoming data.
",1.2. Massively parallel computation,[0],[0]
"this setting of parameters sorting can be done in O(1) rounds (Goodrich et al., 2011) while sparse graph connectivity takes O(log n)(Rastogi et al., 2013; Kiveris et al., 2014) which is conjectured to be optimal (Karloff et al., 2010; Beame et al., 2013; Rastogi et al., 2013; Roughgarden et al., 2016).",1.2. Massively parallel computation,[0],[0]
It is folklore that an O(log n)-round algorithm for MST in sparse graphs can be obtained via a simulation of Boruvka’s algorithm in MPC.,1.2. Massively parallel computation,[0],[0]
We use these facts extensively in this paper.,1.2. Massively parallel computation,[0],[0]
"While scalable algorithms with provable guarantees for other popular clustering methods such as k-means and kmedian are known (Bahmani et al., 2012; Balcan et al., 2013)",1.3. Our results and previous work,[0],[0]
we are not aware of any such algorithms for SingleLinkage Clustering 2.,1.3. Our results and previous work,[0],[0]
"Also despite the fact that scalable heuristics exist for k-SLC and MST computation for vector data, e.g. (Jin et al., 2015), the only MPC algorithm with provable guarantees in this area that we are aware of is (Andoni et al., 2014)3.",1.3. Our results and previous work,[0],[0]
"For other recent work on geometric data structures and algorithms in the MPC model see (Agarwal et al., 2016; Nath et al., 2016) and results on distributed constructions of coresets (Agarwal et al., 2005; Indyk et al., 2014; Bateni et al., 2014).
",1.3. Our results and previous work,[0],[0]
"In (Andoni et al., 2014)",1.3. Our results and previous work,[0],[0]
it is shown that a (1 + )- approximate MST under `2 can be constructed in O(1) rounds of MPC for constant dimension.,1.3. Our results and previous work,[0],[0]
"However, while the overall cost of the MST is a good approximation to the optimum the length of any given edge can be arbitrarily distorted.",1.3. Our results and previous work,[0],[0]
This makes it impossible to directly use this algorithm of for the Single-Linkage Clustering problem.,1.3. Our results and previous work,[0],[0]
"For example, consider an input corresponding to a set of points on the line shown in Figure 3 and k = 2.",1.3. Our results and previous work,[0],[0]
"In this case a (1 + )-approximate MST would not necessarily lead to a (1 + )-approximate clustering as any such clustering would have to have clusters {1, . . .",1.3. Our results and previous work,[0],[0]
", n− 1} and {n} which are at distance 100 from each other.",1.3. Our results and previous work,[0],[0]
"Moreover, the algorithm of (Andoni et al., 2014) will indeed introduce edges of length Ω( n) into its approximate MST between the first n − 1 points if run on this example.",1.3. Our results and previous work,[0],[0]
"Hence for the MST constructed this way the basic approach of removing the longest edge to obtain a 2-SLC will result in two clusters which are at distance 1 with a very large probability.
",1.3. Our results and previous work,[0],[0]
"2With the exception of recent work of (Derakhshan et al., 2017) who consider a more general graph metric setting and hence get results which are inherently different from our work as representation of the metric requires Θ(n2) space
3For general graph metrics an MST algorithm in MPC is given in (Karloff et al., 2010).",1.3. Our results and previous work,[0],[0]
"In our case using this algorithm directly would imply a quadratic increase in space since our graph is implicitly given by n2 distances between the vectors and hence constructing the graph explicitly is infeasible under the overall space restriction.
",1.3. Our results and previous work,[0],[0]
In this paper we show how to overcome this difficulty and give a different family of algorithms which allow to compute an approximate Single-Linkage Clustering under various distance metrics.,1.3. Our results and previous work,[0],[0]
"While in (Andoni et al., 2014) only `2 metric is considered here we further extend this framework so that it also applies to `1 and `∞ with similar performance guarantees.",1.3. Our results and previous work,[0],[0]
"Perhaps most interestingly, while an arbitrarily good MST approximation can be computed inO(1) rounds of MPC (for fixed dimension) our algorithms for k-SLC run in O(log n) rounds.",1.3. Our results and previous work,[0],[0]
"As it turns out, such an increase is likely to be necessary.",1.3. Our results and previous work,[0],[0]
We justify it through a number of hardness results.,1.3. Our results and previous work,[0],[0]
Our results show that even for k = 2 assuming two most popular conjectures in the MPC literature regarding complexity of sparse connectivity no o(log n)round algorithm can compute k-SLC for sufficiently large dimension of the data with better than some fixed constantfactor approximation that depends on the distance metric used.,1.3. Our results and previous work,[0],[0]
"See Table 1 for a summary of these results4.
",1.3. Our results and previous work,[0],[0]
In order to complete the picture of approximability of kSLC under the most frequently used `p distances we also give algorithms and hardness results under Hamming distance (commonly referred to as `0).,1.3. Our results and previous work,[0],[0]
In contrast to other distances studied in this paper we are able to completely resolve approximability of the k-SLC problem for constantdimensional data in this case.,1.3. Our results and previous work,[0],[0]
"As we show, there exists an exact algorithm for d = O(1) that runs in O(log n) rounds of MPC while under Conjecture 3.1 no algorithm running in o(log n) rounds can obtain better than 2-approximation even for d = 2.",1.3. Our results and previous work,[0],[0]
See Table 1 for details.,1.3. Our results and previous work,[0],[0]
"`1, `2, `∞ Our algorithms under `1, `2 and `∞ all share the same high-level structure: we tackle the problem of the input having O(n2) edges by first constructing a sparsifier that only has O(n log n) edges and then run an MST algorithm on this sparsifier.",1.4. Our techniques,[0],[0]
In order to construct a sparsifier we execute a (1+ )-approximate MST algorithmO(log n) times and collect all edges of the MSTs constructed in these executions.,1.4. Our techniques,[0],[0]
"We then run an exact O(log n)-round exact MST algorithm on this set of O(n log n) edges and output clusters resulting from removing k−1 longest edges of
4While our algorithms work in the most restricted known version of MPC model, our hardness results also hold in more relaxed versions for which hardness of sparse connectivity is conjectured, see (Roughgarden et al., 2016) for further details.",1.4. Our techniques,[0],[0]
"Furthermore, in hardness results for `0 and `1 that require dimension d = Ω(n)",1.4. Our techniques,[0],[0]
"the result holds for O(1)-sparse vectors, i.e. the overall input size is still O(n) words.
",1.4. Our techniques,[0],[0]
the resulting MST.,1.4. Our techniques,[0],[0]
Note that the executions of the (1 + )- approximate MST algorithm can be done in parallel and hence it is the second step that introduces O(log n) rounds into the overall complexity of the algorithm.,1.4. Our techniques,[0],[0]
"Our algorithms under `1, `2 and `∞ are given in Section 2.",1.4. Our techniques,[0],[0]
"Assuming the same high-level structure this approach is unlikely to be improved as there are no known algorithms for solving MST in sparse graphs in o(log n) rounds.
",1.4. Our techniques,[0],[0]
"Hardness In fact, we make the above observation formal by giving reductions from two most popular problems conjectured to require Ω(log n) rounds in the MPC model: sparse connectivity (Conjecture 3.1) and a stronger “one cycle vs. two cycles” problem (Conjecture 3.2).",1.4. Our techniques,[0],[0]
Our reductions follow the same general strategy – we introduce a vector vi ∈,1.4. Our techniques,[0],[0]
Rn for each vertex in the input graph.,1.4. Our techniques,[0],[0]
"This vector is initially set to be ei, the i-th standard unit vector.",1.4. Our techniques,[0],[0]
"Then for each edge (i, j) adjacent to the vertex i we update the coordinate j of the vector by adding a carefully chosen value ξ.",1.4. Our techniques,[0],[0]
This ensures that the for pairs of points which are connected by an edge the distance between their correponding vectors is different from the distance between points which are not connected by an edge.,1.4. Our techniques,[0],[0]
The parameter ξ is then chosen to maximize the ratio of distances in these two cases.,1.4. Our techniques,[0],[0]
"Details are given in Section 3.
",1.4. Our techniques,[0],[0]
`0,1.4. Our techniques,[0],[0]
"Under `0 (Hamming distance) we can’t construct a (1 + )-approximate MST using (Andoni et al., 2014) and hence our algorithms and hardness results are quite different.",1.4. Our techniques,[0],[0]
Using sorting as a primitive we construct an auxiliary graph and then run an O(log n)-round connectivity algorithm on it d times.,1.4. Our techniques,[0],[0]
This way we obtain an exact MST and hence an exact k-SLC for any value of k. Details are given in Section A.,1.4. Our techniques,[0],[0]
Our hardness reduction in this case is also quite different as we construct a hard instance by creating a set of points in 2D instead of using high-dimensional vectors.,1.4. Our techniques,[0],[0]
Hence our result rules out an o(log n)-round 2- approximation even for d = 2.,1.4. Our techniques,[0],[0]
See Section 3.2 for details.,1.4. Our techniques,[0],[0]
We implemented our algorithm (for `2 distances) in Java on Apache Spark and empirically evaluated the performance.,1.5. Experimental results,[0],[0]
The largest datasets we used were the SIFT10M and HIGGS datasets from the UCI ML repository which has been used widely in literature (≈ 11 × 107).,1.5. Experimental results,[0],[0]
Note that storage of the n2 adjacency would take nearly 960TB of memory and hence building a complete graph locally is infeasible.,1.5. Experimental results,[0],[0]
We observed speedups of several orders of magnitude compared to our benchmark sequential Prim’s algorithm when using 200 reducers.,1.5. Experimental results,[0],[0]
We remark that the speedup is not just due to the parallelism in our algorithm but also due to the use of approximation which is helpful even if the algorithm is executed locally.,1.5. Experimental results,[0],[0]
See Section 4.,1.5. Experimental results,[0],[0]
"At a high level our k-SLC algorithm for `2 is very simple and can be described as follows:
Algorithm 1 Simplified k-SLC Algorithm for `2 Input: vectors v1, . .",2. Algorithms,[0],[0]
.,2. Algorithms,[0],[0]
", vn ∈",2. Algorithms,[0],[0]
Rd E′ =,2. Algorithms,[0],[0]
∅ Repeat O(log n),2. Algorithms,[0],[0]
"times sequentially:
E = set of edges of a (1 + )-approximate MST E′ = E ∪ E′
Run Boruvka’s MST algorithm on E′ and remove k − 1 longest edges to obtain the clustering.
",2. Algorithms,[0],[0]
In order for the above algorithm to produce an approximate k-SLC it is important however that the MST constructed during sequential repetitions obeys certain properties.,2. Algorithms,[0],[0]
"As we show below, for `2 these properties hold for the algorithm of (Andoni et al., 2014).",2. Algorithms,[0],[0]
"Furthermore, in order to extend this approach to `1, `∞ a more detailed analysis is required.
2.1.",2. Algorithms,[0],[0]
"Partition-based algorithm for `1, `2, `∞
Theorem 2.1.",2. Algorithms,[0],[0]
"For each of the three metrics `1, `2 and `∞ for any constants 0 < η ≤ 3, 0 < α < 1/2 such that η = Ω(s 2α−1 2d ) there exists an O(log n)-round MPC algorithm that computes (1+η)-approximate k-Single-Linkage Clustering for any constant dimension d given as an input set of vectors v1, . . .",2. Algorithms,[0],[0]
", vn ∈ Rd.",2. Algorithms,[0],[0]
The algorithm works simultaneously for all values of k under these metrics.,2. Algorithms,[0],[0]
The algorithm is randomized and produces correct result with high probability.,2. Algorithms,[0],[0]
"Given access to machines with RAM space s it uses Õ(n/s) machines and time at most Õ(s) per round on each machine.
",2. Algorithms,[0],[0]
"In this section we describe a generic partition-based algorithm, Algorithm 2.1, that is used to prove the above theorem.",2. Algorithms,[0],[0]
"We also give analysis of its approximation guarantee.
",2. Algorithms,[0],[0]
"Algorithm 2.1 relies on (a, b, c)-distance-preserving partitions and uses Algorithm 3 which we describe in Section B.
We start by recalling standard definitions of distance preserving hierarchical partitions.",2. Algorithms,[0],[0]
"Let M(S, ρ) be a metric space with distance function ρ.",2. Algorithms,[0],[0]
"For S′ ⊆ S we denote its diameter as ∆(S′) = supx,y∈S′ ρ(x, y).",2. Algorithms,[0],[0]
"A deterministic hierarchical partition P with L levels is defined as a sequence P = (P0, . . .",2. Algorithms,[0],[0]
", PL) where PL = {S} and each level P` is a subdivision of P`+1.",2. Algorithms,[0],[0]
For a partition Pi we call its parts cells.,2. Algorithms,[0],[0]
The diameter at level i is defined as ∆(Pi) = maxC∈Pi ∆(C).,2. Algorithms,[0],[0]
The degree of a cell C ∈ P` is deg(C),2. Algorithms,[0],[0]
= |{C ′ ∈ P`−1 : C ′ ⊆ C}|.,2. Algorithms,[0],[0]
The degree of a hierarchical partition is the maximum degree of any of its cells.,2. Algorithms,[0],[0]
The unique cell at level ` containing a point x is denoted as C`(x).,2. Algorithms,[0],[0]
We say that a partition is indexable if this cell can be computed based on x and `.,2. Algorithms,[0],[0]
A randomized hierarchical partition is a distribution over deterministic hierarchical partitions.,2. Algorithms,[0],[0]
Definition 2.1 (Distance-preserving partition).,2. Algorithms,[0],[0]
"For parameters a ∈ (0, 1), b, c ∈ R+",2. Algorithms,[0],[0]
"and γ > 1 a randomized hierarchical partition P of a metric space with L levels is (a, b, c)-distance-preserving with approximation γ if the degree of all deterministic partitions in its support is at most c and the following properties are satisfied for ∆` = γa L−`∆(S):
1.",2. Algorithms,[0],[0]
(Bounded diameter),2. Algorithms,[0],[0]
"For every deterministic partition P = (P0, . . .",2. Algorithms,[0],[0]
", PL) in the support of P and for all ` ∈ {0, . . .",2. Algorithms,[0],[0]
", L} it holds that ∆(P`) ≤ ∆`.",2. Algorithms,[0],[0]
"2. (Probability of cutting an edge) For every x, y ∈ S and for all ` ∈ {0, . . .",2. Algorithms,[0],[0]
", L}:
Pr P∼P
[C`(x) 6= C`(y)] ≤",2. Algorithms,[0],[0]
"b ρ(x, y)
∆` .
",2. Algorithms,[0],[0]
"Let M(S, ρ) be a metric space and w : S × S → R+ be a weight function w(x, y) = ρ(x, y).",2. Algorithms,[0],[0]
We think of w as representing weights of edges in a complete graph.,2. Algorithms,[0],[0]
"Let MSTi(w) denote the weight of the i-th Minimum Spanning Tree edge of this graph sorted in non-decreasing order.
",2. Algorithms,[0],[0]
"Algorithm 2 Partition-based Distributed k-SLC Algorithm Input: vectors v1, . . .",2. Algorithms,[0],[0]
", vn ∈ Rd, parameters η, α, p E = ∅",2. Algorithms,[0],[0]
"Set a = s−α/d, b = poly(d), c = sα, L = O(log1/a n)
",2. Algorithms,[0],[0]
"Set = min (
η 6c1Lb , η3c2 )",2. Algorithms,[0],[0]
"Repeat O(log n) times sequentially:
Sample partition P with L levels from (a, b, c)-distance-preserving family wP",2. Algorithms,[0],[0]
Execute unit step Algorithm 3 for each cell in P with parameter E′ =,2. Algorithms,[0],[0]
"set of edges output in the previous step E = E ∪ E′
Run Boruvka’s MST algorithm on E and remove k − 1 longest edges to obtain the clustering.
",2. Algorithms,[0],[0]
Letw+ :,2. Algorithms,[0],[0]
"S×S → R+ be a random family of functions that satisfies that for each x, y it holds that w(x, y) ≤ w+(x, y) and E[w+(x, y)] ≤ (1 + γ)w(x, y) for some fixed γ > 0.",2. Algorithms,[0],[0]
Note that the weights given by this random family to different pairs might be correlated with each other.,2. Algorithms,[0],[0]
Definition 2.2 (Crossing edge).,2. Algorithms,[0],[0]
"For a partition (C1, . . .",2. Algorithms,[0],[0]
", Ct) of S we say that a pair of points (x, y) crosses this partition if x ∈ Ci and y ∈",2. Algorithms,[0],[0]
Cj for i 6=,2. Algorithms,[0],[0]
j. Definition 2.3 (Cut-preserving spanning tree).,2. Algorithms,[0],[0]
"We say that T is an α-cut-preserving spanning tree for w : S × S → R+ if for every partition (C1, C2) of S there exists an edge in T that crosses this partition and is at most α times longer than the shortest such edge with respect to w.
As we show below Algorithm 2.1 can be seen as performing the following experiment: draw k functionsw1, . . .",2. Algorithms,[0],[0]
", wk i.i.d at random from the family w+.",2. Algorithms,[0],[0]
Compute a (1 + δ)cut-preserving spanning tree Ti for each wi.,2. Algorithms,[0],[0]
"Then for each (x, y) ∈ S×S define w′i(x, y) = w(x, y)",2. Algorithms,[0],[0]
"if (x, y) is in this spanning tree and w′i(x, y) =",2. Algorithms,[0],[0]
+∞ otherwise.,2. Algorithms,[0],[0]
"Then for all (x, y) ∈ S × S define w̄k(x, y) = minki=1",2. Algorithms,[0],[0]
"w′i(x, y).",2. Algorithms,[0],[0]
"The final run of Boruvka’s MST algorithm is then executed on w̄k.
",2. Algorithms,[0],[0]
"Indeed, random family of functionsw+ satisfying the properties described above is constructed by Algoirthm 2.1 as follows from a result (Andoni et al., 2014) given.",2. Algorithms,[0],[0]
"It is important to note that cut-preserving spanning tree computations for random function samples from this family required above can be also performed as guaranteed by the following lemma: Lemma 2.2 ((Andoni et al., 2014), Lemmas 3.4 and 3.13).",2. Algorithms,[0],[0]
"Given access to an (a, b, c)-distance-preserving partition with L levels and approximation γ for M(S, ρ) there exists an MPC algorithm that runs inO(1) rounds and constructs a random family of weight functions wP which satisfies:
ρ(i, j) ≤ wP",2. Algorithms,[0],[0]
"(i, j) and E[wP (i, j)] ≤ (1 + c1 Lb) ρ(i, j).
",2. Algorithms,[0],[0]
"Furthermore, execution of unit step Algorithm 3 for all cells in this partition for a random function w∗ sampled from wP produces a (1 + c2 )-cut-preserving spanning tree T for w∗.
Let w(i, j) = ‖vi",2. Algorithms,[0],[0]
"− vj‖2, w+ = wP and let γ = c1 d and δ = c2 .
",2. Algorithms,[0],[0]
Lemma 2.3.,2. Algorithms,[0],[0]
Let n = |S|.,2. Algorithms,[0],[0]
There is a large enough constant c > 0,2. Algorithms,[0],[0]
"such that if k = c log n then for all i it holds that:
Pr w1,...,wk
[MSTi(w̄k) ≥ (1+2γ)(1+δ)MSTi(w)] ≤ n−Ω(1).
",2. Algorithms,[0],[0]
Proof.,2. Algorithms,[0],[0]
"Fix (x, y) ∈ S × S and let ∆(x, y) = w+(x, y)",2. Algorithms,[0],[0]
"− w(x, y).",2. Algorithms,[0],[0]
"Because ∆(x, y) ≥ 0 and E[∆(x, y)] ≤ γw(x, y) with probability at least 1/2 it holds that ∆(x, y) ≤ 2γw(x, y) by Markov inequality.",2. Algorithms,[0],[0]
"If k = c log n then with probability 1 − 1/nc there exists i such that wi(x, y)",2. Algorithms,[0],[0]
"− w(x, y) ≤ 2γw(x, y).",2. Algorithms,[0],[0]
"By a union bound over all n2 pairs (x, y) with probability 1−1/nc−2 for each such pair a corresponding index exists.",2. Algorithms,[0],[0]
"Below we refer to this event as E and condition on it.
",2. Algorithms,[0],[0]
Proposition 2.4.,2. Algorithms,[0],[0]
"Let (C1, . . .",2. Algorithms,[0],[0]
", Ct) be an arbitrary partition of S. Let (",2. Algorithms,[0],[0]
"x∗, y∗) ∈",2. Algorithms,[0],[0]
S × S be the closest w.r.t w pair of points that belong to different parts of this partition.,2. Algorithms,[0],[0]
"Then conditioned on the event E there exists a pair of points (x′, y′) that crosses this partition and:
w(x∗, y∗) ≤",2. Algorithms,[0],[0]
"w̄k(x′, y′)",2. Algorithms,[0],[0]
"≤ (1 + 2γ)(1 + δ)w(x∗, y∗).
",2. Algorithms,[0],[0]
Proof.,2. Algorithms,[0],[0]
"First, consider the case when t = 2 and consider any partition (C1, C2) of S. Let (x∗, y∗) be the shortest edge that crosses this partition, i.e. (x∗, y∗) := argminx∈C1,y∈C2 w(x, y).",2. Algorithms,[0],[0]
"Conditioned on E there exists i such that wi(x∗, y∗) ≤ (1 + 2γ)w(x∗, y∗).",2. Algorithms,[0],[0]
"Furthermore, there exists an edge (x′, y′) in the (1 + δ)-cutpreserving spanning tree Ti constructed for wi that has length w′i(x ′, y′) = wi(x ′, y′)",2. Algorithms,[0],[0]
≤,2. Algorithms,[0],[0]
"(1 + δ)wi(x∗, y∗) ≤ (1 + 2γ)(1 + δ)w(x∗, y∗).",2. Algorithms,[0],[0]
"On the other hand, because wi ≥ w for every pair (x, y) that crosses the partition (C1, C2) it holds that wi(x, y) ≥ w(x∗, y∗).",2. Algorithms,[0],[0]
"Combining these two facts we conclude that in Ti there exists some edge (x′, y′) that crosses the cut and satisfies w(x∗, y∗) ≤",2. Algorithms,[0],[0]
"w′i(x
′, y′)",2. Algorithms,[0],[0]
"≤ (1+2γ)(1+δ)w(x∗, y∗).",2. Algorithms,[0],[0]
"By definition of w̄k the same holds for it as well, i.e. w(x∗, y∗) ≤",2. Algorithms,[0],[0]
"w̄k(x′, y′)",2. Algorithms,[0],[0]
"≤ (1 + 2γ)(1 + δ)w(x∗, y∗).
",2. Algorithms,[0],[0]
Now suppose t > 2.,2. Algorithms,[0],[0]
"For i = 1, . . .",2. Algorithms,[0],[0]
", t define a family of cuts (Si, Ti) where Si = Ci and Ti = ∪j 6=iCj .",2. Algorithms,[0],[0]
"Let (x∗i , y ∗ i ) be the shortest pair crossing the cut (Si, Ti).",2. Algorithms,[0],[0]
"If (x∗, y∗) is the shortest edge that crosses (C1, . .",2. Algorithms,[0],[0]
.,2. Algorithms,[0],[0]
", Ct) then we have w(x∗, y∗) = mini w(x∗i , y ∗ i ).",2. Algorithms,[0],[0]
"Let i
∗ = argmini w(x ∗",2. Algorithms,[0],[0]
"i , y ∗ i ).",2. Algorithms,[0],[0]
"Then using the argument above for t = 2 there exists (x′, y′) such that x′ ∈ Si∗ , y ∈ Ti∗ and:
w(x∗, y∗) = w(x∗i∗ , y ∗ i∗)
≤ w̄k(x′, y′) ≤ (1 + 2γ)(1 + δ)w(x∗i∗ , y∗i∗) = (1 + 2γ)(1 + δ)w(x∗, y∗).
",2. Algorithms,[0],[0]
"Given Proposition 2.4 the rest of the proof is the same as analysis of approximate Kruskal’s algorithm in (Indyk, 2000), we give the proof here for completeness.",2. Algorithms,[0],[0]
Since edges output by Kruskal’s algorithm are produced in the order of non-decreasing weight MSTi is the i-th edge that is output.,2. Algorithms,[0],[0]
Consider executions of Kruskal’s algorithm on weights w and w̄k.,2. Algorithms,[0],[0]
"Let the edges output by the former execution be e1, . . .",2. Algorithms,[0],[0]
", en−1 in order.",2. Algorithms,[0],[0]
"Let the edges output by the latter execution be e′1, . . .",2. Algorithms,[0],[0]
", e ′",2. Algorithms,[0],[0]
"n−1.
To prove Lemma 2.3 it suffices to show that conditioned on E it holds that w(ei) ≤ w(e′i) ≤ (1 + 2γ)w(ei) for all i. The first inequality here essentially follows from the fact that the weight of the i-th MST edge is a monotone function of the weights and w ≤ w̄k.
",2. Algorithms,[0],[0]
The i-th edge in Kruskal’s algorithm is constructed by joining two closest clusters among n,2. Algorithms,[0],[0]
− i + 1 clusters constructed so far.,2. Algorithms,[0],[0]
"Let these clusters in the execution of Kruskal’s algorithm on w̄k be denoted as C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1.",2. Algorithms,[0],[0]
The key observation is that there exists an index i∗ ≤,2. Algorithms,[0],[0]
"i such that endpoints of the edge ei∗ belong to different parts of the partition C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1.",2. Algorithms,[0],[0]
"Indeed, edges e1, . . .",2. Algorithms,[0],[0]
", ei form a forest and thus having all such edges be inside C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1 would be a contradiction.
",2. Algorithms,[0],[0]
"Let (x∗, y∗) be the closest w.r.t to w pair of points in different parts of the partition C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1.",2. Algorithms,[0],[0]
"By applying Proposition 2.4 to ei∗ there exists a pair of points (x′, y′) whose endpoints belong to different parts of the partition C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1 and w̄k(x′, y′)",2. Algorithms,[0],[0]
"≤ (1 + 2γ)w(x∗, y∗).",2. Algorithms,[0],[0]
"Putting everything together we have:
w(e′i) ≤ w̄k(e′i)",2. Algorithms,[0],[0]
"w ≤ w̄k
≤ w̄k(x′, y′)",2. Algorithms,[0],[0]
"≤ (1 + 2γ)(1 + δ)w(x∗, y∗) Proposition 2.4 ≤ (1 + 2γ)(1 + δ)w(ei∗) ≤",2. Algorithms,[0],[0]
"(1 + 2γ)(1 + δ)w(ei)
",2. Algorithms,[0],[0]
"The second inequality follows because e′i is shortest edge w.r.t w̄k that crosses (C1, . . .",2. Algorithms,[0],[0]
", Cn−i+1).",2. Algorithms,[0],[0]
The last inequality follows because i∗ ≤,2. Algorithms,[0],[0]
"i, edge weights are non-decreasing.
",2. Algorithms,[0],[0]
"Putting everything together we obtain analysis of approximation guaranteed by Algorithm 2.1.
",2. Algorithms,[0],[0]
Theorem 2.5.,2. Algorithms,[0],[0]
"For η ≤ 3 and p = 1, 2,∞ Algorithm 2.1 constructs a spanning tree forw(i, j) = ‖vi−vj‖p for each t its t-th longest edge (x, y) has weight w(x, y) ≤ (1 + η)MSTk(w).",2. Algorithms,[0],[0]
"This guarantee holds with high probability over the randomness used in Algorithm 2.1.
",2. Algorithms,[0],[0]
Proof.,2. Algorithms,[0],[0]
Note that takingw+ = wP,2. Algorithms,[0],[0]
"forw(i, j) = ‖vi−vj‖p where p = 1, 2,∞ satisfies conditions of Lemma 2.3 by Lemma 2.2.",2. Algorithms,[0],[0]
Hence our algorithm constructs a function w̄k with properties required for Lemma 2.3.,2. Algorithms,[0],[0]
Since c1 Lb ≤ η/6 and c2 ≤ η/3,2. Algorithms,[0],[0]
we can set δ = η/6,2. Algorithms,[0],[0]
"and γ = η/3 in Lemma 2.3 and hence for η ≤ 3:
Pr [E1] ≥ Pr [E2] ≥ 1− 1
poly(n) .
where E1 is the event that MSTi(w̄k) ≤ (1 + η)MSTi(w) and E2 is the event that MSTi(w̄k) ≤ (1 + 2γ)(1 + δ)MSTi(w).
",2. Algorithms,[0],[0]
After w̄k is constructed by running Boruvka’s algorithm on it we find an MST exactly and hence the approximation guarantee for each of the MST edges follows.,2. Algorithms,[0],[0]
"We use Solve-and-Sketch (SAS) framework of (Andoni et al., 2014) for computing an approximate minimum spanning tree.",2.2. Solve-and-Sketch framework and unit step,[0],[0]
"SAS framework works with a partition P = (P0, . . .",2.2. Solve-and-Sketch framework and unit step,[0],[0]
", PL) of the input M(S, ρ), sampled from a randomized (a, b, c)-partition P .",2.2. Solve-and-Sketch framework and unit step,[0],[0]
"Then SAS algorithm proceeds through L levels, and in level ` a unit step algorithm Au is executed in each cellC of the partition P`, with input the union of the outputs of the unit steps applied to the children of C. The unit step also outputs a subset of the edges of a spanning tree in addition to the input for the next level.",2.2. Solve-and-Sketch framework and unit step,[0],[0]
Once the unit step has been executed for the root cell of partition at level PL (and hence also for all other cells),2.2. Solve-and-Sketch framework and unit step,[0],[0]
the computation is complete.,2.2. Solve-and-Sketch framework and unit step,[0],[0]
"We give the description of the unit step algorithm below (Algorithm 3).
",2.2. Solve-and-Sketch framework and unit step,[0],[0]
Definition 2.4 (δ-covering).,2.2. Solve-and-Sketch framework and unit step,[0],[0]
"Let M = (S, ρ) be a metric space and let δ > 0 .",2.2. Solve-and-Sketch framework and unit step,[0],[0]
"A set S′ ⊆ S is a δ-covering if for any point x ∈ S, there is a point y ∈ S′ such that ρ(x, y) ≤ δ.
3.",2.2. Solve-and-Sketch framework and unit step,[0],[0]
Hardness of k-SLC,2.2. Solve-and-Sketch framework and unit step,[0],[0]
"The following two conjectures are widely used in the MPC literature (Karloff et al., 2010; Beame et al., 2013; Rastogi et al., 2013; Roughgarden et al., 2016).",3.1. Hardness under `1 and `2,[0],[0]
"Note that the second conjecture is stronger and hence can potentially be used to get stronger hardness results.
",3.1. Hardness under `1 and `2,[0],[0]
Conjecture 3.1 (Sparse connectivity hardness).,3.1. Hardness under `1 and `2,[0],[0]
"If s = nα for a constant α < 1 then solving connectivity on an input
Algorithm 3 Unit Step at Level `, Input: Cell C ∈ P`, a collection V (C) of points in C, and a partition Q = {Q1, . . .",3.1. Hardness under `1 and `2,[0],[0]
Qk} of V (C) into previously computed connected components.,3.1. Hardness under `1 and `2,[0],[0]
"Output: V ′ ⊆ V , an 2∆`-covering for C, the partition Q(V ′) induced by Q on V ′. θ := 0 while k > 1 and θ ≤ ∆` do
Let",3.1. Hardness under `1 and `2,[0],[0]
τ,3.1. Hardness under `1 and `2,[0],[0]
=,3.1. Hardness under `1 and `2,[0],[0]
min,3.1. Hardness under `1 and `2,[0],[0]
"i,j i 6=j minu∈Qi,v∈Qj ρ(u, v) Find u ∈ Qi and v ∈ Qj for some i and j such that i 6= j and ρ(u, v) ≤ (1 + )τ .",3.1. Hardness under `1 and `2,[0],[0]
"θ := ρ(u, v)",3.1. Hardness under `1 and `2,[0],[0]
"if θ ≤ ∆` then
Output tree edge (u, v).",3.1. Hardness under `1 and `2,[0],[0]
"Merge Qi and Qj and update Q and k.
end if end while
graph with n vertices and O(n) edges requires Ω(log n) rounds of MPC.
",3.1. Hardness under `1 and `2,[0],[0]
Conjecture 3.2 (One cycle vs. two cycles hardness).,3.1. Hardness under `1 and `2,[0],[0]
"If s = nα for a constant α < 1 then distinguishing the following two instances requires Ω(log n) rounds of MPC: 1) a cycle on n vertices, 2) two cycles on n/2 vertices each.
",3.1. Hardness under `1 and `2,[0],[0]
Theorem 3.3.,3.1. Hardness under `1 and `2,[0],[0]
"No o(log n)-round MPC algorithm can achieve approximation for 2-SLC:
1.",3.1. Hardness under `1 and `2,[0],[0]
"Better than ( √ 2 + √
2 − ) under `2 for d = Ω(log n/ 2) under Conjecture 3.2.",3.1. Hardness under `1 and `2,[0],[0]
2.,3.1. Hardness under `1 and `2,[0],[0]
Better than 3 under `1 for O(1)-sparse vectors and d = Ω(n) under Conjecture 3.2.,3.1. Hardness under `1 and `2,[0],[0]
"3. Better than ( √
2 − ) under `2 for d = Ω(log n/ 2) under Conjecture 3.1.",3.1. Hardness under `1 and `2,[0],[0]
4.,3.1. Hardness under `1 and `2,[0],[0]
"Better than 2 under `1 for O(1)-sparse vectors and d = Ω(n) under Conjecture 3.1.
",3.1. Hardness under `1 and `2,[0],[0]
Proof.,3.1. Hardness under `1 and `2,[0],[0]
"We give proof of Part 1 here, other proofs are similar and are deferred to Appendix E.",3.1. Hardness under `1 and `2,[0],[0]
"Given an instance of the “one cycle vs. two cycles problem” we reduce it to the 2-SLC problem as follows:
1.",3.1. Hardness under `1 and `2,[0],[0]
Create a vector v′i ∈,3.1. Hardness under `1 and `2,[0],[0]
Rn for each vertex where v′i = ei and ei is the i-th standard unit vector.,3.1. Hardness under `1 and `2,[0],[0]
2.,3.1. Hardness under `1 and `2,[0],[0]
"For each edge (a, b) in the input graph update the corresponding vectors as v′a = v ′ a+ξeb",3.1. Hardness under `1 and `2,[0],[0]
and v ′,3.1. Hardness under `1 and `2,[0],[0]
b = v ′,3.1. Hardness under `1 and `2,[0],[0]
"b+ξea
where ξ = 1√ 2 .",3.1. Hardness under `1 and `2,[0],[0]
3.,3.1. Hardness under `1 and `2,[0],[0]
"Apply Johnson-Lindenstrauss transform to v′1, . . .",3.1. Hardness under `1 and `2,[0],[0]
", v ′ n
to construct v1, . . .",3.1. Hardness under `1 and `2,[0],[0]
", vn ∈ Rd where d = O(log n/ 2).
",3.1. Hardness under `1 and `2,[0],[0]
Note that the above reduction can be performed in only a constant number of MPC rounds.,3.1. Hardness under `1 and `2,[0],[0]
"Indeed, Step 1 can be done locally by partitioning vectors between machines and to perform Step 2 we can send each edge (a, b) to the ma-
chines holding vectors va and vb.",3.1. Hardness under `1 and `2,[0],[0]
For Step 3 note that for each i we have vi = Mv′i where M is the JohnsonLindenstrauss matrix and each v′i has at most 3 non-zero entries.,3.1. Hardness under `1 and `2,[0],[0]
"Hence, all vi can be computed in one round of MPC with O(log n/ 2) communication per vector.
",3.1. Hardness under `1 and `2,[0],[0]
Proposition 3.4.,3.1. Hardness under `1 and `2,[0],[0]
"If (i, j) is an edge in the input graph then ‖v′i",3.1. Hardness under `1 and `2,[0],[0]
"− v′j‖2 = √ 2( √ 2− √ 2), otherwise ‖v′i − v′j‖2 = 2.
",3.1. Hardness under `1 and `2,[0],[0]
Proof.,3.1. Hardness under `1 and `2,[0],[0]
"Indeed, if there is an edge (i, j) in the input then there exist two other edges (i, i′) and (j, j′) and hence, the non-zero entries of v′i and v ′ j are as follows: v ′ ii = 1, v ′ ii′ = ξ, v′ij = ξ, v ′ jj = 1, v ′",3.1. Hardness under `1 and `2,[0],[0]
"jj′ = ξ, v ′",3.1. Hardness under `1 and `2,[0],[0]
ji = ξ.,3.1. Hardness under `1 and `2,[0],[0]
Hence ‖v′i,3.1. Hardness under `1 and `2,[0],[0]
"− v′j‖2 =√
2(1− ξ)2 + 2ξ2.",3.1. Hardness under `1 and `2,[0],[0]
"On the other hand, if there is no edge (i, j) then there exist four edges (i, i′), (i, i′′), (j, j′) and (j, j′′) and non-zero entries of v′i and v ′",3.1. Hardness under `1 and `2,[0],[0]
"j are: v ′ ii = 1, v ′ ii′ = ξ, v′ii′′ = ξ, v ′",3.1. Hardness under `1 and `2,[0],[0]
"jj = 1, v ′",3.1. Hardness under `1 and `2,[0],[0]
"jj′ = ξ, v ′",3.1. Hardness under `1 and `2,[0],[0]
jj′′ = ξ.,3.1. Hardness under `1 and `2,[0],[0]
"Hence ‖v′i −
v′j‖2 = √ 2 + 4ξ2.",3.1. Hardness under `1 and `2,[0],[0]
"Maximum of the ratio √ 2+4ξ2√ 2(1−ξ)2+2ξ2 is achieved when ξ = 1/ √ 2 and equals √ 2 + √ 2.
",3.1. Hardness under `1 and `2,[0],[0]
"By Proposition 3.4, if the input graph is one cycle then the cost of 2-SLC of v′1, . . .",3.1. Hardness under `1 and `2,[0],[0]
", v ′ n equals √ 2 √ 2− √
2, otherwise it is 2.",3.1. Hardness under `1 and `2,[0],[0]
"As Johnson-Lindenstrauss transform preserves all pairwise distances up to a multiplicative (1 ± ) factor with high probability the same is true for the cost of 2-SLC of v1, . . .",3.1. Hardness under `1 and `2,[0],[0]
", vn up to ± error.",3.1. Hardness under `1 and `2,[0],[0]
"This completes the proof.
3.2.",3.1. Hardness under `1 and `2,[0],[0]
"Hardness of Hamming k-SLC
Theorem 3.5.",3.1. Hardness under `1 and `2,[0],[0]
"No algorithm for computing Hamming kSLC cost for d = 2 in o(log n) rounds of MPC can achieve better than 2-approximation under Conjecture 3.1.
",3.1. Hardness under `1 and `2,[0],[0]
Proof.,3.1. Hardness under `1 and `2,[0],[0]
"Let G(V,E) be an instance of sparse connectivity.",3.1. Hardness under `1 and `2,[0],[0]
"Our reduction to Hamming 2-SLC constructs an input set of 2-dimensional vectors as follows: 1) for each vertex i ∈ V create a vector (i, i), 2) or each edge (i, j) ∈ E create a vector (i, j).",3.1. Hardness under `1 and `2,[0],[0]
Clearly this reduction can be performed in a constant number of rounds of MPC and the resulting instance has |V |+ |E| = O(n) many vectors.,3.1. Hardness under `1 and `2,[0],[0]
We will show that if the input graph is connected the cost of Hamming 2-SLC of the input equals 1 and the cost is 2 otherwise.,3.1. Hardness under `1 and `2,[0],[0]
"Indeed, note that the distances between resulting vectors are always either 1 or 2.",3.1. Hardness under `1 and `2,[0],[0]
If G is connected then it is easy to construct a connected spanning subgraph in the resulting Hamming graph where each edge has cost 1.,3.1. Hardness under `1 and `2,[0],[0]
"Indeed, consider a subgraph that for each edge (i, j) in the input graph contains two edges: one between vectors (i, i) and (i, j) and another between vectors (j, j) and (i, j).",3.1. Hardness under `1 and `2,[0],[0]
"Clearly, if the input graph is connected then this is a connected spanning subgraph.",3.1. Hardness under `1 and `2,[0],[0]
"Hence the Hamming MST cost of the con-
structed point set equals |V |+ |E|−1 and the Hamming 2- SLC cost equals 1.",3.1. Hardness under `1 and `2,[0],[0]
"On the other hand, if G is disconnected then consider any partitioning (S, T ) of G into connected components.",3.1. Hardness under `1 and `2,[0],[0]
"Clearly, any two vectors representing vertices belonging to different parts of this partition in our reduction are at distance 2 from each other.",3.1. Hardness under `1 and `2,[0],[0]
This implies that the Hamming MST cost is at least |V |+ |E| and the Hamming 2-SLC cost is 2.,3.1. Hardness under `1 and `2,[0],[0]
"Small datasets Four standard clustering datasets used in the literature were taken for experimental evaluation: 1) Image dataset, d = 3, n = 34112 (house images, https://cs.joensuu.fi/sipu/datasets/), 2) KDDCUP04Bio dataset , d = 10, n = 145751 (preprocessed to select 10 numerical dimensions out of 74, accessed via the link above), 3) Shuttle data set from the UCI ML repository, d = 9, n = 43500.",4. Experiments,[0],[0]
"4) US Census dataset from the UCI ML repository, d = 8, n = 2548285.
",4. Experiments,[0],[0]
"Due to page limitations here, we only show plots for the largest Census dataset.",4. Experiments,[0],[0]
Other plots are deferred to Appendix D. Figure 4 shows dependence of speedup as a function of approximation.,4. Experiments,[0],[0]
We observe a dramatic increase in the speedup at around approximation 1.26 due to the fact the local inputs start to fit in L2-cache.,4. Experiments,[0],[0]
"Figure 5 shows dependence of approximation on k for the census data.
",4. Experiments,[0],[0]
"Large datasets In order to test scalability, we took the largest real-valued vector datasets from the UCI ML repository: SIFT10M and HIGGS.",4. Experiments,[0],[0]
Both the datasets have approximately 11 million entries.,4. Experiments,[0],[0]
"Thus, constructing the full matrix of distances in memory is clearly infeasible as the size of this matrix would be roughly 960TB in both cases5.",4. Experiments,[0],[0]
Dimension reduction for this data was done using PCA for d = 3.,4. Experiments,[0],[0]
Results are given in Table 2.,4. Experiments,[0],[0]
We implemented Algorithm 2.1 in Java on Apache Spark 2.0.2 for Hadoop 2.7.3.,4.1. Experimental setup,[0],[0]
"Experiments were performed on two different setups:
Google Cloud Dataproc (GCD) platform on two cluster 5Assuming 8-byte double-precision arithmetic.
",4.1. Experimental setup,[0],[0]
"configurations: 1) single-core 1 master / 7 worker (1m/7w) cluster, 2) dual-core 1 master / 3 worker (1m/3w) cluster.",4.1. Experimental setup,[0],[0]
Each core had an Intel Xeon E5 processor at 2.2–2.6 GHz and 3.75GB RAM + 10GB HDD space.,4.1. Experimental setup,[0],[0]
"Due to the limitations of the free tier access on GCD the total number of cores in a cluster is limited to 8, which is still sufficient to demonstrate at least an order of magnitude speedup over the benchmark sequential algorithm.",4.1. Experimental setup,[0],[0]
"This setup was used for the small datasets.
",4.1. Experimental setup,[0],[0]
Local Simulation with 200 reducers on a Dell XPS13,4.1. Experimental setup,[0],[0]
Laptop with an Intel core I5 processor and 8GB RAM.,4.1. Experimental setup,[0],[0]
This setup was used for the large datasets.,4.1. Experimental setup,[0],[0]
This research was supported by NSF Award 1657477.,5. Acknowledgements,[0],[0]
"The authors would like to thank Alexandr Andoni, Aleksandar Nikolov and Krzysztof Onak for multiple discussions of (Andoni et al., 2014) and its relationship to the singlelinkage clustering problem which led to this work.",5. Acknowledgements,[0],[0]
"We present first massively parallel (MPC) algorithms and hardness of approximation results for computing Single-Linkage Clustering of n input d-dimensional vectors under Hamming, `1, `2 and `∞ distances.",abstractText,[0],[0]
All our algorithms run in O(log n) rounds of MPC for any fixed d and achieve (1 + )-approximation for all distances (except Hamming for which we show an exact algorithm).,abstractText,[0],[0]
We also show constant-factor inapproximability results for o(log n)-round algorithms under standard MPC hardness assumptions (for sufficiently large dimension depending on the distance used).,abstractText,[0],[0]
Efficiency of implementation of our algorithms in Apache Spark is demonstrated through experiments on the largest available vector datasets from the UCI machine learning repository exhibiting speedups of several orders of magnitude.,abstractText,[0],[0]
Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under `p Distances,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1042–1048, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
Citation sentences (citances) to a reference article have been extensively studied for summarization tasks. However, citances might not accurately represent the content of the cited article, as they often fail to capture the context of the reported findings and can be affected by epistemic value drift. Following the intuition behind the TAC (Text Analysis Conference) 2014 Biomedical Summarization track, we propose a system that identifies text spans in the reference article that are related to a given citance. We refer to this problem as citance-reference spans matching. We approach the problem as a retrieval task; in this paper, we detail a comparison of different citance reformulation methods and their combinations. While our results show improvement over the baseline (up to 25.9%), their absolute magnitude implies that there is ample room for future improvement.",text,[0],[0]
The size of scientific literature has increased dramatically during recent decades.,1 Introduction,[0],[0]
"In biomedical domain for example, PubMed – the largest repository of biomedical literature – contains more than 24 million articles.",1 Introduction,[0],[0]
"Thus, there is a need for concise presentation of important findings in the scientific articles being published.",1 Introduction,[0],[0]
Text summarization of scientific articles is a method for such presentation.,1 Introduction,[0],[0]
"One obvious form of scientific summaries, is the abstract of the articles.",1 Introduction,[0],[0]
Another type of scientific summaries relates to citance-based summaries which are summaries created using the set of citations to a reference article.,1 Introduction,[0],[0]
"This kind of summary covers some aspects of the reference article which might not be present in its abstract (Elkiss et al., 2008).
",1 Introduction,[0],[0]
"Citances often cover important and novel insights about findings or aspects of a paper that others
have found interesting; thus, they capture contributions that had an impact on the research community (Elkiss et al., 2008; Qazvinian and Radev, 2008).
",1 Introduction,[0],[0]
"In the past, many have focused on citance extraction and citance-based summarization.",1 Introduction,[0],[0]
"Example of citance extraction include (Siddharthan and Teufel, 2007), who used a machine learning approach with linguistic, lexical, statistical and positional features, and (Kaplan et al., 2009), who studied a coreference resolution based approach.",1 Introduction,[0],[0]
Citance extraction has been also studied in the context of automatic summarization.,1 Introduction,[0],[0]
"For example, (Qazvinian and Radev, 2010) proposed a framework based on probabilistic inference to identify citances, while (Abu-Jbara and Radev, 2011) approached the problem as a classification task.",1 Introduction,[0],[0]
"In the biomedical domain, the use of citances was first studied by (Nakov et al., 2004).
",1 Introduction,[0],[0]
"While useful, citances by themselves lack the appropriate evidence to capture the exact content of the original paper, such as circumstances, data and assumptions under which certain findings were obtained.",1 Introduction,[0],[0]
"Citance-based summaries might also modify the epistemic value of a claim presented in the cited work (De Waard and Maat, 2012); that is, they might report a preliminary result or a claim as a definite fact (example in figure 1).
",1 Introduction,[0],[0]
"Recently, a new track at TAC has been introduced to explore ways to generate better citance-based
1042
summaries1.",1 Introduction,[0],[0]
"One way to achieve this, is to link citances to text spans in the reference article to obtain a more informative collection of sentences representing the reference article (figure 2).",1 Introduction,[0],[0]
"A framework designed to solve such problem requires two components: (i) a method to identify the most relevant spans of text in the reference text and (ii) a system to automatically generate a summary given a set of citances and reference spans.
",1 Introduction,[0],[0]
"In this paper, we propose an information retrieval approach designed to address the first task.",1 Introduction,[0],[0]
"We explore the impact of several query reformulation techniques – some domain independent, others tailored to biomedical literature – on the performance of the system.",1 Introduction,[0],[0]
"Furthermore, we apply combined reformulations, which yields an additional improvement over any single method (25% over the baseline).
",1 Introduction,[0],[0]
"As a related area, passage retrieval in biomedical articles has been studied in the context of the genomics track (Hersh et al., 2006; Hersh et al., 2007) and in following efforts (Urbain et al., 2008; Urbain et al., 2009; Chen et al., 2011).",1 Introduction,[0],[0]
"In these works, the goal is to find passages that relate to a given term or keyword (e.g. GeneRIF).",1 Introduction,[0],[0]
"In contrast, our system considers citances as queries, which are substantially longer than keyword-based queries and have a syntactical structure.
",1 Introduction,[0],[0]
"In summary, our contributions are: (i)",1 Introduction,[0],[0]
"A search-based, unsupervised (thus easily scalable to other domains) approach to citance-reference spans matching and (ii) adaptation of various query reformulation techniques for the citatnce-refrence span matching.",1 Introduction,[0],[0]
The goal of the proposed system is to retrieve text spans from the reference paper that match the finding(s) each citance is referring to.,2 Methodology,[0],[0]
We approach this problem as a search task.,2 Methodology,[0],[0]
That is we consider the citance as a query and the reference text spans as documents.,2 Methodology,[0],[0]
"Then, using a retrieval model along with query reformulation, we find the most relevant text spans to a given citance.",2 Methodology,[0],[0]
"Our methodology consist of the following steps:
1.",2 Methodology,[0],[0]
"Create sentence level index from the reference article.
",2 Methodology,[0],[0]
"1http://www.nist.gov/tac/2014/BiomedSumm/
2.",2 Methodology,[0],[0]
Apply query reformulation to the given citance and retrieve the most relevant spans.,2 Methodology,[0],[0]
3.,2 Methodology,[0],[0]
Rerank and merge the retrieved spans that correctly describe the citance.,2 Methodology,[0],[0]
We will describe each step in the following sections.,2 Methodology,[0],[0]
"To create an index of spans, each reference article is tokenized at a sentence level using the Punkt tokenize (Kiss and Strunk, 2006).",2.1 Creating the index,[0],[0]
"Because each relevant reference span in the reference text can be formed by several consecutive sentences (according to the annotation guidelines, each span can consist of one up to five consecutive sentences), we index text spans comprised of one up to five sentences.",2.1 Creating the index,[0],[0]
"We evaluated the performance of several retrieval models during experimentation, i.e. vector space model (Salton et al., 1975), probabilistic BM25 (Robertson and Zaragoza, 2009), divergence from randomness (DFR) (Amati and Van Rijsbergen, 2002), and language models (Ponte and Croft, 1998) with Dirichlet priors.",2.2 Retrieval model,[0],[0]
All models showed very similar performances (with only DFR constantly underperforming all other models) and we did not observe any statistically significant differences between each set of runs.,2.2 Retrieval model,[0],[0]
"Therefore, we opted for the vector space model as our retrieval model.",2.2 Retrieval model,[0],[0]
We apply several query reformulation techniques to the citance to better retrieve the related text spans.,2.3 Query reformulation,[0],[0]
We leverage both general and domain specific query reformulations for this purpose.,2.3 Query reformulation,[0],[0]
"Specifically,
we use biomedical concepts, ontology information, keyphrases and the syntactic structure of the citance.
2.3.1.",2.3 Query reformulation,[0],[0]
"Unmodified query (baseline): The citance after removing stop words, numeric values and citation markers (i.e. the actual indicator of the citation) serves as our baseline.
2.3.2.",2.3 Query reformulation,[0],[0]
Biomedical concepts (UMLS-reduce): We remove from the query those terms that do not map to any medical concept in the UMLS1 metathesaurus.,2.3 Query reformulation,[0],[0]
"We use MetaMap (Aronson, 2001) to map biomedical expressions in the citances to UMLS concepts.",2.3 Query reformulation,[0],[0]
"More specifically, our heuristic greedily matches the longest expressions in the citance to concepts in the UMLS metathesaurus; such strategy was deemed the most appropriate after experimenting with various matching approaches.",2.3 Query reformulation,[0],[0]
"We limited the scope of UMLS-reduce to SNOMED Clinical Terms (Bos et al., 2006) collection of UMLS and the “preferred concepts” (i.e., concepts that are determined by the National Library of Medicine to provide the best representation for a concept); terms that are not mapped to any UMLS concept were removed.
2.3.3.",2.3 Query reformulation,[0],[0]
"Noun phrases (NP): Citances include many important biological concepts, often appearing as noun phrases.",2.3 Query reformulation,[0],[0]
"For this reason, we reformulate citance by only keeping noun phrases and filtering out other parts of speech.",2.3 Query reformulation,[0],[0]
"We retain noun phrases that consist of up to 3 terms, as longer phrases were empirically determined to be too specific.",2.3 Query reformulation,[0],[0]
"Stopwords are removed from noun phrases.
",2.3 Query reformulation,[0],[0]
2.3.4.,2.3 Query reformulation,[0],[0]
Keyword based (KW): We consider a statistical measure for identifying key terms in the citance.,2.3 Query reformulation,[0],[0]
"Specifically, we computed the idf 2 of the terms in the citance in a domain-specific corpus to evaluate their importance.",2.3 Query reformulation,[0],[0]
"Given the domain of our dataset, we used the Open Access Subset of PubMed Central3.",2.3 Query reformulation,[0],[0]
"We filter out the terms whose idf value is less than a fixed threshold (after empirical evaluation, this threshold was set to 2.5).
2.3.5.",2.3 Query reformulation,[0],[0]
Biomedical expansion (UMLS-expand): The terminology used by the citing author and the referenced author is not necessarily identical.,2.3 Query reformulation,[0],[0]
"Multiple 1http://www.nlm.nih.gov/research/umls/ 2Inverted Document Frequency 3http://www.ncbi.nlm.nih.gov/pmc/
terms or multi-word expressions can be mapped to the same concepts and each author might use their own choice of terms for describing a concept.",2.3 Query reformulation,[0],[0]
"In this approach, we add related terminology to the important concepts in the citance to solve this issue.",2.3 Query reformulation,[0],[0]
"Since our dataset consists of articles from biomedical literature, we took advantage of the UMLS metathesaurus to expand terms or multi-word expressions with their synonyms.",2.3 Query reformulation,[0],[0]
We did not enforce any threshold for the number of terms added by UMLS-expand.,2.3 Query reformulation,[0],[0]
"However, in order to prevent query drift, we expanded citances using only UMLS’s ”preferred concepts” and concepts from the ”SNOMED Clinical Terms” (SNOMED CT) terminology.
2.3.6.",2.3 Query reformulation,[0],[0]
"Combined reformulation: Due to the narrative structure of citances and their relative long length, using all citance terms for expansion is likely to cause query drift.",2.3 Query reformulation,[0],[0]
"Therefore, we first reduce the citance using one of previously described reduction approaches and then apply query expansion.",2.3 Query reformulation,[0],[0]
"In detail, we evaluated the combination of noun phrases and UMLS expansion, as well as UMLS reduction and expansion.",2.3 Query reformulation,[0],[0]
"Due to our indexing strategy described in section 2.1, some text spans retrieved by the search engine could overlap with each other.",2.4 Combining retrieved spans,[0],[0]
"Intuitively, if a span containing multiple contiguous sentences {s1, . .",2.4 Combining retrieved spans,[0],[0]
.,2.4 Combining retrieved spans,[0],[0]
", sl} is retrieved alongside any of its constituent sentences si, its relevance score should be increased to account for the relevance of si.
",2.4 Combining retrieved spans,[0],[0]
We exploited such intuition by adding the score of each span with the score of any of the constituent sentences or sub-spans retrieved alongside it.,2.4 Combining retrieved spans,[0],[0]
"After the score is updated, the constituent sentences or sub-spans are removed from the list of retrieved results.",2.4 Combining retrieved spans,[0],[0]
"Finally, because the number of reference spans indicated by the annotators in our data set is at most three, the system returns the top three results.
",2.4 Combining retrieved spans,[0],[0]
"It is worth mentioning that we also looked at some other query reformulation approaches such as pseudo relevance feedback (Buckley et al., 1995) and Wikipedia based biomedical term filtering (Cohan et al., 2014); however, our experimentations should that these methods performed substantially worse than the baseline, consequently, we do not report those results nor their relevant discussions.",2.4 Combining retrieved spans,[0],[0]
The system was evaluated on TAC 2014 Biomedical Summarization track training dataset.,3 Evaluation and Dataset,[0],[0]
"It consists of 20 topics, each of which contains between 10 to 20 citing articles and 1 reference article.",3 Evaluation and Dataset,[0],[0]
"For each topic, four domain experts were asked to identify the appropriate reference spans for each citance in the reference text.",3 Evaluation and Dataset,[0],[0]
"To better understand the dataset, we analyzed the agreement between annotators (table 1).",3 Evaluation and Dataset,[0],[0]
"This table shows that the overall agreement is relatively low.
",3 Evaluation and Dataset,[0],[0]
We used two sets of metrics for evaluation of the task.,3 Evaluation and Dataset,[0],[0]
The first one is based on the weighted overlaps between the retrieved spans and the correct spans designated by annotators and is meant to reward spans overlapping with the ground truth.,3 Evaluation and Dataset,[0],[0]
"Weighted recall and precision for a system returning span S with respect to a set of M annotators, consisting of gold spans G1, ..., GM are defined as follows:
",3 Evaluation and Dataset,[0],[0]
Recall def=,3 Evaluation and Dataset,[0],[0]
"∑M
i=1 |S",3 Evaluation and Dataset,[0],[0]
∩Gi|∑M i=1,3 Evaluation and Dataset,[0],[0]
"|Gi|
Prec def=",3 Evaluation and Dataset,[0],[0]
"∑M
",3 Evaluation and Dataset,[0],[0]
i=1 |S ∩Gi|,3 Evaluation and Dataset,[0],[0]
"M × |S| (1)
",3 Evaluation and Dataset,[0],[0]
"The overall score of the system is the mean F-1 (harmonic mean of the weighted precision and recall) over all the topics.
",3 Evaluation and Dataset,[0],[0]
"Based on the weighted F-1 score, a method could be penalized for retrieving any spans that are not indicated as gold spans by the annotators.",3 Evaluation and Dataset,[0],[0]
"Even if those spans are semantically similar to the gold spans, they will not receive any score.",3 Evaluation and Dataset,[0],[0]
"This is not ideal because, as the high disagreement shown in table 1 implies, gold spans by offset locations are highly controversial.",3 Evaluation and Dataset,[0],[0]
"For this reason, we also considered ROUGE-L (Lin, 2004) as another evalua-
tion metric, as it rewards a method for retrieving spans that are similar to the gold spans.",3 Evaluation and Dataset,[0],[0]
"Specifically, ROUGE-L, takes into account the sentence similarity by considering the longest in sequence n-grams between the retrieved spans and gold spans.",3 Evaluation and Dataset,[0],[0]
"The problem of matching citations with cited spans in scientific articles is a new task and to the best of our knowledge, there is no prior work on this task.",4 Results and discussion,[0],[0]
"Thus to evaluate the effectiveness of our different methods, we compared the performance of our proposed approaches against the unmodified query baseline.",4 Results and discussion,[0],[0]
"The results are shown in Table 2.
",4 Results and discussion,[0],[0]
"Interestingly, we observe that UMLS-reduce performs worse than the baseline in terms of F-1.",4 Results and discussion,[0],[0]
This can be attributed to the fact that multiple expressions in the biomedical literature can be used to refer to the same concept.,4 Results and discussion,[0],[0]
"Such diversity is not captured by UMLS-reduce, as it only performs query reduction.",4 Results and discussion,[0],[0]
"Moreover, a citance often contains expressions that, while not mapping to any biomedical concepts, provide useful context and therefore are fundamental in conveying the meaning of the citance (we will refer to such expressions as supporting expressions in the reminder of the paper).",4 Results and discussion,[0],[0]
"These supporting expressions are not captured by UMLS-reduce.
",4 Results and discussion,[0],[0]
NP outperforms the baseline (+18.8% F-1).,4 Results and discussion,[0],[0]
"This outcome is expected, as most important biomedical concepts in the citance are noun phrases.",4 Results and discussion,[0],[0]
"Moreover, supporting expressions are also captured, as most of them are noun phrases.
",4 Results and discussion,[0],[0]
"KW also shows promising results (+11.5% F-1 and +15.2% ROUGE-L F-1 improvement), proving that the idf of the terms in citance over a large biomedical corpus is a valid measure of their informativeness for this task.
",4 Results and discussion,[0],[0]
"When comparing KW and NP, we notice that the former obtains higher precision values than the latter; this outcome is reversed with respect to recall (i.e., NP’s recall is higher than KW’s).",4 Results and discussion,[0],[0]
"Such behavior can be motivated by the fact that NP, as it extracts noun phrases that are likely to appear in the gold reference span, has a higher chance of retrieving relevant sections of the reference text.",4 Results and discussion,[0],[0]
"However, NP is more likely to retrieve non-relevant spans, as the extracted noun phrases, which are often describing the main findings of the cited paper, are preva-
lent throughout the reference article.",4 Results and discussion,[0],[0]
"On the other hand, KW selects highly discriminative terms which are highly effective in retrieving some relevant reference spans, but might not appear in others.
",4 Results and discussion,[0],[0]
"We observe that UMLS-expand, by adding related concepts to the query, achieves significant improvement over the baseline in terms of recall (+8.1%).",4 Results and discussion,[0],[0]
"Such improvement is expected, as UMLS-expand augments the citance with all possible formulations of the detected biomedical concepts.",4 Results and discussion,[0],[0]
"However, its precision is only comparable with the baseline, as it does not remove any noisy terms from the citance.",4 Results and discussion,[0],[0]
"Interestingly, we notice that its ROUGE-L precision greatly outperforms the baseline (+22.2%).",4 Results and discussion,[0],[0]
"This behavior is motivated by the fact that UMLS-expand, even when not retrieving all the correct reference spans, extracts certain parts of the reference articles that share many biomedical concepts with the gold spans, thus achieving high structural similarity.
",4 Results and discussion,[0],[0]
The two combined methods (NP + UMLS-expand and UMLS-reduce + UMLS-expand) obtain the best overall performance compared to the baseline.,4 Results and discussion,[0],[0]
UMLS-reduce + UMLS-expand obtains the highest recall among all methods.,4 Results and discussion,[0],[0]
This outcome directly depends on the fact that all the synonyms of a certain biomedical concept are captured using UMLSexpand.,4 Results and discussion,[0],[0]
"However, unlike UMLS-expand, this combined method also achieves statistically significant improvement in terms of precision, as UMLS-reduce removes terms that can cause query drift.
",4 Results and discussion,[0],[0]
"NP + UMLS-expand has the highest overall performance, achieving a 25.9% increase over the baseline in terms of F-1, and an 18.8% increase in terms of ROUGE-L F-1.",4 Results and discussion,[0],[0]
"As previously mentioned, noun phrases are highly effective in identifying relevant biomedical concepts, as well as supporting expres-
sions.",4 Results and discussion,[0],[0]
"Given the addition of UMLS-expand, synonyms of the extracted noun phrases are also considered, further increasing the chance of retrieving relevant reference spans.
",4 Results and discussion,[0],[0]
"The limited performance of all methods in terms of the overall weighted F-1 and ROUGE-L scores is expected due to the difficulty of the task, as further corroborated by the low agreement between annotators.",4 Results and discussion,[0],[0]
"As previously stated, this makes the task particularly challenging for any system, as identifying the most appropriate reference spans is highly nontrivial even for domain experts.",4 Results and discussion,[0],[0]
"Nevertheless, while full agreement between domain experts is not present, as it is shown in table 1, more than 60% of the time, annotators agree – at least partially – on the position of the reference spans.",4 Results and discussion,[0],[0]
This makes the task worth exploring.,4 Results and discussion,[0],[0]
"In this paper, we propose an information retrieval approach for the problem of matching reference text spans with citances.",5 Conclusion,[0],[0]
Our approach takes advantage of several general and domain specific query reformulation techniques.,5 Conclusion,[0],[0]
Our best performing method obtains a significant increase over the baseline (25.9% F-1).,5 Conclusion,[0],[0]
"However, as the absolute performance of the system indicates, the task of identifying matching reference spans to a given citance is highly non trivial.",5 Conclusion,[0],[0]
This fact is also reflected by the high disagreement between domain experts annotations and suggests that further exploration of the task is needed.,5 Conclusion,[0],[0]
This work was partially supported by NSF (grant CNS-1204347).,Acknowledgments,[0],[0]
Citation sentences (citances) to a reference article have been extensively studied for summarization tasks.,abstractText,[0],[0]
"However, citances might not accurately represent the content of the cited article, as they often fail to capture the context of the reported findings and can be affected by epistemic value drift.",abstractText,[0],[0]
Following the intuition behind the TAC (Text Analysis Conference) 2014,abstractText,[0],[0]
"Biomedical Summarization track, we propose a system that identifies text spans in the reference article that are related to a given citance.",abstractText,[0],[0]
We refer to this problem as citance-reference spans matching.,abstractText,[0],[0]
"We approach the problem as a retrieval task; in this paper, we detail a comparison of different citance reformulation methods and their combinations.",abstractText,[0],[0]
"While our results show improvement over the baseline (up to 25.9%), their absolute magnitude implies that there is ample room for future improvement.",abstractText,[0],[0]
Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach,title,[0],[0]
"Modern datasets, from text documents and images to social graphs, are often represented as a large matrix A ∈ Rm×n.",1. Introduction,[0],[0]
"In many application domains, including database queries, data mining, network transactions and sensor networks (see e.g. (Liberty, 2013; Wei et al., 2016; Huang & Kasiviswanathan, 2015) for recent examples), the input matrix A is presented to the algorithm as a data stream, i.e., a sequence of items/updates that can take several forms.",1. Introduction,[0],[0]
"In the entry-wise (or insertion-only) model, each item specifies (i, j, Aij) and provides the value of one entry, in arbitrary order (and the unspecified entries are set to 0).",1. Introduction,[0],[0]
"The roworder model is similar, except that the items follow the nat-
*Equal contribution 1Johns Hopkins University 2ETH Zurich 3Weizmann Institute of Science 4Nanyang Technological University 5Carnegie Mellon University 6Princeton University.",1. Introduction,[0],[0]
"Correspondence to: Lin F. Yang <lin.yang@princeton.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
ural order (sorted with i as the primary key, and j as the secondary one).",1. Introduction,[0],[0]
"In the turnstile model, each stream item has the form (i, j, δ) and represents an update Aij ← Aij + δ for δ ∈ R (after initializing A to the all-zeros matrix).",1. Introduction,[0],[0]
"These models capture different access patterns, but all three can represent sparse matrices quite efficiently, because zero entries are implicit.",1. Introduction,[0],[0]
"As usual, the key parameters of an algorithm in the data-stream model are its memory (also referred to as storage/space requirements) and its running time (per update and to report its output).",1. Introduction,[0],[0]
"Many properties of a matrix are directly related to its spectral characteristics, i.e., its singular values.",1. Introduction,[0],[0]
"For example, the number of non-zero singular values is just the matrix rank, which determines the degrees of freedom of a corresponding linear system; the maximum and minimum singular values of a matrix determine its condition number, which in turn determines the hardness of many problems, such as optimization problems; the leading singular values of a matrix determine how well a matrix can be represented by the principal components; and so forth.",1. Introduction,[0],[0]
"It is generally hard to compute directly the singular values of a matrix, especially in the streaming model, but luckily, the Schatten norms of the matrix can often be used as surrogates for its spectrum, see e.g. (Zhang et al., 2015; Kong & Valiant, 2016; Di Napoli et al., 2016; Khetan & Oh, 2017).",1. Introduction,[0],[0]
"Formally, the Schatten p-norm of a matrix A ∈ Rm×n is defined, for every p ≥ 1, as
‖A‖Sp := (∑ j≥1 σ p j )1/p ,
where σ1 ≥ · · · ≥ σmin(m,n) are the singular values of A.",1. Introduction,[0],[0]
This definition naturally extends to all 0 <,1. Introduction,[0],[0]
"p < 1 although then it is not a norm, and also to p = 0,∞ by taking the limit.",1. Introduction,[0],[0]
"This is a very important family of matrix norms, and includes as special cases the well-known trace/nuclear norm ||A||∗",1. Introduction,[0],[0]
"= ∑ j≥1 σj = ‖A‖S1 ,",1. Introduction,[0],[0]
"the
Frobenius norm ||A||F = (∑ j≥1 σ 2 j )1/2 = ‖A‖S2 , and the spectral/operator norm ‖A‖op = σ1(A) =",1. Introduction,[0],[0]
||A||S∞ .,1. Introduction,[0],[0]
We study algorithms that approximate the Schatten p-norm of a matrix A presented in a data stream.,1. Introduction,[0],[0]
"While this problem has attracted significant attention lately (Andoni & Nguyên, 2013; Li et al., 2014; Li & Woodruff, 2016a;b; 2017), our results address three new aspects.",1. Introduction,[0],[0]
"First, we design faster and more space-efficient multi-pass algorithms.",1. Introduction,[0],[0]
"Second, we consider the row-order model, which is a com-
mon access pattern for matrix data (see, e.g. (Liberty, 2013)).",1. Introduction,[0],[0]
"Third, we design algorithms with faster update time and/or query time.",1. Introduction,[0],[0]
"The above three aspects were not considered previously for matrix norms, and our work opens the door for further diversification of prevailing models (and thereby of current algorithms).",1. Introduction,[0],[0]
"In particular, our results can be applicable to classical scenarios, e.g., where data is stored on disk (or any media where a linear scan is much faster than random access), and potentially lead to performance improvements in other such domains.",1. Introduction,[0],[0]
"In the next few subsections, we present our contributions in more detail.",1. Introduction,[0],[0]
Our first results rely on a new method for estimating the Schatten p-norm ||A||Sp of a positive semidefinite matrix (PSD) matrix A ∈ Rn×n for integer p ≥ 2.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"This method yields two new streaming algorithms in the turnstile model, which require, respectively, one pass and dp/2e passes over the input.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Both algorithms are at least as good as the previous ones in all three standard performance measures of storage, update time, and query time; and each algorithm offers significant improvements in two out of these three.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Our one-pass algorithm achieves update time O(1) compared with the previous poly(n), and query time O(nω(1−p/2))",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
", where ω ≤ 2.373 is the matrix multiplication exponent (Le Gall, 2014), compared with the previous np−2.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"And our multi-pass algorithm requires storage that is sublinear in n, compared with O(n) previously.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"We note that if p is even, then the above results extend to arbitrary A ∈ Rm×n (and not only PSD) by a standard argument.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"A detailed comparison of the bounds is given in Table 1, and the results themselves appear in Section 3.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Throughout the paper, a matrix is called sparse if it has at most O(1) non-zero entries per row and per column.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"We write Õ(f) as a shorthand for O(f · logO(1) f), and write Oa(f) to indicate that the constant in O-notation depends on some parameter a.
Techniques Our technical innovation is an unbiased estimator of tr(Ap) for a symmetric (and not only PSD) matrix A ∈ Rn×n.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"To see why this is useful, denote the eigenvalues of A by λ1 ≥ · · · ≥ λn, and observe that if A is PSD (or alternatively if p is even), then tr(Ap) =",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"∑ i λ p i =∑
i σi(A) p = ||A||pSp .",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Our estimator has the form
X := tr(G1AG T 2 G2AG T 3 · · ·GpAGT1 ), (1)
where Gi ∈ Rt×n are certain random matrices.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"This estimator X can be computed from the p bilinear sketches {GiAGTi+1}i∈[p] by straightforward matrix multiplication, where Gp+1 := G1 by convention.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"And if, say, t = O(n1−2/p), then each bilinear sketch has dimension O(t2) = O(n2−4/p).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"These determine the streaming algorithm’s storage requirement and query time, and, if the matrices
{Gi}i∈[p] have sparse columns, the updates will be fast.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The main difficulty is to bound the estimator’s variance, which highly depends on the choice of the matrices {Gi}i∈[p].",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The basics of this technique can be seen in the case p = 4, if the Gi’s satisfy the following definition.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
Definition 1.1.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"A random matrix S ∈ Rt×n is called an ( , δ, d)-Johnson-Lindenstrauss Transformation (JLT) if for every V ⊆ Rn of cardinality |V | ≤ d",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"it holds that
Pr [ ∀x ∈ V, ‖Sx‖22 ∈ (1± )‖x‖22 ] ≥ 1− δ.
",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"An ( , δ, d)-JLT can be constructed with t = O( −2 log(d/δ)) rows, which is optimal (see (Kane et al., 2011) or (Jayram & Woodruff, 2013)).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"While using independent N(0, 1/t) Gaussians entries works, there is a construction with only O( −1 log(1/δ))",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"non-zero entries per column (Kane & Nelson, 2014).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The case p = 4 has a particularly short and simple analysis, whenever G1 and G2 are independent ( , δ, n)-JLT matrices, which we can achieve with t = O( −2 log n).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The first idea is to “peel off” Gi from both sides, using that for any PSD matrix M , with high probability tr(GiMGTi ) ∈",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
(1 ± ) tr(M) (see Lemma 3.2 for a precise statement).,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
A second idea is to use the identity tr(BC) = tr(CB) to rewrite tr(AATGT2 G2AA T ) = tr(G2AA,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
TAATGT2 ).,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Now using the first idea once again, we are likely to arrive at an approximation to tr(AAT AAT ) = ||A||S4 .",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
The full details are given in Section 3.1.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The sketching method extends from p = 4 to any integer p ≥ 2, but the simple analysis above breaks (because for p > 4 the “inside” matrix M is no longer PSD) and thus our analysis is much more involved.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"We first analyze Gi’s with independent Gaussian entries, by a careful expansion of the fourth moment of X , which exploits certain cancellations occurring (only) for Gaussians.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"We then consider Gi’s that are sampled from a particular sparse JLT due to (Thorup & Zhang, 2004), and employ a symmetrizationand-decoupling argument to compare the variance of X in this case with that of Gaussian Gi’s.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
We make two technical remarks.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"First, proving E[X] = tr(Ap) is straightforward.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Indeed, by the second idea above, we can rewrite X = tr(G1AG T 2 G2AG T 3 · · ·GpAGT1 ) as X = tr(GT1 G1AG T 2 G2A · · · GTpGpA).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Now using E[GTi Gi] = I together with linearity of trace and of expectation, we obtain that E[X] = tr(Ap).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Second, after setting t = O(n1−2/p) (independent of ), our bound on the variance is O(E[X]2), which we can decrease in a standard way, taking O(1/ 2) repetitions.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
See Sections 3.2 and 3.4 for details.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The multi-pass streaming algorithm is implemented slightly differently, in that G1 ∈ R1×n, i.e., has only one row.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"The other matrices G2, . . .",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
", Gp ∈ Rt×n are as before, although we now set t = O(n1−1/(p−1)).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Our es-
timator X can be computed in dp/2e passes with space only 2t as follows.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"In the first pass, compute vectors XL ← G1AGT2 ∈ R1×t and XR ← GTpAG1 ∈ Rt×1, and then on the i-th pass update XL ← XLGTi AGi+1 and XR ← Gp−i+1AGTp−i+2XR.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
Notice that the computation in each pass is linear in A.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"For even p, after completing p/2 passes, compute and output X ′ = XLXR ∈ R (and similarly for odd p).",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"This X ′ is similar to the estimator X described above, except for the new dimensions of the Gi’s.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
See Sections 3.3 and 3.4.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
This multi-pass algorithm offers a very significant space savings over the one-pass algorithm.,1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"It is also a bit surprising because its space is getting close to the corresponding vector norm, namely, the `p-norm on Rn, for which the optimal space for O(p) passes is Õ(n1−2/p) bits.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"In fact, for the vector norm, O(p) passes do not significantly reduce the storage needed compared with one pass, which stands in sharp contrast to Schatten p-norms.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"As mentioned before, if p is even then the algorithms extends to arbitrary A ∈ Rm×n by a standard argument.",1.1. New Estimator for PSD Matrices (or Even p),[0],[0]
"Recent work (Li & Woodruff, 2016a) has improved the storage lower bound for estimating Schatten p-norms for non-integer values of p, by showing that (1 + )- approximation (in the one-pass entry-wise model) requires storage n1−g( ), for some function g( ) → 0 as → 0, even for a sparse matrix.",1.2. Lower Bound for PSD Matrices,[0],[0]
"This contrasts with our algorithms for PSD matrices (from Section 1.1), where the exponent is independent of and bounded away from 1.",1.2. Lower Bound for PSD Matrices,[0],[0]
"However, the hard distribution used by (Li & Woodruff, 2016a) is not over PSD matrices, leaving open the possibility that PSD matrices admit algorithms that use storage O(nc) for c < 1 independent of .",1.2. Lower Bound for PSD Matrices,[0],[0]
"We close this gap in Section 4, by adapting the lower bound of (Li & Woodruff, 2016a) to PSD matrices, to show, for every non-integer p > 0, a storage lower bound of Ω(n1−g
′( )) for some function g′( ) → 0 as → 0 (again, in the one-pass entry-wise model and even for a sparse matrix).",1.2. Lower Bound for PSD Matrices,[0],[0]
A key feature of our lower bounds for PSD matrices is that they hold in the model in which each entry of the matrix occurs exactly once in the stream.,1.2. Lower Bound for PSD Matrices,[0],[0]
"This models applications where the matrix resides in external memory and
is being streamed through main memory; in such a model multiple updates to an entry may not appear.",1.2. Lower Bound for PSD Matrices,[0],[0]
"While it is possible to obtain lower bounds for PSD matrices by embedding the multiplayer SET-DISJOINTNESS lower bound (Bar-Yossef et al., 2002) for vectors onto the diagonal of a matrix, to apply such lower bounds the diagonal entries need to be incremented repeatedly, that is, one such diagonal entry needs to be updated nΩ(1) times.",1.2. Lower Bound for PSD Matrices,[0],[0]
"In contrast, in our lower bounds each matrix entry occurs exactly once in the stream, i.e., there are no updates to entries.",1.2. Lower Bound for PSD Matrices,[0],[0]
"For sparse matrices, estimating Schatten p-norms in the row-order model can be reduced to estimating Schatten (p/2)-norms in the turnstile model.",1.3. Results for Row-Order Model,[0],[0]
Consider estimating ‖A‖pSp for some sparse matrixA.,1.3. Results for Row-Order Model,[0],[0]
"The algorithm first forms ATA = ∑ iA T i Ai “on the fly”, by reading each row Ai and immediately generating a stream of updates that corresponds to the non-zero entries in ATi Ai, and then it can just estimate the Schatten (p/2)-norm of that stream, because ‖ATA‖p/2Sp/2 = ‖A‖ p Sp
.",1.3. Results for Row-Order Model,[0],[0]
"Observe that each row Ai has only O(1) non-zero entries, hence also ATi Ai has only O(1) non-zero entries, and the algorithm only needs O(1) space to generate the updates to ATA.",1.3. Results for Row-Order Model,[0],[0]
"Moreover, since A is sparse, also ATA is sparse.",1.3. Results for Row-Order Model,[0],[0]
"It was shown in (Li & Woodruff, 2016a) how to estimate the Schatten p-norm, for an even integer p, using Õp, (n1−2/p) bits of space, even in the turnstile model.",1.3. Results for Row-Order Model,[0],[0]
"For p ∈ 4Z, the above yields an algorithm in the row-order model that uses Õp, (n1−4/p) bits of space for sparse matrices.",1.3. Results for Row-Order Model,[0],[0]
"In Sections C and D, we study the problem in the row-order model for all p > 0.",1.3. Results for Row-Order Model,[0],[0]
"When p is not an even integer, we prove that (1 + )-approximating the Schatten p-norm in the one-pass entry-wise model requires Ω (n1−g( )) bits of space where g( ) → 0 as → 0.",1.3. Results for Row-Order Model,[0],[0]
"This bound holds even for sparse matrices, in which case it is almost tight.",1.3. Results for Row-Order Model,[0],[0]
"When p ≥ 4 is an even integer, we prove a lower bound of Ωp(n
1−4/p) bits of space, matching up to logarithmic factors the algorithm from above for p ∈ 4Z. For the remaining case p ≡ 2 (mod 4), we present an algorithm using Õp, (n
1−4/(p+2)) space, leaving a slight polynomial gap from the lower bound of Ωp(n1−4/p).",1.3. Results for Row-Order Model,[0],[0]
"The aforementioned algorithm of (Li et al., 2014) uses a single sketching matrix G, for example, if A is PSD, then their sketch is S = GAGT , where G ∈ Rt×n is a Gaussian matrix.",1.4. Previous Work,[0],[0]
"Its estimate for ||A||Sp is produced by summing over all “cycles” Si1,i2Si2,i3 · · ·Sip,i1 , where i1, . . .",1.4. Previous Work,[0],[0]
", ip ∈",1.4. Previous Work,[0],[0]
[t] are distinct.,1.4. Previous Work,[0],[0]
Our sketch improves upon theirs in both update time and query time.,1.4. Previous Work,[0],[0]
"The only other streaming algorithm for Schatten p-norms that we are aware of is that of (Li & Woodruff, 2016a) (Theorem 7), which uses space O(n1− 2 p poly(1 , log n)) but works only for matrices that have O(1)-entries per row and per column.",1.4. Previous Work,[0],[0]
"One possible approach to improve the update time would be to replace the Gaussian matrices in (Li et al., 2014) with a distribution over matrices that admit a fast multiplication algorithm.",1.4. Previous Work,[0],[0]
"The analysis done in (Li et al., 2014) relies on the Gaussian entries (rotational invariance, in particular), so the replacement matrix should preserve the distribution of the sketch.",1.4. Previous Work,[0],[0]
"Kapralov, Potluru, and Woodruff (Kapralov et al., 2016) present just such a distribution on matrices G̃, where the multiplication G̃A can be computed quickly and G̃A is close to GA in total variation distance.",1.4. Previous Work,[0],[0]
"Unfortunately, under the distribution of (Kapralov et al., 2016), or any other with a similar guarantee on total variation distance, each coordinate update to A results in a dense rankone update to the sketch, which means that the update time is not improved.",1.4. Previous Work,[0],[0]
"Several strong lower bounds are known for approximating Schatten p-norms and other matrix functions, both for the dimension of a sketch and for storage requirement (bits).",1.4. Previous Work,[0],[0]
"Li, Nguyen and Woodruff (Li et al., 2014) prove that for 0 ≤ p < 2 every linear sketch that can approximate rank and Schatten p-norm must have dimension Ω( √ n) and every bilinear sketch must have dimension Ω(n1− ).",1.4. Previous Work,[0],[0]
"Li and Woodruff (Li & Woodruff, 2016b) show that every linear sketch for Schatten p-norms, p ≥ 2, requires dimension Ω(n2−4/p).",1.4. Previous Work,[0],[0]
"In (Li & Woodruff, 2016a), they prove space complexity lower bounds that hold even when the input matrix is sparse.",1.4. Previous Work,[0],[0]
"Specifically, they show that one-pass streaming algorithms which (1± )-approximate various functions of the singular values, including Schatten p-norms when p
is not an even integer, require Ω(n1−g( )) bits of space for some function g( )→ 0 as → 0.",1.4. Previous Work,[0],[0]
"Additional space lower bounds, e.g., for p ∈",1.4. Previous Work,[0],[0]
"[1, 2), can be deduced from a general statement of (Andoni et al., 2015), see Table 1 of (Li & Woodruff, 2016a).",1.4. Previous Work,[0],[0]
The space bounds of sketching algorithms in the turnstile model are stated in terms of sketch dimension (number of entries).,2. Notation and Preliminaries,[0],[0]
"The number of bits required can be larger by a log nM factor, where M is the absolute ratio of the largest element in the matrix to the smallest.",2. Notation and Preliminaries,[0],[0]
"We call a matrix a Gaussian matrix if its entries are independent N(0, 1) random variables.",2. Notation and Preliminaries,[0],[0]
"A matrix G of dimension t × n is a column-normalized Gaussian matrix if G = G′/ √ t, where G′ is a Gaussian matrix.",2. Notation and Preliminaries,[0],[0]
Now-standard techniques such as Nisan’s pseudorandom generator or k-wise independence can be used to derandomize Gaussian matrices for use in sketching algorithms.,2. Notation and Preliminaries,[0],[0]
Column-normalized Gaussian matrices serve as JLTs.,2. Notation and Preliminaries,[0],[0]
"In particular, there exists a constant c such that if G be a t× n column-normalized Gaussian matrix with t ≥ c 2 log",2. Notation and Preliminaries,[0],[0]
"d δ , then G is a ( , δ, d)-JLT (Indyk & Motwani, 1998).",2. Notation and Preliminaries,[0],[0]
"The main result in this section is a new one-pass streaming algorithm for estimating the Schatten p-norm, for integer p ≥ 2.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"When p is odd, it additionally requires that the input matrix is PSD.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"The first version of this algorithm, described in Section 3.2, has the same storage requirement of Õp(n2−4/p/ 2) bits as the previous algorithm of (Li et al., 2014) that uses cycle sums, but has a simpler analysis and faster query time1, which is roughly matrix multiplication time, nω , instead of np.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Moreover, it is based on a new method that leads to a dp/2e-pass algorithm with storage requirement Õp(n1−1/(p−1)/ 2) bits, as described
1In (Kong & Valiant, 2016), Kong and Valiant independently improve the algorithm in (Li et al., 2014) to the same runtime as Theorem 3.3 in this paper by considering only “increasing cycles”.
in Section 3.3.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Previously, the algorithm in Theorem 6.1 of (Woodruff, 2014) has the same number of passes but larger storage requirement O(n/ 2).2",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Finally, we improve the update time, as described in Section 3.4, by employing the sketching matrices Gi that are certain sparse matrices instead of Gaussians.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"We start in Section 3.1 with the case p = 4, which is based on the same sketch but is significantly easier to analyse.
3.1.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Schatten 4-Norm using JLT matrices Theorem 3.1.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Let G1, G2 ∈ Rt×n be independent ( , δn , 1)-JLT matrices.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Then for every A ∈ R n×m,
Pr [
tr(G1AA TGT2 G2AA TGT1 ) ∈ (1±2 )2||A||4S4 ] = 1−2δ.
Thus, one can find a (1± )-approximation to the Schatten4 norm of a general matrixA ∈ Rn×m using a linear sketch of dimension O( −2n log n).
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Before proving the theorem, we remark that if each column of Gi has only s non-zero entries, it is easy to see that the update time of this linear sketch is O(s), assuming any entry of G1 and G2 can be accessed in O(1) time (in a streaming algorithm, the entries are usually computed from a small random seed in polylog(n) time).",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"The query time is dominated by multiplying a matrix of size t×n with one of size n× t, and thus takes O(tω ·n/t) = Õ(nω/ 2(ω−1)) time.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Now we prove Theorem 3.1, for which we need the following lemma.
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Lemma 3.2.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Let G ∈ Rt×n be an ( , δ/n, 1)-JLT matrix.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Then for every PSD matrix A ∈ Rn×n,
Pr [ tr(GAGT ) ∈ (1± ) tr(A) ]",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"≥ 1− δ.
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Proof.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"By the Spectral Theorem, A = UΛUT , where Λ is a diagonal matrix and U is an orthonormal matrix.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Then G′ = GU is still an ( , δ/n, 1)-JLT.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Thus
tr(GAGT ) = tr(G′ΛG′T ) =",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"tr( √ ΛG′TG′ √ Λ)
= n∑ i=1",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
λie T i G ′TG′ei = n∑ i=1,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"λi||G′ei||22.
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"By the JLT guarantee and a union bound, with probability at least 1−δ, for all i ∈",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
[n] we have ||G′ei||22 ∈,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"[1− , 1+ ], in which case tr(GAGT ) ∈ (1± ) tr(A).
of Theorem 3.1.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Apply Lemma 3.2 to the PSD matrix AATAAT , to get that with probability at least 1 − δ (over the choice of G2),
tr(G2AA TAATGT2 ) ∈ (1± 2 ) tr(AATAAT )
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"= (1± 2 )||A||S4 , 2We note that also in Theorem 6.1 of (Woodruff, 2014)",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"it is required that p is even or that the input matrix is PSD, but this is erroneously omitted.
where the left-hand side is equal to tr(AATGT2 G2AA T ), by the identity tr(MMT ) = tr(MTM).",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"Now suppose (by conditioning) that G2 is already fixed, and apply the same lemma to the PSD matrixAATGT2",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"G2AA
T , to get that with probability at least 1− δ (over the choice of G1),
tr(G1AA TGT2 G2AA TGT1 ) ∈ (1±2 )",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"tr(AATGT2 G2AAT ).
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
The proof follows by a union bound.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"The linear sketch of A consists of the two matrices G1A and G2A, which suffices to estimate ||A||4S4 as above with δ = 1/8.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"This sketch is linear and its dimension is 2tn, where we can use say Gaussians to obtain t =",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"O( −2 log n).
3.2.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Schatten p-norm,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Using Gaussians We now design a sketch for Schatten-p norm that uses column-normalized Gaussian matrices.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"We will later extend and refine it to improve the per-update processing time.
",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
Theorem 3.3.,3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"For every 0 < < 1/2 and integer p ≥ 2, there is an algorithm that outputs a (1 ± )- approximation to the Schatten-p norm of a PSD matrix A ∈ Rn×n using a randomized linear sketch of dimension s = Op( −2n2−4/p).",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
"The update time (for each entry in A) is O(s) and the query time (for computing the estimate) is O( −2n(1−2/p)ω), where ω < 2.373 is the matrix multiplication constant.",3. New Estimator for PSD Matrices (and Integer p),[0],[0]
A ∈ Rn×m.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
The first part of the theorem (for PSD matrices) follows directly from Proposition 3.4 below.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"The proposition is applicable to all symmetric matrices, but ‖A‖pSp = tr(A
p) only for PSD matrices or even p.","If p is even, the above algorithm extends to a general matrix",[0],[0]
"The linear sketch stores GiAGTi+1 for i = 1, . . .","If p is even, the above algorithm extends to a general matrix",[0],[0]
", p, where by convention Gp+1 = G1, repeated independently in parallel Op(1/ 2) times.","If p is even, the above algorithm extends to a general matrix",[0],[0]
"Thus, the sketch has dimension Op( −2t2).","If p is even, the above algorithm extends to a general matrix",[0],[0]
The estimator is obtained by computing the Op(1/ 2) independent copies of X and reporting their average.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"To analyze its accuracy, notice that a PSD matrix A satisfies E[X] = tr(Ap) = ||A||pp.","If p is even, the above algorithm extends to a general matrix",[0],[0]
Then setting t = n1−2/p gives Var(X) ≤ Op(||A||Sp)2p and averaging multiple independent copies of X reduces the variance.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"The second part (for general matrices), follows by using the same sketch for the symmetric matrix B = ( 0 A AT 0 ) , because the nonzero singular values of B are those of A repeated twice and ||B||pSp = 2||A|| p Sp
= 2 tr(Ap), where the last equality uses the assumption that p is even.","If p is even, the above algorithm extends to a general matrix",[0],[0]
"Because the correctness of the algorithm comes from bounding the variance of X , it is enough that the entries in each Gaussian matrix are four-wise independent, which is crucial for applications with limited storage like streaming.
","If p is even, the above algorithm extends to a general matrix",[0],[0]
Proposition 3.4.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"For integer p ≥ 2 and t ≥ 1, let
G1, . . .","If p is even, the above algorithm extends to a general matrix",[0],[0]
", Gp be independent t × n column-normalized Gaussian matrices.","If p is even, the above algorithm extends to a general matrix",[0],[0]
"Then for every symmetric matrix A ∈ Rn×n, the estimator X = tr ( G1AG T 2 G2A . . .G","If p is even, the above algorithm extends to a general matrix",[0],[0]
"T p GpAG T 1 ) satisfies
E[X] = tr(Ap) and Var(X) =
Op 1+b p2 c+1∑ z=2 ( n1− 2 p t )z","If p is even, the above algorithm extends to a general matrix",[0],[0]
+ p∑,"If p is even, the above algorithm extends to a general matrix",[0],[0]
z=2 ( n1− 2 z t )z ||A||2pSp .,"If p is even, the above algorithm extends to a general matrix",[0],[0]
The full proof of this proposition is postponed to Section E. We outline the general idea here.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"It is standard that a Gaussian matrix is rotational invariant, i.e., G and GU are identically distributed for any orthogonal matrix U .","If p is even, the above algorithm extends to a general matrix",[0],[0]
"Thus, by the Spectral Theorem, instead of considering symmetric matrix A = UΛUT , we can consider only its diagonalization Λ. The proof of this proposition proceeds first by expanding X in terms of inner products of columns of the matrix G, i.e., X = ∑ i1,i2,...,ip∈[n] λi1λi2 . . .","If p is even, the above algorithm extends to a general matrix",[0],[0]
λip ·,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"〈g (1) i1 , g (1) i2 〉· 〈g(2)i2 , g (2) i3 〉 . . .","If p is even, the above algorithm extends to a general matrix",[0],[0]
"〈g(p)ip , g (p) i1 〉, where λi is the i-th eigenvalue of A and g(j)ij is the ij-th column of Gj .","If p is even, the above algorithm extends to a general matrix",[0],[0]
We then expand E(X2).,"If p is even, the above algorithm extends to a general matrix",[0],[0]
The non-zero terms in E(X2) are composed by only those terms of even powers in every eigenvalue.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
"Computing the expectation of each term is straightforward because the entries of G are independent Gaussian random variables, but the crux of the proof is in bounding the sum of the terms.","If p is even, the above algorithm extends to a general matrix",[0],[0]
We introduce a collection of diagrams that aid in enumerating the terms according to their structure and computing the sum.,"If p is even, the above algorithm extends to a general matrix",[0],[0]
The proof of Proposition 3.4 relies on the matricesGi being Gaussians in two places.,3.3. Multi-Pass Algorithm,[0],[0]
"First, we assume that the matrix A is diagonal, and in general we need to consider GiU instead of Gi.",3.3. Multi-Pass Algorithm,[0],[0]
"Second, the columns of these matrices have small variance/moments, as described in (7)-(8).",3.3. Multi-Pass Algorithm,[0],[0]
"We now generalize the proof to relax these requirements (e.g., to 4-wise independence) and obtain a multi-pass algorithm.
",3.3. Multi-Pass Algorithm,[0],[0]
Lemma 3.5.,3.3. Multi-Pass Algorithm,[0],[0]
For integers p ≥ 2 and 1 ≤ t′ ≤,3.3. Multi-Pass Algorithm,[0],[0]
"t, let G1 ∈ Rt′×n and G2, . . .",3.3. Multi-Pass Algorithm,[0],[0]
", Gp ∈ Rt×n be independent columnnormalized Gaussian matrices with 4-wise independent entries.",3.3. Multi-Pass Algorithm,[0],[0]
"Then for every symmetric matrix A ∈ Rn×n, the estimator X = tr ( G1AG T 2 G2A . . .G",3.3. Multi-Pass Algorithm,[0],[0]
"T pGpAG T 1 ) satisfies
E[X] = tr(Ap) and Var(X) =
Op
( 1 + bp/2c∑ z=2 nz−1−2(z−1)/p",3.3. Multi-Pass Algorithm,[0],[0]
t′tz−1 +,3.3. Multi-Pass Algorithm,[0],[0]
p∑ z=2,3.3. Multi-Pass Algorithm,[0],[0]
"nz−2 t′tz−1 ) ‖A‖2pSp .
",3.3. Multi-Pass Algorithm,[0],[0]
"The proof of this lemma is postponed to Section F. It is a direct corollary of the proof of Proposition 3.4, except that t′, the size of the first sketch matrix, is emphasized.
",3.3. Multi-Pass Algorithm,[0],[0]
"We can now use the above sketch to approximate the Schatten p-norm using Õ(n1−1/(p−1)) bits of space with dp/2e passes over the input.
Theorem 3.6.",3.3. Multi-Pass Algorithm,[0],[0]
Let p ≥ 2 be an even integer.,3.3. Multi-Pass Algorithm,[0],[0]
"There is a dp/2e-pass streaming algorithm, that on input matrix A ∈ Rn×m with n ≥ m given as a stream, outputs an estimate X such that with probability at least 0.9, X ∈ (1± )||A||pSpand uses Op(n
1−1/(p−1)/ 2) words of space.",3.3. Multi-Pass Algorithm,[0],[0]
The above extends to all integers p ≥ 2 if A is PSD.,3.3. Multi-Pass Algorithm,[0],[0]
The full proof is presented in Appendix A. We here sketch the proof.,3.3. Multi-Pass Algorithm,[0],[0]
"We take G1 ∈ R1×n and G2, G3, . . .",3.3. Multi-Pass Algorithm,[0],[0]
", Gp ∈ Rt×n as independent column normalized Gaussian matrix, where t = O(n1−1/(p−1)).",3.3. Multi-Pass Algorithm,[0],[0]
We then show an algorithm that computes in dp/2e-pass the estimator X = G1AG T 2 G2 . . .,3.3. Multi-Pass Algorithm,[0],[0]
"GpAG T 1 and uses space at most t. As shown in Lemma 3.5, X = tr(X) is a unbiased estimator for Schatten p-norm with constant variance.",3.3. Multi-Pass Algorithm,[0],[0]
"By repeating the algorithm O(1/ 2) times in parallel, we reach the desired accuracy.",3.3. Multi-Pass Algorithm,[0],[0]
"Since Gaussian matrices are dense, a change to one coordinate of the input matrix A may lead to a change of every entry in the sketch.",3.4. Faster Update Time,[0],[0]
This means long update times for a streaming algorithm based on the sketch.,3.4. Faster Update Time,[0],[0]
"In this section we extend our result for Gaussian sketching matrices to a distribution over {−1, 0, 1} valued matrices with only one non-zero entry per column.",3.4. Faster Update Time,[0],[0]
"The new sketch can be used to improve the update time of algorithms in the last two sections.
",3.4. Faster Update Time,[0],[0]
Definition 3.7 (Sparse ZD-sketch).,3.4. Faster Update Time,[0],[0]
"Let Dt,n be the distribution over matrices G := ZD ∈ Rt×n, where Z = (z1, z2, . . .",3.4. Faster Update Time,[0],[0]
", zn) ∈ Rt×n and D = diag(d1, d2, . .",3.4. Faster Update Time,[0],[0]
.,3.4. Faster Update Time,[0],[0]
", dn) are generated as follows.",3.4. Faster Update Time,[0],[0]
Let h :,3.4. Faster Update Time,[0],[0]
"[n] → [t] be a 4-wise independent hash function, and set Zi,j = 1{i=h(j)}, i.e., in each zj only the h(j)-th coordinate is set to 1, and all other coordinates are 0.",3.4. Faster Update Time,[0],[0]
"The diagonal entries of D are four-wise independent uniform {−1, 1} random variables, and D is independent from Z.
Notice that each column of G has a single non-zero entry, which is actually a random sign, and the n columns are four-wise independent.",3.4. Faster Update Time,[0],[0]
"This random matrix G is similar to the sketching matrix used in (Thorup & Zhang, 2004) to speed up the update time when estimating the second frequency moment of a vector in Rn.",3.4. Faster Update Time,[0],[0]
"Also note that the ZDsketch is a version of sparse JL matrices (see e.g., (Kane & Nelson, 2014; Dasgupta et al., 2010)).",3.4. Faster Update Time,[0],[0]
In this paper we do not aim at optimizing the sparsity as we focus on approximating Schatten norms.,3.4. Faster Update Time,[0],[0]
It is fairly easy to show that ZD-sketch works for approximating Schatten p-norm of matrices with all entries nonnegative.,3.4. Faster Update Time,[0],[0]
"The proof is presented in Section G. We now show that the conclusion of Theorem 3.3 and Theorem 3.6
still hold if we replace the Gaussian matrices in the sketch with independent samples from the sparse ZD-sketch.",3.4. Faster Update Time,[0],[0]
A major difficulty that arises in replacing the Gaussian matrix with the sparse ZD-sketch is the latter’s lack of rotational invariance.,3.4. Faster Update Time,[0],[0]
"To prove Theorem 3.3 we were able to expand X2 in terms of the eigenvalues of A and compute the expectation term-by-term, but this is not possible for the sparse ZD-sketch.",3.4. Faster Update Time,[0],[0]
"For example, let G be a Gaussian matrix.",3.4. Faster Update Time,[0],[0]
"For any orthogonal matrix U , the matrix GU is again a Gaussian matrix with an identical distribution to G.",3.4. Faster Update Time,[0],[0]
This does not hold for sparse ZD-sketch.,3.4. Faster Update Time,[0],[0]
"As a consequence, in the expansion of E(X2) in the proof of Proposition 3.4, the non-zero terms would also include those monomials of odd powers of λi(A).",3.4. Faster Update Time,[0],[0]
"For example, for the Schatten 3-norm, one cannot bound ∑ i1,i2,...i6∈[n] ∏6 j=1 λij by O(‖A‖6S3).",3.4. Faster Update Time,[0],[0]
But this term appears in the expansion of E(X2) of the Schatten 3-norm estimator if using the sparse ZD-sketch matrices.,3.4. Faster Update Time,[0],[0]
"To resolve this problem, we use a technique similar to the proof of the Hanson-Wright Inequality in (Rudelson & Vershynin, 2013) to bound the variance of X .",3.4. Faster Update Time,[0],[0]
The proof is composed of three major steps.,3.4. Faster Update Time,[0],[0]
The first step is to decouple the dependent summands by injecting independence.,3.4. Faster Update Time,[0],[0]
The second step is to replace the independent random vectors with fully independent Gaussian vectors while preserving the variance.,3.4. Faster Update Time,[0],[0]
We can then apply our techniques for Gaussians to bound the variance of the final random variable.,3.4. Faster Update Time,[0],[0]
"The case p = 1 is useful to illustrate the technique, even though Schatten 1-norm approximation can be easily accomplished in other ways.",3.4. Faster Update Time,[0],[0]
LetG ∈ Rt×n be the sparse JLT matrix and let A ∈ Rn×n be PSD.,3.4. Faster Update Time,[0],[0]
"The sketch is GAGT and
tr(GAGT )− tr(A)",3.4. Faster Update Time,[0],[0]
= ∑,3.4. Faster Update Time,[0],[0]
"i 6=j ai,j〈gi, gj〉.",3.4. Faster Update Time,[0],[0]
"(2)
Since i 6= j, gi and gj are independent.",3.4. Faster Update Time,[0],[0]
However the summands are subtly dependent.,3.4. Faster Update Time,[0],[0]
"We first decouple the summand by choosing δi ∼Bernoulli(1/2), and write 〈gi, gj〉 = 4E(δi(1 − δj)〈gi, gj〉).",3.4. Faster Update Time,[0],[0]
"Let V = {i : δi =
1}, then ∑ i 6=j ai,j〈gi, gj〉 = 4Eδ ∑ i∈V,j∈V̄ ai,j〈gi, gj〉.",3.4. Faster Update Time,[0],[0]
"Thus conditioning on δ and {gj : j ∈ V̄ }, the set {〈gi, ∑ j∈V̄ ai,jgj〉 : i ∈ V } is a set of independent random variables.",3.4. Faster Update Time,[0],[0]
"We can match these random variables with Gaussian random variables of the same variance, and thus replace gi with independent Gaussian vectors.",3.4. Faster Update Time,[0],[0]
"The same process can be repeated for gj : j ∈ V̄ , and replace every vector gi :",3.4. Faster Update Time,[0],[0]
i ∈,3.4. Faster Update Time,[0],[0]
[n] by independent Gaussian vectors.,3.4. Faster Update Time,[0],[0]
"This lets us apply similar techniques as used in the proof of Proposition 3.4 to bound the variance of the resulting random variable, and thus bound the variance of the original random variable tr(GAGT )− tr(A).",3.4. Faster Update Time,[0],[0]
"The analogue of (2) for the case of our general estimator, X − tr(Ap), is much more complicated than the p = 1 case.",3.4. Faster Update Time,[0],[0]
"We observe that these terms can be grouped as a sum of products of consecutive walks, i.e., ai1,i2ai2,i3 . . .",3.4. Faster Update Time,[0],[0]
"aiz,jz+1〈g (z+1) jz+1 , g (z+1) iz+1
〉 for some z. Notice that 〈g(z
′) j′",3.4. Faster Update Time,[0],[0]
", g (z′) j′",3.4. Faster Update Time,[0],[0]
〉 = 1 for any j′ and z′.,3.4. Faster Update Time,[0],[0]
"For each walk, we
can apply a similar idea to replace the gi vectors with independent Gaussian vectors.",3.4. Faster Update Time,[0],[0]
"Again, we apply similar techniques as used in the proof of Proposition 3.4 to bound the variance of each group.",3.4. Faster Update Time,[0],[0]
"As a result, when replacing the Gaussian matrices by sparse JLT matrices, Lemma 3.5 still holds.",3.4. Faster Update Time,[0],[0]
"Using the sparse ZD-sketch, we are able to achieve the same space bound and query time as in Theorem 3.3 and Theorem 3.6.",3.4. Faster Update Time,[0],[0]
But our update time is improved to O(1/ 2).,3.4. Faster Update Time,[0],[0]
We present the full statement of our theorem below.,3.4. Faster Update Time,[0],[0]
"The full proof can be found in (Braverman et al., 2016).
",3.4. Faster Update Time,[0],[0]
Theorem 3.8.,3.4. Faster Update Time,[0],[0]
"For every 0 < < 1/2 and integer p ≥ 2, there is a randomized one-pass streaming algorithm",3.4. Faster Update Time,[0],[0]
"A with space requirement O(n2−4/p/ 2), that given as input a PSD matrix A ∈ Rn×n, outputs with high probability a (1 + )-approximation of ||A||pSp .",3.4. Faster Update Time,[0],[0]
"The algorithm processes an update in time O(1/ 2), and computes the output (after the updates) in time O(n(1−2/p)ω)/ 2, where ω < 3 is the matrix multiplication constant.
",3.4. Faster Update Time,[0],[0]
"There is similarly a randomized dp/2e-pass streaming algorithm B with space requirement O(n1−1/(p−1)/ 2), update time in a pass O(1/ 2), and output time O(n(1−2/p)/ 2).",3.4. Faster Update Time,[0],[0]
"For even p ≥ 2, both algorithms extend to general input",3.4. Faster Update Time,[0],[0]
In this section we show the lower bounds for sketching Schatten-norms for PSD matrices.,4. Lower Bound For PSD Matrices,[0],[0]
This lower bounds suggest that our upper bound is nearly tight.,4. Lower Bound For PSD Matrices,[0],[0]
"The proof is presented in Section B.
Theorem 4.1.",4. Lower Bound For PSD Matrices,[0],[0]
Suppose that p > 0,4. Lower Bound For PSD Matrices,[0],[0]
"andX ∈ Rn×n is a PSD matrix given in the entry-wise streaming model.
",4. Lower Bound For PSD Matrices,[0],[0]
"(a) When p ∈ Z, there is c = c(p) > 0",4. Lower Bound For PSD Matrices,[0],[0]
"such that every one-pass streaming algorithm that (1+c)-approximates ‖X‖Sp with probability 2/3 must use Ωp(n1−2/p) bits of space for even p, and Ωp(n1−2/(p−1)) bits of space for odd",4. Lower Bound For PSD Matrices,[0],[0]
"p.
(b)",4. Lower Bound For PSD Matrices,[0],[0]
"When p 6∈ Z, for every integer t ≥ 2, there is c = c(p, t) > 0",4. Lower Bound For PSD Matrices,[0],[0]
"such that every one-pass streaming algorithm that (1 + c)-approximates ‖X‖Sp with probability 2/3 must use Ωp,t(n1−1/t) bits of space.
",4. Lower Bound For PSD Matrices,[0],[0]
"We remark that all lower bounds in Theorem 4.1 even hold for sparse matrices, since the hard instances are sparse.",4. Lower Bound For PSD Matrices,[0],[0]
"The lower bounds for non-integers p and even integers p are strengthenings of the same lower bounds in (Li & Woodruff, 2016a), and are almost tight and tight up to polylogarithmic factors, respectively.",4. Lower Bound For PSD Matrices,[0],[0]
In this section we show numerical experiments that illustrate the performance of our Schatten-norm estimator described in Section 3.,5. Experiments,[0],[0]
"We consider two sets of synthetic inputs, which roughly represent the extreme cases for all inputs.",5. Experiments,[0],[0]
"One is a matrix drawn from a standard Gaussian distribution, i.e., A = GG>, where each entry of G ∈ Rn×n is an independent N (0, 1) random variable.",5. Experiments,[0],[0]
"The other is a matrix drawn from a Bernoulli distribution, i.e., A = BB>, where each entry of B ∈ Rn×n is an independent B(0.5) random variable.",5. Experiments,[0],[0]
We chose n = 200 in both cases.,5. Experiments,[0],[0]
We construct our estimator using the native pseudo-random generator in matlab as our hash function.,5. Experiments,[0],[0]
We measure the error of our estimator when varying the hidden constant in our choice of k (recall that our sketching matrix is of size k × n for k = O(n1−2/p)) and varying p.,5. Experiments,[0],[0]
These results are presented in Figure 1.,5. Experiments,[0],[0]
"We then compared the update time of our sparse estimator with the estimators described in (Li & Woodruff, 2016a) and (Kong & Valiant, 2016) that are based on a dense Gaussian distribution.",5. Experiments,[0],[0]
The result is shown in Figure 2.,5. Experiments,[0],[0]
"Our estimators are comparably or a little less accurate than theirs but are two orders of
magnitude faster in terms of update time.",5. Experiments,[0],[0]
"In Figure 2, we used our ZD sketch from Definition 3.7.",5. Experiments,[0],[0]
"Since each update only updates a single entry to the matrix, the update time is almost 0.",5. Experiments,[0],[0]
"On the other hand, the dense Gaussian sketch requires at least Θ(n2−4/p) operations.",5. Experiments,[0],[0]
"This material is based upon work supported in part by the National Science Foundation under Grants No. 1447639, 1650041, 1652257 and CCF-1815840, the ONR Award N00014-18-1-2364, the Israel Science Foundation grant #897/13 and by a Minerva Foundation grant.",Acknowledgment,[0],[0]
"Given the prevalence of large scale linear algebra problems in machine learning, recently there has been considerable effort in characterizing which functions can be approximated efficiently of a matrix in the data stream model.",abstractText,[0],[0]
"We study a number of aspects of estimating matrix norms – an important class of matrix functions – in a stream that have not previously been considered: (1) multi-pass algorithms, (2) algorithms that see the underlying matrix one row at a time, and (3) time-efficient algorithms.",abstractText,[0],[0]
"Our multi-pass and row-order algorithms use less memory than what is provably required in the single-pass and entrywise-update models, and thus give separations between these models (in terms of memory).",abstractText,[0],[0]
"Moreover, all of our algorithms are considerably faster than previous ones.",abstractText,[0],[0]
"We also prove a number of lower bounds, and obtain for instance, a near-complete characterization of the memory required of row-order algorithms for estimating Schatten p-norms of sparse matrices.",abstractText,[0],[0]
We complement our results with numerical experiments.,abstractText,[0],[0]
"Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order",title,[0],[0]
"Bayesian optimization (BO) has become a popular and effective way for black-box optimization of nonconvex, expensive functions in robotics, machine learning, computer vision, and many other areas of science and engineering (Brochu et al., 2009; Calandra et al., 2014; Krause & Ong, 2011; Lizotte et al., 2007; Snoek et al., 2012; Thornton et al., 2013; Wang et al., 2017).",1. Introduction,[0],[0]
"In BO, a prior is posed on the (unknown) objective function, and the uncertainty given by the associated posterior is the basis for an acquisition function that guides the selection of the next point to query the function.",1. Introduction,[0],[0]
"The selection of queries and hence the acquisition function is critical for the success of the method.
",1. Introduction,[0],[0]
"Different BO techniques differ in this acquisition function.
",1. Introduction,[0],[0]
"1Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Massachusetts, USA.",1. Introduction,[0],[0]
"Correspondence to: Zi Wang <ziw@csail.mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Among the most popular ones range the Gaussian process upper confidence bound (GP-UCB) (Auer, 2002; Srinivas et al., 2010), probability of improvement (PI) (Kushner, 1964), and expected improvement (EI) (Moc̆kus, 1974).",1. Introduction,[0],[0]
"Particularly successful recent additions are entropy search (ES) (Hennig & Schuler, 2012) and predictive entropy search (PES) (Hernández-Lobato et al., 2014), which aim to maximize the mutual information between the queried points and the location of the global optimum.
",1. Introduction,[0],[0]
"ES and PES are effective in the sense that they are queryefficient and identify a good point within competitively few iterations, but determining the next query point involves very expensive computations.",1. Introduction,[0],[0]
"As a result, these methods are most useful if the black-box function requires a lot of effort to evaluate, and are relatively slow otherwise.",1. Introduction,[0],[0]
"Moreover, they rely on estimating the entropy of the arg max of the function.",1. Introduction,[0],[0]
"In high dimensions, this estimation demands a large number of samples from the input space, which can quickly become inefficient.
",1. Introduction,[0],[0]
"We propose a twist to the viewpoint of ES and PES that retains the information-theoretic motivation and empirically successful query-efficiency of those methods, but at a much reduced computational cost.",1. Introduction,[0],[0]
The key insight is to replace the uncertainty about the arg max with the uncertainty about the maximum function value.,1. Introduction,[0],[0]
"As a result, we refer to our new method as Max-value Entropy Search (MES).",1. Introduction,[0],[0]
"As opposed to the arg max, the maximum function value lives in a one-dimensional space, which greatly facilitates the estimation of the mutual information via sampling.",1. Introduction,[0],[0]
"We explore two strategies to make the entropy estimation efficient: an approximation by a Gumbel distribution, and a Monte Carlo approach that uses random features.
",1. Introduction,[0],[0]
"Our contributions are as follows: (1) MES, a variant of the entropy search methods, which enjoys efficient computation and simple implementation; (2) an intuitive analysis which establishes the first connection between ES/PES and the previously proposed criteria GP-UCB, PI and EST (Wang et al., 2016), where the bridge is formed by MES; (3) a regret bound for a variant of MES, which, to our knowledge, is the first regret bound established for any variant of the entropy search methods; (4) an extension of MES to the high dimensional settings via additive Gaussian processes; and (5) empirical evaluations which demon-
strate that MES identifies good points as quickly or better than ES/PES, but is much more efficient and robust in estimating the mutual information, and therefore much faster than its input-space counterparts.
",1. Introduction,[0],[0]
"After acceptance of this work, we learned that Hoffman & Ghahramani (2015) independently arrived at the acquisition function in Eq.",1. Introduction,[0],[0]
(5).,1. Introduction,[0],[0]
"Yet, our approximation (Eq. (6)) is different, and hence the actual acquisition function we evaluate and analyze is different.",1. Introduction,[0],[0]
Our goal is to maximize a black-box function f : X → R where X ⊂ Rd and X is compact.,2. Background,[0],[0]
"At time step t, we select point xt and observe a possibly noisy function evaluation yt = f(xt) +",2. Background,[0],[0]
"t, where t ∼ N (0, σ2) are i.i.d.",2. Background,[0],[0]
Gaussian variables.,2. Background,[0],[0]
"We use Gaussian processes (Rasmussen & Williams, 2006) to build a probabilistic model of the blackbox function to be optimized.",2. Background,[0],[0]
"For high dimensional cases, we use a variant of the additive Gaussian process (Duvenaud et al., 2011; Kandasamy et al., 2015).",2. Background,[0],[0]
"For completeness, we here introduce some basics of GP and add-GP.",2. Background,[0],[0]
"Gaussian processes (GPs) are distributions over functions, and popular priors for Bayesian nonparametric regression.",2.1. Gaussian Processes,[0],[0]
"In a GP, any finite set of function values has a multivariate Gaussian distribution.",2.1. Gaussian Processes,[0],[0]
"A Gaussian process GP (µ, k) is fully specified by a mean function µ(x) and covariance (kernel) function k(x,x′).",2.1. Gaussian Processes,[0],[0]
"Let f be a function sampled from GP (µ, k).",2.1. Gaussian Processes,[0],[0]
"Given the observations Dt = {(xτ , yτ )}tτ=1, we obtain the posterior mean µt(x) = kt(x) T(Kt+σ",2.1. Gaussian Processes,[0],[0]
"2I)−1yt and posterior covariance kt(x,x ′) =",2.1. Gaussian Processes,[0],[0]
"k(x,x′)−kt(x)T(Kt+σ2I)−1kt(x′) of the function via the kernel matrix Kt =",2.1. Gaussian Processes,[0],[0]
"[k(xi,xj)]xi,xj∈Dt and kt(x) =",2.1. Gaussian Processes,[0],[0]
"[k(xi,x)]xi∈Dt (Rasmussen & Williams, 2006).",2.1. Gaussian Processes,[0],[0]
"The posterior variance is σ2t (x) = kt(x,x).",2.1. Gaussian Processes,[0],[0]
"Additive Gaussian processes (add-GP) were proposed in (Duvenaud et al., 2011), and analyzed in the BO setting in (Kandasamy et al., 2015).",2.2. Additive Gaussian Processes,[0],[0]
"Following the latter, we assume that the function f is a sum of independent functions sampled from Gaussian processes that are active on disjoint sets Am of input dimensions.",2.2. Additive Gaussian Processes,[0],[0]
"Precisely, f(x) = ∑M m=1 f
(m)(xAm), with Ai ∩ Aj = ∅ for all i 6= j, | ∪Mi=1 Ai| = d, and f (m) ∼ GP (µ(m), k(m)), for all m ≤ M (M ≤ d < ∞).",2.2. Additive Gaussian Processes,[0],[0]
"As a result of this decomposition, the function f is distributed according to GP ( ∑M m=1 µ (m), ∑M m=1 k
(m)).",2.2. Additive Gaussian Processes,[0],[0]
"Given a set of noisy observations Dt = {(xτ , yτ )}tτ=1 where yτ ∼ N (f(xτ ), σ2), the posterior mean and
covariance of the function component f (m) can be inferred as µ(m)t (x) = k",2.2. Additive Gaussian Processes,[0],[0]
(m) t (x) T(Kt + σ,2.2. Additive Gaussian Processes,[0],[0]
"2I)−1yt and k(m)t (x,x ′)",2.2. Additive Gaussian Processes,[0],[0]
"= k(m)(x,x′)",2.2. Additive Gaussian Processes,[0],[0]
"− k(m)t (x)T(Kt + σ2I)−1k (m) t (x ′), where k(m)t (x) =",2.2. Additive Gaussian Processes,[0],[0]
"[k (m)(xi,x)]xi∈Dt
andKt =",2.2. Additive Gaussian Processes,[0],[0]
"[∑M m=1 k (m)(xi,xj) ]",2.2. Additive Gaussian Processes,[0],[0]
"xi,xj∈Dt .",2.2. Additive Gaussian Processes,[0],[0]
"For simplicity, we use the shorthand k(m)(x,x′) = k(m)(xAm ,x′Am).",2.2. Additive Gaussian Processes,[0],[0]
"We use two types of evaluation criteria for BO, simple regret and inference regret.",2.3. Evaluation Criteria,[0],[0]
"In each iteration, we choose to evaluate one input xt to “learn” where the arg max of the function is.",2.3. Evaluation Criteria,[0],[0]
"The simple regret rT = maxx∈X f(x) − maxt∈[1,T ] f(xt) measures the value of the best queried point so far.",2.3. Evaluation Criteria,[0],[0]
"After all queries, we may infer an arg max of the function, which is usually chosen as x̃T = arg maxx∈X µT (x) (Hennig & Schuler, 2012; Hernández-Lobato et al., 2014).",2.3. Evaluation Criteria,[0],[0]
We denote the inference regret as RT = maxx∈X f(x) − f(x̃T ) which characterizes how satisfying our inference of the arg max is.,2.3. Evaluation Criteria,[0],[0]
Entropy search methods use an information-theoretic perspective to select where to evaluate.,3. Max-value Entropy Search,[0],[0]
They find a query point that maximizes the information about the location x∗ = arg maxx∈X f(x) whose value y∗ = f(x∗) achieves the global maximum of the function f .,3. Max-value Entropy Search,[0],[0]
"Using the negative differential entropy of p(x∗|Dt) to characterize the uncertainty about x∗, ES and PES use the acquisition functions
αt(x) = I({x, y};x∗ | Dt) (1) = H (p(x∗ | Dt))− E",3. Max-value Entropy Search,[0],[0]
"[H(p(x∗ | Dt ∪ {x, y}))]",3. Max-value Entropy Search,[0],[0]
(2) = H(p(y,3. Max-value Entropy Search,[0],[0]
"| Dt,x))− E",3. Max-value Entropy Search,[0],[0]
"[H(p(y | Dt,x,x∗))] .",3. Max-value Entropy Search,[0],[0]
"(3)
ES uses formulation (2), in which the expectation is over p(y|Dt,x), while PES uses the equivalent, symmetric formulation (3), where the expectation is over p(x∗|Dt).",3. Max-value Entropy Search,[0],[0]
"Unfortunately, both p(x∗|Dt) and its entropy is analytically intractable and have to be approximated via expensive computations.",3. Max-value Entropy Search,[0],[0]
"Moreover, the optimum may not be unique, adding further complexity to this distribution.
",3. Max-value Entropy Search,[0],[0]
We follow the same information-theoretic idea but propose a much cheaper and more robust objective to compute.,3. Max-value Entropy Search,[0],[0]
"Instead of measuring the information about the argmax x∗, we use the information about the maximum value y∗ = f(x∗).",3. Max-value Entropy Search,[0],[0]
"Our acquisition function is the gain in mutual information between the maximum y∗ and the next point we query, which can be approximated analytically by evaluating the entropy of the predictive distribution:
αt(x) = I({x, y}; y∗",3. Max-value Entropy Search,[0],[0]
"| Dt) (4) = H(p(y | Dt,x))− E[H(p(y",3. Max-value Entropy Search,[0],[0]
"| Dt,x, y∗))] (5)
",3. Max-value Entropy Search,[0],[0]
≈ 1 K ∑ y∗∈Y∗ [ γy∗(x)ψ(γy∗(x)),3. Max-value Entropy Search,[0],[0]
2Ψ(γy∗(x)),3. Max-value Entropy Search,[0],[0]
− log(Ψ(γy∗(x))),3. Max-value Entropy Search,[0],[0]
"] (6)
where ψ is the probability density function and Ψ the cumulative density function of a normal distribution, and γy∗(x) = y∗−µt(x) σt(x)
.",3. Max-value Entropy Search,[0],[0]
The expectation in Eq.,3. Max-value Entropy Search,[0],[0]
"(5) is over p(y∗|Dn), which is approximated using Monte Carlo estimation by sampling a set of K function maxima.",3. Max-value Entropy Search,[0],[0]
"Notice that the probability in the first term p(y|Dt,x) is a Gaussian distribution with mean µt(x) and variance kt(x,x).",3. Max-value Entropy Search,[0],[0]
"The probability in the second term p(y|Dn,x, y∗) is a truncated Gaussian distribution: given y∗, the distribution of y needs to satisfy",3. Max-value Entropy Search,[0],[0]
y < y∗.,3. Max-value Entropy Search,[0],[0]
"Importantly, while ES and PES rely on the expensive, d-dimensional distribution p(x∗|Dt), here, we use the one-dimensional p(y∗|Dn), which is computationally much easier.
",3. Max-value Entropy Search,[0],[0]
It may not be immediately intuitive that the value should bear sufficient information for a good search strategy.,3. Max-value Entropy Search,[0],[0]
"Yet, the empirical results in Section 5 will demonstrate that this strategy is typically at least as good as ES/PES.",3. Max-value Entropy Search,[0],[0]
"From a formal perspective, Wang et al. (2016) showed how an estimate of the maximum value implies a good search strategy (EST).",3. Max-value Entropy Search,[0],[0]
"Indeed, Lemma 3.1 will make the relation between EST and a simpler, degenerate version of MES explicit.
",3. Max-value Entropy Search,[0],[0]
"Hence, it remains to determine how to sample y∗.",3. Max-value Entropy Search,[0],[0]
We propose two strategies: (1) sampling from an approximation via a Gumbel distribution; and (2) sampling functions from the posterior Gaussian distribution and maximizing the functions to obtain samples of y∗.,3. Max-value Entropy Search,[0],[0]
We present the MES algorithm in Alg. 1.,3. Max-value Entropy Search,[0],[0]
"The marginal distribution of f(x) for any x is a onedimensional Gaussian, and hence the distribution of y∗ may be viewed as the maximum of an infinite collection of dependent Gaussian random variables.",3.1. Gumbel Sampling,[0],[0]
"Since this distribution is difficult to compute, we make two simplifications.",3.1. Gumbel Sampling,[0],[0]
"First, we replace the continuous set X by a discrete (finite), dense subset X̂ of representative points.",3.1. Gumbel Sampling,[0],[0]
"If we select X̂ to be an - cover of X and the function f is Lipschitz continuous with constant L, then we obtain a valid upper bound on f(X) by adding L to any upper bound on f(X̂).
",3.1. Gumbel Sampling,[0],[0]
"Second, we use a “mean field” approximation and treat the function values at the points in X̂ as independent.",3.1. Gumbel Sampling,[0],[0]
"This approximation tends to over-estimate the maximum; this follows from Slepian’s lemma if k(x, x′) ≥ 0.",3.1. Gumbel Sampling,[0],[0]
"Such upper bounds still lead to optimization strategies with vanishing regret, whereas lower bounds may not (Wang et al., 2016).
",3.1. Gumbel Sampling,[0],[0]
We sample from the approximation p̂(y∗|Dn) via its cumulative distribution function (CDF) P̂r[y∗,3.1. Gumbel Sampling,[0],[0]
"< z] =∏
x∈X̂ Ψ(γz(x)).",3.1. Gumbel Sampling,[0],[0]
"That means we sample r uniformly from
Algorithm 1 Max-value Entropy Search (MES) 1: function MES (f,D0) 2: for t = 1, · · · , T do 3: αt−1(·)←APPROX-MI (Dt−1) 4: xt ← arg maxx∈X αt−1(x) 5: yt ← f(xt) + t, t ∼ N (0, σ2) 6:",3.1. Gumbel Sampling,[0],[0]
"Dt ← Dt−1 ∪ {xt, yt} 7: end for 8: end function
9: function Approx-MI (Dt) 10: if Sample with Gumbel then 11: approximate Pr[ŷ∗ < y] with G(a, b) 12: sample a K-length vector r ∼ Unif([0, 1]) 13: y∗ ← a− b log(− log r) 14: else 15: for i = 1, · · · ,K do 16: sample f̃ ∼ GP (µt, kt | Dt) 17: y∗(i) ← maxx∈X f̃(x) 18: end for 19: y∗",3.1. Gumbel Sampling,[0],[0]
← [y∗(i)]Ki=1 20: end if 21: return αt(·) in Eq.,3.1. Gumbel Sampling,[0],[0]
"(6) 22: end function
[0, 1] and find z such that Pr[y∗ < z",3.1. Gumbel Sampling,[0],[0]
] = r.,3.1. Gumbel Sampling,[0],[0]
"A binary search for z to accuracy δ requires O(log 1δ ) queries to the CDF, and each query takes O(|X̂|)",3.1. Gumbel Sampling,[0],[0]
"≈ O(nd) time, so we obtain an overall time of O(M |X̂| log 1δ ) for drawing M samples.
",3.1. Gumbel Sampling,[0],[0]
"To sample more efficiently, we propose a O(M + |X̂| log 1δ )-time strategy, by approximating the CDF by a Gumbel distribution: P̂r[y∗",3.1. Gumbel Sampling,[0],[0]
"< z] ≈ G(a, b) = e−e − z−a
b .",3.1. Gumbel Sampling,[0],[0]
"This choice is motivated by the Fisher-Tippett-Gnedenko theorem (Fisher, 1930), which states that the maximum of a set of i.i.d.",3.1. Gumbel Sampling,[0],[0]
Gaussian variables is asymptotically described by a Gumbel distribution (see the appendix for further details).,3.1. Gumbel Sampling,[0],[0]
This does not in general extend to non-i.i.d.,3.1. Gumbel Sampling,[0],[0]
"Gaussian variables, but we nevertheless observe that in practice, this approach yields a good and fast approximation.
",3.1. Gumbel Sampling,[0],[0]
"We sample from the Gumbel distribution via the Gumbel quantile function: we sample r uniformly from [0, 1], and let the sample be y = G−1(a, b) = a − b log(− log r).",3.1. Gumbel Sampling,[0],[0]
We set the appropriate Gumbel distribution parameters a and b by percentile matching and solve the two-variable linear equations a − b log(− log r1) = y1 and a − b log(− log r2),3.1. Gumbel Sampling,[0],[0]
"= y2, where Pr[y∗ < y1]",3.1. Gumbel Sampling,[0],[0]
= r1 and Pr[y∗ < y2] = r2.,3.1. Gumbel Sampling,[0],[0]
"In practice, we use r1 = 0.25 and r2 = 0.75 so that the scale of the approximated Gumbel distribution is proportional to the interquartile range of the CDF P̂r[y∗ < z].",3.1. Gumbel Sampling,[0],[0]
"For an alternative sampling strategy we follow (HernándezLobato et al., 2014): we draw functions from the posterior GP and then maximize each of the sampled functions.",3.2. Sampling y∗ via Posterior Functions,[0],[0]
"Given the observations Dt = {(xτ , yτ )tτ=1}, we can approximate the posterior Gaussian process using a 1-hiddenlayer neural network f̃(x) = aTtφ(x) where φ(x) ∈ RD is a vector of feature functions (Neal, 1996; Rahimi et al., 2007) and the Gaussian weight at ∈ RD is distributed according to a multivariate Gaussian N (νt,Σt).
",3.2. Sampling y∗ via Posterior Functions,[0],[0]
Computing φ(x).,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"By Bochner’s theorem (Rudin, 2011), the Fourier transform k̂ of a continuous and translation-invariant kernel k is guaranteed to be a probability distribution.",3.2. Sampling y∗ via Posterior Functions,[0],[0]
"Hence we can write the kernel of the GP to be k(x,x′) =",3.2. Sampling y∗ via Posterior Functions,[0],[0]
Eω∼k̂(ω)[e iωT(x−x′)],3.2. Sampling y∗ via Posterior Functions,[0],[0]
"= Ec∼U [0,2π]Ek̂[2 cos(ω",3.2. Sampling y∗ via Posterior Functions,[0],[0]
"Tx + c) cos(ωTx′ + c)] and approximate the expectation by k(x,x′)",3.2. Sampling y∗ via Posterior Functions,[0],[0]
≈ φT(x)φ(x′) where φi(x) = √ 2 D cos(ω T,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"i x + ci), ωi ∼ κ̂(ω), and ci ∼ U [0, 2π] for i = 1, . . .",3.2. Sampling y∗ via Posterior Functions,[0],[0]
", D.
Computing νt,Σt.",3.2. Sampling y∗ via Posterior Functions,[0],[0]
"By writing the GP as a random linear combination of feature functions aTt φ(x), we are defining the mean and covariance of the GP to be µt(x) = νTφ(x) and k(x,x′) = φ(x)TΣtφ(x′).",3.2. Sampling y∗ via Posterior Functions,[0],[0]
Let Z =,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"[z1, · · · , zt] ∈ RD×t, where",3.2. Sampling y∗ via Posterior Functions,[0],[0]
zτ := φ(xτ ) ∈ RD.,3.2. Sampling y∗ via Posterior Functions,[0],[0]
The GP posterior mean and covariance in Section 2.1 become µt(x) =,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"z TZ(ZTZ + σ2I)−1yt and kt(x,x ′) =",3.2. Sampling y∗ via Posterior Functions,[0],[0]
zTz′ − zTZ(ZTZ + σ2I)−1ZTz′.,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"Because Z(ZTZ + σ2I)−1 = (ZZT+σ2I)−1Z, we can simplify the above equations and obtain νt = σ−2ΣtZtyt and Σt = (ZZ Tσ−2 + I)−1.
",3.2. Sampling y∗ via Posterior Functions,[0],[0]
"To sample a function from this random 1-hidden-layer neural network, we sample ã from N (νt,Σt) and construct the sampled function f̃ = ãTφ(x).",3.2. Sampling y∗ via Posterior Functions,[0],[0]
Then we optimize f̃ with respect to its input to get a sample of the maximum of the function maxx∈X f̃(x).,3.2. Sampling y∗ via Posterior Functions,[0],[0]
"As a side effect, our new acquisition function draws connections between ES/PES and other popular BO methods.",3.3. Relation to Other BO Methods,[0],[0]
The connection between MES and ES/PES follows from the information-theoretic viewpoint; the following lemma makes the connections to other methods explicit.,3.3. Relation to Other BO Methods,[0],[0]
Lemma 3.1.,3.3. Relation to Other BO Methods,[0],[0]
"The following methods are equivalent:
1.",3.3. Relation to Other BO Methods,[0],[0]
"MES, where we only use a single sample y∗ for αt(x); 2. EST with m = y∗; 3.",3.3. Relation to Other BO Methods,[0],[0]
"GP-UCB with β 1 2 = minx∈X
y∗−µt(x) σt(x) ;
4. PI with θ = y∗.
This equivalence no longer holds if we useM > 1 samples of y∗ in MES.
",3.3. Relation to Other BO Methods,[0],[0]
Proof.,3.3. Relation to Other BO Methods,[0],[0]
"The equivalence among 2,3,4 is stated in Lemma 2.1 in (Wang et al., 2016).",3.3. Relation to Other BO Methods,[0],[0]
What remains to be shown is the equivalence between 1 and 2.,3.3. Relation to Other BO Methods,[0],[0]
"When using a single y∗ in MES, the next point to evaluate is chosen by maximizing αt(x) = γy∗(x)
",3.3. Relation to Other BO Methods,[0],[0]
ψ(γy∗ (x)),3.3. Relation to Other BO Methods,[0],[0]
2Ψ(γy∗ (x)),3.3. Relation to Other BO Methods,[0],[0]
"− log(Ψ(γy∗(x))) and γy∗ = y∗−µt(x) σt(x)
.",3.3. Relation to Other BO Methods,[0],[0]
"For EST with m = y∗, the next point to evaluate is chosen by minimizing γy∗(x).",3.3. Relation to Other BO Methods,[0],[0]
Let us define a function g(u) = u ψ(u)2Ψ(u),3.3. Relation to Other BO Methods,[0],[0]
− log(Ψ(u)).,3.3. Relation to Other BO Methods,[0],[0]
"Clearly, αt(x) = g(γy∗(x)).",3.3. Relation to Other BO Methods,[0],[0]
"Because g(u) is a monotonically decreasing function, maximizing g(γy∗(x)) is equivalent to minimizing γy∗(x).",3.3. Relation to Other BO Methods,[0],[0]
Hence 1 and 2 are equivalent.,3.3. Relation to Other BO Methods,[0],[0]
"The connection with EST directly leads to a bound on the simple regret of MES, when using only one sample of y∗.",3.4. Regret Bound,[0],[0]
"We prove Theorem 3.2 in the appendix.
Theorem 3.2.",3.4. Regret Bound,[0],[0]
"Let F be the cumulative probability distribution for the maximum of any function f sampled from GP (µ, k) over the compact search space X ⊂ Rd, where k(x,x′) ≤ 1,∀x,x′ ∈ X. Let f∗ = maxx∈X f(x) and w = F (f∗) ∈ (0, 1), and assume the observation noise is iidN (0, σ).",3.4. Regret Bound,[0],[0]
"If in each iteration t, the query point is chosen as xt = arg maxx∈X γyt∗(x) ψ(γyt∗ (x))
",3.4. Regret Bound,[0],[0]
"2Ψ(γyt∗ (x))− log(Ψ(γyt∗(x))),
where γyt∗(x) = yt∗−µt(x) σt(x) and yt∗ is drawn from F , then with probability at least 1 − δ, in T ′",3.4. Regret Bound,[0],[0]
= ∑T i=1,3.4. Regret Bound,[0],[0]
"logw δ 2πi number of iterations, the simple regret satisfies
rT ′",3.4. Regret Bound,[0],[0]
"≤ √ CρT T (νt∗ + ζT ) (7)
where C = 2/ log(1 + σ−2) and ζT = (2 log(πTδ ))",3.4. Regret Bound,[0],[0]
1 2 ; π satisfies ∑T i=1,3.4. Regret Bound,[0],[0]
π −1,3.4. Regret Bound,[0],[0]
"i ≤ 1 and πt > 0, and t∗ = arg maxt νt with νt , minx∈X,yt∗>f∗ γyt∗(x), and ρT is the maximum information gain of at most T selected points.",3.4. Regret Bound,[0],[0]
"In practice we do not know the hyper-parameters of the GP, so we must adapt our GP model as we observe more data.",3.5. Model Adaptation,[0],[0]
A standard way to learn the GP hyper-parameters is to optimize the marginal data likelihood with respect to the hyperparameters.,3.5. Model Adaptation,[0],[0]
"As a full Bayesian treatment, we can also draw samples of the hyper-parameters using slice sampling (Vanhatalo et al., 2013), and then marginalize out the hyperparameters in our acquisition function in Eq.",3.5. Model Adaptation,[0],[0]
(6).,3.5. Model Adaptation,[0],[0]
"Namely, if we use E to denote the set of sampled settings for the GP hyper-parameters, our acquisition function becomes αt(x) = ∑ η∈E ∑ y∗∈Y∗ [ γηy∗(x)ψ(γ η y∗(x)) 2Ψ(γηy∗(x))",3.5. Model Adaptation,[0],[0]
"− log(Ψ(γηy∗(x))) ] ,
where γηy∗(x) = y∗−µηt (x) σηt (x)
and the posterior inference on the mean function µηt and σ η t depends on the GP hyperparameter setting η.",3.5. Model Adaptation,[0],[0]
"Similar approaches have been used in (Hernández-Lobato et al., 2014; Snoek et al., 2012).",3.5. Model Adaptation,[0],[0]
The high-dimensional input setting has been a challenge for many BO methods.,4. High Dimensional MES with Add-GP,[0],[0]
We extend MES to this setting via additive Gaussian processes (Add-GP).,4. High Dimensional MES with Add-GP,[0],[0]
"In the past, AddGP has been used and analyzed for GP-UCB (Kandasamy et al., 2015), which assumed the high dimensional blackbox function is a summation of several disjoint lower dimensional functions.",4. High Dimensional MES with Add-GP,[0],[0]
"Utilizing this special additive structure, we overcome the statistical problem of having insufficient data to recover a complex function, and the difficulty of optimizing acquisition functions in high dimensions.
",4. High Dimensional MES with Add-GP,[0],[0]
"Since the function components f (m) are independent, we can maximize the mutual information between the input in the active dimensions Am and maximum of f (m) for each component separately.",4. High Dimensional MES with Add-GP,[0],[0]
"Hence, we have a separate acquisition function for each component, where y(m) is the evaluation of f (m):
α (m) t (x) = I({xAm , y(m)}; y (m) ∗ | Dt) (8)
= H(p(y(m)",4. High Dimensional MES with Add-GP,[0],[0]
"| Dt,xAm))
",4. High Dimensional MES with Add-GP,[0],[0]
− E[H(p(y(m),4. High Dimensional MES with Add-GP,[0],[0]
"| Dt,xAm , y(m)∗ ))] (9)
≈ ∑ y (m) ∗ γ(m)y∗ (x) ψ(γ",4. High Dimensional MES with Add-GP,[0],[0]
(m) y∗ (x)),4. High Dimensional MES with Add-GP,[0],[0]
2Ψ(γ (m) y∗ (x)),4. High Dimensional MES with Add-GP,[0],[0]
− log(Ψ(γ(m)y∗ (x))),4. High Dimensional MES with Add-GP,[0],[0]
"(10)
where γ(m)y∗ (x) = y(m)∗ −µ (m) t (x)
σ",4. High Dimensional MES with Add-GP,[0],[0]
"(m) t (x)
.",4. High Dimensional MES with Add-GP,[0],[0]
"Analogously to the non-
additive case, we sample y(m)∗ , separately for each function component.",4. High Dimensional MES with Add-GP,[0],[0]
"We select the final xt by choosing a sub-vector x
(m) t ∈ arg maxx(m)∈Am α (m) t (x (m)) and concatenating the components.
",4. High Dimensional MES with Add-GP,[0],[0]
Sampling y(m)∗ with a Gumbel distribution.,4. High Dimensional MES with Add-GP,[0],[0]
"The Gumbel sampling from Section 3.1 directly extends to sampling y(m)∗ , approximately.",4. High Dimensional MES with Add-GP,[0],[0]
"We simply need to sample from the component-wise CDF P̂r[y(m)∗ < z] =∏
x∈X̂ Ψ(γ (m) y (x))), and use the same Gumbel approxi-
mation.
",4. High Dimensional MES with Add-GP,[0],[0]
Sampling y(m)∗ via posterior functions.,4. High Dimensional MES with Add-GP,[0],[0]
The additive structure removes some connections on the input-to-hidden layer of our 1-hidden-layer neural network approximation f̃(x) = aTtφ(x).,4. High Dimensional MES with Add-GP,[0],[0]
"Namely, for each feature function φ there exists a unique group m such that φ is only active on xAm ,
and φ(x) = √
2 D cos(ω TxAm + c) where R|Am| 3 ω ∼
κ̂(m)(ω) and c ∼ U [0, 2π].",4. High Dimensional MES with Add-GP,[0],[0]
"Similar to the non-additive case, we may draw a posterior sample at ∼ N (νt,Σt) where νt = σ−2ΣtZtyt and Σt = (ZZ
Tσ−2 + I)−1.",4. High Dimensional MES with Add-GP,[0],[0]
Let Bm = {i : φi(x) is active on xAm}.,4. High Dimensional MES with Add-GP,[0],[0]
The posterior sample for the function component f (m) is f̃ (m)(x) =,4. High Dimensional MES with Add-GP,[0],[0]
"(aBmt )
TφBm(xAm).",4. High Dimensional MES with Add-GP,[0],[0]
"Then we can maximize f̃ (m) to obtain a sample for y(m)∗ .
",4. High Dimensional MES with Add-GP,[0],[0]
The algorithm for the additive max-value entropy search method (add-MES) is shown in Algorithm 2.,4. High Dimensional MES with Add-GP,[0],[0]
"The function APPROX-MI does the pre-computation for approximating the mutual information in a similar way as in Algorithm 1, except that it only acts on the active dimensions in them-th group.
",4. High Dimensional MES with Add-GP,[0],[0]
"Algorithm 2 Additive Max-value Entropy Search 1: function Add-MES (f,D0) 2: for t = 1, · · · , T do 3: for m = 1, · · · ,M do 4: α(m)t−1(·)←APPROX-MI (Dt−1) 5:",4. High Dimensional MES with Add-GP,[0],[0]
"xAmt ← arg maxxAm∈XAm α (m) t−1(x)
6: end for 7: yt ← f(xt) + t, t ∼ N (0, σ2) 8:",4. High Dimensional MES with Add-GP,[0],[0]
"Dt ← Dt−1 ∪ {xt, yt} 9: end for
10: end function",4. High Dimensional MES with Add-GP,[0],[0]
"In this section, we probe the empirical performance of MES and add-MES on a variety of tasks.",5. Experiments,[0],[0]
"Here, MES-G denotes MES with y∗ sampled from the approximate Gumbel distribution, and MES-R denotes MES with y∗ computed by maximizing a sampled function represented by random features.",5. Experiments,[0],[0]
"Following (Hennig & Schuler, 2012; HernándezLobato et al., 2014), we adopt the zero mean function and non-isotropic squared exponential kernel as the prior for the GP.",5. Experiments,[0],[0]
"We compare to methods from the entropy search family, i.e., ES and PES, and to other popular Bayesian optimization methods including GP-UCB (denoted by UCB), PI, EI and EST.",5. Experiments,[0],[0]
"The parameter for GP-UCB was set according to Theorem 2 in (Srinivas et al., 2010); the parameter for PI was set to be the observation noise σ.",5. Experiments,[0],[0]
"For the functions with unknown GP hyper-parameters, every 10 iterations, we learn the GP hyper-parameters using the same approach as was used by PES (Hernández-Lobato et al., 2014).",5. Experiments,[0],[0]
"For the high dimensional tasks, we follow (Kandasamy et al., 2015) and sample the additive structure/GP parameters with the highest data likelihood when they are unknown.",5. Experiments,[0],[0]
We evaluate performance according to the simple regret and inference regret as defined in Section 2.3.,5. Experiments,[0],[0]
"We used the open source Matlab implementation of PES, ES and EST (Hennig & Schuler, 2012; Hernández-Lobato
50 100 150 200
et al., 2014; Wang et al., 2016).",5. Experiments,[0],[0]
Our Matlab code and test functions are available at https://github.com/ zi-w/Max-value-Entropy-Search/.,5. Experiments,[0],[0]
"We begin with a comparison on synthetic functions sampled from a 3-dimensional GP, to probe our conjecture that MES is much more robust to the number of y∗ sampled to estimate the acquisition function than PES is to the number of x∗ samples.",5.1. Synthetic Functions,[0],[0]
"For PES, we sample 100 (PES 100), 10 (PES 10) and 1 (PES 1) argmaxes for the acquisition function.",5.1. Synthetic Functions,[0],[0]
"Similarly, we sample 100, 10, 1 y∗ values for MES-R and MES-G. We average the results on 100 functions sampled from the same Gaussian kernel with scale parameter 5.0 and bandwidth parameter 0.0625, and observation noise N (0, 0.012).
",5.1. Synthetic Functions,[0],[0]
Figure 1 shows the simple and inference regrets.,5.1. Synthetic Functions,[0],[0]
"For both regret measures, PES is very sensitive to the the number of x∗ sampled for the acquisition function: 100 samples lead to much better results than 10 or 1.",5.1. Synthetic Functions,[0],[0]
"In contrast, both MES-G and MES-R perform competitively even with 1 or 10 samples.",5.1. Synthetic Functions,[0],[0]
"Overall, MES-G is slightly better than MESR, and both MES methods performed better than other ES methods.",5.1. Synthetic Functions,[0],[0]
MES methods performed better than all other methods with respect to simple regret.,5.1. Synthetic Functions,[0],[0]
"For inference regret, MES methods performed similarly to EST, and much better than all other methods including PES and ES.
",5.1. Synthetic Functions,[0],[0]
"In Table 1, we show the runtime of selecting the next input per iteration1 using GP-UCB, PI, EI, EST, ES, PES, MES-R and MES-G on the synthetic data with fixed GP hyper-parameters.",5.1. Synthetic Functions,[0],[0]
"For PES and MES-R, every x∗ or y∗ requires running an optimization sub-procedure, so their running time grows noticeably with the number of samples.",5.1. Synthetic Functions,[0],[0]
"MES-G avoids this optimization, and competes with the fastest methods EI and UCB.
",5.1. Synthetic Functions,[0],[0]
"In the following experiments, we set the number of x∗ sampled for PES to be 200, and the number of y∗ sampled for MES-R and MES-G to be 100 unless otherwise mentioned.",5.1. Synthetic Functions,[0],[0]
"We test on three challenging optimization test functions: the 2-dimensional eggholder function, the 10-dimensional Shekel function and the 10-dimensional Michalewicz function.",5.2. Optimization Test Functions,[0],[0]
All of these functions have many local optima.,5.2. Optimization Test Functions,[0],[0]
"We randomly sample 1000 points to learn a good GP hyperparameter setting, and then run the BO methods with the same hyper-parameters.",5.2. Optimization Test Functions,[0],[0]
The first observation is the same for all methods.,5.2. Optimization Test Functions,[0],[0]
We repeat the experiments 10 times.,5.2. Optimization Test Functions,[0],[0]
"The averaged simple regret is shown in the appendix, and the inference regret is shown in Table 2.",5.2. Optimization Test Functions,[0],[0]
"On the 2-d eggholder function, PES was able to achieve better function values faster than all other methods, which verified the good performance of PES when sufficiently many x∗ are sampled.",5.2. Optimization Test Functions,[0],[0]
"However, for higher-dimensional test functions, the 10-d Shekel and 10-d Michalewicz function, MES methods performed much better than PES and ES, and MES-G performed better than all other methods.",5.2. Optimization Test Functions,[0],[0]
"Next, we experiment with Levenberg-Marquardt optimization for training a 1-hidden-layer neural network.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"The 4 parameters we tune with BO are the number of neurons, the damping factor µ, the µ-decrease factor, and the µ-increase factor.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"We test regression on the Boston housing dataset
1All the timing experiments were run exclusively on an Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"The function evaluation time is excluded.
and classification on the breast cancer dataset (Bache & Lichman, 2013).",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"The experiments are repeated 20 times, and the neural network’s weight initialization and all other parameters are set to be the same to ensure a fair comparison.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
Both of the datasets were randomly split into train/validation/test sets.,5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
We initialize the observation set to have 10 random function evaluations which were set to be the same across all the methods.,5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"The averaged simple regret for the regression L2-loss on the validation set of the Boston housing dataset is shown in Fig. 2(a), and the classification accuracy on the validation set of the breast cancer dataset is shown in Fig. 2(b).",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"For the classification problem on the breast cancer dataset, MES-G, PES and UCB achieved a similar simple regret.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"On the Boston housing dataset, MES methods achieved a lower simple regret.",5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
We also show the inference regrets for both datasets in Table 3.,5.3. Tuning Hyper-parameters for Neural Networks,[0],[0]
"We use BO to do active learning for the pre-image learning problem for pushing (Kaelbling & Lozano-Pérez, 2017).",5.4. Active Learning for Robot Pushing,[0],[0]
"The function we optimize takes as input the pushing action of the robot, and outputs the distance of the pushed object to the goal location.",5.4. Active Learning for Robot Pushing,[0],[0]
"We use BO to minimize the function in
order to find a good pre-image for pushing the object to the designated goal location.",5.4. Active Learning for Robot Pushing,[0],[0]
"The first function we tested has a 3-dimensional input: robot location (rx, ry) and pushing duration tr.",5.4. Active Learning for Robot Pushing,[0],[0]
"We initialize the observation size to be one, the same across all methods.",5.4. Active Learning for Robot Pushing,[0],[0]
"The second function has a 4-dimensional input: robot location and angle (rx, ry, rθ), and pushing duration tr.",5.4. Active Learning for Robot Pushing,[0],[0]
We initialize the observation to be 50 random points and set them the same for all the methods.,5.4. Active Learning for Robot Pushing,[0],[0]
We select 20 random goal locations for each function to test if BO can learn where to push for these locations.,5.4. Active Learning for Robot Pushing,[0],[0]
We show the simple regret in Fig. 4 and the inference regret in Table 4.,5.4. Active Learning for Robot Pushing,[0],[0]
MES methods performed on a par with or better than their competitors.,5.4. Active Learning for Robot Pushing,[0],[0]
"In this section, we test our add-MES algorithm on high dimensional black-box function optimization problems.",5.5. High Dimensional BO with Add-MES,[0],[0]
"First we compare add-MES and add-GP-UCB (Kandasamy et al., 2015) on a set of synthetic additive functions with known additive structure and GP hyper-parameters.",5.5. High Dimensional BO with Add-MES,[0],[0]
"Each function component of the synthetic additive function is active on at most three input dimensions, and is sampled from a GP with zero mean and Gaussian kernel (bandwidth = 0.1 and scale = 5).",5.5. High Dimensional BO with Add-MES,[0],[0]
"For the parameter of addGP-UCB, we follow (Kandasamy et al., 2015) and set β
(m) t = |Am| log 2t/5.",5.5. High Dimensional BO with Add-MES,[0],[0]
We set the number of y (m) ∗ sampled for each function component in add-MES-R and addMES-G to be 1.,5.5. High Dimensional BO with Add-MES,[0],[0]
"We repeat each experiment for 50 times
for each dimension setting.",5.5. High Dimensional BO with Add-MES,[0],[0]
The results for simple regret are shown in Fig. 3.,5.5. High Dimensional BO with Add-MES,[0],[0]
Add-MES methods perform much better than add-GP-UCB in terms of simple regret.,5.5. High Dimensional BO with Add-MES,[0],[0]
"Interestingly, add-MES-G works better in lower dimensional cases where d = 10, 20, 30, while add-MES-R outperforms both add-MES-G and add-GP-UCB for higher dimensions where d = 50, 100.",5.5. High Dimensional BO with Add-MES,[0],[0]
"In general, MES-G tends to overestimate the maximum of the function because of the independence assumption, and MES-R tends to underestimate the maximum of the function because of the imperfect global optimization of the posterior function samples.",5.5. High Dimensional BO with Add-MES,[0],[0]
"We conjecture that MES-R is better for settings where exploitation is preferred over exploration (e.g., not too many local optima), and MES-G works better if exploration is preferred.
To further verify the performance of add-MES in high dimensional problems, we test on two real-world high dimensional experiments.",5.5. High Dimensional BO with Add-MES,[0],[0]
"One is a function that returns the distance between a goal location and two objects being pushed
by a robot which has 14 parameters2.",5.5. High Dimensional BO with Add-MES,[0],[0]
"The other function returns the walking speed of a planar bipedal robot, with 25 parameters to tune (Westervelt et al., 2007).",5.5. High Dimensional BO with Add-MES,[0],[0]
"In Fig. 5, we show the simple regrets achieved by add-GP-UCB and addMES.",5.5. High Dimensional BO with Add-MES,[0],[0]
Add-MES methods performed competitively compared to add-GP-UCB on both tasks.,5.5. High Dimensional BO with Add-MES,[0],[0]
"We proposed a new information-theoretic approach, maxvalue entropy search (MES), for optimizing expensive black-box functions.",6. Conclusion,[0],[0]
"MES is competitive with or better than previous entropy search methods, but at a much lower computational cost.",6. Conclusion,[0],[0]
"Via additive GPs, MES is adaptable to high-dimensional settings.",6. Conclusion,[0],[0]
"We theoretically connected MES to other popular Bayesian optimization methods including entropy search, GP-UCB, PI, and EST, and showed a bound on the simple regret for a variant of MES.",6. Conclusion,[0],[0]
"Empirically, MES performs well on a variety of tasks.
",6. Conclusion,[0],[0]
"2We implemented the function in (Catto, 2011).",6. Conclusion,[0],[0]
We thank Prof. Leslie Pack Kaelbling and Prof. Tomás Lozano-Pérez for discussions on active learning and Dr. William Huber for his solution to “Extreme Value Theory - Show: Normal to Gumbel” at stats.,Acknowledgements,[0],[0]
"stackexchange.com, which leads to our Gumbel approximation in Section 3.1.",Acknowledgements,[0],[0]
"We gratefully acknowledge support from NSF CAREER award 1553284, NSF grants 1420927 and 1523767, from ONR grant N00014-14-10486, and from ARO grant W911NF1410433.",Acknowledgements,[0],[0]
We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing computational resources.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.",Acknowledgements,[0],[0]
Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques.,abstractText,[0],[0]
"Both rely on a compelling information-theoretic motivation, and maximize the information gained about the arg max of the unknown function; yet, both are plagued by the expensive computation for estimating entropies.",abstractText,[0],[0]
"We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value.",abstractText,[0],[0]
"We show relations of MES to other Bayesian optimization methods, and establish a regret bound.",abstractText,[0],[0]
"We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden.",abstractText,[0],[0]
"In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.",abstractText,[0],[0]
Max-value Entropy Search for Efficient Bayesian Optimization,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368–2378 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP.",1 Introduction,[0],[0]
"Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016).",1 Introduction,[0],[0]
"Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain.",1 Introduction,[0],[0]
"Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios.
",1 Introduction,[0],[0]
"For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs.",1 Introduction,[0],[0]
"When the system generates a candidate semantic parse during training, the quality needs to be indirectly measured by
comparing the derived answers from the knowledge base and the provided labeled answers.
",1 Introduction,[0],[0]
"This setting of implicit supervision increases the difficulty of learning a neural model, not only because the signals are vague and noisy, but also delayed.",1 Introduction,[0],[0]
"For instance, among different semantic parses that result in the same answers, typically only few of them correctly represent the meaning of the question.",1 Introduction,[0],[0]
"Moreover, the correctness of answers corresponding to a parse can only be evaluated through an external oracle (e.g., executing the query on the knowledge base) after the parse is fully constructed.",1 Introduction,[0],[0]
"Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present.
",1 Introduction,[0],[0]
"In this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals.",1 Introduction,[0],[0]
"By casting structured-output learning as a search problem, the key insight in MMRN is the
1Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available.
2368
special mechanism of rewards.",1 Introduction,[0],[0]
Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure.,1 Introduction,[0],[0]
"The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of the current action.",1 Introduction,[0],[0]
"On the other hand, the implicit supervision can be viewed as a source of delayed rewards, where the reward of the actions can only be revealed later.",1 Introduction,[0],[0]
"We unify these two types of reward signals by using a maximum margin update, inspired by structured SVM (Joachims et al., 2009).
",1 Introduction,[0],[0]
"The effectiveness of MMRN is demonstrated on three NLP tasks: named entity recognition, entity linking and semantic parsing.",1 Introduction,[0],[0]
"MMRN outperforms the current best results on CoNLL-2003 named entity recognition dataset (Tjong Kim Sang and De Meulder, 2003), reaching 91.4% F1, in the close setting where no gazetteer is allowed.",1 Introduction,[0],[0]
It also performs comparably to the existing state-of-theart systems on entity linking.,1 Introduction,[0],[0]
Models for these two tasks are trained using explicit supervision.,1 Introduction,[0],[0]
"For semantic parsing, where only implicit supervision signals are provided, MMRN is able to learn from delayed rewards, improving the entity linking component and the overall semantic parsing framework jointly, and outperforms the best published system by 1.4% absolute on the WebQSP dataset (Yih et al., 2016).
",1 Introduction,[0],[0]
"In the rest of the paper, we survey the most related work in Sec. 2 and give an in-depth discussion on comparing MMRN and other learning frameworks in Sec. 7.",1 Introduction,[0],[0]
"We start the description of our method from the search formulation and the state–action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5.",1 Introduction,[0],[0]
Sec. 6 reports the experimental results and Sec. 8 concludes the paper.,1 Introduction,[0],[0]
Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP).,2 Related Work,[0],[0]
Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables.,2 Related Work,[0],[0]
"These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009).",2 Related Work,[0],[0]
"Later, the learning to search framework is pro-
posed (Daumé and Marcu, 2005; Daumé et al., 2009), which casts the structured prediction task as a general search problem.",2 Related Work,[0],[0]
"Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015).
",2 Related Work,[0],[0]
"Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quattoni et al., 2007).",2 Related Work,[0],[0]
"The main difference compared to our framework is the existence of the external environment when learning from implicit signals.
",2 Related Work,[0],[0]
"Upadhyay et al. (2016) first proposed the idea of learning from implicit supervision, and is the most related paper to our work.",2 Related Work,[0],[0]
"Compared to their linear algorithm, our framework is more principled and general as we integrate the concept of margin in our method.",2 Related Work,[0],[0]
"Furthermore, we also extend the framework using neural models.",2 Related Work,[0],[0]
"In our framework, predicting the best structured output, inference, is formulated as a state/action search problem.",3 Search-based Inference,[0],[0]
Our search space can be described as follows.,3 Search-based Inference,[0],[0]
"The initial state, s0, is the starting point of the search process.",3 Search-based Inference,[0],[0]
"We define γ(s) as the set of all feasible actions that can be taken at s, and denote s′ = τ(s, a) as the transition function, where s′ is the new state after taking action a from s. A path h is a sequence of state–action pairs, starting with the initial state: h = {(s0, a0), . . .",3 Search-based Inference,[0],[0]
", (sk, ak)}, where si = τ(si−1, ai−1), ∀i = 1, . . .",3 Search-based Inference,[0],[0]
", k.",3 Search-based Inference,[0],[0]
"We denote h ; ŝ, if ŝ = τ(sk, ak), the final state which the path h leads to.",3 Search-based Inference,[0],[0]
A path essentially is a partial or complete structured prediction.,3 Search-based Inference,[0],[0]
"For each input x, we define H(x) to be the set of all possible paths for the input.",3 Search-based Inference,[0],[0]
"We also define E(x) = {h | h ∈ H(x),h ; ŝ, γ(ŝ) = ∅}, which is all possible paths that lead to terminal states.
",3 Search-based Inference,[0],[0]
"Given a state s and an action a, the scoring function fθ(s, a) measures the quality of an immediate action with respect to the current state, where θ is the model parameters.",3 Search-based Inference,[0],[0]
The score of a path h is defined as the sum of the scores for state-action pairs in h: fθ(h) =,3 Search-based Inference,[0],[0]
"∑k i=0 fθ(si, ai).",3 Search-based Inference,[0],[0]
"During test time, inference is to find the best path in E(x): arg maxh∈E(x) fθ(h;x).",3 Search-based Inference,[0],[0]
"In practice, inference is often approximated by beam search when no efficient algorithm exists.
",3 Search-based Inference,[0],[0]
"In the remaining of this section, we describe the states and actions in the targeted tasks in this work: named entity recognition, entity linking and semantic parsing.",3 Search-based Inference,[0],[0]
The the model and learning algorithm will be discussed in Sec. 4 and Sec. 5.,3 Search-based Inference,[0],[0]
"The task of named entity recognition (NER) is to identify entity mentions in a sentence, as well as to assign their types, such as Person or Location.",3.1 Named entity recognition,[0],[0]
"Following the conventional setting, we treat it as a sequence labeling problem using the standard BIOES encoding.",3.1 Named entity recognition,[0],[0]
"For instance, a “B-LOC” tag on a word means that the word is the beginning of a multi-word location entity.
",3.1 Named entity recognition,[0],[0]
"Given a sentence as input, the states represent the tags assigned to the words.",3.1 Named entity recognition,[0],[0]
"Starting from the initial state, s0, where no tag has been assigned, the search process explores the sequence tagging from the left-to-right order.",3.1 Named entity recognition,[0],[0]
"For each word, the actions are the legitimate tags that can be assigned to it, which depend on previous actions.",3.1 Named entity recognition,[0],[0]
"For example, if the “S-PER” tag (“S” means a single word entity) has been assigned to the previous word, then an action of labeling the current word with either “I-PER” or “E-PER” cannot can be taken.",3.1 Named entity recognition,[0],[0]
The search reaches a terminal state when all words in the sentence have been tagged.,3.1 Named entity recognition,[0],[0]
"The problem of entity linking (EL) is similar to NER, but instead of tagging the mention using a small set of generic entity types, the goal here is to ground the mention to a specific entity, stored in a knowledge base or described by a Wikipedia page.",3.2 Entity linking,[0],[0]
"For example, consider the sentence “nfl news: draft results for giants” and assume that the mention candidates “nfl” and “giants” are given.",3.2 Entity linking,[0],[0]
A state reflects how we have assigned the entity labels to these candidates.,3.2 Entity linking,[0],[0]
"Following the same leftto-right order and starting from the empty assignment s0, the first action to take is to assign the entity label to the first candidate “nfl”.",3.2 Entity linking,[0],[0]
"A legitimate action set can be all the entities that have been associated with this mention in the training set (e.g., “National Football League” or “National Fertilizers Limited”).",3.2 Entity linking,[0],[0]
"Once the action is completed, the transition function will bring the focus to the next mention candidate (i.e., “giants”).",3.2 Entity linking,[0],[0]
The search reaches a terminal state when all the candidate mentions in the sentence have been linked.,3.2 Entity linking,[0],[0]
"Our third targeted task is semantic parsing (SP), which is a task of mapping a text utterance to a formal meaning representation.",3.3 Semantic parsing,[0],[0]
"In this paper, we focus on a specific type of semantic parsing problem that maps a natural language question to a structured query, which is executed on a knowledge base to retrieve the answer to the original question.
",3.3 Semantic parsing,[0],[0]
"Figure 2 shows the semantic parses of an example question “who played meg in season 1 of family guy”, assuming the knowledge base is Freebase (Bollacker et al., 2008).",3.3 Semantic parsing,[0],[0]
An entity linking component plays an important role by mapping “meg” to MegGriffin and “season 1 of family guy” to FamilyGuySeason1.,3.3 Semantic parsing,[0],[0]
"Predicates like cast, actor and character are also from the knowledge base that define the relationships between these entities and the answer.",3.3 Semantic parsing,[0],[0]
Together the semantic parse in λ-calculus is shown in the top of Figure 2.,3.3 Semantic parsing,[0],[0]
"Equivalently, the semantic parse can be represented as a query graph (Figure 2 bottom), which is used in the STAGG system (Yih et al., 2015).",3.3 Semantic parsing,[0],[0]
"The nodes are either grounded entities or variables, where x is the answer entity.",3.3 Semantic parsing,[0],[0]
"The edges denote the relationship between two entities.
",3.3 Semantic parsing,[0],[0]
"Regardless of the choice of the formal language, the process of constructing the semantic parse is typically formulated as a search problem.",3.3 Semantic parsing,[0],[0]
"A state is essentially a partial or complete semantic parse, and an action is to extend the current semantic parse by adding a new relation or constraint.
Different from previous systems which treat entity linking as a static component, our search space consists of the search space of both entity linking and semantic parsing.",3.3 Semantic parsing,[0],[0]
"That is, the search space is the union of the search space of entity linking described in Section 3.2 and the search space of the semantic parses, which we describe below.",3.3 Semantic parsing,[0],[0]
"Integrating search spaces allows the model to use implicit signals to update both the semantic parsing
and the entity linking systems.",3.3 Semantic parsing,[0],[0]
"To the best of our knowledge, this is the first work that jointly learns the entity linking and semantic parsing systems.
",3.3 Semantic parsing,[0],[0]
Our search space is defined as follows.,3.3 Semantic parsing,[0],[0]
"Starting from the initial state s0, the model first explores the entity linking search space.",3.3 Semantic parsing,[0],[0]
Once the entity linking assignment are assigned (e.g. FamilyGuySeason1 in Figure 2.),3.3 Semantic parsing,[0],[0]
"The second phase is then to determine the main relationship between the topic entity and the answer (e.g., the cast-actor chain between FamilyGuySeason1 and x).",3.3 Semantic parsing,[0],[0]
"Constraints (e.g., the character is MegGriffin) that describe the additional properties that the answer needs to have are added last.",3.3 Semantic parsing,[0],[0]
"In this case, any state that is a legitimate semantic parse (consisting of one topic entity and one main relationship, as well as zero or more constraints) can lead to a terminal state.",3.3 Semantic parsing,[0],[0]
"In this section, we introduce the learning framework of MMRN, which includes two main components: reward and max-margin loss.",4 Maximum Margin Reward Networks,[0],[0]
The former is a mechanism for using implicit and explicit supervision signals in a unified way; the latter formally defines the learning objective.,4 Maximum Margin Reward Networks,[0],[0]
The key insight of MMRN is that different types of supervision signals can be represented using the appropriate design of the reward function.,4.1 Reward,[0],[0]
"A reward function is defined over a state–action pair R(s, a), representing the true quality of taking action a in the state s.",4.1 Reward,[0],[0]
"The reward for a path can be formally defined as: R(h) = ∑k i=0R(si, ai).",4.1 Reward,[0],[0]
"Intuitively, when the annotated action sequences (explicit supervision signals) exist, the model only needs to learn to imitate the annotated sequence.",4.1 Reward,[0],[0]
"For instance, when learning NER in the fully supervised setting, the equivalent way of using Hamming distance is to define the reward R(s, a) to be 1 if a matches the annotated sequence at the current state, and 0 otherwise.
",4.1 Reward,[0],[0]
"In the setting where only implicit supervision is available, the reward function can still be designed to capture the signals.",4.1 Reward,[0],[0]
"For instance, when only the question–answer pairs exist for learning the semantic parser, the reward can be defined by comparing the answers derived from a candidate parse and the labeled answers.",4.1 Reward,[0],[0]
"More formally, assume that s = τ(s′, a) is the state after applying
action a to state s′. Let Y (s) be the set of predicted answers generated from state s, and Y (s) = {} when s is not a legitimate semantic parse.",4.1 Reward,[0],[0]
"The reward function R(s′, a) can be defined by comparing Y (s) and the labeled answers, A, to the input question.",4.1 Reward,[0],[0]
"While a set similarity function like the Jaccard coefficient can be used as the reward function, we chose the F1 score in this work as it was used as the evaluation metric in previous work (Berant et al., 2013).",4.1 Reward,[0],[0]
Figure 3 shows an example of this reward function.,4.1 Reward,[0],[0]
"The MMRN learning algorithm can be viewed as an extension of M3N (Taskar et al., 2004) and Structured SVM (Joachims et al., 2009; Yu and Joachims, 2009).",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"The learning algorithm takes three steps, where the first two involve two different search procedures.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"The final step is to update the models with respect to the inference results.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Finding the best path The first search step is to find the best path h∗ by solving the following optimization problem:
h∗ = arg max h∈E(x) R(h; y) + fθ(h).",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"(1)
The first term defines the path that has the highest reward.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Because it is possible that several paths share the same reward, the second term leverages the current model and serves as the tie-breaker, where is a hyper-parameter that is set to a small positive number in our experiments.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"When explicit supervision is available, solving Eq.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
(1) is trivial – the search simply returns the annotated sequence.,4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"In the case of implicit supervision, where true rewards are only revealed for complete action sequences, the search problem becomes difficult as the rewards of early state–action
pairs are zeros.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"In this situation, the search algorithm uses the model score fθ to guide the search.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"One possible design is to use beam search for the optimization problem, where the search procedure follows the current model in the early stage (given thatR(h) = 0).",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"After generating several complete action sequences, the true reward function is then used to find h∗. The tie-breaker also picks the best sequence when there are multiple sequences that lead to the same reward.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Note that h∗ can change between iterations because of the tie-breaker.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Finding the most violated path Once h∗ is found, it is used as our reference path.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"We would like to update the model so that the scoring function fθ will behave similarly to the reward R. More formally, we aim to update the model parameters θ to satisfy the following constraint.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"fθ(h∗)− fθ(h) ≥ R(h∗)−R(h),∀h.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
The constraint implies that the “best” action sequence should rank higher than any other sequence by a margin computed from rewards as R(h∗),4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
− R(h),4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
.,4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"The degree of violation of this constraint, with respect to h, is thus (R(h∗)−R(h))",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
− (fθ(h∗)− fθ(h)),4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
= fθ(h) − R(h)− fθ(h∗) +R(h∗).,4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"The max-margin loss is defined accordingly:
L(h,h∗) = max(fθ(h)−R(h)−fθ(h∗)+R(h∗), 0)
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"L(h,h∗) is our optimization goal, where we want to update the model by fixing the biggest violation.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Note that the associated constraint is only violated when L(h,h∗) is positive.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"To find the path h in this step that maximizes the violation is equivalent to maximizing fθ(h) − R(h), given that the rest of the terms are constant with respect to h.
When there exist only explicit supervision signals, our objective function reduces to the one for optimizing structured SVM without regularization.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"For implicit signals, we find h∗ approximately before we optimize the margin loss.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"In this case, the search is not exact as the reward signals are delayed.",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Nevertheless, we found the margin loss worked well empirically, as it kept decreasing in general until being stable.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
Algorithm 1 summarizes the learning procedure of MMRN.,4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
Search is used in both Line 2 and 3.,4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"In Line 4, the algorithm performs a gradient update to modify all the model parameters.
",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Algorithm 1 Maximum Margin Reward Networks 1: for a random labeled data (x, y) do 2: h∗ ← arg max
h∈E(x) R(h; y) + fθ(h)
3: ĥ← arg max h∈E(x) fθ(h)−R(h; y) 4: update θ by minimizing L(ĥ,h∗) 5: end for",4.2 Max-Margin Loss & Learning Algorithm,[0],[0]
"Although the learning algorithm of MMRN is simple and general, the quality of the learned model is dictated by the effectiveness of the search procedure.",4.3 Practical Considerations,[0],[0]
"Increasing the beam size generally helps improve the model, but also slows down the training, and has a limited effect when dealing with a large search space.",4.3 Practical Considerations,[0],[0]
Domain-specific heuristics for pruning search space should thus be used when available.,4.3 Practical Considerations,[0],[0]
"For instance, in the task of semantic parsing, when the reward of a legitimate semantic parse is 0, it implies that none of the derived answers is included in the labeled set of answers.",4.3 Practical Considerations,[0],[0]
"When all the possible follow-up actions can only make the semantic parse stricter (e.g., adding constraints), and result in a subset of the current derived answers, it is clear that the rewards of all these new states are 0 as well.",4.3 Practical Considerations,[0],[0]
"Paths from this state can thus be pruned.
",4.3 Practical Considerations,[0],[0]
Another strategy for improving search quality is to use approximated reward in the early stage of search.,4.3 Practical Considerations,[0],[0]
"Very often the true rewards at this stage are 0, and are not useful to guide the search to find the best path.",4.3 Practical Considerations,[0],[0]
The approximated reward function can be thought of as estimating whether there exists a high-reward state that is reachable from the current state.,4.3 Practical Considerations,[0],[0]
"The effectiveness of this strategy has been demonstrated successfully by several recent efforts (Mnih et al., 2013; Krishnamurthy et al., 2015; Silver et al., 2016; Narasimhan et al., 2016).",4.3 Practical Considerations,[0],[0]
"While the learning algorithm of MMRN described in Sec. 4 is general, the exact model design is taskdependent.",5 Neural Architectures,[0],[0]
"In this section, we describe in detail the neural network architectures of the three targeted tasks, named entity recognition, entity linking and semantic parsing.",5 Neural Architectures,[0],[0]
"Recall that NER is formulated as a sequence labeling problem, and each action is to label a word with a tag using the BIOES encoding (cf. Sec. 3.1).
",5.1 Named Entity Recognition,[0],[0]
"The model of the action scoring function fθ(s, a) is depicted in Figure 4, which is basically the dot product of the action embedding and state embedding.",5.1 Named Entity Recognition,[0],[0]
"The action embedding is initialized randomly for each action, but can be fine-tuned during training (i.e. back-propagate the error through the network and update the word/entity type embeddings).",5.1 Named Entity Recognition,[0],[0]
"The state embedding is the concatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action.",5.1 Named Entity Recognition,[0],[0]
We also include the orthographic embeddings proposed by Limsopatham and Collier (2016).,5.1 Named Entity Recognition,[0],[0]
An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2).,5.2 Entity Linking,[0],[0]
"As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings.",5.2 Entity Linking,[0],[0]
"All these embeddings, except the feature vectors, are fine-tuned during training.
",5.2 Entity Linking,[0],[0]
Some unique properties of our entity linking model are worth noticing.,5.2 Entity Linking,[0],[0]
"First, we add mention context embeddings from a bidirectional LSTM module as additional input.",5.2 Entity Linking,[0],[0]
"While using LSTMs is a common practice for sequence labeling, it is not usually used for short-text entity linking.",5.2 Entity Linking,[0],[0]
"For each mention, we only extract the output from the bi-LSTM module at the start and end tokens of the mention, and concatenate them as the mention context embeddings.",5.2 Entity Linking,[0],[0]
"Second, we construct entity embeddings using the average of its Freebase (Bollacker et al., 2008) type embeddings2,
2We use only the 358 most frequent Freebase entity types.
",5.2 Entity Linking,[0],[0]
"Avg.{… Statistic features
Input 𝑥
Two hidden layers
=
Average of entity type embeddings
f𝜃(𝑠, 𝑎)
State 𝑠 determines the mention index 𝑚 Action 𝑎 determines the entity index
Mention 𝑚
Figure 5:",5.2 Entity Linking,[0],[0]
"The action scoring model for EL.
initialized using pre-trained embeddings.",5.2 Entity Linking,[0],[0]
Adding these two types of embeddings has shown to improve the performance in our experiments.,5.2 Entity Linking,[0],[0]
"Our semantic parsing model follows the STAGG system (Yih et al., 2015), which uses a stagewise search procedure to expand the candidate semantic parses gradually (cf. Sec. 3.3).",5.3 Semantic Parsing,[0],[0]
"Compared to the original system, we make two notable changes.",5.3 Semantic Parsing,[0],[0]
"First, we use a two-layer feed-forward neural network to replace the original linear ranker that scores the candidate semantic parses.",5.3 Semantic Parsing,[0],[0]
"Second, instead of using a separately trained entity linking system, we incorporate our entity linking networks described in Sec.",5.3 Semantic Parsing,[0],[0]
5.2 as part of the semantic parsing model.,5.3 Semantic Parsing,[0],[0]
The training process will thus fine tune the entity linking component to improve the semantic parsing system.,5.3 Semantic Parsing,[0],[0]
It is important to have a general machine learning model working for both implicit and explicit supervision signals.,6 Experiments,[0],[0]
"We valid our learning framework when the explicit supervision signals are presented, as well as demonstrate the support of the scenario where supervision signals are mixed.
",6 Experiments,[0],[0]
"Specifically, in this section, we report the experimental results of MMRN on named entity recognition and entity linking, both using explicit supervision, and on semantic parsing, using implicit supervision.",6 Experiments,[0],[0]
"In all our experiments, we tuned hyperparameters on the development set (each task respectively), and then re-trained the models on the combination of the training and development set.",6 Experiments,[0],[0]
"We use the CoNLL-2003 shared task data for the NER experiments, where the standard evaluation
metric is the F1 score.",6.1 Named entity recognition,[0],[0]
"The pre-trained word embeddings are 100-dimension GloVe vectors trained on 6 billion tokens (Pennington et al., 2014)3.",6.1 Named entity recognition,[0],[0]
"The search procedure is conducted using beam search, and the reward function is simply the number of correct tag assignments to the words.
",6.1 Named entity recognition,[0],[0]
"The results are shown in Table 1, compared with recently proposed systems based on neural models.",6.1 Named entity recognition,[0],[0]
"When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers).",6.1 Named entity recognition,[0],[0]
"Notice that when beam size is 5, the performance drops to 90.03.",6.1 Named entity recognition,[0],[0]
This demonstrates the importance of search quality when applying MMRN.,6.1 Named entity recognition,[0],[0]
"For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang
3Available at http://nlp.stanford.edu/projects/glove/ 4NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas.",6.2 Entity linking,[0],[0]
"upenn.edu/challenge.html
and Chang, 2014; Yang and Chang, 2015; Yang et al., 2016)",6.2 Entity linking,[0],[0]
.,6.2 Entity linking,[0],[0]
"We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output.",6.2 Entity linking,[0],[0]
An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with the gold mention boundary.,6.2 Entity linking,[0],[0]
"Interested readers can refer to (Carmel et al., 2014) for more detail.
",6.2 Entity linking,[0],[0]
"We initialize the word embeddings from pretrained GloVe vectors trained on the twitter corpus, and type embeddings from the pre-trained skip-gram model (Mikolov et al., 2013)5.",6.2 Entity linking,[0],[0]
Sizes of both word embeddings are set to 200.,6.2 Entity linking,[0],[0]
"Inference is done using a dynamic programming algorithm.
",6.2 Entity linking,[0],[0]
"Results of entity linking experiments are presented in Table 2, which are compared with those of S-MART (Yang and Chang, 2015)6 and NTEL (Yang et al., 2016)7, two state-of-the-art entity linking systems for short texts.",6.2 Entity linking,[0],[0]
Our MMRN-EL is comparable to the best system.,6.2 Entity linking,[0],[0]
"We also conducted two ablation studies by removing the entity type vectors (MMRN-EL - Entity), and by removing the LSTM vectors (MMRN-EL - LSTM).",6.2 Entity linking,[0],[0]
"Both show significant performance drops, which validates the importance of these two additional input vectors.",6.2 Entity linking,[0],[0]
"For semantic parsing, we use the dataset WebQSP8",6.3 Semantic parsing,[0],[0]
"(Yih et al., 2016) in our experiments.",6.3 Semantic parsing,[0],[0]
"This dataset is a clean and enhanced version of the widely used WebQuestions dataset (Berant et al., 2013), which consists of pairs of questions and answers found in Freebase.",6.3 Semantic parsing,[0],[0]
"Compared to WebQuestions, WebQSP excludes questions with ambiguous intent, and provides verified answers and full semantic parses to the remaining 4,737 questions.
",6.3 Semantic parsing,[0],[0]
"We follow the implicit supervision setting in (Yih et al., 2016), using 3, 098 question–answer pairs for training, and 1, 639 for testing.",6.3 Semantic parsing,[0],[0]
A subset of 620 pairs from the training set is used for hyperparameter tuning.,6.3 Semantic parsing,[0],[0]
"Because there can be multiple answers to a question, the quality of a semantic parser is measured using the averaged F1 score of the predicted answers.
",6.3 Semantic parsing,[0],[0]
5Available at https://code.google.com/archive/p/word2vec/ 6The winning system of the NEEL challenge.,6.3 Semantic parsing,[0],[0]
"7To have a fair comparison, we compare to the results of
NTEL which do not use pretrained user embedding.",6.3 Semantic parsing,[0],[0]
"8Available at http://aka.ms/WebQSP
We experiment with two configurations of incorporating the entity linking component.",6.3 Semantic parsing,[0],[0]
MMRNPIPELINE trains an MMRN-EL model using the entity linking labels in WebQSP separately.,6.3 Semantic parsing,[0],[0]
"Given a question, the entities in it are first predicted, and used as input to the semantic parsing system.",6.3 Semantic parsing,[0],[0]
"In contrast, MMRN-JOINT incorporates the MMRN-EL model in the whole framework.",6.3 Semantic parsing,[0],[0]
"During this joint training process, 15 entity link results are sampled according to the current MMRN-EL model, and passed to the downstream networks.",6.3 Semantic parsing,[0],[0]
"In both cases, we use the previous entity linking model trained on the NEEL dataset to initialize the parameters.",6.3 Semantic parsing,[0],[0]
"As discussed in Sec. 4.1, in this implicit supervision setting, we directly set the (delayed) reward function to be the F1 score, which can be obtained by comparing the annotated answers with predicted answers.
",6.3 Semantic parsing,[0],[0]
Table 3 summarizes the results of the MMRNbased semantic parsing systems and other strong baselines.,6.3 Semantic parsing,[0],[0]
The SP column reports the averaged F1 scores.,6.3 Semantic parsing,[0],[0]
"Compared to the pipeline approach (MMRN-PIPELINE), the joint learning framework (MMRN-JOINT) improves significantly, reaching 68.1% F1.",6.3 Semantic parsing,[0],[0]
"To compare different learning methods, we also apply REINFORCE (Williams, 1992), a popular policy gradient algorithm, to train our joint model using the same setting and reward function.9 MMRN-JOINT outperforms REINFORCE and its variant, REINFORECE+, which re-normalizes the probabilities of the sampled candidate sequences.",6.3 Semantic parsing,[0],[0]
Its result is also better than the state-of-the-art STAGG system.,6.3 Semantic parsing,[0],[0]
Note that we use the same architectures and initialization procedures for MMRN-PIPELINE/JOINT and REINFORCE/REINFORCE+.,6.3 Semantic parsing,[0],[0]
"Therefore, the superior performance of MMRN-JOINT shows that the joint learning plays a crucial role in addition to the choices of architecture.",6.3 Semantic parsing,[0],[0]
"Comparing to STAGG, note that Yih et al. (2016) did not jointly train the entity linker and semantic parser together, but they did improve the results by taking the top 10 predictions of their entity linking system for re-ranking parses.",6.3 Semantic parsing,[0],[0]
"Our algorithm further allows to update the entity linker with the labels for semantic parsing and shows superior performance.
",6.3 Semantic parsing,[0],[0]
"Our joint model also improves the entity linking prediction on the questions in WebQSP using the implicit signals (the EL columns in Ta-
9The REINFORCE algorithm uses warm initialization— the entity linking parameters are initialized using the model trained on the NEEL dataset.
",6.3 Semantic parsing,[0],[0]
ble 3).,6.3 Semantic parsing,[0],[0]
The F1 score of MMRN-JOINT on entity linking is 2.4 points higher than the baseline MMRNPIPELINE.,6.3 Semantic parsing,[0],[0]
Note that the entity linking results of MMRN-PIPELINE (line 1) are exactly the results of the entity linking component MMRN-EL.,6.3 Semantic parsing,[0],[0]
"The result is also better than REINFORCE, and comparable to REINFORCE+.
",6.3 Semantic parsing,[0],[0]
"Recently Liang et al. (2016) proposed Neural Symbolic Machine (NSM) and reported the best result of 69.0 F1 score on the WebQSP dataset using the weak supervision settings.10 The NSM architecture for semantic parsing is significantly different from the architecture used in (Yih et al., 2016) and the one used in this paper.",6.3 Semantic parsing,[0],[0]
"In contrast, MMRN is a general learning framework that allows joint training on existing models (i.e. entity linking and semantic parsing modules).",6.3 Semantic parsing,[0],[0]
This allows MMRN to use the labels of semantic parsing task as implicit supervision signals for the entity linking module.,6.3 Semantic parsing,[0],[0]
It would be interesting to apply MMRN on the newly proposed architectures as well.,6.3 Semantic parsing,[0],[0]
"We discuss several issues that are highly related to MMRN in this section.
",7 Discussion,[0],[0]
"Learning to Search There are two main differences between MMRN and search-based algorithms, such as SEARN (Daumé et al., 2009) and DAGGER (Ross et al., 2011).",7 Discussion,[0],[0]
"First, both SEARN and DAGGER focus on imitation learning, assuming explicit supervision signals exist.",7 Discussion,[0],[0]
"They use a two-step model learning approach:
10The paper is published after the submission of this paper.
",7 Discussion,[0],[0]
(1) create cost-sensitive examples by listing state– action pairs and their corresponding (estimated) losses; (2) apply cost-aware training algorithms.,7 Discussion,[0],[0]
"In contrast, MMRN directly updates the parameters using back-propagation based on search results of each example.",7 Discussion,[0],[0]
"Second, SEARN mixes the optimal and current policies during learning, while MMRN performs search twice and simply pushes the current policy towards the optimal one.",7 Discussion,[0],[0]
"Recently, Chang et al. (2015) extend this line of work and discuss different roll-in and roll-out strategies during training for structured contextual bandit settings.",7 Discussion,[0],[0]
"As MMRN uses two search procedures, there is no need to mix different search policies.
",7 Discussion,[0],[0]
Reinforcement Learning,7 Discussion,[0],[0]
"In many reinforcement learning scenarios, the search space is not fully controllable by the agent.",7 Discussion,[0],[0]
"For example, a chess playing agent cannot control the move made by its opponent, and has to commit a single move and wait for the opponent.",7 Discussion,[0],[0]
"Note that the agent can still think ahead and build a search tree, but only one move can be made in the end.",7 Discussion,[0],[0]
"In contrast, in scenarios like semantic parsing, the whole search space is controlled by the agent itself.",7 Discussion,[0],[0]
"Therefore, from the initial state, we can explore several search paths and get their real rewards.",7 Discussion,[0],[0]
"This may explain why MMRN can be more efficient than REINFORCE, as MMRN can use the reward signals of multiple paths more effectively.",7 Discussion,[0],[0]
"In addition, MMRN is not a probabilistic model, so it does not need to handle normalization issues, which often causes large variance in estimating the gradient direction when optimizing the expected reward.
",7 Discussion,[0],[0]
Semantic Parsing MMRN can be applied for many semantic parsing tasks.,7 Discussion,[0],[0]
"One key step is to design the right approximated reward for a given task to guide the beam search to nd the reference parses in MMRN, given that the actual reward is often very sparse.",7 Discussion,[0],[0]
"In our companion paper, (Iyyer et al., 2017), we used a simple form of approximated reward to get feedback as early as possible during search.",7 Discussion,[0],[0]
"In other words, the semantic parse will be executed as soon as the parse is executable (even if the parse is still not completed) during search.",7 Discussion,[0],[0]
The execution results will be used to calculate the Jaccard coefficient with respect to the labeled answers as the approximated rewards.,7 Discussion,[0],[0]
"The use of approximated reward has been proven to be effective in (Iyyer et al., 2017).
",7 Discussion,[0],[0]
"An important research direction for semantic
parsing is to reduce the supervision cost.",7 Discussion,[0],[0]
"In (Yih et al., 2016), the authors demonstrated that labeling semantic parses is possible and often more effective with a sophisticated labeling interface.",7 Discussion,[0],[0]
"However, collecting answers may still be easier or faster for certain problems or annotators.",7 Discussion,[0],[0]
This suggests that we could allow the annotators to choose to label semantic parses or answers in order to minimize the supervision cost.,7 Discussion,[0],[0]
MMRN would be an ideal learning algorithm for this scenario.,7 Discussion,[0],[0]
"This paper proposes Maximum Margin Reward Networks, a structured learning framework that can learn from both explicit and implicit supervision signals.",8 Conclusion,[0],[0]
"In the future, we plan to apply Maximum Margin Reward Networks on other structured learning tasks.",8 Conclusion,[0],[0]
Improving MMRN for dealing with large search space is an important future direction as well.,8 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
The first author is partly sponsored by DARPA under agreement number FA8750-13-2-0008.,Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government.",Acknowledgments,[0],[0]
"Neural networks have achieved state-ofthe-art performance on several structuredoutput prediction tasks, trained in a fully supervised fashion.",abstractText,[0],[0]
"However, annotated examples in structured domains are often costly to obtain, which thus limits the applications of neural networks.",abstractText,[0],[0]
"In this work, we propose Maximum Margin Reward Networks, a neural networkbased framework that aims to learn from both explicit (full structures) and implicit supervision signals (delayed feedback on the correctness of the predicted structure).",abstractText,[0],[0]
"On named entity recognition and semantic parsing, our model outperforms previous systems on the benchmark datasets, CoNLL-2003 and WebQuestionsSP.",abstractText,[0],[0]
Maximum Margin Reward Networks for Learning from Explicit and Implicit Supervision,title,[0],[0]
"✏
2
1 + log
1 comparisons, optimal up to a
constant factor. We then derive a general framework that uses noisy binary search to speed up many ranking algorithms, and combine it with merge sort to obtain a ranking algorithm that uses O n
✏
2
log n(log log n)3 comparisons for = 1 n , optimal up to a (log log n)3 factor.",text,[0],[0]
Maximum selection and sorting using pairwise comparisons are computer-science staples taught in most introductory classes and used in many applications.,1.1. Background,[0],[0]
"In fact, sorting, also known as ranking, was once claimed to utilize 25% of all computer cycles, e.g., (Mukherjee, 2011).
",1.1. Background,[0],[0]
"In many applications, the pairwise comparisons produce only random outcomes.",1.1. Background,[0],[0]
"In sports, tournaments rank teams based on pairwise matches whose outcomes are probabilistic in nature.",1.1. Background,[0],[0]
"For example, Microsoft’s TrueSkill (Herbrich et al., 2006) software matches and ranks thousands of Xbox gamers based on individual game results.",1.1. Background,[0],[0]
"And in online advertising, out of a myriad of possible ads, each web page may display only a few, and a user will typically select at most one.",1.1. Background,[0],[0]
"Based on these random comparisons, ad companies such as Google, Microsoft, or Yahoo, rank the ads’ appeal (Radlinski & Joachims, 2007; Radlinski et al., 2008).
",1.1. Background,[0],[0]
"These and related applications have brought about a resur1University of California, San Diego 2Google Research.",1.1. Background,[0],[0]
"Correspondence to: Venkatadheeraj Pichapati <dheerajpv7@ucsd.edu>.
",1.1. Background,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1.1. Background,[0],[0]
"Copyright 2017 by the author(s).
gence of interest in maximum selection and ranking using noisy comparisons.",1.1. Background,[0],[0]
"Several probabilistic models were considered, including the popular Bradley-TerryLuce (Bradley & Terry, 1952) and its Plackett-Luce (PL) generalization (Plackett, 1975; Luce, 2005).",1.1. Background,[0],[0]
"Yet even for such specific models, the number of pairwise comparisons needed, or sample complexity, of maximum selection and ranking was known only to within a log n factor.",1.1. Background,[0],[0]
We consider a significantly broader class of models and yet propose algorithms that are optimal up to a constant factor for maximum selection and up to (log log n)3 for ranking.,1.1. Background,[0],[0]
"Noiseless comparison assumes an unknown underlying ranking r(1), . . .",1.2. Notation,[0],[0]
",r(n) of the elements in {1, . . .",1.2. Notation,[0],[0]
",n} such that if two elements are compared, the higher-ranked one is selected.",1.2. Notation,[0],[0]
"Similarly for noisy comparisons, we assume an unknown ranking of the elements, but now if two elements i and j are compared, i is chosen with some unknown probability p(i, j) and j is chosen with probability p(j, i) = 1 p(i, j), where if i is higher-ranked, then p(i, j) 1
2 .",1.2. Notation,[0],[0]
"Repeated comparisons are independent of each other.
",1.2. Notation,[0],[0]
"Let p̃(i, j) = p(i, j) 1 2 reflect the additional probability by which i is preferable to j. Note that p̃(j, i) = p̃(i, j) and p̃(i, j) 0 if r(i) > r(j).",1.2. Notation,[0],[0]
"|p̃(i, j)| can also be seen as a measure of dissimilarity between i and j. Following (Yue & Joachims, 2011), we assume that two natural properties, satisfied for example by the PL model, hold whenever r(i) > r(j) >",1.2. Notation,[0],[0]
"r(k): Strong Stochastic Transitivity (SST), p̃(i, k) max(p̃(i, j), p̃(j, k)), and Stochastic Triangle Inequality (STI), p̃(i, k)  ",1.2. Notation,[0],[0]
"p̃(i, j) + p̃(j, k).",1.2. Notation,[0],[0]
"Two types of algorithms have been proposed for maximum selection and ranking under noisy comparisons: nonadaptive or offline (Rajkumar & Agarwal, 2014; Negahban et al., 2012; 2016; Jang et al., 2016) where the comparison pairs are chosen in advance, and adaptive or online where the comparison pairs are selected sequentially based on previous comparison results.",1.2. Notation,[0],[0]
"We focus on the latter.
",1.2. Notation,[0],[0]
"We specify the desired output via the (✏, )-PAC paradigm (Yue & Joachims, 2011; Szörényi et al., 2015) that requires the output to likely closely approximate the intended outcome.",1.2. Notation,[0],[0]
"Specifically, given ✏, > 0, with prob-
ability 1 , maximum selection must output an ✏- maximum element i such that for all j, p(i, j) 1
2 ✏.",1.2. Notation,[0],[0]
"Similarly, with probability 1 , the ranking algorithm must output an ✏-ranking r0(1), . . .",1.2. Notation,[0],[0]
",r0(n) such that whenever r0(i) > r0(j), p(i, j) 1
2
✏.",1.2. Notation,[0],[0]
In Section 2 we review past work and summarize our contributions.,1.3. Outline,[0],[0]
In Section 3 we describe and analyze our maximum-selection algorithm.,1.3. Outline,[0],[0]
In Section 4 we propose and evaluate the ranking algorithm.,1.3. Outline,[0],[0]
In Section 5 we experimentally compare our algorithms with existing ones.,1.3. Outline,[0],[0]
In Section 6 we mention some future directions.,1.3. Outline,[0],[0]
Several researchers studied algorithms that with probability 1 find the exact maximum and ranking.,2.1. Related work,[0],[0]
"(Feige et al., 1994) considered a simple model where the elements are ranked, and p̃(i, j) =",2.1. Related work,[0],[0]
✏ whenever r(i) > r(j).,2.1. Related work,[0],[0]
"(Busa-Fekete et al., 2014a) considered comparison probabilities p(i, j) satisfying the Mallows model (Mallows, 1957).",2.1. Related work,[0],[0]
"And (Urvoy et al., 2013; Busa-Fekete et al., 2014b; Heckel et al., 2016) considered general comparison probabilities, without an underlying ranking assumption, and derived rankings based on Copeland- and Borda-counts, and random-walk procedures.",2.1. Related work,[0],[0]
"As expected, when the comparison probabilities approach half, the above algorithms require arbitrarily many comparisons.
",2.1. Related work,[0],[0]
"To achieve finite complexity even with near-half comparison probabilities, researchers adopted the PAC paradigm.",2.1. Related work,[0],[0]
"For the PAC model with SST and STI constraints, (Yue & Joachims, 2011) derived a maximum-selection algorithm with sample complexity O n
✏
2
log
n
✏
and used it to bound
the regret of the problem’s dueling-bandits variant.",2.1. Related work,[0],[0]
"Related results appeared in (Syrgkanis et al., 2016).",2.1. Related work,[0],[0]
"For the PL model, (Szörényi et al., 2015) derived a PAC ranking algorithm with sample complexity O( n
✏
2 log n log n ✏ ).
",2.1. Related work,[0],[0]
"Deterministic adversarial versions of the problem were considered by (Ajtai et al., 2015), and by (Acharya et al., 2014a; 2016) who were motivated by density estimation (Acharya et al., 2014b).",2.1. Related work,[0],[0]
"We consider (✏, )-PAC adaptive maximum selection and ranking using pairwise comparisons under SST and STI constraints.",2.2. New results,[0],[0]
"Note that when ✏ 1
2 or 1 1/n for maximum selection and 1 1/n2 for ranking, any output is correct.",2.2. New results,[0],[0]
"We show for ✏ < 1/4, < 1
2
and any n:
• Maximum-selection algorithm with sample complexity",2.2. New results,[0],[0]
"O n
✏
2
1 + log
1 , optimal up to a constant factor.
",2.2. New results,[0],[0]
"• Ranking algorithm with O n ✏ 2 (log n)3 log n sample
complexity.
",2.2. New results,[0],[0]
"• General framework that converts any ranking algorithm with sample complexity O n
✏
2
(log n)x log n
into a ranking algorithm that for 1 n has sample complexity",2.2. New results,[0],[0]
"O n
✏
2
log n(log log n)x .
",2.2. New results,[0],[0]
"• Using the above framework, a ranking algorithm with sample complexity",2.2. New results,[0],[0]
"O n
✏
2
log n(log log n)3 for = 1 n .
",2.2. New results,[0],[0]
•,2.2. New results,[0],[0]
"An ⌦ n ✏ 2 log n lower bound on the sample complex-
ity of any PAC ranking algorithm, matching our algorithm’s sample complexity up to a (log log n)3 factor.",2.2. New results,[0],[0]
We propose a simple maximum-selection algorithm based on Knockout tournaments.,3.1. Algorithm outline,[0],[0]
Knockout tournaments are used to find a maximum element under non-noisy comparisons.,3.1. Algorithm outline,[0],[0]
"Knockout tournament of n elements runs in dlog ne rounds where in each round it randomly pairs the remaining elements and proceeds the winners to next round.
",3.1. Algorithm outline,[0],[0]
"Our algorithm, given in KNOCKOUT uses O n
✏
2
1 + log
1
comparisons and O(n) memory
to find an ✏-maximum.",3.1. Algorithm outline,[0],[0]
"(Yue & Joachims, 2011) uses O n
✏
2
log
n
✏
comparisons and O(n2) memory to find an
✏-maximum.",3.1. Algorithm outline,[0],[0]
Hence we get log n-factor improvement in the number of comparisons and also we use linear memory compared to quadratic memory.,3.1. Algorithm outline,[0],[0]
"From (Zhou & Chen, 2014) it can be inferred that the best PAC maximum selection algorithm requires ⌦ n
✏
2
1 + log
1
comparisons,
hence up to constant factor, KNOCKOUT is optimal.
",3.1. Algorithm outline,[0],[0]
"(Yue & Joachims, 2011; Szörényi et al., 2015) eliminate elements one by one until only ✏-maximums are remaining.",3.1. Algorithm outline,[0],[0]
"Since they potentially need n 1 eliminations, in order to appply union bound they had to ensure that each eliminated element is not an ✏-maximum w.p. 1 /n, requiring O(log(n/ ))",3.1. Algorithm outline,[0],[0]
comparisons for each eliminated element and hence a superlinear sample complexity O(n log(n/ )).,3.1. Algorithm outline,[0],[0]
"In contrast, KNOCKOUT eliminates elements in log n rounds.",3.1. Algorithm outline,[0],[0]
"Since in Knockout tournaments, number of elements decrease exponentially with each round, we afford to endure more error in the initial rounds and less error in the latter rounds by repeating comparison between each pair more times in latter rounds.",3.1. Algorithm outline,[0],[0]
"Specifically, let b
i be the highest-ranked element (according to the unobserved underlying ranking) at the beginning of round i. KNOCKOUT makes sure that w.p. 1
2
i , p̃(b i , b i+1 )  ",3.1. Algorithm outline,[0],[0]
"✏ i by repeating
comparison between each pair in round i for O ⇣ 1
✏
2 i
log
2
i
⌘
times.",3.1. Algorithm outline,[0],[0]
"Choosing ✏ i = c✏
2
i/3 with c = 21/3 1, we make sure that comparison complexity is O n
✏
2
1 + log
1 and by
union bound and STI, w.p. 1 , p̃(b 1 , bdlogne+1) Pdlogne+1 i=1",3.1. Algorithm outline,[0],[0]
"c✏
2
i/3  ✏.",3.1. Algorithm outline,[0],[0]
"For 1, a relaxed notion of SST, called -stochastic transitivity (Yue & Joachims, 2011), requires that if r(i) > r(j) >",3.1. Algorithm outline,[0],[0]
"r(k), then max(p̃(i, j), p̃(j, k))  · p̃(i, k).",3.1. Algorithm outline,[0],[0]
Our results apply to this general notion of -stochastic transitivity and the analysis of KNOCKOUT is presented under this model.,3.1. Algorithm outline,[0],[0]
"KNOCKOUT uses O ⇣ n 4
✏
2
1 + log
1 ⌘ com-
parisons.
",3.1. Algorithm outline,[0],[0]
Remark 1.,3.1. Algorithm outline,[0],[0]
"(Yue & Joachims, 2011) considered a different definition of ✏-maximum as an element i that is at most ✏ dissimilar to true maximum i.e., for j with r(j) = n, p̃(j, i)  ✏.",3.1. Algorithm outline,[0],[0]
"Note that this definition is less restrictive than ours, hence requires fewer comparisons.",3.1. Algorithm outline,[0],[0]
"Under this definition, (Yue & Joachims, 2011) used O ⇣ n 6
✏
2
log
n
✏
⌘ com-
parisons to find an ✏-maximum whereas a simple modification of KNOCKOUT shows that O ⇣ n 2
✏
2
1 + log
1 ⌘ com-
parisons suffice.",3.1. Algorithm outline,[0],[0]
"Hence we also get a significant improvement in the exponent of .
",3.1. Algorithm outline,[0],[0]
"To simplify the analysis, we assume that n is a power of 2, otherwise we can add 2dlogne n dummy elements that lose to every original element with probability 1.",3.1. Algorithm outline,[0],[0]
Note that all ✏-maximums will still be from the original set.,3.1. Algorithm outline,[0],[0]
We start with a subroutine COMPARE that compares two elements.,3.2. Algorithm,[0],[0]
"It compares two elements i, j and maintains empirical probability p̂
i , a proxy for p(i, j).",3.2. Algorithm,[0],[0]
"It also maintains a confidence value ĉ s.t., w.h.p., p̂
",3.2. Algorithm,[0],[0]
"i 2 (p(i, j) ĉ, p(i, j)+ ĉ).",3.2. Algorithm,[0],[0]
"COMPARE stops if it is confident about the winner or if it reaches its comparison budget m. It outputs the element with more wins breaking ties randomly.
",3.2. Algorithm,[0],[0]
Algorithm 1,3.2. Algorithm,[0],[0]
COMPRARE Input: element,3.2. Algorithm,[0],[0]
"i, element j, bias ✏, confidence .",3.2. Algorithm,[0],[0]
"Initialize: p̂
i
=
1
2 , ĉ = 1 2 , m = 1 2✏ 2 log 2 , r = 0, w i = 0.
1.",3.2. Algorithm,[0],[0]
"while (|p̂ i
1 2 |  ĉ ✏ and r  m) (a) Compare i and j. if i wins w
i = w i + 1.
(b) r = r + 1, p̂ i = w i
r
, ĉ = q 1
2r
log
4r
2 .
",3.2. Algorithm,[0],[0]
"if p̂ i  1 2 Output: j. else Output: i.
",3.2. Algorithm,[0],[0]
"We show that COMPARE w.h.p., outputs the correct winner if the elements are well seperated.
",3.2. Algorithm,[0],[0]
Lemma 2.,3.2. Algorithm,[0],[0]
"If p̃(i, j) ✏, then Pr(COMPARE(i, j, ✏, ) 6=",3.2. Algorithm,[0],[0]
"i)  .
",3.2. Algorithm,[0],[0]
"Note that instead of using fixed number of comparisons, COMPARE stops the comparisons adaptively if it is confident about the winner.",3.2. Algorithm,[0],[0]
"If |p̃(i, j)| ✏, COMPARE stops much before comparison budget 1
2✏
2
log
2 and hence works better in practice.
",3.2. Algorithm,[0],[0]
Now we present the subroutine KNOCKOUT-ROUND that we use in main algorithm KNOCKOUT.,3.2. Algorithm,[0],[0]
KNOCKOUT-ROUND takes a set S and outputs a set of size |S|/2.,3.2.1. KNOCKOUT-ROUND,[0],[0]
"It randomly pairs elements, compares each pair using COMPARE, and returns the set of winners.",3.2.1. KNOCKOUT-ROUND,[0],[0]
"We will later show that maximum element in the output set will be comparable to maximum element in the input set.
",3.2.1. KNOCKOUT-ROUND,[0],[0]
"Algorithm 2 KNOCKOUT-ROUND Input: Set S, bias ✏, confidence .",3.2.1. KNOCKOUT-ROUND,[0],[0]
"Initialize: Set O = ;.
1.",3.2.1. KNOCKOUT-ROUND,[0],[0]
"Pair elements in S randomly.
",3.2.1. KNOCKOUT-ROUND,[0],[0]
2.,3.2.1. KNOCKOUT-ROUND,[0],[0]
"for every pair (i, j):
Add COMPARE(i, j, ✏, ) to O.
Output: O
Note that comparisons between each pair can be handled by a different processor and hence this algorithm can be easily parallelized.
S can have several maximum elements.",3.2.1. KNOCKOUT-ROUND,[0],[0]
Comparison probabilities corresponding to all maximum elements will be essentially same because of STI.,3.2.1. KNOCKOUT-ROUND,[0],[0]
"We define max(S) to be the maximum element with the least index, namely,
max(S) def=",3.2.1. KNOCKOUT-ROUND,[0],[0]
"S ⇣ min{i : p̃(S(i), S(j)) 0",3.2.1. KNOCKOUT-ROUND,[0],[0]
"8j} ⌘ .
",3.2.1. KNOCKOUT-ROUND,[0],[0]
Lemma 3.,3.2.1. KNOCKOUT-ROUND,[0],[0]
"KNOCKOUT-ROUND(S, ✏, ) uses |S| 4✏ 2 log 2 comparisons and with probability 1 ,
p̃ max(S),max ⇣ KNOCKOUT-ROUND(S, ✏, ) ⌘!  ",3.2.1. KNOCKOUT-ROUND,[0],[0]
✏.,3.2.1. KNOCKOUT-ROUND,[0],[0]
Now we present the main algorithm KNOCKOUT.,3.2.2. KNOCKOUT,[0],[0]
KNOCKOUT takes an input set S and runs log n rounds of KNOCKOUT-ROUND halving the size of S at the end of each round.,3.2.2. KNOCKOUT,[0],[0]
"Recall that KNOCKOUT-ROUND makes sure that maximum element in the output set is comparable to
maximum element in the input set.",3.2.2. KNOCKOUT,[0],[0]
"Using this, KNOCKOUT makes sure that the output element is comparable to maximum element in the input set.
",3.2.2. KNOCKOUT,[0],[0]
"Since the size of S gets halved after each round, KNOCKOUT compares each pair more times in the latter rounds.",3.2.2. KNOCKOUT,[0],[0]
"Hence the bias between maximum element in input set and maximum element in output set is small in latter rounds.
",3.2.2. KNOCKOUT,[0],[0]
"Algorithm 3 KNOCKOUT Input: Set S, bias ✏, confidence , stochasticity .",3.2.2. KNOCKOUT,[0],[0]
"Initialize: i = 1, S = set of all elements, c = 21/3 1.",3.2.2. KNOCKOUT,[0],[0]
"while |S| > 1
1.",3.2.2. KNOCKOUT,[0],[0]
"S = KNOCKOUT-ROUND ⇣ S, c✏
2
2
i/3 , 2",3.2.2. KNOCKOUT,[0],[0]
"i
⌘ .
2.",3.2.2. KNOCKOUT,[0],[0]
"i = i+ 1.
",3.2.2. KNOCKOUT,[0],[0]
"Output: the unique element in S.
Note that KNOCKOUT uses only memory of set S and hence O(n) memory suffices.",3.2.2. KNOCKOUT,[0],[0]
Theorem 4 shows that KNOCKOUT outputs an ✏-maximum with probability 1 .,3.2.2. KNOCKOUT,[0],[0]
It also bounds the number of comparisons used by the algorithm.,3.2.2. KNOCKOUT,[0],[0]
Theorem 4.,3.2.2. KNOCKOUT,[0],[0]
"KNOCKOUT(S, ✏, ) uses O ⇣
4|S| ✏ 2 1 + log 1
⌘ comparisons and with proba-
bility at least 1 , outputs an ✏-maximum.",3.2.2. KNOCKOUT,[0],[0]
"We propose a ranking algorithm that with probability at least 1 1
n
uses O ⇣ n logn(log logn) 3
✏
2
⌘ comparisons and out-
puts an ✏-ranking.",4. Ranking,[0],[0]
"Notice that we use only ˜O ⇣ n logn
✏
2
⌘ comparisons for = 1
n
where as (Szörényi et al., 2015) uses O n(log n)2/✏2 comparisons even for constant error probability .",4. Ranking,[0],[0]
"Furthermore (Szörényi et al., 2015) provided these guarantees only under Plackett-Luce model which is more restrictive compared to ours.",4. Ranking,[0],[0]
"Also, their algorithm uses O(n2) memory compared to O(n) memory requirement of ours.",4. Ranking,[0],[0]
"Our main algorithm BINARY-SEARCH-RANKING assumes the existence of a ranking algorithm RANK-x that with probability at least 1 uses O n
✏
2
(log n)x log n com-
parisons and outputs an ✏-ranking for any > 0, ✏ > 0 and some x > 1.",4. Ranking,[0],[0]
"We also present a RANK-x algorithm with x = 3.
",4. Ranking,[0],[0]
Observe that we need RANK-x algorithm to work for any model that satisfies SST and STI.,4. Ranking,[0],[0]
"(Szörényi et al., 2015) showed that their algorithm works for Plackett-Luce model but not for more general model.",4. Ranking,[0],[0]
"So we present a RANK-x
algorithm that works for general model.
",4. Ranking,[0],[0]
"The main algorithm BINARY-SEARCH-RANKING randomly selects n
(logn)
x elements (anchors) and rank them using RANK-x .",4. Ranking,[0],[0]
"The algorithm has then effectively created n
(logn)
x bins, each between two successively ranked anchors.",4. Ranking,[0],[0]
"Then for each element, the algorithm identifies the bin it belongs to using a noisy binary search algorithm.",4. Ranking,[0],[0]
"The algorithm then ranks the elements within each bin using RANK-x .
",4. Ranking,[0],[0]
"We first present MERGE-RANK, a RANK-3 algorithm.",4. Ranking,[0],[0]
"We present a simple ranking algorithm MERGE-RANK that uses O ⇣ n(logn) 3
✏
2
log
n ⌘",4.1. Merge Ranking,[0],[0]
"comparisons, O(n) memory and
with probability 1 outputs an ✏-ranking.",4.1. Merge Ranking,[0],[0]
"Thus MERGE-RANK is a RANK-x algorithm for x = 3.
",4.1. Merge Ranking,[0],[0]
"Similar to Merge Sort, MERGE-RANK divides the elements into two sets of equal size, ranks them separately and combines the sorted sets.",4.1. Merge Ranking,[0],[0]
"Due to the noisy nature of comparisons, MERGE-RANK compares two elements i, j sufficient times, so that the comparison output is correct with high probability when |p̃(i, j)| ✏
logn .",4.1. Merge Ranking,[0],[0]
"Put differently, MERGE-RANK is same as the typical Merge Sort, except it uses COMPARE as the comparison function.",4.1. Merge Ranking,[0],[0]
"Due to lack of space, MERGE-RANK is presented in Appendix A.
Let’s define the error of an ordered set S as the maximum distance between two wrongly ordered items in S, namely,
err(S) def = max 1ij|S| p̃(S(i), S(j)).
",4.1. Merge Ranking,[0],[0]
"We show that when we merge two ordered sets, the error of the resulting ordered set will be at most ✏
logn more than the maximum of errors of individual ordered sets.
",4.1. Merge Ranking,[0],[0]
Observe that MERGE-RANK is a recursive algorithm and the error of a singleton set is 0.,4.1. Merge Ranking,[0],[0]
"Two singleton sets each containing a unique element from the input set merge to form a set with two elements with an error at most 2✏
logn , then two sets with two elements merge to form a set with four elements with an error of at most 3✏
logn and henceforth.",4.1. Merge Ranking,[0],[0]
"Thus the error of the output ordered set is bounded by ✏.
",4.1. Merge Ranking,[0],[0]
Lemma 5 shows that MERGE-RANK can output an ✏- ranking of S with probability 1 .,4.1. Merge Ranking,[0],[0]
It also bounds the number of comparisons used by the algorithm.,4.1. Merge Ranking,[0],[0]
Lemma 5.,4.1. Merge Ranking,[0],[0]
"MERGE-RANK ⇣ S, ✏
log |S| , |S|2
⌘ takes
O ⇣
|S|(log |S|)3 ✏ 2 log |S|
⌘ comparisons and with probability
1 , outputs an ✏-ranking.",4.1. Merge Ranking,[0],[0]
"Hence, MERGE-RANK is a RANK-3 algorithm.
",4.1. Merge Ranking,[0],[0]
Now we present our main ranking algorithm.,4.1. Merge Ranking,[0],[0]
We first sketch the algorithm outline below.,4.2. BINARY-SEARCH-RANKING,[0],[0]
We then provide a proof outline.,4.2. BINARY-SEARCH-RANKING,[0],[0]
Our algorithm is stated in BINARY-SEARCH-RANKING.,4.2.1. ALGORITHM OUTLINE,[0],[0]
"It can be summarized in three major parts.
",4.2.1. ALGORITHM OUTLINE,[0],[0]
"Creating anchors: (Steps 1 to 3) BINARY-SEARCHRANKING first selects a set S0 of n
(logn)
x random elements (anchors) and ranks them using RANK-x .",4.2.1. ALGORITHM OUTLINE,[0],[0]
"At the end of this part, there are n
(logn)
x ranked anchors.",4.2.1. ALGORITHM OUTLINE,[0],[0]
"Equivalently, the algorithm creates n
(logn)
",4.2.1. ALGORITHM OUTLINE,[0],[0]
"x 1 bins, each bin between two successively ranked anchors.
",4.2.1. ALGORITHM OUTLINE,[0],[0]
"Coarse ranking: (Step 4) After forming the bins, the algorithm uses a random walk on a binary search tree, to find which bin each element belongs to.",4.2.1. ALGORITHM OUTLINE,[0],[0]
"INTERVAL-BINARYSEARCH is similar to the noisy binary search algorithm in (Feige et al., 1994).",4.2.1. ALGORITHM OUTLINE,[0],[0]
It builds a binary search tree with the bins as the leaves and it does a random walk over this tree.,4.2.1. ALGORITHM OUTLINE,[0],[0]
"Due to lack of space the algorithm INTERVAL-BINARYSEARCH is presented in Appendix B but more intuition is given later in this section.
",4.2.1. ALGORITHM OUTLINE,[0],[0]
"Ranking within each bin: (Step 5) For each bin, we show that the number of elements far from both anchors is bounded.",4.2.1. ALGORITHM OUTLINE,[0],[0]
The algorithm checks elements inside a bin whether they are close to any of the bin’s anchors.,4.2.1. ALGORITHM OUTLINE,[0],[0]
"For the elements that are close to anchors, the algorithm ranks them close to the anchor.",4.2.1. ALGORITHM OUTLINE,[0],[0]
And for the elements that are away from both anchors the algorithm ranks them using RANK-x and outputs the resulting ranking.,4.2.1. ALGORITHM OUTLINE,[0],[0]
Creating anchors In Step 1 of the algorithm we select n/(log n)x random elements.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Since these are chosen uniformly random, they lie nearly uniformly in the set S. This intuition is formalized in the next lemma.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 6.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Consider a set S of n elements.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If we select n
(logn)
x elements uniformly randomly from S and build an ordered set S0 s.t. p̃(S0(i), S0(j)) 0",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"8i > j , then with probability 1 1
n
4
, for any ✏ > 0 and all k,
|{e 2 S : p̃(e, S0(k))",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"> ✏, p̃(S0(k+1), e) >",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"✏}|  5(log n)x+1.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"In Step 2, we use RANK-x to rank S0.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Lemma 7 shows the guarantee of ranking S0.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 7.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"After Step 2 of the BINARY-SEARCHRANKING with probability 1 1
n
6
, S0 is ✏0-ranked.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"At the end of Step 2, we have n (logn)",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"x 1 bins, each between two successively ranked anchors.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Each bin has a left
Algorithm 4 BINARY-SEARCH-RANKING Input: Set S, bias ✏.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Initialize: ✏0 = ✏/16, ✏00 = ✏/15, and So = ;.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"S
j = ;, C
j = ; and B j = ;, for 1  j  j n
(logn)
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"x
k + 2.
1.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Form a set S0 with j n
(logn)
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"x
k random elements from
S. Remove these elements from S.
2.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Rank S0 using RANK-x S0, ✏0, 1
n
6
.
3.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Add dummy element a at the beginning of S0 such that p(a, e) = 0",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
8e 2 SSS0.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Add dummy element b at the end of S0 such that p(b, e) = 1 8e 2 SSS0.
4.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"for e 2 S: (a) k = INTERVAL-BINARY-SEARCH(S0, e, ✏00).",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"(b) Insert e in S
k
.
5.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"for j = 1 to j n
(logn)
x
k + 2:
(a) for e 2 S j
: i. if COMPARE2(e, S0(j), 10✏00 2 log n) 2⇥
1
2 6✏00, 1 2
+ 6✏00 ⇤ , insert e in C j
.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"ii. else if COMPARE2(e, S0(j +
1), 10✏00 2 log n) 2 ⇥ 1 2 6✏00, 1 2 + 6✏00 ⇤ , then insert e in C j+1
.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"iii. else insert e in B
j .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"(b) Rank B
j
using RANK-x B
j , ✏00, 1 n 4 .
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"(c) Append S0(j), C j , B j in order at the end of So.
Output:",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"So
anchor and a right anchor .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We say that an element belongs to a bin if it wins over the bin’s left anchor with probability 1
2 and wins over the bin’s right anchor with probability  1
2 .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Notice that some elements might win over S0(1) with probability < 1
2 and thus not belong to any bin.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"So in Step 3, we add a dummy element a at the beginning of S0 where a loses to every element in S S S0 with probability 1.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"For similar reasons we add a dummy element b to the end of S0 where every element in S S S0 loses to b with probability 1.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Coarse Ranking Note that S0(i) and S0(i+ 1) are respectively the left and right anchors of the bin S
i
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Algorithm 5 COMPARE2 Input: element i, element j, number of comparisons m.
1.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Compare i and j for m times and return the fraction of times i wins over j.
Since S0 is ✏0-ranked and the comparisons are noisy, it is hard to find a bin S
i for an element e such that p(e, S0(i))",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"1
2 and p(S0(i + 1), e) 1 2 .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We call a bin S i a ✏00 nearly correct bin for an element e if p(e, S0(i))",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"1
2 ✏00 and p(S0(i+ 1), e) 1
2 ✏00 for some ✏00 >",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
✏0.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"In Step 4, for each element we find an ✏00-nearly correct bin using INTERVAL-BINARY-SEARCH .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Next we describe an outline of INTERVAL-BINARY-SEARCH.
INTERVAL-BINARY-SEARCH first builds a binary search tree of intervals (see Appendix B) as follows: the root node is the entire interval between the first and the last elements in S0.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Each non-leaf node interval,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
I has two children corresponding to the left and right halves of I .,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"The leaves of the tree are the bins between two successively ranked anchors.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"To find an ✏00-nearly correct bin for an element e, the algorithm starts at the root of the binary search tree and at every non-leaf node corresponding to interval I , it checks if e belongs to I or not by comparing e with I’s left and right anchors.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If e loses to left anchor or wins against the right anchor, the algorithm backtracks to current node’s parent.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If e wins against I’s left anchor and loses to its right one, the algorithm checks if e belongs to the left or right child by comparing e with the middle element of I and moves accordingly.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"When at a leaf node, the algorithm checks if e belongs to the bin by maintaining a counter.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If e wins against the bin’s left anchor and loses to the bin’s right anchor, it increases the counter by one or otherwise it decreases the counter by one.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
If the counter is less than 0 the algorithm backtracks to the bin’s parent.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"By repeating each comparison several times, the algorithm makes a correct decision with probability 19
20
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Note that there could be several ✏00-nearly correct bins for e and even though at each step the algorithm moves in the direction of one of them, it could end up moving in a loop and never reaching one of them.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We thus run the algorithm for 30 log n steps and terminate.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
If the algorithm is at a leaf node by 30 log n steps and the counter is more than 10 log n,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
we show that the leaf node bin is a ✏00-nearly correct bin for e and the algorithm outputs the leaf node.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If not, the algorithm puts in a set Q all the anchors visited so far and orders Q according to S0.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We select 30 log n steps to ensure that if there is only one nearly correct bin, then the algorithm outputs that bin w.p. 1 1
n
6 .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Also we do not want too many steps so as to bound the size of Q.
By doing a simple binary search in Q using BINARYSEARCH (see Appendix B) we find an anchor f 2 Q such that |p̃(e, f)|  4✏00.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Since INTERVAL-BINARY-
SEARCH ran for at most 30 log n steps, Q can have at most 60 log n elements and hence BINARY-SEARCH can search effectively by repeating each comparison O(log n) times to maintain high confidence.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Next paragraph explains how BINARY-SEARCH finds such an element f .
BINARY-SEARCH first compares e with the middle element m of Q for O(log n) times.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If the fraction of wins for e is between 1
2 3✏00 and 1 2 + 3✏00, then w.h.p. |p̃(e,m)|  4✏00 and hence BINARY-SEARCH outputs m. If the fraction of wins for e is less than 1
2 3✏00, then w.h.p.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"p̃(e,m)  2✏00 and hence it eliminates all elements to the right of m in Q. If the fraction of wins for e is more than 1
2 +3✏00, then w.h.p.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"p̃(e,m) 2✏00 and hence it eliminates all elements to the left of m in Q.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"It continues this process until it finds an element f such that the fraction of wins for e is between 1
2 3✏00 and 1 2 + 3✏00.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"In next Lemma, we show that INTERVAL-BINARYSEARCH achieves to find a 5✏00-nearly correct bin for every element.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 8.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"For any element e 2 S, Step 4 of BINARY-SEARCH-RANKING places e in bin S
l such that p̃(e, S0(l))",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"> 5✏00 and p̃(S0(l+1), e) > 5✏00 with probability 1 1
n
5
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Ranking within each bin Once we have identified the bins, we rank the elements inside each bin.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"By Lemma 6, inside each bin all elements are close to the bin’s anchors except at most 5(log n)x+1 of them.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
The algorithm finds the elements close to anchors in Step 5a by comparing each element in the bin with the bin’s anchors.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"If an element in bin S
j is close to bin’s anchors S0(j) or S0(j + 1) , the algorithm moves it to the set C
j
or C j+1 accordingly and if it is far away from both, the algorithm moves it to the set B
j .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
The following two lemmas state that this separating process happens accurately with high probability.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
The proofs of these results follow from the Chernoff bound and hence omitted.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 9.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"At the end of Step 5a, for all j, 8e 2 C
j , |p̃(e, S0(j))| < 7✏00 with probability 1 1
n
3 .",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 10.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"At the end of Step 5a, for all j, 8e 2 B
j , min(p̃(e, S0(j)), p̃(S0(j + 1), e))",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"> 5✏00 with probability 1 1
n
3
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Combining Lemmas 6, 7 and 10 next lemma shows that the size of B
j is bounded for all j. Lemma 11.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"At the end of Step 5a, |B
j |  5(log n)x+1 for all j, with probability 1 3
n
3
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Since all the elements in C j are already close to an anchor, they need not be ranked.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"By Lemma 11 with probability 1 3
n
3 the number of elements in B j is at most 5(log n)x+1.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We use RANK-x to rank each B
j and output the final ranking.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 12 shows that all B j ’s are ✏00-ranked at the end of Step 5b.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Proof follows from properties of RANK-x and union bound.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Lemma 12.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"At the end of Step 5b, all B
j s are ✏00-ranked with probability 1 1
n
3
.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Combining the above set of results yields our main result.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Theorem 13.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Given access to RANK-x, BINARYSEARCH-RANKING with probability 1 1
n
, uses
O ⇣ n logn(log logn)",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"x
✏
2
⌘ comparisons and outputs an ✏-
ranking.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Using MERGE-RANK as a RANK-x algorithm with x = 3 leads to the following corollary.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Corollary 14.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"BINARY-SEARCH-RANKING uses O ⇣ n logn(log logn) 3
✏
2
⌘ comparisons and outputs an ✏-
ranking with probability 1 1 n .
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Using PALPAC-AMPRR (Szörényi et al., 2015) as a RANK-x algorithm with x = 1 leads to the following corollary over PL model.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Corollary 15.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"Over PL model, BINARY-SEARCHRANKING with probability 1 1
n
uses
O ⇣ n logn log logn
✏
2
⌘ comparisons and outputs an ✏-ranking.
",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"It is well known that to rank a set of n values under the noiseless setting, ⌦(n log n) comparisons are necessary.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"We show that under the noisy model, ⌦ n
✏
2
log
n samples
are necessary to output an ✏-ranking and hence our algorithm is near-optimal.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
Theorem 16.,4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
"For ✏  1
4 ,  1 2
, there exists a noisy model that satisfies SST and STI such that to output an ✏-ranking with probability 1 , ⌦ n
✏
2
log
n comparisons are
necessary.",4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING,[0],[0]
We compare the performance of our algorithms with that of others over simulated data.,5. Experiments,[0],[0]
"Similar to (Yue & Joachims, 2011), we consider the stochastic model where p(i, j) = 0.6 8i <",5. Experiments,[0],[0]
j. Note that this model satisfies both SST and STI.,5. Experiments,[0],[0]
We find 0.05-maximum with error probability = 0.1.,5. Experiments,[0],[0]
Observe that i = 1 is the only 0.05-maximum.,5. Experiments,[0],[0]
"We compare the sample complexity of KNOCKOUT with that of BTMPAC (Yue & Joachims, 2011), MallowsMPI (Busa-Fekete et al., 2014a), and AR (Heckel et al., 2016).",5. Experiments,[0],[0]
"BTM-PAC is an (✏, )-PAC algorithm for the same model considered in this paper.",5. Experiments,[0],[0]
MallowsMPI finds a Condorcet winner which exists under our general model.,5. Experiments,[0],[0]
AR finds the maximum according to Borda scores.,5. Experiments,[0],[0]
"We also tried PLPAC (Szörényi et al., 2015), developed originally for PL model but the algorithm could not meet guarantees of = 0.1 under this
model and hence omitted.",5. Experiments,[0],[0]
"Note that in all the experiments the reported numbers are averaged over 100 runs.
",5. Experiments,[0],[0]
"In Figure 1, we compare the sample complexity of algorithms when there are 7, 10 and 15 elements.",5. Experiments,[0],[0]
Our algorithm outperforms all the others.,5. Experiments,[0],[0]
BTM-PAC performs much worse in comparison to others because of high constants in the algorithm.,5. Experiments,[0],[0]
"Further BTM-PAC allows comparing an element with itself since the main objective in (Yue & Joachims, 2011) is to reduce the regret.",5. Experiments,[0],[0]
"We exclude BTM-PAC for further experiments with higher number of elements.
",5. Experiments,[0],[0]
"In Figure 2, we compare the algorithms when there are 50, 100, 200 and 500 elements.",5. Experiments,[0],[0]
Our algorithm outperforms others for higher number of elements too.,5. Experiments,[0],[0]
Performance of AR gets worse as the number of elements increases since Borda scores of the elements get closer to each other and hence AR takes more comparisons to eliminate an element.,5. Experiments,[0],[0]
"Notice that number of comparisons is in logarithmic scale and hence the performance of MallowsMPI appears to be close to that of ours.
",5. Experiments,[0],[0]
"As noted in (Szörényi et al., 2015), sample complexity of MallowsMPI gets worse as p̃(i, j) gets close to 0.",5. Experiments,[0],[0]
"To
show the pronounced effect, we use the stochastic model p(1, j) = 0.6 8j > 1, p(i, j) = 0.5 + q̃ 8j >",5. Experiments,[0],[0]
"i, i > 1 where q̃ < 0.1, and the number of elements is 15.",5. Experiments,[0],[0]
Here too we find 0.05-maximum with = 0.1.,5. Experiments,[0],[0]
Note that i = 1 is the only 0.05-maximum in this stochastic model.,5. Experiments,[0],[0]
"In Figure 3, we compare the algorithms for different values of q̃: 0.01, 0.005 and 0.001.",5. Experiments,[0],[0]
"As discussed above, the performance of MallowsMPI gets much worse whereas our algorithm’s performance stays unchanged.",5. Experiments,[0],[0]
"The reason is that MallowsMPI finds the Condorcet winner using successive elimination technique and as q̃ gets closer to 0, MallowsMPI takes more comparisons for each elimination.",5. Experiments,[0],[0]
"Our algorithm tries to find an alternative which defeats Condorcet winner with probability 0.5 0.05 and hence for alternatives that are very close to each other, our algorithm declares either one of them as winner after comparing them for certain number of times.
",5. Experiments,[0],[0]
Next we evaluate KNOCKOUT on Mallows model which does not satisfy STI.,5. Experiments,[0],[0]
Mallows is a parametric model which is specified by single parameter .,5. Experiments,[0],[0]
"As in (Busa-Fekete et al., 2014a), we consider n = 10 elements and various values for : 0.03, 0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95 and 0.99.",5. Experiments,[0],[0]
"Here again we seek to find 0.05-maximum with = 0.05.
",5. Experiments,[0],[0]
"As we can see in Figure 4, sample complexity of KNOCKOUT and MallowsMPI is essentially same under small values of but KNOCKOUT outperforms MallowsMPI as gets close to 1 since comparison probabilities grow closer to 1/2.",5. Experiments,[0],[0]
"Surprisingly, for all values of except for 0.99, KNOCKOUT returned Condorcet winner in all runs.",5. Experiments,[0],[0]
"For = 0.99, KNOCKOUT returned second best element in 10 runs out of 100.",5. Experiments,[0],[0]
"Note that p̃(1, 2) = 0.0025 and hence KNOCKOUT still outputed a 0.05-maximum.",5. Experiments,[0],[0]
"Even though we could not show theoretical guarantees of KNOCKOUT under Mallows model, our simulations suggest that it can perform well even under this model.
",5. Experiments,[0],[0]
"For the stochastic model p(i, j) = 0.6 8i < j, we run our MERGE-RANK algorithm to find an ✏-ranking with = 0.1.",5. Experiments,[0],[0]
Figure 5 shows that sample complexity does not increase a lot with decreasing ✏.,5. Experiments,[0],[0]
"We attribute this to the subroutine COMPARE that finds the winner faster when the elements are more dissimilar.
",5. Experiments,[0],[0]
Some more experiments are provided in Appendix G.,5. Experiments,[0],[0]
We studied maximum selection and ranking using noisy comparisons for broad comparison models satisfying SST and STI.,6. Conclusion,[0],[0]
"For maximum selection we presented a simple algorithm with linear, hence optimal, sample complexity.",6. Conclusion,[0],[0]
"For ranking we presented a framework that improves the performance of many ranking algorithms and applied it to merge ranking to derive a near-optimal algorithm.
",6. Conclusion,[0],[0]
"We conducted several experiments showing that our algorithms perform well and out-perform existing algorithms on simulated data.
",6. Conclusion,[0],[0]
The maximum-selection experiments suggest that our algorithm performs well even without STI.,6. Conclusion,[0],[0]
It would be of interest to extend our theoretical guarantees to this case.,6. Conclusion,[0],[0]
"For ranking, it would be interesting to close the (log log n)3 ratio between the upper- and lower- complexity bounds.",6. Conclusion,[0],[0]
"We thank Yi Hao and Vaishakh Ravindrakumar for very helpful discussions and suggestions, and NSF for supporting this work through grants CIF-1564355 and CIF1619448.",7. Acknowledgements,[0],[0]
"We consider (✏, )-PAC maximum-selection and ranking using pairwise comparisons for general probabilistic models whose comparison probabilities satisfy strong stochastic transitivity and stochastic triangle inequality.",abstractText,[0],[0]
"Modifying the popular knockout tournament, we propose a simple maximum-selection algorithm that uses O n ✏ 2 1 + log 1 comparisons, optimal up to a constant factor.",abstractText,[0],[0]
"We then derive a general framework that uses noisy binary search to speed up many ranking algorithms, and combine it with merge sort to obtain a ranking algorithm that uses O n ✏ 2 log n(log log n)3 comparisons for = 1 n , optimal up to a (log log n)3 factor.",abstractText,[0],[0]
Maximum Selection and Ranking under Noisy Comparisons,title,[0],[0]
Abstract reasoning is a hallmark of human intelligence.,1. Introduction,[0],[0]
"A famous example is Einstein’s elevator thought experiment, in which Einstein reasoned that an equivalence relation exists between an observer falling in uniform acceleration and an observer in a uniform gravitational field.",1. Introduction,[0],[0]
"It was the ability to relate these two abstract concepts that allowed him to derive the surprising predictions of general relativity, such as the curvature of space-time.
",1. Introduction,[0],[0]
"A human’s capacity for abstract reasoning can be estimated
*Equal contribution, ordered by surname.",1. Introduction,[0],[0]
"1DeepMind, London, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: <{barrettdavid; felixhill; adamsantoro}@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"A B C D
E F G H
A B C D
E F G H (a) (b)
C on
te xt
P an
el s
An sw
er P
an el
s
+1
+1
XOR(panel 1, panel 2)
Figure 1.",1. Introduction,[0],[0]
Raven-style Progressive Matrices.,1. Introduction,[0],[0]
In (a) the underlying abstract rule is an arithmetic progression on the number of shapes along the columns.,1. Introduction,[0],[0]
"In (b) there is an XOR relation on the shape positions along the rows (panel 3 = XOR(panel 1, panel 2)).",1. Introduction,[0],[0]
Other features such as shape type do not factor in.,1. Introduction,[0],[0]
"A is the correct choice for both.
surprisingly effectively using simple visual IQ tests, such as Raven’s Progressive Matrices (RPMs) (Figure 1) (Raven et al., 1938).",1. Introduction,[0],[0]
The premise behind RPMs is simple: one must reason about the relationships between perceptually obvious visual features – such as shape positions or line colors – to choose an image that completes the matrix.,1. Introduction,[0],[0]
"For example, perhaps the size of squares increases along the rows, and the correct image is that which adheres to this size relation.",1. Introduction,[0],[0]
"RPMs are strongly diagnostic of abstract verbal, spatial and mathematical reasoning ability, discriminating even among populations of highly educated subjects (Snow et al., 1984).
",1. Introduction,[0],[0]
"Since one of the goals of AI is to develop machines with similar abstract reasoning capabilities to humans, to aid scientific discovery for instance, it makes sense to ask whether visual IQ tests can help to understand learning machines.",1. Introduction,[0],[0]
"Unfortunately, even in the case of humans such tests can be invalidated if subjects prepare too much, since test-specific heuristics can be learned that shortcut the need for generallyapplicable reasoning (Te Nijenhuis et al., 2001; Flynn, 1987).",1. Introduction,[0],[0]
"This potential pitfall is even more acute in the case of neural networks, given their striking capacity for memorization
ar X
iv :1
80 7.
04 22
5v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 1
1 Ju
l 2 01
8
(Zhang et al., 2016) and ability to exploit superficial statistical cues (Jo & Bengio, 2017; Szegedy et al., 2013).
",1. Introduction,[0],[0]
"Nonetheless, we contend that visual intelligence tests can help to better understand learning and reasoning in machines (Fleuret et al., 2011), provided they are coupled with a principled treatment of generalisation.",1. Introduction,[0],[0]
Suppose we are concerned with whether a model can robustly infer the notion of ‘monotonically increasing’.,1. Introduction,[0],[0]
"In its most abstract form, this principle can apply to the quantity of shapes or lines, or even the intensity of their colour.",1. Introduction,[0],[0]
We can construct training data that instantiates this notion for increasing quantities or sizes and we can construct test data that only involves increasing colour intensities.,1. Introduction,[0],[0]
Generalisation to the test set would then be evidence of an abstract and flexible application of what it means to monotonically increase.,1. Introduction,[0],[0]
"In this way, a dataset with explicitly defined abstract semantics (e.g., relations, attributes, pixels, etc.), allows us to curate training and testing sets that precisely probe the generalisation dimensions of abstract reasoning in which we are interested.
",1. Introduction,[0],[0]
"To this end, we have developed a large dataset of abstract visual reasoning questions where the underlying abstract semantics can be precisely controlled.",1. Introduction,[0],[0]
"This approach allows us to address the following questions: (1) Can state-of-the-art neural networks find solutions – any solutions – to complex, human-challenging abstract reasoning tasks if trained with plentiful training data?",1. Introduction,[0],[0]
"(2) If so, how well does this capacity generalise when the abstract content of training data is specifically controlled for?
To begin, we describe and motivate our dataset, outline a procedure for automatic generation of data, and detail the generalisation regimes we chose to explore.",1. Introduction,[0],[0]
"Next, we establish a number of strong baselines, and show that well known architectures that use only convolutions, such as ResNet-50 (He et al., 2016), struggle.",1. Introduction,[0],[0]
"We designed a novel variant of the Relation Network (Santoro et al., 2017; Raposo et al., 2017), a neural network with specific structure designed to encourage relation-level comparisons and reasoning.",1. Introduction,[0],[0]
We found that this model substantially outperforms other wellknown architectures.,1. Introduction,[0],[0]
"We then study this top-performing model on our proposed generalisation tests and find that it generalises well in certain test regimes (e.g. applying known abstract relationships in novel combinations), but fails notably in others (such as applying known abstract relationships to unfamiliar entities).",1. Introduction,[0],[0]
"Finally, we propose a means to improve generalisation: the use of auxiliary training to encourage our model to provide an explanation for its solutions.",1. Introduction,[0],[0]
"In 1936 the psychologist John Raven introduced the now famous human IQ test: Raven’s Progressive Matrices (RPM)
(Raven et al., 1938).",2. Procedurally generating matrices,[0],[0]
"RPMs consist of an incomplete 3× 3 matrix of context images (see figure 1), and some (typically 8) candidate answer images.",2. Procedurally generating matrices,[0],[0]
"The subject must decide which of the candidate images is the most appropriate choice to complete the matrix.
",2. Procedurally generating matrices,[0],[0]
"It is thought that much of the power of RPMs as diagnostic of human intelligence derives from the way they probe eductive or fluid reasoning (Jaeggi et al., 2008).",2. Procedurally generating matrices,[0],[0]
"Since no definition of an ‘appropriate” choice is provided, it is in possible in principle to come up with a reason supporting any of the candidate answers.",2. Procedurally generating matrices,[0],[0]
"To succeed, however, the subject must assess all candidate answers, all plausible justifications for those answers, and identify the answer with the strongest justification.",2. Procedurally generating matrices,[0],[0]
"In practice, the right answer tends to be the one that can be explained with the simplest justification using the basic relations underlying the matrices.
",2. Procedurally generating matrices,[0],[0]
"Although Raven hand-designed each of the matrices in his tests, later research typically employed some structured generative model to create large numbers of questions.",2. Procedurally generating matrices,[0],[0]
"In this setting, a potential answer is correct if it is consistent with the underlying generative model, and success rests on the ability to invert the model.",2. Procedurally generating matrices,[0],[0]
Here we describe our process for creating RPM-like matrices.,2.1. Automatic generation of PGMs,[0],[0]
We call our dataset the Procedurally Generated Matrices (PGM) dataset.,2.1. Automatic generation of PGMs,[0],[0]
"To generate PGMs, we take inspiration from Carpenter et al. (1990), who identified and catalogued
the relations that commonly underlie RPMs, as well as Wang & Su (2015), who outlined one process for creating an automatic generator.
",2.1. Automatic generation of PGMs,[0],[0]
The first step is to build an abstract structure for the matrices.,2.1. Automatic generation of PGMs,[0],[0]
"This is done by randomly sampling from the following primitive sets:
• relation types (R, with elements r): progression, XOR, OR, AND, consistent union1 • object types (O, with elements o): shape, line • attribute types (A, with elements a): size, type, colour, position, number
The structure S of a PGM is a set of triples, S =",2.1. Automatic generation of PGMs,[0],[0]
{,2.1. Automatic generation of PGMs,[0],[0]
"[r, o, a] : r ∈ R, o ∈ O, a ∈ A}.",2.1. Automatic generation of PGMs,[0],[0]
These triples determine the challenge posed by a particular matrix.,2.1. Automatic generation of PGMs,[0],[0]
"For instance, if S contains the triple [progression, shape, colour], the PGM will exhibit a progression relation, instantiated on the colour (greyscale intensity) of shapes.",2.1. Automatic generation of PGMs,[0],[0]
"Challenging PGMs exhibit relations governed by multiple such triples: we permit up to four relations per matrix (1 ≤ |S| ≤ 4).
",2.1. Automatic generation of PGMs,[0],[0]
"Each attribute type a ∈ A (e.g. colour) can take one of a finite number of discrete values v ∈ V (e.g. 10 integers between [0, 255] denoting greyscale intensity).",2.1. Automatic generation of PGMs,[0],[0]
"So a given structure has multiple realisations depending on the randomly chosen values for the attribute types, but all of these realisations share the same underlying abstract challenge.",2.1. Automatic generation of PGMs,[0],[0]
The choice of r constrains the values of v that can be realized.,2.1. Automatic generation of PGMs,[0],[0]
"For instance, if r is progression, the values of v must strictly increase along rows or columns in the matrix, but can vary randomly within this constraint.",2.1. Automatic generation of PGMs,[0],[0]
"See the appendix for the full list of relations, attribute types, values, their hierarchical organisation, and other statistics of the dataset.
",2.1. Automatic generation of PGMs,[0],[0]
We use Sa to denote the set of attributes among the triples in S .,2.1. Automatic generation of PGMs,[0],[0]
"After setting values for the colour attribute, we then choose values for all other attributes a 6∈ Sa in one of two ways.",2.1. Automatic generation of PGMs,[0],[0]
"In the distracting setting, we allow these values to vary at random provided that they do not induce any further meaningful relations.",2.1. Automatic generation of PGMs,[0],[0]
"Otherwise, the a 6∈ Sa take a single value that remains consistent across the matrix (for example, perhaps all the shapes are the exact same size).",2.1. Automatic generation of PGMs,[0],[0]
"Randomly varying values across the matrix is a type of distraction common to Raven’s more difficult Progressive Matrices.
",2.1. Automatic generation of PGMs,[0],[0]
"Thus, the generation process consists of: (1) Sampling 1- 4 triples, (2) Sampling values v ∈ V for each a ∈ Sa, adhering to the associated relation r, (3) Sampling values v ∈ V for each a 6∈ Sa, ensuring no spurious relation is induced, (4) Rendering the symbolic form into pixels.
",2.1. Automatic generation of PGMs,[0],[0]
"1Consistent union is a relation wherein the three panels contain elements from some common set, e.g., shape types {square, circle, triangle }.",2.1. Automatic generation of PGMs,[0],[0]
The ordering of the panels containing the elements does not matter.,2.1. Automatic generation of PGMs,[0],[0]
"Generalisation in neural networks has been subject of lots of recent debate, with some emphasising the successes (LeCun et al., 2015) and others the failures (Garnelo et al., 2016; Lake & Baroni, 2017; Marcus, 2018).",2.2. Generalisation Regimes,[0],[0]
"Our choice of regimes is informed by this, but is in no way exhaustive.
",2.2. Generalisation Regimes,[0],[0]
"(1) Neutral In both training and test sets, the structures S can contain any triples [r, o, a] for r ∈ R, o ∈",2.2. Generalisation Regimes,[0],[0]
O and a ∈ A.,2.2. Generalisation Regimes,[0],[0]
"The training and test sets are disjoint, but this separation was at the level of the input variables (i.e., the pixel manifestations of the matrices).
",2.2. Generalisation Regimes,[0],[0]
(2) Interpolation; (3) Extrapolation,2.2. Generalisation Regimes,[0],[0]
"As in the neutral split, S consisted of any triples [r, o, a].",2.2. Generalisation Regimes,[0],[0]
"For interpolation, in the training set, when a = colour or a = size (the ordered attributes), the values of a were restricted to evenindexed members of the discrete set Va, whereas in the test set only odd-indexed values were permitted.",2.2. Generalisation Regimes,[0],[0]
"For extrapolation, the values of a were restricted to the lower half of their discrete set of values Va during training, whereas in the test set they took values in the upper half.",2.2. Generalisation Regimes,[0],[0]
"Note that all S contained some triple [r, o, a] with a = colour or a = size.",2.2. Generalisation Regimes,[0],[0]
"Thus, generalisation is required for every question in the test set.
(4) Held-out Attribute shape-colour or (5) line-type S in the training set contained no triples with o = shape and a = colour.",2.2. Generalisation Regimes,[0],[0]
All structures governing puzzles in the test set contained at least one triple with o = shape and a = colour.,2.2. Generalisation Regimes,[0],[0]
"For comparison, we included a similar split in which triples were held-out if o = line and a = type.
6: Held-out Triples In our dataset, there are 29 possible unique triples [r, o, a].",2.2. Generalisation Regimes,[0],[0]
"We allocated seven of these for the test set, at random, but such that each of the a ∈ A was represented exactly once in this set.",2.2. Generalisation Regimes,[0],[0]
"These held-out triples never occurred in questions in the training set, and every S in the test set contained at least one of them.
7: Held-out Pairs of Triples All S contained at least two triples, of which 400 are viable2",2.2. Generalisation Regimes,[0],[0]
"([r1, o1, a1], [r2, o2, a2]) = (t1, t2).",2.2. Generalisation Regimes,[0],[0]
We randomly allocated 360 to the training set and 40 to the test set.,2.2. Generalisation Regimes,[0],[0]
"Members (t1, t2) of the 40 held-out pairs did not occur together in structures S in the training set, and all structures S had at least one such pair (t1, t2) as a subset.
2Certain triples, such as [progression, shape, number] and [progression, shape, XOR] cannot occur together in the same PGM
8: Held-out Attribute Pairs S contained at least two triples.",2.2. Generalisation Regimes,[0],[0]
"There are 20 (unordered) viable pairs of attributes (a1, a2) such that for some ri, oi, ([r1, o1, a1],",2.2. Generalisation Regimes,[0],[0]
"[r2, o2, a2]) is a viable triple pair.",2.2. Generalisation Regimes,[0],[0]
"([r1, o1, a1], [r2, o2, a2]) = (t1, t2).",2.2. Generalisation Regimes,[0],[0]
We allocated 16 of these pairs for training and four for testing.,2.2. Generalisation Regimes,[0],[0]
"For a pair (a1, a2) in the test set, S in the training set contained triples with a1 and a2.",2.2. Generalisation Regimes,[0],[0]
"In the test set, all S contained triples with a1 and a2.",2.2. Generalisation Regimes,[0],[0]
We first compared the performance of several standard deep neural networks on the neutral split of the PGM dataset.,3. Models and Experimental Setup,[0],[0]
"We also developed a novel architecture based on Relation Networks (Santoro et al., 2017), that we call the Wild Relation Network (WReN), named in recognition of Mary Wild who contributed to the development of Raven’s progressive matrices along with her husband John Raven.
",3. Models and Experimental Setup,[0],[0]
The input consisted of the eight context panels and eight multiple-choice panels.,3. Models and Experimental Setup,[0],[0]
"Each panel is an 80 × 80 pixel image; so, the panels were presented as a set of 16 feature maps.
",3. Models and Experimental Setup,[0],[0]
Models were trained to produce the label of the correct missing panel as an output answer by optimising a softmax cross entropy loss.,3. Models and Experimental Setup,[0],[0]
"We trained all networks by stochastic gradient descent using the ADAM optimiser (Kingma & Ba, 2014).",3. Models and Experimental Setup,[0],[0]
"For each model, hyper-parameters were chosen using a grid sweep to select the model with smallest loss estimated on a held-out validation set.",3. Models and Experimental Setup,[0],[0]
We used the validation loss for early-stopping and we report performance values on a held-out test set.,3. Models and Experimental Setup,[0],[0]
"For hyper-parameter settings and further details on all models see appendix A.
CNN-MLP:",3. Models and Experimental Setup,[0],[0]
"We implemented a standard four layer convolutional neural network with batch normalization and ReLU non-linearities (LeCun et al., 2015).",3. Models and Experimental Setup,[0],[0]
The set of PGM input panels was treated as a set of separate greyscale input feature maps for the CNN.,3. Models and Experimental Setup,[0],[0]
"The convolved output was passed through a two-layer, fully connected MLP using a ReLU non-linearity between linear layers and dropout of 0.5 on the penultimate layer.",3. Models and Experimental Setup,[0],[0]
"Note that this is the type of model applied to Raven-style sequential reasoning questions by Hoshen & Werman (2017).
",3. Models and Experimental Setup,[0],[0]
ResNet: We used a standard implementation of the ResNet-50 architecture as described in He et al. (2016).,3. Models and Experimental Setup,[0],[0]
"As before, each of the context panels and multiple-choice panels was treated as an input feature map.",3. Models and Experimental Setup,[0],[0]
"We also trained a selection of ResNet variants, including ResNet-101, ResNet152, and several custom-built smaller ResNets.",3. Models and Experimental Setup,[0],[0]
"The best performing model was ResNet-50.
",3. Models and Experimental Setup,[0],[0]
LSTM:,3. Models and Experimental Setup,[0],[0]
"We implemented a standard LSTM module (Hochreiter & Schmidhuber, 1997), based on Zaremba et al. (2014).",3. Models and Experimental Setup,[0],[0]
"Since LSTMs are designed to process inputs sequentially, we first passed each panel (context panels and multiple choice panels) sequentially and independently through a small 4-layer CNN, tagged the CNN’s output with a onehot label indicating the panel’s position (the top left PGM panel is tagged with label 1, the top-middle PGM panel is tagged with label 2 etc.), and passed the resulting sequence of labelled embeddings to the LSTM.",3. Models and Experimental Setup,[0],[0]
The final hidden state of the LSTM was passed through a linear layer to produce logits for the softmax cross entropy loss.,3. Models and Experimental Setup,[0],[0]
"The network was trained using batch normalization after each convolutional layer and drop-out was applied to the LSTM hidden state.
",3. Models and Experimental Setup,[0],[0]
Wild Relation Network (WReN): Our novel WReN model (fig.,3. Models and Experimental Setup,[0],[0]
"3) applied a Relation Network module (Santoro et al., 2017) multiple times to infer the inter-panel relationships.
",3. Models and Experimental Setup,[0],[0]
"The model output a 1-d score sk for a given candidate multiple-choice panel, with label k ∈",3. Models and Experimental Setup,[0],[0]
"[1, 8].",3. Models and Experimental Setup,[0],[0]
"The choice with the highest score was selected as the answer a using a softmax function σ across all scores: a = σ([s1, . . .",3. Models and Experimental Setup,[0],[0]
", s8]).",3. Models and Experimental Setup,[0],[0]
"The score of a given multiple-choice panel was evaluated using a Relation Network (RN):
sk = RN(Xk)
",3. Models and Experimental Setup,[0],[0]
= fφ,3. Models and Experimental Setup,[0],[0]
"( ∑ y,z∈Xk gθ(y, z) ) , (1)
where Xk = {x1, x2, ..., x8} ⋃ {ck}, ck is the vector representation of the multiple choice panel k, and xi the representation of",3. Models and Experimental Setup,[0],[0]
context panel i.,3. Models and Experimental Setup,[0],[0]
"The input vector representations were produced by processing each panel independently through a small CNN and tagging it with a panel label, similar to the LSTM processing described above, followed by a linear projection.",3. Models and Experimental Setup,[0],[0]
The functions fφ,3. Models and Experimental Setup,[0],[0]
"and gθ are MLPs.
",3. Models and Experimental Setup,[0],[0]
"The structure of the WReN model is well matched to the problem of abstract reasoning, because it forms representations of pair-wise relations (using gθ), in this case, between each context panel and a given multiple choice candidate, and between context panels themselves.",3. Models and Experimental Setup,[0],[0]
The function fφ integrates information about context-context relations and context-multiple-choice relations to provide a score.,3. Models and Experimental Setup,[0],[0]
"Also the WReN model calculates a score for each multiple-choice candidate independently, allowing the network to exploit weight-sharing across multiple-choice candidates.
",3. Models and Experimental Setup,[0],[0]
"Wild-ResNet: We also implemented a novel variant of the ResNet architecture in which one multiple-choice candidate panel, along with the eight context panels were provided as input, instead of providing all eight multiple-choices and eight context panels as input as in the standard ResNet.",3. Models and Experimental Setup,[0],[0]
"In
this way, the Wild-ResNet is designed to provide a score for each candidate panel, independent of the other candidates.",3. Models and Experimental Setup,[0],[0]
The candidate with the highest score is the output answer.,3. Models and Experimental Setup,[0],[0]
"This is similar to the WReN model described above, but using a ResNet instead of a Relation Network for computing a candidate score.
",3. Models and Experimental Setup,[0],[0]
"Context-blind ResNet: A fully-blind model should be at chance performance level, which for the PGM task is 12.5%.",3. Models and Experimental Setup,[0],[0]
"However, sufficiently strong models can learn to exploit statistical regularities in multiple-choice problems using the choice inputs alone, without considering the context (Johnson et al., 2017).",3. Models and Experimental Setup,[0],[0]
"To understand the extent to which this was possible, we trained a ResNet-50 model with only the eight multiple-choice panels as input.",3. Models and Experimental Setup,[0],[0]
We explored auxiliary training as a means to improve generalisation performance.,3.1. Training on auxiliary information,[0],[0]
"We hypothesized that a model trained to predict the relevant relation, object and attribute types involved in each PGM might develop representations that were more amenable to generalisation.",3.1. Training on auxiliary information,[0],[0]
"To test this, we constructed “meta-targets” encoding the relation, object and attribute types present in PGMs as a binary string.",3.1. Training on auxiliary information,[0],[0]
"The strings were of length 12, with elements following the syntax: (shape, line, color, number, position, size, type, progression, XOR, OR, AND, consistent union).",3.1. Training on auxiliary information,[0],[0]
"We encoded each triple in this binary form, then performed an OR operation across all binary-encoded triple to produce the metatarget.",3.1. Training on auxiliary information,[0],[0]
"That is, OR([101000010000], [100100010000]) =",3.1. Training on auxiliary information,[0],[0]
[101100010000].,3.1. Training on auxiliary information,[0],[0]
"The models then predicted these labels
using a sigmoid unit for each element, trained with cross entropy.",3.1. Training on auxiliary information,[0],[0]
A scaling factor β determined the influence of this loss relative to the loss computed for the answer panel targets: Ltotal = Ltarget + βLmeta-target.,3.1. Training on auxiliary information,[0],[0]
We set β to a non-zero value when we wish to explore the impact of auxiliary meta-target training.,3.1. Training on auxiliary information,[0],[0]
"We first compared all models on the Neutral train/test split, which corresponds most closely to traditional supervised learning regimes.",4.1. Comparing models on PGM questions,[0],[0]
"Perhaps surprisingly given their effectiveness as powerful image processors, CNN models failed almost completely at PGM reasoning problems (Table 1), achieving performance marginally better than our baseline - the context-blind ResNet model which is blind to the context and trained on only the eight candidate answers.",4.1. Comparing models on PGM questions,[0],[0]
The ability of the LSTM to consider individual candidate panels in sequence yielded a small improvement relative to the CNN.,4.1. Comparing models on PGM questions,[0],[0]
"The best performing ResNet variant was ResNet-50, which outperformed the LSTM.",4.1. Comparing models on PGM questions,[0],[0]
"ResNet-50 has significantly more convolutional layers than our simple CNN model, and hence has a greater capacity for reasoning about its input features.
",4.1. Comparing models on PGM questions,[0],[0]
The best performing model was the WReN model.,4.1. Comparing models on PGM questions,[0],[0]
"This strong performance may be partly due to the Relation Network module, which was was designed explicitly for reasoning about the relations between objects, and partly due to the scoring structure.",4.1. Comparing models on PGM questions,[0],[0]
"Note that the scoring structure is not sufficient to explain the improved performance as
the WReN model substantially outperformed the best WildResNet model, which also had a scoring structure.",4.1. Comparing models on PGM questions,[0],[0]
"Questions involving a single [r, o, a] triple were easier than those involving multiple triples.",4.2. Performance on different question types,[0],[0]
"Interestingly, PGMs with three triples proved more difficult than those with four.",4.2. Performance on different question types,[0],[0]
"Although the problem is apparently more complex with four triples, there is also more available evidence for any solution.",4.2. Performance on different question types,[0],[0]
"Among PGMs involving a single triple, OR (64.7%) proved to be an easier relation than XOR (53.2%).",4.2. Performance on different question types,[0],[0]
"PGMs with structures involving lines (78.3%) were easier than those involving shapes (46.2%) and those involving shape-number were much easier (80.1%) than those involving shape-size (26.4%).This suggests that the model struggled to discern fine-grained differences in size compared to more salient changes such as the absence or presence of lines, or the quantity of shapes.",4.2. Performance on different question types,[0],[0]
"For more details of performance by question type, see Appendix Tables 7, 8.",4.2. Performance on different question types,[0],[0]
The results reported thus far were on questions that included distractor attribute values (see Fig. 4).,4.3. Effect of distractors,[0],[0]
"The WReN model performed notably better when these distractors were removed (79.3% on the validation and 78.3% on the test set, compared with 63.0% and 62.6% with distractors).",4.3. Effect of distractors,[0],[0]
"We compared the best performing WReN model on each of the generalisation regimes (Table 1), and observed notable differences in the ability of the model to generalise.",4.4. Generalisation,[0],[0]
"Interpo-
lation was the least problematic regime (generalisation error 14.6%).",4.4. Generalisation,[0],[0]
"Note that performance on both the Interpolation and Extrapolation training sets was higher than on the neutral training set because certain attributes (size, colour) have half as many values in those cases, which reduces the complexity of the task.3
After Interpolation, the model generalised best in regimes where the test questions involved novel combinations of otherwise familiar [r, o, a] triples (Held-out Attribute Pairs and Held-out Triple Pairs).",4.4. Generalisation,[0],[0]
"This indicates that the model learned to combine relations and attributes, and did not simply memorize combinations of triples as distinct structures in their own right.",4.4. Generalisation,[0],[0]
"However, worse generalisation in the case of Held-out Triples suggests that the model was less able to induce the meaning of unfamiliar triples from its knowledge of their constituent components.",4.4. Generalisation,[0],[0]
"Moreover, it could not understand relations instantiated on entirely novel attributes (Heldout line-type , Held-out shape-colour).",4.4. Generalisation,[0],[0]
The worst generalisation was observed on the Extrapolation regime.,4.4. Generalisation,[0],[0]
"Given that these questions have the same abstract semantic structure as interpolation questions, the failure to generalise may stem from the model’s failure to perceive inputs outside of the range of its prior experience.",4.4. Generalisation,[0],[0]
We then explored the impact of auxiliary training on abstract reasoning and generalisation by training our models with symbolic meta targets as described in Section 3.1.,4.5. Effect of auxiliary training,[0],[0]
"In the neutral regime, we found that auxiliary training led to a 13.9% improvement in test accuracy.",4.5. Effect of auxiliary training,[0],[0]
"Critically, this improvement in the overall ability of the model to capture the data also applied to other generalisation regimes.",4.5. Effect of auxiliary training,[0],[0]
"The difference was clearest in the cases where the model was required to recombine familiar triples into novel combinations: (56.3% accuracy on Held-out triple pairs, up from 41.9%, and 51.7% accuracy on Held-out attribute pairs, up from 27.2%).",4.5. Effect of auxiliary training,[0],[0]
"Thus, the pressure to represent abstract semantic principles such that they can be decoded simply into discrete symbolic explanations seems to improve the ability of the model to productively compose its knowledge.",4.5. Effect of auxiliary training,[0],[0]
"This finding aligns with previous observations about the benefits of discrete channels for knowledge representation (Andreas et al., 2016) and the benefit of inducing explanations or rationales (Ling et al., 2017).",4.5. Effect of auxiliary training,[0],[0]
"In addition to improving performance, training with metatargets provides a means to measure which shapes, attributes,
3Since test questions focus on held-out phenomena, test sets in different regimes may have differing underlying complexity.",4.6. Analysis of auxiliary training,[0],[0]
"Absolute performance cannot therefore be compared across different regimes.
and relations the model believes are present in a given PGM, providing insight into the model’s decisions.",4.6. Analysis of auxiliary training,[0],[0]
"Using these predictions, we asked how the WReN model’s accuracy varied as a function of its meta-target predictions.",4.6. Analysis of auxiliary training,[0],[0]
"Unsurprisingly, the WReN model achieved a test accuracy of 87.4% when its meta-target predictions were correct, compared to only 34.8% when its predictions were incorrect.
",4.6. Analysis of auxiliary training,[0],[0]
"The meta-target prediction can be broken down into predictions of object, attribute, and relation types.",4.6. Analysis of auxiliary training,[0],[0]
We leveraged these fine-grained predictions to ask how the WReN model’s accuracy varied as a function of its predictions on each of these properties independently.,4.6. Analysis of auxiliary training,[0],[0]
"The model accuracy increased somewhat when the shape meta-target prediction was correct (78.2%) compared to being incorrect (62.2%), and when attribute meta-target prediction was correct (79.5%) compared to being incorrect (49.0%).",4.6. Analysis of auxiliary training,[0],[0]
"However, for the relation property, the difference between a correct and incorrect meta-target prediction was substantial (86.8% vs. 32.1%).",4.6. Analysis of auxiliary training,[0],[0]
"This result suggests that predicting the relation property correctly is most critical to task success.
",4.6. Analysis of auxiliary training,[0],[0]
"The model’s prediction certainty, defined as the mean absolute difference of the meta-target predictions from 0.5, was predictive of the model’s performance, suggesting that the meta-target prediction certainty is an accurate measure of the model’s confidence in an answer choice (Figure 5; qualitatively similar for sub-targets; Appendix Figures 6-8).",4.6. Analysis of auxiliary training,[0],[0]
"Various computational models for solving RPMs have been proposed in the cognitive science literature (see (Lovett & Forbus, 2017) for a thorough review).",5. Related work,[0],[0]
The emphasis in these studies is on understanding the operations and comparisons commonly applied by humans.,5. Related work,[0],[0]
"They typically factor out raw perception in favour of symbolic inputs, and hard-code strategies described by cognitive theories.",5. Related work,[0],[0]
"In contrast, we
consider models that process input from raw pixels and study how they infer, from knowledge of the correct answer, the processes and representations necessary to resolve the task.",5. Related work,[0],[0]
"Much as we do, Hoshen & Werman (2017) trained neural networks to complete the rows or columns of Ravenstyle matrices from raw pixels.",5. Related work,[0],[0]
"They found that a CNNbased model induced visual relations such as rotation or reflection, but they did not address the problem of resolving complete RPMs.",5. Related work,[0],[0]
Our experiments showed that such models perform poorly on full RPM questions.,5. Related work,[0],[0]
"Moreover, Hoshen & Werman (2017) do not study generalisation to questions that differ substantively from their training data.",5. Related work,[0],[0]
"Wang & Su (2015) present a method for automatically generating Ravenstyle matrices and verify their generator on humans, but do not attempt any modelling.",5. Related work,[0],[0]
"Our method for automatically generating RPM-style questions borrowed extensively from the insights in that work.
",5. Related work,[0],[0]
"There is prior work emphasising both the advantages (Clark & Etzioni, 2016) and limitations (Davis, 2014) of apply-
ing standardized tests in AI (see Marcus et al. (2016) and contributed articles for a review).",5. Related work,[0],[0]
"Approaches based on standardized testing generally focus on measuring the general knowledge of systems, while we focus on models’ abilities to generalize learned information.",5. Related work,[0],[0]
One of the long-standing goals of artificial intelligence is to develop machines with abstract reasoning capabilities that equal or better those of humans.,6. Discussion,[0],[0]
"Though there has also been substantial progress in both reasoning and abstract representation learning in neural nets (Botvinick et al., 2017; LeCun et al., 2015; Higgins et al., 2016; 2017), the extent to which these models exhibit anything like general abstract reasoning is the subject of much debate (Garnelo et al., 2016; Lake & Baroni, 2017; Marcus, 2018).",6. Discussion,[0],[0]
The research presented here was therefore motivated by two main goals.,6. Discussion,[0],[0]
"(1) To understand whether, and (2) to understand how, deep neural networks might be able to solve abstract visual reasoning problems.
",6. Discussion,[0],[0]
"Our answer to (1) is that, with important caveats, neural networks can indeed learn to infer and apply abstract reasoning principles.",6. Discussion,[0],[0]
"Our best performing model learned to solve complex visual reasoning questions, and to do so, it needed to induce and detect from raw pixel input the presence of abstract notions such as logical operations and arithmetic progressions, and apply these principles to never-before observed stimuli.",6. Discussion,[0],[0]
"Importantly, we found that the architecture of the model made a critical difference to its ability to learn and execute such processes.",6. Discussion,[0],[0]
"While standard visualprocessing models such as CNNs and ResNets performed poorly, a model that promoted the representation of, and comparison between parts of the stimuli performed very well.",6. Discussion,[0],[0]
"We found ways to improve this performance via additional supervision: the training outcomes and the model’s ability to generalise were improved if it was required to decode its representations into symbols corresponding to the reason behind the correct answer.
",6. Discussion,[0],[0]
"When considering (2), it is important to note that our models were solving a very different problem from that solved by human subjects taking Raven-style IQ tests.",6. Discussion,[0],[0]
"The model’s world was highly constrained, and its experience consisted of a small number of possible relations instantiated in finite sets of attributes and values across hundreds of thousands of examples.",6. Discussion,[0],[0]
It is highly unlikely that the model’s solutions match those applied by successful humans.,6. Discussion,[0],[0]
This difference becomes clear when we study the ability of the model to generalise.,6. Discussion,[0],[0]
"Unlike humans, who must transfer knowledge distilled from their experience in everyday life to the unfamiliar setting of visual reasoning problems, our models exhibited transfer across question sets with a high degree of perceptual and structural uniformity.",6. Discussion,[0],[0]
"When required to
interpolate between known attribute values, and also when applying known abstract content in unfamiliar combinations, the models generalised notably well.",6. Discussion,[0],[0]
"Even within this constrained domain, however, they performed strikingly poorly when required to extrapolate to inputs beyond their experience, or to deal with entirely unfamiliar attributes.
",6. Discussion,[0],[0]
"In this latter behaviour, the model differs in a crucial way from humans; a human that could apply a relation such as XOR to the colour of lines would almost certainly have no trouble applying it to the colour of shapes.",6. Discussion,[0],[0]
"On the other hand, even the human ability to extend apparently welldefined principles to novel objects has limits; this is precisely why RPMs are such an effective discriminator of human IQ.",6. Discussion,[0],[0]
"For instance, a human subject might be uncertain what it means to apply XOR to the size or shape of sets of objects, even if he or she had learned to do so perfectly in the case of colors.
",6. Discussion,[0],[0]
"An important contribution of this work is the introduction of the PGM dataset, as a tool for studying both abstract reasoning and generalisation in models.",6. Discussion,[0],[0]
"Generalisation is a multi-faceted phenomenon; there is no single, objective way in which models can or should generalise beyond their experience.",6. Discussion,[0],[0]
"The PGM dataset provides a means to measure the generalization ability of models in different ways, each of which may be more or less interesting to researchers depending on their intended training setup and applications.
",6. Discussion,[0],[0]
Designing and instantiating meaningful train/test distinctions to study generalisation in the PGM dataset was simplified by the objective semantics of the underlying generative model.,6. Discussion,[0],[0]
"Similar principles could be applied to more naturalistic data, particularly with crowdsourced human input.",6. Discussion,[0],[0]
"For instance, image processing models could be trained to identify black horses and tested on whether they can detect white horses, or trained to detect flying seagulls, flying sparrows and nesting seagulls, and tested on the detection of nesting sparrows.",6. Discussion,[0],[0]
"This approach was taken for one particular generalisation regime by Ramakrishnan et al. (2017), who tested VQA models on images containing objects that were not observed in the training data.",6. Discussion,[0],[0]
"The PGM dataset extends and formalises this approach, with regimes that focus not only on how models could respond to novel factors or classes in the data, but also novel combinations of known factors etc.
",6. Discussion,[0],[0]
"In the next stage of this research, we will explore strategies for improving generalisation, such as meta-learning, and will further explore the use of richly structured, yet generally applicable, inductive biases.",6. Discussion,[0],[0]
We also hope to develop a deeper understanding of the solutions learned by the WReN model when solving Raven-style matrices.,6. Discussion,[0],[0]
"Finally, we wish to end by inviting our colleagues across the machine learning community to participate in our new abstract reasoning challenge.",6. Discussion,[0],[0]
"We would like to thank David Raposo, Daniel Zoran, Murray Shanahan, Sergio Gomez, Yee Whye Teh and Daan Wierstra for helpful discussions and all the DeepMind team for their support.",ACKNOWLEDGMENTS,[0],[0]
A.1.,A. Appendix,[0],[0]
"PGM Dataset
",A. Appendix,[0],[0]
"Altogether there are 1.2M training set questions, 20K validation set questions, and 200K testing set questions.
",A. Appendix,[0],[0]
When creating the matrices we aimed to use the full Cartesian productR×A for construction structures S .,A. Appendix,[0],[0]
"However, some relation-attribute combinations are problematic, such as a progression on line type, and some attributes interact in interesting ways (such as number and position, which are in some sense tied), restricting the type of relations we can apply to these attributes.",A. Appendix,[0],[0]
"The final list of relevant relations per attribute type, broken down by object type (shape vs. line) is:
shape: size: progression, XOR, OR, AND, consistent union color: progression, XOR, OR, AND, consistent union number: progression, consistent union position: XOR, OR, AND type: progression, XOR, OR, AND, consistent union line: color: progression, XOR, OR, AND, consistent union type: XOR, OR, AND, consistent union
Since the number and position attribute types are tied (for example, having an arithmetic progression on number whilst having an XOR relation on position is not possible), we forbid number and position from co-occurring in the same matrix.",A. Appendix,[0],[0]
"Otherwise, all other ((r, o, a), (r, o, a)) combinations occurred unless specifically controlled for in the generalisation regime.
",A. Appendix,[0],[0]
"We created a similar list for possible values for a given attribute:
shape: color: 10 evenly spaced greyscale intensities in [0, 1] size: 10 scaling factors evenly spaced in [0, 1] 4 number: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 position ((x, y) coordinates in a (0, 1) plot):
(0.25, 0.75), (0.75, 0.75), (0.75, 0.25), (0.25, 0.25), (0.5, 0.5), (0.5, 0.25), (0.5, 0.75), (0.25, 0.5), (0.75, 0.5)
type: circle, triangle, square, pentagon, hexagon, 4The actual specific values used for size are numbers particular to the matplotlib implementation of the plots, and hence depend on the scale of the plot and axes, etc.
octagon, star
line: color: 10 evenly spaced greyscale intensity in [0, 1] type: diagonal down, diagonal up, vertical, horizontal, diamond, circle
A.2.",A. Appendix,[0],[0]
"Examples of Raven-style PGMs
Given the radically different way in which visual reasoning tests are applied to humans (no prior experience) and to our models (controlled training and test splits), we believe it would be misleading to provide a human baseline for our results.",A. Appendix,[0],[0]
"However, for a sense of the difficulty of the task, we present here a set of 18 questions generated from the neutral splits.",A. Appendix,[0],[0]
Note that the values are filtered for human readability.,A. Appendix,[0],[0]
In the dataset there are 10 greyscale intensity values for shape and line colour and 10 sizes for each shape.,A. Appendix,[0],[0]
"In the following, we restrict to 4 clearly-distinct values for each of these attributes.",A. Appendix,[0],[0]
"Best viewed on a digital monitor, zoomed in (see next page).",A. Appendix,[0],[0]
"Informal human testing revealed wide variability: participants with a lot of experience with the tests could score well (> 80%), while others who came to the test blind would often fail to answer all the questions.
",A. Appendix,[0],[0]
"A B C D
E F G H
A B C D
E F G H
A B C D
E F G H
(1) (2) (3)
A B C D
E F G H
A B C D
E F G H
A B C D
E F G H
(4) (5) (6)
A B C D
E F G H
A B C D
E F G H
A B C D
E F G H
(7) (8) (9)
",A. Appendix,[0],[0]
"A B C D
E F G H
A B C D
E F G H
A B C D
E F G H
(10) (11) (12)
A B C D
E F G H
A B C D
E F G H
A B C D
E F G H
(13) (14) (15)
A B C D
E F G H
A B C D
E F G H
(16) (17) (18)
A B C D
E F G H",A. Appendix,[0],[0]
"Here we provide additional details for all our models, including the exact hyper-parameter settings that we considered.",B. Model details,[0],[0]
"Throughout this section, we will use the notation",B. Model details,[0],[0]
"[x, y, z, w] to describe CNN and MLP size.",B. Model details,[0],[0]
"For a CNN, this notation refers to the number of kernels per layer: x kernels in the first layer, y kernels in the second layer, z kernels in the third layer and w kernels in the fourth layer.",B. Model details,[0],[0]
"For the MLP, it refers to the number of units per layer: x units in the first layer, y units in the second layer, z units in the third layer and w units in the fourth layer.
",B. Model details,[0],[0]
"All models were trained using the Adam optimiser, with expoential decay rate parameters β1 = 0.9, β2 = 0.999, = 10−8.",B. Model details,[0],[0]
"We also used a distributed training setup, using 4 GPU-workers per model.",B. Model details,[0],[0]
Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate.,abstractText,[0],[0]
"Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test.",abstractText,[0],[0]
"To succeed at this challenge, models must cope with various generalisation ‘regimes’ in which the training and test data differ in clearlydefined ways.",abstractText,[0],[0]
"We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better.",abstractText,[0],[0]
"When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others.",abstractText,[0],[0]
We further show that the model’s ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers.,abstractText,[0],[0]
"Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks.",abstractText,[0],[0]
Our freely-available dataset should motivate further progress in this direction.,abstractText,[0],[0]
Measuring abstract reasoning in neural networks,title,[0],[0]
"When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations E
P
[h(Z)]",1. Introduction,[0],[0]
"= R p(x)h(x)dx under a target dis-
tribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample aver-
1Stanford University, Palo Alto, CA USA 2Microsoft Research New England, Cambridge, MA USA.",1. Introduction,[0],[0]
"Correspondence to: Jackson Gorham <jgorham@stanford.edu>, Lester Mackey <lmackey@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ages E Q
n",1. Introduction,[0],[0]
"[h(X)] = 1 n
P n
i=1
h(x i ).",1. Introduction,[0],[0]
"However, many exact MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.
",1. Introduction,[0],[0]
"Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures – the Stein discrepancies – that measure how well E
Q
n
approximates E P while avoiding explicit integration under P .",1. Introduction,[0],[0]
Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family – the graph Stein discrepancies – were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P .,1. Introduction,[0],[0]
"Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points.
",1. Introduction,[0],[0]
"This closed form represents a significant practical advantage, as no linear program solvers are necessary, and the computation of the discrepancy can be easily parallelized.",1. Introduction,[0],[0]
"However, as we will see in Section 3.2, not all kernel Stein discrepancies are suitable for our setting.",1. Introduction,[0],[0]
"In particular, in dimension d 3, the kernel Stein discrepancies previously recommended in the literature fail to detect when a sample is not converging to the target.",1. Introduction,[0],[0]
"To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions.
",1. Introduction,[0],[0]
"After formally describing our goals for measuring sample quality in Section 2, we outline our strategy, based on Stein’s method, for constructing and analyzing practical quality measures at the start of Section 3.",1. Introduction,[0],[0]
"In Section 3.1, we define our family of closed-form quality measures – the kernel Stein discrepancies (KSDs) – and establish several
appealing practical properties of these measures.",1. Introduction,[0],[0]
"We analyze the convergence properties of KSDs in Sections 3.2 and 3.3, showing that previously proposed KSDs fail to detect non-convergence and proposing practical convergencedetermining alternatives.",1. Introduction,[0],[0]
"Section 4 illustrates the value of convergence-determining kernel Stein discrepancies in a variety of applications, including hyperparameter selection, sampler selection, one-sample hypothesis testing, and sample quality improvement.",1. Introduction,[0],[0]
"Finally, in Section 5, we conclude with a discussion of related and future work.
",1. Introduction,[0],[0]
Notation We will use µ to denote a generic probability measure and ) to denote the weak convergence of a sequence of probability measures.,1. Introduction,[0],[0]
"We will use k·k
r for r 2",1. Introduction,[0],[0]
"[1,1] to represent the `r norm on Rd and occasionally refer to a generic norm k·k with associated dual norm kak⇤ , sup
b2Rd,kbk=1 ha, bi for vectors a 2 Rd.",1. Introduction,[0],[0]
"We let e
j be the j-th standard basis vector.",1. Introduction,[0],[0]
"For any function g : Rd ! Rd0 , we define M
0 (g) , sup x2Rdkg(x)k
2 , M
1 (g) , sup x 6=ykg(x) g(y)k
2 /kx yk 2 , and rg as the gradient with components (rg(x))
",1. Introduction,[0],[0]
"jk , r x
k
g j (x).",1. Introduction,[0],[0]
"We further let g 2 Cm indicate that g is m times continuously differentiable and g 2 Cm
0 indicate that g 2 Cm and rlg is vanishing at infinity for all l 2 {0, . . .",1. Introduction,[0],[0]
",m}.",1. Introduction,[0],[0]
"We define C(m,m) (respectively, C(m,m)
b
and C(m,m) 0 ) to be the set of functions k : Rd ⇥",1. Introduction,[0],[0]
Rd !,1. Introduction,[0],[0]
"R with (x, y) 7!",1. Introduction,[0],[0]
"rl
x rl y
k(x, y) continuous (respectively, continuous and uniformly bounded, continuous and vanishing at infinity) for all l 2 {0, . .",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
",m}.",1. Introduction,[0],[0]
Consider a target distribution P with continuously differentiable (Lebesgue) density p supported on all of Rd.,2. Quality measures for samples,[0],[0]
"We assume that the score function b , r log p can be evaluated1 but that, for most functions of interest, direct integration under P is infeasible.",2. Quality measures for samples,[0],[0]
"We will therefore approximate integration under P using a weighted sample Q
n =P n
i=1
q n",2. Quality measures for samples,[0],[0]
(x i ),2. Quality measures for samples,[0],[0]
"x
i
with sample points x 1 , . . .",2. Quality measures for samples,[0],[0]
",",2. Quality measures for samples,[0],[0]
"x n 2 Rd and q n
a probability mass function.",2. Quality measures for samples,[0],[0]
"We will make no assumptions about the origins of the sample points; they may be the output of a Markov chain or even deterministically generated.
",2. Quality measures for samples,[0],[0]
"Each Q n offers an approximation E Q
n",2. Quality measures for samples,[0],[0]
"[h(X)] =P n
i=1
q n",2. Quality measures for samples,[0],[0]
"(x i )h(x i ) for each intractable expectation E P
[h(Z)], and our aim is to effectively compare the quality of the approximation offered by any two samples targeting P .",2. Quality measures for samples,[0],[0]
"In particular, we wish to produce a quality measure that (i) identifies when a sequence of samples is converging to the target, (ii) determines when a sequence of samples is not converging to the target, and (iii) is efficiently computable.",2. Quality measures for samples,[0],[0]
"Since our interest is in approx-
1No knowledge of the normalizing constant is needed.
",2. Quality measures for samples,[0],[0]
"imating expectations, we will consider discrepancies quantifying the maximum expectation error over a class of test functions",2. Quality measures for samples,[0],[0]
"H:
dH(Qn, P ) , sup h2H |E",2. Quality measures for samples,[0],[0]
P,2. Quality measures for samples,[0],[0]
[h(Z)],2. Quality measures for samples,[0],[0]
E Q n,2. Quality measures for samples,[0],[0]
[h(X)]|.,2. Quality measures for samples,[0],[0]
"(1)
When H is large enough, for any sequence of probability measures (µ
m
) m 1, dH(µm, P ) ! 0",2. Quality measures for samples,[0],[0]
only if µm ) P .,2. Quality measures for samples,[0],[0]
"In this case, we call (1) an integral probability metric (IPM) (Müller, 1997).",2. Quality measures for samples,[0],[0]
"For example, when H = BLk·k2 , {h : Rd !",2. Quality measures for samples,[0],[0]
"R |M
0 (h) +",2. Quality measures for samples,[0],[0]
"M 1 (h)  1}, the IPM d BLk·k2 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution.",2. Quality measures for samples,[0],[0]
"Alternatively, when H = Wk·k2 , {h : Rd !",2. Quality measures for samples,[0],[0]
"R |M1(h)  1} is the set of 1-Lipschitz functions, the IPM dWk·k in (1) is known as the Wasserstein metric.
",2. Quality measures for samples,[0],[0]
"An apparent practical problem with using the IPM dH as a sample quality measure is that E
P",2. Quality measures for samples,[0],[0]
[h(Z)] may not be computable for h 2 H.,2. Quality measures for samples,[0],[0]
"However, if H were chosen such that E P
[h(Z)]",2. Quality measures for samples,[0],[0]
"= 0 for all h 2 H, then no explicit integration under P would be necessary.",2. Quality measures for samples,[0],[0]
"To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein’s method for characterizing distributional convergence.",2. Quality measures for samples,[0],[0]
"Stein’s method (Stein, 1972) provides a three-step recipe for assessing convergence in distribution:
1.",3. Stein’s method with kernels,[0],[0]
Identify a Stein operator T that maps functions g : Rd !,3. Stein’s method with kernels,[0],[0]
"Rd from a domain G to real-valued functions T g such that
E P",3. Stein’s method with kernels,[0],[0]
"[(T g)(Z)] = 0 for all g 2 G. For any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as
S(µ, T ,G) , sup g2G |E µ",3. Stein’s method with kernels,[0],[0]
"[(T g)(X)]| = dT G(µ, P ) (2)
which, crucially, avoids explicit integration under P .
",3. Stein’s method with kernels,[0],[0]
2.,3. Stein’s method with kernels,[0],[0]
Lower bound the Stein discrepancy by an IPM dH known to dominate weak convergence.,3. Stein’s method with kernels,[0],[0]
"This can be done once for a broad class of target distributions to ensure that µ
m ) P whenever S(µ m , T ,G) !",3. Stein’s method with kernels,[0],[0]
0,3. Stein’s method with kernels,[0],[0]
"for a sequence of probability measures (µ
m
)
m 1 (Desideratum (ii)).
3.",3. Stein’s method with kernels,[0],[0]
"Provide an upper bound on the Stein discrepancy ensuring that S(µ
m , T ,G) ! 0",3. Stein’s method with kernels,[0],[0]
"under suitable convergence of µ
m
to P (Desideratum (i)).
",3. Stein’s method with kernels,[0],[0]
"While Stein’s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality.",3. Stein’s method with kernels,[0],[0]
"The subsections to follow develop a specific, practical instantiation of the abstract Stein’s method recipe based on reproducing kernel Hilbert spaces.",3. Stein’s method with kernels,[0],[0]
An empirical analysis of the Stein discrepancies recommended by our theory follows in Section 4.,3. Stein’s method with kernels,[0],[0]
"A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017),
(T g)(x) , 1 p(x) d dx (p(x)g(x))",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"= g(x)b(x) + g0(x).
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Inspired by the generator method of Barbour (1988; 1990) and Götze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"The resulting Langevin Stein operator
(T P g)(x) , 1 p(x) hr, p(x)g(x)i = hg(x), b(x)i+ hr, g(x)i for functions g : Rd !",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Rd was independently developed, without connection to Stein’s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Notably, the Langevin Stein operator depends on P only through its score function b = r log p and hence is computable even when the normalizing constant of p is not.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity.
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Hereafter, we will let k : Rd ⇥Rd !",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) K
k
of functions from Rd !",3.1. Selecting a Stein operator and a Stein set,[0],[0]
R.,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"That is, K k
is a Hilbert space of functions such that, for all x 2 Rd, k(x, ·) 2 K
k and f(x) = hf, k(x, ·)iK
k
whenever f 2 K k .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"We let k·kK k
be the norm induced from the inner product on K
k
.
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"With this definition, we define our kernel Stein set G k,k·k as the set of vector-valued functions g =",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(g 1 , . . .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
", g d
) such that each component function g
j belongs to K k
and the vector of their norms kg
j kK k
belongs to the k·k⇤ unit ball:2
G k,k·k , {g = (g1, . . .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
", gd) | kvk⇤  1 for vj , kgjkK
k }.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"The following result, proved in Section B, establishes that this is an acceptable domain for T
P
.
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
Proposition 1 (Zero mean test functions).,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"If k 2 C(1,1) b and E P",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"[kr log p(Z)k 2 ] < 1, then E P",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"[(T P
g)(Z)]",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"= 0 for all g 2 G
k,k·k.
2Our analyses and algorithms support each gj belonging to a different RKHS Kk
j
, but we will not need that flexibility here.
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(µ, T
P ,G k,k·k).",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"When k·k = k·k
2 , this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016).",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Our next result shows that, for any k·k, the KSD admits a closed-form solution.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
Proposition 2 (KSD closed form).,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Suppose k 2 C(1,1), and, for each j 2 {1, . . .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"d}, define the Stein kernel
kj 0",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(x, y) , 1 p(x)p(y)",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"r x j r y j (p(x)k(x, y)p(y))",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(3)
= b j (x)b j (y)k(x, y) + b j (x)r y
j
k(x, y)
+ b j",3.1. Selecting a Stein operator and a Stein set,[0],[0]
(,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"y)r x
j
k(x, y) +r x
j
r y
j
k(x, y).
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"If P d
j=1
E µ h kj 0",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(X,X) 1/2",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"i < 1, then S(µ, T P ,G k,k·k) =
kwk",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"where w j , q E µ⇥µ[k j
0 (X, ˜X)] with X, ˜X iid⇠ µ. The proof is found in Section C. Notably, when µ is the discrete measure Q
n
=
P n
i=1
q n",3.1. Selecting a Stein operator and a Stein set,[0],[0]
(x i ),3.1. Selecting a Stein operator and a Stein set,[0],[0]
"x
i , the KSD reduces to evaluating each kj
0
at pairs of support points as w j
=qP n
i,i 0 =1
q n",3.1. Selecting a Stein operator and a Stein set,[0],[0]
(x i ),3.1. Selecting a Stein operator and a Stein set,[0],[0]
kj 0,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(x i , x i 0 )q n",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"(x i 0 ), a computation which
is easily parallelized over sample pairs and coordinates j.
Our Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k
0 =P d
j=1
kj 0 to develop nonparametric control variates.",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Each term w
j in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between µ and P measured with respect to the Stein kernel kj
0 .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"In standard uses of MMD, an arbitrary kernel function is selected, and one must be able to compute expectations of the kernel function under P .",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Here, this requirement is satisfied automatically, since our induced kernels are chosen to have mean zero under P .
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"For clarity we will focus on the specific kernel Stein set choice G
k , G k,k·k2 for the remainder of the paper, but our
results extend directly to KSDs based on any k·k, since all KSDs are equivalent in a strong sense: Proposition 3 (Kernel Stein set equivalence).",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"Under the assumptions of Proposition 2, there are constants c
d , c0 d > 0",3.1. Selecting a Stein operator and a Stein set,[0],[0]
"depending only on d and k·k such that c
d S(µ, T P ,G k,k·k)  S(µ, TP ,Gk,k·k2) 
c0 d S(µ, T P ,G k,k·k).
",3.1. Selecting a Stein operator and a Stein set,[0],[0]
The short proof is found in Section D.,3.1. Selecting a Stein operator and a Stein set,[0],[0]
"We next aim to establish conditions under which the KSD S(µ
m , T P ,G k ) ! 0",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
only if µ m ) P (Desideratum (ii)),3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:
Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"A distribution P is distantly dissipative if  0 , lim inf r!1 (r) > 0 for
(r) = inf{ 2 hb(x)",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"b(y),x yikx",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
yk22 : kx yk2 = r}.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"(4)
Examples of distributions in P include finite Gaussian mixtures with common covariance and all distributions strongly log-concave outside of a compact set, including Bayesian linear, logistic, and Huber regression posteriors with Gaussian priors (see Gorham et al., 2016, Section 4).",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Moreover, when d = 1, membership in P is sufficient to provide a lower bound on the KSD for most common kernels including the Gaussian, Matérn, and inverse multiquadric kernels.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
Theorem 5 (Univariate KSD detects non-convergence).,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Suppose that P 2 P and k(x, y) =",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
(x y) for 2 C2 with a non-vanishing generalized Fourier transform.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"If d = 1, then S(µ
m , T P ,G k ) ! 0",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
only if µ m ) P .,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
The proof in Section E provides a lower bound on the KSD in terms of an IPM known to dominate weak convergence.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"However, our next theorem shows that in higher dimensions S(Q
n , T P ,G k
) can converge to 0 without the sequence (Q
n
) n",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
1 converging to any probability measure.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
This deficiency occurs even when the target is Gaussian.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
Theorem 6 (KSD fails with light kernel tails).,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Suppose k 2 C(1,1)
b
and define the kernel decay rate
(r) , sup{max(|k(x, y)|, kr x k(x, y)k 2 ,
|hr x ,r y k(x, y)i|) :",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
kx yk 2 r}.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"If d 3, P = N (0, I
d ), and (r) = o(r ↵) for ↵ , ( 1 2
1
d
) 1, then S(Q n , T P ,G k ) !",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
0 does not imply Q n ),3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
P .,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Theorem 6 implies that KSDs based on the commonly used Gaussian kernel, Matérn kernel, and compactly supported kernels of Wendland (2004, Theorem 9.13) all fail to detect non-convergence when d 3.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"In addition, KSDs based on the inverse multiquadric kernel (k(x, y) = (c2 + kx yk2
2
) )",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
for < 1 fail to detect non-convergence for any d > 2 /( + 1).,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"The proof in Section F shows that the violating sample sequences (Q
n
) n 1 are simple to construct, and we provide an empirical demonstration of this failure to detect non-convergence in Section 4.
",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
The failure of the KSDs in Theorem 6 can be traced to their inability to enforce uniform tightness.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"A sequence of probability measures (µ
m
) m 1 is uniformly tight if for every ✏ > 0, there is a finite number R(✏) such that lim sup
m
µ m (kXk 2 > R(✏))  ✏.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
Uniform tightness implies that no mass in the sequence of probability measures escapes to infinity.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"When the kernel k decays more rapidly than the score function grows, the KSD ignores excess mass in the tails and hence can be driven to zero by a
non-tight sequence of increasingly diffuse probability measures.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
The following theorem demonstrates uniform tightness is the missing piece to ensure weak convergence.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
Theorem 7 (KSD detects tight non-convergence).,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Suppose that P 2 P and k(x, y) =",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
(x y) for 2 C2 with a nonvanishing generalized Fourier transform.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"If (µ
m
) m 1 is uniformly tight, then S(µ
m , T P ,G k ) !",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
0,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
only if µ m ) P .,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Our proof in Section G explicitly lower bounds the KSD S(µ, T
P ,G k
) in terms of the bounded Lipschitz metric d BLk·k(µ, P ), which exactly metrizes weak convergence.
",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Ideally, when a sequence of probability measures is not uniformly tight, the KSD would reflect this divergence in its reported value.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"To achieve this, we consider the inverse multiquadric (IMQ) kernel k(x, y) =",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"(c2 + kx yk2
2
)
for some < 0 and c > 0.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"While KSDs based on IMQ kernels fail to determine convergence when < 1 (by Theorem 6), our next theorem shows that they automatically enforce tightness and detect non-convergence whenever 2 ( 1, 0).",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
Theorem 8 (IMQ KSD detects non-convergence).,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"Suppose P 2 P and k(x, y) =",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"(c2 + kx yk2
2
) for c > 0 and 2 ( 1, 0).",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"If S(µ
m , T P ,G k ) !",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"0, then µ m ) P .",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"The proof in Section H provides a lower bound on the KSD in terms of the bounded Lipschitz metric d
BLk·k(µ, P ).",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
The success of the IMQ kernel over other common characteristic kernels can be attributed to its slow decay rate.,3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"When P 2 P and the IMQ exponent > 1, the function class T
P G k
contains unbounded (coercive) functions.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"These functions ensure that the IMQ KSD S(µ
m , T P ,G k )
goes to 0 only if (µ m ) m 1 is uniformly tight.",3.2. Lower bounding the kernel Stein discrepancy,[0],[0]
"The usual goal in upper bounding the Stein discrepancy is to provide a rate of convergence to P for particular approximating sequences (µ
m
) 1 m=1
.",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"Because we aim to directly compute the KSD for arbitrary samples Q
n , our chief purpose in this section is to ensure that the KSD S(µ
m , T P ,G k ) will converge to zero when µ m
is converging to P (Desideratum (i)).",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
Proposition 9 (KSD detects convergence).,3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"If k 2 C(2,2)
b
and r log p is Lipschitz with E P",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"[kr log p(Z)k2 2 ] < 1, then S(µ
m , T P ,G k ) ! 0",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"whenever the Wasserstein distance dWk·k2 (µm, P ) !",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
0.,3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"Proposition 9 applies to common kernels like the Gaussian, Matérn, and IMQ kernels, and its proof in Section I provides an explicit upper bound on the KSD in terms of the Wasserstein distance dWk·k2 .",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
When Qn = 1 n P n i=1,3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
x,3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"i for
x",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"i iid⇠ µ, (Liu et al., 2016, Thm. 4.1) further implies that S(Q
n , T P ,G k ) ) S(µ, T P ,G k
) at an O(n 1/2) rate under continuity and integrability assumptions on µ.",3.3. Upper bounding the kernel Stein discrepancy,[0],[0]
"We next conduct an empirical evaluation of the KSD quality measures recommended by our theory, recording all timings on an Intel Xeon CPU E5-2650 v2 @ 2.60GHz.",4. Experiments,[0],[0]
"Throughout, we will refer to the KSD with IMQ base kernel k(x, y) =",4. Experiments,[0],[0]
"(c2 + kx yk2
2
) , exponent = 1 2 , and c = 1 as the IMQ KSD.",4. Experiments,[0],[0]
"Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.github.io/ SteinDiscrepancy.jl/.",4. Experiments,[0],[0]
"Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dWk·k2 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015).",4.1. Comparing discrepancies,[0],[0]
We adopt a bimodal Gaussian mixture with p(x) /,4.1. Comparing discrepancies,[0],[0]
e 1 2kx+ e1k22 + e 1 2kx e1k22 and = 1.5 as our target P and generate a first sample point sequence i.i.d.,4.1. Comparing discrepancies,[0],[0]
from the target and a second sequence i.i.d.,4.1. Comparing discrepancies,[0],[0]
"from one component of the mixture, N ( e
1 , I d ).",4.1. Comparing discrepancies,[0],[0]
"As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n 0.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample.",4.1. Comparing discrepancies,[0],[0]
"This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy.
",4.1. Comparing discrepancies,[0],[0]
The middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P .,4.1. Comparing discrepancies,[0],[0]
"Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.0.4 linear program solver for the graph Stein discrepancy.",4.1. Comparing discrepancies,[0],[0]
We find that the two methods have nearly identical runtimes when d = 1 but that the KSD is 10 to 1000 times faster when d = 4.,4.1. Comparing discrepancies,[0],[0]
"In addition, the KSD is straightforwardly parallelized and does not require access to a linear program solver, making it an appealing practical choice for a quality measure.
",4.1. Comparing discrepancies,[0],[0]
"Finally, the right panel displays the optimal Stein func-
tions, g j (y) = E Q n",4.1. Comparing discrepancies,[0],[0]
"[ b j (X)k(X,y)+r x j k(X,y) ]
S(Q n ,T P ,G k ) , recovered by the IMQ KSD when d = 1 and n = 103.",4.1. Comparing discrepancies,[0],[0]
"The associated
test functions h(y) =",4.1. Comparing discrepancies,[0],[0]
(T P g)(y),4.1. Comparing discrepancies,[0],[0]
= P d j=1 EQn,4.1. Comparing discrepancies,[0],[0]
"[k j 0(X,y)]
S(Q n ,T P ,G k ) are the mean-zero functions under P that best discriminate the target P and the sample Q
n .",4.1. Comparing discrepancies,[0],[0]
"As might be expected, the optimal test function for the single component sample features large magnitude values in the oversampled region far from the missing mode.",4.1. Comparing discrepancies,[0],[0]
Theorem 6 established that kernels with rapidly decaying tails yield KSDs that can be driven to zero by offtarget sample sequences.,4.2. The importance of kernel choice,[0],[0]
Our next experiment provides an empirical demonstration of this issue for a multivariate Gaussian target P = N,4.2. The importance of kernel choice,[0],[0]
"(0, I
d ) and KSDs based on the popular Gaussian (k(x, y) = e kx yk 2 2/2) and Matérn (k(x, y) = (1 + p 3kx",4.2. The importance of kernel choice,[0],[0]
"yk
2
)",4.2. The importance of kernel choice,[0],[0]
"e p 3kx yk2 ) radial kernels.
",4.2. The importance of kernel choice,[0],[0]
"Following the proof Theorem 6 in Section F, we construct an off-target sequence (Q
n
)",4.2. The importance of kernel choice,[0],[0]
"n 1 that sends S(Qn, TP ,Gk) to 0 for these kernel choices whenever d 3.",4.2. The importance of kernel choice,[0],[0]
"Specifically, for each n, we let Q
n
=
1
n
P n
i=1
",4.2. The importance of kernel choice,[0],[0]
"x
i where, for all i and j, kx
i k 2  2n1/d log n and kx i x j k 2
2 log n. To select these sample points, we independently sample candidate points uniformly from the ball {x : kxk
2  2n1/d log n}, accept any points not within 2 log n Euclidean distance of any previously accepted point, and terminate when n points have been accepted.
",4.2. The importance of kernel choice,[0],[0]
"For various dimensions, Figure 2 displays the result of applying each KSD to the off-target sequence (Q
n
)
n 1 and an “on-target” sequence of points sampled i.i.d. from P .",4.2. The importance of kernel choice,[0],[0]
"For comparison, we also display the behavior of the IMQ KSD which provably controls tightness and dominates weak convergence for this target by Theorem 8.",4.2. The importance of kernel choice,[0],[0]
"As predicted, the Gaussian and Matérn KSDs decay to 0 under the off-target sequence and decay more rapidly as the dimension d increases; the IMQ KSD remains bounded away from 0.",4.2. The importance of kernel choice,[0],[0]
"The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) / ⇡(x) Q L
l=1
⇡(y l |x) for ⇡(·) a prior distribution on Rd and ⇡(y
l |x)",4.3. Selecting sampler hyperparameters,[0],[0]
"the likelihood of a datapoint y l
.",4.3. Selecting sampler hyperparameters,[0],[0]
"A standard slice sampler must evaluate the likelihood of all L datapoints to draw each new sample point x
i .",4.3. Selecting sampler hyperparameters,[0],[0]
"To reduce this cost, the approximate slice sampler introduces a tuning parameter ✏ which determines the number of datapoints that contribute to an approximation of the slice sampling step; an appropriate setting of this parameter is imperative for accurate inference.",4.3. Selecting sampler hyperparameters,[0],[0]
"When ✏ is too small, relatively few sample points will be generated in a given amount of sampling time, yielding sample expectations with high Monte Carlo variance.",4.3. Selecting sampler hyperparameters,[0],[0]
"When ✏ is too large, the large approximation error will produce biased samples that no longer resemble the target.
",4.3. Selecting sampler hyperparameters,[0],[0]
"To assess the suitability of the KSD for tolerance parameter selection, we take as our target P the bimodal Gaussian mixture model posterior of (Welling & Teh, 2011).",4.3. Selecting sampler hyperparameters,[0],[0]
"For an array of ✏ values, we generated 50 independent approximate slice sampling chains with batch size 5, each with a
budget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3.",4.3. Selecting sampler hyperparameters,[0],[0]
"ESS, which does not detect Markov chain bias, is maximized at the largest hyperparameter evaluated (✏ = 10 1), while the KSD is minimized at an intermediate value (✏ = 10 2).",4.3. Selecting sampler hyperparameters,[0],[0]
The right panel of Figure 3 shows representative samples produced by several settings of ✏.,4.3. Selecting sampler hyperparameters,[0],[0]
"The sample produced by the ESS-selected chain is significantly overdispersed, while the sample from ✏ = 0 has minimal coverage of the second mode due to
its small sample size.",4.3. Selecting sampler hyperparameters,[0],[0]
The sample produced by the KSDselected chain best resembles the posterior target.,4.3. Selecting sampler hyperparameters,[0],[0]
"Using 4 cores, the longest KSD computation with n = 103 sample points took 0.16s.",4.3. Selecting sampler hyperparameters,[0],[0]
"Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS).",4.4. Selecting samplers,[0],[0]
"In the full version of SGFS (termed SGFS-f), a d⇥ d matrix must be inverted to draw each new sample point.",4.4. Selecting samplers,[0],[0]
"Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point.",4.4. Selecting samplers,[0],[0]
"Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution.",4.4. Selecting samplers,[0],[0]
"Hence we will use the KSD – a quality measure that accounts for asymptotic bias – to evaluate and choose between these samplers.
",4.4. Selecting samplers,[0],[0]
"Specifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1).",4.4. Selecting samplers,[0],[0]
"The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 104 MNIST handwritten digit images.",4.4. Selecting samplers,[0],[0]
"From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9.",4.4. Selecting samplers,[0],[0]
"After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 ⇥ 104 points and d = 51 dimensions (including the intercept term).",4.4. Selecting samplers,[0],[0]
Figure 4 displays the IMQ KSD applied to the first n points in each sample.,4.4. Selecting samplers,[0],[0]
"As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a
Hamiltonian Monte Carlo chain with 105 iterates.",4.4. Selecting samplers,[0],[0]
Both the KSD and the surrogate ground truth suggest that the moderate speed-up provided by SGFS-d (0.0017s per sample vs. 0.0019s for SGFS-f) is outweighed by the significant loss in inferential accuracy.,4.4. Selecting samplers,[0],[0]
"However, the KSD assessment does not require access to an external trustworthy ground truth sample.",4.4. Selecting samplers,[0],[0]
The longest KSD computation took 400s using 16 cores.,4.4. Selecting samplers,[0],[0]
"While our investigation of the KSD was motivated by the desire to develop practical, trustworthy tools for sample quality comparison, the kernels recommended by our theory can serve as drop-in replacements in other inferential tasks that make use of kernel Stein discrepancies.",4.5. Beyond sample quality comparison,[0],[0]
"Chwialkowski et al. (2016) recently used the KSD S(Q
n , T P ,G k
) to develop a hypothesis test of whether a given sample from a Markov chain was drawn from a target distribution P (see also Liu et al., 2016).",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"However, the authors noted that the KSD test with their default Gaussian base kernel k experienced a considerable loss of power as the dimension d increased.",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"We recreate their experiment and show that this loss of power can be avoided by using our default IMQ kernel with = 1
2 and c = 1.",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"Following (Chwialkowski et al., 2016, Section 4) we draw z",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"i iid⇠ N (0, I d )",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"and u i
iid⇠ Unif[0, 1] to generate a sample (x
i
)
n
i=1
with x i = z",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"i + u i e 1 for n = 500 and various dimensions d. Using the authors’ code (modified to include an IMQ kernel), we compare the power of the Gaussian KSD test, the IMQ KSD test, and the standard normality test of Baringhaus & Henze (1988) (B&H) to discern whether the sample (x
i
)
500
i=1 came from the null distribution P = N (0, I
d
).",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"The results, averaged over 400 simula-
tions, are shown in Table 1.",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"Notably, the IMQ KSD experiences no power degradation over this range of dimensions, thus improving on both the Gaussian KSD and the standard B&H normality tests.",4.5.1. ONE-SAMPLE HYPOTHESIS TESTING,[0],[0]
"Liu & Lee (2016) recently used the KSD S(Q n , T P ,G k ) as a means of improving the quality of a sample.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"Specifically, given an initial sample Q
n supported on x 1 , . . .",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
", x n , they minimize S( ˜Q
n , T P ,G k ) over all measures ˜Q n
supported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = T
P G",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"k
.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e 1hkx yk 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"Using the authors’ code, we recreate the experiment from (Liu & Lee, 2016, Fig. 2b) and introduce a KSD objective with an IMQ kernel k(x, y) = (1 + 1
h
kx yk2 2 )
1/2 with bandwidth selected in the same fashion.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"The starting sample is given by Q
n
=
1
n
P n
i=1
",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"x
i for n = 100, various dimensions d, and each sample point drawn i.i.d. from P = N (0, I
d ).",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"For the initial sample and the optimized samples produced by each KSD, Figure 5 displays the mean squared error (MSE) 1
d
kE P",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"[Z] E ˜
Q
n
",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"[X]k2 2
averaged across 500 independently generated initial samples.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
"Out of the box, the IMQ kernel produces better mean estimates than the standard Gaussian.",4.5.2. IMPROVING SAMPLE QUALITY,[0],[0]
The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.,5. Related and future work,[0],[0]
"For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.
",5. Related and future work,[0],[0]
"For an approximation µ with continuously differentiable density r, Chwialkowski et al. (2016, Thm. 2.1) and Liu et al. (2016, Prop. 3.3) established that if k is C
0 - universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E µ",5. Related and future work,[0],[0]
"[k 0 (X,X) +",5. Related and future work,[0],[0]
kr log p(X) r(X) k2 2 ],5. Related and future work,[0],[0]
"< 1 for k 0 , Pd j=1 kj 0
, then S(µ, T
P ,G k
) = 0",5. Related and future work,[0],[0]
only if µ = P .,5. Related and future work,[0],[0]
"However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense.",5. Related and future work,[0],[0]
"Indeed, Gaussian and Matérn kernels are C
0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P .",5. Related and future work,[0],[0]
"On compact domains,
where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Schölkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence.
",5. Related and future work,[0],[0]
"While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators.",5. Related and future work,[0],[0]
"In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016).
",5. Related and future work,[0],[0]
"In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence.",5. Related and future work,[0],[0]
A reader may also wonder for which distributions outside of P the KSD dominates weak convergence.,5. Related and future work,[0],[0]
"The following theorem, proved in Section J, shows that no KSD with a C
0 kernel dominates weak convergence when the target has a bounded score function.",5. Related and future work,[0],[0]
Theorem 10 (KSD fails for bounded scores).,5. Related and future work,[0],[0]
"If r log p is bounded and k 2 C(1,1)
0
, then S(Q n , T P ,G k ) !",5. Related and future work,[0],[0]
"0 does not imply Q
n ) P .",5. Related and future work,[0],[0]
"However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator T
P
with diffusion Stein operators of the form (T g)(x)",5. Related and future work,[0],[0]
"= 1
p(x) hr, p(x)(a(x) +",5. Related and future work,[0],[0]
c(x))g(x)i.,5. Related and future work,[0],[0]
An analogous construction should yield convergence-determining diffusion KSDs for P outside of P .,5. Related and future work,[0],[0]
"Our results also extend to targets P supported on a convex subset X of Rd by choosing k to satisfy p(x)k(x, ·) ⌘ 0 for all x on the boundary of X .",5. Related and future work,[0],[0]
Approximate Markov chain Monte Carlo (MCMC) offers the promise of more rapid sampling at the cost of more biased inference.,abstractText,[0],[0]
"Since standard MCMC diagnostics fail to detect these biases, researchers have developed computable Stein discrepancy measures that provably determine the convergence of a sample to its target distribution.",abstractText,[0],[0]
This approach was recently combined with the theory of reproducing kernels to define a closed-form kernel Stein discrepancy (KSD) computable by summing kernel evaluations across pairs of sample points.,abstractText,[0],[0]
"We develop a theory of weak convergence for KSDs based on Stein’s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails provably determine convergence for a large class of target distributions.",abstractText,[0],[0]
"The resulting convergence-determining KSDs are suitable for comparing biased, exact, and deterministic sample sequences and simpler to compute and parallelize than alternative Stein discrepancies.",abstractText,[0],[0]
"We use our tools to compare biased samplers, select sampler hyperparameters, and improve upon existing KSD approaches to one-sample hypothesis testing and sample quality improvement.",abstractText,[0],[0]
Measuring Sample Quality with Kernels,title,[0],[0]
"Citations have long been used to characterize the state of a scientific field and to identify influential works. However, writers use citations for different purposes, and this varied purpose influences uptake by future scholars. Unfortunately, our understanding of how scholars use and frame citations has been limited to small-scale manual citation analysis of individual papers. We perform the largest behavioral study of citations to date, analyzing how scientific works frame their contributions through different types of citations and how this framing affects the field as a whole. We introduce a new dataset of nearly 2,000 citations annotated for their function, and use it to develop a state-of-the-art classifier and label the papers of an entire field: Natural Language Processing. We then show how differences in framing affect scientific uptake and reveal the evolution of the publication venues and the field as a whole. We demonstrate that authors are sensitive to discourse structure and publication venue when citing, and that how a paper frames its work through citations is predictive of the citation count it will receive. Finally, we use changes in citation framing to show that the field of NLP is undergoing a significant increase in consensus.",text,[0],[0]
"Authors use citations to frame their contributions and connect to an intellectual lineage (Latour, 1987).",1 Introduction,[0],[0]
"An author’s scientific frame employs citations in multiple ways (Figure 1) so as to build a strong
Unlike CITE, we use the method of CITE, which has been used previously for parsing (CITE).
",1 Introduction,[0],[0]
"Contrast Use Background
Figure 1:",1 Introduction,[0],[0]
"Examples of citation functionality.
and multifaceted argument.",1 Introduction,[0],[0]
"These differences in citations have been examined extensively within the context of a single paper (Swales, 1986; White, 2004; Ding et al., 2014).",1 Introduction,[0],[0]
"However, we know relatively little about how these citation frames develop over time within a field and what impact they have on scientific uptake.
",1 Introduction,[0],[0]
Answering these questions has been largely hindered by the lack of a dataset showing how citations function at the field scale.,1 Introduction,[0],[0]
"Here, we perform the first field-scale study of citation framing by first developing a state-of-the-art method for automatically classifying citation function and then applying this method to an entire field’s literature to quantify the effects and evolution of framing.
",1 Introduction,[0],[0]
Analyzing large-scale changes in citation framing requires an accurate method for classifying the function a citation plays towards furthering an argument.,1 Introduction,[0],[0]
"Due to the difficulty of interpreting citation intent, many prior works performed manual analysis (Moravcsik and Murugesan, 1975; Swales, 1990; Harwood, 2009) and only recently have automated approaches been developed (Teufel et al., 2006b; Valenzuela et al., 2015).",1 Introduction,[0],[0]
"Here, we unify core aspects of several prior citation annotation schemes (White, 2004; Ding et al., 2014; Hernández-Alvarez and Gomez, 2016).",1 Introduction,[0],[0]
"Using this scheme, we create
391
Transactions of the Association for Computational Linguistics, vol. 6, pp.",1 Introduction,[0],[0]
"391–406, 2018.",1 Introduction,[0],[0]
Action Editor: Katrin Erk.,1 Introduction,[0],[0]
"Submission batch: 8/2017; Revision batch: 12/2017; Published 6/2018.
",1 Introduction,[0],[0]
c©2018 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
one of the largest annotated corpora of citations and use it to train a high-accuracy method for automatically labeling a corpus.,1 Introduction,[0],[0]
"We apply our method to label the field of NLP, with over 134,127 citations in over 20,000 papers from nearly forty years of work.
",1 Introduction,[0],[0]
Our work provides four key contributions for understanding how authors frame their citations.,1 Introduction,[0],[0]
We introduce a new large-scale representative corpus of citation function and state-of-the-art methodology for classifying citations by function.,1 Introduction,[0],[0]
We demonstrate that citations reflect the discourse structure of a paper but that this structure is significantly influenced by publication venue.,1 Introduction,[0],[0]
"Third, we show that differences in a paper’s citation framing have a significant and meaningful impact on future scientific uptake as measured through future citations.",1 Introduction,[0],[0]
"Finally, by examining changes in the usage of citation functions, we show that the scholarly NLP community has evolved in how its authors frame their work, reflecting the maturation and growth of the field as a rapid discovery science (Collins, 1994).",1 Introduction,[0],[0]
We publicly release our dataset and code to enable future research.,1 Introduction,[0],[0]
"Citations play a key role in supporting authors’ contributions throughout a scientific paper.1 Multiple schemes have been proposed on how to classify these different roles, ranging from a handful of classes (Nanba and Okumura, 1999; Pham and Hoffmann, 2003) to twenty or more (Garfield, 1979; Garzone and Mercer, 2000).",2 A Corpus for Citation Function,[0],[0]
"While suitable for expert manual analysis, many schemes include either fine-grained distinctions that are too rare to reliably identify or subjective classifications that require detailed knowledge of the field or author (Ziman, 1968; Swales, 1990; Harwood, 2009).",2 A Corpus for Citation Function,[0],[0]
"Motivated by the desire to automatically examine large-scale trends in scholarly behavior, we address these issues by unifying the common aspects of multiple approaches in a single classification.",2 A Corpus for Citation Function,[0],[0]
"Our classification captures the broad thematic functions a citation can serve in the discourse, e.g., pro-
1For notational clarity, we use the term reference for the work that is cited and citation for the mention of it in the text.
viding background or serving as contrast (Oppenheim and Renn, 1978; Spiegel-Rüsing, 1977; Teufel et al., 2006a; Garfield, 1979; Garzone and Mercer, 2000; Abu-Jbara et al., 2013).2 Citation function reflects the specific purpose a citation plays with respect to the current paper’s contributions.",2.1 Classification Scheme,[0],[0]
"We unify the functional roles common in several classifications, e.g., (Spiegel-Rüsing, 1977; Garfield, 1979; Peritz, 1983; Teufel et al., 2006a; Harwood, 2009; Dong and Schäfer, 2011), into the six classes shown in Table 1, along with their description and example.
",2.1 Classification Scheme,[0],[0]
Our annotation scheme is similar to the six classes of Abu-Jbara et al. (2013) and the twelve-class scheme of Teufel (2000).,2.1 Classification Scheme,[0],[0]
"The former has separate classes for comparison and for contrast, whereas the latter has multiple finer-grained distinctions for different kinds of comparison and contrasts.",2.1 Classification Scheme,[0],[0]
"Here, we collapse these distinctions into a single class, COMPARISON AND CONTRAST, that signals the author is making some form of alignment between their work and another.",2.1 Classification Scheme,[0],[0]
"In practice, we found that many citation contexts with alignments—such as this one— contain signals of both comparison and contrast; for our intended analyses, we considered this alignment signalling more important than whether the author was comparing or contrasting.",2.1 Classification Scheme,[0],[0]
"Additionally, we introduce the FUTURE class to indicate that authors have forward-looking references for how their work might be applied later; these references are important for establishing a temporal lineage between works, and as we show later in §4, are the most frequent citation type in papers’ Conclusion sections.",2.1 Classification Scheme,[0],[0]
"Our adapted scheme enables us to conduct detailed analyses of the narrative structure of papers, venue citation pattern and evolution, and modeling the evolution of the whole field.",2.1 Classification Scheme,[0],[0]
"Annotation guidelines were created using a pilot study of 10 papers sampled from the ACL Anthology Reference Corpus (ARC) (Bird et al., 2008).
",2.2 Annotation Process and Dataset,[0],[0]
"2Another potential theme is citation sentiment (Athar, 2014; Kumar, 2016), but we omit this theme from our field-scale analysis because researchers have shown that negative sentiment is rare in practice (Chubin and Moitra, 1975; Vinkler, 1998; Case and Higgins, 2000) and can be quite subjective to classify due to textual mixtures of praise and criticism (Peritz, 1983; Swales, 1986; Brooks, 1986; Teufel, 2000).
",2.2 Annotation Process and Dataset,[0],[0]
Annotators completed two rounds of pre-annotation to discuss their process and design guidelines.,2.2 Annotation Process and Dataset,[0],[0]
"All citations were then doubly-annotated by two trained annotators with expertise in NLP using the Brat tool (Stenetorp et al., 2012) and were then fully adjudicated to ensure quality.",2.2 Annotation Process and Dataset,[0],[0]
"Following best practices for annotating citations (Athar, 2014), annotators saw an extended context before and after the citing sentence, provided from the output of ParsCit.",2.2 Annotation Process and Dataset,[0],[0]
Annotators were instructed to skip any instances whose context was corrupted or whose citance text did not match the regular citation style for ACL,2.2 Annotation Process and Dataset,[0],[0]
"venues.3
The citation scheme was applied to a random sample of 52 papers drawn from the ARC.",2.2 Annotation Process and Dataset,[0],[0]
"Each paper was processed using ParsCit (Councill et al., 2008) to extract citations and their references.",2.2 Annotation Process and Dataset,[0],[0]
"As expected from prior studies (Teufel et al., 2006a; Dong and
3A small number of citation instances in our sample occurred in contexts where the surrounding text was malformed, which we attribute to being OCR errors, the citation being in the middle of a math-related context whose symbols were not converted, or where the citation occurred within a table or figure whose structure was treated as the surrounding text.",2.2 Annotation Process and Dataset,[0],[0]
"In all cases, we viewed in the instance as unsuitable for use as a training example since it contained little meaningful context.",2.2 Annotation Process and Dataset,[0],[0]
These cases accounted for less than 10 instances in our data.,2.2 Annotation Process and Dataset,[0],[0]
"A second set of instances were excluded when ParsCit mislabeled the span of a citation, either shortening it or increasing it to multiple citations’ text.",2.2 Annotation Process and Dataset,[0],[0]
These wrong-spans occurred in less than 10 instances in our sample.,2.2 Annotation Process and Dataset,[0],[0]
"A third set of citation instances were excluded due to citation style difference, where a paper in an earlier iteration of a conference used numeric citations, e.g., “[12].”",2.2 Annotation Process and Dataset,[0],[0]
These were excluded to ensure uniformity in the data and occurred in two papers that were excluded from in our initial sample.,2.2 Annotation Process and Dataset,[0],[0]
"As these errors are sufficiently rare in our sample (<4%), we do not perform any further correction for these errors in the larger, un-annotated data.
Schäfer, 2011), some citation functions were infrequent.",2.2 Annotation Process and Dataset,[0],[0]
"We therefore attempted to oversample the infrequent classes FUTURE, EXTENSION, and MOTIVATION, by using keywords biased toward extracting citing sentences of a particular class (such as the word “future” for the FUTURE class).",2.2 Annotation Process and Dataset,[0],[0]
The resulting citing sentences were then annotated and could potentially be assigned to any class.,2.2 Annotation Process and Dataset,[0],[0]
"In total, 1436 citations in context were annotated for the fully-labeled 52 papers (mean 27.6 citations/paper) and 533 supplemental contexts from 133 papers were added by targeted sampling, bringing the total number of instances to 1969.",2.2 Annotation Process and Dataset,[0],[0]
Table 2 shows the class distribution in the final dataset.,2.2 Annotation Process and Dataset,[0],[0]
"Consistent with prior work, the majority of citations are BACKGROUND (Moravcsik and Murugesan, 1975; Spiegel-Rüsing, 1977; Teufel et al., 2006b).",2.2 Annotation Process and Dataset,[0],[0]
The structure of a scientific article provides multiple cues for a citation’s purpose.,3 Automatically Classifying Citations,[0],[0]
"Our work draws on multiple approaches (Hernández-Alvarez and Gomez, 2016) to develop a classifier based on (1) structural features describing where the citation is located, (2) lexical and grammatical features for
how the citation is described, (3) field features that take into account venue or other external information, and (4) usage features on how the reference is cited throughout the paper.",3 Automatically Classifying Citations,[0],[0]
"Table 3 shows our features, which includes ten novel feature types, in addition to several drawn from recent systems (Teufel, 2000; Teufel et al., 2006b; Dong and Schäfer, 2011; Wan and Liu, 2014; Valenzuela et al., 2015; Zhu et al., 2015).
",3 Automatically Classifying Citations,[0],[0]
Function Pattern COMP.,3 Automatically Classifying Citations,[0],[0]
OR CON.,3 Automatically Classifying Citations,[0],[0]
@SIMILAR ADJ to @REFERENTIAL @USE COMP.,3 Automatically Classifying Citations,[0],[0]
OR CON.,3 Automatically Classifying Citations,[0],[0]
the @RESEARCH NOUN of #N EXTENDS @CHANGE,3 Automatically Classifying Citations,[0],[0]
NOUN of #N ’s EXTENDS @CHANGE,3 Automatically Classifying Citations,[0],[0]
NOUN of citation ’s MOTIVATION @INSPIRATION by #N USES @1ST,3 Automatically Classifying Citations,[0],[0]
"PERSON PRONOUN (NOM) @USE the #N USES the #N corpus USES #D #N #N citation
Table 4:",3 Automatically Classifying Citations,[0],[0]
Examples of bootstrapped patterns learned and their associated class where @ denotes a lexical class and # denotes a part of speech wild card.,3 Automatically Classifying Citations,[0],[0]
"Following, we describe in detail the three main categories of novel features.
",3.1 Features,[0],[0]
"Pattern-based Features Patterns provide a powerful mechanism for capturing regularity in citation usage (Dong and Schäfer, 2011).",3.1 Features,[0],[0]
"Our patterns are a sequence of cue phrases, parts of speech, or lexical categories, like positive-sentiment words or specific categories that allow generalizations across phrases like “we extend” and “we build upon.”",3.1 Features,[0],[0]
"We began with the largest publicly-available list of citation patterns (Teufel, 2000) and extended it with 132 new patterns and 13 new lexical categories based on a manual analysis of the corpus.
",3.1 Features,[0],[0]
"We then used bootstrapping to automatically identify new patterns as follows: Each annotated context was converted into fixed-length patterns using (a) our 42 lexical categories, (b) part of speech wild cards, or (c) the tokens directly.",3.1 Features,[0],[0]
"To avoid semantic drift (Riloff and Jones, 1999), a bootstrapped pattern was only included as a feature if the majority of its occurrences were with a single citation function.4 Table 4 shows examples of these bootstrapped patterns.
",3.1 Features,[0],[0]
"Previous patterns primarily use cues from the same sentence as the citation (Teufel, 2000).",3.1 Features,[0],[0]
"However, authors often use multiple sentences to indicate a citation’s purpose (Abu-Jbara and Radev, 2012; Ritchie et al., 2008; He et al., 2011; Kataria et al., 2011).",3.1 Features,[0],[0]
"For example, authors may first introduce a work positively, only to contrast with it in later sen-
4For computational efficiency, patterns were restricted to having between 3 and 8 tokens and at most two part of speech wild cards.",3.1 Features,[0],[0]
"Due to its high frequency, patterns for BACKGROUND were required to occur in at least 100 contexts.
1) algorithm parameter model training method clustering 2) measure score metric information similarity distance 3) % result accuracy report achieve performance system 4) training weight feature och model set algorithm error 5) work related previous paper problem approach present
Table 5:",3.1 Features,[0],[0]
"The most probable words from five example topics learned from citation contexts.
",3.1 Features,[0],[0]
"tences (Peritz, 1983; Brooks, 1986; Mercer et al., 2004).",3.1 Features,[0],[0]
"Indeed the average text pertaining to a citation spans 1.6 sentences in the ARC (Small, 2011).
",3.1 Features,[0],[0]
We therefore induce bootstrapped patterns specific to the citation sentence as well as the preceding and following sentences.,3.1 Features,[0],[0]
"Ultimately, 805 new bootstrapped patterns were added for the citing sentence, 669 for the preceding context, and 1159 for the following context, a total of over four times the number of manually curated patterns.
",3.1 Features,[0],[0]
Topic-based Features A context’s thematic framing can point to the purpose of a citation even in the absence of explicit cues.,3.1 Features,[0],[0]
"For example, a citation in a context describing system performances and results is likely to be a COMPARE OR CONTRAST, whereas one describing methodology is more likely to be USES.",3.1 Features,[0],[0]
"We quantify this thematic framing by using features based on topic models, computed over the sentence containing the citation and also over the paragraph containing the citing sentence.",3.1 Features,[0],[0]
"For each type of context, a topic model is trained over 321,129 respective contexts from the ARC.",3.1 Features,[0],[0]
"Table 5 shows example topics.
",3.1 Features,[0],[0]
"Prototypical Argument Features We also explored richer grammatical features, drawing on selectional preferences reflecting expectations for predicate arguments (Erk, 2007).",3.1 Features,[0],[0]
We construct a prototype for each citation function by identifying the frequent arguments seen in different syntactic positions.,3.1 Features,[0],[0]
"For example, EXTENDS citations occur frequently as objects of verbs such as “follow” and “use”, whereas USES citations have techniques or artifact words as dependents; Table 6 shows more examples.",3.1 Features,[0],[0]
"Each class’s selectional preferences are represented using a vector for the argument at each relation type, constructed by summing the vectors of all words appearing in it.",3.1 Features,[0],[0]
"Each function is represented as a separate feature whose value is the
average similarity of an instance’s arguments with the class’s preferences for all observed syntactic relationships (i.e., how similar are the syntacticallyrelated words to the function’s preferences).",3.1 Features,[0],[0]
"Our work differs from dependency-based features from prior work that use separate features for each unique dependency path and argument (Athar and Teufel, 2012; Abu-Jbara et al., 2013); in contrast, we use a single feature for each path with distributed representation for its arguments, which allows our features to generalize to similar words that are unseen in the training data.",3.1 Features,[0],[0]
"Models All models were trained using a Random Forest classifier, which is robust to overfitting even with large numbers of features (Fernández-Delgado et al., 2014).",3.2 Experimental Setup,[0],[0]
"After limited grid search over possible configurations,5 we set parameter values as follows.",3.2 Experimental Setup,[0],[0]
The number of random trees is 2500 and we required each leaf to match at least 5 instances.,3.2 Experimental Setup,[0],[0]
"To overcome the class imbalance, we use SMOTE (Chawla et al., 2002) to generate synthetic examples in the training fold using the 5 nearest neighbors.",3.2 Experimental Setup,[0],[0]
"The
5The grid search was performed using the following parameter ranges: number of trees [100, 500, 1000, 2500]; maximum number of depth of the decision tree as n 10 or √ n, where n is the number of features; minimum leaf size in decision tree [2, . . .",3.2 Experimental Setup,[0],[0]
", 10]; number of topics [50, 100, 250, 500]; and whether to use Smote (Chawla et al., 2002).
",3.2 Experimental Setup,[0],[0]
"classifier is implemented using SciKit (Pedregosa et al., 2011) and syntactic processing was done using CoreNLP",3.2 Experimental Setup,[0],[0]
"(Manning et al., 2014).",3.2 Experimental Setup,[0],[0]
"Selectional preferences used pretrained 300-dimensional GloVe vectors from the 840B token Common Crawl (Pennington et al., 2014).",3.2 Experimental Setup,[0],[0]
"The topic model features used an LDA with 100 topics.
",3.2 Experimental Setup,[0],[0]
Data Annotated data is crucial for developing high accuracy for rare citation classes.,3.2 Experimental Setup,[0],[0]
"Therefore, we integrate portions of the dataset of Teufel (2010),6 which has fine-grained citation function labeled for ACL-related documents using the annotation scheme of Teufel et al. (2006b).",3.2 Experimental Setup,[0],[0]
We map their 12 function classes into our six classes (see Appendix A).,3.2 Experimental Setup,[0],[0]
"When combining the two datasets, we omit the data labeled with their BACKGROUNDequivalent class to reduce the effects of a large majority class and because instances of the FUTURE class are merged into BACKGROUND according to their scheme.",3.2 Experimental Setup,[0],[0]
"The resulting citation function dataset contains 3,083 instances.
",3.2 Experimental Setup,[0],[0]
Evaluation Evaluation is performed using crossvalidation where each fold leaves out all citations of a single paper.,3.2 Experimental Setup,[0],[0]
"Stratifying by paper instead of instance is critical: since multiple citations may appear in the same sentence, instance-based stratification would leak information between training and test.",3.2 Experimental Setup,[0],[0]
"We also note that when performing crossvalidation, we compute the bootstrapped patterns and prototypical argument features using only contexts from the training data.",3.2 Experimental Setup,[0],[0]
"We report macroaveraged F1 scores across the six function classes.
",3.2 Experimental Setup,[0],[0]
Comparison Systems We compare against three state-of-the-art systems which all use similar citation function classifications.,3.2 Experimental Setup,[0],[0]
"Abu-Jbara et al. (2013) use a combination of lexicons, structural, and syntactic features for classification.",3.2 Experimental Setup,[0],[0]
Instances are classified using a linear kernel SVM.,3.2 Experimental Setup,[0],[0]
Their described method also uses a second CRF-based classifier to include neighboring sentences in the citation context.,3.2 Experimental Setup,[0],[0]
"As the dataset for this citation-span classifier is not public, we are unable to reproduce this part of their system.",3.2 Experimental Setup,[0],[0]
"However, the authors note that us-
6Their original data may be obtained at http://www. cl.cam.ac.uk/˜sht25/CFC.html and we distribute a re-annotated version of this with our data.
",3.2 Experimental Setup,[0],[0]
"ing the citing sentence alone is the correct context in 80% of the instances, so we view our implementation as a close approximation.",3.2 Experimental Setup,[0],[0]
"Dong and Schäfer (2011) classify citations using a small set of lexicons and discourse features, which includes regular expressions on sentence parts of speech for capturing syntactic cues.",3.2 Experimental Setup,[0],[0]
"Their model uses a naive bayes classifier, which was shown to work well for their data.",3.2 Experimental Setup,[0],[0]
Teufel (2000) is the most similar model to ours as it uses a subset of our lexical features and lexicons; the model uses a k-nearest neighbor classifier.,3.2 Experimental Setup,[0],[0]
"We note that the original implementation used a custom syntactic tool for identifying aspects like verb tense, which we replaced with CoreNLP.",3.2 Experimental Setup,[0],[0]
"We compare against the system Teufel (2000) instead of the system Teufel et al. (2006b) because the latter includes pattern-based features that are not fully specified or publicly available; however, the two systems are similar in their description.",3.2 Experimental Setup,[0],[0]
"For all three compared systems, we use identical parameter values as reported in the papers.",3.2 Experimental Setup,[0],[0]
Baselines Two baselines are used for comparison: a Random baseline that selects a function at chance and a Majority-class baseline that labels all instances with the most frequent citation function BACKGROUND.,3.2 Experimental Setup,[0],[0]
"Our methods substantially outperformed the closest state of the art and both baselines for both classification tasks, as shown in Table 7.",3.3 Results and Discussion,[0],[0]
"All improvements over comparison systems are statistically significant (McNemar’s, p≤0.01).",3.3 Results and Discussion,[0],[0]
"The closest-performing system was that of Abu-Jbara et al. (2013), which also had a heavily-lexicon based approach.
",3.3 Results and Discussion,[0],[0]
"An ablation test suggests that each of our novel
features contributed to the final performance.",3.3 Results and Discussion,[0],[0]
"Notably, we observe that selectional preference and bootstrapped lexicon features had the largest impact on performance; both features capture local information indicating this type of information is important for recognizing function.",3.3 Results and Discussion,[0],[0]
"While multiple prior works have focused on patterns to recognize function, our results suggest that machine learned patterns and contextual regularities (topics or word vectors) provide highly-accurate information.",3.3 Results and Discussion,[0],[0]
"Indeed, examining the feature weighting in the random forest shows that features for structure (e.g., section number), topic, and selectional preference comprised most of the 100 highest-weighted features (76%).
",3.3 Results and Discussion,[0],[0]
The use of conjunctive features by the Random Forest was critical for high performance.,3.3 Results and Discussion,[0],[0]
"All other non-conjunctive classifiers we tried resulted in substantially lower Macro F1: Naive Bayes, 0.286; knearest neighbor, 0.255 (k=3); and Linear-kernel SVM, 0.393 (C=1).7
The resulting classifier performance is sufficient to apply it to the entire ARC dataset for the analyses in the next four sections.",3.3 Results and Discussion,[0],[0]
"Nonetheless, errors remain.",3.3 Results and Discussion,[0],[0]
Our error analysis revealed that a main challenge is incorporating information external to the citing sentence.,3.3 Results and Discussion,[0],[0]
"Consider the following example:
BilderNetle is our new data set of German noun-toImageNet synset mappings.",3.3 Results and Discussion,[0],[0]
"ImageNet is a largescale and widely used image database, built on top of WordNet, which maps words into groups of images, called synsets (Deng et al., 2009).
",3.3 Results and Discussion,[0],[0]
"Here the citing sentence appears much like a BACKGROUND citation when read in isolation; however, the preceding sentence reveals that the citing work’s data is based on the citation, making its function USES though no explicit cues suggest this in the citing sentence.",3.3 Results and Discussion,[0],[0]
"Thus, our error analysis supports the observation of Abu-Jbara et al. (2013) that citation context identification is an important step towards improving performance and models with richer textual understanding are needed to understand how the
7We observed mixed results when using a random forest with other approaches.",3.3 Results and Discussion,[0],[0]
Replacing the k-nearest neighbors classifier used in Teufel (2000) with a random forest improves citation function classification by 0.119 Macro F1.,3.3 Results and Discussion,[0],[0]
"In contrast, replacing the SVM model used by Abu-Jbara et al. (2013) decreased performance by 0.072 Macro F1.",3.3 Results and Discussion,[0],[0]
"We speculate that the larger feature space of Teufel (2000), which is more similar to our features space, is more conducive to conjunctive features.
citation relates to the broader context and narrative outside of the sentence.
",3.3 Results and Discussion,[0],[0]
"In the next four sections, we apply our classifier trained on our combined dataset (2600 citation instances) to the ACL Anthology to study what citation functions can tell us about scientific uptake and author behavior.",3.3 Results and Discussion,[0],[0]
"Scientific papers commonly follow a structured section narrative to frame their contributions: Introduction, Methodology, Results, and Discussion (Skelton, 1994; Nwogu, 1997).",4 Narrative Structure of Citation Function,[0],[0]
"Each section in the narrative adopts argumentative moves designed to convince the reader of the work’s claims (Swales, 1986; Swales, 1990).",4 Narrative Structure of Citation Function,[0],[0]
"We hypothesize that this narrative is mirrored in how authors use their citations in sections, with the citation’s function serving to further evoke section’s intended rhetorical frame (Goffman, 1974; Gumperz, 1982).
",4 Narrative Structure of Citation Function,[0],[0]
"To test this hypothesis, the function classifier was applied to all 21,474 papers of the latest 2016 release of the ACL Anthology.",4 Narrative Structure of Citation Function,[0],[0]
"This yielded a dataset of 134,127 citations between papers in the ARC.",4 Narrative Structure of Citation Function,[0],[0]
"The resulting distributions of citation function (Figure 2), show that authors’ citation framing indeed parallels the expected rhetorical framing seen in the writing: (1) establishing an intellectual lineage via BACKGROUND citations in the Introduction, Motivation, and Related Work sections to (2) introducing methodology with USES citations in the Methodology and Evaluation sections, (3) a large increase in COMPARISON OR CONTRAST for related literature in the Results and Discussions, and finally (4) closing comparisons and pointers to future directions.
",4 Narrative Structure of Citation Function,[0],[0]
"These trends also mirror the thematic structure identified in full-paper textual analyses (Skelton, 1994; Nwogu, 1997).",4 Narrative Structure of Citation Function,[0],[0]
"By showing that a section contains citations serving a variety of functions, our findings further point to a new direction for citation placement studies (Hu et al., 2013; Ding et al., 2013; Bertin et al., 2016), which have largely treated all citations within a section as equivalent.",4 Narrative Structure of Citation Function,[0],[0]
"Each publication venue has its own expectation for the types of work it accepts, e.g., the degree of polish or depth of experiments.",5 Venues and Citation Patterns,[0],[0]
"As such, each venue has a distinct genre of writing, from the tentative results of workshop papers to journal papers with substantial synthesis.",5 Venues and Citation Patterns,[0],[0]
To what degree do venue genres affect the way authors cite?,5 Venues and Citation Patterns,[0],[0]
"To answer this, we used the same experimental setup as the previous section.",5 Venues and Citation Patterns,[0],[0]
"Figure 3 shows citation function by venue for the 134,127 citations.
",5 Venues and Citation Patterns,[0],[0]
We find that similar venue types have similar distributions of citation framing.,5 Venues and Citation Patterns,[0],[0]
"Journals have the highest percentage of BACKGROUND citations, suggesting that their extra space and wider temporal scope lends itself more to positioning.",5 Venues and Citation Patterns,[0],[0]
"Conference venues devote proportionally more space to contrast and comparison with other work, presumably because new NLP work is first presented at conferences and hence acceptance requires demonstrating the proposed technique is better than existing ones.",5 Venues and Citation Patterns,[0],[0]
"Workshops, by contrast, have relatively little comparison and instead use more BACKGROUND; the experimental nature of workshop papers presumably results in fewer potential prospects for compar-
ison.",5 Venues and Citation Patterns,[0],[0]
"Similarly, the SemEval workshops focus on rapidly developing new systems for a shared task, which is reflected in the papers framing as primarily USES citations and relatively little COMPARE OR CONTRAST, as the venue’s shared task provides the broader framing connecting papers to related work.",5 Venues and Citation Patterns,[0],[0]
The growth of the ACL community has been accompanied by the creation of new publication venues.,6 Venue Evolution,[0],[0]
How have these new venues evolved by possibly becoming institutionalized and resembling established conferences or becoming stylistically distinct and capturing different representations of knowledge?,6 Venue Evolution,[0],[0]
Citation framing provides an ideal lens for observing this evolution by measuring the degree to which a newer venue’s papers’ framing mirrors that used by papers in established venues.,6 Venue Evolution,[0],[0]
"Here, we examine venue evolution in the ACL through its workshops.
",6 Venue Evolution,[0],[0]
Conferences within the ACL community frequently have collocated workshops that focus on a particular theme and have their own proceedings.,6 Venue Evolution,[0],[0]
"The number of workshops has increased substantially with the growth of the field, from around ten workshops in the 1990s to over 100 workshops by 2010, with many workshops having multiple iterations across the years.",6 Venue Evolution,[0],[0]
"This growth has led to the observation that ACL workshops have become like mini-conferences rather than venues for early-stage research and discussion (Daumé III, 2016).",6 Venue Evolution,[0],[0]
"Are workshops becoming more conference-like and, if so, is this a general trend or primarily seen in longrunning workshops?",6 Venue Evolution,[0],[0]
We hypothesize that multiple iterations of the same workshop create institutional knowledge and community norms that leads to more conference-like papers over time.,6 Venue Evolution,[0],[0]
"Here, we test this hypothesis by measuring whether workshop papers have become more similar in their citation framing to papers from the main conferences.
",6 Venue Evolution,[0],[0]
Experimental Setup We repeat the classification setup from the previous experiment.,6 Venue Evolution,[0],[0]
We compare the average framing of a paper within a venue in a given year with the distribution for the two main conferences (ACL and NAACL) within that year.,6 Venue Evolution,[0],[0]
"Distributions are compared using the JensenShannon Divergence, where 1 indicates that the venues are citing identically.
",6 Venue Evolution,[0],[0]
Results Workshops consistently became more conference-like in their papers’ citation framing (Figure 4).,6 Venue Evolution,[0],[0]
"Further, this trend in increasing similarity was seen much more for both long-running workshops, suggesting that multiple-iteration workshops create their own conference-like norms that attract more conference-like papers each year.",6 Venue Evolution,[0],[0]
We speculate that the increasing similarity of workshops to conferences indicates the field has begun to congeal.,6 Venue Evolution,[0],[0]
"Early workshops were like satellite conferences on peripheral topics, but as the field grows and the methods standardize, a norm of publication emerges such that conferences and workshops all resemble an institutionalized standard.",6 Venue Evolution,[0],[0]
Multiple iterations of a work accelerate this process by further establishing publication norms within a sub-community.,6 Venue Evolution,[0],[0]
"The scholarly narrative told through citations provides the reader with support for its claims and technical competence (Latour, 1987, p. 34).",7 Predicting Future Impact,[0],[0]
"This framing could affect how the work is perceived and, ultimately, how it is received and cited within the com-
munity (Shi et al., 2010).",7 Predicting Future Impact,[0],[0]
"Does the frame evoked by a paper through its citation functions (the way it compares to related work, or motivates, or points to the future) affect its reception?
",7 Predicting Future Impact,[0],[0]
"Experimental Setup To quantify how a paper’s citation framing affects its future uptake, we constructed a negative binomial regression to predict the cumulative number of citations a paper received within the first five years after publication, which is known to be highly representative of the eventual citation count (Wang et al., 2013; Stern, 2014).",7 Predicting Future Impact,[0],[0]
"In addition to variables for how the paper cites, we include variables from state-of-the-art features for predicting the citation count (Yan et al., 2011; Yogatama et al., 2011; Yan et al., 2012; Chakraborty et al., 2014; Dong et al., 2016), described below.",7 Predicting Future Impact,[0],[0]
"We compare against a baseline regression model without citation framing and test whether the model’s fit is improved when the framing is included as features.
",7 Predicting Future Impact,[0],[0]
"All papers with at least five years of publication history in the anthology were considered, yielding a set of 10,434 papers.",7 Predicting Future Impact,[0],[0]
"We used negative binomial models, which are more appropriate than linear regression as citation counts are non-negative discrete counts, and compared them by using Akaike Information Criterion (AIC).",7 Predicting Future Impact,[0],[0]
"AIC measures each model’s goodness of fit in proportion to the number of independent variables; when comparing models, the model with the minimal AIC is preferred (Akaike, 1974).",7 Predicting Future Impact,[0],[0]
"If citation framing helps to explain future impact, we should see a lower AIC despite the penalty for including more variables to the model.
",7 Predicting Future Impact,[0],[0]
Five types of non-citation features were included.,7 Predicting Future Impact,[0],[0]
"To model the amount of attention received by different research areas, each paper is associated with its distribution over 100 topics, built using LDA over the ARC.",7 Predicting Future Impact,[0],[0]
"To capture diversity, we include the entropy of the topic distribution.",7 Predicting Future Impact,[0],[0]
We include the publication year since the size of the field changes over time.,7 Predicting Future Impact,[0],[0]
"Multi-author papers are known to receive higher citation counts (Gazni and Didegah, 2011), partially due to the effects of self-citation (Fowler and Aksnes, 2007), and therefore we include the number of authors on the paper.",7 Predicting Future Impact,[0],[0]
"To reflect how integrated the paper is, we include the number of references.8
8To control for collinearity between citation-related predic-
Results Knowledge of how a paper frames its contributions helps improve predicting its future impact, with a statistically significant improvement in AIC when the distribution of citation functions is added (likelihood ratio test, p ≤ 0.01).
",7 Predicting Future Impact,[0],[0]
Table 8 shows which types of citations are significantly predictive of higher impact (p ≤ 0.01).,7 Predicting Future Impact,[0],[0]
Two main insights can be made from these results.,7 Predicting Future Impact,[0],[0]
"First, papers maximize their future impact when framed as integrating many other technologies via USES citations.",7 Predicting Future Impact,[0],[0]
"Second, works that frame their contributions through COMPARISON OR CONTRAST rather than BACKGROUND are more likely to have a higher impact.",7 Predicting Future Impact,[0],[0]
"Latour (1987, p. 54) has suggested that authors may deflect criticism of their work (improving its perception) by claiming it as an extension, rather than comparing it with prior work.",7 Predicting Future Impact,[0],[0]
"However, we did not observe this effect in how authors frame
tors, we regress out the number of citations from the citation function counts (Kutner et al., 2004; O’brien, 2007).",7 Predicting Future Impact,[0],[0]
"Finally, we include the publication venue, using the individual conference or workshop in which the paper was published to control for variations in prestige between venues.",7 Predicting Future Impact,[0],[0]
"The resulting model has a variance inflation factor of < 10 for all variables.
",7 Predicting Future Impact,[0],[0]
"their work as COMPARISON OR CONTRAST or EXTENDS, with both having significant positive effects.",7 Predicting Future Impact,[0],[0]
"As scientific fields evolve, new subfields initially emerge around methods or technologies which become a focus of collective puzzle-solving and continual improvement (Moody, 2004).",8 The Growth of Rapid Discovery Science,[0],[0]
"NLP has witnessed the emergence of several such subfields from the early grammar based approaches in the 1950s1970s, to the statistical revolution in the 1990s, to the recent deep learning models (Spärck Jones, 2001; Anderson et al., 2012).",8 The Growth of Rapid Discovery Science,[0],[0]
"Collins (1994) proposed that a field can undergo a particular shift, referring to it as rapid discovery science, when the field (a) reaches high consensus on research topics as well as methods and technologies, and (b) then develops genealogies of methods and technologies that continually improve on one another.",8 The Growth of Rapid Discovery Science,[0],[0]
"Over time, there is increased consensus on core approaches, and the field’s periphery is extended to new research puzzles rather than contesting prior efforts.",8 The Growth of Rapid Discovery Science,[0],[0]
"Collins claims this shift characterizes natural sciences, but not many social sciences, which are instead more likely to engage in continual contesting and turnover of core methods and assumptions (Evans et al., 2016).
",8 The Growth of Rapid Discovery Science,[0],[0]
We argue that a shift to rapid discovery science should be visible in the way citations are used to frame works in the field as a whole.,8 The Growth of Rapid Discovery Science,[0],[0]
"Specifically, we expect that as consensus is reached (1) authors are expected to have fewer comparisons to other works and instead can simply acknowledge past work as background and (2) the remaining comparisons concentrate on fewer works, reflecting those works’ status as accepted benchmarks of performance.",8 The Growth of Rapid Discovery Science,[0],[0]
"Further, we expect that as a methodological lineage develops we should also observe an increased concentration of USES citations on papers describing methods and data.
",8 The Growth of Rapid Discovery Science,[0],[0]
"We propose that the increased use of shared evaluations, and the statistical methodology borrowed originally from electrical engineering (Hall et al., 2008; Anderson et al., 2012) has led NLP to undergo a shift towards rapid discovery science.
",8 The Growth of Rapid Discovery Science,[0],[0]
"Experimental Setup We repeat the setup of previous experiments and measure the expected citation
frame of a paper per year using all papers published in that year.
",8 The Growth of Rapid Discovery Science,[0],[0]
"Results The NLP field shows a significant increase in consensus consistent with the rise in rapid discovery science, evidenced through two main trends.
",8 The Growth of Rapid Discovery Science,[0],[0]
"First, NLP authors use a decreasing number of comparison and contrast citations (r= -0.899, p ≤ 0.01) as seen in Figure 5.",8 The Growth of Rapid Discovery Science,[0],[0]
"Instead of comparing to others, it seems that authors simply acknowledge prior work as BACKGROUND, which had a
corresponding increase in relative frequency.",8 The Growth of Rapid Discovery Science,[0],[0]
"Despite an increase in BACKGROUND citations, the total percentage of non-methodological citations still declines (r= -0.663, p ≤ 0.01), with authors instead increasingly including more USES citations.",8 The Growth of Rapid Discovery Science,[0],[0]
"Latour (1987, p. 50) argues that such non-methodological references are critical to an author’s defense of an idea.",8 The Growth of Rapid Discovery Science,[0],[0]
We therefore interpret the observed decrease in non-methodological references as signaling a reduced need for authors to defend aspects of their work.,8 The Growth of Rapid Discovery Science,[0],[0]
"Authors are able to compare against fewer papers due to the field’s growing consensus on the validity of the problem and methodological contribution.
",8 The Growth of Rapid Discovery Science,[0],[0]
Note that there is a small but significant increase in the number of non-methodological references between 2009 and 2011.,8 The Growth of Rapid Discovery Science,[0],[0]
"This transition corresponds to the date at which ACL venues began allowing unlimited references (2010 for ACL, 2011 for NAACL, etc.).",8 The Growth of Rapid Discovery Science,[0],[0]
"Unlimited extra space for citations acted to modify authors’ citation framing behavior; given unlimited space, authors chose to include proportionally more non-methodological citations.9
In the second trend, authors are more likely to use and compare against the same set of papers, as shown in Figure 6 by the rise in expected incom-
9Note that this change acts against the general decrease in non-methodological; considering only 1980-2009, the decrease in non-methodological is even larger (r= -0.568, p ≤ 0.01 ).
",8 The Growth of Rapid Discovery Science,[0],[0]
"ing citations to those works compared against (r= 0.734, p ≤ 0.01) and used (r=0.889, p ≤ 0.01).",8 The Growth of Rapid Discovery Science,[0],[0]
"For example, in 1991, authors compared with a diffuse group of parsing papers, e.g., (Shieber, 1988; Pereira and Warren, 1983; Haas, 1989), with such papers receiving at most three citations that year; whereas in 2000, most comparisons were to a core set of parsing papers, e.g., (Collins, 1999; Buchholz et al., 1999; Collins, 1997), with a much sharper (lower entropy) distribution of citations.",8 The Growth of Rapid Discovery Science,[0],[0]
These trends show the increased incorporation of prior work to form a lineage of method technologies as well as show increased consensus on which works are sufficient for comparing against in order to establish a claim.,8 The Growth of Rapid Discovery Science,[0],[0]
"These results also empirically confirm the observation of Spärck Jones (2001) that a major trend in NLP in the 1990s was an increase in reusable technologies and evaluations, like the BNC (Leech, 1992) and the Penn Treebank (Marcus et al., 1993).
",8 The Growth of Rapid Discovery Science,[0],[0]
"More broadly, our work points to the future of NLP as a quickly moving field of high consensus and suggests that artifacts that facilitate consensus such as shared tasks and open source research software will be necessary to continue this trend.",8 The Growth of Rapid Discovery Science,[0],[0]
"Authors cite works for different reasons (or function), so regarding them as equivalent signals is potentially problematic.",9 Conclusion,[0],[0]
"Many fluff citations exist, while some less common ones are substantively relevant to the paper’s argument.",9 Conclusion,[0],[0]
"A careful analysis of citation reveals that authors cite works for multiple reasons—as background, motivation, extension, use, contrast, or future.",9 Conclusion,[0],[0]
"When authors utilize some forms of citation over others they can significantly influence how their own work gets perceived and taken up by others (Latour, 1987).",9 Conclusion,[0],[0]
"Simply put, citation functions help frame an article’s reception.",9 Conclusion,[0],[0]
"Moreover, a differentiation of citation functions affords a deeper understanding of how scholars develop arguments for different publication venues as well as how these venues may demand different forms of knowledge representation and arguments over time.",9 Conclusion,[0],[0]
"In fact, these modes of citation help us understand the state of research efforts and their evolution more broadly for entire scientific fields like NLP.",9 Conclusion,[0],[0]
"In this paper, we relate all this using a new cor-
pus annotated with citation function and by developing a state-of-the-art classifier for revealing scientific framing.",9 Conclusion,[0],[0]
"In doing so, we demonstrate the importance of novel unsupervised features related to topic models and argument structure, and label all the citations for an entire field.
",9 Conclusion,[0],[0]
"We then show that citation framing reveals salient behaviors of writers, readers, and the field as a whole: (1) authors are sensitive to discourse structure and venue when citing, (2) ACL workshops have evolved to become more like the mainstream conferences, with multi-iteration workshops being quicker to establish conference-like norms, (3) the way in which an author frames their work aids in predicting its future impact as the number of citations its receives, with the community favoring works that integrate many new technologies and also relate to prior work through comparison and contrast, and (4) the NLP field as a whole has seen increased consensus in what constitutes valid work— with a reduced need for positioning and excessive comparison—demonstrating its shift towards rapid discovery science.",9 Conclusion,[0],[0]
"All data, materials, and code for all systems are available at https://github.com/ davidjurgens/citation-function.",9 Conclusion,[0],[0]
"The authors thank Jure Leskovec, Vinod Prabhakaran, Will Hamilton, and the other members of the Stanford NLP Group for helpful discussions and comments and thank Min-Yen Kan for hosting the ACL Anthology and help with data.",Acknowledgements,[0],[0]
"We also thank the area chair, Katrin Erk, and reviewers for their helpful comments and suggestions.",Acknowledgements,[0],[0]
"This work is also partially supported by the NSF under award IIS1633036, the Stanford Data Science Initiative, the Brown Institute for Media Innovation, and the Science Surveyor project.",Acknowledgements,[0],[0]
"As a part of training the classifier, instances from Teufel (2010) are used to supplement rare classes.",A Conversion of Teufel (2010) Data,[0],[0]
"Their data uses the scheme of Teufel et al. (2006b), which similar to our scheme but has several finegrained distinctions.",A Conversion of Teufel (2010) Data,[0],[0]
"We convert the instances from their dataset as follows:
Teufel et al. (2006b) classification Our Label Weak Comparison or Contrast CoCoGM Comparison or Contrast CoCo Comparison or Contrast CoCoR0 Comparison or Contrast CoCoXY",A Conversion of Teufel (2010) Data,[0],[0]
Background PBas Extends PUse,A Conversion of Teufel (2010) Data,[0],[0]
Uses PModi Extends PMot Motivation PSim Comparison or Contrast PSup,A Conversion of Teufel (2010) Data,[0],[0]
Comparison or Contrast Neut Background CoMetN Comparison or Contrast CoGoaN Comparison or Contrast CoMet Comparison or Contrast CoCoN Comparison or Contrast CoCoM Comparison or Contrast CoResN Comparison or Contrast Note that we omit instances whose converted class is BACKGROUND in order to reduce the effects of a large majority class and because instances of the FUTURE class are merged into BACKGROUND according to their scheme.,A Conversion of Teufel (2010) Data,[0],[0]
Citations have long been used to characterize the state of a scientific field and to identify influential works.,abstractText,[0],[0]
"However, writers use citations for different purposes, and this varied purpose influences uptake by future scholars.",abstractText,[0],[0]
"Unfortunately, our understanding of how scholars use and frame citations has been limited to small-scale manual citation analysis of individual papers.",abstractText,[0],[0]
"We perform the largest behavioral study of citations to date, analyzing how scientific works frame their contributions through different types of citations and how this framing affects the field as a whole.",abstractText,[0],[0]
"We introduce a new dataset of nearly 2,000 citations annotated for their function, and use it to develop a state-of-the-art classifier and label the papers of an entire field: Natural Language Processing.",abstractText,[0],[0]
We then show how differences in framing affect scientific uptake and reveal the evolution of the publication venues and the field as a whole.,abstractText,[0],[0]
"We demonstrate that authors are sensitive to discourse structure and publication venue when citing, and that how a paper frames its work through citations is predictive of the citation count it will receive.",abstractText,[0],[0]
"Finally, we use changes in citation framing to show that the field of NLP is undergoing a significant increase in consensus.",abstractText,[0],[0]
Measuring the Evolution of a Scientific Field through Citation Frames,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 648–658 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Several psycholinguistic studies in the last two decades have brought extensive evidence that humans activate a rich array of event knowledge during sentence processing: verbs (e.g. arrest) activate expectations about their typical arguments (e.g. cop, thief )",1 Introduction,[0],[0]
"(McRae et al., 1998; Altmann and Kamide, 1999; Ferretti et al., 2001; McRae et al., 2005; Hare et al., 2009; Matsuki et al., 2011), and nouns activate other nouns typically co-occurring in the same events (Kamide et al., 2003; Bicknell et al., 2010).",1 Introduction,[0],[0]
"Subjects are able to determine the plausibility of a noun for a given argument role and quickly use this knowledge to anticipate upcoming linguistic input (McRae and Matsuki, 2009).",1 Introduction,[0],[0]
This phenomenon is referred to in the literature as thematic fit.,1 Introduction,[0],[0]
"Thematic fit estimation
has been extensively used in sentence comprehension studies on constraint-based models, mainly as a predictor variable allowing to disambiguate between possible structural analyses.1 More in general, thematic fit is considered as a key factor in a variety of studies concerned with structural ambiguity (Vandekerckhove et al., 2009).
",1 Introduction,[0],[0]
"Starting from the work of Erk et al. (2010), several distributional semantic methods have been proposed to compute the extent to which nouns fulfill the requirements of verb-specific thematic roles, and their performances have been evaluated against human-generated judgments (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Sayeed et al., 2015, 2016; Greenberg et al., 2015a,b).",1 Introduction,[0],[0]
"Most research on thematic fit estimation has focused on count-based vector representations (as distinguished from prediction-based vectors).2 Indeed, in their comparison between highdimensional explicit vectors and low-dimensional neural embeddings, Baroni et al. (2014) found that thematic fit estimation is the only benchmark on which prediction models are lagging behind stateof-the-art performance.",1 Introduction,[0],[0]
"This is consistent with Sayeed et al. (2016)’s observation that “thematic fit modeling is particularly sensitive to linguistic detail and interpretability of the vector space”.
",1 Introduction,[0],[0]
The present work sets itself among the unsupervised approaches to thematic fit estimation.,1 Introduction,[0],[0]
"By relying on explicit and interpretable count-based vector representations, we propose a simple, cognitively-inspired, and efficient thematic fit model using information extracted from dependency-parsed corpora.",1 Introduction,[0],[0]
"The key features of our proposal are a) prototypical representations of verb-specific thematic roles, based on feature weighting and filtering of second order contexts
1For an overview on constraint-based models, see MacDonald and Seidenberg (2006).
",1 Introduction,[0],[0]
"2We adopt the terminology from Baroni et al. (2014).
",1 Introduction,[0],[0]
"648
(i.e. contexts that are salient for many of the typical fillers of a given verb-specific thematic role), and b) a similarity measure which computes the Weighted Overlap (WO) between prototypes and candidate fillers.3",1 Introduction,[0],[0]
"Erk et al. (2010) were, at the best of our knowledge, the first authors to measure the correlation between human-elicited thematic fit ratings and the scores assigned by a syntax-based Distributional Semantic Model (DSM).",2 Related Work,[0],[0]
"More specifically, their gold standard consisted of the human judgments collected by McRae et al. (1998) and Padó (2007).",2 Related Work,[0],[0]
"The plausibility of each verb-filler pair was computed as the similarity between new candidate nouns and previously attested exemplars for each specific verb-role pairing (as already proposed in Erk (2007)).
",2 Related Work,[0],[0]
"Baroni and Lenci (2010) evaluated their Distributional Memory (henceforth DM)4 framework on the same datasets, adopting an approach to the task that has become dominant in the literature: for each verb role, they built a prototype vector by averaging the dependency-based vectors of its most typical fillers.",2 Related Work,[0],[0]
"The higher the similarity of a noun with a role prototype, the higher its plausibility as a filler for that role.",2 Related Work,[0],[0]
"Lenci (2011) has later extended the model to account for the dynamic update of the expectations on an argument, depending on how another role is filled.",2 Related Work,[0],[0]
"By using the same DM tensor, this study tested an additive and a multiplicative model (Mitchell and Lapata, 2010) to compose and update the expectations on the patient filler of the subject-verb-object triples of the Bicknell dataset (Bicknell et al., 2010).
",2 Related Work,[0],[0]
"The thematic fit models proposed by Sayeed and Demberg (2014) and Sayeed et al. (2015) are similar to Baroni and Lenci’s, but their DSMs were built by using the roles assigned by the SENNA semantic role labeler (Collobert et al., 2011) to define the feature space.",2 Related Work,[0],[0]
"These authors argued that the prototype-based method with dependencies works well when applied to the agent and to the patient role (which are almost always syntactically realized as subjects and objects), but
3Code: https://github.com/esantus/Thematic Fit 4In this paper, we will make reference to two different models of DM: DepDM and TypeDM.",2 Related Work,[0],[0]
"DepDM counts the frequency of dependency links between words (e.g. read, obj, book), while TypeDM uses the variety of surface forms that express the link between words, rather than the link itself.
that it might be problematic to apply it to different roles, such as instruments and locations, as the construction of the prototype would have to rely on prepositional complements as typical fillers, and the meaning of prepositions can be ambiguous.",2 Related Work,[0],[0]
"Comparing their results with Baroni and Lenci (2010), the authors showed that their system outperforms the syntax-based model DepDM and almost matches the scores of the best performing TypeDM, which uses hand-crafted rules.",2 Related Work,[0],[0]
"Moreover, they were the first to evaluate thematic role plausibility for roles other than agent and patient, as they computed the scores also for the instruments and for the locations of the Ferretti datasets (Ferretti et al., 2001).
",2 Related Work,[0],[0]
"Greenberg et al. (2015a,b) further developed the TypeDM and the role-based models, investigating the effects of verb polysemy on human thematic fit judgments and introducing a hierarchical agglomerative clustering algorithm into the prototype creation process.",2 Related Work,[0],[0]
"Their goal was to cluster together typical fillers into multiple prototypes, corresponding to different verb senses, and their results showed constant improvements of the performance of the DM-based model.
",2 Related Work,[0],[0]
"Finally, Tilk et al. (2016) presented two neural network architectures for generating probability distributions over selectional preferences for each thematic role.",2 Related Work,[0],[0]
"Their models took advantage of supervised training on two role-labeled corpora to optimize the distributional representation for thematic fit modeling, and managed to obtain significant improvements over the other systems on almost all the evaluation datasets.",2 Related Work,[0],[0]
"They also evaluated their model on the task of composing and updating verb argument expectations, obtaining a performance comparable to Lenci (2011).",2 Related Work,[0],[0]
"As pointed out by Sayeed et al. (2016), most works on unsupervised thematic fit estimation vary in the method adopted for constructing the prototypes.",3 Methodology,[0],[0]
"The semantic role prototype is usually a vector, obtained by averaging the most typical fillers, and plausibility of new fillers depends on their similarity to the prototype, assessed by means of vector cosine (the standard similarity measure for DSMs; see Turney and Pantel (2010)).
",3 Methodology,[0],[0]
"Its merits notwithstanding, we argue that this method is not optimal for characterizing roles.",3 Methodology,[0],[0]
"Distributional vectors are typically built as out-of-
context representations, and they conflate different senses.",3 Methodology,[0],[0]
"By building the prototype as the centroid of a cluster of vectors and measuring then the thematic fit with vector cosine, the plausibility score is inevitably affected by many contexts that are irrelevant for the specific verb-argument combination.5",3 Methodology,[0],[0]
This is likely to be one of the main reasons behind the difficulties of modeling roles other than agent and patient with syntax-based DSMs.,3 Methodology,[0],[0]
"We claim that improving the prototype representation might lead to a better characterization of thematic roles, and to a better treatment of polysemy.
",3 Methodology,[0],[0]
"When a verb and an argument are composed, humans are intuitively able to select only the part of the potential meaning of the words that is relevant for the concept being expressed (e.g. in The player hit the ball, humans would certainly exclude from the meaning of ball semantic dimensions that are strictly related to its dancing sense).",3 Methodology,[0],[0]
"In other words, not all the features of the semantic representations are active, and the composition process makes some features more ‘prominent’, while moving others to the background.6
Although we are not aware of experimental works specifically dedicated to verb-argument composition, a similar idea has been supported in studies on conceptual combinations (Hampton, 1997, 2007): when a head and a modifier are combined, their interaction affects the saliency of the features in the original concepts.",3 Methodology,[0],[0]
"For example, in racing car, the most salient properties would be those related to SPEED, whereas in family car SPACE properties would probably be more prominent.",3 Methodology,[0],[0]
Yeh and Barsalou (2006) used a property priming experiment to show how the concept features activated during language comprehension vary across the background situations described by the sentence they occur in.,3 Methodology,[0],[0]
"When concepts are combined in a sentence, the features that are relevant for the specific combination are activated and are then easier to verify for human subjects.
",3 Methodology,[0],[0]
"The same could be true for linguisticallyderived properties of lexical meaning: Simmons et al. (2008) brought neuroimaging evidence of the early activation of word association areas during property generation tasks, and Santos et al. (2011)
",3 Methodology,[0],[0]
"5For an overview on the limitations of vector cosine, see: Li and Han (2013); Dinu et al. (2015); Schnabel et al. (2015); Faruqui et al. (2016); Santus et al. (2016a).
",3 Methodology,[0],[0]
"6An early proposal going in this direction is the predication theory by Kintsch (2001), which exploited Latent Semantic Analysis to select only the vector features that are appropriate for predicate-argument composition.
",3 Methodology,[0],[0]
showed that word associates are often among the properties generated for a given concept.,3 Methodology,[0],[0]
"Such findings suggest that, while we combine concepts, both embodied simulations and word distributions influence property salience (Barsalou et al., 2008).
",3 Methodology,[0],[0]
"Our model makes the following assumptions:
• the composition between a verb role representation and an argument shares the same cognitive mechanism underlying conceptual combinations;
• at least part of semantic representations is derived from, and/or mirrored in, linguistic data.7 Consistently, the process of selecting the relevant features of the concepts being composed corresponds to modify the salience of the dimensions of distributional vectors;
• thematic fit computation is carried out on the basis of the activation and selection of salient features of a verb thematic role prototype and of the candidate argument filler vectors.
",3 Methodology,[0],[0]
"We rely on syntax-based DSMs, using dependency relations to approximate verb-specific roles and to identify their most typical fillers: for agents/patients, we extract the most frequent subjects/objects, for instruments we use the prepositional complements introduced by with, and for locations those introduced by either on, at or in.
",3 Methodology,[0],[0]
"Assuming that the linguistic features of distributional vectors correspond to the properties of conceptual composition processes, a candidate filler can be represented as a sorted distributional vector of the filler term, in which the most salient contexts occupy the top positions.",3 Methodology,[0],[0]
"Similarly, the abstract representation of a verb-specific role is a sorted prototype-vector, whose features derive from the sum of the most typical filler vectors for that verb-specific role.
",3 Methodology,[0],[0]
"Differently from Baroni and Lenci, the core and novel aspect of our proposal, described in the following subsections, is that we do not simply measure the correlation between all the features of candidate and prototype vectors (as vector cosine would do on unsorted vectors), but rather we rank and filter the features, computing the weighted overlap with a rank-based similarity measure inspired by APSyn, a recent proposal by Santus
7See also the so-called ’strong version’ of the Distributional Hypothesis (Miller and Charles, 1991; Lenci, 2008).
",3 Methodology,[0],[0]
"et al. (2016a,b,c) which has shown interesting results in synonymy detection and similarity estimation.",3 Methodology,[0],[0]
"As we will show in the next sections, the new metric assigns high scores to candidate fillers sharing many salient contexts with the verb-specific role prototype.",3 Methodology,[0],[0]
The first step of our method consists in identifying the typical fillers of a verb-specific role.,3.1 Typical Fillers,[0],[0]
"Following Baroni and Lenci (2010), we weighted the raw cooccurrences between verbs, syntactic relations and fillers in the TypeDM tensor of DM with Positive Local Mutual Information (PLMI; Evert (2004)).
",3.1 Typical Fillers,[0],[0]
"Given the co-occurrence count Ovrf of the verb v, a syntactic relation r and the filler f , we computed the expected count Evrf under the assumption of statistical independence:
PLMI(v, r, f) = log ( Ov,r,f Ev,r,f ) ∗Ov,r,f (1)
",3.1 Typical Fillers,[0],[0]
"From the ranked list of (v,r,f) tuples, for each slot, we selected as typical fillers the top k lexemes with the highest PLMI scores (see examples in Table 1, Typical Fillers column).",3.1 Typical Fillers,[0],[0]
"In our experiments, we report results for k = {10, 30, 50}.",3.1 Typical Fillers,[0],[0]
"To represent the typical fillers, the candidate fillers and the verb-specific role prototypes (which are obtained by summing their typical filler vectors), we built a syntax-based DSM.",3.2 Role Prototype Vectors,[0],[0]
"This includes relation:word contexts, like sbj:dog, obj:apple, etc..
Contexts were weighted with Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990), Bullinaria and Levy (2012), Levy et al. (2015)).",3.2 Role Prototype Vectors,[0],[0]
"Given a context c and a word w, the PPMI is defined as follows:
PPMI(w, c) = max(PMI(w, c), 0) (2)
PMI(w, c) = log
( P (w, c)
P (w)P (c)
)",3.2 Role Prototype Vectors,[0],[0]
"= log ( |w, c|D |w||c| ) (3)
where w is the target word, c is the given context, P(w,c) is the probability of co-occurrence, and D is the collection of observed word-context pairs.8
8A variant of this DSM weighted with PLMI (which is simply the PPMI multiplied by the word-context frequency) was also built, but because of its lower and inconsistent per-
The context c of the prototype vector P representing a thematic role has a value corresponding to the sum of the values of c for each of the k typical fillers used to build P .",3.2 Role Prototype Vectors,[0],[0]
The contexts of P are then sorted according to their weight.,3.2 Role Prototype Vectors,[0],[0]
"Desirably, the highest-ranking contexts for a role prototype will be those that are more strongly associated with many of its typical fillers.",3.2 Role Prototype Vectors,[0],[0]
"Such second order contexts correspond to the most salient features of the verb-specific thematic role, as they are salient for many role fillers (some examples are reported in Table 1, Top Second Order Contexts column).
",3.2 Role Prototype Vectors,[0],[0]
"In summary, we built centroid vectors for our verb-specific thematic roles by means of second order contexts, which are first order",3.2 Role Prototype Vectors,[0],[0]
dependencybased contexts of the most typical fillers of a verbspecific role.,3.2 Role Prototype Vectors,[0],[0]
"Since we are interested only in the most salient contexts, we ranked the centroid contexts according to their PPMI score, and we took the resulting rank as a distributional characterization of the thematic roles.",3.2 Role Prototype Vectors,[0],[0]
Filtering the prototype dimensions according to syntactic criteria might be useful to improve our role representations.,3.3 Filtering the Contexts,[0],[0]
"It is, indeed, reasonable to hypothesize that predicates co-occurring with the typical patients of a verb are more relevant for the characterization of its patient role than – let’s say – prepositional complements, as they correspond to other actions that are typically performed on the same patients.
",3.3 Filtering the Contexts,[0],[0]
"Imagine that apple, pizza, cake etc. are among the most salient fillers for the OBJ slot of to eat, and that OBJ-1:slice-v,",3.3 Filtering the Contexts,[0],[0]
"OBJ-1:devour-v, SBJ:kidn, INSTRUMENT:fork-n, LOCATION:table-n are some of the most salient contexts of the prototype.9",3.3 Filtering the Contexts,[0],[0]
Things that are typically sliced and/or devoured are more likely to be good fillers for the patient role to eat than things that are simply located on a table or that are patients of actions performed by kids.,3.3 Filtering the Contexts,[0],[0]
"To test this hypothesis, we evaluated the performance of the system in three different settings, each of which selecting:
formance we will not discuss it further.",3.3 Filtering the Contexts,[0],[0]
"Santus et al. (2016c) previously showed that their rank-based measure performs worse on PLMI-weighted vectors, as they are biased towards frequent contexts.
",3.3 Filtering the Contexts,[0],[0]
"9Our DSM also makes use of inverse syntactic dependencies: target SYN-1 context means that target is linked to context by the dependency relation SYN (e.g. meal OBJ-1 devour means that meal is OBJ of devour).
",3.3 Filtering the Contexts,[0],[0]
"• only predicates in a subject/object relation (SO setting);
• only prepositional complements (PREP setting);
• both of them (ALL setting).",3.3 Filtering the Contexts,[0],[0]
Our hypothesis is that fillers whose salienceranked vector has a large overlap with the prototype representation should have a high thematic fit.,3.4 Computing the Thematic Fit,[0],[0]
"Such overlap should take into account not only the number of shared features, but also their respective ranks in the salience-ranked vectors.
",3.4 Computing the Thematic Fit,[0],[0]
"When the prototype has been computed and the candidate filler vector has also been sorted, we can measure the Weighted Overlap by adapting APSyn (Santus et al., 2016a,b,c) to our needs:
WO(wx, wy) = ∑
∀f",3.4 Computing the Thematic Fit,[0],[0]
"(x[1:N]∩y[1:N])
1
avg(rx(f), ry(f))",3.4 Computing the Thematic Fit,[0],[0]
"(4)
where for every feature f in the intersection between the top N features of the sorted vectors x, x[1:N ], and y, y[1:N ], we sum 1 divided by the average rank of the shared feature in x and y, rx(f) and ry(f) (N is a tunable parameter).
",3.4 Computing the Thematic Fit,[0],[0]
"This measure assigns the maximum score to vectors sharing exactly the same dimensions, in the same salience ranking.",3.4 Computing the Thematic Fit,[0],[0]
"The lower the rank of a shared context in the sorted vector, the smaller its contribution to the thematic fit score.",3.4 Computing the Thematic Fit,[0],[0]
"If the feature set intersection is empty, the score will be 0.
Differently from cosine similarity, which conflates multiple senses, measuring the Weighted Overlap between prototype and candidate filler can improve the estimation of the thematic fit by favoring the appropriate word senses: for example, for a verb-argument pair like embracev–communism-n, communism-n is likely to intersect and to increase the saliency (through the average rank) only of the second-order features of embrace-v referring to its abstract sense.",3.4 Computing the Thematic Fit,[0],[0]
Datasets.,4 Experiments,[0],[0]
"We tested our method on three popular datasets for thematic fit estimation, namely McRae et al. (1998), Ferretti et al. (2001) and Padó (2007).",4 Experiments,[0],[0]
All the datasets contain human plausibility judgments for verb-role-filler triples.,4 Experiments,[0],[0]
"McRae and Padó include scores for agent and patient roles, whereas Ferretti includes instruments and locations (see Table 2 for the coverage of each system for the datasets).",4 Experiments,[0],[0]
Metrics.,4 Experiments,[0],[0]
Performance is evaluated as the Spearman correlation between the scores of the systems and the human plausibility judgments.,4 Experiments,[0],[0]
Fillers.,4 Experiments,[0],[0]
"In order to make our results more comparable with previous studies, the typical fillers for each verb role were extracted from the TypeDM tensor of the Distributional Memory framework (see Section 3.1).10 Those were the same fillers used by Baroni and Lenci (2010) and Greenberg et al. (2015b).",4 Experiments,[0],[0]
DSM.,4 Experiments,[0],[0]
"Distributional information is derived from the concatenation of two corpora: the British National Corpus (Leech, 1992) and Ukwac (Baroni et al., 2009).",4 Experiments,[0],[0]
"Both were parsed with the Maltparser (Nivre and Hall, 2005).",4 Experiments,[0],[0]
"From this concatenation, we built a dependency-based DSMs, weighted with PPMI, containing 20,145 targets (i.e. nouns and verbs with frequency above 1000) and 94,860 contexts.",4 Experiments,[0],[0]
"The syntactic relations taken into account were: sbj, sbj-1, obj, obj-1, at-1, in-1, on-1, with-1.",4 Experiments,[0],[0]
Settings.,4 Experiments,[0],[0]
"To prove our hypotheses and verify the consistency of the system, we tested a large range of settings, varying:
10http://clic.cimec.unitn.it/dm/
• the number of fillers used to build the prototype, with the most typical values in the literature ranging between 10 and 50.",4 Experiments,[0],[0]
"We report the results for 10, 30 and 50 fillers
• the types of the dependency relations used for calculating the overlap: we report results for the SO, PREP and ALL settings;
• the value of N , that is the number of top contexts that we take into account when computing the weighted overlap.",4 Experiments,[0],[0]
"Table 3 reports the scores for our best setting, while the performances for other values of N are discussed in the Section 5.
",4 Experiments,[0],[0]
Baseline and State of the Art.,4 Experiments,[0],[0]
"As a baseline, we use the thematic fit model by Baroni and Lenci (2010), with no ranking of the features of the prototypes and with vector cosine as a similarity metric.11 Results are reported for 10, 30 and 50 fillers.",4 Experiments,[0],[0]
"For reference, we also report the results of state-of-the-art models, both the unsupervised (Baroni and Lenci, 2010; Sayeed and Demberg, 2014; Greenberg et al., 2015b) and the supervised ones (Tilk et al., 2016).",4 Experiments,[0],[0]
Table 3 describes the performance of the best setting (weight: PPMI; N=2000).,5 Results,[0],[0]
"In the first three rows, the table shows the scores obtained by our
11This baseline is equivalent to the approach of Baroni and Lenci (2010), except for the fact that it is applied on a standard dependency-based DSM and not on TypeDM, which combines dependency links and handcrafted lexico-syntactic patterns: see Section 2.
system varying the types of dependency contexts (i.e. ALL, SO, PREP) and the number of fillers considered for the prototype (i.e. 10, 30 and 50).",5 Results,[0],[0]
"The other rows respectively show i) the scores obtained by calculating the vector cosine between the role prototype vector (i.e. the vector obtained by summing the most typical fillers, with no salience ranking of the dimensions) and the candidate filler vector and ii) the scores reported in the literature for the best unsupervised and supervised models.
",5 Results,[0],[0]
"At a glance, our best scores always outperform the reimplementation of Baroni and Lenci, being mostly competitive with the state of the art models.",5 Results,[0],[0]
"More precisely, for agents and patients the performance is close to the reported scores for DM, when only predicates are used in the WO calculation, as hypothesized in Section 3.3.",5 Results,[0],[0]
The neural network of Tilk and colleagues retains a significant advantage on our models only for the McRae dataset.,5 Results,[0],[0]
"Our system, however, shows a remarkable improvements on the Ferretti’s datasets, and specifically on Ferretti-Instruments, when only complements are used (see Section 3.3), outperforming even the supervised and more complex model by Tilk et al. (2016), which has access to semantic roles information.",5 Results,[0],[0]
"Compared to the other unsupervised models, our system has a statistically significant advantage over Baroni and Lenci (2010) on the locations dataset and over Sayeed and Demberg (2014) on the locations and on the instruments dataset (p < 0.05).12
At the best of our knowledge, the result for the
12p-values computed with Fisher’s r-to-z transformation.
instruments is the best reported until now in the literature.",5 Results,[0],[0]
"This is particularly interesting because – as pointed out by Sayeed and Demberg (2014) – instruments and locations are difficult to model for a dependency-based system, given the ambiguity of prepositional phrases (e.g. with does not only encode instruments, but it can also encode other roles, such as in I ate a pizza with Mark).",5 Results,[0],[0]
We think this is the main reason behind the different trend observed for the Instruments datasets with respect to the number of the fillers (see Table 3 and Figure 1).,5 Results,[0],[0]
"Unlike all the other datasets, instrument prototypes built with more fillers tend to be more noisy and therefore to pull down both the vector cosine and WO performance (this is partially true also for locations, where the performances – for cosine and WO with a lower number of contexts – drop with more than 30 fillers: see Figure 1).",5 Results,[0],[0]
"Systems based on semantic role labeling have an advantage in this sense, as they do not have to deal with prepositional ambiguity.
",5 Results,[0],[0]
"Our results show that, by weighting and filtering the features of the role prototype, dependency-based approaches can be successful in modeling roles other than agent and patient, eventually dealing also with the ambiguity of prepositional phrases.
",5 Results,[0],[0]
Settings.,5 Results,[0],[0]
"Apart from the above-mentioned exceptions, the best scores are obtained building the prototypes with a higher number of fillers, typically with 50, and calculating the WO only with a syntactically-filtered set of contexts.",5 Results,[0],[0]
"More specifically, Padó and McRae benefit from the calculation of WO using only second order subject-object predicates (i.e. SO), while FerrettiInstruments and Ferretti-Locations benefit from the exclusive use of prepositional complements (i.e. PREP).",5 Results,[0],[0]
"On the other hand, the opposite setting (e.g. SO for Ferretti-Instruments and FerrettiLocations and PREP for Padó and McRae) leads to much lower scores, whereas the full vectors (i.e.
ALL) tend to have a stable-but-not-excellent performances on all datasets.
",5 Results,[0],[0]
"As briefly mentioned above, in our experiments, we tested both PPMI and PLMI as weighting measures.",5 Results,[0],[0]
"Table 3 only reports PPMI scores because it performs more regularly than PLMI, whose behaviour is often unpredictable.
",5 Results,[0],[0]
"A parameter that has an impact on the performance of our system is the value of N , which is the number of second order contexts that are considered when calculating the WO.",5 Results,[0],[0]
"We have noticed that the performance of WO is directly related to the growth of N , and this can be noticed in Figure 1, where WO is plotted for the different values of N with every combination of dataset and number of fillers.",5 Results,[0],[0]
"For space reasons, the plot only contains the performance for the best type of second order contexts for each dataset (i.e. SO for Padó and McRae and COMP for FerrettiLocations and Ferretti-Instruments).",5 Results,[0],[0]
"As it can be seen in Figure 1, the scores of WO tend to grow with the growth of N in all datasets.",5 Results,[0],[0]
"Interestingly, they are largely above the competitive baseline in most of the cases, the only exceptions being Padó (where a large N is necessary to outperform the baseline) and Ferretti-Locations with 10 fillers (prepositional ambiguity might have caused the introduction of noisy fillers among the top ones).
",5 Results,[0],[0]
Agent & Patient.,5 Results,[0],[0]
"In order to further evaluate our system, we have split Padó and McRae datasets into agent and patient subsets.",5 Results,[0],[0]
Figure 2 describes the performance of WO and vector cosine baseline while varying N and the number of fillers.,5 Results,[0],[0]
"The plot shows a clearly better performance of WO for the agent role (i.e. subject), especially when N is equal or over 1000 (note that the value of N has little impact in the agent subset of the McRae dataset).",5 Results,[0],[0]
"Such advantage, however, is reduced for the patient role (i.e. object).",5 Results,[0],[0]
"This is particularly interesting because we do not observe large drops in performance for the vector cosine
between agent and patient role (except for Padó, k = 10).",5 Results,[0],[0]
"The drop is particularly noticeable in Padó, a dataset which has several non-constraining verbs (especially for the patient role: a similar observation was also made by Tilk et al. (2016)).",5 Results,[0],[0]
"As the constraints on the typical fillers of such verbs are very loose, we hypothesize that it is more difficult to find a set of salient features that are shared by many typical fillers.",5 Results,[0],[0]
"Therefore, estimations based on the whole vectors turn out to be more reliable.",5 Results,[0],[0]
"This can be confirmed by looking at the worst correlated words reported in Lexemes column, in Table 4.",5 Results,[0],[0]
"We performed an error analysis to verify – for the best settings of WO in each dataset – the correlation between vector cosine and WO scores (see Table 5), and the peculiarities of the entries with the strongest and the weakest correlation (see Table 4).
",5.1 Error Analysis,[0],[0]
"We found that WO and vector cosine always have a high correlation (i.e. above 0.80), with the highest correlations reported for McRae and Ferretti-Instruments.",5.1 Error Analysis,[0],[0]
"Looking at Table 4 we can also observe that:
• the average gold value of the 35 most (4.65) and least (4.56) correlated items does not substantially differ from the average gold value calculated on the full datasets (4.31), meaning that the distribution of likely and unlikely fillers among the best and worst correlated items is similar to the one in the datasets (i.e. no bias can be identified);
• both measures have difficulties on the same test items (probably because of loose semantic constraints), but report their best performances on different pairs (see Overlap and Lexemes columns);
• syntactically, vector cosine correlates better with objects, while WO is more balanced between objects and subjects, often showing a preference for the latter (see the distribution in Syntax column).",5.1 Error Analysis,[0],[0]
"In this paper, we have introduced an unsupervised distributional method for modeling predicateargument thematic fit judgments which works purely on syntactic information.
",6 Conclusions,[0],[0]
"The method, inspired by cognitive and psycholinguistic findings, consists in: i) extracting and filtering the most salient second order contexts for each verb-specific role, i.e. the most salient semantic dimensions of typical verb-specific role fillers; and then ii) estimating the thematic fit as a weighted overlap between the top features of the candidate fillers and of the prototypes.",6 Conclusions,[0],[0]
"Once tested on some popular datasets of thematic fit judgments, our method consistently outperforms a baseline re-implementing the thematic fit model of Baroni and Lenci (2010) and proves to be competitive with state of the art models.",6 Conclusions,[0],[0]
"It even registered the best performance on the Ferretti-Instruments dataset and it is the second best on the FerrettiLocations, which were known to be particularly hard to model for dependency-based approaches.
",6 Conclusions,[0],[0]
"Our method is simple, economic and efficient, it works purely on syntactic dependencies (so it does not require a role-labeled corpus) and achieves good results even with no supervised training.",6 Conclusions,[0],[0]
"Finally, it offers linguistically and cognitively grounded insights on the process of prototype creation and contextual feature salience, preparing the ground for further speculations and optimizations.",6 Conclusions,[0],[0]
"For example, future work might aim at identifying strategies for tuning the parameter N to account for the different degrees of selectivity of each verb-specific role.",6 Conclusions,[0],[0]
"Another possible extension would be the inclusion of a mechanism for updating the role prototypes depending on how the other roles are filled, which would be the key for a more realistic and dynamic model of thematic fit expectations (Lenci, 2011).",6 Conclusions,[0],[0]
"We would like to thank the anonymous reviewers for their helpful suggestions.
",Acknowledgments,[0],[0]
This work has been carried out thanks to the support of the A*MIDEX grant (nANR-11-IDEX0001-02) funded by the French Government “Investissements d’Avenir” program.,Acknowledgments,[0],[0]
"In this paper, we introduce a new distributional method for modeling predicateargument thematic fit judgments.",abstractText,[0],[0]
"We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes.",abstractText,[0],[0]
"Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-theart system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems.",abstractText,[0],[0]
"Moreover, it provides an explicit representation of the features characterizing verbspecific semantic roles.",abstractText,[0],[0]
Measuring Thematic Fit with Distributional Feature Overlap,title,[0],[0]
"deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2colbased convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memoryoverhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory subsystem efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.",text,[0],[0]
"Deep neural network (DNN) consists of many layers to perform a task such as image classification/recognition, speech recognition, natural language translation, and so on.",1. Introduction,[0],[0]
"Among these layers, the convolution layer is one of the most important, but the slowest and most memory-intensive ones in advanced/modern convolutional DNN (Abuzaid et al., 2015; Chen et al., 2016; Cong & Xiao, 2014; Denton et al., 2014; Park et al.,
1IBM T. J. Watson Research Center, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Minsik Cho <minsikcho@us.ibm.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016a; Vasilache et al., 2014).",1. Introduction,[0],[0]
"To address the performance issues in convolutional layers, efficient/approximation algorithms have been proposed (Chellapilla et al., 2006; Denton et al., 2014; Jaderberg et al., 2014; Jia, 2014; Vasilache et al., 2014), tailed implementations for limited cases have been actively investigated (Lavin, 2015), and industrial-strength libraries are offered (Chetlur et al., 2014).
",1. Introduction,[0],[0]
"However, the previous approaches have not directly addressed the memory consumption problem.",1. Introduction,[0],[0]
"This is becoming a critical issue as DNNs are getting in end-point devices with limited memory (e.g., mobile/IOT devices) (Chen et al., 2015; Collins & Kohli, 2014; Gong et al., 2014; Kim et al., 2015; Lebedev et al., 2014; Wang & Cheng, 2016) so as to minimize response delay (e.g., better user experience) and network overhead (Han et al., 2015; Lane et al., 2016; 2015).",1. Introduction,[0],[0]
"On the other hand, the reduced memory consumption leads to smaller SRAM usage, which can save energy consumption (e.g., leakage current) on mobile devices (Park et al., 2015).",1. Introduction,[0],[0]
"Moreover, memory footprint itself has critical impact on convolution computation efficiency (Li et al., 2016; Park et al., 2016b).",1. Introduction,[0],[0]
"Therefore, minimizing memory footprint in convolution is critical for future deep-learning applications on wide variety of devices and platforms.
",1. Introduction,[0],[0]
"In this paper, we propose a new memory-efficient convolution algorithm, MEC which can reduce memory-overhead and further improve the performance of computing convolution in DNN.",1. Introduction,[0],[0]
"MEC uses a simple yet novel way of lowering the input matrix in a highly compact way, while still exploiting fast matrix-matrix multiplication available in a highly-optimized package such as BLAS (Jia, 2014).",1. Introduction,[0],[0]
"The reduced memory footprint improves memory sub-system efficiency (i.e., improves cache locality), so that MEC accelerates the convolution computation itself without compromising accuracy.",1. Introduction,[0],[0]
"Through extensive experiments on both mobile and server platforms with CPU/GPU, we show that MEC can be a very generic/efficient algorithm suitable to various platforms with memory constraints.",1. Introduction,[0],[0]
"Further, the key ideas in MEC should be beneficial/complementary to any variant of conventional im2col-based convolution by reducing either memory consumption or memory-bus traffic (i.e., less traffic from global memory to shared memory on GPU) (Chellapilla et al., 2006; Chetlur et al., 2014; Jia,
Table 1. Notations.
",1. Introduction,[0],[0]
"a : b SEQUENCE {a, a+ 1, ... b− 1} A[a, b] MATRIX ELEMENT A[a : b, c :",1. Introduction,[0],[0]
"d] SUB-MATRIX A[i, j], i ∈",1. Introduction,[0],[0]
"a : b, j ∈ c :",1. Introduction,[0],[0]
"d
I INPUT TENSOR in × ih ×",1. Introduction,[0],[0]
iw × ic K KERNEL TENSOR kh,1. Introduction,[0],[0]
×,1. Introduction,[0],[0]
kw ×,1. Introduction,[0],[0]
ic × kc,1. Introduction,[0],[0]
O OUTPUT TENSOR in ×,1. Introduction,[0],[0]
oh × ow × kc L LOWERED TENSOR in ×,1. Introduction,[0],[0]
ow × ih × kw ×,1. Introduction,[0],[0]
"ic sh, sw KERNEL STRIDE
2014).
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
We review related works and present preliminaries in Section 2.,1. Introduction,[0],[0]
"Section 3 presents our proposed algorithm, MEC.",1. Introduction,[0],[0]
Experimental results are in Section 4.,1. Introduction,[0],[0]
Section 5 concludes this paper.,1. Introduction,[0],[0]
Notation used in this paper is listed in Table 1.,2.1. Notations,[0],[0]
"For integers we use small letters, for tensors and matrices we use capital letters.",2.1. Notations,[0],[0]
We adopt the C-language convention as representing tensors and matrices in row-major order.,2.1. Notations,[0],[0]
"For example, a p×q×r tensor is an array of pqr elements.",2.1. Notations,[0],[0]
"The array can be interpreted as consisting of p sections, each divided into q subsections, each having r elements.",2.1. Notations,[0],[0]
"The same array can also be interpreted as p × qr matrix, or as pq × r matrix, etc.",2.1. Notations,[0],[0]
"We specifically interpret a tensor as a matrix when it requires matrix operations, otherwise (i.e., for data movement) we keep the tensor form.",2.1. Notations,[0],[0]
"If we work with a math library, such as cuBLAS (cuBLAS), which requires columnmajor order, then we still use the same row-major representation, but interpret all matrices as being transposed.
",2.1. Notations,[0],[0]
We use the notation a : b to denote a sub-matrix.,2.1. Notations,[0],[0]
"Thus, an m×n matrix could be written as A[0 : m, 0 : n].",2.1. Notations,[0],[0]
"The most common form of a sub-matrix will be of the form A[i : i+p, j : j+q].",2.1. Notations,[0],[0]
"It is a p×q sub-matrix with top left corner at the element A[i, j], which can be easily represented in the BLAS interface without moving any elements by having leading dimension ld = n.
The subject of this paper is 2-dimensional convolution",2.1. Notations,[0],[0]
"O = I ⋆ K with strides sh, sw.",2.1. Notations,[0],[0]
For simplicity of explanation any padding with zeroes is assumed to have been already applied to the input I .,2.1. Notations,[0],[0]
"The output matrix O will have the dimensions
oh,w = ih,w − kh,w
sh,w + 1 (1)",2.1. Notations,[0],[0]
"Due to the importance of DNN, several techniques for efficient convolution computation have been proposed (Chetlur et al., 2014; Perkins, 2016).",2.2. Previous Work,[0],[0]
"The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).",2.2. Previous Work,[0],[0]
"MEC provides the same functionality with reduced memory requirements.
",2.2. Previous Work,[0],[0]
"• im2col-based convolution transforms/lowers the input matrix into a Toeplitz matrix with redundancy
(a.k.a, lowered matrix) such that convolution can be performed as fast matrix-matrix multiplication, which can take advantage of highly optimized linear algebra packages including BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014).
",2.2. Previous Work,[0],[0]
"• FFT-based convolution relies on the fact that convolution can be done as simple multiplication in the fre-
quency domain.",2.2. Previous Work,[0],[0]
"However, FFT-based convolution incurs memory-overhead because all the kernels must be padded to be at the same size as the input matrix.",2.2. Previous Work,[0],[0]
"Thus, memory-overhead becomes really high when kernels are relatively smaller (e.g., 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).
",2.2. Previous Work,[0],[0]
"• Winograd-based convolution is based on the Coppersmith-Winograd algorithm (Winograd, 1980)
which shows how to reduce multiplication counts at a cost of more addition counts and a large number of intermediate products.",2.2. Previous Work,[0],[0]
"It is shown in (Lavin, 2015; Park et al., 2016a) that Winograd-based convolution can be efficient for small kernels on GPU.
",2.2. Previous Work,[0],[0]
"In contrast to the above schemes, which do not degrade accuracy, various approximation strategies have been proposed including low-rank/monochromatic approximation (Denton et al., 2014; Jaderberg et al., 2014), vector quantization (Gong et al., 2014), finetuning (Lebedev et al., 2014), and DCT (Discrete Cosine Transform)/hashing (Lebedev et al., 2014).",2.2. Previous Work,[0],[0]
"In this section, we propose our algorithm for convolution, MEC, with detailed examples.",3. Algorithm,[0],[0]
"The main goal of MEC is to reduce memory-overhead during convolution, which can be beneficial for any convolutional DNN in three aspects:
• MEC can enable training or inferencing with a larger model for a given memory capacity.
",3. Algorithm,[0],[0]
"• MEC can allow larger mini-batch sizes to speedup turn-around/per-epoch-latency during training.
",3. Algorithm,[0],[0]
"• MEC can accelerate computation by improving memory sub-system efficiency (e.g. more cache hits).
",3. Algorithm,[0],[0]
"In contrast to the widely-adopted im2col-based convolution (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014), MEC performs compact/BLAS-friendly lowering such that memory-overhead can be minimized without degrading performance/accuracy.",3. Algorithm,[0],[0]
"Section 3.1 motivates MEC, and Section 3.2 highlights the key idea in MEC.",3. Algorithm,[0],[0]
Section 3.3 formally presents MEC with implementation details.,3. Algorithm,[0],[0]
"In this section, we review im2col-based convolution and its pros and cons with Fig. 1 which sketches direct convolution in (a) and im2col-based convolution using BLAS in (b).",3.1. Motivation,[0],[0]
"In direct convolution, one element of the output matrix O is produced by a dot-product between the kernel K and a sub-matrix of the input I .",3.1. Motivation,[0],[0]
The sub-matrices are obtained by sliding K over I in both dimensions.,3.1. Motivation,[0],[0]
"Each subsequent sub-matrix is obtained by sliding the distance sh or sw, respectively.",3.1. Motivation,[0],[0]
"For example, Fig. 1 (a) shows two sub-matrices in gray and dotted boxes w.r.t.",3.1. Motivation,[0],[0]
"the 3 × 3 kernel are processed to generate the corresponding output values in gray and dotted boxes (i.e., 3 and 4), respectively.
",3.1. Motivation,[0],[0]
Direct convolution is simple and straightforward without memory-overhead.,3.1. Motivation,[0],[0]
"However, it is known that the same convolution can be done more efficiently with a lowered matrix (a.k.a. im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig. 1 (b).",3.1. Motivation,[0],[0]
"Specifically, each sub-matrix instance w.r.t.",3.1. Motivation,[0],[0]
K is linearized into a row of the lowered matrix L as in (b).,3.1. Motivation,[0],[0]
"For example, the gray and dotted sub-matrices in (a) are transformed into the gray and dotted rows in (b), respectively.",3.1. Motivation,[0],[0]
"Then the output ma-
trix O = L × K, can be computed efficiently by optimized libraries (cuBLAS; Kågström et al., 1998; MKL; OpenBLAS).",3.1. Motivation,[0],[0]
"im2col-based convolution is generic enough to be used in any DNN on both mobile/IoT and high-end platforms (Chetlur et al., 2014; Lane et al., 2015).
",3.1. Motivation,[0],[0]
"The major drawback of im2col-based convolution is that it comes with memory-overhead of temporarily storing the lowered matrix L with dimension
inohow × khkwkc (2)
which shows that the memory requirement grows quadratically with problem size.",3.1. Motivation,[0],[0]
"The example in Fig. 1 (b) shows that the lowered matrix has size 25×9, which is even lager than the original input matrix.",3.1. Motivation,[0],[0]
"MEC mainly aims to perform the same convolution yet with less memory-overhead, while improving computational efficiency.",3.1. Motivation,[0],[0]
"In this section, we highlight the key idea in our memoryefficient convolution algorithm, MEC based on a compact lowering scheme.",3.2. MEC Overview,[0],[0]
The main reason why the im2colbased algorithm has large memory-overhead is because there is a significant amount of redundancy in the lowered matrix when sh or sw is small and K is large.,3.2. MEC Overview,[0],[0]
"And, the overhead becomes even worse when K is relatively smaller than I which occurs frequently in the state-ofthe-art DNN architectures (He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014; Szegedy et al., 2014).",3.2. MEC Overview,[0],[0]
"In order to reduce memory-overhead, therefore, it is critical to reduce the amount of redundancy in the lowered matrix and keep the computation pattern BLAS-compatible (otherwise, the poor computation itself may slow down the entire convolution).
",3.2. MEC Overview,[0],[0]
MEC overcomes such challenges by lowering multiple columns at once rather than each single individual submatrix w.r.t.,3.2. MEC Overview,[0],[0]
K. Consider the example in Fig. 2 for key ideas and details.,3.2. MEC Overview,[0],[0]
"MEC copies sub-matrices W (shaded in Fig. 2) of size ih × kw (which is 7× 3) into one row of L.
",3.2. MEC Overview,[0],[0]
"For example, A is the first partition of I , A = I[0 : 7, 0 : 3].",3.2. MEC Overview,[0],[0]
"Then, we slide W by sw (which is 1) to the right and create another partition B = I[0 : 7, 1 : 4].",3.2. MEC Overview,[0],[0]
"As we continue this process in Fig. 2, there will be 5 horizontal partitions, {A,B,C,D,E} in L eventually.",3.2. MEC Overview,[0],[0]
"The resulting lowered matrix, L has dimensions 5 × 21, which is 54% smaller than one in Fig. 1 with dimensions 25× 9.
",3.2. MEC Overview,[0],[0]
"Once the lowered matrix L is formed, MEC multiplies L by K in a way significantly different from im2col-based algorithms.",3.2. MEC Overview,[0],[0]
"MEC creates another set of vertical partitions, {P,Q,R, S, T} within L, where each partition is of size of ow × khkw (which is 5 × 9).",3.2. MEC Overview,[0],[0]
Each subsequent partition is obtained by shifting to the right by shkw (which is 3) elements.,3.2. MEC Overview,[0],[0]
"For example, P = L[0 : 5, 0 : 9] and Q = L[0 : 5, 3 : 12].",3.2. MEC Overview,[0],[0]
"Then each row of the output matrix O is the product between one of the partitions in {P,Q,R, S, T} and K. Rows in O in Fig. 2 are annotated with the corresponding source partitions.
",3.2. MEC Overview,[0],[0]
These multiplications rely on the BLAS gemm interface in three ways.,3.2. MEC Overview,[0],[0]
"First, the kh × kw matrix K is interpreted as a khkw × 1 matrix.",3.2. MEC Overview,[0],[0]
"Second, the partitions {P,Q,R, S, T} are specified by providing a pointer to the initial element and ld = ihkw, which is the entire length of one row of L. Thirdly, each row of O is formed by 5 separate gemm calls between {P,Q,R, S, T} and K.",3.2. MEC Overview,[0],[0]
"Although the number of gemm calls increases, the total number of mult/add operations remains identical to that of the im2col-based convolution, keeping computationally complexity same.
",3.2. MEC Overview,[0],[0]
"Intuitively, MEC eliminates the vertical redundancy in the conventional im2col-based convolution.",3.2. MEC Overview,[0],[0]
"Then it recovers the information by merely shifting the vertical partitions (i.e., P,Q,R, S, T ) by a constant interval.",3.2. MEC Overview,[0],[0]
These submatrix manipulations are made efficient by keeping the pattern BLAS compatible.,3.2. MEC Overview,[0],[0]
"The lowering in MEC is highly efficient as we move fewer elements from I to smaller L,
Algorithm 1 O = V anillaMEC(I,K, s)
1: Allocate O with ohow elements 2: Allocate L with owihkw elements 3: Interpret L as ow × ih",3.2. MEC Overview,[0],[0]
× kw tensor 4: for w ∈ 0 :,3.2. MEC Overview,[0],[0]
"ow, h ∈ 0 : ih in parallel do 5: L[w, h, 0 : kw] = I[h, sww : sww + kw] 6: end for 7: Interpret L as ow × ihkw matrix 8: Interpret K as khkw × 1 matrix 9:",3.2. MEC Overview,[0],[0]
Interpret O,3.2. MEC Overview,[0],[0]
"as oh × ow matrix
10: for h ∈ 0",3.2. MEC Overview,[0],[0]
": oh in parallel do 11: O[h, 0 :",3.2. MEC Overview,[0],[0]
"ow] = L[0 : ow, shkwh : shkwh+ khkw]×K 12: end for 13:",3.2. MEC Overview,[0],[0]
"Return O
compared with im2col-based convolution, saving memorybus traffic as well.
",3.2. MEC Overview,[0],[0]
The process is stated in Algorithm 1 where in = ic = kc = 1.,3.2. MEC Overview,[0],[0]
"It first allocates the output O and temporary L. The first loop in line 4 forms the matrix L, which copies kw consecutive elements from I to L, and all these copies can be done in parallel.",3.2. MEC Overview,[0],[0]
"The second loop in line 10 forms the output O. Each execution of the body is done by one gemm call, and those matrix multiplications can be parallelized.",3.2. MEC Overview,[0],[0]
"In this section, we present the complete MEC by extending Algorithm 1 to Algorithm 2 in order to handle channels (ic and kc) and mini-batches (in), and discuss the implementation details in the context of deep-learning (mainly about image format issue).",3.3. MEC Algorithm,[0],[0]
"Due to the compact lowering in MEC, it is computationally advantageous to use I in in × ih × iw × ic (or n-h-w-c) as in Table 2, because it ensures vertical redundant pixels to be eliminated and re-
covered in a contiguous memory space.
",3.3. MEC Algorithm,[0],[0]
"Algorithm 2 O = MEC(I,K, s)
1: Allocate O with inohowkc elements 2: Allocate L with inowihkwic elements 3: Interpret L as in ×",3.3. MEC Algorithm,[0],[0]
"ow × ih × kw × ic tensor 4: for n ∈ 0 : in, w ∈ 0 :",3.3. MEC Algorithm,[0],[0]
"ow, h ∈ 0 : ih in parallel do 5: L[n, w, h, 0 : kw, 0 :",3.3. MEC Algorithm,[0],[0]
"ic] = I[n, h, sww : sww+kw, 0 : ic] 6: end for 7: Interpret K as khkwic × kc matrix 8: if ow ≤ T and |O| ≤ |L| then 9: Interpret L as inow × ihkwic matrix
10:",3.3. MEC Algorithm,[0],[0]
Interpret O as oh × inowkc matrix 11: for h ∈ 0,3.3. MEC Algorithm,[0],[0]
": oh in parallel do 12: O[h, 0 :",3.3. MEC Algorithm,[0],[0]
"inowkc] = L[0 : inow, shkwich : shkwich+khkwic]×K 13: end for 14: Copy L = O 15:",3.3. MEC Algorithm,[0],[0]
Interpret L as,3.3. MEC Algorithm,[0],[0]
oh × in × owkc tensor 16:,3.3. MEC Algorithm,[0],[0]
Interpret O as in ×,3.3. MEC Algorithm,[0],[0]
"oh × owkc tensor 17: for n ∈ 0 : in, h ∈ 0",3.3. MEC Algorithm,[0],[0]
": oh in parallel do 18: O[n, h, 0 : owkc] = L[h, n, 0 : owkc] 19: end for 20: else 21: Interpret L as in matrices of ow × ihkwic 22:",3.3. MEC Algorithm,[0],[0]
"Interpret O as in matrices of oh × owkc 23: for n ∈ 0 : in, h ∈ 0",3.3. MEC Algorithm,[0],[0]
": oh in parallel do 24: O[n][h, 0 : owkc] =",3.3. MEC Algorithm,[0],[0]
"L[n][0 : ow, shkwich : shkwich+khkwic]×K 25: end for 26: end if 27:",3.3. MEC Algorithm,[0],[0]
Return O as in ×,3.3. MEC Algorithm,[0],[0]
"oh × owkc tensor
Based on I as in × ih × iw × ic, Algorithm 2 still has the same key idea in presence of channels and mini-batches.",3.3. MEC Algorithm,[0],[0]
"The lowering step lines 4-6 in Algorithm 1 is similar to
lines 4-6 in Algorithm 2.",3.3. MEC Algorithm,[0],[0]
"However, the parallel multiplication loop in lines 10-12 in Algorithm 1 extends to lines 8-25 in Algorithm 2 mainly due to the image format issue.
",3.3. MEC Algorithm,[0],[0]
"A direct extension of Algorithm 1 would interpret O as oh × inowkc matrix, and perform oh multiplications for convolution of the whole mini-batch.",3.3. MEC Algorithm,[0],[0]
"This leads to the output format h-n-w-c, which is different from the input format of I .",3.3. MEC Algorithm,[0],[0]
"This may be acceptable in DNNs, where each convolution layer is followed by a pooling layer expecting h-n-w-c format and generating the standard n-h-w-c format.",3.3. MEC Algorithm,[0],[0]
"However, it would be troublesome in a network where all layers expect and produce the n-h-w-c format.",3.3. MEC Algorithm,[0],[0]
"Therefore, we provide two solutions depicted in Fig. 3 to handle such format-related issues.
",3.3. MEC Algorithm,[0],[0]
Solution A (Lines 9 to 19 of Algorithm 2),3.3. MEC Algorithm,[0],[0]
"First we per-
form the direct extension of Algorithm 1 (lines 9 - 13) and end up with O in format h-n-w-c. Then, we transform O into n-h-w-c format (lines 14-19) where we repurpose L as an auxiliary space.
",3.3. MEC Algorithm,[0],[0]
Solution B (lines 21 to 25 of Algorithm 2),3.3. MEC Algorithm,[0],[0]
"We can han-
dle the in samples in the mini-batch separately as in line 21, resulting in inoh parallel/batched gemm calls with smaller inputs, as opposed to oh gemm calls with larger inputs.",3.3. MEC Algorithm,[0],[0]
"This will directly generate O in n-h-w-c.
In terms of complexity, both solutions perform the same number of floating point multiplications.",3.3. MEC Algorithm,[0],[0]
"In practice, however, the size of sub-matrices can impact performance, particularly on implementation-sensitive platform like GPU.",3.3. MEC Algorithm,[0],[0]
"Therefore, MEC tries to find a good trade-off between Solution A and B with a tunable parameter T in line 8.",3.3. MEC Algorithm,[0],[0]
"(In addition, Solution A is available only if L can be used as an auxiliary space, i.e. it is at least as large as O).",3.3. MEC Algorithm,[0],[0]
"T is a platform-dependent parameter (e.g., on CPU vs. GPU, or
on GPU-compute capability), and we found T around 100 to be a good threshold for latest GPUs.",3.3. MEC Algorithm,[0],[0]
"In this section, we analyze the memory saving in MEC over im2col-based convolution.",3.4. Analysis,[0],[0]
"The size of the lowered matrix, L in MEC is: inowihkwkc (3)
",3.4. Analysis,[0],[0]
"In comparison with the lowered matrix of im2col (see Eq. (2)), there is approximately a factor of kh.",3.4. Analysis,[0],[0]
"For a more exact comparison, let us form their difference R.
R = inkc(ohowkhkw",3.4. Analysis,[0],[0]
"− owihkw)
=",3.4. Analysis,[0],[0]
"inkcowkw(ohkh − ih) = inkcowkw( ih − kh
sh kh",3.4. Analysis,[0],[0]
+ kh,3.4. Analysis,[0],[0]
"− ih)
",3.4. Analysis,[0],[0]
"= inkcowkw(ih − kh)( kh sh − 1) (4)
Since ih > kh, MEC always reduces memory footprint as long as kh > sh (i.e., there is an overlap between kernel instances).",3.4. Analysis,[0],[0]
"Note that in case kh ≤ sh, there is no redundant information to eliminate.",3.4. Analysis,[0],[0]
"We implemented MEC for CPU/GPU in C++ with multithreaded OpenBLAS, OpenMP, and cuBLAS (cuBLAS) using single 32-bit precision.",4. Experimental Results,[0],[0]
"We also implemented a fully parallelized im2col-based convolution on CPU/GPU (Jia, 2014) with the same libraries.",4. Experimental Results,[0],[0]
"We compared MEC with other open-source convolution packages in C++, in order to make fair point-by-point comparison and accurately capture the memory-overhead and performance.",4. Experimental Results,[0],[0]
We downloaded an open-source FFT-based convolution (cuFFT; Theano-FFT) for GPU.,4. Experimental Results,[0],[0]
"We took an open-
source Winograd-based convolution (Falcon, 2016) and optimized it to reduce memory-overhead for CPU, and further modified/optimized it for GPU following (Lavin, 2015; Park et al., 2016a).",4. Experimental Results,[0],[0]
"The brief descriptions of the convolution algorithms in this section are as follows:
Conv.cpu Conventional im2col-based convolution for
CPU with openBLAS/openMP
Conv.gpu Conventional im2col-based convolution for
GPU with cuBLAS
Wino.cpu Winograd-based F (2×2, 3×3) convolution for CPU (applicable only when kh = kw = 3)
",4. Experimental Results,[0],[0]
"Wino.gpu Winograd-based F (2 × 2, 3 × 3) convolution for GPU (applicable only when kh = kw = 3)
FFT.gpu FFT-based convolution for GPU with cuFFT
MEC.cpu MEC for CPU with OpenBLAS/OpenMP
MEC.gpu MEC for GPU with cuBLAS
Note that it is performance-critical to combine multiple sgemm calls into a single cublasSgemmBatched call in MEC.gpu.",4. Experimental Results,[0],[0]
"When modifying/optimizing Wino.gpu, we tried to make the best trade-off between parallelism and memory-overhead (i.e., global memory) by utilizing register/shared-memory as much as possible, and ensured experiments representative.",4. Experimental Results,[0],[0]
"Please see Appendix for details on Wino.gpu optimization.
",4. Experimental Results,[0],[0]
"For thorough comparison, we built a comprehensive benchmark set consisting of 12 unique convolution layers, cv1-cv12 from various public DNNs (He et al., 2015; Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Szegedy et al., 2014) as in Table 2.",4. Experimental Results,[0],[0]
"The runtime in our experiments is measured as a wall-clock time by a standard C++ library, running each algorithm 10 times and reporting the average.",4. Experimental Results,[0],[0]
"Our experiments were performed on the two platforms:
Mobile Android phone with ARM7 (MSM8960) for user-
side inference and training (mini-bath size=1)
",4. Experimental Results,[0],[0]
"Server Linux server with Intel CPU (E5-2680) and Nvidia
GPU (P100) for inference and training (mini-bath size=32)
",4. Experimental Results,[0],[0]
"We present our results in Fig. 4, and made the following summaries:
• (a) plots the factor by which MEC.cpu improves memory-overhead and performance over Conv.cpu
for cv1 on Server-CPU.",4. Experimental Results,[0],[0]
"While the kernel K is fixed at
11×11, sh = sw varies from 1 to 10 on the x-axis.",4. Experimental Results,[0],[0]
We can clearly observe that both memory-overhead and runtime improve with a larger k/s ratio as explained in Eq.,4. Experimental Results,[0],[0]
"(4).
",4. Experimental Results,[0],[0]
• (b) supports that MEC can substantially reduce the memory-overhead.,4. Experimental Results,[0],[0]
"Compared with Conv.cpu, the im-
provement is as large as 3.4x with high k/s ratio, and is on average 3.2x.",4. Experimental Results,[0],[0]
"For cv6-cv12, MEC.cpu improves memory-overhead by 5.9x on average, compared with Wino.cpu.
",4. Experimental Results,[0],[0]
"• (c) shows that MEC.cpu is overall 20% faster than Conv.cpu on Mobile, yet can be over 90% faster
for some layers like cv6. MEC.cpu is faster than Wino.cpu on 5 benchmarks out of 7.
",4. Experimental Results,[0],[0]
"• (d) shows that on Server-CPU, MEC.cpu overall shows about 8.8x better runtime than Conv.cpu.
",4. Experimental Results,[0],[0]
"Compared with Wino.cpu, performance is highly dependent on the benchmarks: it is similar or faster for
cv7,cv8, and cv9.",4. Experimental Results,[0],[0]
"• (e) presents memory-overheads from various algo-
rithms on Server-GPU.",4. Experimental Results,[0],[0]
"MEC.gpu shows the least
As observed, MEC shows greater performance boost on Server-CPU than on Mobile or Server-GPU, because Server-CPU is very sensitive to memory-footprint due to the complex cache-architecture.",4. Experimental Results,[0],[0]
"For the example of cv10, we observed through Valgrind cache simulation (Valgrind) that the last-level cache miss in MEC.cpu is 0.3%, substantially smaller than 4% in Conv.cpu, on a default cache system.",4. Experimental Results,[0],[0]
"Mobile has tiny/simple caches, and GPU does not have a sophisticated memory sub-system (deep/big cache hierarchy) to benefit from large memory footprint reduction.",4. Experimental Results,[0],[0]
"Also, cuBLAS is highly optimized to efficiently use fast shared-memory.",4. Experimental Results,[0],[0]
"Overall, MEC is all-around player on both Mobile or Server-CPU/GPU that has no limitation on kernel configuration, incurs the least memory-overhead, yet offers high-performance.
",4. Experimental Results,[0],[0]
"In practice, some convolution layers appear more frequently than others.",4. Experimental Results,[0],[0]
"Therefore, we applied MEC.cpu and Conv.cpu to ResNet-101 in (He et al., 2015) and estimated the weighted impact on memory-overhead and runtime on Mobile as in Table 3, which shows that MEC.cpu can reduce the memory-overhead by 3x and improve runtime by 20% for a large scale convolutional DNN.",4. Experimental Results,[0],[0]
"In this paper, we presented MEC, a memory-efficient convolution algorithm for deep learning.",5. Conclusion,[0],[0]
We proposed a novel matrix lowering scheme to improve memory efficiency for MEC which also improves the computational efficiency due to reduced memory footprint.,5. Conclusion,[0],[0]
"We can clearly observe through extensive experiments that MEC needs the least memory-overhead, yet offers high-performance in most cases on both mobile and server platforms without any restriction, positioning MEC as an attractive convolution engine on various platforms.",5. Conclusion,[0],[0]
"MEC is well suited for DNN-based applications in memory-constrained environment such as mobile/IoT, while allowing to increase the learning capacity of DNN on high-end server systems.
",5. Conclusion,[0],[0]
"Appendix
In this appendix, we sketch Wino.gpu optimizations in Section 4 in detail.",5. Conclusion,[0],[0]
"Our Wino.gpu are all hand-tuned/fullyunrolled F (2 × 2, 3 × 3) which can fit into the instruction cache in GPU (Lavin, 2015) for maximum performance.",5. Conclusion,[0],[0]
"We started with an open-source package (Falcon, 2016) and followed the techniques in (Lavin, 2015; Park et al., 2016a) to improve it for GPU.",5. Conclusion,[0],[0]
"We mainly focused on the high-level optimization including the following:
• For a given input matrix, all transformed kernel and input matrices across all kernels/channels are com-
puted in full parallel for maximum GPU utilization.
",5. Conclusion,[0],[0]
"• The output matrix is computed by multiplying all pairs of the transformed kernel and input matrices in full
parallel for maximum GPU utilization.
",5. Conclusion,[0],[0]
"• All intermediate products from multiplications are kept in thread registers first and reduced using shared-
memory.
",5. Conclusion,[0],[0]
"• All loops are manually unrolled for maximum performance.
",5. Conclusion,[0],[0]
"• Read-only cache ( ldg) is actively used when computing the output matrix with transformed kernel and
input matrices which are shared across blocks.",5. Conclusion,[0],[0]
"Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed.",abstractText,[0],[0]
Direct convolution is simple but suffers from poor performance.,abstractText,[0],[0]
"As an alternative, multiple indirect methods have been proposed including im2colbased convolution, FFT-based convolution, or Winograd-based algorithm.",abstractText,[0],[0]
"However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption.",abstractText,[0],[0]
"In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memoryoverhead substantially and accelerates convolution process.",abstractText,[0],[0]
"MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed.",abstractText,[0],[0]
"Additionally, the reduced memory footprint improves memory subsystem efficiency, improving performance.",abstractText,[0],[0]
"Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.",abstractText,[0],[0]
MEC: Memory-efficient Convolution for Deep Neural Network ,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 178–187, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Automated implicit semantic role labeling (iSRL) has emerged as a novel area of interest in the recent years.,1 Introduction,[0],[0]
"In contrast to traditional SRL, which aims to detect events (e.g., verbal or nominal predicates) together with their associated semantic roles (agent, theme, recipient, etc.) as overtly realized in the current sentence, iSRL extends this analysis with locally unexpressed linguistic items.",1 Introduction,[0],[0]
"Hence, iSRL requires to broaden the scope beyond isolated sentences to the surrounding discourse.",1 Introduction,[0],[0]
"As an illustration, consider the following example from Roth and Frank (2013):
El Salvador is now the only Latin American country which still has troops in [Iraq].",1 Introduction,[0],[0]
"Nicaragua, Honduras and the Dominican Republic have withdrawn their troops [∅].
",1 Introduction,[0],[0]
"In the second sentence, a standard SRL parser would ideally identify withdraw as the main verbal predicate.",1 Introduction,[0],[0]
"In its thematic relation to the other words within the same sentence, all countries serve as the overtly expressed (explicit) agents, and are thus labeled as arguments A0.1 Semantically, they are the action performers, whereas
1For details on all PropBank labels used in our study, see Palmer et al. (2005).
",1 Introduction,[0],[0]
troops would carry the patient role A1 as the entity which undergoes the action of being withdrawn.,1 Introduction,[0],[0]
"However, given these explicit role annotations for A0 and A1 in the second sentence, the standard system would definitely fail to infer the underlying, linguistically unexpressed, i.e., non-overt realization of an implicit argument of withdraw (denoted by [∅]) about source information.",1 Introduction,[0],[0]
"Its corresponding realization is associated with Iraq in the preceding sentence, which is outside of the scope of any standard SRL parser.",1 Introduction,[0],[0]
"The resulting implicit role has the label A2.
",1 Introduction,[0],[0]
Many role realizations are suppressed on the surface level.,1 Introduction,[0],[0]
"The automated detection of such implicit roles and their fillers, which are also called null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005), is a challenging task.",1 Introduction,[0],[0]
"Yet, if uncovered, NIs provide highly beneficial ‘supplementary’ information which in turn can be incorporated into practical, downstream NLU applications, like automated text summarization, recognizing textual entailment or question answering.
",1 Introduction,[0],[0]
"Current issues in iSRL Corpus data with manually annotated implicit roles is extremely sparse and hard to obtain, and annotation efforts have emerged only recently; cf. Ruppenhofer et al. (2010), Gerber and Chai (2012), and also Feizabadi and Padó (2015) for an attempt to enlarge the number of annotation instances by combination of scarce resources.",1 Introduction,[0],[0]
"As a result, most state-ofthe-art iSRL systems cannot be trained in a supervised setting and thus integrate custom, rule-based components to detect NIs (we elaborate on related work in Section 2).",1 Introduction,[0],[0]
"To this end, a predicate’s overt roles are matched against a predefined predicatespecific template.",1 Introduction,[0],[0]
"Informally, all roles found in the template but not in the text are regarded as null instantiations.",1 Introduction,[0],[0]
"Such pattern-based methods perform satisfactorily, yet there are drawbacks: (1) They are inflexible and absolute according to
178
their type, in that they assume that all candidate NIs are equally likely to be missing, which is unrealistic given the variety of different linguistic contexts in which predicates co-occur with their semantic roles.",1 Introduction,[0],[0]
"(2) They are expensive in that they require handcrafted, idiosyncratic rules (Ruppenhofer et al., 2011) and rich background knowledge in the form of language-specific lexical resources, such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) or NomBank (Meyers et al., 2004).",1 Introduction,[0],[0]
"Dictionaries providing information about each predicate and status of the individual roles (e.g., whether they can serve as implicit elements or not) are costly, and for most other languages not available to the same extent as for English.",1 Introduction,[0],[0]
"(3) Most earlier studies heuristically restrict implicit arguments to core roles2 only (Tonelli and Delmonte, 2010; Silberer and Frank, 2012), but this is problematic as it ignores the fact that implicit non-core roles also provide valid and valuable information.",1 Introduction,[0],[0]
"Our approach remains agnostic regarding the role inventory, and can address both core and non-core arguments.",1 Introduction,[0],[0]
"Yet, in accordance with the limited evaluation data and in line with earlier literature, we had to restrict ourselves to evaluate NI predictions for core arguments only.
",1 Introduction,[0],[0]
"Our contribution We propose a novel, generic approach to infer information about implicit roles which does not rely on the availability of manually annotated gold data.",1 Introduction,[0],[0]
"Our focus is exclusively on NI role identification, i.e., per-predicate detection of the missing implicit semantic role(s) given their overtly expressed explicit role(s) (without finding filler elements) as we believe that it serves as a crucial preprocessing step and still bears great potential for improvement.",1 Introduction,[0],[0]
"We treat NI identification separately from the resolution of their fillers, also because not all NIs are resolvable from the context.",1 Introduction,[0],[0]
"In order to facilitate a more flexible mechanism, we propose to condition on the presence of other roles, and primarily argue that NI detection should be probabilistic instead of rulebased.",1 Introduction,[0],[0]
"More specifically, we predict implicit arguments using large corpora from which we build a background knowledge base of predicates, cooccurring (explicit) roles and their probabilities.",1 Introduction,[0],[0]
"With such a memory-based approach, we gener-
2Core roles are obligatory arguments of a predicate.",1 Introduction,[0],[0]
"Informally, non-core roles are optional arguments often realized as adjuncts or modifiers.
",1 Introduction,[0],[0]
alize over large quantities of explicit roles to find evidence for implicit information in a mildly supervised manner.,1 Introduction,[0],[0]
"Our proposed models are largely domain independent, include a sense distinction for predicates, and are not bound to a specific release of a hand-maintained dictionary.",1 Introduction,[0],[0]
Our approach is portable across languages in that training data can be created using projected SRL annotations.,1 Introduction,[0],[0]
"Unlike most earlier approaches, we employ a generic role set which is based on PropBank/NomBank rather than FrameNet: The PropBank format comprises a relatively small role inventory which is better suited to obtain statistical generalizations than the great variety of highly specific FrameNet roles.",1 Introduction,[0],[0]
"While FrameNet roles seem to be more fine-grained, their greater number arises mostly from predicate-specific semantic roles, whose specific semantics can be recovered from PropBank annotations by pairing semantic roles with the predicate.
",1 Introduction,[0],[0]
"Yet another motivation of our work is related to the recent development of AMR parsing (Banarescu et al., 2013, Abstract Meaning Representation) which aims at modeling the semantic representation of a sentence while abstracting from syntactic idiosyncrasies.",1 Introduction,[0],[0]
"This particular appraoch makes extensive use of the PropBank-style framesets, as well, and would greatly benefit from the integration of information on implicit roles.
",1 Introduction,[0],[0]
The paper is structured as follows: Section 2 outlines related work in which we exclusively focus on how previous research has handled the sole identification of NIs.,1 Introduction,[0],[0]
Sect.,1 Introduction,[0],[0]
3 describes our approach to probabilistic NI detection; Sect. 4 presents two experiments and their evaluation; Sect. 5 concludes our work.,1 Introduction,[0],[0]
"In the context of the 2010 SemEval Shared Task on Linking Events and Their Participants in Discourse3 on implicit argument resolution, Ruppenhofer et al. (2010) have released a data set of fiction novels with manual NI role annotations for diverse predicates.",2 Related Work,[0],[0]
The data has been referred to by various researchers in the community for direct or indirect evaluation of their results.,2 Related Work,[0],[0]
"The NIs in the data set are further subdivided into two categories: Definite NIs (DNIs) are locally unexpressed arguments which can be resolved to elements in the proceeding or following discourse;
3http://semeval2.fbk.eu/semeval2.php
Indefinite NIs (INIs) are elements for which no antecedent can be identified in the surrounding context.4 Also, the evaluation data comes in two flavors: a base format which is compliant with the FrameNet paradigm and a CoNLL-based PropBank format.",2 Related Work,[0],[0]
"Previous research has exclusively focused on the former.
",2 Related Work,[0],[0]
Chen et al. (2010) present an extension of an existing FrameNet-style parser (SEMAFOR) to handle implicit elements in text.,2 Related Work,[0],[0]
"The identification of NIs is guided by the assumption that, whenever the traditional SRL parser returns the default label involved in a non-saturated analysis for a sentence, an implicit role has to be found in the context instead.",2 Related Work,[0],[0]
"Additional FrameNet-specific heuristics are employed in which, e.g., the presence of one particular role in a frame makes the identification of another implicit role redundant.5
Tonelli and Delmonte (2010, VENSES++) present a deep semantic approach to NI resolution whose system-specific output is mapped to FrameNet valency patterns.",2 Related Work,[0],[0]
"For the detection of NIs, they assume that these are always core arguments, i.e., non-omissible roles in the interaction with a specific predicate.",2 Related Work,[0],[0]
It is unclear how different predicate senses are handled by their approach.,2 Related Work,[0],[0]
"Moreover, not all types of NIs can be detected, resulting in a low overall recall of identified NIs, also having drawbacks for nouns.",2 Related Work,[0],[0]
"Again using FrameNet-specific modeling assumptions, their work has been significantly refined in Tonelli and Delmonte (2011).
",2 Related Work,[0],[0]
"Despite their good performance in the overall task, Silberer and Frank (2012, S&F) give a rather vague explanation regarding NI identification in text.",2 Related Work,[0],[0]
"Using a FrameNet API, the authors restrict their analysis only to the core roles by excluding “conceptually redundant” roles without further elaboration.
Laparra and Rigau (2013) propose a deterministic algorithm to detect NIs on grounds of discourse coherence: It predicts an NI for a predicate if the corresponding role has been explicitly realized for the same predicate in the preceding discourse but is currently unfilled.",2 Related Work,[0],[0]
"Their approach is promising but ignorant of INIs.
",2 Related Work,[0],[0]
"Earlier, Laparra and Rigau (2012, L&R) introduce a statistical approach to identifying NIs similar to ours in that they rely on frequencies from
4The average F-score annotator agreement for frame assignments is about .75",2 Related Work,[0],[0]
"(Ruppenhofer et al., 2010).
",2 Related Work,[0],[0]
5Cf.,2 Related Work,[0],[0]
"CoreSet and Exludes relationship in FrameNet.
",2 Related Work,[0],[0]
overt arguments to predict implicit arguments.,2 Related Work,[0],[0]
"For each predicate template (frame), their algorithm computes all Frame Element patterns, i.e., all cooccurring overt roles and their frequencies.",2 Related Work,[0],[0]
For NI identification a given predicate and its overtly expressed roles are matched against the most frequent pattern not violated by the explicit arguments.,2 Related Work,[0],[0]
Roles of the pattern which are not overtly expressed in the text are predicted as missing NIs.,2 Related Work,[0],[0]
"Even though their approach outperforms all previous results in terms of NI detection, Laparra and Rigau (2012) only estimate the raw frequencies from a very limited training corpus, raising the question whether all patterns are actually sufficiently robust.",2 Related Work,[0],[0]
"Also, the authors disregard all the valuable less frequent patterns and limit their analysis to only a subtype of NI instances which are resolvable from the context.
",2 Related Work,[0],[0]
"Finally, Gerber and Chai (2012) describe a supervised model for implicit argument resolution on the NomBank corpus which—unlike the previous literature—follows the PropBank annotation format.",2 Related Work,[0],[0]
"However, NI detection is still done by dictionary lookup, and the analysis is limited to only a small set of predicates with only one unambiguous sense.",2 Related Work,[0],[0]
"Again limiting NIs to only core roles, the authors empirically demonstrate that this simplification accounts for 8% of the overall error rate of their system.",2 Related Work,[0],[0]
"Memory-based learning for NLP (Daelemans and van den Bosch, 2009) is a lazy learning technique which keeps a record of training instances in the form of a background knowledge base (BKB).",3.1 Memory-Based Learning,[0],[0]
Classification compares new items directly to the stored items in the BKB via a distance metric.,3.1 Memory-Based Learning,[0],[0]
"In semantics, the method has been applied by, e.g., Peñas and Hovy (2010) for semantic enrichment, and Chiarcos (2012) to infer (implicit markers for) discourse relations.",3.1 Memory-Based Learning,[0],[0]
"Here, we adopt its methodology to identify null-instantiated argument roles in text.",3.1 Memory-Based Learning,[0],[0]
"More precisely, we setup a BKB of probablistic predicate-role co-occurrences and estimate thresholds which serve as a trigger for the prediction of an implicit role (a slight modification of the distance metric).",3.1 Memory-Based Learning,[0],[0]
We elaborate on this methodology in Section 4.,3.1 Memory-Based Learning,[0],[0]
"We train our model on a subset of the WaCkypedia EN6 corpus (Baroni et al., 2009).",3.2 Data & Preprocessing,[0],[0]
The data set provides a 2008 Wikipedia dump from which we extracted the tokens and sentences.,3.2 Data & Preprocessing,[0],[0]
We have further divided the dump into pieces of growing size (cumulatively by 100 sentences) and applied MATE7,3.2 Data & Preprocessing,[0],[0]
"(Björkelund et al., 2009) for the automatic detection of semantic roles to the varying portions and annotated them with SRL information.",3.2 Data & Preprocessing,[0],[0]
"For each sentence, MATE identifies the predicates and all of its associated core and noncore arguments.8 MATE has been used in previous research on implicit elements in text (Roth and Frank, 2013) and provides semantic roles with a sense disambiguation for both verbal and nominal predicates.",3.2 Data & Preprocessing,[0],[0]
The resulting output is based on the PropBank format.,3.2 Data & Preprocessing,[0],[0]
We build a probablistic model from annotated predicate-role co-occurrences as follows: 1.,3.3 Model Generation,[0],[0]
"For every sentence, record all distinct predicate
instances and their associated roles.",3.3 Model Generation,[0],[0]
2.,3.3 Model Generation,[0],[0]
"For every predicate instance, sort the role labels
lexicographically (not the role fillers), disregarding their sequential order.",3.3 Model Generation,[0],[0]
(We thus obtain a normalized template of role co-occurrences for each frame instantiation.),3.3 Model Generation,[0],[0]
3. Compute the frequencies for all templates associated with the same predicate.,3.3 Model Generation,[0],[0]
4.,3.3 Model Generation,[0],[0]
"By relative frequency estimation, derive all conditional probabilities of the form:
P (r|R, PREDICATE)
with R being the role inventory of the SRL parser, R ⊆ R a (sub)set of explicitly realized semantic roles, and r ∈ R \ R an arbitrary semantic role.",3.3 Model Generation,[0],[0]
"When we try to gather information on null instantiated roles, r is typically an unrealized role label.",3.3 Model Generation,[0],[0]
"The PREDICATE consists of the lemma of the corresponding verb or noun, optionally followed by sense number (if predicates are sense-disambiguated) and its part of speech (V/N), e.g., PLAY.01.N.
6http://wacky.sslmit.unibo.it/doku.php?id=corpora 7http://code.google.com/p/mate-tools/ 8In order to minimize the noise in the data, we attempted to resplit unrealistically long sentences (> 90 tokens) by means of the Stanford Core NLP module (Manning et al., 2014).",3.3 Model Generation,[0],[0]
"All resulting splits > 70 tokens were rejected.
",3.3 Model Generation,[0],[0]
"We build models from SRL data in PropBank format, both manually and automatically annotated.",3.3 Model Generation,[0],[0]
"We experiment with models for two different styles of predicates: Sense-ignorant or SI models represent predicates by lemma and part of speech (PLAY.N), sense-disambiguated or SD models represent predicates by lemma, sense number and part of speech (PLAY.01.N, PLAY.02.N, etc.).",3.3 Model Generation,[0],[0]
"In accordance with previous iSRL studies, we evaluate our model on the SemEval data set (Ruppenhofer et al., 2010).",3.4 Annotated Data,[0],[0]
"However, to the best of our knowledge, this is the first study to focus on the PropBank version of this data set.",3.4 Annotated Data,[0],[0]
It has been derived semi-automatically from the FrameNet base format using hand-crafted mapping rules (as part of the data set) for both verbs and nouns.,3.4 Annotated Data,[0],[0]
"For example, a conversion for the predicate fear in FrameNet’s EXPERIENCER FOCUS frame is defined as fear.01 (its first sense) with the roles EXPERIENCER and CONTENT mapped to PropBank labels A0 and A1, respectively.",3.4 Annotated Data,[0],[0]
"In accordance with the mapping patterns, the resulting distribution of NIs varies slightly from the base format.",3.4 Annotated Data,[0],[0]
"Table 1 shows the label distribution of overt roles, DNIs, INIs for both the FrameNet and PropBank versions, respectively.",3.4 Annotated Data,[0],[0]
Some information is lost while the general proportions remain similar to the base format.,3.4 Annotated Data,[0],[0]
"This is also due to the fact that for some parts of speech (e.g., for adjectives) no mappings are defined, even though some of them are annotated with NI information in the FrameNet version.",3.4 Annotated Data,[0],[0]
"Moreover, mapping rules exist only for core roles A0-A4 (agent, patient, . . . ).",3.4 Annotated Data,[0],[0]
"As a consequence, we restrict our analysis to these five (unique) roles, even though our models described in this work incorporate probabilistic information for all possible roles in R, i.e., A0-A4, but also for non-core (modifier) roles, such as AM-TEMP (temporal), AM-LOC (location), etc.",3.4 Annotated Data,[0],[0]
"To evaluate the general usefulness of our memorybased approach to detect implicit roles, we set up a simplified framework for predicates with exactly one overt argument and one NI annotated in the SemEval data (for all verbs and all nouns and from both the train and test files to obtain a reasonably large sample; no differentiation of DNIs and INIs).",4.1 Experiment 1,[0],[0]
This pattern accounts for 189 instances—roughly 9% of the data samples in the SemEval set.,4.1 Experiment 1,[0],[0]
We divided the instances into two subsets based on the predicate’s part of speech.,4.1 Experiment 1,[0],[0]
The label distributions over overt and null instantiated roles for both verbal and nominal predicates are given in Table 2.,4.1 Experiment 1,[0],[0]
Predict the role of the single missing NI (A0–A4) for each given predicate instance.,4.1.1 Task Description,[0],[0]
We trained one sense-disambiguated (SD) gold model for verbs (PB) and one for nouns (NB) according to Sect.,4.1.2 Predicting Null Instantiations,[0],[0]
"3.3 on the complete PropBank and the complete NomBank, respectively.",4.1.2 Predicting Null Instantiations,[0],[0]
"This was compared with 30 separate SD and SI models on varying portions of the automatically annotated WaCkypedia EN dump: These were trained on the first k sentences each, in order to make their prediction quality comparable, while k ranges from 50 sentences for the smallest model to k = 10 million for the largest model (≈ 15 of the whole corpus).",4.1.2 Predicting Null Instantiations,[0],[0]
"For NI role prediction, we return ni, i.e., the maximally probable unrealized semantic role given the overt argument oj plus the predicate:
ni = arg max n∈R\R
P (n|oj , PREDICATE),
where R = {oj}, the predicate’s single explicit role andR = {A0..A4} ⊃ R, the role inventory.",4.1.2 Predicting Null Instantiations,[0],[0]
The prediction accuracies for verbal and nominal predicates are illustrated in Figure 1.,4.1.3 Results & Evaluation,[0],[0]
"Although the number of instances in the data sets is small, some general trends are clearly visible.",4.1.3 Results & Evaluation,[0],[0]
"Our major findings are:
By increasing the number of training sentences the performance of the SD and the SI-based classification models steadily increases as well.",4.1.3 Results & Evaluation,[0],[0]
"The trend is the same for both verbs and for nouns, even though training in the nominal domain requires more data to obtain similarly good results.",4.1.3 Results & Evaluation,[0],[0]
"More precisely, models trained on only 50k sentences already have an adequate performance on test data for verbs (≈76% with the SD model).",4.1.3 Results & Evaluation,[0],[0]
"To reach a similar performance on nouns, we need to increase the training size roughly by a factor of 5.
",4.1.3 Results & Evaluation,[0],[0]
"Likewise, the performance of the SD models is better in general than the one of the SI models throughout all models analyzing verbal predicates, but only marginally better for nouns.
",4.1.3 Results & Evaluation,[0],[0]
"Both the SD and the SI models outperform the majority class baseline for both parts of speech.9
Also, with 800k sentences for nouns and only 50k sentences for verbs, both SD model types reach accuracies equal to or greater than the supervised PB and NB (gold) models which have been trained on the complete PropBank and NomBank corpus including sense distinctions, respectively.
",4.1.3 Results & Evaluation,[0],[0]
The classification accuracies for the SD models reach their saturated maxima for verbs at around 91.27% (115/126) with 6 million training sentences and 85.71% (54/63) with 2.85 million sentences for nouns.,4.1.3 Results & Evaluation,[0],[0]
"For verbs, a χ2 test confirms a significant (p < .01) improvement of our best model over the PB gold model.",4.1.3 Results & Evaluation,[0],[0]
"On the sparse evaluation data for nouns, the improvement over the NB gold model is, however, not significant.
",4.1.3 Results & Evaluation,[0],[0]
"Taken together, the improvements confirm that memory-based learning over mass data of automatically annotated (explicit) semantic roles can actually outperform gold models constructed from corpora with manual SRL annotations, even if the tools for automated mass annotation were trained on the very same corpora used to build the gold models (PropBank, NomBank).",4.1.3 Results & Evaluation,[0],[0]
"Also, the experiment demonstrated the feasibility of predicting implicit roles solely using information about the distribution of explicit roles.",4.1.3 Results & Evaluation,[0],[0]
"For the artificially
935.71% with only 1k training sentences (verbs), 52.38% with 50k sentences (nouns).
simplified NI patterns in Experiment 1, already small portions of automatically annotated SRL data are sufficient to yield adequate results for both types (DNIs and INIs).",4.1.3 Results & Evaluation,[0],[0]
Sense disambiguation of predicates generally increases the performance.10,4.1.3 Results & Evaluation,[0],[0]
The setup from the previous experiment is by far too simplistic compared to a real linguistic scenario.,4.2 Experiment 2,[0],[0]
"Usually, a predicate can have an arbitrary number of overt arguments, and similarly the number of missing NIs varies.",4.2 Experiment 2,[0],[0]
"To tackle this problem, we take the original train and test split (744 vs. 929 unrestricted frame instances of the form: any combination of overt roles vs. any combination of NI roles per predicate).",4.2 Experiment 2,[0],[0]
"Again, we do not draw a distinction between DNIs and INIs, but treat them generally as NIs.",4.2 Experiment 2,[0],[0]
Table 3 shows the distribution of the different NI role patterns in the test data.,4.2 Experiment 2,[0],[0]
"Given a predicate and its overtly expressed arguments (ranging from any combination of A0 to A4 or none), predict the correct set of null instantiations (which can also be empty or contain up to five different implicit elements).
10A simple error analysis of the misclassified noun instances revealed that classification on the test data suffers from sparsity issues: In the portions of the WaCkypedia EN that we used for model building, three predicates were not attested (twice murder.01 and once murderer.01).",4.2.1 Task Description,[0],[0]
This has a considerable impact on test results.,4.2.1 Task Description,[0],[0]
"We distinguish two main types of classifiers: supervised classifiers are directly obtained from NI annotations in the SemEval training data, mildly supervised classifiers instead use only information about (automatically obtained) explicitly realized semantic roles in a given corpus, hybrid classifiers combine both sources of information.",4.2.2 Predicting Null Instantiations,[0],[0]
We estimated all parameters optimizing F-measure on the train section of the SemEval data set.,4.2.2 Predicting Null Instantiations,[0],[0]
Their performance is evaluated on its test section.,4.2.2 Predicting Null Instantiations,[0],[0]
"We aim to demonstrate that mildly supervised classifiers are capable of predicting implicit roles, and to study whether NI annotations can be used to improve their performance.",4.2.2 Predicting Null Instantiations,[0],[0]
Baseline:,4.2.2 Predicting Null Instantiations,[0],[0]
"Given the diversity of possible patterns, it is hard to decide how a suitable and competitive baseline should be defined: predicting the majority class means not to predict anything.",4.2.2 Predicting Null Instantiations,[0],[0]
"So, instead, we predict implicit argument roles randomly, but in a way that emulates their frequency distribution in the SemEval data (cf. Tab. 3), i.e., predict
no NIs with a probability of 76.0% (706/929), A1 with 38.6% (86/929), etc.",4.2.2 Predicting Null Instantiations,[0],[0]
"The baseline scores are averaged over 100 runs of this random ‘classifier’, further referred to as A. Supervised classifier: Supervised classifiers, as understood here, are classifiers that use the information obtained from manual NI annotations.",4.2.2 Predicting Null Instantiations,[0],[0]
We set up two predictors B1 and B2 tuned on the SemEval training set: B1 is obtained by counting for each predicate its most frequent NI role pattern.,4.2.2 Predicting Null Instantiations,[0],[0]
"For instance, for seem.02—once annotated with implicit A1, but twice without implicit arguments—B1 would predict an empty set of NIs.",4.2.2 Predicting Null Instantiations,[0],[0]
"B2 is similar toB1 but conditions NI role patterns not only on the predicate, but also on its explicit arguments.11 For prediction, these classifiers consult the most frequent NI pattern observed for a predicate (B2: plus its overt arguments).",4.2.2 Predicting Null Instantiations,[0],[0]
"If a test predicate is unknown (i.e., not present in the training data), we predict the majority class (empty set) for NI.",4.2.2 Predicting Null Instantiations,[0],[0]
Mildly supervised classifier: Mildly supervised classifiers do not take any NI annotation into account.,4.2.2 Predicting Null Instantiations,[0],[0]
"Instead, they rely on explicitly realized semantic roles observed in a corpus, but use explicit NI annotations only to estimate prediction thresholds.",4.2.2 Predicting Null Instantiations,[0],[0]
"We describe an extension of our prediction method from Exp. 1 and present eight parameter-based classification algorithms for our best-performing SD model from Exp. 1, trained on 6 million sentences.
",4.2.2 Predicting Null Instantiations,[0],[0]
We define prediction for classifier C0 as follows:,4.2.2 Predicting Null Instantiations,[0],[0]
"Given a predicate PREDICATE, the role inventory R = {A0..A4}, its (possibly empty) set of overt roles R ⊆ R and a fixed, predicateindependent threshold t0.",4.2.2 Predicting Null Instantiations,[0],[0]
We start by optimizing threshold t0 on all predicate instances with no given overt argument.,4.2.2 Predicting Null Instantiations,[0],[0]
"If there is no overt role and an unrealized role ni ∈ R for which it is true that
11Specifically, we extract finer-grained patterns, e.g., evening.01[A1] → {}=2, {A2}=3, where a predicate is associated with its overt role(s) (left side of the arrow).",4.2.2 Predicting Null Instantiations,[0],[0]
"The corresponding implicit role patterns and their number of occurrence is shown to the right.
",4.2.2 Predicting Null Instantiations,[0],[0]
"P (ni|PREDICATE) > t0, then predict ni as an implicit role.",4.2.2 Predicting Null Instantiations,[0],[0]
"If there is an overt role oj ∈ R and an unrealized role ni ∈ R\R for which it is true that P (ni|oj ,PREDICATE) > t0, then predict ni as an implicit role.",4.2.2 Predicting Null Instantiations,[0],[0]
"Note that C0 requires that this condition to hold for one oj , not all explicit arguments of the predicate instance (logical disjunction).
",4.2.2 Predicting Null Instantiations,[0],[0]
"We refine this classifier by introducing an additional parameter that accounts for the group of overtly realized frames with exactly one overt argument, i.e., C1 predicts ni if P (ni|oj ,PREDICATE) >",4.2.2 Predicting Null Instantiations,[0],[0]
"t1; for all other configurations the procedure is the same as in C0, i.e., the threshold t0 is applied.
",4.2.2 Predicting Null Instantiations,[0],[0]
"Classifiers C2, C3 and C4 extend C1 accordingly and introduce additional thresholds t2, t3, t4 for the respective number of overt arguments.",4.2.2 Predicting Null Instantiations,[0],[0]
"For example, C3 predicts ni if P (ni|oj1 , oj2 , oj3 ,PREDICATE) >",4.2.2 Predicting Null Instantiations,[0],[0]
"t3, for configurations with less arguments, it relies on C2, etc.",4.2.2 Predicting Null Instantiations,[0],[0]
"Our general intuition here is to see whether the increasing number of specialized parameters for increasingly marginal groups of frames is justified by the improvements we achieve in this way.
",4.2.2 Predicting Null Instantiations,[0],[0]
"A final classifier C4n,v extends C4 by distinguishing verbal and nominal predicates, yielding a total of ten parameters t0n ..t4n , t0v ..t0n .",4.2.2 Predicting Null Instantiations,[0],[0]
"Hybrid classifier: To explore to what extent explicit NI annotations improve the classification results, we combine the best-performing and most elaborate mildly supervised classifier C4n,v with the supervised classifiers B1 and B2: For predicates encountered in the training data, C4n,v,B1 (resp., C4n,v,B2 ) uses B1 (resp., B2) to predict the most frequent pattern observed for the predicate; for unknown predicates, apply the threshold-based procedure of C4n,v .",4.2.2 Predicting Null Instantiations,[0],[0]
Table 4 contains the evaluation scores for the individual parameter-based classifiers.,4.2.3 Results & Evaluation,[0],[0]
All classifiers demonstrate significant improvements over the random baseline.,4.2.3 Results & Evaluation,[0],[0]
"Also the mildly supervised
classifiers outperform the supervised algorithms in terms of F1 score and recall.",4.2.3 Results & Evaluation,[0],[0]
"However, detecting NIs by the supervised classifiers is very accurate in terms of high precision.",4.2.3 Results & Evaluation,[0],[0]
"Classifier B2 outperforms B1 as a result of directly incorporating additional information about the overt arguments.
",4.2.3 Results & Evaluation,[0],[0]
"Concerning our parameter-based classifiers, the main observations are: First, the overall performance (F1 score) increases from C0 to C4 (yet not significantly).",4.2.3 Results & Evaluation,[0],[0]
"Secondly, with more parameters, recall decreases while precision increases.",4.2.3 Results & Evaluation,[0],[0]
"We can observe, however, that improvements from C2 to C4 are marginal, at best, due to the sparsity of predicates with two or more overt arguments.",4.2.3 Results & Evaluation,[0],[0]
Similar problems related to data sparsity have been reported in Chen et al. (2010).,4.2.3 Results & Evaluation,[0],[0]
"Results for C3 and C4 are identical, as no predicate with more than three overt arguments occurred in the test data.",4.2.3 Results & Evaluation,[0],[0]
"Encoding the distinction between verbal and nominal predicates into the classifier again slightly increases the performance.
",4.2.3 Results & Evaluation,[0],[0]
"A combination of the high-precision supervised classifiers and the best performing mildly supervised algorithm yields a significant boost in performance (Tab. 4, last two columns).",4.2.3 Results & Evaluation,[0],[0]
"The optimal parameter values for all classifiers C4n,v estimated on the train section of the SemEval data set are given in Table 5.
",4.2.3 Results & Evaluation,[0],[0]
"In Table 6, we report the performance of our best classifier C4n,v,B2 with detailed label scores.",4.2.3 Results & Evaluation,[0],[0]
Its overall NI recognition rate of 0.81 (recall) outperforms the state-of-the-art in implicit role identification:,4.2.3 Results & Evaluation,[0],[0]
"cf. L&P (0.66), SEMAFOR (0.63), S&F (0.58), T&D (0.54), VENSES++ (0.08).12
Summarizing our results, Exp. 2 has shown that combining supervised and mildly supervised strategies to NI detection achieves the best results on the SemEval test set.",4.2.3 Results & Evaluation,[0],[0]
"Concerning the mildly supervised, parameter-based classifiers, it
12Note that only an indirect comparison of these scores is possible due to the aforementioned difference between data formats and also because none of the other systems report precision scores for their pattern-based NI detection systems.
has proven beneficial to incorporate a maximum of available information on overtly expressed arguments in order to determine implicit roles.",4.2.3 Results & Evaluation,[0],[0]
"Our best-performing classifier achieves NI recognition rate beyond state-of-the-art.
",4.2.3 Results & Evaluation,[0],[0]
"Interestingly, memory-based learning offers the capability to detect both DNIs (resolvable from context), as well as INIs (not resolvable from context), simply by learning patterns from local explicit role realizations.",4.2.3 Results & Evaluation,[0],[0]
"Subsequent experiments should extend this approach to distinguish between the two types, as well, which we have treated equivalently in our settings.",4.2.3 Results & Evaluation,[0],[0]
First promising experiments in this direction are being conducted in Chiarcos and Schenk (2015).,4.2.3 Results & Evaluation,[0],[0]
"We have presented a novel, statistical method to infer evidence for implicit roles from their explicit realizations in large amounts of automatically annotated SRL data.",5 Summary and Outlook,[0],[0]
We conclude that—especially when annotated training data is sparse—memorybased approaches to implicit role detection seem highly promising.,5 Summary and Outlook,[0],[0]
"With a much greater degree of flexibility, they offer an alternative solution to static rule-/template-based methods.
",5 Summary and Outlook,[0],[0]
"Despite its simplicity, we demonstrated the suitability of our approach: It is competitive with state-of-the-art systems in terms of the overall recognition rate, however, still suffers in precision of the respective null instantiated arguments.",5 Summary and Outlook,[0],[0]
"Thus, directions for future research should consider integrating additional contextual features, and would benefit from the complete role inventory of our models (including non-core roles).",5 Summary and Outlook,[0],[0]
"In this extended setting, we would like to experiment with other machine learning approaches to assess whether the accuracy of the detected NIs can be increased.",5 Summary and Outlook,[0],[0]
"Also, we plan to apply the memorybased strategy described in this paper to NI resolution (on top their detection), and in this context, examine more closely the characteristic (possibly contrastive) distributions of DNIs and INIs.",5 Summary and Outlook,[0],[0]
"We propose a generic, memory-based approach for the detection of implicit semantic roles.",abstractText,[0],[0]
"While state-of-the-art methods for this task combine hand-crafted rules with specialized and costly lexical resources, our models use large corpora with automated annotations for explicit semantic roles only to capture the distribution of predicates and their associated roles.",abstractText,[0],[0]
We show that memory-based learning can increase the recognition rate of implicit roles beyond the state-of-the-art.,abstractText,[0],[0]
Memory-Based Acquisition of Argument Structures and its Application to Implicit Role Detection,title,[0],[0]
"Zhang et al. (2017a) found that deep convolutional neural networks (CNNs) are capable of memorizing the entire data even with corrupted labels, where some or all true labels are replaced with random labels.",1. Introduction,[0],[0]
It is a consensus that deeper CNNs usually lead to better performance.,1. Introduction,[0],[0]
"However, the ability of deep CNNs to overfit or memorize the corrupted labels can lead to very poor generalization performance (Zhang et al., 2017a).",1. Introduction,[0],[0]
"Recently, Neyshabur et al. (2017) and Arpit et al. (2017) proposed deep learning generalization theories to explain this interesting phenomenon.
",1. Introduction,[0],[0]
"This paper studies how to overcome the corrupted label for deep CNNs, so as to improve generalization performance
1Google Inc., Mountain View, United States 2Stanford University, Stanford, United States.",1. Introduction,[0],[0]
"Correspondence to: Lu Jiang <lujiang@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
on the clean test data.",1. Introduction,[0],[0]
"Although learning models on weakly labeled data might not be novel, improving deep CNNs on corrupted labels is clearly an under-studied problem and worthy of exploration, as deep CNNs are more prone to overfitting and memorizing corrupted labels (Zhang et al., 2017a).",1. Introduction,[0],[0]
"To address this issue, we focus on training very deep CNNs from scratch, such as resnet-101 (He et al., 2016) or inception-resnet (Szegedy et al., 2017) which has a few hundred layers and orders-of-magnitude more parameters than the number of training samples.",1. Introduction,[0],[0]
"These networks can achieve the state-of-the-art result but perform poorly when trained on corrupted labels.
",1. Introduction,[0],[0]
"Inspired by the recent success of Curriculum Learning (CL), this paper tackles this problem using CL (Bengio et al., 2009), a learning paradigm inspired by the cognitive process of human and animals, in which a model is learned gradually using samples ordered in a meaningful sequence.",1. Introduction,[0],[0]
A curriculum specifies a scheme under which training samples will be gradually learned.,1. Introduction,[0],[0]
CL has successfully improved the performance on a variety of problems.,1. Introduction,[0],[0]
"In our problem, our intuition is that a curriculum, similar to its role in education, may provide meaningful supervision to help a student overcome corrupted labels.",1. Introduction,[0],[0]
"A reasonable curriculum can help the student focus on the samples whose labels have a high chance of being correct.
",1. Introduction,[0],[0]
"However, for the deep CNNs, we need to address two limitations of the existing CL methodology.",1. Introduction,[0],[0]
"First, existing curriculums are usually predefined and remain fixed during training, ignoring the feedback from the student.",1. Introduction,[0],[0]
"The learning procedure of deep CNNs is quite complicated, and may not be accurately modeled by the predefined curriculum.",1. Introduction,[0],[0]
"Second, the alternating minimization, commonly used in CL and self-paced learning (Kumar et al., 2010) requires alternative variable updates, which is difficult for training very deep CNNs via mini-batch stochastic gradient descent.
",1. Introduction,[0],[0]
"To this end, we propose a method to learn the curriculum from data by a network called MentorNet.",1. Introduction,[0],[0]
"MentorNet learns a data-driven curriculum to supervise the base deep CNN, namely StudentNet.",1. Introduction,[0],[0]
MentorNet can be learned to approximate an existing predefined curriculum or discover new data-driven curriculums from data.,1. Introduction,[0],[0]
"The learned data-driven curriculum can be updated a few times taking into account of
the StudentNet’s feedback.",1. Introduction,[0],[0]
"Whenever MentorNet is learned or updated, we fix its parameter and use it together with StudentNet to minimize the learning objective, where MentorNet controls the timing and attention to learn each sample.",1. Introduction,[0],[0]
"At the test time, StudentNet makes predictions alone without MentorNet.
",1. Introduction,[0],[0]
The proposed method improves existing curriculum learning in two aspects.,1. Introduction,[0],[0]
"First, our curriculum is learned from data rather than predefined by human experts.",1. Introduction,[0],[0]
It takes into account of the feedback from StudentNet and can be dynamically adjusted during training.,1. Introduction,[0],[0]
"Intuitively, this resembles a “collaborative” learning paradigm, where the curriculum is determined by the teacher and student together.",1. Introduction,[0],[0]
"Second, in our algorithm, the learning objective is jointly minimized using MentorNet and StudentNet via mini-batch stochastic gradient descent.",1. Introduction,[0],[0]
"Therefore, the algorithm can be conveniently parallelized to train deep CNNs on big data.",1. Introduction,[0],[0]
"We show the convergence and empirically verify it on largescale benchmarks.
",1. Introduction,[0],[0]
We verify our method on four benchmarks.,1. Introduction,[0],[0]
Results show that it can significantly improve the performance of deep CNNs trained on both controlled and real-world corrupted training data.,1. Introduction,[0],[0]
"Notably, to the best of our knowledge, it achieves the best-published result on WebVision (Li et al., 2017a), a large benchmark containing 2.2 million images of real-world noisy labels.",1. Introduction,[0],[0]
"To summarize, the contribution of this paper is threefold:
• We propose a novel method to learn data-driven curriculums for deep CNNs trained on corrupted labels.
",1. Introduction,[0],[0]
• We discuss an algorithm to perform curriculum learning for deep networks via mini-batch stochastic gradient descent.,1. Introduction,[0],[0]
• We verify our method on 4 benchmarks and achieve the best-published result on the WebVision benchmark.,1. Introduction,[0],[0]
"We formulate our problem based on the model in (Kumar et al., 2010) and (Jiang et al., 2015).",2. Preliminary on Curriculum Learning,[0],[0]
"Consider a classification problem with the training set D = {(x1,y1), · · · , (xn,yn)}, where xi denotes the ith observed sample and yi ∈ {0, 1}m is the noisy label vector over m classes.",2. Preliminary on Curriculum Learning,[0],[0]
"Let gs(xi,w) denote the discriminative function of a neural network called StudentNet, parameterized by w ∈ Rd.",2. Preliminary on Curriculum Learning,[0],[0]
"Further, let L(yi, gs(xi,w)), a mdimensional column vector, denote the loss over m classes.",2. Preliminary on Curriculum Learning,[0],[0]
"Introduce the latent weight variable, v ∈ Rn×m, and optimize the objective:
min w∈Rd,v∈[0,1]n×m
F(w,v) =
1
n n∑ i=1",2. Preliminary on Curriculum Learning,[0],[0]
"vTi L(yi,gs(xi,w))",2. Preliminary on Curriculum Learning,[0],[0]
"+G(v;λ) + θ‖w‖22 (1)
where ‖·‖2 is the l2 norm for weight decay, and data augmentation and dropout are subsumed inside gs.",2. Preliminary on Curriculum Learning,[0],[0]
vi ∈,2. Preliminary on Curriculum Learning,[0],[0]
"[0, 1]m×1 is a vector to represent the latent weight variable for the i-th sample.",2. Preliminary on Curriculum Learning,[0],[0]
"The functionG defines a curriculum, parameterized by λ.",2. Preliminary on Curriculum Learning,[0],[0]
This paper focuses on the one-hot label.,2. Preliminary on Curriculum Learning,[0],[0]
"For notation convenience, denote the loss L(yi,gs(xi,w))",2. Preliminary on Curriculum Learning,[0],[0]
"= `i, vi as a scalar vi, and yi as an integer yi ∈",2. Preliminary on Curriculum Learning,[0],[0]
"[1,m].
In the existing literature, alternating minimization (Csiszar, 1984), or its related variants, is commonly employed to minimize the training objective, e.g. in (Kumar et al., 2010; Ma et al., 2017a; Jiang et al., 2014).",2. Preliminary on Curriculum Learning,[0],[0]
"This is an algorithmic paradigm where w and v are alternatively minimized, one at a time while the other is held fixed.",2. Preliminary on Curriculum Learning,[0],[0]
"When v is fixed, the weighted loss is typically minimized by stochastic gradient descent.",2. Preliminary on Curriculum Learning,[0],[0]
"When w is fixed, we compute vk = arg minv F(vk−1,wk) using the most recently updated wk at epoch k.",2. Preliminary on Curriculum Learning,[0],[0]
"For example, Kumar et al. (2010) employed G(v) = −λ‖v‖1.",2. Preliminary on Curriculum Learning,[0],[0]
"When w is fixed, the optimal v can be easily derived by:
v∗i =",2. Preliminary on Curriculum Learning,[0],[0]
1(`i ≤,2. Preliminary on Curriculum Learning,[0],[0]
"λ),∀i ∈",2. Preliminary on Curriculum Learning,[0],[0]
"[1, n], (2) where 1 is the indicator function.",2. Preliminary on Curriculum Learning,[0],[0]
"Eq. (2) intuitively explains the predefined curriculum in (Kumar et al., 2010), known as self-paced learning.",2. Preliminary on Curriculum Learning,[0],[0]
"First, when updating v with a fixed w, a sample of smaller loss than the threshold λ is treated as an “easy” sample, and will be selected in training (v∗i = 1).",2. Preliminary on Curriculum Learning,[0],[0]
"Otherwise, it will not be selected (v∗i = 0).",2. Preliminary on Curriculum Learning,[0],[0]
"Second, when updating w with a fixed v, the classifier is trained only on the selected “easy” samples.",2. Preliminary on Curriculum Learning,[0],[0]
The hyperparameter λ controls the learning pace and corresponds to the “age” of the model.,2. Preliminary on Curriculum Learning,[0],[0]
"When λ is small, only samples of small loss will be considered.",2. Preliminary on Curriculum Learning,[0],[0]
"As λ grows, more samples of larger loss will be gradually added to train a more “mature” model.
",2. Preliminary on Curriculum Learning,[0],[0]
"As shown, the function G specifies a curriculum, i.e., a sequence of samples with their corresponding weights to be used in training.",2. Preliminary on Curriculum Learning,[0],[0]
"When w is fixed, its optimal solution, e.g. Eq. (2), computes the time-varying weight that controls the timing and attention to learn every sample.",2. Preliminary on Curriculum Learning,[0],[0]
"Recent studies discovered multiple predefined curriculums and verified them in many real-world applications, e.g., in (Fan et al., 2017; Ma et al., 2017a; Sangineto et al., 2016; Fan et al., 2017; Chang et al., 2017).
",2. Preliminary on Curriculum Learning,[0],[0]
This paper studies learning curriculum from data.,2. Preliminary on Curriculum Learning,[0],[0]
"In the rest of this paper, Section 3 presents an approach to learn data-driven curriculum by MentorNet.",2. Preliminary on Curriculum Learning,[0],[0]
Section 4 discusses an algorithm to optimize Eq.,2. Preliminary on Curriculum Learning,[0],[0]
(1) using MentorNet and StudentNet together via mini-batch training.,2. Preliminary on Curriculum Learning,[0],[0]
Existing curriculums are either predetermined as an analytic expression of G or a function to compute sample weights.,3. Learning Curriculum from Data,[0],[0]
"Such predefined curriculums cannot be adjusted accordingly, taking into account of the feedback from the student.",3. Learning Curriculum from Data,[0],[0]
"This
section discusses a new way to learn data-driven curriculum by a neural network, called MentorNet.",3. Learning Curriculum from Data,[0],[0]
The MentorNet gm is learned to compute time-varying weights for each training sample.,3. Learning Curriculum from Data,[0],[0]
Let Θ denote the parameters in gm.,3. Learning Curriculum from Data,[0],[0]
"Given a fixed w, our goal is to learn an Θ∗ to compute the weight:
gm(zi; Θ ∗)",3. Learning Curriculum from Data,[0],[0]
"= arg min vi∈[0,1] F(w,v),∀i ∈",3. Learning Curriculum from Data,[0],[0]
"[1, n] (3)
where zi = φ(xi, yi,w) indicates the input feature to MentorNet about the i-th sample.",3. Learning Curriculum from Data,[0],[0]
"MentorNet can be learned to 1) approximate existing curriculums or 2) discover new curriculums from data.
",3.1. Learning Curriculum,[0],[0]
Learning to approximate predefined curriculums.,3.1. Learning Curriculum,[0],[0]
Our first task is to learn a MentorNet to approximate a predefined curriculum.,3.1. Learning Curriculum,[0],[0]
"To do so, we minimize the objective in Eq.",3.1. Learning Curriculum,[0],[0]
"(1):
arg min Θ ∑ (xi,yi)∈D gm(zi; Θ)`i +G(gm(zi; Θ);λ)",3.1. Learning Curriculum,[0],[0]
"(4)
Eq. (4) applies for both convex and non-convex G. This paper employs the following predefined curriculum.",3.1. Learning Curriculum,[0],[0]
"It is derived from (Jiang et al., 2015) and works well in our experiments.",3.1. Learning Curriculum,[0],[0]
"As will be discussed later, it is also related to robust non-convex penalties.
",3.1. Learning Curriculum,[0],[0]
G(v;λ),3.1. Learning Curriculum,[0],[0]
= n∑ i=1,3.1. Learning Curriculum,[0],[0]
1 2,3.1. Learning Curriculum,[0],[0]
"λ2v 2 i − (λ1 + λ2)vi, (5)
where λ1, λ2 ≥ 0 are hyper-parameters.",3.1. Learning Curriculum,[0],[0]
"As G is convex, there exists a closed-form solution for the optimal value of Eq.",3.1. Learning Curriculum,[0],[0]
(3).,3.1. Learning Curriculum,[0],[0]
"Given a fixed w, define Fw(v) = ∑n i=1",3.1. Learning Curriculum,[0],[0]
"f(vi):
f(vi) = vi`i + 1
2 λ2v
2 i − (λ1 + λ2)vi (6)
",3.1. Learning Curriculum,[0],[0]
"The minima are obtained at ∇vFw(v) = 0, and can be decoupled by setting ∂f/∂vi = 0.",3.1. Learning Curriculum,[0],[0]
"We then have:
gm(zi; Θ ∗)",3.1. Learning Curriculum,[0],[0]
"= { 1(`i ≤ λ1) λ2 = 0 min(max(0, 1− `i−λ1λ2 ), 1) λ2 6= 0 ,
(7) where Θ∗ is the optimal MentorNet parameter obtained by SGD.",3.1. Learning Curriculum,[0],[0]
The closed-form solution in Eq.,3.1. Learning Curriculum,[0],[0]
(7) gives some intuitions about the curriculum.,3.1. Learning Curriculum,[0],[0]
"When λ2 = 0, it is similar to self-paced learning (Kumar et al., 2010) i.e. only “easy” samples of `i < λ1 will be selected in training (gm(zi; Θ∗) = 1).",3.1. Learning Curriculum,[0],[0]
"When λ2 6= 0, samples of loss `i ≥ λ2 +λ1 will not be selected in training.",3.1. Learning Curriculum,[0],[0]
These samples represent the “hard” samples of greater loss.,3.1. Learning Curriculum,[0],[0]
"Otherwise, samples will be weighted linearly w.r.t.",3.1. Learning Curriculum,[0],[0]
1−,3.1. Learning Curriculum,[0],[0]
(`i − λ1)/λ2.,3.1. Learning Curriculum,[0],[0]
"As in (Kumar et al., 2010), the hyper-parameters λ1 and λ2 control the learning pace.
",3.1. Learning Curriculum,[0],[0]
Learning data-driven curriculums.,3.1. Learning Curriculum,[0],[0]
Our next task is to learn a curriculum solely derived from labeled data.,3.1. Learning Curriculum,[0],[0]
"To this end, Θ is learned on another dataset D′ =
{(φ(xi, yi,w), v∗i )}, where (xi, yi) is sampled from D and |D′| |D|.",3.1. Learning Curriculum,[0],[0]
"v∗i is a given annotation and we assume it approximates the optimal weight, i.e., v∗i ' arg minvi∈[0,1] F(v,w).",3.1. Learning Curriculum,[0],[0]
"In this paper, we assign binary labels to v∗i , where v ∗",3.1. Learning Curriculum,[0],[0]
i = 1 iff yi is a correct label.,3.1. Learning Curriculum,[0],[0]
"As v∗i is binary, Θ is learned by minimizing the cross-entropy loss between v∗i and g(zi; Θ).",3.1. Learning Curriculum,[0],[0]
"Intuitively, this process is similar to a mock test for the teacher (MentorNet) to learn to update her teaching strategy (curriculum).",3.1. Learning Curriculum,[0],[0]
"The student (StudentNet) provides features φ(·, ·,w) for the mock test using the latest model w.",3.1. Learning Curriculum,[0],[0]
The teacher can learn an updated curriculum from the data to better supervise the latest student model.,3.1. Learning Curriculum,[0],[0]
"The learned curriculum is jointly determined by the teacher and student together.
",3.1. Learning Curriculum,[0],[0]
"The information on the correct label may not always be available on the target dataset D. In this case, we learn the curriculum on a different small dataset where the correct labels are available.",3.1. Learning Curriculum,[0],[0]
"Intuitively, it resembles first learning a teaching strategy with the student on one topic and transfer the strategy on a similar topic.",3.1. Learning Curriculum,[0],[0]
"Empirically, Section 5.1 substantiates that the learned curriculum on a small subset of CIFAR-10 can be applied to the target CIFAR-100 dataset.
",3.1. Learning Curriculum,[0],[0]
A burn-in period is introduced before learning Θ.,3.1. Learning Curriculum,[0],[0]
"In the first 20% training epoch of the StudentNet, MentorNet is initialized and fixed as gm(zi; Θ∗) = ri, where ri ∼ Bernoulli(p) is the Bernoulli random variable.",3.1. Learning Curriculum,[0],[0]
This is equivalent to randomly dropping out p% training samples.,3.1. Learning Curriculum,[0],[0]
"We found that the burn-in process helps StudentNet stabilize the prediction and focus on learning simple and common patterns.
MentorNet architecture.",3.1. Learning Curriculum,[0],[0]
We found that MentorNet can have a simple architecture.,3.1. Learning Curriculum,[0],[0]
Appendix D shows that even MentorNet based on the two-layer perceptron can reasonably approximate the existing curriculum in the literature.,3.1. Learning Curriculum,[0],[0]
"Nevertheless, we use a MentorNet architecture shown in Fig. 1, which works reasonably well compared to classical network architectures.",3.1. Learning Curriculum,[0],[0]
"It takes the input of a mini-batch of samples, and outputs their corresponding sample weights.",3.1. Learning Curriculum,[0],[0]
"The feature zi = φ(xi, yi,w) includes the loss, loss difference to the moving average, label and epoch percentage.",3.1. Learning Curriculum,[0],[0]
`pt maintains an exponential moving average on the p-th percentile of the loss in each mini-batch.,3.1. Learning Curriculum,[0],[0]
"For a sample, its loss ` and loss difference ` − `pt over the last few epochs can be encoded by a bidirectional LSTM network to capture the prediction variance (Chang et al., 2017).",3.1. Learning Curriculum,[0],[0]
"We verify the LSTM encoder in the experiments in Appendix D. For simplicity, we set the step size of the LSTM to 1 in Section 5.1 and only consider the loss and the loss difference of the current epoch.
",3.1. Learning Curriculum,[0],[0]
The label and the training epoch percentage are encoded by two separate embedding layers.,3.1. Learning Curriculum,[0],[0]
The epoch percentage is represented as an integer between 0 and 99.,3.1. Learning Curriculum,[0],[0]
"It is used to indicate the StudentNet’s training progress, where 0 rep-
resents the first and 99 represents the last training epoch.",3.1. Learning Curriculum,[0],[0]
"The concatenated outputs from the LSTM and the embedding layers are fed into two fully-connected layers fc1, fc2, where fc2 uses the sigmoid activation to ensure the output weights bounded between 0 and 1.",3.1. Learning Curriculum,[0],[0]
"The last layer in Fig. 1 is a probabilistic sampling layer, and is used to implement the sample dropout in the burn-in process on the already learned MentorNet.",3.1. Learning Curriculum,[0],[0]
"MentorNet is a general framework for both predefined and data-driven curriculum learning, where various curriculums can be learned by the same MentorNet structure with different parameters.",3.2. Discussions,[0],[0]
This framework is conceptually general and practically flexible as we can switch curriculums by attaching different MentorNets without modifying the pipeline.,3.2. Discussions,[0],[0]
"Therefore, we also learn MentorNets for predefined curriculums.",3.2. Discussions,[0],[0]
"For predefined curriculums where G is unknown, we directly minimize the error between the MentorNet’s outputs and desired weights.",3.2. Discussions,[0],[0]
"For example, the desired weight for focal loss (Lin et al., 2017b) is computed by:
v∗i =",3.2. Discussions,[0],[0]
"[1− exp{−`i}]γ , (8) where γ is a hyperparameter for smoothing the distribution.
",3.2. Discussions,[0],[0]
This paper tackles the problem of overcoming corrupted labels.,3.2. Discussions,[0],[0]
It is interesting to analyze why the learned curriculum can improve the generalization performance.,3.2. Discussions,[0],[0]
"It turns out that StudentNet, when jointly learned with MentorNet, may optimize an underlying robust objective and the objective is also related to the robust M-estimator (Huber, 2011).
",3.2. Discussions,[0],[0]
"To show this, let v∗(λ, x) represent the optimal weight function for a loss variable x, and we define:
v∗(λ, x)",3.2. Discussions,[0],[0]
"= argminv∈[0,1] vx+G(v, λ).",3.2. Discussions,[0],[0]
"(9)
As gm is an approximator to Eq. (9), its property can then be analyzed by the function v∗(λ, x).",3.2. Discussions,[0],[0]
"Meng et al.(2015) investigated the insights of self-paced objective function, and proved that the optimization of SPL algorithm is intrinsically equivalent to minimizing a robust loss function.",3.2. Discussions,[0],[0]
"They showed that given a fixed λ and a decreasing v∗(λ, x) with respect to x, the underlying objective of Eq.",3.2. Discussions,[0],[0]
"(1) can be
obtained by:
Fλ(w)",3.2. Discussions,[0],[0]
"= 1
n n∑ i=1",3.2. Discussions,[0],[0]
∫,3.2. Discussions,[0],[0]
"`i 0 v∗(λ, x)dx, (10)
",3.2. Discussions,[0],[0]
"Based on it, the underlying learning objective of the curriculum in Eq.",3.2. Discussions,[0],[0]
(5) can then be derived.,3.2. Discussions,[0],[0]
Remark 1.,3.2. Discussions,[0],[0]
"When λ1, λ2 are fixed and λ2 6= 0, the underlying objective function of the curriculum in Eq.",3.2. Discussions,[0],[0]
"(5) is calculated from:
Fλ(w)= 1
n n∑ i=1  ",3.2. Discussions,[0],[0]
`i `i ≤,3.2. Discussions,[0],[0]
λ1 (λ2 + 2λ1)/2 `i ≥ λ2 + λ1 θ`i−`2i /(2λ2)−,3.2. Discussions,[0],[0]
"(θ−1)2λ2 2 otherwise
(11) where θ = (λ2 + λ1)/λ2.",3.2. Discussions,[0],[0]
"When θ = 1 it is equivalent to the minimax concave penalty (Zhang, 2010).
",3.2. Discussions,[0],[0]
As shown in Eq.,3.2. Discussions,[0],[0]
"(11), the underlying objective has a form of Fλ(w) = ∑ i ρ(`i)/n, where ρ is the penalty function in M-estimator (Candes et al., 2008).",3.2. Discussions,[0],[0]
"Particularly, when θ = 1, ρ(`) is equivalent to the minimax concave plus penalty (Zhang, 2010), a popular non-convex robust loss.",3.2. Discussions,[0],[0]
The result indicates the learned MentorNet that approximates our predefined curriculum in Eq.,3.2. Discussions,[0],[0]
"(5) leads to an underlying robust objective of the StudentNet.
",3.2. Discussions,[0],[0]
"For the data-driven curriculum, if the learned MentorNet satisfies certain conditions, we have: Proposition 1.",3.2. Discussions,[0],[0]
"Suppose (x, y) denotes a training sample and its corrupted label.",3.2. Discussions,[0],[0]
"For simplicity, let the MentorNet input φ(x, y,w) = ` be the loss computed by the StudentNet model parameter w.",3.2. Discussions,[0],[0]
"The MentorNet gm(`; Θ) = v, where v is the sample weight.",3.2. Discussions,[0],[0]
"If gm decreases with respect to `, then there exists an underlying robust objective F :
F (w) = 1
n n∑ i=1 ρ(`i),
where ρ(`i)",3.2. Discussions,[0],[0]
= ∫,3.2. Discussions,[0],[0]
"`i
0 gm(x; Θ)dx.",3.2. Discussions,[0],[0]
"In the special cases, ρ(`)
degenerates to the robust M-estimator: Huber (Huber et al., 1964) and the log-sum penalty (Candes et al., 2008).
",3.2. Discussions,[0],[0]
The proposition indicates that there exist some learned MentorNets that are related to the robust M-estimator.,3.2. Discussions,[0],[0]
"On noisy
data, the effect of the robust objective is evident, i.e., preventing StudentNet from being dominated by corrupted labels.",3.2. Discussions,[0],[0]
"Fig. 2 visualizes curves of the sample loss ` = yi−gs(xi,w) and the learning objective for the Huber loss (Huber et al., 1964), log-sum penalty (Candes et al., 2008), self-paced (Kumar et al., 2010), and our learned data-driven curriculum.",3.2. Discussions,[0],[0]
"We use the best learned curriculum Θ∗ on CIFAR-10 in our experiments and plot |gm(φ(x, y,w); Θ∗) ×",3.2. Discussions,[0],[0]
`| since the G in the objective function is unknown.,3.2. Discussions,[0],[0]
"As shown, all curves are robust to great loss to different extents.",3.2. Discussions,[0],[0]
The corrupted labels in our problem are harmful.,3.2. Discussions,[0],[0]
"As the sample loss grows bigger beyond some value, MentorNet starts to sharply decrease the sample’s weight.",3.2. Discussions,[0],[0]
The subtlety of learned curriculum is difficult to be predefined by the analytic expression.,3.2. Discussions,[0],[0]
Proposition 1 does not guarantee there is an underlying robust objective for every learned MentorNet.,3.2. Discussions,[0],[0]
"Instead, it shows MentorNet’s capability of learning such robust objective.",3.2. Discussions,[0],[0]
"The alternating minimization algorithm (Csiszar, 1984) used in related work is intractable for deep CNNs, especially on big datasets, for two important reasons.",4. The Algorithm,[0],[0]
"First, in the subroutine of minimizing w when fixing v, stochastic gradient descent often takes many steps before converging.",4. The Algorithm,[0],[0]
This means that it can take a long time before moving past this single sub-step.,4. The Algorithm,[0],[0]
"However, such computation is often wasteful, particularly in the initial part of training, because, when v is far away from the optimal point, there is not much gain in finding the exact optimal w corresponding to this v. Second, the subroutine of minimizing v when fixing w is often difficult, because the fixed vector v may not only consume a considerable amount of the memory but also hinder the parallel training on multiple machines.",4. The Algorithm,[0],[0]
"Therefore, optimizing the objective with deep CNNs requires some thought on the algorithmic level.
",4. The Algorithm,[0],[0]
To minimize Eq.,4. The Algorithm,[0],[0]
"(1), we propose an algorithm called SPADE (Scholastic gradient PArtial DEscent).",4. The Algorithm,[0],[0]
The algorithm optimizes the StudentNet model parameter w jointly with a given MentorNet.,4. The Algorithm,[0],[0]
It provides a simple and elegant way to minimize w and v stochastically over mini-batches.,4. The Algorithm,[0],[0]
"As a general approach, it can also take an input of G. Let",4. The Algorithm,[0],[0]
"Ξt = {(xj , yj)}bj=1 denotes a mini-batch of b samples, fetched uniformly at random and vtΞ =",4. The Algorithm,[0],[0]
"[v t 1, ..., v t b] represent the sample weights in Ξt.",4. The Algorithm,[0],[0]
"The MentorNet computes:
vtΞ =gm(φ(Ξt,w t−1))=arg",4. The Algorithm,[0],[0]
"min vΞ F(wt−1,vt−1), (12)
where φ is the feature extraction function defined in Eq.",4. The Algorithm,[0],[0]
(3).,4. The Algorithm,[0],[0]
"Θ denotes the learned MentorNet discussed in Section 3.1.
",4. The Algorithm,[0],[0]
"As shown in Algorithm 1, for w, a stochastic gradient is computed (via a mini-batch) and applied (Step 12), where αt is the learning rate.",4. The Algorithm,[0],[0]
"For the latent weight variables v, gradient descent is only applied to a small subset thereof
parameters corresponding only to the mini-batch (Step 9 or 11).",4. The Algorithm,[0],[0]
The partial gradient update on weight parameters is performed when G is used (Step 9).,4. The Algorithm,[0],[0]
"Otherwise, we directly apply the weights computed by the learned MentorNet (Step 11).",4. The Algorithm,[0],[0]
"In both cases, the weights are computed on-the-fly within a mini-batch and thus do not need to be fixed.",4. The Algorithm,[0],[0]
"As a result, the algorithm can be conveniently parallelized across multiple machines.
",4. The Algorithm,[0],[0]
Algorithm 1 SPADE for minimizing Eq.,4. The Algorithm,[0],[0]
"(1) Input :Dataset D, a predefined G or a learned gm(·;",4. The Algorithm,[0],[0]
"Θ) Output :The model parameter w of StudentNet.
1 Initialize w0,v0, t = 0 2 while Not Converged do 3 Fetch a mini-batch Ξt uniformly at random 4 For every (xi, yi) in Ξt compute φ(xi, yi,wt) 5 if update curriculum then 6 Θ← Θ∗, where Θ∗ is learned in Sec. 3.1 7 end 8 if G is used then 9 vtΞ ← vt−1Ξ",4. The Algorithm,[0],[0]
"− αt∇vF(w
t−1,vt−1)|Ξt 10 end 11 else vtΞ ← gm(φ(Ξt,wt−1); Θ) ; 12 wt ← wt−1 − αt∇wF(wt−1,vt)|Ξt 13 t← t+ 1 14 end 15 return wt
The curriculum can change during training.",4. The Algorithm,[0],[0]
MentorNet is updated a few times in Algorithm 1.,4. The Algorithm,[0],[0]
"In Step 6, the MentorNet parameter Θ is updated to adapt to the most recent model parameters of StudentNet.",4. The Algorithm,[0],[0]
"In experiments, we update Θ twice after the learning rate is changed.",4. The Algorithm,[0],[0]
"Each time, a datadriven curriculum is learned from the data generated by the most recent w using the method discussed in Section 3.1.",4. The Algorithm,[0],[0]
"The update is consistent with existing curriculum learning methodology (Bengio et al., 2009; Kumar et al., 2010) and the difference here is that for each update, the curriculum is learned rather than specified by human experts.
",4. The Algorithm,[0],[0]
"Under standard assumptions, Theorem 1 shows that the algorithm stabilizes and converges to a stationary point (convergence to global/local minima cannot be guaranteed unless in specially structured non-convex objectives (Chen et al., 2018; Zhou et al., 2017b;a)).",4. The Algorithm,[0],[0]
The proof is in Appendix B. The theorem is a characterization of stability of the model parameters,4. The Algorithm,[0],[0]
w.,4. The Algorithm,[0],[0]
"For the weight parameters v, as it is restricted in a compact set, convergence to a stationary point is not always guaranteed.",4. The Algorithm,[0],[0]
"As the model parameters is more important, we only provide a detailed characterization of the model parameter.
",4. The Algorithm,[0],[0]
Theorem 1.,4. The Algorithm,[0],[0]
"Let the objective F(w,v) defined in Eq.",4. The Algorithm,[0],[0]
"(1) be differentiable, L(·) be Lipschitz continuous in w and ∇vG(·) be Lipschitz continuous in v.",4. The Algorithm,[0],[0]
"Let wt,vt be iterates from Algorithm 1 and ∑∞ t=0 αt = ∞, ∑∞ t=0 α 2 t < ∞ .",4. The Algorithm,[0],[0]
"Then, limt→∞ E[‖∇wF(wt,vt)‖22] = 0.",4. The Algorithm,[0],[0]
"For the manually designed curriculums, it may be unclear
where or even whether such predefined curriculum would converge via mini-batch training.",4. The Algorithm,[0],[0]
Theorem 1 shows that the learned curriculum can converge and produce a stable StudentNet model.,4. The Algorithm,[0],[0]
The algorithm can be used to replace the alternating minimization method in related work.,4. The Algorithm,[0],[0]
This section empirically verifies the proposed method on four benchmarks of controlled corrupted labels in Section 5.1 and real-world noisy labels in Section 5.2.,5. Experiments,[0],[0]
This section validates MentorNet on the controlled corrupted label.,5.1. Experiments on controlled corrupted labels,[0],[0]
"We follow a common setting in (Zhang et al., 2017a) to train deep CNNs, where the label of each image is independently changed to a uniform random class with probability p, where p is noise fraction and is set to 0.2, 0.4 and 0.8.",5.1. Experiments on controlled corrupted labels,[0],[0]
"The labels of validation data remain clean for evaluation.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"Dataset and StudentNet: We use the same benchmarks in (Zhang et al., 2017a): CIFAR-10, CIFAR-100 and ImageNet.",5.1. Experiments on controlled corrupted labels,[0],[0]
CIFAR-10 and CIFAR-100 (,5.1. Experiments on controlled corrupted labels,[0],[0]
"Krizhevsky & Hinton, 2009) consist of 32 × 32 color images arranged in 10 and 100 classes.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Both datasets contain 50,000 training and 10,000 validation images.",5.1. Experiments on controlled corrupted labels,[0],[0]
ImageNet ILSVRC2012,5.1. Experiments on controlled corrupted labels,[0],[0]
"(Deng et al., 2009) contain about 1.2 million training and 50k validation images, split into 1,000 classes.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Each image is resized to 299x299 with 3 color channels.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"We employ 3 recent deep CNNs as our StudentNets: inception (Szegedy et al., 2016), resnet-101 (He et al., 2016) with wide filters (Zagoruyko & Komodakis, 2016) and inceptionresnet v2 (Szegedy et al., 2017).",5.1. Experiments on controlled corrupted labels,[0],[0]
"Table 1 shows their #model parameters, training, and validation accuracy when we train them on the clean training data (noise= 0).",5.1. Experiments on controlled corrupted labels,[0],[0]
"As shown, they achieve reasonable accuracy on each task.
",5.1. Experiments on controlled corrupted labels,[0],[0]
Baselines: MentorNet is compared against the following baselines:,5.1. Experiments on controlled corrupted labels,[0],[0]
"FullMode is the standard StudentNet trained using l2 weight decay, dropout (Srivastava et al., 2014) and data augmentation (Krizhevsky et al., 2012).",5.1. Experiments on controlled corrupted labels,[0],[0]
The hyperparameters are set to the best ones found on the clean training data.,5.1. Experiments on controlled corrupted labels,[0],[0]
"Unless specified otherwise, for a fair comparison, the StudentNet with the same hyperparameters is used in all baseline and our model.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Forgetting was introduced in (Arpit et al., 2017), in which the dropout parameter is searched in the range of (0.2-0.9).",5.1. Experiments on controlled corrupted labels,[0],[0]
"Self-paced (Kumar et al., 2010) and Focal Loss (Lin et al., 2017b) represent well-known predefined curriculums in the literature.",5.1. Experiments on controlled corrupted labels,[0],[0]
"We
implemented Reed (2014) and Goldberger (Goldberger & Ben-Reuven, 2017) as the recent weakly-supervised learning methods.",5.1. Experiments on controlled corrupted labels,[0],[0]
"The above baseline methods are a mixture of the curriculum learning and the recent methods dealing with corrupted labels.
",5.1. Experiments on controlled corrupted labels,[0],[0]
Our Model: MentorNet PD is the network learned using our predefined curriculum in Eq.,5.1. Experiments on controlled corrupted labels,[0],[0]
(5) using no additional clean labels.,5.1. Experiments on controlled corrupted labels,[0],[0]
MentorNet DD is the learned data-driven curriculum.,5.1. Experiments on controlled corrupted labels,[0],[0]
"It is trained on 5,000 images of true labels, randomly sampled from the CIFAR-10 training set.",5.1. Experiments on controlled corrupted labels,[0],[0]
The same data are used to learn MentorNet DD on CIFAR-100.,5.1. Experiments on controlled corrupted labels,[0],[0]
Note CIFAR-10 and CIFAR-100 are two different datasets that have not only different classes but also the different number of classes.,5.1. Experiments on controlled corrupted labels,[0],[0]
"Therefore, it is fair to compare MentorNet DD with other methods using no true labels on CIFAR-100.",5.1. Experiments on controlled corrupted labels,[0],[0]
Algorithm 1 is used to optimize the StudentNet.,5.1. Experiments on controlled corrupted labels,[0],[0]
The decay factor in computing the loss moving average is set to 0.95.,5.1. Experiments on controlled corrupted labels,[0],[0]
The loss percentile in the moving average is set by the cross-validation.,5.1. Experiments on controlled corrupted labels,[0],[0]
"As mentioned, a burn-in process is used in the first 20% training epoch for both MentorNet DD and MentorNet PD.",5.1. Experiments on controlled corrupted labels,[0],[0]
"More details are discussed in Appendix E.
We first show the comparison to the baseline method on CIFAR-10 and CIFAR-100 in Table 2.",5.1. Experiments on controlled corrupted labels,[0],[0]
"On both datasets, each method is verified with two StudentNets (resnet-101 and inception) under the noise fraction of 0.2, 0.4, and 0.8.",5.1. Experiments on controlled corrupted labels,[0],[0]
"As we see on both datasets, MentorNet improves FullModel across different noise fractions, and the learned data-driven curriculum (MentorNet DD) achieves the best results.",5.1. Experiments on controlled corrupted labels,[0],[0]
The improvement is more significant for the deeper CNN model resnet-101.,5.1. Experiments on controlled corrupted labels,[0],[0]
"For example, on the CIFAR-10 of 40% noise, MentorNet DD (with resnet-101) yields an absolute 20% gain over FullModel.",5.1. Experiments on controlled corrupted labels,[0],[0]
"After inspecting the result, we found that it may be because Mentor DD learns a more appropriate curriculum to give high weights to samples of correct labels.",5.1. Experiments on controlled corrupted labels,[0],[0]
"As a result, it helps the StudentNet focus on samples of correct labels.",5.1. Experiments on controlled corrupted labels,[0],[0]
"The results indicate that the learned MentorNet can improve the generalization performance of recent deep CNNs, and outperform the predefined curriculums (Self-paced and Focal Loss).
",5.1. Experiments on controlled corrupted labels,[0],[0]
"Fig. 3 plots the training and test error on the clean validation data, under a representative setting: resnet-101 on CIFAR100 of 40% noise, where the x-axis denotes the training iteration.",5.1. Experiments on controlled corrupted labels,[0],[0]
The y-axis is the validation error on the clean validation in Fig. 3(a) and the mini-batch training error on corrupted labels in Fig. 3(b).,5.1. Experiments on controlled corrupted labels,[0],[0]
"For MentorNet, the training error is computed by ∑ i vi`i.",5.1. Experiments on controlled corrupted labels,[0],[0]
The figure shows two insights.,5.1. Experiments on controlled corrupted labels,[0],[0]
"First, the training error of MentorNet approaches zero.",5.1. Experiments on controlled corrupted labels,[0],[0]
This empirically verifies the convergence of the model.,5.1. Experiments on controlled corrupted labels,[0],[0]
"Second, MentorNet can overcome the overfitting to the corrupted label.",5.1. Experiments on controlled corrupted labels,[0],[0]
"While the training error is decreasing, the test error does not increase in Fig. 3(a).",5.1. Experiments on controlled corrupted labels,[0],[0]
It suggests that the learned curriculum is beneficial for StudentNet.,5.1. Experiments on controlled corrupted labels,[0],[0]
"The sharp change
Table 2.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Comparison of validation accuracy on CIFAR-10 and CIFAR-100 under different noise fractions.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"Resnet-101 StudentNet Inception StudentNet
CIFAR-100",5.1. Experiments on controlled corrupted labels,[0],[0]
CIFAR-10 CIFAR-100,5.1. Experiments on controlled corrupted labels,[0],[0]
CIFAR-10,5.1. Experiments on controlled corrupted labels,[0],[0]
Method 0.2 0.4 0.8 0.2 0.4 0.8 0.2 0.4 0.8 0.2 0.4 0.8 FullModel 0.60 0.45 0.08 0.82 0.69 0.18 0.43 0.38 0.15 0.76 0.73 0.42 Forgetting 0.61 0.44 0.16 0.78 0.63 0.35 0.42 0.37 0.17 0.76 0.71 0.44 Self-paced 0.70 0.55 0.13 0.89 0.85 0.28 0.44 0.38 0.14 0.80 0.74 0.33,5.1. Experiments on controlled corrupted labels,[0],[0]
Focal Loss 0.59 0.44 0.09 0.79 0.65 0.28 0.43 0.38 0.15 0.77 0.74 0.40,5.1. Experiments on controlled corrupted labels,[0],[0]
Reed Soft 0.62 0.46 0.08 0.81 0.63 0.18 0.42 0.39 0.12 0.78 0.73 0.39,5.1. Experiments on controlled corrupted labels,[0],[0]
"MentorNet PD 0.72 0.56 0.14 0.91 0.77 0.33 0.44 0.39 0.16 0.79 0.74 0.44 MentorNet DD 0.73 0.68 0.35 0.92 0.89 0.49 0.46 0.41 0.20 0.79 0.76 0.46
round the 20k iteration in Fig. 3 is due to the learning rate change.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Besides, our result is consistent with (Zhang et al., 2017a) that deep CNNS is able to get 0 training error on the corrupted training data.",5.1. Experiments on controlled corrupted labels,[0],[0]
Forgetting (the dashed curve) is the only one that does not converge within 30k steps.,5.1. Experiments on controlled corrupted labels,[0],[0]
"As indicated in (Arpit et al., 2017), it is because forgetting reduces the speed at which DNNs memorize.",5.1. Experiments on controlled corrupted labels,[0],[0]
"As suggested in (Zhang et al., 2017b), a not converged model might yield a better result, e.g., stop the model at 20K in Fig. 3.",5.1. Experiments on controlled corrupted labels,[0],[0]
"However, as it is hard to predetermine the time for early stopping, our focus is comparing the converged model.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"Fig. 4 illustrates the best learned data-driven curriculum in our experiments, where the z-axis denotes the weights computed by gm; the y and x axes denote the sample loss and the loss difference to the moving average, where λ is the loss moving average.",5.1. Experiments on controlled corrupted labels,[0],[0]
Two observations can be found in Fig. 4.,5.1. Experiments on controlled corrupted labels,[0],[0]
"First, the learned curriculum changes during the training of the StudentNet.",5.1. Experiments on controlled corrupted labels,[0],[0]
Fig. 4 (a) and (b) are MentorNet learned at different epochs.,5.1. Experiments on controlled corrupted labels,[0],[0]
"As shown, (a) assigns greater weights to samples of big loss more aggressively.",5.1. Experiments on controlled corrupted labels,[0],[0]
"Second, the learned curriculums in Fig. 4 generally satisfy the condition in Proposition 1, i.e., the weight generally decreases with the loss.",5.1. Experiments on controlled corrupted labels,[0],[0]
"It suggests that joint learning of StudentNet and MentorNet optimizes an underlying robust objective.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"Table 3 compares to recent published results under the set-
w eight
w eight
(a) epoch percentage=21 (b) epoch percentage=76
Figure 4.",5.1. Experiments on controlled corrupted labels,[0],[0]
"The data-driven curriculums learned by MentorNet with the resnet-101 at epoch 21 in (a) and 76 in (b).
",5.1. Experiments on controlled corrupted labels,[0],[0]
ting: CIFAR of 40% noise fraction.,5.1. Experiments on controlled corrupted labels,[0],[0]
"We cite the number in (Azadi et al., 2016), and implement other methods using the same resnet-101 StudentNet.",5.1. Experiments on controlled corrupted labels,[0],[0]
"The results show that our result is comparable and even better than the state-of-the-art.
",5.1. Experiments on controlled corrupted labels,[0],[0]
"To verify MentorNet for large-scale training, we apply our method on the ImageNet ILSVRC12",5.1. Experiments on controlled corrupted labels,[0],[0]
"(Deng et al., 2009) benchmark to improve the inception-resnet v2 (Szegedy et al., 2017) model.",5.1. Experiments on controlled corrupted labels,[0],[0]
We train the model on the ImageNet of 40% noise.,5.1. Experiments on controlled corrupted labels,[0],[0]
"Inspired by (Zhang et al., 2017a), we start with an inception-resnet (NoReg) with no regularization (NoReg) and add weight decay, dropout, and data augmentation to the model.",5.1. Experiments on controlled corrupted labels,[0],[0]
Table 4 shows the comparison.,5.1. Experiments on controlled corrupted labels,[0],[0]
"As shown, MentorNet improves the performance of both the inception-resnet without regularization (NoReg) and with full regularization (FullModel).",5.1. Experiments on controlled corrupted labels,[0],[0]
It also outperforms the forgetting baseline (dropout keep probability = 0.2).,5.1. Experiments on controlled corrupted labels,[0],[0]
The results suggest that MentorNet can improve deep CNNs on the large-scale training on corrupted labels.,5.1. Experiments on controlled corrupted labels,[0],[0]
"To verify MentorNet on real-world noisy labels, we conduct experiments on the large WebVision benchmark (Li et al., 2017a).",5.2. Experiments on real-world noisy labels,[0],[0]
"It contains 2.4 million images of real-world noisy labels, crawled from the web using the 1,000 concepts in ImageNet ILSVRC12.",5.2. Experiments on real-world noisy labels,[0],[0]
"We download the resized images from
the official website1.",5.2. Experiments on real-world noisy labels,[0],[0]
"The inception-resenet v2 (Szegedy et al., 2017) is used as our StudentNet, trained using a distributed asynchronized momentum optimizer on 50 GPUs.",5.2. Experiments on real-world noisy labels,[0],[0]
"Since the dataset is very big, for quick experiments, we compare baseline methods using the Google image subset on the first 50 classes.",5.2. Experiments on real-world noisy labels,[0],[0]
We use Mini to denote this subset and Entire for the entire WebVision.,5.2. Experiments on real-world noisy labels,[0],[0]
"All the models are evaluated on the clean ILSVRC12 and WebVision validation set.
",5.2. Experiments on real-world noisy labels,[0],[0]
Table 5 lists the comparison result.,5.2. Experiments on real-world noisy labels,[0],[0]
"As we see, the proposed MentorNet significantly improves baseline methods on real-world noisy labels.",5.2. Experiments on real-world noisy labels,[0],[0]
The method marked by the start indicates it uses a pre-trained ImageNet model to obtain additional 30k labels for 118 classes.,5.2. Experiments on real-world noisy labels,[0],[0]
"Following the same protocol, MentorNet* is trained using the additional labels.",5.2. Experiments on real-world noisy labels,[0],[0]
The results show that our method outperforms the baseline methods on real-world noisy labels.,5.2. Experiments on real-world noisy labels,[0],[0]
"To the best of our knowledge, it achieves the best-published result on the WebVision (Li et al., 2017a) benchmark.",5.2. Experiments on real-world noisy labels,[0],[0]
"Curriculum learning (CL), proposed by Bengio et al. (2009), is a learning paradigm in which a model is learned by gradually including from easy to complex samples in training so as to increase the learning entropy (Bengio et al., 2009).",6. Related Work,[0],[0]
"From the human behavioral perspective, Khan et al. (2011) have shown that CL is consistent with the principle of human teaching.",6. Related Work,[0],[0]
"CL has been empirically verified in a variety of problems, such as computer vision (Supancic & Ramanan, 2013; Chen & Gupta, 2015), natural language
1https://www.vision.ee.ethz.ch/webvision/download.html
processing (Turian et al., 2010), multitask learning (Graves et al., 2017).",6. Related Work,[0],[0]
A common CL approach is to predefine a curriculum.,6. Related Work,[0],[0]
"For example, Kumar et al. (2010) proposed a curriculum called self-paced learning which favors training samples of smaller loss.",6. Related Work,[0],[0]
"After that, many predefined curriculums were proposed, e.g., in (Supancic & Ramanan, 2013; Jiang et al., 2014; 2015; Sangineto et al., 2016; Chang et al., 2017; Ma et al., 2017a;b).",6. Related Work,[0],[0]
"For example, Jiang et al. (2014) introduced a curriculum of using easy and diverse samples.",6. Related Work,[0],[0]
Fan et al. (2017) proposed to use predefined sample weighting schemes as an implicit way to define a curriculum.,6. Related Work,[0],[0]
"Previous work has shown that predefined curriculums are useful in overcoming noisy labels (Chen & Gupta, 2015; Liang et al., 2016; Lin et al., 2017a).",6. Related Work,[0],[0]
"In parallel to CL, the sample weighting schemes were also studied in (Lin et al., 2017a; Wang et al., 2017; Fan et al., 2018; Dehghani et al., 2018).",6. Related Work,[0],[0]
"Compared to the existing work, our paper presents a new way of learning data-driven curriculums for deep networks trained on corrupted labels.
",6. Related Work,[0],[0]
Our work is related to the weakly-supervised learning methods.,6. Related Work,[0],[0]
"Among recent contributions, Reed et al. (2014) developed a robust loss to model “prediction consistency”.",6. Related Work,[0],[0]
Menon et al. (2015) used class-probability estimation to study the corruption process.,6. Related Work,[0],[0]
Sukhbaatar et al. (2014) proposed a noise transformation to estimate the noise distribution.,6. Related Work,[0],[0]
The transformation matrix needs to be periodically updated and is non-trivial to learn.,6. Related Work,[0],[0]
"To address the issue, Goldberger et al. (2017) proposed to add an additional softmax layer end-to-end with the base model.",6. Related Work,[0],[0]
Azadi et al. (2016) tackled this problem by a regularizer called AIR.,6. Related Work,[0],[0]
This method was shown to be effective but it relied on additional clean labels to train the representation.,6. Related Work,[0],[0]
"More recently, methods utilized additional labels for label cleaning (Veit et al., 2017), knowledge distillation (Li et al., 2017b) or semi-supervised learning (Vahdat, 2017; Dehghani et al., 2017).",6. Related Work,[0],[0]
"Different from previous work, we focus on learning curriculum to train very deep CNNs on corrupted labels from scratch.",6. Related Work,[0],[0]
"In addition, clean labels are not always needed for our method.",6. Related Work,[0],[0]
"In Section 5.1, the MentorNet is learned on a small subset of CIFAR-10 and applied to CIFAR-100",6. Related Work,[0],[0]
"In this paper, we presented a novel method for training deep CNNs on corrupted labels.",7. Conclusions,[0],[0]
Our work was built on curriculum learning and advanced the methodology by proposing to learn data-driven curriculum via a neural network called MentorNet.,7. Conclusions,[0],[0]
We proposed an algorithm for jointly optimizing deep CNNs with MentorNet on large-scale data.,7. Conclusions,[0],[0]
We conducted comprehensive experiments on datasets of controlled and real-world noise.,7. Conclusions,[0],[0]
Our empirical results showed that generalization performance of deep CNNs trained on corrupted labels can be effectively improved by the learned data-driven curriculum.,7. Conclusions,[0],[0]
"The authors would like to thank anonymous reviewers for helpful comments and Deyu Meng, Sergey Ioffe, and Chong Wang for meaningful discussions and kind support.",Acknowledgements,[0],[0]
Recent deep networks are capable of memorizing the entire data even when the labels are completely random.,abstractText,[0],[0]
"To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet.",abstractText,[0],[0]
"During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct.",abstractText,[0],[0]
"Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet.",abstractText,[0],[0]
Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data.,abstractText,[0],[0]
"Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.",abstractText,[0],[0]
MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,title,[0],[0]
"Neural network learning is typically slow, where back propagation usually dominates the computational cost during the learning process.",1. Introduction,[0],[0]
Back propagation entails a high computational cost because it needs to compute full gradients and update all model parameters in each learning step.,1. Introduction,[0],[0]
"It is not uncommon for a neural network to have a massive number of model parameters.
",1. Introduction,[0],[0]
"In this study, we propose a minimal effort back propagation method, which we call meProp, for neural network learning.",1. Introduction,[0],[0]
"The idea is that we compute only a very small but critical portion of the gradient information, and update only the corresponding minimal portion of the parameters in each learning step.",1. Introduction,[0],[0]
"This leads to sparsified gradients,
1School of Electronics Engineering and Computer Science, Peking University, China 2MOE Key Laboratory of Computational Linguistics, Peking University, China.",1. Introduction,[0],[0]
"Correspondence to: Xu Sun <xusun@pku.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
such that only highly relevant parameters are updated and other parameters stay untouched.,1. Introduction,[0],[0]
"The sparsified back propagation leads to a linear reduction in the computational cost.
",1. Introduction,[0],[0]
"To realize our approach, we need to answer two questions.",1. Introduction,[0],[0]
The first question is how to find the highly relevant subset of the parameters from the current sample in stochastic learning.,1. Introduction,[0],[0]
We propose a top-k search method to find the most important parameters.,1. Introduction,[0],[0]
"Interestingly, experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass.",1. Introduction,[0],[0]
This does not result in a larger number of training iterations.,1. Introduction,[0],[0]
"The proposed method is general-purpose and it is independent of specific models and specific optimizers (e.g., Adam and AdaGrad).
",1. Introduction,[0],[0]
The second question is whether or not this minimal effort back propagation strategy will hurt the accuracy of the trained models.,1. Introduction,[0],[0]
"We show that our strategy does not degrade the accuracy of the trained model, even when a very small portion of the parameters is updated.",1. Introduction,[0],[0]
"More interestingly, our experimental results reveal that our strategy actually improves the model accuracy in most cases.",1. Introduction,[0],[0]
"Based on our experiments, we find that it is probably because the minimal effort update does not modify weakly relevant parameters in each update, which makes overfitting less likely, similar to the dropout effect.
",1. Introduction,[0],[0]
"The contributions of this work are as follows:
• We propose a sparsified back propagation technique for neural network learning, in which only a small subset of the full gradient is computed to update the model parameters.",1. Introduction,[0],[0]
Experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass.,1. Introduction,[0],[0]
"This does not result in a larger number of training iterations.
",1. Introduction,[0],[0]
"• Surprisingly, our experimental results reveal that the accuracy of the resulting models is actually improved, rather than degraded.",1. Introduction,[0],[0]
"We demonstrate this effect by conducting experiments on different deep learning models (LSTM and MLP), various optimization methods (Adam and AdaGrad), and diverse tasks (natural language processing and image recognition).",1. Introduction,[0],[0]
We propose a simple yet effective technique for neural network learning.,2. Proposed Method,[0],[0]
The forward propagation is computed as usual.,2. Proposed Method,[0],[0]
"During back propagation, only a small subset of the full gradient is computed to update the model parameters.",2. Proposed Method,[0],[0]
The gradient vectors are “quantized” so that only the top-k components in terms of magnitude are kept.,2. Proposed Method,[0],[0]
We first present the proposed method and then describe the implementation details.,2. Proposed Method,[0],[0]
"Forward propagation of neural network models, including feedforward neural networks, RNN, LSTM, consists of linear transformations and non-linear transformations.",2.1. meProp,[0],[0]
"For simplicity, we take a computation unit with one linear transformation and one non-linear transformation as an example:
y =Wx (1)
z = σ(y) (2)
where W ∈ Rn×m,",2.1. meProp,[0],[0]
"x ∈ Rm, y ∈",2.1. meProp,[0],[0]
"Rn, z ∈ Rn, m is the dimension of the input vector, n is the dimension of the output vector, and σ is a non-linear function (e.g., relu, tanh, and sigmoid).",2.1. meProp,[0],[0]
"During back propagation, we need to compute the gradient of the parameter matrix W and the input vector x:
∂z
∂Wij",2.1. meProp,[0],[0]
"= σ
′",2.1. meProp,[0],[0]
"ix T j (1 ≤ i ≤ n, 1 ≤ j ≤ m) (3)
∂z",2.1. meProp,[0],[0]
∂xi = ∑ j WTijσ ′,2.1. meProp,[0],[0]
"j (1 ≤ j ≤ n, 1 ≤",2.1. meProp,[0],[0]
"i ≤ m) (4)
",2.1. meProp,[0],[0]
"where σ ′
i ∈",2.1. meProp,[0],[0]
Rn means ∂zi∂yi .,2.1. meProp,[0],[0]
"We can see that the computational cost of back propagation is directly proportional to the dimension of output vector n.
The proposed meProp uses approximate gradients by keeping only top-k elements based on the magnitude values.",2.1. meProp,[0],[0]
"That is, only the top-k elements with the largest absolute values are kept.",2.1. meProp,[0],[0]
"For example, suppose a vector v = 〈1, 2, 3,−4〉, then top2(v) = 〈0, 0, 3,−4〉.",2.1. meProp,[0],[0]
We denote the indices of vector σ ′,2.1. meProp,[0],[0]
"(y)’s top-k values as {t1, t2, ..., tk}(1 ≤",2.1. meProp,[0],[0]
"k ≤ n), and the approximate gradient of the parameter matrix W and input vector x is:
∂z
∂Wij",2.1. meProp,[0],[0]
"← σ
′",2.1. meProp,[0],[0]
"ix T j if i ∈ {t1, t2, ..., tk} else 0 (5)
∂z",2.1. meProp,[0],[0]
∂xi ← ∑ j WTijσ ′,2.1. meProp,[0],[0]
"j if j ∈ {t1, t2, ..., tk} else 0 (6)
",2.1. meProp,[0],[0]
"As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost.
",2.1. meProp,[0],[0]
Figure 1 is an illustration of meProp for a single computation unit of neural models.,2.1. meProp,[0],[0]
The original back propagation uses the full gradient of the output vectors to compute the gradient of the parameters.,2.1. meProp,[0],[0]
"The proposed method selects the top-k values of the gradient of the output vector, and backpropagates the loss through the corresponding subset of the total model parameters.
",2.1. meProp,[0],[0]
"As for a complete neural network framework with a loss L, the original back propagation computes the gradient of the parameter matrix W as:
∂L ∂W = ∂L ∂y · ∂y ∂W
(7)
while the gradient of the input vector x is:
∂L ∂x = ∂y ∂x · ∂L ∂y
(8)
The proposed meProp selects top-k elements of the gradient ∂L∂y to approximate the original gradient, and passes
them through the gradient computation graph according to the chain rule.",2.1. meProp,[0],[0]
"Hence, the gradient of W goes to:
∂L
∂W ← topk(
∂L ∂y ) · ∂y ∂W
(9)
while the gradient of the vector x is:
∂L ∂x ← ∂y ∂x · topk( ∂L ∂y ) (10)
",2.1. meProp,[0],[0]
Figure 2 shows an illustration of the computational flow of meProp.,2.1. meProp,[0],[0]
"The forward propagation is the same as traditional forward propagation, which computes the output vector via a matrix multiplication operation between two input tensors.",2.1. meProp,[0],[0]
The original back propagation computes the full gradient for the input vector and the weight matrix.,2.1. meProp,[0],[0]
"For meProp, back propagation computes an approximate gradient by keeping top-k values of the backward flowed gradient and masking the remaining values to 0.
",2.1. meProp,[0],[0]
Figure 3 further shows the computational flow of meProp for the mini-batch case.,2.1. meProp,[0],[0]
"We have coded two neural network models, including an LSTM model for part-of-speech (POS) tagging, and a feedforward NN model (MLP) for transition-based dependency
parsing and MNIST image recognition.",2.2. Implementation,[0],[0]
"We use the optimizers with automatically adaptive learning rates, including Adam (Kingma & Ba, 2014) and AdaGrad (Duchi et al., 2011).",2.2. Implementation,[0],[0]
"In our implementation, we make no modification to the optimizers, although there are many zero elements in the gradients.
",2.2. Implementation,[0],[0]
Most of the experiments on CPU are conducted on the framework coded in C# on our own.,2.2. Implementation,[0],[0]
"This framework builds a dynamic computation graph of the model for each sample, making it suitable for data in variable lengths.",2.2. Implementation,[0],[0]
"A typical training procedure contains four parts: building the computation graph, forward propagation, back propagation, and parameter update.",2.2. Implementation,[0],[0]
We also have an implementation based on the PyTorch framework for GPU based experiments.,2.2. Implementation,[0],[0]
The proposed method aims to reduce the complexity of the back propagation by reducing the elements in the computationally intensive operations.,2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"In our preliminary observations, matrix-matrix or matrix-vector multiplication consumed more than 90% of the time of back propagation.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"In our implementation, we apply meProp only to the back propagation from the output of the multiplication to its inputs.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"For other element-wise operations (e.g., activation functions), the original back propagation procedure is kept, be-
cause those operations are already fast enough compared with matrix-matrix or matrix-vector multiplication operations.
",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"If there are multiple hidden layers, the top-k sparsification needs to be applied to every hidden layer, because the sparsified gradient will again be dense from one layer to another.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"That is, in meProp the gradients are sparsified with a top-k operation at the output of every hidden layer.
",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"While we apply meProp to all hidden layers using the same k of top-k, usually the k for the output layer could be different from the k for the hidden layers, because the output layer typically has a very different dimension compared with the hidden layers.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"For example, there are 10 tags in the MNIST task, so the dimension of the output layer is 10, and we use an MLP with the hidden dimension of 500.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"Thus, the best k for the output layer could be different from that of the hidden layers.
2.2.2.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"CHOICE OF TOP-k ALGORITHMS
Instead of sorting the entire vector, we use the well-known min-heap based top-k selection method, which is slightly changed to focus on memory reuse.",2.2.1. WHERE TO APPLY MEPROP,[0],[0]
The algorithm has a time complexity of O(n log k) and a space complexity of O(k).,2.2.1. WHERE TO APPLY MEPROP,[0],[0]
"Riedmiller and Braun (1993) proposed a direct adaptive method for fast learning, which performs a local adaptation of the weight update according to the behavior of the error function.",3. Related Work,[0],[0]
Tollenaere (1990) also proposed an adaptive acceleration strategy for back propagation.,3. Related Work,[0],[0]
"Dropout (Srivastava et al., 2014) is proposed to improve training speed and reduce the risk of overfitting.",3. Related Work,[0],[0]
"Sparse coding is a class of unsupervised methods for learning sets of over-complete bases to represent data efficiently (Olshausen & Field, 1996).",3. Related Work,[0],[0]
Ranzato et al. (2006) proposed a sparse autoencoder model for learning sparse over-complete features.,3. Related Work,[0],[0]
"The proposed method is quite different compared with those prior studies on back propagation, dropout, and sparse coding.
",3. Related Work,[0],[0]
"The sampled-output-loss methods (Jean et al., 2015) are limited to the softmax layer (output layer) and are only based on random sampling, while our method does not have those limitations.",3. Related Work,[0],[0]
"The sparsely-gated mixture-ofexperts (Shazeer et al., 2017) only sparsifies the mixtureof-experts gated layer and it is limited to the specific setting of mixture-of-experts, while our method does not have those limitations.",3. Related Work,[0],[0]
"There are also prior studies focusing on reducing the communication cost in distributed systems (Seide et al., 2014; Dryden et al., 2016), by quanti-
zing each value of the gradient from 32-bit float to only 1-bit.",3. Related Work,[0],[0]
Those settings are also different from ours.,3. Related Work,[0],[0]
"To demonstrate that the proposed method is generalpurpose, we perform experiments on different models (LSTM/MLP), various training methods (Adam/AdaGrad), and diverse tasks.
",4. Experiments,[0],[0]
"Part-of-Speech Tagging (POS-Tag): We use the standard benchmark dataset in prior work (Collins, 2002), which is derived from the Penn Treebank corpus, and use sections 0-18 of the Wall Street Journal (WSJ) for training (38,219 examples), and sections 22-24 for testing (5,462 examples).",4. Experiments,[0],[0]
The evaluation metric is per-word accuracy.,4. Experiments,[0],[0]
"A popular model for this task is the LSTM model (Hochreiter & Schmidhuber, 1997),1 which is used as our baseline.
",4. Experiments,[0],[0]
"1In this work, we use the bi-directional LSTM (Bi-LSTM) as the implementation of LSTM.
",4. Experiments,[0],[0]
Transition-based Dependency Parsing (Parsing):,4. Experiments,[0],[0]
"Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) for evaluation.",4. Experiments,[0],[0]
"We follow the standard split of the corpus and use sections 2-21 as the training set (39,832 sentences, 1,900,056 transition examples),2 section 22 as the development set (1,700 sentences, 80,234 transition examples) and section 23 as the final test set (2,416 sentences, 113,368 transition examples).",4. Experiments,[0],[0]
The evaluation metric is unlabeled attachment score (UAS).,4. Experiments,[0],[0]
"We implement a parser using MLP following Chen and Manning (2014), which is used as our baseline.
",4. Experiments,[0],[0]
MNIST Image Recognition (MNIST):,4. Experiments,[0],[0]
"We use the MNIST handwritten digit dataset (LeCun et al., 1998) for evaluation.",4. Experiments,[0],[0]
"MNIST consists of 60,000 28×28 pixel training images and additional 10,000 test examples.",4. Experiments,[0],[0]
Each image contains a single numerical digit (0-9).,4. Experiments,[0],[0]
"We select the first 5,000 images of the training images as the development set and the rest as the training set.",4. Experiments,[0],[0]
The evaluation metric is per-image accuracy.,4. Experiments,[0],[0]
We use the MLP model as the baseline.,4. Experiments,[0],[0]
We set the dimension of the hidden layers to 500 for all the tasks.,4.1. Experimental Settings,[0],[0]
"For POS-Tag, the input dimension is 1 (word)× 50 (dim per word) + 7 (features) × 20 (dim per feature) = 190, and the output dimension is 45.",4.1. Experimental Settings,[0],[0]
"For Parsing, the input dimension is 48 (features) × 50 (dim per feature) = 2400, and the output dimension is 25.",4.1. Experimental Settings,[0],[0]
"For MNIST, the input dimension is 28 (pixels per row)× 28 (pixels per column)× 1 (dim per pixel) = 784, and the output dimension is 10.",4.1. Experimental Settings,[0],[0]
"As discussed in Section 2, the optimal k of top-k for the
2A transition example consists of a parsing context and its optimal transition action.
output layer could be different from the hidden layers, because their dimensions could be very different.",4.1. Experimental Settings,[0],[0]
"For Parsing and MNIST, we find using the same k for the output and the hidden layers works well, and we simply do so.",4.1. Experimental Settings,[0],[0]
"For another task, POS-Tag, we find the the output layer should use a different k from the hidden layers.",4.1. Experimental Settings,[0],[0]
"For simplicity, we do not apply meProp to the output layer for POS-Tag, because in this task we find the computational cost of the output layer is almost negligible compared with other layers.
",4.1. Experimental Settings,[0],[0]
The hyper-parameters are tuned based on the development data.,4.1. Experimental Settings,[0],[0]
"For the Adam optimization method, we find the default hyper-parameters work well on development sets, which are as follows: the learning rate α = 0.001, and β1 = 0.9, β2 = 0.999, = 1×10−8.",4.1. Experimental Settings,[0],[0]
"For the AdaGrad learner, the learning rate is set to α = 0.01, 0.01, 0.1 for POSTag, Parsing, and MNIST, respectively, and = 1× 10−6.",4.1. Experimental Settings,[0],[0]
The experiments on CPU are conducted on a computer with the INTEL(R) Xeon(R) 3.0GHz CPU.,4.1. Experimental Settings,[0],[0]
The experiments on GPU are conducted on NVIDIA GeForce GTX 1080.,4.1. Experimental Settings,[0],[0]
"In this experiment, the LSTM is based on one hidden layer and the MLP is based on two hidden layers (experiments on more hidden layers will be presented later).",4.2. Experimental Results,[0],[0]
"We conduct experiments on different optimization methods, including AdaGrad and Adam.",4.2. Experimental Results,[0],[0]
"Since meProp is applied to the linear transformations (which entail the major computational cost), we report the linear transformation related backprop time as Backprop Time.",4.2. Experimental Results,[0],[0]
"It does not include non-linear activations, which usually have only less than 2% computational cost.",4.2. Experimental Results,[0],[0]
"The total time of back propagation, including nonlinear activations, is reported as Overall Backprop Time.",4.2. Experimental Results,[0],[0]
"Based on the development set and prior work, we set the mini-batch size to 1 (sentence), 10,000 (transition examples), and 10 (images) for POS-Tag, Parsing, and MNIST, respectively.",4.2. Experimental Results,[0],[0]
"Using 10,000 transition examples for Parsing follows Chen and Manning (2014).
",4.2. Experimental Results,[0],[0]
"Table 1 shows the results based on different models and
different optimization methods.",4.2. Experimental Results,[0],[0]
"In the table, meProp means applying meProp to the corresponding baseline model, h = 500 means that the hidden layer dimension is 500, and k = 20 means that meProp uses top-20 elements (among 500 in total) for back propagation.",4.2. Experimental Results,[0],[0]
"Note that, for fair comparisons, all experiments are first conducted on the development data and the test data is not observable.",4.2. Experimental Results,[0],[0]
"Then, the optimal number of iterations is decided based on the optimal score on development data, and the model of this iteration is used upon the test data to obtain the test scores.
",4.2. Experimental Results,[0],[0]
"As we can see, applying meProp can substantially speed up the back propagation.",4.2. Experimental Results,[0],[0]
It provides a linear reduction in the computational cost.,4.2. Experimental Results,[0],[0]
"Surprisingly, results demonstrate that we can update only 1–4% of the weights at each back propagation pass.",4.2. Experimental Results,[0],[0]
This does not result in a larger number of training iterations.,4.2. Experimental Results,[0],[0]
"More surprisingly, the accuracy of the resulting models is actually improved rather than decreased.",4.2. Experimental Results,[0],[0]
"The main reason could be that the minimal effort update does not modify weakly relevant parameters, which makes overfitting less likely, similar to the dropout effect.
",4.2. Experimental Results,[0],[0]
"Table 2 shows the overall forward propagation time, the overall back propagation time, and the training time by summing up forward and backward propagation time.",4.2. Experimental Results,[0],[0]
"As we can see, back propagation has the major computational cost in training LSTM/MLP.
",4.2. Experimental Results,[0],[0]
The results are consistent among AdaGrad and Adam.,4.2. Experimental Results,[0],[0]
The results demonstrate that meProp is independent of specific optimization methods.,4.2. Experimental Results,[0],[0]
"For simplicity, in the following experiments the optimizer is based on Adam.",4.2. Experimental Results,[0],[0]
"In Figure 4 (left), we vary the k of top-k meProp to compare the test accuracy on different ratios of meProp backprop.",4.3. Varying Backprop Ratio,[0],[0]
"For example, when k=5, it means that the backprop ratio is 5/500=1%.",4.3. Varying Backprop Ratio,[0],[0]
The optimizer is Adam.,4.3. Varying Backprop Ratio,[0],[0]
"As we can see, meProp achieves consistently better accuracy than the baseline.",4.3. Varying Backprop Ratio,[0],[0]
"The best test accuracy of meProp, 98.15% (+0.33),
is actually better than the one reported in Table 1.",4.3. Varying Backprop Ratio,[0],[0]
It will be interesting to check the role of top-k elements.,4.4. Top-k vs. Random,[0],[0]
Figure 4 (middle) shows the results of top-k meProp vs. random meProp.,4.4. Top-k vs. Random,[0],[0]
The random meProp means that random elements (instead of top-k ones) are selected for back propagation.,4.4. Top-k vs. Random,[0],[0]
"As we can see, the top-k version works better than the random version.",4.4. Top-k vs. Random,[0],[0]
It suggests that top-k elements contain the most important information of the gradients.,4.4. Top-k vs. Random,[0],[0]
We still have a question: does the top-k meProp work well simply because the original model does not require that big dimension of the hidden layers?,4.5. Varying Hidden Dimension,[0],[0]
"For example, the meProp (topk=5) works simply because the LSTM works well with the hidden dimension of 5, and there is no need to use the hidden dimension of 500.",4.5. Varying Hidden Dimension,[0],[0]
"To examine this, we perform experiments on using the same hidden dimension as k, and the results are shown in Table 3.",4.5. Varying Hidden Dimension,[0],[0]
"As we can see, however, the results of the small hidden dimensions are much worse than those of meProp.
",4.5. Varying Hidden Dimension,[0],[0]
"In addition, Figure 4 (right) shows more detailed curves by varying the value of k.",4.5. Varying Hidden Dimension,[0],[0]
"In the figure, different k gives different backprop ratio for meProp and different hidden dimension ratio for LSTM/MLP.",4.5. Varying Hidden Dimension,[0],[0]
"As we can see, the answer to that question is negative: meProp does not rely on redundant hidden layer elements.",4.5. Varying Hidden Dimension,[0],[0]
"Since we have observed that meProp can reduce overfitting of deep learning, a natural question is that if meProp is reducing the same type of overfitting risk as dropout.",4.6. Adding Dropout,[0],[0]
"Thus, we use development data to find a proper value of the dropout rate on those tasks, and then further add meProp to check if further improvement is possible.
",4.6. Adding Dropout,[0],[0]
Table 4 shows the results.,4.6. Adding Dropout,[0],[0]
"As we can see, meProp can achieve further improvement over dropout.",4.6. Adding Dropout,[0],[0]
"In particular, meProp has an improvement of 0.46 UAS on Parsing.",4.6. Adding Dropout,[0],[0]
The results suggest that the type of overfitting that meProp reduces is probably different from that of dropout.,4.6. Adding Dropout,[0],[0]
"Thus, a model should be able to take advantage of both meProp and dropout to reduce overfitting.",4.6. Adding Dropout,[0],[0]
Another question is whether or not meProp relies on shallow models with only a few hidden layers.,4.7. Adding More Hidden Layers,[0],[0]
"To answer this question, we also perform experiments on more hidden layers, from 2 hidden layers to 5 hidden layers.",4.7. Adding More Hidden Layers,[0],[0]
We find setting the dropout rate to 0.1 works well for most cases of different numbers of layers.,4.7. Adding More Hidden Layers,[0],[0]
"For simplicity of comparison, we set the same dropout rate to 0.1 in this experiment.",4.7. Adding More Hidden Layers,[0],[0]
Table 5 shows that adding the number of hidden layers does not hurt the performance of meProp.,4.7. Adding More Hidden Layers,[0],[0]
"For implementing meProp on GPU, the simplest solution is to treat the entire mini-batch as a “big training example”, where the top-k operation is based on the averaged values of all examples in the mini-batch.",4.8. Speedup on GPU,[0],[0]
"In this way, the big sparse matrix of the mini-batch will have consistent sparse
patterns among examples, and this consistent sparse matrix can be transformed into a small dense matrix by removing the zero values.",4.8. Speedup on GPU,[0],[0]
"We call this implementation as simple unified top-k. This experiment is based on PyTorch.
",4.8. Speedup on GPU,[0],[0]
"Despite its simplicity, Table 6 shows the good performance of this implementation, which is based on the mini-batch size of 50.",4.8. Speedup on GPU,[0],[0]
We also find the speedup on GPU is less significant when the hidden dimension is low.,4.8. Speedup on GPU,[0],[0]
"The reason is that our GPU’s computational power is not fully consumed by the baseline (with small hidden layers), so that the normal back propagation is already fast enough, making it hard for meProp to achieve substantial speedup.",4.8. Speedup on GPU,[0],[0]
"For example, supposing a GPU can finish 1000 operations in one cycle, there could be no speed difference between a method with 100 and a method with 10 operations.",4.8. Speedup on GPU,[0],[0]
"Indeed, we find MLP (h=64) and MLP (h=512) have almost the same GPU speed even on forward propagation (i.e., without meProp), while theoretically there should be an 8x difference.",4.8. Speedup on GPU,[0],[0]
"With GPU, the forward propagation time of MLP (h=64) and MLP (h=512) is 572ms and 644ms, respectively.",4.8. Speedup on GPU,[0],[0]
"This provides evidence for our hypothesis that our GPU is not fully consumed with the small hidden dimensions.
",4.8. Speedup on GPU,[0],[0]
"Thus, the speedup test on GPU is more meaningful for the heavy models, such that the baseline can at least fully consume the GPU’s computational power.",4.8. Speedup on GPU,[0],[0]
"To check this, we test the GPU speedup on synthetic data of matrix multiplication with a larger hidden dimension.",4.8. Speedup on GPU,[0],[0]
"Indeed, Table 7 shows that meProp achieves much higher speed than the traditional backprop with the large hidden dimension.",4.8. Speedup on GPU,[0],[0]
"Furthermore, we test the GPU speedup on MLP with the large hidden dimension (Dryden et al., 2016).",4.8. Speedup on GPU,[0],[0]
"Table 8 shows that
meProp also has substantial GPU speedup on MNIST with the large hidden dimension.",4.8. Speedup on GPU,[0],[0]
"In this experiment, the speedup is based on Overall Backprop Time (see the prior definition).",4.8. Speedup on GPU,[0],[0]
"Those results demonstrate that meProp can achieve good speedup on GPU when it is applied to heavy models.
",4.8. Speedup on GPU,[0],[0]
"Finally, there are potentially other implementation choices of meProp on GPU.",4.8. Speedup on GPU,[0],[0]
"For example, another natural solution is to use a big sparse matrix to represent the sparsified gradient of the output of a mini-batch.",4.8. Speedup on GPU,[0],[0]
"Then, the sparse matrix multiplication library can be used to accelerate the computation.",4.8. Speedup on GPU,[0],[0]
This could be an interesting direction of future work.,4.8. Speedup on GPU,[0],[0]
"The POS tagging task is a well-known benchmark task, with the accuracy reports from 97.2% to 97.4% (Toutanova et al., 2003; Sun, 2014; Shen et al., 2007; Tsuruoka et al., 2011; Collobert et al., 2011; Huang et al., 2015).",4.9. Related Systems on the Tasks,[0],[0]
"Our method achieves 97.31% (Table 4).
",4.9. Related Systems on the Tasks,[0],[0]
"For the transition-based dependency parsing task, existing approaches typically can achieve the UAS score from 91.4 to 91.5 (Zhang & Clark, 2008; Nivre et al., 2007; Huang & Sagae, 2010).",4.9. Related Systems on the Tasks,[0],[0]
"As one of the most popular transition-based parsers, MaltParser (Nivre et al., 2007) has 91.5 UAS.",4.9. Related Systems on the Tasks,[0],[0]
Chen and Manning (2014) achieves 92.0 UAS using neural networks.,4.9. Related Systems on the Tasks,[0],[0]
"Our method achieves 91.99 UAS (Table 4).
",4.9. Related Systems on the Tasks,[0],[0]
"For MNIST, the MLP based approaches can achieve 98– 99% accuracy, often around 98.3% (LeCun et al., 1998; Simard et al., 2003; Ciresan et al., 2010).",4.9. Related Systems on the Tasks,[0],[0]
Our method achieves 98.37% (Table 5).,4.9. Related Systems on the Tasks,[0],[0]
"With the help from convolutional layers and other techniques, the accuracy can be improved to over 99% (Jarrett et al., 2009; Ciresan et al., 2012).",4.9. Related Systems on the Tasks,[0],[0]
"Our method can also be improved with those additional techniques, which, however, are not the focus of this paper.",4.9. Related Systems on the Tasks,[0],[0]
"The back propagation in deep learning tries to modify all parameters in each stochastic update, which is inefficient and may even lead to overfitting due to unnecessary modification of many weakly relevant parameters.",5. Conclusions,[0],[0]
"We propose a minimal effort back propagation method (meProp), in which we compute only a very small but critical portion of the gradient, and modify only the corresponding small portion of the parameters in each update.",5. Conclusions,[0],[0]
This leads to very sparsified gradients to modify only highly relevant parameters for the given training sample.,5. Conclusions,[0],[0]
The proposed meProp is independent of the optimization method.,5. Conclusions,[0],[0]
"Experiments show that meProp can reduce the computational cost of back propagation by one to two orders of magnitude via updating only 1–4% parameters, and yet improve the model accuracy in most cases.",5. Conclusions,[0],[0]
The authors would like to thank the anonymous reviewers for insightful comments and suggestions on this paper.,Acknowledgements,[0],[0]
"This work was supported in part by National Natural Science Foundation of China (No. 61673028), National High Technology Research and Development Program of China (863 Program, No. 2015AA015404), and an Okawa Research Grant (2016).",Acknowledgements,[0],[0]
We propose a simple yet effective technique for neural network learning.,abstractText,[0],[0]
The forward propagation is computed as usual.,abstractText,[0],[0]
"In back propagation, only a small subset of the full gradient is computed to update the model parameters.",abstractText,[0],[0]
The gradient vectors are sparsified in such a way that only the top-k elements (in terms of magnitude) are kept.,abstractText,[0],[0]
"As a result, only k rows or columns (depending on the layout) of the weight matrix are modified, leading to a linear reduction (k divided by the vector dimension) in the computational cost.",abstractText,[0],[0]
"Surprisingly, experimental results demonstrate that we can update only 1–4% of the weights at each back propagation pass.",abstractText,[0],[0]
This does not result in a larger number of training iterations.,abstractText,[0],[0]
"More interestingly, the accuracy of the resulting models is actually improved rather than degraded, and a detailed analysis is given.",abstractText,[0],[0]
meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting,title,[0],[0]
"Consider the following common academic (or similar) hiring scenario: The dean has promised your department 3 faculty slots, in any areas.",1. Introduction,[0],[0]
Your goal is to hire the best candidates possible — but how should you identify them?,1. Introduction,[0],[0]
"An immediate problem is that candidates are incomparable across subfields, because, among other things, standards of publication, citation counts, and letter-writing styles can vary considerably across subfields.",1. Introduction,[0],[0]
"An attractive way to rank candidates is according to how strong they are relative to others working in the same field, to whom they are directly comparable.",1. Introduction,[0],[0]
"If we model each subfield as corresponding to a different distribution over metrics that are monotonically increasing in candidate quality, this is the value we get when we evaluate the CDF function of the distribution on a candidate’s realized value.",1. Introduction,[0],[0]
"But because
1University of Pennsylvania, Philadelphia, USA.",1. Introduction,[0],[0]
Correspondence to: Zhiwei Steven Wu,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"steven7woo@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"the number of candidates each year is small, simply comparing each candidate to their direct competitors this year — i.e. taking their empirical CDF values as truth — would lead to a noisy ranking: it could be that due to chance, the best candidate this year in subfield",1. Introduction,[0],[0]
"A would be a mediocre candidate in a typical year, and the top two candidates in subfield B would each be the top candidate in a typical year.",1. Introduction,[0],[0]
"We would prefer to evaluate our success by considering the unknown true CDF value of each candidate.1 Similar situations, in which we must select a high quality set of candidates from multiple, mutually incomparable groups, arise frequently.",1. Introduction,[0],[0]
"Some affirmative action policies are premised on the assertion that SAT scores and other measures may not be directly comparable across different groups (e.g. due to only advantaged groups having the financial resources for test preparation courses and multiple retakes).
",1. Introduction,[0],[0]
"For various reasons, in these settings we may also be concerned with the fairness of our choices.2 But what should fairness mean?",1. Introduction,[0],[0]
"In this paper, we take inspiration from (Dwork et al., 2012) who propose that fairness should mean that “similar individuals are treated similarly”, where “similarity” is measured with respect to some task specific metric.",1. Introduction,[0],[0]
"In our setting, the natural task-specific metric is the true within-group CDF value for each individual.",1. Introduction,[0],[0]
"On its own, this is compatible with the goal of selecting the best candidates, but in our work, the main obstacle is that we do not know the true CDF value of each individual, and can only approximate this from data.",1. Introduction,[0],[0]
We study the degree to which fairness and optimality are compatible with one another in this setting.,1. Introduction,[0],[0]
We study a setting in which we wish to select k individuals out of a pool of n for some task.,1.1. Our Results,[0],[0]
"The individuals are drawn from d populations, each represented by a different
1Letters of recommendation often seek to communicate this information, with statements like “This candidate is among the top 5 students I have seen in my 16 years as a professor.”
2With respect to men’s and women’s sports, equal opportunity is legislated in Title IX.",1.1. Our Results,[0],[0]
"With respect to faculty hiring, fairness concerns can arise because the proportion of women can vary substantially across subfields.",1.1. Our Results,[0],[0]
"For example, as reported in (Cohoon et al., 2011), the percentage of female authors varies from 10% to 44% across ACM conferences, when averaged over the 10 year period from 1998-2008.
distribution over real",1.1. Our Results,[0],[0]
numbers.3 The number of draws from each distribution may differ.,1.1. Our Results,[0],[0]
"The “quality” of an individual is defined to be their (true) CDF value, as evaluated on the distribution from which they were drawn.",1.1. Our Results,[0],[0]
"An algorithm is evaluated based on the (expected) quality of the k individuals it selects.
",1.1. Our Results,[0],[0]
The meritocratic fairness definition we propose informally asks that lower quality individuals are never (probabilistically) favored over higher quality individuals.,1.1. Our Results,[0],[0]
"When formulating this definition, we have a choice as to how to incorporate randomness.",1.1. Our Results,[0],[0]
"The strongest formulation possible (ex-post fairness) does not involve randomness, and simply requires that every individual actually selected has quality at least that of every individual not selected.",1.1. Our Results,[0],[0]
"The weakest formulation (ex-ante fairness) incorporates the randomness of the selection of the population from the underlying distribution, and informally requires that for any pair of individuals, the higher quality individual is selected with weakly higher probability than the lower quality individual, where the randomness is over the realization of the population from the underlying distributions, as well as any internal randomness of the mechanism.",1.1. Our Results,[0],[0]
"An intermediate formulation (ex-interim fairness) requires informally that higher quality individuals be selected with weakly higher probability than lower quality individuals, where the probability is computed over the randomness of the mechanism, but not over the selection of the population.",1.1. Our Results,[0],[0]
"Roughly speaking, these choices correspond to what an individual may know and still be satisfied by a promise of “fairness”.",1.1. Our Results,[0],[0]
"Individuals should be satisfied with ex-post fairness even after the choices of the mechanism are made, with full knowledge of the applicant pool — that is, they should be satisfied with the actual outcome, regardless of the algorithm used to reach it.",1.1. Our Results,[0],[0]
"In contrast, individuals with full knowledge of the applicant pool should still be satisfied with ex-interim fairness before the mechanism makes its decisions — that is, they should feel satisfied that the algorithm used is fair.",1.1. Our Results,[0],[0]
"An individual should only be satisfied by ex-ante fairness if she has no knowledge of the applicant pool (and so can consider it a random variable) before the choices are made.
",1.1. Our Results,[0],[0]
"Given such a spectrum of fairness constraints, we observe that the strongest ex-post fairness is impossible to achieve, whereas the weakest ex-ante fairness is sometimes easy to achieve: when the population sizes are the same, it is satisfied by the mechanism that simply selects the k individuals with highest empirical CDF values.4",1.1. Our Results,[0],[0]
"Our main results
3We study the simple setting in which each individual is represented by a 1-dimensional “score” — e.g. a credit score, a time in the 100m dash, etc. — which itself may encapsulate or summarize many features into a single value.",1.1. Our Results,[0],[0]
"Generalizing this work to richer representations is an interesting direction for future work.
",1.1. Our Results,[0],[0]
"4However, for the cases in which the populations are not the same size, we do not know of better utility guarantees for ex-ante
therefore concern the cost (in terms of the expected quality of the selected applicants) of asking for the stronger notion of ex-interim fairness.",1.1. Our Results,[0],[0]
"We show that satisfying an exact variant of this constraint requires the selection algorithm to select uniformly at random amongst all individuals, and hence obtain only trivial utility guarantees, but that subject to an approximate relaxation of this constraint, it is possible to recover asymptotically optimal utility bounds.",1.1. Our Results,[0],[0]
"We show that when we further relax the problem, to allow the algorithm to select approximately k individuals (rather than exactly k), it is possible to recover asymptotically optimal utility bounds while satisfying ex-post fairness guarantees within each sub-population, and approximate ex-interim fairness guarantees across populations.",1.1. Our Results,[0],[0]
We summarize our results in Table 1.,1.1. Our Results,[0],[0]
"We complement our theoretical results with empirical simulations which emphasize that both the utility and fairness guarantees of our algorithms are better in practice than our theorems promise.
",1.1. Our Results,[0],[0]
"Finally, we remark on an interesting property of our upper bounds: they are oblivious, in the sense that they do not make use of the raw scores associated with each individual — only their empirical CDF ranking.",1.1. Our Results,[0],[0]
"As such, our upper bounds can be viewed as universal distributions over permutations (of empirical CDF rankings) that satisfy a fairness guarantee, rather than algorithms.",1.1. Our Results,[0],[0]
"Our lower bounds apply not just to oblivious algorithms, but to any algorithm, even those that can make use of raw scores (or indeed, even knowledge of the family of distributions from which populations are drawn).",1.1. Our Results,[0],[0]
"This paper fits into a rapidly growing line of work studying “fairness” in learning settings that is now too large to summarize fully, and so we discuss only the most closely related work.",1.2. Related Work,[0],[0]
"Our definition of fairness is in the spirit of (Dwork et al., 2012), who propose that individual fairness should mean that “similar individuals are treated similarly” with respect to some underlying task-specific metric.",1.2. Related Work,[0],[0]
"As with the work of (Joseph et al., 2016; Jabbari et al., 2016), we define the metric to be a measure of quality already present in the model (in our case, the CDF values of individuals) but unknown to the algorithm, except through samples.",1.2. Related Work,[0],[0]
It is this necessity to learn the underlying metric that poses the tension between the fairness constraint and the accuracy goal.,1.2. Related Work,[0],[0]
"Although in this line of work, we adopt a definition that merely requires “better individuals be treated better” according to the true unknown metric, this necessarily requires that “similar individuals be treated similarly” with respect to empirical estimates of the metric.
",1.2. Related Work,[0],[0]
"Technically, our work includes adaptations of techniques in
fairness than those we derive for the stronger notion of ex-interim fairness.
differential privacy (Dwork et al., 2006).",1.2. Related Work,[0],[0]
"Specifically, we adopt variants of the “report noisy max” algorithm (Dwork & Roth, 2014), and Raskhodnikova and Smith’s “exponential mechanism for scores with varying sensitivities” (Raskhodnikova & Smith, 2016), which is itself a variant of the exponential mechanism (McSherry & Talwar, 2007).",1.2. Related Work,[0],[0]
"There are d different populations, indexed by j. For each population j, there is a pool of candidates with their raw scores (and henceforth observations) drawn i.i.d.",2. Model and Preliminaries,[0],[0]
from some unknown continuous distribution Fj over R. Let F = F1 × · · ·,2. Model and Preliminaries,[0],[0]
× Fd denote the product distribution.,2. Model and Preliminaries,[0],[0]
"We will slightly abuse notation and write xij to denote both the individual i in the population j and her associated observation, and write X to denote the set of all candidates.",2. Model and Preliminaries,[0],[0]
"Let mj be the size of the candidate pool from population j, n = ∑ jmj be the size of the total population, and m = minjmj be the smallest population size.",2. Model and Preliminaries,[0],[0]
"Each individual xij is associated with the following values.
",2. Model and Preliminaries,[0],[0]
•,2. Model and Preliminaries,[0],[0]
A cumulative distribution function (CDF) value Fj(xij) =,2. Model and Preliminaries,[0],[0]
PrFj,2. Model and Preliminaries,[0],[0]
"[x < xij ],5 and an empirical CDF value F̂j(xij) = 1mj ∑mj",2. Model and Preliminaries,[0],[0]
i′=1 1[x <,2. Model and Preliminaries,[0],[0]
xi′j,2. Model and Preliminaries,[0],[0]
"].
",2. Model and Preliminaries,[0],[0]
• A complementary cumulative distribution function (CCDF) value: pij = 1 − Fj(xij) and an empirical CCDF value p̂ij = 1mj ∑mj,2. Model and Preliminaries,[0],[0]
i′=1 1[x,2. Model and Preliminaries,[0],[0]
"≥ xi′j ].
",2. Model and Preliminaries,[0],[0]
"A selection algorithm A takes all the n observations X drawn from different distributions as input, and (randomly) selects k individuals as outputs.",2. Model and Preliminaries,[0],[0]
"We will write A(X,xij) (or Aij for simplicity) to denote the selection probability over the individual xij .",2. Model and Preliminaries,[0],[0]
The utility for selecting an individual xij is her true CDF value Fj(xij).,2. Model and Preliminaries,[0],[0]
"Equivalently, the loss for selecting an individual xij is the true CCDF value pij .",2. Model and Preliminaries,[0],[0]
"The loss for an algorithmA on input X is then defined as
L(A, X) = 1 k ∑ xij∈X A(X,xij)(1−Fj(xij))
",2. Model and Preliminaries,[0],[0]
5We adopt a slightly different definition from the standard one: Fj(xij) =,2. Model and Preliminaries,[0],[0]
"PrFj [x ≤ xij ].
and the expected loss of the algorithm is EX∼F",2. Model and Preliminaries,[0],[0]
"[L(A, X)].",2. Model and Preliminaries,[0],[0]
Our goal is design selection algorithms subject to a meritocratic fairness notion that requires that less qualified candidates (in terms of CDF values) are never preferred over more qualified ones.,2.1. Fairness Formulation,[0],[0]
"We will present three different formulations of such notion based on the different forms of randomness we are considering.
",2.1. Fairness Formulation,[0],[0]
"First, the weakest formulation is the following ex-ante fairness, which guarantees fairness over the randomness of both the random draws of the candidates and the coin flips of the algorithm.
",2.1. Fairness Formulation,[0],[0]
Definition 2.1 (Ex-Ante Fairness).,2.1. Fairness Formulation,[0],[0]
"An algorithm A satisfies ex-ante fairness if for any pair of candidates xij , xi′j′ with CDF values Fj(xij) > Fj′(xi′j′), their selection probabilities (when they are in the pool) satisfy
E [A(X,xij)]",2.1. Fairness Formulation,[0],[0]
"≥ E [A(X,xi′j′)]
where the expectations are taken over the (n − 2) random draws of all the other candidates.
",2.1. Fairness Formulation,[0],[0]
"An intermediate formulation of fairness is the following exinterim fairness, which guarantees fairness over the randomness of the algorithms (but not the realizations of X) on almost all of inputs drawn from the distribution.
",2.1. Fairness Formulation,[0],[0]
Definition 2.2 (Exact Ex-Interim Fairness).,2.1. Fairness Formulation,[0],[0]
"Let δ ∈ (0, 1).",2.1. Fairness Formulation,[0],[0]
"An algorithm A satisfies δ-exact ex-interim fairness if with probability at least 1− δ over the realized observations X , for any pair of individuals xij , xi′j′ ∈ X ,
A(X,xij) > A(X,xi′j′)",2.1. Fairness Formulation,[0],[0]
"only if Fj(xij) > Fj′(xi′j′)
We also consider the following relaxation:
Definition 2.3 (Approximate Ex-Interim Fairness).",2.1. Fairness Formulation,[0],[0]
"An algorithm A satisfies (ε, δ)-approximate ex-interim fairness if with probability at least 1− δ over the realized observations X , for any pair of individuals xij , xi′j′ ∈ X ,
A(X,xij) >",2.1. Fairness Formulation,[0],[0]
"eεA(X,xi′j′)",2.1. Fairness Formulation,[0],[0]
"only if Fj(xij) > Fj′(xi′j′)
Remark 2.4.",2.1. Fairness Formulation,[0],[0]
"We note that this relaxation of ex-interim fairness bears a similarity to the definition of differential
privacy (Dwork et al., 2006), and indeed, techniques from the differential privacy literature will prove useful in designing algorithms to satisfy it.
",2.1. Fairness Formulation,[0],[0]
"Perhaps the strongest formulation is the following ex-post fairness condition, which requires that an individual is selected only if a more qualified individual is also selected.",2.1. Fairness Formulation,[0],[0]
Definition 2.5 (Ex-post Fairness).,2.1. Fairness Formulation,[0],[0]
"An algorithmA satisfies ex-post fairness if any pair of individuals xij and xi′j′ such that Fj(xij) > Fj′(xi′j′), the individual xi′j′ is admitted only if xij is also selected.
",2.1. Fairness Formulation,[0],[0]
"Note that any algorithm that satisfies ex-post fairness must admit a prefix of individuals from each population, which is also sufficient to guarantee within population ex-post fairness, but that this is not sufficient to satisfy the constraint between populations.
",2.1. Fairness Formulation,[0],[0]
"It is not hard to see that satisfying ex-post fairness in the generality that we have defined it is impossible, since it requires perfectly selecting the k true best CDF values from only sample data.",2.1. Fairness Formulation,[0],[0]
"Thus, the primary focus of our paper is on ex-interim fairness.",2.1. Fairness Formulation,[0],[0]
"Unless we specify differently, the term “fair” and “fairness”refer to ex-interim fairness.",2.1. Fairness Formulation,[0],[0]
"A special class of selection algorithms is the class of oblivious algorithms, which select candidates with probabilities that only depend on their empirical CDF values, not on their observations.",2.2. Oblivious Algorithms,[0],[0]
Definition 2.6 (Oblivious Algorithms).,2.2. Oblivious Algorithms,[0],[0]
An algorithm A is oblivious if for any pair of input observations X and X ′,2.2. Oblivious Algorithms,[0],[0]
"that induce the same empirical CDF values over the candidates, A(X) = A(X ′).
",2.2. Oblivious Algorithms,[0],[0]
All of our algorithms presented in this paper are oblivious.,2.2. Oblivious Algorithms,[0],[0]
"As a result, we need to make no assumption on the underlying distributions to achieve both fairness and utility guarantees.",2.2. Oblivious Algorithms,[0],[0]
"Moreover, the utility guarantee of an oblivious algorithm can be characterized as follows.",2.2. Oblivious Algorithms,[0],[0]
Lemma 2.7.,2.2. Oblivious Algorithms,[0],[0]
"The expected loss achieved by any oblivious algorithm A is the expected average empirical CCDF values among the selected candidates.
",2.2. Oblivious Algorithms,[0],[0]
A very simple example of an oblivious algorithm is GREEDY which selects the k individuals with the highest empirical CDF values (breaking ties uniformly at random).,2.2. Oblivious Algorithms,[0],[0]
Lemma 2.8.,2.2. Oblivious Algorithms,[0],[0]
"Suppose that the populations sizes are the same, that is, mj = m for each j. The algorithm GREEDY satisfies ex-ante fairness and has an expected loss at most k2n + 1 m .
To simplify our bounds on the expected loss, we will use k/2n as our benchmark and define the regret of an algorithm A to beR(A) = EX∼F",2.2. Oblivious Algorithms,[0],[0]
"[L(A, X)]− k2n .",2.2. Oblivious Algorithms,[0],[0]
"In this section, we provide an algorithm that satisfies approximate fairness in the sense of Definition 2.3.",3. An Approximately Fair Algorithm,[0],[0]
"We will present our solution in three steps.
1.",3. An Approximately Fair Algorithm,[0],[0]
"First, we provide confidence intervals for the candidates’ CCDF values pij based on their empirical CCDF values p̂ij .",3. An Approximately Fair Algorithm,[0],[0]
"As we show, our bound has a tighter dependence on pij , which gives better utility guarantee than using the standard DKW inequality of Dvoretzky et al. (1956).
2.",3. An Approximately Fair Algorithm,[0],[0]
"Next, we give a simple subroutine NOISYTOP that randomly selects k individuals out of n based on their “scores”.",3. An Approximately Fair Algorithm,[0],[0]
We show that individuals with similar scores will have close selection probabilities under this subroutine.,3. An Approximately Fair Algorithm,[0],[0]
"This subroutine is similar to the “Report Noisy Max” algorithm (Dwork & Roth, 2014).
",3. An Approximately Fair Algorithm,[0],[0]
3.,3. An Approximately Fair Algorithm,[0],[0]
"Then, we will use the deviation bound in the first step to assign scores to the candidates.",3. An Approximately Fair Algorithm,[0],[0]
We show that running NOISYTOP based on these scores give approximate fairness and low regret guarantees.,3. An Approximately Fair Algorithm,[0],[0]
These scores are computed in a way similar to the generalized exponential mechanism of Raskhodnikova & Smith (2016).,3. An Approximately Fair Algorithm,[0],[0]
"We will first give the following concentration inequality specialized for the uniform distribution over (0, 1).
",3.1. Confidence Intervals for CCDF Values,[0],[0]
Lemma 3.1.,3.1. Confidence Intervals for CCDF Values,[0],[0]
Fix any n ∈,3.1. Confidence Intervals for CCDF Values,[0],[0]
N.,3.1. Confidence Intervals for CCDF Values,[0],[0]
"Let x1, x2, . . .",3.1. Confidence Intervals for CCDF Values,[0],[0]
", xn be i.i.d.",3.1. Confidence Intervals for CCDF Values,[0],[0]
"draws from the uniform distribution over (0, 1).",3.1. Confidence Intervals for CCDF Values,[0],[0]
"Then with probability at least 1− δ, for any p ∈ (0, 1),
|p− p̂| ≤",3.1. Confidence Intervals for CCDF Values,[0],[0]
"√ ln(2n/δ)
(√ 3p
n",3.1. Confidence Intervals for CCDF Values,[0],[0]
"+
2
n
)
",3.1. Confidence Intervals for CCDF Values,[0],[0]
where p̂ = 1n,3.1. Confidence Intervals for CCDF Values,[0],[0]
∑n i=1,3.1. Confidence Intervals for CCDF Values,[0],[0]
"1[xi < p].
",3.1. Confidence Intervals for CCDF Values,[0],[0]
"To translate this result into a deviation bound on the CCDF values, first note that CCDF values for any distribution Fj are drawn from the uniform distribution over (0, 1), so the bound applies immediately to the CCDF values.",3.1. Confidence Intervals for CCDF Values,[0],[0]
"By a standard calculation, we can also get a bound in terms of the empirical CCDF value p̂ij as shown below.
",3.1. Confidence Intervals for CCDF Values,[0],[0]
Lemma 3.2.,3.1. Confidence Intervals for CCDF Values,[0],[0]
For each j ∈,3.1. Confidence Intervals for CCDF Values,[0],[0]
"[d], draw mj points",3.1. Confidence Intervals for CCDF Values,[0],[0]
Xj = {xij} mj i=1,3.1. Confidence Intervals for CCDF Values,[0],[0]
i.i.d.,3.1. Confidence Intervals for CCDF Values,[0],[0]
from Fj .,3.1. Confidence Intervals for CCDF Values,[0],[0]
"For each point xij , let pij be its true CCDF value and p̂ij be its empirical CCDF value in Fj .",3.1. Confidence Intervals for CCDF Values,[0],[0]
"Then with probability at least 1− δ over the n random draws,
|pij",3.1. Confidence Intervals for CCDF Values,[0],[0]
"− p̂ij | ≤ 9 √ p̂ij m ln(2n/δ)
where m = minjmj and n = ∑d j=1mj .
",3.1. Confidence Intervals for CCDF Values,[0],[0]
Remark 3.3.,3.1. Confidence Intervals for CCDF Values,[0],[0]
The standard DKW inequality gives a bound of Õ( √ 1/m).,3.1. Confidence Intervals for CCDF Values,[0],[0]
Our bound gives a tighter dependence for small empirical CCDF value p̂ij .,3.1. Confidence Intervals for CCDF Values,[0],[0]
"For example, when p̂ij = 1/m, we obtain a bound of Õ(1/m).",3.1. Confidence Intervals for CCDF Values,[0],[0]
6,3.1. Confidence Intervals for CCDF Values,[0],[0]
"Given a set of individuals with scores Y = {y1, . . .",3.2. The NoisyTop Subroutine,[0],[0]
", yn}, the subroutine NOISYTOP will first perturb each score by adding independent noise drawn from the Laplace distribution,7 and output the k individuals with the minimum noisy scores (ties broken arbitrarily).",3.2. The NoisyTop Subroutine,[0],[0]
We will now show that NOISYTOP has the following desirable “Lipschitz” property—individuals with similar scores are chosen with similar probabilities.,3.2. The NoisyTop Subroutine,[0],[0]
"This is crucial for obtaining approximate fairness.
",3.2. The NoisyTop Subroutine,[0],[0]
"Algorithm 1 NOISYTOP({y1, y2, . . .",3.2. The NoisyTop Subroutine,[0],[0]
", yn}, α, k) Input: n numbers {y1, y2, . . .",3.2. The NoisyTop Subroutine,[0],[0]
", yn} and parameter α
For each i ∈",3.2. The NoisyTop Subroutine,[0],[0]
[n]: let ỹi = yi + Lap(α),3.2. The NoisyTop Subroutine,[0],[0]
"Output: the k indices with the smallest ỹi
Lemma 3.4.",3.2. The NoisyTop Subroutine,[0],[0]
"Let i, j ∈",3.2. The NoisyTop Subroutine,[0],[0]
[n] be such that ∆ = yi,3.2. The NoisyTop Subroutine,[0],[0]
− yj ≥ 0.,3.2. The NoisyTop Subroutine,[0],[0]
"Let Pi and Pj denote the probabilities that the two indices i and j are output by NOISYTOP({y1, y2, . . .",3.2. The NoisyTop Subroutine,[0],[0]
", yn}, α) respectively.",3.2. The NoisyTop Subroutine,[0],[0]
"Then Pi ≤ Pj ≤ Pi exp(2∆/α).
",3.2. The NoisyTop Subroutine,[0],[0]
Proof.,3.2. The NoisyTop Subroutine,[0],[0]
"Let ỹi and ỹj be the noisy scores for i or j. We will introduce a new random variable Q to denote the value of the (k − 1)-st lowest noisy value, not counting ỹi and ỹj .",3.2. The NoisyTop Subroutine,[0],[0]
We will slightly abuse notation and write Pr[R = r],3.2. The NoisyTop Subroutine,[0],[0]
as a shorthand for the pdf of any random variable R evaluated at r.,3.2. The NoisyTop Subroutine,[0],[0]
"The ratio PiPj can then be written as∫ q∈R Pr[Q = q] (∫ t∈R Pr[ỹj = t] Pr[ỹi < min{t, q}]dt )",3.2. The NoisyTop Subroutine,[0],[0]
"dq∫
q∈R Pr[Q = q] (∫ t∈R Pr[ỹi = t] Pr[ỹj",3.2. The NoisyTop Subroutine,[0],[0]
"< min{t, q}]dt )",3.2. The NoisyTop Subroutine,[0],[0]
"dq
(1)
For any fixed value r ∈ R, we also have the following based on the Laplace distribution,
Pr[ỹi = r] Pr[ỹj",3.2. The NoisyTop Subroutine,[0],[0]
"= r] =
1 2α exp ( − |r−yi|α ) 1 2α exp ( − |r−yj |α
) = exp ( |r",3.2. The NoisyTop Subroutine,[0],[0]
− yj | α,3.2. The NoisyTop Subroutine,[0],[0]
− |r,3.2. The NoisyTop Subroutine,[0],[0]
"− yi| α
)",3.2. The NoisyTop Subroutine,[0],[0]
By the triangle inequality we know that |r−yj |−|r−yi| ≤,3.2. The NoisyTop Subroutine,[0],[0]
"∆. It follows that for any t and q,
exp(−∆/α) ≤ Pr[ỹi = t] Pr[ỹj",3.2. The NoisyTop Subroutine,[0],[0]
"= t] ≤ exp(∆/α) and,
6As shown in Corollary 3.8, this also gives an improvement over regret when k is small (Õ( √ k/n) versus Õ( √ 1/n)).
",3.2. The NoisyTop Subroutine,[0],[0]
"7The Laplace distribution Lap(b) has density function f(x) = exp(−|x|/b).
Pr[ỹi",3.2. The NoisyTop Subroutine,[0],[0]
"< min{q, t}] Pr[ỹj",3.2. The NoisyTop Subroutine,[0],[0]
"< min{q, t}] = ∫ r<min{q,t} Pr[ỹi",3.2. The NoisyTop Subroutine,[0],[0]
"= r] dr∫ r<min{q,t} Pr[ỹj",3.2. The NoisyTop Subroutine,[0],[0]
"= r] dr
≤ exp(∆/α)
",3.2. The NoisyTop Subroutine,[0],[0]
"Plugging these bounds into Equation (1), we get PiPj ≤ exp(2∆/α).",3.2. The NoisyTop Subroutine,[0],[0]
The inequality that Pi/Pj ≤ 1 follows directly from yi ≥ yj .,3.2. The NoisyTop Subroutine,[0],[0]
We will present our algorithm FAIRTOP by combining the methods in the previous two sections.,3.3. Wrapping Up,[0],[0]
"In the light of Lemma 3.2, we will define the following confidence interval width function on the empirical CCDF values
c(p̂) = 9 ln(2n/δ) √ p̂/m
and a normalized score function s(p̂) = p̂/c(p̂).",3.3. Wrapping Up,[0],[0]
"We have that any candidate is guaranteed a score not much lower than a less qualified one.
",3.3. Wrapping Up,[0],[0]
Lemma 3.5.,3.3. Wrapping Up,[0],[0]
"Let x, y ∈",3.3. Wrapping Up,[0],[0]
"[0, 1] be the (true) CCDF values for two individuals",3.3. Wrapping Up,[0],[0]
"such that x ≤ y. Let x̂, ŷ be the empirical CCDF values respectively.",3.3. Wrapping Up,[0],[0]
Suppose that |x−,3.3. Wrapping Up,[0],[0]
x̂| ≤,3.3. Wrapping Up,[0],[0]
c(x̂) and |y,3.3. Wrapping Up,[0],[0]
"− ŷ| ≤ c(ŷ), then s(x̂)− s(ŷ) ≤ 1.
Algorithm 2 FAIRTOP(X = {xij}, ε, δ, k,m) Input: candidates’ observations X , fairness parameters ε, δ, number of selected individuals k, and smallest population size m
For each individual xij ∈ X Compute the empirical CCDF value p̂ij and the associated score s(p̂ij)
",3.3. Wrapping Up,[0],[0]
"Run NOISYTOP({s(p̂ij}, 2/ε, k)
",3.3. Wrapping Up,[0],[0]
"Our algorithm FAIRTOP (presented in Algorithm 2) proceeds by first computing the normalized score of every candidates based on their empirical CCDF values, and then calling NOISYTOP to output k individuals.",3.3. Wrapping Up,[0],[0]
"We will first establish the approximate fairness guarantee.
",3.3. Wrapping Up,[0],[0]
Theorem 3.6.,3.3. Wrapping Up,[0],[0]
"The algorithm FAIRTOP instantiated with parameters ε and δ satisfies (ε, δ)-approximate fairness.
",3.3. Wrapping Up,[0],[0]
Proof sketch.,3.3. Wrapping Up,[0],[0]
"By Lemma 3.2, we know that with probability 1 − δ, for every candidate xij , the true and empirical CCDF values satisfy |pij",3.3. Wrapping Up,[0],[0]
− p̂ij | ≤ c(p̂ij).,3.3. Wrapping Up,[0],[0]
"This means that for any pair of individuals a and a′ with CCDF values pa < pa′ (that is, a is more qualified than a′), we also have s(p̂a) ≤ s(p̂a′) + 1 by Lemma 3.5.",3.3. Wrapping Up,[0],[0]
"Finally, by the result of Lemma 3.4 and the instantiation of NOISYTOP, we guarantee that a′ will not be selected with substantially higher probability: Aa exp(ε) ≥ Aa′ , which recovers the approximate fairness guarantee.
",3.3. Wrapping Up,[0],[0]
"Our algorithm also has a diminishing regret guarantee:
Theorem 3.7.",3.3. Wrapping Up,[0],[0]
"Fix any β ∈ (0, 1).",3.3. Wrapping Up,[0],[0]
"Then with probability at least 1 − β, the algorithm FAIRTOP instantiated with fairness parameters ε and δ has regret bounded by(
1
ε
√( k
n +
1
m
) 1
m +
1
mε2
) ·",3.3. Wrapping Up,[0],[0]
"polylog(n, 1/β, 1/δ)
",3.3. Wrapping Up,[0],[0]
"Thus for example, as the smallest sampled population size m grows (fixing k and ε), our regret rapidly approaches 0.",3.3. Wrapping Up,[0],[0]
"To understand the utility guarantee better, we will state the regret bound for the following natural scaling, which is also examined in the simulations of Section 7:
Corollary 3.8.",3.3. Wrapping Up,[0],[0]
Consider an instance with two population of sizesm1 andm2 such thatm1 = αm2 for some constant α ≥ 1.,3.3. Wrapping Up,[0],[0]
"Suppose we instantiate FAIRTOP with parameter ε = Θ(1), then the regret is at most Õ (√ k m ) .",3.3. Wrapping Up,[0],[0]
"In this section, we provide a variant of the FAIRTOP algorithm that satisfies approximate ex-interim fairness across different populations, but also ex-post fairness within each population.",4. Within Population Ex-Post Fairness,[0],[0]
"The key idea here is that since we know the ranking of the candidates true qualities within each population, we can guarantee ex-post fairness within populations as long as we select a prefix of candidates in each population.",4. Within Population Ex-Post Fairness,[0],[0]
"This will however come at a cost — our algorithm will no longer select exactly k individuals, but only approximately k individuals.
",4. Within Population Ex-Post Fairness,[0],[0]
"Similar to FAIRTOP, the algorithm ABOVETHRE (presented in Algorithm 3) also computes the normalized scores for each candidate.",4. Within Population Ex-Post Fairness,[0],[0]
"Instead of perturbing the scores, ABOVETHRE computes a noisy threshold",4. Within Population Ex-Post Fairness,[0],[0]
Tj for each population by adding Laplace noise to s(k/n).,4. Within Population Ex-Post Fairness,[0],[0]
The algorithm then selects all candidates with scores above the noisy threshold.,4. Within Population Ex-Post Fairness,[0],[0]
"Because the algorithm selects a prefix of the raw scores within each population, within population ex-post fairness is immediate.",4. Within Population Ex-Post Fairness,[0],[0]
"We also show that ABOVETHRE also achieves approximate ex-interim fairness.
",4. Within Population Ex-Post Fairness,[0],[0]
Theorem 4.1.,4. Within Population Ex-Post Fairness,[0],[0]
"The algorithm ABOVETHRE instantiated with fairness parameters ε and δ satisfies both (ε, δ)approximate ex-interim fairness and ex-post fairness within each population.
",4. Within Population Ex-Post Fairness,[0],[0]
"Note that were the algorithm to take all the individuals with scores above s(k/n), it would select a (k/n) fraction from each population and therefore select k people in total.",4. Within Population Ex-Post Fairness,[0],[0]
"Due to the noisy thresholds, the algorithm will only select approximately k individuals.",4. Within Population Ex-Post Fairness,[0],[0]
"We will now establish the utility guarantee of ABOVETHRE and show that the number of selected individuals is roughly k± Õ( √ k) when m = Θ(n).
",4. Within Population Ex-Post Fairness,[0],[0]
"Algorithm 3 ABOVETHRE(X = {xij}, ε, δ, k,m)",4. Within Population Ex-Post Fairness,[0],[0]
"Input: observations X , fairness parameters ε, δ, target number of selected individuals k, smallest population size m
",4. Within Population Ex-Post Fairness,[0],[0]
For each individual xij Compute her empirical CCDF value p̂ij and the associated score s(p̂ij),4. Within Population Ex-Post Fairness,[0],[0]
For each population j Compute a noisy threshold,4. Within Population Ex-Post Fairness,[0],[0]
"Tj = s(k/n)+νj where νj is drawn from Lap(1/ε) Select candidates xij with scores s(p̂ij) above Tj
Theorem 4.2.",4. Within Population Ex-Post Fairness,[0],[0]
"Fix any β ∈ (0, 1).",4. Within Population Ex-Post Fairness,[0],[0]
"With probability at least 1 − β, the algorithm ABOVETHRE instantiated with fairness parameters ε and δ has regret bounded by(
1
mε2",4. Within Population Ex-Post Fairness,[0],[0]
"+
",4. Within Population Ex-Post Fairness,[0],[0]
"√ k
ε √ mn
) ·",4. Within Population Ex-Post Fairness,[0],[0]
"polylog(n, d, 1/δ, 1/β),
and selects a total number of k̂ individuals with
|k",4. Within Population Ex-Post Fairness,[0],[0]
"− k̂| ≤ d+
( n
mε2",4. Within Population Ex-Post Fairness,[0],[0]
"+
√ nk ε √ m
) ·",4. Within Population Ex-Post Fairness,[0],[0]
"polylog(n, d, 1/δ, 1/β)",4. Within Population Ex-Post Fairness,[0],[0]
"We will show that it is impossible to achieve exact exinterim fairness with non-trivial regret guarantees.
",5. Lower Bound for Exact Fairness,[0],[0]
Theorem 5.1.,5. Lower Bound for Exact Fairness,[0],[0]
Fix any δ < 0.0002 and any δ-fair algorithm A.,5. Lower Bound for Exact Fairness,[0],[0]
"There exist two distributions F1 and F2 over the two populations such that if algorithm A takes m observations drawn from each distribution as input, and must select at least k = Ω(m1/2+α) individuals for any α > 0, A incurs a regret of Ω(1).
",5. Lower Bound for Exact Fairness,[0],[0]
"The main idea is to show that there exist distributions F1 and F2 such that any fair algorithm will essentially have to select uniformly at random across Ω(m) individuals, which incurs regret Ω(1).",5. Lower Bound for Exact Fairness,[0],[0]
We will proceed via Bayesian reasoning.,5. Lower Bound for Exact Fairness,[0],[0]
"Suppose that the observations from the two populations are drawn from two different unit-variance Gaussian distributions N (µ1, 1) and N (µ2, 1), and both means µ1 and µ2 are themselves drawn from the prior N (0, 1).",5. Lower Bound for Exact Fairness,[0],[0]
"The following lemma characterizes the posterior distribution on the mean given a collection of observations.
",5. Lower Bound for Exact Fairness,[0],[0]
Lemma 5.2.,5. Lower Bound for Exact Fairness,[0],[0]
"(Murphy, 2007) Suppose that a mean parameter µ is drawn from a prior distribution N (0, 1).",5. Lower Bound for Exact Fairness,[0],[0]
Let D =,5. Lower Bound for Exact Fairness,[0],[0]
"(x1, x2, . . .",5. Lower Bound for Exact Fairness,[0],[0]
", xm) be m i.i.d.",5. Lower Bound for Exact Fairness,[0],[0]
"draws from the distributionN (µ, 1).",5. Lower Bound for Exact Fairness,[0],[0]
"Then the posterior distribution of µ conditioned on D is the Gaussian distribution N (µ̂, σ2), where µ̂ = ∑ i xi
m+1 and σ 2 = 1m+1 .
",5. Lower Bound for Exact Fairness,[0],[0]
"The result above shows that conditioned on any m draws from the Gaussian distribution, there is constant probability that the true mean will be bounded away from the posterior maximum likelihood estimate by at least Ω(1/ √ m).",5. Lower Bound for Exact Fairness,[0],[0]
"With this observation, we will partition the real line into the following intervals:",5. Lower Bound for Exact Fairness,[0],[0]
"Given any posterior mean µ̂ any integer r ≥ 1, let the two intervals I+r (µ̂) and I−r (µ̂) be
I+r (µ̂) =",5. Lower Bound for Exact Fairness,[0],[0]
"[µ̂+ (r − 1)/ √ m, µ̂+ r/ √ m] and I−r (µ̂) =",5. Lower Bound for Exact Fairness,[0],[0]
"[µ̂− r/ √ m, µ̂− (r − 1)/ √ m]
The intervals capture the uncertainty we have regarding the CDF values of the observations xij .",5. Lower Bound for Exact Fairness,[0],[0]
Let,5. Lower Bound for Exact Fairness,[0],[0]
"Xj = (x1j , x2j , . . .",5. Lower Bound for Exact Fairness,[0],[0]
", xmj} denote the m draws from each distributionFj , µ̂j = ∑ i xij m+1 be the posterior mean for µj conditioned on the draws.",5. Lower Bound for Exact Fairness,[0],[0]
Consider two individuals xi1 and xi′2,5. Lower Bound for Exact Fairness,[0],[0]
such that xi1 ∈ I+r (µ̂1) and xi′2 ∈ I+r+1(µ̂2).,5. Lower Bound for Exact Fairness,[0],[0]
"Even though (xi′2 − µ̂2) > (xi1 − µ̂1), there is a constant probability that their CDF values satisfy F1(xi1) > F2(xi′2).",5. Lower Bound for Exact Fairness,[0],[0]
"Any fair algorithm therefore must play these two individuals in these “neighboring” intervals with equal probabilities.
",5. Lower Bound for Exact Fairness,[0],[0]
"Next, we show that with high probability over the realizations of the true mean µ and the m draws X , all of the O(m logm) intervals around the posterior mean will be “hit” by points in X .",5. Lower Bound for Exact Fairness,[0],[0]
Lemma 5.3.,5. Lower Bound for Exact Fairness,[0],[0]
"Fix any c < 1 and β ∈ (0, 1).",5. Lower Bound for Exact Fairness,[0],[0]
"Let mean µ be drawn from N (0, 1), r̂ = √ cm logm + 1 and X = (x1, x2, . . .",5. Lower Bound for Exact Fairness,[0],[0]
", xm) be m i.i.d.",5. Lower Bound for Exact Fairness,[0],[0]
"draws from N (µ, 1).",5. Lower Bound for Exact Fairness,[0],[0]
"Let µ̂ = ∑ i xi
m+1 .",5. Lower Bound for Exact Fairness,[0],[0]
"Then except with probability 2r̂ exp ( −m
(1/2−c/2)",5. Lower Bound for Exact Fairness,[0],[0]
"√ 2A
)",5. Lower Bound for Exact Fairness,[0],[0]
"+ 3β over the joint realizations of µ
and X , the following holds
• for all r ≤ r̂",5. Lower Bound for Exact Fairness,[0],[0]
"− 2 √
2 ln(2/β), there exist two draws x+r , x − r ∈",5. Lower Bound for Exact Fairness,[0],[0]
"X such that x+r ∈ I+r (µ̂) and x−r ∈ I−r (µ̂); • the number of points that are bigger than µ̂ + (r̂ − 2 √ 2 ln(2/β))/ √ m is no more than
m1−c/2 +m1/2−c/4 √ 3 ln(1/β)
",5. Lower Bound for Exact Fairness,[0],[0]
We show that the event that all of the consecutive intervals are occupied for both populations will force a fair algorithm to play all the individuals in these intervals with equal probability.,5. Lower Bound for Exact Fairness,[0],[0]
"More formally, fix any c and sufficiently small constant β, let r̂ = √ cn log n+ 1 and let Y = {xij | xij ∈
I+r (µ̂j) ∨ xij ∈ I−r (µ̂j) for some r ≤ r̂ − 2 √
2 ln(2/β)}.",5. Lower Bound for Exact Fairness,[0],[0]
"Consider the following events:
• FULLCHAIN(X1, X2): for all r ≤ r̂ − 2 √
2 ln(2/β) and j ∈ {1, 2}, both the intervals I+r (µ̂j), I−r (µ̂j) contain at least one point in Xj ,
• UARCHAIN(A, X1, X2): the points in Y are selected by the algorithm",5. Lower Bound for Exact Fairness,[0],[0]
"A with equal probabilities.
",5. Lower Bound for Exact Fairness,[0],[0]
Lemma 5.4.,5. Lower Bound for Exact Fairness,[0],[0]
Fix any δ-fair algorithm A for some δ < 0.0002.,5. Lower Bound for Exact Fairness,[0],[0]
"With probability at least 1/2 over the realizations of µ1, µ2, X1 and X2, the event FULLCHAIN(X1, X2) implies UARCHAIN(A, X1, X2).
",5. Lower Bound for Exact Fairness,[0],[0]
Proof sketch for Theorem 5.1.,5. Lower Bound for Exact Fairness,[0],[0]
"The combination of Lemmas 5.3 and 5.4 shows that with constant probability over µ1, µ2 and X , A will need to select Ω(m) individuals with equal probabilities, which leads to an expected regret of Ω(1) over the draws of µ1, µ2 This means there exist distributionsF1 = N (µ∗1, 1) andF2",5. Lower Bound for Exact Fairness,[0],[0]
"= N (µ∗2, 1) under which A incurs Ω(1) regret.",5. Lower Bound for Exact Fairness,[0],[0]
"We briefly mention an extension to the sequential batch setting, in which the algorithm selects individuals in T rounds.",6. Sequential Batch Setting,[0],[0]
"In each round t, for each population j, there aremj new candidates with their observations drawn i.i.d.",6. Sequential Batch Setting,[0],[0]
"from the distribution Fj , At each round t, the algorithm needs to select k individuals from this pool.",6. Sequential Batch Setting,[0],[0]
Let Stj be the set of observations from population j accumulated after the first t rounds.,6. Sequential Batch Setting,[0],[0]
"In particular, for any observation x and population j, let the historical CCDF value be q̂tj(x) =
1 |mjt| ∑ x′∈Stj
1[x′ < x].",6. Sequential Batch Setting,[0],[0]
"As t grows large, the empirical CCDF values become better estimates for the true CCDF values.",6. Sequential Batch Setting,[0],[0]
"We give a variant of the FAIRTOP algorithm that achieves (ε, δ)-approximate fairness in every round, and incurs average regret over time diminishing as Õ ( 1
ε √ mT
) .",6. Sequential Batch Setting,[0],[0]
"We conclude by discussing some illustrative simulation results for FAIRTOP, along with comparisons to simpler algorithms without fairness guarantees.",7. Simulations,[0],[0]
"The simulations were conducted on data in which the raw scores for each population i = 1, 2 were drawn from N (µi, 1) respectively, and the µi themselves were chosen randomly fromN (0, 1).",7. Simulations,[0],[0]
"Thus befitting the motivation for our model, the raw scores are not directly comparable between populations.",7. Simulations,[0],[0]
"While we varied the population sizes, they were held in the fixed ratio m1/m2 = 2 and k = d0.1(m1 +m2)e.
",7. Simulations,[0],[0]
"For such a simulation with population sizes m1 = 100 and m2 = 50, Figure 1(a) shows the underlying scores computed by FAIRTOP (which depend only on the empirical CDF values) for each member of both populations, but sorted according to their true CDF values so that the transpositions that occur between emprical and true CDFs are apparent; the red points are for the larger population and green for the smaller.",7. Simulations,[0],[0]
"Overlaid on this arc of underlying scores is a black plot illustrating sample post-noise scores
when ε = 10.",7. Simulations,[0],[0]
"As we can see, re-sorting the points by their noisy scores will result in a significant amount of additional reshuffling.
",7. Simulations,[0],[0]
"Figure 1(b) illustrates the induced distribution over chosen individuals; here we show the results of resampling the Laplace noise (again at ε = 10) for 100,000 trials, and choosing the top k post-noise scores across populations.",7. Simulations,[0],[0]
The ordering is again by true CDF values and the same color coding is used.,7. Simulations,[0],[0]
At this value of ε the distribution is biased towards better true CDF values but still enjoys strong fairness properties.,7. Simulations,[0],[0]
"For example, the “unfairness ratio” (maximum ratio of the number of times a worse CDF value is chosen to a better CDF value is chosen) is only 1.56 (note that this is substantially stronger than the bound of e10 guaranteed by our theorem).",7. Simulations,[0],[0]
"It is also visually clear that FAIRTOP is treating similar CDF values similarly, both within and between populations.
",7. Simulations,[0],[0]
"Nevertheless, the regret of FAIRTOP for these population sizes and ε is nontrivial (roughly 0.20 regret compared to the best k true CDF values).",7. Simulations,[0],[0]
"Of course, as per Theorem 3.7 by increasing ε we can reduce regret to any desired level at the expense of weakened fairness guarantees.",7. Simulations,[0],[0]
"However, as per Corollary 3.8 even for fixed ε (and therefore fixed fairness properties), regret diminishes rapidly in the natural scaling where the population sizes grow, but in a fixed ratio.",7. Simulations,[0],[0]
"This is illustrated empirically in Figure 1(c), where for varying choices of ε we plot regret as m1,m2 → ∞ with m1/m2 = 2.
",7. Simulations,[0],[0]
We now briefly compare the properties of FAIRTOP to simpler approaches that generally enjoy lower regret but have no fairness properties.,7. Simulations,[0],[0]
Perhaps the simplest is to pick the k highest ranked individuals by empirical CDF rank.,7. Simulations,[0],[0]
"This method will in general have very low regret, but since it is deterministic, any trial in which it doesn’t select the top k true CDF values has no fairness guarantee (i.e. the unfairness ratio will be infinite), and this happens in approx-
imately approximately 87% of trials under the simulation parameters above (and approaches 100% as populations grow in fixed ratio).
",7. Simulations,[0],[0]
"Perhaps the most natural “learning” approach is to use the raw scores to obtain estimated population means µ̂i (or more generally to estimate the unknown parameters of some known or assumed parametric form) and then use the CDFs of N (µ̂1, 1) and N (µ̂2, 1) to select the k best individuals across the two populations.",7. Simulations,[0],[0]
"This again has generally lower regret than FAIRTOP, but is deterministic and without fairness guarantees, with approximately 53% of trials resulting in unbounded unfairness ratio (approaching 100% as populations grow in fixed ratio).
",7. Simulations,[0],[0]
But the main drawback of such a learning approach in comparison to the data-oblivious FAIRTOP is its need for realizability.,7. Simulations,[0],[0]
"For instance, if we change the population 2 scores to be drawn from the uniform distribution over a wide range, but the learning approach continues to assume normality in each population, it will virtually always choose only members of population 2, a clear and dramatic violation of any intuitive notion of fairness.",7. Simulations,[0],[0]
This is of course due the fact that the highest scores in population 2 appear to have extraordinarily high CDF values when (incorrectly) assumed to have been drawn from a normal distribution.,7. Simulations,[0],[0]
"In contrast FAIRTOP, since it doesn’t even consider the actual scores but only generic properties of the relationship between empirical and true CDF values, will behave exactly the same, in both fairness and regret, regardless of how the underlying scores are generated.",7. Simulations,[0],[0]
"We consider the problem of selecting a pool of individuals from several populations with incomparable skills (e.g. soccer players, mathematicians, and singers) in a fair manner.",abstractText,[0],[0]
"The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own population, which permits cross-population comparisons.",abstractText,[0],[0]
"We study algorithms which attempt to select the highest quality subset despite the fact that true CDF values are not known, and can only be estimated from the finite pool of candidates.",abstractText,[0],[0]
"Specifically, we quantify the regret in quality imposed by “meritocratic” notions of fairness, which require that individuals are selected with probability that is monotonically increasing in their true quality.",abstractText,[0],[0]
"We give algorithms with provable fairness and regret guarantees, as well as lower bounds, and provide empirical results which suggest that our algorithms perform better than the theory suggests.",abstractText,[0],[0]
Meritocratic Fairness for Cross-Population Selection,title,[0],[0]
Learning from examples is the process of inferring a general rule from a finite set of examples.,1. Introduction,[0],[0]
"It is well known in statistics (e.g., Devroye et al. (1996))",1. Introduction,[0],[0]
that learning cannot take place without prior assumptions.,1. Introduction,[0],[0]
"Recent work in deep neural networks has achieved significant success in using prior knowledge in the implementation of structural
1The Viterbi Faculty of Electrical Engineering, Technion - Israel Institute of Technology, Haifa, Israel.",1. Introduction,[0],[0]
"Correspondence to: Ron Amit <ronamit@campus.technion.ac.il>, Ron Meir <rmeir@ee.technion.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"constraints, e.g., convolutions and weight sharing (LeCun et al., 2015).",1. Introduction,[0],[0]
"However, often the relevant prior information for a given task is not clear, and there is a need for building it through learning from previous interactions with the world.",1. Introduction,[0],[0]
"Learning from previous experience can take several forms: Continual Learning (Kirkpatrick et al., 2017) - a learning agent is trained on a sequence of tasks, aiming to solve the current task while maintaining good performance on previous tasks.",1. Introduction,[0],[0]
"Multi-Task Learning (Caruana, 1997) - a learning agent learns how to solve several observed tasks, while exploiting their shared structure.",1. Introduction,[0],[0]
"Domain Adaptation (Ben-David et al., 2010) - a learning agent solves a ‘target’ learning task using ‘source’ tasks (both are observed, but usually the target is predominantly unlabeled).",1. Introduction,[0],[0]
"We work within the framework of Meta-Learning / Learning-toLearn / Inductive Transfer (Thrun & Pratt, 1997; Vilalta & Drissi, 2002) 1 in which a ‘meta-learner’ extracts knowledge from several observed tasks to facilitate the learning of new tasks by a ‘base-learner’ (see Figure 1).",1. Introduction,[1.0],"['We work within the framework of Meta-Learning / Learning-toLearn / Inductive Transfer (Thrun & Pratt, 1997; Vilalta & Drissi, 2002) 1 in which a ‘meta-learner’ extracts knowledge from several observed tasks to facilitate the learning of new tasks by a ‘base-learner’ (see Figure 1).']"
In this setup the meta-learner must generalize from a finite set of observed tasks.,1. Introduction,[0],[0]
"The performance is evaluated when learning related new tasks (which are unavailable to the meta-learner) .
",1. Introduction,[1.0000000161919482],['The performance is evaluated when learning related new tasks (which are unavailable to the meta-learner) .']
"As a motivational example, consider the case in which a meta-learner observes many image classification tasks of natural images, and uses a CNN to learn each task.",1. Introduction,[0],[0]
"The meta-learner might learn a prior which fixes the lower layers of the network to extract generic image features, but allows variation in the higher layers to adapt to new classes.",1. Introduction,[0],[0]
"Thus, new tasks can be learned using fewer examples than learning from scratch (e.g., Yosinski et al. (2014)).",1. Introduction,[1.0],"['Thus, new tasks can be learned using fewer examples than learning from scratch (e.g., Yosinski et al. (2014)).']"
"Generally, other scenarios might instead benefit from sharing other parts of the network (e.g., Yin & Pan (2017)).",1. Introduction,[0],[0]
"In our framework the prior is automatically inferred from the observed tasks, rather than being manually inserted by the algorithm designer.
",1. Introduction,[0],[0]
The notion of ‘task-environment’ was formulated by Baxter (2000).,1. Introduction,[0],[0]
"In analogy to the standard single-task learning where data is sampled from an unknown distribution, Baxter
1In our setting all observed tasks are available simultaneously to the meta-learner.",1. Introduction,[0],[0]
"The setting in which task are observed sequentially is often termed as Lifelong Learning (Thrun, 1996; Alquier et al., 2017)
suggested a setting where tasks are sampled from an unknown task distribution (environment), so that knowledge acquired from previous tasks can be used in order to improve performance on a novel task.",1. Introduction,[0],[0]
"Baxter’s work not only provided an interesting and mathematically precise perspective for meta-learning, but also provided generalization bounds demonstrating the potential improvement in performance due to prior knowledge.
",1. Introduction,[0],[0]
"In this paper we work within the framework formulated by Baxter (2000), and, following the setup in Pentina & Lampert (2014), provide generalization error bounds within the PAC-Bayes framework.",1. Introduction,[0],[0]
"These bounds are then used to develop a practical learning algorithm that is applied to neural networks, demonstrating the utility of our approach.",1. Introduction,[0],[0]
The main contributions of this work are the following.,1. Introduction,[1.0],['The main contributions of this work are the following.']
(i) An improved and tighter bound in the theoretical framework of Pentina & Lampert (2014) derived using a technique which can extend different single-task PAC-Bayes bounds to the meta-learning setup.,1. Introduction,[0],[0]
(ii) A principled meta-learning method and its implementation using probabilistic feedforward neural networks.,1. Introduction,[0],[0]
"(iii) Empirical demonstration of the performance enhancement compared to naive approaches as well as recent methods in this field.
",1. Introduction,[0],[0]
"Related Work While there have been many recent developments in meta-learning (e.g., Edwards & Storkey (2016); Andrychowicz et al. (2016); Finn et al. (2017))",1. Introduction,[0],[0]
", most of them were not based on generalization error bounds, which is the focus of the present work.",1. Introduction,[0],[0]
"An elegant extension of generalization error bounds to meta-learning was provided by Pentina & Lampert (2014), mentioned above (extended in Pentina & Lampert (2015)).",1. Introduction,[0],[0]
"Their work, however, did not provide a practical algorithm applicable to deep neural networks.",1. Introduction,[0],[0]
"More recently, Dziugaite & Roy (2017) developed a single-task algorithm based on PAC-Bayes bounds that was demonstrated to yield good performance in simple classification tasks with deep networks.",1. Introduction,[0],[0]
Other recent theoretical approaches to meta or multitask learning (e.g. Maurer (2005; 2009); Ruvolo & Eaton (2013); Maurer et al. (2016); Alquier et al. (2017)) provide increasingly general bounds but have not led to practical algorithms for neural networks.,1. Introduction,[0],[0]
"In the common setting for learning, a set of independent samples, S = {zi}mi=1, from a space of examples Z , is given, each sample drawn from an unknown probability distribution D, namely zi ∼ D. We will use the notation S ∼ Dm to denote the distribution over the full sample.",2. Preliminaries: PAC-Bayes Learning,[0.9557442179608516],"['The prior knowledge comes in the form of a distribution over hypotheses, P ∈ M. When learning a new task, the base learner uses the observed task’s data S and the prior P to output a posterior distribution Q(S, P ) overH.']"
"In supervised learning, the samples are input/output pairs zi =",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"(xi, yi).",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"The usual learning goal is, based on S, to find a hypothesis h ∈ H, where H is the so-called hypothesis space, that minimizes the expected loss function E`(h, z), where `(h, z) is a loss function bounded in [0, 1] .",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"As the distribution D is unknown, learning consists of selecting an appropriate h based on the sample S. In classification H is a space of classifiers mapping the input space to a finite set of classes.",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"As noted in the Introduction, an inductive bias is required for effective learning.",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"While in the standard approach to learning, described in the previous paragraph, one usually selects a single classifier (e.g., the one minimizing the empirical error), the PAC-Bayes framework, first formulated by McAllester (1999), considers the construction of a complete probability distribution overH, and the selection of a single hypothesis h ∈ H based on this distribution.",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"Since this distribution depends on the data it is referred to as a posterior distribution and will be denoted by Q. We note that while the term ‘posterior’ has a Bayesian connotation, the framework is not necessarily Bayesian, and the posterior does not need to be related to the prior through the likelihood function as in standard Bayesian analysis.
",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"The PAC-Bayes framework has been widely studied in recent years, and has given rise to significant flexibility in learning, and, more importantly, to some of the best generalization bounds available Seeger (2002); Catoni (2007); Audibert (2010); Lever et al. (2013).",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"Recent works analyzed transfer-learning in neural networks with PAC-Bayes tools (Galanti et al., 2016; McNamara & Balcan, 2017).",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"The framework has been recently extended to the meta-learning setting by Pentina & Lampert (2014), and will be extended and applied to neural networks in the present contribution.",2. Preliminaries: PAC-Bayes Learning,[0],[0]
"Following the notation introduced above we define the expected error er (h,D) , E
z∼D `(h, z) and the empirical error êr (h, S) , (1/m) ∑m j=1 ` (h, zi) for a single hypothesis h ∈ H.",2.1. Single-task Problem Formulation,[0],[0]
"Since the distribution D is unknown, er (h,D) cannot be directly computed.",2.1. Single-task Problem Formulation,[0],[0]
"In the PAC-Bayes setting the learner outputs a distribution over the entire hypothesis space H, i.e, the goal is to provide a posterior distribution Q ∈ M, where M denotes the set of distributions over H. The expected error and empirical error are then given in this setting by averaging over the posterior distribution Q ∈ M, namely er (Q,D) , E
h∼Q er (h,D) and
êr (Q,S) , E h∼Q êr (h, S), respectively.",2.1. Single-task Problem Formulation,[0],[0]
In this section we introduce a PAC-Bayes bound for the single-task setting.,2.2. PAC-Bayes Generalization Bound,[0],[0]
The bound will also serve us for the meta-learning setting in the next sections.,2.2. PAC-Bayes Generalization Bound,[0],[0]
"PAC-Bayes bounds are based on specifying some ‘prior’ reference distribution P ∈M, that must not depend on the observed data S.",2.2. PAC-Bayes Generalization Bound,[0],[0]
The distribution over hypotheses Q which is provided as an output from the learning process is called the posterior (since it is allowed to depend on S) 2.,2.2. PAC-Bayes Generalization Bound,[0],[0]
The classical PACBayes theorem for single-task learning was formulated by McAllester (1999).,2.2. PAC-Bayes Generalization Bound,[0],[0]
Theorem 1 (McAllester’s single-task bound).,2.2. PAC-Bayes Generalization Bound,[0],[0]
Let P ∈M be some prior distribution overH.,2.2. PAC-Bayes Generalization Bound,[0],[0]
"Then for any δ ∈ (0, 1], the following inequality holds uniformly for all posteriors distributions Q ∈M with probability at least 1− δ,
er (Q,D) ≤ êr",2.2. PAC-Bayes Generalization Bound,[0],[0]
"(Q,S) +
√ D(Q||P )",2.2. PAC-Bayes Generalization Bound,[0],[0]
"+ log mδ
2(m− 1)
where D(Q||P ) is the Kullback-Leibler (KL) divergence, D(Q||P ) , E
h∼Q log Q(h)P (h) .
",2.2. PAC-Bayes Generalization Bound,[0],[0]
"Theorem 1 can be interpreted as stating that with high probability the expected error er (Q,D) is upper bounded by the empirical error plus a complexity term.",2.2. PAC-Bayes Generalization Bound,[0],[0]
"Since, with high probability, the bound holds uniformly for all Q ∈M, it holds also for data dependent Q.",2.2. PAC-Bayes Generalization Bound,[0],[0]
By choosing Q that minimizes the bound we obtain a learning algorithm with generalization guarantees.,2.2. PAC-Bayes Generalization Bound,[1.0],['By choosing Q that minimizes the bound we obtain a learning algorithm with generalization guarantees.']
"Note that PAC-Bayes bounds express a trade-off between fitting the data (empirical error) and a complexity/regularization term (distance from prior) which encourages selecting a ‘simple’ hypothesis, namely one similar to the prior.",2.2. PAC-Bayes Generalization Bound,[0],[0]
The specific choice of P affects the bound’s tightness and so should express prior knowledge about the problem.,2.2. PAC-Bayes Generalization Bound,[0],[0]
"Generally, we want the prior to be close to posteriors which can achieve low training error.",2.2. PAC-Bayes Generalization Bound,[0],[0]
In this section we introduce the meta-learning setting.,3. PAC-Bayes Meta-Learning,[0],[0]
In this setting a meta-learning agent observes several ‘training’ tasks from the same task environment.,3. PAC-Bayes Meta-Learning,[0],[0]
"The meta-learner must extract some common knowledge (‘learned prior’) from these tasks, which will be used for learning new tasks from the same environment.",3. PAC-Bayes Meta-Learning,[0],[0]
"In the literature this setting
2As noted above, the terms ‘prior’ and ‘posterior’ might be misleading, since, this is not a Bayesian inference setting (the prior and posterior are not connected through the Bayes rule).",3. PAC-Bayes Meta-Learning,[0],[0]
"However, PAC-Bayes and Bayesian analysis have interesting and practical connections, as we will see in the next sections (see also Germain et al. (2016)).
is often called learning-to-learn, lifelong-learning, metalearning or bias learning (Baxter, 2000).",3. PAC-Bayes Meta-Learning,[0],[0]
We will formulate the problem and provide a generalization bound which will later lead to a practical algorithm.,3. PAC-Bayes Meta-Learning,[0],[0]
Our work extends Pentina & Lampert (2014) and establishes a potentially tighter bound.,3. PAC-Bayes Meta-Learning,[0],[0]
"Furthermore, we will demonstrate how to apply this result practically to deep neural networks using stochastic learning.",3. PAC-Bayes Meta-Learning,[0],[0]
The meta-learning problem formulation follows Pentina & Lampert (2014).,3.1. Meta-Learning Problem Formulation,[0],[0]
"We assume all tasks share the sample space Z , hypothesis spaceH and loss function ` :",3.1. Meta-Learning Problem Formulation,[0],[0]
H×Z,3.1. Meta-Learning Problem Formulation,[0],[0]
"→ [0, 1].",3.1. Meta-Learning Problem Formulation,[0],[0]
"The learning tasks differ in the unknown sample distribution Dt associated with each task t. The meta-learning agent observes the training sets S1, ..., Sn corresponding to n different tasks.",3.1. Meta-Learning Problem Formulation,[0],[0]
The number of samples in task i is denoted by mi.,3.1. Meta-Learning Problem Formulation,[0],[0]
Each observed dataset Si is assumed to be generated from an unknown sample distribution Si ∼ Dmii .,3.1. Meta-Learning Problem Formulation,[0],[0]
"As in Baxter (2000), we assume that the sample distributions Di are generated i.i.d.",3.1. Meta-Learning Problem Formulation,[0],[0]
from an unknown tasks distribution τ .,3.1. Meta-Learning Problem Formulation,[0],[0]
The goal of the meta-learner is to extract some knowledge from the observed tasks that will be used as prior knowledge for learning new (yet unobserved) tasks from τ .,3.1. Meta-Learning Problem Formulation,[0],[0]
"The prior knowledge comes in the form of a distribution over hypotheses, P ∈ M.",3.1. Meta-Learning Problem Formulation,[0],[0]
"When learning a new task, the base learner uses the observed task’s data S and the prior P to output a posterior distribution Q(S, P )",3.1. Meta-Learning Problem Formulation,[0],[0]
overH. We assume that all tasks are learned via the same learning process.,3.1. Meta-Learning Problem Formulation,[0],[0]
"Namely, for a given S and P there is a specific output Q(S, P ).",3.1. Meta-Learning Problem Formulation,[0],[0]
Hence the base learner Q is a mapping: Q :,3.1. Meta-Learning Problem Formulation,[0],[0]
"Zm ×M→M. 3
The quality of a prior P is measured by the expected loss when using it to learn new tasks, as defined by,
er (P, τ) , E (D,m)∼τ E S∼Dm E h∼Q(S,P )",3.1. Meta-Learning Problem Formulation,[0],[0]
E z∼D,3.1. Meta-Learning Problem Formulation,[0],[0]
"`(h, z).",3.1. Meta-Learning Problem Formulation,[0],[0]
"(1)
The expectation is taken w.r.t.",3.1. Meta-Learning Problem Formulation,[0],[0]
"(i) tasks drawn from the task environment, (ii) training samples, (iii) hypotheses drawn from the posterior which is learned based on the training samples and prior (iv) a ‘test’ sample.
",3.1. Meta-Learning Problem Formulation,[0],[0]
"As described in section 2, in the single-task PAC-Bayes framework, the learner assumes a prior over hypotheses P (h), then observes the training samples and outputs a posterior distribution over hypotheses Q(h).",3.1. Meta-Learning Problem Formulation,[0],[0]
"In an analogous way, in the meta-learning PAC-Bayes framework, the meta-learner assumes a prior distribution over priors, a ‘hyper-prior’ P(P ), observes the training tasks, and then outputs a distribution over priors, a ‘hyper-posterior’ Q(P ).
",3.1. Meta-Learning Problem Formulation,[0],[0]
"We emphasize that while the hyper-posterior is learned us-
3In the next section we will use stochastic optimization methods as learning algorithms, but we can assume convergence to a same solution for any execution with a given S and P .
",3.1. Meta-Learning Problem Formulation,[0],[0]
"ing the observed tasks, the goal is to use it for learning new, independent task from the environment.",3.1. Meta-Learning Problem Formulation,[0],[0]
"When encountering a new task, the learner samples a prior from the hyper posterior Q(P ), and then use it for learning.",3.1. Meta-Learning Problem Formulation,[0],[0]
"Ideally, the performance of the hyper-posterior Q is measured by the expected loss of learning new tasks using priors drawn from Q. This quantity is denoted as the transfer error
er (Q, τ) ,",3.1. Meta-Learning Problem Formulation,[0],[0]
E P∼Q,3.1. Meta-Learning Problem Formulation,[0],[0]
"er (P, τ) .",3.1. Meta-Learning Problem Formulation,[0],[0]
"(2)
While er (Q, τ) is not computable, we can however evaluate the average empirical risk when learning the observed tasks using priors drawn from Q, which is denoted as the empirical multi-task error
êr (Q, S1, ..., Sn) , E P∼Q
1
n n∑ i=1 êr",3.1. Meta-Learning Problem Formulation,[0],[0]
"(Q(Si, P ), Si) , (3)
",3.1. Meta-Learning Problem Formulation,[0],[0]
"In the single-task PAC-Bayes setting one selects a prior P ∈M before seeing the data, and updates it to a posterior Q ∈ M after observing the training data.",3.1. Meta-Learning Problem Formulation,[0],[0]
"In the present meta-learning setup, following Pentina & Lampert (2014), one selects an initial hyper-prior distribution P , essentially a distribution over prior distributions P , and, following the observation of the data from all tasks, updates it to a hyperposterior distribution Q. As a simple example, assume the initial prior P is a Gaussian distribution over neural network weights, characterized by a mean and covariance.",3.1. Meta-Learning Problem Formulation,[0],[0]
A hyper distribution would correspond in this case to a distribution over the mean and covariance of P .,3.1. Meta-Learning Problem Formulation,[0],[0]
In this section we present a novel bound on the transfer error in the meta-learning setup.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"The theorem is proved in section A.1 of the supplementary material.
",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
Theorem 2 (Meta-learning PAC-Bayes bound).,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
Let Q :,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"Zm ×M→M be a base learner, and let P be some predefined hyper-prior distribution.",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"Then for any δ ∈ (0, 1] the following inequality holds uniformly for all hyper-posterior distributions Q with probability at least 1− δ, 4
er (Q, τ) ≤ 1 n n∑ i=1",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
E P∼Q,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"êri (Qi, Si) (4)
+ 1
n n∑ i=1",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
√√√√D(Q||P),3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
+,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
EP∼Q D(Qi||P ),3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
+ log 2nmiδ 2(mi,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"− 1)
+
√ D(Q||P) + log 2nδ
2(n− 1) ,
where Qi , Q(Si, P ).
",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"4The probability is taken over sampling of (Di,mi) ∼ τ",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"and Si ∼ Dmii , i = 1, ..., n.
Notice that the transfer error (2) is bounded by the empirical multi-task error (3) plus two complexity terms.",3.2. Meta-Learning PAC-Bayes Bound,[0.9613801769505231],['Notice that the transfer error (2) is bounded by the empirical multi-task error (3) plus two complexity terms.']
The first is the average of the task-complexity terms of the observed tasks.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
This term converges to zero in the limit of a large number of samples in each task (mi →∞).,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
The second is an environment-complexity term.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
This term converges to zero if infinite number of tasks is observed from the task environment (n→∞).,3.2. Meta-Learning PAC-Bayes Bound,[1.0],['This term converges to zero if infinite number of tasks is observed from the task environment (n→∞).']
"As in Pentina & Lampert (2014), our proof is based on two main steps.",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"The second step, similarly to Pentina & Lampert (2014), bounds the transfer-risk at the task-environment level (i.e, the error caused by observing only a finite number of tasks) by the average expected error in the observed tasks plus the environment-complexity term.",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
The first step differs from Pentina & Lampert (2014).,3.2. Meta-Learning PAC-Bayes Bound,[1.0],['The first step differs from Pentina & Lampert (2014).']
"Instead of using a single joint bound on the average expected error, we use a single-task PAC-Bayes theorem to bound the expected error in each task separately (when learned using priors from the hyper-posterior), and then use a union bound argument.",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
By doing so our bound takes into account the specific number of samples in each observed task (instead of their harmonic mean).,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"Therefore our bound is better adjusted the observed data set.
",3.2. Meta-Learning PAC-Bayes Bound,[0.9999999929720772],['Therefore our bound is better adjusted the observed data set.']
Our proof technique can utilize different single-task bounds in each of the two steps.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"In section A.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina & Lampert (2014).",3.2. Meta-Learning PAC-Bayes Bound,[1.0],"['In section A.1 we use McAllester’s bound (Theorem 1), which is tighter than the lemma used in Pentina & Lampert (2014).']"
"Therefore, the complexity terms
are in the form of √
1 mD(Q||P ) instead of 1√ m D(Q||P )
",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
as in Pentina & Lampert (2014).,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
This means the bound is tighter 5.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"In section A.2 we demonstrate how our technique can use other, possibly tighter, single-task bounds.",3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
In Section 5 we will empirically evaluate the different bounds as meta-learning objectives and show that the improved tightness is critical for performance.,3.2. Meta-Learning PAC-Bayes Bound,[0],[0]
"As in the single-task case, the bound of Theorem 2 can be evaluated from the training data and so can serve as a minimization objective for a principled meta-learning algorithm.",4. Meta-Learning Algorithm,[0],[0]
"Since the bound holds uniformly for all Q, it is ensured to hold also for the minimizer of the objective Q∗. Provided that the bound is tight enough, the algorithm will approximately minimize the transfer-risk itself, avoiding overfitting to the observed tasks.",4. Meta-Learning Algorithm,[0],[0]
"In this section we will derive a practical learning procedure that can applied to a large family of differentiable models, including deep neural networks.",4. Meta-Learning Algorithm,[0],[0]
"In this section we choose a specific form of hyper-posterior distribution Q which enables practical implementation.
5E.g., Seldin et al. (2012) Theorems 5 and 6
Given a parametric family of priors { Pθ̃ : θ̃ ∈ RNP } , NP ∈ N, the space of hyper-posteriors consists of all distributions over RNP .",4.1. Hyper-Posterior Model,[0],[0]
"We will limit our search to a family of isotropic Gaussian distributions defined by Qθ , N ( θ, κ2QINP×NP ) , where κQ > 0 is a predefined constant.",4.1. Hyper-Posterior Model,[0],[0]
"Notice that Q appears in the bound (4) in two forms (i) divergence from the hyper-prior D(Q||P) and (ii) expectations over P ∼ Q.
By setting the hyper-prior as zero-mean isotropic Gaussian, P = N ( 0, κ2PINP×NP ) , where κP > 0 is another constant, we get a simple form for the KL-divergence term, D(Qθ||P) = 12κ2P ‖θ‖ 2 2 .",4.1. Hyper-Posterior Model,[0.9587498826683829],"['By setting the hyper-prior as zero-mean isotropic Gaussian, P = N ( 0, κ2PINP×NP ) , where κP > 0 is another constant, we get a simple form for the KL-divergence term, D(Qθ||P) = 12κ2P ‖θ‖ 2 2 .']"
"Note that the hyper-prior acts as a regularization term which prefers solutions with small L2 norm.
",4.1. Hyper-Posterior Model,[1.000000031389446],['Note that the hyper-prior acts as a regularization term which prefers solutions with small L2 norm.']
The expectations can be approximated by averaging several Monte-Carlo samples of P .,4.1. Hyper-Posterior Model,[0],[0]
"Notice that sampling from Qθ means adding Gaussian noise to the parameters θ during training, θ̃ = θ + εP , εP ∼ N ( 0, κ2QINP×NP ) .",4.1. Hyper-Posterior Model,[0],[0]
"This means the learned parameters must be robust to perturbations, which encourages selecting solutions which are less prone to over-fitting.",4.1. Hyper-Posterior Model,[0],[0]
"The term appearing on the RHS of the meta-learning bound in (4) can be compactly written as
J(θ) , 1
n n∑ i=1 Ji(θ)",4.2. Joint Optimization,[0],[0]
"+ Υ(θ), (5)
where we defined,
Ji(θ) , E θ̃∼Qθ
êri ( Qi(Si, Pθ̃), Si ) (6)
+ √√√√D(Qθ||P) + Eθ̃∼Qθ D(Q(Si, Pθ̃)||Pθ̃)+ log 2nmiδ 2(mi",4.2. Joint Optimization,[0],[0]
"− 1) ,
Υ(θ) ,
√ D(Qθ||P) + log 2nδ
2(n− 1) .",4.2. Joint Optimization,[0],[0]
"(7)
Theorem 2 allows us to choose any procedure Q(Si, P ) :",4.2. Joint Optimization,[0],[0]
Zmi ×M→M as a base learner.,4.2. Joint Optimization,[0],[0]
We will use a procedure which minimizes Ji(θ) due to the following advantages: (i),4.2. Joint Optimization,[0],[0]
It minimizes a bound on the expected error of the observed task 6.,4.2. Joint Optimization,[0],[0]
(ii) It uses the prior knowledge gained from the prior P to get a tighter bound and a better learning objective.,4.2. Joint Optimization,[0],[0]
"(iii) As will be shown next, formulating the single task learning as an optimization problem enables joint learning of the shared prior and the task posteriors.
",4.2. Joint Optimization,[0],[0]
"To formulate the single-task learning as an optimization problem, we choose a parametric form for the posterior of
6See section A.1 in the supplementary material.
",4.2. Joint Optimization,[0],[0]
"each task Qφi , φi ∈ RNQ (see section 4.3 for an explicit example).",4.2. Joint Optimization,[0],[0]
"The base-learning algorithm can be formulated as φ∗i = argminφi Ji(θ, φi), where we abuse notation by denoting the term Ji(θ) evaluated with posterior parameters φi as Ji(θ, φi).",4.2. Joint Optimization,[0],[0]
"The meta-learning problem of minimizing J(θ) over θ can now be written more explicitly,
min θ,φ1,...,φn
{ 1
n n∑ i=1",4.2. Joint Optimization,[0],[0]
"Ji(θ, φi) + Υ(θ)
} .",4.2. Joint Optimization,[0],[0]
"(8)
The optimization process is illustrated in Figure 2.",4.2. Joint Optimization,[0.9999999737962195],['(8) The optimization process is illustrated in Figure 2.']
In this section we make the meta-learning optimization problem (8) more explicit by defining a model for the posterior and prior distributions.,4.3. Distributions Model,[0],[0]
"First, we define the hypothesis class H as a family of functions parameterized by a weight vector{ hw : w ∈ Rd } .",4.3. Distributions Model,[1.0],"['First, we define the hypothesis class H as a family of functions parameterized by a weight vector{ hw : w ∈ Rd } .']"
"Given this parameterization, the posterior and prior are distributions over Rd.
",4.3. Distributions Model,[1.000000092900091],"['Given this parameterization, the posterior and prior are distributions over Rd.']"
"We will present an algorithm for any differentiable model 7, but our aim is to use neural network (NN) architectures.",4.3. Distributions Model,[0],[0]
"In fact, we will use Stochastic NNs (Graves, 2011; Blundell et al., 2015) since in our setting the weights are random and we are optimizing their posterior distribution.",4.3. Distributions Model,[0],[0]
The techniques presented next will be mostly based on Blundell et al. (2015).,4.3. Distributions Model,[0],[0]
"Next we define the posteriors Qφi , i = 1, ..., n, and the prior Pθ as factorized Gaussian distributions8,
Pθ(w)",4.3. Distributions Model,[0],[0]
= d∏,4.3. Distributions Model,[0],[0]
"k=1 N ( wk;µP,k, σ 2 P,k ) (9)
Qφi(w) = d∏",4.3. Distributions Model,[0],[0]
k=1 N,4.3. Distributions Model,[0],[0]
"( wk;µi,k, σ 2",4.3. Distributions Model,[0],[0]
"i,k ) (10)
7The only assumption on { hw : w ∈ Rd } is that the loss function `(hw, z) is differentiable w.r.t w. 8This choice makes optimization easier, but in principle we can use other distributions as long as the density function is differentiable w.r.t.",4.3. Distributions Model,[0],[0]
"the parameters.
where for each task, the posterior parameters vector φi = (µi, ρi) ∈ R2d is composed of the means and log-variances of each weight , µi,k and ρi,k = log σ2P,k, k = 1, ..., d. 9",4.3. Distributions Model,[0],[0]
"The shared prior vector θ = (µP , ρP )",4.3. Distributions Model,[0],[0]
∈ R2d has a similar structure.,4.3. Distributions Model,[0],[0]
"Since we aim to use deep models where d could be in the order of millions, distributions with more parameters might be impractical.
",4.3. Distributions Model,[0],[0]
"Since Qφi and Pθ are factorized Gaussian distributions the KL-divergence, D(Qφi ||Pθ), takes a simple analytic form,
1
2 d∑ k=1
{ log
σ2P,k σ2i,k + σ2i,k +",4.3. Distributions Model,[0],[0]
"(µi,k − µP,k)
2
σ2P,k",4.3. Distributions Model,[0],[0]
"− 1
} .",4.3. Distributions Model,[0],[0]
(11),4.3. Distributions Model,[0],[0]
"As an underlying optimization method, we will use stochastic gradient descent (SGD).",4.4. Optimization Technique,[0],[0]
"In each iteration, the algorithm takes a parameter step in a direction of an estimated negative gradient.",4.4. Optimization Technique,[0],[0]
"As is well known, lower variance facilitates convergence and its speed.",4.4. Optimization Technique,[0],[0]
Recall that each single-task bound is composed of an empirical error term and a complexity term (6).,4.4. Optimization Technique,[0],[0]
"The complexity term is a simple function of D(Qφi ||Pθ) (11), which can easily be differentiated analytically.",4.4. Optimization Technique,[0],[0]
"However, evaluating the gradient of the empirical error term is more challenging.
",4.4. Optimization Technique,[0],[0]
"Recall the definition of the empirical error, êr (Qφi , Si) = Ew∼Qφi (1/mi) ∑mi j=1 ` (hw, zi,j).",4.4. Optimization Technique,[0],[0]
This term poses two major challenges.,4.4. Optimization Technique,[0],[0]
(i),4.4. Optimization Technique,[0],[0]
The data set Si could be very large making it expensive to cycle over all the mi samples.,4.4. Optimization Technique,[0],[0]
(ii),4.4. Optimization Technique,[0],[0]
"The term ` (hw, zj) might be highly non-linear in w, rendering the expectation intractable.",4.4. Optimization Technique,[0],[0]
"Still, we can get an unbiased and low variance estimate of the gradient.
",4.4. Optimization Technique,[0],[0]
"First, instead of using all of the data for each gradient estimation we will use a randomly sampled mini-batch S′i ⊂ Si.",4.4. Optimization Technique,[0],[0]
"Next, we require an estimate of a gradient of the form ∇φ E
w∼Qφ f(w) which is a common problem in
machine learning.",4.4. Optimization Technique,[0],[0]
"We will use the ‘re-parametrization trick’ (Kingma & Welling, 2013; Rezende et al., 2014) which is an efficient and low variance method 10 .",4.4. Optimization Technique,[0],[0]
The re-parametrization trick is easily applicable in our setup since we are using Gaussian distributions.,4.4. Optimization Technique,[0],[0]
"The trick is based on describing the Gaussian distribution w ∼ Qφi (9) as first drawing ε ∼ N (0̄, Id×d) and then applying the deterministic function w(φi, ε) =",4.4. Optimization Technique,[0],[0]
"µi + σi ε (where
9Note that we use ρ = log σ2 as a parameter in order to keep the parameters unconstrained (while σ2 = exp(ρ) is guaranteed to be strictly positive).
",4.4. Optimization Technique,[0],[0]
"10In fact, we will use the ‘local re-parameterization trick’ (Kingma et al., 2015) in which we sample a different ε for each data point in the batch, which reduces the variance of the estimate.",4.4. Optimization Technique,[0],[0]
"To make the computation more efficient with neural-networks, the random number generation is performed w.r.t the activations instead of the weights (see Kingma et al. (2015) for more details.).
",4.4. Optimization Technique,[0],[0]
is an element-wise multiplication).,4.4. Optimization Technique,[0],[0]
"Therefore, we get ∇φ E
w∼Qφ f(w) =",4.4. Optimization Technique,[0],[0]
"∇φ E ε∼N (0̄,Id×d) f(w(φi, ε)).",4.4. Optimization Technique,[0],[0]
The expectation can be approximated by averaging a small number of Monte-Carlo samples with reasonable accuracy.,4.4. Optimization Technique,[0],[0]
"For a fixed sampled ε, the gradient∇φf(w(φi, ε)) is easily computable with backpropagation.
",4.4. Optimization Technique,[0],[0]
"In summary, the Meta-Learning by Adjusting Priors (MLAP) algorithm is composed of two phases In the first phase (Algorithm 1, termed “meta-training”) several observed “training tasks” are used to learn a prior.",4.4. Optimization Technique,[0],[0]
"In the second phase (Algorithm 2, termed “meta-testing”) the previously learned prior is used for the learning of a new task (which was unobserved in the first phase).",4.4. Optimization Technique,[0],[0]
Note that the first phase can be used independently as a multi-task learning method.,4.4. Optimization Technique,[0],[0]
Both algorithms are described in pseudo-code in the supplementary material (section A.4) 11 12.,4.4. Optimization Technique,[0],[0]
In this section we demonstrate the performance of our transfer method with image classification tasks solved by deep neural networks.,5. Experimental Demonstration,[0],[0]
"In image classification, the data samples, z , (x, y), consist of a an image, x, and a label, y.",5. Experimental Demonstration,[0],[0]
The hypothesis class { hw : w ∈ Rd } is the set of neural networks with a given architecture (which will be specified later).,5. Experimental Demonstration,[0],[0]
"As a loss function `(hw, z) we will use the cross-entropy loss.",5. Experimental Demonstration,[0],[0]
"While the theoretical framework is defined with a bounded loss, in our experiments we use an unbounded loss function in the learning objective.",5. Experimental Demonstration,[0],[0]
"Still, we can have theoretical guarantees on a variation of the loss which is clipped to [0, 1].",5. Experimental Demonstration,[0],[0]
"Furthermore, in practice the loss function is almost always smaller than one.
",5. Experimental Demonstration,[0],[0]
"We conduct two experiments with two different task environments, based on augmentations of the MNIST dataset (LeCun, 1998).",5. Experimental Demonstration,[0],[0]
"In the first environment, termed permuted labels, each task is created by a random permutation of the labels.",5. Experimental Demonstration,[0],[0]
"In the second environment, termed permuted pixels, each task is created by a permutation of the image pixels.",5. Experimental Demonstration,[0],[0]
"The pixel permutations are created by a limited number of location swaps to ensure that the tasks stay reasonably related.
",5. Experimental Demonstration,[0],[0]
"In both experiments, the meta-training set is composed of tasks from the environment with 60, 000 training examples.",5. Experimental Demonstration,[0],[0]
"Following the meta-training phase, the learned prior is used to learn a new meta-test task with fewer training samples (2, 000).",5. Experimental Demonstration,[0],[0]
"The network architecture used for the permutedlabels experiment is a small CNN with 2 convolutionallayers, a linear hidden layer and a linear output layer.",5. Experimental Demonstration,[0],[0]
"In
11Code is available at: https://github.com/ ron-amit/meta-learning-adjusting-priors.
",5. Experimental Demonstration,[0],[0]
"12For a visual illustration of the algorithm using a toy example see section A.6 in the supplementary material.
",5. Experimental Demonstration,[0],[0]
the permuted-pixels experiment we used a fully-connected network with 3 hidden layers and a linear output layer.,5. Experimental Demonstration,[0],[0]
"See section A.5 for more implementation details.
",5. Experimental Demonstration,[0],[0]
We compare the average test error of learning a new task from each environment when using the following methods.,5. Experimental Demonstration,[0],[0]
"As a baseline, we measure the performance of learning from scratch, i.e., with no transfer from the meta-training tasks.",5. Experimental Demonstration,[0],[0]
Scratch-D: deterministic (standard) learning from scratch.,5. Experimental Demonstration,[0],[0]
"Scratch-S: stochastic learning from scratch (using a stochastic network with no prior/complexity term).
",5. Experimental Demonstration,[0],[0]
Other methods transfer knowledge from only one of the meta-training tasks.,5. Experimental Demonstration,[0],[0]
Warm-start: Standard learning with initial weights taken from the standard learning of a single task from the meta-training set.,5. Experimental Demonstration,[0],[0]
"Oracle: Same as the previous method, but some of the layers are frozen (unchanged from their initial value) depending on the experiment.",5. Experimental Demonstration,[0],[0]
In the permuted labels experiment all layers besides the output are frozen.,5. Experimental Demonstration,[0],[0]
In the permuted pixels we freeze all layers except the input layer.,5. Experimental Demonstration,[0],[0]
"We refer to this method as ‘oracle’ since the transfer technique is tailored to each task-environment, while the other methods are applied identically in any environment (and so must learn to adjust to the environment automatically).
",5. Experimental Demonstration,[0],[0]
"Finally, we compare methods which transfer knowledge from all of the training tasks: MLAP-M: The objective is based on Theorem 2 - the meta-learning bound obtained using Theorem 1 (McAllester’s single-task bound).",5. Experimental Demonstration,[0],[0]
MLAP-S:,5. Experimental Demonstration,[0],[0]
"The objective is based on the meta-learning bound derived from Seeger’s single-task bound (see section A.2 in the supplementary material, eq.(18)).",5. Experimental Demonstration,[0],[0]
MLAP-PL:,5. Experimental Demonstration,[0],[0]
"In this method we use the main theorem of Pentina & Lampert (2014) as an objective for the algorithm, instead of Theorem 2.",5. Experimental Demonstration,[0],[0]
MLAPVB:,5. Experimental Demonstration,[0],[0]
In this method the learning objective is derived from a Hierarchal Bayesian framework using variational Bayes tools 13.,5. Experimental Demonstration,[0],[0]
"Averaged: Each of the training tasks is learned in a standard way to obtain a weights vector, wi.",5. Experimental Demonstration,[0],[0]
"The learned prior is set as an isotropic Gaussian with unit variances and a mean vector which is the average of wi, i = 1, .., n.",5. Experimental Demonstration,[0],[0]
This prior is used for meta-testing as in MLAP-S. MAML:,5. Experimental Demonstration,[0],[0]
The Model-Agnostic-Meta-Learning (MAML) algorithm by Finn et al. (2017).,5. Experimental Demonstration,[0],[0]
"In MAML the base learner takes few gradient steps from an initial point, θ, to adapt to a task.",5. Experimental Demonstration,[0],[0]
The meta-learner optimizes θ based on the sum of losses on the observed tasks after base-learning.,5. Experimental Demonstration,[0],[0]
"We tested several hyper-parameters and report the best results (see details in the supplementary material A.5).
",5. Experimental Demonstration,[0],[0]
Table 1 summarizes the results for the permuted labels experiment with 5 training-tasks and the permuted pixels experiment with 200 pixel swaps and 10 training-tasks.,5. Experimental Demonstration,[0],[0]
"In
13See section A.3 in the supplementary material for details.",5. Experimental Demonstration,[0],[0]
"The explicit learning objective is in equation (23).
",5. Experimental Demonstration,[0],[0]
the permuted labels experiment the best results are obtained with the “oracle” method.,5. Experimental Demonstration,[0],[0]
Recall that the oracle method has the “unfair” advantage of a “hand-engineered” transfer technique which is based on knowledge about the problem.,5. Experimental Demonstration,[0],[0]
"In contrast, the other methods must automatically learn the task environment by observing several tasks.
",5. Experimental Demonstration,[0],[0]
The MLAP-M and MLAP-S variants of the MLAP algorithm improves considerably over learning from scratch and over the naive warm-start transfer.,5. Experimental Demonstration,[0],[0]
They even improve over the “oracle” method in the permuted pixels experiment.,5. Experimental Demonstration,[0],[0]
The result of the MLAP-VB are close to the MLAP-M and MLAP-S variants.,5. Experimental Demonstration,[0],[0]
However the MLAP-PL variant performed much worse since the complexity terms are disproportionately large compared to the empirical error terms.,5. Experimental Demonstration,[0],[0]
This demonstrates the importance of using the tight generalization bound developed in our work as a learning objective.,5. Experimental Demonstration,[0],[0]
The results for the “averaged-prior” method are about the same as learning from scratch.,5. Experimental Demonstration,[0],[0]
"Due to the high non-linearity of the problem, averaging weights was not expected to perform well.
",5. Experimental Demonstration,[0],[0]
The results of the MLAP algorithm are slightly better than MAML.,5. Experimental Demonstration,[0],[0]
Note that in MAML the meta-learning only infers an initial point for base-learning.,5. Experimental Demonstration,[0],[0]
Thus there is a trade-off in choosing the number of adaptation steps.,5. Experimental Demonstration,[0],[0]
Taking many gradient steps exploits a larger number of samples but the effect of the initial weights diminishes.,5. Experimental Demonstration,[0],[0]
"Also, taking a large number of steps is computationally infeasible in meta-training.",5. Experimental Demonstration,[0],[0]
"Therefore MAML is especially suited for few-shot learning, which is not the case in our experiment.",5. Experimental Demonstration,[0],[0]
"In our method we infer a prior that serves both as an initial point and as a regularizer which can fix some of the weights, while allowing variation in others, depending on the amount of data.",5. Experimental Demonstration,[0],[0]
Recent work by Grant et al. (2018) showed that MAML can be interpreted as an approximate empirical Bayes procedure.,5. Experimental Demonstration,[0],[0]
"This interesting perspective, differs from the present
contribution that is based on generalization bounds within a non-Bayesian setting.
",5. Experimental Demonstration,[0],[0]
Next we investigate whether using more training tasks improves the quality of the learned prior.,5. Experimental Demonstration,[0],[0]
"In Figure 3 we plot the average test error of learning a new task based on the number of training-tasks in the different environments, namely the permuted labels environment, and the permuted pixels environment with 100, 200, 300 pixel swaps.",5. Experimental Demonstration,[0],[0]
We used the MLAP-S variant of the algorithm.,5. Experimental Demonstration,[0],[0]
"The results clearly show that the more tasks are used to learn the prior, the better the performance on the new task.",5. Experimental Demonstration,[0],[0]
"For example, in the permuted labels case, a prior that is learned based on one or two tasks leads to negative transfer, i.e, worse results than standard learning from scratch (with no transfer), which achieves 2.27% error.",5. Experimental Demonstration,[0],[0]
"However after observing 3 or more tasks, the transfered prior facilitates learning with lower expected error.",5. Experimental Demonstration,[0],[0]
"In the permuted pixels experiment, standard learning from scratch achieves 7.9% test error.",5. Experimental Demonstration,[0],[0]
The number of training tasks needed for positive transfer depends on the number of pixels swapped.,5. Experimental Demonstration,[0],[0]
"A higher number of swaps means larger variation in the task environment and more training-tasks are needed to learn a beneficial prior.
",5. Experimental Demonstration,[0],[0]
Analysis of learned prior Qualitative examination of the learned prior affirms that it has indeed adjusted to each task environment.,5. Experimental Demonstration,[0],[0]
In Figure 4 we inspect the average logvariance parameter the learned prior assigns to the weights of each layer in the network.,5. Experimental Demonstration,[0],[0]
Higher values of this parameter indicate that the weight is more flexible to change.,5. Experimental Demonstration,[0],[0]
"i.e, it is more weakly penalized for deviating form the nominal prior value.",5. Experimental Demonstration,[0],[0]
In the permuted-labels experiment the learned prior assigns low variance to the lower layers (fixed representation) and high variance to the output layer (which enable easy adjustment to different label permutations).,5. Experimental Demonstration,[0],[0]
"As expected, in the permuted-pixels experiment the opposite phenomenon occurs.",5. Experimental Demonstration,[0],[0]
"The mapping from the final hidden layer to the output becomes fixed, and the mapping from the input to the final hidden layer (representation) has more flexibility to change in light of the task data.",5. Experimental Demonstration,[0],[0]
"We have presented a framework for meta-learning, motivated by extended PAC-Bayes generalization bounds, and implemented through the adjustment of a learned prior, based on tasks encountered so far.",6. Discussion and Future Work,[0],[0]
"The framework bears conceptual similarity to the empirical Bayes method while not being Bayesian, and is implemented at the level of tasks rather than samples (see Section A.3 in the supplementary material for details about a Bayesian perspective).",6. Discussion and Future Work,[0],[0]
"Combining the flexibility of the approach, with the rich representational structure of deep neural networks, and learning through gradient based methods leads to an efficient procedure for meta-learning, as motivated theoretically and demonstrated empirically.",6. Discussion and Future Work,[0],[0]
"While our experimental results are preliminary, we believe that our work attests to the utility of using rigorous performance bounds to derive learning algorithms, and demonstrates that tighter bounds indeed lead to improved performance.
",6. Discussion and Future Work,[0],[0]
There are several open issues to consider.,6. Discussion and Future Work,[0],[0]
"First, the current version learns to solve all available tasks in parallel, while a more useful procedure should be sequential in nature.",6. Discussion and Future Work,[0],[0]
This can be easily incorporated into our framework by updating the prior following each novel task.,6. Discussion and Future Work,[0],[0]
"Second, our method requires training stochastic models which is challenging due to the the high-variance gradients.",6. Discussion and Future Work,[0],[0]
We would like to develop new methods within our framework which have more stable convergence and are easier to apply in larger scale problems.,6. Discussion and Future Work,[0],[0]
"Third, there is much current effort in applying meta-learning ideas to reinforcement learning, for example, Teh et al. (2017) presents a heuristically motivated framework that is conceptually similar to ours.",6. Discussion and Future Work,[0],[0]
An interesting challenge would be to extend our techniques to derive meta-learning algorithms for reinforcement learning based on performance bounds.,6. Discussion and Future Work,[0],[0]
"We thank Asaf Cassel, Guy Tennenholtz, Baruch Epstein, Daniel Soudry, Elad Hoffer and Tom Zahavy for helpful discussions of this work, and the anonymous reviewers for their helpful comment.",ACKNOWLEDGMENTS,[0],[0]
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.,ACKNOWLEDGMENTS,[0],[0]
The work was partially supported by the Ollendorff Center of the Viterbi Faculty of Electrical Engineering at the Technion.,ACKNOWLEDGMENTS,[0],[0]
"In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks.",abstractText,[0],[0]
"Under the assumption that future tasks are ‘related’ to previous tasks, the accumulated knowledge should be learned in a way which captures the common structure across learned tasks, while allowing the learner sufficient flexibility to adapt to novel aspects of new tasks.",abstractText,[0],[0]
"We present a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to metalearning.",abstractText,[0],[0]
"Learning takes place through the construction of a distribution over hypotheses based on the observed tasks, and its utilization for learning a new task.",abstractText,[0],[0]
"Thus, prior knowledge is incorporated through setting an experience-dependent prior for novel tasks.",abstractText,[0],[0]
We develop a gradient-based algorithm which minimizes an objective function derived from the bounds and demonstrate its effectiveness numerically with deep neural networks.,abstractText,[0],[0]
"In addition to establishing the improved performance available through meta-learning, we demonstrate the intuitive way by which prior information is manifested at different levels of the network.",abstractText,[0],[0]
Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory,title,[0],[0]
