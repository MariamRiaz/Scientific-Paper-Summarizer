{"sections": [{"heading": "1. Introduction", "text": "In machine learning and social network problems, information is often encoded in matrix form. User profiles in social networks can be embedded into feature matrices; item profiles in recommendation systems can also be modeled as matrices. Many medical imaging modalities, such as MRI and CT, also represent data as a stack of images. These matrices have underlying connections that can come from spatial or temporal proximity, or observed similarities between the items being described, etc. A weighted graph\n*Equal contribution 1Department of Mathematics, Stanford University, California, USA 2Department of Electrical Engineering, Stanford University, California, USA 3Department of Statistics, Stanford University, California, USA. Correspondence to: Qingyun Sun <qysun@stanford.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ncan be built to represent the connections between matrices. Therefore we propose matrix networks as a general model for data representation. A matrix network is defined by a weighted graph whose nodes are matrices.\nDue to the limitations of data acquisition processes, sometimes we can only observe a subset of entries from each data matrix. The fraction of entries we observe may vary from matrix to matrix. In many real problems, a subset of matrices can be completely unobserved, leaving no information for ordinary matrix completion methods to recover the missing matrices. To our knowledge, we are the first to examine this novel sampling scheme.\nAs an example, in the following MRI image sequence (figure 1(a)), we sample each frame of the MRI images with i.i.d. Bernoulli distribution p = 0.2, and 2 out of 88 frames are completely unobserved, shown in figure 1(b). If we perform matrix completion by nuclear norm minimization on\nindividual frames, we are not able to recover the completely unobserved matrices (figure 1(c)). When we build a network on the image frames, in this case, an one-dimensional chain representing the sequence, and assume that the matrices following graph Fourier transform are low-rank, we are able to recover the missing frames, as shown in figure 1(d).\nThe ability to recover all matrices from partial observations, especially inferring matrices that are totally unobserved, is crucial to many applications such as the cold start problem in networks. Illustrated in figure 2, new items or users in a network, which does not have much information available, need to aggregate information from the network to have an initial estimate of their feature matrices, in order to support inference and decisions.\nSince we model the matrices as nodes on a graph, information from other matrices makes it possible to recover the missing ones. To use such information, we make the structural assumption that the matrix network is low-rank in spectral space, i.e., the matrix network is the graph convolution of two low-rank matrix networks. In the MRI example, we verify the spectral low-rank assumption in figure 3. For all the matrices after the graph Fourier transform, singular values quickly decrease to almost zero, demonstrating that they are in fact low-rank.\nWe make the following major contributions in this paper:\nWe define a novel modeling framework for collections of matrices using matrix network. We propose a new method to complete a stack of related matrices. We provide a mathematically solid exact recovery guarantee and numerically characterize the precise success regime. We give a convolutional imputation algorithm to efficiently complete large scale matrix networks."}, {"heading": "2. Related work", "text": "Low-rank matrix recovery is an important field of research. (25; 26) proposed and (36; 48) improved the soft-impute\nalgorithm as an iterative method to solve large-scale matrix completion problems. The soft-impute algorithm inspired our imputation algorithm. There was also a long line of works building theoretical tools to analyze the recovery guarantee for matrix completion (9; 7; 10; 8; 15; 22; 32; 31). Besides matrix completion, Gross analyzed the problem of efficiently recovering a low-rank matrix from a fraction of observations in any basis (24). These works enlightened our exact recovery analysis.\nLow-rank matrix recovery could be viewed as a \u201cnoncommutative analog\u201d of compressed sensing by replacing the sparse vector with a low-rank matrix. In compressed sensing, recovery of the sparse vector with a block diagonal sensing matrix was studied by the recent work (37), which demonstrated a phase transition that was different from the well-known phase transition for classical Gaussian/Fourier sensing matrices given by a series of works including (16; 3; 39). In our low-rank matrices recovery problem, our novel sampling scheme also corresponded to a block diagonal operator. We likewise demonstrated a new phase transition phenomenon.\nTensors could be considered as matrix networks when we ignored the network structure. Tensor completion coincides with the matrix network completion when the adjacent matrix is diagonal and the graph is just isolated points with no edge and the graph eigenbasis is the coordinate basis. Several works on tensor completion defined the nuclear norm for tensors as linear combinations of the nuclear norm of its unfoldings (21; 34; 19; 50; 49). Besides the common CP and Tucker decompositions of tensors, the recent work (51; 30; 35) defined the t-product using convolution operators between tensor fibers, which was close to the convolution of matrix network using the discrete Fourier transform matrix, and they applied the method in indoor localization. Departing from previous work, we considered a new sampling scheme in which some matrices were com-\npletely unobserved, and the undersampling ratio could be highly unbalanced for the other observed matrices. This sampling scheme was not considered before, yet it is very natural under the matrix network model. Under an incoherent eigenbasis, we can recover one completely unobserved measurement matrix because its information is well spread across all the spectral matrices with low-rank structure.\nNetworks is an important modelling framework for relations and interactions (29; 20; 52; 54; 53). Graph Laplacian based regularization has been used in semi-supervised learning in (18; 1; 2; 38) and in PCA (44) and low-rank matrix recovery (41; 23). In (41; 23; 44) a regularization term is used for a single matrix, where both the row vectors and column vectors of the matrix are assumed to be connected with graphs.\nThe notion of graph Fourier transform is rooted in spectral graph theory (11), it is the cornerstone of graph harmonic analysis(13; 12). The coherence of graph Fourier transform is studied in (40; 46), and the examples of large graphs with low-coherence (non-local) eigenvectors include different classes of random graphs(14; 17; 47), and non-random regular graph(5). Graph Fourier transform and graph convolution are widely used in data analysis and machine learning, for example, in (4; 55; 28; 43; 45). Recent advances in the field of convolutional neural networks by (6; 27) used this idea to extend neural networks from working on Euclidean grids to working on graphs."}, {"heading": "3. Mathematical definitions", "text": "Matrix network. First, consider a weighted graphG with N nodes and an adjacent matrix W \u2208 RN\u00d7N , where Wij is the weight on the edge between node i and j. In the following, we use J to represent the set of nodes on the graph.\nWe define a matrix network by augmenting this weighted graph G with a matrix-valued function A on the node set. The function A maps each node i in the graph to a matrix A(i) of sizem\u00d7n. We define a L2 norm \u2016\u00b7\u20162 on the matrix network by the squared sum of all entries in all matrices of the network. And we define the sum of nuclear norm as \u2016 \u00b7 \u2016\u2217,1, \u2016A\u2016\u2217,1 = \u2211N i=1 \u2016A(i)\u2016\u2217.\nGraph Fourier transform. The graph Fourier transform is an analog of the Discrete Fourier Transform. For a weighted undirected graph G and its adjacent matrix W , the normalized graph Laplacian is defined as L = I \u2212 D\u22121/2WD\u22121/2, where D is a diagonal matrix with entries Dii = \u2211 jWij . The graph Fourier transform matrix U is defined using UL = EU, where E is the diagonal matrix of the eigenvalues of L. Here, U is a unitary N \u00d7N matrix, and the eigenvectors of L are the row vectors of U . We rank\nthe eigenvalues in descending order and identify the k-th eigenvalue with its index k for simplicity.\nFor a matrix network A, we define its graph Fourier transform A\u0302 = UA, as a stack of N matrices in the spectral space of the graph. Each matrix is a linear combination of matrices on the graph, weighted by the graph Fourier basis. A\u0302(k) = \u2211 i\u2208J U(k, i)A(i).\nIntuitively, if we view the matrix networkA as a set ofm\u00d7n scalar functions on the graph, the graph Fourier transform on matrix network is applying the graph Fourier transform on each function individually. Using tensor notation, the element of A is A(i, a, b), and the graph Fourier transform U can be represented by a big block diagonal matrix U \u2297 I where each block is U of sizeN2, and there are (mn)2 such blocks.\nWe remark that the discrete Fourier transform is one special example of the graph Fourier transform. When the graph is a periodic grid, L is the discrete Laplacian matrix, and the eigenvectors are just the basis vectors for the discrete Fourier transform, which are sine and cosine functions with different frequencies. We define the graph Fourier coherence as \u03bd(U) = maxk,s |Uk,s|, following (40; 46). We know that \u03bd(U) \u2208 [ 1\u221a\nN , 1]. When \u03bd(U) is close to 1\u221a N , the eigenvec-\ntors are non-local, for example, the discrete Fourier transform case, different classes of random graphs(14; 17; 47), and non-random regular graph(5). When \u00b5(U) is close to 1, certain eigenvectors may be highly localized, especially when the graph has vertices whose degrees are significantly higher or lower than the average degree, say, in a star-like tree graph, or when the graph has many triangles, as discussed in (42). We will show in the following section that graphs with low coherence (close to 1\u221a\nN ) is preferred for\nthe imputation problem.\nConvolution of matrix networks. We can extend the definition of convolution to matrix networks. For two matrix networks X,Y on the same graph, we define their convolution as\n\u0302(X ? Y )(k) = X\u0302(k)Y\u0302 (k).\nThen X\u0302 ? Y is a stack of matrices where each matrix is the matrix multiplication of X\u0302(k) and Y\u0302 (k). Convolution on a graph is defined as multiplication in the spectral space by generalizing the convolution theorem since it is not clear how to define convolution in the original space."}, {"heading": "4. Completion problem with missing matrices", "text": "Imagine that we observe a few entries \u2126(i) of each matrix A(i). We define the sampling rates as pi = |\u2126(i)|/(mn). The projection operator P\u2126 is defined to project the full matrix network to our partial observation by only retaining entries in the set \u2126 = \u22c3 \u2126(i).\nThe sampling rate can vary from matrix to matrix. The main novel sampling scheme we include here is that a subset of matrices may be completely unobserved, namely pi = 0. This sampling scheme almost has not been discussed in depth in the literature. The difficulty lies in the fact that if a matrix is fully unobserved, there is no information at all from itself for the recovery, therefore we must leverage the information from other observed matrices.\nTo focus on understanding the essence of this difficulty, it is worth considering the extreme sampling scheme where each matrix is either fully observed or fully missing, which we call node undersampling.\nTo recover missing entries, we need structural assumptions about the matrix network A. We propose the assumption that A can be well-approximated by the convolution X ? Y of two matrix networks X,Y of size m \u00d7 r and r \u00d7 n, for some r much smaller than m and n. We will show that under this assumption, accurate completion is possible even if a significant fraction of the matrices are completely unobserved.\nWe formulate the completion problem as follows. Let A0 = X0 ? Y 0 be a matrix network of size m\u00d7 n, as the ground truth, whereX0 and Y 0 are matrices of sizem\u00d7r and r\u00d7n on the same network. After the graph Fourier transform, A\u03020(k) are rank r matrices. Our observations are A\u2126 = P\u2126(A) = P\u2126(A\n0 + W ), each entry of W is sampled i.i.d from N(0, \u03c32/n).\nWe first consider the noiseless setting where \u03c3 = 0. We can consider the following nuclear norm minimization problem, as a convex relaxation of rank minimization problem,\nminimize M\u0302 \u2016M\u0302\u2016\u2217,1, subject to A\u2126 = P\u2126(U\u2217M\u0302)\nAs an extension to include noise, we can consider the convex optimization problem in Lagrange form with regularization parameters \u03bbk,\nL\u03bb(M\u0302) = 1\n2 \u2016A\u2126 \u2212 P\u2126U\u2217M\u0302\u201622 + N\u2211 k=1 \u03bbk\u2016M\u0302(k)\u2016\u2217.\nWe can also consider the bi-convex formulation, which is to minimize the following objective function,\nL\u03bb(X,Y ) = \u2016A\u2126 \u2212 P\u2126(X ? Y )\u201622 + \u2211N k=1 \u03bbk(\u2016X\u0302(k)\u201622 + \u2016Y\u0302 (k)\u201622).\nThis formulation is non-convex but it is computationally efficient in large-scale applications.\nOne remark is that when we choose the regularization parameter \u03bbk to be Ek, the eigenvalues of the graph Laplacian\nL, and view X as a (nr)\u00d7N dimensional matrix,\nN\u2211 k=1 Ek\u2016X\u0302(k)\u201622 = Tr(X\u2217U\u2217EUX) = Tr(X\u2217LX),\nthen our regularizer is related to the graph Laplacian regularizer from (41; 23; 44)."}, {"heading": "5. Exact recovery guarantee", "text": "Let us now analyze the theoretical problem: what condition is needed for the non-uniform sampling \u2126 and for the rank of A\u0302 such that our algorithm is guaranteed to perform accurate recovery with high probability? We focus on the noiseless case, \u03c3 = 0, and for simplicity of results, we assume m = n.\nWe first prove that one sufficient condition is that average sampling rate p = 1N \u2211N i=1 pi = |\u2126|/(Nn2) is greater than O( rn log 2(nN)). It is worth pointing out that the condition is only about the average sampling rate, therefore it includes the interesting case that a subset of matrices is completely unobserved.\nAnalysis on exact recovery guarantee The matrix incoherence condition is a standard assumption for low-rank matrix recovery problems.\nLet the SVD of A\u0302(k) be A\u0302(k) = V1(k)E(k)V \u22172 (k). We define P1 and P2 as the direct sum of the projection matrix, P1(k) = V1(k)V1(k)\u2217, P2(k) = V2(k)V2(k)\u2217. We define the subspace T as the direct sum of the subspaces T (k), where T (k) is spanned by the column vectors of V1(k) and V2(k). Then we define the projection onto T as PT (M\u0302) = (V1V \u22171 M\u0302 + M\u0302V2V \u2217 2 \u2212 V1V \u22171 M\u0302V2V \u22172 ). We define its complement as PT\u22a5 = I \u2212 PT . We define sign(A\u0302(k)) = V1(k)V \u2217 2 (k) as the sign matrix of the singular values of A\u0302.\nIn matrix completion, for a r\u2212dimensional subspace of dimension n, spanned by V with an orthogonal projection PV , the coherence \u00b5(V ) is defined as\n\u00b5(V ) = n\nr max i \u2016PV ei\u20162.\nHere we introduce an averaged coherence\nDefinition 5.1 For the graph Fourier transform U\u2217, let the column vector of U\u2217 be uk. We now define the incoherence condition with coherence \u00b5 for the stack of spectral subspaces V1(k), V2(k) as\nmax{ \u2211N k=1 \u2016uk\u20162\u221e\u00b5(V1(k)), \u2211N k=1 \u2016uk\u20162\u221e\u00b5(V2(k))} = \u00b5.\nThe coherence of graph Fourier transform U\u2217 is defined as\n\u03bd(U\u2217) = maxk \u2016uk\u2016\u221e. We remark that\n\u00b5 \u2264 \u03bd(U\u2217)2 max{ \u2211N k=1 \u00b5(V1(k)), \u2211N k=1 \u00b5(V2(k))}\n\u2264 \u03bd(U\u2217)2N maxNk=1 max{\u00b5(V1(k)), \u00b5(V2(k))}\nIn the following, we show that the sampling rate threshold is proportional to \u00b5, which is upper bounded by \u03bd(U\u2217)2N maxNk=1 max{\u00b5(V1(k)), \u00b5(V2(k))}. This upper bound suggests that for the imputation would prefer low coherence graph such that \u03bd(U\u2217) is close to 1\u221a\nN .\nTheorem 1 We assume that A is a matrix network on a graph G, and its graph Fourier transform A\u0302(k) are a sequence of matrices, each of them is at most rank r, and A\u0302 satisfy the incoherence condition with coherence \u00b5. And we observe a matrix network A\u2126 on the graph G, for a subset of node in \u2126 random sampled from the network, node i on the network is sampled with probability pi, we define the average sampling rate p = 1N \u2211N i=1 pi = |\u2126|/(Nn2), and defineR = 1pP\u2126U \u2217.\nThen we prove that for any sampling probability distribution {pi}, as long as the average sampling rate p > C\u00b5 rn log\n2(Nn) for some constants C, the solution to the optimization problem\nminimize M\u0302 \u2016M\u0302\u2016\u2217,1, subject to A\u2126 = RM\u0302\nis unique and is exactly A\u0302 with probability 1 \u2212 (Nn)\u2212\u03b3 ,where \u03b3 = log(Nn)16 .\nProof sketch The proof of this theorem is given in the supplementary material. We sketch the steps of the proof here.\n\u2022 We will prove that for any nonzero M\u0302 6= A\u0302, we define \u2206 = M\u0302 \u2212 A\u0302, then we want to show either R\u2206 6= 0, or \u2016A\u0302+ \u2206\u2016\u2217,1 > \u2016A\u0302\u2016\u2217,1. We define the inner product: \u3008M\u03021, M\u03022\u3009 = \u2211 k\u3008M\u03021(k), M\u03022(k)\u3009, then\n\u2016A\u0302\u2016\u2217,1 = \u3008sign(A\u0302), A\u0302\u3009. We define a decomposition \u2206 = PT\u2206 + PT\u22a5\u2206 \u2206 = \u2206T + \u2206 \u22a5 T . For R\u2206 = 0, we show that\n\u2016A\u0302+ \u2206\u2016\u2217,1 \u2265 \u2016A\u0302\u2016\u2217,1 + \u3008sign(A\u0302) + sign(\u2206\u22a5T ),\u2206\u3009.\n\u2022 Now we want to estimate\ns\u2206 \u2206 = \u3008sign(A\u0302) + sign(\u2206\u22a5T ),\u2206\u3009.\nSince R\u2206 = 0, \u2206 \u2208 range(R)\u22a5. We want to construct a dual certificate K \u2208 range(R), such that for k = 3 + 12 log2(r) + log2(n) + log2(N), with probability 1\u2212 (Nn)\u2212\u03b3 ,\n\u2016PT (K)\u2212 sign(A\u0302)\u20162 \u2264 ( 12 ) k \u221a r,\n\u2016PT\u22a5(K)\u2016 \u2264 12 .\n\u2022 Given the existence of the dual certificate, we have\ns\u2206 = \u3008sign(A\u0302) + sign(\u2206\u22a5T )\u2212K,\u2206\u3009\nWe can break down s\u2206 as\n\u3008sign(\u2206\u22a5T )\u2212PT\u22a5(K),\u2206\u22a5T \u3009+\u3008sign(A\u0302)\u2212PT (K),\u2206T \u3009\nthen with probability 1\u2212 (Nn)\u2212\u03b3 , we get\ns\u2206 \u2265 \u2016\u2206\u22a5T \u2016\u2217,1 \u2212 1\n2 \u2016\u2206\u22a5T \u20162 \u2212 (\n1 2 )k \u221a r\u2016\u2206T \u20162.\n\u2022 We can show that for all \u2206 \u2208 range(R)\u22a5, with probability 1\u2212 (Nn)\u2212\u03b3 ,\n\u2016\u2206T \u20162 < 2nN\u2016\u2206\u22a5T \u20162.\nUsing this fact,\ns\u2206 \u2265 1\n2 \u2016\u2206\u22a5T \u20162 \u2212 (\n1 2 )k \u221a r2nN\u2016\u2206\u22a5T \u2016 \u2265 1 4 \u2016\u2206\u22a5T \u20162\nTherfore, when M\u0302 is a minimizer, we must have \u2206\u22a5T = 0, otherwise \u2016A\u0302 + \u2206\u2016\u2217,1 < \u2016A\u0302\u2016\u2217,1. Since \u2016\u2206T \u20162 is bounded by \u2016\u2206\u22a5T \u20162, we also have \u2206T = 0, then \u2206 = 0. Therefore, M\u0302 is the unique mininizer, and M\u0302 = A\u0302. This ends the proof.\n\u2022 Now we add remarks for some of the important techinical steps. The propositions with high probability guarantee rely on a concentration result. Since E(PTRPT ) = PT , we control the probability of deviation P[\u2016PT \u2212 PTRPT \u2016 > t] via operatorBernstein inequality( see theorem 6 of (24)), use the condition p = C\u00b5 rn log\n2(Nn), let t = 1/2, then with probability 1 \u2212 (nN)\u2212\u03b3 , where \u03b3 = log(Nn)16 , \u2016PT \u2212 PTRPT \u2016 < 1/2.\n\u2022 We construct a dual certificate via a method called \"golfing\", this technique was invented in (24). We construct the dual certificate K by the following construction: We decompose \u2126 as the union of k subset \u2126t, where each entry is sampled independently so that E(|\u2126t| = pt = 1 \u2212 (1 \u2212 p)1/k, and define Rt = 1ptP\u2126tU\n\u2217. Define H0 = sign(A\u0302),Kt =\u2211t j=1RjHj\u22121, Ht = sign(A\u0302) \u2212 PTKt. Then the dual certificate is defined as K = Kk. Using the operator-Bernstein concentration inequality, we can verify the two conditions:\nThe first condition: \u2016PT (K) \u2212 sign(A\u0302)\u20162 = \u2016Hk\u2016 \u2264 \u2016PT \u2212 PTRPT \u2016\u2016Ht\u22121\u20162 \u2264 12\u2016Ht\u22121\u20162 \u2264 ( 12 ) k\u2016 sign(A\u0302)\u2016 \u2264 ( 12 ) k \u221a r. The second condition, we can apply operatorBernstein inequality again for a sequence of tj = 1/(4 \u221a r), so that \u2016PT\u22a5RjHj\u22121\u2016 \u2264 ti\u2016Hj\u22121\u20162,\nand since \u2016Hj\u20162 \u2264 \u221a r2\u2212j , then \u2016PT\u22a5(K)\u2016 \u2264\u2211k\nj=1 ti\u2016Hj\u22121\u20162 \u2264 1 4 \u2211k j=1 2 \u2212(j\u22121) < 1/2."}, {"heading": "6. Convolutional imputation algorithm", "text": "Now we propose a convolutional imputation algorithm that effectively finds the minimizer of the optimization problem for a sequence of regularization parameters.\nIterative imputation algorithm. The vanilla version of our imputation algorithm iteratively performs imputation of Aimpute = P\u2126(A) + P\u22a5\u2126 (A\nest) and singular value softthreshold of A\u0302impute to solve the nuclear norm regularization problem. In the following, we denote singular value soft-threshold as S\u03bb(A\u0302) = V1(\u03a3 \u2212 \u03bbI)+V \u22172 ,where (\u00b7)+ is the projection operator on the semi-definite cone, and A\u0302 = V1\u03a3V \u2217 2 is the singular value decomposition.\nIterative Imputation: input P\u2126(A).\nInitialization Aest0 = 0, t = 0. for \u03bb1 > \u03bb2 > . . . > \u03bbC , where \u03bbj = (\u03bbjk), k = 1, . . . , N do\nrepeat Aimpute = P\u2126(A) + P \u22a5 \u2126 (A est t ).\nA\u0302impute = UAimpute. A\u0302estt+1(k) = S\u03bbjk (A\u0302impute(k)). Aestt+1 = U\u22121A\u0302estt+1. t=t+1.\nuntil \u2016Aestt \u2212Aestt\u22121\u20162/\u2016Aestt\u22121\u20162 < . Assign A\u03bbj = Aestt .\nend for output The sequence of solutions A\u03bb1 , . . . , A\u03bbC .\nIn the vanilla imputation algorithm, computing the full SVD on each iteration is very expensive for large matrices. For efficiency, we can use alternating ridge regression to compute reduced-rank SVD instead. Due to the limited space, we omit the detailed algorithm description here.\nRegularization path. The sequence of regularization parameters is chosen such that \u03bb1k > \u03bb 2 k > . . . > \u03bb C k for each k. The solution for each iteration with \u03bbs is a warm start for the next iteration with \u03bbs+1. Our recommended choice is to choose \u03bb1k as the largest singular value for A\u0302\nimpute(k), and decay \u03bbs at a constant speed \u03bbs+1 = c\u03bbs.\nConvergence. Our algorithm is a natural extension of softimpute (25), which is a special case of the proximal gradient algorithm for nuclear norm minimization, as demonstrated by (48), and the convergence of the algorithm is guaranteed.\nHere we show that the solution of our imputation algorithm converges asymptotically to a minimizer of the objective L\u03bb(M\u0302) in an elegant argument. We show that each step of our imputation algorithm is minimizing a surro-\ngate Q\u03bb(M\u0302 |M\u0302old) = \u2016A\u2126 + P\u22a5\u2126 U\u22121M\u0302old \u2212 U\u22121M\u0302\u20162 +\u2211N k=1 \u03bbk\u2016M\u0302(k)\u2016\u2217.\nTheorem 2 The imputation algorithm produces a sequence of iterates M\u0302 t\u03bb as the minimizer of the successive optimization objective\nM\u0302 t+1\u03bb = argmin Q\u03bb(M\u0302 |M\u0302 t \u03bb).\nThe sequence of iterates that converges to the minimizer M\u0302\u2217 of L\u03bb(M\u0302).\nWe put the proof of the convergence theorem in the appendix. The main idea of the proof is to show that\n\u2022 Q\u03bb decreases after every iteration.\n\u2022 M\u0302 t\u03bb is a Cauchy sequence.\n\u2022 The limit point is a stationary point of L\u03bb\nComputational complexity. Now we analyze the computational complexity of the imputation algorithm. The cost of the graph Fourier transform on matrix network is O(mnN2). When the graph is a periodic lattice, using fast Fourier transform(FFT), it is reduced to O(mnN logN). The cost of SVD is O(min(mn2,m2n)N) for computing singular value soft-threshold. Replacing SVD with alternating ridge regression reduces the complexity to O(r2nN). Therefore, the cost of each iteration is the sum of the cost of both parts, and the total cost would be that times total iteration steps."}, {"heading": "7. Experimental results", "text": "Numerical verification of the exact recovery To focus on the essential difficulty of the problem, we study the noiseless, node sampling setting: In each imputation experiment, we first generate a stack of low-rank matrices in the spectral space, A\u0302(k) = X0(k)TY 0(k) for i.i.d Gaussian random matrix X0(k), Y 0(k) \u2208 Rr\u00d7n. We also generate a random graph G. Then we compute the matrix network A by the inverse graph Fourier transform and obtain our observation by node undersampling of A. Then we send the observed matrices and the graph G to the imputation algorithm to get the solution M\u0302 . We measure the relative mean square error (rMSE) \u2016M\u0302 \u2212 A\u0302\u2016/\u2016A\u0302\u2016.\nWe set (n,N) = (50, 100) for all our experiments, and vary the undersampling ratio p and rank r. For each set of parameters (p, r), we repeat the experiment multiple times and compute the success rate of exact recovery. In figure 4 on the upper panel we show the rMSE when the graphs are one-dimensional chains of length N . When r/n is large and p is small, the rMSE is approximately equal to the undersampling ratio p, which means the optimization\nfailed to recover the ground truth matrices. On the opposite side, when r/n is small and p is large, the rMSE is very small, indicating we have successfully recovered the missing matrices. The transition between the two regions is very sharp. We also show the success rate on the lower panel of figure 4, which demonstrates a phase transition.\nFeature matrices on Facebook network We take the ego networks from the SNAP Facebook dataset (33). The combined network forms a connected graph with 4039 nodes and 88234 edges. All the edges have equal weights. The feature matrices on each of the nodes were generated by randomly generating X(k), Y (k) \u2208 C1\u00d750 in the spectral domain, and doing the inverse graph Fourier transform to get A = U\u22121(X(k)Y (k)). The observation is generated by sampling Nobs matrices at sampling rate p, and adding i.i.d. Gaussian noise with mean 0 and variance \u03c32/50 to all observed entries. Here Nobs < N = 4039 and the other matrices are completely unobserved.\nWe run our iterative imputation algorithm to recover A from this observation with varying parameters Nobs, p, and \u03c3, and calculate the MSE between our estimation and the ground truth. The results are summarized in Table 1. When there is no additive noise, we can recover all the matrices very well even with only 20% of entries observed across the matrix network. It works well both when doing node undersampling and more uniform undersampling. When there is additive noise, the MSE between reconstruction and the ground truth will grow proportionally.\nMRI completion We use a cardiac MRI scan dataset for the completion task. The stack of MRI images scans through a human torso. The frames are corrupted, several frames are missing, and the other frames are sampled i.i.d. from a Bernoulli distribution with p = 0.2. Our completion result is demonstrated in figure 1 at first page as the motivating example. In the 88 frames there are 2 frames missing, and we only sampled 20% of the rest of frames i.i.d. from a Bernoulli distribution with p = 0.2. We compare with the baseline method where we solve a tensor completion problem using nuclear norm minimization. Relative MSE for all frames are plotted in figure 5. The baseline method failed at missed frames and significantly under-performed the convolutional imputation method.\nSPECT completion We imputed a cardiac SPECT scan dataset. The SPECT scan captures the periodic movement of a heart, and we have a temporal sequence at a fixed spatial slice. The sequence has 36 frames, capturing 4 periods of heart beats. 4 consecutive frames out of the 36 frames are missing and the other frames are sampled i.i.d. from a Bernoulli distribution with p = 0.2. We try to recover the whole image stack from the observations and compare our method with two baseline methods. The first baseline method assumes each individual frame is low-rank and minimizes the sum of nuclear norms. The second baseline method adds the graph regularizer from (41; 23; 44) , in addition to the low-rank assumption on each frame. Minimizing the sum of nuclear norm fails to recover completely missing frames. Our algorithm performs better than tensor completion with graph regularizer on the SPECT scan, since in spectral domain we can use the periodicity to help aggregate information, while using graph regularizer only propagates information between neighbors. This is demon-\nstrated in figure 6. The first row shows the ground truth, and the second row overlays the ground truth (in red channel) with the completion result using our convolutional imputation algorithm (in green channel). The third row overlays the ground truth with the completion result using tensor completion with graph regularizer. The completion result with our algorithm matches the ground truth very well, while the completion result with tensor completion using graph\nregularizer is biased towards the average of neighboring frames, showing red and green rings on the edges.A quantitative comparison on the SPECT scan completion is given in figure 7. Our imputation algorithm\u2019s relative MSE between reconstruction and the ground truth is significantly smaller than the baselines\u2019. It is worth pointing out that our method\u2019s recovery performance at the missing frames are comparable to that at the partially observed frames, while the first baseline completely fails at the missing frames and the second baseline performs significantly worse."}, {"heading": "8. Discussion", "text": "In practice, when you are given a tensor or a stack of matrices, there are two ways to formulate it into a matrix network.\nOne is to use the knowledge of physical or geometrical relation to naturally determine the graph. The graph of the matrix network is given in the facebook network and the graph is naturally constructed as a 1-d equal-weighted chain in the MRI and SPECT datasets, based on the nature of the datasets.\nThe other is to construct the graph using an explicit constructive methods. Finding a graph with good graph Fourier transform relies on problem structure and domain knowledge. One suggested universal way is to construct a lattice or a d\u2212regular graph, then assign the weight on each edge as some distance metric of two matrices, for example, the distance metric could be computed using Gaussian kernels. We suggest that the coherence \u00b5 we defined before could be used as a criterion to measure how good the graph Fourier transform is. From the bound on \u00b5 by the coherence of the graph Fourier transform and the maximum coherence over all spectral matrices, we know that we want to search for graph with low coherence. This leads to interesting dictionary learning problem where we want to learn a unitary dictionary as the graph Fourier transform.\nTo conclude, treating a series of matrices with relations as a matrix network is a useful modeling framework since a matrix network has operations like the graph Fourier transform and convolution. This framework allows us to complete the matrices when some of them are completely unobserved, using the spectral low-rank structural assumption. We provided an exact recovery guarantee and discovered a new phase transition phenomenon for the completion algorithm."}], "year": 2018, "references": [{"title": "Learning on graph with laplacian regularization", "authors": ["Rie Kubota Ando", "Tong Zhang"], "venue": "Advances in neural information processing systems,", "year": 2007}, {"title": "Semi-supervised learning with regularized lapla-  Convolutional Imputation of Matrix Networks cian", "authors": ["Konstantin Avrachenkov", "Pavel Chebotarev", "Alexey Mishenin"], "venue": "Optimization Methods and Software,", "year": 2017}, {"title": "Universality in polytope phase transitions and iterative algorithms", "authors": ["Mohsen Bayati", "Marc Lelarge", "Andrea Montanari"], "venue": "Information Theory Proceedings (ISIT),", "year": 2012}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "authors": ["Mikhail Belkin", "Partha Niyogi"], "venue": "In Advances in neural information processing systems,", "year": 2002}, {"title": "Non-localization of eigenfunctions on large regular graphs. Israel", "authors": ["Shimon Brooks", "Elon Lindenstrauss"], "venue": "Journal of Mathematics,", "year": 2013}, {"title": "Spectral networks and locally connected networks on graphs", "authors": ["Joan Bruna", "Wojciech Zaremba", "Arthur Szlam", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.6203,", "year": 2013}, {"title": "A singular value thresholding algorithm for matrix completion", "authors": ["Jian-Feng Cai", "Emmanuel Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization,", "year": 1956}, {"title": "Matrix completion with noise", "authors": ["Emmanuel Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE,", "year": 2010}, {"title": "Exact matrix completion via convex optimization", "authors": ["Emmanuel Cand\u00e8s", "Benjamin Recht"], "venue": "Communications of the ACM,", "year": 2012}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "authors": ["Emmanuel Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Spectral graph theory", "authors": ["Fan RK Chung"], "venue": "Number 92. American Mathematical Soc.,", "year": 1997}, {"title": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps", "authors": ["Ronald R Coifman", "Stephane Lafon", "Ann B Lee", "Mauro Maggioni", "Boaz Nadler", "Frederick Warner", "Steven W Zucker"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "year": 2005}, {"title": "Diffusion wavelets", "authors": ["Ronald R Coifman", "Mauro Maggioni"], "venue": "Applied and Computational Harmonic Analysis,", "year": 2006}, {"title": "Eigenvectors of random graphs: Nodal domains", "authors": ["Yael Dekel", "James R Lee", "Nathan Linial"], "venue": "Random Structures & Algorithms,", "year": 2011}, {"title": "Minimax risk of matrix denoising by singular value thresholding", "authors": ["David Donoho", "Matan Gavish"], "venue": "The Annals of Statistics,", "year": 2014}, {"title": "Counting faces of randomly projected polytopes when the projection radically lowers dimension", "authors": ["David Donoho", "Jared Tanner"], "venue": "Journal of the American Mathematical Society,", "year": 2009}, {"title": "Sparse regular random graphs: spectral density and eigenvectors", "authors": ["Ioana Dumitriu", "Soumik Pal"], "venue": "The Annals of Probability,", "year": 2012}, {"title": "Asymptotic behavior of lp-based laplacian regularization in semi-supervised learning", "authors": ["Ahmed El Alaoui", "Xiang Cheng", "Aaditya Ramdas", "Martin J Wainwright", "Michael I Jordan"], "venue": "29th Annual Conference on Learning Theory,", "year": 2016}, {"title": "Tucker factorization with missing data with application to low-rank tensor completion", "authors": ["Marko Filipovi\u0107", "Ante Juki\u0107"], "venue": "Multidimensional systems and signal processing,", "year": 2015}, {"title": "Finding cliques in social networks: A new distribution-free model", "authors": ["Jacob Fox", "Tim Roughgarden", "C Seshadhri", "Fan Wei", "Nicole Wein"], "venue": "arXiv preprint arXiv:1804.07431,", "year": 2018}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "authors": ["Silvia Gandy", "Benjamin Recht", "Isao Yamada"], "venue": "Inverse Problems,", "year": 2011}, {"title": "The optimal hard threshold for singular values", "authors": ["Matan Gavish", "David L Donoho"], "venue": "IEEE Transactions on Information Theory,", "year": 2014}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "authors": ["David Gross"], "venue": "IEEE Transactions on Information Theory,", "year": 2011}, {"title": "softimpute: Matrix completion via iterative soft-thresholded svd", "authors": ["T Hastie", "R Mazumder"], "venue": "R package version,", "year": 2015}, {"title": "Matrix completion and low-rank svd via fast alternating least squares", "authors": ["Trevor Hastie", "Rahul Mazumder", "Jason D Lee", "Reza Zadeh"], "venue": "J. Mach. Learn. Res,", "year": 2015}, {"title": "Deep convolutional networks on graph-structured data", "authors": ["Mikael Henaff", "Joan Bruna", "Yann LeCun"], "venue": "arXiv preprint arXiv:1506.05163,", "year": 2015}, {"title": "Multiresolution graph fourier transform for compression of piecewise smooth images", "authors": ["Wei Hu", "Gene Cheung", "Antonio Ortega", "Oscar C Au"], "venue": "IEEE Transactions on Image Processing,", "year": 2015}, {"title": "Social and Economic Networks", "authors": ["M.O. Jackson"], "year": 2008}, {"title": "Tensor\u2013 tensor products with invertible linear transforms", "authors": ["Eric Kernfeld", "Misha Kilmer", "Shuchin Aeron"], "venue": "Linear Algebra and its Applications,", "year": 2015}, {"title": "Matrix completion from a few entries", "authors": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Matrix completion from noisy entries", "authors": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Journal of Machine Learning Research,", "year": 2010}, {"title": "Learning to discover social circles in ego networks. Advances in neural information processing systems, pages 539\u2013547", "authors": ["Jure Leskovec", "Julian J Mcauley"], "year": 2012}, {"title": "Tensor completion for estimating missing values in visual data", "authors": ["Ji Liu", "Przemyslaw Musialski", "Peter Wonka", "Jieping Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2013}, {"title": "Adaptive sampling of rf fingerprints for fine-grained indoor localization", "authors": ["Xiao-Yang Liu", "Shuchin Aeron", "Vaneet Aggarwal", "Xiaodong Wang", "Min-You Wu"], "venue": "IEEE Transactions on Mobile Computing,", "year": 2016}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "authors": ["Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani"], "venue": "Journal of machine learning research,", "year": 2010}, {"title": "Sparsity/undersampling tradeoffs in anisotropic undersampling, with applications in mr imaging/spectroscopy", "authors": ["Hatef Monajemi", "David Donoho"], "venue": "arXiv preprint arXiv:1702.03062,", "year": 2017}, {"title": "Semisupervised learning with the graph laplacian: The limit of infinite unlabelled data", "authors": ["Boaz Nadler", "Nathan Srebro", "Xueyuan Zhou"], "venue": "Advances in neural information processing systems,", "year": 2009}, {"title": "The proportional mean decomposition: a bridge between the gaussian and bernoulli ensembles", "authors": ["Samet Oymak", "Babak Hassibi"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "year": 2015}, {"title": "Global and local uncertainty principles for signals on graphs", "authors": ["Nathanael Perraudin", "Benjamin Ricaud", "David Shuman", "Pierre Vandergheynst"], "venue": "arXiv preprint arXiv:1603.03030,", "year": 2016}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "authors": ["Nikhil Rao", "Hsiang-Fu Yu", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "Advances in neural information processing systems,", "year": 2015}, {"title": "On the phase transition phenomenon of graph laplacian eigenfunctions on trees (recent development and scientific applications in wavelet analysis)", "authors": ["Naoki Saito", "Ernest Woei"], "year": 2011}, {"title": "Discrete signal processing on graphs: Graph fourier transform", "authors": ["Aliaksei Sandryhaila", "Jos\u00e9 MF Moura"], "year": 2013}, {"title": "Compressive pca for low-rank matrices on graphs", "authors": ["Nauman Shahid", "Nathanael Perraudin", "Gilles Puy", "Pierre Vandergheynst"], "venue": "IEEE transactions on Signal and Information Processing over Networks,", "year": 2016}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "authors": ["David I Shuman", "Sunil K Narang", "Pascal Frossard", "Antonio Ortega", "Pierre Vandergheynst"], "venue": "IEEE Signal Processing Magazine,", "year": 2013}, {"title": "Sparse random graphs: Eigenvalues and eigenvectors", "authors": ["Linh V Tran", "Van H Vu", "Ke Wang"], "venue": "Random Structures & Algorithms,", "year": 2013}, {"title": "Accelerated inexact soft-impute for fast large-scale matrix completion", "authors": ["Quanming Yao", "James T Kwok"], "venue": "Twenty- Fourth International Joint Conference on Artificial Intelligence,", "year": 2015}, {"title": "Simultaneous rectification and alignment via robust recovery of low-rank tensors", "authors": ["Xiaoqin Zhang", "Di Wang", "Zhengyuan Zhou", "Yi Ma"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Hybrid singular value thresholding for tensor completion", "authors": ["Xiaoqin Zhang", "Zhengyuan Zhou", "Di Wang", "Yi Ma"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence,", "year": 2014}, {"title": "Exact tensor completion using t-svd", "authors": ["Zemin Zhang", "Shuchin Aeron"], "venue": "IEEE Transactions on Signal Processing,", "year": 2016}, {"title": "Dynamics on linear influence network games under stochastic environments", "authors": ["Zhengyuan Zhou", "Nicholas Bambos", "Peter Glynn"], "venue": "In International Conference on Decision and Game Theory for Security,", "year": 2016}, {"title": "Deterministic and stochastic wireless networks games: Equilibrium, dynamics and price of anarchy", "authors": ["Zhengyuan Zhou", "Nicholas Bambos", "Peter Glynn"], "venue": "Operations Research,", "year": 2018}, {"title": "A game-theoretical formulation of influence networks", "authors": ["Zhengyuan Zhou", "Benjamin Yolken", "Reiko Ann Miura-Ko", "Nicholas Bambos"], "venue": "In American Control Conference (ACC),", "year": 2016}, {"title": "Approximating signals supported on graphs", "authors": ["Xiaofan Zhu", "Michael Rabbat"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "year": 2012}], "id": "SP:c6ffee0cfd2b107b9ac954bd716d9b8b4ed8fbfd", "authors": [{"name": "Qingyun Sun", "affiliations": []}, {"name": "Mengyuan Yan", "affiliations": []}, {"name": "David Donoho", "affiliations": []}, {"name": "Stephen Boyd", "affiliations": []}], "abstractText": "A matrix network is a family of matrices, with relatedness modeled by a weighted graph. We consider the task of completing a partially observed matrix network. We assume a novel sampling scheme where a fraction of matrices might be completely unobserved. How can we recover the entire matrix network from incomplete observations? This mathematical problem arises in many applications including medical imaging and social networks. To recover the matrix network, we propose a structural assumption that the matrices have a graph Fourier transform which is low-rank. We formulate a convex optimization problem and prove an exact recovery guarantee for the optimization problem. Furthermore, we numerically characterize the exact recovery regime for varying rank and sampling rate and discover a new phase transition phenomenon. Then we give an iterative imputation algorithm to efficiently solve the optimization problem and complete large scale matrix networks. We demonstrate the algorithm with a variety of applications such as MRI and Facebook user network.", "title": "Convolutional Imputation of Matrix Networks"}