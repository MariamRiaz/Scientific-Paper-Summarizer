2	31	Technically speaking, as defined by the ACE 2005 dataset1, a benchmark for event extraction (Grishman et al., 2005), the event extraction task can be divided into two subtasks, i.e., event detection (identifying and classifying event triggers) and argument extraction (identifying arguments of event triggers and labeling their roles).
3	32	In event extraction, it is a common phenomenon that multiple events exist in the same sentence.
4	42	Extracting the correct multiple events from those ∗ ldc2006t06 sentences is much more difficult than in the oneevent-one-sentence cases because those various types of events are often associated with each other.
5	34	For example, in the sentence “He left the company, and planned to go home directly.”, the trigger word left may trigger a Transport (a person left a place) event or an End-Position (a person retired from a company) event.
10	30	Most of them exploiting various features (Liu et al., 2016b; Yang and Mitchell, 2016; Li et al., 2013; Keith et al., 2017; Liu et al., 2016a; Li et al., 2015), introducing memory vectors and matrices (Nguyen et al., 2016), introducing more transition arcs (Sha et al., 2018), keeping more contextual information (Chen et al., 2015) into sentence-level sequential modeling methods like RNNs and CRFs.
11	21	Some also seek features in document-level methods (Liao and Grishman, 2010; Ji and Grishman, 2008).
12	37	However, sentencelevel sequential modeling methods suffer a lot from the low efficiency in capturing very longrange dependencies while the feature-based methods require extensive human engineering, which also largely affects model performance.
14	27	An intuitive way to alleviate this phenomenon is to introduce shortcut arcs represented by linguistic resources like dependency parsing trees to drain the information flow from a point to its target through fewer transitions.
15	11	Comparing to sequential order, modeling with these arcs often successfully reduce the needed hops from one event trigger to another in the same sentences.
16	24	In Figure 1, for example, there are two events: a Die event triggered by the word killed with four arguments in red and an Attack event triggered by the word barrage with three arguments in blue.
17	34	We need six hops from killed to barrage according to sequential order, but only three hops according to the arcs in dependency parsing tree (along the nmodarc from killed to witnesses, along the acl-arc from witnesses to called, and along the xcomp-arc from called to barrage).
21	10	And then we utilize the syntactic contextual representations to extract triggers and arguments jointly by a self-attention mechanism to aggregate information especially keeping the associations between multiple events.
30	40	Firstly, in our in- • The positional embedding vector of wi: If wc is the current word, we encode the rela- tive distance i − c from wi to wc as a realvalued vector by looking up the randomly initialized position embedding table (Nguyen et al., 2016; Liu et al., 2017; Nguyen and Grishman, 2018).
31	24	• The entity type label embedding vector of wi: Similarly to the POS-tagging label em- bedding vector of wi, we annotate the entity mentions in a sentence using BIO annotation schema and transform the entity type labels to real-valued vectors by looking up the embedding table.
35	41	In V , each vi is the node representing token wi in W .
39	19	For example, in the dependency parsing tree shown in Figure 1, there are four arcs in the subgraph with only two nodes “killed” and “witnesses”: the dependency arc with the type label K(“killed”, “witnesses”) = nmod, the revresed dependency arc with the additional type label K(“witnesses”, “killed”) = nmod′, and the two self-loops of “killed” and “witnesses” with type label K(“killed”, “killed”) = K(“witnesses”, “witnesses”) = loop.
48	13	As not all types of edges are equally informative for the downstream task, moreover, there are also noises in the generated syntactic parsing structures; we apply gates on the edges to weight their individual importances.
50	18	With this additional gating mechanism, the final syntactic GCN computation is formulated as h(k+1)v = f( ∑ u∈N (v) g(k)u,v(W (k) K(u,v)h (k) u + b (k) K(u,v))) (4) As stacking k layers of GCNs can model information in k hops, and sometimes the length of shortcut path between two triggers is less than k, to avoid information over-propagating, we adapt highway units (Srivastava et al., 2015), which allow unimpeded information flowing across stacking GCN layers.
61	10	Besides, predicting a trigger label for a token should take other possible trigger candidates into consideration.
62	19	To capture the associations between triggers in a sentence, we design a self-attention mechanism to aggregate information especially keeping the associations between multiple events.
66	62	For each entity-trigger pair, as both the entity and the trigger candidate are likely to be a subsequence of tokens, we aggregate the context vectors of subsequences to trigger candidate vector Ti and entity vector Ej by average pooling along the sequence length dimension.
67	10	Then we concatenate them together and feed into a fully-connected network to predict the argument role as: yaij = softmax(Wa[Ti, Ej ] + ba) (13) where yaij is the final output of which role the jth entity plays in the event triggered by the i-th trigger candidate.
71	28	Due to the data sparsity in the ACE 2005 dataset, we adapt our joint negative log-likelihood loss func- tion by adding a bias item as: J(θ) = − N ∑ p=1 ( np ∑ i=1 I(yti)log(p(yti |θ)) +β tp ∑ i=1 ep ∑ j=1 log(p(yaij |θ))) (14) where N is the number of sentences in training corpus; np, tp and ep are the number of tokens, extracted trigger candidates and entities of the p-th sentence; I(yti) is an indicating function, if yti is notO, it outputs a fixed positive floating number α bigger than one, otherwise one; β is also a floating number as a hyper-parameter like α.
73	15	The ACE 2005 dataset annotate 33 event subtypes and 36 role classes, along with the NONE class and BIO annotation schema, we will classify each token into 67 categories in event detection and 37 categories in argument extraction.
74	11	To comply with previous work, we use the same data split as the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018).
77	229	Also, we follow the criteria of the previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2016b; Yang and Mitchell, 2016; Nguyen et al., 2016; Sha et al., 2018) to judge the correctness of the predicted event mentions.
78	15	Hyperparameter Setting For all the experiments below, in the word representation module, we use 300 dimensions for the embeddings and 50 dimensions for the rest three embeddings including pos-tagging embedding, positional embedding and entity type embedding.
89	19	To evaluate the effect of our framework for alleviating the multiple events phenomenon, we divide the test data into two parts (1/1 and 1/N) following Nguyen et al. (2016); Chen et al. (2015) and perform evaluations separately.
90	12	1/1 means that one sentence only has one trigger or one argument plays a role in one sentence; otherwise, 1/N is used.
91	26	Table 2 illustrates the performance (F1 scores) of JRNN (Nguyen et al., 2016), DMCNN (Chen et al., 2015), the two baseline model Embedding+T and CNN in Chen et al. (2015) and our framework in trigger classification subtask and argument role labeling subatsk.
