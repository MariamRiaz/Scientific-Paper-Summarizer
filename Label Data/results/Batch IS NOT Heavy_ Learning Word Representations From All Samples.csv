0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Neural networks have proven to be highly effective at many tasks in natural language.,1 Introduction,[0],[0]
"For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).",1 Introduction,[0],[0]
"However, neural networks can be complicated to design and train well.",1 Introduction,[0],[0]
"Many decisions need to be made, and performance can be highly dependent on making them correctly.",1 Introduction,[0],[0]
"Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.
",1 Introduction,[0],[0]
"In this paper, we focus on the choice of the sizes of hidden layers.",1 Introduction,[0],[0]
"We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that
they can be removed from the network.",1 Introduction,[0],[0]
"Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.
",1 Introduction,[0],[0]
"Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.",1 Introduction,[0],[0]
"The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes.",1 Introduction,[0],[0]
"In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.",1 Introduction,[0],[0]
The result is that fewer experiments are needed to obtain models that perform well and are correctly sized.,1 Introduction,[0],[0]
Language models are often used in natural language processing tasks involving generation of text.,2 Background,[0],[0]
"For instance, in machine translation, the language model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances.
",2 Background,[0],[0]
"Current language models are usually n-gram models, which look at the previous (n− 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data.",2 Background,[0],[0]
"These models are simple but very effective in improving the performance of natural language systems.
",2 Background,[0],[0]
"However, n-gram models suffer from some limitations, such as data sparsity and memory usage.",2 Background,[0],[0]
"As an alternative, researchers have begun exploring the use of neural networks for language modeling.",2 Background,[0],[0]
"For modeling n-grams, the most common approach is the feedforward network of Bengio et
908
al. (2003), shown in Figure 1.",2 Background,[0],[0]
"Each node represents a unit or “neuron,” which has a real valued activation.",2 Background,[0],[0]
The units are organized into real-vector valued layers.,2 Background,[0],[0]
The activations at each layer are computed as follows.,2 Background,[0],[0]
(We assume n = 3; the generalization is easy.),2 Background,[0],[0]
"The two preceding words, w1, w2, are mapped into lowerdimensional word embeddings,
x1",2 Background,[0],[0]
= A:w1 x2 =,2 Background,[0],[0]
"A:w2
then passed through two hidden layers,
y = f(B1x1 +",2 Background,[0],[0]
B2x2 + b) z,2 Background,[0],[0]
"= f(Cy + c)
where f is an elementwise nonlinear activation (or transfer) function.",2 Background,[0],[0]
"Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few.",2 Background,[0],[0]
"Finally, the result is mapped via a softmax to an output probability distribution,
P (wn | w1 · · ·wn−1) ∝",2 Background,[0],[0]
exp([Dz + d]wn).,2 Background,[0],[0]
"The parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants.
",2 Background,[0],[0]
"Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.",2 Background,[0],[0]
"In this paper, we use and extend their implementation.",2 Background,[0],[0]
Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network.,3 Methods,[0],[0]
"The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units.",3 Methods,[0],[0]
"Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting.",3 Methods,[0],[0]
"It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder.
",3 Methods,[0],[0]
Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible.,3 Methods,[0],[0]
"The goal is to end up with a trained network
that also has the optimal number of units in each layer.
",3 Methods,[0],[0]
We do this by adding a regularizer to the objective function.,3 Methods,[0],[0]
"For simplicity, consider a single layer without bias, y = f(Wx).",3 Methods,[0],[0]
Let L(W) be the negative log-likelihood of the model.,3 Methods,[0],[0]
"Instead of minimizing L(W) alone, we want to minimize L(W)",3 Methods,[0],[0]
"+ λR(W), where R(W) is a convex regularizer.",3 Methods,[0],[0]
"The `1 norm, R(W) = ‖W‖1 =∑
i,j |Wij |, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size.",3 Methods,[0],[0]
"However, we are interested not only in reducing the number of parameters but the number of units.",3 Methods,[0],[0]
"To do this, we need a different regularizer.
",3 Methods,[0],[0]
"We assume activation functions that satisfy f(0) = 0, such as the hyperbolic tangent or rectified linear unit (f(x) = max{0, x}).",3 Methods,[0],[0]
"Then, if we push the incoming weights of a unit yi to zero, that is, Wij = 0 for all j (as well as the bias, if any: bi = 0), then yi = f(0) = 0 is independent of the previous layers and contributes nothing to subsequent layers.",3 Methods,[0],[0]
So the unit can be removed without affecting the network at all.,3 Methods,[0],[0]
"Therefore, we need a regularizer that pushes all the incoming connection weights to a unit together towards zero.
",3 Methods,[0],[0]
"Here, we experiment with two, the `2,1 norm and the `∞,1 norm.1 The `2,1 norm on a ma-
1In the notation `p,q , the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms.",3 Methods,[0],[0]
"Contrary to more common usage, in this paper, the groups are rows, not columns.
",3 Methods,[0],[0]
"trix W is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖2 = ∑ i ∑ j W 2ij  12 .,3 Methods,[0],[0]
"(1) (If there are biases bi, they should be included as well.)",3 Methods,[0],[0]
"This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero.",3 Methods,[0],[0]
"The `∞,1 norm is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖∞ = ∑ i max j |Wij |.,3 Methods,[0],[0]
"(2)
Again, this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s).
",3 Methods,[0],[0]
Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row.,3 Methods,[0],[0]
Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero.,3 Methods,[0],[0]
"However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply.",4 Optimization,[0],[0]
"The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.",4 Optimization,[0],[0]
"Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).",4.1 Proximal gradient method,[0],[0]
"Our objective function can be split into two parts, a convex and differentiable part (L) and a
convex but non-differentiable part (λR).",4.1 Proximal gradient method,[0],[0]
"In proximal gradient descent, we alternate between improving L alone and λR alone.",4.1 Proximal gradient method,[0],[0]
Let u be the parameter values from the previous iteration.,4.1 Proximal gradient method,[0],[0]
"We compute new parameter values w using:
v← u− η∇L(u) (3)
w← arg max w",4.1 Proximal gradient method,[0],[0]
( 1 2η ‖w − v‖2 + λR(w) ),4.1 Proximal gradient method,[0],[0]
"(4)
and repeat until convergence.",4.1 Proximal gradient method,[0],[0]
The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution.,4.1 Proximal gradient method,[0],[0]
"In the rest of this section, we provide some justification for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the `2 and `∞ norms.
",4.1 Proximal gradient method,[0],[0]
We can think of the gradient descent update (3) on L as follows.,4.1 Proximal gradient method,[0],[0]
"Approximate L around u by the tangent plane,
L̄(v) = L(u) +∇L(u)(v − u) (5)
and move v to minimize L̄, but don’t move it too far from u; that is, minimize
F (v) = 1",4.1 Proximal gradient method,[0],[0]
2η ‖v,4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(v).
",4.1 Proximal gradient method,[0],[0]
"Setting partial derivatives to zero, we get
∂F ∂v = 1 η
(v − u) +∇L(u) = 0 v = u− η∇L(u).
",4.1 Proximal gradient method,[0],[0]
"By a similar strategy, we can derive the second step (4).",4.1 Proximal gradient method,[0],[0]
"Again we want to move w to minimize the objective function, but don’t want to move it too far from u; that is, we want to minimize:
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(w) + λR(w).
",4.1 Proximal gradient method,[0],[0]
Note that we have not approximated R by a tangent plane.,4.1 Proximal gradient method,[0],[0]
We can simplify this by substituting in (3).,4.1 Proximal gradient method,[0],[0]
"The first term becomes
1 2η ‖w",4.1 Proximal gradient method,[0],[0]
− u‖2 = 1 2η ‖w,4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)‖2
= 1 2η ‖w − v‖2 −∇L(u)(w − v)
+ η
2 ‖∇L(u)‖2
and the second term becomes
L̄(w) = L(u) +∇L(u)(w − u) = L(u) +∇L(u)(w",4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)).
",4.1 Proximal gradient method,[0],[0]
"The ∇L(u)(w − v) terms cancel out, and we can ignore terms not involving w, giving
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− v‖2 + λR(w) + const.
which is minimized by the update (4).",4.1 Proximal gradient method,[0],[0]
"Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for λR (4).",4.1 Proximal gradient method,[0],[0]
The latter can often be done exactly (without approximating R by a tangent plane).,4.1 Proximal gradient method,[0],[0]
"We show next how to do this for the `2 and `∞ norms.
4.2 `2 and `2,1 regularization Since the `2,1 norm on matrices (1) is separable into the `2 norm of each row, we can treat each row separately.",4.1 Proximal gradient method,[0],[0]
"Thus, for simplicity, assume that we have a single row and want to minimize
G(w) = 1 2η ‖w − v‖2 +",4.1 Proximal gradient method,[0],[0]
"λ‖w‖+ const.
",4.1 Proximal gradient method,[0],[0]
"The minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3):
∂G ∂w = 1 η (w − v) + λ w‖w‖",4.1 Proximal gradient method,[0],[0]
"= 0.
",4.1 Proximal gradient method,[0],[0]
"Clearly, w and v must have the same direction and differ only in magnitude, that is, w = α v‖v‖ .",4.1 Proximal gradient method,[0],[0]
"Substituting this into the above equation, we get the solution
α = ‖v‖",4.1 Proximal gradient method,[0],[0]
− ηλ.,4.1 Proximal gradient method,[0],[0]
"Therefore the update is
w = α v ‖v‖
α = max(0, ‖v‖ − ηλ).",4.1 Proximal gradient method,[0],[0]
"As above, since the `∞,1 norm on matrices (2) is separable into the `∞ norm of each row, we can treat each row separately; thus, we want to minimize
G(w) = 1 2η ‖w − v‖2 + λmax j |xj |+ const.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the solution can be characterized as: Decrease all of the maximal |xj | until the total decrease reaches ηλ or all the xj are zero.","4.3 `∞ and `∞,1 regularization",[0],[0]
"See Figure 4.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"If we pre-sort the |xj | in nonincreasing order, it’s easy to see how to compute this: for ρ = 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n, see if there is a value ξ ≤ xρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"such that decreasing all the x1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", xρ to ξ amounts to a total decrease of ηλ.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The largest ρ for which this is possible gives the correct solution.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"But this situation seems similar to another optimization problem, projection onto the `1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.","4.3 `∞ and `∞,1 regularization",[0],[0]
"In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the `1 projection of v is exactly what is cut out by the `∞ proximal operator, and vice versa (Figure 4).
","4.3 `∞ and `∞,1 regularization",[0],[0]
Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1.,"4.3 `∞ and `∞,1 regularization",[0],[0]
It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"If so, it recursively searches the right side; if not, the
left side.","4.3 `∞ and `∞,1 regularization",[0],[0]
"At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) – the only difference from Duchi et al.’s algorithm.
","4.3 `∞ and `∞,1 regularization",[0],[0]
This algorithm is asymptotically faster than that of Quattoni et al. (2009).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"They reformulate `∞,1 regularization as a constrained optimization problem (in which the `∞,1 norm is bounded by µ) and provide a solution inO(n log n) time.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The method shown here is simpler and faster because it can work on each row separately.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Algorithm 1 Linear-time algorithm for the proximal operator of the `∞ norm.
1: procedure UPDATE(w, δ) 2: lo, hi← 1, n 3: s← 0 4:","4.3 `∞ and `∞,1 regularization",[0],[0]
"while lo ≤ hi do 5: select md randomly from lo, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi 6: ρ← PARTITION(w, lo,md, hi) 7: ξ ← 1ρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"( s+ ∑ρ i=lo |xi| − δ
) 8: if ξ ≤ |xρ| then 9: s← s+∑ρi=lo |xi|
10: lo← ρ+ 1 11: else 12: hi← ρ− 1 13: ρ← hi 14: ξ ← 1ρ (s− δ) 15: for i← 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n","4.3 `∞ and `∞,1 regularization",[0],[0]
"do 16: xi ← min(max(xi,−ξ), ξ) 17: procedure PARTITION(w, lo,md, hi) 18: swap xlo and xmd 19: i← lo + 1 20: for j ← lo + 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi do 21: if xj ≥ xlo then 22: swap xi and xj 23: i← i+ 1 24: swap xlo and xi−1 25: return i− 1","4.3 `∞ and `∞,1 regularization",[0],[0]
"We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions.",5 Experiments,[0],[0]
"We use two hidden layers of rectified linear units (Nair and Hinton, 2010).
",5 Experiments,[0],[0]
"2These extensions have been contributed to the NPLM project.
",5 Experiments,[0],[0]
"We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5.",5 Experiments,[0],[0]
"After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens.",5 Experiments,[0],[0]
"For both corpora, we hold out a validation set of 5,000 tokens.",5 Experiments,[0],[0]
"We train each model for 10 iterations over the training data.
",5 Experiments,[0],[0]
Our experiments break down into three parts.,5 Experiments,[0],[0]
"First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings.",5 Experiments,[0],[0]
"Second, we take a closer look at how the model evolves through the training process.",5 Experiments,[0],[0]
"Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system.",5 Experiments,[0],[0]
"We first look at the impact that the `∞,1 regularizer has on the perplexity of our validation set.",5.1 Evaluating perplexity and network size,[0],[0]
The main results are shown in Table 1.,5.1 Evaluating perplexity and network size,[0],[0]
"For λ ≤ 0.01, the regularizer seems to have little impact: no hidden units are pruned, and perplexity is also not affected.",5.1 Evaluating perplexity and network size,[0],[0]
"For λ = 1, on the other hand, most hidden units are pruned – apparently too many, since perplexity is worse.",5.1 Evaluating perplexity and network size,[0],[0]
"But for λ = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity.",5.1 Evaluating perplexity and network size,[0],[0]
"We found this to be consistent across all our experiments, varying n-gram size, initial hidden layer size, and vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus.,5.1 Evaluating perplexity and network size,[0],[0]
"These numbers look very similar to those on Europarl: again λ = 0.1 works best, and, counter to expectation, even the final number of units is similar.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 3 shows the result of varying the vocabulary size: again λ = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 4 shows results using the `2,1 norm (Europarl corpus, 5-grams, 100k vocabulary).",5.1 Evaluating perplexity and network size,[0],[0]
"Since this is a different regularizer, there isn’t any reason to expect that λ behaves the same way, and indeed, a smaller value of λ seems to work best.",5.1 Evaluating perplexity and network size,[0],[0]
We also studied the evolution of the network over the training process to gain some insights into how the method works.,5.2 A closer look at training,[0],[0]
"The first question we want to
answer is whether the method is simply removing units, or converging on an optimal number of units.",5.2 A closer look at training,[0],[0]
"Figure 5 suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially.",5.2 A closer look at training,[0],[0]
"But if we start with a smaller number of units, the method still prunes away about 50 units.
",5.2 A closer look at training,[0],[0]
"Next, we look at the behavior over time of different regularization strengths λ.",5.2 A closer look at training,[0],[0]
"We found that not only does λ = 1 prune out too many units, it does so at the very first iteration (Figure 6, above), perhaps prematurely.",5.2 A closer look at training,[0],[0]
"By contrast, the λ = 0.1 run prunes out units gradually.",5.2 A closer look at training,[0],[0]
"By plotting these curves together with perplexity (Figure 6, below), we can see that the λ = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (λ =
0.01) or pruning first and then fitting (λ = 1).",5.2 A closer look at training,[0],[0]
"We can also visualize the weight matrix itself over time (Figure 7), for λ = 0.1.",5.2 A closer look at training,[0],[0]
"It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune.",5.2 A closer look at training,[0],[0]
We also looked at the impact of our method on statistical machine translation systems.,5.3 Evaluating on machine translation,[0],[0]
"We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext.",5.3 Evaluating on machine translation,[0],[0]
We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data.,5.3 Evaluating on machine translation,[0],[0]
"Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regularization (λ = 0).
",5.3 Evaluating on machine translation,[0],[0]
We built our system using the newscommentary dataset v8.,5.3 Evaluating on machine translation,[0],[0]
We tuned our model using newstest13 and evaluated using newstest14.,5.3 Evaluating on machine translation,[0],[0]
"After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets.
",5.3 Evaluating on machine translation,[0],[0]
"Table 5 shows that the addition of a neural LM helps substantially over the baseline, with improvements of up to 2 BLEU.",5.3 Evaluating on machine translation,[0],[0]
"Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly different (p ≥ 0.05), consistent with the negligible perplexity difference between these models.",5.3 Evaluating on machine translation,[0],[0]
"On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model.",5.3 Evaluating on machine translation,[0],[0]
"The decrease is statistically significant, but small compared with the overall benefit of adding a neural LM.",5.3 Evaluating on machine translation,[0],[0]
Researchers have been exploring the use of neural networks for language modeling for a long time.,6 Related Work,[0],[0]
Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.,6 Related Work,[0],[0]
"Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.",6 Related Work,[0],[0]
Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.,6 Related Work,[0],[0]
Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).,6 Related Work,[0],[0]
"Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.",6 Related Work,[0],[0]
"Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.
",6 Related Work,[0],[0]
"Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures.",6 Related Work,[0],[0]
"RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).",6 Related Work,[0],[0]
Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.,6 Related Work,[0],[0]
"In future work, we plan on exploring how our method could improve these more complicated neural models as well.
",6 Related Work,[0],[0]
Automatically limiting the size of neural networks is an old idea.,6 Related Work,[0],[0]
"The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.",6 Related Work,[0],[0]
"The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned.",6 Related Work,[0],[0]
"The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously.",6 Related Work,[0],[0]
"Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization.",6 Related Work,[0],[0]
"Our method develops on this idea by using a mixed norm to prune units, rather than parameters.
",6 Related Work,[0],[0]
"Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.",6 Related Work,[0],[0]
"However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size.",6 Related Work,[0],[0]
"Thus, dropout and our method seem to be complementary.",6 Related Work,[0],[0]
"We have presented a method for auto-sizing a neural network during training by removing units using a `∞,1 regularizer.",7 Conclusion,[0],[0]
"This regularizer drives a unit’s input weights as a group down to zero, allowing the unit to be pruned.",7 Conclusion,[0],[0]
"We can thus prune units out of our network during training with minimal impact to held-out perplexity or downstream performance of a machine translation system.
",7 Conclusion,[0],[0]
"Our results showed empirically that the choice
of a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus.",7 Conclusion,[0],[0]
"Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting.",7 Conclusion,[0],[0]
"This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes.
",7 Conclusion,[0],[0]
We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on perplexity and machine translation.,7 Conclusion,[0],[0]
"However, this method is general enough that it should be applicable to other domains, both inside natural language processing and outside.",7 Conclusion,[0],[0]
"As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimentation and quick exploration will become increasingly important.",7 Conclusion,[0],[0]
"We would like to thank Tomer Levinboim, Antonios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback.",Acknowledgments,[0],[0]
Neural networks have been shown to improve performance across a range of natural-language tasks.,abstractText,[0],[0]
"However, designing and training them can be complicated.",abstractText,[0],[0]
"Frequently, researchers resort to repeated experimentation to pick optimal settings.",abstractText,[0],[0]
"In this paper, we address the issue of choosing the correct number of units in hidden layers.",abstractText,[0],[0]
"We introduce a method for automatically adjusting network size by pruning out hidden units through `∞,1 and `2,1 regularization.",abstractText,[0],[0]
We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.,abstractText,[0],[0]
We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.,abstractText,[0],[0]
Auto-Sizing Neural Networks: With Applications to n-gram Language Models,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"There has been a staggering increase in progress on generative modeling in recent years, built largely upon fundamental advances such as generative adversarial networks (Goodfellow et al., 2014), variational inference (Kingma & Welling, 2013), and autoregressive density estimation (van den Oord et al., 2016c).",1. Introduction,[0],[0]
"These have led to breakthroughs in state-ofthe-art generation of natural images (Karras et al., 2017) and audio (van den Oord et al., 2016a), and even been used for unsupervised learning of disentangled representations (Higgins et al., 2017; Chen et al., 2016).",1. Introduction,[0],[0]
"These domains often have real-valued distributions with underlying metrics; that is, there is a domain-specific notion of similarity between data points.",1. Introduction,[0],[0]
"This similarity is ignored by the predominant work-horse of generative modeling, the Kullback-Leibler (KL) divergence.",1. Introduction,[0],[0]
"Progress is now being made towards algorithms that optimize with respect to these underlying metrics (Arjovsky et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"*Equal contribution 1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Georg Ostrovski <ostrovski@google.com>, Will Dabney <wdabney@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we present a novel approach to generative modeling, that, while strikingly different from existing methods, is grounded in the well-understood statistical methods of quantile regression.",1. Introduction,[0],[0]
"Unlike the majority of recent work, we approach generative modeling without the use of the KL divergence, and without explicitly approximating a likelihood model.",1. Introduction,[0],[0]
"Like GANs, in this way we produce an implicitly defined model, but unlike GANs our optimization procedure is inherently stable and lacks degenerate solutions which cause loss of diversity and mode collapse.
",1. Introduction,[0],[0]
"Much of the recent research on GANs has been focused on improving stability (Radford et al., 2015; Arjovsky et al., 2017; Daskalakis et al., 2017) and sample diversity (Gulrajani et al., 2017; Salimans et al., 2016; 2018).",1. Introduction,[0],[0]
"By stark contrast, methods such as PixelCNN (van den Oord et al., 2016b) readily produce high diversity, but due to their use of KL divergence are unable to make reasonable trade-offs between likelihood and perceptual similarity (Theis et al., 2015; Bellemare et al., 2017; Bousquet et al., 2017).
",1. Introduction,[0],[0]
"Our proposed method, autoregressive implicit quantile networks (AIQN), combines the benefits of both: a loss function that respects the underlying metric of the data leading to improved perceptual quality, and a stable optimization process leading to highly diverse samples.",1. Introduction,[0],[0]
"While there has been an increasing tendency towards complex architectures (Chen et al., 2017; Salimans et al., 2017) and multiple objective loss functions to overcome these challenges, AIQN is conceptually simple and does not rely on any special architecture or optimization techniques.",1. Introduction,[0],[0]
"Empirically it proves to be robust to hyperparameter variations and easy to optimize.
",1. Introduction,[0],[0]
"Our work is motivated by the recent advances achieved by reframing GANs in terms of optimal transport, leading to the Wasserstein GAN algorithm (Arjovsky et al., 2017), as well as work towards understanding the relationship between optimal transport and both GANs and VAEs (Bousquet et al., 2017).",1. Introduction,[0],[0]
"In agreement with these results, we focus on loss functions grounded in perceptually meaningful metrics.",1. Introduction,[0],[0]
"We build upon recent work in distributional reinforcement learning (Dabney et al., 2018a), which has begun to bridge the gap between approaches in reinforcement learning and unsupervised learning.",1. Introduction,[0],[0]
"Towards a practical algorithm we base our experimental results on Gated PixelCNN (van den Oord et al., 2016b), and show that using AIQN significantly im-
proves objective performance on CIFAR-10 and ImageNet 32x32 in terms of Fréchet Inception Distance (FID) and Inception score, as well as subjective perceptual quality in image samples and inpainting.",1. Introduction,[0],[0]
"We begin by establishing some notation, before turning to a review of three of the most prevalent methods for generative modeling.",2. Background,[0],[0]
"Calligraphic letters (e.g.X ) denote sets or spaces, capital letters (e.g. X) denote random variables, and lower case letters (e.g. x) indicate values.",2. Background,[0],[0]
"A probability distribution with random variable X ∈ X is denoted pX ∈P(X ), its cumulative distribution function (c.d.f.)",2. Background,[0],[0]
"FX , and inverse c.d.f. or quantile function QX = F−1X .",2. Background,[0],[0]
When probability distributions or quantile functions are parameterized by some θ,2. Background,[0],[0]
"we will write pθ or Qθ recognizing that here we do not view θ as a random variable.
",2. Background,[0],[0]
"Perhaps the simplest way to approach generative modeling of a random variableX ∈ X is by fixing some discretization ofX into n separate values, say x1, . . .",2. Background,[0],[0]
", xn ∈ X , and parameterize the approximate distribution with pθ(xi) ∝",2. Background,[0],[0]
exp(θi).,2. Background,[0],[0]
"This type of categorical parameterization is widely used, only slightly less commonly when X does not lend itself naturally to such a partitioning.",2. Background,[0],[0]
"Typically, the parameters θ are optimized to minimize the Kullback-Leibler (KL) divergence between observed values of X and the model pθ, θ∗ = arg minθDKL(pX‖pθ).",2. Background,[0],[0]
"However, this is only tractable whenX is a small discrete set or at best low-dimensional.",2. Background,[0],[0]
A common method for extending a generative model or density estimator to multivariate distributions is to factor the density as a product of scalarvalued conditional distributions.,2. Background,[0],[0]
"Let X = (X1, . . .",2. Background,[0],[0]
", Xn), then for any permutation of the dimensions σ :",2. Background,[0],[0]
"Nn → Nn,
pX(x) =",2. Background,[0],[0]
"n∏ i=1 pXσ(i)(xσ(i)|xσ(1), . . .",2. Background,[0],[0]
", xσ(i−1)).",2. Background,[0],[0]
"(1)
When the conditional density is modeled by a simple (e.g. Gaussian) base distribution, the ordering of the dimensions can be crucial (Papamakarios et al., 2017).",2. Background,[0],[0]
"However, it is common practice to choose an arbitrary ordering and rely upon a more powerful conditional model to avoid these problems.",2. Background,[0],[0]
"This class of models includes PixelRNN and PixelCNN (van den Oord et al., 2016c;b), MAF (Papamakarios et al., 2017), MADE (Germain et al., 2015), and many others.",2. Background,[0],[0]
"Fundamentally, all these approaches use the KL divergence as their loss function.
",2. Background,[0],[0]
"Another class of methods, generally known as latent variable methods, can bypass the need for autoregressive models using a different modeling assumption.",2. Background,[0],[0]
"Specifically, consider the Variational Autoencoder (VAE) (Kingma & Welling, 2013; Rezende et al., 2014), which represents
pθ as the marginalization over a latent random variable Z ∈ Z .",2. Background,[0],[0]
"The VAE is trained to maximize an approximate lower bound of the log-likelihood of the observations:
log pθ(x) ≥ −DKL(qθ(z|x)‖p(z))",2. Background,[0],[0]
+,2. Background,[0],[0]
E,2. Background,[0],[0]
"[log pθ(x|z)] .
",2. Background,[0],[0]
"Although VAEs are straightforward to implement and optimize, and effective at capturing structure in highdimensional spaces, they often miss fine-grained detail, resulting in blurry images.
",2. Background,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) pose the problem of learning a generative model as a two-player zero-sum game between a discriminator D, attempting to distinguish between x ∼ pX (real data) and x ∼ pθ (generated data), and a generator G, attempting to generate data indistinguishable from real data.",2. Background,[0],[0]
"The generator is an implicit latent variable model that reparameterizes samples, typically from an isotropic Gaussian distribution, into values in X .",2. Background,[0],[0]
"The original formulation of GANs,
arg min G sup D",2. Background,[0],[0]
[ E X log(D(X)),2. Background,[0],[0]
"+ E Z log(1−D(G(Z))) ] ,
can be seen as minimizing a lower-bound on the JensenShannon divergence (Goodfellow et al., 2014; Bousquet et al., 2017).",2. Background,[0],[0]
"That is, even in the case of GANs we are often minimizing functions of the KL divergence1.
",2. Background,[0],[0]
"Many recent advances have come from principled combinations of these three fundamental methods (Makhzani et al., 2015; Dumoulin et al., 2016; Rosca et al., 2017).",2. Background,[0],[0]
"A common perspective in generative modeling is that the choice of model should encode existing metric assumptions about the domain, combined with a generic likelihoodfocused loss such as the KL divergence.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Under this view, the KL’s general applicability and robust optimization properties make it a natural choice, and most implementations of the methods we reviewed in the previous section attempt to, at least indirectly, minimize a version of the KL.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"On the other hand, as every model inevitably makes tradeoffs when constrained by capacity or limited training, it is desirable for its optimization goal to incentivize trade-offs prioritizing approximately correct solutions, when the data space is endowed with a metric supporting a meaningful (albeit potentially subjective) notion of approximation.",2.1. Distance Metrics and Loss Functions,[0],[0]
"It has been argued (Theis et al., 2015; Bousquet et al., 2017; Arjovsky et al., 2017; Bellemare et al., 2017) that the KL may not always be appropriate from this perspective, by making sub-optimal trade-offs between likelihood and similarity.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"1The Jensen-Shannon divergence is the sum of KLs between distributions P,Q and their uniform mixture M = 0.5(P +Q): JSD(P ||Q)",2.1. Distance Metrics and Loss Functions,[0],[0]
"= 0.5(DKL(P ||M) +DKL(Q||M)).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Indeed, many limitations of existing models can be traced back to the use of KL, and the resulting trade-offs in approximate solutions it implies.",2.1. Distance Metrics and Loss Functions,[0],[0]
"For instance, its use appears to play a central role in one of the primary failure modes of VAEs, that of blurry samples.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Zhao et al. (2017) argue that the Gaussian posterior pθ(x|z) implies an overly simple model, which, when unable to perfectly fit the data, is forced to average (thus creating blur), and is not incentivized by the KL towards an alternative notion of approximate solution.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Theis et al. (2015) emphasized that an improvement of log-likelihood does not necessarily translate to higher perceptual quality, and that the KL loss is more likely to produce atypical samples than some other training criteria.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"We offer an alternative perspective: a good model should encode assumptions about the data distribution, whereas a good loss should encode the notion of similarity, that is, the underlying metric on the data space.",2.1. Distance Metrics and Loss Functions,[0],[0]
"From this point of view, the KL corresponds to an actual absence of explicit underlying metric, with complete focus on probability.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"The optimal transport metrics Wc, for underlying metric c(x, x′), and in particular the p-Wasserstein distance, when c is an Lp metric, have frequently been proposed as being well-suited replacements to KL (Bousquet et al., 2017; Genevay et al., 2017).",2.1. Distance Metrics and Loss Functions,[0],[0]
"Briefly, the advantages are (1) avoidance of mode collapse (no need to choose between spreading over modes or collapsing to a single mode as in KL), and (2) the ability to trade off errors and incentivize approximations that respect the underlying metric.
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recently, Arjovsky et al. (2017) introduced the Wasserstein
GAN, reposing the two-player game as the estimation of the gradient of the 1-Wasserstein distance between the data and generator distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"They reframe this in terms of the dual form of the 1-Wasserstein, with the critic estimating a function f which maximally separates the two distributions.",2.1. Distance Metrics and Loss Functions,[0],[0]
"While this is an exciting line of work, it still faces limitations when the critic solution is approximate, i.e. when f∗ is not found before each update.",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this case, due to insufficient training of the critic (Bellemare et al., 2017) or limitations of the function approximator, the gradient direction produced can be arbitrarily bad (Bousquet et al., 2017).
",2.1. Distance Metrics and Loss Functions,[0],[0]
"Thus, we are left with the question of how to minimize a distribution loss respecting an underlying metric.",2.1. Distance Metrics and Loss Functions,[0],[0]
"Recent work in distributional reinforcement learning has proposed the use of quantile regression as a method for minimizing the 1-Wasserstein in the univariate case when approximating using a mixture of Dirac functions (Dabney et al., 2018b).",2.1. Distance Metrics and Loss Functions,[0],[0]
"In this section, we review quantile regression as a method for estimating the quantile function of a distribution at specific points, i.e. its inverse cumulative distribution function.",2.2. Quantile Regression,[0],[0]
"This leads to recent work on approximating a distribution by a neural network approximation of its quantile function, acting as a reparameterization of a random sample from the uniform distribution.
",2.2. Quantile Regression,[0],[0]
"The quantile regression loss (Koenker & Hallock, 2001) for a quantile at τ ∈",2.2. Quantile Regression,[0],[0]
"[0, 1] and error u (positive for underestimation and negative for overestimation) is given by ρτ (u) =",2.2. Quantile Regression,[0],[0]
(τ − I{u ≤,2.2. Quantile Regression,[0],[0]
0})u.,2.2. Quantile Regression,[0],[0]
It is an asymmetric loss function penalizing underestimation by weight τ and overestimation by weight 1,2.2. Quantile Regression,[0],[0]
− τ .,2.2. Quantile Regression,[0],[0]
"For a given scalar distribution Z with c.d.f. FZ and a quantile τ , the inverse c.d.f. q = F−1Z (τ) minimizes the expected quantile regression loss Ez∼Z",2.2. Quantile Regression,[0],[0]
[ρτ (z − q)].,2.2. Quantile Regression,[0],[0]
Using this loss allows one to train a neural network to approximate a scalar distribution represented by its inverse c.d.f.,2.2. Quantile Regression,[0],[0]
"For this, the network can output a fixed grid of quantiles (Dabney et al., 2018b), with the respective quantile regression losses being applied to each output independently.",2.2. Quantile Regression,[0],[0]
"A more effective approach is to provide the desired quantile τ as an additional input to the network, and train it to output the corresponding value of F−1Z (τ).",2.2. Quantile Regression,[0],[0]
"The implicit quantile network (IQN) model (Dabney et al., 2018a) reparameterizes a sample τ ∼ U([0, 1]) through a deterministic function to produce samples from the underlying data distribution.",2.2. Quantile Regression,[0],[0]
These two methods can be seen to belong to the top-right and bottom-right categories in Figure 1.,2.2. Quantile Regression,[0],[0]
"An IQN Qθ can be trained by stochastic gradient descent on the quantile regression loss, with u = z −Qθ(τ) and training samples (z, τ) drawn from z ∼ Z and τ ∼ U([0, 1]).
",2.2. Quantile Regression,[0],[0]
"One drawback to the quantile regression loss is that gradients do not scale with the magnitude of the error, but instead with the sign of the error and the quantile weight τ .",2.2. Quantile Regression,[0],[0]
This increases gradient variance and can negatively impact the final model’s sample quality.,2.2. Quantile Regression,[0],[0]
"Increasing the batch size, and thus averaging over more values of τ , would have the effect of lowering this variance.",2.2. Quantile Regression,[0],[0]
"Alternatively, we can smooth the gradients as the model converges by allowing errors, under some threshold κ, to be scaled with their magnitude, reverting to an expectile loss.",2.2. Quantile Regression,[0],[0]
"This results in the Huber quantile loss (Huber, 1964; Dabney et al., 2018b):
ρκτ (u) =
{ |τ−I{u≤0}| 2κ u
2, if |u| ≤ κ, |τ −",2.2. Quantile Regression,[0],[0]
I{u ≤ 0}|(|u|,2.2. Quantile Regression,[0],[0]
"− 12κ), otherwise.",2.2. Quantile Regression,[0],[0]
(2),2.2. Quantile Regression,[0],[0]
"Let X = (X1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", Xn) ∈",3. Autoregressive Implicit Quantiles,[0],[0]
X1 × · · · × Xn = X be an ndimensional random variable.,3. Autoregressive Implicit Quantiles,[0],[0]
"We begin by analyzing the effect of two naive applications of IQN to modeling the distribution of X .
",3. Autoregressive Implicit Quantiles,[0],[0]
"First, suppose we use the same quantile target, τ ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1], for every output dimension.",3. Autoregressive Implicit Quantiles,[0],[0]
"The only modification to IQN would be to output n dimensions instead of 1, the loss being applied to each output dimension independently.",3. Autoregressive Implicit Quantiles,[0],[0]
This is equivalent to assuming that the dimensions of X are comonotonic.,3. Autoregressive Implicit Quantiles,[0],[0]
"Two random variables are comonotonic if and only if they can be expressed as nondecreasing (deterministic) functions of a single random variable (Dhaene et al., 2006).",3. Autoregressive Implicit Quantiles,[0],[0]
Thus a joint quantile function for a comonotonic X can be written as F−1X (τ) =,3. Autoregressive Implicit Quantiles,[0],[0]
"(F−1X1 (τ), F −1 X2
(τ), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F−1Xn(τ)).",3. Autoregressive Implicit Quantiles,[0],[0]
"While there are many interesting uses for comonotonic random variables, we believe this assumption is too strong to be useful more broadly.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Second, one could use a separate value τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] for each Xi, with the IQN being unchanged from the first case.",3. Autoregressive Implicit Quantiles,[0],[0]
This corresponds to making an independence assumption on the dimensions of X .,3. Autoregressive Implicit Quantiles,[0],[0]
"Again we would expect this to be an unreasonably restrictive modeling assumption for many domains, such as the case of natural images.
",3. Autoregressive Implicit Quantiles,[0],[0]
"Now, we turn to our proposed approach of extending IQN to multivariate distributions.",3. Autoregressive Implicit Quantiles,[0],[0]
We fix an ordering of the n dimensions.,3. Autoregressive Implicit Quantiles,[0],[0]
"If the density function pX is expressed as a product of conditional likelihoods, as in Equation 1, then the joint c.d.f. can be written as
FX(x) = P(X1 ≤",3. Autoregressive Implicit Quantiles,[0],[0]
"x1, . .",3. Autoregressive Implicit Quantiles,[0],[0]
.,3. Autoregressive Implicit Quantiles,[0],[0]
", Xn ≤ xn),
",3. Autoregressive Implicit Quantiles,[0],[0]
= n∏ i=1,3. Autoregressive Implicit Quantiles,[0],[0]
"FXi|Xi−1,...,X1(xi).
",3. Autoregressive Implicit Quantiles,[0],[0]
"Furthermore, for τjoint = ∏n i=1 τi, we can write the jointquantile function of X as
F−1X (τjoint) =",3. Autoregressive Implicit Quantiles,[0],[0]
"(F −1 X1 (τ1), . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", F −1 Xn|Xn−1,...(τn)).
",3. Autoregressive Implicit Quantiles,[0],[0]
"This approach has been used previously by Koenker & Xiao (2006), who introduced a quantile autoregression model for quantile regression on time-series.
",3. Autoregressive Implicit Quantiles,[0],[0]
We propose to extend IQN to an autoregressive model of the above conditional form of a joint-quantile function.,3. Autoregressive Implicit Quantiles,[0],[0]
"Denoting X1:i = X1 × · · · × Xi, let X̃ := ⋃n i=0 X1:i be the space of ‘partial’ data points.",3. Autoregressive Implicit Quantiles,[0],[0]
We can define the autoregressive IQN as a deterministic functionQθ :,3. Autoregressive Implicit Quantiles,[0],[0]
X̃ ×,3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1]n → X̃ , mapping partial samples x̃ ∈ X̃ and quantile targets τi ∈",3. Autoregressive Implicit Quantiles,[0],[0]
"[0, 1] to estimates of F−1X .",3. Autoregressive Implicit Quantiles,[0],[0]
We can then train Qθ using a quantile regression loss (Equation 2).,3. Autoregressive Implicit Quantiles,[0],[0]
"For generation, one can iterate x1:i = Qθ(x1:i−1, τi), on a sequence of growing partial samples2 x1:i−1 and independently sampled τi ∼ U([0, 1]), for i = 1, . . .",3. Autoregressive Implicit Quantiles,[0],[0]
", n, to finally obtain a sample x = x1:n.",3. Autoregressive Implicit Quantiles,[0],[0]
"As previously mentioned, for the restricted model class of a uniform mixture of Diracs, quantile regression can be shown to minimize the 1-Wasserstein metric (Dabney et al., 2018b).",3.1. Quantile Regression and the Wasserstein,[0],[0]
"We extend this analysis for the case of arbitrary approximate quantile functions, and find that quantile regression minimizes a closely related divergence which we call quantile divergence, defined, for any distributions P and Q, as
q(P,Q) := ∫ 1 0",3.1. Quantile Regression and the Wasserstein,[0],[0]
[∫ F−1Q (τ),3.1. Quantile Regression and the Wasserstein,[0],[0]
"F−1P (τ) (FP (x)− τ)dx ] dτ.
",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Indeed, the expected quantile loss of any parameterized quantile function Q̄θ equals, up to a constant, the quantile divergence between P and the distribution Qθ implicitly defined by Q̄θ:
E τ∼U([0,1])",3.1. Quantile Regression and the Wasserstein,[0],[0]
[ E z∼P [ρτ (z − Q̄θ(τ))],3.1. Quantile Regression and the Wasserstein,[0],[0]
"] = q(P,Qθ) + h(P ),
where h(P ) does not depend on Qθ.",3.1. Quantile Regression and the Wasserstein,[0],[0]
"Thus quantile regression minimizes the quantile divergence q(P,Qθ) and the sample gradient ∇θρτ (z − Q̄θ(τ))",3.1. Quantile Regression and the Wasserstein,[0],[0]
"(for τ ∼ U([0, 1]) and z ∼ P ) is an unbiased estimate of ∇θq(P,Qθ).",3.1. Quantile Regression and the Wasserstein,[0],[0]
See Appendix for proofs.,3.1. Quantile Regression and the Wasserstein,[0],[0]
"Although IQN does not directly model the log-likelihood of the data distribution, observe that we can still query the implied density at a point (Jones, 1992):
∂
∂τ F−1X (τ) =
1
pX(F −1 X (τ))
.
",3.2. Quantile Density Function,[0],[0]
"Indeed, this quantity, known as the sparsity function (Tukey, 1965) or the quantile-density function (Parzen, 1979) plays
2Throughout we understand x0 = x1:0 ∈ X1:0 to denote the ‘empty tuple’, and the function Qθ to map this to a single unconditional sample x1 = x1:1 = Qθ(x0, τ1).
",3.2. Quantile Density Function,[0],[0]
"a central role in the analysis of quantile regression models (Koenker, 1994).",3.2. Quantile Density Function,[0],[0]
"A common approach involves choosing a bandwidth parameter h and estimating this quantity through finite-differences around the value of interest as (F−1X (τ+h)−F−1X (τ−h))/2h (Siddiqui, 1960).",3.2. Quantile Density Function,[0],[0]
"However, as we have the full quantile function, the quantile-density function can be computed exactly using a single step of back-propagation to compute ∂F
−1(τ) ∂τ .",3.2. Quantile Density Function,[0],[0]
"As this only allows
querying the density given the value of τ , application to general likelihoods would require finding the value of τ that produces the closest approximation to the query point x.",3.2. Quantile Density Function,[0],[0]
"Though arguably too inefficient for training, this could potentially be used to interrogate the model.",3.2. Quantile Density Function,[0],[0]
"To test our proposed method, which is architecturally compatible with many generative model approaches, we wanted to compare and contrast IQN, that is quantile regression and quantile reparameterization, with a method trained with an explicit parameterization to minimize KL divergence.",4. PixelIQN,[0],[0]
"A natural choice for this was PixelCNN, specifically we build upon the Gated PixelCNN of van den Oord et al. (2016b).
",4. PixelIQN,[0],[0]
"The Gated PixelCNN takes as input an image x ∼ X , sampled from the training distribution at training time, and potentially all zeros or partially generated at generation time, as well as a location-dependent context s.",4. PixelIQN,[0],[0]
"The model consists of a number of residual layer blocks, whose structure is chosen to allow each output pixel to be a function of all preceding input pixels (in a raster-scan order).",4. PixelIQN,[0],[0]
"At its core, each layer block computes two gated activations of the form
y = tanh(Wk,f ∗ x+ Vk,f ∗ s) σ(Wk,g ∗ x+",4. PixelIQN,[0],[0]
"Vk,g ∗ s), with k the layer index, ∗ denoting convolution, and Vk,f and Vk,g being 1 × 1 convolution kernels.",4. PixelIQN,[0],[0]
"See Figure 2 for a full schematic depiction of a Gated PixelCNN layer
block.",4. PixelIQN,[0],[0]
"After a number of such layer blocks, the PixelCNN produces a final output layer with shape (n, n, 3, 256), with a softmax across the final dimension, corresponding to the approximate conditional likelihood for the value of each pixel-channel.",4. PixelIQN,[0],[0]
"That is, the conditional likelihood is the product of these individual autoregressive models,
p(x|s) = 3n2∏ i=1",4. PixelIQN,[0],[0]
"p(xi|x1, . . .",4. PixelIQN,[0],[0]
", xi−1, si).
",4. PixelIQN,[0],[0]
"Typically the location-dependent conditioning term was used to condition on class labels, but here, we will use it to condition on the sample point3 τ ∈",4. PixelIQN,[0],[0]
"[0, 1]3n2 .",4. PixelIQN,[0],[0]
"Thus, in addition to the input image x we input, in place of s, the sample points τ = (τ1, . . .",4. PixelIQN,[0],[0]
", τ3n2) to be reparameterized, with each τi ∼ U([0, 1]).",4. PixelIQN,[0],[0]
"Finally, our network outputs only the full sample image of shape (n, n, 3), without the need for an additional softmax layer.",4. PixelIQN,[0],[0]
Note that the number of τ values generated exactly corresponds to the number of random draws from softmax distributions in the original PixelCNN.,4. PixelIQN,[0],[0]
"We are simply changing the role of the randomness, from a draw at the output to a part of the input.
",4. PixelIQN,[0],[0]
"Architecturally, our proposed model, PixelIQN, is exactly the network given by van den Oord et al. (2016b), with the one exception that we output only a single value per pixel-channel and do not require the softmax activations.
",4. PixelIQN,[0],[0]
"In PixelCNN training is done by passing the training image through the network, and training each output softmax distribution using the KL divergence between the training image and the approximate distribution,∑
i
DKL(δxi , p(·|x1, . . .",4. PixelIQN,[0],[0]
", xi−1)).
",4. PixelIQN,[0],[0]
"For PixelIQN, the input is the training image x and a sample point τ ∼ U([0, 1]3n2).",4. PixelIQN,[0],[0]
"The output values Qx(τ) ∈ R3n 2 are interpreted as the approximate quantile function at τ , Qx(τ)i = QX(τi|xi−1, . . .), trained with a single step of quantile regression towards the observed sample",4. PixelIQN,[0],[0]
"x:∑
i
",4. PixelIQN,[0],[0]
"ρκτi(xi −QX(τi|xi−1, . .",4. PixelIQN,[0],[0]
.)).,4. PixelIQN,[0],[0]
"We begin by demonstrating PixelIQN on CIFAR-10 (Krizhevsky & Hinton, 2009).",4.1. CIFAR-10,[0],[0]
"For comparison, we train both a baseline Gated PixelCNN and a PixelIQN.",4.1. CIFAR-10,[0],[0]
"Both models correspond to the 15-layer network variant in (van den Oord et al., 2016b), see Appendix for detailed hyperparameters and training procedure.",4.1. CIFAR-10,[0],[0]
"The two methods have substantially different loss functions, so we performed a
3Conditioning on labels remains possible (see Section 4.2).
hyperparameter search using a short training run, with the same number (500) of hyperparameter configurations evaluated for both models.",4.1. CIFAR-10,[0],[0]
"For all results, we report full training runs using the best found hyperparameters in each case.",4.1. CIFAR-10,[0],[0]
"The evaluation metric used for the hyperparameter search was the Fréchet Inception Distance (FID) (Heusel et al., 2017), see Appendix for details.",4.1. CIFAR-10,[0],[0]
"In addition to FID, we report Inception score (Salimans et al., 2016) for both models.
",4.1. CIFAR-10,[0],[0]
Figure 4 (left) shows Inception score and FID for both models evaluated at several points throughout training.,4.1. CIFAR-10,[0],[0]
"The fully trained PixelCNN achieves an Inception score and FID of 4.6 and 65.9 respectively, while PixelIQN substantially outperforms it with an Inception score of 5.3 and FID of 49.5.",4.1. CIFAR-10,[0],[0]
"This also compares favorably with e.g. WGAN (Arjovsky et al., 2017), which reaches an Inception score of 3.8.",4.1. CIFAR-10,[0],[0]
"For subjective evaluations, we give samples from both models in Figure 3.",4.1. CIFAR-10,[0],[0]
Samples coming from PixelIQN are much more visually coherent.,4.1. CIFAR-10,[0],[0]
"Of note, the PixelIQN model achieves
a performance level comparable to that of the fully trained PixelCNN with only about one third the number of training updates (and about one third of the wall-clock time).",4.1. CIFAR-10,[0],[0]
"Next, we turn to the small ImageNet dataset (Russakovsky et al., 2015), first used for generative modeling in the PixelRNN work (van den Oord et al., 2016c).",4.2. ImageNet 32x32,[0],[0]
"Again, we evaluate using FID and Inception score.",4.2. ImageNet 32x32,[0],[0]
"For this much harder dataset, we base our PixelCNN and PixelIQN models on the larger 20-layer variant used in (van den Oord et al., 2016b).",4.2. ImageNet 32x32,[0],[0]
"Due to substantially longer training time for this model, we did not perform additional hyperparameter tuning, and mostly used the same hyperparameter values as in the previous sections for both models; details can be found in the Appendix.
",4.2. ImageNet 32x32,[0],[0]
Figure 4 shows Inception score and FID throughout training of PixelCNN and PixelIQN.,4.2. ImageNet 32x32,[0],[0]
"Again, PixelIQN substan-
tially outperforms the baseline in terms of final performance and sample complexity.",4.2. ImageNet 32x32,[0],[0]
"For final scores and a comparison to state-of-the-art GAN models, see Table 1.",4.2. ImageNet 32x32,[0],[0]
Figure 5 shows random (non-cherry-picked) samples from both models.,4.2. ImageNet 32x32,[0],[0]
"Compared to PixelCNN, PixelIQN samples appear to have superior quality with more global consistency and less ‘high-frequency noise’.
",4.2. ImageNet 32x32,[0],[0]
"In Figure 6, we show the inpainting performance of PixelIQN, by fixing the top half of a validation set image as input and sampling repeatedly from the model to generate different completions.",4.2. ImageNet 32x32,[0],[0]
We note that the model consistently generates plausible completions with significant diversity between different completion samples for the same input image.,4.2. ImageNet 32x32,[0],[0]
"Meanwhile, WGAN-GP has been seen to produce deterministic completions (Bellemare et al., 2017).
",4.2. ImageNet 32x32,[0],[0]
"Following (van den Oord et al., 2016b), we also trained a class-conditional PixelIQN variant, providing to the model the one-hot class label corresponding to a training image (in addition to a τ sample).",4.2. ImageNet 32x32,[0],[0]
"Samples from a class-conditional model can be expected to have higher visual quality, as the class label provides log2(1000)",4.2. ImageNet 32x32,[0],[0]
"≈ 10 bits of information, see Figure 7.",4.2. ImageNet 32x32,[0],[0]
"As seen in Figure 4 and Table 1, class conditioning also further improves Inception score and FID.",4.2. ImageNet 32x32,[0],[0]
"To generate each sample for the computation of these scores, we sample one of 1000 class labels randomly, then generate an image conditioned on this label via the trained model.
",4.2. ImageNet 32x32,[0],[0]
"Finally, motivated by the very long training time for the large PixelCNN model (approximately 1 day per 100K training steps, on 16 NVIDIA Tesla P100 GPUs), we also trained smaller 15-layer versions of the models (same as the ones used on CIFAR-10) on the small ImageNet dataset.",4.2. ImageNet 32x32,[0],[0]
"For comparison, these take approximately 12 hours for 100K training steps on a single P100 GPU, or less than 3 hours on 8 P100 GPUs.",4.2. ImageNet 32x32,[0],[0]
"As expected, little PixelCNN, while suitable
for the CIFAR-10 dataset, fails to achieve competitive scores on the ImageNet dataset, achieving Inception score 5.1 and FID 66.4.",4.2. ImageNet 32x32,[0],[0]
"Astonishingly, little PixelIQN on this dataset reaches Inception score 7.3 and FID 38.5, see Figure 4 (right).",4.2. ImageNet 32x32,[0],[0]
"It thereby not only outperforms the little PixelCNN, but also the larger 20-layer version!",4.2. ImageNet 32x32,[0],[0]
"This strongly supports the hypothesis that PixelCNN, and potentially many other models, are constrained not only by their model capacity, but crucially also by the sub-optimal trade-offs made by their log-likelihood training criterion, failing to align with perceptual or evaluation metrics.",4.2. ImageNet 32x32,[0],[0]
Most existing generative models for images belong to one of two classes.,5. Discussion and Conclusions,[0],[0]
"The first are likelihood-based models, trained with an elementwise KL reconstruction loss, which,
while perceptually meaningless, provides robust optimization properties and high sample diversity.",5. Discussion and Conclusions,[0],[0]
"The second are GANs, trained based on a discriminator loss, typically better aligned with a perceptual metric and enabling the generator to produce realistic, globally consistent samples.",5. Discussion and Conclusions,[0],[0]
"Their advantages come at the cost of a harder optimization problem, high parameter sensitivity, and most importantly, a tendency to collapse modes of the data distribution.
",5. Discussion and Conclusions,[0],[0]
"AIQNs are a new, fundamentally different, technique for generative modeling.",5. Discussion and Conclusions,[0],[0]
"By using a quantile regression loss instead of KL divergence, they combine some of the best properties of the two model classes.",5. Discussion and Conclusions,[0],[0]
"By their nature, they preserve modes of the learned distribution, while producing perceptually appealing high-quality samples.",5. Discussion and Conclusions,[0],[0]
The inevitable approximation trade-offs a generative model makes when constrained by capacity or insufficient training can vary significantly depending on the loss used.,5. Discussion and Conclusions,[0],[0]
"We argue that the proposed quantile regression loss aligns more effectively with a given metric and therefore makes subjectively more advantageous trade-offs.
",5. Discussion and Conclusions,[0],[0]
Devising methods for quantile regression over multidimensional outputs is an active area of research.,5. Discussion and Conclusions,[0],[0]
"New methods are continuing to be investigated (Carlier et al., 2016; Hallin & Miroslav, 2016), and a promising direction for future work is to find ways to use these to replace autoregressive models.",5. Discussion and Conclusions,[0],[0]
"One approach to reducing the computational burden of such models is to apply AIQN to the latent dimensions
of a VAE.",5. Discussion and Conclusions,[0],[0]
"Similar in spirit to Rosca et al. (2017), this would use the VAE to reduce the dimensionality of the problem and the AIQN to sample from the true latent distribution.",5. Discussion and Conclusions,[0],[0]
"In the Appendix we give preliminary results using such an technique, on CelebA 64× 64 (Liu et al., 2015).",5. Discussion and Conclusions,[0],[0]
"We have shown that IQN, computationally cheap and technically simple, can be readily applied to existing architectures, PixelCNN and VAE (Appendix), improving robustness and sampling quality of the underlying model.",5. Discussion and Conclusions,[0],[0]
"We demonstrated that PixelIQN produces more realistic, globally coherent samples, and improves Inception score and FID.
",5. Discussion and Conclusions,[0],[0]
We further point out that many recent advances in generative models could be easily combined with our proposed method.,5. Discussion and Conclusions,[0],[0]
"Recent algorithmic improvements to GANs such as mini-batch discrimination and progressive growing (Salimans et al., 2016; Karras et al., 2017), while not strictly necessary in our work, could be applied to further improve performance.",5. Discussion and Conclusions,[0],[0]
"PixelCNN++ (Salimans et al., 2017) is an architectural improvement of PixelCNN, with several beneficial modifications supported by experimental evidence.",5. Discussion and Conclusions,[0],[0]
"Although we have built upon the original Gated PixelCNN in this work, we believe all of these modifications to be compatible with our work, except for the use of a mixture of logistics in place of PixelCNN’s softmax.",5. Discussion and Conclusions,[0],[0]
"As we have entirely replaced this model component, this change does not map onto our model.",5. Discussion and Conclusions,[0],[0]
"Of note, the motivation behind this change closely mirrors our own, in looking for a loss that respects the underlying metric between examples.",5. Discussion and Conclusions,[0],[0]
"The recent PixelSNAIL model (Chen et al., 2017) achieves stateof-the-art modeling performance by enhancing PixelCNN with ELU nonlinearities, modified block structure, and an attention mechanism.",5. Discussion and Conclusions,[0],[0]
"Again, all of these are fully compatible with our work and should improve results further.
",5. Discussion and Conclusions,[0],[0]
"Finally, the implicit quantile formulation lifts a number of architectural restrictions of previous generative models.",5. Discussion and Conclusions,[0],[0]
"Most importantly, the reparameterization as an inverse c.d.f. allows to learn distributions over continuous ranges without pre-specified boundaries or quantization.",5. Discussion and Conclusions,[0],[0]
"This enables modeling continuous-valued variables, for example for generation of sound (van den Oord et al., 2016a), opening multiple interesting avenues for further investigation.",5. Discussion and Conclusions,[0],[0]
We would like to acknowledge the important role many of our colleagues at DeepMind played for this work.,Acknowledgements,[0],[0]
"We especially thank Aäron van den Oord and Sander Dieleman for invaluable advice on the PixelCNN model; Ivo Danihelka and Danilo J. Rezende for careful reading and insightful comments on an earlier version of the paper; Igor Babuschkin, Alexandre Galashov, Dominik Grewe, Jacob Menick, and Mihaela Rosca for technical help.",Acknowledgements,[0],[0]
"We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression.",abstractText,[0],[0]
"AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity.",abstractText,[0],[0]
The method can be applied to many existing models and architectures.,abstractText,[0],[0]
"In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherrypicked samples, and inpainting results.",abstractText,[0],[0]
We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.,abstractText,[0],[0]
Autoregressive Quantile Networks for Generative Modeling,title,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features.
",1. Motivation and Summary of Results,[0],[0]
Definition 1.,1. Motivation and Summary of Results,[0],[0]
"Formally, suppose we have a function F :",1. Motivation and Summary of Results,[0],[0]
"Rn → [0, 1] that represents a deep network, and an input x = (x1, . . .",1. Motivation and Summary of Results,[0],[0]
", xn) ∈ Rn.",1. Motivation and Summary of Results,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x
′) =",1. Motivation and Summary of Results,[0],[0]
"(a1, . . .",1. Motivation and Summary of Results,[0],[0]
", an) ∈ Rn where ai is the contribution of xi to the prediction F (x).
",1. Motivation and Summary of Results,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",1. Motivation and Summary of Results,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013;
",1. Motivation and Summary of Results,[0],[0]
"*Equal contribution 1Google Inc., Mountain View, USA.",1. Motivation and Summary of Results,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,1. Motivation and Summary of Results,[0],[0]
">, Ankur Taly <ataly@google.com>.
",1. Motivation and Summary of Results,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Motivation and Summary of Results,[0],[0]
"Copyright 2017 by the author(s).
",1. Motivation and Summary of Results,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).
",1. Motivation and Summary of Results,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",1. Motivation and Summary of Results,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",1. Motivation and Summary of Results,[0],[0]
There are also other applications of attribution.,1. Motivation and Summary of Results,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,1. Motivation and Summary of Results,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",1. Motivation and Summary of Results,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,1. Motivation and Summary of Results,[0],[0]
We give such an example in Section 6.2.,1. Motivation and Summary of Results,[0],[0]
Attributions could also be used by developers in an exploratory sense.,1. Motivation and Summary of Results,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",1. Motivation and Summary of Results,[0],[0]
"In Section 6.3, we give such an example.
",1. Motivation and Summary of Results,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,1. Motivation and Summary of Results,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",1. Motivation and Summary of Results,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",1. Motivation and Summary of Results,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,1. Motivation and Summary of Results,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,1. Motivation and Summary of Results,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.
",1. Motivation and Summary of Results,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.
",1. Motivation and Summary of Results,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",1. Motivation and Summary of Results,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.
",1. Motivation and Summary of Results,[0],[0]
Remark 1.,1. Motivation and Summary of Results,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,1. Motivation and Summary of Results,[0],[0]
"A common way for humans to perform attribution relies on counter-
ar X
iv :1
70 3.
01 36
5v 2
[ cs
.L",1. Motivation and Summary of Results,[0],[0]
"G
] 1
3 Ju
n 20
17
factual intuition.",1. Motivation and Summary of Results,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,1. Motivation and Summary of Results,[0],[0]
"In a deep network, we model the absence using a single baseline input.",1. Motivation and Summary of Results,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",1. Motivation and Summary of Results,[0],[0]
"For instance, in object recognition networks, it is the black image.",1. Motivation and Summary of Results,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",1. Motivation and Summary of Results,[0],[0]
We now discuss two axioms (desirable characteristics) for attribution methods.,2. Two Fundamental Axioms,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,2. Two Fundamental Axioms,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",2. Two Fundamental Axioms,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.
Gradients.",2. Two Fundamental Axioms,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",2. Two Fundamental Axioms,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",2. Two Fundamental Axioms,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",2. Two Fundamental Axioms,[0],[0]
An attribution method satisfies Sensitivity(a),2.1. Axiom: Sensitivity(a),[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,2.1. Axiom: Sensitivity(a),[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)
",2.1. Axiom: Sensitivity(a),[0],[0]
Gradients violate Sensitivity(a):,2.1. Axiom: Sensitivity(a),[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",2.1. Axiom: Sensitivity(a),[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,2.1. Axiom: Sensitivity(a),[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Fig-
ure 2).
",2.1. Axiom: Sensitivity(a),[0],[0]
Other back-propagation based approaches.,2.1. Axiom: Sensitivity(a),[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,2.1. Axiom: Sensitivity(a),[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",2.1. Axiom: Sensitivity(a),[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).
",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",2.1. Axiom: Sensitivity(a),[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,2.1. Axiom: Sensitivity(a),[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",2.1. Axiom: Sensitivity(a),[0],[0]
"We defer the specific counterexamples to Appendix B.
Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",2.1. Axiom: Sensitivity(a),[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,2.1. Axiom: Sensitivity(a),[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",2.1. Axiom: Sensitivity(a),[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",2.1. Axiom: Sensitivity(a),[0],[0]
"Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",2.2. Axiom: Implementation Invariance,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",2.2. Axiom: Implementation Invariance,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",2.2. Axiom: Implementation Invariance,[0],[0]
"Such a definition does not refer to implementation details.
",2.2. Axiom: Implementation Invariance,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B.
First, notice that gradients are invariant to implementation.",2.2. Axiom: Implementation Invariance,[0],[0]
"In fact, the chain-rule for gradients ∂f∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",2.2. Axiom: Implementation Invariance,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",2.2. Axiom: Implementation Invariance,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",2.2. Axiom: Implementation Invariance,[0],[0]
"This is exactly how backpropagation works.
",2.2. Axiom: Implementation Invariance,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,2.2. Axiom: Implementation Invariance,[0],[0]
"Un-
fortunately, the chain rule does not hold for discrete gradients in general.",2.2. Axiom: Implementation Invariance,[0],[0]
Formally f(x1)−f(x0)g(x1)−g(x0) 6=,2.2. Axiom: Implementation Invariance,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.
",2.2. Axiom: Implementation Invariance,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",2.2. Axiom: Implementation Invariance,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",2.2. Axiom: Implementation Invariance,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",2.2. Axiom: Implementation Invariance,[0],[0]
It is undesirable that attributions differ for such reasons.,2.2. Axiom: Implementation Invariance,[0],[0]
We are now ready to describe our technique.,3. Our Method: Integrated Gradients,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.
",3. Our Method: Integrated Gradients,[0],[0]
"Formally, suppose we have a function F :",3. Our Method: Integrated Gradients,[0],[0]
"Rn → [0, 1] that represents a deep network.",3. Our Method: Integrated Gradients,[0],[0]
"Specifically, let x ∈",3. Our Method: Integrated Gradients,[0],[0]
"Rn be the input at hand, and x′ ∈ Rn be the baseline input.",3. Our Method: Integrated Gradients,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.
",3. Our Method: Integrated Gradients,[0],[0]
"We consider the straightline path (in Rn) from the baseline x′ to the input x, and compute the gradients at all points along the path.",3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,3. Our Method: Integrated Gradients,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.
The integrated gradient along the ith dimension for an input x and baseline x′ is defined as follows.",3. Our Method: Integrated Gradients,[0],[0]
"Here, ∂F (x)∂xi is the gradient of F (x) along the ith dimension.
",3. Our Method: Integrated Gradients,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,3. Our Method: Integrated Gradients,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),3. Our Method: Integrated Gradients,[0],[0]
"∂xi dα
(1)",3. Our Method: Integrated Gradients,[0],[0]
Axiom: Completeness.,3. Our Method: Integrated Gradients,[0],[0]
"Integrated gradients satisfy an
axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x′.",3. Our Method: Integrated Gradients,[0],[0]
This axiom is identified as being desirable by Deeplift and LRP.,3. Our Method: Integrated Gradients,[0],[0]
"It is a sanity check that the attribution method is somewhat comprehensive in its accounting, a property that is clearly desirable if the networks score is used in a numeric sense, and not just to pick the top label, for e.g., a model estimating insurance premiums from credit features of individuals.
",3. Our Method: Integrated Gradients,[0],[0]
"This is formalized by the proposition below, which instantiates the fundamental theorem of calculus for path integrals.
",3. Our Method: Integrated Gradients,[0],[0]
Proposition 1.,3. Our Method: Integrated Gradients,[0],[0]
"If F : Rn → R is differentiable almost everywhere 1 then
Σni=1IntegratedGradsi(x)",3. Our Method: Integrated Gradients,[0],[0]
"= F (x)− F (x′)
",3. Our Method: Integrated Gradients,[0],[0]
"For most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero (F (x′)",3. Our Method: Integrated Gradients,[0],[0]
≈ 0).,3. Our Method: Integrated Gradients,[0],[0]
"(For image models, the black image baseline indeed satisfies this property.)",3. Our Method: Integrated Gradients,[0],[0]
"In such cases, there is an intepretation of the resulting attributions that ignores the baseline and amounts to distributing the output to the individual input features.
",3. Our Method: Integrated Gradients,[0],[0]
Remark 2.,3. Our Method: Integrated Gradients,[0],[0]
Integrated gradients satisfies Sensivity(a) because Completeness implies Sensivity(a) and is thus a strengthening of the Sensitivity(a) axiom.,3. Our Method: Integrated Gradients,[0],[0]
"This is because Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable.",3. Our Method: Integrated Gradients,[0],[0]
Attributions generated by integrated gradients satisfy Implementation Invariance since they are based only on the gradients of the function represented by the network.,3. Our Method: Integrated Gradients,[0],[0]
Prior literature has relied on empirically evaluating the attribution technique.,4. Uniqueness of Integrated Gradients,[0],[0]
"For instance, in the context of an object recognition task, (Samek et al., 2015) suggests that we select the top k pixels by attribution and randomly vary their intensities and then measure the drop in score.",4. Uniqueness of Integrated Gradients,[0],[0]
"If the attribution method is good, then the drop in score should be large.",4. Uniqueness of Integrated Gradients,[0],[0]
"However, the images resulting from pixel perturbation could be unnatural, and it could be that the scores drop simply because the network has never seen anything like it in training.",4. Uniqueness of Integrated Gradients,[0],[0]
"(This is less of a concern with linear or logistic models where the simplicity of the model ensures that ablating a feature does not cause strange interactions.)
",4. Uniqueness of Integrated Gradients,[0],[0]
"A different evaluation technique considers images with human-drawn bounding boxes around objects, and computes the percentage of pixel attribution inside the box.",4. Uniqueness of Integrated Gradients,[0],[0]
"While for most objects, one would expect the pixels located on the object to be most important for the prediction, in some cases the context in which the object occurs may also contribute to the prediction.",4. Uniqueness of Integrated Gradients,[0],[0]
"The cabbage butterfly image from Figure 2 is a good example of this where the pixels on the leaf are also surfaced by the integrated gradients.
",4. Uniqueness of Integrated Gradients,[0],[0]
"Roughly, we found that every empirical evaluation technique we could think of could not differentiate between ar-
1Formally, this means the function F is continuous everywhere and the partial derivative of F along each input dimension satisfies Lebesgue’s integrability condition, i.e., the set of discontinuous points has measure zero.",4. Uniqueness of Integrated Gradients,[0],[0]
"Deep networks built out of Sigmoids, ReLUs, and pooling operators satisfy this condition.
tifacts that stem from perturbing the data, a misbehaving model, and a misbehaving attribution method.",4. Uniqueness of Integrated Gradients,[0],[0]
This was why we turned to an axiomatic approach in designing a good attribution method (Section 2).,4. Uniqueness of Integrated Gradients,[0],[0]
"While our method satisfies Sensitivity and Implementation Invariance, it certainly isn’t the unique method to do so.
",4. Uniqueness of Integrated Gradients,[0],[0]
We now justify the selection of the integrated gradients method in two steps.,4. Uniqueness of Integrated Gradients,[0],[0]
"First, we identify a class of methods called Path methods that generalize integrated gradients.",4. Uniqueness of Integrated Gradients,[0],[0]
We discuss that path methods are the only methods to satisfy certain desirable axioms.,4. Uniqueness of Integrated Gradients,[0],[0]
"Second, we argue why integrated gradients is somehow canonical among the different path methods.",4. Uniqueness of Integrated Gradients,[0],[0]
Integrated gradients aggregate the gradients along the inputs that fall on the straightline between the baseline and the input.,4.1. Path Methods,[0],[0]
"There are many other (non-straightline) paths that monotonically interpolate between the two points, and each such path will yield a different attribution method.",4.1. Path Methods,[0],[0]
"For instance, consider the simple case when the input is two dimensional.",4.1. Path Methods,[0],[0]
"Figure 1 has examples of three paths, each of which corresponds to a different attribution method.
",4.1. Path Methods,[0],[0]
"Formally, let γ = (γ1, . . .",4.1. Path Methods,[0],[0]
", γn) :",4.1. Path Methods,[0],[0]
"[0, 1] → Rn be a smooth function specifying a path in Rn from the baseline x′ to the input x, i.e., γ(0)",4.1. Path Methods,[0],[0]
"= x′ and γ(1) = x.
Given a path function γ, path integrated gradients are obtained by integrating the gradients along the path γ(α) for α ∈",4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
"Formally, path integrated gradients along the ith dimension for an input x is defined as follows.
",4.1. Path Methods,[0],[0]
PathIntegratedGradsγi (x) ::= ∫ 1 α=0 ∂F (γ(α)) ∂γi(α) ∂γi(α) ∂α,4.1. Path Methods,[0],[0]
"dα
(2) where ∂F (x)∂xi is the gradient of F along the i
th dimension at x.
Attribution methods based on path integrated gradients are
collectively known as path methods.",4.1. Path Methods,[0],[0]
Notice that integrated gradients is a path method for the straightline path specified γ(α) = x′ + α× (x− x′) for α ∈,4.1. Path Methods,[0],[0]
"[0, 1].",4.1. Path Methods,[0],[0]
Remark 3.,4.1. Path Methods,[0],[0]
All path methods satisfy Implementation Invariance.,4.1. Path Methods,[0],[0]
"This follows from the fact that they are defined using the underlying gradients, which do not depend on the implementation.",4.1. Path Methods,[0],[0]
"They also satisfy Completeness (the proof is similar to that of Proposition 1) and Sensitvity(a) which is implied by Completeness (see Remark 2).
",4.1. Path Methods,[0],[0]
"More interestingly, path methods are the only methods that satisfy certain desirable axioms.",4.1. Path Methods,[0],[0]
"(For formal definitions of the axioms and proof of Proposition 2, see Friedman (Friedman, 2004).)
",4.1. Path Methods,[0],[0]
Axiom: Sensitivity(b).,4.1. Path Methods,[0],[0]
"(called Dummy in (Friedman, 2004))",4.1. Path Methods,[0],[0]
"If the function implemented by the deep network does not depend (mathematically) on some variable, then the attribution to that variable is always zero.
",4.1. Path Methods,[0],[0]
This is a natural complement to the definition of Sensitivity(a) from Section 2.,4.1. Path Methods,[0],[0]
"This definition captures desired insensitivity of the attributions.
",4.1. Path Methods,[0],[0]
Axiom: Linearity.,4.1. Path Methods,[0],[0]
"Suppose that we linearly composed two deep networks modeled by the functions f1 and f2 to form a third network that models the function a×f1+b×f2, i.e., a linear combination of the two networks.",4.1. Path Methods,[0],[0]
Then we’d like the attributions for a× f1 + b× f2 to be the weighted sum of the attributions for f1 and f2 with weights a and b respectively.,4.1. Path Methods,[0],[0]
"Intuitively, we would like the attributions to preserve any linearity within the network.",4.1. Path Methods,[0],[0]
Proposition 2.,4.1. Path Methods,[0],[0]
"(Theorem 1 (Friedman, 2004))",4.1. Path Methods,[0],[0]
"Path methods are the only attribution methods that always satisfy Implementation Invariance, Sensitivity(b), Linearity, and Completeness.",4.1. Path Methods,[0],[0]
Remark 4.,4.1. Path Methods,[0],[0]
"We note that these path integrated gradients have been used within the cost-sharing literature in economics where the function models the cost of a project as a function of the demands of various participants, and the attributions correspond to cost-shares.",4.1. Path Methods,[0],[0]
"Integrated gradients correspond to a cost-sharing method called AumannShapley (Aumann & Shapley, 1974).",4.1. Path Methods,[0],[0]
Proposition 2 holds for our attribution problem because mathematically the cost-sharing problem corresponds to the attribution problem with the benchmark fixed at the zero vector.,4.1. Path Methods,[0],[0]
(Implementation Invariance is implicit in the cost-sharing literature as the cost functions are considered directly in their mathematical form.),4.1. Path Methods,[0],[0]
"In this section, we formalize why the straightline path chosen by integrated gradients is canonical.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"First, observe that it is the simplest path that one can define mathematically.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Second, a natural property for attribution methods is to preserve symmetry, in the following sense.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Symmetry-Preserving.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Two input variables are symmetric w.r.t.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
a function if swapping them does not change the function.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For instance, x and y are symmetric w.r.t.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
F,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"if and only if F (x, y) = F (y, x) for all values of x and y.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"An attribution method is symmetry preserving, if for all inputs that have identical values for symmetric variables and baselines that have identical values for symmetric variables, the symmetric variables receive identical attributions.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"E.g., consider the logistic model Sigmoid(x1 + x2 + . . . ).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
x1 and x2 are symmetric variables for this model.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"For an input where x1 = x2 = 1 (say) and baseline where x1 = x2 = 0 (say), a symmetry preserving method must offer identical attributions to x1 and x2.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"It seems natural to ask for symmetry-preserving attribution methods because if two variables play the exact same role in the network (i.e., they are symmetric and have the same values in the baseline and the input) then they ought to receive the same attrbiution.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Theorem 1.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Integrated gradients is the unique path method that is symmetry-preserving.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"The proof is provided in Appendix A.
Remark 5.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If we allow averaging over the attributions from multiple paths, then are other methods that satisfy all the axioms in Theorem 1.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In particular, there is the method by Shapley-Shubik (Shapley & Shubik, 1971) from the cost sharing literature, and used by (Lundberg & Lee, 2016; Datta et al., 2016) to compute feature attributions (though they were not studying deep networks).",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In this method, the attribution is the average of those from n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
extremal paths; here n is the number of features.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Here each such path considers an ordering of the input features, and sequentially changes the input feature from its value at the baseline to its value at the input.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
This method yields attributions that are different from integrated gradients.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"If the function of interest is min(x1, x2), the baseline is x1 = x2 = 0, and the input is x1 = 1, x2 = 3, then integrated gradients attributes the change in the function value entirely to the critical variable x1, whereas Shapley-Shubik assigns attributions of 1/2 each; it seems somewhat subjective to prefer one result over the other.
",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"We also envision other issues with applying Shapley-Shubik to deep networks: It is computationally expensive; in an object recognition network that takes an 100X100 image as input, n is 10000, and n!",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
is a gigantic number.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Even if one samples few paths randomly, evaluating the attributions for a single path takes n calls to the deep network.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"In contrast, integrated gradients is able to operate with 20 to 300 calls.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
"Further, the Shapley-Shubik computation visit
inputs that are combinations of the input and the baseline.",4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
It is possible that some of these combinations are very different from anything seen during training.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
We speculate that this could lead to attribution artifacts.,4.2. Integrated Gradients is Symmetry-Preserving,[0],[0]
Selecting a Benchmark.,5. Applying Integrated Gradients,[0],[0]
A key step in applying integrated gradients is to select a good baseline.,5. Applying Integrated Gradients,[0],[0]
"We recommend that developers check that the baseline has a near-zero score— as discussed in Section 3, this allows us to interpret the attributions as a function of the input.",5. Applying Integrated Gradients,[0],[0]
But there is more to a good baseline:,5. Applying Integrated Gradients,[0],[0]
"For instance, for an object recogntion network it is possible to create an adversarial example that has a zero score for a given input label (say elephant), by applying a tiny, carefully-designed perturbation to an image with a very different label (say microscope) (cf.",5. Applying Integrated Gradients,[0],[0]
"(Goodfellow et al., 2015)).",5. Applying Integrated Gradients,[0],[0]
The attributions can then include undesirable artifacts of this adversarially constructed baseline.,5. Applying Integrated Gradients,[0],[0]
"So we would additionally like the baseline to convey a complete absence of signal, so that the features that are apparent from the attributions are properties only of the input, and not of the baseline.",5. Applying Integrated Gradients,[0],[0]
"For instance, in an object recognition network, a black image signifies the absence of objects.",5. Applying Integrated Gradients,[0],[0]
The black image isn’t unique in this sense—an image consisting of noise has the same property.,5. Applying Integrated Gradients,[0],[0]
"However, using black as a baseline may result in cleaner visualizations of “edge” features.",5. Applying Integrated Gradients,[0],[0]
"For text based networks, we have found that the allzero input embedding vector is a good baseline.",5. Applying Integrated Gradients,[0],[0]
"The action of training causes unimportant words tend to have small norms, and so, in the limit, unimportance corresponds to the all-zero baseline.",5. Applying Integrated Gradients,[0],[0]
"Notice that the black image corresponds to a valid input to an object recognition network, and is also intuitively what we humans would consider absence of signal.",5. Applying Integrated Gradients,[0],[0]
"In contrast, the all-zero input vector for a text network does not correspond to a valid input; it nevertheless works for the mathematical reason described above.
",5. Applying Integrated Gradients,[0],[0]
Computing Integrated Gradients.,5. Applying Integrated Gradients,[0],[0]
The integral of integrated gradients can be efficiently approximated via a summation.,5. Applying Integrated Gradients,[0],[0]
"We simply sum the gradients at points occurring at sufficiently small intervals along the straightline path from the baseline x′ to the input x.
IntegratedGradsapproxi (x) ::=
(xi − x′i)× Σmk=1 ∂F (x′+ k",5. Applying Integrated Gradients,[0],[0]
m×(x−x ′))),5. Applying Integrated Gradients,[0],[0]
"∂xi × 1m
(3)
",5. Applying Integrated Gradients,[0],[0]
Here m is the number of steps in the Riemman approximation of the integral.,5. Applying Integrated Gradients,[0],[0]
Notice that the approximation simply involves computing the gradient in a for loop which should be straightforward and efficient in most deep learning frameworks.,5. Applying Integrated Gradients,[0],[0]
"For instance, in TensorFlow, it amounts to calling tf.gradients in a loop over the set of inputs (i.e., x′ + km × (x − x ′) for k = 1, . . .",5. Applying Integrated Gradients,[0],[0]
",m), which
could also be batched.",5. Applying Integrated Gradients,[0],[0]
"In practice, we find that somewhere between 20 and 300 steps are enough to approximate the integral (within 5%); we recommend that developers check that the attributions approximately adds up to the difference beween the score at the input and that at the baseline (cf. Proposition 1), and if not increase the step-size m.",5. Applying Integrated Gradients,[0],[0]
The integrated gradients technique is applicable to a variety of deep networks.,6. Applications,[0],[0]
"Here, we apply it to two image models, two natural language models, and a chemistry model.",6. Applications,[0],[0]
"We study feature attribution in an object recognition network built using the GoogleNet architecture (Szegedy et al., 2014) and trained over the ImageNet object recognition dataset (Russakovsky et al., 2015).",6.1. An Object Recognition Network,[0],[0]
We use the integrated gradients method to study pixel importance in predictions made by this network.,6.1. An Object Recognition Network,[0],[0]
The gradients are computed for the output of the highest-scoring class with respect to pixel of the input image.,6.1. An Object Recognition Network,[0],[0]
"The baseline input is the black image, i.e., all pixel intensities are zero.
",6.1. An Object Recognition Network,[0],[0]
Integrated gradients can be visualized by aggregating them along the color channel and scaling the pixels in the actual image by them.,6.1. An Object Recognition Network,[0],[0]
Figure 2 shows visualizations for a bunch of images2.,6.1. An Object Recognition Network,[0],[0]
"For comparison, it also presents the corresponding visualization obtained from the product of the image with the gradients at the actual image.",6.1. An Object Recognition Network,[0],[0]
Notice that integrated gradients are better at reflecting distinctive features of the input image.,6.1. An Object Recognition Network,[0],[0]
Diabetic retinopathy (DR) is a complication of the diabetes that affects the eyes.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"Recently, a deep network (Gulshan et al., 2016) has been proposed to predict the severity grade for DR in retinal fundus images.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"The model has good predictive accuracy on various validation datasets.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
"We use integrated gradients to study feature importance for this network; like in the object recognition case, the baseline is the black image.",6.2. Diabetic Retinopathy Prediction,[0],[0]
"Feature importance explanations are important for this network as retina specialists may use it to build trust in the network’s predictions, decide the grade for borderline cases, and obtain insights for further testing and screening.
",6.2. Diabetic Retinopathy Prediction,[0],[0]
Figure 3 shows a visualization of integrated gradients for a retinal fundus image.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The visualization method is a bit different from that used in Figure 2.,6.2. Diabetic Retinopathy Prediction,[0],[0]
"We aggregate integrated gradients along the color channel and overlay them on the
2More examples can be found at https://github.com/ ankurtaly/Attributions
actual image in gray scale with positive attribtutions along the green channel and negative attributions along the red channel.",6.2. Diabetic Retinopathy Prediction,[0],[0]
Notice that integrated gradients are localized to a few pixels that seem to be lesions in the retina.,6.2. Diabetic Retinopathy Prediction,[0],[0]
The interior of the lesions receive a negative attribution while the periphery receives a positive attribution indicating that the network focusses on the boundary of the lesion.,6.2. Diabetic Retinopathy Prediction,[0],[0]
Automatically answering natural language questions (over semi-structured data) is an important problem in artificial intelligence (AI).,6.3. Question Classification,[0],[0]
"A common approach is to semantically parse the question to its logical form (Liang, 2016) using a set of human-authored grammar rules.",6.3. Question Classification,[0],[0]
An alternative approach is to machine learn an end-to-end model provided there is enough training data.,6.3. Question Classification,[0],[0]
An interesting question is whether one could peek inside machine learnt models to derive new rules.,6.3. Question Classification,[0],[0]
"We explore this direction for a sub-problem of semantic parsing, called question classification, using the method of integrated gradients.
",6.3. Question Classification,[0],[0]
The goal of question classification is to identify the type of answer it is seeking.,6.3. Question Classification,[0],[0]
"For instance, is the quesiton seeking a yes/no answer, or is it seeking a date?",6.3. Question Classification,[0],[0]
"Rules for solving this problem look for trigger phrases in the question, for e.g., a “when” in the beginning indicates a date seeking question.",6.3. Question Classification,[0],[0]
"We train a model for question classification using the the text categorization architecture proposed by (Kim, 2014) over the WikiTableQuestions dataset (Pasupat & Liang, 2015).",6.3. Question Classification,[0],[0]
We use integrated gradients to attribute predictions down to the question terms in order to identify new trigger phrases for answer type.,6.3. Question Classification,[0],[0]
"The baseline input is the all zero embedding vector.
",6.3. Question Classification,[0],[0]
Figure 4 lists a few questions with constituent terms highlighted based on their attribution.,6.3. Question Classification,[0],[0]
"Notice that the attributions largely agree with commonly used rules, for e.g., “how many” indicates a numeric seeking question.",6.3. Question Classification,[0],[0]
"In addition, attributions help identify novel question classification rules, for e.g., questions containing “total number” are seeking numeric answers.",6.3. Question Classification,[0],[0]
"Attributions also point out undesirable correlations, for e.g., “charles” is used as trigger for a yes/no question.",6.3. Question Classification,[0],[0]
"We applied our technique to a complex, LSTM-based Neural Machine Translation System (Wu et al., 2016).",6.4. Neural Machine Translation,[0],[0]
We attribute the output probability of every output token (in form of wordpieces) to the input tokens.,6.4. Neural Machine Translation,[0],[0]
Such attributions “align” the output sentence with the input sentence.,6.4. Neural Machine Translation,[0],[0]
"For
baseline, we zero out the embeddings of all tokens except the start and end markers.",6.4. Neural Machine Translation,[0],[0]
Figure 5 shows an example of such an attribution-based alignments.,6.4. Neural Machine Translation,[0],[0]
We observed that the results make intuitive sense.,6.4. Neural Machine Translation,[0],[0]
"E.g. “und” is mostly attributed to “and”, and “morgen” is mostly attributed to “morning”.",6.4. Neural Machine Translation,[0],[0]
We use 100 − 1000 steps (cf. Section 5) in the integrated gradient approximation; we need this because the network is highly nonlinear.,6.4. Neural Machine Translation,[0],[0]
"We apply integrated gradients to a network performing Ligand-Based Virtual Screening which is the problem of predicting whether an input molecule is active against a certain target (e.g., protein or enzyme).",6.5. Chemistry Models,[0],[0]
"In particular, we consider a network based on the molecular graph convolution architecture proposed by (Kearnes et al., 2016).
",6.5. Chemistry Models,[0],[0]
The network requires an input molecule to be encoded by hand as a set of atom and atom-pair features describing the molecule as an undirected graph.,6.5. Chemistry Models,[0],[0]
"Atoms are featurized using a one-hot encoding specifying the atom type (e.g., C, O, S, etc.), and atom-pairs are featurized by specifying either the type of bond (e.g., single, double, triple, etc.) between the atoms, or the graph distance between them.",6.5. Chemistry Models,[0],[0]
"The baseline input is obtained zeroing out the feature vectors for atom and atom-pairs.
",6.5. Chemistry Models,[0],[0]
We visualize integrated gradients as heatmaps over the the atom and atom-pair features with the heatmap intensity depicting the strength of the contribution.,6.5. Chemistry Models,[0],[0]
Figure 6 shows the visualization for a specific molecule.,6.5. Chemistry Models,[0],[0]
"Since integrated gradients add up to the final prediction score (see Proposition 1), the magnitudes can be use for accounting the contributions of each feature.",6.5. Chemistry Models,[0],[0]
"For instance, for the molecule in the figure, atom-pairs that have a bond between them cumulatively contribute to 46% of the prediction score, while all other pairs cumulatively contribute to only −3%.
",6.5. Chemistry Models,[0],[0]
Identifying Degenerate Features.,6.5. Chemistry Models,[0],[0]
"We now discuss how attributions helped us spot an anomaly in the W1N2 architecture in (Kearnes et al., 2016).",6.5. Chemistry Models,[0],[0]
"On applying the integrated gradients method to this network, we found that several atoms in the same molecule received identical attribution despite being bonded to different atoms.",6.5. Chemistry Models,[0],[0]
"This is surprising as one would expect two atoms with different neighborhoods to be treated differently by the network.
",6.5. Chemistry Models,[0],[0]
"On investigating the problem further, in the network architecture, the atoms and atom-pair features were not fully convolved.",6.5. Chemistry Models,[0],[0]
"This caused all atoms that have the same atom type, and same number of bonds of each type to contribute identically to the network.",6.5. Chemistry Models,[0],[0]
We already covered closely related work on attribution in Section 2.,7. Other Related work,[0],[0]
We mention other related work.,7. Other Related work,[0],[0]
"Over the last few years, there has been a vast amount work on demystifying the inner workings of deep networks.",7. Other Related work,[0],[0]
"Most of this work has been on networks trained on computer vision tasks, and deals with understanding what a specific neuron computes (Erhan et al., 2009; Le, 2013) and interpreting the representations captured by neurons during a prediction (Mahendran & Vedaldi, 2015; Dosovitskiy & Brox, 2015; Yosinski et al., 2015).",7. Other Related work,[0],[0]
"In contrast, we focus on understanding the network’s behavior on a specific input in terms of the base level input features.",7. Other Related work,[0],[0]
"Our technique quantifies the importance of each feature in the prediction.
",7. Other Related work,[0],[0]
"One approach to the attribution problem proposed first by (Ribeiro et al., 2016a;b), is to locally approximate the behavior of the network in the vicinity of the input being explained with a simpler, more interpretable model.",7. Other Related work,[0],[0]
An appealing aspect of this approach is that it is completely agnostic to the implementation of the network and satisfies implemenation invariance.,7. Other Related work,[0],[0]
"However, this approach does not guarantee sensitivity.",7. Other Related work,[0],[0]
"There is no guarantee that the local region explored escapes the “flat” section of the pre-
diction function in the sense of Section 2.",7. Other Related work,[0],[0]
The other issue is that the method is expensive to implement for networks with “dense” input like image networks as one needs to explore a local region of size proportional to the number of pixels and train a model for this space.,7. Other Related work,[0],[0]
"In contrast, our technique works with a few calls to the gradient operation.
",7. Other Related work,[0],[0]
"Attention mechanisms (Bahdanau et al., 2014) have gained popularity recently.",7. Other Related work,[0],[0]
"One may think that attention could be used a proxy for attributions, but this has issues.",7. Other Related work,[0],[0]
"For instance, in a LSTM that also employs attention, there are many ways for an input token to influence an output token: the memory cell, the recurrent state, and “attention”.",7. Other Related work,[0],[0]
Focussing only an attention ignores the other modes of influence and results in an incomplete picture.,7. Other Related work,[0],[0]
The primary contribution of this paper is a method called integrated gradients that attributes the prediction of a deep network to its inputs.,8. Conclusion,[0],[0]
"It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.
",8. Conclusion,[0],[0]
A secondary contribution of this paper is to clarify desirable features of an attribution method using an axiomatic framework inspired by cost-sharing literature from economics.,8. Conclusion,[0],[0]
"Without the axiomatic approach it is hard to tell whether the attribution method is affected by data artifacts, network’s artifacts or artifacts of the method.",8. Conclusion,[0],[0]
"The axiomatic approach rules out artifacts of the last type.
",8. Conclusion,[0],[0]
"While our and other works have made some progress on understanding the relative importance of input features in a deep network, we have not addressed the interactions between the input features or the logic employed by the network.",8. Conclusion,[0],[0]
So there remain many unanswered questions in terms of debugging the I/O behavior of a deep network.,8. Conclusion,[0],[0]
"We would like to thank Samy Bengio, Kedar Dhamdhere, Scott Lundberg, Amir Najmi, Kevin McCurley, Patrick Riley, Christian Szegedy, Diane Tang for their feedback.",ACKNOWLEDGMENTS,[0],[0]
We would like to thank Daniel Smilkov and Federico Allocati for identifying bugs in our descriptions.,ACKNOWLEDGMENTS,[0],[0]
"We would like to thank our anonymous reviewers for identifying bugs, and their suggestions to improve presentation.",ACKNOWLEDGMENTS,[0],[0]
Proof.,A. Proof of Theorem 1,[0],[0]
Consider a non-straightline path γ :,A. Proof of Theorem 1,[0],[0]
"[0, 1] → Rn from baseline to input.",A. Proof of Theorem 1,[0],[0]
"W.l.o.g., there exists t0 ∈",A. Proof of Theorem 1,[0],[0]
"[0, 1] such that for two dimensions i, j, γi(t0) > γj(t0).",A. Proof of Theorem 1,[0],[0]
"Let (t1, t2) be the maximum real open interval containing t0 such that γi(t) > γj(t) for all t in (t1, t2), and let a = γi(t1) = γj(t1), and b = γi(t2) = γj(t2).",A. Proof of Theorem 1,[0],[0]
Define function f,A. Proof of Theorem 1,[0],[0]
: x ∈,A. Proof of Theorem 1,[0],[0]
"[0, 1]n → R as 0 if min(xi, xj) ≤ a, as (b − a)2 if max(xi, xj) ≥ b, and as (xi − a)(xj − a) otherwise.",A. Proof of Theorem 1,[0],[0]
"Next we compute the attributions of f at x = 〈1, . . .",A. Proof of Theorem 1,[0],[0]
", 1〉n with baseline x′ = 〈0, . . .",A. Proof of Theorem 1,[0],[0]
", 0〉n.",A. Proof of Theorem 1,[0],[0]
"Note that xi and xj are symmetric, and should get identical attributions.",A. Proof of Theorem 1,[0],[0]
For t /∈,A. Proof of Theorem 1,[0],[0]
"[t1, t2], the function is a constant, and the attribution of f is zero to all variables, while for t ∈ (t1, t2), the integrand of attribution of f is γj(t)",A. Proof of Theorem 1,[0],[0]
"− a to xi, and γi(t)",A. Proof of Theorem 1,[0],[0]
"− a to xj , where the latter is always strictly larger by our choice of the interval.",A. Proof of Theorem 1,[0],[0]
"Integrating, it follows that xj gets a larger attribution than xi, contradiction.",A. Proof of Theorem 1,[0],[0]
"We show that the methods DeepLift and Layer-wise relevance propagation (LRP) break the implementation invariance axiom, and the Deconvolution and Guided backpropagation methods break the sensitivity axiom.
",B. Attribution Counter-Examples,[0],[0]
"Figure 7 provides an example of two equivalent networks
f(x1, x2) and g(x1, x2) for which DeepLift and LRP yield different attributions.
",B. Attribution Counter-Examples,[0],[0]
"First, observe that the networks f and g are of the form f(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(h(x1, x2)) and f(x1, x2) = ReLU(k(x1, x2))",B. Attribution Counter-Examples,[0],[0]
"3, where
h(x1, x2) =",B. Attribution Counter-Examples,[0],[0]
"ReLU(x1)− 1− ReLU(x2) k(x1, x2)",B. Attribution Counter-Examples,[0],[0]
"= ReLU(x1 − 1)− ReLU(x2)
",B. Attribution Counter-Examples,[0],[0]
Note that h and k are not equivalent.,B. Attribution Counter-Examples,[0],[0]
They have different values whenever x1 < 1.,B. Attribution Counter-Examples,[0],[0]
But f and g are equivalent.,B. Attribution Counter-Examples,[0],[0]
"To prove this, suppose for contradiction that f and g are different for some x1, x2.",B. Attribution Counter-Examples,[0],[0]
Then it must be the case that ReLU(x1)− 1 6= ReLU(x1 − 1).,B. Attribution Counter-Examples,[0],[0]
"This happens only when x1 < 1, which implies that f(x1, x2) = g(x1, x2) = 0.
",B. Attribution Counter-Examples,[0],[0]
Now we leverage the above example to show that Deconvolution and Guided back-propagation break sensitivity.,B. Attribution Counter-Examples,[0],[0]
"Consider the network f(x1, x2) from Figure 7.",B. Attribution Counter-Examples,[0],[0]
"For a fixed value of x1 greater than 1, the output decreases linearly as x2 increases from 0 to x1 − 1.",B. Attribution Counter-Examples,[0],[0]
"Yet, for all inputs, Deconvolutional networks and Guided back-propagation results in zero attribution for x2.",B. Attribution Counter-Examples,[0],[0]
"This happens because for all inputs the back-propagated signal received at the node ReLU(x2) is negative and is therefore not back-propagated through the ReLU operation (per the rules of deconvolution and guided back-propagation; see (Springenberg et al., 2014) for details).",B. Attribution Counter-Examples,[0],[0]
"As a result, the feature x2 receives zero
3 ReLU(x) is defined as max(x, 0).
",B. Attribution Counter-Examples,[0],[0]
attribution despite the network’s output being sensitive to it.,B. Attribution Counter-Examples,[0],[0]
"We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works.",abstractText,[0],[0]
We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy.,abstractText,[0],[0]
"We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods.",abstractText,[0],[0]
We use the axioms to guide the design of a new attribution method called Integrated Gradients.,abstractText,[0],[0]
Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator.,abstractText,[0],[0]
"We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Motivation and Summary of Results We study the problem of attributing the prediction of a deep network to its input features.,abstractText,[0],[0]
Definition 1.,abstractText,[0],[0]
"Formally, suppose we have a function F :",abstractText,[0],[0]
R,abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network, and an input x =",abstractText,[0],[0]
"(x1, . . .",abstractText,[0],[0]
", xn) ∈",abstractText,[0],[0]
R.,abstractText,[0],[0]
"An attribution of the prediction at input x relative to a baseline input x′ is a vector AF (x, x ′) =",abstractText,[0],[0]
"(a1, . . .",abstractText,[0],[0]
", an) ∈ R where ai is the contribution of xi to the prediction F (x).",abstractText,[0],[0]
"For instance, in an object recognition network, an attribution method could tell us which pixels of the image were responsible for a certain label being picked (see Figure 2).",abstractText,[0],[0]
"The attribution problem was previously studied by various papers (Baehrens et al., 2010; Simonyan et al., 2013; Equal contribution Google Inc., Mountain View, USA.",abstractText,[0],[0]
Correspondence to: Mukund Sundararajan <mukunds@google.com,abstractText,[0],[0]
">, Ankur Taly <ataly@google.com>.",abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
"Shrikumar et al., 2016; Binder et al., 2016; Springenberg et al., 2014).",abstractText,[0],[0]
"The intention of these works is to understand the inputoutput behavior of the deep network, which gives us the ability to improve it.",abstractText,[0],[0]
"Such understandability is critical to all computer programs, including machine learning models.",abstractText,[0],[0]
There are also other applications of attribution.,abstractText,[0],[0]
They could be used within a product driven by machine learning to provide a rationale for the recommendation.,abstractText,[0],[0]
"For instance, a deep network that predicts a condition based on imaging could help inform the doctor of the part of the image that resulted in the recommendation.",abstractText,[0],[0]
This could help the doctor understand the strengths and weaknesses of a model and compensate for it.,abstractText,[0],[0]
We give such an example in Section 6.2.,abstractText,[0],[0]
Attributions could also be used by developers in an exploratory sense.,abstractText,[0],[0]
"For instance, we could use a deep network to extract insights that could be then used in a rulebased system.",abstractText,[0],[0]
"In Section 6.3, we give such an example.",abstractText,[0],[0]
A significant challenge in designing an attribution technique is that they are hard to evaluate empirically.,abstractText,[0],[0]
"As we discuss in Section 4, it is hard to tease apart errors that stem from the misbehavior of the model versus the misbehavior of the attribution method.",abstractText,[0],[0]
"To compensate for this shortcoming, we take an axiomatic approach.",abstractText,[0],[0]
In Section 2 we identify two axioms that every attribution method must satisfy.,abstractText,[0],[0]
Unfortunately most previous methods do not satisfy one of these two axioms.,abstractText,[0],[0]
"In Section 3, we use the axioms to identify a new method, called integrated gradients.",abstractText,[0],[0]
"Unlike previously proposed methods, integrated gradients do not need any instrumentation of the network, and can be computed easily using a few calls to the gradient operation, allowing even novice practitioners to easily apply the technique.",abstractText,[0],[0]
"In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network.",abstractText,[0],[0]
"These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.",abstractText,[0],[0]
Remark 1.,abstractText,[0],[0]
Let us briefly examine the need for the baseline in the definition of the attribution problem.,abstractText,[0],[0]
A common way for humans to perform attribution relies on counterar X iv :1 70 3.,abstractText,[0],[0]
01,abstractText,[0],[0]
36 5v 2,abstractText,[0],[0]
[ cs .L G ] 1 3 Ju n 20 17 Axiomatic Attribution for Deep Networks factual intuition.,abstractText,[0],[0]
When we assign blame to a certain cause we implicitly consider the absence of the cause as a baseline for comparing outcomes.,abstractText,[0],[0]
"In a deep network, we model the absence using a single baseline input.",abstractText,[0],[0]
"For most deep networks, a natural baseline exists in the input space where the prediction is neutral.",abstractText,[0],[0]
"For instance, in object recognition networks, it is the black image.",abstractText,[0],[0]
"The need for a baseline has also been pointed out by prior work on attribution (Shrikumar et al., 2016; Binder et al., 2016).",abstractText,[0],[0]
2.,abstractText,[0],[0]
Two Fundamental Axioms We now discuss two axioms (desirable characteristics) for attribution methods.,abstractText,[0],[0]
We find that other feature attribution methods in literature break at least one of the two axioms.,abstractText,[0],[0]
"These methods include DeepLift (Shrikumar et al., 2016; 2017), Layer-wise relevance propagation (LRP) (Binder et al., 2016), Deconvolutional networks (Zeiler & Fergus, 2014), and Guided back-propagation (Springenberg et al., 2014).",abstractText,[0],[0]
"As we will see in Section 3, these axioms will also guide the design of our method.",abstractText,[0],[0]
Gradients.,abstractText,[0],[0]
"For linear models, ML practitioners regularly inspect the products of the model coefficients and the feature values in order to debug predictions.",abstractText,[0],[0]
"Gradients (of the output with respect to the input) is a natural analog of the model coefficients for a deep network, and therefore the product of the gradient and feature values is a reasonable starting point for an attribution method (Baehrens et al., 2010; Simonyan et al., 2013); see the third column of Figure 2 for examples.",abstractText,[0],[0]
"The problem with gradients is that they break sensitivity, a property that all attribution methods should satisfy.",abstractText,[0],[0]
2.1.,abstractText,[0],[0]
Axiom: Sensitivity(a),abstractText,[0],[0]
An attribution method satisfies Sensitivity(a),abstractText,[0],[0]
if for every input and baseline that differ in one feature but have different predictions then the differing feature should be given a non-zero attribution.,abstractText,[0],[0]
"(Later in the paper, we will have a part (b) to this definition.)",abstractText,[0],[0]
Gradients violate Sensitivity(a):,abstractText,[0],[0]
"For a concrete example, consider a one variable, one ReLU network, f(x) = 1 − ReLU(1−x).",abstractText,[0],[0]
Suppose the baseline is x = 0 and the input is x = 2.,abstractText,[0],[0]
"The function changes from 0 to 1, but because f becomes flat at x = 1, the gradient method gives attribution of 0 to x. Intuitively, gradients break Sensitivity because the prediction function may flatten at the input and thus have zero gradient despite the function value at the input being different from that at the baseline.",abstractText,[0],[0]
"This phenomenon has been reported in previous work (Shrikumar et al., 2016).",abstractText,[0],[0]
"Practically, the lack of sensitivity causes gradients to focus on irrelevant features (see the “fireboat” example in Figure 2).",abstractText,[0],[0]
Other back-propagation based approaches.,abstractText,[0],[0]
A second set of approaches involve back-propagating the final prediction score through each layer of the network down to the individual features.,abstractText,[0],[0]
"These include DeepLift, Layer-wise relevance propagation (LRP), Deconvolutional networks (DeConvNets), and Guided back-propagation.",abstractText,[0],[0]
"These methods differ in the specific backpropagation logic for various activation functions (e.g., ReLU, MaxPool, etc.).",abstractText,[0],[0]
"Unfortunately, Deconvolution networks (DeConvNets), and Guided back-propagation violate Sensitivity(a).",abstractText,[0],[0]
This is because these methods back-propogate through a ReLU node only if the ReLU is turned on at the input.,abstractText,[0],[0]
"This makes the method similar to gradients, in that, the attribution is zero for features with zero gradient at the input despite a non-zero gradient at the baseline.",abstractText,[0],[0]
"We defer the specific counterexamples to Appendix B. Methods like DeepLift and LRP tackle the Sensitivity issue by employing a baseline, and in some sense try to compute “discrete gradients” instead of (instantaeneous) gradients at the input.",abstractText,[0],[0]
(The two methods differ in the specifics of how they compute the discrete gradient).,abstractText,[0],[0]
"But the idea is that a large, discrete step will avoid flat regions, avoiding a breakage of sensitivity.",abstractText,[0],[0]
"Unfortunately, these methods violate a different requirement on attribution methods.",abstractText,[0],[0]
2.2.,abstractText,[0],[0]
"Axiom: Implementation Invariance Two networks are functionally equivalent if their outputs are equal for all inputs, despite having very different implementations.",abstractText,[0],[0]
"Attribution methods should satisfy Implementation Invariance, i.e., the attributions are always identical for two functionally equivalent networks.",abstractText,[0],[0]
"To motivate this, notice that attribution can be colloquially defined as assigning the blame (or credit) for the output to the input features.",abstractText,[0],[0]
Such a definition does not refer to implementation details.,abstractText,[0],[0]
"We now discuss intuition for why DeepLift and LRP break Implementation Invariance; a concrete example is provided in Appendix B. First, notice that gradients are invariant to implementation.",abstractText,[0],[0]
"In fact, the chain-rule for gradients ∂f ∂g = ∂f ∂h · ∂h ∂g is essentially about implementation invariance.",abstractText,[0],[0]
"To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system.",abstractText,[0],[0]
"The gradient of output f to input g can be computed either directly by ∂f ∂g , ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h.",abstractText,[0],[0]
This is exactly how backpropagation works.,abstractText,[0],[0]
Methods like LRP and DeepLift replace gradients with discrete gradients and still use a modified form of backpropagation to compose discrete gradients into attributions.,abstractText,[0],[0]
"UnAxiomatic Attribution for Deep Networks fortunately, the chain rule does not hold for discrete gradients in general.",abstractText,[0],[0]
Formally f(x1)−f(x0) g(x1)−g(x0) 6=,abstractText,[0],[0]
"f(x1)−f(x0) h(x1)−h(x0) · h(x1)−h(x0) g(x1)−g(x0) , and therefore these methods fail to satisfy implementation invariance.",abstractText,[0],[0]
"If an attribution method fails to satisfy Implementation Invariance, the attributions are potentially sensitive to unimportant aspects of the models.",abstractText,[0],[0]
"For instance, if the network architecture has more degrees of freedom than needed to represent a function then there may be two sets of values for the network parameters that lead to the same function.",abstractText,[0],[0]
"The training procedure can converge at either set of values depending on the initializtion or for other reasons, but the underlying network function would remain the same.",abstractText,[0],[0]
It is undesirable that attributions differ for such reasons.,abstractText,[0],[0]
3.,abstractText,[0],[0]
Our Method: Integrated Gradients We are now ready to describe our technique.,abstractText,[0],[0]
"Intuitively, our technique combines the Implementation Invariance of Gradients along with the Sensitivity of techniques like LRP or DeepLift.",abstractText,[0],[0]
"Formally, suppose we have a function F : R",abstractText,[0],[0]
→,abstractText,[0],[0]
"[0, 1] that represents a deep network.",abstractText,[0],[0]
"Specifically, let x ∈ R be the input at hand, and x′ ∈ R be the baseline input.",abstractText,[0],[0]
"For image networks, the baseline could be the black image, while for text models it could be the zero embedding vector.",abstractText,[0],[0]
"We consider the straightline path (in R) from the baseline x′ to the input x, and compute the gradients at all points along the path.",abstractText,[0],[0]
Integrated gradients are obtained by cumulating these gradients.,abstractText,[0],[0]
"Specifically, integrated gradients are defined as the path intergral of the gradients along the straightline path from the baseline x′ to the input x.",abstractText,[0],[0]
The integrated gradient along the i dimension for an input x and baseline x′ is defined as follows.,abstractText,[0],[0]
"Here, ∂F (x) ∂xi is the gradient of F (x) along the i dimension.",abstractText,[0],[0]
IntegratedGradsi(x) ::= (xi−x ′,abstractText,[0],[0]
i)× ∫ 1 α=0 ∂F (x′+α×(x−x′)),abstractText,[0],[0]
Axiomatic Attribution for Deep Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1863–1873 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1863
We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",text,[0],[0]
Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization.,1 Introduction,[0],[0]
"Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a
phrase-structure or dependency tree, then semantically analyzed.",1 Introduction,[0],[0]
"Pipelines, which make “hard” (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing.
",1 Introduction,[0],[0]
"Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (He et al., 2017; Oepen et al., 2017; Ji and Smith, 2017), we argue that pipelines can be treated as layers in neural architectures for NLP tasks.",1 Introduction,[0],[0]
"Several solutions are readily available: • Reinforcement learning (most notably the
REINFORCE algorithm; Williams, 1992), and structured attention (SA; Kim et al., 2017).",1 Introduction,[0],[0]
These methods replace argmax with a sampling or marginalization operation.,1 Introduction,[0],[0]
"We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors.",1 Introduction,[0],[0]
"• The straight-through estimator (STE; Hin-
ton, 2012) treats discrete decisions as if they were differentiable and simply passes through gradients.",1 Introduction,[0],[0]
"While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.",1 Introduction,[0],[0]
"We will find, experimentally, that the quality of intermediate representations degrades substantially under STE.
",1 Introduction,[0],[0]
"This paper introduces a new method, the structured projection of intermediate gradients optimization technique (SPIGOT; §2), which defines a proxy for the gradient of a loss function with respect to the input to argmax.",1 Introduction,[0],[0]
"Unlike STE’s gradient proxy, SPIGOT aims to respect the constraints
in the argmax problem.",1 Introduction,[0],[0]
"SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto.",1 Introduction,[0],[0]
"We show empirically that SPIGOT works even when the maximization and the projection are done approximately.
",1 Introduction,[0],[0]
"We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle (§3).",1 Introduction,[0],[0]
"These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task.",1 Introduction,[0],[0]
"The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.
",1 Introduction,[0],[0]
"Our experiments (§4) show that our architecture improves over a state-of-the-art semantic dependency parser, and that SPIGOT offers stronger performance than a pipeline, SA, and STE.",1 Introduction,[0],[0]
"On sentiment classification, we show that semantic parsing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives.",1 Introduction,[0],[0]
Our analysis considers how the behavior of the intermediate parser is affected by the end task (§5).,1 Introduction,[0],[0]
Our code is open-source and available at https:// github.com/Noahs-ARK/SPIGOT.,1 Introduction,[0],[0]
Our aim is to allow a (structured) argmax layer in a neural network to be treated almost like any other differentiable function.,2 Method,[0],[0]
"This would allow us to place, for example, a syntactic parser in the middle of a neural network, so that the forward calculation simply calls the parser and passes the parse tree to the next layer, which might derive syntactic features for the next stage of processing.
",2 Method,[0],[0]
"The challenge is in the backward computation, which is key to learning with standard gradientbased methods.",2 Method,[0],[0]
"When its output is discrete as we assume here, argmax is a piecewise constant function.",2 Method,[0],[0]
"At every point, its gradient is either zero or undefined.",2 Method,[0],[0]
"So instead of using the true gradient, we will introduce a proxy for the gradient of the loss function with respect to the inputs to argmax, allowing backpropagation to proceed through the argmax layer.",2 Method,[0],[0]
Our proxy is designed as an improvement to earlier methods (discussed below) that completely ignore constraints on the argmax operation.,2 Method,[0],[0]
"It accomplishes this through a projec-
tion of the gradients.",2 Method,[0],[0]
"We first lay out notation, and then briefly review max-decoding and its relaxation (§2.1).",2 Method,[0],[0]
"We define SPIGOT in §2.2, and show how to use it to backpropagate through NLP pipelines in §2.3.
Notation.",2 Method,[0],[0]
"Our discussion centers around two tasks: a structured intermediate task followed by an end task, where the latter considers the outputs of the former (e.g., syntactic-then-semantic parsing).",2 Method,[0],[0]
"Inputs are denoted as x, and end task outputs as y. We use z to denote intermediate structures derived from x. We will often refer to the intermediate task as “decoding”, in the structured prediction sense.",2 Method,[0],[0]
"It seeks an output ẑ = argmaxz∈Z S from the feasible set Z , maximizing a (learned, parameterized) scoring function S for the structured intermediate task.",2 Method,[0],[0]
"L denotes the loss of the end task, which may or may not also involve structured predictions.",2 Method,[0],[0]
We use ∆k−1 = {p ∈,2 Method,[0],[0]
"Rk | 1>p = 1,p ≥ 0} to denote the (k − 1)-dimensional simplex.",2 Method,[0],[0]
"We denote the domain of binary variables as B = {0, 1}, and the unit interval as U =",2 Method,[0],[0]
"[0, 1].",2 Method,[0],[0]
"By projection of a vector v onto a set A, we mean the closest point in A to v, measured by Euclidean distance: projA(v) = argminv′∈A ‖v′ − v‖2.",2 Method,[0],[0]
"Decoding problems are typically decomposed into a collection of “parts”, such as arcs in a dependency tree or graph.",2.1 Relaxed Decoding,[0],[0]
"In such a setup, each element of z, zi, corresponds to one possible part, and zi takes a boolean value to indicate whether the part is included in the output structure.",2.1 Relaxed Decoding,[0],[0]
"The scoring function S is assumed to decompose into a vector s(x) of part-local, input-specific scores:
ẑ = argmax z∈Z S(x, z) =",2.1 Relaxed Decoding,[0],[0]
"argmax z∈Z
z>s(x) (1)
In the following, we drop s’s dependence on x for clarity.
",2.1 Relaxed Decoding,[0],[0]
"In many NLP problems, the output space Z can be specified by linear constraints (Roth and Yih, 2004):
",2.1 Relaxed Decoding,[0],[0]
A [ z ψ ],2.1 Relaxed Decoding,[0],[0]
"≤ b, (2)
where ψ are auxiliary variables (also scoped by argmax), together with integer constraints (typically, each zi ∈ B).
",2.1 Relaxed Decoding,[0],[0]
"The problem in Equation 1 can be NP-complete in general, so the {0, 1} constraints are often relaxed to [0, 1] to make decoding tractable (Martins et al., 2009).",2.1 Relaxed Decoding,[0],[0]
"Then the discrete combinatorial problem over Z is transformed into the optimization of a linear objective over a convex polytope P={p ∈ Rd |Ap≤b}, which is solvable in polynomial time (Bertsimas and Tsitsiklis, 1997).",2.1 Relaxed Decoding,[0],[0]
"This is not necessary in some cases, where the argmax can be solved exactly with dynamic programming.",2.1 Relaxed Decoding,[0],[0]
"We now view structured argmax as an activation function that takes a vector of input-specific partscores s and outputs a solution ẑ. For backpropagation, to calculate gradients for parameters of s, the chain rule defines:
∇sL = J ∇ẑL, (3) where the Jacobian matrix J = ∂ẑ∂s contains the derivative of each element of ẑ with respect to each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is either zero (almost everywhere) or undefined (in the case of ties).
",2.2 From STE to SPIGOT,[0],[0]
"One solution, taken in structured attention, is to replace the argmax with marginal inference and a softmax function, so that ẑ encodes probability distributions over parts (Kim et al., 2017; Liu and Lapata, 2018).",2.2 From STE to SPIGOT,[0],[0]
"As discussed in §1, there are two reasons to avoid this modification.",2.2 From STE to SPIGOT,[0],[0]
"Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example (Eisner, 2016; Friesen and Domingos, 2016); in general marginal inference can be #P-complete.",2.2 From STE to SPIGOT,[0],[0]
"Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model.
",2.2 From STE to SPIGOT,[0],[0]
"In another line of work, argmax is augmented with a strongly-convex penalty on the solutions (Martins and Astudillo, 2016; Amos and Kolter, 2017; Niculae and Blondel, 2017; Niculae et al., 2018; Mensch and Blondel, 2018).",2.2 From STE to SPIGOT,[0],[0]
"However, their approaches require solving a relaxation even when exact decoding is tractable.",2.2 From STE to SPIGOT,[0],[0]
"Also, the penalty will bias the solutions found by the decoder, which may be an undesirable conflation of computational and modeling concerns.
",2.2 From STE to SPIGOT,[0],[0]
"A simpler solution is the STE method (Hinton, 2012), which replaces the Jacobian matrix in Equation 3 by the identity matrix.",2.2 From STE to SPIGOT,[0],[0]
"This method has been demonstrated to work well when used to “backpropagate” through hard threshold functions (Bengio et al., 2013; Friesen and Domingos, 2018) and categorical random variables (Jang et al., 2016; Choi et al., 2017).
",2.2 From STE to SPIGOT,[0],[0]
"Consider for a moment what we would do if ẑ were a vector of parameters, rather than intermediate predictions.",2.2 From STE to SPIGOT,[0],[0]
"In this case, we are seeking points in Z that minimize L; denote that set of minimizers by Z∗. Given ∇ẑL and step size η, we would update ẑ to be ẑ",2.2 From STE to SPIGOT,[0],[0]
"− η∇ẑL. This update, however, might not return a value in the feasible set Z , or even (if we are using a linear relaxation) the relaxed set P .
",2.2 From STE to SPIGOT,[0],[0]
SPIGOT therefore introduces a projection step that aims to keep the “updated” ẑ in the feasible set.,2.2 From STE to SPIGOT,[0],[0]
"Of course, we do not directly update ẑ; we continue backpropagation through s and onward to the parameters.",2.2 From STE to SPIGOT,[0],[0]
"But the projection step nonetheless alters the parameter updates in the way that our proxy for “∇sL” is defined.
",2.2 From STE to SPIGOT,[0],[0]
"The procedure is defined as follows:
p̂ = ẑ− η∇ẑL, (4a)",2.2 From STE to SPIGOT,[0],[0]
"z̃ = projP(p̂), (4b) ∇sL , ẑ−",2.2 From STE to SPIGOT,[0],[0]
"z̃. (4c)
",2.2 From STE to SPIGOT,[0],[0]
"First, the method makes an “update” to ẑ as if it contained parameters (Equation 4a), letting p̂ denote the new value.",2.2 From STE to SPIGOT,[0],[0]
"Next, p̂ is projected back onto the (relaxed) feasible set (Equation 4b), yielding a feasible new value z̃.",2.2 From STE to SPIGOT,[0],[0]
"Finally, the gradients with respect to s are computed by Equation 4c.
",2.2 From STE to SPIGOT,[0],[0]
"Due to the convexity of P , the projected point z̃ will always be unique, and is guaranteed to be no farther than p̂ from any point in Z∗ (Luenberger and Ye, 2015).1 Compared to STE, SPIGOT in-
1Note that this property follows from P’s convexity, and we do not assume the convexity of L.
volves a projection and limits ∇sL to a smaller space to satisfy constraints.",2.2 From STE to SPIGOT,[0],[0]
"See Figure 1 for an illustration.
",2.2 From STE to SPIGOT,[0],[0]
"When efficient exact solutions (such as dynamic programming) are available, they can be used.",2.2 From STE to SPIGOT,[0],[0]
"Yet, we note that SPIGOT does not assume the argmax operation is solved exactly.",2.2 From STE to SPIGOT,[0],[0]
"Using SPIGOT, we now devise an algorithm to “backpropagate” through NLP pipelines.",2.3 Backpropagation through Pipelines,[0],[0]
"In these pipelines, an intermediate task’s output is fed into an end task for use as features.",2.3 Backpropagation through Pipelines,[0],[0]
"The parameters of the complete model are divided into two parts: denote the parameters of the intermediate task model byφ (used to calculate s), and those in the end task model as θ.2 As introduced earlier, the end-task loss function to be minimized is L, which depends on both φ and θ.
Algorithm 1 describes the forward and backward computations.",2.3 Backpropagation through Pipelines,[0],[0]
"It takes an end task training pair 〈x,y〉, along with the intermediate task’s feasible set Z , which is determined by x. It first runs the intermediate model and decodes to get intermediate structure ẑ, just as in a standard pipeline.",2.3 Backpropagation through Pipelines,[0],[0]
"Then forward propagation is continued into the end-task model to compute loss L, using ẑ to define input features.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation in the endtask model computes ∇θL and ∇ẑL, and ∇sL is then constructed using Equations 4.",2.3 Backpropagation through Pipelines,[0],[0]
"Backpropagation then continues into the intermediate model, computing∇φL.
Due to its flexibility, SPIGOT is applicable to many training scenarios.",2.3 Backpropagation through Pipelines,[0],[0]
"When there is no 〈x, z〉 training data for the intermediate task, SPIGOT can be used to induce latent structures for the end-task (Yogatama et al., 2017; Kim et al., 2017; Choi et al., 2017, inter alia).",2.3 Backpropagation through Pipelines,[0],[0]
"When intermediate-task training data is available, one can use SPIGOT to adopt joint learning by minimizing an interpolation of L (on end-task data 〈x,y〉) and an intermediate-task loss function L̃ (on intermediate task data 〈x, z〉).",2.3 Backpropagation through Pipelines,[0],[0]
This is the setting in our experiments; note that we do not assume any overlap in the training examples for the two tasks.,2.3 Backpropagation through Pipelines,[0],[0]
"In this section we discuss how to compute approximate projections for the two intermediate tasks
2Nothing prohibits tying across pre-argmax parameters and post-argmax parameters; this separation is notationally convenient but not at all necessary.
",3 Solving the Projections,[0],[0]
"Algorithm 1 Forward and backward computation with SPIGOT. 1: procedure SPIGOT(x,y,Z) 2: Construct A, b such that Z = {p ∈",3 Solving the Projections,[0],[0]
Zd | Ap ≤ b} 3: P ← {p ∈ Rd | Ap ≤ b} .,3 Solving the Projections,[0],[0]
Relaxation 4: Forwardprop and compute sφ(x) 5: ẑ← argmaxz∈Z z>sφ(x) .,3 Solving the Projections,[0],[0]
"Intermediate decoding 6: Forwardprop and compute L given x, y, and ẑ 7: Backprop and compute∇θL and∇ẑL 8: z̃← projP(ẑ− η∇ẑL) .",3 Solving the Projections,[0],[0]
"Projection 9: ∇sL← ẑ− z̃ 10: Backprop and compute∇φL 11: end procedure
considered in this work, arc-factored unlabeled dependency parsing and first-order semantic dependency parsing.
",3 Solving the Projections,[0],[0]
"In early experiments we observe that for both tasks, projecting with respect to all constraints of their original formulations using a generic quadratic program solver was prohibitively slow.",3 Solving the Projections,[0],[0]
"Therefore, we construct relaxed polytopes by considering only a subset of the constraints.3",3 Solving the Projections,[0],[0]
"The projection then decomposes into a series of singly constrained quadratic programs (QP), each of which can be efficiently solved in linear time.
",3 Solving the Projections,[0],[0]
The two approximate projections discussed here are used in backpropagation only.,3 Solving the Projections,[0],[0]
"In the forward pass, we solve the decoding problem using the models’ original decoding algorithms.
",3 Solving the Projections,[0],[0]
Arc-factored unlabeled dependency parsing.,3 Solving the Projections,[0],[0]
"For unlabeled dependency trees, we impose [0, 1] constraints and single-headedness constraints.4
Formally, given a length-n input sentence, excluding self-loops, an arc-factored parser considers d = n(n − 1) candidate arcs.",3 Solving the Projections,[0],[0]
"Let i→j denote an arc from the ith token to the jth, and σ(i→j) denote its index.",3 Solving the Projections,[0],[0]
"We construct the relaxed feasible set by:
PDEP = p ∈",3 Solving the Projections,[0],[0]
Ud ∣∣∣∣∣∣ ∑,3 Solving the Projections,[0],[0]
i 6=j pσ(i→j) =,3 Solving the Projections,[0],[0]
"1,∀j  , (5) i.e., we consider each token j individually, and force single-headedness by constraining the number of arcs incoming to j to sum to 1.",3 Solving the Projections,[0],[0]
"Algorithm 2 summarizes the procedure to project onto PDEP.
",3 Solving the Projections,[0],[0]
"3A parallel work introduces an active-set algorithm to solve the same class of quadratic programs (Niculae et al., 2018).",3 Solving the Projections,[0],[0]
"It might be an efficient approach to solve the projections in Equation 4b, which we leave to future work.
4",3 Solving the Projections,[0],[0]
"It requires O(n2) auxiliary variables and O(n3) additional constraints to ensure well-formed tree structures (Martins et al., 2013).
",3 Solving the Projections,[0],[0]
"Line 3 forms a singly constrained QP, and can be solved in O(n) time (Brucker, 1984).
",3 Solving the Projections,[0],[0]
Algorithm 2 Projection onto the relaxed polytope PDEP for dependency tree structures.,3 Solving the Projections,[0],[0]
"Let bold σ(·→j) denote the index set of arcs incoming to j. For a vector v, we use vσ(·→j) to denote vector [vk]k∈σ(·→j).
1: procedure DEPPROJ(p̂) 2: for j = 1, 2, . . .",3 Solving the Projections,[0],[0]
", n",3 Solving the Projections,[0],[0]
"do 3: z̃σ(·→j) ← proj∆n−2 ( p̂σ(·→j) ) 4: end for 5: return z̃ 6: end procedure
First-order semantic dependency parsing.",3 Solving the Projections,[0],[0]
"Semantic dependency parsing uses labeled bilexical dependencies to represent sentence-level semantics (Oepen et al., 2014, 2015, 2016).",3 Solving the Projections,[0],[0]
"Each dependency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic relations.",3 Solving the Projections,[0],[0]
"Figure 2 diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree.
",3 Solving the Projections,[0],[0]
"We use a state-of-the-art semantic dependency parser (Peng et al., 2017) that considers three types of parts: heads, unlabeled arcs, and labeled arcs.",3 Solving the Projections,[0],[0]
Let σ(i `→ j) denote the index of the arc from i to j with semantic role `.,3 Solving the Projections,[0],[0]
"In addition to [0, 1] constraints, we constrain that the predictions for labeled arcs sum to the prediction of their associated unlabeled arc:
PSDP { p ∈ Ud ∣∣∣∣∣∑ ` p σ(i `→j) = pσ(i→j), ∀i 6= j } .
(6)
",3 Solving the Projections,[0],[0]
This ensures that exactly one label is predicted if and only if its arc is present.,3 Solving the Projections,[0],[0]
The projection onto PSDP can be solved similarly to Algorithm 2.,3 Solving the Projections,[0],[0]
We drop the determinism constraint imposed by Peng et al. (2017) in the backward computation.,3 Solving the Projections,[0],[0]
"We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification.",4 Experiments,[0],[0]
"In this experiment we consider an intermediate syntactic parsing task, followed by seman-
… became dismayed at
poss arg1
arg2
’sG-2 connections arrested traffickersto drug
arg2 compound
root
arg2 arg1 arg2
tic dependency parsing as the end task.",4.1 Syntactic-then-Semantic Parsing,[0],[0]
"We first briefly review the neural network architectures for the two models (§4.1.1), and then introduce the datasets (§4.1.2) and baselines (§4.1.3).",4.1 Syntactic-then-Semantic Parsing,[0],[0]
Syntactic dependency parser.,4.1.1 Architectures,[0],[0]
"For intermediate syntactic dependencies, we use the unlabeled arc-factored parser of Kiperwasser and Goldberg (2016).",4.1.1 Architectures,[0],[0]
"It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayerperceptron (MLP) to score each potential dependency.",4.1.1 Architectures,[0],[0]
"One notable modification is that we replace their use of Chu-Liu/Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967) with the Eisner algorithm (Eisner, 1996, 2000), since our dataset is in English and mostly projective.
",4.1.1 Architectures,[0],[0]
Semantic dependency parser.,4.1.1 Architectures,[0],[0]
We use the basic model of Peng et al. (2017) (denoted as NEURBOPARSER) as the end model.,4.1.1 Architectures,[0],[0]
"It is a first-order parser, and uses local factors for heads, unlabeled arcs, and labeled arcs.",4.1.1 Architectures,[0],[0]
NEURBOPARSER does not use syntax.,4.1.1 Architectures,[0],[0]
"It first encodes an input sentence with a two-layer BiLSTM, and then computes part scores with two-layer tanh-MLPs.",4.1.1 Architectures,[0],[0]
"Inference is conducted with AD3 (Martins et al., 2015).",4.1.1 Architectures,[0],[0]
"To add syntactic features to NEURBOPARSER, we concatenate a token’s contextualized representation to that of its syntactic head, predicted by the intermediate parser.",4.1.1 Architectures,[0],[0]
"Formally, given length-n input sentence, we first run a BiLSTM.",4.1.1 Architectures,[0],[0]
We use the concatenation of the two hidden representations hj =,4.1.1 Architectures,[0],[0]
[ −→ h j ; ←−,4.1.1 Architectures,[0],[0]
h j ] at each position j as the contextualized token representations.,4.1.1 Architectures,[0],[0]
"We then concatenate
hj with the representation of its head hHEAD(j) by
h̃j =",4.1.1 Architectures,[0],[0]
[hj ;hHEAD(j)],4.1.1 Architectures,[0],[0]
= hj ;∑ i 6=j ẑσ(i→j),4.1.1 Architectures,[0],[0]
"hi  , (7)
where ẑ ∈ Bn(n−1) is a binary encoding of the tree structure predicted by by the intermediate parser.",4.1.1 Architectures,[0],[0]
We then use h̃j anywhere hj would have been used in NEURBOPARSER.,4.1.1 Architectures,[0],[0]
"In backpropagation, we compute ∇ẑL with an automatic differentiation toolkit (DyNet; Neubig et al., 2017).
",4.1.1 Architectures,[0],[0]
"We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015).",4.1.1 Architectures,[0],[0]
"We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work.
",4.1.1 Architectures,[0],[0]
"We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models.
",4.1.1 Architectures,[0],[0]
Training procedure.,4.1.1 Architectures,[0],[0]
"Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models.",4.1.1 Architectures,[0],[0]
"We jointly train both models from scratch, by randomly sampling an instance from the union of their training data at each step.",4.1.1 Architectures,[0],[0]
"In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials.",4.1.1 Architectures,[0],[0]
"• For semantic dependencies, we use the
English dataset from SemEval 2015 Task 18 (Oepen et al., 2015).",4.1.2 Datasets,[0],[0]
"Among the three formalisms provided by the shared task, we consider DELPH-IN MRS-derived dependencies (DM) and Prague Semantic Dependencies (PSD).6 It includes §00–19 of the WSJ corpus as training data, §20 and §21 for development and in-domain test data, resulting in a 33,961/1,692/1,410 train/dev./test split, and
5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach.
",4.1.2 Datasets,[0],[0]
"6We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting.
",4.1.2 Datasets,[0],[0]
"1,849 out-of-domain test instances from the Brown corpus.7 •",4.1.2 Datasets,[0],[0]
"For syntactic dependencies, we use the Stanford Dependency (de Marneffe and Manning, 2008) conversion of the the Penn Treebank WSJ portion (Marcus et al., 1993).",4.1.2 Datasets,[0],[0]
"To avoid data leak, we depart from standard split and use §20 and §21 as development and test data, and the remaining sections as training data.",4.1.2 Datasets,[0],[0]
"The number of training/dev./test instances is 40,265/2,012/1,671.",4.1.2 Datasets,[0],[0]
We compare to the following baselines: • A pipelined system (PIPELINE).,4.1.3 Baselines,[0],[0]
"The pre-
trained parser achieves 92.9 test unlabeled attachment score (UAS).8
7The organizers remove, e.g., instances with cyclic graphs, and thus only a subset of the WSJ corpus is included.",4.1.3 Baselines,[0],[0]
"See Oepen et al. (2015) for details.
",4.1.3 Baselines,[0],[0]
8 Note that this number is not comparable to the parsing literature due to the different split.,4.1.3 Baselines,[0],[0]
"As a sanity check, we found in preliminary experiments that the same parser archi-
• Structured attention networks (SA; Kim et al., 2017).",4.1.3 Baselines,[0],[0]
"We use the inside-outside algorithm (Baker, 1979) to populate z with arcs’ marginal probabilities, use log-loss as the objective in training the intermediate parser.",4.1.3 Baselines,[0],[0]
"• The straight-through estimator (STE; Hinton,
2012), introduced in §2.2.",4.1.3 Baselines,[0],[0]
Table 1 compares the semantic dependency parsing performance of SPIGOT to all five baselines.,4.1.4 Empirical Results,[0],[0]
"FREDA3 (Peng et al., 2017) is a state-of-the-art variant of NEURBOPARSER that is trained using multitask learning to jointly predict three different semantic dependency graph formalisms.",4.1.4 Empirical Results,[0],[0]
"Like the basic NEURBOPARSER model that we build from, FREDA3 does not use any syntax.",4.1.4 Empirical Results,[0],[0]
"Strong DM performance is achieved in a more recent work by using joint learning and an ensemble (Peng et al., 2018), which is beyond fair comparisons to the models discussed here.
",4.1.4 Empirical Results,[0],[0]
We found that using syntactic information improves semantic parsing performance: using pipelined syntactic head features brings 0.5– 1.4% absolute labeled F1 improvement to NEURBOPARSER.,4.1.4 Empirical Results,[0],[0]
"Such improvements are smaller compared to previous works, where dependency path and syntactic relation features are included (Almeida and Martins, 2015; Ribeyre et al., 2015; Zhang et al., 2016), indicating the potential to get better performance by using more syntactic information, which we leave to future work.
",4.1.4 Empirical Results,[0],[0]
Both STE and SPIGOT use hard syntactic features.,4.1.4 Empirical Results,[0],[0]
"By allowing backpropation into the intermediate syntactic parser, they both consistently outperform PIPELINE.",4.1.4 Empirical Results,[0],[0]
"On the other hand, when marginal syntactic tree structures are used, SA outperforms PIPELINE only on the out-of-domain PSD test set, and improvements under other cases are not observed.
",4.1.4 Empirical Results,[0],[0]
"Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain.",4.1.4 Empirical Results,[0],[0]
"For PSD, SPIGOT achieves similar performance to STE on in-domain test set, but has a 0.5% absolute labeled F1 improvement on out-of-domain data, where syntactic parsing is less accurate.
",4.1.4 Empirical Results,[0],[0]
"tecture achieves 93.5 UAS when trained and evaluated with the standard split, close to the results reported by Kiperwasser and Goldberg (2016).",4.1.4 Empirical Results,[0],[0]
Our second experiment uses semantic dependency graphs to improve sentiment classification performance.,4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"We are not aware of any efficient algorithm that solves marginal inference for semantic dependency graphs under determinism constraints, so we do not include a comparison to SA.",4.2 Semantic Dependencies for Sentiment Classification,[0],[0]
"Here we use NEURBOPARSER as the intermediate model, as described in §4.1.1, but with no syntactic enhancements.
",4.2.1 Architectures,[0],[0]
Sentiment classifier.,4.2.1 Architectures,[0],[0]
We first introduce a baseline that does not use any structural information.,4.2.1 Architectures,[0],[0]
"It learns a one-layer BiLSTM to encode the input sentence, and then feeds the sum of all hidden states into a two-layer ReLU-MLP.
",4.2.1 Architectures,[0],[0]
"To use semantic dependency features, we concatenate a word’s BiLSTM-encoded representation to the averaged representation of its heads, together with the corresponding semantic roles, similarly to that in Equation 7.9 Then the concatenation is fed into an affine transformation followed by a ReLU activation.",4.2.1 Architectures,[0],[0]
"The rest of the model is kept the same as the BiLSTM baseline.
",4.2.1 Architectures,[0],[0]
Training procedure.,4.2.1 Architectures,[0],[0]
"We use structured hinge loss to train the semantic dependency parser, and log-loss for the sentiment classifier.",4.2.1 Architectures,[0],[0]
"Due to the discrepancy in the training data size of the two tasks (33K vs. 7K), we pre-train a semantic dependency parser, and then adopt joint training together with the classifier.",4.2.1 Architectures,[0],[0]
"In the joint training stage, we randomly sample 20% of the semantic dependency training instances each epoch.",4.2.1 Architectures,[0],[0]
Implementations are detailed in the supplementary materials.,4.2.1 Architectures,[0],[0]
"For semantic dependencies, we use the DM dataset introduced in §4.1.2.
We consider a binary classification task using the Stanford Sentiment Treebank (Socher et al., 2013).",4.2.2 Datasets,[0],[0]
It consists of roughly 10K movie review sentences from Rotten Tomatoes.,4.2.2 Datasets,[0],[0]
"The full dataset includes a rating on a scale from 1 to 5 for each constituent (including the full sentences), resulting in more than 200K instances.",4.2.2 Datasets,[0],[0]
"Following previous work (Iyyer et al., 2015), we only use full-sentence
9In a well-formed semantic dependency graph, a token may have multiple heads.",4.2.2 Datasets,[0],[0]
"Therefore we use average instead of the sum in Equation 7.
instances, with neutral instances excluded (3s) and the remaining four rating levels converted to binary “positive” or “negative” labels.",4.2.2 Datasets,[0],[0]
"This results in a 6,920/872/1,821 train/dev./test split.",4.2.2 Datasets,[0],[0]
Table 2 compares our SPIGOT method to three baselines.,4.2.3 Empirical Results,[0],[0]
"Pipelined semantic dependency predictions brings 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines.",4.2.3 Empirical Results,[0],[0]
In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.,4.2.3 Empirical Results,[0],[0]
We examine here how the intermediate model is affected by the end-task training signal.,5 Analysis,[0],[0]
"Is the endtask signal able to “overrule” intermediate predictions?
",5 Analysis,[0],[0]
We use the syntactic-then-semantic parsing model (§4.1) as a case study.,5 Analysis,[0],[0]
Table 3 compares a pipelined system to one jointly trained using SPIGOT.,5 Analysis,[0],[0]
"We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems’ syntactic predictions agree (SAME), or not (DIFF).",5 Analysis,[0],[0]
"The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6.",5 Analysis,[0],[0]
"Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing.",5 Analysis,[0],[0]
"Furthermore, SPIGOT has an overall less detrimental effect on the intermediate parser than STE: using SPIGOT, intermediate dev. parsing UAS drops to 92.5 from the 92.9 pipelined performance, while STE reduces it to 91.8.
",5 Analysis,[0],[0]
We then take a detailed look and categorize the changes in intermediate trees by their correlations with the semantic graphs.,5 Analysis,[0],[0]
"Specifically, when a modifier m’s head is changed from h to h′ in the
tree, we consider three cases: (a) h′ is a head of m in the semantic graph; (b) h′ is a modifier of m in the semantic graph; (c) h is the modifier of m in the semantic graph.",5 Analysis,[0],[0]
The first two reflect modifications to the syntactic parse that rearrange semantically linked words to be neighbors.,5 Analysis,[0],[0]
"Under (c), the semantic parser removes a syntactic dependency that reverses the direction of a semantic dependency.",5 Analysis,[0],[0]
"These cases account for 17.6%, 10.9%, and 12.8%, respectively (41.2% combined) of the total changes.",5 Analysis,[0],[0]
"Making these changes, of course, is complicated, since they often require other modifications to maintain well-formedness of the tree.",5 Analysis,[0],[0]
Figure 2 gives an example.,5 Analysis,[0],[0]
Joint learning in NLP pipelines.,6 Related Work,[0],[0]
"To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia).",6 Related Work,[0],[0]
"However, joint inference can sometimes be prohibitively expensive.",6 Related Work,[0],[0]
"Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia).
",6 Related Work,[0],[0]
Differentiable optimization.,6 Related Work,[0],[0]
"Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013).",6 Related Work,[0],[0]
Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs.,6 Related Work,[0],[0]
"However, they both require that the intermediate objective has an invertible Hessian, limiting their application
in NLP.",6 Related Work,[0],[0]
"In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2012; Goodfellow et al., 2013; Brakel et al., 2013).",6 Related Work,[0],[0]
This comes at a high computational cost due to the second-order derivative computation during backpropagation.,6 Related Work,[0],[0]
"Moreover, constrained optimization problems (like many NLP problems) often require projection steps within the procedure, which can be difficult to differentiate through (Belanger and McCallum, 2016; Belanger et al., 2017).",6 Related Work,[0],[0]
"We presented SPIGOT, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers.",7 Conclusion,[0],[0]
"SPIGOT devises a proxy for the gradients with respect to argmax’s inputs, employing a projection that aims to respect the constraints in the intermediate task.",7 Conclusion,[0],[0]
"We empirically evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an intermediate semantic parser.",7 Conclusion,[0],[0]
"Experiments show that SPIGOT achieves stronger performance than baselines under both settings, and outperforms stateof-the-art systems on semantic dependency parsing.",7 Conclusion,[0],[0]
Our implementation is available at https: //github.com/Noahs-ARK/SPIGOT.,7 Conclusion,[0],[0]
"We thank the ARK, Julian Michael, Minjoon Seo, Eunsol Choi, and Maxwell Forbes for their helpful comments on an earlier version of this work, and the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS-1562364.,Acknowledgments,[0],[0]
"We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers.",abstractText,[0],[0]
"SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017).",abstractText,[0],[0]
"Like socalled straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed.",abstractText,[0],[0]
"We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification.",abstractText,[0],[0]
"We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.",abstractText,[0],[0]
Backpropagating through Structured Argmax using a SPIGOT,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 153–161 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Natural language understanding (NLU) is a key component of dialog systems for commercial personal digital assistants (PDAs) such as Amazon Alexa, Google Home, Microsoft Cortana and Apple Siri.",1 Introduction,[0],[0]
"The task of the NLU component is to map input user utterances into a semantic frame consisting of domain, intent and slots (Kurata et al., 2016).",1 Introduction,[0],[0]
"The semantic frame is used by the dialog manager for state tracking and action selection.
",1 Introduction,[0],[0]
"Slot tagging can be formulated as a sequence classification task where each input word in the user utterance must be classified as belonging to one of the slot types in a predefined schema (Sarikaya et al., 2016).",1 Introduction,[0],[0]
"In a standard NLU architecture, each new domain defines a new domainspecific schema for its slots.",1 Introduction,[0],[0]
"Figure 1 shows examples of annotated queries from three different domains relevant to a typical commercial digital
assistant.",1 Introduction,[0],[0]
"Since the schemas for different domains can vary, the usual strategy is to train a separate slot tagging model for each new domain.",1 Introduction,[0],[0]
"However, the number of domains increases rapidly as the PDAs are required to support new scenarios and training a separate slot tagging model for each new domain becomes prohibitively expensive in terms of annotation costs.
",1 Introduction,[0],[0]
"Even though different domains have different slot tagging schemas, some classes of slots appear across a number of domains, as suggested by the examples in Figure 1.",1 Introduction,[0],[0]
"Both travel and flight status have date and time related slots, and all three domains have the location slot.",1 Introduction,[0],[0]
Reusing annotated data for these common slots would allow us to train models with better accuracy using less data.,1 Introduction,[0],[0]
"However, since both the input distribution and the label distribution are different across domains, we must use domain adaptation methods to train on the joint data (Daume, 2007; Kim et al.,
153
2016c; Blitzer et al., 2006).",1 Introduction,[0],[0]
"In this data-driven adaptation approach, we build a repository of annotated data containing date, time, location and other reusable slots.",1 Introduction,[0],[0]
We then combine relevant data from the reusable repository with the domain specific data during model training.,1 Introduction,[0],[0]
"Figure 2(a) shows an example of this architecture where reusable date/time data is used for training travel domain.
",1 Introduction,[0],[0]
"A drawback of the data-driven adaptation approach is that as the repository of data for reusable slots grows, the training time for new domains increases.",1 Introduction,[0],[0]
"The training data for a new domain might be in the hundreds of samples, while the training data for the reusable slots might contain hundreds of thousands of samples.",1 Introduction,[0],[0]
"This increase in training time makes iterative refinement difficult in the initial design of new domains, which is when the ability to deploy new models quickly is crucial.
",1 Introduction,[0],[0]
"An alternative strategy is to use model-driven adaptation approaches (Kim et al., 2017b) as shown in Figure 2(b).",1 Introduction,[0],[0]
"Here, instead of retraining on the data for the reusable slots, we train “expert” models for these slots, and use the output of these models directly when training new domains.",1 Introduction,[0],[0]
"Using model-driven adaptation ensures that model training time is proportional to the data size of new
target domains, as opposed to the large data size for reusable slots, allowing for faster training.
",1 Introduction,[0],[0]
"In this paper, we present a model-driven adaptation approach for slot tagging called Bag of Experts (BoE).",1 Introduction,[0],[0]
"In Section 2, we first describe how this approach can be applied to two popular machine learning methods used for slot tagging: Long Short Term Memory (LSTM) and Conditional Random Fields (CRF) models.",1 Introduction,[0],[0]
"We then describe a dataset of 10 target domains and 2 reusable domains that we’ve collected for use in a commercial digital assistant, in Section 3.",1 Introduction,[0],[0]
"Using this data, we conduct experiments comparing the BoE models with their non-expert counterparts, and show that BoE models can lead to significant F1-score improvements.",1 Introduction,[0],[0]
The experimental setup is described in Section 4.1 and the results are discussed in Section 4.3.,1 Introduction,[0],[0]
This is followed by a survey of related work in Section 5 and the conclusion in Section 6.,1 Introduction,[0],[0]
"We first describe our LSTM and CRF models for slot tagging, followed by their BoE variants: LSTM-BoE and CRF-BoE. Tensorflow (Abadi et al., 2015) was used for implementing the LSTM models, while a custom C++ implementation was
used for the CRF models.",2 Approaches,[0],[0]
"For our LSTM model, we follow a standard bidirectional LSTM architecture (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).",2.1 LSTM,[0],[0]
Let w1...,2.1 LSTM,[0],[0]
wn denote the input word sequence.,2.1 LSTM,[0],[0]
"For every input word wi, let fCi and b C i be the outputs of the forward and backward character level LSTMs respectively, and let mi be the word embedding (initialized either randomly or with pretrained embeddings).",2.1 LSTM,[0],[0]
"The input to the word level LSTMs, gi, is the concatenation of these three vectors:
gi =",2.1 LSTM,[0],[0]
"[f C i ; b C i ;mi]
where both fCi , b C i ∈",2.1 LSTM,[0],[0]
R25 and mi has the same dimensions as the pre-trained embeddings.,2.1 LSTM,[0],[0]
"The forward and backward word level LSTMs take gi as input and produce fWi and b W i , which are then concatenated to produce hi:
hi =",2.1 LSTM,[0],[0]
"[f W i , b W i ]
",2.1 LSTM,[0],[0]
"where fWi , b W i ∈ R100, making hi ∈ R200.",2.1 LSTM,[0],[0]
hi is then input to a dense feed forward layer with a softmax activation to predict the label probabilities for each word.,2.1 LSTM,[0],[0]
"We train using stochastic gradient descent with Adam (Kingma and Ba, 2015).",2.1 LSTM,[0],[0]
"To avoid overfitting, we also use dropout on top of mi and hi layers, with a default dropout keep probability of 0.8.",2.1 LSTM,[0],[0]
"We experiment with some variations
of this default LSTM architecture, the results are described in Section 4.2.",2.1 LSTM,[0],[0]
We now describe the LSTM Bag of Experts (LSTM-BoE) architecture.,2.2 LSTM-BoE,[0],[0]
Let e1...,2.2 LSTM-BoE,[0],[0]
ek ∈ E be the set of reusable expert domains.,2.2 LSTM-BoE,[0],[0]
"For each expert ej , we train a separate LSTM with the architecture described in Section 2.1.",2.2 LSTM-BoE,[0],[0]
"Let heji be the bi-directional word LSTM output for expert ej on word wi.
",2.2 LSTM-BoE,[0],[0]
"When training on a target domain, for each word wi, we first compute the character level LSTMs fCi , b C i similarly to Section 2.1.",2.2 LSTM-BoE,[0],[0]
"We then compute a BoE representation for this word as:
hE = ∑
ei∈E h ej",2.2 LSTM-BoE,[0],[0]
"i
The input to the word level LSTM for word wi in the target domain is now a concatenation of the character level LSTM outputs (fCi , b C i ), the word embedding mi, and hE :
gi =",2.2 LSTM-BoE,[0],[0]
[f C i ; b C i ;mi;h,2.2 LSTM-BoE,[0],[0]
"E ]
gi is then input to the word level LSTM for the target domain to produce hi in the same way as Section 2.1.",2.2 LSTM-BoE,[0],[0]
"This architecture is similar to the one presented in (Kim et al., 2017b), with the exception that in their architecture, hE is concatenated with the word level LSTM output hi for the target
domain.",2.2 LSTM-BoE,[0],[0]
"In our architecture, we add hE before the word-level LSTM in order to capture long-range dependencies of label prediction for a word on expert predictions for context words.",2.2 LSTM-BoE,[0],[0]
"Conditional Random Fields (CRF) are a popular family of models that have been proven to work well in a variety of sequence tagging NLP applications (Lafferty et al., 2001).",2.3 CRF,[0],[0]
"For our experiments, we use a standard linear-chain CRF architecture with n-gram and context features.
",2.3 CRF,[0],[0]
"In particular, for each token, we use unigram, bigram and trigram features, along with previous and next unigrams, bigrams, and trigrams for context length of up to 3 words.",2.3 CRF,[0],[0]
"We also use a skip bigram feature created by concatenating the current unigram and skip-one unigram.
",2.3 CRF,[0],[0]
We train our CRF using stochastic gradient descent with L1 regularization to prevent overfitting.,2.3 CRF,[0],[0]
"The L1 coefficient was set to 0.1 and we use a learning rate of 0.1 with exponential decay for learning rate scheduling (Tsuruoka et al., 2009).",2.3 CRF,[0],[0]
"Similar to the LSTM-BoE model, we first train a CRF model cj for each of the reusable expert domains ej ∈ E. When training on a target domain, for every query word wi, a one-hot label vector",2.4 CRF-BoE,[0],[0]
l j,2.4 CRF-BoE,[0],[0]
i is emitted by each expert CRF model cj .,2.4 CRF-BoE,[0],[0]
"The length of the label vector lji is the number of labels in the expert domain, with the value corresponding to the label predicted by cj for word wi set to 1, and values for all other labels set to 0.",2.4 CRF-BoE,[0],[0]
"For each word, the label vectors for all the expert CRF models are concatenated and provided as features for the target domain CRF training, along with the n-gram features.",2.4 CRF-BoE,[0],[0]
We built a dataset of 10 target domains for experimentation.,3.1 Target Domains,[0],[0]
Table 1 shows the list of domains as well as some statistics and example utterances.,3.1 Target Domains,[0],[0]
"We treated these as new domains - that is, we do not have real interaction data with users for these domains.",3.1 Target Domains,[0],[0]
"The annotated data is therefore prepared in two steps.
",3.1 Target Domains,[0],[0]
"First, utterances are obtained using crowdsourcing, where workers are provided with prompts for different intents of a domain and asked to generate
natural language utterances corresponding to those intents.",3.1 Target Domains,[0],[0]
"Next, the generated utterances are annotated by a different set of crowd workers, using the slot schema for each domain.",3.1 Target Domains,[0],[0]
"Inter-annotator agreement as well as manual inspection are used to ensure data quality in both stages.
",3.1 Target Domains,[0],[0]
The amount of data collected varies for each domain based on its complexity and business priority.,3.1 Target Domains,[0],[0]
Dataset size statistics for the data used in our experiments are presented in section 4.1.,3.1 Target Domains,[0],[0]
"Test and dev data are sampled at 10% of the total annotated data, with stratified sampling used in order to preserve the distribution of the intents.",3.1 Target Domains,[0],[0]
We experiment with two domains containing reusable slots: timex and location.,3.2 Reusable Domains,[0],[0]
"The timex domain consists of utterances containing the slots date, time and duration.",3.2 Reusable Domains,[0],[0]
"The location domain consists of utterances containing location, location type and place name slots.",3.2 Reusable Domains,[0],[0]
"Both of these types of slots appear in more than 20 of a set of 40 domains developed for use in our commercial personal assistant, making them ideal candidates for reuse.1
1Several other candidate reusable domains exist, including: the name domain containing the slot contact name; the number domain containing the slots rating, quantity and price; and the reference domain containing the slots ordinal (whose values include “first”, “second” or “third”) and order ref (with values such as “before” or “after”).",3.2 Reusable Domains,[0],[0]
"All of these slots appear in more than 25% of the available domains.
",3.2 Reusable Domains,[0],[0]
Data for these domains was sampled from the input utterances from our commercial digital assistant.,3.2 Reusable Domains,[0],[0]
Each reusable domain contains about a million utterances.,3.2 Reusable Domains,[0],[0]
There is no overlap between utterances in the target domains used for our experiments and utterances in the reusable domains.,3.2 Reusable Domains,[0],[0]
"The data for the reusable domains is sampled from other domains available to the digital assistant, not including our target domains.
",3.2 Reusable Domains,[0],[0]
Grouping the reusable slots into domains in this way provides additional opportunities for a commercial system: the trained reusable domain models can be used in other related products which need to identify time and location related entities.,3.2 Reusable Domains,[0],[0]
Models trained on the timex and location data have F1-scores of 96% and 89% respectively on test data from their respective domains.,3.2 Reusable Domains,[0],[0]
We want to verify if BoE models can improve slot tagging performance by using the information from reusable domains.,4.1 Experimental Setup,[0],[0]
"To simulate the low data scenario for the initial model training, we create three training datasets by sampling 2000, 1000 and 500 training examples from every domain.",4.1 Experimental Setup,[0],[0]
"We use stratified sampling to maintain the input distribution of the intents across the three training datasets.
",4.1 Experimental Setup,[0],[0]
"For each training dataset, we train the four models as described in Section 2 and compute the precision, recall and F1-score on the test data.",4.1 Experimental Setup,[0],[0]
Fixed seeds are used when training all models to make the results reproducible.,4.1 Experimental Setup,[0],[0]
"Table 3 summarizes these results, with only F1-scores reported to save space.",4.1 Experimental Setup,[0],[0]
We describe these results in Section 4.3.,4.1 Experimental Setup,[0],[0]
"Using the dev data set for the 10 domains, we experimented with using different pretrained embeddings, dropout probabilities and a CRF output layer in our LSTM architecture.",4.2 LSTM architecture variants,[0],[0]
The results are summarized in Table 2.,4.2 LSTM architecture variants,[0],[0]
"For each of the 10 domains, we trained using each variant with 10 different seeds, and computed the mean F1-score for each domain.",4.2 LSTM architecture variants,[0],[0]
"For comparing two variants, we computed the mean difference in the F1-scores over the 10 domains and its p-value.
",4.2 LSTM architecture variants,[0],[0]
"We tried word level Glove embeddings of 100, 200 and 300 dimensions as well as 500- dimensional word embeddings trained over the ut-
terances from our commercial PDA logs.",4.2 LSTM architecture variants,[0],[0]
"Both 100 and 200 dimensional Glove embeddings led to statistically significant improvements, but the word embeddings trained over our logs led to the biggest improvement.",4.2 LSTM architecture variants,[0],[0]
"We also tried using a CRF output layer (Lample et al., 2016) and different values of dropout keep probability, but none of them gave statistically significant improvements over the default model.",4.2 LSTM architecture variants,[0],[0]
"Based on this, we used PDA trained 500-dimensional word embeddings for our final experiments on test data.",4.2 LSTM architecture variants,[0],[0]
Table 3(a) shows the F1-scores obtained by the different methods for the training data set of 2000 training instances for each of the 10 domains.,4.3 Results and Discussion,[0],[0]
LSTM based models in general perform better than the CRF based models.,4.3 Results and Discussion,[0],[0]
The LSTM models have a statistically significant average improvement of 3.14 absolute F1-score over the CRF models.,4.3 Results and Discussion,[0],[0]
"The better performance of LSTM over CRF can be explained by the LSTM being able to use information over longer contexts to make predictions, while the CRF model is limited to at most the previous and next 3 words.
",4.3 Results and Discussion,[0],[0]
The results in Table 3(a) also show that both the CRF-BoE and LSTM-BoE outperform the basic CRF and LSTM models.,4.3 Results and Discussion,[0],[0]
LSTM-BoE has a statistically significant mean improvement of 1.92 points over LSTM.,4.3 Results and Discussion,[0],[0]
"CRF-BoE also shows an average improvement of 2.19 points over the CRF model, but the results are not statistically significant.",4.3 Results and Discussion,[0],[0]
"Looking at results for individual domains, the highest improvement for BoE models are seen for transportation and travel.",4.3 Results and Discussion,[0],[0]
"This can be explained by these domains having a high frequency of timex and location slots, as shown in Table 4.
",4.3 Results and Discussion,[0],[0]
"The shopping model shows a regression for BoE models, and a reason could be the low frequency of expert slots (Table 4).",4.3 Results and Discussion,[0],[0]
"However, low frequency of expert slots does not always mean that BoE methods can’t help, as shown by the improvement in the purchase domain.",4.3 Results and Discussion,[0],[0]
"Finally, for sports, social network and deals domains, the LSTM-BoE improves over LSTM, while CRFBoE does not improve over CRF.",4.3 Results and Discussion,[0],[0]
"Our hypothesis is that given the query patterns for these domains, the dense vector output used by LSTM-BoE is able to transfer some information, while the categorical label output used by CRF-BoE is not.
",4.3 Results and Discussion,[0],[0]
"Table 3(b) shows the results with 500 and 1000
training data instances.",4.3 Results and Discussion,[0],[0]
Note that the improvements are even higher for the experiments with smaller training data.,4.3 Results and Discussion,[0],[0]
"In particular, LSTM-BoE shows an improvement of 4.63 in absolute F1score over LSTM when training with 500 instances.",4.3 Results and Discussion,[0],[0]
"Thus, as we reduce the amount of training data in the target domain, the performance improvement from BoE models is even higher.
",4.3 Results and Discussion,[0],[0]
"As an example, in the purchase domain, the LSTM-BoE model achieves an F1-score of 70.66% with only 500 training instances, while even with 2000 training instances the CRF model achieves an F1-score of only 66.24%.",4.3 Results and Discussion,[0],[0]
Thus the LSTM-BoE model achieves better F1-score with only one-fourth the training data.,4.3 Results and Discussion,[0],[0]
"Similarly, for flight status, travel, and transportation domains, the LSTM-BoE model gets better performance with 500 training instances, compared to a CRF model with 2000 training instances.",4.3 Results and Discussion,[0],[0]
"The LSTMBoE architecture, therefore, allows us to reuse the domain experts to produce better performing mod-
els with much lower data annotation costs.",4.3 Results and Discussion,[0],[0]
"As the target domain training data increases, the contribution due to domain experts goes down, but more experimentation is needed to establish the threshold at which it is no longer useful to add experts.",4.3 Results and Discussion,[0],[0]
"Early methods for slot-tagging used rule-based approaches (Ward and Issar, 1994).",5 Related Work,[0],[0]
"Much of the later work on supervised learning focused on CRFs, for example (Sarikaya et al., 2016), or neural networks (Deoras and Sarikaya, 2013; Yao et al., 2013; Liu et al., 2015; Celikyilmaz and HakkaniTur, 2015).",5 Related Work,[0],[0]
"Unsupervised (or weakly-supervised) methods also were used for NLU tasks, primarily leveraging search query click logs (Hakkani-Tur et al., 2011a,b, 2013) and knowledge graphs (Tur et al., 2012; Heck and Hakkani-Tur, 2012; Heck et al., 2013); hybrid methods, for example as described in (Kim et al., 2015a; Celikyilmaz et al., 2015; Chen et al., 2016), also exist.",5 Related Work,[0],[0]
"Our approach
in this paper is a purely supervised one.",5 Related Work,[0],[0]
"Transfer learning is a vast area of research, with too many publications for an exhaustive list.",5 Related Work,[0],[0]
We discuss some of the recent work most relevant to our methods.,5 Related Work,[0],[0]
"In (Kim et al., 2015b), the slot labels from across different domains are mapped into a shared space using Canonical Correlation Analysis (CCA) and automatically-induced embeddings over the label space.",5 Related Work,[0],[0]
"These label representations allow mapping of label types between different domains, which makes it possible to apply standard data-driven domain adaptation approaches (Daume, 2007).",5 Related Work,[0],[0]
"They also introduce a model-driven adaptation technique based on training a hidden unit CRF (HUCRF) on the source domain, which is then used to initialize the training for the target domain.",5 Related Work,[0],[0]
"The limitation of this approach is that only one source domain can be used, while multiple experts can be used in the proposed BoE approach.
",5 Related Work,[0],[0]
"(Kim et al., 2016a) build a single, universal slot tagging model, and constrain the decoding process to subsets of slots for various domains; this process assumes that a mapping of slot tags in the new domain to the ones in the universal slot model has already been generated.",5 Related Work,[0],[0]
"A related work by (Kim et al., 2016b) directly predicts the required schema prior to performing the constrained decoding.",5 Related Work,[0],[0]
"These approaches are attractive because only one universal model needs to be trained, but do not work in cases when a new domain contains a mixture of new and existing slots.",5 Related Work,[0],[0]
"Our approach allows transfer of partial knowledge in such cases.
",5 Related Work,[0],[0]
"(Kim et al., 2016c) uses a neural version of the approach first described in (Daume, 2007), by using existing annotated data in a variety of domains
to adapt the slot tag models of new domains where the tag space is partly shared.",5 Related Work,[0],[0]
"The drawback of such data-driven domain adaptation is the increase in training time as more experts are added.
",5 Related Work,[0],[0]
"An expert-based adaptation, similar to the techniques applied in this paper, was first described in (Kim et al., 2017b).",5 Related Work,[0],[0]
"(Jaech et al., 2016) use multitask learning, training a bidirectional LSTM with character-level embeddings, trained jointly to produce slot tags for a number of travel-related domains.",5 Related Work,[0],[0]
"Finally, (Kim et al., 2017a) frame the problem of temporal shift in data of a single domain (and the related problem of bootstrapping a new domain with imperfectly-matched synthetic data) as one of domain adaptation, applying adversarial training approaches.
",5 Related Work,[0],[0]
A number of researchers also investigated bootstrapping NLU systems using zero-shot learning.,5 Related Work,[0],[0]
"(Dauphin et al., 2014; Kumar et al., 2017) both investigated domain classification; most relevant to us is the work by (Bapna et al., 2017), who studied full semantic frame tagging using zero-shot learning, by projecting the tags into a shared embedding space, similar to work done by (Kim et al., 2015b).",5 Related Work,[0],[0]
We experimented with Bag of Experts (BoE) architectures for CRF and LSTM based slot tagging models.,6 Conclusion,[0],[0]
Our experimental results over a set of 10 domains show that BoE architectures are able to use the information from reusable expert models to perform significantly better than their nonexpert counterparts.,6 Conclusion,[0],[0]
"In particular, the LSTM-BoE model shows a statistically significant improvement of 1.92% over the LSTM model on average when training with 2000 instances.",6 Conclusion,[0],[0]
"When training with 500 instances, the improvement of LSTM-BoE model over LSTM is even higher at 4.63%.",6 Conclusion,[0],[0]
"For multiple domains, an LSTM-BoE model trained on only 500 instances is able to outperform a baseline CRF model trained over 4 times the data.",6 Conclusion,[0],[0]
"Thus, the BoE approach produces high performing models for slot tagging at much lower annotation costs.",6 Conclusion,[0],[0]
We would like to thank Ahmed El Kholy for his comments and feedback on an earlier version of this paper.,Acknowledgments,[0],[0]
"Also, thanks to Kyle Williams and Zhaleh Feizollahi for their help with code and data collection.",Acknowledgments,[0],[0]
"Slot tagging, the task of detecting entities in input user utterances, is a key component of natural language understanding systems for personal digital assistants.",abstractText,[0],[0]
"Since each new domain requires a different set of slots, the annotation costs for labeling data for training slot tagging models increases rapidly as the number of domains grow.",abstractText,[0],[0]
"To tackle this, we describe Bag of Experts (BoE) architectures for model reuse for both LSTM and CRF based models.",abstractText,[0],[0]
"Extensive experimentation over a dataset of 10 domains drawn from data relevant to our commercial personal digital assistant shows that our BoE models outperform the baseline models with a statistically significant average margin of 5.06% in absolute F1score when training with 2000 instances per domain, and achieve an even higher improvement of 12.16% when only 25% of the training data is used.",abstractText,[0],[0]
Bag of Experts Architectures for Model Reuse in Conversational Language Understanding,title,[0],[0]
The stochastic multi-armed bandit (MAB) problem is a prominent framework for capturing the explorationexploitation tradeoff in online decision making and experiment design.,1. Introduction,[0],[0]
"The MAB problem proceeds in discrete sequential rounds, where in each round, the player pulls one
1 Department of Mathematics and Statistics, Lancaster University, Lancaster, UK 2 Department of Industrial Engineering and Operations Research, Columbia University, New York, NY, USA 3 DeepMind, London, UK 4 Department of Computing Science, University of Alberta, Edmonton, AB, Canada.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Ciara Pike-Burke <ciara.pikeburke@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of the K possible arms.",1. Introduction,[0],[0]
"In the classic stochastic MAB setting, the player immediately observes stochastic feedback from the pulled arm in the form of a ‘reward’ which can be used to improve the decisions in subsequent rounds.",1. Introduction,[0],[0]
One of the main application areas of MABs is in online advertising.,1. Introduction,[0],[0]
"Here, the arms correspond to adverts, and the feedback would correspond to conversions, that is users buying a product after seeing an advert.",1. Introduction,[0],[0]
"However, in practice, these conversions may not necessarily happen immediately after the advert is shown, and it may not always be possible to assign the credit of a sale to a particular showing of an advert.",1. Introduction,[0],[0]
"A similar challenge is encountered in many other applications, e.g., in personalized treatment planning, where the effect of a treatment on a patient’s health may be delayed, and it may be difficult to determine which out of several past treatments caused the change in the patient’s health; or, in content design applications, where the effects of multiple changes in the website design on website traffic and footfall may be delayed and difficult to distinguish.
",1. Introduction,[0],[0]
"In this paper, we propose a new bandit model to handle online problems with such ‘delayed, aggregated and anonymous’ feedback.",1. Introduction,[0],[0]
"In our model, a player interacts with an environment ofK actions (or arms) in a sequential fashion.",1. Introduction,[0],[0]
At each time step the player selects an action which leads to a reward generated at random from the underlying reward distribution.,1. Introduction,[0],[0]
"At the same time, a nonnegative random integer-valued delay is also generated i.i.d.",1. Introduction,[0],[0]
from an underlying delay distribution.,1. Introduction,[0],[0]
Denoting this delay by τ ≥ 0,1. Introduction,[0],[0]
"and the index of the current round by t, the reward generated in round t will arrive at the end of the (t + τ)th round.",1. Introduction,[0],[0]
"At the end of each round, the player observes only the sum of all the rewards that arrive in that round.",1. Introduction,[0],[0]
"Crucially, the player does not know which of the past plays have contributed to this aggregated reward.",1. Introduction,[0],[0]
"We call this problem multi-armed bandits with delayed, aggregated anonymous feedback (MABDAAF).",1. Introduction,[0],[0]
"As in the standard MAB problem, in MABDAAF, the goal is to maximize the cumulative reward from T plays of the bandit, or equivalently to minimize the regret.",1. Introduction,[0],[0]
"The regret is the total difference between the reward of the optimal action and the actions taken.
",1. Introduction,[0],[0]
"If the delays are all zero, the MABDAAF problem reduces to the standard (stochastic) MAB problem, which has been studied considerably (e.g., Thompson, 1933; Lai & Robbins, 1985; Auer et al., 2002; Bubeck & Cesa-Bianchi,
2012).",1. Introduction,[0],[0]
"Compared to the MAB problem, the job of the player in our problem appears to be significantly more difficult since the player has to deal with (i) that some feedback from the previous pulls may be missing due to the delays, and (ii) that the feedback takes the form of the sum of an unknown number of rewards of unknown origin.
",1. Introduction,[0],[0]
"An easier problem is when the observations are delayed, but they are non-aggregated and non-anonymous: that is, the player has to only deal with challenge (i) and not (ii).",1. Introduction,[0],[0]
"Here, the player receives delayed feedback in the shape of action-reward pairs that inform the player of both the individual reward and which action generated it.",1. Introduction,[0],[0]
"This problem, which we shall call the (non-anonymous) delayed feedback bandit problem, has been studied by Joulani et al. (2013), and later followed up by Mandel et al. (2015) (for bounded delays).",1. Introduction,[0],[0]
"Remarkably, they show that compared to the standard (non-delayed) stochastic MAB setting, the regret will increase only additively by a factor that scales with the expected delay.",1. Introduction,[0],[0]
"For delay distributions with a finite expected delay, E[τ ], the worst case regret scales with O( √ KT log T + KE[τ ]).",1. Introduction,[0],[0]
"Hence, the price to pay for the delay in receiving the observations is negligible.",1. Introduction,[0],[0]
"QPM-D of Joulani et al. (2013) and SBD of Mandel et al. (2015) place received rewards into queues for each arm, taking one whenever a base bandit algorithm suggests playing the arm.",1. Introduction,[0],[0]
"Throughout, we take UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. Joulani et al. (2013) also present a direct modification of the UCB1 algorithm.",1. Introduction,[0],[0]
All of these algorithms achieve the stated regret.,1. Introduction,[0],[0]
"None of them require any knowledge of the delay distributions, but they all rely heavily upon the non-anonymous nature of the observations.
",1. Introduction,[0],[0]
"While these results are encouraging, the assumption that the rewards are observed individually in a non-anonymous fashion is limiting for most practical applications with delays (e.g., recall the applications discussed earlier).",1. Introduction,[0],[0]
How big is the price to be paid for receiving only aggregated anonymous feedback?,1. Introduction,[0],[0]
Our main result is to prove that essentially there is no extra price to be paid provided that the value of the expected delay (or a bound on it) is available.,1. Introduction,[0],[0]
"In particular, this means that detailed knowledge of which action led to a particular delayed reward can be replaced by the much weaker requirement that the expected delay, or a bound on it, is known.",1. Introduction,[0],[0]
"Fig. 1 summarizes the relationship between the non-delayed, the delayed and the new problem
by showing the leading terms of the regret.",1. Introduction,[0],[0]
"In all cases, the dominant term is √ KT .",1. Introduction,[0],[0]
"Hence, asymptotically, the delayed, aggregated anonymous feedback problem is no more difficult than the standard multi-armed bandit problem.",1. Introduction,[0],[0]
We now consider what sort of algorithm will be able to achieve the aforementioned results for the MABDAAF problem.,1.1. Our Techniques and Results,[0],[0]
"Since the player only observes delayed, aggregated anonymous rewards, the first problem we face is how to even estimate the mean reward of individual actions.",1.1. Our Techniques and Results,[0],[0]
"Due to the delays and anonymity, it appears that to be able to estimate the mean reward of an action, the player wants to have played it consecutively for long stretches.",1.1. Our Techniques and Results,[0],[0]
"Indeed, if the stretches are sufficiently long compared to the mean delay, the observations received during the stretch will mostly consist of rewards of the action played in that stretch.",1.1. Our Techniques and Results,[0],[0]
"This naturally leads to considering algorithms that switch actions rarely and this is indeed the basis of our approach.
",1.1. Our Techniques and Results,[0],[0]
"Several popular MAB algorithms are based on choosing the action with the largest upper confidence bound (UCB) in each round (e.g., Auer et al., 2002; Cappé et al., 2013).",1.1. Our Techniques and Results,[0],[0]
UCB-style algorithms tend to switch arms frequently and will only play the optimal arm for long stretches if a unique optimal arm exists.,1.1. Our Techniques and Results,[0],[0]
"Therefore, for MABDAAF, we will consider alternative algorithms where arm-switching is more tightly controlled.",1.1. Our Techniques and Results,[0],[0]
The design of such algorithms goes back at least to the work of Agrawal et al. (1988) where the problem of bandits with switching costs was studied.,1.1. Our Techniques and Results,[0],[0]
The general idea of these rarely switching algorithms is to gradually eliminate suboptimal arms by playing arms in phases and comparing each arm’s upper confidence bound to the lower confidence bound of a leading arm at the end of each phase.,1.1. Our Techniques and Results,[0],[0]
"Generally, this sort of rarely switching algorithm switches arms onlyO(log T ) times.",1.1. Our Techniques and Results,[0],[0]
"We base our approach on one such algorithm, the so-called Improved UCB1 algorithm of Auer & Ortner (2010).
",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm alone will not be sufficient for MABDAAF.,1.1. Our Techniques and Results,[0],[0]
"The remaining problem, and where the bulk of our contribution lies, is to construct appropri-
1The adjective “Improved” indicates that the algorithm improves upon the regret bounds achieved by UCB1.",1.1. Our Techniques and Results,[0],[0]
The improvement replaces log(T )/∆j,1.1. Our Techniques and Results,[0],[0]
by log(T∆2j )/∆j,1.1. Our Techniques and Results,[0],[0]
"in the regret bound.
ate confidence bounds and adjust the length of the periods of playing each arm to account for the delayed, aggregated anonymous feedback.",1.1. Our Techniques and Results,[0],[0]
"In particular, in the confidence bounds attention must be paid to fine details: it turns out that unless the variance of the observations is dealt with, there is a blow-up by a multiplicative factor of K. We avoid this by an improved analysis involving Freedman’s inequality (Freedman, 1975).",1.1. Our Techniques and Results,[0],[0]
"Further, to handle the dependencies between the number of plays of each arm and the past rewards, we combine Doob’s optimal skipping theorem (Doob, 1953) and Azuma-Hoeffding inequalities.",1.1. Our Techniques and Results,[0],[0]
Using a rarely switching algorithm for MABDAAF means we must also consider the dependencies between the elimination of arms in one phase and the corruption of observations in the next phase (ie. past plays can influence both whether an arm is still active and the corruption of its next plays).,1.1. Our Techniques and Results,[0],[0]
"We deal with this through careful algorithmic design.
",1.1. Our Techniques and Results,[0],[0]
"Using the above, we provide an algorithm that achieves worst case regret of O( √ KT logK + KE[τ ] log T ) using only knowledge of the expected delay, E[τ ].",1.1. Our Techniques and Results,[0],[0]
We then show that this regret can be improved by using a more careful martingale argument that exploits the fact that our algorithm is designed to remove most of the dependence between the corruption of future observations and elimination of arms.,1.1. Our Techniques and Results,[0],[0]
"Particularly, if the delays are bounded with known bound 0 ≤",1.1. Our Techniques and Results,[0],[0]
"d ≤ √ T/K, we can recover worst case regret ofO( √ KT logK+KE[τ ]), matching that of Joulani et al. (2013).",1.1. Our Techniques and Results,[0],[0]
"If the delays are unbounded but have known variance V(τ), we show that the problem independent regret can be reduced to O( √ KT logK +KE[τ ] +KV(τ)).",1.1. Our Techniques and Results,[0],[0]
We have already discussed several of the most relevant works to our own.,1.2. Related Work,[0],[0]
"However, there has also been other work looking at different flavors of the bandit problem with delayed (non-anonymous) feedback.",1.2. Related Work,[0],[0]
"For example, Neu et al. (2010) and Cesa-Bianchi et al. (2016) consider nonstochastic bandits with fixed constant delays; Dudik et al. (2011) look at stochastic contextual bandits with a constant delay and Desautels et al. (2014) consider Gaussian Process bandits with a bounded stochastic delay.",1.2. Related Work,[0],[0]
The general observation that delay causes an additive regret penalty in stochastic bandits and a multiplicative one in adversarial bandits is made in Joulani et al. (2013).,1.2. Related Work,[0],[0]
The empirical performance of K-armed stochastic bandit algorithms in delayed settings was investigated in Chapelle & Li (2011).,1.2. Related Work,[0],[0]
A further related problem is the ‘batched bandit’ problem studied by Perchet et al. (2016).,1.2. Related Work,[0],[0]
Here the player must fix a set of time points at which to collect feedback on all plays leading up to that point.,1.2. Related Work,[0],[0]
"Vernade et al. (2017) consider delayed Bernoulli bandits where some observations could also be censored (e.g., no conversion is ever actually observed if the delay exceeds some threshold) but require
complete knowledge of the delay distribution.",1.2. Related Work,[0],[0]
"Crucially, here and in all the aforementioned works, the feedback is always assumed to take the form of arm-reward pairs and knowledge of the assignment of rewards to arms underpins the suggested algorithms, rendering them unsuitable for MABDAAF.",1.2. Related Work,[0],[0]
"To the best of our knowledge, ours is the first work to develop algorithms to deal with delayed, aggregated anonymous feedback in the bandit setting.",1.2. Related Work,[0],[0]
The reminder of this paper is organized as follows: In the next section (Section 2) we give the formal problem definition.,1.3. Organization,[0],[0]
We present our algorithm in Section 3.,1.3. Organization,[0],[0]
"In Section 4, we discuss the performance of our algorithm under various delay assumptions; known expectation, bounded support with known bound and expectation, and known variance and expectation.",1.3. Organization,[0],[0]
This is followed by a numerical illustration of our results in Section 5.,1.3. Organization,[0],[0]
We conclude in Section 6.,1.3. Organization,[0],[0]
There are K > 1 actions or arms in the set A. Each action j ∈,2. Problem Definition,[0],[0]
A is associated with a reward distribution ζj and a delay distribution δj .,2. Problem Definition,[0],[0]
"The reward distribution is supported in [0, 1] and the delay distribution is supported on N .=",2. Problem Definition,[0],[0]
"{0, 1, . . .",2. Problem Definition,[0],[0]
}.,2. Problem Definition,[0],[0]
"We denote by µj the mean of ζj , µ∗ = µj∗ = maxj µj and define ∆j = µ∗ − µj to be the reward gap, that is the expected loss of reward each time action j is chosen instead of an optimal action.",2. Problem Definition,[0],[0]
"Let (Rl,j , τl,j)l∈N,j∈A be an infinite array of random variables defined on the probability space (Ω,Σ, P ) which are mutually independent.",2. Problem Definition,[0],[0]
"Further, Rl,j follows the distribution ζj and τl,j follows the distribution δj .",2. Problem Definition,[0],[0]
"The meaning of these random variables is that if the player plays action j at time l, a payoff of Rl,j will be added to the aggregated feedback that the player receives at the end of the (l + τl,j)th play.",2. Problem Definition,[0],[0]
"Formally, if Jl ∈ A denotes the action chosen by the player at time l = 1, 2, . . .",2. Problem Definition,[0],[0]
", then the observation received at the end of the tth play is
Xt = t∑ l=1",2. Problem Definition,[0],[0]
K∑ j=1,2. Problem Definition,[0],[0]
"Rl,j × I{l + τl,j = t, Jl = j}.
",2. Problem Definition,[0],[0]
"For the remainder, we will consider i.i.d. delays across arms.",2. Problem Definition,[0],[0]
"We also assume discrete delay distributions, although most results hold for continuous delays by redefining the event {τl,j = t− l} as {t− l− 1 < τl,j ≤ t− l} in Xt.",2. Problem Definition,[0],[0]
"In our analysis, we will sum over stochastic index sets.",2. Problem Definition,[0],[0]
For a stochastic index set I and random variables {Zn}n∈N we denote such sums as ∑ t∈I Zt .,2. Problem Definition,[0],[0]
= ∑ t∈N I{t ∈,2. Problem Definition,[0],[0]
"I} × Zt.
",2. Problem Definition,[0],[0]
Regret definition,2. Problem Definition,[0],[0]
"In most bandit problems, the regret is the cumulative loss due to not playing an optimal action.
",2. Problem Definition,[0],[0]
"In the case of delayed feedback, there are several possible ways to define the regret.",2. Problem Definition,[0],[0]
One option is to consider only the loss of the rewards received before horizon T (as in Vernade et al. (2017)).,2. Problem Definition,[0],[0]
"However, we will not use this definition.",2. Problem Definition,[0],[0]
"Instead, as in Joulani et al. (2013), we consider the loss of all generated rewards and define the (pseudo-)regret by
RT = T∑ t=1",2. Problem Definition,[0],[0]
(µ∗ − µJt) = Tµ∗,2. Problem Definition,[0],[0]
"− T∑ t=1 µJt .
",2. Problem Definition,[0],[0]
This includes the rewards received after the horizon T and does not penalize large delays as long as an optimal action is taken.,2. Problem Definition,[0],[0]
"This definition is natural since, in practice, the player should eventually receive all outstanding reward.
",2. Problem Definition,[0],[0]
"Lai & Robbins (1985) showed that the regret of any algorithm for the standard MAB problem must satisfy,
lim inf T→∞",2. Problem Definition,[0],[0]
"E[RT ] log(T )
",2. Problem Definition,[0],[0]
"≥ ∑
j:∆j>0
∆j",2. Problem Definition,[0],[0]
"KL(ζj , ζ∗) , (1)
where KL(ζj , ζ∗) is the KL-divergence between the reward distributions of arm j and an optimal arm.",2. Problem Definition,[0],[0]
Theorem 4 of Vernade et al. (2017) shows that the lower bound in (1) also holds for delayed feedback bandits with no censoring and their alternative definition of regret.,2. Problem Definition,[0],[0]
We therefore suspect (1) should hold for MABDAAF.,2. Problem Definition,[0],[0]
"However, due to the specific problem structure, finding a lower bound for MABDAAF is non-trivial and remains an open problem.
",2. Problem Definition,[0],[0]
"Assumptions on delay distribution For our algorithm for MABDAAF, we need some assumptions on the delay distribution.",2. Problem Definition,[0],[0]
"We assume that the expected delay, E[τ ], is bounded and known.",2. Problem Definition,[0],[0]
"This quantity is used in the algorithm.
",2. Problem Definition,[0],[0]
Assumption 1,2. Problem Definition,[0],[0]
"The expected delay E[τ ] is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
"We then show that under some further mild assumptions on the delay, we can obtain better algorithms with even more efficient regret guarantees.",2. Problem Definition,[0],[0]
"We consider two settings: delay distributions with bounded support, and bounded variance.
",2. Problem Definition,[0],[0]
Assumption 2 (Bounded support),2. Problem Definition,[0],[0]
There exists some constant d > 0 known to the algorithm,2. Problem Definition,[0],[0]
"such that the support of the delay distribution is bounded by d.
Assumption 3 (Bounded variance)",2. Problem Definition,[0],[0]
"The variance, V(τ), of the delay is bounded and known to the algorithm.
",2. Problem Definition,[0],[0]
In fact the known expected value and known variance assumption can be replaced by a ‘known upper bound’ on the expected value and variance respectively.,2. Problem Definition,[0],[0]
"However, for simplicity, in the remaining, we use E[τ ] and V(τ) directly.",2. Problem Definition,[0],[0]
The next sections provide algorithms and regret analysis for different combinations of the above assumptions.,2. Problem Definition,[0],[0]
Our algorithm is a phase-based elimination algorithm based on the Improved UCB algorithm by Auer & Ortner (2010).,3. Our Algorithm,[0],[0]
The general structure is as follows.,3. Our Algorithm,[0],[0]
"In each phase, each arm is played multiple times consecutively.",3. Our Algorithm,[0],[0]
"At the end of the phase, the observations received are used to update mean estimates, and any arm with an estimated mean below the best estimated mean by a gap larger than a ‘separation gap tolerance’ is eliminated.",3. Our Algorithm,[0],[0]
"This separation tolerance is decreased exponentially over phases, so that it is very small in later phases, eliminating all but the best arm(s) with high probability.",3. Our Algorithm,[0],[0]
"An alternative formulation of the algorithm is that at the end of a phase, any arm with an upper confidence bound lower than the best lower confidence bound is eliminated.",3. Our Algorithm,[0],[0]
"These confidence bounds are computed so that with high probability they are more (less) than the true mean, but within the separation gap tolerance.",3. Our Algorithm,[0],[0]
The phase lengths are then carefully chosen to ensure that the confidence bounds hold.,3. Our Algorithm,[0],[0]
"Here we assume that the horizon T is known, but we expect that this can be relaxed as in Auer & Ortner (2010).
",3. Our Algorithm,[0],[0]
"Algorithm overview Our algorithm, ODAAF, is given in Algorithm 1.",3. Our Algorithm,[0],[0]
"It operates in phases m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Define Am to be the set of active arms in phase m. The algorithm takes parameter nm which defines the number of samples of each active arm required by the end of phase m.
In Step 1 of phase m of the algorithm, each active arm j is played repeatedly for nm − nm−1 steps.",3. Our Algorithm,[0],[0]
We record all timesteps where arm j was played in the first m phases (excluding bridge periods) in the set Tj(m).,3. Our Algorithm,[0],[0]
The active arms are played in any arbitrary but fixed order.,3. Our Algorithm,[0],[0]
"In Step 2, the nm observations from timesteps in Tj(m) are averaged to obtain a new estimate X̄m,j of µj .",3. Our Algorithm,[0],[0]
"Arm j is eliminated if X̄m,j is further than ∆̃m from maxj′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
A further nuance in the algorithm structure is the ‘bridge period’ (see Figure 2).,3. Our Algorithm,[0],[0]
The algorithm picks an active arm j ∈ Am+1 to play in this bridge period for nm − nm−1 steps.,3. Our Algorithm,[0],[0]
"The observations received during the bridge period are discarded, and not used for computing confidence intervals.",3. Our Algorithm,[0],[0]
The significance of the bridge period is that it breaks the dependence between confidence intervals calculated in phasem and the delayed payoffs seeping into phasem+1.,3. Our Algorithm,[0],[0]
Without the bridge period this dependence would impair the validity of our confidence intervals.,3. Our Algorithm,[0],[0]
"However, we suspect that, in practice, it may be possible to remove it.
",3. Our Algorithm,[0],[0]
Choice of nm A key element of our algorithm design is the careful choice of nm.,3. Our Algorithm,[0],[0]
"Since nm determines the number of times each active (possibly suboptimal) arm is played, it clearly has an impact on the regret.",3. Our Algorithm,[0],[0]
"Furthermore, nm needs to be chosen so that the confidence bounds on the estimation error hold with given probability.",3. Our Algorithm,[0],[0]
"The main chal-
Algorithm 1 Optimism for Delayed, Aggregated Anonymous Feedback (ODAAF)",3. Our Algorithm,[0],[0]
"Input: A set of arms, A; a horizon, T ; choice of nm for
each phase m = 1, 2, . .",3. Our Algorithm,[0],[0]
..,3. Our Algorithm,[0],[0]
"Initialization: Set ∆̃1 = 1/2 (tolerance), the set of active
arms A1 = A. Let Ti(1) =",3. Our Algorithm,[0],[0]
"∅, i ∈",3. Our Algorithm,[0],[0]
"A, m = 1 (phase index), t = 1 (round index) while t ≤ T do
Step 1: Play arms.",3. Our Algorithm,[0],[0]
"for j ∈ Am do
Let Tj(m) =",3. Our Algorithm,[0],[0]
"Tj(m− 1) while |Tj(m)| ≤ nm and t ≤ T do
Play arm j, receive Xt.",3. Our Algorithm,[0],[0]
Add t to Tj(m).,3. Our Algorithm,[0],[0]
Increment t by 1. end while end for Step 2: Eliminate sub-optimal arms.,3. Our Algorithm,[0],[0]
"For every arm in j ∈ Am, compute X̄m,j as the average of observations at time steps t ∈ Tj(m).",3. Our Algorithm,[0],[0]
"That is,
X̄m,j = 1 |Tj(m)| ∑
t∈Tj(m)
",3. Our Algorithm,[0],[0]
"Xt .
",3. Our Algorithm,[0],[0]
"Construct Am+1 by eliminating actions j ∈ Am with
X̄m,j +",3. Our Algorithm,[0],[0]
"∆̃m < max j′∈Am X̄m,j′ .
",3. Our Algorithm,[0],[0]
Step 3:,3. Our Algorithm,[0],[0]
"Decrease Tolerance.
",3. Our Algorithm,[0],[0]
Set ∆̃m+1 = ∆̃m2 .,3. Our Algorithm,[0],[0]
Step 4: Bridge period.,3. Our Algorithm,[0],[0]
Pick an arm j ∈ Am+1 and play it νm = nm − nm−1 times while incrementing t ≤ T .,3. Our Algorithm,[0],[0]
Discard all observations from this period.,3. Our Algorithm,[0],[0]
Do not add t to Tj(m).,3. Our Algorithm,[0],[0]
"Increment phase index m.
end while
lenge is developing these confidence bounds from delayed, aggregated anonymous feedback.",3. Our Algorithm,[0],[0]
"Handling this form of feedback involves a credit assignment problem of deciding which samples can be used for a given arm’s mean estimation, since each sample is an aggregate of rewards from multiple previously played arms.",3. Our Algorithm,[0],[0]
This credit assignment problem would be hopeless in a passive learning setting without further information on how the samples were generated.,3. Our Algorithm,[0],[0]
"Our algorithm utilizes the power of active learning to design the phases in such a way that the feedback can be effectively ‘decensored’ without losing too many samples.
",3. Our Algorithm,[0],[0]
"A naive approach to defining the confidence bounds for delays bounded by a constant d ≥ 0 would be to observe that,∣∣∣∣ ∑
t∈Tj(m)\Tj(m−1)
",3. Our Algorithm,[0],[0]
Xt,3. Our Algorithm,[0],[0]
"− ∑
t∈Tj(m)\Tj(m−1)
Rt,j ∣∣∣∣ ≤ d,
since all rewards are in [0, 1].",3. Our Algorithm,[0],[0]
"Then we could use Hoeffding’s inequality to boundRt,Jt (see Appendix F) and select
nm = C1 log(T ∆̃
2 m)
∆̃2m + C2md ∆̃m
for some constants C1, C2.",3. Our Algorithm,[0],[0]
This corresponds to worst case regret of O( √ KT logK + K log(T )d).,3. Our Algorithm,[0],[0]
"For d E[τ ] and large T , this is significantly worse than that of Joulani et al. (2013).",3. Our Algorithm,[0],[0]
"In Section 4, we show that, surprisingly, it is possible to recover the same rate of regret as Joulani et al. (2013), but this requires a significantly more nuanced argument to get tighter confidence bounds and smaller nm.",3. Our Algorithm,[0],[0]
"In the next section, we describe this improved choice of nm for every phase m ∈ N and its implications on the regret, for each of the three cases mentioned previously: (i) Known and bounded expected delay (Assumption 1), (ii) Bounded delay with known bound and expected value (Assumptions 1 and 2), (iii) Delay with known and bounded variance and expectation (Assumptions 1 and 3).",3. Our Algorithm,[0],[0]
"In this section, we specify the choice of parameters nm and provide regret guarantees for Algorithm 1 for each of the three previously mentioned cases.",4. Regret Analysis,[0],[0]
"First, we consider the setting with the weakest assumption on delay distribution: we only assume that the expected delay, E[τ ], is bounded and known.",4.1. Known and Bounded Expected Delay,[0],[0]
No assumption on the support or variance of the delay distribution is made.,4.1. Known and Bounded Expected Delay,[0],[0]
"The regret analysis for this setting will not use the bridge period, so Step 4 of the algorithm could be omitted in this case.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Choice of nm Here, we use Algorithm 1 with
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2mE[τ ] ∆̃m (2)
for some large enough constants C1, C2.",4.1. Known and Bounded Expected Delay,[0],[0]
"The exact value of nm is given in Equation (14) in Appendix B.
Estimation of error bounds We bound the error between X̄m,j and µj by ∆̃m/2.",4.1. Known and Bounded Expected Delay,[0],[0]
"In order to do this we first bound the corruption of the observations received during timesteps Tj(m) due to delays.
",4.1. Known and Bounded Expected Delay,[0],[0]
Fix a phase m and arm j ∈ Am.,4.1. Known and Bounded Expected Delay,[0],[0]
Then the observations Xt in the period t ∈ Tj(m),4.1. Known and Bounded Expected Delay,[0],[0]
"\ Tj(m− 1) are composed of two types of rewards: a subset of rewards from plays of arm j in this period, and delayed rewards from some of the plays before this period.",4.1. Known and Bounded Expected Delay,[0],[0]
The expected value of observations from this period would be (nm − nm−1)µj but for the rewards entering and leaving this period due to delay.,4.1. Known and Bounded Expected Delay,[0],[0]
"Since the reward is bounded by 1, a simple observation is that expected discrepancy between the sum of observations in this period and the quantity (nm − nm−1)µj is bounded by the expected delay E[τ",4.1. Known and Bounded Expected Delay,[0],[0]
"],
E  ∑ t∈Tj(m)\Tj(m−1)",4.1. Known and Bounded Expected Delay,[0],[0]
(Xt − µj)  ≤ E[τ ].,4.1. Known and Bounded Expected Delay,[0],[0]
"(3) Summing this over phases ` = 1, . .",4.1. Known and Bounded Expected Delay,[0],[0]
".m gives a bound
|E[X̄m,j ]",4.1. Known and Bounded Expected Delay,[0],[0]
− µj | ≤ mE[τ ] |Tj(m)| = mE[τ ] nm .,4.1. Known and Bounded Expected Delay,[0],[0]
"(4)
Note that given the choice of nm in (2), the above is smaller than ∆̃m/2, when large enough constants are used.",4.1. Known and Bounded Expected Delay,[0],[0]
"Using this, along with concentration inequalities and the choice of nm from (2), we can obtain the following high probability bound.",4.1. Known and Bounded Expected Delay,[0],[0]
"A detailed proof is provided in Appendix B.1.
",4.1. Known and Bounded Expected Delay,[0],[0]
Lemma 1,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1 and the choice of nm given by (2), the estimates X̄m,j constructed by Algorithm 1 satisfy the following: For every fixed arm j and phase m, with probability 1− 3
T ∆̃2m , either j /∈",4.1. Known and Bounded Expected Delay,[0],[0]
"Am, or:
X̄m,j − µj ≤",4.1. Known and Bounded Expected Delay,[0],[0]
"∆̃m/2 .
",4.1. Known and Bounded Expected Delay,[0],[0]
"Regret bounds Using Lemma 1, we derive the following regret bounds in the current setting.
",4.1. Known and Bounded Expected Delay,[0],[0]
Theorem 2,4.1. Known and Bounded Expected Delay,[0],[0]
"Under Assumption 1, the expected regret of Algorithm 1 is upper bounded as
E[RT ] ≤ K∑ j=1 j 6=j∗ O ( log(T∆2j ) ∆j + log(1/∆j)E[τ ] ) .",4.1. Known and Bounded Expected Delay,[0],[0]
"(5)
Proof: Given Lemma 1, the proof of Theorem 2 closely follows the analysis of the Improved UCB algorithm of Auer & Ortner (2010).",4.1. Known and Bounded Expected Delay,[0],[0]
"Lemma 1 and the elimination condition in Algorithm 1 ensure that, with high probability, any suboptimal arm j will be eliminated by phase mj = log(1/∆j), thus incurring regret at most nmj∆j",4.1. Known and Bounded Expected Delay,[0],[0]
"We then substitute in nmj from (2), and sum over all suboptimal arms.",4.1. Known and Bounded Expected Delay,[0],[0]
A detailed proof is in Appendix B.2.,4.1. Known and Bounded Expected Delay,[0],[0]
"As in Auer & Ortner (2010), we avoid a union bound over all arms (which would result in an extra logK) by (i) reasoning about the regret of each arm individually, and (ii) bounding the regret resulting
from erroneously eliminating the optimal arm by carefully controlling the probability it is eliminated in each phase.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Considering the worst-case values of ∆j (roughly √ K/T ), we obtain the following problem independent bound.
",4.1. Known and Bounded Expected Delay,[0],[0]
"Corollary 3 For any problem instance satisfying Assumption 1, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.1. Known and Bounded Expected Delay,[0],[0]
O( √ KT log(K) +KE[τ ] log(T )).,4.1. Known and Bounded Expected Delay,[0],[0]
"If the delay is bounded by some constant d ≥ 0 and a single arm is played repeatedly for long enough, we can restrict the number of arms corrupting the observation",4.2. Delay with Bounded Support,[0],[0]
Xt at a given time t.,4.2. Delay with Bounded Support,[0],[0]
"In fact, if each arm j is played consecutively for more than d rounds, then at any time t ∈ Tj(m), the observation Xt will be composed of the rewards from at most two arms: the current arm j, and previous arm j′. Further, from the elimination condition, with high probability, arm j′ will have been eliminated if it is clearly suboptimal.",4.2. Delay with Bounded Support,[0],[0]
We can then recursively use the confidence bounds for arms j and j′ from the previous phase to bound |µj,4.2. Delay with Bounded Support,[0],[0]
− µj′ |.,4.2. Delay with Bounded Support,[0],[0]
"Below, we formalize this intuition to obtain a tighter bound on |X̄m,j − µj",4.2. Delay with Bounded Support,[0],[0]
|,4.2. Delay with Bounded Support,[0],[0]
"for every arm j and phase m, when each active arm is played a specified number of times per phase.
",4.2. Delay with Bounded Support,[0],[0]
"Choice of nm Here, we define,
nm = C1 log(T ∆̃
2 m) ∆̃2m + C2E[τ ]",4.2. Delay with Bounded Support,[0],[0]
"∆̃m (6)
+",4.2. Delay with Bounded Support,[0],[0]
"min { md, C3 log(T ∆̃ 2 m)
∆̃2m + C4mE[τ ] ∆̃m } for some large enough constants C1, C2, C3, C4 (see Appendix C, Equation (18) for the exact values).",4.2. Delay with Bounded Support,[0],[0]
"This choice of nm means that for large d, we essentially revert back to the choice of nm from (2) for the unbounded case, and we gain nothing by using the bound on the delay.",4.2. Delay with Bounded Support,[0],[0]
"However, if d is not large, the choice of nm in (6) is smaller than (2) since the second term now scales with E[τ ] rather than mE[τ ].
",4.2. Delay with Bounded Support,[0],[0]
"Estimation of error bounds In this setting, by the elimination condition and bounded delays, the expectation of each reward entering Tj(m) will be within ∆̃m−1 of µj , with high probability.",4.2. Delay with Bounded Support,[0],[0]
"Then, using knowledge of the upper bound of the support of τ , we can obtain a tighter bound and get an error bound similar to Lemma 1 with the smaller value of nm in (6).",4.2. Delay with Bounded Support,[0],[0]
We prove the following proposition.,4.2. Delay with Bounded Support,[0],[0]
"Since ∆̃m = 2−m, this is considerably tighter than (3).
",4.2. Delay with Bounded Support,[0],[0]
Proposition 4,4.2. Delay with Bounded Support,[0],[0]
"Assume ni − ni−1 ≥ d for phases i = 1, . . .",4.2. Delay with Bounded Support,[0],[0]
",m. Define Em−1 as the event that all arms j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am satisfy error bounds |X̄m−1,j − µj | ≤ ∆̃m−1/2.",4.2. Delay with Bounded Support,[0],[0]
"Then, for
every arm j ∈",4.2. Delay with Bounded Support,[0],[0]
"Am,
E  ∑ t∈Tj(m)\Tj(m−1)",4.2. Delay with Bounded Support,[0],[0]
(Xt − µj) ∣∣∣∣Em−1  ≤ ∆̃m−1E[τ ].,4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
Consider a fixed arm j ∈ Am.,4.2. Delay with Bounded Support,[0],[0]
The expected value of the sum of observations Xt for t ∈ Tj(m),4.2. Delay with Bounded Support,[0],[0]
\ Tj(m− 1) would be (nm − nm−1)µj were it not for some rewards entering and leaving this period due to the delays.,4.2. Delay with Bounded Support,[0],[0]
"Because of the i.i.d. assumption on the delay, in expectation, the number of rewards leaving the period is roughly the same as the number of rewards entering this period, i.e., E[τ ].",4.2. Delay with Bounded Support,[0],[0]
(Conditioning on Em−1 does not effect this due to the bridge period).,4.2. Delay with Bounded Support,[0],[0]
"Since nm − nm−1 ≥ d, the reward coming into the period Tj(m)\Tj(m−1) can only be from the previous arm j′. All rewards leaving the period are from arm j. Therefore the expected difference between rewards entering and leaving the period is (µj − µj′)E[τ ].",4.2. Delay with Bounded Support,[0],[0]
"Then, if µj is close to µj′ , the total reward leaving the period is compensated by total reward entering.",4.2. Delay with Bounded Support,[0],[0]
"Due to the bridge period, even when j is the first arm played in phase m, j′ ∈ Am, so it was not eliminated in phase m − 1.",4.2. Delay with Bounded Support,[0],[0]
"By the elimination condition in Algorithm 1, if the error bounds |X̄m−1,j−µj | ≤ ∆̃m−1/2 are satisfied for all arms in Am, then |µj",4.2. Delay with Bounded Support,[0],[0]
− µj′ | ≤ ∆̃m−1.,4.2. Delay with Bounded Support,[0],[0]
"This gives the result.
",4.2. Delay with Bounded Support,[0],[0]
"Repeatedly using Proposition 4 we get,
m∑ i=1",4.2. Delay with Bounded Support,[0],[0]
E  ∑ t∈Tj(i)\Tj(i−1) (Xt − µj) ∣∣∣∣Ei−1  ≤ 2E[τ ] since ∑m i=1,4.2. Delay with Bounded Support,[0],[0]
"∆̃i−1 = ∑m−1 i=0 2
−i ≤ 2.",4.2. Delay with Bounded Support,[0],[0]
"Then, observe that P(ECi ) is small.",4.2. Delay with Bounded Support,[0],[0]
This bound is an improvement of a factor of m compared to (4).,4.2. Delay with Bounded Support,[0],[0]
"For the regret analysis, we derive a high probability version of the above result.",4.2. Delay with Bounded Support,[0],[0]
"Using this, and the choice of nm ≥ Ω ( log(T ∆̃2m)
",4.2. Delay with Bounded Support,[0],[0]
"∆̃2m + E[τ ] ∆̃m
) from (6), for
large enough constants, we derive the following lemma.",4.2. Delay with Bounded Support,[0],[0]
"A detailed proof is given in Appendix C.1.
",4.2. Delay with Bounded Support,[0],[0]
Lemma 5,4.2. Delay with Bounded Support,[0],[0]
"Under Assumptions 1 of known expected delay and 2 of bounded delays, and choice of nm given in (6), the estimates X̄m,j obtained by Algorithm 1 satisfy the following: For any arm j and phase m, with probability at least 1− 12
T ∆̃2m , either j /∈",4.2. Delay with Bounded Support,[0],[0]
"Am or
X̄m,j − µj ≤ ∆̃m/2.
",4.2. Delay with Bounded Support,[0],[0]
"Regret bounds We now give regret bounds for this case.
",4.2. Delay with Bounded Support,[0],[0]
Theorem 6,4.2. Delay with Bounded Support,[0],[0]
"Under Assumption 1 and bounded delay Assumption 2, the expected regret of Algorithm 1 satisfies
E[RT ]",4.2. Delay with Bounded Support,[0],[0]
"≤ K∑
j=1;j 6=j∗ O
( log(T∆2j )
∆j + E[τ",4.2. Delay with Bounded Support,[0],[0]
"]
+ min { d, log(T∆2j )
",4.2. Delay with Bounded Support,[0],[0]
"∆j + log(
1
∆j )",4.2. Delay with Bounded Support,[0],[0]
"E[τ ]
}) .
",4.2. Delay with Bounded Support,[0],[0]
Proof: (Sketch).,4.2. Delay with Bounded Support,[0],[0]
"Given Lemma 5, the proof is similar to that of Theorem 2.",4.2. Delay with Bounded Support,[0],[0]
"The full proof is in Appendix C.2.
",4.2. Delay with Bounded Support,[0],[0]
"Then, if d ≤ √
T logK K + E[τ ], we get the following
problem independent regret bound which matches that of Joulani et al. (2013).
",4.2. Delay with Bounded Support,[0],[0]
Corollary 7 For any problem instance satisfying Assumptions 1 and 2 with d ≤,4.2. Delay with Bounded Support,[0],[0]
"√ T logK K +E[τ ], the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.2. Delay with Bounded Support,[0],[0]
O( √ KT log(K) +KE[τ ]).,4.2. Delay with Bounded Support,[0],[0]
"If the delay is unbounded but well behaved in the sense that we know (a bound on) the variance, then we can obtain similar regret bounds to the bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
"Intuitively, delays from the previous phase will only corrupt observations in the current phase if their delays exceed the length of the bridge period.",4.3. Delay with Bounded Variance,[0],[0]
"We control this by using the bound on the variance to bound the tails of the delay distributions.
",4.3. Delay with Bounded Variance,[0],[0]
"Choice of nm Let V(τ) be the known variance (or bound on the variance) of the delay, as in Assumption 3.",4.3. Delay with Bounded Variance,[0],[0]
"Then, we use Algorithm 1 with the following value of nm,
nm = C1 log(T ∆̃2m)
∆̃2m + C2 E[τ ] + V(τ) ∆̃m
(7)
for some large enough constants C1, C2.",4.3. Delay with Bounded Variance,[0],[0]
"The exact value of nm is given in Appendix D, Equation (25).
",4.3. Delay with Bounded Variance,[0],[0]
"Regret bounds We get the following instance specific and problem independent regret bound in this case.
",4.3. Delay with Bounded Variance,[0],[0]
Theorem 8,4.3. Delay with Bounded Variance,[0],[0]
"Under Assumption 1 and Assumption 3 of known (bound on) the expectation and variance of the delay, and choice of nm from (7), the expected regret of Algorithm 1 can be upper bounded by,
E[RT ]",4.3. Delay with Bounded Variance,[0],[0]
"≤ K∑
j=1:µj 6=µ∗ O
( log(T∆2j )
∆j + E[τ ] + V(τ)
) .
",4.3. Delay with Bounded Variance,[0],[0]
Proof: (Sketch).,4.3. Delay with Bounded Variance,[0],[0]
See Appendix D.2.,4.3. Delay with Bounded Variance,[0],[0]
"We use Chebychev’s inequality to get a result similar to Lemma 5 and then use a similar argument to the bounded delay case.
",4.3. Delay with Bounded Variance,[0],[0]
"Corollary 9 For any problem instance satisfying Assumptions 1 and 3, the expected regret of Algorithm 1 satisfies
E[RT ] ≤",4.3. Delay with Bounded Variance,[0],[0]
"O( √ KT log(K) +KE[τ ] +KV(τ)).
",4.3. Delay with Bounded Variance,[0],[0]
"Remark If E[τ ] ≥ 1, then the delay penalty can be reduced to O(KE[τ ] +KV(τ)/E[τ ]) (see Appendix D).
",4.3. Delay with Bounded Variance,[0],[0]
"Thus, it is sufficient to know a bound on variance to obtain regret bounds similar to those in bounded delay case.",4.3. Delay with Bounded Variance,[0],[0]
Note that this approach is not possible just using knowledge of the expected delay since we cannot guarantee that the reward entering phase i is from an arm active in phase i− 1.,4.3. Delay with Bounded Variance,[0],[0]
"We compared the performance of our algorithm (under different assumptions) to QPM-D (Joulani et al., 2013) in various experimental settings.",5. Experimental Results,[0],[0]
"In these experiments, our aim was to investigate the effect of the delay on the performance of the algorithms.",5. Experimental Results,[0],[0]
"In order to focus on this, we used a simple setup of two arms with Bernoulli rewards and µ = (0.5, 0.6).",5. Experimental Results,[0],[0]
"In every experiment, we ran each algorithm to horizon T = 250000 and used UCB1 (Auer et al., 2002) as the base algorithm in QPM-D. The regret was averaged over 200 replications.",5. Experimental Results,[0],[0]
"For ease of reading, we define ODAAF to be our algorithm using only knowledge of the expected delay, with nm defined as in (2) and run without a bridge period, and ODAAF-B and ODAAF-V to be the versions of Algorithm 1 that use a bridge period and information on the bounded support and the finite variance of the delay to define nm as in (6) and (7) respectively.
",5. Experimental Results,[0],[0]
We tested the algorithms with different delay distributions.,5. Experimental Results,[0],[0]
"In the first case, we considered bounded delay distributions whereas in the second case, the delays were unbounded.",5. Experimental Results,[0],[0]
"In Fig. 3a, we plotted the ratios of the regret of ODAAF and ODAAF-B (with knowledge of d, the delay bound) to the regret of QPM-D. We see that in all cases the ratios converge to a constant.",5. Experimental Results,[0],[0]
"This shows that the regret of our algorithm is essentially of the same order as that of QPM-D. Our algorithm predetermines the number of times to play each active arm per phase (the randomness appears in whether an arm is active), so the jumps in the regret are it changing arm.",5. Experimental Results,[0],[0]
"This occurs at the same points in all replications.
",5. Experimental Results,[0],[0]
Fig.,5. Experimental Results,[0],[0]
3b shows a similar story for unbounded delays with mean E[τ ],5. Experimental Results,[0],[0]
= 50 (where N+ denotes the the half normal distribution).,5. Experimental Results,[0],[0]
The ratios of the regret of ODAAF and ODAAF-V (with knowledge of the delay variance) to the regret of QPM-D again converge to constants.,5. Experimental Results,[0],[0]
"Note that in this case, these constants, and the location of the jumps, vary with the delay distribution and V(τ).",5. Experimental Results,[0],[0]
"When the variance of the delay is small, it can be seen that using the variance information leads to improved performance.",5. Experimental Results,[0],[0]
"However, for exponential delays where V(τ) = E[τ ]2, the large variance causes nm to be large and so the suboptimal arm is played more, increasing the regret.",5. Experimental Results,[0],[0]
"In this case ODAAF-V had only just eliminated the suboptimal arm at time T .
",5. Experimental Results,[0],[0]
It can also be illustrated experimentally that the regret of our algorithms and that of QPM-D all increase linearly in E[τ ].,5. Experimental Results,[0],[0]
This is shown in Appendix E. We also provide an experimental comparison to Vernade et al. (2017) in Appendix E.,5. Experimental Results,[0],[0]
"We have studied an extension of the multi-armed bandit problem to bandits with delayed, aggregated anonymous feedback.",6. Conclusion,[0],[0]
"Here, a sum of observations is received after some stochastic delay and we do not learn which arms contributed to each observation.",6. Conclusion,[0],[0]
"In this more difficult setting, we have proven that, surprisingly, it is possible to develop an algorithm that performs comparably to those for the simpler delayed feedback bandits problem, where the assignment of rewards to plays is known.",6. Conclusion,[0],[0]
"Particularly, using only knowledge of the expected delay, our algorithm matches the worst case regret of Joulani et al. (2013) up to a logarithmic factor.",6. Conclusion,[0],[0]
"This logarithmic factors can be removed using an improved analysis and slightly more information about the delay; if the delay is bounded, we achieve the same worst case regret as Joulani et al. (2013), and for unbounded delays with known finite variance, we have an extra additive V(τ) term.",6. Conclusion,[0],[0]
We supported these claims experimentally.,6. Conclusion,[0],[0]
"Note that while our algorithm matches the order of regret of QPM-D, the constants are worse.",6. Conclusion,[0],[0]
"Hence, it is an open problem to find algorithms with better constants.",6. Conclusion,[0],[0]
CPB would like to thank the EPSRC funded EP/L015692/1 STOR-i centre for doctoral training and Sparx.,Acknowledgments,[0],[0]
We would like to thank the reviewers for their helpful comments.,Acknowledgments,[0],[0]
"We study a variant of the stochastic K-armed bandit problem, which we call “bandits with delayed, aggregated anonymous feedback”.",abstractText,[0],[0]
"In this problem, when the player pulls an arm, a reward is generated, however it is not immediately observed.",abstractText,[0],[0]
"Instead, at the end of each round the player observes only the sum of a number of previously generated rewards which happen to arrive in the given round.",abstractText,[0],[0]
"The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a particular reward is lost.",abstractText,[0],[0]
"The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback?",abstractText,[0],[0]
"Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the expected delay.",abstractText,[0],[0]
"In this paper, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the expected delay (or a bound on it) is known.",abstractText,[0],[0]
"We provide an algorithm that matches the worst case regret of the non-anonymous problem exactly when the delays are bounded, and up to logarithmic factors or an additive variance term for unbounded delays.",abstractText,[0],[0]
"Bandits with Delayed, Aggregated Anonymous Feedback",title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3739–3748 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3739",text,[0],[0]
Single-document summarization methods can be divided into two categories: extractive and abstractive.,1 Introduction,[0],[0]
"Extractive summarization systems form summaries by selecting and copying text snippets from the document, while abstractive methods aim to generate concise summaries with paraphrasing.",1 Introduction,[0],[0]
"This work is primarily concerned with extractive
∗Equal contribution.
summarization.",1 Introduction,[0],[0]
"Though abstractive summarization methods have made strides in recent years, extractive techniques are still very attractive as they are simpler, faster, and more reliably yield semantically and grammatically correct sentences.
",1 Introduction,[0],[0]
"Many extractive summarizers work by selecting sentences from the input document (Luhn, 1958; Mihalcea and Tarau, 2004; Wong et al., 2008; Kågebäck et al., 2014; Yin and Pei, 2015; Cao et al., 2015; Yasunaga et al., 2017).",1 Introduction,[0],[0]
"Furthermore, a growing trend is to frame this sentence selection process as a sequential binary labeling problem, where binary inclusion/exclusion labels are chosen for sentences one at a time, starting from the beginning of the document, and decisions about later sentences may be conditioned on decisions about earlier sentences.",1 Introduction,[0],[0]
"Recurrent neural networks may be trained with stochastic gradient ascent to maximize the likelihood of a set of ground-truth binary label sequences (Cheng and Lapata, 2016; Nallapati et al., 2017).",1 Introduction,[0],[0]
"However, this approach has two well-recognized disadvantages.",1 Introduction,[0],[0]
"First, it suffers from exposure bias, a form of mismatch between training and testing data distributions which can hurt performance (Ranzato et al., 2015; Bahdanau et al., 2017; Paulus et al., 2018).",1 Introduction,[0],[0]
"Second, extractive labels must be generated by a heuristic, as summarization datasets do not generally include ground-truth extractive labels; the ultimate performance of models trained on such labels is thus fundamentally limited by the quality of the heuristic.
",1 Introduction,[0],[0]
"An alternative to maximum likelihood training
is to use reinforcement learning to train the model to directly maximize a measure of summary quality, such as the ROUGE score between the generated summary and a ground-truth abstractive summary (Wu and Hu, 2018).",1 Introduction,[0],[0]
"This approach has become popular because it avoids exposure bias, and directly optimizes a measure of summary quality.",1 Introduction,[0],[0]
"However, it also has a number of downsides.",1 Introduction,[0],[0]
"For one, the search space is quite large: for a document of length T , there are 2T possible extractive summaries.",1 Introduction,[0],[0]
This makes the exploration problem faced by the reinforcement learning algorithm during training very difficult.,1 Introduction,[0],[0]
"Another issue is that due to the sequential nature of selection, the model is inherently biased in favor of selecting earlier sentences over later ones, a phenomenon which we demonstrate empirically in Section 7.",1 Introduction,[0],[0]
"The first issue can be resolved to a degree using either a cumbersome maximum likelihood-based pre-training step (using heuristically-generated labels) (Wu and Hu, 2018), or placing a hard upper limit on the number of sentences selected.",1 Introduction,[0],[0]
"The second issue is more problematic, as it is inherent to the sequential binary labeling setting.
",1 Introduction,[0],[0]
"In the current work, we introduce BANDITSUM, a novel method for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"This method does away with the sequential binary labeling setting, instead formulating extractive summarization as a contextual bandit.",1 Introduction,[0],[0]
"This move greatly reduces the size of the space that must be explored, removes the need to perform supervised pre-training, and prevents systematically privileging earlier sentences over later ones.",1 Introduction,[0],[0]
"Although the strong performance of Lead-3 indicates that good sentences often occur early in the source article, we show in Sections 6 and 7 that the contextual bandit setting greatly improves model performance when good sentences occur late without sacrificing performance when good sentences occur early.
",1 Introduction,[0],[0]
"Under this reformulation, BANDITSUM takes the document as input and outputs an affinity for each of the sentences therein.",1 Introduction,[0],[0]
"An affinity is a real number in [0, 1] which quantifies the model’s propensity for including a sentence in the summary.",1 Introduction,[0],[0]
These affinities are then used in a process of repeated sampling-without-replacement which does not privilege earlier sentences over later ones.,1 Introduction,[0],[0]
"BANDITSUM is free to process the document as a whole before yielding affinities, which permits
affinities for different sentences in the document to depend on one another in arbitrary ways.",1 Introduction,[0],[0]
"In our technical section, we show how to apply policy gradient reinforcement learning methods to this setting.
",1 Introduction,[0],[0]
"The contributions of our work are as follows:
• We propose a theoretically grounded method, based on the contextual bandit formalism, for training neural network-based extractive summarizers with reinforcement learning.",1 Introduction,[0],[0]
"Based on this training method, we propose the BANDITSUM system for extractive summarization.
",1 Introduction,[0],[0]
"• We perform experiments demonstrating that BANDITSUM obtains state-of-the-art performance on a number of datasets and requires significantly fewer update steps than competing approaches.
",1 Introduction,[0],[0]
"• We perform human evaluations showing that in the eyes of human judges, summaries created by BANDITSUM are less redundant and of higher overall quality than summaries created by competing approaches.
",1 Introduction,[0],[0]
"• We provide evidence, in the form of experiments in which models are trained on subsets of the data, that the improved performance of BANDITSUM over competitors stems in part from better handling of summary-worthy sentences that come near the end of the document (see Section 7).",1 Introduction,[0],[0]
Extractive summarization has been widely studied in the past.,2 Related Work,[0],[0]
"Recently, neural network-based methods have been gaining popularity over classical methods (Luhn, 1958; Gong and Liu, 2001; Conroy and O’leary, 2001; Mihalcea and Tarau, 2004; Wong et al., 2008), as they have demonstrated stronger performance on large corpora.",2 Related Work,[0],[0]
Central to the neural network-based models is the encoderdecoder structure.,2 Related Work,[0],[0]
"These models typically use either a convolution neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Pei, 2015; Cao et al., 2015), a recurrent neural network (Chung et al., 2014; Cheng and Lapata, 2016; Nallapati et al., 2017), or a combination of the two (Narayan et al., 2018; Wu and Hu, 2018) to create sentence and document representations, using word embeddings (Mikolov et al., 2013; Pennington et al.,
2014) to represent words at the input level.",2 Related Work,[0],[0]
"These vectors are then fed into a decoder network to generate the output summary.
",2 Related Work,[0],[0]
"The use of reinforcement learning (RL) in extractive summarization was first explored by Ryang and Abekawa (2012), who proposed to use the TD(λ) algorithm to learn a value function for sentence selection.",2 Related Work,[0],[0]
Rioux et al. (2014) improved this framework by replacing the learning agent with another TD(λ) algorithm.,2 Related Work,[0],[0]
"However, the performance of their methods was limited by the use of shallow function approximators, which required performing a fresh round of reinforcement learning for every new document to be summarized.",2 Related Work,[0],[0]
"The more recent work of Paulus et al. (2018) and Wu and Hu (2018) use reinforcement learning in a sequential labeling setting to train abstractive and extractive summarizers, respectively, while Chen and Bansal (2018) combines both approaches, applying abstractive summarization to a set of sentences extracted by a pointer network (Vinyals et al., 2015) trained via REINFORCE.",2 Related Work,[0],[0]
"However, pre-training with a maximum likelihood objective is required in all of these models.
",2 Related Work,[0],[0]
The two works most similar to ours are Yao et al. (2018) and Narayan et al. (2018).,2 Related Work,[0],[0]
"Yao et al. (2018) recently proposed an extractive summarization approach based on deep Q learning, a type of reinforcement learning.",2 Related Work,[0],[0]
"However, their approach is extremely computationally intensive (a minimum of 10 days before convergence), and was unable to achieve ROUGE scores better than the best maximum likelihood-based approach.",2 Related Work,[0],[0]
"Narayan et al. (2018) uses a cascade of filters in order to arrive at a set of candidate extractive summaries, which we can regard as an approximation of the true action space.",2 Related Work,[0],[0]
They then use an approximation of a policy gradient method to train their neural network to select summaries from this approximated action space.,2 Related Work,[0],[0]
"In contrast, BANDITSUM samples directly from the true action space, and uses exact policy gradient parameter updates.",2 Related Work,[0],[0]
Our approach formulates extractive summarization as a contextual bandit which we then train an agent to solve using policy gradient reinforcement learning.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"A bandit is a decision-making formalization in which an agent repeatedly chooses one of several actions, and receives a reward based on
this choice.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"The agent’s goal is to quickly learn which action yields the most favorable distribution over rewards, and choose that action as often as possible.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"In a contextual bandit, at each trial, a context is sampled and shown to the agent, after which the agent selects an action and receives a reward; importantly, the rewards yielded by the actions may depend on the sampled context.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
The agent must quickly learn which actions are favorable in which contexts.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Contextual bandits are a subset of Markov Decision Processes in which every episode has length one.
",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Extractive summarization may be regarded as a contextual bandit as follows.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Each document is a context, and each ordered subset of a document’s sentences is a different action.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Formally, assume that each context is a document d consisting of sentences s = (s1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", sNd), and that each action is a length-M sequence of unique sentence indices",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"i = (i1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", iM ) where it ∈ {1, . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", Nd}, it 6=",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"it′ for t 6= t′, and M is an integer hyper-parameter.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"For each i, the extractive summary induced by i is given by (si1 , . . .",3 Extractive Summarization as a Contextual Bandit,[0],[0]
", siM ).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"An action i taken in context d is given a reward R(i, a), where a is the gold-standard abstractive summary that is paired with document d, andR is a scalar reward function quantifying the degree of match between a and the summary induced by i.
A policy for extractive summarization is a neural network pθ(·|d), parameterized by a vector θ, which, for each input document d, yields a probability distribution over index sequences.",3 Extractive Summarization as a Contextual Bandit,[0],[0]
Our goal is to find parameters θ which cause pθ(·|d) to assign high probability to index sequences that induce extractive summaries that a human reader would judge to be of high-quality.,3 Extractive Summarization as a Contextual Bandit,[0],[0]
"We achieve this by maximizing the following objective function with respect to parameters θ:
J(θ) =",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"E [R(i, a)] (1)
where the expectation is taken over documents d paired with gold-standard abstractive summaries a, as well as over index sequences i generated according to pθ(·|d).",3 Extractive Summarization as a Contextual Bandit,[0],[0]
"Ideally, we would like to maximize (1) using gradient ascent.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"However, the required gradient cannot be obtained using usual techniques (e.g. simple backpropagation) because i must be discretely sampled in order to compute R(i, a).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Fortunately, we can use the likelihood ratio gradient estimator from reinforcement learning and stochastic optimization (Williams, 1992; Sutton et al., 2000), which tells us that the gradient of this function can be computed as:
∇θJ(θ) = E",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"[∇θ log pθ(i|d)R(i, a)]",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"(2)
where the expectation is taken over the same variables as (1).
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"Since we typically do not know the exact document distribution and thus cannot evaluate the expected value in (2), we instead estimate it by sampling.",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"We found that we obtained the best performance when, for each update, we first sample one document/summary pair (d, a), then sample B index sequences i1, . . .",3.1 Policy Gradient Reinforcement Learning,[0],[0]
", iB from pθ(·|d), and finally take the empirical average:
∇θJ(θ)",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"≈ 1
B B∑ b=1",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"∇θ log pθ(ib|d)R(ib, a) (3)
",3.1 Policy Gradient Reinforcement Learning,[0],[0]
"This overall learning algorithm can be regarded as an instance of the REINFORCE policy gradient algorithm (Williams, 1992).",3.1 Policy Gradient Reinforcement Learning,[0],[0]
There are many possible choices for the structure of pθ(·|d); we opt for one that avoids privileging early sentences over later ones.,3.2 Structure of pθ(·|d),[0],[0]
"We first decompose pθ(·|d) into two parts: πθ, a deterministic function which contains all the network’s parameters, and µ, a probability distribution parameterized by the output of πθ.",3.2 Structure of pθ(·|d),[0],[0]
"Concretely:
pθ(·|d) = µ(·|πθ(d)) (4)
",3.2 Structure of pθ(·|d),[0],[0]
"Given an input document d, πθ outputs a realvalued vector of sentence affinities whose length is equal to the number of sentences in the document (i.e. πθ(d) ∈ RNd) and whose elements fall in the range [0, 1].",3.2 Structure of pθ(·|d),[0],[0]
"The t-th entry π(d)t may be roughly interpreted as the network’s propensity to include sentence st in the summary of d.
",3.2 Structure of pθ(·|d),[0],[0]
"Given sentence affinities πθ(d), µ implements a process of repeated sampling-withoutreplacement.",3.2 Structure of pθ(·|d),[0],[0]
"This proceeds by repeatedly normalizing the set of affinities corresponding to sentences that have not yet been selected, thereby obtaining a probability distribution over unselected sentences, and sampling from that distribution to obtain a new sentence to include.",3.2 Structure of pθ(·|d),[0],[0]
"This normalizeand-sample step is repeated M times, yielding M unique sentences to include in the summary.
",3.2 Structure of pθ(·|d),[0],[0]
"At each step of sampling-without-replacement, we also include a small probability of sampling uniformly from all remaining sentences.",3.2 Structure of pθ(·|d),[0],[0]
"This is used to achieve adequate exploration during training, and is similar to the -greedy technique from reinforcement learning.
",3.2 Structure of pθ(·|d),[0],[0]
"Under this sampling scheme, we have the following expression for pθ(i|d):
M∏ j=1
(
Nd − j + 1 + (1− )π(d)ij z(d)− ∑j−1",3.2 Structure of pθ(·|d),[0],[0]
"k=1 π(d)ik
) (5)
where z(d) = ∑
t π(d)t.",3.2 Structure of pθ(·|d),[0],[0]
"For index sequences that have length different from M , or that contain duplicate indices, we have pθ(i|d) = 0.",3.2 Structure of pθ(·|d),[0],[0]
"Using this expression, it is straightforward to use automatic differentiation software to compute ∇θ log pθ(i|d), which is required for the gradient estimate in (3).",3.2 Structure of pθ(·|d),[0],[0]
"Our sample-based gradient estimate can have high variance, which can slow the learning.",3.3 Baseline for Variance Reduction,[0],[0]
"One potential cause of this high variance can be seen by inspecting (3), and noting that it basically acts to change the probability of a sampled index sequence to an extent determined by the reward R(i, a).",3.3 Baseline for Variance Reduction,[0],[0]
"However, since ROUGE scores are always positive, the probability of every sampled index sequence is increased, whereas intuitively, we would prefer to decrease the probability of sequences that receive a comparatively low reward, even if it is positive.",3.3 Baseline for Variance Reduction,[0],[0]
"This can be remedied by the introduction of a so-called baseline which is subtracted from all rewards.
",3.3 Baseline for Variance Reduction,[0],[0]
"Using a baseline r, our sample-based estimate of∇θJ(θ) becomes:
1
B B∑ i=1",3.3 Baseline for Variance Reduction,[0],[0]
"∇θ log pθ(ib|d)(R(ib, a)− r) (6)
",3.3 Baseline for Variance Reduction,[0],[0]
"It can be shown that the introduction of r does not bias the gradient estimator and can significantly reduce its variance if chosen appropriately (Sutton et al., 2000).
",3.3 Baseline for Variance Reduction,[0],[0]
"There are several possibilities for the baseline, including the long-term average reward and the average reward across different samples for one document-summary pair.",3.3 Baseline for Variance Reduction,[0],[0]
"We choose an approach known as self-critical reinforcement learning, in which the test-time performance of the current model is used as the baseline (Ranzato et al., 2015;
Rennie et al., 2017; Paulus et al., 2018).",3.3 Baseline for Variance Reduction,[0],[0]
"More concretely, after sampling the document-summary pair (d, a), we greedily generate an index sequence using the current parameters θ:
igreedy =",3.3 Baseline for Variance Reduction,[0],[0]
argmax,3.3 Baseline for Variance Reduction,[0],[0]
"i
pθ(i|d) (7)
and calculate the baseline for the current update as r = R(igreedy, a).",3.3 Baseline for Variance Reduction,[0],[0]
This baseline has the intuitively satisfying property of only increasing the probability of a sampled label sequence when the summary it induces is better than what would be obtained by greedy decoding.,3.3 Baseline for Variance Reduction,[0],[0]
"A final consideration is a concrete choice for the reward function R(i, a).",3.4 Reward Function,[0],[0]
"Throughout this work we use:
R(i, a) = 1
3 (ROUGE-1f (i, a) +
ROUGE-2f (i, a) +",3.4 Reward Function,[0],[0]
"ROUGE-Lf (i, a)).",3.4 Reward Function,[0],[0]
"(8)
The above reward function optimizes the average of all the ROUGE variants (Lin, 2004) while balancing precision and recall.",3.4 Reward Function,[0],[0]
"In this section, we discuss the concrete instantiations of the neural network πθ that we use in our experiments.",4 Model,[0],[0]
"We break πθ up into two components: a document encoder fθ1, which outputs a sequence of sentence feature vectors (h1, . . .",4 Model,[0],[0]
", hNd) and a decoder gθ2 which yields sentence affinities:
h1, . . .",4 Model,[0],[0]
", hNd = fθ1(d) (9)
πθ(d) = gθ2(h1, . . .",4 Model,[0],[0]
", hNd) (10)
Encoder.",4 Model,[0],[0]
"Features for each sentence in isolation are first obtained by applying a word-level Bidirectional Recurrent Neural Network (BiRNN) to the embeddings for the words in the sentence, and averaging the hidden states over words.",4 Model,[0],[0]
A separate sentence-level BiRNN is then used to obtain a representations hi for each sentence in the context of the document.,4 Model,[0],[0]
Decoder.,4 Model,[0],[0]
"A multi-layer perceptron is used to map from the representation ht of each sentence through a final sigmoid unit to yield sentence affinities πθ(d).
",4 Model,[0],[0]
"The use of a bidirectional recurrent network in the encoder is crucial, as it allows the network to
process the document as a whole, yielding representations for each sentence that take all other sentences into account.",4 Model,[0],[0]
"This procedure is necessary to deal with some aspects of summary quality such as redundancy (avoiding the inclusion of multiple sentences with similar meaning), which requires the affinities for different sentences to depend on one another.",4 Model,[0],[0]
"For example, to avoid redundancy, if the affinity for some sentence is high, then sentences which express similar meaning should have low affinities.",4 Model,[0],[0]
"In this section, we discuss the setup of our experiments.",5 Experiments,[0],[0]
We first discuss the corpora that we used and our evaluation methodology.,5 Experiments,[0],[0]
"We then discuss the baseline methods against which we compared, and conclude with a detailed overview of the settings of the model parameters.",5 Experiments,[0],[0]
"Three datasets are used for our experiments: the CNN, the Daily Mail, and combined CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016).",5.1 Corpora,[0],[0]
"We use the standard split of Hermann et al. (2015) for training, validating, and testing and the same setting without anonymization on the three corpus as See et al. (2017).",5.1 Corpora,[0],[0]
"The Daily Mail corpus has 196,557 training documents, 12,147 validation documents and 10,397 test documents; while the CNN corpus has 90,266/1,220/1,093 documents, respectively.",5.1 Corpora,[0],[0]
"The models are evaluated based on ROUGE (Lin, 2004).",5.2 Evaluation,[0],[0]
We obtain our ROUGE scores using the standard pyrouge package1 for the test set evaluation and a faster python implementation of the ROUGE metric2 for training and evaluating on the validation set.,5.2 Evaluation,[0],[0]
"We report the F1 scores of ROUGE1, ROUGE-2, and ROUGE-L, which compute the uniform, bigram, and longest common subsequence overlapping with the reference summaries.",5.2 Evaluation,[0],[0]
"We compare BANDITSUM with other extractive methods including: the Lead-3 model, SummaRuNNer (Nallapati et al., 2017), Refresh
1https://pypi.python.org/pypi/pyrouge/ 0.1.3
2We use the modified version based on https:// github.com/pltrdy/rouge
(Narayan et al., 2018), RNES (Wu and Hu, 2018), DQN (Yao et al., 2018), and NN-SE (Cheng and Lapata, 2016).",5.3 Baselines,[0],[0]
The Lead-3 model simply produces the leading three sentences of the document as the summary.,5.3 Baselines,[0],[0]
"We use 100-dimensional Glove embeddings (Pennington et al., 2014) as our embedding initialization.",5.4 Model Settings,[0],[0]
"We do not limit the sentence length, nor the maximum number of sentences per document.",5.4 Model Settings,[0],[0]
"We use one-layer BiLSTM for word-level RNN, and two-layers BiLSTM for sentence-level RNN.",5.4 Model Settings,[0],[0]
The hidden state dimension is 200 for each direction on all LSTMs.,5.4 Model Settings,[0],[0]
"For the decoder, we use a feedforward network with one hidden layer of dimension 100.
",5.4 Model Settings,[0],[0]
"During training, we use Adam (Kingma and Ba, 2015) as the optimizer with the learning rate of 5e−5, beta parameters (0, 0.999), and a weight decay of 1e−6, to maximize the objective function defined in equation (1).",5.4 Model Settings,[0],[0]
We employ gradient clipping of 1 to regularize our model.,5.4 Model Settings,[0],[0]
"At each iteration, we sample B = 20 times to estimate the gradient defined in equation 3.",5.4 Model Settings,[0],[0]
"For our system, the reported performance is obtained within two epochs of training 3.
",5.4 Model Settings,[0],[0]
"At the test time, we pick sentences sorted by the predicted probabilities until the length limit is reached.",5.4 Model Settings,[0],[0]
The full-length ROUGE F1 score is used as the evaluation metric.,5.4 Model Settings,[0],[0]
"For M , the number of sentences selected per summary, we use a value of 3, based on our validation results as well as on the settings described in Nallapati et al. (2017).",5.4 Model Settings,[0],[0]
"In this section, we present quantitative results from the ROUGE evaluation and qualitative results based on human evaluation.",6 Experiment Results,[0],[0]
"In addition, we demonstrate the stability of our RL model by comparing the validation curve of BANDITSUM with SummaRuNNer (Nallapati et al., 2017) trained with a maximum likelihood objective.",6 Experiment Results,[0],[0]
"We present the results of comparing BANDITSUM to several baseline algorithms4 on the CNN/Daily
3Our code can be found at https://github.com/ yuedongP/summarization_RL
4 Due to different pre-processing methods and different numbers of selected sentences, several papers report different Lead scores (Narayan et al., 2018; See et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"We use
Mail corpus in Tables 1 and 2.",6.1 Rouge Evaluation,[0],[0]
"Compared to other extractive summarization systems, BANDITSUM achieves performance that is significantly better than two RL-based approaches, Refresh (Narayan et al., 2018) and DQN (Yao et al., 2018), as well as SummaRuNNer, the state-of-the-art maximum liklihood-based extractive summarizer (Nallapati et al., 2017).",6.1 Rouge Evaluation,[0],[0]
"BANDITSUM performs a little better than RNES (Wu and Hu, 2018) in terms of ROUGE-1 and slightly worse in terms of ROUGE2.",6.1 Rouge Evaluation,[0],[0]
"However, RNES requires pre-training with the maximum likelihood objective on heuristicallygenerated extractive labels; in contrast, BANDITSUM is very light-weight and converges significantly faster.",6.1 Rouge Evaluation,[0],[0]
"We discuss the advantage of framing the extractive summarization based on the contextual bandit (BANDITSUM) over the sequential binary labeling setting (RNES) in the discussion Section 7.
",6.1 Rouge Evaluation,[0],[0]
"We also noticed that different choices for the policy gradient baseline (see Section 3.3) in BANDITSUM affect learning speed, but do not significantly affect asymptotic performance.",6.1 Rouge Evaluation,[0],[0]
"Models trained with an average reward baseline learned most quickly, while models trained with three different baselines (greedy, average reward in a
the test set provided by Narayan et al. (2018).",6.1 Rouge Evaluation,[0],[0]
"Since their Lead score is a combination of Lead-3 for CNN and Lead4 for Daily Mail, we recompute the Lead-3 scores for both CNN and Daily Mail with the preprocessing steps used in See et al. (2017).",6.1 Rouge Evaluation,[0],[0]
"Additionally, our results are not directly comparable to results based on the anonymized dataset used by Nallapati et al. (2017).
batch, average global reward) all perform roughly the same after training for one epoch.",6.1 Rouge Evaluation,[0],[0]
Models trained without a baseline were found to underperform other baseline choices by about 2 points of ROUGE score on average.,6.1 Rouge Evaluation,[0],[0]
We also conduct a qualitative evaluation to understand the effects of the improvements introduced in BANDITSUM on human judgments of the generated summaries.,6.2 Human Evaluation,[0],[0]
"To assess the effect of training with RL rather than maximum likelihood, in the first set of human evaluations we compare BANDITSUM with the state-of-the-art maximum likelihood-based model SummaRuNNer.",6.2 Human Evaluation,[0],[0]
"To evaluate the importance of using an exact, rather than approximate, policy gradient to optimize ROUGE scores, we perform another human evaluation comparing BANDITSUM and Refresh, an RL-based method that uses the an approximation of the policy gradient.
",6.2 Human Evaluation,[0],[0]
We follow a human evaluation protocol similar to the one used in Wu and Hu (2018).,6.2 Human Evaluation,[0],[0]
"Given a set of N documents, we ask K volunteers to evaluate the summaries extracted by both systems.",6.2 Human Evaluation,[0],[0]
"For each document, a reference summary, and a pair of randomly ordered extractive summaries (one generated by each of the two models) is presented to the volunteers.",6.2 Human Evaluation,[0],[0]
"They are asked to compare and rank the extracted summaries along three dimensions: overall, coverage, and non-redundancy.
",6.2 Human Evaluation,[0],[0]
"To compare with SummaRuNNer, we randomly sample 57 documents from the test set of Daily-
Mail and ask 5 volunteers to evaluate the extracted summaries.",6.2 Human Evaluation,[0],[0]
"While comparing with Refresh, we use the 20 documents (10 CNN and 10 DailyMail) provided by Narayan et al. (2018) to 4 volunteers.",6.2 Human Evaluation,[0],[0]
Tables 3 and 4 show the results of human evaluation in these two settings.,6.2 Human Evaluation,[0],[0]
BANDITSUM is shown to be better than Refresh and SummaRuNNer in terms of overall quality and nonredundancy.,6.2 Human Evaluation,[0],[0]
"These results indicate that the use of the true policy gradient, rather than the approximation used by Refresh, improves overall quality.",6.2 Human Evaluation,[0],[0]
"It is interesting to observe that, even though BANDITSUM does not have an explicit redundancy avoidance mechanism, it actually outperforms the other systems on non-redundancy.",6.2 Human Evaluation,[0],[0]
Reinforcement learning methods are known for sometimes being unstable during training.,6.3 Learning Curve,[0],[0]
"However, this seems to be less of a problem for BANDITSUM, perhaps because it is formulated as a contextual bandit rather than a sequential labeling problem.",6.3 Learning Curve,[0],[0]
"We show this by comparing the validation curves generated by BANDITSUM and the state-of-the-art maximum likelihood-based model – SummaRuNNer (Nallapati et al., 2017) (Figure 1).
",6.3 Learning Curve,[0],[0]
"From Figure 1, we observe that BANDITSUM converges significantly more quickly to good results than SummaRuNNer.",6.3 Learning Curve,[0],[0]
"Moreover, there is less variance in the performance of BANDITSUM.
",6.3 Learning Curve,[0],[0]
One possible reason is that extractive summarization does not have well-defined supervised labels.,6.3 Learning Curve,[0],[0]
There exists a mismatch between the provided labels and human-generated abstractive summaries.,6.3 Learning Curve,[0],[0]
"Hence, the gradient, computed from the maximum likelihood loss function, is not optimizing the evaluation metric of interest.",6.3 Learning Curve,[0],[0]
"Another important message is that both models are still far from the estimated upper bound5, which shows that there is still significant room for improvement.",6.3 Learning Curve,[0],[0]
"On CNN/Daily mail dataset, our model’s timeper-epoch is about 25.5 hours on a TITAN Xp.",6.4 Run Time,[0],[0]
"We trained the model for 3 epochs, which took about 76 hours in total.",6.4 Run Time,[0],[0]
"For comparison, DQN took about 10 days to train on a GTX 1080 (Yao et al., 2018).",6.4 Run Time,[0],[0]
"Refresh took about 12 hours on a single GPU to train (Narayan et al., 2018).",6.4 Run Time,[0],[0]
Note that this figure does not take into account the significant time required by Refresh for pre-computing ROUGE scores.,6.4 Run Time,[0],[0]
"We conjecture that the contextual bandit (CB) setting is a more suitable framework for modeling extractive summarization than the sequential binary labeling setting, especially in the cases when good summary sentences appear later in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"The intuition behind this is that models based on the sequential labeling setting are affected by the order of the decisions, which biases towards selecting sentences that appear earlier in the document.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"By contrast, our CB-based RL model has more flexibility and freedom to explore the search space, as it samples the sentences without replacement based on the affinity scores.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Note that although we do not explicitly make the selection decisions in a sequential fashion, the sequential information about dependencies between sentences is implicitly embedded in the affinity scores, which are produced by bidirectional RNNs.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We provide empirical evidence for this conjecture by comparing BANDITSUM to the sequential RL model proposed by Wu and Hu (2018) (Figure 2) on two subsets of the data: one with good
5The supervised labels for the upper bound estimation are obtained using the heuristic described in Nallapati et al. (2017).
summary sentences appearing early in the article, while the other contains articles where good summary sentences appear late.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Specifically, we construct two evaluation datasets by selecting the first 50 documents (Dearly, i.e., best summary occurs early) and the last 50 documents (Dlate, i.e., best summary occurs late) from a sample of 1000 documents that is ordered by the average extractive label index idx.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given an article with n sentences indexed from 1, . . .",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
", n and a greedy extractive labels set with three sentences (i, j, k)6, the average index for the extractive label is computed by idx= (i+ j + k)/3n.
",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Given these two subsets of the data, three different models (BANDITSUM, RNES and RNES3) are trained and evaluated on each of the two datasets without extractive labels.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Since the original sequential RL model (RNES) is unstable without supervised pre-training, we propose the RNES3 model that is limited to select no more then three sentences.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Starting with random initializations without supervised pre-training, we train each model ten times for 100 epochs and plot the learning curve of the average ROUGE-F1 score computed based on the trained model in Figure 2.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"We can clearly see that BANDITSUM finds a better so-
6For each document, a length-3 extractive summary with near-optimal ROUGE score is selected following the heuristic proposed by Nallapati et al. (2017).
lution more quickly than RNES and RNES3 on both datasets.",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"Moreover, it displays a significantly speed-up in the exploration and finds the best solution when good summary sentences appeared later in the document (Dlate).",7 Discussion: Contextual Bandit Setting Vs. Sequential Full RL Labeling,[0],[0]
"In this work, we presented a contextual bandit learning framework, BANDITSUM , for extractive summarization, based on neural networks and reinforcement learning algorithms.",8 Conclusion,[0],[0]
BANDITSUM does not require sentence-level extractive labels and optimizes ROUGE scores between summaries generated by the model and abstractive reference summaries.,8 Conclusion,[0],[0]
"Empirical results show that our method performs better than or comparable to state-of-the-art extractive summarization models which must be pre-trained on extractive labels, and converges using significantly fewer update steps than competing approaches.",8 Conclusion,[0],[0]
"In future work, we will explore the direction of adding an extra coherence reward (Wu and Hu, 2018) to improve the quality of extracted summaries in terms of sentence discourse relation.",8 Conclusion,[0],[0]
The research was supported in part by Natural Sciences and Engineering Research Council of Canada (NSERC).,Acknowledgements,[0],[0]
The authors would like to thank Compute Canada for providing the computational resources.,Acknowledgements,[0],[0]
"In this work, we propose a novel method for training neural networks to perform singledocument extractive summarization without heuristically-generated extractive labels.",abstractText,[0],[0]
"We call our approach BANDITSUM as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action).",abstractText,[0],[0]
A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score.,abstractText,[0],[0]
"We perform a series of experiments demonstrating that BANDITSUM is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches.",abstractText,[0],[0]
"In addition, we show empirically that BANDITSUM performs significantly better than competing approaches when good summary sentences appear late in the source document.",abstractText,[0],[0]
BANDITSUM: Extractive Summarization as a Contextual Bandit,title,[0],[0]
The advancement of modern society is driven by the development of Integrated Circuits (IC).,1. Introduction,[0],[0]
"Unlike the digital circuits where the design flow is already highly automated, the automation of analog circuit design is still a challenging problem.
",1. Introduction,[0],[0]
"Traditionally, the design parameters of analog circuits like widths and lengths of transistors are manually calculated by designers with their experience and the understanding of the design specifications.",1. Introduction,[0],[0]
"However, due to the progress
1State Key Lab of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China 2Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX, U.S.A. Correspondence to: Fan Yang <yangfan@fudan.edu.cn>, Xuan Zeng <xzeng@fudan.edu.cn>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"of IC manufacture technology forecasted by Moore’s law, the circuit devices become more and more complicated, and the parasitic effect of the circuits can no longer be ignored.",1. Introduction,[0],[0]
"On the other hand, the demands for high-performance, lowpower analog circuits are increasing.",1. Introduction,[0],[0]
It is much more difficult to meet the performance and time-to-market requirements with manual circuit design.,1. Introduction,[0],[0]
"Automated analog circuit design has thus attracted much research interest in the past decade (Rutenbar et al., 2007).
",1. Introduction,[0],[0]
The analog circuit design automation problems can be formulated as optimization problems.,1. Introduction,[0],[0]
"The aim is to find the optimal design parameters that provide the best circuit performance, which can be represented by a figure of merit (FOM) real-valued function.",1. Introduction,[0],[0]
"Prior works about analog circuit optimization include offline model-based approaches (Colleran et al., 2003; Daems et al., 2003; Wang et al., 2014) and simulation-based approaches.",1. Introduction,[0],[0]
The offline model-based methods try to build global models of the FOM via manual calculation or regression with simulated data and then optimize the cheap-to-evaluate models.,1. Introduction,[0],[0]
The problem with this approach is that the accurate models are usually hard to get.,1. Introduction,[0],[0]
"For example, in Wang et al. (2014), 100,000 randomly simulated points are used to train a sparse polynomial model for an amplifier circuit with ten design parameters.
",1. Introduction,[0],[0]
"Simulation-based methods, instead, treat the performances of the circuits as black-box functions.",1. Introduction,[0],[0]
The performances are obtained from circuit simulations.,1. Introduction,[0],[0]
Global optimization algorithms are directly applied to the black-box functions.,1. Introduction,[0],[0]
"For simulation-based circuit optimization methods, metaheuristic algorithms (Phelps et al., 2000; Liu et al., 2009) are widely used.",1. Introduction,[0],[0]
"Although these algorithms can explore the whole design space, they have relatively low convergence rate.",1. Introduction,[0],[0]
"When the circuit simulation takes a long time, both model-based and simulation-based approaches can be very time-consuming.
",1. Introduction,[0],[0]
"In recent years, the Gaussian process (GP) (Rasmussen, 2006) model has been introduced for the automated design of analog circuits to reduce the required number of circuit simulations.",1. Introduction,[0],[0]
"In Liu et al. (2014), GP is combined with differential evolution algorithm.",1. Introduction,[0],[0]
"Recently, Bayesian optimization (BO) (Shahriari et al., 2016) algorithm has also been applied for analog circuit optimization.",1. Introduction,[0],[0]
"In Lyu et al. (2017),
Bayesian optimization algorithm is firstly introduced for the single- and multi-objective optimization of general analog circuits and has shown to be much more efficient compared with other simulation-based approaches.",1. Introduction,[0],[0]
"In Wang et al. (2017), Bayesian optimization algorithm is combined with adaptive Monte-Carlo sampling to optimize the yield of analog circuits and static random-access memory (SRAM).
",1. Introduction,[0],[0]
Bayesian optimization algorithm is a well-studied algorithm and has demonstrated to be promising for the automated design of analog circuits.,1. Introduction,[0],[0]
"However, the standard Bayesian optimization algorithm is sequential.",1. Introduction,[0],[0]
It chooses only one point at each iteration by optimizing the acquisition function.,1. Introduction,[0],[0]
It is often desirable to select a batch of points at each iteration.,1. Introduction,[0],[0]
"The sequential property of Bayesian optimization limits its further applications in multi-core computer systems.
",1. Introduction,[0],[0]
Bayesian optimization algorithm has been extended to enable batch selection.,1. Introduction,[0],[0]
"Some prior works, like the qEI (Chevalier & Ginsbourger, 2013), qKG (Wu & Frazier, 2016) and parallel predictive entropy search (PPES) (Shah & Ghahramani, 2015) approaches, consider to search for the optimal batch selection for a specific acquisition function.",1. Introduction,[0],[0]
"These methods usually involve some approximations or MonteCarlo sampling, and thus scale poorly as the batch size increases.",1. Introduction,[0],[0]
"Other works, including the simulation matching (SM) (Azimi et al., 2010) method, the batch-UCB (BUCB, BLCB for minimization problems) (Desautels et al., 2014) method, the parallel UCB with pure exploration (GP-UCBPE) (Contal et al., 2013) method, and the local penalization (LP) (González et al., 2016) method, adopted the greedy strategies that select individual points until the batch is filled.
",1. Introduction,[0],[0]
All the batch Bayesian optimization algorithms mentioned above choose to use single acquisition function.,1. Introduction,[0],[0]
"And except for the SM method (Azimi et al., 2010) and LP method (González et al., 2016) which can use arbitrary acquisition function, other parallelization methods rely on a specific acquisition function.",1. Introduction,[0],[0]
"The UCB acquisition function must be used for BUCB and GP-UCB-PE, and the knowledge gradient (KG) acquisition function must be used for the qKG algorithm.",1. Introduction,[0],[0]
"As is stated in Hoffman et al. (2011), no single acquisition function can always outperform other acquisition functions.",1. Introduction,[0],[0]
"Relying on one acquisition function may result in poor performance.
",1. Introduction,[0],[0]
"In this paper, we propose to parallelize Bayesian optimization algorithm via the Multi-objective ACquisition Ensemble (MACE).",1. Introduction,[0],[0]
The proposed MACE method exploits the disagreement between different acquisition functions to enable batch selection.,1. Introduction,[0],[0]
"At each iteration, after the GP model is updated, multiple acquisition functions are selected.",1. Introduction,[0],[0]
We then perform multi-objective optimization to find the Pareto front (PF) of the acquisition functions.,1. Introduction,[0],[0]
The PF represents the best trade-off between these acquisition functions.,1. Introduction,[0],[0]
"When batch
evaluations are possible, we can sample multiple points on the PF to accelerate the optimization.
",1. Introduction,[0],[0]
"The MACE algorithm is tested using several analytical benchmark functions and two real-world analog circuits, including an operational amplifier with ten design parameters and a class-E power amplifier with twelve design parameters.",1. Introduction,[0],[0]
"The BLCB method (Desautels et al., 2014), local penalization method with expected improvement acquisition function (EI-LP) (González et al., 2016), qEI (Chevalier & Ginsbourger, 2013) and qKG (Wu & Frazier, 2016) methods are compared with MACE.",1. Introduction,[0],[0]
The proposed MACE method achieved competitive performance when compared with the state-of-the-art algorithms listed in the paper.,1. Introduction,[0],[0]
"In this section, we will present the problem formulation of analog circuit optimization, and review the background of Gaussian process regression and Bayesian optimization.",2. Background,[0],[0]
"When designing integrated circuits, the designers have to decide what circuit topology to use and then set the corrsponding design parameters.",2.1. Problem Formulation,[0],[0]
"In this work, we handle the scenarios where the topology of the analog circuit is fixed.",2.1. Problem Formulation,[0],[0]
"This is practical as there are usually a lot of classical topologies for a given design task, so unlike digital circuits, choosing appropriate topology is relatively easy.
",2.1. Problem Formulation,[0],[0]
"Once the circuit topology is fixed, the designer has to choose the appropriate design parameters according to the specifications and the circuit device model.",2.1. Problem Formulation,[0],[0]
What we want to do is automatically searching for the optimal design parameters.,2.1. Problem Formulation,[0],[0]
"This problem can then be formulated as a bound-constrained black-box optimization problem:
minimize FOM(x), (1)
where x ∈ D is the vector of design variables, FOM(x) is the objective constructed from the design specifications, the FOM(x) can be deterministric or noisy depending on the design specifications.",2.1. Problem Formulation,[0],[0]
"Given the design parameters x, the FOM value can be obtained by commercial circuit simulators like HSPICE or Spectre.",2.1. Problem Formulation,[0],[0]
"The objective function FOM(x) in (1) can be approximated by Gaussian process (GP) model (Rasmussen, 2006).",2.2. Gaussian Process Regression,[0],[0]
The GP model is the most commonly used model for Bayesian optimization.,2.2. Gaussian Process Regression,[0],[0]
The advantage of GP is that it provides a well-calibrated uncertainty of prediction.,2.2. Gaussian Process Regression,[0],[0]
"GP is characterized by a mean function m(x) and a covariance function k(x,x′).",2.2. Gaussian Process Regression,[0],[0]
"In this work, we use squared-exponential ARD kernel (Rasmussen, 2006), and a constant mean function
m(x) = µ0 for all our experiments.",2.2. Gaussian Process Regression,[0],[0]
"By default, we assume the objective function evaluations are influenced by i.i.d. noise t ∼ N(0, σ2n) and set the noise level σ2n as a hyperparameter.",2.2. Gaussian Process Regression,[0],[0]
"The introduction of the i.i.d noise also helps to improve the numerical stability.
",2.2. Gaussian Process Regression,[0],[0]
"Denote the training set as {X,y} where X = {x1, . . .",2.2. Gaussian Process Regression,[0],[0]
",xN} and y = {y1, . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", yN}, given a new data point x, the prediction of f(x) is not a scalar value, but a predictive distribution
f(x) ∼ N(µ(x), σ2(x)), (2)
where µ(x) and σ2(x) can be expressed as
µ(x) = µ0 + k(x, X)[K +",2.2. Gaussian Process Regression,[0],[0]
σ 2 nI] −1(y,2.2. Gaussian Process Regression,[0],[0]
"− µ0) σ2(x) = k(x,x)− k(x, X)[K + σ2nI]−1k(X,x),
(3)",2.2. Gaussian Process Regression,[0],[0]
"where k(x, X) = (k(x,x1), . .",2.2. Gaussian Process Regression,[0],[0]
.,2.2. Gaussian Process Regression,[0],[0]
", k(x,xN ))",2.2. Gaussian Process Regression,[0],[0]
"T and k(X,x) = k(x, X)T .",2.2. Gaussian Process Regression,[0],[0]
"The µ(x) can be viewed as the prediction of the function value, while the σ2(x) is a measure of uncertainty of the prediction.",2.2. Gaussian Process Regression,[0],[0]
"Bayesian optimization (Shahriari et al., 2016) was proposed for the optimization of expensive black-box functions.",2.3. Bayesian Optimization,[0],[0]
"It consists of two essential ingredients, i.e., the probabilistic surrogate models and the acquisition functions.",2.3. Bayesian Optimization,[0],[0]
The probabilistic surrogate models provide predictions with uncertainties.,2.3. Bayesian Optimization,[0],[0]
The acquisition functions make use of the predictive distribution to explore the state space.,2.3. Bayesian Optimization,[0],[0]
"The procedure of Bayesian optimization is summarized in Algorithm 1.
",2.3. Bayesian Optimization,[0],[0]
"Algorithm 1 Bayesian Optimization Require: Number of initial sampling points Ninit, number
of iterations Niter 1: Randomly sample Ninit points in the design space 2:",2.3. Bayesian Optimization,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",2.3. Bayesian Optimization,[0],[0]
", Niter do 4: Construct the acquisition function 5: Find xt that optimizes the acquisition function 6: Sample yt = f(xt) 7: Update probabilistic surrogate model 8: end for 9: Return best f(x) recorded during iterations
In Bayesian optimization described in Algorithm 1, the acquisition function is used to balance the exploration and exploitation during the optimization.",2.3. Bayesian Optimization,[0],[0]
The acquisition function considers both the predictive value and the uncertainty.,2.3. Bayesian Optimization,[0],[0]
There are a lot of existing acquisition functions.,2.3. Bayesian Optimization,[0],[0]
"Examples include the lower confidence bound (LCB), the probability of improvement (PI), and the expected improvement (EI).
",2.3. Bayesian Optimization,[0],[0]
"The LCB function is defined as follows:
LCB(x) = µ(x)− κσ(x), (4)
where the µ(x) and the σ(x) are the predictive value and uncertainty of GP defined in (3), κ is a parameter that balances the exploitation and exploration.
",2.3. Bayesian Optimization,[0],[0]
"Following the suggestion of (Srinivas et al., 2010; Brochu et al., 2010), the κ in (4) is defined as:
κ = √ ντt τt",2.3. Bayesian Optimization,[0],[0]
"= 2 log(t d/2+2π2/3δ),
(5)
where t is the number of current iteration, ν and δ are two user-defined parameters.",2.3. Bayesian Optimization,[0],[0]
"We fix ν = 0.5 and δ = 0.05 in this paper for the proposed MACE algorithm and our implementation of the BLCB algorithm.
",2.3. Bayesian Optimization,[0],[0]
"The PI and EI functions are defined as
PI(x) = Φ(λ) EI(x) = σ(x)(λΦ(λ) + φ(λ))",2.3. Bayesian Optimization,[0],[0]
λ,2.3. Bayesian Optimization,[0],[0]
= τ,2.3. Bayesian Optimization,[0],[0]
− ξ,2.3. Bayesian Optimization,[0],[0]
"− µ(x)
σ(x) ,
(6)
where τ is the current best value objective value, and ξ is a small positive jitter to improvement the ability of exploration.",2.3. Bayesian Optimization,[0],[0]
The Φ(.) and φ(.) functions are the CDF and PDF functions of normal distribution.,2.3. Bayesian Optimization,[0],[0]
"In our implementation of the MACE algorithm, we fix ξ = 1e-3.
",2.3. Bayesian Optimization,[0],[0]
"There are also other acquisition functions, like the knowledge gradient (Scott et al., 2011) function, predictive entropy search (Hernández-Lobato et al., 2014), and the max-value entropy search(Wang & Jegelka, 2017).",2.3. Bayesian Optimization,[0],[0]
"A portfolio of several acquisition functions is also possible (Hoffman et al., 2011).",2.3. Bayesian Optimization,[0],[0]
We will present the proposed batch Bayesian optimization algorithm in this section.,3. Proposed Batch Bayesian Optimization Algorithm,[0],[0]
"Unlike single-objective optimization, there are multiple objectives to optimize in multi-objective optimization problems(Marler & Arora, 2004).",3.1. Multi-objective Optimization,[0],[0]
"The multi-objective optimization problem is formulated as
minimize f1(x), . . .",3.1. Multi-objective Optimization,[0],[0]
", fm(x).",3.1. Multi-objective Optimization,[0],[0]
"(7)
The multiple objectives to be optimized can be conflicting so that it is usually impossible to find a single solution that is the optimum of all objectives.",3.1. Multi-objective Optimization,[0],[0]
"The goal of multi-objective optimization algorithms is to approximate the Pareto front of
the objectives.",3.1. Multi-objective Optimization,[0],[0]
A solution x1 is said to dominate x2 if ∀i ∈ {1 . .,3.1. Multi-objective Optimization,[0],[0]
".m}, fi(x1) ≤ fi(x2) and ∃j ∈ {1 . .",3.1. Multi-objective Optimization,[0],[0]
".m}, fj(x1) < fj(x2).",3.1. Multi-objective Optimization,[0],[0]
A design is Pareto-optimal if it is not dominated by any other point in the design space and dominates at least one point.,3.1. Multi-objective Optimization,[0],[0]
"The whole set of the Pareto-optimal points in the design space is called the Pareto set, and the set of Pareto-optimal points in the objective space is called the Pareto front.",3.1. Multi-objective Optimization,[0],[0]
"It is often unlikely to get the whole Pareto front as there might be infinite points on the Paret front, multi-objective optimization algorithms try to find a set of evenly distributed solutions that approximate the true Pareto front.
",3.1. Multi-objective Optimization,[0],[0]
"There exist many mature multi-objective optimization algorithms, like the non-dominated sorting based genetic algorithm (NSGA-II) (Deb et al., 2002), and the multiobjective evolutionary algorithm based on decomposition (MOEA/D) (Zhang & Li, 2007).",3.1. Multi-objective Optimization,[0],[0]
"In this paper, the multi-objective optimization based on differential evolution (DEMO) (Robič & Filipič, 2005) is used to solve multiobjective optimization problems, but other multi-objective optimization algorithms can also be applied.",3.1. Multi-objective Optimization,[0],[0]
"Each acquisition function represents a unique selection strategy, different acquisition functions may not agree with each other about where to sample the next point.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For example, the value of LCB function always decreases as the σ(x) increases.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"However, for the PI function, when σ(x) increases, the value of PI would decrease when µ(x) < τ , and increase when µ(x) > τ .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"For the EI function, if the function is noiseless, the values of EI function at already sampled points would always be worse than the EI values at any
unsampled locations, while this property does not hold for the LCB function.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Algorithm 2 Multi-objective Acquisition Ensemble Algorithm Require: Number of initial sampling points Ninit, number
of iterations Niter, batch size B. 1: Randomly sample Ninit points in the design space 2:",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Construct initial GP model 3: for t = 1, 2, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", Niter do 4: Construct the LCB, EI and PI functions according to (4) and (6) 5: Find the Pareto front of LCB, EI, PI functions using the DEMO algorithm 6: Randomly sample B points x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB from the Pareto-optimal points 7: Evaluate x1, . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
",xB to get y1 = f(x1), . . .",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
", yB = f(xB) 8: Update the GP model 9: end for
10: Return best f(x) recorded during iterations
With multi-objective optimization, the best trade-off between acquisition functions can be captured by the Pareto front of these acquisition functions.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can then sample on the Pareto front to obtain multiple candidate points for the objective function evaluations.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm is described in Algorithm 2.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In the proposed MACE algorithm, the LCB, EI, and PI acquisition functions are selected.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Other acquisition functions like KG and PES can also be incorporated into the MACE framework.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In each iteration, the following multi-objective
optimization problem is constructed:
minimize LCB(x), − EI(x), − PI(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"(8)
Then the DEMO multi-objective optimization algorithm (Robič & Filipič, 2005) is applied to solve the multiobjective problem in (8).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"Once the Pareto front of LCB, EI and PI is obtained, the candidate points are then randomly sampled from the Pareto front.
",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"In Figure 1, we illustrate the proposed MACE algorithm using an example of a real-world amplifier circuit.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The optimization objective is to maximize the phase margin (PM) of the amplifier, so the FOM is defined as FOM(x) = −PM(x).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The width of one of its transistor is the design variable.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
We sweep the width of the transistor and perform HSPICE simulations to get the FOM values.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The curve of FOM values is plotted in Figure 1(a) (the blue line).,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
Several points are randomly sampled from the FOM curve to train the GP model.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"The LCB, EI, PI functions and the Pareto front of the acquisition functions are plotted in Figure 1(b).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"We can see from Figure 1(b) that the optimal locations of the three acquisition functions are different, while their best trade-off is captured by the Pareto front.",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The Pareto set that represents the best trade-off between the three acquisition functions is the interval,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
"[43, 50.4], as plotted in Figure 1(a).",3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The candidate points for the next batch of evaluations are randomly sampled from the Pareto set.,3.2. Batch Bayesian Optimization via Multi-objective Acquisition Function Ensemble,[0],[0]
The proposed MACE algorithm1 was tested using eight benchmark functions and two real-world analog circuits.,4. Experimental Results,[0],[0]
"Four state-of-the-art parallel Bayesian optimization methods were compared, including the BLCB algorithm (Desautels et al., 2014), the local penalization method with EI acquisition function (EI-LP) (González et al., 2016), the qEI and qKG methods (Chevalier & Ginsbourger, 2013; Wu & Frazier, 2016).",4. Experimental Results,[0],[0]
"2
For the MACE, BLCB, and EI-LP method, the ARD squared-exponential kernel is used and the GP models are fitted by maximum likelihood estimations (MLE); for the qKG and qEI methods, the ARD Matern52 kernels are used, and the GP hyperparameters are integrated via MCMC sampling.",4. Experimental Results,[0],[0]
"The Matern52 kernel and MCMC integration are the default strategies of the qKG and qEI implementations and it is unclear in the documentation about how to change the GP settings.
",4. Experimental Results,[0],[0]
1Available at https://github.com/Alaya-in-Matrix/MACE 2We implemented the BLCB algorithm as the available open source implementations only allow discrete input.,4. Experimental Results,[0],[0]
"For the EI-LP method, the code is downloaded from https://github.com/SheffieldML/GPyOpt.",4. Experimental Results,[0],[0]
The code for qEI and qKG is downloaded from https://github.com/wujian16/CornellMOE.,4. Experimental Results,[0],[0]
"We tested the MACE algorithm and other parallel BO methods using eight commonly used benchmark functions, as summarized in Table 1.
",4.1. Benchmark Problems,[0],[0]
"For all functions except the two 10D functions, we set the number of initial random sampling to Ninit = 20 and the number of iterations toNiter = 45.",4.1. Benchmark Problems,[0],[0]
"Batch size is set toB = 4, the total number of function evaluations is Ninit +B × Niter.",4.1. Benchmark Problems,[0],[0]
"For the 10D Ackley and 10D Rosenbrock functions, we set Ninit = 100 and Niter = 175.",4.1. Benchmark Problems,[0],[0]
"The experiments were repeated ten times to average the random fluctuations.
",4.1. Benchmark Problems,[0],[0]
We also ran the MACE algorithm in sequential mode and compared with the EI and LCB acquisition functions.,4.1. Benchmark Problems,[0],[0]
"The sequential EI and LCB based Bayesian optimization are implemented by setting the batch size B = 1 for EI-LP and BLCB respectively.
",4.1. Benchmark Problems,[0],[0]
"The mean convergence plots of the tested algorithms on the benchmark functions are given in Figure 2, the statistics of the final regrets are listed in Table 2.",4.1. Benchmark Problems,[0],[0]
"As can be seen in Figure 2 and Table 2, when running in sequential mode, the MACE algorithm is competitive with the LCB and EI acquisition functions.",4.1. Benchmark Problems,[0],[0]
"The sequential MACE (MACE-1) algorithm gave better performances than the sequential EI (EI-1) and sequential LCB (LCB-1) algorithms in the Eggholder, Branin, Hartmann6, Ackley10, and Rosenbrock10 functions.",4.1. Benchmark Problems,[0],[0]
"Also, the parallel MACE (MACE-4) gave the best performances among all the tested algorithms for six out of the eight benchmark functions, and has shown dramatic speedup compared to the sequential MACE.",4.1. Benchmark Problems,[0],[0]
"We also performed additional experiments with varied batch sizes, the detail of those experimental results can be seen in the supplementary materials.",4.1. Benchmark Problems,[0],[0]
"We report the time spent on the ten-dimensional Rosenbrock function optimization with B = 4 as a measure of the algorithm overhead, for the ten-dimensional Rosenbrock function, it took MACE about 11 hours to finish all the Niter = 175 iterations, the BLCB algorithm took about five hours, for the EI-LP algorithm, it took only one hour to finish the optimization.",4.1. Benchmark Problems,[0],[0]
"The overheads for qEI and qKG are much larger, it took more than two days for qKG and qEI to
finish the optimization of the ten-dimensional Rosenbrock function.",4.1. Benchmark Problems,[0],[0]
"The operational amplifier (Wang et al., 2014) shown in Figure 3 is used to test Bayesian optimization algorithms.",4.2. Operational Amplifier,[0],[0]
The circuit is designed using the 180nm process.,4.2. Operational Amplifier,[0],[0]
"It has 10 design parameters, including the lengths and widths of transistors, the resistance of the resistors and the capacitance of the capacitors.",4.2. Operational Amplifier,[0],[0]
"The circuit is simulated using the commercial HSPICE circuit simulator.
",4.2. Operational Amplifier,[0],[0]
"We want to maximize the gain, unit gain frequency (UGF) and the phase margin (PM) for this amplifier.",4.2. Operational Amplifier,[0],[0]
"The Figure of Merit FOM is constructed as
FOM = −1.2× gain − 10×UGF − 1.6× PM .
",4.2. Operational Amplifier,[0],[0]
"For this circuit, we compared the MACE algorithm with the BLCB and EI-LP algorithms.",4.2. Operational Amplifier,[0],[0]
"The qKG and qEI are not compared as the computation of qEI and qKG acquisition functions become very slow for the ten-dimensional functions.
",4.2. Operational Amplifier,[0],[0]
We run the algorithms in sequential mode and batch mode.,4.2. Operational Amplifier,[0],[0]
"For the batch mode, the batch size is set to B = 4.",4.2. Operational Amplifier,[0],[0]
"The number of initial random sampling is set to Ninit = 100, and the number of iterations is set to Niter = 100.
",4.2. Operational Amplifier,[0],[0]
The mean convergence plot for the sequential and batch runs are given in Figure 4.,4.2. Operational Amplifier,[0],[0]
The mean and standard deviation of the final optimized FOM values are listed in Table 3.,4.2. Operational Amplifier,[0],[0]
"As can be seen, on average, the batch MACE algorithm had the fastest convergence rate compared with the sequential MACE algorithm and other parallel algorithms.",4.2. Operational Amplifier,[0],[0]
"It should also be noted that the final optimized FOM values given by MACE-4 have very small deviation (0.105) compared with other algorithms.
",4.2. Operational Amplifier,[0],[0]
Table 2.,4.2. Operational Amplifier,[0],[0]
"Statistics of the regrets of the benchmark functions
Eggholder Branin Alpine1 Hartmann6
MACE-1 87.65±75.83 1.05e-5±1.31e-5 2.66305±1.05844 0.0646869±0.0621189 LCB-1 153.9±112.8 6.86e-5±1.13e-4 5.66812±1.76973 0.125565±0.122684 EI-1",4.2. Operational Amplifier,[0],[0]
172.8±132.2 1.62e-2±1.63e-2 2.46061±1.56079 0.110561±0.146809,4.2. Operational Amplifier,[0],[0]
MACE-4,4.2. Operational Amplifier,[0],[0]
46.38±40.89 4.62e-6±6.64e-6 0.903805±0.835209 0.0275738±0.052254 BLCB-4 56.86±35.91 4.32e-5±6.33e-5 1.8843±0.938873 0.06447±0.0621176 EI-LP-4 44.68±56.45,4.2. Operational Amplifier,[0],[0]
"2.11e-2±1.84e-2 1.0059±0.456865 0.0540446±0.0558557 qKG-4 106.4±67.64 2.65e-1±2.70e-1 3.01513±1.13414 0.47134±0.18939 qEI-4 72.13±52.08 3.29e-4±1.14e-3 2.7074±1.05145 0.186088±0.116323
Ackley2 Rosenbrock2 Ackley10",4.2. Operational Amplifier,[0],[0]
"Rosenbrock10
MACE-1 1.71474±1.12154 0.026173±0.051189 3.1348±0.447874 499.697±300.899 LCB-1 1.624±0.926437 0.0201124±0.0205367 3.14797±0.519164 517.944±288.955 EI-1 1.0136±0.985858",4.2. Operational Amplifier,[0],[0]
13.5508±9.52734 18.8006±0.652136 1367.08±637.507 MACE-4 1.07906±0.886466 0.00095416±0.00093729 2.56439±0.535488 158.116±50.0024 BLCB-4 1.40051±1.02849 0.00191986±0.00180895 3.27543±0.735501 406.819±127.351 EI-LP-4 0.284265±0.24634 2.73645±2.05923,4.2. Operational Amplifier,[0],[0]
18.2682±0.608564 721.351±327.365 qKG-4 5.59394±1.80595 5.03976±3.72014 18.197±0.764103 705.112±412.762,4.2. Operational Amplifier,[0],[0]
qEI-4 2.87373±1.02405 10.1881±15.0432,4.2. Operational Amplifier,[0],[0]
"18.3686±0.501869 655.208±340.954
Vin
C0
C1
Vdd1=2.5V
Vg
Vdd2=1.8V
M4
M3
M2
M1
L3 C2
C3
Vout
RLC1
L1
Cc
R0
R1
L2
Figure 5.",4.2. Operational Amplifier,[0],[0]
Schematic of the power amplifier,4.2. Operational Amplifier,[0],[0]
The class-E power amplifier shown in Figure 5 is used to test Bayesian optimization algorithms.,4.3. Class-E Power Amplifier,[0],[0]
"The circuit is designed using the 180nm process with 12 design parameters, the circuit is simulated by the commercial HSPICE circuit simulator to get its performances.
",4.3. Class-E Power Amplifier,[0],[0]
"For this power amplifier, we aim to maximize the power added efficiency (PAE) and the output power (Pout), the Figure of Merit FOM is constructed as
FOM = −3× PAE − Pout .
",4.3. Class-E Power Amplifier,[0],[0]
"The MACE, BLCB, and EI-LP algorithms were tested in both sequential and batch modes.",4.3. Class-E Power Amplifier,[0],[0]
The number of initial sampling is Ninit = 100.,4.3. Class-E Power Amplifier,[0],[0]
"The number of iterations is
Niter = 100.",4.3. Class-E Power Amplifier,[0],[0]
The batch size is set to B = 4.,4.3. Class-E Power Amplifier,[0],[0]
"The total number of HSPICE simulations is 500 for each batch run and 200 for each sequential run.
",4.3. Class-E Power Amplifier,[0],[0]
The optimization results of the class-E power amplifier are given in Figure 6 and Table 4.,4.3. Class-E Power Amplifier,[0],[0]
We can see that the MACE outperformed the BLCB and EI-LP in both sequential and batch mode.,4.3. Class-E Power Amplifier,[0],[0]
"For the batch runs, the MACE converges fastest among the three algorithms, while the sequential MACE (MACE-1) has comparable performance as the batch EI-LP (EI-LP-4) method.",4.3. Class-E Power Amplifier,[0],[0]
"In this paper, a batch Bayesian optimization algorithm is proposed for the automation of analog circuit design.",5. Conclusion,[0],[0]
The parallelization is achieved via the multi-objective ensemble of acquisition functions.,5. Conclusion,[0],[0]
"In each iteration, the candidate points are sampled from the Pareto front of multiple acquisition functions.",5. Conclusion,[0],[0]
"We compared the proposed MACE algorithm using analytical benchmark functions and real-world circuits, it is shown that the MACE algorithm is competitive compared with the state-of-the-art methods listed in the paper.",5. Conclusion,[0],[0]
"This research was supported partly by the National Major Science and Technology Special Project of China (2017ZX01028101-003), partly by National Key Research and Development Program of China 2016YFB0201304, partly by National Natural Science Foundation of China (NSFC) research projects 61774045, 61574044, 61474026, 61574046, 61674042, and 61628402 and partly by the Recruitment Program of Global Experts (the Thousand Talents Plan).",Acknowledgements,[0],[0]
Bayesian optimization methods are promising for the optimization of black-box functions that are expensive to evaluate.,abstractText,[0],[0]
"In this paper, a novel batch Bayesian optimization approach is proposed.",abstractText,[0],[0]
The parallelization is realized via a multi-objective ensemble of multiple acquisition functions.,abstractText,[0],[0]
"In each iteration, the multi-objective optimization of the multiple acquisition functions is performed to search for the Pareto front of the acquisition functions.",abstractText,[0],[0]
The batch of inputs are then selected from the Pareto front.,abstractText,[0],[0]
The Pareto front represents the best trade-off between the multiple acquisition functions.,abstractText,[0],[0]
Such a policy for batch Bayesian optimization can significantly improve the efficiency of optimization.,abstractText,[0],[0]
The proposed method is compared with several state-of-the-art batch Bayesian optimization algorithms using analytical benchmark functions and real-world analog integrated circuits.,abstractText,[0],[0]
The experimental results show that the proposed method is competitive compared with the state-of-the-art algorithms.,abstractText,[0],[0]
Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1853–1862 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1853",text,[0],[0]
"Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010).",1 Introduction,[1.0],"['Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010).']"
"Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954).",1 Introduction,[1.0],"['Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954).']"
"A representative method is skip-gram (SG) (Mikolov et al., 2013a,b), which realizes the hypothesis using a
∗The first two authors contributed equally to this paper and share the first-authorship.
",1 Introduction,[0],[0]
shallow neural network model.,1 Introduction,[0],[0]
"The other family of methods is count-based, such as GloVe (Pennington et al., 2014) and LexVec (Salle et al., 2016a,b), which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.
",1 Introduction,[0],[0]
"By far, most state-of-the-art embedding methods rely on SGD and negative sampling for optimization.",1 Introduction,[0],[0]
"However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples (Chen et al., 2018; Yuan et al., 2016), as shown in Figure 1.",1 Introduction,[0],[0]
"Essentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken.",1 Introduction,[0],[0]
"Moreover, SGD exhibits dramatic fluctuation and suffers from overshooting on local minimums (Ruder, 2016).",1 Introduction,[0],[0]
"These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.
",1 Introduction,[0.9999999268766473],"['These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.']"
"To address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning.",1 Introduction,[0],[0]
"In contrast to SGD, batch learning does not involve any sampling procedure and computes the gradient over all training samples.",1 Introduction,[0],[0]
"As such, it can easily converge to a better optimum in a more stable way.",1 Introduction,[0],[0]
"Nevertheless, a well-known
difficulty in applying full batch learning lies in the expensive computational cost for large-scale data.",1 Introduction,[0],[0]
"Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size.",1 Introduction,[1.0],"['Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size.']"
"This high complexity is unaffordable in practice, since |V |2 can easily reach billion level or even higher.
",1 Introduction,[0],[0]
"In this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning.",1 Introduction,[0],[0]
"To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embedding, which allows fast optimization with memorization strategies.",1 Introduction,[0],[0]
"Specifically, the acceleration is achieved by reformulating the expensive loss over all negative samples using a partition and a decouple operation.",1 Introduction,[0],[0]
"By decoupling and caching the bottleneck terms, we succeed to use all samples for each parameter update in a manageable time complexity which is mainly determined by the positive samples.",1 Introduction,[0],[0]
"The main contributions of this work are summarized as follows:
• We present a fine-grained weighted least square loss for learning word embeddings.",1 Introduction,[0],[0]
"Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.
",1 Introduction,[0],[0]
• We propose an efficient and exact optimization algorithm based on full batch gradient optimization.,1 Introduction,[0],[0]
"It has a comparable time complexity with SGD, but being more effective and stable due to the consideration of all samples in each parameter update.
",1 Introduction,[0],[0]
"• We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method.",1 Introduction,[0],[0]
"Mikolov et al. (2013a,b) proposed the skip-gram model to learn word embeddings.",2.1 Skip-gram with Negative Sampling,[0],[0]
"SG formulates the problem as a predictive task, aiming at predicting the proper context c for a target word w within a local window.",2.1 Skip-gram with Negative Sampling,[0],[0]
"To speed up the training process, it applies the negative sampling (Mikolov et al., 2013b) to approximate the full softmax.",2.1 Skip-gram with Negative Sampling,[0],[0]
"That is,
each positive (w, c) pair is trained with n randomly sampled negative pairs (w,wi).",2.1 Skip-gram with Negative Sampling,[0],[0]
"The sampled loss function of SG is defined as
LSGwc =log σ(UwŨ T c )+ n∑ i=1",2.1 Skip-gram with Negative Sampling,[0],[0]
"Ewi∼Pn(w) log σ(−UwŨ T wi)
where Uw and Ũc denote the k-dimensional embedding vectors for word w and context c. Pn(w) is the distribution from which negative context wi is sampled.
",2.1 Skip-gram with Negative Sampling,[0],[0]
"Plenty of research has been done based on SG, such as the use of prior knowledge from another source (Kumar and Araki, 2016; Liu et al., 2015a; Bollegala et al., 2016), incorporating word type information (Cao and Lu, 2017; Niu et al., 2017), character level n-gram models (Bojanowski et al., 2016; Joulin et al., 2016) and jointly learning with topic models like LDA (Shi et al., 2017; Liu et al., 2015b).",2.1 Skip-gram with Negative Sampling,[0],[0]
Mikolov et al. (2013b) showed that the unigram distribution raised to the 3/4th power as Pn(w) significantly outperformed both the unigram and the uniform distribution.,2.2 Importance of the Sampling Distribution,[0],[0]
This suggests that the sampling distribution (of negative words) has a great impact on the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Furthermore, Chen et al. (2018) and Guo et al. (2018) recently found that replacing the original sampler with adaptive samplers could result in better performance.",2.2 Importance of the Sampling Distribution,[0],[0]
The adaptive samplers are used to find more informative negative examples during the training process.,2.2 Importance of the Sampling Distribution,[0],[0]
"Compared with the original word-frequency based sampler, adaptive samplers adapt to both the target word and the current state of the model.",2.2 Importance of the Sampling Distribution,[0],[0]
They also showed that the finegrained samplers not only speeded up the convergence but also significantly improved the embedding quality.,2.2 Importance of the Sampling Distribution,[0],[0]
"Similar observations were also found in other fields like collaborative filtering (Yuan et al., 2016).",2.2 Importance of the Sampling Distribution,[0],[0]
"While being effective, it is proven that negative sampling is a biased approximation and does not converges to the same loss as the full softmax — regardless of how many update steps have been taken (Bengio and Senécal, 2008; Blanc and Rendle, 2017).",2.2 Importance of the Sampling Distribution,[0],[0]
"Another line of research is the count-based embedding, such as GloVe (Pennington et al., 2014).",2.3 Count-based Embedding Methods,[0],[0]
"GloVe performs a biased MF on the word-context co-occurrence statistics, which is a common ap-
proach in the field of collaborative filtering (Koren, 2008).",2.3 Count-based Embedding Methods,[0],[0]
"However, GloVe only formulates the loss on positive entries of the co-occurrence matrix, meaning that negative signals about wordcontext co-occurrence are discarded.",2.3 Count-based Embedding Methods,[0],[0]
"A remedy solution is LexVec (Salle et al., 2016a,b) which integrates negative sampling into MF.",2.3 Count-based Embedding Methods,[0],[0]
"Some other methods (Li et al., 2015; Stratos et al., 2015; Ailem et al., 2017) also use MF to approximate the word-context co-occurrence statistics.",2.3 Count-based Embedding Methods,[0],[0]
"Although predictive models and count-based models seem different at first glance, Levy and Goldberg (2014) proved that SG with negative sampling is implicitly factorizing a shifted pointwise mutual information (PMI) matrix, which means that the two families of embedding models resemble each other to a certain degree.
",2.3 Count-based Embedding Methods,[0],[0]
Our proposed method departs from all above methods by using the full batch gradient optimizer to learn from all (positive and negative) samples.,2.3 Count-based Embedding Methods,[0],[0]
We propose a fast learning algorithm to show that such batch learning is not “heavy” even with tens of billions of training examples.,2.3 Count-based Embedding Methods,[0],[0]
"In this work, we adopt the regression loss that is commonly used in count-based models (Pennington et al., 2014; Stratos et al., 2015; Ailem et al., 2017) to perform matrix factorization on word cooccurrence statistics.",3 AllVec Loss,[0],[0]
"As highlighted, to retain the modeling fidelity, AllVec eschews using any sampling but optimizes the loss on all positive and negative word-context pairs.
",3 AllVec Loss,[0],[0]
"Given a word w and a symmetric window of win contexts, the set of positive contexts can be obtained by sliding through the corpus.",3 AllVec Loss,[0],[0]
"Let c denote a specific context, Mwc be the number of cooccurred (w, c) pairs in the corpus within the window.",3 AllVec Loss,[0],[0]
"Mwc=0 means that the pair (w, c) has never been observed, i.e. the negative signal.",3 AllVec Loss,[0],[0]
"rwc is the association coefficient between w and c, which is calculated from Mwc.",3 AllVec Loss,[0],[0]
"Specifically, we use r+wc to denote the ground truth value for positive (w, c) pairs and a constant value r−(e.g., 0 or -1) for negative ones since there is no interaction between w and c in negative pairs.",3 AllVec Loss,[0],[0]
"Finally, with all positive and negative pairs considered, a regular loss function can be given as Eq.(1), where V is the vocabulary and S is the set of positive pairs.",3 AllVec Loss,[0],[0]
"α+wc and α−wc represent the weight for positive and negative
(w, c) pairs, respectively.",3 AllVec Loss,[0],[0]
"L = ∑
(w,c)∈S α+wc(r + wc − UwŨTc )2︸ ︷︷ ︸
LP + ∑ (w,c)∈(V×V )",3 AllVec Loss,[0],[0]
"\S
α−wc(r − − UwŨTc )2︸ ︷︷ ︸
LN
(1)
When it comes to r+wc, there are several choices.",3 AllVec Loss,[0],[0]
"For example, GloVe applies the log of Mwc with bias terms for w and c. However, research from Levy and Goldberg (2014) showed that the SG model with negative sampling implicitly factorizes a shifted PMI matrix.",3 AllVec Loss,[0],[0]
"The PMI value for a (w, c) pair can be defined as
PMIwc = log P (w, c)
P (w)P (c) = log MwcM∗∗ Mw∗M∗c",3 AllVec Loss,[0],[0]
"(2)
where ‘*’ denotes the summation of all corresponding indexes (e.g., Mw∗= ∑ c∈V Mwc).",3 AllVec Loss,[0],[0]
"Inspired by this connection, we set r+wc as the positive point-wise mutual information (PPMI) which has been commonly used in the NLP literature (Stratos et al., 2015; Levy and Goldberg, 2014).",3 AllVec Loss,[0],[0]
"Sepcifically, PPMI is the positive version of PMI by setting the negative values to zero.",3 AllVec Loss,[0],[0]
"Finally, r+wc is defined as
r+wc = PPMIwc = max(PMIwc, 0) (3)",3 AllVec Loss,[0],[0]
"Regarding α+wc, we follow the design in GloVe, where it is defined as
α+wc =
{ (Mwc/xmax) ρ",3.1 Weighting Strategies,[0],[0]
"Mwc < xmax
1 Mwc ≥ xmax (4)
",3.1 Weighting Strategies,[0],[0]
"As for the weight for negative instances α−wc, considering that there is no interaction between w and negative c, we set α−wc as α − c (or α − w), which means that the weight is determined by the word itself rather than the word-context interaction.",3.1 Weighting Strategies,[0],[0]
Note that either α−wc = α − c or α − wc = α,3.1 Weighting Strategies,[0],[0]
− w does not influence the complexity of AllVec learning algorithm described in the next section.,3.1 Weighting Strategies,[0],[0]
"The design of α−c is inspired by the frequency-based oversampling scheme in skip-gram and missing data reweighting in recommendation (He et al., 2016).",3.1 Weighting Strategies,[0],[0]
The intuition is that a word with high frequency is more likely to be a true negative context word if there is no observed word-context interactions.,3.1 Weighting Strategies,[0],[0]
"Hence, to effectively differentiate the positive and negative examples, we assign a higher weight for the negative examples that have a higher word fre-
quency, and a smaller weight for infrequent words.",3.1 Weighting Strategies,[0],[0]
"Formally, α−wc is defined as
α−wc = α − c = α0 M δ∗c∑",3.1 Weighting Strategies,[0],[0]
"c∈V M δ ∗c
(5)
where α0 can be seen as a global weight to control the overall importance of negative samples.",3.1 Weighting Strategies,[0],[0]
α0 = 0 means that no negative information is utilized in the training.,3.1 Weighting Strategies,[0],[0]
The exponent δ is used for smoothing the weights.,3.1 Weighting Strategies,[0],[0]
"Specially, δ = 0 means a uniform weight for all negative examples and δ = 1 means that no smoothing is applied.",3.1 Weighting Strategies,[0],[0]
"Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1).",4 Fast Batch Gradient Optimization,[1.0],"['Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1).']"
"In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product.",4 Fast Batch Gradient Optimization,[1.0],"['In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product.']"
"As can be seen, the major computational cost in Eq.(1) lies in the term LN , because the size of (V×V ) \S is very huge, which typically contains over billions of negative examples.",4.1 Loss Partition,[0],[0]
"To this end, we show our first key design that separates the loss of negative samples into the difference between the loss on all samples and that on positive samples1.",4.1 Loss Partition,[0],[0]
"The loss partition serves as the prerequisite for the efficient computation of full batch gradients.
",4.1 Loss Partition,[0.9999998881897462],['The loss partition serves as the prerequisite for the efficient computation of full batch gradients.']
"LN= ∑ w∈V ∑ c∈V α−c (r −−UwŨTc )2− ∑ (w,c)∈S α−c (r −− UwŨTc )2 (6)
By replacing LN in Eq.(1) with Eq.(6), we can obtain a new loss function with a more clear structure.",4.1 Loss Partition,[0],[0]
We further simplify the loss function by merging the terms on positive examples.,4.1 Loss Partition,[0],[0]
"Finally, we achieve a reformulated loss
L = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2
︸ ︷︷ ︸ LA
+ ∑
(w,c)∈S
(α+wc − α−c )(∆− UwŨTc ) 2
︸ ︷︷ ︸ L
P ′
+C (7)
where ∆ =",4.1 Loss Partition,[0.9966631413980421],"['Finally, we achieve a reformulated loss L = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2 ︸ ︷︷ ︸ LA + ∑ (w,c)∈S (α+wc − α−c )(∆− UwŨTc ) 2 ︸ ︷︷ ︸ L P ′ +C (7) where ∆ = (α+wcr + wc − α−c r−)/(α+wc − α−c ).']"
(α+wcr + wc − α−c r−)/(α+wc − α−c ).,4.1 Loss Partition,[0],[0]
It can be seen that the new loss function consists of two components: the loss LA on the whole V ×V training examples and LP ′ on positive examples.,4.1 Loss Partition,[0],[0]
"The major computation now lies in LA which has
1The idea here is similar to that used in (He et al., 2016; Li et al., 2016) for a different problem.
",4.1 Loss Partition,[0],[0]
a time complexity of O(k|V |2).,4.1 Loss Partition,[0],[0]
"In the following, we show how to reduce the huge volume of computation by a simple mathematical decouple.",4.1 Loss Partition,[0],[0]
"To clearly show the decouple operation, we rewrite LA as L̃A by omitting the constant term α−c (r
−)2.",4.2 Decouple,[0],[0]
"Note that uwd and ũcd denote the d-th element in Uw and Ũc, respectively.
",4.2 Decouple,[0],[0]
"L̃A = ∑ w∈V ∑ c∈V α−c k∑ d=0 uwdũcd k∑ d′=0 uwd′ ũcd′
− 2r− ∑ w∈V ∑ c∈V α−c k∑",4.2 Decouple,[0],[0]
"d=0 uwdũcd
(8)
Now we show our second key design that is based on a decouple manipulation for the inner product operation.",4.2 Decouple,[0],[0]
"Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.
",4.2 Decouple,[1.0000000332459547],"['Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.']"
L̃A = k∑ d=0 k∑ d′=0 ∑ w∈V uwduwd′,4.2 Decouple,[0],[0]
"∑ c∈V α−c ũcdũcd′
− 2r− k∑
d=0 ∑ w∈V uwd ∑ c∈V α−c ũcd
(9)
",4.2 Decouple,[0],[0]
"An important feature in Eq.(9) is that the original inner product terms are disappeared, while in the new equation ∑ c∈V α",4.2 Decouple,[0],[0]
− c ũcdũcd′ and ∑ c∈V α,4.2 Decouple,[0],[0]
− c ũcd are “constant” values relative to uwduwd′ and uwd respectively.,4.2 Decouple,[0],[0]
This means that they can be pre-calculated before training in each iteration.,4.2 Decouple,[0],[0]
"Specifically, we define pwdd′ , p c",4.2 Decouple,[0],[0]
"dd′ , q w d and q c d as the pre-calculated terms
pwdd′",4.2 Decouple,[0],[0]
=,4.2 Decouple,[0],[0]
∑,4.2 Decouple,[0],[0]
"w∈V uwduwd′ q w d = ∑ w∈V uwd
pcdd′ = ∑ c∈V α−c ũcdũcd′ q c",4.2 Decouple,[0],[0]
d = ∑ c∈V α−c ũcd (10),4.2 Decouple,[0],[0]
Then the computation of L̃A can be simplified to∑k d=0 ∑k d′=0 p w dd′p,4.2 Decouple,[0],[0]
c,4.2 Decouple,[0],[0]
dd′,4.2 Decouple,[0],[0]
"− 2r−qwd qcd.
",4.2 Decouple,[0],[0]
"It can be seen that the time complexity to compute all pwdd′ is O(|V |k2), and similarly, O(|V |k2) for pcdd′ andO(|V |k) for qwd and qcd.",4.2 Decouple,[0],[0]
"With all terms pre-calculated before each iteration, the time complexity of computing L̃A is justO(k2).",4.2 Decouple,[0],[0]
"As a result, the total time complexity of computing LA is decreased toO(2|V |k2+2|V |k+k2)",4.2 Decouple,[0],[0]
"≈ O(2|V |k2), which is much smaller than the originalO(k|V |2).",4.2 Decouple,[0],[0]
"Moreover, it’s worth noting that our efficient computation for L̃A is strictly equal to its original value, which means AllVec does not introduce any approximation in evaluating the loss function.
",4.2 Decouple,[0],[0]
"Finally, we can derive the batch gradients for
uwd and ũcd as ∂L
∂uwd = k∑ d′=0 uwd′p c dd′",4.2 Decouple,[0],[0]
"− ∑ c∈I+w Λ · ũcd − r−qcd
∂L
∂ũcd = k∑ d′=0 ũcd′p w dd′α",4.2 Decouple,[0],[0]
"− c− ∑ w∈I+c Λ · uwd − r−α−c qwd (11) where I+w denotes the set of positive contexts for w, I+c denotes the set of positive words for c and Λ = (α+wc−α−c )(∆−UwŨTc ).",4.2 Decouple,[0],[0]
"Algorithm 1 shows the training procedure of AllVec.
",4.2 Decouple,[0],[0]
"Algorithm 1 AllVec learning Input: corpus Γ, win, α0, δ, iter, learning rate η Output: embedding matrices U and Ũ
1: Build vocabulary V from Γ 2:",4.2 Decouple,[0],[0]
"Obtain all positive (w, c) and Mwc from Γ 3: Compute all r+wc, α + wc and α − c 4: Initialize U and Ũ 5: for i = 1, ..., iter do 6: for d ∈ {0, .., k} do 7: Compute and store qcd .O(|V |k) 8: for d′ ∈ {0, .., k} do 9: Compute and store pcdd′ .O(|V",4.2 Decouple,[0],[0]
"|k 2) 10: end for 11: end for 12: for w ∈ V do 13: Compute Λ .O(|S|k) 14: for d ∈ {0, .., k} do 15: Update uwd .O(|S|k + |V |k2) 16: end for 17: end for 18: Repeat 6-17 for ũcd .O(2|S|k+2|V |k2) 19: end for",4.2 Decouple,[0],[0]
"In the following, we show that AllVec can achieve the same time complexity with negative sampling based SGD methods.
",4.3 Time Complexity Analysis,[0],[0]
"Given the sample size n, the total time complexity for SG is O((n + 1)|S|k), where n + 1 denotes n negative samples and 1 positive example.",4.3 Time Complexity Analysis,[0],[0]
"Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).
",4.3 Time Complexity Analysis,[0.999999956638726],"['Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).']"
"For the ease of discussion, we denote c as the average number of positive contexts for a word in the training corpus, i.e. |S| = c|V | (c ≥ 1000 in most cases).",4.3 Time Complexity Analysis,[0],[0]
"We then obtain the ratio
4|S|k + 4|V |k2
(n+ 1)|S|k =
4
n+ 1 (1 +
k c ) (12)
where k is typically set from 100 to 300 (Mikolov et al., 2013a; Pennington et al., 2014), resulting in k ≤ c.",4.3 Time Complexity Analysis,[0],[0]
"Hence, we can give the lower and upper bound for the ratio:
4
n+1",4.3 Time Complexity Analysis,[0],[0]
"<
",4.3 Time Complexity Analysis,[0],[0]
"4|S|k+4|V |k2
(n+1)|S|k = 4 n+1 (1+ k c )≤ 8 n+1",4.3 Time Complexity Analysis,[0],[0]
"(13)
",4.3 Time Complexity Analysis,[0],[0]
The above analysis suggests that the complexity of AllVec is same as that of SGD with negative sample size between 3 and 7.,4.3 Time Complexity Analysis,[0],[0]
"In fact, considering that c is much larger than k in most datasets, the major cost of AllVec comes from the part 4|S|k (see Section 5.4 for details), which is linear with respect to the number of positive samples.",4.3 Time Complexity Analysis,[0],[0]
"We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).
",5 Experiments,[1.0000000028442764],"['We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).']"
Word analogy task.,5 Experiments,[0],[0]
"The task aims to answer questions like, “a is to b as c is to ?”.",5 Experiments,[0],[0]
"We adopt the Google testbed2 which contains 19, 544 such questions in two categories: semantic and syntactic.",5 Experiments,[0],[0]
"The semantic questions are usually analogies about people or locations, like “king is to man as queen is to ?”, while the syntactic questions focus on forms or tenses, e.g., “swimming is to swim as running to ?”.
",5 Experiments,[0],[0]
Word similarity tasks.,5 Experiments,[0],[0]
"We perform evaluation on six datasets, including MEN (Bruni et al., 2012), MC (Miller and Charles, 1991), RW (Luong et al., 2013), RG (Rubenstein and Goodenough, 1965), WS-353 Similarity (WSim) and Relatedness (WRel) (Finkelstein et al., 2001).",5 Experiments,[0],[0]
"We compute the spearman rank correlation between the similarity scores calculated based on the trained embeddings and human labeled scores.
QVEC.",5 Experiments,[0],[0]
QVEC is an intrinsic evaluation metric of word embeddings based on the alignment to features extracted from manually crafted lexical resources.,5 Experiments,[0],[0]
"QVEC has shown strong correlation with the performance of embeddings in several semantic tasks (Tsvetkov et al., 2015).
",5 Experiments,[0],[0]
"We compare AllVec with the following word embedding methods.
",5 Experiments,[0],[0]
"• SG: This is the original skip-gram model with SGD and negative sampling (Mikolov et al., 2013a,b).",5 Experiments,[0],[0]
"• SGA: This is the skip-gram model with an
adaptive sampler (Chen et al., 2018).",5 Experiments,[0],[0]
"2https://code.google.com/archive/p/word2vec/
For all baselines, we use the original implementation released by the authors.",5 Experiments,[0],[0]
"We evaluate the performance of AllVec on four real-world corpora, namely Text83, NewsIR4, Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
Wiki-sub is a subset of 2017 Wikipedia dump5.,5.1 Datasets and Experimental Setup,[0],[0]
"All corpora have been pre-processed by a standard pipeline (i.e. removing non-textual elements, lowercasing and tokenization).",5.1 Datasets and Experimental Setup,[0],[0]
"Table 1 summarizes the statistics of these corpora.
",5.1 Datasets and Experimental Setup,[0],[0]
"To obtain Mwc for positive (w, c) pairs, we follow GloVe where word pairs that are xwords apart contribute 1/x to Mwc.",5.1 Datasets and Experimental Setup,[0],[0]
The window size is set as win = 8.,5.1 Datasets and Experimental Setup,[0],[0]
"Regarding α+wc, we set xmax = 100 and ρ = 0.75.",5.1 Datasets and Experimental Setup,[0],[0]
"For a fair comparison, the embedding size k is set as 200 for all models and corpora.",5.1 Datasets and Experimental Setup,[0],[0]
"AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like (Bayer et al., 2017; Bradley et al., 2011) second order methods.",5.1 Datasets and Experimental Setup,[0],[0]
"For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all.",5.1 Datasets and Experimental Setup,[0],[0]
The setting is also suggested by Mikolov et al. (2013b).,5.1 Datasets and Experimental Setup,[0],[0]
Other detailed hyper-parameters are reported in Table 2.,5.1 Datasets and Experimental Setup,[0],[0]
We present results on the word analogy task in Table 2.,5.2 Accuracy Comparison,[0],[0]
"As shown, AllVec achieves the highest total accuracy (Tot.) in all corpora, particu-
3http://mattmahoney.net/dc/text8.zip 4http://research.signalmedia.co/newsir16/signal-
dataset.html 5https://dumps.wikimedia.org/enwiki/
larly in smaller corpora (Text8 and NewsIR).",5.2 Accuracy Comparison,[0],[0]
"The reason is that in smaller corpora the number of positive (w, c) pairs is very limited, thus making use of negative examples will bring more benefits.",5.2 Accuracy Comparison,[0],[0]
"Similar reason also explains the poor accuracy of GloVe in Text8, because GloVe does not consider negative samples.",5.2 Accuracy Comparison,[0],[0]
"Even in the very large corpus (Wiki-all), ignoring negative samples still results in sub-optimal performance.
",5.2 Accuracy Comparison,[0],[0]
"Our results also show that SGA achieves better performance than SG, which demonstrates the importance of a good sampling strategy.",5.2 Accuracy Comparison,[0],[0]
"However, regardless what sampler (except the full softmax sampling) is utilized and how many updates are taken, sampling is still a biased approach.",5.2 Accuracy Comparison,[0],[0]
"AllVec achieves the best performance because it is trained on the whole batch data for each parameter update rather than a fraction of sampled data.
",5.2 Accuracy Comparison,[0],[0]
Another interesting observation is AllVec performs better in semantic tasks in general.,5.2 Accuracy Comparison,[0],[0]
"The reason is that our model utilizes global co-occurrence statistics, which capture more semantic signals than syntactic signals.",5.2 Accuracy Comparison,[0],[0]
"While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks.",5.2 Accuracy Comparison,[1.0],"['While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks.']"
"We argue that the main reason is because AllVec can distill useful signals from negative examples, while GloVe simply ignores all negative information.",5.2 Accuracy Comparison,[0],[0]
"By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies.",5.2 Accuracy Comparison,[1.0],"['By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies.']"
"However, Rekabsaz et al. (2017) argues that these local-window based methods may suffer from the topic shifting issue.
",5.2 Accuracy Comparison,[0],[0]
Table 3 and Table 4 provide results in the word similarity and QVEC tasks.,5.2 Accuracy Comparison,[0],[0]
"We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples.",5.2 Accuracy Comparison,[1.0],"['We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples.']"
"Interestingly, although GloVe performs well in semantic analogy tasks, it shows extremely worse results in word similarity and QVEC.",5.2 Accuracy Comparison,[0],[0]
The reason shall be the same as that it performs poorly in syntactic tasks.,5.2 Accuracy Comparison,[0],[0]
"In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words.",5.3 Impact of α−c,[1.0],"['In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words.']"
We show the performance change of word analogy tasks on NewsIR in Figure 2 by tuning α0 and δ.,5.3 Impact of α−c,[0],[0]
"Results in other corpora show similar trends thus are omitted due to space limitation.
",5.3 Impact of α−c,[0],[0]
"Table 2: Results (“Tot.” denotes total accuracy) on the word analogy task.
",5.3 Impact of α−c,[0],[0]
"Corpus Text8 NewsIR
para.",5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
Tot.,5.3 Impact of α−c,[0],[0]
para.,5.3 Impact of α−c,[0],[0]
Sem.,5.3 Impact of α−c,[0],[0]
Syn.,5.3 Impact of α−c,[0],[0]
"Tot.
SG 1e-4 8 25 47.51 32.26 38.60 1e-5 10 10 70.81 47.48 58.10 SGA 6e-3 - - 48.10 33.78 39.74 6e-3 - - 71.74 48.71 59.20 GloVe 10 15 1 45.11 26.89 34.47 50 8 1 78.79 41.58 58.52 LexVec 1e-4 25 - 51.87 31.78 40.14 1e-5 10 - 76.11 39.09 55.95 AllVec 350 0.75 - 56.66 32.42 42.50 100 0.8 - 78.47 48.33 61.57
Wiki-sub Wiki-all
SG 1e-5 10 5 72.05 55.88 63.24 1e-5 10 5 73.91 61.91 67.37 SGA 6e-3 - - 73.93 56.10 63.81 6e-3 - - 75.11 61.94 67.92 GloVe 100 8 1 77.22 53.16 64.13 100 8 1 77.38 58.94 67.33 LexVec 1e-5 5 - 75.95 52.78 63.33 1e-5 5 - 76.31 56.83 65.48 AllVec 100 0.75 - 76.66 54.72 64.75 50 0.75 - 77.64 60.96 68.52
The parameter columns (para.)",5.3 Impact of α−c,[0],[0]
for each model are given from left to right as follows.,5.3 Impact of α−c,[0],[0]
"SG: subsampling of frequent words, window size and the number of negative samples; SGA: λ (Chen et al., 2018) that controls the distribution of the rank, the other parameters are the same with SG; GloVe:",5.3 Impact of α−c,[0],[0]
"xmax, window size and symmetric window; LexVec: subsampling of frequent words and the number of negative samples; AllVec: the negative weight α0 and δ.",5.3 Impact of α−c,[0],[0]
"Boldface denotes the highest total accuracy.
",5.3 Impact of α−c,[0],[0]
Figure 2(a) shows the impact of the overall weight α0 by setting δ as 0.75 (inspired by the setting of skip-gram).,5.3 Impact of α−c,[0],[0]
"Clearly, we observe that all results (including semantic, syntactic and total accuracy) have been greatly improved when α0 increases from 0 to a larger value.",5.3 Impact of α−c,[0],[0]
"As mentioned before, α0 = 0 means that no negative information is considered.",5.3 Impact of α−c,[0],[0]
This observation verifies that negative samples are very important for learning good embeddings.,5.3 Impact of α−c,[0],[0]
It also helps to explain why GloVe performs poorly on syntactic tasks.,5.3 Impact of α−c,[1.0],['It also helps to explain why GloVe performs poorly on syntactic tasks.']
"In addition, we find that in all corpora the optimal results are usually obtained when α0 falls in the range of 50 to 400.",5.3 Impact of α−c,[0],[0]
"For example, in the NewIR corpus as shown, AllVec achieves the best performance when α0 = 100.",5.3 Impact of α−c,[0],[0]
Figure 2(b) shows the impact of δ with α0 = 100.,5.3 Impact of α−c,[0],[0]
"As mentioned before, δ = 0 denotes a uniform value for all negative words and δ = 1 denotes that no smoothing is applied to word frequency.",5.3 Impact of α−c,[0],[0]
We can see that the total accuracy is only around 55% when δ = 0.,5.3 Impact of α−c,[0],[0]
"By increasing its value, the performance is gradually improved, achieving the highest score when δ is around 0.8.",5.3 Impact of α−c,[0],[0]
Further increase of δ will degrade the total accuracy.,5.3 Impact of α−c,[0],[0]
This analysis demonstrates the effectiveness of the proposed negative weighting scheme.,5.3 Impact of α−c,[0],[0]
Figure 3(a) compares the convergence between AllVec and GloVe on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Clearly, AllVec ex-
hibits a more stable convergence due to its full batch learning.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, GloVe has a more dramatic fluctuation because of the one-sample learning scheme.",5.4 Convergence Rate and Runtime,[0],[0]
Figure 3(b) shows the relationship between the embedding size k and runtime on NewsIR.,5.4 Convergence Rate and Runtime,[0],[0]
"Although the analysis in Section 4.3 demonstrates that the time complexity of AllVec is O(4|S|k + 4|V |k2), the actual runtime shows a near linear relationship with k.",5.4 Convergence Rate and Runtime,[0],[0]
"This is because 4|V |k2/4|S|k = k/c, where c generally ranges from 1000 ∼ 6000 and k is set from 200 to 300 in practice.",5.4 Convergence Rate and Runtime,[0],[0]
"The above ratio explains the fact that 4|S|k dominates the complexity, which is linear
with k and |S|.",5.4 Convergence Rate and Runtime,[0],[0]
We also compare the overall runtime of AllVec and SG on NewsIR and show the results in Table 5.,5.4 Convergence Rate and Runtime,[0],[0]
"As can be seen, the runtime of AllVec falls in the range of SG-3 and SG-7 in a single iteration, which confirms the theoretical analysis in Section 4.3.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast with SG, AllVec needs more iterations to converge.",5.4 Convergence Rate and Runtime,[0],[0]
"The reason is that each parameter in SG is updated many times during each iteration, although only one training example is used in each update.",5.4 Convergence Rate and Runtime,[0],[0]
"Despite this, the total run time of AllVec is still in a feasible range.",5.4 Convergence Rate and Runtime,[1.0],"['Despite this, the total run time of AllVec is still in a feasible range.']"
"Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.
",5.4 Convergence Rate and Runtime,[0.9999999403412826],"['Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.']"
"In practice, the runtime of our model in each iteration can be further reduced by increasing the number of parallel workers.",5.4 Convergence Rate and Runtime,[0],[0]
"Although baseline methods like SG and GloVe can also be parallelized, the stochastic gradient steps in these methods unnecessarily influence each other as there is no exact way to separate these updates for different workers.",5.4 Convergence Rate and Runtime,[0],[0]
"In other words, the parallelization of SGD is not well suited to a large number of work-
ers.",5.4 Convergence Rate and Runtime,[0],[0]
"In contrast, the parameter updates in AllVec are completely independent of each other, therefore AllVec does not have the update collision issue.",5.4 Convergence Rate and Runtime,[0],[0]
"This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words.",5.4 Convergence Rate and Runtime,[1.0],"['This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words.']"
"As such, AllVec can provide a near linear scaling without any approximation since there is no potential conflicts between updates.",5.4 Convergence Rate and Runtime,[0],[0]
"In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation.",6 Conclusion,[1.0],"['In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation.']"
"In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization.",6 Conclusion,[1.0],"['In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization.']"
"Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models.",6 Conclusion,[1.0],"['Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models.']"
"In future, we will extend
our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.",6 Conclusion,[1.00000001967346],"['In future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.']"
"Moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word embedding process.",6 Conclusion,[0],[0]
"Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.
",6 Conclusion,[0.9999999339900789],"['Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.']"
Acknowledgements.,6 Conclusion,[0],[0]
"This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its IRC@SG Funding Initiative.",6 Conclusion,[0],[0]
Joemon M.Jose and Xiangnan,6 Conclusion,[0],[0]
He are corresponding authors.,6 Conclusion,[0],[0]
Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations.,abstractText,[0],[0]
"However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution.",abstractText,[0],[0]
"Besides, SGD suffers from dramatic fluctuation due to the onesample learning scheme.",abstractText,[0],[0]
"In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples.",abstractText,[0],[0]
"Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples.",abstractText,[0],[0]
We evaluate AllVec on several benchmark tasks.,abstractText,[0],[0]
"Experiments show that AllVec outperforms samplingbased SGD methods with comparable efficiency, especially for small training corpora.",abstractText,[0],[0]
Batch IS NOT Heavy: Learning Word Representations From All Samples,title,[0],[0]
