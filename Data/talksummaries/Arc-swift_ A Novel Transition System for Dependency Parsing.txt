8	21	In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence).
9	19	Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).
15	14	Parser states are usually written as (σ|i, j|β,A), where σ|i denotes the stack with token i on the top, j|β denotes the buffer with token j at its leftmost, and A the set of dependency arcs.
16	37	Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse.
17	37	A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1 Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing.
18	33	It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack.
25	18	To illustrate the limitation of local transitions, consider parsing the following sentences: I ate fish with ketchup.
26	13	The two sentences have almost identical structures, with the notable difference that the prepositional phrase is complementing the direct object in the first case, and the main verb in the second.
29	15	Making the correct transition requires information about context words “ate” and “fish”, as well as “?”.
30	39	Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).
31	15	In contrast, inspired by graph-based parsers, we propose arc-swift, which defines non-local transitions as shown in Figure 2.
32	13	This allows direct comparison of different attachment points, and provides a direct solution to parsing the two example sentences.
40	12	We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD).
42	17	We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations.
43	21	Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification.
44	64	For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input.
50	11	The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).
51	27	For each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD.
54	14	We also analyzed the performance of parsers on attachments of different distances.
59	11	We study the computational efficiency of the arc-swift parser by comparing it to an arc-eager parser.
60	21	On the PTBSD development set, the average transition sequence length per sentence of arc-swift is 77.5% of that of arc-eager.
62	19	In contrast, beam search with beam size 2 for arc-eager requires evaluating 4 times the number of transition candidates compared to greedy parsing, which results in a UAS 0.14% worse and LAS 0.22% worse for arc-eager compared to greedily decoded arcswift.
63	17	We automatically extracted all labelled attachment errors by error type (incorrect attachment or relation), and categorized a few top parser errors by hand into linguistic constructions.
64	32	Results on PTB-SD are shown in Table 3.7 We note that the arc-swift parser improves accuracy on prepositional phrase (PP) and conjunction attachments, while it remains comparable to other parsers on other common errors.
80	11	In this paper, we introduced arc-swift, a novel transition system for dependency parsing.
85	55	Our model setup is similar to that of (Kiperwasser and Goldberg, 2016) (See Figure 5).
86	51	We employ two blocks of bidirectional long short-term memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997) that share very similar structures, one for part-of-speech (POS) tagging, the other for parsing.
88	44	As the input to the tagger BiLSTM, we represent words with 100-dimensional word embeddings, initialized with GloVe vectors (Pennington et al., 2014).8 The output distribution of the tagger classifier is used to compute a weighted sum of 32- dimensional POS embeddings, which is then concatenated with the output of the tagger BiLSTM (800-dimensional per token) as the input to the parser BiLSTM.
90	77	These representations are later merged according to the parser state to make transition predictions.
91	32	For traditional transition systems, we follow (Kiperwasser and Goldberg, 2016) by featurizing the top 3 tokens on the stack and the leftmost token in the buffer.
