2	37	Hypergraphs have been used in several learning applications such as clustering of categorical data (Gibson et al., 1998), multi-label classification (Sun et al., 2008), Laplacian sparse coding (Gao et al., 2013), image classification (Yu et al., 2012), image retrieval (Huang et al., 2010), mapping users across different social networks (Tan et al., 2014) and predicting edge labels in hypernode graphs (Ricatte et al., 2014).
3	72	In this paper, we consider semi-supervised learning on an edge-weighted hypergraph H = (V,E,w), with a set L of labeled vertices, whose labels are given by f∗L ∈ {−1,+1}L. The task is to predict the labels of the unlabeled vertices N , with the working principle that vertices contained in a hyperedge e ∈ E are more similar to one another if the edge weight we is larger.
4	22	This problem is also known as transductive inference and has been studied in (Zhou et al., 2006) and (Hein et al., 2013).
5	19	However, the methods in (Zhou et al., 2006) have been criticized by (Agarwal et al., 2006), because essentially a hypergraph is converted into a normal graph.
6	64	For instance, given a hyperedge e containing vertices S, either (i) a clique is added between the vertices in S, or (ii) a star is formed by adding a new vertex ve connecting every vertex in S to ve.
7	57	Then, a standard convex program using a regularization potential function for normal graphs can be applied (Zhu et al., 2003).
8	6	By choosing appropriate edge weights, it was shown in (Agarwal et al., 2006) that the two approaches are equivalent to the following convex program relaxation: min Φold(f) := 1 2 ∑ e∈E we ∑ {u,v}∈(e2) (fu − fv)2 subject to fu ∈ [−1, 1], ∀u ∈ V fu = f ∗ u , ∀u ∈ L. On the other hand, it was proposed in (Hein et al., 2013) that the following regularization function is more suitable to capture hyperedge expansion: Φnew(f) := 1 2 ∑ e∈E we · (max u∈e fu −min v∈e fv) 2.
10	47	In (Hein et al., 2013), a squared loss function was added by considering the convex program with objective function Φnew(f) + µ ‖f − f∗‖22 on f ∈ [−1, 1]V , where µ > 0 is a parameter to be tuned, f∗L is given by the labeled vertices L, and for the unlabeled vertices f∗N = 0.
12	4	However, as a result, unlabeled vertices have a tendency to acquire f values close to 0.
13	5	This might remove useful information as illustrated in the following example.
14	11	In Figure 1.1, vertices a, b ∈ L are labeled as +1 and c ∈ L is labeled as −1.
15	4	There are three (undirected) edges: {a, x}, {b, x} and {x, y, c}, each with unit weight.
17	21	Hence, this solution gives no useful information regarding the label for vertex y.
18	106	On the other hand, if we just use the objective function Φnew(f) with the constraints fL = f∗L, then in an optimal solution, fx = 13 , but fy could be anywhere in the confidence interval [−1, 13 ].
19	55	Hence, in this case, we could use the average value − 13 to predict −1 for vertex y.
20	4	In this paper, we revisit the approach used in (Hein et al., 2013) and consider several extensions and simplifications.
21	9	We summarize our results and give an outline of the paper as follows.
23	11	Inspired also from the recent result on Laplacians for directed normal graphs (Yoshida, 2016), we introduce a semisupervised learning framework using directed hypergraphs that can capture higher order causal relationships.
24	5	This notion of directed hypergraph was first introduced in (Gallo et al., 1993), who considered applications in propositional logic, analyzing dependency in relational database, and traffic analysis.
29	27	Confidence Interval for Unlabeled Vertices.
35	24	We remark that our framework is very easy to understand, because it is a variation on the well-known gradient descent.
36	28	In contrast, the primal-dual approach in (Hein et al., 2013) considers the convex conjugate of the primal objective and involves complicated update operations on the primal and dual variables.
37	30	The subgradient used in our approach gives the update direction, and we can actually solve exactly the same convex program with a much simpler method.
38	78	Section 5, we revisit some datasets in the UCI Machine Learning Repository (Lichman, 2013), and experiments confirm that our prediction model based on confidence interval gives better accuracy than that in (Hein et al., 2013).
39	81	Our simpler subgradient method takes more iterations than the primal-dual method (Hein et al., 2013), but each iteration is much faster.
40	135	Experiments show that overall both methods have similar running times, and the subgradient method has an advantage when the number of vertices is much larger than the number of edges.
41	35	Moreover, using the DBLP dataset (Ley, 2009), our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy.
43	3	We consider an edge-weighted directed hypergraph H = (V,E,w) with vertex set V (with n = |V |), edge set E and weight function w : E → R+.
45	18	In our application, each vertex v ∈ V is supposed to have a label in {−1,+1}.
46	27	Intuitively, the directed hypergraph attempts to capture the rule that for each edge e ∈ E, if there is a vertex in Te having label +1, then it is more likely for vertices in He to receive label +1.
