{"sections": [{"heading": "1. Introduction", "text": "Bayesian optimization (BO) has recently gained considerable traction due to its capability of finding the global maximum of a highly complex (e.g., non-convex, no closedform expression nor derivative), noisy black-box objective\n1Ludwig-Maximilians-Universita\u0308t, Munich, Germany. A substantial part of this research was performed during his student exchange program at the National University of Singapore under the supervision of Bryan Kian Hsiang Low and culminated in his Bachelor\u2019s thesis. 2Department of Computer Science, National University of Singapore, Republic of Singapore. Correspondence to: Bryan Kian Hsiang Low <lowkh@comp.nus.edu.sg>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nfunction with a limited budget of (often costly) function evaluations, consequently witnessing its use in an increasing diversity of application domains such as robotics, environmental sensing/monitoring, automatic machine learning, among others (Brochu et al., 2010; Shahriari et al., 2016). A number of acquisition functions (e.g., probability of improvement or expected improvement (EI) over the currently found maximum (Brochu et al., 2010), entropybased (Villemonteix et al., 2009; Hennig & Schuler, 2012; Herna\u0301ndez-Lobato et al., 2014), and upper confidence bound (UCB) (Srinivas et al., 2010)) have been devised to perform BO: They repeatedly select an input for evaluating/querying the black-box function (i.e., until the budget is depleted) that intuitively trades off between sampling where the maximum is likely to be given the current, possibly imprecise belief of the function modeled by a Gaussian process (GP) (i.e., exploitation) vs. improving the GP belief of the function over the entire input domain (i.e., exploration) to guarantee finding the global maximum.\nThe rapidly growing affordability and availability of hardware resources (e.g., computer clusters, sensor networks, robot teams/swarms) have motivated the recent development of BO algorithms that can repeatedly select a batch of inputs for querying the black-box function in parallel instead. Such batch/parallel BO algorithms can be classified into two types: On one extreme, batch BO algorithms like multi-points EI (q-EI) (Chevalier & Ginsbourger, 2013), parallel predictive entropy search (PPES) (Shah & Ghahramani, 2015), and the parallel knowledge gradient method (q-KG) (Wu & Frazier, 2016) jointly optimize the batch of inputs and hence scale poorly in the batch size. On the other extreme, greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; Gonza\u0301lez et al., 2016) boost the scalability by selecting the inputs of the batch one at a time. We argue that such a highly suboptimal approach to gain scalability is an overkill: In practice, each function evaluation is often much more computationally and/or economically costly (e.g., hyperparameter tuning for deep learning, drug testing on human subjects), which justifies dedicating more time to obtain better BO performance. In this paper, we show that it is in fact possible to jointly optimize the batch of inputs and still preserve scalability in the batch size by giving practitioners the flexibility to trade off BO performance for time efficiency.\nTo achieve this, we first observe that, interestingly, batch BO can be perceived as a cooperative multi-agent decision making problem whereby each agent optimizes a separate input of the batch while coordinating with the other agents doing likewise. To the best of our knowledge, this has not been considered in the BO literature. In particular, if batch BO can be framed as some known class of multi-agent decision making problems, then it can be solved efficiently and scalably by the latter\u2019s state-of-the-art solvers. The key technical challenge would therefore be to investigate how batch BO can be cast as one of such to exploit its advantage of scalability in the number of agents (hence, batch size) while at the same time theoretically guaranteeing the resulting BO performance.\nTo tackle the above challenge, this paper presents a novel distributed batch BO algorithm (Section 3) that, in contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; Gonza\u0301lez et al., 2016), can jointly optimize a batch of inputs and, unlike the batch BO algorithms (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size. To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem (DCOP) in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size. Our proposed distributed batch GPUCB (DB-GP-UCB) algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order. We provide a theoretical guarantee for the convergence rate of our DBGP-UCB algorithm via bounds on its cumulative regret. We empirically evaluate the cumulative regret incurred by our DB-GP-UCB algorithm and its scalability in the batch size on synthetic benchmark objective functions and a realworld optimization problem (Section 4)."}, {"heading": "2. Problem Statement, Background, and Notations", "text": "Consider the problem of sequentially optimizing an unknown objective function f : D \u2192 R where D \u2282 Rd denotes a domain of d-dimensional input feature vectors. We consider the domain to be discrete as it is known how to generalize results to a continuous, compact domain via suitable discretizations (Srinivas et al., 2010). In each iteration t = 1, . . . , T , a batch Dt \u2282 D of inputs is selected for evaluating/querying f to yield a corresponding column vector yDt , (yx) > x\u2208Dt of noisy observed outputs yx , f(x)+ with i.i.d. Gaussian noise \u223c N (0, \u03c32n) and noise variance \u03c32n.\nRegret. Supposing our goal is to get close to the global maximum f(x\u2217) as rapidly as possible where x\u2217 , arg maxx\u2208D f(x), this can be achieved by minimizing a standard batch BO objective such as the batch or full cumulative regret (Contal et al., 2013; Desautels et al., 2014): The notion of regret intuitively refers to a loss in reward from not knowing x\u2217 beforehand. Formally, the instantaneous regret incurred by selecting a single input x to evaluate its corresponding f is defined as rx , f(x\u2217)\u2212 f(x). Assuming a fixed cost of evaluating f for every possible batch Dt of the same size, the batch and full cumulative regrets are, respectively, defined as sums (over iteration t = 1, . . . , T ) of the smallest instantaneous regret incurred by any input within every batch Dt, i.e., RT , \u2211T t=1 minx\u2208Dt rx, and of the instantaneous regrets incurred by all inputs of every batch Dt, i.e., R\u2032T ,\u2211T t=1 \u2211 x\u2208Dt rx. The convergence rate of a batch BO algorithm can then be assessed based on some upper bound on the average regret RT /T or R\u2032T /T (Section 3) since the currently found maximum after T iterations is no further away from f(x\u2217) than RT /T or R\u2032T /T . It is desirable for a batch BO algorithm to asymptotically achieve no regret, i.e., limT\u2192\u221eRT /T = 0 or limT\u2192\u221eR\u2032T /T = 0, implying that it will eventually converge to the global maximum.\nGaussian Processes (GPs). To guarantee no regret (Section 3), the unknown objective function f is modeled as a sample of a GP. Let {f(x)}x\u2208D denote a GP, that is, every finite subset of {f(x)}x\u2208D follows a multivariate Gaussian distribution (Rasmussen & Williams, 2006). Then, the GP is fully specified by its prior mean mx , E[f(x)] and covariance kxx\u2032 , cov[f(x), f(x\u2032)] for all x,x\u2032 \u2208 D, which, for notational simplicity (and w.l.o.g.), are assumed to be zero, i.e., mx = 0, and bounded, i.e., kxx\u2032 \u2264 1, respectively. Given a column vector yD1:t-1 , (yx) > x\u2208D1:t-1 of noisy observed outputs for some set D1:t\u22121 , D1 \u222a . . . \u222a Dt\u22121 of inputs after t\u2212 1 iterations, a GP model can perform probabilistic regression by providing a predictive distribution p(fDt |yD1:t-1) = N (\u00b5Dt ,\u03a3DtDt) of the latent outputs fDt , (f(x)) > x\u2208Dt for any set Dt \u2286 D of inputs selected in iteration t with the following posterior mean vector and covariance matrix:\n\u00b5Dt,KDtD1:t-1(KD1:t-1D1:t-1+\u03c3 2 nI) \u22121yD1:t-1 , \u03a3DtDt,KDtDt\u2212KDtD1:t-1(KD1:t-1D1:t-1+\u03c32nI)\u22121KD1:t-1Dt (1) where KBB\u2032 , (kxx\u2032)x\u2208B,x\u2032\u2208B\u2032 for all B,B\u2032 \u2282 D. GP-UCB and its Greedy Batch Variants. Inspired by the UCB algorithm for the multi-armed bandit problem, the GP-UCB algorithm (Srinivas et al., 2010) selects, in each iteration, an input x \u2208 D for evaluating/querying f that trades off between sampling close to an expected maximum (i.e., with large posterior mean \u00b5{x}) given the current GP belief of f (i.e., exploitation) vs. that of high predictive un-\ncertainty (i.e., with large posterior variance \u03a3{x}{x}) to improve the GP belief of f over D (i.e., exploration), that is, maxx\u2208D \u00b5{x} + \u03b2 1/2 t \u03a3 1/2\n{x}{x} where the parameter \u03b2t > 0 is set to trade off between exploitation vs. exploration for bounding its cumulative regret.\nExisting generalizations of GP-UCB such as GP batch UCB (GP-BUCB) (Desautels et al., 2014) and GP-UCB with pure exploration (GP-UCB-PE) (Contal et al., 2013) are greedy batch BO algorithms that select the inputs of the batch one at a time (Section 1). Specifically, to avoid selecting the same input multiple times within a batch (hence reducing to GP-UCB), they update the posterior variance (but not the posterior mean) after adding each input to the batch, which can be performed prior to evaluating its corresponding f since the posterior variance is independent of the observed outputs (1). They differ in that GPBUCB greedily adds each input to the batch using GP-UCB (without updating the posterior mean) while GP-UCB-PE selects the first input using GP-UCB and each remaining input of the batch by maximizing only the posterior variance (i.e., pure exploration). Similarly, a recently proposed UCB-DPP-SAMPLE algorithm (Kathuria et al., 2016) selects the first input using GP-UCB and the remaining inputs by sampling from a determinantal point process (DPP). Like GP-BUCB, GP-UCB-PE, and UCB-DPP-SAMPLE, we can theoretically guarantee the convergence rate of our DB-GP-UCB algorithm, which, from a theoretical point of view, signifies an advantage of GP-UCB-based batch BO algorithms over those (e.g., q-EI and PPES) inspired by other acquisition functions such as EI and PES. Unlike these greedy batch BO algorithms (Contal et al., 2013; Desautels et al., 2014), our DB-GP-UCB algorithm can jointly optimize the batch of inputs while still preserving scalability in batch size by casting as a DCOP to be described next.\nDistributed Constraint Optimization Problem (DCOP). A DCOP can be defined as a tuple (X ,V,A, h,W) that comprises a set X of input random vectors, a set V of |X | corresponding finite domains (i.e., a separate domain for each random vector), a set A of agents, a function h : X \u2192 A assigning each input random vector to an agent responsible for optimizing it, and a setW , {wn}n=1,...,N of non-negative payoff functions such that each function wn defines a constraint over only a subset Xn \u2286 X of input random vectors and represents the joint payoff that the corresponding agents An , {h(x)|x \u2208 Xn} \u2286 A achieve. Solving a DCOP involves finding the input values ofX that maximize the sum of all functions w1, . . . , wn (i.e., social welfare maximization), that is, maxX \u2211N n=1 wn(Xn). To achieve a truly decentralized solution, each agent can only optimize its local input random vector(s) based on the assignment function h but communicate with its neighboring agents: Two agents are considered neighbors if there is a function/constraint involving input random vectors that\nthe agents have been assigned to optimize. Complete and approximation algorithms exist for solving a DCOP; see (Chapman et al., 2011; Leite et al., 2014) for reviews of such algorithms."}, {"heading": "3. Distributed Batch GP-UCB (DB-GP-UCB)", "text": "A straightforward generalization of GP-UCB (Srinivas et al., 2010) to jointly optimize a batch of inputs is to simply consider summing the GP-UCB acquisition function over all inputs of the batch. This, however, results in selecting the same input |Dt| times within a batch, hence reducing to GP-UCB, as explained earlier in Section 2. To resolve this issue but not suffer from the suboptimal behavior of greedy batch BO algorithms such as GP-BUCB (Desautels et al., 2014) and GP-UCB-PE (Contal et al., 2013), we propose a batch variant of GP-UCB that jointly optimizes a batch of inputs in each iteration t = 1, . . . , T according to\nmaxDt\u2282D 1 >\u00b5Dt + \u03b1 1/2 t I[fD;yDt |yD1:t-1 ]1/2 (2)\nwhere the parameter \u03b1t > 0, which performs a similar role to that of \u03b2t in GP-UCB, is set to trade off between exploitation vs. exploration for bounding its cumulative regret (Theorem 1) and the conditional mutual information1 I[fD;yDt |yD1:t-1 ] can be interpreted as the information gain on f over D (i.e., equivalent to fD , (f(x))>x\u2208D) by selecting the batch Dt of inputs for evaluating/querying f given the noisy observed outputs yD1:t-1 from the previous t \u2212 1 iterations. So, in each iteration t, our proposed batch GP-UCB algorithm (2) selects a batch Dt \u2282 D of inputs for evaluating/querying f that trades off between sampling close to expected maxima (i.e., with a large sum of posterior means 1>\u00b5Dt = \u2211 x\u2208Dt \u00b5{x}) given the current GP belief of f (i.e., exploitation) vs. that yielding a large information gain I[fD;yDt |yD1:t-1 ] on f over D to improve its GP belief (i.e., exploration). It can be derived that I[fD;yDt |yD1:t-1 ] = 0.5 log |I+\u03c3\u22122n \u03a3DtDt | (Appendix A), which implies that the exploration term in (2) can be maximized by spreading the batch Dt of inputs far apart to achieve large posterior variance individually and small magnitude of posterior covariance between them to encourage diversity.\nUnfortunately, our proposed batch variant of GP-UCB (2) involves evaluating prohibitively many batches of inputs (i.e., exponential in the batch size), hence scaling poorly in the batch size. However, we will show in this section that our batch variant of GP-UCB is, interestingly, amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the\n1In contrast to the BO algorithm of Contal et al. (2014) that also uses mutual information, our work here considers batch BO by exploiting the correlation information between inputs of a batch in our acquisition function in (2) to encourage diversity.\nefficiency of its state-of-the-art solvers for achieving linear time in the batch size.\nMarkov Approximation. The key idea is to design the structure of a matrix \u03a8DtDt whose log-determinant can closely approximate that of \u03a8DtDt , I + \u03c3 \u22122 n \u03a3DtDt residing in the I[fD;yDt |yD1:t-1 ] term in (2) and at the same time be decomposed into a sum of log-determinant terms, each of which is defined by submatrices of \u03a8DtDt that all depend on only a subset of the batch. Such a decomposition enables our resulting approximation of (2) to be formulated as a DCOP (Section 2).\nAt first glance, our proposed idea may be naively implemented by constructing a sparse block-diagonal matrix \u03a8DtDt using, say, the N > 1 diagonal blocks of \u03a8DtDt . Then, log |\u03a8DtDt | can be decomposed into a sum of logdeterminants of its diagonal blocks2, each of which depends on only a disjoint subset of the batch. This, however, entails an issue similar to that discussed at the beginning of this section of selecting the same |Dt|/N inputs N times within a batch due to the assumption of independence of outputs between different diagonal blocks of \u03a8DtDt . To address this issue, we significantly relax this assumption and show that it is in fact possible to construct a more refined, dense matrix approximation \u03a8DtDt by exploiting a Markov assumption, which consequently correlates the outputs between all its constituent blocks and is, perhaps surprisingly, still amenable to the decomposition to achieve scalability in the batch size.\nSpecifically, evenly partition the batch Dt of inputs into N \u2208 {1, . . . , |Dt|} disjoint subsets Dt1, . . . ,DtN and \u03a8DtDt (\u03a8DtDt ) into N \u00d7N square blocks, i.e., \u03a8DtDt , [\u03a8DtnDtn\u2032 ]n,n\u2032=1,...,N (\u03a8DtDt , [\u03a8DtnDtn\u2032 ]n,n\u2032=1,...,N ). Our first result below derives a decomposition of the logdeterminant of any symmetric positive definite block matrix \u03a8DtDt into a sum of log-determinant terms, each of which is defined by a separate diagonal block of the Cholesky factor of \u03a8\n\u22121 DtDt :\nProposition 1. Consider the Cholesky factorization of a symmetric positive definite \u03a8\n\u22121 DtDt , U >U where Cholesky factor U , [Unn\u2032 ]n,n\u2032=1,...,N (i.e., partitioned into N \u00d7 N square blocks) is an upper triangular block matrix (i.e., Unn\u2032 = 0 for n > n\u2032). Then, log |\u03a8DtDt | =\u2211N n=1 log |(U>nnUnn)\u22121|.\nIts proof (Appendix B) utilizes properties of the determinant and that the determinant of an upper triangular block matrix is a product of determinants of its diagonal blocks (i.e., |U | = \u220fNn=1 |Unn|). Proposition 1 reveals a subtle possibility of imposing some structure on the inverse of\n2The determinant of a block-diagonal matrix is a product of determinants of its diagonal blocks.\n\u03a8DtDt such that each diagonal block Unn of its Cholesky factor (and hence each log |(U>nnUnn)\u22121| term) will depend on only a subset of the batch. The following result presents one such possibility: Proposition 2. LetB \u2208 {1, . . . , N\u22121} be given. If \u03a8\u22121DtDt is B-block-banded3, then\n(U>nnUnn) \u22121 = \u03a8DtnDtn \u2212\u03a8DtnDBtn\u03a8 \u22121 DBtnDBtn\u03a8DBtnDtn\n(3) for n = 1, . . . , N where \u03b7 , min(n + B,N), DBtn , \u22c3\u03b7 n\u2032=n+1Dtn\u2032 , \u03a8DtnDBtn , [\u03a8DtnDtn\u2032 ]n\u2032=n+1,...,\u03b7 , \u03a8DBtnDBtn , [\u03a8Dtn\u2032Dtn\u2032\u2032 ]n\u2032,n\u2032\u2032=n+1,...,\u03b7, and \u03a8DBtnDtn , \u03a8 > DtnDBtn .\nIts proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 1). Proposition 2 indicates that if \u03a8\n\u22121 DtDt is B-block-banded (Fig. 1b), then\neach log |(U>nnUnn)\u22121| term depends on only the subset Dtn \u222a DBtn = \u22c3\u03b7 n\u2032=nDtn\u2032 of the batch Dt (Fig. 1c).\nOur next result defines a structure of \u03a8DtDt in terms of the blocks within the B-block band of \u03a8DtDt to induce a B-block-banded inverse of \u03a8DtDt :\nProposition 3. Let\n\u03a8DtnDtn\u2032,  \u03a8DtnDtn\u2032 if |\u2206| \u2264 B, \u03a8DtnDBtn\u03a8 \u22121 DBtnDBtn\n\u03a8DBtnDtn\u2032 if \u2206 < \u2212B, \u03a8DtnDBtn\u2032 \u03a8\u22121DB tn\u2032D B tn\u2032 \u03a8DB tn\u2032Dtn\u2032 if \u2206 > B;\n(4) where \u2206 , n\u2212n\u2032 for n, n\u2032 = 1, . . . , N (see Fig. 1a). Then, \u03a8 \u22121 DtDt is B-block-banded (see Fig. 1b).\nIts proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 3). It can be observed from (4) and Fig. 1 that (a) though \u03a8\n\u22121 DtDt is a sparse\nB-block-banded matrix, \u03a8DtDt is a dense matrix approximation for B = 1, . . . , N \u2212 1; (b) when B = N \u2212 1 or N = 1, \u03a8DtDt = \u03a8DtDt ; and (c) the blocks within the B-block band of \u03a8DtDt (i.e., |n \u2212 n\u2032| \u2264 B) coincide with that of \u03a8DtDt while each block outside the Bblock band of \u03a8DtDt (i.e., |n \u2212 n\u2032| > B) is fully specified by the blocks within the B-block band of \u03a8DtDt (i.e., |n \u2212 n\u2032| \u2264 B) due to its recursive series of |n \u2212 n\u2032| \u2212 B reduced-rank approximations (Fig. 1a). Note, however, that the log |(U>nnUnn)\u22121| terms (3) for n = 1, . . . , N depend on only the blocks within (and not outside) the B-block band of \u03a8DtDt (Fig. 1c).\nRemark 1. Proposition 3 provides an attractive principled interpretation: Let \u03b5x , \u03c3\u22121n (yx \u2212 \u00b5{x}) denote a scaled\n3A block matrix P , [Pnn\u2032 ]n,n\u2032=1,...,N (i.e., partitioned into N \u00d7N square blocks) is B-block-banded if any block Pnn\u2032 outside its B-block band (i.e., |n\u2212 n\u2032| > B) is 0.\nresidual incurred by the GP predictive mean (1). Its covariance is then cov[\u03b5x, \u03b5x\u2032 ] = \u03a8{x}{x\u2032}. In the same spirit as a Gaussian Markov random process, imposing a B-th order Markov property on the residual process {\u03b5x}x\u2208Dt is equivalent to approximating \u03a8DtDt with \u03a8DtDt (4) whose inverse isB-block-banded. In other words, if |n\u2212n\u2032| > B, then {\u03b5x}x\u2208Dtn and {\u03b5x}x\u2208Dtn\u2032 are conditionally independent given {\u03b5x}x\u2208Dt\\(Dtn\u222aDtn\u2032 ). This conditional independence assumption therefore becomes more relaxed with a larger batch Dt. Proposition 2 demonstrates the importance of such a B-th order Markov assumption (or, equivalently, the sparsity ofB-block-banded \u03a8\n\u22121 DtDt ) to achieving\nscalability in the batch size.\nRemark 2. Regarding the approximation quality of \u03a8DtDt (4), the following result (see Appendix C for its proof) shows that the Kullback-Leibler (KL) distance of \u03a8DtDt from \u03a8DtDt measures an intuitive notion of the approximation error of \u03a8DtDt being the difference in information gain when relying on our Markov approximation, which can be bounded by some quantity \u03bdt:\nProposition 4. Let the KL distance DKL(\u03a8, \u03a8\u0303) , 0.5(tr(\u03a8\u03a8\u0303\u22121) \u2212 log |\u03a8\u03a8\u0303\u22121| \u2212 |Dt|) between two symmetric positive definite |Dt| \u00d7 |Dt| matrices \u03a8 and \u03a8\u0303 measure the error of approximating \u03a8 with \u03a8\u0303. Also, let I\u0303[fD;yDt |yD1:t-1 ] , 0.5 log |\u03a8DtDt | denote the approximated information gain, and C \u2265 I[f{x};yDt |yD1:t-1 ] for all x \u2208 D and t \u2208 N. Then, for all t \u2208 N,\nDKL(\u03a8DtDt ,\u03a8DtDt) = I\u0303[fD;yDt |yD1:t-1 ]\u2212 I[fD;yDt |yD1:t-1 ] \u2264 (exp(2C)\u2212 1) I[fD;yDt |yD1:t-1 ] , \u03bdt .\nProposition 4 implies that the approximated information gain I\u0303[fD;yDt |yD1:t-1 ] is never smaller than the exact information gain I[fD;yDt |yD1:t-1 ] since DKL(\u03a8DtDt ,\u03a8DtDt) \u2265 0 with equality when N = 1, in which case \u03a8DtDt = \u03a8DtDt (4). Thus, intuitively, our proposed Markov approximation hallucinates information into \u03a8DtDt to yield an optimistic estimate of the information gain (by selecting a particular batch), ultimately making our resulting algorithm overconfident in selecting a batch. This overconfidence is information-theoretically quantified by the approximation error DKL(\u03a8DtDt ,\u03a8DtDt) \u2264 \u03bdt. Remark 3. The KL distanceDKL(\u03a8DtDt ,\u03a8DtDt) of \u03a8DtDt from \u03a8DtDt is also the least among all |Dt|\u00d7|Dt|matrices with a B-block-banded inverse, as proven in Appendix D.\nDCOP Formulation. By exploiting the approximated information gain I\u0303[fD;yDt |yD1:t-1 ] (Proposition 4), Proposition 1, (3), and (4), our batch variant of GP-UCB (2) can be reformulated in an approximate sense4 to a distributed batch GP-UCB (DB-GP-UCB) algorithm5 that jointly optimizes a batch of inputs in each iteration t = 1, . . . , T according to\nDt , arg max Dt\u2282D N\u2211 n=1 wn(Dtn \u222a DBtn)\nwn(Dtn \u222a DBtn) , 1>\u00b5Dtn+(0.5\u03b1t log |\u03a8DtnDtn|DBtn |) 1/2\n(5) with \u03a8DtnDtn|DBtn ,\u03a8DtnDtn\u2212\u03a8DtnDBtn\u03a8 \u22121 DBtnDBtn \u03a8DBtnDtn .\n4Note that our acquisition function (5) uses \u2211N\nn=1(log | \u00b7 |) 1/2 instead of ( \u2211N\nn=1 log | \u00b7 |) 1/2 to enable the decomposition.\n5Pseudocode for DB-GP-UCB is provided in Appendix E.\nNote that (5) is equivalent to our batch variant of GP-UCB (2) when N = 1. It can also be observed that (5) is naturally formulated as a multi-agent DCOP (Section 2) whereby every agent an \u2208 A is responsible for optimizing a disjoint subset Dtn of the batch Dt for n = 1, . . . , N and each function wn defines a constraint over only the subset Dtn \u222a DBtn = \u22c3\u03b7 n\u2032=nDtn\u2032 of the batch Dt and represents the joint payoff that the corresponding agents An , {an\u2032}\u03b7n\u2032=n \u2286 A achieve. As a result, (5) can be efficiently and scalably solved by the state-of-the-art DCOP algorithms (Chapman et al., 2011; Leite et al., 2014). For example, the time complexity of an iterative message-passing algorithm called max-sum (Farinelli et al., 2008) scales exponentially in only the largest arity maxn\u2208{1,...,N} |Dtn \u222a DBtn| = (B+1)|Dt|/N of the functionsw1, . . . , wN . Given a limited time budget, a practitioner can set a maximum arity of \u03c9 for any function wn, after which the number N of functions is adjusted to d(B + 1)|Dt|/\u03c9e so that the time incurred by max-sum to solve the DCOP in (5) is O(|D|\u03c9\u03c93B|Dt|)6 per iteration (i.e., linear in the batch size |Dt| by assuming \u03c9 and the Markov orderB to be constants). In contrast, our batch variant of GP-UCB (2) incurs exponential time in the batch size |Dt|. The max-sum algorithm is also amenable to a distributed implementation on a cluster of parallel machines to boost scalability further. If a solution quality guarantee is desired, then a variant of maxsum called bounded max-sum (Rogers et al., 2011) can be used7. Finally, the Markov order B can be varied to trade off between the approximation quality of \u03a8DtDt (4) and the time efficiency of max-sum in solving the DCOP in (5).\nRegret Bounds. Our main result to follow derives probabilistic bounds on the cumulative regret of DB-GP-UCB:\nTheorem 1. Let \u03b4 \u2208 (0, 1) be given, C1 , 4/ log(1 + \u03c3\u22122n ), \u03b3T , maxD1:T\u2282D I[fD;yD1:T ], \u03b1t , C1|Dt| exp(2C) log(|D|t2\u03c02/(6\u03b4)), and \u03bd\u0304T , \u2211T t=1 \u03bdt. Then, for the batch and full cumulative regrets incurred by our DB-GP-UCB algorithm (5),\nRT \u2264 2 ( T |DT |\u22122\u03b1TN(\u03b3T + \u03bd\u0304T ) )1/2 and R\u2032T \u2264 2 (T\u03b1TN(\u03b3T + \u03bd\u0304T )) 1/2\nhold with probability of at least 1\u2212 \u03b4. 6We assume the use of online sparse GP models (Csato\u0301 &\nOpper, 2002; Hensman et al., 2013; Hoang et al., 2015; 2017; Low et al., 2014b; Xu et al., 2014) that can update the GP predictive/posterior distribution (1) in constant time in each iteration.\n7Bounded max-sum is previously used in (Rogers et al., 2011) to solve a related maximum entropy sampling problem (Shewry & Wynn, 1987) formulated as a DCOP. But, the largest arity of any function wn in this DCOP is still the batch size |Dt| and, unlike the focus of our work here, no attempt is made in (Rogers et al., 2011) to reduce it, thus causing max-sum and bounded max-sum to incur exponential time in |Dt|. In fact, our proposed Markov approximation can be applied to this problem to reduce the largest arity of any function wn in this DCOP to again (B + 1)|Dt|/N .\nIts proof (Appendix F), when compared to that of GP-UCB (Srinivas et al., 2010) and its greedy batch variants (Contal et al., 2013; Desautels et al., 2014), requires tackling the additional technical challenges associated with jointly optimizing a batch Dt of inputs in each iteration t. Note that the uncertainty sampling based initialization strategy proposed by Desautels et al. (2014) can be employed to replace the \u221a exp(2C) term (i.e., growing linearly in |Dt|) appearing in our regret bounds by a kernel-dependent constant factor of C \u2032 that is independent of |Dt|; values of C \u2032 for the most commonly-used kernels are replicated in Table 2 in Appendix G (see section 4.5 in (Desautels et al., 2014) for a more detailed discussion on this issue).\nTable 1 in Appendix G compares the bounds on RT of DB-GP-UCB (5), GP-UCB-PE, GP-BUCB, GP-UCB, and UCB-DPP-SAMPLE. Compared to the bounds on RT of GP-UCB-PE and UCB-DPP-SAMPLE, our bound includes the additional kernel-dependent factor of C \u2032, which is similar to GP-BUCB. In fact, our regret bound is of the same form as that of GP-BUCB except that our bound incorporates a parameter N of our Markov approximation and an upper bound \u03bd\u0304T on the cumulative approximation error, both of which vanish for our batch variant of GP-UCB (2):\nCorollary 1. For our batch variant of GP-UCB (2), the cumulative regrets reduce to RT \u2264 2 ( T |DT |\u22122\u03b1T \u03b3T\n)1/2 and R\u2032T \u2264 2 (T\u03b1T \u03b3T ) 1/2.\nCorollary 1 follows directly from Theorem 1 and by noting that for our batch variant (2), N = 1 (since \u03a8DtDt then trivially reduces to \u03a8DtDt ) and \u03bdt = 0 for t = 1, . . . , T .\nFinally, the convergence rate of our DB-GP-UCB algorithm is dominated by the growth behavior of \u03b3T + \u03bd\u0304T . While it is well-known that the bounds on the maximum mutual information \u03b3T established for the commonly-used linear, squared exponential, and Mate\u0301rn kernels in (Srinivas et al., 2010; Kathuria et al., 2016) (i.e., replicated in Table 2 in Appendix G) only grow sublinearly in T , it is not immediately clear how the upper bound \u03bd\u0304T on the cumulative approximation error behaves. Our next result reveals that \u03bd\u0304T in fact only grows sublinearly in T as well: Corollary 2. \u03bd\u0304T \u2264 (exp(2C)\u2212 1)\u03b3T .\nCorollary 2 follows directly from the definitions of \u03bdt in Proposition 4 and \u03bd\u0304T and \u03b3T in Theorem 1 and applying the chain rule for mutual information. Since \u03b3T grows sublinearly in T for the above-mentioned kernels (Srinivas et al., 2010) and C can be chosen to be independent of T (e.g., C , \u03b3|Dt|\u22121) (Desautels et al., 2014), it follows from Corollary 2 that \u03bd\u0304T grows sublinearly in T . As a result, Theorem 1 guarantees sublinear cumulative regrets for the above-mentioned kernels, which implies that our DBGP-UCB algorithm (5) asymptotically achieves no regret, regardless of the degree of our proposed Markov approxi-\nmation (i.e., configuration of [N,B]). Thus, our batch variant of GP-UCB (2) achieves no regret as well."}, {"heading": "4. Experiments and Discussion", "text": "This section evaluates the cumulative regret incurred by our DB-GP-UCB algorithm (5) and its scalability in the batch size empirically on two synthetic benchmark objective functions such as Branin-Hoo (Lizotte, 2008) and gSobol (Gonza\u0301lez et al., 2016) (Table 3 in Appendix H) and a real-world pH field of Broom\u2019s Barn farm (Webster & Oliver, 2007) (Fig. 3 in Appendix H) spatially distributed over a 1200 m by 680 m region discretized into a 31 \u00d7 18 grid of sampling locations. These objective functions and the real-world pH field are each modeled as a sample of a GP whose prior covariance is defined by the widelyused squared exponential kernel kxx\u2032 , \u03c32s exp(\u22120.5(x\u2212 x\u2032)>\u039b\u22122(x\u2212 x\u2032)) where \u039b , diag[`1, . . . , `d] and \u03c32s are its length-scale and signal variance hyperparameters, respectively. These hyperparameters together with the noise variance \u03c32n are learned using maximum likelihood estimation (Rasmussen & Williams, 2006).\nThe performance of our DB-GP-UCB algorithm (5) is compared with the state-of-the-art batch BO algorithms such as GP-BUCB (Desautels et al., 2014), GP-UCB-PE (Contal et al., 2013), SM-UCB (Azimi et al., 2010), q-EI (Chevalier & Ginsbourger, 2013), and BBO-LP by plugging in GP-UCB (Gonza\u0301lez et al., 2016), whose implementations8 are publicly available. These batch BO algorithms are evaluated using a performance metric that measures the cumulative regret incurred by a tested algorithm:\u2211T t=1 f(x\n\u2217) \u2212 f(x\u0303t) where x\u0303t , arg maxxt\u2208D \u00b5{xt} (1) is the recommendation of the tested algorithm after t batch evaluations. For each experiment, 5 noisy observations are randomly selected and used for initialization. This is independently repeated 64 times and we report the resulting mean cumulative regret incurred by a tested algorithm. All experiments are run on a Linux system with Intelr Xeonr E5-2670 at 2.6GHz with 96 GB memory.\nFor our experiments, we use a fixed budget of T |DT | = 64 function evaluations and analyze the trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) on the performance of the tested algorithms. This experimental setup represents a practical scenario of costly function evaluations: On one hand, when a function evaluation is computationally costly (i.e., time-consuming), it is more desirable to evaluate f for a larger batch (e.g., |DT | = 16) of inputs in parallel in each iteration t (i.e., if hardware resources permit) to reduce the total time needed (hence smaller T ). On the other hand,\n8Details on the used implementations are given in Table 4 in Appendix I. We implemented DB-GP-UCB in MATLAB to exploit the GPML toolbox (Rasmussen & Williams, 2006).\nwhen a function evaluation is economically costly, one may be willing to instead invest more time (hence larger T ) to evaluate f for a smaller batch (e.g., |DT | = 2) of inputs in each iteration t in return for a higher frequency of information and consequently a more adaptive BO to achieve potentially better performance. In some settings, both factors may be equally important, that is, moderate values of |DT | and T are desired. To the best of our knowledge, such a form of empirical analysis does not seem to be available in the batch BO literature.\nFig. 2 shows results9 of the cumulative regret incurred by the tested algorithms to analyze their trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) using a fixed budget of T |DT | = 64 function evaluations for the Branin-Hoo function (left column), gSobol function (middle column), and real-world pH field (right column). Our DB-GP-UCB algorithm uses the configurations of [N,B] = [4, 2], [8, 5], [16, 10] in the experiments with batch size |DT | = 4, 8, 16, respectively; in the case of |DT | = 2, we use our batch variant of GPUCB (2) which is equivalent to DB-GP-UCB whenN = 1. It can be observed that DB-GP-UCB achieves lower cumulative regret than GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP in all experiments (with the only exception being the gSobol function for the smallest batch size of |DT | = 2 where BBO-LP performs slightly better) since DB-GPUCB can jointly optimize a batch of inputs while GPBUCB, GP-UCB-PE, SM-UCB, and BBO-LP are greedy batch algorithms that select the inputs of a batch one at time. Note that as the real-world pH field is not as wellbehaved as the synthetic benchmark functions (see Fig. 3 in Appendix H), the estimate of the Lipschitz constant by BBO-LP is potentially worse, hence likely degrading its performance. Furthermore, DB-GP-UCB can scale to a much larger batch size of 16 than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES (Shah & Ghahramani, 2015) and q-KG (Wu & Frazier, 2016): Results of q-EI are not available for |DT | \u2265 4 as they require a prohibitively huge computational effort to be obtained10 while PPES can only operate with a small batch size of up to 3 for the Branin-Hoo function and up to 4 for other functions, as reported in (Shah & Ghahramani, 2015), and q-KG can only operate with a small batch size of 4 for all tested functions (including the Branin-Hoo function and four others), as reported in (Wu & Frazier, 2016). The scalability of DB-GP-UCB is attributed to our proposed Markov approximation of our\n9Error bars are omitted in Fig. 2 to preserve the readability of the graphs. A replication of the graphs in Fig. 2 including standard error bars is provided in Appendix K.\n10In the experiments of Gonza\u0301lez et al. (2016), q-EI can reach a batch size of up to 10 but performs much worse than GP-BUCB, which is likely due to a considerable downsampling of possible batches available for selection in each iteration.\nbatch variant of GP-UCB (2) (Section 3), which can then be naturally formulated as a multi-agent DCOP (5) in order to fully exploit the efficiency of one of its state-of-the-art solvers called max-sum (Farinelli et al., 2008). In the experiments with the largest batch size of |DT | = 16, we have reduced the number of iterations in max-sum to less than 5 without waiting for convergence to preserve the efficiency of DB-GP-UCB, thus sacrificing its BO performance. Nevertheless, DB-GP-UCB can still outperform the other tested\nbatch BO algorithms.\nWe have also investigated and analyzed the trade-off between approximation quality and time efficiency of our DPGP-UCB algorithm and reported the results in Appendix J due to lack of space. To summarize, it can be observed from our results that the approximation quality improves near-linearly with an increasing Markov order B at the expense of higher computational cost (i.e., exponential in B)."}, {"heading": "5. Conclusion", "text": "This paper develops a novel distributed batch GP-UCB (DB-GP-UCB) algorithm for performing batch BO of highly complex, costly-to-evaluate, noisy black-box objective functions. In contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; Gonza\u0301lez et al., 2016), our DB-GP-UCB algorithm can jointly optimize a batch of inputs and, unlike (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size. To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the efficiency of its state-of-the-art solvers such as max-sum (Farinelli et al., 2008; Rogers et al., 2011) for achieving linear time in the batch size. Our proposed DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order. We provide a theoretical guarantee for the convergence rate of our DB-GP-UCB algorithm via bounds on its cumulative regret. Empirical evaluation on synthetic benchmark objective functions and a real-world pH field shows that our DB-GP-UCB algorithm can achieve lower cumulative regret than the greedy batch BO algorithms such as GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP, and scale to larger batch sizes than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES, and q-KG. For future work, we plan to generalize DB-GP-UCB (a) to the nonmyopic context by appealing to existing literature on nonmyopic BO (Ling et al., 2016) and active learning (Cao et al., 2013; Hoang et al., 2014a;b; Low et al., 2008; 2009; 2011; 2014a) as well as (b) to be performed by a multi-robot team to find hotspots in environmental sensing/monitoring by seeking inspiration from existing literature on multi-robot active sensing/learning (Chen et al., 2012; 2013b; 2015; Low et al., 2012; Ouyang et al., 2014). For applications with a huge budget of function evaluations, we like to couple DB-GP-UCB with the use of parallel/distributed sparse GP models (Chen et al., 2013a; Hoang et al., 2016; Low et al., 2015) to represent the belief of the unknown objective function efficiently."}, {"heading": "Acknowledgements", "text": "This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2, MOE2016-T2-2-156. Erik A. Daxberger would like to thank Volker Tresp for his advice throughout this research project."}], "year": 2017, "references": [{"title": "A nonparametric approach to noisy and costly optimization", "authors": ["B.S. Anderson", "A.W. Moore", "D. Cohn"], "venue": "In Proc. ICML,", "year": 2000}, {"title": "Block matrices with L-blockbanded inverse: Inversion algorithms", "authors": ["A. Asif", "J.M.F. Moura"], "venue": "IEEE Trans. Signal Processing,", "year": 2005}, {"title": "Batch Bayesian optimization via simulation matching", "authors": ["J. Azimi", "A. Fern", "X.Z. Fern"], "venue": "In Proc. NIPS, pp", "year": 2010}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "authors": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "year": 2010}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "authors": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "In Proc. AAMAS, pp", "year": 2013}, {"title": "A unifying framework for iterative approximate bestresponse algorithms for distributed constraint optimisation problems", "authors": ["A. Chapman", "A. Rogers", "N.R. Jennings", "D. Leslie"], "venue": "The Knowledge Engineering Review,", "year": 2011}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "authors": ["J. Chen", "K.H. Low", "Tan", "C.K.-Y", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "In Proc. UAI,", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "authors": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "Tan", "C.K.-Y", "P. Jaillet"], "venue": "In Proc. UAI, pp", "year": 2013}, {"title": "Gaussian processbased decentralized data fusion and active sensing for mobility-on-demand system", "authors": ["J. Chen", "K.H. Low", "Tan", "C.K.-Y"], "venue": "In Proc. RSS,", "year": 2013}, {"title": "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobilityon-demand systems", "authors": ["J. Chen", "K.H. Low", "P. Jaillet", "Y. Yao"], "venue": "IEEE Trans. Autom. Sci. Eng.,", "year": 2015}, {"title": "Fast computation of the multi-points expected improvement with applications in batch selection", "authors": ["C. Chevalier", "D. Ginsbourger"], "venue": "In Proc. 7th International Conference on Learning and Intelligent Optimization,", "year": 2013}, {"title": "Parallel Gaussian process optimization with upper confidence bound and pure exploration", "authors": ["E. Contal", "D. Buffoni", "A. Robicquet", "N. Vayatis"], "venue": "In Proc. ECML/PKDD,", "year": 2013}, {"title": "Gaussian process optimization with mutual information", "authors": ["E. Contal", "V. Perchet", "N. Vayatis"], "venue": "In Proc. ICML, pp", "year": 2014}, {"title": "Sparse online Gaussian processes", "authors": ["L. Csat\u00f3", "M. Opper"], "venue": "Neural Computation,", "year": 2002}, {"title": "Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization", "authors": ["T. Desautels", "A. Krause", "J.W. Burdick"], "venue": "JMLR, 15:4053\u20134103,", "year": 2014}, {"title": "Decentralised coordination of low-power embedded devices using the max-sum algorithm", "authors": ["A. Farinelli", "A. Rogers", "A. Petcu", "N.R. Jennings"], "venue": "In Proc. AAMAS,", "year": 2008}, {"title": "Batch Bayesian optimization via local penalization", "authors": ["J. Gonz\u00e1lez", "Z. Dai", "P. Hennig", "N.D. Lawrence"], "venue": "In Proc. AISTATS,", "year": 2016}, {"title": "Entropy search for information-efficient global optimization", "authors": ["P. Hennig", "C.J. Schuler"], "year": 2012}, {"title": "Gaussian processes for big data", "authors": ["J. Hensman", "N. Fusi", "N.D. Lawrence"], "venue": "In Proc. UAI,", "year": 2013}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "authors": ["J.M. Hern\u00e1ndez-Lobato", "M.W. Hoffman", "Z. Ghahramani"], "venue": "In Proc. NIPS,", "year": 2014}, {"title": "A generalized stochastic variational Bayesian hyperparameter learning framework for sparse spectrum Gaussian process regression", "authors": ["Q.M. Hoang", "T.N. Hoang", "K.H. Low"], "venue": "In Proc. AAAI,", "year": 2017}, {"title": "Active learning is planning: Nonmyopic -Bayesoptimal active learning of Gaussian processes", "authors": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "In Proc. ECML/PKDD Nectar Track,", "year": 2014}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "authors": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "In Proc. ICML,", "year": 2014}, {"title": "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data", "authors": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "In Proc. ICML, pp", "year": 2015}, {"title": "A distributed variational inference framework for unifying parallel sparse Gaussian process regression models", "authors": ["T.N. Hoang", "Q.M. Hoang", "K.H. Low"], "venue": "In Proc. ICML,", "year": 2016}, {"title": "Batched Gaussian process bandit optimization via determinantal point processes", "authors": ["T. Kathuria", "A. Deshpande", "P. Kohli"], "venue": "In Proc. NIPS,", "year": 2016}, {"title": "Distributed constraint optimization problems: Review and perspectives", "authors": ["A.R. Leite", "F. Enembreck", "Barth\u00e8s", "J.-P. A"], "venue": "Expert Systems with Applications,", "year": 2014}, {"title": "Gaussian process planning with Lipschitz continuous reward functions: Towards unifying Bayesian optimization, active learning, and beyond", "authors": ["C.K. Ling", "K.H. Low", "P. Jaillet"], "venue": "In Proc. AAAI,", "year": 2016}, {"title": "Practical Bayesian optimization", "authors": ["D.J. Lizotte"], "venue": "Ph.D. Thesis, University of Alberta,", "year": 2008}, {"title": "Adaptive multirobot wide-area exploration and mapping", "authors": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. AAMAS, pp", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "authors": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. ICAPS,", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "authors": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "In Proc. AAMAS,", "year": 2011}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "authors": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "In Proc. AAMAS, pp", "year": 2012}, {"title": "Generalized online sparse Gaussian processes with application to persistent mobile robot localization", "authors": ["K.H. Low", "N. Xu", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "In Proc. ECML/PKDD Nectar Track,", "year": 2014}, {"title": "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation", "authors": ["K.H. Low", "J. Yu", "J. Chen", "P. Jaillet"], "venue": "In Proc. AAAI,", "year": 2015}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "authors": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "In Proc. AAMAS,", "year": 2014}, {"title": "Gaussian Processes for Machine Learning", "authors": ["C.E. Rasmussen", "C.K.I. Williams"], "year": 2006}, {"title": "Bounded approximate decentralised coordination via the max-sum", "authors": ["A. Rogers", "A. Farinelli", "R. Stranders", "N.R. Jennings"], "venue": "algorithm. AIJ,", "year": 2011}, {"title": "Parallel predictive entropy search for batch global optimization of expensive objective functions", "authors": ["A. Shah", "Z. Ghahramani"], "venue": "In Proc. NIPS,", "year": 2015}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "authors": ["B. Shahriari", "K. Swersky", "Z. Wang", "R.P. Adams", "N. de Freitas"], "venue": "Proceedings of the IEEE,", "year": 2016}, {"title": "Maximum entropy sampling", "authors": ["M.C. Shewry", "H.P. Wynn"], "venue": "J. Applied Statistics,", "year": 1987}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "authors": ["N. Srinivas", "A. Krause", "S. Kakade", "M. Seeger"], "venue": "In Proc. ICML,", "year": 2010}, {"title": "An informational approach to the global optimization of expensiveto-evaluate functions", "authors": ["J. Villemonteix", "E. Vazquez", "E. Walter"], "venue": "J. Glob. Optim.,", "year": 2009}, {"title": "Geostatistics for Environmental Scientists", "authors": ["R. Webster", "M. Oliver"], "year": 2007}, {"title": "The parallel knowledge gradient method for batch Bayesian optimization", "authors": ["J. Wu", "P. Frazier"], "venue": "In Proc. NIPS,", "year": 2016}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "authors": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. Ozgul"], "venue": "In Proc. AAAI,", "year": 2014}], "id": "SP:7d10c2419c91bc6db57b96a5af917d4e267686ad", "authors": [{"name": "Erik A. Daxberger", "affiliations": []}, {"name": "Bryan Kian", "affiliations": []}, {"name": "Hsiang Low", "affiliations": []}], "abstractText": "This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions. In contrast to existing batch BO algorithms, DBGP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size. To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size. Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order. We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret. Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the stateof-the-art batch BO algorithms.", "title": "Distributed Batch Gaussian Process Optimization"}