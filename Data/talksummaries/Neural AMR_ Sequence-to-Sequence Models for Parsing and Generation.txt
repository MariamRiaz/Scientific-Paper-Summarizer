17	1	We also provide extensive ablative and qualitative analysis, quantifying the contributions that come from preprocessing and the paired training procedure.
60	1	To reduce the effect of sparsity, we use an external unannotated corpus of sentences Se, and a procedure which pairs the training of the parser and generator.
61	1	Our procedure is described in Algorithm 1, and first trains a parser on the datasetD of pairs of sentences and AMR graphs.
68	1	Every iteration of self-training has three phases: (1) parsing samples from a large, unlabeled corpus Se, (2) creating a new set of parameters by training on Se, and (3) fine-tuning those parameters on the original paired data.
78	1	Following Pourdamghani et al. (2016) we also remove senses from all concepts for AMR generation only.
88	1	We record this mapping for use during testing of generation models.
97	1	To produce fine-grained named entities, we run the Stanford NER system and first try to replace any identified span with a fine-grained category based on alignments observed during training.
98	1	If this fails, we anonymize the sentence using the coarse categories predicted by the NER system, which are also categories in AMR.
101	1	For example, in Figure 2, starting at meet the order contains meet, :ARG0, person, :ARG1-of, expert, :ARG2-of, group, :ARG2-of, :ARG1-of, :ARG0.4 The order traverses children in the sequence they are presented in the AMR.
105	1	In case the node has only one child we omit the scope markers (denoted with left “(”, and right “)” parentheses), thus significantly reducing the number of generated tokens.
106	1	Figure 2(d) contains an example showing all of the preprocessing techniques and scope markers that we use in our full model.
110	1	Table 2 summarizes statistics about the original dataset and the extracted portions of Gigaword.
111	1	We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5.
112	1	We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1).
115	1	Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8.
122	1	During prediction we perform decoding using beam search and set the beam size to 5 both for parsing and generation.
128	1	All other models that we compare against use semantic resources, such as WordNet, dependency parsers or CCG parsers (models marked with * were trained with less data, but only evaluate on newswire text; the rest evaluate on the full test set, containing text from blogs).
134	1	Sparsity Reduction Even after anonymization of open class vocabulary entries, we still encounter a great deal of sparsity in vocabulary given the small size of the AMR corpus, as shown in Table 2.
158	1	We found that the majority of pairs of AMR edges (57.6%) always occurred in the same relative order, therefore revealing no extra generation order information.8 Of the examples corresponding to edge pairs that showed variation, 70.3% appeared in an order consistent with the order they were realized in the sentence.
167	1	In the third example there are greater parts of the graph that are missing, such as the whole sub-graph headed by expert.
172	1	We applied sequence-to-sequence models to the tasks of AMR parsing and AMR generation, by carefully preprocessing the graph representation and scaling our models via pretraining on millions of unlabeled sentences sourced from Gigaword corpus.
174	1	We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8).
175	13	For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)).
176	70	This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014).
177	69	Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000).
