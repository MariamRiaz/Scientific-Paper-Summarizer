14	32	While most previous research has been devoted to any one of these two tasks, our paper presents, to our knowledge, the first approach for learning mappings between object annotations and textual descriptions.
25	23	We evaluate OBJ2TEXT in the task of object layout captioning, and image captioning.
29	40	For the first task, object layouts are derived from ground-truth bounding box annotations, and in the second task object layouts are obtained using the outputs of an object detector over the input image.
49	319	In this section we describe our base OBJ2TEXT model for encoding object layouts to produce text (section 4.1), as well as two further variations to use our model to generate captions for real images: OBJ2TEXT-YOLO which uses the YOLO object detector (Redmon and Farhadi, 2017) to generate layouts of object locations from real images (section 4.2), and OBJ2TEXT-YOLO + CNN-RNN which further combines the previous model with an encoder-decoder image captioning which uses a convolutional neural network to encode the image (section 4.3).
50	171	OBJ2TEXT is a sequence-to-sequence model that encodes an input object layout as a sequence, and decodes a textual description by predicting the next word at each time step.
54	12	At inference time we encode an input layout 〈o, l〉 into its representation hL, and sample a sentence word by word based on p(st|hL, s<t) as computed by the decoder in time-step t. Finding the optimal sentence s∗ = arg maxs p(s|hL) requires the evaluation of an exponential number of sentences as in each time-step we have K number of choices for a word vocabulary of size K. As a common practice for an approximate solution, we follow (Vinyals et al., 2015) and use beam search to limit the choices for words at each time-step by only using the ones with the highest probabilities.
55	12	Encoder: The encoder at each time-step t takes as input a pair 〈ot, lt〉, where ot is the object category encoded as a one-hot vector of size V , and lt = [Bxt , B y t , B w t , B h t ] is the location configuration vector that contains left-most position, topmost position, and the width and height of the bounding box corresponding to object ot, all normalized in the range [0,1] with respect to input image dimensions.
56	51	ot and lt are mapped to vectors with the same size k and added to form the input xt to one time-step of the LSTM-based encoder as follows: xt = Woot + (Wllt + bl), xt ∈ Rk, (3) in which Wo ∈ Rk×V is a categorical embedding matrix (the word encoder), and Wl ∈ Rk×4 and bias bl ∈ Rk are parameters of a linear transformation unit (the object location encoder).
58	12	(4) We use the last hidden state vector hL = heT1 as the encoded representation of the input layout 〈ot, lt〉 to generate the corresponding description s. Decoder: The decoder takes the encoded layout hL as input and generates a sequence of multinomial distributions over a vocabulary of words using an LSTM neural language model.
60	78	By setting hd−1 = 0 and cd−1 = 0 for the initial hidden state and cell state, the layout representation is encoded into the decoder network at the 0 time step as a regular input: hd0 = LSTM(h d −1, hL; W2).
63	13	This model takes an image as input, extracts an object layout (object categories and locations) with a state-of-the-art object detection model YOLO (Redmon and Farhadi, 2017), and uses OBJ2TEXT as described in section 4.1 to generate a natural language description of the input layout and hence, the input image.
74	32	The qualities of generated descriptions are evaluated using both human evaluations and automatic metrics.
92	14	For this, we setup a task on Amazon Mechanical Turk where users are presented with an image and two alternative captions, and they have to choose the caption that best describes the image.
98	18	These results show that our model is effectively using both object locations and counts to generate better captions, and absence of any one of these two cues affects performance.
102	16	These results indicate that 1) perhaps not surprisingly, object counts is useful for generating better quality descriptions, and 2) object location information when properly encoded, is an important cue for generating more accurate descriptions.
111	66	These results show that meaningful descriptions could be generated solely based on object categories and locations information, even without access to color and texture input.
113	31	These results show that explicitly encoded object counts and location information, which is often overlooked in traditional image captioning approaches, could boost the performance of existing models.
114	16	Intuitively, object lay- out and visual features are complementary: neural network models for visual feature extraction are trained on a classification task where object-level information such as number of instances and locations are ignored in the objective.
115	28	Object layouts on the other hand, contain categories and their bounding-boxes but don’t have access to rich image features such as image background, object attributes and objects with categories not present in the object detection vocabulary.
116	21	Figure 5 provides a three-way comparison of captions generated by the three image captioning models, with preferred captions by human evaluators annotated in bold text.
117	44	Analysis on actual outputs gives us insights into the benefits of combing object layout information and visual features obtained using a CNN.
118	100	Our OBJ2TEXT-YOLO model makes many mistakes because of lack of image context information since it only has access to object layout, while CNN-RNN makes many mistakes because the visual recognition model is imperfect at predicting the correct content.
120	19	In this work we only explored encoding spatial information with object labels, but object la- bels could be readily augmented with rich semantic features that are more detailed descriptions of objects or image regions.
121	31	For example, the work of You et al. (2016) and Yao et al. (2016b) showed that visual features trained with semantic concepts (text entities mentioned in captions) instead of object labels is useful for image captioning, although they didn’t consider encoding semantic concepts with spatial information.
123	182	In addition, the fusion of object counts and spatial information with CNN visual features could in principle benefit other vision and language tasks such as visual question answering.
124	58	We leave these possible extensions as future work.
125	132	We introduced OBJ2TEXT, a sequence-tosequence model to generate visual descriptions for object layouts where only categories and locations are specified.
126	65	Our proposed model shows that an orderless visual input representation of concepts is not enough to produce good descriptions, but object extents, locations, and object counts, all contribute to generate more accurate image descriptions.
127	224	Crucially we show that our encoding mechanism is able to capture useful spatial information using an LSTM network to produce image descriptions, even when the input is provided as a sequence rather than as an explicit 2D representation of objects.
128	138	Additionally, using our proposed OBJ2TEXT model in combination with an existing image captioning model and a robust object detector we showed improved results in the task of image captioning.
