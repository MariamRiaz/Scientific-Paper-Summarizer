27	14	We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks.
29	65	Our capsule network, depicted in Figure 1, is a variant of the capsule networks proposed in Sabour et al. (2017).
38	7	We have described the process by which one feature is extracted from one filter.
40	38	This is the first capsule layer in which the capsules replace the scalar-output feature detectors of CNNs with vector-output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words.
41	16	Suppose pi ∈ Rd denotes the instantiated parameters of a capsule, where d is the dimension of the capsule.
43	21	For each matrix multiplication, we have a window sliding over each N - gram vector denoted as Mi ∈ RB , then the corresponding N -gram phrases in the form of capsule are produced with pi = (W b)TMi.
44	11	The filter W b multiplies each N -gram vector in {Mi}L−K1+1i=1 with stride of 1 to produce a column-list of capsules p ∈ R(L−K1+1)×d, each capsule pi ∈ Rd in the column-list is computed as pi = g(W bMi + b1) (3) where g is nonlinear squash function through the entire vector, b1 is the capsule bias term.
49	10	The first one shares weights W t1 ∈ RN×d×d across child capsules in the layer below, where N is the number of parent capsules in the layer above.
53	24	For each potential parent, the capsule network can increase or decrease the connection strength by dynamic routing, which is more effective than the primitive routing strategies such as max-pooling in CNN that essentially detects whether a feature is present in any position of the text, but loses spatial information about the feature.
54	63	We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules: Orphan Category Inspired by Sabour et al. (2017), an additional “orphan” category is added to the network, which can capture the “background” information of the text such as stop words and the words that are unrelated to specific categories, helping the capsule network model the child-parent relationship more efficiently.
56	92	Leaky-Softmax We explore Leaky-Softmax Sabour et al. (2017) in the place of standard softmax while updating connection strength between the children capsules and their parents.
63	12	In this layer, each capsule is connected only to a local region K2 × C spatially in the layer below.
64	12	Those capsules in the region multiply transformation matrices to learn child-parent relationships followed by routing by agreement to produce parent capsules in the layer above.
70	19	Here, H is the number of child capsules in the layer below, E is the number of categories plus an extra orphan category.
71	10	We explore two capsule architectures (denoted as Capsule-A and Capsule-B) to integrate these four components in different ways, as depicted in Figure 2.
73	12	All the other layers are capsule layers starting with a B × d primary capsule layer with 32 filters (C = 32), followed by a 3 × C × d × d (K2 = 3) convolutional capsule layer with 16 filters (D = 16) and a fully connected capsule layer in sequence.
87	5	In our experiments, the evaluation metric is classification accuracy.
90	4	In particular, our model substantially and consistently outperforms the simple deep neural networks such as LSTM, Bi-LSTM and CNN-rand by a noticeable margin on all the experimental datasets.
103	4	In this section, we investigate the capability of capsule network on multi-label text classification by using only the single-label samples as training data.
106	22	This dataset consists of 10,788 documents from the Reuters financial newswire service, where each document contains either multiple labels or a single label.
108	10	For dev and training, we only use the single-label documents in the Reuters dev and training sets.
111	12	Following (Sorower, 2010), we adopt Micro Averaged Precision (Precision), Micro Averaged Recall (Recall) and Micro Averaged F1 scores (F1) as the evaluation metrics for multi-label text classification.
113	6	In addition, we also measure the Exact Match Ratio (ER) which considers partially correct prediction as incorrect and only counts fully correct samples.
119	38	In addition, the good results on Reuters-Full also indicate that the capsule network has robust superiority over competitors on single-label documents.
121	62	The connection strength shows the importance of each primary capsule for text categories, acting like a parallel attention mechanism.
128	8	The histograms are used to show the intensity of connection strengths between primary capsules and the fully connected capsules, as shown in Table 6 (bottom line).
130	46	The routing procedure correctly routes the votes into the Interest Rates and Money/Foreign Exchange categories.
132	17	From Figure 3, we observe that the Capsule-B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 iteration.
156	8	Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the disturbance of noisy capsules.
158	47	More importantly, capsule networks also show significant improvement when transferring single-label to multi-label text classifications over the co baseline methods.
