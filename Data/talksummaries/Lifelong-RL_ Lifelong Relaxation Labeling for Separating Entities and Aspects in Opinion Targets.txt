0	31	A core problem of opinion mining or sentiment analysis is to identify each opinion/sentiment target and to classify the opinion/sentiment polarity on the target (Liu, 2012).
1	44	For example, in a review sentence for a car, one wrote “Although the engine is slightly weak, this car is great.” The person is positive (opinion polarity) about the car (opinion target) as a whole, but slightly negative (opinion polarity) about the car’s engine (opinion target).
4	25	“Engine” in the above sentence is just one aspect of the car, while “this car” refers to the whole car.
5	26	Note that in (Liu, 2012), an entity is called a general aspect.
16	38	We only focus on target classification after the targets have been extracted.
17	14	Note that an entity here can be a named entity, a prod- 225 uct category, or an abstract product (e.g., “this machine” and “this product”).
19	28	An aspect is a part or attribute of an entity, e.g., “battery” and “price” of the entity “camera”.
32	31	It has also saved all the graphs and classification results from those past domains in a Knowledge Base (KB).
65	25	Relaxation Labeling (RL) is an unsupervised graphbased label propagation algorithm that works iteratively.
99	34	We use two kinds of text clues, called type modifiers M(t) and relation modifiers MR to compute the initial label distribution P (L(ti)) and conditional label distribution P (L(ti)|L(tj)) respectively.
105	26	Relation Modifier: Given two targets, ti and tj , we use Mtj (ti) to denote the relation modifier that the label of target ti is influenced by the label of target tj .
108	15	For example, in “price and service”, “and service” indicates a conjunction modifier for “price” and vice versa.
109	26	Entity-aspect modifier mA|E : A possessive expression indicates an entity and an aspect relation.
120	27	The initial label probability distribution of target t is computed based on CmE (t), i.e., P 0(L(t)) = { PmE (L(t)) if CmE (t) > α PmA(L(t)) if CmE (t) ≤ α (3) Here, we have two pre-defined distributions: PmE and PmA , which have a higher probability on entity and aspect respectively.
125	17	They are filled with values to model the label influence from neighbors and can be found in Section 6.
127	25	Due to the fact that the review corpus du+1 in the current task domain may not be very large and that we use high quality syntactic rules to extract relations to build the graph to ensure precision, the number of relations extracted can be small and insufficient to produce a graph that is information rich with accurate initial probabilities.
128	30	We thus apply LML to help using knowledge learned in the past.
130	43	Our prior knowledge includes type modifiers, relation modifiers and labels of targets obtained from past domains in D. Each record in the KB is stored as a 9-tuple: (d, ti, tj ,M d(ti),M d(tj), C d m,tj (ti), C d m,ti(tj), L d(ti), L d(tj)) where d ∈ D is a past domain; ti and tj are two targets; Md(ti), Md(tj) are their type modifiers, Cdm,tj (ti) and C d m,ti(tj) are counts for relation modifiers; Ld(ti) and Ld(tj) are labels decided by RL.
131	20	For example, the sentence “This camera’s battery is good” forms: (d, camera, battery,mE ,mA, CmE|A,battery(camera) = 1, CmA|E ,camera(battery) = 1, entity, aspect) .
132	35	It means that in the past domain d, “camera” and “battery” are extracted targets.
135	42	The pattern “camera’s battery” contributes one count for both relation modifiers CmE|A,battery(camera) and CmA|E ,camera(battery).
136	22	RL has labeled “camera” as entity and “battery” as aspect in d. The next two subsections present how to use the knowledge in the KB to improve the initial assignments for the label distributions, conditional label distributions and neighborhood weight distributions in order to achieve better final labeling/classification results for the current/new domain du+1.
140	29	Label Consistency Check: Since RL makes mistakes, we need to ensure that relation modifiers in a record in the KB are consistent with target labels in that past domain.
179	18	For example, after labeling domains like “Cellphone”, “Laptop”, “Tablet,” and “E-reader”, we may have a good sense that “camera” is likely to be an aspect.
216	17	But our type modifier (TM) does that, i.e., if an opinion target appears after “this” or “these” in at least two sentences, TM labels the target as an entity; otherwise an aspect.
254	15	This paper studied the problem of classifying opinion targets into entities and aspects.
255	28	To the best of our knowledge, this problem has not been attempted in the unsupervised opinion target extraction setting.
256	122	But this is an important problem because without separating or classifying them one will not know whether an opinion is about an entity as a whole or about a specific aspect of an entity.
257	74	This paper proposed a novel method based on relaxation labeling and the paradigm of lifelong machine learning to solve the problem.
258	29	Experimental results showed the effectiveness of the proposed method.
