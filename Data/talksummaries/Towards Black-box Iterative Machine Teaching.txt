14	18	We call such problem black-box machine teaching.
16	13	This setting is interesting in the sense that it can both relax the assumptions for iterative machine teaching and improve our understanding on human learning.
23	2	To validate our theoretical findings, we conduct extensive experiments on both synthetic data and real image data.
37	16	The teacher model observes a sample A (e.g. image, text, etc.)
42	20	The learner uses a linear model 〈w, x̃〉 where w is its model parameter and updates it with SGD (if guided by a passive teacher).
45	77	For simplicity, we assume there exists a unknown one-to-one mapping G from the teacher’s feature space to the student’s feature space such that x̃=G(x).
46	11	However, the conclusions in this paper are also applicable to injective mappings.
47	20	Unless specified, we assume that y = ỹ by default.
48	122	In each iteration, the teacher will provide a training example to the learner and the learner will update its model using this example.
49	33	The teacher cannot directly observe the model parameter w of the student.
50	19	In this paper, the active teacher is allowed to query the learner with a few examples every certain number of iterations.
51	16	The learner can only return to the teacher its prediction 〈wt, x̃〉 in the regression scenario, its predicted label sign(〈wt, x̃〉) or confidence score S(〈wt, x̃〉) in the classification scenario, where wt is the student’s model parameter at t-th iteration and S(·) is some nonlinear function.
52	20	Note that the teacher and student preserve the same loss function `(·, ·).
53	38	Similar to (Liu et al., 2017a), we consider three ways for the teacher to provide examples to the learner: Synthesis-based teaching.
54	19	In this scenario, the space of provided examples is X = {x ∈ Rd, ‖x‖ ≤ R} Y = R (Regression) or {−1, 1} (Classification).
56	5	It is straightforward to add them back.
57	7	of provided examples is (αi ∈ R) X = { x| ‖x‖ ≤ R, x = Σki=1αixi, xi ∈ D } ,D = {x1, .
59	30	This scenario further restrict the knowledge pool for samples.
60	123	The teacher can pick examples from X ×Y: X = {x| ‖x‖ ≤ R, x = γxi, xi ∈ D, γ ∈ R},D = {x1, .
62	3	To address the cross-space iterative machine teaching, we propose the active teaching algorithm, which actively queries its student for its prediction output.
64	41	Then without loss of generality, we will discuss three specific examples: least square regression (LSR) learner for regression, logistic regression (LR) and support vector machine (SVM) learner for classification (Friedman et al., 2001).
66	3	The student will return its predictions to the teacher.
70	7	For example, we usually have F (z) = z for regression and F (z)=sign(z) or F (z)= 11+exp(−z) for classification.
71	6	Based on our assumption that there is an unknown mapping from teacher’s feature to student’s feature, there also exists a mapping from the model parameters of the teacher to those of the student.
72	120	These active queries enables the teacher to estimate the student’s corresponding model parameter “in the teacher’s space” and maintain a virtual learner, the teacher’s estimation of the real learner, in its own space.
73	41	The teacher will decide which example to provide based on its current virtual learner model.
74	47	The ideal virtual learner v will have the same prediction output as the real learner, i.e. 〈v, x〉=〈w, x̃〉 where x̃=G(x).
75	16	Equivalently, v=G>(w) always holds for the ideal virtual learner, where G> is the conjugate mapping of G. Note that for the purpose of analysis, we assume that G is a generic linear operator, though our analysis can easily extends to general cases.
76	73	In fact, one of the most important challenges in active teaching is to recover a virtual student that approximates the real leaner as accurately as possible.
77	77	The estimation error of the teacher may affect the quality of training examples that the teacher provides for the real learner.
